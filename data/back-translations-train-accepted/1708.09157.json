{"id": "1708.09157", "review": {"conference": "acl", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2017", "title": "Cross-lingual, Character-Level Neural Morphological Tagging", "abstract": "Even for common NLP tasks, sufficient supervision is not available in many languages -- morphological tagging is no exception. In the work presented here, we explore a transfer learning scheme, whereby we train character-level recurrent neural taggers to predict morphological taggings for high-resource languages and low-resource languages together. Learning joint character representations among multiple related languages successfully enables knowledge transfer from the high-resource languages to the low-resource ones, improving accuracy by up to 30%", "histories": [["v1", "Wed, 30 Aug 2017 08:14:34 GMT  (729kb,D)", "http://arxiv.org/abs/1708.09157v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ryan cotterell", "georg heigold"], "accepted": true, "id": "1708.09157"}, "pdf": {"name": "1708.09157.pdf", "metadata": {"source": "CRF", "title": "Cross-lingual, Character-Level Neural Morphological Tagging", "authors": ["Ryan Cotterell", "Georg HeigoldK"], "emails": ["@Center", "ryan.cotterell@jhu.edu", "georg.heigold@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "For the majority of languages in the world, however, sufficient large-scale annotation is not available, and obtaining it would often be unfeasible. Accordingly, an important way forward in a resource-poor NLP is to develop methods that allow for the creation of high-quality tools from small amounts of data. In this work, we focus on transfer learning - we form a recurring neural tagger for a resource-poor language together with a tagger for a related high-resource language. Forcing models to share characteristics between languages enables large increases in precision while maintaining (or even improving) a recurring neural tagger for a high-resource language. Recurring neural networks represent the state of art for a variety of tasks in the NLP."}, {"heading": "2 Morphological Tagging", "text": "In fact, the situation is that most people are able to survive on their own and that they are able to survive on their own. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "3 Character-Level Neural Transfer", "text": "Our formulation of transfer learning builds on the work of multi-task learning (Caruana, 1997; Collobert et al., 2011b). We treat each language as a task and train a common model for all tasks. First, we discuss the current state of the art in morphological marking: a recurring neural network at the sign level. Afterwards, we examine three extensions of the architecture that enable the transfer learning scenario. All of our proposals force the embedding of characters for both the source and target languages to share the same vector space, but contain different mechanisms through which the model can learn language-specific features."}, {"heading": "3.1 Character-Level Neural Networks", "text": "It is not the first time that we have had to deal with the question of whether it is a \"new\" or \"new\" model, but rather a \"new,\" \"new\" or \"new\" or \"new\" or \"new.\" It is a \"new,\" \"new\" or \"new,\" \"new,\" \"new\" or \"new.\" It is a \"new,\" \"new,\" \"new\" or \"new,\" \"new\" or \"new,\" \"new,\" new, \"new\" or \"new,\" \"new\" or \"new,\" new \"or\" new, \"new\" or \"new,\" new \"or\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new,\" new, \"new, new,\" new, \"new,\" new, new, \"new, new,\" new, new, \"new, new,\" new, new, new, \"new, new, new, new,\" new, new \"new, new\" new, new \"new, new\" new, new \"new\" new, new \"new, new\" new, new \"new, new, new\" new \"new\" new \"new, new\" new, new \"new, new\" new, new \"new\" new, new \"new\" new, new \"new\" new, new \"new\" new, new \"new\" new, new \"new\" new, new \"new\" new, new \"new\" new, new \"new\" new, new \"new\" new \"new, new\" new \"new, new\" new \"new, new\" new \"new\" new, new \"new\" new, new \"new\" new \"new\" new \"new, new\" new, new \"new\" new, new, new \"new\" new \"new\" new \"new, new\" new, new, new \"new, new, new\" new, new, new, \"new\" new \"new, new\" \"new\" new, new, \"new, new, new,\" new, \"new\" \"\" new, new, new \"\" new, \"new\" \"\" new, \"new\" \"\" \"new, new,\" \"new,\" new, \"new,\" new, new, new, \"\" new, new, \"new,\" new \""}, {"heading": "3.2 Cross-Lingual Morphological Transfer as Multi-Task Learning", "text": "We are trying to learn a set of split character embeddings for taggers in both languages. (We are trying to define a set of split character embeddings for taggers in both languages together by optimizing a common loss function, which is the highly resource specific LSTMsoftmaxlineararp (t1) p character (tN) p character (tN) v1 vN...... (a) vanilla architecture for the exact values used in the experimentation.softmaxlinear LSTMsoftmaxlinearp (t1) p character (tN) v1 vN...... (b) language languages, the morphological tagging.softmaxlinearearp (t1) p languages."}, {"heading": "4 Experiments", "text": "Empirically, we ask our architectures three questions: i) How well can we transfer morphological tagging models in any architecture from languages with high resources to languages with low resources? (Does one of the three languages outperform the others?) ii) How much annotated data in the language with low resources do we need? iii) How closely do languages have to be related to each other to achieve a good transfer?"}, {"heading": "4.1 Experimental Languages", "text": "We experiment with language families: Romance (Indo-European), North Germanic (Indo-European), Slavic (Indo-European) and Uralic. In the Romance subgroup of the larger Indo-European family we experiment with Catalan (ca), French (fr), Italian (it), Portuguese (pt), Romanian (ro) and Spanish (es). In the North Germanic family we experiment with Danish (da), Norwegian (no) and Swedish (sv). In the Slavic family we experiment with Bulgarian (bg), Czech (bg), Polish (pl), Russian (ru), Slovak (sk) and Ukrainian (UK). Finally, in the Uralian family we experiment with Estonian (et), Finnish (fi) and Hungarian (hu)."}, {"heading": "4.2 Datasets", "text": "We use the morphological tagging datasets of the Universal Dependencies (UD) tree banks (the concatenation of the fourth and sixth columns of the file format) (Nivre et al., 2016). We list the size of the training, development and testing stages of the UD tree banks that we have used in Table 2. We also list the number of unique morphological tags in each language in Table 3, which serves as an approximate measure of the morphological complexity of each language. Crucially, the data is commented consistently across languages so that words in the different languages have the same syntactic-semantic function (see \u00a7 2 for a discussion)."}, {"heading": "4.3 Baselines", "text": "In our work we look at two baselines: First, we look at the MARMOT tagger (Mu \u00bc ller et al., 2013), currently the most powerful non-neural model. The source code for MARMOT is freely available online, 5 which allows us to conduct fully controlled experiments with this model. Second, we look at the alignment-based projection approach of Buys and Botha (2016).6 We discuss each of the two baselines one after the other."}, {"heading": "4.3.1 Higher-Order CRF Tagger", "text": "The MARMOT tagger is the leading non-neural approach to morphological marking. This baseline is important because non-neural, feature-based approaches have proven empirically to be more efficient in the sense that their learning curves tend to be steeper. Therefore, in a resource-constrained environment, it would be negligent not to consider a function-based approach. Note that this is not a transfer approach, but only the use of resource-poor data."}, {"heading": "4.3.2 Alignment-based Projection", "text": "The projection approach of Buys and Botha (2016) offers an alternative method of transfer learning. The idea is to construct pseudo-annotations for bittexts taking into account cross-lingual alignments (Och and Ney, 2003) and then train a standard tagger based on the projected annotations. The specific tagger is the WSABIE model by Weston et al. (2011), which - like our approach - is a discriminatory first-order neural model. However, unlike ours, its network is flat. We compare the two methods more precisely in \u00a7 6."}, {"heading": "4.3.3 Architecture Study", "text": "A primary goal of our experiments is to determine which of our three suggested neural transfer techniques is superior. Although our experiments focus on morphological marking, these architectures are more general in nature, as they may be be5http: / / cistern.cis.lmu.de / marmot / 6We do not have access to the code since the model was developed in the industry, so we compare the numbers cited in the original paper, as well as additional numbers provided to us by the first author in personal communication. Strictly speaking, the numbers will not be comparable, but we hope that they provide insight into the relative performance of the tagger. Easily applicable to other tasks, such as parsing or machine translation. Additionally, we examine the feasibility of multi-source transfer, i.e. the case in which we have multiple source languages. All of our architectures generalize without complications to the multi-source case."}, {"heading": "4.4 Experimental Details", "text": "We train our models under the following conditions: evaluation metrics. We evaluate the average accuracy per token used for both POS tagging and morphological tagging and per feature F1 as in Buys and Botha (2016). The per feature F1 calculates a key F k1 for each key in the tags of the target language by asking if the key attribute pair ki = vi is contained in the predicted tag. Afterwards, the key-specific F k1 values are averaged equally. Note that F1 is a more flexible metric, as there is partial recognition that some of the attributes in the bundle are correct, where accuracy does not matter. Hyper parameters. Our networks are four layers deep (two LSTM layers for the character Embedder, i.e. to calculate vi and two LSTM layers for the tagger, i.e. to make some of the attributes in the bundle correct, where accuracy does not matter."}, {"heading": "5 Results and Discussion", "text": "In fact, it is so that we are able to trump ourselves. (...) It is not so that we are able to trump ourselves. (...) It is not so that we are able to trump ourselves. (...) It is so that we are able to trump ourselves. (...) It is so that we are able to trump ourselves. (...) It is so that we are able to trump ourselves. (...) It is so that we are able to trump ourselves. (...) It is so. (...) It is so. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (.).). (...). (...). (.).). (...). (.). (.).). (.). (...). (.).). (.). (.).). (. (.). (.).).). (...). (.). (.). (...).). (. (.).).). (. (.).).). (. (...). (...). (. (.). (.). (.).).). (.). (. (.). (.).). (.).). (.).). (...). (.). (...).).). (...).). (. (...).). (...).).). (...). (. (...). (.).). (.).).). (.). ("}, {"heading": "6 Related Work", "text": "To facilitate intellectual digestion, we divide the discussion of related work thematically into three parts."}, {"heading": "6.1 Alignment-Based Distant Supervision.", "text": "Most cross-language work in the NLP - with an emphasis on morphology or otherwise - has focused on indirect monitoring rather than transferring learning, and the goal in such a regime is to provide loud labels obtained through direct communication with the authors. First, we used a slightly newer version of UD to include more languages: we used v2 while they used v1.2. There are slight differences in the morphological labels used between these versions. Also, in the | Dt | = 1000 setting, we are trained on significantly more data than the models in Buys and Botha (2016). A much fairer comparison is with our models with | Dt | = 100. Also, we compare their method with their standard (not) setup. This method is fair in that we evaluate in the same way, but it disadvantages their approach, which cannot predict tags that are not in the source language."}, {"heading": "6.2 Character-level NLP.", "text": "Our work also follows a current trend in NLP, where traditional neural representations at the word level are replaced by representations at the sign level for a variety of tasks, such as POS tagging dos Santos and Zadrozny (2014), parsing (Ballesteros et al., 2015), language modeling (Ling et al., 2015), mood analysis (Zhang et al., 2015), and the tagger by Heigold et al. (2017), upon whose work we build. Our work is also related to current work on morphological generation at the sign level using neural architectures (Faruqui et al., 2016; Rastogi et al., 2016)."}, {"heading": "6.3 Neural Cross-lingual Transfer in NLP.", "text": "In terms of methodology, however, our proposal is similar to recent work in the fields of language and machine translation - we discuss it one after the other. In language recognition, Heigold et al. (2013) trains a lingual neural acoustic model for five Romance languages. The architecture is similar to our multilingual Softmax approach. Dependency analyses similarly benefit from linguistic learning (Guo et al., 2015, 2016).In neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), recent work (Firat et al., 2016; Zoph and Knight, 2016; Johnson et al., 2016) has explored the possibility of jointly training translation models for a variety of languages. Our work addresses a different task, but the underlying philosophical motivation is similar, i.e. it addresses resource-poor NLP through multi-task transfer learning. Can et al. (2017) offer a similar method for linguistic transfer in morphological translation."}, {"heading": "7 Conclusion", "text": "We have presented three recurrent neural network architectures at character level for multi-task transfer of morphological markers. We have presented an empirical evaluation of the technique across 18 languages from four different language families, showing widespread applicability of the method. We have found that transferring morphological markers is a highly practical endeavor between related languages, and generally, the closer languages move, the easier the transfer of morphology becomes. Our technique exceeds two strong baselines proposed in previous work. In addition, we are defining standard, low-resource training splits in UD for future research in the field of low-resource morphological marking. Future work should focus on extending the neural morphological marker to a common lemmatizer (Mu \u00bc ller et al., 2015) and evaluating its functionality in the resource-poor environment."}, {"heading": "Acknowledgements", "text": "We would like to thank Jan Buys and Jan Botha for helping us to compare the figures reported in their paper. We would like to thank Schloss Dagstuhl Leibniz-Zentrum f\u00fcr Informatik for hosting the event at which work on this paper began."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["Miguel Ballesteros", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Ballesteros et al\\.,? 2015", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Cross-lingual morphological tagging for low-resource languages", "author": ["Jan Buys", "Jan A. Botha."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1954\u20131964, Berlin, Germany. Association for Com-", "citeRegEx": "Buys and Botha.,? 2016", "shortCiteRegEx": "Buys and Botha.", "year": 2016}, {"title": "Multitask learning", "author": ["Rich Caruana."], "venue": "Machine Learning, 28(1):41\u201375.", "citeRegEx": "Caruana.,? 1997", "shortCiteRegEx": "Caruana.", "year": 1997}, {"title": "Torch7: A Matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet."], "venue": "BigLearn, NIPS Workshop.", "citeRegEx": "Collobert et al\\.,? 2011a", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa."], "venue": "Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011b", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "The SIGMORPHON 2016 shared task\u2014 morphological reinflection", "author": ["Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden."], "venue": "Proceedings of the 14th SIGMORPHON Workshop on Computational", "citeRegEx": "Cotterell et al\\.,? 2016", "shortCiteRegEx": "Cotterell et al\\.", "year": 2016}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "author": ["Dipanjan Das", "Slav Petrov."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL),", "citeRegEx": "Das and Petrov.,? 2011", "shortCiteRegEx": "Das and Petrov.", "year": 2011}, {"title": "Transitionbased dependency parsing with stack long shortterm memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual", "citeRegEx": "Dyer et al\\.,? 2015", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Morphological inflection generation using character sequence to sequence learning", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Com-", "citeRegEx": "Faruqui et al\\.,? 2016", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Automatically inducing a part-of-speech tagger by projecting from multiple source languages across aligned corpora", "author": ["Victoria Fossum", "Steven P. Abney."], "venue": "Second International Joint Conference on Natural Language Processing (IJCNLP), pages 862\u2013", "citeRegEx": "Fossum and Abney.,? 2005", "shortCiteRegEx": "Fossum and Abney.", "year": 2005}, {"title": "Learning a part-of-speech tagger from two hours of annotation", "author": ["Dan Garrette", "Jason Baldridge."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies", "citeRegEx": "Garrette and Baldridge.,? 2013", "shortCiteRegEx": "Garrette and Baldridge.", "year": 2013}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18(5-6):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Cross-lingual dependency parsing based on distributed representations", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the", "citeRegEx": "Guo et al\\.,? 2015", "shortCiteRegEx": "Guo et al\\.", "year": 2015}, {"title": "A representation learning framework for multi-source transfer parsing", "author": ["Jiang Guo", "Wanxiang Che", "David Yarowsky", "Haifeng Wang", "Ting Liu."], "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI), pages 2734\u20132740.", "citeRegEx": "Guo et al\\.,? 2016", "shortCiteRegEx": "Guo et al\\.", "year": 2016}, {"title": "Tagging inflective languages: Prediction of morphological categories for a rich structured tagset", "author": ["Jan Haji\u010d", "Barbora Hladk\u00e1."], "venue": "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th Inter-", "citeRegEx": "Haji\u010d and Hladk\u00e1.,? 1998", "shortCiteRegEx": "Haji\u010d and Hladk\u00e1.", "year": 1998}, {"title": "An extensive empirical evaluation of character-based morphological tagging for 14 languages", "author": ["Georg Heigold", "Guenter Neumann", "Josef van Genabith."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Compu-", "citeRegEx": "Heigold et al\\.,? 2017", "shortCiteRegEx": "Heigold et al\\.", "year": 2017}, {"title": "Neural morphological tagging from characters for morphologically rich languages", "author": ["Georg Heigold", "G\u00fcnter Neumann", "Josef van Genabith."], "venue": "CoRR, abs/1606.06640.", "citeRegEx": "Heigold et al\\.,? 2016", "shortCiteRegEx": "Heigold et al\\.", "year": 2016}, {"title": "Multilingual acoustic models using distributed deep neural networks", "author": ["Georg Heigold", "Vincent Vanhoucke", "Andrew Senior", "Patrick Nguyen", "Marc\u2019Aurelio Ranzato", "Matthieu Devin", "Jeffrey Dean"], "venue": "In IEEE International Conference on Acous-", "citeRegEx": "Heigold et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Heigold et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "One-shot neural cross-lingual transfer for paradigm completion", "author": ["Katharina Kann", "Ryan Cotterell", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), Vancouver, Canada. Asso-", "citeRegEx": "Kann et al\\.,? 2017", "shortCiteRegEx": "Kann et al\\.", "year": 2017}, {"title": "Archi (Caucasian \u2013 Daghestanian)", "author": ["Aleksandr E. Kibrik."], "venue": "The Handbook of Morphology, pages 455\u2013476. Blackwell Oxford.", "citeRegEx": "Kibrik.,? 1998", "shortCiteRegEx": "Kibrik.", "year": 1998}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning (ICML),", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis."], "venue": "Proceedings of the 2015", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "On the state of the art of evaluation in neural language models", "author": ["G\u00e1bor Melis", "Chris Dyer", "Phil Blunsom."], "venue": "arXiv preprint arXiv:1707.05589.", "citeRegEx": "Melis et al\\.,? 2017", "shortCiteRegEx": "Melis et al\\.", "year": 2017}, {"title": "Joint lemmatization and morphological tagging with Lemming", "author": ["Thomas M\u00fcller", "Ryan Cotterell", "Alexander Fraser", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages", "citeRegEx": "M\u00fcller et al\\.,? 2015", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2015}, {"title": "Efficient higher-order CRFs for morphological tagging", "author": ["Thomas M\u00fcller", "Helmut Schmid", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 322\u2013332, Seattle, Washington,", "citeRegEx": "M\u00fcller et al\\.,? 2013", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 2013}, {"title": "Linguistic typology requires crosslinguistic formal categories", "author": ["Frederick J. Newmeyer."], "venue": "Linguistic Typology, 11(1):133\u2013157.", "citeRegEx": "Newmeyer.,? 2007", "shortCiteRegEx": "Newmeyer.", "year": 2007}, {"title": "Universal dependencies v1: A multilingual treebank collection", "author": ["Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Haji\u010d", "Christopher D. Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira"], "venue": null, "citeRegEx": "Nivre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2016}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz Josef Och", "Hermann Ney."], "venue": "Computational Linguistics, 29(1):19\u201351.", "citeRegEx": "Och and Ney.,? 2003", "shortCiteRegEx": "Och and Ney.", "year": 2003}, {"title": "Tagging and morphological disambiguation of Turkish text", "author": ["Kemal Oflazer", "\u00cclker Kuru\u00f6z."], "venue": "Proceedings of the Fourth Conference on Applied Natural Language Processing, pages 144\u2013149. Association for Computational Linguistics.", "citeRegEx": "Oflazer and Kuru\u00f6z.,? 1994", "shortCiteRegEx": "Oflazer and Kuru\u00f6z.", "year": 1994}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan T. McDonald."], "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC), pages 2089\u20132096.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Weighting finite-state transductions with neural context", "author": ["Pushpendre Rastogi", "Ryan Cotterell", "Jason Eisner."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language", "citeRegEx": "Rastogi et al\\.,? 2016", "shortCiteRegEx": "Rastogi et al\\.", "year": 2016}, {"title": "Conll 2017 shared task: Multilingual parsing from raw text to universal dependencies", "author": ["Li."], "venue": "Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1\u201319, Vancouver, Canada. Association", "citeRegEx": "Li.,? 2017", "shortCiteRegEx": "Li.", "year": 2017}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Jake Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}, {"title": "Multi-source neural translation", "author": ["Barret Zoph", "Kevin Knight."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 30\u201334, San", "citeRegEx": "Zoph and Knight.,? 2016", "shortCiteRegEx": "Zoph and Knight.", "year": 2016}], "referenceMentions": [{"referenceID": 33, "context": ", multilingual part-of-speech tagging (Plank et al., 2016), syntactic parsing (Dyer et al.", "startOffset": 38, "endOffset": 58}, {"referenceID": 8, "context": ", 2016), syntactic parsing (Dyer et al., 2015; Zeman et al., 2017), morphological paradigm completion (Cotterell et al.", "startOffset": 27, "endOffset": 66}, {"referenceID": 25, "context": ", 2016, 2017) and language modeling (Sundermeyer et al., 2012; Melis et al., 2017); recently, such models have also improved morphological tagging (Heigold et al.", "startOffset": 36, "endOffset": 82}, {"referenceID": 5, "context": "If the learned representations for all of the tasks are embedded jointly into a shared vector space, the various tasks reap benefits from each other and often performance improves for all (Collobert et al., 2011b).", "startOffset": 188, "endOffset": 213}, {"referenceID": 22, "context": "10,000 (Kibrik, 1998).", "startOffset": 7, "endOffset": 21}, {"referenceID": 31, "context": "This bundle of key-attributes pairs is typically termed a morphological tag and we may view the goal of morphological tagging to label each word in its sentential context with the appropriate tag (Oflazer and Kuru\u00f6z, 1994; Haji\u010d and Hladk\u00e1, 1998).", "startOffset": 196, "endOffset": 246}, {"referenceID": 16, "context": "This bundle of key-attributes pairs is typically termed a morphological tag and we may view the goal of morphological tagging to label each word in its sentential context with the appropriate tag (Oflazer and Kuru\u00f6z, 1994; Haji\u010d and Hladk\u00e1, 1998).", "startOffset": 196, "endOffset": 246}, {"referenceID": 29, "context": "All of the experiments in this paper make use of the universal morphological tag set available in the Universal Dependencies (UD) (Nivre et al., 2016).", "startOffset": 130, "endOffset": 150}, {"referenceID": 32, "context": "POS lends itself nicely to a universal annotation scheme (Petrov et al., 2012) and traditional NER is limited to a small number of cross-linguistically compliant categories, e.", "startOffset": 57, "endOffset": 78}, {"referenceID": 29, "context": "Even universal dependency arcs employ cross-lingual labels (Nivre et al., 2016).", "startOffset": 59, "endOffset": 79}, {"referenceID": 28, "context": "See Newmeyer (2007) for a linguistic treatment of cross-lingual annotation.", "startOffset": 4, "endOffset": 20}, {"referenceID": 3, "context": "Our formulation of transfer learning builds on work in multi-task learning (Caruana, 1997; Collobert et al., 2011b).", "startOffset": 75, "endOffset": 115}, {"referenceID": 5, "context": "Our formulation of transfer learning builds on work in multi-task learning (Caruana, 1997; Collobert et al., 2011b).", "startOffset": 75, "endOffset": 115}, {"referenceID": 17, "context": "Character-level neural networks currently constitute the state of the art in morphological tagging (Heigold et al., 2017).", "startOffset": 99, "endOffset": 121}, {"referenceID": 23, "context": "which may be seen as a 0th order conditional random field (CRF) (Lafferty et al., 2001) with parameter vector \u03b8.", "startOffset": 64, "endOffset": 87}, {"referenceID": 27, "context": "As an aside, it is quite interesting that a model with the factorization in Equation (1) outperforms the MARMOT model (M\u00fcller et al., 2013), which focused on modeling higher-order interactions between the morphological tags, e.", "startOffset": 118, "endOffset": 139}, {"referenceID": 20, "context": "where W \u2208 R|T |\u00d7n is an embedding matrix, b \u2208 R|T | is a bias vector and positional embeddings ei 4 are taken from a concatenation of the output of two long short-term memory recurrent neural networks (LSTMs) (Hochreiter and Schmidhuber, 1997), folded forward and backward, respectively, over a sequence of input vectors.", "startOffset": 209, "endOffset": 243}, {"referenceID": 13, "context": "This constitutes a bidirectional LSTM (Graves and Schmidhuber, 2005).", "startOffset": 38, "endOffset": 68}, {"referenceID": 33, "context": "This architecture is the context bidirectional recurrent neural network of Plank et al. (2016). Finally, we derive each word embedding vector vi from a characterlevel bidirectional LSTM embedder.", "startOffset": 75, "endOffset": 95}, {"referenceID": 30, "context": "This bidirectional LSTM is the sequence bidirectional recurrent neural network of Plank et al. (2016). Note a concatenation of the sequence of character symbols \u3008ci1 , .", "startOffset": 82, "endOffset": 102}, {"referenceID": 17, "context": "We direct the reader to Heigold et al. (2017) for a more in-depth discussion of this and various additional architectures for the computation of vi; the architecture we have presented in Equation (5) is competitive with the best performing setting in Heigold et al.", "startOffset": 24, "endOffset": 46}, {"referenceID": 17, "context": "Combining (a) with the character embeddings in (c) gives the vanilla morphological tagging architecture of Heigold et al. (2017). Combining (a) with (d) yields the language-universal softmax architecture and (b) and (c) yields our joint model for language identification and tagging.", "startOffset": 107, "endOffset": 129}, {"referenceID": 5, "context": "(Collobert et al., 2011b).", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": ", for dependency parsing (Guo et al., 2016), has shared word-level embeddings rather than character-level embeddings.", "startOffset": 25, "endOffset": 43}, {"referenceID": 17, "context": "Next, inspired by the architecture of Heigold et al. (2013), we consider a language-specific softmax layer, i.", "startOffset": 38, "endOffset": 60}, {"referenceID": 29, "context": "We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the 4th and 6th columns of the file format) (Nivre et al., 2016).", "startOffset": 162, "endOffset": 182}, {"referenceID": 27, "context": "First, we consider the MARMOT tagger (M\u00fcller et al., 2013),", "startOffset": 37, "endOffset": 58}, {"referenceID": 2, "context": "Second, we consider the alignment-based projection approach of Buys and Botha (2016).6 We discuss each of the two baselines in turn.", "startOffset": 63, "endOffset": 85}, {"referenceID": 2, "context": "The projection approach of Buys and Botha (2016) provides an alternative method for transfer learn-", "startOffset": 27, "endOffset": 49}, {"referenceID": 30, "context": "The idea is to construct pseudo-annotations for bitext given cross-lingual alignments (Och and Ney, 2003).", "startOffset": 86, "endOffset": 105}, {"referenceID": 30, "context": "The idea is to construct pseudo-annotations for bitext given cross-lingual alignments (Och and Ney, 2003). Then, one trains a standard tagger using the projected annotations. The specific tagger employed is the WSABIE model of Weston et al. (2011), which\u2014like our approach\u2014 is a 0th-order discriminative neural model.", "startOffset": 87, "endOffset": 248}, {"referenceID": 2, "context": "We evaluate using average per token accuracy, as is standard for both POS tagging and morphological tagging, and per feature F1 as employed in Buys and Botha (2016). The per feature F1 calculates a key F k 1 for each key in the target language\u2019s tags by asking if the keyattribute pair ki=vi is in the predicted tag.", "startOffset": 143, "endOffset": 165}, {"referenceID": 4, "context": "We used Torch 7 (Collobert et al., 2011a) to configure the computation graphs implementing the network architectures.", "startOffset": 16, "endOffset": 41}, {"referenceID": 2, "context": "As shown in Table 5 and Table 6, our model outperforms the projection tagger of Buys and Botha (2016) even though our approach does not utilize bitext, large-scale alignment or monolingual corpora\u2014rather, all transfer between languages happens through the forced sharing of characterlevel features.", "startOffset": 80, "endOffset": 102}, {"referenceID": 2, "context": "7 We would like to highlight some issues of comparability with the results in Buys and Botha (2016). Strictly speaking, the results are not comparable and our improvement over their method should be taken with a grain of salt.", "startOffset": 78, "endOffset": 100}, {"referenceID": 2, "context": "We compare on only those languages in Buys and Botha (2016). Note that tag-level accuracy was not reported in the original B&B paper, but was acquired through personal communication with the first author.", "startOffset": 38, "endOffset": 60}, {"referenceID": 12, "context": "We note, however, that this does not necessitate a large number of human annotation hours (Garrette and Baldridge, 2013).", "startOffset": 90, "endOffset": 120}, {"referenceID": 2, "context": "Also, in the |Dt| = 1000 setting, we are training on significantly more data than the models in Buys and Botha (2016). A much fairer comparison is to our models with |Dt| = 100.", "startOffset": 96, "endOffset": 118}, {"referenceID": 11, "context": "While follow-up work (Fossum and Abney, 2005; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2012) has continually demonstrated the efficacy of projecting simple part-of-speech annotations, Buys and Botha (2016) were the first to show the use of bitext-based projection for the training of a morphological tagger for low-resource languages.", "startOffset": 21, "endOffset": 91}, {"referenceID": 7, "context": "While follow-up work (Fossum and Abney, 2005; Das and Petrov, 2011; T\u00e4ckstr\u00f6m et al., 2012) has continually demonstrated the efficacy of projecting simple part-of-speech annotations, Buys and Botha (2016) were the first to show the use of bitext-based projection for the training of a morphological tagger for low-resource languages.", "startOffset": 21, "endOffset": 91}, {"referenceID": 32, "context": "training the tagger in the low-resource language through annotations projected over aligned bitext with a high-resource language. This method of projection was first introduced by Yarowsky and Ngai (2001) for the projection of POS annotation.", "startOffset": 85, "endOffset": 205}, {"referenceID": 2, "context": ", 2012) has continually demonstrated the efficacy of projecting simple part-of-speech annotations, Buys and Botha (2016) were the first to show the use of bitext-based projection for the training of a morphological tagger for low-resource languages.", "startOffset": 99, "endOffset": 121}, {"referenceID": 2, "context": "As we also discuss the training of a morphological tagger, our work is most closely related to Buys and Botha (2016) in terms of the task itself.", "startOffset": 95, "endOffset": 117}, {"referenceID": 2, "context": "Table 6: Comparison of our approach to various baselines for low-resource tagging under F1 to allow for a more complete comparison to the model of Buys and Botha (2016). All architectures presented in this work are used in their multi-source setting.", "startOffset": 147, "endOffset": 169}, {"referenceID": 2, "context": "small mount of seed target language data perform poorly (Buys and Botha, 2016).", "startOffset": 56, "endOffset": 78}, {"referenceID": 1, "context": ", POS tagging dos Santos and Zadrozny (2014), parsing (Ballesteros et al., 2015), language modeling (Ling et al.", "startOffset": 54, "endOffset": 80}, {"referenceID": 24, "context": ", 2015), language modeling (Ling et al., 2015), sentiment analysis (Zhang et al.", "startOffset": 27, "endOffset": 46}, {"referenceID": 36, "context": ", 2015), sentiment analysis (Zhang et al., 2015) as well as the tagger of Heigold et al.", "startOffset": 28, "endOffset": 48}, {"referenceID": 9, "context": "Our work is also related to recent work on character-level morphological generation using neural architectures (Faruqui et al., 2016; Rastogi et al., 2016).", "startOffset": 111, "endOffset": 155}, {"referenceID": 34, "context": "Our work is also related to recent work on character-level morphological generation using neural architectures (Faruqui et al., 2016; Rastogi et al., 2016).", "startOffset": 111, "endOffset": 155}, {"referenceID": 1, "context": ", POS tagging dos Santos and Zadrozny (2014), parsing (Ballesteros et al., 2015), language modeling (Ling et al., 2015), sentiment analysis (Zhang et al., 2015) as well as the tagger of Heigold et al. (2017), whose work we build upon.", "startOffset": 55, "endOffset": 208}, {"referenceID": 15, "context": "In speech recognition, Heigold et al. (2013) train a cross-lingual neural acoustic model on five Romance languages.", "startOffset": 23, "endOffset": 45}, {"referenceID": 0, "context": "In neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), recent work (Firat et al.", "startOffset": 30, "endOffset": 77}, {"referenceID": 10, "context": ", 2015), recent work (Firat et al., 2016; Zoph and Knight, 2016; Johnson et al., 2016) has explored the possibility of jointly train translation models for a wide variety of languages.", "startOffset": 21, "endOffset": 86}, {"referenceID": 37, "context": ", 2015), recent work (Firat et al., 2016; Zoph and Knight, 2016; Johnson et al., 2016) has explored the possibility of jointly train translation models for a wide variety of languages.", "startOffset": 21, "endOffset": 86}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2015), recent work (Firat et al., 2016; Zoph and Knight, 2016; Johnson et al., 2016) has explored the possibility of jointly train translation models for a wide variety of languages. Our work addresses a different task, but the undergirding philosophical motivation is similar, i.e., attack low-resource NLP through multi-task transfer learning. Kann et al. (2017) offer a similar method for cross-lingual transfer in morphological", "startOffset": 8, "endOffset": 390}, {"referenceID": 26, "context": "Future work should focus on extending the neural morphological tagger to a joint lemmatizer (M\u00fcller et al., 2015) and evaluate its functionality in the low-resource setting.", "startOffset": 92, "endOffset": 113}], "year": 2017, "abstractText": "Even for common NLP tasks, sufficient supervision is not available in many languages\u2014morphological tagging is no exception. In the work presented here, we explore a transfer learning scheme, whereby we train character-level recurrent neural taggers to predict morphological taggings for high-resource languages and low-resource languages together. Learning joint character representations among multiple related languages successfully enables knowledge transfer from the high-resource languages to the low-resource ones, improving accuracy by up to 30%.", "creator": "LaTeX with hyperref package"}}}