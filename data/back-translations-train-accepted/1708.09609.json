{"id": "1708.09609", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2017", "title": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation", "abstract": "One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums. Each of these forums constitutes its own \"fine-grained domain\" in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums.", "histories": [["v1", "Thu, 31 Aug 2017 08:18:12 GMT  (65kb,D)", "http://arxiv.org/abs/1708.09609v1", "To appear at EMNLP 2017"]], "COMMENTS": "To appear at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["greg durrett", "jonathan k kummerfeld", "taylor berg-kirkpatrick", "rebecca s portnoff", "sadia afroz", "damon mccoy", "kirill levchenko", "vern paxson"], "accepted": true, "id": "1708.09609"}, "pdf": {"name": "1708.09609.pdf", "metadata": {"source": "CRF", "title": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation", "authors": ["Greg Durrett", "UT Austin", "Jonathan K. Kummerfeld", "Taylor Berg-Kirkpatrick", "Rebecca S. Portnoff", "Sadia Afroz", "Damon McCoy", "Kirill Levchenko", "Vern Paxson"], "emails": ["gdurrett@cs.utexas.edu", "jkummerf@umich.edu", "tberg@cs.cmu.edu", "rsportnoff@cs.berkeley.edu", "sadia@icsi.berkeley.edu", "mccoy@nyu.edu", "klevchen@cs.ucsd.edu", "vern@berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Dataset and Annotation", "text": "We are looking at several forums that differ in the nature of the products traded: \u2022 Darcode: Cybercriminal goods, including exploit kits, spam services, ransomware programs and clandestine botnets. \u2022 Hack forums: A mixture of cyber security and computer games blackhat and non-cybercrime products. \u2022 Blackhat: Blackhat Search Engine Optimization techniques. \u2022 Zeros: tools and services for data theft. Table 1 gives some statistics of these forums. These are the same forums that were used to investigate product activity in Portnoff et al. (2017). We collected all available posts and commented on a subset of them. In total we commented on 130,336 tokens; taking into account several comments, our commentators considered 478,176 tokens in the process of identifying the data. Figure 1 shows two examples of posts from Darcode. In addition to aspects of the comment we describe below, we see that the text may present itself as common abbreviations of web products, and may also show other features."}, {"heading": "2.1 Annotation Process", "text": "In fact, most of us are able to play by the rules we have set ourselves, \"he said in an interview with\" Welt. \""}, {"heading": "2.2 Discussion", "text": "Since we comment on entities in a context-sensitive way (i.e., only comment on those in the product context), our task resembles a post-level information4 In preliminary notes, we found that content in the middle of the post typically described features or gave instructions without explicitly mentioning the product. Most posts are not affected by this rule: 96% of darcode, 77% of hack forums, 84% of blackhat, and 93% of cancelled posts are less than 20 lines. However, the cutoff reduced the effort for comments on the tail of very long posts.Extraction tasks are still significant. The product information in a post can be considered as a listing place that needs to be filled in the style of TAC KBP (Surdeanu, 2013; Surdeanu and Ji, 2014), with the token-level comments being notes that represent provenance information. However, we have decided to fully anchor the task at the post-level to allow us to decide whether or not to include the token event at the IST level."}, {"heading": "3 Evaluation Metrics", "text": "Given the different views on this task and its different requirements for different potential applications, we describe and motivate several different yardsticks below. Choosing the metric will affect the system design, as we discuss in the following paragraphs. Token-level accuracy We can follow the approach used in token-level tasks such as calculating precision, recall, and formula 1 over the set of tokens that are labeled as products.Typical product extraction (per post) For many applications, the primary goal of the extraction task is more consistent with the KBP-style slot filling, where we care about the amount of products that are extracted from a particular post. Without a domain-specific lexicon that contains complete synsets of products (such as something that might recognize that hack and access are synonymous), it is difficult to evaluate this in a way that we categorize the product categories."}, {"heading": "3.1 Phrase-level Evaluation", "text": "As mentioned in the previous section, we have not commented on noun phrases, but we may actually be interested in identifying them. In Figure 1, for example, extracting Backconnect Bot is more useful than extracting Bot in isolation, since Bot is a less specific characterization of the product.We can convert our token annotations into annotations at the phrase level by projecting our annotations at the level of the noun phrase, which is based on the output of an automatic parser. We used the Chen and Manning (2014) parser to analyze all sentences of each post. For each token marked with a nominal tag (N *), we projected this mark onto the largest NP that contains it, with a length of less than or equal to 7; most product NPs are shorter than this, and when the parser predicts a longer NP, our analysis determines that it will typically reflect the entire Phrasting of the product (sometimes we will not use the Phrasting as an overall error for the product)."}, {"heading": "4 Models", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4.1 Basic Results", "text": "Our learning-based systems significantly exceed the baseline in terms of the metrics for which they are optimized; the post-level system undercuts the binary classifier in terms of token evaluation, but is superior not only to posterior level accuracy, but also to product type F1. This gives credence to our hypothesis that the selection of a product is sufficient to characterize a large fraction of the entries. Comparing the automatic systems with the performance of human commentators, we see a significant gap. Note that the F1 brand of our best commentator was 89.8 and the accuracy of NP entries was 100%; a careful, well-trained commentator can achieve a very high performance, indicating a high skyline.The metric noun phrase generally seems to be more forgiving, as token distinctions within noun phrases are deleted. The posterior level NP system achieves an F score of 78 product type accuracy at this level, while an improvement on this level 88 is sufficient at the posterior level."}, {"heading": "5 Domain Adaptation", "text": "Table 2 showed only results for training and ratings within the same forum (darcode). However, we would like to use our system to extract product events from a variety of forums, so we are interested in how well the system can be generalized in a new forum. Tables 3 and 4 show the complete results of several systems in the rating settings within a forum and a crossforum. Performance deteriorates in the cross-forum environment compared to the forum, e.g. at NP level F1, a model trained by the hack forum is worse in darcode task 14.6 F1 than a model trained by darcode (61.2 vs. 75.8). Differences in the way the systems adapt between different forums are examined in more detail in Section 5.4. In the next sections, we will examine several possible methods for improving results in crossforum settings and trying to build a more cross-domain system. These techniques generally reflect two possible hypotheses about the cross-domain challenges."}, {"heading": "5.1 Brown Clusters", "text": "To test Hypothesis 1, we are investigating whether additional lexical information helps identify product-like words in new domains. A classic semi-supervised technique for exploiting unlabeled target data is to fire traits via word clusters or word vectors (Turian et al., 2010). These traits should generalize well across the domains on which the clusters are formed: If product words occur in similar contexts in domains and therefore end up in the same cluster, a model based on domain-limited data should be able to learn that cluster identity is indicative of products. We are actually forming brown clusters based on our unlabeled data from both darcode and hack forums (see Table 1 for sizes). We are using the implementation of Liang (2005) to learn 50 clusters. 10 During inspection, these clusters actually capture some of the semantics that are relevant to the service, such as the statistical cluster ID, which has many members in the cluster ID, for example."}, {"heading": "5.2 Type-level Annotation", "text": "Another approach that follows Hypothesis 1 is to use small amounts of monitored data. A cheap approach to annotating data in a new area is to use type-level annotations (Garrette and Baldridge, 2013; Garrette et al., 2013). Our token-level annotation standard is relatively complex to learn, but a researcher could easily provide a few sample products for a new forum based on just a few minutes of reading posts and analyzing the forum. Given the data we have already noted, we can simulate this process by repeating10This value was selected based on developer-defined experiments. 11We could also use vector representations of words here, but in initial experiments, these brown clusters did not exceed, which is consistent with the results of Turian et al. (2010), which provide a similar performance between Brown clusters and word vectors for chunking and NER.Marked data and product names are more common, and product names are the most frequently commented on."}, {"heading": "5.3 Token-level Annotation", "text": "We now turn to methods that could address Hypothesis 2. Assuming that the problem of domain transmission is more complex, we actually want to use tagged data in the target domain, rather than trying to transfer functions based on information at the type level. Specifically, we are interested in cases where a relatively small number of tagged posts (less than 100) could provide a significant benefit for customization; a researcher could plausibly do this remark within a few hours.We are looking at two ways to exploit the tagged target domain data: the first is to simply take these posts as additional training data; the second is to also apply the \"frustratingly simple\" domain matching method of thumb III (2007).Within this framework, each feature fired in our model is actually fired twice: one copy is domain-general and one is linked to the domain la-bel (here the name of the forum)."}, {"heading": "5.4 Analysis", "text": "In order to understand the variable performance and shortcomings of the domain adaptation approaches we have studied, it is useful to examine our two initial hypotheses and to characterize the data sets a little further. To do this, we break down the system performance of products we have seen in education against new products. As our systems are based on lexical and characteristic features, we expect them to better predict products we have seen before."}, {"heading": "6 Conclusion", "text": "We present a new set of entries from cybercrime marketplaces with product references, a task where IE and NER merge. Learning-based methods degrade performance when matched to 13While a hyperparameter that controls the precision / recall compromise could theoretically be matched to the target domain, it is difficult to do so in a robust, principled manner without having access to an extensive annotated record from that domain, which makes evaluation even more difficult and makes it more difficult to set up apple-to-apple comparisons between domains. New forums, and while we are exploring methods for fine-grain domain matching in this data, effective methods for this task are still an open question. Our records used in this work are available at https: / / evidencebasedsecurity.org / forums / product extractor code at http: / gicubmaster / ccithum.com / product /"}, {"heading": "Acknowledgments", "text": "This work was supported in part by the National Science Foundation with funding from CNS-1237265 and CNS-1619620, the Office of Naval Research with funding from MURI N000140911081, the Center for Long-Term Cybersecurity, and gifts from Google. We thank all who provided forum data for our analysis, in particular Scraping Hub and SRI for their support in collecting data for this study. All opinions, findings and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors."}], "references": [{"title": "Learning Latent Personas of Film Characters", "author": ["David Bamman", "Brendan O\u2019Connor", "Noah A. Smith"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "Bamman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bamman et al\\.", "year": 2013}, {"title": "A Fast and Accurate Dependency Parser using Neural Networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Frustratingly Easy Domain Adaptation", "author": ["Hal Daume III."], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL).", "citeRegEx": "III.,? 2007", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer."], "venue": "Journal of Machine Learning Research, 12:2121\u20132159.", "citeRegEx": "Duchi et al\\.,? 2011", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["Jenny Rose Finkel", "Trond Grenager", "Christopher Manning."], "venue": "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Finkel et al\\.,? 2005", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Measuring nominal scale agreement among many raters", "author": ["J.L. Fleiss."], "venue": "Psychological Bulletin, 76(5):378\u2013382.", "citeRegEx": "Fleiss.,? 1971", "shortCiteRegEx": "Fleiss.", "year": 1971}, {"title": "Learning a Part-of-Speech Tagger from Two Hours of Annotation", "author": ["Dan Garrette", "Jason Baldridge."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Garrette and Baldridge.,? 2013", "shortCiteRegEx": "Garrette and Baldridge.", "year": 2013}, {"title": "Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages", "author": ["Dan Garrette", "Jason Mielens", "Jason Baldridge."], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Garrette et al\\.,? 2013", "shortCiteRegEx": "Garrette et al\\.", "year": 2013}, {"title": "Foreebank: Syntactic Analysis of Customer Support Forums", "author": ["Rasoul Kaljahi", "Jennifer Foster", "Johann Roturier", "Corentin Ribeyre", "Teresa Lynn", "Joseph Le Roux."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Pro-", "citeRegEx": "Kaljahi et al\\.,? 2015", "shortCiteRegEx": "Kaljahi et al\\.", "year": 2015}, {"title": "Tagging and Linking Web Forum Posts", "author": ["Su Nam Kim", "Li Wang", "Timothy Baldwin."], "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL).", "citeRegEx": "Kim et al\\.,? 2010", "shortCiteRegEx": "Kim et al\\.", "year": 2010}, {"title": "Cards Stolen in Target Breach Flood Underground Markets", "author": ["Brian Krebs"], "venue": null, "citeRegEx": "Krebs.,? \\Q2013\\E", "shortCiteRegEx": "Krebs.", "year": 2013}, {"title": "Who\u2019s Selling Credit Cards from Target", "author": ["Brian Krebs"], "venue": null, "citeRegEx": "Krebs.,? \\Q2013\\E", "shortCiteRegEx": "Krebs.", "year": 2013}, {"title": "Frame-Semantic Role Labeling with Heterogeneous Annotations", "author": ["Meghana Kshirsagar", "Sam Thomson", "Nathan Schneider", "Jaime Carbonell", "Noah A. Smith", "Chris Dyer."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Compu-", "citeRegEx": "Kshirsagar et al\\.,? 2015", "shortCiteRegEx": "Kshirsagar et al\\.", "year": 2015}, {"title": "An Empirical Analysis of Optimization for Max-Margin NLP", "author": ["Jonathan K. Kummerfeld", "Taylor Berg-Kirkpatrick", "Dan Klein."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Kummerfeld et al\\.,? 2015", "shortCiteRegEx": "Kummerfeld et al\\.", "year": 2015}, {"title": "Semi-Supervised Learning for Natural Language Processing", "author": ["Percy Liang."], "venue": "Master\u2019s Thesis, Massachusetts Institute of Technology.", "citeRegEx": "Liang.,? 2005", "shortCiteRegEx": "Liang.", "year": 2005}, {"title": "Classifying User Forum Participants: Separating the Gurus from the Hacks, and Other Tales of the Internet", "author": ["Marco Lui", "Timothy Baldwin."], "venue": "Proceedings of the Australasian Language Technology Association Workshop (ALTA).", "citeRegEx": "Lui and Baldwin.,? 2010", "shortCiteRegEx": "Lui and Baldwin.", "year": 2010}, {"title": "The Stanford CoreNLP Natural Language Processing Toolkit", "author": ["Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky."], "venue": "Proceedings of 52nd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Manning et al\\.,? 2014", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Automatic domain adaptation for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).", "citeRegEx": "McClosky et al\\.,? 2010", "shortCiteRegEx": "McClosky et al\\.", "year": 2010}, {"title": "The ACE 2005 Evaluation Plan", "author": ["NIST."], "venue": "NIST.", "citeRegEx": "NIST.,? 2005", "shortCiteRegEx": "NIST.", "year": 2005}, {"title": "Learning to Extract International Relations from Political Context", "author": ["Brendan O\u2019Connor", "Brandon M. Stewart", "Noah A. Smith"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "O.Connor et al\\.,? \\Q2013\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2013}, {"title": "Tools for Automated Analysis of Cybercriminal Markets", "author": ["Rebecca S. Portnoff", "Sadia Afroz", "Greg Durrett", "Jonathan K. Kummerfeld", "Taylor Berg-Kirkpatrick", "Damon McCoy", "Kirill Levchenko", "Vern Paxson."], "venue": "Proceedings of the 26th Interna-", "citeRegEx": "Portnoff et al\\.,? 2017", "shortCiteRegEx": "Portnoff et al\\.", "year": 2017}, {"title": "Online) Subgradient Methods for Structured Prediction", "author": ["Nathan J. Ratliff", "Andrew Bagnell", "Martin Zinkevich."], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Ratliff et al\\.,? 2007", "shortCiteRegEx": "Ratliff et al\\.", "year": 2007}, {"title": "Overview of the TAC2013 Knowledge Base Population Evaluation: English Slot Filling and Temporal Slot Filling", "author": ["Mihai Surdeanu."], "venue": "Proceedings of the TAC-KBP 2013 Workshop.", "citeRegEx": "Surdeanu.,? 2013", "shortCiteRegEx": "Surdeanu.", "year": 2013}, {"title": "Overview of the English Slot Filling Track at the TAC2014 Knowledge Base Population Evaluation", "author": ["Mihai Surdeanu", "Heng Ji."], "venue": "Proceedings of the TAC-KBP 2014 Workshop.", "citeRegEx": "Surdeanu and Ji.,? 2014", "shortCiteRegEx": "Surdeanu and Ji.", "year": 2014}, {"title": "Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition", "author": ["Erik F. Tjong Kim Sang", "Fien De Meulder."], "venue": "Proceedings of the Conference on Natural Language Learning (CoNLL).", "citeRegEx": "Sang and Meulder.,? 2003", "shortCiteRegEx": "Sang and Meulder.", "year": 2003}, {"title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL).", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Predicting Thread Discourse Structure over Technical Web Forums", "author": ["Li Wang", "Marco Lui", "Su Nam Kim", "Joakim Nivre", "Timothy Baldwin."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).", "citeRegEx": "Wang et al\\.,? 2011", "shortCiteRegEx": "Wang et al\\.", "year": 2011}, {"title": "Learning Structural SVMs with Latent Variables", "author": ["Chun-Nam John Yu", "Thorsten Joachims."], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning (ICML).", "citeRegEx": "Yu and Joachims.,? 2009", "shortCiteRegEx": "Yu and Joachims.", "year": 2009}, {"title": "Aspect-augmented Adversarial Networks for Domain Adaptation", "author": ["Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola."], "venue": "Transactions of the Association for Computational Linguistics (TACL).", "citeRegEx": "Zhang et al\\.,? 2017", "shortCiteRegEx": "Zhang et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 17, "context": "However, conducting automatic analysis is difficult because this data is outof-domain for conventional NLP models, which harms the performance of both discrete models (McClosky et al., 2010) and deep models (Zhang et al.", "startOffset": 167, "endOffset": 190}, {"referenceID": 28, "context": ", 2010) and deep models (Zhang et al., 2017).", "startOffset": 24, "endOffset": 44}, {"referenceID": 20, "context": "case studies on these particular forums (Portnoff et al., 2017), including a study of marketplace activity on bulk hacked accounts versus users selling their own accounts.", "startOffset": 40, "endOffset": 63}, {"referenceID": 8, "context": "While prior work has dealt with these messy characteristics for syntax (Kaljahi et al., 2015) and for discourse (Lui and Baldwin, 2010; Kim et al.", "startOffset": 71, "endOffset": 93}, {"referenceID": 15, "context": ", 2015) and for discourse (Lui and Baldwin, 2010; Kim et al., 2010; Wang et al., 2011), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective.", "startOffset": 26, "endOffset": 86}, {"referenceID": 9, "context": ", 2015) and for discourse (Lui and Baldwin, 2010; Kim et al., 2010; Wang et al., 2011), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective.", "startOffset": 26, "endOffset": 86}, {"referenceID": 26, "context": ", 2015) and for discourse (Lui and Baldwin, 2010; Kim et al., 2010; Wang et al., 2011), our work is the first to tackle forum data (and marketplace forums specifically) from an information extraction perspective.", "startOffset": 26, "endOffset": 86}, {"referenceID": 6, "context": "to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013), token-level annotation (Daume III, 2007), and", "startOffset": 141, "endOffset": 194}, {"referenceID": 7, "context": "to annotate data for every new forum that might need to be analyzed, we explore several methods for adaptation, mixing type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013), token-level annotation (Daume III, 2007), and", "startOffset": 141, "endOffset": 194}, {"referenceID": 25, "context": "semi-supervised approaches (Turian et al., 2010; Kshirsagar et al., 2015).", "startOffset": 27, "endOffset": 73}, {"referenceID": 12, "context": "semi-supervised approaches (Turian et al., 2010; Kshirsagar et al., 2015).", "startOffset": 27, "endOffset": 73}, {"referenceID": 20, "context": "These are the same forums used to study product activity in Portnoff et al. (2017). We collected", "startOffset": 60, "endOffset": 83}, {"referenceID": 16, "context": "We preprocessed the data using the tokenizer and sentence-splitter from the Stanford CoreNLP toolkit (Manning et al., 2014).", "startOffset": 101, "endOffset": 123}, {"referenceID": 5, "context": "We use the Fleiss\u2019 Kappa measurement (Fleiss, 1971), treating our task as a token-level annotation where every token is annotated as either a product or not.", "startOffset": 37, "endOffset": 51}, {"referenceID": 22, "context": "The product information in a post can be thought of as a list-valued slot to be filled in the style of TAC KBP (Surdeanu, 2013; Surdeanu and Ji, 2014), with the token-level annotations constituting provenance information.", "startOffset": 111, "endOffset": 150}, {"referenceID": 23, "context": "The product information in a post can be thought of as a list-valued slot to be filled in the style of TAC KBP (Surdeanu, 2013; Surdeanu and Ji, 2014), with the token-level annotations constituting provenance information.", "startOffset": 111, "endOffset": 150}, {"referenceID": 18, "context": "Our approach also resembles the fully token-level annotations of entity and event information in the ACE dataset (NIST, 2005).", "startOffset": 113, "endOffset": 125}, {"referenceID": 1, "context": "We used the parser of Chen and Manning (2014) to parse all sentences of each post.", "startOffset": 22, "endOffset": 46}, {"referenceID": 4, "context": "The most relevant off-theshelf system is an NER tagging model; we retrain the Stanford NER system on our data (Finkel et al., 2005).", "startOffset": 110, "endOffset": 131}, {"referenceID": 27, "context": "Our post-level system is formulated as an instance of a latent SVM (Yu and Joachims, 2009).", "startOffset": 67, "endOffset": 90}, {"referenceID": 21, "context": "We trained all of the learned models by subgradient descent on the primal form of the objective (Ratliff et al., 2007; Kummerfeld et al., 2015).", "startOffset": 96, "endOffset": 143}, {"referenceID": 13, "context": "We trained all of the learned models by subgradient descent on the primal form of the objective (Ratliff et al., 2007; Kummerfeld et al., 2015).", "startOffset": 96, "endOffset": 143}, {"referenceID": 3, "context": "We use AdaGrad (Duchi et al., 2011) to speed convergence in the presence of a large weight vector with heterogeneous feature types.", "startOffset": 15, "endOffset": 35}, {"referenceID": 25, "context": "A classic semisupervised technique for exploiting unlabeled target data is to fire features over word clusters or word vectors (Turian et al., 2010).", "startOffset": 127, "endOffset": 148}, {"referenceID": 14, "context": "We use Liang (2005)\u2019s implementation to learn 50 clusters.", "startOffset": 7, "endOffset": 20}, {"referenceID": 6, "context": "Another approach following Hypothesis 1 is to use small amounts of supervised data, One cheap approach for annotating data in a new domain is to exploit type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013).", "startOffset": 175, "endOffset": 228}, {"referenceID": 7, "context": "Another approach following Hypothesis 1 is to use small amounts of supervised data, One cheap approach for annotating data in a new domain is to exploit type-level annotation (Garrette and Baldridge, 2013; Garrette et al., 2013).", "startOffset": 175, "endOffset": 228}, {"referenceID": 25, "context": "That is consistent with the results of Turian et al. (2010) who showed similar performance between Brown clusters and word vectors for chunking and NER.", "startOffset": 39, "endOffset": 60}, {"referenceID": 2, "context": "much more effective than the other way around, and using domain features as in Daume III (2007) gives little benefit over na\u0131\u0308ve use of the new data.", "startOffset": 85, "endOffset": 96}, {"referenceID": 2, "context": "The second is to also employ the \u201cfrustratingly easy\u201d domain adaptation method of Daume III (2007). In this framework, each feature fired in our model is actually fired twice: one copy is domaingeneral and one is conjoined with the domain label (here, the name of the forum).", "startOffset": 88, "endOffset": 99}], "year": 2017, "abstractText": "One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums. Each of these forums constitutes its own \u201cfine-grained domain\u201d in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums.1", "creator": "LaTeX with hyperref package"}}}