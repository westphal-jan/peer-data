{"id": "1709.00149", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2017", "title": "Learning what to read: Focused machine reading", "abstract": "Recent efforts in bioinformatics have achieved tremendous progress in the machine reading of biomedical literature, and the assembly of the extracted biochemical interactions into large-scale models such as protein signaling pathways. However, batch machine reading of literature at today's scale (PubMed alone indexes over 1 million papers per year) is unfeasible due to both cost and processing overhead. In this work, we introduce a focused reading approach to guide the machine reading of biomedical literature towards what literature should be read to answer a biomedical query as efficiently as possible. We introduce a family of algorithms for focused reading, including an intuitive, strong baseline, and a second approach which uses a reinforcement learning (RL) framework that learns when to explore (widen the search) or exploit (narrow it). We demonstrate that the RL approach is capable of answering more queries than the baseline, while being more efficient, i.e., reading fewer documents.", "histories": [["v1", "Fri, 1 Sep 2017 04:09:42 GMT  (38kb,D)", "http://arxiv.org/abs/1709.00149v1", "6 pages, 1 figure, 1 algorithm, 2 tables, accepted to EMNLP 2017"]], "COMMENTS": "6 pages, 1 figure, 1 algorithm, 2 tables, accepted to EMNLP 2017", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.IR cs.LG", "authors": ["enrique noriega-atala", "marco antonio valenzuela-esc\u00e1rcega", "clayton morrison", "mihai surdeanu"], "accepted": true, "id": "1709.00149"}, "pdf": {"name": "1709.00149.pdf", "metadata": {"source": "CRF", "title": "Learning what to read: Focused machine reading", "authors": ["Enrique Noriega-Atala", "Marco A. Valenzuela-Esc\u00e1rcega", "Clayton T. Morrison", "Mihai Surdeanu"], "emails": ["enoriega@email.arizona.edu", "marcov@email.arizona.edu", "claytonm@email.arizona.edu", "msurdeanu@email.arizona.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it is so far that it is only about half, \"he said in an interview with the Deutsche Presse-Agentur:\" It is not yet so far that we have grown old as if we have been able to find a solution. \""}, {"heading": "2 Related Work", "text": "We point the interested reader to the BioNLP community (Ne \u0301 dellec et al., 2013; Kim et al., 2012, 2009 and others) as a starting point. However, most of this work focuses on how to read, not what to read when setting a goal. To our knowledge, we are the first to focus on the latter task. Increased learning has been used to achieve state-of-the-art in several tasks of natural language processing (NLP) and information gathering (IR), for example, RL has been used to guide IR and filter irrelevant web content (Seo and Zhang, 2000; Zhang and Seo, 2001). More recently, RL has been combined with deep learning, e.g. to improve correction resolution (Clark and Manning, 2016)."}, {"heading": "3 Focused Reading", "text": "Here we are looking at focused reading for the biomedical field, and we are focusing on binary promotion / inhibition interactions between biochemical units. In this context, the machine read (or IE) component produces a directed graph in which recesses represent participants in an interaction (e.g. protein, gene, or a biological process), and edges represent directed activation interactions. Edge markings indicate whether the controller unit has a positive (promoting) or negative (inhibiting) interaction with the controlled participant. Figure 1 shows a sample edge in this graph. We are using REACH5, an open IE system (Valenzuela-Esca), which indicates whether the controller unit has a positive (promoting) or negative (inhibiting) interaction extracted from the structured biomedical text and constructed the above graph. We are coupling this IE system with a Lucene6 index to fetch back the biomedical papers."}, {"heading": "4 Baseline Algorithm and Evaluation", "text": "The most important functions that affect the search behavior of the algorithms are end effect 73 and end effect 65. Here, we describe a baseline-focused reading in which we follow the intuition that some participants in a biological graph tend to have more participants than others, and are therefore more likely to find interactions between participants in general. Our heuristic approach is to find new participants who currently have the most intuitive and promising channels."}, {"heading": "5 Reinforcement Learning for Focussed Reading", "text": "We analyzed the baseline's behavior in the evaluation to identify the conditions under which it was unable to find pathways. We concluded that some of the errors could have been avoided if we had used a dif-7Dynamic Cell Environment model of the pancreas. Conditions for making different decisions depend on the current state of G, and earlier query behavior may actually have an impact on later query options, making this an iterative decision problem and a natural fit for an RL formulation. Inspired by this observation, we consider RL for finding a better policy for CHOOSEQUERY. We point to an instance of the focused reading algorithm with a learned CHOSEQUERY policy that we learned."}, {"heading": "6 Analysis", "text": "The results are summarized in Table 2. Similar to Section 5, we grouped the features into five different groups, and we measured the effects of removing a feature group at once. Overall, the amount of paths found has no significant variance, but the efficiency of the search (amount of papers read and number of queries executed) depends on several feature groups. Features (f1), (f2), and (f4) have a large effect on both the number of papers read and the number of queries executed. Removing the feature (f5) actually reduces the number of papers read by about 2K with a minimal reduction in the paths found, suggesting that this task could benefit from feature selection. RL Policy Error Analysis: Finally, we analyzed the execution track of eighteen (20% of errors) of the queries found with a minimal reduction in the number of paths found."}, {"heading": "7 Discussion and future work", "text": "We have presented a generic focused reading algorithm, an intuitive, strong base algorithm that instantiates it, and formulated an RL approach that learns how to efficiently query the paper repository that feeds the machine reading component. We have shown that RL-based focused reading is more efficient than the baseline (i.e., it reads 24% fewer essays), while it answers 7% more queries. There are many exciting directions this work can go in. First, a larger part of the focused reading algorithm can be subjected to RL, with CHOOSEENDPOINTS policy being the clear next candidate. Second, we can expand focused reading to efficiently search for multiple paths between S and D. Finally, we will include additional biological constraints (e.g. the focus on paths that exist in certain species) in the search itself."}, {"heading": "Acknowledgments", "text": "This work was partially funded by the DARPA Big Mechanism program under the ARO Contract W911NF-14-1-0395. Dr. Mihai Surdeanu discloses a financial interest in Lum.Ai. This interest has been disclosed to the Institutional Review Committee of the University of Arizona and is managed in accordance with its conflict of interest policy."}], "references": [{"title": "Deep reinforcement learning for mention-ranking coreference models", "author": ["Kevin Clark", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1609.08667 .", "citeRegEx": "Clark and Manning.,? 2016", "shortCiteRegEx": "Clark and Manning.", "year": 2016}, {"title": "DARPA\u2019s Big Mechanism program", "author": ["Paul R. Cohen."], "venue": "Physical Biology 12(4):045008.", "citeRegEx": "Cohen.,? 2015", "shortCiteRegEx": "Cohen.", "year": 2015}, {"title": "The genia event and protein coreference tasks of the bionlp shared task", "author": ["Jin-Dong Kim", "Ngan Nguyen", "Yue Wang", "Jun\u2019ichi Tsujii", "Toshihisa Takagi", "Akinori Yonezawa"], "venue": null, "citeRegEx": "Kim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "Overview of bionlp\u201909 shared task on event extraction", "author": ["Jin-Dong Kim", "Tomoko Ohta", "Sampo Pyysalo", "Yoshinobu Kano", "Jun\u2019ichi Tsujii"], "venue": "In Proceedings of the Workshop on Current Trends", "citeRegEx": "Kim et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2009}, {"title": "Improving information extraction by acquiring external evidence with reinforcement learning", "author": ["Karthik Narasimhan", "Adam Yala", "Regina Barzilay."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Narasimhan et al\\.,? 2016", "shortCiteRegEx": "Narasimhan et al\\.", "year": 2016}, {"title": "Overview of bionlp shared task 2013", "author": ["Claire N\u00e9dellec", "Robert Bossy", "Jin-Dong Kim", "JungJae Kim", "Tomoko Ohta", "Sampo Pyysalo", "Pierre Zweigenbaum."], "venue": "Proceedings of the BioNLP Shared Task 2013 Workshop. pages 1\u20137.", "citeRegEx": "N\u00e9dellec et al\\.,? 2013", "shortCiteRegEx": "N\u00e9dellec et al\\.", "year": 2013}, {"title": "Publication growth in biological sub-fields: patterns, predictability and sustainability", "author": ["Marco Pautasso."], "venue": "Sustainability 4(12):3234\u20133247.", "citeRegEx": "Pautasso.,? 2012", "shortCiteRegEx": "Pautasso.", "year": 2012}, {"title": "A reinforcement learning agent for personalized information filtering", "author": ["Young-Woo Seo", "Byoung-Tak Zhang."], "venue": "Proceedings of the 5th international conference on Intelligent user interfaces. ACM, pages 248\u2013251.", "citeRegEx": "Seo and Zhang.,? 2000", "shortCiteRegEx": "Seo and Zhang.", "year": 2000}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S Sutton", "Andrew G Barto."], "venue": "MIT press Cambridge.", "citeRegEx": "Sutton and Barto.,? 1998", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Undiscovered public knowledge", "author": ["Don R Swanson."], "venue": "The Library Quarterly 56(2):103\u2013118.", "citeRegEx": "Swanson.,? 1986", "shortCiteRegEx": "Swanson.", "year": 1986}, {"title": "A domain-independent rule-based framework for event extraction", "author": ["Marco A. Valenzuela-Esc\u00e1rcega", "Gustave HahnPowell", "Thomas Hicks", "Mihai Surdeanu."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Valenzuela.Esc\u00e1rcega et al\\.,? 2015", "shortCiteRegEx": "Valenzuela.Esc\u00e1rcega et al\\.", "year": 2015}, {"title": "Personalized web-document filtering using reinforcement learning", "author": ["Byoung-Tak Zhang", "Young-Woo Seo."], "venue": "Applied Artificial Intelligence 15(7):665\u2013685.", "citeRegEx": "Zhang and Seo.,? 2001", "shortCiteRegEx": "Zhang and Seo.", "year": 2001}], "referenceMentions": [{"referenceID": 9, "context": "However, scientists are faced with a problem of \u201cundiscovered public knowledge,\u201d as they struggle to read and assimilate all of this information (Swanson, 1986).", "startOffset": 145, "endOffset": 160}, {"referenceID": 6, "context": "Furthermore, the literature is growing at an exponential rate (Pautasso, 2012); PubMed1 has been adding more than a million papers per year since 2011.", "startOffset": 62, "endOffset": 78}, {"referenceID": 5, "context": "Large ongoing efforts, such as the BioNLP task community (N\u00e9dellec et al., 2013; Kim et al., 2012, 2009) and the DARPA Big Mechanism Pro-", "startOffset": 57, "endOffset": 104}, {"referenceID": 1, "context": "gram (Cohen, 2015), are making progress in advancing methods for machine reading and assembly of extracted biochemical interactions into large-scale models.", "startOffset": 5, "endOffset": 18}, {"referenceID": 7, "context": "For example, RL has been used to guide IR and filter irrelevant web content (Seo and Zhang, 2000; Zhang and Seo, 2001).", "startOffset": 76, "endOffset": 118}, {"referenceID": 11, "context": "For example, RL has been used to guide IR and filter irrelevant web content (Seo and Zhang, 2000; Zhang and Seo, 2001).", "startOffset": 76, "endOffset": 118}, {"referenceID": 0, "context": ", for improving coreference resolution (Clark and Manning, 2016).", "startOffset": 39, "endOffset": 64}, {"referenceID": 4, "context": "Finally, RL has been used to improve the efficiency of IE by learning how to incrementally reconcile new information and help choose what to look for next (Narasimhan et al., 2016), a task close to ours.", "startOffset": 155, "endOffset": 180}, {"referenceID": 0, "context": ", for improving coreference resolution (Clark and Manning, 2016). Finally, RL has been used to improve the efficiency of IE by learning how to incrementally reconcile new information and help choose what to look for next (Narasimhan et al., 2016), a task close to ours. This serves as an inspiration for the work we present here, but with a critical difference: Narasimhan et al. (2016) focus on slot filling using a pre-existing template.", "startOffset": 40, "endOffset": 387}, {"referenceID": 10, "context": "tem (Valenzuela-Esc\u00e1rcega et al., 2015), to extract interactions from unstructured biomedical text and construct the graph above.", "startOffset": 4, "endOffset": 39}, {"referenceID": 8, "context": "We trained the RL Query Policy using the SARSA (Sutton and Barto, 1998) RL algorithm.", "startOffset": 47, "endOffset": 71}], "year": 2017, "abstractText": "Recent efforts in bioinformatics have achieved tremendous progress in the machine reading of biomedical literature, and the assembly of the extracted biochemical interactions into large-scale models such as protein signaling pathways. However, batch machine reading of literature at today\u2019s scale (PubMed alone indexes over 1 million papers per year) is unfeasible due to both cost and processing overhead. In this work, we introduce a focused reading approach to guide the machine reading of biomedical literature towards what literature should be read to answer a biomedical query as efficiently as possible. We introduce a family of algorithms for focused reading, including an intuitive, strong baseline, and a second approach which uses a reinforcement learning (RL) framework that learns when to explore (widen the search) or exploit (narrow it). We demonstrate that the RL approach is capable of answering more queries than the baseline, while being more efficient, i.e., reading fewer documents.", "creator": "LaTeX with hyperref package"}}}