{"id": "1709.00575", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2017", "title": "Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection", "abstract": "The ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding. Yet, the majority of metaphor processing systems to date rely on hand-engineered features and there is still no consensus in the field as to which features are optimal for this task. In this paper, we present the first deep learning architecture designed to capture metaphorical composition. Our results demonstrate that it outperforms the existing approaches in the metaphor identification task.", "histories": [["v1", "Sat, 2 Sep 2017 13:13:06 GMT  (254kb,D)", "http://arxiv.org/abs/1709.00575v1", "EMNLP 2017"]], "COMMENTS": "EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["marek rei", "luana bulat", "douwe kiela", "ekaterina shutova"], "accepted": true, "id": "1709.00575"}, "pdf": {"name": "1709.00575.pdf", "metadata": {"source": "CRF", "title": "Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection", "authors": ["Marek Rei", "Luana Bulat", "Douwe Kiela", "Ekaterina Shutova"], "emails": ["marek.rei@cl.cam.ac.uk,", "luana.bulat@cl.cam.ac.uk,", "ekaterina.shutova@cl.cam.ac.uk,", "dkiela@fb.com"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where it only takes one year to find a solution."}, {"heading": "2 Related Work", "text": "In fact, most people are able to recognize themselves and understand what they are doing."}, {"heading": "3 Supervised Similarity Network", "text": "Our method is inspired by the results of Shutova et al. (2016), which showed that the cosinal similarity between the neural embedding of the two words in a phrase is characteristic of its metaphor. For example, the phrase \"colored personality\" gets a score: s = cos (xc, xp) (1), where xc is the embedding for colored and xp is the embedding for personality. The combined phrase is classified as metaphorical, based on a threshold optimized on a developmental dataset. In this paper, we propose several extensions of this general idea, creating a supervised version of the metric of cosinal similarity that can be optimized based on training data to be more suitable for metaphor recognition."}, {"heading": "3.1 Word Representation Gating", "text": "The direct comparison of the vector representations of both words treats each of the embeddings as an independent entity. In reality, however, the word meanings vary and adapt to the context. In the case of a metaphorical language (e.g. \"healing crime\"), the source domain properties of the verb (e.g. healing) are projected onto the target word (e.g. crime), leading to the interaction of the two domains in interpreting the metaphor. To integrate this idea into the method of metaphor recognition, we can construct a gating function that modifies the representation of one word on the basis of the other. Given the embeddings x1 and x2, the gating values are predicted as a nonlinear transformation of x1 and applied to x2 by elementary multiplication: g = \u03c3 (Wgx1) (2) x 2 g (3), Wg being a weight matrix optimized during training, the signature is a great function for activation of the network and then a relative change in the sense of the function."}, {"heading": "3.2 Vector Space Mapping", "text": "The next step is to implement site-specific mappings for the word embeddings. The original method uses word embeddings that have been pre-trained using the distribution-specific skip program lens (Mikolov et al., 2013a). While this optimizes the vectors for predicting context words, there is no reason to believe that the same space is also optimal for the metaphor recognition task. To address this deficiency, we allow the model to learn a mapping from the skip gram vector space into a new metaphor-specific vector space: z1 = tanh (Wz1x1) (4) z2 = tanh (Wz2 x 2) (5), with Wz1 and Wz2 being weight matrices, z1 and z2 being the new position-specific word representations. While the original x1 and x2 embeddings on a large, non-occurring corpus would be better suited to differentiating between the individual word embeddings, where the resulting metaphors are unaffected by the transformations."}, {"heading": "3.3 Weighted Cosine", "text": "When the vectors x1 and x2 are normalized to the unit length, the cosinal similarity between them corresponds to their point product, which in turn corresponds to their elementary multiplication, followed by a sum of all elements: cos (x1, x2). However, there follows a single starting neuron with all intermediate weights set to the value of 1. Such a network would compute the same sum over the elemental multiplication, with the value of the cosinal similarity being directly multiplied. Since there is no reason to suppose that all embedding dimensions are equally important when recognizing metaphors, we can explore other strategies for weighting the similarity calculation. Rei and Briscoe (2014) used a fixed formula to calculate cosinal dimensions to capture this cosine function and supplement the cosine function."}, {"heading": "3.4 Prediction and Optimisation", "text": "Based on the vector d, we can output a prediction for the word pair that shows whether it is literal or metaphorical: y = \u03c3 (Wyd) (9), where Wy is a weight matrix, \u03c3 is the logistic activation function and y is a real prediction with values between 0 and 1. We optimize the model on the basis of a commented training data set, while minimizing the following hinge loss function: E = \u2211 k qk (10) qk = {(y-y) 2 if | y-y | > 0,4 0, otherwise (11) where y is the predicted value, y-y is the true designation, and k iterates over all training examples. Equation 11 optimizes the model to minimize the square error between predicted and true designations. However, this only occurs for training examples where the predicted error is not close enough to the desired result, and minimizes itk over all training examples to optimize the square error between the predicted model 11 and the optimal equation."}, {"heading": "4 Word Representations", "text": "Following Bulat et al. (2017), we are experimenting with two types of semantic vectors: word embedding in Skip-gram format and attribute-based representation. Word embedding is 100-dimensional and was trained using the standard log-linear Skipgram negative sampling model by Mikolov et al. (2013b) on Wikipedia for 3 epochs, using a symmetrical window with 5 and 10 negative samples per word-context pairing. We are using the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017) following Fagarasan Fagarasan et al. (2015). These representations were induced by partial feedback of squares to learn a cross-modal mapping function between the word embedding described above and the McRae et al. (2005) characteristic semantic space."}, {"heading": "5 Datasets", "text": "Since these datasets contain examples of different senses (both metaphorical and literal) of the same verbs or adjectives, they allow us to test the extent to which our model is able to distinguish between different senses of words, rather than simply selecting the most common class for a particular word. (2016) Verbs that find between three and ten senses and extract the sentences they illustrate in the corresponding glosses result in a total of 1639 verbs used in sentences. Each of these verbs was commented on by 10 annotators via crowdsourcing platform CrowdFlower1. Mohammad et al. chose verbs that were designated as metaphorical or literal by at least 70% of the annotators to create their datasets."}, {"heading": "6 Experiments and Results", "text": "Values for these hyperparameters were selected experimentally to avoid conclusions based on random results."}, {"heading": "7 The Effects of Training Data", "text": "The results in Section 6 show that the performance of the TSV dataset is higher than that of the MOH dataset, which is probably due to the fact that the former has more examples of training available. Therefore, we conducted an additional experiment to investigate the impact of the size of the dataset on the performance of metaphor recognition. Gutie \u0301 rrez et al. (2016) commented on a dataset of adjective phrases as literal or metaphorical, and we are able to use this as an additional training resource. Although it contains only 23 unique adjectives, the total number of phrases reaches 8,592. We remove all phrases occurring in the TSV development or test data, then gradually add the remaining examples to the TSV training data and evaluate on the TSV TEST. Figure 2 shows a graph of system performance when training data is increased at intervals of 500. There is a very rapid performance increase to about 2,000 training points limited to approximately 1,000 TRAIN during the existing TRAS3."}, {"heading": "8 Qualitative analysis", "text": "The architecture in Section 3 also functions as a semantic composition model, extracting the meaning of the phrase by combining the meanings of its components with each other. Therefore, we conducted a qualitative experiment to investigate: (1) how well traditional compositional methods capture metaphors without fine-tuning; and (2) whether the monitored representations retain their domain-specific semantic information. To this end, we construct three vector spaces and visualize some examples from the TSV training set using t-SNE (Van Der Maaten and Hinton, 2008). Figure 3 contains examples of three different compositional techniques: The additive method simply summarizes the embedding of the skip grams for both words (above); the multiplicative method multiplies the embedding of the skip grams (center); the final system uses layer m from the SSN model to represent the superordinate boundaries as metaphors (below). The visualization shows that the phaphysical models are both comparative and very structured, but it is a cluster system."}, {"heading": "9 Conclusion", "text": "In this paper, we presented the first deep-learning architecture designed to capture metaphorical composition and evaluated it using a metaphor identification task. Firstly, we demonstrated that the proposed framework exceeds both a metaphor-agnostic baseline (a feed-forward neural network) and previous corpus-driven approaches to metaphor identification; the results showed that it is beneficial to construct a specialized metaphor-recognition network architecture that includes a gating function to capture the interaction between the source and target domains, word embedding assigned to a metaphor-specific space, and optimizations using a hinged-loss function. Secondly, our qualitative analysis shows that our supervised similarity network learns phrases with a very clear boundary for metaphoric representation, as opposed to traditional compositional methods. Finally, we show that our model can meet with sufficient metaphorical knowledge or a large-scale educational set."}, {"heading": "Acknowledgments", "text": "Ekaterina Shutova's research is supported by the Leverhulme Trust Early Career Fellowship."}], "references": [{"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Bisson", "Josh Bleecher Snyder", "Nicolas Bouchard", "Nicolas Boulanger-Lewandowski", "Others."], "venue": "arXiv e-prints, abs/1605.0:19.", "citeRegEx": "Bisson et al\\.,? 2016", "shortCiteRegEx": "Bisson et al\\.", "year": 2016}, {"title": "Semantic classifications for detection of verb metaphors", "author": ["Beata Beigman Klebanov", "Chee Wee Leong", "E. Dario Gutierrez", "Ekaterina Shutova", "Michael Flor."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Klebanov et al\\.,? 2016", "shortCiteRegEx": "Klebanov et al\\.", "year": 2016}, {"title": "A tiered approach to the recognition of metaphor", "author": ["David Bracewell", "Marc Tomlinson", "Michael Mohler", "Bryan Rink."], "venue": "Computational Linguistics and Intelligent Text Processing, 8403:403\u2013414.", "citeRegEx": "Bracewell et al\\.,? 2014", "shortCiteRegEx": "Bracewell et al\\.", "year": 2014}, {"title": "Modelling metaphor with attribute-based semantics", "author": ["Luana Bulat", "Stephen Clark", "Ekaterina Shutova."], "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2017).", "citeRegEx": "Bulat et al\\.,? 2017", "shortCiteRegEx": "Bulat et al\\.", "year": 2017}, {"title": "Metaphor in Educational Discourse", "author": ["Lynne Cameron."], "venue": "Continuum, London.", "citeRegEx": "Cameron.,? 2003", "shortCiteRegEx": "Cameron.", "year": 2003}, {"title": "TokenLevel Metaphor Detection using Neural Networks", "author": ["Erik-L\u00e2n Do Dinh", "Iryna Gurevych."], "venue": "Proceedings of the Fourth Workshop on Metaphor in NLP.", "citeRegEx": "Dinh and Gurevych.,? 2016", "shortCiteRegEx": "Dinh and Gurevych.", "year": 2016}, {"title": "Evaluating the premises and results of four metaphor identification systems", "author": ["Jonathan Dunn."], "venue": "Proceedings of CICLing\u201913, pages 471\u2013486, Samos, Greece.", "citeRegEx": "Dunn.,? 2013", "shortCiteRegEx": "Dunn.", "year": 2013}, {"title": "From distributional semantics to feature norms: grounding semantic models in human perceptual data", "author": ["Luana Fagarasan", "Eva Maria Vecchi", "Stephen Clark."], "venue": "Proceedings of the 11th International Conference on Computational Semantics", "citeRegEx": "Fagarasan et al\\.,? 2015", "shortCiteRegEx": "Fagarasan et al\\.", "year": 2015}, {"title": "Background to FrameNet", "author": ["Charles Fillmore", "Christopher Johnson", "Miriam Petruck."], "venue": "International Journal of Lexicography, 16(3):235\u2013250.", "citeRegEx": "Fillmore et al\\.,? 2003", "shortCiteRegEx": "Fillmore et al\\.", "year": 2003}, {"title": "Catching metaphors", "author": ["Matt Gedigian", "John Bryant", "Srini Narayanan", "Branimir Ciric."], "venue": "In Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 41\u201348, New York.", "citeRegEx": "Gedigian et al\\.,? 2006", "shortCiteRegEx": "Gedigian et al\\.", "year": 2006}, {"title": "Literal and Metaphorical Senses", "author": ["E. Dar\u0131\u0301o Guti\u00e9rrez", "Ekaterina Shutova", "Tyler Marghetis", "Benjamin K. Bergen"], "venue": null, "citeRegEx": "Guti\u00e9rrez et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Guti\u00e9rrez et al\\.", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom."], "venue": "Advances in Neural Information Processing Systems, pages 1693\u2013", "citeRegEx": "Hermann et al\\.,? 2015", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Identifying metaphorical word use with tree kernels", "author": ["Dirk Hovy", "Shashank Shrivastava", "Sujay Kumar Jauhar", "Mrinmaya Sachan", "Kartik Goyal", "Huying Li", "Whitney Sanders", "Eduard Hovy."], "venue": "Proceedings of the First Workshop on Metaphor in NLP,", "citeRegEx": "Hovy et al\\.,? 2013", "shortCiteRegEx": "Hovy et al\\.", "year": 2013}, {"title": "Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics", "author": ["Douwe Kiela", "L\u00e9on Bottou."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-14).", "citeRegEx": "Kiela and Bottou.,? 2014", "shortCiteRegEx": "Kiela and Bottou.", "year": 2014}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher."], "venue": "CoRR, abs/1506.07285.", "citeRegEx": "Kumar et al\\.,? 2015", "shortCiteRegEx": "Kumar et al\\.", "year": 2015}, {"title": "Metaphors We Live By", "author": ["George Lakoff", "Mark Johnson."], "venue": "University of Chicago Press, Chicago.", "citeRegEx": "Lakoff and Johnson.,? 1980", "shortCiteRegEx": "Lakoff and Johnson.", "year": 1980}, {"title": "Semantic feature production norms for a large set of living and nonliving things", "author": ["Ken McRae", "George S Cree", "Mark S Seidenberg", "Chris McNorgan."], "venue": "Behavior Research Methods, 37.", "citeRegEx": "McRae et al\\.,? 2005", "shortCiteRegEx": "McRae et al\\.", "year": 2005}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tom\u00e1\u0161 Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of the International Conference on Learning Representations (ICLR 2013).", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proceedings of ICLR, Scottsdale, AZ.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Metaphor as a medium for emotion: An empirical study", "author": ["Saif M Mohammad", "Ekaterina Shutova", "Peter D Turney."], "venue": "Proceedings of *SEM 2016.", "citeRegEx": "Mohammad et al\\.,? 2016", "shortCiteRegEx": "Mohammad et al\\.", "year": 2016}, {"title": "Semantic signatures for example-based linguistic metaphor detection", "author": ["Michael Mohler", "David Bracewell", "Marc Tomlinson", "David Hinote."], "venue": "Proceedings of the First Workshop on Metaphor in NLP, pages 27\u201335, Atlanta, Georgia.", "citeRegEx": "Mohler et al\\.,? 2013", "shortCiteRegEx": "Mohler et al\\.", "year": 2013}, {"title": "Looking for Hyponyms in Vector Space", "author": ["Marek Rei", "Ted Briscoe."], "venue": "Proceedings of the Eighteenth Conference on Computational Natural Language Learning (CoNLL 2014), pages 68\u201377.", "citeRegEx": "Rei and Briscoe.,? 2014", "shortCiteRegEx": "Rei and Briscoe.", "year": 2014}, {"title": "Black Holes and White Rabbits : Metaphor Identification with Visual Features", "author": ["Ekaterina Shutova", "Douwe Kiela", "Jean Maillard."], "venue": "Proceedings of NAACL-HLT 2016.", "citeRegEx": "Shutova et al\\.,? 2016", "shortCiteRegEx": "Shutova et al\\.", "year": 2016}, {"title": "Unsupervised metaphor identification using hierarchical graph factorization clustering", "author": ["Ekaterina Shutova", "Lin Sun."], "venue": "Proceedings of NAACL 2013, Atlanta, GA, USA.", "citeRegEx": "Shutova and Sun.,? 2013", "shortCiteRegEx": "Shutova and Sun.", "year": 2013}, {"title": "Metaphor identification using verb and noun clustering", "author": ["Ekaterina Shutova", "Lin Sun", "Anna Korhonen."], "venue": "Proceedings of Coling 2010, pages 1002\u20131010, Beijing, China.", "citeRegEx": "Shutova et al\\.,? 2010", "shortCiteRegEx": "Shutova et al\\.", "year": 2010}, {"title": "Metaphor corpus annotated for source - target domain mappings", "author": ["Ekaterina Shutova", "Simone Teufel."], "venue": "Proceedings of LREC 2010, pages 3255\u2013 3261, Malta.", "citeRegEx": "Shutova and Teufel.,? 2010", "shortCiteRegEx": "Shutova and Teufel.", "year": 2010}, {"title": "Robust extraction of metaphor from novel data", "author": ["Tomek Strzalkowski", "George Aaron Broadwell", "Sarah Taylor", "Laurie Feldman", "Samira Shaikh", "Ting Liu", "Boris Yamrom", "Kit Cho", "Umit Boz", "Ignacio Cases", "Kyle Elliot."], "venue": "Proceedings of the", "citeRegEx": "Strzalkowski et al\\.,? 2013", "shortCiteRegEx": "Strzalkowski et al\\.", "year": 2013}, {"title": "Metaphor Detection with Cross-Lingual Model Transfer", "author": ["Yulia Tsvetkov", "Leonid Boytsov", "Anatole Gershman", "Eric Nyberg", "Chris Dyer."], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014),", "citeRegEx": "Tsvetkov et al\\.,? 2014", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2014}, {"title": "Literal and metaphorical sense identification through concrete and abstract context", "author": ["Peter D. Turney", "Yair Neuman", "Dan Assaf", "Yohai Cohen."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP \u201911,", "citeRegEx": "Turney et al\\.,? 2011", "shortCiteRegEx": "Turney et al\\.", "year": 2011}, {"title": "Visualizing high-dimensional data using tsne", "author": ["Laurens Van Der Maaten", "Geoffrey Hinton."], "venue": "Journal of Machine Learning Research, 9.", "citeRegEx": "Maaten and Hinton.,? 2008", "shortCiteRegEx": "Maaten and Hinton.", "year": 2008}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Self-adaptive hierarchical sentence model", "author": ["Han Zhao", "Zhengdong Lu", "Pascal Poupart."], "venue": "arXiv preprint arXiv:1504.05070.", "citeRegEx": "Zhao et al\\.,? 2015", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Metaphor is pervasive in our everyday communication, enriching it with sophisticated imagery and helping us to reconcile our experience in the world with our conceptual system (Lakoff and Johnson, 1980).", "startOffset": 176, "endOffset": 202}, {"referenceID": 4, "context": "Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010).", "startOffset": 104, "endOffset": 145}, {"referenceID": 25, "context": "Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010).", "startOffset": 104, "endOffset": 145}, {"referenceID": 12, "context": "They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al.", "startOffset": 88, "endOffset": 138}, {"referenceID": 9, "context": ", 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain", "startOffset": 57, "endOffset": 80}, {"referenceID": 6, "context": "types (Dunn, 2013), concreteness (Turney et al.", "startOffset": 6, "endOffset": 18}, {"referenceID": 28, "context": "types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al.", "startOffset": 33, "endOffset": 54}, {"referenceID": 26, "context": ", 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al.", "startOffset": 22, "endOffset": 49}, {"referenceID": 27, "context": ", 2013) and WordNet supersenses (Tsvetkov et al., 2014).", "startOffset": 32, "endOffset": 55}, {"referenceID": 24, "context": "In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word", "startOffset": 121, "endOffset": 166}, {"referenceID": 23, "context": "In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word", "startOffset": 121, "endOffset": 166}, {"referenceID": 2, "context": "embeddings (Bracewell et al., 2014; Shutova et al., 2016).", "startOffset": 11, "endOffset": 57}, {"referenceID": 22, "context": "embeddings (Bracewell et al., 2014; Shutova et al., 2016).", "startOffset": 11, "endOffset": 57}, {"referenceID": 14, "context": "Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which suggests that designing a specialised neural network architecture for metaphor detection will lead to improved performance.", "startOffset": 86, "endOffset": 152}, {"referenceID": 31, "context": "Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which suggests that designing a specialised neural network architecture for metaphor detection will lead to improved performance.", "startOffset": 86, "endOffset": 152}, {"referenceID": 17, "context": "word embeddings (Mikolov et al., 2013a) and the cognitively-driven attribute-based vectors (Bulat et al.", "startOffset": 16, "endOffset": 39}, {"referenceID": 3, "context": ", 2013a) and the cognitively-driven attribute-based vectors (Bulat et al., 2017), as well as a combination thereof.", "startOffset": 60, "endOffset": 80}, {"referenceID": 9, "context": "Gedigian et al. (2006) classified verbs related to MOTION and CURE within the domain of financial", "startOffset": 0, "endOffset": 23}, {"referenceID": 8, "context": "They used the maximum entropy classifier and the verbs\u2019 nominal arguments and their FrameNet roles (Fillmore et al., 2003) as features, reporting encouraging results.", "startOffset": 99, "endOffset": 122}, {"referenceID": 6, "context": "Dunn (2013) used a logistic regression classifier and high-level prop-", "startOffset": 0, "endOffset": 12}, {"referenceID": 26, "context": "Tsvetkov et al. (2014) used random forest classifier and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses.", "startOffset": 0, "endOffset": 23}, {"referenceID": 12, "context": "The work of Hovy et al. (2013) is notable as they focused on compositional rather than categorical features.", "startOffset": 12, "endOffset": 31}, {"referenceID": 22, "context": "Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora.", "startOffset": 0, "endOffset": 22}, {"referenceID": 22, "context": "Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora. For example, the feature vector for politics would contain GAME or MECHANISM terms among the frequent features. As a result, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source domain. Shutova et al. (2010) exploit this property of co-occurrence vectors to identify new", "startOffset": 0, "endOffset": 473}, {"referenceID": 23, "context": "Shutova and Sun (2013) used hierarchical clustering to derive a network of concepts in which metaphorical associations are learned in an unsupervised way.", "startOffset": 0, "endOffset": 23}, {"referenceID": 10, "context": "Guti\u00e9rrez et al. (2016) investigated metaphorical composition in the compositional distributional semantics framework.", "startOffset": 0, "endOffset": 24}, {"referenceID": 17, "context": "(2017) used dense skipgram word embeddings (Mikolov et al., 2013a) instead of the sparse distributional features.", "startOffset": 43, "endOffset": 66}, {"referenceID": 13, "context": "They learned linguistic and visual representations for both words and phrases, using skipgram and convolutional neural networks (Kiela and Bottou, 2014) respectively.", "startOffset": 128, "endOffset": 152}, {"referenceID": 18, "context": "The more recent approaches of Shutova et al. (2016) and Bulat et al.", "startOffset": 30, "endOffset": 52}, {"referenceID": 3, "context": "(2016) and Bulat et al. (2017) used dense skipgram word embeddings (Mikolov et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 3, "context": "(2016) and Bulat et al. (2017) used dense skipgram word embeddings (Mikolov et al., 2013a) instead of the sparse distributional features. Shutova et al. (2016) investigated a set of metaphor identification methods using linguistic and visual features.", "startOffset": 11, "endOffset": 160}, {"referenceID": 16, "context": "(2017) presented a metaphor identification method that uses representations constructed from human property norms (McRae et al., 2005).", "startOffset": 114, "endOffset": 134}, {"referenceID": 3, "context": "Bulat et al. (2017) have", "startOffset": 0, "endOffset": 20}, {"referenceID": 22, "context": "Our method is inspired by the findings of Shutova et al. (2016), who showed that the cosine similarity between neural embeddings of the two words in a phrase is indicative of its metaphoricity.", "startOffset": 42, "endOffset": 64}, {"referenceID": 17, "context": "trained using the distributional skip-gram objective (Mikolov et al., 2013a).", "startOffset": 53, "endOffset": 76}, {"referenceID": 3, "context": "Following Bulat et al. (2017) we experiment with two types of semantic vectors: skip-gram word embeddings and attribute-based representations.", "startOffset": 10, "endOffset": 30}, {"referenceID": 17, "context": "The word embeddings are 100-dimensional and were trained using the standard log-linear skipgram model with negative sampling of Mikolov et al. (2013b) on Wikipedia for 3 epochs, using a symmetric window of 5 and 10 negative samples per word-context pair.", "startOffset": 128, "endOffset": 151}, {"referenceID": 3, "context": "We use the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017), following Fagarasan et al.", "startOffset": 63, "endOffset": 83}, {"referenceID": 3, "context": "We use the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017), following Fagarasan et al. (2015). These representations were induced by using partial least squares regression to learn a cross-modal mapping function between the word embeddings described above and the McRae et al.", "startOffset": 63, "endOffset": 118}, {"referenceID": 3, "context": "We use the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017), following Fagarasan et al. (2015). These representations were induced by using partial least squares regression to learn a cross-modal mapping function between the word embeddings described above and the McRae et al. (2005) property-norm semantic space.", "startOffset": 63, "endOffset": 308}, {"referenceID": 27, "context": "Since TSVTRAIN and TSV-TEST were constructed differently, we follow previous work (Tsvetkov et al., 2014; Shutova et al., 2016; Bulat et al., 2017) and report performance on TSV-TEST.", "startOffset": 82, "endOffset": 147}, {"referenceID": 22, "context": "Since TSVTRAIN and TSV-TEST were constructed differently, we follow previous work (Tsvetkov et al., 2014; Shutova et al., 2016; Bulat et al., 2017) and report performance on TSV-TEST.", "startOffset": 82, "endOffset": 147}, {"referenceID": 3, "context": "Since TSVTRAIN and TSV-TEST were constructed differently, we follow previous work (Tsvetkov et al., 2014; Shutova et al., 2016; Bulat et al., 2017) and report performance on TSV-TEST.", "startOffset": 82, "endOffset": 147}, {"referenceID": 3, "context": "For both word representations we use the same embeddings as Bulat et al. (2017), which makes the results directly comparable and", "startOffset": 60, "endOffset": 80}, {"referenceID": 30, "context": "The network was optimised using AdaDelta (Zeiler, 2012) for controlling adaptive learning", "startOffset": 41, "endOffset": 55}, {"referenceID": 27, "context": "The original Fscore by Tsvetkov et al. (2014) is still the highest, as they used a range of highly-engineered features that require manual annotation, such as", "startOffset": 23, "endOffset": 46}, {"referenceID": 21, "context": "(2014) - - - 85 Shutova et al. (2016) linguistic - 73 80 76 multimodal - 67 96 79 Bulat et al.", "startOffset": 16, "endOffset": 38}, {"referenceID": 3, "context": "(2016) linguistic - 73 80 76 multimodal - 67 96 79 Bulat et al. (2017) - 85 71 77", "startOffset": 51, "endOffset": 71}, {"referenceID": 22, "context": "Our setup is more similar to the linguistic experiments by Shutova et al. (2016), where metaphor detection is performed using pretrained word embeddings.", "startOffset": 59, "endOffset": 81}, {"referenceID": 3, "context": "Recently, Bulat et al. (2017) compared different types of embeddings and showed that attribute-based representations", "startOffset": 10, "endOffset": 30}, {"referenceID": 3, "context": "Using attribute vectors instead of skip-gram embeddings gives a slight improvement, especially on the recall metric, which is consistent with the findings by Bulat et al. (2017).", "startOffset": 158, "endOffset": 178}, {"referenceID": 25, "context": "cluding the system by Tsvetkov et al. (2014) which requires hand-annotated features, the proposed similarity network outperforms all the previous systems, even improving over the multimodal system by Shutova et al.", "startOffset": 22, "endOffset": 45}, {"referenceID": 22, "context": "(2014) which requires hand-annotated features, the proposed similarity network outperforms all the previous systems, even improving over the multimodal system by Shutova et al. (2016) without requiring any", "startOffset": 162, "endOffset": 184}, {"referenceID": 3, "context": "The attribute-based SSN also improves over Bulat et al. (2017) by 5.", "startOffset": 43, "endOffset": 63}, {"referenceID": 22, "context": "Shutova et al. (2016) reported 75% F-score on this dataset with a multimodal system, after randomly separating a subset for testing.", "startOffset": 0, "endOffset": 22}, {"referenceID": 16, "context": "In this setting, the attribute-based representations did not improve performance \u2013 this is expected, as the attribute norms by McRae et al. (2005) are designed for nouns, whereas the MOH dataset is centered on verbs.", "startOffset": 127, "endOffset": 147}, {"referenceID": 10, "context": "Guti\u00e9rrez et al. (2016) annotated a dataset of adjective-noun phrases as being literal or metaphorical, and we are able to use this as an additional training resource.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": "The value on the Tsvetkov training data is different from the result in Table 3, which is due to the original attribute embeddings by Bulat et al. (2017) only containing representations for the vocabulary in the TSV dataset.", "startOffset": 134, "endOffset": 154}, {"referenceID": 3, "context": "The value on the Tsvetkov training data is different from the result in Table 3, which is due to the original attribute embeddings by Bulat et al. (2017) only containing representations for the vocabulary in the TSV dataset. In order to include the data from Guti\u00e9rrez et al. (2016), we recreated the attribute vectors for a larger vocabulary, which results in a slightly different baseline performance.", "startOffset": 134, "endOffset": 283}], "year": 2017, "abstractText": "The ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding. Yet, the majority of metaphor processing systems to date rely on handengineered features and there is still no consensus in the field as to which features are optimal for this task. In this paper, we present the first deep learning architecture designed to capture metaphorical composition. Our results demonstrate that it outperforms the existing approaches in the metaphor identification task.", "creator": "LaTeX with hyperref package"}}}