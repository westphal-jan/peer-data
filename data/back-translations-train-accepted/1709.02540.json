{"id": "1709.02540", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "The Expressive Power of Neural Networks: A View from the Width", "abstract": "The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that \\emph{depth-bounded} (e.g. depth-$2$) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for \\emph{width-bounded} ReLU networks: width-$(n+4)$ ReLU networks, where $n$ is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-$n$ ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an \\emph{exponential} bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a \\emph{polynomial} bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth is more effective than width for the expressiveness of ReLU networks.", "histories": [["v1", "Fri, 8 Sep 2017 05:00:20 GMT  (1261kb,D)", "http://arxiv.org/abs/1709.02540v1", "accepted by NIPS 2017"], ["v2", "Thu, 28 Sep 2017 01:28:09 GMT  (1261kb,D)", "http://arxiv.org/abs/1709.02540v2", "accepted by NIPS 2017 ( with some typos fixed)"], ["v3", "Wed, 1 Nov 2017 08:50:32 GMT  (3296kb,D)", "http://arxiv.org/abs/1709.02540v3", "accepted by NIPS 2017 ( with some typos fixed)"]], "COMMENTS": "accepted by NIPS 2017", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zhou lu", "hongming pu", "feicheng wang", "zhiqiang hu", "liwei wang"], "accepted": true, "id": "1709.02540"}, "pdf": {"name": "1709.02540.pdf", "metadata": {"source": "CRF", "title": "The Expressive Power of Neural Networks: A View from the Width", "authors": ["Zhou Lu", "Hongming Pu", "Feicheng Wang", "Liwei Wang"], "emails": ["1400010739@pku.edu.cn", "phmhappier@163.com", "18813027826@163.com", "huzq@pku.edu.cn", "wanglw@cis.pku.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "1.1 Related Work", "text": "As one of the most classic works, Cybenko [3] has proven that a fully connected sigmoid neural network with a single hidden layer can universally approximate any continuous univariate function on a limited domain with arbitrarily small errors. Barron [1], Hornik et al. [9], Funahashi [6] have achieved similar results, they also generalize the sigmoid function to a large class of activation functions and show that universal approximation is essentially implied by the network structure. Delalleau et al. [4] have shown that there is a family of functions that can be represented much more efficiently with deep networks than with flat ones. As the development and success of deep neural networks has lately been much more work discussing the expressive power of neural networks with theoretical values."}, {"heading": "2 Preliminaries", "text": "A neural network is a directional calculation curve in which the nodes are units of calculation and the edges describe the connection pattern between the nodes. Each node receives as input a weighted sum of activations flowing through the edges, applies a kind of activation function and releases output over the edges to other nodes. Neural networks are often organized in layers so that nodes only receive signals from the previous layer and only transmit signals to the next layer. A fully connected neural network is a layered neural network in which there is a connection between all two nodes in adjacent layers. In this paper we examine the fully connected ReLU network, which is a fully connected neural network with Rectifier Linear Unit (ReLU) is an activation function. The ReLU function LeLU: R \u2192 R can be formally defined as ReLU (x, 0) (1) as an integral network."}, {"heading": "3 Width-bounded ReLU Networks as Universal Approximator", "text": "The following theorems are the main result of this section 1. Theorem 1 (Universal Approximation Theorem for Width-Bounded ReLU Networks) For each Lebesgue integral function f: Rn \u2192 R and any > 0, there is a fully connected ReLU network with a width of dm \u2264 n + 4, so that the function FA, which is represented by this network, is fully fulfilled. (x) \u2212 FA (x) < (3) The proof of this theorem is lengthy and is shifted to the complementary material. Here, we provide an informal description of the high-grade ideal function and any predefined approximation accuracy that we explicitly construct (n + 4) ReLU network so that it can approximate the function of the given accuracy."}, {"heading": "4 Width Efficiency vs. Depth Efficiency", "text": "It goes deeper and deeper than a trend in recent years, starting with the 8-layer AlexNet [10], the 19-layer VGG [12], the 22-layer GoogLeNet [13], and finally the 152-layer and 1001-layer ResNets [8]. However, the superiority of greater depth has been extensively demonstrated in the applications of many areas. ResNet has largely advanced the state-of-the-art performance in computer vision related fields, which is claimed solely due to the extremely deep representations. Despite the great practical success, theories about the role of depth are still limited. Theoretical understanding of the strength of depth starts with analyzing depth efficiency by demonstrating the existence of deep neural networks that cannot be realized by a flat network whose size is exponentially larger. However, we argue that even for a comprehensive understanding of depth itself we need to study the dual problem of width efficiency."}, {"heading": "4.1 Experiments", "text": "We are conducting extensive experiments to provide some insights into the upper limit of such an approach. To this end, we are examining a number of network architectures with different widths; for each network architecture, we are sampling the parameters that together with the architecture represent the function we want to approximate as narrower networks; the approximation is empirically calculated as the mean square error between the target function and the approximator function, which is evaluated on a series of evenly placed inputs; for simplicity and clarity, we refer to the target architectures, which represent the assigned parameters as the target network, and the corresponding network architectures for approximator functions as the approximator networks. To be detailed, the target networks are fully connected ReLU networks of the input dimension n, the output dimension 1, width 2k2, and depth 3, for n."}, {"heading": "5 Conclusion", "text": "In this paper, we analyze the expressiveness of neural networks with a breadth view that differs from many previous work that focused on the depth view. We establish the Universal Approximation Theorem for Width-Bounded ReLU Networks, as opposed to the well-known Universal Approximation Theorem, which examines depth-limited networks. Our result shows a phase transition in terms of expressiveness when the width of a ReLU network varies in the given input dimension. We also examine the role of width for the expressiveness of neural networks: We prove that a wide network cannot be approximated by a narrow network unless with more polynomial nodes, which set a lower limit on the number of nodes for approximation. We imagine open problems whether exponential lower or polynomial upper limits hold width efficiency for the width efficiency of neural networks, which we consider crucial for a greener way to the understanding of the green print."}, {"heading": "A Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Theorem 1", "text": "We prove this theory by constructing a network architecture that can approach any function. < < / p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p."}, {"heading": "A.2 Proof of Theorem 2", "text": "The proof is long and complicated, so we first define some notations for convenience thereafter. We denote the network of A = FA, the function represented by the entire network of FA, the function represented by the kth layer of the network of Fk, A, the function represented by the kth layer of Fk, j, A, the function represented by the first k layers of the network, after being represented by Rk, A. Here we will introduce 2 definitions inspired by the depth in neural networks (Telgarsky, 2016).Definition 1: A set X is a linear block if there are t linear functions (qi)."}, {"heading": "A.3 Proof of Theorem 3", "text": "The proof: We denote the input of ~ x = (x1, x2, > >,..., xn) b, and the value of the first layer of the nodes of A by y = (y1, y2,..., ym), here m < n and letyi = (bi + m \u2211 j = 1 aijxj) + where i = 1, 2, \u00b7, n, j = 1, 2, \u00b7 \u00b7, m.bi and aij parameters of A.Since m < n, there is a non-zero vector x0 in Rn0 that meets a positive vector x0 in Rn0, the ~ x0 \u00b2 span {b1 + j = 1 a1jxj, \u00b7, bn + j = m \u00b2 j = 1 anjxj} Since changes along x0 do not affect the first layer of the net, the first layer of A is determined by itself, the first layer of A is also constant."}, {"heading": "A.4 Proof of Theorem 4", "text": "We first prove the case with the input dimension n = 1, then the extension to n > 1 cases is trivial.Proof. We select 2k4 different points x (1), x (2),.., x (2k 4) and consider functions represented by the ReLU network on them. Here, we define x (i + 2k 2j) different points x (1), x -2 \u2212 \u2212 k2, i = 1,., 2k2, j = 0, 1,., k2 \u2212 1For each ReLU network A we define a 2k4-dimensional vectorfA = (FA (x (1)), FA (x (2))),., FA (x 2k4)))).We start our proof by defining 2 lemmas.Lemma 5: We define E0 = (a (1),..., a (2k 4): 0 < a (i + 2k)."}], "references": [{"title": "R.Approximation and estimation bounds for artificial neural networks", "author": ["Barron", "Andrew"], "venue": "Machine Learning", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1994}, {"title": "Shashua.On the Expressive Power of Deep Learning: A Tensor Analysis", "author": ["Nadav Cohen", "Or Sharir", "Amnon"], "venue": "COLT", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Shamir.The Power of Depth for Feedforward", "author": ["Ronen Eldan", "Ohad"], "venue": "Neural Networks. COLT", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Nearly-tight VC-dimension bounds for piecewise linear neural networks.COLT", "author": ["Nick Harvey", "Chris liaw", "Abbas Mehrabian"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Sun.Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Halbert.Multilayer feedforward networks are universal approximators", "author": ["Hornik", "Kurt", "Stinchcombe", "Maxwell", "White"], "venue": "Neural networks", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "Hinton.ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Srikant.Why deep neural networks for funtion approximation", "author": ["R. Shiyu Liang"], "venue": "ICLR", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2017}], "referenceMentions": [{"referenceID": 0, "context": "compact domain to any desired accuracy [1][3][6][9].", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "compact domain to any desired accuracy [1][3][6][9].", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "[5] show the existence of a 3-layer network, which cannot be realized by any 2-layer to more than a constant accuracy if the size is subexponential in the dimension.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] prove the existence of classes of deep convolutional ReLU networks that cannot be realized by shallow ones if its size is no more than an exponential bound.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "In fact, as pointed out in [2], proving existence is inevitable; There is always a positive measure of network parameters such that deep nets can\u2019t be realized by shallow ones without substantially larger size.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "Barron [1], Hornik et al.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "[9] ,Funahashi [6] achieved similar results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[11] showed that in order to approximate a function which is \u0398(log 1 )-order derivable with error universally, a deep network with O(log 1 ) layers and O(poly log 1 ) weights can do but \u03a9(poly 1 ) weights will be required if there is only o(log 1 ) layers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[7] provided a nearly-tight bound for VC-dimension of neural networks, that the VC-dimension for a network with W weights and L layers will have a O(WL logW ) but \u03a9(WL log WL ) VC-dimension.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Going deeper and deeper has been a trend in recent years, starting from the 8-layer AlexNet [10], the 19-layer VGG [12], the 22-layer GoogLeNet [13], and finally to the 152-layer and 1001-layer ResNets [8].", "startOffset": 92, "endOffset": 96}, {"referenceID": 4, "context": "Going deeper and deeper has been a trend in recent years, starting from the 8-layer AlexNet [10], the 19-layer VGG [12], the 22-layer GoogLeNet [13], and finally to the 152-layer and 1001-layer ResNets [8].", "startOffset": 202, "endOffset": 205}, {"referenceID": 2, "context": "However, at the quantitative level, this theorem is very different to the depth efficiency theorems in [14][5][2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "However, at the quantitative level, this theorem is very different to the depth efficiency theorems in [14][5][2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 4, "context": "The parameters of approximator network are randomly initialized according to [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "[1] Barron, Andrew R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Machine Learning 1994 [2] Nadav Cohen, Or Sharir, Amnon Shashua.", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "NIPS 2011 [5] Ronen Eldan, Ohad Shamir.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Neural networks 1989 [7] Nick Harvey, Chris liaw, Abbas Mehrabian.", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "CVPR 2016: 770-778 [9] Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert.", "startOffset": 19, "endOffset": 22}, {"referenceID": 6, "context": "Neural networks 1989 [10] Alex Krizhevsky, Ilya Sutskever, Geoffrey E.", "startOffset": 21, "endOffset": 25}, {"referenceID": 7, "context": "NIPS 2012: 1106-1114 [11] Shiyu Liang, R.", "startOffset": 21, "endOffset": 25}], "year": 2017, "abstractText": "The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n+ 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth is more effective than width for the expressiveness of ReLU networks.", "creator": "LaTeX with hyperref package"}}}