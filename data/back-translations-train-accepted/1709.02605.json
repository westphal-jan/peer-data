{"id": "1709.02605", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Gaussian Quadrature for Kernel Features", "abstract": "Kernel methods have recently attracted resurgent interest, matching the performance of deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that $O(\\epsilon^{-2})$ samples are required to achieve an approximation error of at most $\\epsilon$. In this paper, we investigate some alternative schemes for constructing feature maps that are deterministic, rather than random, by approximating the kernel in the frequency domain using Gaussian quadrature. We show that deterministic feature maps can be constructed, for any $\\gamma &gt; 0$, to achieve error $\\epsilon$ with $O(e^{\\gamma} + \\epsilon^{-1/\\gamma})$ samples as $\\epsilon$ goes to 0. We validate our methods on datasets in different domains, such as MNIST and TIMIT, showing that deterministic features are faster to generate and achieve comparable accuracy to the state-of-the-art kernel methods based on random Fourier features.", "histories": [["v1", "Fri, 8 Sep 2017 09:17:59 GMT  (36kb)", "http://arxiv.org/abs/1709.02605v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tri dao", "christopher de sa", "christopher r\\'e"], "accepted": true, "id": "1709.02605"}, "pdf": {"name": "1709.02605.pdf", "metadata": {"source": "CRF", "title": "Gaussian Quadrature for Kernel Features", "authors": ["Tri Dao", "Christopher De Sa", "Christopher R\u00e9"], "emails": ["trid@stanford.edu,", "cdesa@cs.cornell.edu,", "chrismre@cs.stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 170 9.02 605v 1 [cs.L G] 8S ep2 01"}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "2 Related Work", "text": "Other recent work has analyzed the generalization power of the random Fourier characteristic algorithm [17] and improved the limits of its maximum error [27, 29]. While we focus here on deterministic approaches to the Fourier, we are transforming the integral and comparing it to the Monte Carlo estimates, which are not the only methods available to us."}, {"heading": "3 Kernels and Quadrature", "text": "We start with a brief overview of kernels. A kernel function encodes the similarity between sample pairs. < / p > In this work, we focus on shifts of invariant kernels (those that meet k (x, y) = k (x \u2212 y), overloading the definition of k to also refer to a function k (x, x) that is positively defined and correctly scaled. A kernel is positively defined if its gram matrix is always positively defined for all non-trivial inputs, and it is correctly scaled if k (x, x) = 1 for all x. In this setting, we use a theorem [23] that also provides the \"key recognition\" behind the random characteristics of Fourier. Theorem 1 (Bochner's theorem). A continuous shift invariantly properly scaled kernel: Rd \u00b7 Rd \u2192 R is positively defined if and only if a probability form is correct."}, {"heading": "3.1 Gaussian Quadrature", "text": "Gaussian squaring is one of the most popular techniques for one-dimensional numerical integration. The basic idea is to approximate integrals of the shape below a certain degree so that the approximation is accurate for all polynomials; D-points are sufficient for polynomials of the degree up to 2D \u2212 1. While the points and weights used by Gaussian squares depend on both the distribution of the polynomials and the parameter D, they can be efficiently calculated with orthogonal polynomials [10, 30]. Gaussian squaring provides precise results in the integration of functions well approximated by polynomials covering all subgaussian densities. Definition 2 (subgaussian distribution) says that a distribution of the kernels with parameter b and for all subordinate nuclei that can be subordinated."}, {"heading": "3.2 Polynomially-Exact Rules", "text": "Since the Gaussian kernels (2) -2 exp (12) exp (12) exp (12), in d = 25 dimensions with D = 1000 and more exact to degree R = 2).Because the Gaussian kernels (2) -2 exp (12) exp (12), in d = 25 dimensions with D = 1000 and more exact to degree R = 2. (12) exp (12) exp (12) exp (12), in d = 25 dimensions with D = 1000 and more exact to degree R = 2. (12) exp (12) exp (12), in d = 25 dimensions with D = 1000 and more exact to degree R = 2. (12) exp (12) exp (12), in d = 25 dimensions with D = 1000 and more exact to degree R = 2. (12)"}, {"heading": "3.3 Dense Grid Quadrature", "text": "The simplest way to do this is with a lattice construction (also known as a Tensor product construction). A dense lattice construction begins by incorporating the integral (1) in k (u) = Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor) Tensor (1) Tensor (1) Tensor (1) Tensor (1) Tensor) Tensor (1)"}, {"heading": "3.4 Sparse Grid Quadrature", "text": "The curse of dimensionality for squaring in the high dimensions has been studied in the numerical integration scenery for decades. [1] One of the most popular existing curse circumvention techniques is called a sparse grid or smolyak squaring [2], which was originally developed to solve partial differential equations. [2] Instead of taking the tensor product of the one-dimensional square rule, we include only points up to a fixed total plane A, so that we can construct a linear combination of dense grid square rules that achieve a similar error with exponentially fewer points than a single larger square rule. [3] The detailed construction is given in Appenditure B. Compared with polynomically accurate rules that can be quickly and easily calculated a sparse grid square rule (see Algorithm 4.1 out of [12]).To measure the performance of the sparse grid grid map, we construct the preceding 25 dimensions for a section similar to a square map."}, {"heading": "3.5 Reweighted grid quadrature", "text": "We now describe a data adaptive method to square a given number of samples: a reweighting of the grid points to minimize the difference between the approximate and the exact kernel on a small subset of data. Adapting the grid to the data distribution results in better kernel approximation. We approach the grid (x \u2212 y) with k (x \u2212 y) to minimize the difference between the approximate and the exact kernel on a small subset of data. D i = 1 ai cos (x \u2212 y)), where ai \u2265 0, as k is real. We first select the set of potential grid points to 1,., as D by sampling from a dense grid of Gaussian square points. To solve the weights a1."}, {"heading": "4 Sparse ANOVA Quadrature", "text": "One type of kernel commonly used in machine learning, for example in structural models that we can relate to each other, is the sparse ANOVA rate. (It is) every single rate of real data sets (18, 20), as we will see them in Section 5. (It is) a random kernel of this type can be written (x, y) = [S] k1 (xi \u2212 yi), where S is a set of subsets of variables in {1,., and k1 is a one-dimensional kernel that we will not discuss here, including the use of various one-dimensional kernels for each element of the products, and the weighting of the sum.) Sparse ANOVA kernels are used to create thrifty dependencies between variables.Two variables are related to each other when they are related."}, {"heading": "5 Experiments", "text": "In order to evaluate the performance of a deterministic network, we analysed the accuracy of a sparsely measured ANOVA core on the one hand, and the ability to position ourselves on the other hand. (...) It is about the way we have behaved in recent years. (...) It is about the way we have behaved. (...) It is about the way we have behaved. (...) It is about the way we have behaved. (...) It is about the way we act. (...) It is about the way we act. (...) It is about the way we act. (...) It is about the way we act. \"(...) It is about the way we act. (...) It is about the way we act. (...) It is about the way we act."}, {"heading": "6 Conclusion", "text": "We presented deterministic characteristic maps for kernel machines. We demonstrated that we can achieve better scaling with the desired accuracy compared to the state-of-the-art method, random Fourier features. We described several ways to construct these characteristic maps, including polynomically exact squares, dense grid constructions, sparse grid constructions and reweighted grid constructions. Our results can be applied well to the case of sparse ANOVA cores and achieve significant improvements (depending on dimension d) over random Fourier features. Finally, we evaluated our results experimentally and showed that ANOVA cores with deterministic feature maps can generate a comparable accuracy to the state-of-the-art methods based on random Fourier features on real data sets. ANOVA cores are an example of how structure can be used to define better cores."}, {"heading": "Acknowledgments", "text": "We thank the Defense Advanced Research Projects Agency (DARPA) for its support of the SIMPLEX program under numbers N66001-15-C-4043, DARPA FA8750-12-2-0335 and FA8750-13-2-0039, DOE 108845, National Institute of Health (NIH) U54EB020405, the National Science Foundation (NSF) under numbers CCF-1563078, the Office of Naval Research (ONR) under numbers N000141210041 and N000141310129, the Moore Foundation, the Okawa Research Grant, American Family Insurance, Accenture, Toshiba and Intel. This material is based on research sponsored by DARPA under contract number FA8750-17-2-0095. The U.S. government is authorized to reproduce and distribute government reports, regardless of any copyright notices, Toshiba and Intel."}, {"heading": "A Dense grid construction", "text": "If we allow the Gaussian squaring rule to apply to each integral, then we can approximate k with k (u) = d = 1Lk (l) = 1ai, l exp (j\u03c9i, le T i u). If we define al = q = i = 1 ai, li and \u03c9l = \u2211 d i = 1 \u03c9i, liei, then we have the tensor product square rulek (u) = 1 {1... Li} al exp (j\u03c9T l u) (4) over D = L = Li points - we can simplify this to L d if each Li = L."}, {"heading": "B Sparse grid construction", "text": "Let us start by allowing Gi (L) to be the approximation of ki (ui) resulting from the application of the one-dimensional Gaussian quadrature rule with L-points: For the corresponding sample points and weights, this means that we can decompose ki (ui) as infinite sumki (ui) = 1al exp (jui\u03c9l). One of the properties of Gaussian quadrature is that it lies exactly in the boundary of the capital L. In particular, this boundary means that we can represent ki (ui) as upper sumki (ui) = Gi (1) + \u00b2 m = 1 (Gi (2 m) \u2212 Gi (2 \u2212 1)) = \u00b2 m \u00b2 m \u00b2, for suitably defined i, m \u00b2 m \u00b2. In order to represent k (u) as infinite sumki (ui) = Gi (1) m \u00b2, it is sufficient to form the product k (u) = 1 \u00b2 m \u00b2, we can only form this square of this 1 square that k (1)."}, {"heading": "C Proofs", "text": "C.1 Proof of theorem 2To prove these theorems, we need an answer (2) to each x, (x + 12) log x (2) log x (x + 12) log x (x + 1) log x (x + 1) log x (x + 1) log x (x + 1) log x (x + 1) log x (x + 1) log x (x + 1) log x (x + 1) log x (x + 1) log x (x + 1) log x (x + 1) log x (p2) log x (p2) log x (p2) log x! We now prove the theorem.Proof the theorem 2. For each x, define x (x) p-th moment is the error function, asp-th moment is limited by E (x) (x)."}], "references": [{"title": "On the equivalence between quadrature rules and random features", "author": ["Francis Bach"], "venue": "arXiv preprint arXiv:1502.06800,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A method for numerical integration on an automatic computer", "author": ["Charles W Clenshaw", "Alan R Curtis"], "venue": "Numerische Mathematik,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1960}, {"title": "Maximum likelihood linear transformations for hmm-based speech recognition", "author": ["Mark JF Gales"], "venue": "Computer speech & language,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "DARPA TIMIT acoustic phonetic continuous speech corpus CDROM", "author": ["J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett", "N.L. Dahlgren"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Convolutional sequence to sequence learning", "author": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N Dauphin"], "venue": "arXiv preprint arXiv:1705.03122,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2017}, {"title": "Structural Modelling with Sparse Kernels", "author": ["S.R. Gunn", "J.S. Kandola"], "venue": "Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Structural modelling with sparse kernels", "author": ["Steve R. Gunn", "Jaz S. Kandola"], "venue": "Machine learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Fast and accurate computation of Gauss\u2013Legendre and Gauss\u2013Jacobi quadrature nodes and weights", "author": ["Nicholas Hale", "Alex Townsend"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Kernel methods in machine learning", "author": ["Thomas Hofmann", "Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": "The annals of statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Sparse grid quadrature in high dimensions with applications in finance and insurance, volume 77", "author": ["Markus Holtz"], "venue": "Springer Science & Business Media,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Kernel methods match deep neural networks on TIMIT", "author": ["Po-Sen Huang", "Haim Avron", "Tara N Sainath", "Vikas Sindhwani", "Bhuvana Ramabhadran"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Analysis of numerical methods", "author": ["Eugene Isaacson", "Herbert Bishop Keller"], "venue": "Courier Corporation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "arXiv preprint arXiv:1408.5882,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "On the sample complexity of random Fourier features for online learning: How many random Fourier features do we need", "author": ["Ming Lin", "Shifeng Weng", "Changshui Zhang"], "venue": "ACM Trans. Knowl. Discov. Data,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "How to scale up kernel methods to be as good as deep neural nets", "author": ["Zhiyun Lu", "Avner May", "Kuan Liu", "Alireza Bagheri Garakani", "Dong Guo", "Aur\u00e9lien Bellet", "Linxi Fan", "Michael Collins", "Brian Kingsbury", "Michael Picheny", "Fei Sha"], "venue": "URL http://arxiv.org/abs/1411.4000", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Fast and accurate digit classification", "author": ["Subhransu Maji", "Jitendra Malik"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Kernel approximation methods for speech recognition", "author": ["Avner May", "Alireza Bagheri Garakani", "Zhiyun Lu", "Dong Guo", "Kuan Liu", "Aur\u00e9lien Bellet", "Linxi Fan", "Michael Collins", "Daniel Hsu", "Brian Kingsbury"], "venue": "arXiv preprint arXiv:1701.03577,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2017}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Fourier analysis on groups", "author": ["Walter Rudin"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1990}, {"title": "Learning with kernels: Support vector machines, regularization, optimization, and beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J Smola"], "venue": "MIT press,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2002}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["Patrice Y Simard", "Dave Steinkraus", "John C Platt"], "venue": "In ICDAR,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "Quadrature and interpolation formulas for tensor products of certain class of functions", "author": ["S.A. Smolyak"], "venue": "Dokl. Akad. Nauk SSSR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1963}, {"title": "Optimal rates for random Fourier features", "author": ["Bharath Sriperumbudur", "Zoltan Szabo"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Support vector regression with anova decomposition kernels. Advances in kernel methods\u2014Support vector learning, pages", "author": ["M Stitson", "Alex Gammerman", "Vladimir Vapnik", "Volodya Vovk", "Chris Watkins", "Jason Weston"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1999}, {"title": "On the error of random Fourier features", "author": ["Dougal J. Sutherland", "Jeff Schneider"], "venue": "In Proceedings of the 31th Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Fast computation of Gauss quadrature nodes and weights on the whole real line", "author": ["Alex Townsend", "Thomas Trogdon", "Sheehan Olver"], "venue": "IMA Journal of Numerical Analysis, page drv002,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Is Gauss quadrature better than Clenshaw\u2013Curtis", "author": ["Lloyd N Trefethen"], "venue": "SIAM review,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Quasi-Monte Carlo feature maps for shift-invariant kernels", "author": ["Jiyan Yang", "Vikas Sindhwani", "Haim Avron", "Michael Mahoney"], "venue": "arXiv preprint arXiv:1412.8293,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}], "referenceMentions": [{"referenceID": 21, "context": "1 Introduction Kernel machines are frequently used to solve a wide variety of problems in machine learning [24].", "startOffset": 107, "endOffset": 111}, {"referenceID": 10, "context": "They have gained resurgent interest and have recently been shown [13, 18, 20] to again match (and sometimes exceed) the performance of deep neural networks for some tasks such as speech recognition on large datasets.", "startOffset": 65, "endOffset": 77}, {"referenceID": 15, "context": "They have gained resurgent interest and have recently been shown [13, 18, 20] to again match (and sometimes exceed) the performance of deep neural networks for some tasks such as speech recognition on large datasets.", "startOffset": 65, "endOffset": 77}, {"referenceID": 17, "context": "They have gained resurgent interest and have recently been shown [13, 18, 20] to again match (and sometimes exceed) the performance of deep neural networks for some tasks such as speech recognition on large datasets.", "startOffset": 65, "endOffset": 77}, {"referenceID": 18, "context": "Rahimi and Recht [21] proposed a solution to this problem: approximating the kernel with an inner product in a higher-dimensional space.", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "In the case of shift-invariant kernels, one technique that was proposed in [21] for constructing the function z is random Fourier features.", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "Rahimi and Recht [21] proved that if the feature map has dimension D = \u03a9\u0303 ( d \u01eb ) then with constant probability, the approximation \u3008z(x), z(y)\u3009 is uniformly \u01eb-close to the true kernel on a bounded set.", "startOffset": 17, "endOffset": 21}, {"referenceID": 17, "context": "Moreover, for a particular class of kernels called sparse ANOVA kernels (also known as convolutional kernels as they are similar to the convolutional layer in CNNs) which have shown stateof-the-art performance in speech recognition [20], deterministic maps require fewer samples than random Fourier features, both in terms of the desired error and the kernel size.", "startOffset": 232, "endOffset": 236}, {"referenceID": 19, "context": "The random Fourier features method has been analyzed in the context of several learning algorithms, and its generalization error has been characterized and compared to that of other kernel-based algorithms [22].", "startOffset": 206, "endOffset": 210}, {"referenceID": 14, "context": "Other recent work has analyzed the generalization performance of the random Fourier features algorithm [17], and improved the bounds on its maximum error [27, 29].", "startOffset": 103, "endOffset": 107}, {"referenceID": 24, "context": "Other recent work has analyzed the generalization performance of the random Fourier features algorithm [17], and improved the bounds on its maximum error [27, 29].", "startOffset": 154, "endOffset": 162}, {"referenceID": 26, "context": "Other recent work has analyzed the generalization performance of the random Fourier features algorithm [17], and improved the bounds on its maximum error [27, 29].", "startOffset": 154, "endOffset": 162}, {"referenceID": 29, "context": "This approach was analyzed in [32], and they show that it achieves an asymptotic error of \u01eb = O ( D\u22121 (log(D)) )", "startOffset": 30, "endOffset": 34}, {"referenceID": 0, "context": "Bach [1] analyzed in detail the connection between quadrature and random feature expansions, thus deriving bounds for the number of samples required to achieve a given average approximation error (though they did not present complexity results regarding maximum error nor suggested new feature maps).", "startOffset": 5, "endOffset": 8}, {"referenceID": 28, "context": "This connection allows us to leverage longstanding deterministic numerical integration methods such as Gaussian quadrature [6, 31] and sparse grids [2].", "startOffset": 123, "endOffset": 130}, {"referenceID": 25, "context": "Sparse ANOVA kernels have been shown [28] to work well for many classification tasks, especially in structural modeling problems that benefit from both the good generalization of a kernel machine and the representational advantage of a sparse model [9].", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "Sparse ANOVA kernels have been shown [28] to work well for many classification tasks, especially in structural modeling problems that benefit from both the good generalization of a kernel machine and the representational advantage of a sparse model [9].", "startOffset": 249, "endOffset": 252}, {"referenceID": 20, "context": "In this setting, our results make use of a theorem [23] that also provides the \u201ckey insight\u201d behind the random Fourier features method.", "startOffset": 51, "endOffset": 55}, {"referenceID": 7, "context": "While the points and weights used by Gaussian quadrature depend both on the distribution \u039b and the parameter D, they can be computed efficiently using orthogonal polynomials [10, 30].", "startOffset": 174, "endOffset": 182}, {"referenceID": 27, "context": "While the points and weights used by Gaussian quadrature depend both on the distribution \u039b and the parameter D, they can be computed efficiently using orthogonal polynomials [10, 30].", "startOffset": 174, "endOffset": 182}, {"referenceID": 11, "context": "2 Polynomially-Exact Rules Since Gaussian quadrature is so successful in one dimension, as commonly done in the numerical analysis literature [14], we might consider using quadrature rules that are multidimensional analogues of Gaussian quadrature \u2014 rules that are accurate for all polynomials up to a certain degree R.", "startOffset": 142, "endOffset": 146}, {"referenceID": 1, "context": "In this paper, we focus on Gaussian quadrature, although we could also use other methods such as Clenshaw-Curtis [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 23, "context": "One of the more popular existing techniques for getting around the curse is called sparse grid or Smolyak quadrature [26], originally developed to solve partial differential equations.", "startOffset": 117, "endOffset": 121}, {"referenceID": 9, "context": "1 from [12]).", "startOffset": 7, "endOffset": 11}, {"referenceID": 8, "context": "4 Sparse ANOVA Quadrature One type of kernel that is commonly used in machine learning, for example in structural modeling, is the sparse ANOVA kernel [11, 8].", "startOffset": 151, "endOffset": 158}, {"referenceID": 5, "context": "4 Sparse ANOVA Quadrature One type of kernel that is commonly used in machine learning, for example in structural modeling, is the sparse ANOVA kernel [11, 8].", "startOffset": 151, "endOffset": 158}, {"referenceID": 15, "context": "These kernels have achieved state-of-the-art performance on large real-world datasets [18, 20], as we will see in Section 5.", "startOffset": 86, "endOffset": 94}, {"referenceID": 17, "context": "These kernels have achieved state-of-the-art performance on large real-world datasets [18, 20], as we will see in Section 5.", "startOffset": 86, "endOffset": 94}, {"referenceID": 13, "context": "5 Experiments To evaluate the performance of deterministic feature maps, we analyzed the accuracy of a sparse ANOVA kernel on the MNIST digit classification task [16] and the TIMIT speech recognition task [5].", "startOffset": 162, "endOffset": 166}, {"referenceID": 3, "context": "5 Experiments To evaluate the performance of deterministic feature maps, we analyzed the accuracy of a sparse ANOVA kernel on the MNIST digit classification task [16] and the TIMIT speech recognition task [5].", "startOffset": 205, "endOffset": 208}, {"referenceID": 16, "context": "79%) on this problem [19].", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "The ANOVA kernel we construct is designed to have a similar structure to the first layer of a convolutional neural network [25].", "startOffset": 123, "endOffset": 127}, {"referenceID": 2, "context": "Each data point correspond to a frame (10ms) of audio data, preprocessed using the standard feature space Maxmimum Likelihood Linear Regression (fMMLR) [4].", "startOffset": 152, "endOffset": 155}, {"referenceID": 17, "context": "We use the same setup as [20], and the performance here matches both that of random Fourier features and deep neural networks in [20].", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "We use the same setup as [20], and the performance here matches both that of random Fourier features and deep neural networks in [20].", "startOffset": 129, "endOffset": 133}, {"referenceID": 12, "context": "Given CNNs\u2019 recent success in other domains beside images, such as sentence classification [15] and machine translation [7], we hope that our work on deterministic feature maps will enable kernel methods such as ANOVA kernels to find new areas of application.", "startOffset": 91, "endOffset": 95}, {"referenceID": 4, "context": "Given CNNs\u2019 recent success in other domains beside images, such as sentence classification [15] and machine translation [7], we hope that our work on deterministic feature maps will enable kernel methods such as ANOVA kernels to find new areas of application.", "startOffset": 120, "endOffset": 123}, {"referenceID": 0, "context": "References [1] Francis Bach.", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "[3] Charles W Clenshaw and Alan R Curtis.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] Mark JF Gales.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] J.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[7] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[8] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[9] Steve R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[10] Nicholas Hale and Alex Townsend.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] Thomas Hofmann, Bernhard Sch\u00f6lkopf, and Alexander J Smola.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] Markus Holtz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] Po-Sen Huang, Haim Avron, Tara N Sainath, Vikas Sindhwani, and Bhuvana Ramabhadran.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[14] Eugene Isaacson and Herbert Bishop Keller.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[15] Yoon Kim.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17] Ming Lin, Shifeng Weng, and Changshui Zhang.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[18] Zhiyun Lu, Avner May, Kuan Liu, Alireza Bagheri Garakani, Dong Guo, Aur\u00e9lien Bellet, Linxi Fan, Michael Collins, Brian Kingsbury, Michael Picheny, and Fei Sha.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] Subhransu Maji and Jitendra Malik.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] Avner May, Alireza Bagheri Garakani, Zhiyun Lu, Dong Guo, Kuan Liu, Aur\u00e9lien Bellet, Linxi Fan, Michael Collins, Daniel Hsu, Brian Kingsbury, et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] Ali Rahimi and Benjamin Recht.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] Ali Rahimi and Benjamin Recht.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] Walter Rudin.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[24] Bernhard Sch\u00f6lkopf and Alexander J Smola.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[25] Patrice Y Simard, Dave Steinkraus, and John C Platt.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[26] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[27] Bharath Sriperumbudur and Zoltan Szabo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[28] M Stitson, Alex Gammerman, Vladimir Vapnik, Volodya Vovk, Chris Watkins, and Jason Weston.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[29] Dougal J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[30] Alex Townsend, Thomas Trogdon, and Sheehan Olver.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] Lloyd N Trefethen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[32] Jiyan Yang, Vikas Sindhwani, Haim Avron, and Michael Mahoney.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Kernel methods have recently attracted resurgent interest, matching the performance of deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that O(\u01eb) samples are required to achieve an approximation error of at most \u01eb. In this paper, we investigate some alternative schemes for constructing feature maps that are deterministic, rather than random, by approximating the kernel in the frequency domain using Gaussian quadrature. We show that deterministic feature maps can be constructed, for any \u03b3 > 0, to achieve error \u01eb with O(e + \u01eb) samples as \u01eb goes to 0. We validate our methods on datasets in different domains, such as MNIST and TIMIT, showing that deterministic features are faster to generate and achieve comparable accuracy to the state-of-the-art kernel methods based on random Fourier features.", "creator": "gnuplot 5.0 patchlevel 6"}}}