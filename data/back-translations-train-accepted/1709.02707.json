{"id": "1709.02707", "review": {"conference": "nips", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Optimally Learning Populations of Parameters", "abstract": "Consider the following fundamental estimation problem: there are $n$ entities, each with an unknown parameter $p_i \\in [0,1]$, and we observe $n$ independent random variables, $X_1,\\ldots,X_n$, with $X_i \\sim $ Binomial$(t, p_i)$. How accurately can one recover the \"histogram\" (i.e. cumulative density function) of the $p_i$s? While the empirical estimates would recover the histogram to earth mover distance $\\Theta(\\frac{1}{\\sqrt{t}})$ (equivalently, $\\ell_1$ distance between the CDFs), we show that, provided $n$ is sufficiently large, we can achieve error $O(\\frac{1}{t})$ which is information theoretically optimal. We also extend our results to the multi-dimensional parameter case, capturing settings where each member of the population has multiple associated parameters. Beyond the theoretical results, we demonstrate that the recovery algorithm performs well in practice on a variety of datasets, providing illuminating insights into several domains, including politics, and sports analytics.", "histories": [["v1", "Fri, 8 Sep 2017 13:53:26 GMT  (180kb,D)", "http://arxiv.org/abs/1709.02707v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kevin tian", "weihao kong", "gregory valiant"], "accepted": true, "id": "1709.02707"}, "pdf": {"name": "1709.02707.pdf", "metadata": {"source": "CRF", "title": "Optimally Learning Populations of Parameters", "authors": ["Kevin Tian"], "emails": ["kjtian@stanford.edu", "whkong@stanford.edu", "valiant@stanford.edu"], "sections": [{"heading": null, "text": "t) (corresponding to '1 distance between CDFs), we show that, provided n is sufficiently large, we can achieve error O (1t), which is theoretically optimal. We also extend our results to the case of multidimensional parameters and capture settings where each member of the population has several associated parameters. Beyond the theoretical results, we show that the recovery algorithm works well in practice on a variety of data sets and provides illuminating insights into various areas, including politics and sports analysis."}, {"heading": "1 Introduction", "text": "This year, it has come to the point where it will be able to put itself at the top, \"he said in an interview with the\" Welt am Sonntag. \""}, {"heading": "1.1 Related Work", "text": "The secondary conditions in which we find ourselves are more diverse than ever before. \"The secondary conditions in which we find ourselves are more diverse than ever before."}, {"heading": "1.2 Our contributions and organization of paper", "text": "The novel theoretical results of this work are the efficient distribution estimation algorithm and the detection of theorems 1 and 2 whose errors asymptotically reach the lower limit. In Section 2 we describe the evidence approach in the univariate environment; the extension to the multidimensional environment is described in Section 2.4. In Section 3 we validate the empirical performance of our approach to synthetic data and illustrate its potential applications in several real scenarios."}, {"heading": "2 Learning the empirical distribution of binomial parameters", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Moment estimation", "text": "Our approach to the method of moments is based on the estimation of the first t moments of P, namely E [pk], where p \u0445 P, for k between 1 and t. We limit ourselves to the first t moments, because, as the proof for the lower limit shows, the distribution of the Xis is determined by the first t moments, and therefore no additional information about the higher moments can be obtained. For 1 \u2264 k \u2264 t there is only one unbiased estimator for pki, which uses all available samples Xij \u0445 Bernoulli (pi) (t k) (1) k) (1) (1) The motivation for this unbiased estimator is the following: for each 1 \u2264 i \u2264 n there is only one unbiased estimator for pki, using all available samples Xij \u0445 Bernoulli (pi). In particular, it should be noted that each k i.i.d. sample of a variable distributed according to Bernoulli (pi) is an unbiased estimator for pki."}, {"heading": "2.2 Earth mover\u2019s distance bounds of recovered distributions", "text": "Given the ability to accurately recreate moments, we show that we can use these estimates to restore a distribution that is not too far from the truth. We define the Waterstone (Earth Mover) distance between two distributions P and Q: Definition 1. The Waterstone, or Earth Mover's, distance between distributions P, Q, is | P \u2212 Q | | W: = inf\u03b3 (P, Q) whose marginals match the distributions. The equivalent dual definition is | P \u2212 Q | W: = Jackson (x, y), where the distance (P, Q) is the set of all couplings to P and Q, namely a distribution whose marginals match the distributions."}, {"heading": "2.3 Our distribution learning algorithm", "text": "The algorithm consists of two steps. First, we calculate estimates of the moments of P from the samples, as described in Eq.1. Next, we use a linear program to assign weights on a mesh of [0, 1] to assign to the estimates. So, the distance between any distribution and its discrete rounding is potentially limited by 2, and the difference in moments spreads also as O (), because the individual contributions of the point masses cannot change at the kth moment more than O (k) < O (). So we choose = 1 m, where m is the number of points in the mesh, fine enough that they do not cover the distance by much.Formally, the algorithm takes as input Xij, for 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 t, parameters s (moments that must be included in the calculation), granularity m, and weights w. Note that the additional parameter that our algorithm needs s, due to the fact that \"we can only choose the first moment.\""}, {"heading": "2.4 Extension: multivariate distribution estimation", "text": "Suppose that each member of a population i of size n has two interconnected binomial parameters p (i, 1), p (i, 2), as in Theorem 2. One could estimate the boundary distribution of p (i, 1) and p (i, 2) separately using algorithm 1, but it is also natural to estimate the joint distribution up to a small Waterstone distance in the 2d sense. This idea can be extended to d-dimensional distributions to binomial parameters. Essentially, the idea is also to include a series of reliable moments represented by multi-indices \u03b1 with | \u03b1 | \u2264 s, for about 1 \u2264 s \u2264 t. In a 2-d setting, for example, the moments for members of the population like Epi \u0445 P [pa (i, 1) p b (i, 2)] would be represented. Again, it remains to be seen how close an interpolating polynomial can come to any Lipschitz function, and the size of the coefficient of that polymer (1) will be subordinated from one."}, {"heading": "3 Empirical performance", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Recovering distributions with known ground truth", "text": "We start by proving the effectiveness of our algorithm on multiple synthetic datasets. We looked at three different options for an underlying distribution P-over [0, 1], then drew n independent samples p1,..., pn \u2190 P. For one parameter t, for each i-value {1,.., n} we then drew Xi \u2190 Binomial (t, pi) and ran our population estimation algorithm on the set X1,..., Xn and measured the extent to which we restored the distribution P-value. In all scenarios n was sufficiently large that there was little difference between the histogram giving the set {p1,...., pn} and the distribution P-value. Results for three options of the underlying distribution P-value are shown in Figure 1.Figure 2. This shows representative charts of CDFs of the recovered histograms and empirical histograms for each of the three distributions P-value considered."}, {"heading": "3.2 Political tendencies on a county level", "text": "We conducted a case study on the political tendencies of counties, using the following rough model: n = 3116 counties each have an intrinsic \"political-tendentious\" parameter pi, which indicates their likelihood of voting Republican voters in a particular election, and there are t = 8 elections taken from the corresponding Bernoulli distribution; the specific dataset we looked at was the voting pattern that is binary for Republican and Democratic parties in each of the counties in the U.S. between 1976 and 2004. Thus, we were able to generate estimates of the first 8 moments, and used Algorithm 1B to recover an estimate of the histogram of that group of episodes. Taking into account 4-8 moments of the recovery algorithm produced CDFs, which were all fairly consistent but with varying degrees of smoothness, one example of a conclusion we can draw is that approximately 310 counties are strongly Democratic in most of their counties and 0.4 in most of their other counties."}, {"heading": "3.3 Game-to-game shooting of NBA players", "text": "We conducted a case study on the shooting of two NBA players. One can think of this experiment as the question of whether NBA players have differences in their intrinsic ability to shoot field goals from game to game (in the world of sports analysis this is the idea of \"hot / cold\" shooting nights).The experimental setup consists in the fact that for each game i of n there is a Pi that represents the latent shot percentage of the player for that game. Surely, one would expect that the empirical shooting of a player varies significantly - learning the distribution of this intrinsic shot percentage allows us to determine how much deviation is caused by natural noise and how much change is caused by the latent parameters. The data set used was the 3-point shooting per game of a player with sufficient statistics on \"3 made pointers\" and \"3 targeted attempts.\" To make estimates for the kest moment, we looked at games where Golden State's Stephen Curry and the Warriors were trying to be at least 3 consistent."}, {"heading": "A Polynomial interpolation, exact moment matching case", "text": "The idea of this section is to prove the existence of a polynomial at a small distance from any fixed Lipschitz h in the norm of infinity (as in the proof of theorem 3). We remember the following classical result [5]: Fact 1: For a given function h = Cs + 1 [0, 1] there is a polynomial of degree s f, which is at most 12s + 1 | | h (s + 1) | | 3 (s + 1) |. From (4) on we would like an upper limit for the | | h \u2212 P | | 3, which is at most h \u2212 h."}, {"heading": "B Error propagation in polynomial interpolation", "text": "As in the proof of the non-robust version in Section A, we have the boundary between the integral principle of h and P \u2212 Q \u2212 Q = Q = Q = Q = Q = Q = Q (1) (8), where the vector of the coefficients of polynomial f is used in interpolation. Thus, it is sufficient to be bound in absolute value the magnitude of the ith coefficient of f. Note: f is a polymial approximation of h that we can construct by interpolation on the Chebyshev node. To be precise, c = V \u2212 1s + 1s is the s + 1 s expectation of s + 1 Vandermonde matrix on the Chebyshev node, and y is the value of h-Lipschitz."}, {"heading": "D Multivariate error analysis", "text": "First, we will prove a stronger version of Lemma 2: Lemma 4 If each Lipschitz function f is based on [0, 1] d, there is a degree s-polynomial p (x) = \u2211 | \u2264 s-a\u03b1x \u03b1, where \u03b1 is multi-index {\u03b11, \u03b12,. \u03b1d} such a sup x-p (x) \u2212 f (x) | \u2264 Cd s, (12) and a\u03b1 \u2264 Ad (2s) d2s3 | \u03b1. This polynomatic approximation problem is basically a reformulation of theorem 1 in [3]. What we need to do is to specify an explicit upper limit of the coefficients. The high-level idea is to first entangle f with a holomorphic function f (= holomorphic function G, which shows that the Maclaurin series of H is a good polynomial approximation of H and also f is the definition of H-S-function, i.e. an open S-S-S connection."}, {"heading": "E Properties of moment estimates", "text": "Remember that the estimator we have derived for pki is (\u2211 1 [Xij = 1] k) (t k) (13), where the counter counts the number of subsets of the quantity k, which are all 1, and the denominator is the number of subsets of the quantity k. We will show the following properties of the estimator: He is the only unbiased estimator who uses all the information and has a restrained variance. First, let us imagine an unbiased estimator f of pki, which uses all the t-bits of information, a function of sufficient statistic m, the number of 1s within the Xij. Then we must have m = 0 f (m) (t m) pmi (1 \u2212 Xipi) t \u2212 m = pki \u00b2 pi (14), which is a polynomial of degree t in pi."}, {"heading": "F Lateness in flights", "text": "We conducted a case study on the 2015 record of flight delays and cancellations, which was evaluated from Kaggle through the Department of Transportation, which records the delay of departure and arrival times on approximately 2,000,000 flights, as well as flight identifiers (such as airline, flight number, departure and arrival cities).A single \"flight\" could be an aircraft that routinely flies every other day from Los Angeles to Seattle. The estimate problem concerns a certain threshold c and a given routine flight, say c = 15 minutes, in order to predict the percentage of instances of that flight that have at least c. We looked at routine flights for which there were records of at least 50 instances. Defining a threshold c, we defined the truth percentages for instances of the \"flight is at least c late\" in order to predict the empirical ratio of all instances of the flight. There were 25,156 such flights, for each given flight, we calculated the substance and the substance of the instances."}], "references": [{"title": "A unified maximum likelihood approach for optimal distribution property estimation", "author": ["Jayadev Acharya", "Hirakendu Das", "Alon Orlitsky", "Ananda Theertha Suresh"], "venue": "arXiv preprint arXiv:1611.02960,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Recent results on pattern maximum likelihood", "author": ["Jayadev Acharya", "Alon Orlitsky", "Shengjun Pan"], "venue": "In Networking and Information Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Multivariate simultaneous approximation", "author": ["Thomas Bagby", "Len Bos", "Norman Levenberg"], "venue": "Constructive Approximation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Learning poisson binomial distributions", "author": ["Constantinos Daskalakis", "Ilias Diakonikolas", "Rocco A Servedio"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Properly learning poisson binomial distributions in almost polynomial time", "author": ["Ilias Diakonikolas", "Daniel M Kane", "Alistair Stewart"], "venue": "In Conference on Learning Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "On the exact space complexity of sketching and streaming small norms", "author": ["Daniel M Kane", "Jelani Nelson", "David P Woodruff"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "The Art of Computer Programming: Volume 1: Fundamental Algorithms (3rd Edition)", "author": ["Donald E. Knuth"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Spectrum estimation from samples", "author": ["Weihao Kong", "Gregory Valiant"], "venue": "arXiv preprint arXiv:1602.00061,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "On modeling profiles instead of values", "author": ["Alon Orlitsky", "Narayana P Santhanam", "Krishnamurthy Viswanathan", "Junan Zhang"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Optimal prediction of the number of unseen species", "author": ["Alon Orlitsky", "Ananda Theertha Suresh", "Yihong Wu"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Inadmissibility of the usual estimator for the mean of a multivariate normal distribution", "author": ["Charles Stein"], "venue": "In Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1956}, {"title": "Estimating the unseen: an n/log(n)-sample estimator for entropy and support size, shown optimal via new clts", "author": ["Gregory Valiant", "Paul Valiant"], "venue": "In Proceedings of the forty-third annual ACM symposium on Theory of computing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Estimating the unseen: improved estimators for entropy and other properties", "author": ["Gregory Valiant", "Paul Valiant"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Instance optimal learning of discrete distributions", "author": ["Gregory Valiant", "Paul Valiant"], "venue": "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Consider the following fundamental estimation problem: there are n entities, each with an unknown parameter pi \u2208 [0, 1], and we observe n independent random variables,X1, .", "startOffset": 113, "endOffset": 119}, {"referenceID": 10, "context": "This ability to \u201cdenoise\u201d the empirical estimate of a parameter based on the observations of a number of independent random variables (in this case, the outcomes of the tosses of the other coins), was first pointed out by Charles Stein in the setting of estimating the means of a set of Gaussians and is known as \u201cStein\u2019s phenomena\u201d [12].", "startOffset": 333, "endOffset": 337}, {"referenceID": 0, "context": ", pn with pi \u2208 [0, 1], and we observe the outcomes of t independent flips of each coin, namely X1, .", "startOffset": 15, "endOffset": 21}, {"referenceID": 0, "context": "There is an algorithm that produces a distribution Q supported on [0, 1], such that with probability at least 1\u2212 \u03b4 over the randomness of X1, .", "startOffset": 66, "endOffset": 72}, {"referenceID": 0, "context": "Theorem 2 Suppose there is a set of n d-tuples of hidden parameters in [0, 1], pi,j , 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 d with pi,j \u2208 [0, 1], and we observe the outcomes of t independent flips of each coin, namely Xi,j , with Xi,j \u223c Binomial(t, pi,j).", "startOffset": 71, "endOffset": 77}, {"referenceID": 0, "context": "Theorem 2 Suppose there is a set of n d-tuples of hidden parameters in [0, 1], pi,j , 1 \u2264 i \u2264 n, 1 \u2264 j \u2264 d with pi,j \u2208 [0, 1], and we observe the outcomes of t independent flips of each coin, namely Xi,j , with Xi,j \u223c Binomial(t, pi,j).", "startOffset": 119, "endOffset": 125}, {"referenceID": 0, "context": "There is an algorithm that produces a distribution Q supported on [0, 1], such that with probability at least 1\u2212 \u03b4 over the randomness of Xi,j ,", "startOffset": 66, "endOffset": 72}, {"referenceID": 10, "context": "The seminal paper of Charles Stein [12] was one of the earliest papers to identify the surprising possibility of leveraging the availability of independent data reflecting a large number of parameters of interest, to partially compensate for having little information about each parameter.", "startOffset": 35, "endOffset": 39}, {"referenceID": 8, "context": "[10, 2, 13, 14, 1]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "[10, 2, 13, 14, 1]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 11, "context": "[10, 2, 13, 14, 1]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "[10, 2, 13, 14, 1]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 0, "context": "[10, 2, 13, 14, 1]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "distribution that do not depend on the labels of the domain of the distribution, such as the entropy or support size of the distribution, or the number of elements likely to be observed in a new, larger sample [11, 15].", "startOffset": 210, "endOffset": 218}, {"referenceID": 13, "context": "distribution that do not depend on the labels of the domain of the distribution, such as the entropy or support size of the distribution, or the number of elements likely to be observed in a new, larger sample [11, 15].", "startOffset": 210, "endOffset": 218}, {"referenceID": 11, "context": "The benefit of pursuing this weaker goal of returning the unlabeled multiset is that it can be learned to significantly higher accuracy for a given sample size\u2014essentially as accurate as the empirical distribution of a sample that is a logarithmic factor larger [13, 15].", "startOffset": 262, "endOffset": 270}, {"referenceID": 13, "context": "The benefit of pursuing this weaker goal of returning the unlabeled multiset is that it can be learned to significantly higher accuracy for a given sample size\u2014essentially as accurate as the empirical distribution of a sample that is a logarithmic factor larger [13, 15].", "startOffset": 262, "endOffset": 270}, {"referenceID": 3, "context": "Also related to the current work are the papers [4, 6] that consider the problem of learning \u201cPoisson Binomials,\u201d namely a sum of independent non-identical Bernoulli (0/1) random variables, given access to samples.", "startOffset": 48, "endOffset": 54}, {"referenceID": 4, "context": "Also related to the current work are the papers [4, 6] that consider the problem of learning \u201cPoisson Binomials,\u201d namely a sum of independent non-identical Bernoulli (0/1) random variables, given access to samples.", "startOffset": 48, "endOffset": 54}, {"referenceID": 0, "context": "Definition 1 The Wasserstein, or earth mover\u2019s, distance between distributions P,Q, is ||P \u2212 Q||W := inf \u03b3\u2208\u0393(P,Q) \u222b [0,1]2d d(x, y)d\u03b3(x, y), where \u0393(P,Q) is the set of all couplings on P and Q, namely a distribution whose marginals agree with the distributions.", "startOffset": 116, "endOffset": 121}, {"referenceID": 0, "context": "[0,1] g(x)d(P \u2212Q)(x) where the supremum is taken over Lipschitz g.", "startOffset": 0, "endOffset": 5}, {"referenceID": 7, "context": "In this section we will outline the proof of the following stronger version of Proposition 1 in [9]:", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "Theorem 3 For two distributions P and Q supported on [0, 1] whose first s moments are \u03b1 and \u03b2 respectively, the Wasserstein distance ||P \u2212 Q||W is bounded by C1 s + C2 \u2211s k=1 2 |\u03b1k \u2212 \u03b2k| for absolute constants C1, C2.", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "First we consider the setting where two distributions P,Q supported on [0, 1] have the exact same first s moments, and bound their Wasserstein distance.", "startOffset": 71, "endOffset": 77}, {"referenceID": 0, "context": "Next, we use a linear program to assign weights on an -net of [0, 1] to match the estimates.", "startOffset": 62, "endOffset": 68}, {"referenceID": 2, "context": "The following theorem from [3] is used:", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "Lemma 2 Given any Lipschitz function f supported on [0, 1], there is a degree s polynomial p(x) such that", "startOffset": 52, "endOffset": 58}, {"referenceID": 0, "context": "sup x\u2208[0,1]d |p(x)\u2212 f(x)| \u2264 Cd s .", "startOffset": 6, "endOffset": 11}, {"referenceID": 0, "context": "We considered three different choices for an underlying distribution P\u0304 over [0, 1], then drew n independent samples p1, .", "startOffset": 77, "endOffset": 83}, {"referenceID": 0, "context": "(a) 3-spike distribution (b) truncated normal (c) Uniform on [0, 1]", "startOffset": 61, "endOffset": 67}, {"referenceID": 0, "context": "15, truncated to be supported on [0, 1]; and (c) the uniform distribution over [0, 1].", "startOffset": 33, "endOffset": 39}, {"referenceID": 0, "context": "15, truncated to be supported on [0, 1]; and (c) the uniform distribution over [0, 1].", "startOffset": 79, "endOffset": 85}, {"referenceID": 0, "context": "(a) 3-spike distribution (b) truncated normal (c) Uniform on [0, 1]", "startOffset": 61, "endOffset": 67}], "year": 2017, "abstractText": "Consider the following fundamental estimation problem: there are n entities, each with an unknown parameter pi \u2208 [0, 1], and we observe n independent random variables,X1, . . . , Xn, withXi \u223cBinomial(t, pi). How accurately can one recover the \u201chistogram\u201d (i.e. cumulative density function) of the pis? While the empirical estimates would recover the histogram to earth mover distance \u0398( 1 \u221a t ) (equivalently, `1 distance between the CDFs), we show that, provided n is sufficiently large, we can achieve error O( 1t ) which is information theoretically optimal. We also extend our results to the multi-dimensional parameter case, capturing settings where each member of the population has multiple associated parameters. Beyond the theoretical results, we demonstrate that the recovery algorithm performs well in practice on a variety of datasets, providing illuminating insights into several domains, including politics, and sports analytics.", "creator": "LaTeX with hyperref package"}}}