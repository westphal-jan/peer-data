{"id": "1709.02828", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Sep-2017", "title": "Globally Normalized Reader", "abstract": "Rapid progress has been made towards question answering (QA) systems that can extract answers from text. Existing neural approaches make use of expensive bi-directional attention mechanisms or score all possible answer spans, limiting scalability. We propose instead to cast extractive QA as an iterative search problem: select the answer's sentence, start word, and end word. This representation reduces the space of each search step and allows computation to be conditionally allocated to promising search paths. We show that globally normalizing the decision process and back-propagating through beam search makes this representation viable and learning efficient. We empirically demonstrate the benefits of this approach using our model, Globally Normalized Reader (GNR), which achieves the second highest single model performance on the Stanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster than bi-attention-flow. We also introduce a data-augmentation method to produce semantically valid examples by aligning named entities to a knowledge base and swapping them with new entities of the same type. This method improves the performance of all models considered in this work and is of independent interest for a variety of NLP tasks.", "histories": [["v1", "Fri, 8 Sep 2017 18:27:50 GMT  (332kb,D)", "http://arxiv.org/abs/1709.02828v1", "Presented at EMNLP 2017"]], "COMMENTS": "Presented at EMNLP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jonathan raiman", "john miller"], "accepted": true, "id": "1709.02828"}, "pdf": {"name": "1709.02828.pdf", "metadata": {"source": "CRF", "title": "Globally Normalized Reader", "authors": ["Jonathan Raiman"], "emails": ["jonathanraiman@baidu.com", "millerjohn@baidu.com"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point where it will be able to put itself at the top, in all areas where it is necessary to position itself, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}, {"heading": "2 Model", "text": "An example of the output of the model is shown in Figure 1, and the network architecture is shown in Figure 2. Formally, d1,..., dn should denote every sentence in the document, and for each sentence di, di, 1,..., di, mi should denote the word vectors corresponding to the words in the sentence. Likewise, q1,..., q should denote the word vectors corresponding to the words in the question. An answer is a tuple a = (i, j, k), which denotes the correct sentence i, the beginning word in the sentence j, and the end word in the sentence k. Let A (d) denote the set of valid answer tuples for document d."}, {"heading": "2.1 Question Encoding", "text": "Each question is encrypted by a stack of bidirectional LSTM (Bi-LSTM) over each word in the question, thus generating hidden states (hfwd1, h bwd 1),.., (h fwd ', h bwd') (Graves and Schmidhuber, 2005). According to Lee et al. (2016), these hidden states are used to calculate a passage-independent question embedding, qindep. Formally, sj = w > q is MLP ([h bwd j; h fwd j]) (1) \u03b1j = exp (sj) \u2211'j \u2032 = 1 exp (sj \u2032) (2) qindep = 1 \u03b1j [h bwd j; h fwd j], (3) where wq is a transferable embedding vector, and MLP is a two-layered neural network with a Relu-non-gravitational force. The question is concatenated by the wwd and wd-up for the final BSTM."}, {"heading": "2.2 Question-Aware Document Encoding", "text": "Depending on the question vector, we calculate a representation of each document word that responds to both the surrounding context and the question. Specifically, each word in the document is represented as a concatenation of its word vector di, j, the question vector q, Boolean characters that indicate whether a word appears or is repeated in the question, and a question-oriented embedding by Lee et al. (2016). The question-oriented embedding qaligni, j is represented by Bysi, j, k = MLP (di, j) > MLP (qk) (4) \u03b1i, j, k = exp (si, j, k) \u2211'k \u2032 = 1 exp (si, j, k \u2032) (5) q align i, j = 1 \u03b1i, j, kqk."}, {"heading": "2.3 Answer Selection", "text": "Each sentence di is represented by the hidden state of the first and last word in the sentence for the backward and forward-looking LSTM or [hbwdi, 1; h fwd i, mi] respectively, and is evaluated by passing this representation through a fully connected level that prints the unnormalized sentence number for the sentence di, which is called \"sent\" (di). After selecting a sentence di, we select the beginning of the response span within the sentence. Any potential opening word di, j is represented as the associated document that encodes [hfwdi, j; h bwd i, j] and is evaluated by passing this encoding through a fully connected layer that outputs the unnormalized beginning word for the word j in the sentence i, referred to as the associated beginning word di, j."}, {"heading": "2.4 Global Normalization", "text": "The results for each stage of our model can be normalized at the local or global level. Previous work has shown that locally normalized models have a weak ability to correct errors made in previous decisions, while globally normalized models are strictly more meaningful than locally normalized models (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004). In a locally normalized model, each decision is made dependent on the previous decision. (7) The probability of an answer a = (i, j, k) is normalized by applying a softmax (a, q) = Psent (i | d, q) \u00b7 Psw (j | i, d, q) \u00b7 Pew (k | j, i, d, q). (7) Each partial decision is normalized locally by applying a softmax (d, q) to the relevant selection model: Psent (i | d, q) = exp (di) = exp (di)))."}, {"heading": "2.5 Objective and Training", "text": "To ensure that learning is efficient, we instead use beam search during training and early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004). Specifically, we approach Z by adding only candidates on the last beam B: Z \u2248 a \u2032 B exp (score (a \u2032, d, q). (16) If the gold sequence falls off the beam t during decoding, a stochastic gradient step is performed on the partial object calculated by step t and normalized above the beam at step t."}, {"heading": "2.6 Implementation", "text": "Our most powerful model uses a stack of 3 BiLSTMs for the question and document encodings and a single Bi-LSTM for the end of the time span prediction. The hidden dimension of all recurring layers is 200. I use the 300-dimensional 8.4B token Common Crawl GloVe vectors (Pennington et al., 2014). Words missing from the common crawl vocabulary are set to zero. In our experiments, all architectures considered sufficient to exceed the training set have. We regulate the models by fixing the word embedding throughout the training, using the input of the Bi-LSTMs with the probability of 0.3 and the input of the fully connected layers with the probability of 0.4 (Srivastava et al al al al al al al al al al al al al al al al al, 2014) and adding the Gaussian sounds to the rebound weights in the overall training with the probability of 0.4 (we fix the STi al al al al al al al al al al al al al al al al al, 2014)."}, {"heading": "3 Type Swaps", "text": "In fact, it is as if most of us are able to keep to the rules that they have imposed on themselves. (...) In fact, it is as if they are able to keep to the rules. (...) In fact, it is as if they are able to keep to the rules. (...) In fact, it is as if they are able to break the rules. (...) It is as if they are able to keep to the rules. (...) It is as if they are able to change the rules. (...) It is as if they are able to keep to keep to the rules. (...) It is as if they are able to keep to keep to the rules. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...)"}, {"heading": "4 Results", "text": "We evaluate our model using the 100,000 sample SQuAD dataset (Rajpurkar et al., 2016) and perform several ablations to evaluate the relative importance of the proposed methods."}, {"heading": "4.1 Learning to Search", "text": "In our first experiment, we aim to quantify the importance of global normalization for the learning and search process. We use T = 104 type swap samples and vary beam width B between 1 and 32 for a locally and globally normalized model, summarizing the exact match and F1 score of the predicted response and ground truth of the model calculated using the evaluation scripts (Rajpurkar et al., 2016) (Table 3). We also report on another measurement, the sentence score, which is a measure of how often the predicted response came from the correct set. This measurement provides a measure of where errors are made during the prediction."}, {"heading": "4.2 Type Swaps", "text": "In our second experiment, we evaluate the impact of the amount of extended data on the performance of our model. In this experiment, we use the best beam sizes for each model (B = 10 for local and B = 32 for global) and vary the augmentation from T = 0 (no augmentation) to T = 5 \u00b7 104. The results of this experiment are summarized in (Table 4). We observe that both models improve in performance with T > 0 and deteriorate in performance with T = 104. In addition, data augmentation and global normalization complement each other. Combined, we obtain an improvement of 1.6 EM and 2.0 F1 over the locally normalized baseline. We also verify that the effects of type swaps are not limited to our specific model by observing the impact of the expanded data on DCN + (Xiong et al., 2016) 6. We note that it greatly reduces the generalization error and contributes to an improvement of F1, with further improvements possible through re-6."}, {"heading": "5 Discussion", "text": "In this section, we will discuss the results presented in Section 4 and explain how they relate to our main claims."}, {"heading": "5.1 Extractive Question Answering as a Search Problem", "text": "Sentences provide a natural and powerful document decomposition for search, which can be easily learned as a search step: for all models and configurations considered, the sentence score was above 88% correct (Table 3) 7. Therefore, sentence selection is the simple part of the problem, and the model can distribute more computation (such as the endword selection Bi-LSTM) to spans that are likely to contain the answer. This approach avoids wasteful work on unpromising spans, and is important for further scaling these methods to long documents."}, {"heading": "5.2 Global Normalization", "text": "The Globally Normalized Reader surpasses previous approaches and achieves the second highest EM behind (Wang et al., 2017), with no bidirectional attention and only scoring spans in its final beam. Increasing the beam width improves results for both locally and globally normalized models (Table 3), suggesting that search errors account for a significant portion of the performance difference between models. Models such as Lee et al. (2016) and Wang and Jiang (2016) overcome this difficulty by classifying all possible spans and thus never skipping a possible answer. Even for large beam sizes, the locally normalized model undercuts these approaches. However, by increasing model flexibility and conducting the search during training, the globally normalized model is able to recover from search errors and gain much of the benefits of evaluating all possible spans."}, {"heading": "5.3 Type-Aware Data Augmentation", "text": "Type swaps, our data multiplication strategy, provides a way to include the nature of the question and the types of named units in the answers in the learning process of our model and reduce sensitivity to surface variations. Existing neural network approaches to extractive QA have so far ignored this information. Extending the data set by additional type-sensitive synthetic examples improves performance by providing better coverage of different response types. The number of augmented samples used improves the performance of all models studied (Table 4-5.) With T-104, 5-104] (EM, F1), the performance of (65.8 \u2192 66.7, 74.0 \u2192 75.0) locally normalized models improves and (66.6 \u2192 68.4, 75.0 \u2192 76.21) 7The objective function difference explains the lower performance of these global compared to locally normalized models based on the Sentence Scores: Local models must always provide the highest probability of corrected sentences and ensure that the highest probability of corrected sentences is that spreads generated in 2017 are higher than those generated by other global models."}, {"heading": "6 Related Work", "text": "The fact is that we see ourselves in a position to be in, and that we are in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be in a position to a position to be in a position to be in a position to be in a position to be in a position to"}, {"heading": "7 Conclusions and Future Work", "text": "In this paper, we provide a methodology that overcomes several limitations of existing approaches to answering extractive questions. In particular, our proposed model, the Globally Normalized Reader, reduces the computational complexity of previous models by interpreting the question as a search and distributing more computation to promising response spans. Empirically, we note that this approach, combined with global normalization and beam search during training, produces state-of-the-art results. Furthermore, we note that a type-conscious data augmentation strategy improves the performance of all models studied to the SQuAD dataset. The method is general and requires only that the training data contain designated units from a large KB. We expect that it will be applicable to other NLP tasks that would benefit from more training data. In the course of future work, we plan to apply the GNR to other questions that answer datasets, such as MS MARCO (Nguyen et al. 2016 newsbleed model, QA search, or other structural analysis, as well as a possibility of finding a triangle)."}, {"heading": "Acknowledgments", "text": "We thank the anonymous critics for their valuable feedback. We also thank Adam Coates, Carl Case, Andrew Gibiansky and Szymon Sidor for thoughtful comments and fruitful discussions. We also thank James Bradbury and Bryan McCann for conducting type swap experiments at DCN +."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "Abadi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Abadi et al\\.", "year": 2016}, {"title": "Influence of pok\u00e9mon go on physical activity: Study and implications", "author": ["Tim Althoff", "Ryen W White", "Eric Horvitz."], "venue": "Journal of Medical Internet Research, 18(12).", "citeRegEx": "Althoff et al\\.,? 2016", "shortCiteRegEx": "Althoff et al\\.", "year": 2016}, {"title": "Globally normalized transition-based neural networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins."], "venue": "arXiv preprint arXiv:1603.06042.", "citeRegEx": "Andor et al\\.,? 2016", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["Jacob Andreas", "Marcus Rohrbach", "Trevor Darrell", "Dan Klein."], "venue": "arXiv preprint arXiv:1601.01705.", "citeRegEx": "Andreas et al\\.,? 2016", "shortCiteRegEx": "Andreas et al\\.", "year": 2016}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["Sharan Chetlur", "Cliff Woolley", "Philippe Vandermersch", "Jonathan Cohen", "John Tran", "Bryan Catanzaro", "Evan Shelhamer."], "venue": "arXiv preprint arXiv:1410.0759.", "citeRegEx": "Chetlur et al\\.,? 2014", "shortCiteRegEx": "Chetlur et al\\.", "year": 2014}, {"title": "Incremental parsing with the perceptron algorithm", "author": ["Michael Collins", "Brian Roark."], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 111. Association for Computational Linguistics.", "citeRegEx": "Collins and Roark.,? 2004", "shortCiteRegEx": "Collins and Roark.", "year": 2004}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Spacy", "author": ["Matthew Honnibal."], "venue": "https:// github.com/explosion/spaCy.", "citeRegEx": "Honnibal.,? 2017", "shortCiteRegEx": "Honnibal.", "year": 2017}, {"title": "Data recombination for neural semantic parsing", "author": ["Robin Jia", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.03622.", "citeRegEx": "Jia and Liang.,? 2016", "shortCiteRegEx": "Jia and Liang.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Data mining and education", "author": ["Kenneth R Koedinger", "Sidney D\u2019Mello", "Elizabeth A McLaughlin", "Zachary A Pardos", "Carolyn P Ros\u00e9"], "venue": "Wiley Interdisciplinary Reviews: Cognitive Science,", "citeRegEx": "Koedinger et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Koedinger et al\\.", "year": 2015}, {"title": "Learning recurrent span representations for extractive question answering", "author": ["Kenton Lee", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das."], "venue": "arXiv preprint arXiv:1611.01436.", "citeRegEx": "Lee et al\\.,? 2016", "shortCiteRegEx": "Lee et al\\.", "year": 2016}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "author": ["Tri Nguyen", "Mir Rosenberg", "Xia Song", "Jianfeng Gao", "Saurabh Tiwary", "Rangan Majumder", "Li Deng."], "venue": "arXiv preprint arXiv:1611.09268.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Distant supervision for relation extraction beyond the sentence boundary", "author": ["Chris Quirk", "Hoifung Poon."], "venue": "arXiv preprint arXiv:1609.04873.", "citeRegEx": "Quirk and Poon.,? 2016", "shortCiteRegEx": "Quirk and Poon.", "year": 2016}, {"title": "Ciseau", "author": ["Jonathan Raiman."], "venue": "https:// github.com/jonathanraiman/ciseau.", "citeRegEx": "Raiman.,? 2017", "shortCiteRegEx": "Raiman.", "year": 2017}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "author": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang."], "venue": "arXiv preprint arXiv:1606.05250.", "citeRegEx": "Rajpurkar et al\\.,? 2016", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "Bidirectional attention flow for machine comprehension", "author": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi."], "venue": "arXiv preprint arXiv:1611.01603.", "citeRegEx": "Seo et al\\.,? 2016", "shortCiteRegEx": "Seo et al\\.", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Newsqa: A machine comprehension dataset", "author": ["Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1611.09830.", "citeRegEx": "Trischler et al\\.,? 2016a", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "A parallel-hierarchical model for machine comprehension on sparse data", "author": ["Adam Trischler", "Zheng Ye", "Xingdi Yuan", "Jing He", "Phillip Bachman", "Kaheer Suleman."], "venue": "arXiv preprint arXiv:1603.08884.", "citeRegEx": "Trischler et al\\.,? 2016b", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Deepstance at semeval-2016 task 6: Detecting stance in tweets using character and word-level cnns", "author": ["Prashanth Vijayaraghavan", "Ivan Sysoev", "Soroush Vosoughi", "Deb Roy."], "venue": "arXiv preprint arXiv:1606.05694.", "citeRegEx": "Vijayaraghavan et al\\.,? 2016", "shortCiteRegEx": "Vijayaraghavan et al\\.", "year": 2016}, {"title": "Wikidata: a free collaborative knowledgebase", "author": ["Denny Vrande\u010di\u0107", "Markus Kr\u00f6tzsch."], "venue": "Communications of the ACM, 57(10):78\u201385.", "citeRegEx": "Vrande\u010di\u0107 and Kr\u00f6tzsch.,? 2014", "shortCiteRegEx": "Vrande\u010di\u0107 and Kr\u00f6tzsch.", "year": 2014}, {"title": "Machine comprehension using match-lstm and answer pointer", "author": ["Shuohang Wang", "Jing Jiang."], "venue": "arXiv preprint arXiv:1608.07905.", "citeRegEx": "Wang and Jiang.,? 2016", "shortCiteRegEx": "Wang and Jiang.", "year": 2016}, {"title": "Gated self-matching networks for reading comprehension and question answering", "author": ["Wenhui Wang", "Nan Yang", "Furu Wei", "Baobao Chang", "Ming Zhou."], "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics.", "citeRegEx": "Wang et al\\.,? 2017", "shortCiteRegEx": "Wang et al\\.", "year": 2017}, {"title": "Structured prediction cascades", "author": ["David Weiss", "Benjamin Taskar."], "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 916\u2013923.", "citeRegEx": "Weiss and Taskar.,? 2010", "shortCiteRegEx": "Weiss and Taskar.", "year": 2010}, {"title": "Fastqa: A simple and efficient neural architecture for question answering", "author": ["Dirk Weissenborn", "Georg Wiese", "Laura Seiffe."], "venue": "arXiv preprint arXiv:1703.04816.", "citeRegEx": "Weissenborn et al\\.,? 2017", "shortCiteRegEx": "Weissenborn et al\\.", "year": 2017}, {"title": "Sequence-to-sequence learning as beam-search optimization", "author": ["Sam Wiseman", "Alexander M Rush."], "venue": "arXiv preprint arXiv:1606.02960.", "citeRegEx": "Wiseman and Rush.,? 2016", "shortCiteRegEx": "Wiseman and Rush.", "year": 2016}, {"title": "Dynamic coattention networks for question answering", "author": ["Caiming Xiong", "Victor Zhong", "Richard Socher."], "venue": "arXiv preprint arXiv:1611.01604.", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Text understanding from scratch", "author": ["Xiang Zhang", "Yann LeCun."], "venue": "arXiv preprint arXiv:1502.01710.", "citeRegEx": "Zhang and LeCun.,? 2015", "shortCiteRegEx": "Zhang and LeCun.", "year": 2015}, {"title": "A neural probabilistic structuredprediction model for transition-based dependency parsing", "author": ["Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen."], "venue": "ACL (1), pages 1213\u20131222.", "citeRegEx": "Zhou et al\\.,? 2015", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}, {"title": "Neural question generation from text: A preliminary study", "author": ["Qingyu Zhou", "Nan Yang", "Furu Wei", "Chuanqi Tan", "Hangbo Bao", "Ming Zhou."], "venue": "arXiv preprint arXiv:1704.01792.", "citeRegEx": "Zhou et al\\.,? 2017", "shortCiteRegEx": "Zhou et al\\.", "year": 2017}], "referenceMentions": [{"referenceID": 1, "context": "Poon, 2016), large scale health impact studies (Althoff et al., 2016), or educational material development (Koedinger et al.", "startOffset": 47, "endOffset": 69}, {"referenceID": 10, "context": ", 2016), or educational material development (Koedinger et al., 2015).", "startOffset": 45, "endOffset": 69}, {"referenceID": 16, "context": "Recent progress in neural-network based extractive question answering models are quickly closing the gap with human performance on several benchmark QA tasks such as SQuAD (Rajpurkar et al., 2016), MS MARCO (Nguyen et al.", "startOffset": 172, "endOffset": 196}, {"referenceID": 12, "context": ", 2016), MS MARCO (Nguyen et al., 2016), or NewsQA (Trischler et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 19, "context": ", 2016), or NewsQA (Trischler et al., 2016a).", "startOffset": 19, "endOffset": 44}, {"referenceID": 17, "context": "They rely extensively on expensive bidirectional attention mechanisms (Seo et al., 2016) or must rank all possible answer spans (Lee et al.", "startOffset": 70, "endOffset": 88}, {"referenceID": 11, "context": ", 2016) or must rank all possible answer spans (Lee et al., 2016).", "startOffset": 47, "endOffset": 65}, {"referenceID": 31, "context": "While data-augmentation for question answering have been proposed (Zhou et al., 2017), current approaches still do not provide training data that can improve the performance of existing systems.", "startOffset": 66, "endOffset": 85}, {"referenceID": 6, "context": ", (h fwd ` , h bwd ` ) (Graves and Schmidhuber, 2005).", "startOffset": 23, "endOffset": 53}, {"referenceID": 6, "context": ", (h fwd ` , h bwd ` ) (Graves and Schmidhuber, 2005). Following Lee et al. (2016), these hidden states are used to compute a passage-independent question embedding, qindep.", "startOffset": 24, "endOffset": 83}, {"referenceID": 11, "context": "Specifically, each word in the document is represented as the concatenation of its word vector di,j , the question vector q, boolean features indicating if a word appears in the question or is repeated, and a question-aligned embedding from Lee et al. (2016). The question-aligned embedding q i,j is given by", "startOffset": 241, "endOffset": 259}, {"referenceID": 2, "context": "Previous work demonstrated that locally-normalized models have a weak ability to correct mistakes made in previous decisions, while globally normalized models are strictly more expressive than locally normalized models (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).", "startOffset": 219, "endOffset": 283}, {"referenceID": 30, "context": "Previous work demonstrated that locally-normalized models have a weak ability to correct mistakes made in previous decisions, while globally normalized models are strictly more expressive than locally normalized models (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).", "startOffset": 219, "endOffset": 283}, {"referenceID": 5, "context": "Previous work demonstrated that locally-normalized models have a weak ability to correct mistakes made in previous decisions, while globally normalized models are strictly more expressive than locally normalized models (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).", "startOffset": 219, "endOffset": 283}, {"referenceID": 2, "context": "Instead, to ensure learning is efficient, we use beam search during training and early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).", "startOffset": 95, "endOffset": 159}, {"referenceID": 30, "context": "Instead, to ensure learning is efficient, we use beam search during training and early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).", "startOffset": 95, "endOffset": 159}, {"referenceID": 5, "context": "Instead, to ensure learning is efficient, we use beam search during training and early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004).", "startOffset": 95, "endOffset": 159}, {"referenceID": 13, "context": "4B token Common Crawl GloVe vectors (Pennington et al., 2014).", "startOffset": 36, "endOffset": 61}, {"referenceID": 18, "context": "4 (Srivastava et al., 2014), and adding gaussian noise to the recurrent weights with \u03c3 = 10\u22126.", "startOffset": 2, "endOffset": 27}, {"referenceID": 9, "context": "999, = 10\u22128 and a batch size of 32 (Kingma and Ba, 2014).", "startOffset": 35, "endOffset": 56}, {"referenceID": 0, "context": "All our experiments are implemented in Tensorflow (Abadi et al., 2016), and we tokenize using Ciseau (Raiman, 2017).", "startOffset": 50, "endOffset": 70}, {"referenceID": 15, "context": ", 2016), and we tokenize using Ciseau (Raiman, 2017).", "startOffset": 38, "endOffset": 52}, {"referenceID": 4, "context": "Despite performing beam-search during training, our model trains to convergence in under 4 hours through the use of efficient LSTM primitives in CuDNN (Chetlur et al., 2014) and batching our computation over examples and search beams.", "startOffset": 151, "endOffset": 173}, {"referenceID": 26, "context": "A similar observation was made in the error analysis of (Weissenborn et al., 2017).", "startOffset": 56, "endOffset": 82}, {"referenceID": 7, "context": "Specifically, we use a part of speech tagger (Honnibal, 2017) to extract nominal groups in the training data and string-match them with entities in Wikidata.", "startOffset": 45, "endOffset": 61}, {"referenceID": 21, "context": "with documents where we can safely assume that the majority of the entities will be contained in a large knowledge base (KB) such as Wikidata Vrande\u010di\u0107 and Kr\u00f6tzsch (2014) we find that simple string matching techniques are sufficiently accurate.", "startOffset": 142, "endOffset": 172}, {"referenceID": 16, "context": "We evaluate our model on the 100,000 example SQuAD dataset (Rajpurkar et al., 2016) and perform several ablations to evaluate the relative importance of the proposed methods.", "startOffset": 59, "endOffset": 83}, {"referenceID": 16, "context": "We use T = 104 Type Swap samples and vary beam width B between 1 and 32 for a locally and globally normalized models and summarize the Exact-Match and F1 score of the model\u2019s predicted answer and ground truth computed using the evaluation scripts from (Rajpurkar et al., 2016) (Table 3).", "startOffset": 252, "endOffset": 276}, {"referenceID": 16, "context": "Model EM F1 Human (Rajpurkar et al., 2016) 80.", "startOffset": 18, "endOffset": 42}, {"referenceID": 16, "context": "Single model Sliding Window (Rajpurkar et al., 2016) 13.", "startOffset": 28, "endOffset": 52}, {"referenceID": 23, "context": "2 Match-LSTM (Wang and Jiang, 2016) 64.", "startOffset": 13, "endOffset": 35}, {"referenceID": 28, "context": "9 DCN (Xiong et al., 2016) 65.", "startOffset": 6, "endOffset": 26}, {"referenceID": 11, "context": "6 Rasor (Lee et al., 2016) 66.", "startOffset": 8, "endOffset": 26}, {"referenceID": 17, "context": "9 Bi-Attention Flow (Seo et al., 2016) 67.", "startOffset": 20, "endOffset": 38}, {"referenceID": 24, "context": "3 R-Net(Wang et al., 2017) 72.", "startOffset": 7, "endOffset": 26}, {"referenceID": 28, "context": "We also verify that the effects of Type Swaps are not limited to our specific model by observing the impact of augmented data on the DCN+ (Xiong et al., 2016)6.", "startOffset": 138, "endOffset": 158}, {"referenceID": 24, "context": "The Globally Normalized Reader outperforms previous approaches and achieves the second highest EM behind (Wang et al., 2017), without using bi-directional attention and only scoring spans in its final beam.", "startOffset": 105, "endOffset": 124}, {"referenceID": 11, "context": "Models such as Lee et al. (2016) and Wang and Jiang (2016) overcome this difficulty by ranking all possible spans and thus never skipping a possible answer.", "startOffset": 15, "endOffset": 33}, {"referenceID": 11, "context": "Models such as Lee et al. (2016) and Wang and Jiang (2016) overcome this difficulty by ranking all possible spans and thus never skipping a possible answer.", "startOffset": 15, "endOffset": 59}, {"referenceID": 26, "context": "Our experiments are conducted on two vastly different architectures and thus these benefits are expected to carry over to different models (Weissenborn et al., 2017; Seo et al., 2016; Wang et al., 2017), and perhaps more broadly in other natural language tasks that contain named entities and have limited supervised data.", "startOffset": 139, "endOffset": 202}, {"referenceID": 17, "context": "Our experiments are conducted on two vastly different architectures and thus these benefits are expected to carry over to different models (Weissenborn et al., 2017; Seo et al., 2016; Wang et al., 2017), and perhaps more broadly in other natural language tasks that contain named entities and have limited supervised data.", "startOffset": 139, "endOffset": 202}, {"referenceID": 24, "context": "Our experiments are conducted on two vastly different architectures and thus these benefits are expected to carry over to different models (Weissenborn et al., 2017; Seo et al., 2016; Wang et al., 2017), and perhaps more broadly in other natural language tasks that contain named entities and have limited supervised data.", "startOffset": 139, "endOffset": 202}, {"referenceID": 2, "context": "Most recently, Andor et al. (2016) and Zhou et al.", "startOffset": 15, "endOffset": 35}, {"referenceID": 2, "context": "Most recently, Andor et al. (2016) and Zhou et al. (2015) demonstrated the effectiveness of globally normalized networks and training with beam search for part of speech tagging and transition-based dependency parsing, while Wiseman and Rush (2016) showed that these techniques could also be applied to sequence-to-sequence models in several application areas including machine translation.", "startOffset": 15, "endOffset": 58}, {"referenceID": 2, "context": "Most recently, Andor et al. (2016) and Zhou et al. (2015) demonstrated the effectiveness of globally normalized networks and training with beam search for part of speech tagging and transition-based dependency parsing, while Wiseman and Rush (2016) showed that these techniques could also be applied to sequence-to-sequence models in several application areas including machine translation.", "startOffset": 15, "endOffset": 249}, {"referenceID": 2, "context": "In their work reinforcement learning was used to learn how to turn on and off computation, while we find that conditional computation can be easily learnt with maximum likelihood and the help of early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004) to guide the training process.", "startOffset": 209, "endOffset": 273}, {"referenceID": 30, "context": "In their work reinforcement learning was used to learn how to turn on and off computation, while we find that conditional computation can be easily learnt with maximum likelihood and the help of early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004) to guide the training process.", "startOffset": 209, "endOffset": 273}, {"referenceID": 5, "context": "In their work reinforcement learning was used to learn how to turn on and off computation, while we find that conditional computation can be easily learnt with maximum likelihood and the help of early updates (Andor et al., 2016; Zhou et al., 2015; Collins and Roark, 2004) to guide the training process.", "startOffset": 209, "endOffset": 273}, {"referenceID": 2, "context": "Learning to search has also been used in context of modular neural networks with conditional computation in the work of Andreas et al. (2016) for image captioning.", "startOffset": 120, "endOffset": 142}, {"referenceID": 25, "context": "Our framework for conditional computation whereby the search space is pruned by a sequence of increasingly complex models is broadly reminiscent of the structured prediction cascades of (Weiss and Taskar, 2010).", "startOffset": 186, "endOffset": 210}, {"referenceID": 19, "context": "Trischler et al. (2016b)", "startOffset": 0, "endOffset": 25}, {"referenceID": 16, "context": "Seo et al. (2016); Wang et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 16, "context": "Seo et al. (2016); Wang et al. (2017) and Xiong et al.", "startOffset": 0, "endOffset": 38}, {"referenceID": 16, "context": "Seo et al. (2016); Wang et al. (2017) and Xiong et al. (2016) make use of a bi-directional attention mechanisms, whereas the GNR is more lightweight and achieves similar results without this type of attention mechanism.", "startOffset": 0, "endOffset": 62}, {"referenceID": 11, "context": "The document representation used by the GNR is very similar to Lee et al. (2016). However, both Lee et al.", "startOffset": 63, "endOffset": 81}, {"referenceID": 11, "context": "The document representation used by the GNR is very similar to Lee et al. (2016). However, both Lee et al. (2016) and Wang and Jiang (2016) must score all O(N2) possible answer spans, making training and inference expensive.", "startOffset": 63, "endOffset": 114}, {"referenceID": 11, "context": "The document representation used by the GNR is very similar to Lee et al. (2016). However, both Lee et al. (2016) and Wang and Jiang (2016) must score all O(N2) possible answer spans, making training and inference expensive.", "startOffset": 63, "endOffset": 140}, {"referenceID": 11, "context": "The document representation used by the GNR is very similar to Lee et al. (2016). However, both Lee et al. (2016) and Wang and Jiang (2016) must score all O(N2) possible answer spans, making training and inference expensive. The GNR avoids this complexity by learning to search during training and outperforms both systems while scoring only O(B) spans. Weissenborn et al. (2017) is a locally normalized model that first predicts start and then end words of each span.", "startOffset": 63, "endOffset": 380}, {"referenceID": 27, "context": "Zhang and LeCun (2015) use a thesaurus to generate new training examples based on synonyms.", "startOffset": 0, "endOffset": 23}, {"referenceID": 20, "context": "Vijayaraghavan et al. (2016) employs a similar method, but uses Word2vec and cosine similarity to find similar words.", "startOffset": 0, "endOffset": 29}, {"referenceID": 8, "context": "Jia and Liang (2016) use a high-precision synchronous context-free grammar to generate new semantic parsing examples.", "startOffset": 0, "endOffset": 21}, {"referenceID": 30, "context": "More recently Zhou et al. (2017) proposed a sequence-to-sequence model to generate diverse and realistic training question-answer pairs on SQuAD.", "startOffset": 14, "endOffset": 33}, {"referenceID": 12, "context": "As future work we plan to apply the GNR to other question answering datasets such as MS MARCO (Nguyen et al., 2016) or NewsQA (Trischler et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 19, "context": ", 2016) or NewsQA (Trischler et al., 2016a), as well as investigate the applicability and benefits of Type Swaps to other tasks like named entity recognition, entity linking, machine translation, and summarization.", "startOffset": 18, "endOffset": 43}], "year": 2017, "abstractText": "Rapid progress has been made towards question answering (QA) systems that can extract answers from text. Existing neural approaches make use of expensive bidirectional attention mechanisms or score all possible answer spans, limiting scalability. We propose instead to cast extractive QA as an iterative search problem: select the answer\u2019s sentence, start word, and end word. This representation reduces the space of each search step and allows computation to be conditionally allocated to promising search paths. We show that globally normalizing the decision process and back-propagating through beam search makes this representation viable and learning efficient. We empirically demonstrate the benefits of this approach using our model, Globally Normalized Reader (GNR), which achieves the second highest single model performance on the Stanford Question Answering Dataset (68.4 EM, 76.21 F1 dev) and is 24.7x faster than bi-attention-flow. We also introduce a data-augmentation method to produce semantically valid examples by aligning named entities to a knowledge base and swapping them with new entities of the same type. This method improves the performance of all models considered in this work and is of independent interest for a variety of NLP tasks.", "creator": "LaTeX with hyperref package"}}}