{"id": "1709.03010", "review": {"conference": "EMNLP", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2017", "title": "Steering Output Style and Topic in Neural Response Generation", "abstract": "We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoder-decoder based language generation. This capability is desirable in a variety of applications, including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge. We decompose the neural generation process into empirically easier sub-problems: a faithfulness model and a decoding method based on selective-sampling. We also describe training and sampling algorithms that bias the generation process with a specific language style restriction, or a topic restriction. Human evaluation results show that our proposed methods are able to restrict style and topic without degrading output quality in conversational tasks.", "histories": [["v1", "Sat, 9 Sep 2017 22:03:11 GMT  (1187kb,AD)", "http://arxiv.org/abs/1709.03010v1", "EMNLP 2017 camera-ready version"]], "COMMENTS": "EMNLP 2017 camera-ready version", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["di wang", "nebojsa jojic", "chris brockett", "eric nyberg"], "accepted": true, "id": "1709.03010"}, "pdf": {"name": "1709.03010.pdf", "metadata": {"source": "CRF", "title": "Steering Output Style and Topic in Neural Response Generation", "authors": ["Di Wang", "Nebojsa Jojic", "Chris Brockett", "Eric Nyberg"], "emails": ["diwang@cs.cmu.edu,", "jojic@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "In recent years, it has become clear that the problem is a problem that must be solved primarily in the way it can be solved, how it can be solved, how it must be solved, how it can be solved, how it must be solved, how it can be solved, how it can come to a solution, how it can come to a solution, how it can come to a solution, how it can come to a solution, how it can come to a solution, how it can come to a solution, how it can come to a solution, how it can come to a solution, how it can come to a solution, how it can come to a solution, how it can come to a solution, how it can come to a solution, how it must come to a solution, how it must come to a solution, how it must come to a solution, how it must come to a solution, how it must come to a solution, how it must come to a solution."}, {"heading": "2 Related Work", "text": "In the last few years it has been shown that it is not just a product, but also a product that is able to establish itself in the models we use, and that is in the form in which it works, and that is in the way in which it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it works, how it is a product that is able to establish itself, how it, how it works, how it works in the form in which it works, how it works, how it works, how it works in which way it works, how it works in which way it works, how it works in which way it works, how it works in which way it works, how it works in which way it works, how it works in which way it works, how it works in which way it is a product, how it works in which way it works in which way it is a product is a product is a product is a product that is a product that is a product that is a product that is a product that is a product that is a product that is a product that is a product that is a product that is a product that is a product that is a product that is a product that is a product that is a product that is a product that is able to establish itself, how it is a product, how it is a product in which is a product in which is a product in which is a product in which is a product in which is a product in which is a way in which is a way in which is a way in which is a product in which is a way in which is a way in which is a product in which is a product in which is a product in which is a"}, {"heading": "3 Neural Encoder-Decoder Background", "text": "Generally, neural encoder decoder models aim to generate a target sequence Y = (y1,..., yTy) that specifies a source sequence X = (x1,.., xTx). Each word in source and target sentences, xt or yt, belongs to the source vocabulary Vx and corresponds to the target vocabulary Vy. First, an encoder converts the source sequence X into a series of context vectors C = {h1, h2,., hTx}, the size of which varies in relation to the length of the source passage. This context presentation is generated using a multi-layered neural network (RNN). The encoder RNN reads the source passage from the first token to the last token, where hi = 1, Ex [xt] is the source text."}, {"heading": "4 Decoding with Selective Sampling", "text": "The default objective function for neural encoder decoder models is the log probability of the given source code. (1) However, as discussed in (Li et al., 2015; Shao et al., 2017), whether an RNN-based neural decoder provides a poor approximation to the above objective capabilities, which tend to generate generic and safe responses that do not have diversity, such as \"I am not sure.\" In Section 7.3, we present a ranking experiment in which we verify that an RNN-based neural decoder provides a poor approximation of the above conditional probabilities, and instead tend to p the target language model (T). The backward-looking model p (S | T) empirically performs much better than p (T | S) on the relevance number ranking task. Therefore, we apply Bayes' rule directly to the 1993, as in Li's instatic translation."}, {"heading": "5 Output style restriction using a small \u2018scenting\u2019 dataset", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "6 Restricting the Output Topic", "text": "We also present a topic that limits the method for neural decoders in each place of the net. Essentially, the model is based on the counting of words (Jojic and Perina, 2011). It helps us to consider linguistic orientation as a topic by indexing the basic distribution mechanisms (Ey = 32) to (Ex = 64) (Ey = 64) in relation to the d-dimensional distribution mechanisms in relation to the d-dimensional discrete grid E. The grids in this paper are two-dimensional and typically range from (Ex = 32) to (Ex = 64) \u00d7 (Ey = 64) in relation to the d-dimensional distribution. The index z indicates a specific word in the vocabulary z = [1]. Z] Thus, the probability that the word z is represented at the d-dimensional discrete place i, and at each place of the net."}, {"heading": "7 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Datasets", "text": "Yahoo! Answer Dataset. We use the Comprehensive Questions and Answers dataset2 to train and validate the performance of various decoding setups with ranking experiments described in Section 7.3. This dataset contains 4.4 million Yahoo Wars answers to questions and the best answers chosen by the user. Unlike conversation datasets, such as the Twitter datasets described below, it contains more relevant and specific answers for each question that leads to less ambiguity in the ranking. Twitter Conversation Dataset. We traced our base encoder decoder models on the Twitter Conversation Triple Dataset, which is described in (Sordoni et al., 2015), which consists of 23 million conversation snippets that randomly consist of a collection of 129M context message response triples from the Twitter Firehose via the 3-month http: / webscope.sandbox.yahoo.com / catalogatype period from June to August 2012? phalogatype?"}, {"heading": "7.2 Network Setup and Implementation", "text": "Each LSTM layer has a memory size of 500. Network weights are randomly initialized based on a uniform distribution (\u2212 0.08, 0.08) and trained with the ADAM Optimizer (Kingma and Ba, 2014) to test the rewriting of algebra word problems. 5See the supplementary material. An initial learning rate of 0.002. Gradients have been truncated so that their standard does not exceed 5. Each mini stack contains 200 answers and their questions. The words of input sets were initially converted into 300-dimensional vector representations used from the RNN-based language modeling tool word2vec (Mikolov et al., 2013)."}, {"heading": "7.3 Validating the Decoding Setup with Ranking", "text": "We performed a ranking evaluation by applying different decoding setups to the Yahoo! Answers dataset. In this context, we wanted to test the relevance judgement capacities of different setups and confirm the necessity of the new decoding method discussed in Section 4. Yahoo! Answers question is used as source S, and its answer is treated as target T. Each test question is associated with a true answer and 19 random answers from the test set. MRR (Mean Reciprocal Rank) and P @ 1 (Precision of Top1) were then used as evaluation metrics. Table 2 shows the evaluation results of the answer ranking: The forward model P (T | S) on its own is close to the performance of random selection in distinguishing between true answer and wrong answer, implying that a naive bar search can produce irrelevant results only via the forward model. One hypothesis was that (T | S) performance is distorted by T (T | P) and T (T) normalization (T | P)."}, {"heading": "7.4 Human Evaluations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.4.1 Systems", "text": "We tested 10 different system configurations to evaluate the overall output quality and its ability to influence the output language style and theme: \u2022 Vanilla scanning of each word in the target. \u2022 Selective scanning as described in Section 4; all following systems use it as well. \u2022 cg-ir uses IR results to generate counter references on grid topics (Sections 6.1 and 7.2). \u2022 rank uses suggestions from the full JFK corpus in Section 5.1. \u2022 Multiply by a JFK language model as in Section 5.2. \u2022 Fine tuning with JFK dataset as in Section 5.3. \u2022 finetune-cg-ir uses IR results as topic references for fine-tuned JFK. \u2022 finetune-cg-topic is forced to use the giventopic reference to fine-tuned JFK. \u2022 Singer-Songwriter-tuned-topic-cstarg-wars."}, {"heading": "7.4.2 Evaluation Setup", "text": "Due to the low consistency between automatic metrics and human perception of conversation tasks (Liu et al., 2016; Stent et al., 2005) and the lack of true reference responses from persona models, we evaluated the quality of our generated text with a group of judges recruited by Amazon Mechanical Turk (AMT). Workers were selected based on their previous approval rate for AMT (> 95%). Each questionnaire was submitted to 3 different workers at the same time. We evaluated our proposed models on the 64 discussion topics. Each of the evaluated methods generated 3 samples for each chat context. To ensure that the ratings between the systems were calibrated, we show the human judges all system results (randomly ranked) for each test case at the same time. For each chat context, we performed three types of evaluations by: Quality Evaluation Workers were provided with the following guidelines: \"Given the chat context, a conversation must continue from a potential conversation to a high-level conversation.\""}, {"heading": "7.4.3 Results", "text": "Overall quality. We conducted opinion polls (MOS) tests for the overall quality assessment of the generated answers using the questionnaires described above. Table 3 shows the MOS results with standard errors. It can be seen that all systems based on selective sampling are significantly better than vanilla sampling basics. In limiting the output style and / or topic, the MOS score results of most systems do not decline significantly, except singers and songwriters who try to generate text-like outputs in response to political debate questions, resulting in uninterpretable strands. Our ranking method uses p (S | T) to select the answer from the original persona corpus, and is thus just as good at styling as the person himself. Since most of our test questionnaires are political, the rank was in fact often able to find related answers in the dataset (JFK), as opposed to language-based approaches, but does not have an elevency."}, {"heading": "8 Conclusions", "text": "In this study, we explored the possibility of controlling the style and content in the output of a neural encoder decoder model. We demonstrated that acquiring highly recognizable styles of famous people, characters, or professionals is possible, and that it is even possible to allow users to influence the topic of conversations. The tools described in the paper are not only useful in conversation systems (e.g. chatbots), but can also be useful as authoring tools in social media. In the latter case, social media users could use neural models as consultants to help create responses to each post the user reads. AMT tests show that these models actually provide increased style recognition without sacrificing quality or relevance."}, {"heading": "Acknowledgments", "text": "We thank Shashank Srivastava, Donald Brinkman, Michel Galley and Bill Dolan for their useful discussions and encouragement. Di Wang is supported by the Tencent Fellowship and Yahoo! Fellowship, which we are grateful for. The code and test data are available at https: / / github.com / digo / steering-response-style-and-topic."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR abs/1409.0.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Latent Dirichlet Allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan."], "venue": "J. Mach. Learn. Res. 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "The Mathematics of Statistical Machine Translation: Parameter Estimation", "author": ["Peter F Brown", "Vincent J Della Pietra", "Stephen A Della Pietra", "Robert L Mercer."], "venue": "Comput. Linguist. 19(2):263\u2013311.", "citeRegEx": "Brown et al\\.,? 1993", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Topic aware neural response generation", "author": ["Xing Chen", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma."], "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, Califor-", "citeRegEx": "Chen et al\\.,? 2017", "shortCiteRegEx": "Chen et al\\.", "year": 2017}, {"title": "Learning Phrase Representations using RNN Encoder\u2013 Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Pro-", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Summed-area Tables for Texture Mapping", "author": ["Franklin C Crow."], "venue": "Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques. New York, NY, USA, pages 207\u2013212.", "citeRegEx": "Crow.,? 1984", "shortCiteRegEx": "Crow.", "year": 1984}, {"title": "Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs", "author": ["Cristian Danescu-Niculescu-Mizil", "Lillian Lee."], "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational", "citeRegEx": "Danescu.Niculescu.Mizil and Lee.,? 2011", "shortCiteRegEx": "Danescu.Niculescu.Mizil and Lee.", "year": 2011}, {"title": "Incorporating Copying Mechanism in Sequence-to-Sequence Learning", "author": ["Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor O.K. Li"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Long Short-Term Memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Comput. 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Multidimensional Counting Grids: Inferring Word Order from Disordered Bags of Words", "author": ["Nebojsa Jojic", "Alessandro Perina."], "venue": "Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence. AUAI Press, Arlington, Vir-", "citeRegEx": "Jojic and Perina.,? 2011", "shortCiteRegEx": "Jojic and Perina.", "year": 2011}, {"title": "Controlling Output Length in Neural Encoder-Decoders", "author": ["Yuta Kikuchi", "Graham Neubig", "Ryohei Sasano", "Hiroya Takamura", "Manabu Okumura."], "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP", "citeRegEx": "Kikuchi et al\\.,? 2016", "shortCiteRegEx": "Kikuchi et al\\.", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P Kingma", "Jimmy Ba."], "venue": "CoRR abs/1412.6.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Experiments in Domain Adaptation for Statistical Machine Translation", "author": ["Philipp Koehn", "Josh Schroeder."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation. Association for Computational Linguistics, Stroudsburg, PA, USA,", "citeRegEx": "Koehn and Schroeder.,? 2007", "shortCiteRegEx": "Koehn and Schroeder.", "year": 2007}, {"title": "A themerewriting approach for generating algebra word problems", "author": ["Rik Koncel-Kedziorski", "Ioannis Konstas", "Luke Zettlemoyer", "Hannaneh Hajishirzi."], "venue": "CoRR abs/1610.06210.", "citeRegEx": "Koncel.Kedziorski et al\\.,? 2016", "shortCiteRegEx": "Koncel.Kedziorski et al\\.", "year": 2016}, {"title": "Correlated Topic Models", "author": ["John D Lafferty", "David M Blei."], "venue": "Y Weiss, P B Sch\u00f6lkopf, and J C Platt, editors, Advances in Neural Information Processing Systems 18, MIT Press, pages 147\u2013154.", "citeRegEx": "Lafferty and Blei.,? 2006", "shortCiteRegEx": "Lafferty and Blei.", "year": 2006}, {"title": "A Persona-Based Neural Conversation Model", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan."], "venue": "arXiv page 10.", "citeRegEx": "Li et al\\.,? 2016", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "A DiversityPromoting Objective Function for Neural Conversation Models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "William B. Dolan."], "venue": "Arxiv pages 110\u2013119.", "citeRegEx": "Li et al\\.,? 2015", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation", "author": ["Chia-Wei Liu", "Ryan Lowe", "Iulian Vlad Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2016}, {"title": "Effective Approaches to Attentionbased Neural Machine Translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lis-", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems 26. pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "TweetMotif : Exploratory search and topic summarization for Twitter", "author": ["B O\u2019Connor", "M Krieger", "D Ahn"], "venue": "4th International AAAI Conference on Weblogs and Social Media", "citeRegEx": "O.Connor et al\\.,? \\Q2010\\E", "shortCiteRegEx": "O.Connor et al\\.", "year": 2010}, {"title": "Skim-reading thousands of documents in one minute: Data indexing and visualization for multifarious search", "author": ["Alessandro Perina", "Dongwoo Kim", "Andrzej Turski", "Nebojsa Jojic."], "venue": "Workshop on Interactive Data Exploration and Analytics (IDEA\u201914)", "citeRegEx": "Perina et al\\.,? 2014", "shortCiteRegEx": "Perina et al\\.", "year": 2014}, {"title": "Data-driven Response Generation in Social Media", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Stroudsburg,", "citeRegEx": "Ritter et al\\.,? 2011", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "A Neural Attention Model for Abstractive Sentence Summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston."], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portu-", "citeRegEx": "Rush et al\\.,? 2015", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural Responding Machine for Short-Text Conversation", "author": ["Lifeng Shang", "Zhengdong Lu", "Hang Li."], "venue": "CoRR abs/1503.0.", "citeRegEx": "Shang et al\\.,? 2015", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "Generating Long and Diverse Responses with Neural Conversation Models", "author": ["Louis Shao", "Stephan Gouws", "Denny Britz", "Anna Goldie", "Brian Strope", "Ray Kurzweil"], "venue": null, "citeRegEx": "Shao et al\\.,? \\Q2017\\E", "shortCiteRegEx": "Shao et al\\.", "year": 2017}, {"title": "A Neural Network Approach to ContextSensitive Generation of Conversational Responses", "author": ["Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "William B. Dolan"], "venue": null, "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Evaluating Evaluation Methods for Generation in the Presence of Variation", "author": ["Amanda Stent", "Matthew Marge", "Mohit Singhai."], "venue": "Proceedings of the 6th International Conference on Computational Linguistics and Intelligent Text Processing.", "citeRegEx": "Stent et al\\.,? 2005", "shortCiteRegEx": "Stent et al\\.", "year": 2005}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Proceedings of the 27th International Conference on Neural Information Processing Systems. MIT Press, Cambridge, MA, USA, pages", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A Neural Conversational Model", "author": ["Orioi Vinyals", "Quoc V. Le."], "venue": "ICML Deep Learning Workshop 2015 37.", "citeRegEx": "Vinyals and Le.,? 2015", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering", "author": ["Di Wang", "Eric Nyberg."], "venue": "Annual Meeting of the Association for Computational Linguistics. pages 707\u2013 712.", "citeRegEx": "Wang and Nyberg.,? 2015", "shortCiteRegEx": "Wang and Nyberg.", "year": 2015}, {"title": "CMU OAQA at TREC 2016 LiveQA: An Attentional Neural Encoder-Decoder Approach for Answer Ranking", "author": ["Di Wang", "Eric Nyberg."], "venue": "Proceedings of The Twenty-Fifth Text REtrieval Conference, TREC 2016, Gaithersburg, Maryland,", "citeRegEx": "Wang and Nyberg.,? 2016", "shortCiteRegEx": "Wang and Nyberg.", "year": 2016}, {"title": "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Peihao Su", "David Vandyke", "Steve J. Young."], "venue": "Proceedings of the 2015 Conference on Em-", "citeRegEx": "Wen et al\\.,? 2015", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1502.03044 .", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 28, "context": "Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al.", "startOffset": 134, "endOffset": 236}, {"referenceID": 4, "context": "Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al.", "startOffset": 134, "endOffset": 236}, {"referenceID": 0, "context": "Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al.", "startOffset": 134, "endOffset": 236}, {"referenceID": 18, "context": "Neural encoder-decoder models have demonstrated great promise in many sequence generation tasks, including neural machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Wu et al., 2016), image captioning (Xu et al.", "startOffset": 134, "endOffset": 236}, {"referenceID": 33, "context": ", 2016), image captioning (Xu et al., 2015), summarization (Rush et al.", "startOffset": 26, "endOffset": 43}, {"referenceID": 23, "context": ", 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al.", "startOffset": 23, "endOffset": 81}, {"referenceID": 7, "context": ", 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al.", "startOffset": 23, "endOffset": 81}, {"referenceID": 10, "context": ", 2015), summarization (Rush et al., 2015; Gu et al., 2016; Kikuchi et al., 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al.", "startOffset": 23, "endOffset": 81}, {"referenceID": 29, "context": ", 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015).", "startOffset": 37, "endOffset": 137}, {"referenceID": 26, "context": ", 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015).", "startOffset": 37, "endOffset": 137}, {"referenceID": 24, "context": ", 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015).", "startOffset": 37, "endOffset": 137}, {"referenceID": 25, "context": ", 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015).", "startOffset": 37, "endOffset": 137}, {"referenceID": 16, "context": ", 2016), and conversation generation (Vinyals and Le, 2015; Sordoni et al., 2015; Shang et al., 2015; Shao et al., 2017; Li et al., 2015).", "startOffset": 37, "endOffset": 137}, {"referenceID": 0, "context": "and quickly achieved state-of-the-art results (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 46, "endOffset": 89}, {"referenceID": 18, "context": "and quickly achieved state-of-the-art results (Bahdanau et al., 2014; Luong et al., 2015).", "startOffset": 46, "endOffset": 89}, {"referenceID": 0, "context": "Specifically, our conversation model is established based on a combination of the models of (Bahdanau et al., 2014) and (Luong et al.", "startOffset": 92, "endOffset": 115}, {"referenceID": 18, "context": ", 2014) and (Luong et al., 2015) that we found to be effective.", "startOffset": 12, "endOffset": 32}, {"referenceID": 22, "context": "This work follows the line of research initiated by (Ritter et al., 2011) and (Vinyals and Le, 2015) who treat generation of conversational dialog as a data-drive statistical machine translation (SMT) problem.", "startOffset": 52, "endOffset": 73}, {"referenceID": 29, "context": ", 2011) and (Vinyals and Le, 2015) who treat generation of conversational dialog as a data-drive statistical machine translation (SMT) problem.", "startOffset": 12, "endOffset": 34}, {"referenceID": 22, "context": "(2015) extended (Ritter et al., 2011) by re-scoring SMT outputs using a neural encoder-decoder model conditioned on conversation history.", "startOffset": 16, "endOffset": 37}, {"referenceID": 22, "context": "This work follows the line of research initiated by (Ritter et al., 2011) and (Vinyals and Le, 2015) who treat generation of conversational dialog as a data-drive statistical machine translation (SMT) problem. Sordoni et al. (2015) extended (Ritter et al.", "startOffset": 53, "endOffset": 232}, {"referenceID": 8, "context": "Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997).", "startOffset": 30, "endOffset": 64}, {"referenceID": 16, "context": "However, as discussed in (Li et al., 2015; Shao et al., 2017), simply conducting beam search over the above objective will tend to generate generic and safe responses that lack diversity, such as \u201cI am not sure\u201d.", "startOffset": 25, "endOffset": 61}, {"referenceID": 25, "context": "However, as discussed in (Li et al., 2015; Shao et al., 2017), simply conducting beam search over the above objective will tend to generate generic and safe responses that lack diversity, such as \u201cI am not sure\u201d.", "startOffset": 25, "endOffset": 61}, {"referenceID": 2, "context": "Therefore, we directly apply Bayes\u2019 rule to Equation 1, as in statistical machine translation (Brown et al., 1993), and use:", "startOffset": 94, "endOffset": 114}, {"referenceID": 16, "context": "Since p(T |S) is empirically biased towards p(T ), in practice, this objective also resembles the Maximum Mutual Information (MMI) objective function in (Li et al., 2015).", "startOffset": 153, "endOffset": 170}, {"referenceID": 32, "context": "Here, we follow a similar process as in (Wen et al., 2015) which generates multiple target hypotheses with stochastic sampling based on p(T |S), and then ranks them with the objective function 2 above.", "startOffset": 40, "endOffset": 58}, {"referenceID": 25, "context": "However, as also observed by (Shao et al., 2017), step-by-step naive sampling can accumulate errors as the sequence gets longer.", "startOffset": 29, "endOffset": 48}, {"referenceID": 12, "context": "Similar ideas have been tested in domain adaptation for statistical machine translation (Koehn and Schroeder, 2007), where both in-domain and", "startOffset": 88, "endOffset": 115}, {"referenceID": 9, "context": "We further introduce a topic restricting method for neural decoders based on the Counting Grid (Jojic and Perina, 2011) model, by treating language", "startOffset": 95, "endOffset": 119}, {"referenceID": 9, "context": "Because the conditional distribution p(kn|`) is a preset uniform distribution over the grid locations inside the window placed at location `, the variable kn can be summed out (Jojic and Perina, 2011), and the generation can directly use the grouped histograms", "startOffset": 176, "endOffset": 200}, {"referenceID": 5, "context": "The CG model can be learned efficiently using the EM algorithm because the inference of the hidden variables, as well as updates of \u03c0 and h can be performed using summed area tables (Crow, 1984), and are thus", "startOffset": 182, "endOffset": 194}, {"referenceID": 21, "context": "overlapping windows have only slightly different h distributions, making CGs especially useful in visualization applications where the grid is shown in terms of the most likely words in the component distributions \u03c0 (Perina et al., 2014).", "startOffset": 216, "endOffset": 237}, {"referenceID": 3, "context": "(Chen et al., 2017) have recently proposed using LDA for topic modeling in Sequence-To-Sequence response generation models.", "startOffset": 0, "endOffset": 19}, {"referenceID": 26, "context": "We trained our base encoder-decoder models on the Twitter Conversation Triple Dataset described in (Sordoni et al., 2015), which consists of 23 million conversational snippets randomly selected from a collection of 129M context-message-response triples extracted from the Twitter Firehose over the 3-month", "startOffset": 99, "endOffset": 121}, {"referenceID": 20, "context": "Twitter specific tokenizer (O\u2019Connor et al., 2010).", "startOffset": 27, "endOffset": 50}, {"referenceID": 11, "context": "08), and are trained with the ADAM optimizer (Kingma and Ba, 2014), with", "startOffset": 45, "endOffset": 66}, {"referenceID": 13, "context": "edu/ 4 Koncel-Kedziorski et al. (2016) also uses Star Wars scripts to test theme rewriting of algebra word problems.", "startOffset": 7, "endOffset": 39}, {"referenceID": 19, "context": "tool word2vec (Mikolov et al., 2013).", "startOffset": 14, "endOffset": 36}, {"referenceID": 16, "context": "Decoding only according to this function will thus result in only low-frequency words and ungrammatical sentences, behavior also noted by (Li et al., 2015; Shao et al., 2017).", "startOffset": 138, "endOffset": 174}, {"referenceID": 25, "context": "Decoding only according to this function will thus result in only low-frequency words and ungrammatical sentences, behavior also noted by (Li et al., 2015; Shao et al., 2017).", "startOffset": 138, "endOffset": 174}, {"referenceID": 17, "context": "metrics and human perception on conversational tasks (Liu et al., 2016; Stent et al., 2005) and the lack of true reference responses from persona models, we evaluated the quality of our generated text with a set of judges recruited from Amazon Mechanical Turk (AMT).", "startOffset": 53, "endOffset": 91}, {"referenceID": 27, "context": "metrics and human perception on conversational tasks (Liu et al., 2016; Stent et al., 2005) and the lack of true reference responses from persona models, we evaluated the quality of our generated text with a set of judges recruited from Amazon Mechanical Turk (AMT).", "startOffset": 53, "endOffset": 91}], "year": 2017, "abstractText": "We propose simple and flexible training and decoding methods for influencing output style and topic in neural encoderdecoder based language generation. This capability is desirable in a variety of applications, including conversational systems, where successful agents need to produce language in a specific style and generate responses steered by a human puppeteer or external knowledge. We decompose the neural generation process into empirically easier sub-problems: a faithfulness model and a decoding method based on selectivesampling. We also describe training and sampling algorithms that bias the generation process with a specific language style restriction, or a topic restriction. Human evaluation results show that our proposed methods are able to restrict style and topic without degrading output quality in conversational tasks.", "creator": "LaTeX with hyperref package"}}}