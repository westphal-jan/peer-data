{"id": "0911.0460", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2009", "title": "Feature-Weighted Linear Stacking", "abstract": "Ensemble methods, such as stacking, are designed to boost predictive accuracy by blending the predictions of multiple machine learning models. Recent work has shown that the use of meta-features, additional inputs describing each example in a dataset, can boost the performance of ensemble methods, but the greatest reported gains have come from nonlinear procedures requiring significant tuning and training time. Here, we present a linear technique, Feature-Weighted Linear Stacking (FWLS), that incorporates meta-features for improved accuracy while retaining the well-known virtues of linear regression regarding speed, stability, and interpretability. FWLS combines model predictions linearly using coefficients that are themselves linear functions of meta-features. This technique was a key facet of the solution of the second place team in the recently concluded Netflix Prize competition. Significant increases in accuracy over standard linear stacking is demonstrated on the Netflix Prize collaborative filtering dataset.", "histories": [["v1", "Tue, 3 Nov 2009 08:17:05 GMT  (18kb)", "https://arxiv.org/abs/0911.0460v1", "17 pages, 1 figure, 2 tables"], ["v2", "Wed, 4 Nov 2009 08:55:28 GMT  (18kb)", "http://arxiv.org/abs/0911.0460v2", "17 pages, 1 figure, 2 tables"]], "COMMENTS": "17 pages, 1 figure, 2 tables", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["joseph sill", "gabor takacs", "lester mackey", "david lin"], "accepted": false, "id": "0911.0460"}, "pdf": {"name": "0911.0460.pdf", "metadata": {"source": "CRF", "title": "Feature-Weighted Linear Stacking", "authors": ["Joseph Sill", "Gabor Takacs", "Lester Mackey", "David Lin"], "emails": ["sill@yahoo.com)", "(gtakacs@sze.hu)", "(lmackey@cs.berkeley.edu)", "david.yang.lin@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 091 1.04 60v2 [cs.LG] 4 N"}, {"heading": "1 Introduction", "text": "This second level of the algorithm is designed to optimally combine model predictions to form a definitive set of predictions. Indeed, many machine learning practitioners have achieved success by applying stacking and related techniques to increase the accuracy of prediction beyond the level achieved by the individual models. In some contexts, stacking is also referred to as blending, and we will use the terms interchangeable here. [23] Since its introduction, modelers have successfully stacked on a variety of problems, including chemometrics, spam filtering [16], and large collections of datasets drawn from the UCI Machine Learning Repository. A prominent recent example of the power of model blending was the Netflix Prize1 collaborative competition. Team BellKor's Pragmatic Chaos won the prize using a mixture of hundreds of different models."}, {"heading": "2 Feature-Weighted Linear Stacking", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Algorithm", "text": "In fact, it is so that it is a way in which one sees oneself in a position to trump oneself. (...) In fact, it is not so that one sees oneself in a position to trump oneself. (...) It is also so that one sees oneself in a position to trump oneself. (...) It is not so that one is in a position to trump oneself. (...) It is as if one is in a position to trump oneself. (...) It is as if one is in a position to trump oneself. (...) It is as if one is in a position to trump oneself. (...) (...) () () () () () (...) () () () () () () () (()) () () () (()) (()) (()) (()) (())) (()) (()) () () () () ()) () () () () () () () ()) () () () () ()) () () () ()) () () ()) () () () () ()) () () ()) () () ()) () () () () () ()) () ()) () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ("}, {"heading": "2.2 Implementation Details", "text": "Let N be the number of data points used in the stacking regression, and let A be the N \u00b7 ML matrix with elements An, (i + L (j \u2212 1) = fj (xn) gi (xn), where xn is the input vector for the tenth data point in X-X. Performing a linear regression with Tikhonov regression amounts to solving the system (ATA + \u03bbI) v = AT y (5), where y represents the vector of the target outputs for the N data points, and \u03bb is a given regulation parameter. The temporal complexity of FWLS is O (NM2L2 + M3L3), with the first term corresponding to the cost of calculating ATA and the second term corresponding to the solution of the linear system. In practice, N is usually much greater than ML and almost all the computational costs of ATA."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Netflix Prize Overview", "text": "As a result, the majority of people who speak up for the rights of women and men are not working for the rights of men and women, but for the rights of women and men who work for the rights of men and women. (...) The majority of people who work for the rights of women and men are women and men. (...) The majority of people who work for the rights of women and men are women and men. (...) The majority of them are women who work for the rights of men and women. (...) The majority of them are women who work for the rights of women and men. \"(...) The majority of them are women who work for the rights of men. (...) The majority of women and men who work for the rights of women.\" (...) The majority of men are \"I am myself.\""}, {"heading": "3.2 Results", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "4 Discussion", "text": "There are several potential extensions to this work, many of which we are pursuing and presenting in a longer paper. In one of the classic essays on stacking [6] Breiman strongly advocates the use of non-negative weights in the use of a linear mixing model. The results we have presented do not in any way limit the vij, but work is under way to evaluate the value of using non-negative weights for FWLS. Another line of research to be pursued will involve truncating the extended space of ML models / meta feature pairs in order to reduce the number of estimated parameters and thus perhaps improve the accuracy and speed with which the mixture can be adjusted. There are a variety of linear model features that may be applicable here. It is also likely that meta features discovered in the course of using FWLS will be used as input for non-linear apertures."}], "references": [{"title": "Applying machine learning for prediction, recommmendation, and integration", "author": ["Xinlong Bao"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "The bellkor solution to the netflix prize", "author": ["Robert Bell", "Yehuda Koren", "Chris Volinsky"], "venue": "http://www.netflixprize.com/", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "The bellkor 2008 solution to the netflix prize", "author": ["Robert Bell", "Yehuda Koren", "Chris Volinsky"], "venue": "http://www.netflixprize", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Scalable collaborative filtering with jointly derived neighborhood interpolation weights", "author": ["Robert M. Bell", "Yehuda Koren"], "venue": "In ICDM,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Stacked regressions", "author": ["Leo Breiman"], "venue": "Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Is combining classifiers with stacking better than selecting the best one", "author": ["Saso D\u1e91eroski", "Bernard \u1e90enko"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Mccv stacked regression for model combination and fast spectral interval selection in multivariate calibration", "author": ["Lu Xu et. al"], "venue": "Chemometrics and Intelligent Laboratory Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Yehuda Koren"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "The bellkor solution to the netflix grand prize", "author": ["Yehuda Koren"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Collaborative filtering with temporal dynamics", "author": ["Yehuda Koren"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Improving regularized singular value decomposition for collaborative filtering", "author": ["A. Paterek"], "venue": "KDD-Cup and Workshop, ACM press,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "The pragmatic theory solution to the netflix grand prize", "author": ["Martin Piotte", "Martin Chabbert"], "venue": "http://www.netflixprize", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "A dynamic integration algorithm for an ensemble of classifiers", "author": ["Seppo Puuronen", "Vagan Terziyan", "Alexey Tsymbal"], "venue": "Foundations of Intelligent Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Stacking classifiers for anti-spam filtering of E-mail", "author": ["Georgios Sakkis", "Ion Androutsopoulos", "Georgios Paliouras", "Vangelis Karkaletsis", "Constantine D. Spyropoulos", "Panagiotis Stamatopoulos"], "venue": "Proceedings of EMNLP-01,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["Ruslan Salakhutdinov", "Andriy Mnih", "Geoffrey E. Hinton"], "venue": "In Zoubin Ghahramani, editor, ICML, volume 227 of ACM International Conference Proceeding Series,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Adjustment of an inverse matrix corresponding to changes in the elements of a given column or 16  a given row of the original matrix", "author": ["Jack Sherman", "Winifred J. Morrison"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1949}, {"title": "Ordinal matrix factorization, September 2009. http:// www.netflixprize.com/community/viewtopic.php?id=1541", "author": ["Joseph Sill"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Scalable collaborative filtering approaches for large recommender", "author": ["G\u00e1bor Tak\u00e1cs", "Istv\u00e1n Pil\u00e1szy", "Botty\u00e1n N\u00e9meth", "Domonkos Tikk"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Issues in stacked generalization", "author": ["Kai Ming Ting", "Ian H. Witten"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "The bigchaos solution to the netflix grand prize", "author": ["Andreas Toscher", "Michael Jahrer", "Robert Bell"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Stacked generalization", "author": ["David H. Wolpert"], "venue": "Neural Networks,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1992}], "referenceMentions": [{"referenceID": 20, "context": "Since its introduction [23], modellers have employed stacking successfuly on a wide variety of problems, including chemometrics [8], spam filtering [16], and large collections of datasets drawn from the UCI Machine learning repository [21, 7].", "startOffset": 23, "endOffset": 27}, {"referenceID": 6, "context": "Since its introduction [23], modellers have employed stacking successfuly on a wide variety of problems, including chemometrics [8], spam filtering [16], and large collections of datasets drawn from the UCI Machine learning repository [21, 7].", "startOffset": 128, "endOffset": 131}, {"referenceID": 13, "context": "Since its introduction [23], modellers have employed stacking successfuly on a wide variety of problems, including chemometrics [8], spam filtering [16], and large collections of datasets drawn from the UCI Machine learning repository [21, 7].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "Since its introduction [23], modellers have employed stacking successfuly on a wide variety of problems, including chemometrics [8], spam filtering [16], and large collections of datasets drawn from the UCI Machine learning repository [21, 7].", "startOffset": 235, "endOffset": 242}, {"referenceID": 5, "context": "Since its introduction [23], modellers have employed stacking successfuly on a wide variety of problems, including chemometrics [8], spam filtering [16], and large collections of datasets drawn from the UCI Machine learning repository [21, 7].", "startOffset": 235, "endOffset": 242}, {"referenceID": 19, "context": "The team BellKor\u2019s Pragmatic Chaos won the $1 million prize using a blend of hundreds of different models [22, 11, 14].", "startOffset": 106, "endOffset": 118}, {"referenceID": 8, "context": "The team BellKor\u2019s Pragmatic Chaos won the $1 million prize using a blend of hundreds of different models [22, 11, 14].", "startOffset": 106, "endOffset": 118}, {"referenceID": 11, "context": "The team BellKor\u2019s Pragmatic Chaos won the $1 million prize using a blend of hundreds of different models [22, 11, 14].", "startOffset": 106, "endOffset": 118}, {"referenceID": 0, "context": "In [2], a system called STREAM (Stacking Recommendation Engines with Additional Meta-Features) which blends recommendation models is presented.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "In a classification problem context [7], Dzeroski and Zenko attempt to augment a linear regression stacking algorithm by meta-features such as the entropy of the predicted class probabilities, although they found that it yielded limited benefit on a suite of tasks from the UC Irvine machine learning repository.", "startOffset": 36, "endOffset": 39}, {"referenceID": 12, "context": "An approach which does not use meta-features per se but which does employ an adaptive approach to blending is described by Puuronen, Terziyan, and Tsymbal [15].", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "Standard linear regression stacking [6] seeks a blended prediction function b of the", "startOffset": 36, "endOffset": 39}, {"referenceID": 2, "context": "In their 2008 Netflix Progress Prize paper [4], Bell, Koren and Volinsky make use of two meta-features (number of user and number of movie ratings) within a linear model in a construction which is similar to the formulation", "startOffset": 43, "endOffset": 46}, {"referenceID": 15, "context": "A faster approach is to use the Sherman-Morrison formula [18] for updating the inverse of a matrix, in which case the second terms in the two preceding expressions can be improved to M3L2 and M2L3, respectively.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "An overview of the techniques used to win the prizes is presented in the papers written by the prize winners [22, 11, 14].", "startOffset": 109, "endOffset": 121}, {"referenceID": 8, "context": "An overview of the techniques used to win the prizes is presented in the papers written by the prize winners [22, 11, 14].", "startOffset": 109, "endOffset": 121}, {"referenceID": 11, "context": "An overview of the techniques used to win the prizes is presented in the papers written by the prize winners [22, 11, 14].", "startOffset": 109, "endOffset": 121}, {"referenceID": 17, "context": "See [20] for an overview of these techniques.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "NSVD1 is an important variation on SVD first proposed by Paterek [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "[5, 10]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 7, "context": "[5, 10]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 14, "context": "Restricted Boltzmann machines (RBMs) [17], a kind of stochastic recurrent neural network, are a third major class of algorithms.", "startOffset": 37, "endOffset": 41}, {"referenceID": 1, "context": "pioneered by Bell, Koren, and Volinsky [3] .", "startOffset": 39, "endOffset": 42}, {"referenceID": 9, "context": "a bias) with each distinct date on which the user rated movies [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 16, "context": "The specific approach used to derive meta-feature 11 is described in [19], although an alternate technique which builds 4 separate models for the probability that the rating is less than or equal to r, 1 \u2264 r \u2264 4 is described in [14].", "startOffset": 69, "endOffset": 73}, {"referenceID": 11, "context": "The specific approach used to derive meta-feature 11 is described in [19], although an alternate technique which builds 4 separate models for the probability that the rating is less than or equal to r, 1 \u2264 r \u2264 4 is described in [14].", "startOffset": 228, "endOffset": 232}, {"referenceID": 4, "context": "In one of the classic papers on stacking [6], Breiman strongly advocates the use of non-negative weights", "startOffset": 41, "endOffset": 44}], "year": 2009, "abstractText": "Ensemble methods, such as stacking, are designed to boost predictive accuracy by blending the predictions of multiple machine learning models. Recent work has shown that the use of meta-features, additional inputs describing each example in a dataset, can boost the performance of ensemble methods, but the greatest reported gains have come from nonlinear procedures requiring significant tuning and training time. Here, we present a linear technique, Feature-Weighted Linear Stacking (FWLS), that incorporates meta-features for improved accuracy while retaining the well-known virtues of linear regression regarding speed, stability, and interpretability. FWLS combines model predictions linearly using coefficients that are themselves linear functions of meta-features. This technique was a key facet of the solution of the second place team in the recently concluded Netflix Prize competition. Significant increases in accuracy over standard linear stacking are demonstrated on the Netflix Prize collaborative filtering dataset.", "creator": "LaTeX with hyperref package"}}}