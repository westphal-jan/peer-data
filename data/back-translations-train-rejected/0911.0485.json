{"id": "0911.0485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Nov-2009", "title": "Novel Intrusion Detection using Probabilistic Neural Network and Adaptive Boosting", "abstract": "This article applies Machine Learning techniques to solve Intrusion Detection problems within computer networks. Due to complex and dynamic nature of computer networks and hacking techniques, detecting malicious activities remains a challenging task for security experts, that is, currently available defense systems suffer from low detection capability and high number of false alarms. To overcome such performance limitations, we propose a novel Machine Learning algorithm, namely Boosted Subspace Probabilistic Neural Network (BSPNN), which integrates an adaptive boosting technique and a semi parametric neural network to obtain good tradeoff between accuracy and generality. As the result, learning bias and generalization variance can be significantly minimized. Substantial experiments on KDD 99 intrusion benchmark indicate that our model outperforms other state of the art learning algorithms, with significantly improved detection accuracy, minimal false alarms and relatively small computational complexity.", "histories": [["v1", "Tue, 3 Nov 2009 04:07:19 GMT  (779kb)", "http://arxiv.org/abs/0911.0485v1", "9 pages IEEE format, International Journal of Computer Science and Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,this http URL"]], "COMMENTS": "9 pages IEEE format, International Journal of Computer Science and Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,this http URL", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["tich phuoc tran", "longbing cao", "dat tran", "cuong duc nguyen"], "accepted": false, "id": "0911.0485"}, "pdf": {"name": "0911.0485.pdf", "metadata": {"source": "CRF", "title": "Novel Intrusion Detection using Probabilistic Neural Network and Adaptive Boosting", "authors": ["Tich Phuoc Tran", "Longbing Cao", "Dat Tran", "Cuong Duc Nguyen"], "emails": ["lbcao}@it.uts.edu.au", "Dat.Tran@canberra.edu.au", "ndcuong@hcmiu.edu.vn"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "RELATED WORKS", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "NETWORK (BSPNN)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Bias-Variance-Computation Dilemma", "text": "First, because of its popularity and well-known properties, we consider Artificial Neural Network (ANN) to be a flexible \"model-free\" learning method. ANN can fit very well with training data and therefore offers low learning bias. However, they are prone to overadjustment, which can lead to instability in generalization [24]. Newer remedies try to improve model stability by reducing variance in generalization at the expense of worse learning bias, i.e. allowing underadjustment. However, underadjustment is not acceptable for some applications that require high classification accuracy. Therefore, a system that can achieve both stable generalization and precise learning is imperative for applications like 84 http: / / sites.google.com / site / ijcsis / ISSN 1947-5500in Intrusion Detection. Mathematically, both bias and variance can be reduced simultaneously, as infinitely large models are impracticable."}, {"heading": "B. Objectives", "text": "This work is inspired by a lightweight ANN model, namely Vector Quantized-Generalized Regression Neural Network (VQ-GRNN) [25], which reduces the non-parametric GRNN to a semi-parametric model by applying vector quantization techniques to the training data, i.e. dividing the input space into a smaller subspace. Compared to the GRNN method, which incorporates each training vector into its structure, VQ-GRNN applies only to a smaller number of input data. This improves the robustness of the algorithm (low variance), but also controls its learning accuracy to a certain degree. To make the VQ-GRNN suitable for intrusion problems, we need to improve its independence."}, {"heading": "C. Model description", "text": "In fact, the fact is that most of them are able to show themselves how they are able to change and change the world, and that they are able to change and change the world in order to change the world."}, {"heading": "DETECTION", "text": "Current IDS suffer from low detection accuracy and insufficient system stability for new and rare security breaches. In this section, we use our BSPNN to identify known and novel attacks in the KDD 99 dataset [1], which contains TCP / IP connection datasets. Each dataset consists of 41 attributes (features) and a target value (labeled data) that indicates whether a connection is normal or an attack. There are 40 types of attacks divided into four important categories, namely probing (gathering information from the target system before an attack), denial of service (DoS) (preventing legitimate requests to a network resource by consuming bandwidth or overloading computing resources), user-to-root (U2R) (attackers with normal user access get privileges from the root user), and remote-to-local (R2L) (unauthorized user ability to execute the Soog.com / ISD 587 commands)."}, {"heading": "A. Experiment Setup", "text": "Considering a test set, the average cost of a classifier is calculated as below [1]: VGpq \u2211 VGO l, r s VGpql, r t2 t (4) WhereN: Total number of connections in datasetConfM (i, j): the entry in the confusion matrix.CostM (i, j): the entry in line i, column j in the cost matrix.2) Datasets Creation First, we consider anomaly detection, where only normal connection records are available for training. Any connections that deviate from these normal records are classified as \"abnormal,\" without further specifying which attack categories it actually belongs to."}, {"heading": "B. Experiment Result", "text": "It is only a matter of time before such an attack occurs. (...) Its detection rates (DR) are displayed in different attack categories. We may detect a general trend of increasing performance as more intruders into training. In particular, detection of R2L attacks requires less known intruders (DR starts from vx) than those of other classs.With the full training set (v u) we test our BSPNN attacks. (DR starts from vx) than the other classes."}], "references": [{"title": "Results of the KDD\u201999 Classifier Learning,", "author": ["C. Elkan"], "venue": "ACM SIGKDD Explorations,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Machine Learning and Data Mining: Introduction to Principles and Algorithms", "author": ["I. Kononenko", "M. Kukar"], "venue": "Horwood Publishing Limited,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "NIDX \u2013 an expert system for realtime network intrusion detection,\" in Proceeding of the Computer Networking", "author": ["D.S. Bauer", "M.E. Koblentz"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1988}, {"title": "State transition analysis: a rulebased intrusion detection approach,", "author": ["K. Ilgun", "R. Kemmerer", "P. Porras"], "venue": "IEEE Transactions on Software Engineering,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Mining Audit Data to Build Intrusion Detection Models,", "author": ["W. Lee", "S. Stolfo", "K. Mok"], "venue": "Proc. Fourth International Conference Knowledge Discovery and Data Mining pp", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "A Data Mining Framework for Building Intrusion Detection", "author": ["W. Lee", "S. Stolfo", "K. Mok"], "venue": "Model,\" Proc. IEEE Symp. Security and Privacy,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Naive Bayes vs. Decision Trees in Intrusion Detection Systems,", "author": ["N.B. Amor", "S. Benferhat", "Z. Elouedi"], "venue": "Proc. ACM Symp. Applied Computing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Winning the KDD99 Classification Cup: Bagged Boosting,", "author": ["B. Pfahringer"], "venue": "SIGKDD Explorations,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "The MP13 Approach to the KDD\u201999 Classifier Learning Contest,", "author": ["V. Miheev", "A. Vopilov", "I. Shabalin"], "venue": "SIGKDD Explorations,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "KDD-99 Classifier Learning Contest: LLSoft\u2019s Results Overview,", "author": ["I. Levin"], "venue": "SIGKDD Explorations,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Effective Value of Decision Tree with KDD 99 Intrusion Detection Datasets for Intrusion Detection", "author": ["J.-H. Lee", "S.-G. Sohn", "J.-H. Ryu", "T.-M. Chung"], "venue": "System,\" in 10th International Conference on Advanced Communication Technology", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Ucles, \"HIDE: A Hierarchical Network Intrusion Detection System Using Statistical Preprocessing and Neural Network Classification,", "author": ["Z. Zhang", "J. Li", "C.N. Manikopoulos", "J. Jorgenson"], "venue": "Proc. IEEE Workshop Information Assurance and Security,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Artificial neural networks for misuse", "author": ["J. Cannady"], "venue": "Proceedings of the National Information Systems Security Conference Arlington,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Intrusion detection using neural networks and support vector machines,", "author": ["S. Mukkamala", "G. Janoski", "A. Sung"], "venue": "in International Joint Conference on Neural Networks (IJCNN)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Kanthamanon, \"Hybrid neural networks for intrusion detection", "author": ["C. Jirapummin", "N. Wattanapongsakorn"], "venue": "Proceedings of The 2002 International Technical Conference On Circuits/Systems,Computers and Communications,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "Intrusion Detection with Unlabeled Data Using Clustering,", "author": ["L. Portnoy", "E. Eskin", "S. Stolfo"], "venue": "Proc. ACM Workshop Data Mining Applied to Security (DMSA),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Fuzzy Clustering for Intrusion Detection,", "author": ["H. Shah", "J. Undercoffer", "A. Joshi"], "venue": "Proc. 12th IEEE International Conference Fuzzy Systems (FUZZ-IEEE \u201903),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Bayesian Event Classification for Intrusion Detection,", "author": ["C. Kruegel", "D. Mutz", "W. Robertson", "F. Valeur"], "venue": "Proc. 19th Annual Computer Security Applications Conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Multi class support vector machine implementation to intrusion detection,", "author": ["T. Ambwani"], "venue": "in Proc. of IJCNN,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Training Genetic Programming on Half a Million Patterns: An Example from Anomaly Detection,", "author": ["D. Song", "M.I. Heywood", "A.N. Zincir-Heywood"], "venue": "IEEE Trans. Evolutionary Computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Modeling Program Behaviors by Hidden Markov Models for Intrusion Detection,", "author": ["W. Wang", "X.H. Guan", "X.L. Zhang"], "venue": "Proc. International Conference Machine Learning and Cybernetics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "A Framework for Constructing Features and Models for Intrusion Detection Systems,", "author": ["W. Lee", "S. Stolfo"], "venue": "Information and System Security,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2000}, {"title": "Layered Approach using Conditional Random Fields for Intrusion Detection,", "author": ["K.K. Gupta", "B. Nath", "R. Kotagiri"], "venue": "IEEE Transactions on Dependable and Secure Computing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Neural Networks for Intelligent Signal Processing", "author": ["A. Zaknich"], "venue": "Sydney: World Scientific Publishing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Introduction to the modified probabilistic neural network for general signal processing applications,", "author": ["A. Zaknich"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "A general regression neural network,", "author": ["D.F. Sp"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1991}, {"title": "A brief introduction to boosting,", "author": ["R.E. Schapire"], "venue": "Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1999}, {"title": "A decision-theoretic generation of on-line learning and an application to boosting,", "author": ["Y. Freund", "R. Schapire"], "venue": "Journal of Computer and System Science,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1997}, {"title": "Bias plus variance decomposition for zeroone loss functions,", "author": ["R. Kohavi", "D. Wolpert"], "venue": "in Proc. of International Conference on Machine Learning Italy,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "Probabilistic neural networks,", "author": ["D.F. Specht"], "venue": "Neural Networks,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1990}, {"title": "Efficient Multiclass Boosting Classification with Active Learning,", "author": ["J. Huang", "S. Ertekin", "Y. Song", "H. Zha", "C.L. Giles"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Using artificial anomalies to detect unknown and known network intrusions,", "author": ["W. Fan", "M. Miller", "S. Stolfo", "W. Lee", "P. Chan"], "venue": "Knowledge and Information Systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "PNrule: A New Framework for Learning Classifier Models in Data Mining,\" in A Case-Study in Network", "author": ["R. Agarwal", "M.V. Joshi"], "venue": "Intrusion Detection,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "The majority of currently existing IDS face a number of challenges such as low detection rates which can miss serious intrusion attacks and high false alarm rates, which falsely classifies a normal connection as an attack and therefore obstructs legitimate user access to the network resources [1].", "startOffset": 294, "endOffset": 297}, {"referenceID": 1, "context": "However, this normally requires infinite training sample sizes (theoretically) [2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": "An Intrusion Detection System (IDS) is defined as a protection system that monitors computers or networks for unauthorized activities based on network traffic or system usage behaviors, thereby detecting if a system is targeted by a network attack such as a denial of service attack [4].", "startOffset": 283, "endOffset": 286}, {"referenceID": 2, "context": "One of the rule-based methods which is commonly used by early IDS is the Expert System (ES) [3, 4].", "startOffset": 92, "endOffset": 98}, {"referenceID": 3, "context": "One of the rule-based methods which is commonly used by early IDS is the Expert System (ES) [3, 4].", "startOffset": 92, "endOffset": 98}, {"referenceID": 4, "context": "developed a data mining framework for the purpose of intrusion detection [5, 6].", "startOffset": 73, "endOffset": 79}, {"referenceID": 5, "context": "developed a data mining framework for the purpose of intrusion detection [5, 6].", "startOffset": 73, "endOffset": 79}, {"referenceID": 6, "context": "Decision trees are one of the most commonly used supervised learning algorithms in IDS [7-11] due to its simplicity, high detection accuracy and fast adaptation.", "startOffset": 87, "endOffset": 93}, {"referenceID": 7, "context": "Decision trees are one of the most commonly used supervised learning algorithms in IDS [7-11] due to its simplicity, high detection accuracy and fast adaptation.", "startOffset": 87, "endOffset": 93}, {"referenceID": 8, "context": "Decision trees are one of the most commonly used supervised learning algorithms in IDS [7-11] due to its simplicity, high detection accuracy and fast adaptation.", "startOffset": 87, "endOffset": 93}, {"referenceID": 9, "context": "Decision trees are one of the most commonly used supervised learning algorithms in IDS [7-11] due to its simplicity, high detection accuracy and fast adaptation.", "startOffset": 87, "endOffset": 93}, {"referenceID": 10, "context": "Decision trees are one of the most commonly used supervised learning algorithms in IDS [7-11] due to its simplicity, high detection accuracy and fast adaptation.", "startOffset": 87, "endOffset": 93}, {"referenceID": 11, "context": "ANN-based IDS [12-15] have achieved great successes in detecting difficult attacks.", "startOffset": 14, "endOffset": 21}, {"referenceID": 12, "context": "ANN-based IDS [12-15] have achieved great successes in detecting difficult attacks.", "startOffset": 14, "endOffset": 21}, {"referenceID": 13, "context": "ANN-based IDS [12-15] have achieved great successes in detecting difficult attacks.", "startOffset": 14, "endOffset": 21}, {"referenceID": 14, "context": "ANN-based IDS [12-15] have achieved great successes in detecting difficult attacks.", "startOffset": 14, "endOffset": 21}, {"referenceID": 15, "context": "For unsupervised intrusion detection, data clustering methods can be applied [16, 17].", "startOffset": 77, "endOffset": 85}, {"referenceID": 16, "context": "For unsupervised intrusion detection, data clustering methods can be applied [16, 17].", "startOffset": 77, "endOffset": 85}, {"referenceID": 6, "context": "Another well-known ML techniques used in IDS is Na\u00efve Bayes classifiers [7].", "startOffset": 72, "endOffset": 75}, {"referenceID": 17, "context": "In [18], the authors apply a Bayesian network for IDS.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Beside popular decision trees and ANN, Support Vector Machines (SVMs) are also a good candidate for intrusion detection systems [14, 19] which can provide real-time detection capability, deal with large dimensionality of data.", "startOffset": 128, "endOffset": 136}, {"referenceID": 18, "context": "Beside popular decision trees and ANN, Support Vector Machines (SVMs) are also a good candidate for intrusion detection systems [14, 19] which can provide real-time detection capability, deal with large dimensionality of data.", "startOffset": 128, "endOffset": 136}, {"referenceID": 19, "context": "Several other AI paradigms including linear genetic programming [20] , Hidden Markov Model [21], Columbia Model [22] and Layered Conditional Random Fields [23] have been applied for the design of IDS.", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "Several other AI paradigms including linear genetic programming [20] , Hidden Markov Model [21], Columbia Model [22] and Layered Conditional Random Fields [23] have been applied for the design of IDS.", "startOffset": 91, "endOffset": 95}, {"referenceID": 21, "context": "Several other AI paradigms including linear genetic programming [20] , Hidden Markov Model [21], Columbia Model [22] and Layered Conditional Random Fields [23] have been applied for the design of IDS.", "startOffset": 112, "endOffset": 116}, {"referenceID": 22, "context": "Several other AI paradigms including linear genetic programming [20] , Hidden Markov Model [21], Columbia Model [22] and Layered Conditional Random Fields [23] have been applied for the design of IDS.", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "However, they are susceptible to overfitting, which can cause instability in generalization [24].", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "in Intrusion Detection [19].", "startOffset": 23, "endOffset": 27}, {"referenceID": 24, "context": "This paper is inspired by a light-weight ANN model, namely Vector Quantized-Generalized Regression Neural Network (VQ-GRNN) [25], which reduces the nonparametric GRNN [26] to a semiparametric model by applying vector quantization techniques on the training data, i.", "startOffset": 124, "endOffset": 128}, {"referenceID": 25, "context": "This paper is inspired by a light-weight ANN model, namely Vector Quantized-Generalized Regression Neural Network (VQ-GRNN) [25], which reduces the nonparametric GRNN [26] to a semiparametric model by applying vector quantization techniques on the training data, i.", "startOffset": 167, "endOffset": 171}, {"referenceID": 23, "context": "This significantly improves the robustness of the algorithm (low variance), but also controls its learning accuracy to some extent [24].", "startOffset": 131, "endOffset": 135}, {"referenceID": 26, "context": "Ensemble methods such as Boosting [27] iteratively learn multiple classifiers (base classifiers) on different distributions of training data.", "startOffset": 34, "endOffset": 38}, {"referenceID": 27, "context": "Amongst popular boosting variants, we choose Adaptive Boosting or AdaBoost [28] to improve performance of VQ-GRNN.", "startOffset": 75, "endOffset": 79}, {"referenceID": 26, "context": "AdaBoost is \u201cadaptive\u201d in the sense that it does not require prior knowledge of the accuracy of these hypotheses [27].", "startOffset": 113, "endOffset": 117}, {"referenceID": 8, "context": "Moreover, it is widely accepted that generalization performance of a combined classifier is not necessarily achieved by combining classifiers with better individual performance but by including independent classifiers in the ensemble [9].", "startOffset": 234, "endOffset": 237}, {"referenceID": 24, "context": "This base learner f is actually a modified version of the emerging VQ-GRNN model [25] (called Modified GRNN Base learner) in which the input data space is reduced significantly (by the Weighted vector quantization module) and its output is computed by a linearly weighted mixture of Radial Basis Function (RBF).", "startOffset": 81, "endOffset": 85}, {"referenceID": 28, "context": "The Diversity Checker measures ensemble diversity by using Kohavi-Wolpert variance [30] (which is denoted by the hypothesis weighting coefficient ).", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "We adapt VQ-GRNN [25] as a base learner in our BSPNN model.", "startOffset": 17, "endOffset": 21}, {"referenceID": 25, "context": "VQ-GRNN is closely related to Specht\u2019s GRNN [26] and PNN [31] classifiers.", "startOffset": 44, "endOffset": 48}, {"referenceID": 29, "context": "VQ-GRNN is closely related to Specht\u2019s GRNN [26] and PNN [31] classifiers.", "startOffset": 57, "endOffset": 61}, {"referenceID": 24, "context": "Using this idea, the VQ-GRNN\u2019s equation can be generalized [25]:", "startOffset": 59, "endOffset": 63}, {"referenceID": 30, "context": "By sufficiently handling the multiclass problem and using confidencerated predictions, SAMME can maximize the distribution margins of the training data [32].", "startOffset": 152, "endOffset": 156}, {"referenceID": 28, "context": "Also, our implementation of Kohavi-Wolpert variance (KW) [30] in the reweighting of hypotheses in the joint classification can effectively enforce the ensemble diversity.", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "In this section, we apply our BSPNN to identify known and novel attacks in the KDD-99 dataset [1], containing TCP/IP connection records.", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "KDD-99 COMPONENT DATASETS [1]", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "Given a test set, the average cost of a classifier is calculated as below [1]:", "startOffset": 74, "endOffset": 77}, {"referenceID": 31, "context": "In [33], artificial anomalies are added to the training data to help the learner discover a boundary around the available training data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "Each cluster contains intrusions that require similar features for effective detection and this method, as detailed in [33], is not influenced by cluster orders.", "startOffset": 119, "endOffset": 123}, {"referenceID": 31, "context": "Table 4 shows that our BSPNN obtains competitive detection rate compared with [33] while achieves significantly lower false alarm rate (1.", "startOffset": 78, "endOffset": 82}, {"referenceID": 7, "context": "Using the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11].", "startOffset": 113, "endOffset": 116}, {"referenceID": 32, "context": "Using the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11].", "startOffset": 149, "endOffset": 153}, {"referenceID": 18, "context": "Using the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11].", "startOffset": 194, "endOffset": 198}, {"referenceID": 22, "context": "Using the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11].", "startOffset": 255, "endOffset": 259}, {"referenceID": 21, "context": "Using the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11].", "startOffset": 280, "endOffset": 284}, {"referenceID": 10, "context": "Using the full training set (v u), we test our BSPNN against other existing methods, including the KDD-99 winner [8], the rule-based PNrule approach [34], the multi-class Support Vector Machine [19], the Layered Conditional Random Fields Framework (LCRF) [23], the Columbia Model [22] and the Decision Tree method [11].", "startOffset": 314, "endOffset": 318}, {"referenceID": 31, "context": "[33] BSPNN", "startOffset": 0, "endOffset": 4}], "year": 2009, "abstractText": "This article applies Machine Learning techniques to solve Intrusion Detection problems within computer networks. Due to complex and dynamic nature of computer networks and hacking techniques, detecting malicious activities remains a challenging task for security experts, that is, currently available defense systems suffer from low detection capability and high number of false alarms. To overcome such performance limitations, we propose a novel Machine Learning algorithm, namely Boosted Subspace Probabilistic Neural Network (BSPNN), which integrates an adaptive boosting technique and a semi-parametric neural network to obtain good trade-off between accuracy and generalty. As the result, learning bias and generalization variance can be significantly minimized. Substantial experiments on KDD-99 intrusion benchmark indicate that our model outperforms other state-of-the-art learning algorithms, with significantly improved detection accuracy, minimal false alarms and relatively small computational complexity. KeywordsIntrusion Detection, Neural Network, Adaptive Boosting", "creator": "PScript5.dll Version 5.2"}}}