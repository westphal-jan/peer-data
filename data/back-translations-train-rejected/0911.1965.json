{"id": "0911.1965", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Nov-2009", "title": "Active Learning for Mention Detection: A Comparison of Sentence Selection Strategies", "abstract": "We propose and compare various sentence selection strategies for active learning for the task of detecting mentions of entities. The best strategy employs the sum of confidences of two statistical classifiers trained on different views of the data. Our experimental results show that, compared to the random selection strategy, this strategy reduces the amount of required labeled training data by over 50% while achieving the same performance. The effect is even more significant when only named mentions are considered: the system achieves the same performance by using only 42% of the training data required by the random selection strategy.", "histories": [["v1", "Tue, 10 Nov 2009 19:52:01 GMT  (40kb,D)", "http://arxiv.org/abs/0911.1965v1", "12 pages, 9 figures"]], "COMMENTS": "12 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["nitin madnani", "hongyan jing", "nanda kambhatla", "salim roukos"], "accepted": false, "id": "0911.1965"}, "pdf": {"name": "0911.1965.pdf", "metadata": {"source": "CRF", "title": "Active Learning for Mention Detection: A Comparison of Sentence Selection Strategies", "authors": ["Nitin Madnani", "Hongyan Jing", "Nanda Kambhatla"], "emails": ["nmadnani@umiacs.umd.edu", "hjing@us.ibm.com", "nanda@us.ibm.com", "roukos@us.ibm.com"], "sections": [{"heading": "1 Introduction", "text": "Given the high cost, it is critical to improve the efficiency of such annotations. Active learning involves the intelligent and non-random selection of samples for human annotations. In this way, it is possible for systems with the same amount of annotations to perform better or achieve the same level of performance with much less annotated data. In this paper, we present several new active learning strategies for the Mention Detection (MD) task, using the terminology used in automatic content extraction conferences. Mentions are references to real-world units that can be named (e.g. John), nominal (e.g. survivors) or pronominal (e.g. he). We propose and examine a variety of sentence selection criteria for active learning, including various sentence selection criteria that combine uncertainty and query results."}, {"heading": "2 Related Work", "text": "Active learning has been used for many NLP applications, most notably for text classification [2 5], it has also been used for the labeling of language components [6], statistical parsing [7 10], chunking nouns [11], segmentation of Japanese words [12], and confusion of conceptual clarity [13]. Few cases are reported where active learning techniques have been used to mention recognition [14], several information extraction strategies have been studied, and they observed that some strategies lead to improved memory behavior, while others improve precision, but it is essential to achieve a significant improvement in both memory behavior and precision for an active learner so that he performs better than random selection. [15] They suggested a multi-criteria-based active learning approach for recognizing benign learning units [16] that they may include a quantitative and informative basis for at least 80% of the varied criteria."}, {"heading": "3 Active Learning", "text": "Active learning techniques are usually divided into two types: uncertainty samples for an individual learner [17] or discrepancy measurements between a panel of learners [18]. In any case, seed data must be provided to create an initial model or model. In the uncertainty-based approach, an individual learner identifies unlabeled samples and provides a consensus value for each predicted label. For manual labeling, samples that have the lowest consensus values are selected. In the query-for-committee approach, a committee of learners is formed in which each learner labels the unlabeled samples. Samples that have the highest discrepancy among committee members are selected for manual labeling. In our work, we experiment with query-for-rejects-based approaches as well as hybrid approaches in which we use a weighted combination of multiple learners."}, {"heading": "3.1 The MALACH Information Extraction Corpus", "text": "The MALACH collection [19, 20] contains 116,000 hours of digitized interviews and testimonies in 32 languages of 52,000 survivors, liberators, rescuers and witnesses of the Holocaust. We are interested in the automatic extraction of information from this corpus of spontaneous conversations. For a small number of English testimonies, we transcribed them manually and commented on the transcripts with three types of information: mentions. Named, nominal and pronominal mentions of 20 categories of entities. Co-reference. Sentences of mentions referring to the same real entity. Relationships. Sentences of relationships between pairs of mentions. For example, in view of the sentence I spent a year in Auschwitz, there is the following relationship: LocatedAt (I, Auschwitz). For the experiments described in this paper, we decided to focus only on mentioning mentions."}, {"heading": "3.2 Granularity of Selection", "text": "A possible problem with the selection of samples at the document level [14] is that a document can only be partially useful from the point of view of learning, and it is impossible to add only the interesting examples to maximize the effect of active learning. This problem is particularly acute in our corpus, since the documents that are survivors \"testimonies are very long and contain a lot of redundant information. Selecting samples as samples, as in [15], has the advantage that all samples are of the same length, but protects against the problem that a human annotator has to comment on mentions by looking only at the tokens. The tokens selected may also contain partial mentions if mention limits are misidentified. Sentries, on the other hand, contain enough non-redundant contexts to capture information that we can use them at the level of active learning [16]."}, {"heading": "3.3 Framework Description", "text": "We propose a new framework for sample selection based on active learning that offers significant savings on the annotation task and can provide better performance for the same amount of annotations. Figure 1 shows the architecture of the framework. As already mentioned, the central idea is to use an overall ensemble of class speakers - each one is different in some ways from the others. Once trained, the class speakers can then be used to detect mentions in the unlabeled sentences. These determined mentions can then be compared for each sentence, and a score assigned to that sentence indicates the match or absence of the same among the class speakers. The sentences where the class speakers disagree the most are intuitively the sentences that can contribute the most to learning. The sentences are then ranked according to this metric and the required number of sampled from this ranking. In the case of active learning, these samples would then be made available to the training sampler and the actual learner."}, {"heading": "3.4 Statistical classi ers", "text": "At each step of the active learning process, we build two maximum entropy-based statistical classes [21] based on the marked data available in this step. There are two dimensions of class teaching - the classifier feature set and the data used for training. In our experiments, we use two different combinations of these dimensions: Each student is trained with the same characteristics but on a specific half of the available marked data. We call this a Data-di-erent (DD) setting. Each class is trained on the basis of all available marked data, but using a different set of characteristics: the internal class uses lexical characteristics derived exclusively from the current token, and the external class uses characteristics that include only surrounding tokens. We call this experimental environment Feature-di-erent (FD)."}, {"heading": "3.5 Sentence scoring metrics", "text": "In fact, most of us are able to put ourselves at the top, in the way that they are able to put themselves at the top."}, {"heading": "4 Results", "text": "We conducted a series of experiments to evaluate the sentence selection strategies presented in the previous section. All strategies perform better than random selection, but the trend sum metric with the feature di erent class si er training setting offers the biggest improvement. Ideally, active learners should be retrained each time a new sample is selected through active learning and commented by a human. In reality, this is rarely done due to time and computational costs. Instead, active learners usually work in batch mode. However, this approach offers no way to distinguish between a sentence that is 100 words long and another that is only 10 words long, a way to define the size of this stack in the number of sentences that need to be added at each step."}, {"heading": "4.1 E ect of di erent scoring metrics", "text": "In the experiments described here, we consider only named and nominal mentions. Weights for all mention categories are set to 1.Random Selection. We have created a baseline performance by randomly selecting sentences from the unnamed data pool. We have 5 runs and averaged numbers from each run. Performance for the random selection strategy is shown in Figure 2, including each run and its average. Average power is used as a basis and shown in later graphs. X-axis represents the data size in the number of words, and Y-axis represents the F-axis."}, {"heading": "4.2 Focusing on speci c types of mentions", "text": "As we have described in the previous section, our system allows us to precisely weight the different types and levels of mentions to focus our learning on specific mention types and levels. We provide the results of two such experiments: one that focuses on the PERSON category, and one that focuses on the mentioned mention types. First, we aim the learning at the mention types of the PERSON type. To achieve this, we weighted the PERSON category twice as strongly as all other mention types. The results are in Figure 7. Note that the F measure along the y axis in the diagram is the performance for the PERSON category.The second experiment was for the mention types. In this experiment, we set the weights to 0 for all other mention types. The result is an active learning spectrum called F20 F20 120 data size 706805K 7505K words (755K)"}, {"heading": "4.3 E ect of step size", "text": "The size of the step used in learning needs to be optimized. On the one hand, achieving higher performance gains through smaller amounts of labeled data that need to be added at each step requires huge savings on the annotation task; on the other hand, retraining active learners at each step requires time and resources. We need to balance the cost of retraining active learners and the gains from smaller batches. We experimented with different step sizes for the characteristic metric (FD) con dencesum metric. The results are in Figure 9. From this graph, it is clear that a step size of 10,000 words could provide the best balance between annotation and performance. Step sizes above 20,000 do not appear to have the right granularity for electronic learning."}, {"heading": "5 Conclusions", "text": "We conducted several active learning experiments to detect mentions of entities in human transcripts of spontaneous conversation language. Specifically, we proposed and compared a variety of sentence selection strategies for active learning. The best strategy uses the sum of the connotation values of a pair of classmates trained with different characteristics. Compared to random sentence selection, this strategy required 50% of the data to achieve the same performance; for the mentioned mentions, the strategy only required 42% of the data required by random selection to achieve the same performance.In the future, we would like to test our active learning strategies on data from a field and data in languages other than English, such as the ACE mention corpus. We would also like to examine active learning for correlation and correlation comments."}, {"heading": "Acknowledgment", "text": "This project was part of research funded by the NSF. Any opinions, conclusions or recommendations expressed in this material are those of the authors and do not necessarily agree with the views of the NSF."}], "references": [{"title": "Automatic Content Extraction", "author": ["ACE"], "venue": "http://www.nist.gov/speech/tests/ace/", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Heterogeneous uncertainty sampling for supervised learning", "author": ["D. Lewis", "J. Catlett"], "venue": "Proceedings of ICML 1994, 11th International Conference on Machine Learning, New Brunswick, NJ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Employing EM in pool-based active learning for text classi cation", "author": ["A.K. McCallum", "K. Nigam"], "venue": "Proceedings of ICML 1998, 15th International Conference on Machine Learning, Madison, US", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Less is more: Active learning with support vector machines", "author": ["G. Schohn", "D. Cohn"], "venue": "Proceedings of ICML 2000, 17th International Conf. on Machine Learning.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Support vector machine active learning with applications to text classi cation", "author": ["S. Tong", "D. Koller"], "venue": "Journal of Machine Learning Research 2", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Committee-based sample selection for probabilistic classi ers", "author": ["S. Argamon-Engelson", "I. Dagan"], "venue": "Journal of Arti cial Intelligence Research 11", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Active learning for natural language parsing and information extraction", "author": ["C.A. Thompson", "M.E. Cali", "R.J. Mooney"], "venue": "Proceedings of ICML 1999, 16th International Conference on Machine Learning.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Active learning for statistical natural language parsing", "author": ["M. Tang", "X. Luo", "S. Roukos"], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Lingusitics, Philadelphia, PA, USA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Example selection for bootstrapping statistical parsers", "author": ["M. Steedman", "R. Hwa", "S. Clark", "M. Osborne", "A. Sarkar", "J. Hockenmacier", "P. Ruhlen", "S. Baker", "J. Crim"], "venue": "Proceedings of HLT-NAACL 2003, Edmonton, Canada", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Ensemble-based active learning for parse selection", "author": ["M. Osborne", "J. Baldridge"], "venue": "Proceedings of HLT-NAACL 2004, Boston, Massachusetts, USA", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Rule writing or annotation: Cost-e cient resource usage for base noun phrase chunking", "author": ["G. Ngai", "D. Yarowsky"], "venue": "Proceedings of 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "An empirical study of active learning with support vector machines for japanese word segmentation", "author": ["M. Sassano"], "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Scaling to very very large corpora for natural language disambiguation", "author": ["M. Banko", "E. Brill"], "venue": "Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, Toulouse, France", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Active learning selection strategies for information extraction", "author": ["A. Finn", "N. Kushmerick"], "venue": "ECML-03 Workshop on Adaptive Text Extraction and Mining, Croatia", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Multi-criteria-based active learning for named entity recognition", "author": ["D. Shen", "J. Zhang", "J. Su", "G. Zhou", "C.L. Tan"], "venue": "Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL'04), Main Volume, Barcelona, Spain", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Investigating the e ects of selective sampling on the annotation task", "author": ["B. Hachey", "B. Alex", "M. Becker"], "venue": "Proceedings of the 9th Conference on Compuational Natural Language Learning(CoNLL), Ann Arbor", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Active learning with statistical models", "author": ["D.A. Cohn", "Z. Ghahramani", "M.I. Jordan"], "venue": "Journal of Arti cial Intelligence Research 4", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "Query by committee", "author": ["H.S. Seung", "M. Opper", "H. Sompolinsky"], "venue": "Computational Learning Theory.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1992}, {"title": "Supporting access to large digital oral history archives", "author": ["S. Gustman", "D.S.D. Oard", "W. Byrne", "M. Picheny", "B. Ramabhadran", "D. Greenberg"], "venue": "Proceedings of the Joint Conference on Digital Libraries.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Building an information retrieval test collection for spontaneous conversational speech", "author": ["D. Oard", "D. Soergel", "D. Doermann", "X. Huang", "G. Murray", "J. Wang", "B. Ramabhadran", "M. Franz", "S. Gustman", "J. May eld", "L. Kharevych", "S. Strassel"], "venue": "Proceedings of SIGIR'04, She eld, U.K.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "A statistical model for multilingual entity detection and tracking", "author": ["R. Florian", "H. Hassan", "A. Ittycheriah", "H. Jing", "N. Kambhatla", "X. Luo", "N. Nicolov", "S. Roukos"], "venue": "Proceedings of HLT-NAACL 2004, Boston, Massachusetts, USA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Here, we employ the terminology used in the Automatic Content Extraction Conferences [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "Active learning has been utilized for many NLP applications, most noticeably for text classi cation [2 5].", "startOffset": 100, "endOffset": 105}, {"referenceID": 4, "context": "Active learning has been utilized for many NLP applications, most noticeably for text classi cation [2 5].", "startOffset": 100, "endOffset": 105}, {"referenceID": 5, "context": "It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].", "startOffset": 77, "endOffset": 83}, {"referenceID": 9, "context": "It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].", "startOffset": 77, "endOffset": 83}, {"referenceID": 10, "context": "It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "It has also been applied for part-of-speech tagging [6], statistical parsing [7 10], noun phrase chunking [11], Japanese word segmentation [12], and confusion set disambiguation [13].", "startOffset": 178, "endOffset": 182}, {"referenceID": 13, "context": "[14] investigated several document, rather than sentence, selection strategies for Information Extraction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] proposed a multi-criteria-based active learning approach for named entity recognition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] investigated a query-by-committee-based active learner for information extraction in the astronomy domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Active learning techniques are usually divided into two types: uncertainty sampling for a single learner [17], or disagreement measurement between a committee of learners [18].", "startOffset": 105, "endOffset": 109}, {"referenceID": 17, "context": "Active learning techniques are usually divided into two types: uncertainty sampling for a single learner [17], or disagreement measurement between a committee of learners [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 18, "context": "The MALACH collection [19, 20] contains 116,000 hours of digitized interviews and testimonies in 32 languages from 52,000 survivors, liberators, rescuers and witnesses of the Holocaust.", "startOffset": 22, "endOffset": 30}, {"referenceID": 19, "context": "The MALACH collection [19, 20] contains 116,000 hours of digitized interviews and testimonies in 32 languages from 52,000 survivors, liberators, rescuers and witnesses of the Holocaust.", "startOffset": 22, "endOffset": 30}, {"referenceID": 13, "context": "A possible problem with selecting samples at the document level [14] is that a document may only be partially useful from a learning point of view and and it is impossible to add only the interesting examples so as to maximize the e ect of active learning.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "Selecting tokens as samples, as in [15], has the advantage that all samples are of equal length but su ers from the problem that a human annotator has to annotate mentions by looking at only the tokens.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "Therefore, we use sentence-level blocks for active learning, as in [16].", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "At each step in the active learning process, we build two maximum-entropy based statistical classi ers [21] using the labeled data available at that step.", "startOffset": 103, "endOffset": 107}], "year": 2014, "abstractText": "We propose and compare various sentence selection strategies for active learning for the task of detecting mentions of entities. The best strategy employs the sum of con dences of two statistical classi ers trained on di erent views of the data. Our experimental results show that, compared to the random selection strategy, this strategy reduces the amount of required labeled training data by over 50% while achieving the same performance. The e ect is even more signi cant when only named mentions are considered: the system achieves the same performance by using only 42% of the training data required by the random selection strategy.", "creator": "TeX"}}}