{"id": "0911.3298", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Nov-2009", "title": "Understanding the Principles of Recursive Neural networks: A Generative Approach to Tackle Model Complexity", "abstract": "Recursive Neural Networks are non-linear adaptive models that are able to learn deep structured information. However, these models have not yet been broadly accepted. This fact is mainly due to its inherent complexity. In particular, not only for being extremely complex information processing models, but also because of a computational expensive learning phase. The most popular training method for these models is back-propagation through the structure. This algorithm has been revealed not to be the most appropriate for structured processing due to problems of convergence, while more sophisticated training methods enhance the speed of convergence at the expense of increasing significantly the computational cost. In this paper, we firstly perform an analysis of the underlying principles behind these models aimed at understanding their computational power. Secondly, we propose an approximate second order stochastic learning algorithm. The proposed algorithm dynamically adapts the learning rate throughout the training phase of the network without incurring excessively expensive computational effort. The algorithm operates in both on-line and batch modes. Furthermore, the resulting learning scheme is robust against the vanishing gradients problem. The advantages of the proposed algorithm are demonstrated with a real-world application example.", "histories": [["v1", "Tue, 17 Nov 2009 13:17:05 GMT  (362kb)", "http://arxiv.org/abs/0911.3298v1", "11 pages, 4 figures, Recurrent Networks session ICANN 2009"]], "COMMENTS": "11 pages, 4 figures, Recurrent Networks session ICANN 2009", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["alejandro chinea"], "accepted": false, "id": "0911.3298"}, "pdf": {"name": "0911.3298.pdf", "metadata": {"source": "CRF", "title": "Understanding the Principles of Recursive Neural Networks: A Generative Approach to Tackle Model Complexity", "authors": ["Alejandro Chinea"], "emails": [], "sections": [{"heading": null, "text": "Keywords: recursive neural networks, structural patterns, machine learning, generative principles."}, {"heading": "1 Introduction", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2 Implications of the Recursive Neural Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Definitions and Notations", "text": "A graph U is a pair (V, E) where V is the set of nodes and E represents the set of edges. In the face of a graph U and a vertex V, pa [v] is the set of parents of v, while ch [v] represents the set of their children. Specifically, the graph contains a set of domain variables characterized by a vector of real and categorical variables. Moreover, each node encodes a fragment of information that plays an important role in the hand. The presence of a branch (v, w) is characterized by a vector of real and categorical variables."}, {"heading": "2.2 Generative Principles of Intelligence", "text": "In general, the concept of intelligence cannot be understood without the existence of two fundamental principles [14]: the maximization of transfer and the principle of exploitability. Essentially, the first principle states that an intelligent system, whenever possible, builds knowledge by maximizing the transfer of previously acquired knowledge. In particular, more complex structures are the result of the composition of previously learned or memorized structures (e.g., for transferring actions that were used in previous situations to handle new situations).However, this mechanism of transfer must be understood as a design process (the resulting structures are more than a simple combination of previously learned structures).Furthermore, the principle of reproducibility is linked to the concept of memory and consequence. It states that a system that indicates intelligence must be able to recover from its actions. Specifically, an intelligent system must be able to derive from its own present state causes in order to determine what is possible, what is not possible, what is not possible without memories."}, {"heading": "2.3 Recursive Networks as Generative Models", "text": "Within the framework of recursive neural networks, the perceived structure of a problem is captured and expressed using graphical models. In particular, the patterns used for the learning and retrieval phases encode not only the fragments of information (e.g.: information that can be characterized by specific, quantifiable and / or measurable attributes) that play an important role in the machine learning problem, but also the logical relationships between them. The nature of such relationships is determined by the application context and attempts to explicitly model the logical correlations between information fragments. In a temporal range, for example, the fragments of information are events and the simultaneous occurrence of two or more events is interpreted as an existing or possible correlation between them. Therefore, this information encoding process contains more knowledge than when such information fragments are viewed in isolation. It is important to note that the notion of information content is strongly linked to the notion of structure. In fact, the fact reflects more complex structures of their origin."}, {"heading": "3 Reducing Complexity of the Training Phase", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Theoretical Background", "text": "The concept of learning in neural networks is associated with minimizing some error functions E (W) by changing the network parameters W. In the case of recursive etworks, the learning phase consists in finding the appropriate model parameters for implementing the state transition network f and the output network g in relation to the given task and data. Without loss of generality, we assume that we express the parameters of the function f and g in a vector W = [w1, w2, w3,...., wm]. Disruption of the error function by a certain point of the model parameters, which can be written as follows: E (W + 0 W) = E (w1 + 0, w2, w2, wm]. Considering the Taylor expansion of the error function around the error W, we obtain: (3) The training phase consists roughly of updating the model parameters after the presentation of a single training pattern (on-line) or batch mode)."}, {"heading": "3.2 Algorithm Description", "text": "The complete algorithmic description is in Figure 2. By inspecting the pseudo-code, the algorithm proceeds as follows: After initializing the parameters of the algorithm (lines 1 and 2), the algorithm enters into two nested loops: The external or control loop is responsible for monitoring the performance of the algorithm. Furthermore, it also performs the update of the model parameters (line 12) according to the derived learning rule (\u2206 wi = -\u03b7gi / \u03c3 (gi). The constant \u03c6 is summed up to the standard deviation of the error gradients to avoid possible numerical problems. Figure 3 shows the pseudo-code of the function S _ Gradients (S stands for structure), which is responsible for calculating the first derivations of the error function with respect to the proposed error parameters. This function takes as arguments a structural pattern U together with its category Y and returns the error gradient vector. < the details of the used mode can be easily found in the internal radio loop [7]."}, {"heading": "3.3 Preliminary Complexity Analysis", "text": "From a mathematical point of view, the proposed algorithm O (W) scales in relation to the memory requirements, with W = Wf-Wg being the number of parameters of the model. Furthermore, the computational costs scale roughly O (NW), with N being the number of patterns in the dataset. In this line, it should be noted that the Quasi-Newton method, which builds iterative approximations to the inverse Hessian approach in general, results in algorithms with the total computational costs of O (NW2) and storage requirements of O (W2). Similarly, the conjugate reverse method results in storage of O (W) at the same computational costs of a Quasi-Newton method. Furthermore, it is important to note that rigorous complexity analysis would require a careful examination of the statistical distribution of gradient errors throughout the optimization process, followed by an analysis of the recursions associated with the 9-generation functions (8)."}, {"heading": "4 Experimental Results", "text": "The performance of the algorithm has been tested for the problem in exposed [21]. Specifically, this application comes from the Intelligent Transport research field and consists of the development of an advanced intersection system. The ultimate goal is to provide the driver with adequate warnings to avoid fatal collisions. For this task, the structural patterns are trees that encode temporal situations at intersections from a depth of one to sixteen levels. Generally, a situation at intersections consists of a series of dynamic (eg: vehicles, pedestrians, traffic lights, etc.) and static units (eg: trees, bushes, traffic signs, etc.) that interact during a variable timeframe. For this application, the pattern is composed of 4000 structural patterns, about half of which represent high-risk situations (eg: situations that lead to collisions).The dynamic aspects of intersections are encoded within the topology of trees, while static aspects are encoded within algorithms within algorithms."}, {"heading": "5 Conclusions", "text": "In this paper, we have described the principles underlying the recursive neural network model. It has been shown that the information content associated with a particular problem has a specific geometry that these models can attempt to exploit. Furthermore, the fact that structured information representations are used translates into a significant gain in information content. In order to address the inherent complexity of these models, a stochastic learning algorithm has been described; the proposed algorithm is able to achieve a good compromise between the speed of convergence and the computational effort required by setting the local learning rate for each weight inversely proportional to the standard deviation of its stochastic gradient; the scaling properties of the algorithm make it suitable for the computing requirements of the recursive model; and the proposed learning scheme can be easily adapted to other recursive models such as contextual models or graphical neural networks."}], "references": [{"title": "Learning Task-Dependent Distributed Structure-Representations by Backpropagation Through Structure", "author": ["C. Goller", "A. Kuchler"], "venue": "Proceedings of the IEEE International Conference on Neural Networks (ICNN 1996), pp. 347-352, Washington", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "A General Framework for Adaptive Processing of Data Structures", "author": ["P. Frasconi", "M Gori", "A. Sperduti"], "venue": "IEEE Transactions on Neural Networks 9 (5), pp. 768-786", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning Protein Secondary Structure From Sequential and Relational Data", "author": ["A. Ceroni", "P. Frasconi", "G. Pollastri"], "venue": "Neural Networks. 18, 1029-1039", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "The Pricipled Design of Large-Scale Recursive Neural Networks Architectures-DAG-RNNs and the Protein Structure Prediction Problem", "author": ["P. Baldi", "G. Pollastri"], "venue": "Journal of Machine Learning Research. 4, 575-602", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Similarity Learning for Graph-based Image Representations", "author": ["C.D. Mauro", "M. Diligenti", "M. Gori", "M. Maggini"], "venue": "Pattern Recognition Letters. 24(8), 1115-1122", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Towards Incremental Parsing of Natural Language Using Recursive Neural Networks", "author": ["F. Costa", "P. Frasconi", "V. Lombardo", "G. Soda"], "venue": "Applied Intelligence. 19, 9-25", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Recursive Processing of Cyclic Graphs", "author": ["M. Bianchini", "M Gori", "L. Sarti", "F. Scarselli"], "venue": "IEEE Transactions on Neural Networks 9 (17), pp. 10-18", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "A New Model for Learning in Graph Domains", "author": ["M. Gori", "G. Monfardini", "L. Scarselli"], "venue": "Proceedings of the 18 IEEE International Joint Conference on Neural Networks, pp. 729734, Montreal", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Adaptive Contextual Processing of Structured Data by Recursive Neural Networks: A Survey of Computational Properties", "author": ["B. Hammer", "A. Micheli", "A. Sperduti"], "venue": "Hammer, B., Hitzler, P. (eds.), Perspectives of Neural-Symbolic Integration, Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Contextual Processing of Structured Data by Recursive Cascade Correlation", "author": ["A. Micheli", "D. Sona", "A. Sperduti"], "venue": "IEEE Transactions on Neural Networks 15(6)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Universal Aproximation Capabilities of Cascade Correlation for Structures", "author": ["B. Hammer", "A. Micheli", "A. Sperduti"], "venue": "Neural Computation", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Support Vector Machine Learning for Interdependent and Structured Output Spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "Brodley, C. E. (Ed.), ICML\u201904: Twenty-first international conference on Machine Learning. ACM Press, New York", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Editorial of the Special issue on Neural Networks and Kernel Methods for Structured Domains", "author": ["B. Hammer", "C. Saunders", "A. Sperdutti"], "venue": "Neural Networks 18 (8), 1015-1018", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "A Generative Theory of Shape", "author": ["M. Leyton"], "venue": "LNCS, vol 2145, pp. 1-76. Springer-Verlag", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Symmetry, Causality, Mind", "author": ["M. Leyton"], "venue": "MIT Press, Massachusetts", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1992}, {"title": "The Computational Brain, MIT", "author": ["P. Churchland", "T. Sejnowski"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1992}, {"title": "Confabulation Theory: The Mechanism of Thought", "author": ["R. Hecht-Nielsen"], "venue": "In : Proceedings of the 20 IEEE International Joint Conference on Neural Networks,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Recursive neural networks were introduced last decade [1],[2] as promising machine learning models for processing data from structured domains (i.", "startOffset": 54, "endOffset": 57}, {"referenceID": 1, "context": "Recursive neural networks were introduced last decade [1],[2] as promising machine learning models for processing data from structured domains (i.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "So far, these models have found applications mainly in bio-informatics [3], [4] although they have also been applied to image analysis [5] and natural language processing tasks [6] between others.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "So far, these models have found applications mainly in bio-informatics [3], [4] although they have also been applied to image analysis [5] and natural language processing tasks [6] between others.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "So far, these models have found applications mainly in bio-informatics [3], [4] although they have also been applied to image analysis [5] and natural language processing tasks [6] between others.", "startOffset": 135, "endOffset": 138}, {"referenceID": 5, "context": "So far, these models have found applications mainly in bio-informatics [3], [4] although they have also been applied to image analysis [5] and natural language processing tasks [6] between others.", "startOffset": 177, "endOffset": 180}, {"referenceID": 6, "context": "Although some advances have been recently reported regarding not only the recursive processing of cyclic structures [7],[8] but the contextual processing of information (i.", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "Although some advances have been recently reported regarding not only the recursive processing of cyclic structures [7],[8] but the contextual processing of information (i.", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "recursive models breaking the causality hypothesis) [9],[10], followed by some basic proposals on generating structured outputs [11],[12], from a practical point of view, the intrinsic complexity of these models together with a computationally hard learning phase has strongly limited the interest of the research community on this kind of models.", "startOffset": 52, "endOffset": 55}, {"referenceID": 9, "context": "recursive models breaking the causality hypothesis) [9],[10], followed by some basic proposals on generating structured outputs [11],[12], from a practical point of view, the intrinsic complexity of these models together with a computationally hard learning phase has strongly limited the interest of the research community on this kind of models.", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "recursive models breaking the causality hypothesis) [9],[10], followed by some basic proposals on generating structured outputs [11],[12], from a practical point of view, the intrinsic complexity of these models together with a computationally hard learning phase has strongly limited the interest of the research community on this kind of models.", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "recursive models breaking the causality hypothesis) [9],[10], followed by some basic proposals on generating structured outputs [11],[12], from a practical point of view, the intrinsic complexity of these models together with a computationally hard learning phase has strongly limited the interest of the research community on this kind of models.", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "Furthermore, it has been recently pointed out [13] that two important future challenges for these models will rely on the design of efficient learning schemes, and tackling appropriately theoretical problems as learning structural transductions or structure inference as occur in various machine learning areas such as the inference of protein structures or parse trees.", "startOffset": 46, "endOffset": 50}, {"referenceID": 13, "context": "Generally speaking, the notion of intelligence cannot be conceived without the existence of two fundamental principles [14]: The maximization of transfer and the recoverability principle.", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "It is has been recently shown [15] that the human information processing system follows these two generative principles.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Furthermore, such a structure is the result of a hierarchically organized information processing system [16],[17] that generates structure by correlating the information processed at the different levels of its hierarchy.", "startOffset": 104, "endOffset": 108}, {"referenceID": 16, "context": "Furthermore, such a structure is the result of a hierarchically organized information processing system [16],[17] that generates structure by correlating the information processed at the different levels of its hierarchy.", "startOffset": 109, "endOffset": 113}, {"referenceID": 0, "context": "Afterwards, the resulting error values are normalized for the three algorithms in the interval [0,1] (the value that is mapped to 0 is the minimum value reached at epoch 20 by the best performing algorithm).", "startOffset": 95, "endOffset": 100}], "year": 2009, "abstractText": "Recursive Neural Networks are non-linear adaptive models that are able to learn deep structured information. However, these models have not yet been broadly accepted. This fact is mainly due to its inherent complexity. In particular, not only for being extremely complex information processing models, but also because of a computational expensive learning phase. The most popular training method for these models is back-propagation through the structure. This algorithm has been revealed not to be the most appropriate for structured processing due to problems of convergence, while more sophisticated training methods enhance the speed of convergence at the expense of increasing significantly the computational cost. In this paper, we firstly perform an analysis of the underlying principles behind these models aimed at understanding their computational power. Secondly, we propose an approximate second order stochastic learning algorithm. The proposed algorithm dynamically adapts the learning rate throughout the training phase of the network without incurring excessively expensive computational effort. The algorithm operates in both on-line and batch modes. Furthermore, the resulting learning scheme is robust against the vanishing gradients problem. The advantages of the proposed algorithm are demonstrated with a real-world application example.", "creator": "PScript5.dll Version 5.2.2"}}}