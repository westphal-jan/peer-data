{"id": "0911.5703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Nov-2009", "title": "Hierarchies in Dictionary Definition Space", "abstract": "A dictionary defines words in terms of other words. Definitions can tell you the meanings of words you don't know, but only if you know the meanings of the defining words. How many words do you need to know (and which ones) in order to be able to learn all the rest from definitions? We reduced dictionaries to their \"grounding kernels\" (GKs), about 10% of the dictionary, from which all the other words could be defined. The GK words turned out to have psycholinguistic correlates: they were learned at an earlier age and more concrete than the rest of the dictionary. But one can compress still more: the GK turns out to have internal structure, with a strongly connected \"kernel core\" (KC) and a surrounding layer, from which a hierarchy of definitional distances can be derived, all the way out to the periphery of the full dictionary. These definitional distances, too, are correlated with psycholinguistic variables (age of acquisition, concreteness, imageability, oral and written frequency) and hence perhaps with the \"mental lexicon\" in each of our heads.", "histories": [["v1", "Mon, 30 Nov 2009 18:15:35 GMT  (18kb)", "http://arxiv.org/abs/0911.5703v1", "9 pages, 5 figues, 2 tables, 12 references, 23rd Annual Conference on Neural Information Processing Systems (NIPS): Workshop on Analyzing Networks and Learning With Graphsthis http URL"]], "COMMENTS": "9 pages, 5 figues, 2 tables, 12 references, 23rd Annual Conference on Neural Information Processing Systems (NIPS): Workshop on Analyzing Networks and Learning With Graphsthis http URL", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["olivier picard", "alexandre blondin-masse", "stevan harnad", "odile marcotte", "guillaume chicoisne", "yassine gargouri"], "accepted": false, "id": "0911.5703"}, "pdf": {"name": "0911.5703.pdf", "metadata": {"source": "CRF", "title": "Hierarchies in Dictionary Definition Space", "authors": ["Olivier Picard", "Alexandre Blondin Mass\u00e9", "Stevan Harnad", "Odile Marcotte", "Guillaume Chicoisne", "Yassine Gargouri"], "emails": ["picard.olivier.2@courrier.uqam.ca,", "harnad@uqam.ca,", "chicoisne.guillaume@uqam.ca,", "yassinegargouri@hotmail.com", "alexandre.blondin.masse@gmail.com", "Odile.Marcotte@gerad.ca"], "sections": [{"heading": null, "text": "ar Xiv: 091 1,57 03"}, {"heading": "1 Introduction", "text": "A category of things (object, action, feature or condition) that we have questioned is the right thing (eating, fighting, fleeing, etc.) with the right kind of things. All kinds can obtain categories by their definition. We are the only kinds that can be obtained by verbal instructions by naming and defining them. The words in our words are almost all the names of categories followed by their definitions. In principle, all categories can be obtained by verbal definitions, but we cannot obtain them in their entirety."}, {"heading": "2 Definitions and Notations", "text": "This section presents all the necessary definitions of graph theory objects examined in this article, and refers the reader to [9] for complete graph theory and discrete mathematical introductions."}, {"heading": "2.1 Graphs", "text": "A directed graph is a pair of G = (V, E), where V is a finite set of elements designated as vertices and E (G). The density of a directed graph is d (G) = | E (G) | / | V (G) | 2.A Vertex u is a predecessor (or successor) of the vertex v, if (u, v). The density of a directed graph is d (G) = | E (G). The sets of predecessors and successors of u are each denoted by N \u2212 (u) and N + (v). The in-degree and out-degree of u are each defined by. (u) = | N \u2212 (u)."}, {"heading": "2.2 Dictionaries", "text": "Let W be a finite set whose elements are called words, and let 2W denote the collection of all subsets of W. A dictionary is a subset D of W x 2W, so for each (w, dw) D: (i) dw 6 = 4.0 (there is no empty definition) and (ii) w / dw (a word cannot be used to define itself) Elements of D are called entries. Thus, an entry is a pair (w, dw), where w is a word and dw is a set of words. The corresponding diagram of a dictionary D, the set dw and the elements of dw are each denoted as a definition (the defined word), the definition of w, and the definitions (the defining words). There is a very natural way to derive a diagram from a dictionary."}, {"heading": "2.3 Grounding Kernel (GK)", "text": "We say that U V is a grounding set (also called feedback vertex substitution) of G when G [V \u2212 U] is acyclic, i.e. when U covers each cycle of G. The problem of finding grounding sets of minimum size (MGSs) is NP-complete, so it is unlikely that an efficient algorithm will be found to solve all instances of this problem. We hope that we will be able to exploit the particular structure of our graphics to circumvent this difficulty, and will report on our efforts in an upcoming paper. Here, we are more interested in extracting hierarchies of definitive removal of dictionary-like graphs. To this end, we let G = (V, E) be a directed graph, and SINKS (G) the amount of its sinks. We define the operator OUT0 by OUT0 (G) = G [V \u2212 SINKS (G)]."}, {"heading": "2.4 Kernel Core (KC)", "text": "We remember two classic relationships on vertices. In view of two vertices u and v of a diagram G, we write u \u2192 v if there is a uv path in G, and we write u \u2194 v if u \u2192 v and v \u2192 u. Note that \u2194 is reflexive, symmetrical and transitive, so it is an equivalence relationship. Therefore, a natural division of the vertices of G. The equivalence classes of this ratio are called the strongly connected components of G. Let G have a diagram and V1, V2,..., Vk the strongly connected components of G. We construct a diagram G = (V, E) as follows: V \u2032 = {V1, V2,..., Vk} and (Vi, Vj), VE, \"if and only if Vi 6 = Vj and there are the strongly connected components of G, so that (u, v) the vertices of G and Vj exist. We call this diagram the SCAPTIC quograph."}, {"heading": "3 Hierarchies", "text": "In this article we will consider two hierarchies: the first is induced by the GK and the second is obtained by the strongly interconnected components.Let G = (V, E) be a directed graph connected to a dictionary. Let K be GK. The GK level of a vertex v, V relative to K will be defined by LGK (v) = {0 if v, K, max {LGK (u) | u, N \u2212 (v)} + 1 otherwise. (1) We will define the GK hierarchy the categorization of a vertex v, V relative to K. The next hierarchy is based on the strongly interconnected components. Let G be a graph and G its SCC quotients. We will define the hierarchy of the GSCC hierarchy (v) of a G function induced by this level.The next hierarchy is based on the strongly interconnected components (v)."}, {"heading": "4 Natural Language Dictionaries", "text": "We have applied the above hierarchies to the study of two dictionaries: CIDE [8] and LDOCE [7]. As with many problems of natural language processing, we are faced with deviations in word morphosynthesis and polysemia (multiple meanings for the same word form). Morphosyntactic deviations were removed using Porter's algorithm [6]. To reduce polysemy, we applied a common approach: we held only the first definition for each word. Finally, we removed loops and words with empty definitions. The number of vertices, the number of edges, and the density of the two dictionaries, together with those of their GK and their \"core\" (KC, the subgraph induced by its larger, strongly connected component, defined below), are listed in Table 2.How many networks derived from natural models are dictionary graphs that meet almost all the criteria of small-world charts."}, {"heading": "5 Psycholinguistic Correlates of Words in the GK, KC, and Definitional Hierarchies", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "6 Concluding Remarks", "text": "In this case, it is a matter of a pure \"yes,\" of a \"yes,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"no,\" of a \"\" no, \"of a\" \"no,\" of a \"\" no, \"of a\" \"no,\" of a \"no,\" of a \"no,\" of a \"\" no, \"of a\" \"no,\" of a \"\" \"no,\" of a \"\" \"\" no, \"of a\" \"\" \"no,\" of a \"\" \"\" no, \"\" \"\" \"\" \"no,\" \"\" \"\" \"\" \"\" \"no,\" \"\" \"\" \"\" \"\" \"no,\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\""}], "references": [{"title": "How is meaning grounded in dictionary definitions", "author": ["A. Blondin Mass\u00e9", "G. Chicoisne", "Y. Gargouri", "S. Harnad", "O. Marcotte", "O. Picard"], "venue": "In TextGraphs \u201908: Proceedings of the 3rd Textgraphs Workshop on Graph-Based Algorithms for Natural Language Processing, Manchester, United Kingdom,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Grounding abstract word definitions in prior concrete experience", "author": ["G. Chicoisne", "A. Blondin Mass\u00e9", "O. Picard", "S. Harnad"], "venue": "6th Int. Conf. on the Mental Lexicon,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Age of acquisition ratings for 3,000 monosyllabic words, Behavior", "author": ["M.J. Cortese", "M.M. Khanna"], "venue": "Research Methods,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "The symbol grounding problem", "author": ["S. Harnad"], "venue": "Physica D 42:335-346,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "Reducibility among combinatorial problems", "author": ["R.M. Karp"], "venue": "Complexity of Computer Computations,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1972}, {"title": "An algorithm for suffix stripping, Program", "author": ["M.F. Porter"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1980}, {"title": "Dictionary of Contemporary English (LDOCE)", "author": ["P. Procter", "Longman"], "venue": "Longman Group Ltd.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1978}, {"title": "Cambridge International Dictionary of English (CIDE)", "author": ["P. Procter"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Discrete mathematics and its applications", "author": ["K.H. Rosen"], "venue": "6th ed. McGraw-Hill,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "The Bristol norms for age of acquisition, imageability, and familiarity, Behavior", "author": ["H. Stadthagen-Gonzalez", "C.J. Davis"], "venue": "Research Methods,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "The large-scale structure of semantic networks: statistical analyses and a model of semantic growth", "author": ["M. Steyvers", "J.B. Tenenbaum"], "venue": "Cognitive Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "MRC Psycholinguistic Database: Machine Usable Dictionary, version 2.00", "author": ["M. Wilson", "O.O. Qx", "P. Quinlan"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1987}], "referenceMentions": [{"referenceID": 3, "context": "This is the \u201csymbol grounding problem\u201d [4] and presumably that other means of acquiring categories is sensorimotor induction.", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "By eliminating all the words that can be reached from other words through definition alone, we have been able to reduce the dictionary to its \u201cgrounding kernel\u201d (GK) \u2013 a set of words (about 10%) \u2013 out of which all the rest of the words can be reached through definition alone [1].", "startOffset": 276, "endOffset": 279}, {"referenceID": 1, "context": "The GK has some striking properties: The words in it are learned at a significantly younger age than the rest of the dictionary and are also more concrete [2], but if the variance correlated with age is removed, the residual GK words are more abstract than the rest of the dictionary.", "startOffset": 155, "endOffset": 158}, {"referenceID": 7, "context": "We hope to be able to compute MGSs for our special cases, but meanwhile the GKs of our dictionaries \u2013 Cambridge International Dictionary of English (CIDE) [8] and Longman Dictionary of Contemporary English (LDOCE) [7] \u2013 already turn out to have more differentiated internal substructure which we begin analyzing further in this article.", "startOffset": 155, "endOffset": 158}, {"referenceID": 6, "context": "We hope to be able to compute MGSs for our special cases, but meanwhile the GKs of our dictionaries \u2013 Cambridge International Dictionary of English (CIDE) [8] and Longman Dictionary of Contemporary English (LDOCE) [7] \u2013 already turn out to have more differentiated internal substructure which we begin analyzing further in this article.", "startOffset": 214, "endOffset": 217}, {"referenceID": 8, "context": "The reader is referred to [9] for complete graph theory and discrete mathematics introductions.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "We have applied the above hierarchies to the study of two dictionaries: CIDE [8] and LDOCE [7].", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "We have applied the above hierarchies to the study of two dictionaries: CIDE [8] and LDOCE [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "Morphosyntactic variation was removed using Porter\u2019s algorithm [6].", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "Like many networks derived from natural models, dictionary graphs satisfy almost all criteria of small-world graphs [11].", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "We analyzed how the structure of dictionary definition space \u2013 in terms of GK, the KC, and the two hierarchies of definitional distance that they induce \u2013 is related to five psycholinguistic variables: AOA: Age of Acquisition - the age at which a word is learned C: Concreteness - degree of concreteness/abstractness of word\u2019s referent I: Imageability - how readily one could generate a mental (visual) image of word\u2019s referent BF: Brown Frequency - spoken frequency of word TLF: Thorndike-Lodge Frequency - written frequency of word The psycholinguistic variables (AOA, C, and I) came from the MRC psycholinguistic database [12].", "startOffset": 625, "endOffset": 629}, {"referenceID": 2, "context": "[3] [10]", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[3] [10]", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "These results are similar to those reported by [11].", "startOffset": 47, "endOffset": 51}, {"referenceID": 1, "context": "These results cast further light on the prior finding of [2] that GK words are learned earlier and more concrete, but that when AOA\u2019s covariance with C is partialled out, C\u2019s polarity shifts: GK words that are not learned earlier are significantly more abstract than the rest of the dictionary.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "This seemed to contradict our previous findings [2] comparing the GK with the rest of the dictionary, so we reanalyzed this hierarchy excluding its bottom level, the GK.", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "The factors underlying the polarity change in concreteness observed in previous work [2] \u2013 with the GK being more concrete and learned earlier than the rest of the dictionary, but more abstract when the covariance with age is partialled out \u2013 has now been further refined: It turns out that the GK consists of a large, strongly connected KC plus a smaller, less interconnected outer layer.", "startOffset": 85, "endOffset": 88}, {"referenceID": 10, "context": "The effects of SCC-induction for the entire dictionary were also similar to those of [11]: In their small-world analyses too, the KC was acquired earlier then the rest of the corpus and more frequently used (see Summary Figure 5).", "startOffset": 85, "endOffset": 89}], "year": 2009, "abstractText": "A dictionary defines words in terms of other words. Definitions can tell you the meanings of words you don\u2019t know, but only if you know the meanings of the defining words. How many words do you need to know (and which ones) in order to be able to learn all the rest from definitions? We reduced dictionaries to their \u201cgrounding kernels\u201d (GKs), about 10% of the dictionary, from which all the other words could be defined. The GK words turned out to have psycholinguistic correlates: they were learned at an earlier age and more concrete than the rest of the dictionary. But one can compress still more: the GK turns out to have internal structure, with a strongly connected \u201ckernel core\u201d (KC) and a surrounding layer, from which a hierarchy of definitional distances can be derived, all the way out to the periphery of the full dictionary. These definitional distances, too, are correlated with psycholinguistic variables (age of acquisition, concreteness, imageability, oral and written frequency) and hence perhaps with the \u201cmental lexicon\u201d in each of our heads.", "creator": "LaTeX with hyperref package"}}}