{"id": "0912.0086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2009", "title": "Learning Mixtures of Gaussians using the k-means Algorithm", "abstract": "One of the most popular algorithms for clustering in Euclidean space is the $k$-means algorithm; $k$-means is difficult to analyze mathematically, and few theoretical guarantees are known about it, particularly when the data is {\\em well-clustered}. In this paper, we attempt to fill this gap in the literature by analyzing the behavior of $k$-means on well-clustered data. In particular, we study the case when each cluster is distributed as a different Gaussian -- or, in other words, when the input comes from a mixture of Gaussians.", "histories": [["v1", "Tue, 1 Dec 2009 19:10:46 GMT  (30kb)", "http://arxiv.org/abs/0912.0086v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kamalika chaudhuri", "sanjoy dasgupta", "rea vattani"], "accepted": false, "id": "0912.0086"}, "pdf": {"name": "0912.0086.pdf", "metadata": {"source": "CRF", "title": "Learning Mixtures of Gaussians Using the k-Means Algorithm", "authors": ["Kamalika Chaudhuri"], "emails": ["kamalika@soe.ucsd.edu", "dasgupta@cs.ucsd.edu", "avattani@cs.ucsd.edu"], "sections": [{"heading": null, "text": "ar Xiv: 091 2.00 86v1 [cs.LG] We analyze three aspects of the k-mean algorithm under this assumption. First, we show that if the input comes from a mixture of two spherical Gaussians, a variant of the 2-mean algorithm succeeds in isolating the subspace containing the means of the mixture components. Second, we show an exact expression for the convergence of our variant of the 2-mean algorithm if the input is a very large number of samples from a mixture of spherical Gaussians. Our analysis does not require a lower limit in terms of separation between the mixture components. Finally, we investigate the sample requirement of the k-mean; for a mixture of 2 spherical Gaussians, we show an upper limit in terms of the number of samples required by a variant of 2 means to come close to the true solution. The sample requirement grows with increasing dimensions of the data and decreasing separations between the means of the number of the means, whereby the number of the upper means is reached by the variant of the upper means."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "2 The Setting", "text": "The k-mean algorithm iteratively refines a partitioning of the input data. At each iteration, k-points are maintained as centers; each input is mapped to its next center. The center of each cluster is then recalculated as an empirical mean of the points mapped to the cluster. This procedure continues until convergence. Our variant of the k-mean is described below. There are two main differences between the actual 2-mean algorithm and our variant. First, we use a separate set of samples in each iteration. Second, we always fix the cluster boundary to be a hyperplane through the origin. If the input is a very large number of samples from a mixture of two identical Gaussians with equal mixing weights, and with the center of the mass at origin, this is exactly 2-mean values initialized with symmetric centers (in relation to the origin)."}, {"heading": "3 Exact Estimation", "text": "In this section we examine the performance of algorithm 2-means-iterate, if you can accurately estimate the vectors - that is, if a very large number of samples from the mix are available. Our main result of this section is Lemma 1, which exactly characterizes the behavior of 2-meansiterate-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-0-"}, {"heading": "4 Finite Samples", "text": "In this section, we analyze algorithm 2-mean iteration when we need to estimate the statistics in each round with a finite number of samples. (We characterize the number of samples required to ensure that 2-mean iteration progresses in each round, and we also characterize the speed of progress when the required number of samples is available. (The main result of this section is the following problem that characterizes the number of samples in round 1, with the angle between \u00b51 and the hyper-plane separator specified in 2-mean iterate.) Notice that it is now a random variable depending on the samples drawn in round 1., t \u2212 1, and given number is a random variable whose value depends on samples in round t. Also, we use Lemos + 1 as the center of the partition Ct in the iteration, and E [ut + 1] is the expected center."}, {"heading": "5 Lower Bounds", "text": "11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 5, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,"}, {"heading": "6.1 Proof of Lemma 1", "text": "In this section, we prove that Lemma 1, which ultimately leads to a proof of Lemma 1. Lemma 21. In this section, we need additional notation. We define, for j = 1, 2: wjt + 1 = Pr [x]. < < p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p) p (x) p (x) p) p \"p (p) p (p) p (p) p (p) p) p (x) p x p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p) p (x) p (x) p (x) p (x) p (x) p (x) p (x) p) p (x) p (x) p (x) p) p (x) p (x) p (p) p) p (x) p (p) p (x) p (x) p (p) p (x) p (x) p (p) p (x) p (x) p (p) p (p) p (p) p (p) p (x) p (p) p (p) p) p (p (p) p (p) p) p (x) p (p) p (p) p (p) p (p)."}, {"heading": "6.2 Proofs of Sample Requirement Bounds", "text": "For the rest of the section, we prove that lemmas 8 and 9, which lead to a proof of lemmas 7, depend on a random value. First, we need some notation. At the time t, we use the notation St + 1 to denote the quantity E [X-1X-Ct + 1], where 1X-Ct + 1 is the indicator function for the event X-Ct + 1, and the expectation is taken over the entire mixture. Subsequently, we also use the notation S-t + 1 to denote the empirical value of St + 1. Our goal is to tie the concentration of certain functions of S-1 + 1 around their expected values if we are given only n samples from the mixture. Recall what we define as the angle between \u00b51 and the hyperplane delimiter in 2-mean iterates is given."}, {"heading": "6.3 Proofs of Lower Bounds", "text": "The proof. (Of Lemma 17) Let P be the plane that encompasses the origin O and the vectors. (Of Lemma 17) Let P be the plane that encompasses the origin O and the vectors. (Of Lemma 17) Let P be the plane that encompasses the origin O and the vectors 1 and 2. (Of Lemma 17) Let P be the projection of D1 along v is a Gaussian N (0, 1) that is distributed along P independently of the projection of D1 (and the same is the case for D2). To calculate the KL divergence of D1 and D2, it is sufficient to calculate the KL divergence of the projections of D1 and D2 along the plane P. (Let x x x x be a vector in P.) Let x x x be a vector in P. (D1, D2) = 1 and D2 along the KL divergence of D2, it is sufficient to calculate the DL divergence of D2, and DP is sufficient along the plane D1."}, {"heading": "6.4.1 The Setting", "text": "We assume that our input is generated by a mixture of k-spherical Gaussians with means \u00b5j, variances (\u03c3j) 2, j = 1,..., k and mixing weights \u03c11,..., \u03c1k. The mixture is centered at the origin in such that \u2211 \u03c1j\u00b5j = 0. We use M to denote the subspace in which the means \u00b51,..., \u00b5k are contained. We use algorithm 2-means-iterate on this input, and our goal is to show that it still converges to a vector in M. Subsequently, in the face of a vector x and a subspace W, we define the angle between x and W as the angle between x and the projection of x on W. As in sections 2 and 3, we examine the angle oscillating between ut and M, and our goal is to show that the cosine of this angle grows when t increases. Our main result of this section is Lem20, the analogy of G2 we can prove the analogy of the behavior of Galogy 1 in S2."}, {"heading": "6.4.2 Notation", "text": "Remember that in due course we will use t to divide the input data, and the projection of u-t along M is by definition cos (\u03b8t).Let b 1 t be a unit vector located in subspace M. So: u-t = cos (\u03b8t) b 1 t + sin (\u03b8t) vtwhere vt is in the orthogonal complement of M and has the norm 1. We define a second vector u-t as follows: u-t = sin (\u03b8t) b 1 t-cos (\u03b8t) vtWe note that < u-t, u-\u0441\u0442\u0430\u0441\u0442\u0438\u0441\u0442\u0438\u0441\u0442\u0438\u0441\u0442\u0438\u0441\u0438\u0441\u0438\u0441\u0442\u0438\u0442\u0438\u0442\u0438\u0442\u0438\u0442\u0438stet, and the projection of u-t on M is sin (\u0442t) b1t \u2212 cos (\u0442t) vtWe are now expanding the set {b1t} to complete an onthorthorthorthorthic base."}, {"heading": "6.4.3 Proof of Lemma 20", "text": "The main idea behind the proof of Lemma 20 is to estimate the norm and the projection of ut + 1. (We do this in three steps.) First, we estimate the projection of ut + 1 along u-t; second, we estimate this projection on u-t, and finally we estimate its projection along b 2 t,., b l t. Now we make the following projections, and observe that the projection of ut + 1 in each direction is perpendicular to these 0, we can define the Lemma.As before, we define Zt + 1 = Pr [x] Ct + 1] Now we make the following claims.Lemma 27. For each t and each other j, Pr [x] x [x] Ct + 1] = any other type of Zt + 1 (\u2212 1) j."}], "references": [{"title": "Learning mixtures of separated nonspherical Gaussians", "author": ["S. Arora", "R. Kannan"], "venue": "Ann. Applied Prob.,", "citeRegEx": "Arora and Kannan.,? \\Q2005\\E", "shortCiteRegEx": "Arora and Kannan.", "year": 2005}, {"title": "On spectral learning of mixtures of distributions", "author": ["D. Achlioptas", "F. McSherry"], "venue": "In COLT,", "citeRegEx": "Achlioptas and McSherry.,? \\Q2005\\E", "shortCiteRegEx": "Achlioptas and McSherry.", "year": 2005}, {"title": "k-means has polynomial smoothed complexity", "author": ["D. Arthur", "B. Manthey", "H. R\u00f6glin"], "venue": "In FOCS,", "citeRegEx": "Arthur et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2009}, {"title": "How slow is the k-means method", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SoCG,", "citeRegEx": "Arthur and Vassilvitskii.,? \\Q2006\\E", "shortCiteRegEx": "Arthur and Vassilvitskii.", "year": 2006}, {"title": "Separating populations with wide data: A spectral analysis", "author": ["A. Blum", "A. Coja-Oghlan", "A.M. Frieze", "S. Zhou"], "venue": "In ISAAC,", "citeRegEx": "Blum et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blum et al\\.", "year": 2007}, {"title": "Isotropic PCA and affine-invariant clustering", "author": ["S.C. Brubaker", "S. Vempala"], "venue": "In FOCS,", "citeRegEx": "Brubaker and Vempala.,? \\Q2008\\E", "shortCiteRegEx": "Brubaker and Vempala.", "year": 2008}, {"title": "Learning Mixtures of Distributions", "author": ["K. Chaudhuri"], "venue": "PhD thesis,", "citeRegEx": "Chaudhuri.,? \\Q2007\\E", "shortCiteRegEx": "Chaudhuri.", "year": 2007}, {"title": "A rigorous analysis of population stratification with limited data", "author": ["K. Chaudhuri", "E. Halperin", "S. Rao", "S. Zhou"], "venue": "In SODA,", "citeRegEx": "Chaudhuri et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chaudhuri et al\\.", "year": 2007}, {"title": "Learning mixtures of distributions using correlations and independence", "author": ["K. Chaudhuri", "S. Rao"], "venue": "In COLT,", "citeRegEx": "Chaudhuri and Rao.,? \\Q2008\\E", "shortCiteRegEx": "Chaudhuri and Rao.", "year": 2008}, {"title": "Learning mixtures of gaussians", "author": ["S. Dasgupta"], "venue": "In FOCS,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "Maximum likelihood from incomplete data via the em algorithm (with discussion)", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "A two-round variant of EM for Gaussian mixtures", "author": ["S. Dasgupta", "L. Schulman"], "venue": "In UAI,", "citeRegEx": "Dasgupta and Schulman.,? \\Q2000\\E", "shortCiteRegEx": "Dasgupta and Schulman.", "year": 2000}, {"title": "Cluster analysis of multivariate data: Efficiency vs. interpretability of classification", "author": ["E. Forgey"], "venue": null, "citeRegEx": "Forgey.,? \\Q1965\\E", "shortCiteRegEx": "Forgey.", "year": 1965}, {"title": "The spectral method for general mixture models", "author": ["R. Kannan", "H. Salmasian", "S. Vempala"], "venue": "In COLT,", "citeRegEx": "Kannan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2005}, {"title": "Mixture Models:Theory", "author": ["B.G. Lindsey"], "venue": "Geometry and Applications. IMS,", "citeRegEx": "Lindsey.,? \\Q1996\\E", "shortCiteRegEx": "Lindsey.", "year": 1996}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Trans. Information Theory,", "citeRegEx": "Lloyd.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd.", "year": 1982}, {"title": "Some methods for classification and analysis of multivariate observations", "author": ["J.B. MacQueen"], "venue": "In Berkeley Symposium on Mathematical Statistics and Probability,", "citeRegEx": "MacQueen.,? \\Q1967\\E", "shortCiteRegEx": "MacQueen.", "year": 1967}, {"title": "Improved smoothed analysis of the k-means method", "author": ["B. Manthey", "H. R\u00f6glin"], "venue": "In SODA,", "citeRegEx": "Manthey and R\u00f6glin.,? \\Q2009\\E", "shortCiteRegEx": "Manthey and R\u00f6glin.", "year": 2009}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "In FOCS,", "citeRegEx": "Ostrovsky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ostrovsky et al\\.", "year": 2006}, {"title": "Strong consistency of k-means clustering", "author": ["D. Pollard"], "venue": "Annals of Statistics,", "citeRegEx": "Pollard.,? \\Q1981\\E", "shortCiteRegEx": "Pollard.", "year": 1981}, {"title": "Mixture densities, maximum likelihood and the em algorithm", "author": ["R. Redner", "H. Walker"], "venue": "SIAM Review,", "citeRegEx": "Redner and Walker.,? \\Q1984\\E", "shortCiteRegEx": "Redner and Walker.", "year": 1984}, {"title": "An investigation of computational and informational limits in gaussian mixture clustering", "author": ["N. Srebro", "G. Shakhnarovich", "S.T. Roweis"], "venue": "In ICML,", "citeRegEx": "Srebro et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2006}, {"title": "k-means takes exponentially many iterations even in the plane", "author": ["A. Vattani"], "venue": "SoCG,", "citeRegEx": "Vattani.,? \\Q2009\\E", "shortCiteRegEx": "Vattani.", "year": 2009}, {"title": "A spectral algorithm for learning mixtures of distributions", "author": ["V. Vempala", "G. Wang"], "venue": "In FOCS,", "citeRegEx": "Vempala and Wang.,? \\Q2002\\E", "shortCiteRegEx": "Vempala and Wang.", "year": 2002}, {"title": "On convergence properties of the em algorithm for gaussian mixtures", "author": ["L. Xu", "M.I. Jordan"], "venue": "Neural Computation,", "citeRegEx": "Xu and Jordan.,? \\Q1996\\E", "shortCiteRegEx": "Xu and Jordan.", "year": 1996}, {"title": "Assaoud, fano and le cam", "author": ["B. Yu"], "venue": null, "citeRegEx": "Yu.,? \\Q1997\\E", "shortCiteRegEx": "Yu.", "year": 1997}], "referenceMentions": [], "year": 2009, "abstractText": "One of the most popular algorithms for clustering in Euclidean space is the k-means algorithm; k-means is difficult to analyze mathematically, and few theoretical guarantees are known about it, particularly when the data is well-clustered. In this paper, we attempt to fill this gap in the literature by analyzing the behavior of k-means on well-clustered data. In particular, we study the case when each cluster is distributed as a different Gaussian \u2013 or, in other words, when the input comes from a mixture of Gaussians. We analyze three aspects of the k-means algorithm under this assumption. First, we show that when the input comes from a mixture of two spherical Gaussians, a variant of the 2-means algorithm successfully isolates the subspace containing the means of the mixture components. Second, we show an exact expression for the convergence of our variant of the 2-means algorithm, when the input is a very large number of samples from a mixture of spherical Gaussians. Our analysis does not require any lower bound on the separation between the mixture components. Finally, we study the sample requirement of k-means; for a mixture of 2 spherical Gaussians, we show an upper bound on the number of samples required by a variant of 2-means to get close to the true solution. The sample requirement grows with increasing dimensionality of the data, and decreasing separation between the means of the Gaussians. To match our upper bound, we show an information-theoretic lower bound on any algorithm that learns mixtures of two spherical Gaussians; our lower bound indicates that in the case when the overlap between the probability masses of the two distributions is small, the sample requirement of k-means is near-optimal.", "creator": "LaTeX with hyperref package"}}}