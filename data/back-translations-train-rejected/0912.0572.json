{"id": "0912.0572", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2009", "title": "Isometric Multi-Manifolds Learning", "abstract": "Isometric feature mapping (Isomap) is a promising manifold learning method. However, Isomap fails to work on data which distribute on clusters in a single manifold or manifolds. Many works have been done on extending Isomap to multi-manifolds learning. In this paper, we first proposed a new multi-manifolds learning algorithm (M-Isomap) with help of a general procedure. The new algorithm preserves intra-manifold geodesics and multiple inter-manifolds edges precisely. Compared with previous methods, this algorithm can isometrically learn data distributed on several manifolds. Secondly, the original multi-cluster manifold learning algorithm first proposed in \\cite{DCIsomap} and called D-C Isomap has been revised so that the revised D-C Isomap can learn multi-manifolds data. Finally, the features and effectiveness of the proposed multi-manifolds learning algorithms are demonstrated and compared through experiments.", "histories": [["v1", "Thu, 3 Dec 2009 03:05:59 GMT  (1791kb,S)", "http://arxiv.org/abs/0912.0572v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["mingyu fan", "hong qiao", "bo zhang"], "accepted": false, "id": "0912.0572"}, "pdf": {"name": "0912.0572.pdf", "metadata": {"source": "CRF", "title": "Isometric Multi-Manifolds Learning", "authors": [], "emails": [], "sections": [{"heading": null, "text": "This year it has come to the point where it will be able to do the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things."}, {"heading": "A. Multi-manifolds learning by new neighborhood graph construction method", "text": "Wu and Chan [23] proposed a split-augment approach to construct a neighborhood graph. Their method can be considered a variation of the k-NN method, and can be summarized as follows: 1. k-NN method is applied to the dataset. Each data point is connected to its neighbors. If the data lies on multiple multiplicities, several separate graph components (data multiplicities) are formed. 2. Each pair of graph components is connected by its closest pair of intercomponent points. This method is easy to implement and has the same computational complexity as the k-NN method. However, since there is only one edge linking all two graph components, geodesics are poorly sewn via components; meanwhile, their low-dimensional embedding can be arbitrarily twisted."}, {"heading": "B. Multi-manifolds learning by decomposition-composition Isomap", "text": "In fact, it is a matter of a way in which people are able, in which they are able, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to unfold, to continue, to reform, to continue, to reform, to continue, to reform, to reform, to reform, to reform, to continue, to continue, to reform."}, {"heading": "C. Constrained Maximum Variance Mapping", "text": "There is also a newly proposed algorithm called Confined Maximum Variance Mapping (CMVM) [25] for multifaceted learning. The CMVM method is proposed to maximize the differences between classes while maintaining class-internal similarity."}, {"heading": "A. The general procedure for isometric multi-manifolds learning", "text": "Many previous methods extend Isomap for multifaceted learning processes by revising the neighboring graph construction step of the Isomap method (23), [26] - [29]. But shortest distances via clusters or data diversity are poor approximations of geodesy. In Isomap, a poor local approximation always leads to the deformation of global low-dimensional embedding. Assuming that it is an open, convex, and compact set of data in Rd, f: [RD < D] is a continuous mapping. f) = M is defined as a d-dimensional parameterized multiplicity. K (x, y) is a specially defined kernel, and a reproducing kernel component Hilbert space (RKHS) H is constructed with this kernel."}, {"heading": "B. A new algorithm for isometric multi-manifolds learning", "text": "Based on the proposed method, we have designed a new multifaceted learning algorithm. As an extension of the Isomap method for multifaceted data, the method is called Multifaceted Isomap (M-Isomap). X is also believed to be interchangeable to represent the matrix [x1, x2, \u00b7, xN], where {xi, i = 1, \u00b7, N} are column vectors in RD.1 The k-CC method is used to construct a neighborhood graph and identify manifolds: Table II shows the time complexity of kNN, K-Min-ST, k-VC methods on neighborhood graph construction. As shown in the table, the k-NN method has the lowest computational complexity O (kN2). For incremental learning, the computational complexity of kNN, k-Ms, and k-STk-graph methods on neighborhood graph construction is."}, {"heading": "3: end for", "text": "It is not in such a way as if it were about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about which it is about a way in which it is about a way in which it is about which it is about a way in which it is about a way in which it is about a way in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is not in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which"}, {"heading": "C. Computational complexity of M-Isomap method", "text": "In the case of the M-Isomap method, the k-CC method O (k + 1) N2) takes time to construct a fully linked graph and to identify the multiplicities. Calculation of the shortest path on each data multiplicity requires O (\u2211 M = 1 S 2 m log S m) time and performing the classical MDS on the distance matrices of data multiplicities requires O (\u2211 M m = 1 S 2 m log S m) time. Calculation of the shortest path on data multiplicities requires O (\u2211 M < n kS mS n) and the determination of f x i, f x j i is O (\u2211 M m < n S mS n). Carrying out the classical MDS on skeleton I requires O (((\u2211 M m = 1 lm) 3) computational time. The time complexity of the smallest fragmentation solution and the MR-QR process is O."}, {"heading": "D. The revised D-C Isomap method", "text": "So it makes more sense for Om to refer to the interinstitutional rather than the interinstitutional and interinstitutional. & # 8222; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & & & # 10; & & # 10; & # 10; & # 10; & & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; & # 10; &"}, {"heading": "B. Real world data sets", "text": "This year it is more than ever before."}, {"heading": "C. Discussion", "text": "In our experiments, there are several important properties that should be taken into account: 1) As k-CC Isomap tries to preserve poor and good approximations of geodesics simultanees, its low dimensional embedding is usually deformed.This method works well when each data diversity has a comparable number of data points and the data diversity cannot be very far apart, and the algorithm does not work well otherwise.12In summary, Table III shows the comparison of the overall performance of the five versions of the isomap algorithms: classical isomap, k-CC isomap, original D-C isomap, revised D-C isomap and M-isomap."}, {"heading": "A. Multi-manifolds learning by new neighborhood graph construction method", "text": "Wu and Chan [23] have proposed a split-augment approach to construct a neighborhood graph. Their method can be considered a variation of the k-NN method and can be summarized as follows: 1. The k-NN method is applied to the dataset; each data point is connected to its neighbors; if the data is based on multiple manifolds, several separate graph components are formed; 2. Each pair of graph components is connected by its closest pair of intercomponent points; this method is easy to implement and has the same computational complexity as the k-NN method; however, there is only one edge that connects all two graph components."}, {"heading": "B. Multi-manifolds learning by decomposition-composition Isomap", "text": "USA, USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA, the USA"}, {"heading": "C. Constrained Maximum Variance Mapping", "text": "Recently, Li et al. in [25] proposed the algorithm Confined Maximal Variance Mapping (CMVM) for multifaceted learning, which is proposed based on the idea of maximizing inequality between classes while maintaining class-internal similarity. IV. IsometricMulti-Manifolds LearningIn this section, we first present a general approach to designing isometric multifaceted learning algorithms, and then present our new multifaceted learning algorithm called MIsomap. Finally, we are revising the original D-C isomap algorithm to expand its scope."}, {"heading": "A. The general procedure for isometric multi-manifolds learning", "text": "Many previous methods extend Isomap to multifaceted learning processes by using the neighbouring graph construction steps of the Isomap algorithms (23), [26] - [29], (32], (34], (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), (34), 44, 44, 44, 44, 44, 44, 44, 44 (44, 44, 44 (44, 44, 44, 44, 44 (44, 44, 44 (44), 44 (44, 44), 44 (44), 44 (44), 44, 44 (44), 44 (44, 44), 44, 44 (44), 44 (44), 44), 44 ("}, {"heading": "B. A new algorithm for isometric multi-manifolds learning", "text": "Based on the proposed method, we are designing a new Multiple Learning Algorithm (KN). As an extension of the classical isomap method to multiform data, the method is referred to as Multiform Isomap or M-Isomap. X is also believed to be interchangeable to represent the matrix [x1, x2, \u00b7, xN] where xi, i-EC and k-VC methods are based on the neighborhood graph. 1) Using the k-CC method to construct a neighborhood and identify multiplicities: Table II shows the temporal complexity of k-NN, K-Min-ST and k-VC methods on the neighborhood graph. As shown in the table, the k-NN method has the lowest computational complexity O (kN2).For incremental learning, the computational complexity of k-NN, k-Ms and k-VC-we are (kO) methods."}, {"heading": "C. Computational complexity of the M-Isomap algorithm", "text": "In the M-Isomap method, the k-CC algorithm O (k + 1) N2) takes time to construct a completely connected graph and identify the manifolds. O (\u2211 M = 1 S 2 m ln S m) takes time to calculate the shortest path through data manifolds and O (\u2211 M = 1 S 3 m) time to perform the classical MDS on the distance matrices of data manifolds. The time complexity of the calculation of the shortest path through data manifolds is O ((\u2211 M m < n kS mS n) and that of findingf xij, f x j i is O (\u2211 M m < n S mS n). Performing the classical MDS on the skeleton I requires O ((((((\u2211 Mm = 1 lm) 3) xity of the search for the smallest square solution) (findingf xij, f x j i is O (\u2211 M m < n S mS n).)) ((((((((((((((((((classical MDS)) mS mS n) n))))))) on the skeleton I (((((((((((((((\u2211 Mm m m m < n kS mS mS n) n) n) n))))), xxity of findingf xij, f, f x j (i is n M m < n S mS mS mS mS mn))))))))). ((((((((findingf xij xij xij) is the time complexity of findingf, f xij) is O (findingf xij))), f (findingf (findingf xij, f is M mS mS mS mS mS mS mS mS mS mS mS mS mS mS mS mS mS n).). The execution of the classical MDS on the skeleton I (((((((((((((((((((((((("}, {"heading": "D. The revised D-C Isomap method", "text": "It is assumed that these points form a triangle in the low dimensions of space. (a) It shows the case where it is the same level in the same plane. (a) It is assumed that these points form a triangle in the low dimensions of space. (a) It is assumed that these points do not lie in the same plane. (b) It is assumed that these points form a triangle in the low dimensions of space. (a) It is assumed that these points form a triangle in the low dimensions. (a) It is assumed that these points lie in the low dimensions of space. (a) It is assumed that these points do not exist in the low dimensions of space. (a) It is assumed that these points lie in the low dimensions of space. (a)"}, {"heading": "B. Real world data sets", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "C. Discussion", "text": "In our experiments, there are several important features that should be taken into account: 1) Since the k-CC isomap tries to preserve poor and good approximations of geodesy at the same time, its low-dimensional embedding is usually deformed; this method works well when each data diversity has a comparable number of data points and the data diversity cannot be very far from each other; the algorithm does not work very differently from the general performance of the five versions of the isomap algorithms: classical isomap, k-CC isomap overcomes some limitations of the original D-C isomap; meanwhile, the robustness of the 12To is summarized; Table III shows the comparison of the overall performance of the five versions of the isomap algorithms: classical isomap, k-CC isomap, revised D-C isomap and M-isomap."}], "references": [{"title": "Principal Component Analysis", "author": ["I.T. Jolliffe"], "venue": "Springer-Varlag, New York, 1989.  13", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Multidimensional Scaling", "author": ["T.F. Cox", "M.A. Cox"], "venue": "Chapman & Hall, London, 2001.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Machine learning for science: State of the art and future prospects", "author": ["E. Mjolsness", "D. De Coste"], "venue": "Science, vol. 293, pp. 2051-2055, Sep. 2001.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Sliva", "J.C. Landford"], "venue": "Science, vol. 290, pp. 2319-2323, Dec. 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Nonlinear dimensionality reduction by local linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science, vol. 290, pp. 2323-2326, Dec. 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "The manifold ways of perception", "author": ["H.S. Seung", "D.D. Lee"], "venue": "Science, vol. 290, pp. 2268-2269, Dec. 2000.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation, vol. 15, no 6, pp. 1373-1396, June 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Hessian eigenmaps: new locally linear embedding techniques for high-dimensional data", "author": ["D. Donoho", "C. Grimes"], "venue": "Proc. Nat. Acad. Sci. USA, vol. 100, no. 10, pp. 5591-5596, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Principal manifolds and nonlinear dimensionality reduction via tangent space alignment", "author": ["Z. Zhang", "H. Zha"], "venue": "SIAM J. Sci. Comput., vol. 26, pp. 313-338, 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps", "author": ["R.R. Coifman", "S. Lafon", "A.B. Lee", "M. Maggoni", "B. Nadler", "F. Warner", "S.W. Zuck"], "venue": "Proc. Nat. Acad. Sci. USA, vol. 102, no. 21, pp. 7426-7431, May. 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Riemannian Manifold Learning", "author": ["T. Lin", "H. Zha"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 30, no. 5, pp. 796-809, May. 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Continuum Isomap for manifold learning", "author": ["H. Zha", "Z. Zhang"], "venue": "Computational Statistics & Data Analysis, vol. 52, issue. 1, pp. 184-200, Sep. 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Incremental nonlinear dimensionality reduction by manifold learning", "author": ["M.H.C. Law", "A.K. Jain"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 28, no. 3, pp. 377-391, March. 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Graph approximations to geodesics on embedded manifolds", "author": ["M. Bernstein", "V. de Silva", "J.C. Langford", "J.B. Tenenbaum"], "venue": "Technical report, Dept. of Psychology, Stanford Univ., Dec. 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"], "venue": "Pattern Recognition, vol. 41, pp. 176-190, May. 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Extended Isomap for pattern classification", "author": ["M.-H. Yang"], "venue": "ICPR 2002: Proc. Int. Conf. Pattern Recognition, vol. 3, Aug. 2002.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Face recognition using Laplacianfaces", "author": ["X. He", "S. Yan", "T. Hu", "P. Niyogi", "H. Zhang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 27, no. 3, pp. 328-340, March. 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Manifold regularization: a geometric framework for learning from examples", "author": ["M. Belkin", "V. Sindhwani", "P. Niyogi"], "venue": "J. Machine Learning Research, vol. 7, pp. 2399-2434, Dec. 2006.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Supervised nonlinear dimensionality reduction for visualization and classification", "author": ["X. Geng", "D. Zhang", "Z. Zhou"], "venue": "IEEE Trans. Systems, Man and Cybern. B, vol. 35, no. 6, pp. 1098-1107, Dec 2005.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "A Spatio-temporal extension to Isomap nonlinear dimension reduction", "author": ["O.C. Jenkins", "M.J. Mataric"], "venue": "ACM. Proc. 21st Int. Conf. on Machine learning, vol. 69, pp. 56-66, 2004.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning to transform time series with a few examples", "author": ["A. Rahimi", "B. Recht", "T. Darrell"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 29, no. 10, pp. 1759-1775, Oct. 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Landford \u201cResponse to Comments on the Isomap algorithm and topological stability,", "author": ["J.B. Tenenbaum", "J.C.V. de Sliva"], "venue": "Sciences, vol. 295,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "An extended Isomap algorithm for learning multiclass manifold,", "author": ["Y. Wu", "K.L. Chan"], "venue": "Proc. of the 2004 Int. Conf. on Machine Learning and Cybernetics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "Nonlinear dimensionality reduction of data lying on the multicluster manifold,", "author": ["D. Meng", "Y. Leung", "T. Fung", "Z. Xu"], "venue": "IEEE Trans. Systems, Man and Cybern. B,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Feature extraction using constrained maximum variance mapping,", "author": ["B. Li", "D. Huang", "C. Wang", "K. Liu"], "venue": "Pattern Recognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "K-Edge Connected Neighborhood Graph for Geodesic Distance Estimation and Nonlinear Projection", "author": ["L. Yang"], "venue": "Proc. of 17th Int. Conf. on Pattern Recognition (ICPR\u201904), vol. 1, pp. 196-199. 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Building k Edge-Disjoint Spanning Trees of Minimum Total Length for Isometric Data Embedding", "author": ["L. Yang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 27, no. 10, pp. 1680-1683, Oct. 2005.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Building k Edge-connected neighborhood graph for distancebased data projection", "author": ["L. Yang"], "venue": "Pattern Recognition Letters, vol. 26, no. 13, pp. 2015-2021, Oct. 2005.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Building k-Connected Neighborhood Graphs for Isometric Data Embedding", "author": ["L. Yang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 28, issue. 5, pp. 827-831, May. 2006.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Incremental Construction of Neighborhood Graphs for Nonlinear Dimensionality Reduction", "author": ["D. Zhao", "L. Yang"], "venue": "Proc. of Int. Conf. on Pattern Recognition, vol. 28, no. 5, pp. 827-831, May. 2006.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "Incremental Isometric Embedding of High- Dimensional Data Using Connected Neighborhood Graphs", "author": ["D. Zhao", "L. Yang"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 31, no. 1, pp. 86-98, Jan. 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimating the intrinsic dimension of data with a fractal-based approach,", "author": ["F. Camastra", "A. Vinciarelli"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence vol. 24,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2002}, {"title": "Maximum likelihood estimation of intrinsic dimension,", "author": ["E. Levina", "P.J. Bickel"], "venue": "NIPS 04: Neural Information Processing Systems,2005", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2005}, {"title": "Comments on \u2019Maximum likelihood estimation of intrinsic dimension", "author": ["D.J.C. MacKay", "Z. Ghahramani"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Intrinsic dimension estimation of manifolds by incising balls,", "author": ["M. Fan", "H. Qiao", "B. Zhang"], "venue": "Pattern Recognition vol. 42,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Characterizing Virtual Eigensignatures for General Purpose Face Recognition", "author": ["B. Graham", "N.M. Allinson"], "venue": "NATO ASI Series F, Computer and Systems Sciences,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1998}, {"title": "Fast multiscale clustering and manifold identification,", "author": ["D. Kushnir", "M. Galun", "A. Brandt"], "venue": "Pattern Recognition,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}], "referenceMentions": [{"referenceID": 23, "context": "Some revisions have been made on the original multi-cluster manifold learning algorithm called D-C Isomap [24] such that the revised D-C Isomap can learn multi-manifolds data.", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "Principal component analysis (PCA) [1] and multidimensional scaling (MDS) [2] are two important linear dimensionality reduction methods.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "Principal component analysis (PCA) [1] and multidimensional scaling (MDS) [2] are two important linear dimensionality reduction methods.", "startOffset": 74, "endOffset": 77}, {"referenceID": 2, "context": "NLDR is used to learn nonlinear intrinsic structure of data, which is considered to be the first step of \u201dmachine learning and pattern recognition: observe and explore the phenomena\u201d [3].", "startOffset": 183, "endOffset": 186}, {"referenceID": 5, "context": "Two interesting nonlinear dimensionality reduction methods based on the notion of manifold learning [6], isometric feature mapping (Isomap) [4] and local linear embedding (LLE) [5], have been introduced in SCIENCE 2000.", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "Two interesting nonlinear dimensionality reduction methods based on the notion of manifold learning [6], isometric feature mapping (Isomap) [4] and local linear embedding (LLE) [5], have been introduced in SCIENCE 2000.", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "Two interesting nonlinear dimensionality reduction methods based on the notion of manifold learning [6], isometric feature mapping (Isomap) [4] and local linear embedding (LLE) [5], have been introduced in SCIENCE 2000.", "startOffset": 177, "endOffset": 180}, {"referenceID": 6, "context": "Laplacian eigenmap [7] utilizes the approximation of the Laplace-Beltrami operator on manifold to provide an optimal embedding.", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "Hessian LLE [8] resembles Laplacian eigenmap by using the approximation of Hessian operator instead of Laplacian operator.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "Local tangent space alignment(LTSA) [9] method learns local geometry by constructing a local tangent space of each data point and then aligns these local tangent spaces into a single global coordinates system with respect to the underlying manifold.", "startOffset": 36, "endOffset": 39}, {"referenceID": 9, "context": "Diffusion maps [10] applies diffusion semigroups to produce multiscale geometries to represent complex structure.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "Riemannian manifold learning (RML) [11] method uses the constructed Riemannian normal coordinate chart to map the input data into a lower dimensional space.", "startOffset": 35, "endOffset": 39}, {"referenceID": 15, "context": "NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.", "startOffset": 231, "endOffset": 235}, {"referenceID": 18, "context": "NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.", "startOffset": 256, "endOffset": 260}, {"referenceID": 19, "context": "NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.", "startOffset": 292, "endOffset": 296}, {"referenceID": 20, "context": "NLDR is fast developed and has been proved very useful in many fields and applications, such as classification using Isomap [16] and laplacian eigenmap [17], geometric based semi-supervised learning method using laplacian eigenmap [18], data visualization [19], time series manifold learning [20], [21] and so on.", "startOffset": 298, "endOffset": 302}, {"referenceID": 12, "context": "As Isomap emphasizes on the global geometric relationship of data points, it is very illustrative in data visualization and pattern analysis [13].", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "Yiming Wu et al [23] introduced a split and augment procedure for neighborhood graph construction which could produce a totally connected neighborhood graph.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "Li Yang [26]\u2013[29] introduced several neighborhood graph construction algorithms using techniques from discrete mathematics, graph theory.", "startOffset": 8, "endOffset": 12}, {"referenceID": 28, "context": "Li Yang [26]\u2013[29] introduced several neighborhood graph construction algorithms using techniques from discrete mathematics, graph theory.", "startOffset": 13, "endOffset": 17}, {"referenceID": 23, "context": "Deyu Meng et al [24] proposed a decomposition and composition Isomap (DC Isomap).", "startOffset": 16, "endOffset": 20}, {"referenceID": 3, "context": "Classical Isometric FeatureMapping and Its Limitations Isomap is able to recover the intrinsic geometric structure and converge as the number of data points increases [4] if data lie on a manifold, .", "startOffset": 167, "endOffset": 170}, {"referenceID": 13, "context": "The properties of Isomap algorithm are well understood [14] [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "The properties of Isomap algorithm are well understood [14] [12].", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "Otherwise, if neighborhood size k or \u01eb is chosen too large to cause short-circuit edges, these edges will have a significant negative influence on the topological stability of Isomap algorithm [22].", "startOffset": 193, "endOffset": 197}, {"referenceID": 22, "context": "Wu and Chan [23] proposed a split-augment approach to construct a neighborhood graph.", "startOffset": 12, "endOffset": 16}, {"referenceID": 25, "context": "Li [26]\u2013[29] introduced four methods to construct a connected neighborhood graph.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "Li [26]\u2013[29] introduced four methods to construct a connected neighborhood graph.", "startOffset": 8, "endOffset": 12}, {"referenceID": 25, "context": "The k minimum spanning trees (k-MST) [26] method repeatly extracts k minimum spanning trees (MSTs) from the complete Euclidean graph of all data points.", "startOffset": 37, "endOffset": 41}, {"referenceID": 26, "context": "Instead of extracting k MSTs, the minimum-k-spanning trees (min-kST) [27] method finds k edge-disjoint spanning trees from the complete Euclidean graph, and the sum of the total edge length of the k edge-disjoint spanning trees attains a minimum.", "startOffset": 69, "endOffset": 73}, {"referenceID": 27, "context": "The kedge-connected (k-EC) [28] method constructs a connected neighborhood graph by adding edges in a non-increasing order from the complete Euclidean graph.", "startOffset": 27, "endOffset": 31}, {"referenceID": 28, "context": "The k-vertices-connected (k-VC) [29] method add edges in a nonincreasing order from the complete Euclidean graph, an edge is added if its two end vertices would be disconnected by removing some k \u2212 1 vertices.", "startOffset": 32, "endOffset": 36}, {"referenceID": 25, "context": "The methods introduced in [26]\u2013[29] advantage over k-NN method for two reasons: First, the local neighbor relationship is affected by the global distribution of data points.", "startOffset": 26, "endOffset": 30}, {"referenceID": 28, "context": "The methods introduced in [26]\u2013[29] advantage over k-NN method for two reasons: First, the local neighbor relationship is affected by the global distribution of data points.", "startOffset": 31, "endOffset": 35}, {"referenceID": 23, "context": "Multi-manifolds learning by decomposition-composition Isomap In [24], Meng et al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 24, "context": "Constrained Maximum Variance Mapping There is also a newly proposed algorithm called constrained maximum variance mapping (CMVM) [25] for multi-manifolds learning.", "startOffset": 129, "endOffset": 133}, {"referenceID": 22, "context": "The general procedure for isometric multi-manifolds learning Many previous methods extend Isomap for multi-manifolds learning by revising the neighborhood graph construction step of the Isomap algorithm [23], [26]\u2013[29].", "startOffset": 203, "endOffset": 207}, {"referenceID": 25, "context": "The general procedure for isometric multi-manifolds learning Many previous methods extend Isomap for multi-manifolds learning by revising the neighborhood graph construction step of the Isomap algorithm [23], [26]\u2013[29].", "startOffset": 209, "endOffset": 213}, {"referenceID": 28, "context": "The general procedure for isometric multi-manifolds learning Many previous methods extend Isomap for multi-manifolds learning by revising the neighborhood graph construction step of the Isomap algorithm [23], [26]\u2013[29].", "startOffset": 214, "endOffset": 218}, {"referenceID": 11, "context": "With the assumptions above, the following theorem is proved by Zha et al [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "Many clustering method could be used, such as K-means, Isodata and methods introduced in [15] [33].", "startOffset": 89, "endOffset": 93}, {"referenceID": 36, "context": "Even if the manifolds overlay with each other, they can still be identified and clustered [39].", "startOffset": 90, "endOffset": 94}, {"referenceID": 31, "context": "For intrinsic dimensionality estimation, many methods can be used: the fractal based method [34], MLE method [35], [36], and the incising ball method [37].", "startOffset": 92, "endOffset": 96}, {"referenceID": 32, "context": "For intrinsic dimensionality estimation, many methods can be used: the fractal based method [34], MLE method [35], [36], and the incising ball method [37].", "startOffset": 109, "endOffset": 113}, {"referenceID": 33, "context": "For intrinsic dimensionality estimation, many methods can be used: the fractal based method [34], MLE method [35], [36], and the incising ball method [37].", "startOffset": 115, "endOffset": 119}, {"referenceID": 34, "context": "For intrinsic dimensionality estimation, many methods can be used: the fractal based method [34], MLE method [35], [36], and the incising ball method [37].", "startOffset": 150, "endOffset": 154}, {"referenceID": 25, "context": "Methods of Li\u2019s works [26]\u2013[29] or the k-CC method is preferred, where the k-CC graph construction method will be described later.", "startOffset": 22, "endOffset": 26}, {"referenceID": 28, "context": "Methods of Li\u2019s works [26]\u2013[29] or the k-CC method is preferred, where the k-CC graph construction method will be described later.", "startOffset": 27, "endOffset": 31}, {"referenceID": 22, "context": "[23] in their split-augment process and well developed and used in [24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[23] in their split-augment process and well developed and used in [24].", "startOffset": 67, "endOffset": 71}, {"referenceID": 29, "context": "For incremental learning, the computational complexity of kNN, k-MSTs and k-VC are O(kN), O(N log N) and O(N log N + kN) respectively [30] [31].", "startOffset": 134, "endOffset": 138}, {"referenceID": 30, "context": "For incremental learning, the computational complexity of kNN, k-MSTs and k-VC are O(kN), O(N log N) and O(N log N + kN) respectively [30] [31].", "startOffset": 139, "endOffset": 143}, {"referenceID": 22, "context": "Compared with the method proposed in [23], k-CC method constructs a neighborhood graph with k inter-manifolds edges, which is able to control the rotation of the embedding of data manifolds.", "startOffset": 37, "endOffset": 41}, {"referenceID": 34, "context": "The incising ball method [37] is utilized to estimate the intrinsic dimensionality, which is simple to implement and always outputs an integer result.", "startOffset": 25, "endOffset": 29}, {"referenceID": 35, "context": "6(a) shows some samples of the faces data [38] which contains face images of five persons 1.", "startOffset": 42, "endOffset": 46}], "year": 2009, "abstractText": "Isometric feature mapping (Isomap) is a promising manifold learning method. However, Isomap fails to work on data which distribute on clusters in a single manifold or manifolds. Many works have been done on extending Isomap to multi-manifolds learning. In this paper, we proposed a new multi-manifolds learning algorithm (M-Isomap) with the help of a general procedure. The new algorithm preserves intramanifold geodesics and multiple inter-manifolds edges faithfully. Compared with previous approaches, this algorithm can isometrically learn data distribute on several manifolds. Some revisions have been made on the original multi-cluster manifold learning algorithm called D-C Isomap [24] such that the revised D-C Isomap can learn multi-manifolds data. Finally, the features and effectiveness of the proposed multi-manifolds learning algorithms are demonstrated and compared through experiments.", "creator": "LaTeX with hyperref package"}}}