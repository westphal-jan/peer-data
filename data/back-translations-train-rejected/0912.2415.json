{"id": "0912.2415", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Dec-2009", "title": "Adapting Heuristic Mastermind Strategies to Evolutionary Algorithms", "abstract": "The art of solving the Mastermind puzzle was initiated by Donald Knuth and is already more than 30 years old; despite that, it still receives much attention in operational research and computer games journals, not to mention the nature-inspired stochastic algorithm literature. In this paper we try to suggest a strategy that will allow nature-inspired algorithms to obtain results as good as those based on exhaustive search strategies; in order to do that, we first review, compare and improve current approaches to solving the puzzle; then we test one of these strategies with an estimation of distribution algorithm. Finally, we try to find a strategy that falls short of being exhaustive, and is then amenable for inclusion in nature inspired algorithms (such as evolutionary or particle swarm algorithms). This paper proves that by the incorporation of local entropy into the fitness function of the evolutionary algorithm it becomes a better player than a random one, and gives a rule of thumb on how to incorporate the best heuristic strategies to evolutionary algorithms without incurring in an excessive computational cost.", "histories": [["v1", "Sat, 12 Dec 2009 12:17:48 GMT  (14kb)", "http://arxiv.org/abs/0912.2415v1", "Accepted at the NICSO'10 conference"]], "COMMENTS": "Accepted at the NICSO'10 conference", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["tomas philip runarsson", "juan j merelo-guervos"], "accepted": false, "id": "0912.2415"}, "pdf": {"name": "0912.2415.pdf", "metadata": {"source": "CRF", "title": "Adapting Heuristic Mastermind Strategies to Evolutionary Algorithms", "authors": ["Thomas Philip Runarsson", "Juan J. Merelo-Guerv\u00f3s"], "emails": ["tpr@hi.is", "jmerelo@geneura.ugr.es"], "sections": [{"heading": null, "text": "ar Xiv: 091 2.24 15v1 [cs.NE] 1 2Keywords: games, mastermind, bulls and cows, search strategies, oracle games"}, {"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2 State of the art", "text": "In fact, the fact is that most of them are able to move to another world in which they are able to find themselves."}, {"heading": "3 Comparison of heuristic strategies", "text": "As already mentioned, over the years there have been a number of different strategies to choose from among the consistent assumptions in Mastermind. This heuristics does not take into account an exhaustive minimax search, but rather a one-sided search. For this reason, we propose to perform a comparison of heuristic methods here, where the bonds are randomly broken. Each strategy is therefore used on all possible secret combinations (they are 64 = 1296), with ten independent runes. Heuristics are compared, entropy, most parts and the worst strategies, as they are carried out by Bestavros and Belal [3]. The worst case refers to the fact that for each possible return code for a particular conjecture assumes the smallest reduction, i.e. the worst case. The actual consistent conjecture is the one that maximizes the strategy that is the worst, is always the expected outcomes 4]."}, {"heading": "4 Estimation of distribution algorithm using local entropy", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5 Heuristics based on a subset of consistent guesses", "text": "According to a tip in one of our earlier papers, Berghman et al. [1] recently proposed an evolutionary algorithm that finds a number of consistent guesses and then uses a strategy to select which of these should be played. [3] The strategy they apply is no different from the expected size strategy. However, it differs in some fundamental ways. In their approach, it is assumed that each consistent guess is in turn the secret and each guess is played against each different secret. The return codes are then used to calculate the size of the set of remaining consistent guesses in the set. An average is then taken over the size of these sets. Here, the crucial difference between the expected size method is that only a subset of all possible consistent guesses is used and some return codes may not be viewed or considered more often than once, which could lead to a bias in the result. In fact, they point out that their Berghman et approach is computationally intensive, which leads them to further reducing that subset of numbers."}, {"heading": "10 4.438 4.468 4.476 4.483 0.016", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "20 4.394 4.423 4.426 4.455 0.021", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "30 4.380 4.413 4.410 4.443 0.020", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "40 4.372 4.398 4.399 4.426 0.017", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 4.429 4.454 4.454 4.477 0.016", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "20 4.424 4.431 4.427 4.451 0.009", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "30 4.393 4.416 4.416 4.435 0.015", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "40 4.383 4.424 4.427 4.448 0.020", "text": "This implies that, at least in this case, the use of a subset of the combination pool, which is about 1 / 10 of the total size, potentially results in a result as good as the use of the entire set; even if it is algorithmically more difficult to find 20 preliminary solutions than a single one, the use of this solution in stochastic search algorithms such as the aforementioned EDA or an evolutionary algorithm promises to combine the accuracy of exhaustive search algorithms with the speed of an EDA or EA. In any case, there is no other option for areas that are larger, and this 1 / 10 gives at least one rule of thumb. How this proportion grows with the size of the search space is still open."}, {"heading": "6 Discussion and Conclusion", "text": "In this paper, we have tried to study and compare the various heuristic strategies for the simplest version of Mastermind in order to come up with a nature-inspired algorithm that is capable of beating it in terms of runtime and scalability. However, the main problem with heuristic strategies is that they must have the entire search space in memory; even the most advanced, which will only run over it once, become unwieldy because they have soonas or an increase. However, evolutionary algorithms are already much more scalable because their performance as a random generator is no better. In this paper, the improvement (or perhaps just clarification) of heuristic and deterministic algorithms is incorporated with a random selection of strategies that we have built into the simplest of these strategies in terms of distribution algorithmic development."}, {"heading": "Acknowledgements", "text": "This work was partly financed by the Spanish MICYT projects NoHNES (Spanish Ministerio de Educacio \u0301 n y Ciencia - TIN2007-68083) and TIN2008-06491-C04-01, as well as the Junta de Andaluc\u0131'a P06-TIC-02025 and P07-TIC-03044. The authors are also very grateful for the traffic jams in Granada, which allowed endless moments of discussion and interaction on this issue."}], "references": [{"title": "Efficient solutions for Mastermind using genetic algorithms", "author": ["L. Berghman", "D. Goossens", "R. Leus"], "venue": "Computers and Operations Research 36(6),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Solving mastermind using GAs and simulated annealing: a case of dynamic constraint optimization", "author": ["J.L. Bernier", "C.I. Herr\u00e1iz", "J.J. Merelo-Guerv\u00f3s", "S. Olmeda", "A. Prieto"], "venue": "Proceedings PPSN, Parallel Problem Solving from Nature IV,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Mastermind, a game of diagnosis strategies. Bulletin of the Faculty of Engineering, Alexandria", "author": ["A. Bestavros", "A. Belal"], "venue": "URL citeseer.ist.psu.edu/bestavros86mastermind.html", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1986}, {"title": "Towards an optimum mastermind strategy", "author": ["R.W. Irving"], "venue": "Journal of Recreational Mathematics 11(2),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1978}, {"title": "A survey of NPcomplete puzzles", "author": ["G. Kendall", "A. Parkes", "K. Spoerer"], "venue": "ICGA Journal 31(1),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "The computer as Master Mind", "author": ["D.E. Knuth"], "venue": "J. Recreational Mathematics 9(1),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1976}, {"title": "An optimal Mastermind strategy", "author": ["K. Koyama", "T.W. Lai"], "venue": "J. Recreational Mathematics", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "Finding a needle in a haystack using hints and evolutionary computation: the case of genetic mastermind. In: A.S.W. Scott Brave (ed.) Late breaking papers at the GECCO99", "author": ["J.J. Merelo-Guerv\u00f3s", "J. Carpio", "P. Castillo", "V.M. Rivas", "G. Romero"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Finding a needle in a haystack using hints and evolutionary computation: the case of evolutionary MasterMind", "author": ["J.J. Merelo-Guerv\u00f3s", "P. Castillo", "V. Rivas"], "venue": "Applied Soft Computing", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Algorithm::Evolutionary, a flexible Perl module for evolutionary computation", "author": ["J.J. Merelo-Guerv\u00f3s", "P.A. Castillo", "E. Alba"], "venue": "Soft Computing", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "From recombination of genes to the estimation of distributions: I. binary parameters", "author": ["H. M\u00fchlenbein", "G. Paass"], "venue": "Lecture notes in computer science 1141,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Some strategies for mastermind", "author": ["E. Neuwirth"], "venue": "Zeitschrift fur Operations Research. Serie B 26(8),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1982}, {"title": "Mastermind is np-complete", "author": ["J. Stuckman", "G.Q. Zhang"], "venue": "CoRR abs/cs/0512049", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}], "referenceMentions": [{"referenceID": 8, "context": "What makes this problem interesting is its relation to other, generally called oracle problems such as circuit and program testing, differential cryptanalysis and other puzzle games (these similarities were reviewed in our previous paper [10]) is the fact", "startOffset": 238, "endOffset": 242}, {"referenceID": 12, "context": "that it has been proved to be NP-complete [14, 5] and that there are several open issues, namely, what is the lowest average number of guesses you can achieve, how to minimize the number of evaluations needed to find them (and thus the run-time of the algorithm), and obviously, how it scales when increasing \u03ba and l.", "startOffset": 42, "endOffset": 49}, {"referenceID": 4, "context": "that it has been proved to be NP-complete [14, 5] and that there are several open issues, namely, what is the lowest average number of guesses you can achieve, how to minimize the number of evaluations needed to find them (and thus the run-time of the algorithm), and obviously, how it scales when increasing \u03ba and l.", "startOffset": 42, "endOffset": 49}, {"referenceID": 8, "context": "This NP completeness implies that it is difficult to find algorithms that solve the problem in a reasonable amount of time, and that is why in our previous work [10, 9, 2] we introduced stochastic evolutionary and simulated annealing algorithms that solved the Mastermind puzzle in the general case, finding solutions in a reasonable amount of time that scaled roughly logarithmically with problem size.", "startOffset": 161, "endOffset": 171}, {"referenceID": 7, "context": "This NP completeness implies that it is difficult to find algorithms that solve the problem in a reasonable amount of time, and that is why in our previous work [10, 9, 2] we introduced stochastic evolutionary and simulated annealing algorithms that solved the Mastermind puzzle in the general case, finding solutions in a reasonable amount of time that scaled roughly logarithmically with problem size.", "startOffset": 161, "endOffset": 171}, {"referenceID": 1, "context": "This NP completeness implies that it is difficult to find algorithms that solve the problem in a reasonable amount of time, and that is why in our previous work [10, 9, 2] we introduced stochastic evolutionary and simulated annealing algorithms that solved the Mastermind puzzle in the general case, finding solutions in a reasonable amount of time that scaled roughly logarithmically with problem size.", "startOffset": 161, "endOffset": 171}, {"referenceID": 5, "context": "One of the earliest strategies, by Knuth [6], is perhaps the most intuitive for Mastermind.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "For example, in [10] an evolutionary algorithm is described for this purpose.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "The path leading to the most successful strategies to date include using the worst case, expected case, entropy [13, 3] and most parts [7] strategies.", "startOffset": 112, "endOffset": 119}, {"referenceID": 2, "context": "The path leading to the most successful strategies to date include using the worst case, expected case, entropy [13, 3] and most parts [7] strategies.", "startOffset": 112, "endOffset": 119}, {"referenceID": 11, "context": "The entropy is then computed as \u2211i=1 pi log2(1/pi), where log2(1/pi) is the information in bit(s) per partition, and can be used to select the next combination to play in Mastermind [13].", "startOffset": 182, "endOffset": 186}, {"referenceID": 3, "context": "The worst case is a one-ply version of Knuth\u2019s approach, but Irving [4] suggested using", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "The heuristics compared are the entropy, most parts and worst case strategy, as performed by Bestavros and Belal [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 3, "context": "Finally, the expected size strategy, [4] is also tested; in this strategy the expected case is used instead of the worst case.", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "The first combination played is always AABC, as proposed by [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 6, "context": "340 [8].", "startOffset": 4, "endOffset": 7}, {"referenceID": 10, "context": "In this case we use an estimation of distribution algorithm [12] EDA included with the Algorithm::Evolutionary Perl module [11], with the whole EDA-solving algorithm available as Algorithm::MasterMind::EDA from CPAN (the comprehensive Perl Archive Network).", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "In this case we use an estimation of distribution algorithm [12] EDA included with the Algorithm::Evolutionary Perl module [11], with the whole EDA-solving algorithm available as Algorithm::MasterMind::EDA from CPAN (the comprehensive Perl Archive Network).", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "The fitness function used previously [10] to find consistent guesses is as follows,", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "[1] proposed an evolutionary algorithm which finds a number of consistent guesses and then uses a strategy to select which one of these should be played.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "first guess, then 55, followed by 12 [1].", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "However, evolutionary algorithms have already been proved [10] to scale much better, the only problem being that their performance as players is no better than a random player.", "startOffset": 58, "endOffset": 62}], "year": 2009, "abstractText": "The art of solving the Mastermind puzzle was initiated by Donald Knuth and is already more than 30 years old; despite that, it still receives much attention in operational research and computer games journals, not to mention the natureinspired stochastic algorithm literature. In this paper we try to suggest a strategy that will allow nature-inspired algorithms to obtain results as good as those based on exhaustive search strategies; in order to do that, we first review, compare and improve current approaches to solving the puzzle; then we test one of these strategies with an estimation of distribution algorithm. Finally, we try to find a strategy that falls short of being exhaustive, and is then amenable for inclusion in nature inspired algorithms (such as evolutionary of particle swarm algorithms). This paper proves that by the incorporation of local entropy into the fitness function of the evolutionary algorithm it becomes a better player than a random one, and gives a rule of thumb on how to incorporate the best heuristic strategies to evolutionary algorithms without incurring in an excessive computational cost.", "creator": "LaTeX with hyperref package"}}}