{"id": "0912.2709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2009", "title": "The Gaussian Surface Area and Noise Sensitivity of Degree-$d$ Polynomials", "abstract": "We provide asymptotically sharp bounds for the Gaussian surface area and the Gaussian noise sensitivity of polynomial threshold functions. In particular we show that if $f$ is a degree-$d$ polynomial threshold function, then its Gaussian sensitivity at noise rate $\\epsilon$ is less than some quantity asymptotic to $\\frac{d\\sqrt{2\\epsilon}}{\\pi}$ and the Gaussian surface area is at most $\\frac{d}{\\sqrt{2\\pi}}$. Furthermore these bounds are asymptotically tight as $\\epsilon\\to 0$ and $f$ the threshold function of a product of $d$ distinct homogeneous linear functions.", "histories": [["v1", "Mon, 14 Dec 2009 19:14:03 GMT  (6kb)", "http://arxiv.org/abs/0912.2709v1", null]], "reviews": [], "SUBJECTS": "cs.CC cs.LG", "authors": ["daniel m kane"], "accepted": false, "id": "0912.2709"}, "pdf": {"name": "0912.2709.pdf", "metadata": {"source": "CRF", "title": "The Gaussian Surface Area and Noise Sensitivity of Degree-d Polynomials", "authors": ["Daniel M. Kane"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 091 2.27 09v1 [cs.CC] 1 4D ec2 009The Gaussian surface and noise sensitivity of graduate polynomials Daniel M. Kane15. December 2009"}, {"heading": "1 Introduction", "text": "We provide asymptotically sharp boundaries for the Gaussian surface and the Gaussian noise sensitivity of the polynomial threshold functions. In particular, we show that if f is a polynomial threshold function of the degree, its Gaussian sensitivity is less than any quantity asymptotic to d \u221a \u03c0 and the Gaussian surface is at most d \u221a 2\u03c0. Furthermore, these boundaries are asymptotically narrow, since they represent the threshold function of a product d of clearly homogeneous linear functions.The sensitivity to noise and the surface surface are both of fundamental interest and useful for the analysis of agnostic learning algorithms (see [6]). In particular, our results imply that the class of polynomial threshold functions of the d agnostic can be learned under the n-dimensional Gaussian distribution in time nO (d2 / 4)."}, {"heading": "1.1 Basic Definitions", "text": "Considering a function f: Rn \u2192 {\u2212 1, 1} we define the Gaussian noise sensitivity at the noise frequency as GNSB (f): = Pr (f (X) 6 = f (Z), where X is a n-dimensional Gaussian random variable, and Z = (1 \u2212) X + \u221a 2Y for Y is an independent n-dimensional Gaussian variable. This is closely related to the Gaussian surface volume of a region of f \u2212 1 (1). Specifically, we define the Gaussian surface variable of a set of A to Z (A): = lim inf \u03b4 \u2192 0 Gaussian volume (A\u03b4\\ A).Where the Gaussian volume of a region R Pr (X \u00b2 R) for X is a Gaussian random variable, we define the Gaussian surface of a set of A to Z (A): = lim inf \u03b4 \u2192 0 Gaussian volume (A).Where the Gaussian volume of a region R is a Gaussian random volume (X \u00b2 R) for X is a Gaussian surface area of A (X), we define the Gaussian surface of a set of A (X = Gaussian surface)."}, {"heading": "1.2 Statement of Results", "text": "We focus on the detection of two main results. We define f as a polynomial threshold function if f (x) = sgn (p (x)) for a certain degree is d polynomial p. We prove the following theorems about such functions: Theorem 1. If f is a polynomial threshold function of degree d, then GNSB (f) \u2264 d arcsin (\u221a 2B-2) \u03c0 \u00b2 d \u00b2 2B = O (d \u221a). Furthermore, this limit is asymptotically narrow as regards the threshold function of a product of different linear functions. Theorem 2. If f is a polynomial threshold function of degree d, then this section 2 will devote some conclusions to the proof of theorem 1, section 3 to the proof of theorem 2 and section 4."}, {"heading": "2 Proof of the Noise Sensitivity Bound", "text": "The proof of theorem 1. We begin by making \u03b8 = arcsin (\u221a -2) come. (1) We must bind p: = GNS\u044b (f) = Pr (f (X) 6 = f (cos) X + sin (\u03b8) Y)). (1) We note that the value of p in Equation 1 remains the same if X and Y are replaced by any X \u2032 and Y \u2032 distributions that are i.i.d. Gaussian distributions. In particular, we define X\u03c6 = cos (\u03c6) Y.Note that the value of p + 2 remains the same if X and Y are replaced by any X and Y distributions that are i.d.Gaussian. Using these distributions, we find that for any other distribution period that sinceX\u03b8 + \u03c6 is = cos: X + sin: Y = cos) Y = cos: Y = cos (capabilities) cos: X \u2212 sin: X \u2212 sin: sin: sin: c."}, {"heading": "3 Proof of the Gaussian Surface Area Bounds", "text": "We begin by proving that the probability that at least one of these character changes lies between X and (1 + 2) X and (1 + 2) X. Therefore, it is sufficient to prove that for each of these character changes, one of these character changes, that it lies between X and (1 + 2), lies between X and (1 + 2) X, that it lies between X and (1 + 2) X, with the probability that it lies between X and (2 + 2) X, that it lies between X and (1 + 2) X, that it lies between X and (1 + 2) X."}, {"heading": "4 Conclusion", "text": "One might hope to generalize these results for other distributions, such as the even distribution at the corners of the hypercube. Unfortunately, several aspects of this evidence are difficult to generalize. Perhaps most importantly, we are losing the symmetry that allowed us to prove our original result in terms of noise sensitivity, and another difficulty would be the ratio between noise sensitivity and surface area. In our case, both are essentially equivalent study quantities. On the other hand, [6] a concept of surface area for hypercube distribution was defined and it was shown that even for linear threshold functions, there could be a gap between noise sensitivity and surface area of up to 90% (\u221a log (n)))."}], "references": [{"title": "The Reverse Isoperimetric Problem for Gaussian Measure", "author": ["K. Ball"], "venue": "Discrete and Computational Geometry,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "Noise sensitivity of Boolean functions and applications to percolation, Inst", "author": ["I. Benjamini", "G. Kalai", "O. Schramm"], "venue": "Hautes Etudes Sci. Publ. Math.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Average sensitivity and noise sensitivity of polynomial threshold functions, Manuscript, available at http://arxiv.org/abs/0909.5011", "author": ["I. Diakonikolas", "P. Raghavendra", "R. Servedio", "L.-Y. Tan"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Bounding the sensitivity of polynomial threshold functions, Manuscript, available at http://arxiv.org/abs/0909.5175", "author": ["P. Harsha", "A. Klivans", "R. Meka"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Learning Geometric Concepts via Gaussian Surface Area", "author": ["Adam R. Klivans", "Ryan O\u2019Donnell", "Rocco A. Servedio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Semigroup proofs of the isoperimetric inequality in Euclidean and Gauss space", "author": ["M. Ledoux"], "venue": "Bull. Sci. Math.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "Noise Stability of Weighted Majority", "author": ["Yuval Peres"], "venue": "Manuscript, available at http://arxiv.org/abs/math/0412377,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}], "referenceMentions": [{"referenceID": 4, "context": "The noise sensitivity and surface area are both of fundamental interest and useful in the analysis of agnostic learning algorithms (see [6]).", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "[7] proves a bound on noise sensitivity in terms of surface area and we relate our bounds essentially by also proving the other direction of this inequality for boolean functions with smooth interface that switch signs a bounded number of times on any line through the origin.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "A bound of \u00d5(\u01eb) noise sensitivity was recently proved by [3] and independently by [5] for multilinear polynomials.", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "A bound of \u00d5(\u01eb) noise sensitivity was recently proved by [3] and independently by [5] for multilinear polynomials.", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "The d = 1 case of this conjecture was proved by [8], improving upon a bound of O(\u01eb) of [2].", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "The d = 1 case of this conjecture was proved by [8], improving upon a bound of O(\u01eb) of [2].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "It is noted in [3] that such a result would imply a similar bound for the Gaussian case.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "On the other hand [6] defined a notion of surface area for the hypercube distribution and proved that for even linear threshold functions there could be a gap between noise sensitivity and surface area of as much as \u0398( \u221a", "startOffset": 18, "endOffset": 21}], "year": 2009, "abstractText": "We provide asymptotically sharp bounds for the Gaussian surface area and the Gaussian noise sensitivity of polynomial threshold functions. In particular we show that if f is a degree-d polynomial threshold function, then its Gaussian sensitivity at noise rate \u01eb is less than some quantity asymptotic to d \u221a 2\u01eb \u03c0 and the Gaussian surface area is at most d \u221a 2\u03c0 . Furthermore these bounds are asymptotically tight as \u01eb \u2192 0 and f the threshold function of a product of d distinct homogeneous linear functions. The noise sensitivity and surface area are both of fundamental interest and useful in the analysis of agnostic learning algorithms (see [6]). In particular our results imply that the class of degree-d polynomial threshold functions is agnostically learnable under the n-dimensional Gaussian distribution in time n 2/\u01eb4). A number of other authors have attempted to prove bounds along these lines. [7] proves a bound on noise sensitivity in terms of surface area and we relate our bounds essentially by also proving the other direction of this inequality for boolean functions with smooth interface that switch signs a bounded number of times on any line through the origin. Our bounds are obtained via a simple computation in the case of d = 1. A bound of \u00d5(\u01eb) noise sensitivity was recently proved by [3] and independently by [5] for multilinear polynomials. There is also interest in related questions for points picked uniformly from vertices of the hypercube rather than with the Gaussian distribution. It is conjectured in [4] that the corresponding noise sensitivity in this case is also always O(d \u221a \u01eb). The d = 1 case of this conjecture was proved by [8], improving upon a bound of O(\u01eb) of [2]. It is noted in [3] that such a result would imply a similar bound for the Gaussian case. Hence our results can be thought of as a first step toward proving this conjecture.", "creator": "LaTeX with hyperref package"}}}