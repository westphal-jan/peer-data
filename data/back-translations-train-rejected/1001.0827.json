{"id": "1001.0827", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2010", "title": "Document Clustering with K-tree", "abstract": "This paper describes the approach taken to the XML Mining track at INEX 2008 by a group at the Queensland University of Technology. We introduce the K-tree clustering algorithm in an Information Retrieval context by adapting it for document clustering. Many large scale problems exist in document clustering. K-tree scales well with large inputs due to its low complexity. It offers promising results both in terms of efficiency and quality. Document classification was completed using Support Vector Machines.", "histories": [["v1", "Wed, 6 Jan 2010 07:51:23 GMT  (52kb)", "http://arxiv.org/abs/1001.0827v1", "12 pages, INEX 2008"]], "COMMENTS": "12 pages, INEX 2008", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.DS", "authors": ["christopher m de vries", "shlomo geva"], "accepted": false, "id": "1001.0827"}, "pdf": {"name": "1001.0827.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["chris@de-vries.id.au", "s.geva@qut.edu.au"], "sections": [{"heading": null, "text": "ar Xiv: 100 1.08 27v1 [cs.IR] 6 Jan 201 0Keywords: INEX, XMLMining, Clustering, K-tree, tree, vector quantification, text classification, support vector machine"}, {"heading": "1 Introduction", "text": "The XML mining track consists of two tasks, classification and clustering. Classification denotes documents in known categories. Clustering groups similar documents without knowledge of categories. The corpus consisted of 114,366 documents and 636,187 document-to-document associations. It is a subset of the XML Wikipedia corpus [1]. Submissions were made for both tasks using several techniques. We introduce K-Tree in the context of information retrieval. K-Tree is a tree-structured clustering algorithm introduced by Geva [2] in the context of signal processing. Due to its low complexity, it is particularly suitable for large collections. Non-negative matrix factorization (NMF) was also used to solve the cluster task. Application of NMF to document clustering was first described by Xu et. al. In SIGIR 2003 [3], negentropy was used to measure the cluster performance of documents provided by labels."}, {"heading": "2 Document Representation", "text": "The content of the document was represented by TF-IDF [9] and BM25 [10]. Stopwords were removed and the remaining terms were contained by the Porter algorithm [11]. TF-IDF is determined by term distributions within each document and the entire collection. Term frequencies in TF-IDF were normalized for the document length. BM25 works with the same concepts as TF-IDF except that there are two tuning parameters. BM25 tuning parameters were set to the same values as for TREC [10], K1 = 2 and b = 0.75. K1 affects the effect of term frequency and b influences on the document length. Links were represented as vector of LF-IDF weighted link frequencies, resulting in a document-to document linkage matrix. The line shows the origin and column shows the target of a link. Each line vector of the document represents a matrix."}, {"heading": "3 Classification Task", "text": "This approach enabled the evaluation of the different document representations. It enabled the selection of the most effective representation for the cluster task. SVMmulticlass [12] was trained with TF-IDF, BM25 and LF-IDF representations of the corpus. BM25 and LF-IDF feature vectors were linked to simultaneously train content and link information. Submissions were made only with BM25, LF-IDF or both, because BM25 performed TF-IDF."}, {"heading": "3.1 Classification Results", "text": "Table 1 lists the results for the classification task, sorted in order of decreasing memory. Recall is simply the accuracy of predicting labels for documents that are not included in the training set. Linking link and content representations did not dramatically improve performance, and further work was subsequently done to improve classification accuracy."}, {"heading": "3.2 Improving Representations", "text": "This year it has come to the point that it will be able to add the aforementioned lcihsrcnlrVo rf\u00fc eid eerwtlrVo rf\u00fc eid eerwtlrteeeaeeeeeeeeeog\u00dfrVrrrsrteeeirteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeegnln rf\u00fc eid nlrlllteeeeeeeeeeeeeeetnrVnlrlrlrlrlrrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeyeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeyeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "4 Document Cluster Quality", "text": "The purity measurement for the trace is calculated by taking the most common label in each cluster = 0.5 =. Micro-purity is the mean purity weighted by cluster size and macro is the unweighted arithmetic mean. If you take the most common label in a cluster, you discard the rest of the information represented by the other labels. This is the fact that the negentropy has been defined. It is the opposite of information entropy [14]. If entropy is a measure of uncertainty associated with a random variable, then negentropy is a measure of certainty. Therefore, it is better if more labels of the same class occur together. If all labels are evenly distributed across all clusters, the lowest possible negentropy is reached."}, {"heading": "5 K-tree", "text": "The K tree algorithm follows a height-balanced cluster tree. It can be downloaded from http: / / ktree.sf.net. It is inspired by the B + tree, where all the records in the leaves are stored at the lowest level of the tree and the internal nodes form a neighboring tree. The k + middle algorithm is used to split when the nodes become full. The constraints placed on the tree are loosened compared to a B + tree, due to the fact that vectors do not contain a total order like real numbers.B + tree of order m1. All the leaves are on the same level. 2. Internal nodes, except the root, contain between m2 and m children. 3. Internal nodes with n children contain n \u2212 1 keys that divide the childrens into a search tree. 4. The root node contains between 2 and m children."}, {"heading": "5.1 Building a K-tree", "text": "The K tree is dynamically constructed when data vectors arrive. First, the tree contains a single empty root node on the leaf plane. Vectors are inserted via a nearest neighboring search that ends at the leaf plane. The root of an empty tree is a leaf, so the nearest tree search is immediately terminated by placing the vector in the root. If m + 1 vectors arrive, the root node cannot contain any further keys. It is split with k means, in which k = 2 use all m + 1 vectors. The two centers that result from the K means, the keys are placed in a new parent. New root and child nodes are constructed and each center is associated with a child. The vectors that are placed with each center of k means in the associated child. This process has created a new root for the tree. It is two levels deep. The root has two keys and two children that form three nodes in total in the tree."}, {"heading": "5.2 K-tree Submissions", "text": "Subsequently, clusters were formed with text only, and this representation was determined by comparing each document with cosmic similarities; the track required a template with 15 clusters, but K-Tree did not produce a fixed number of clusters; therefore, the codebook vectors were characterized by comparing each document with cosmic similarities; the track did not generate a fixed number of clusters; the codebook vectors were clustered by k means; + The codebook vectors are the cluster centrists that exist above the level of the sheets; this reduces the number of vectors used at the level of the clusters."}, {"heading": "6 Non-negative Matrix Factorization", "text": "NMF factorizes a matrix into two matrices in which all elements are \u2265 0. If V is a n \u00b7 m matrix and r is a positive integer in which r < min (n, m), NMF finds two non-negative matrices Wn \u00b7 r and Hr \u00b7 m, so that V \u2248 WH. Applying this method to cluster V is a term-document matrix. Each column in H represents a document and the largest value of its cluster. Each row in H is a cluster and each column is a document.The projected gradient method was used to solve the NMF problem [16]. V was an 8000 x 114366 term-document matrix with BM25 weighted terms. The algorithm ran for a maximum of 70 iterations. It produced the W and H matrices. The cluster affiliation was determined by the maximum value in the columns of H. NMF."}, {"heading": "7 Clustering Task", "text": "Each team has presented at least one solution of 15 clusters, which allows direct comparison between different approaches. It only makes sense to compare the results where the number of clusters is the same. K-tree performed well compared to the rest of the field according to macro and micro-purity measurements. The difference in macro and micro-purity for K-tree submissions can be explained by the unequal distribution of cluster sizes. Figure 9 shows that many of the higher purity lumps are small. Macro-purity is simply the average purity for all clusters. It does not take cluster size into account by weighting the purity on average by cluster size. Three types of clusters appear when using the x-axis in Figure 9 in thirds. There are very high purity lumps that are easy to find. In the middle there are some smaller clusters that have different purity grades."}, {"heading": "8 Future Work", "text": "Work in this area is divided into two categories: XML mining and K-tree. Further work in the XML mining area will include a better representation of the structure. For example, linking information via a modified measure of similarity for documents can be included in clustering. Future work with the K-tree algorithm will include various strategies to improve the quality of cluster results. This requires additional maintenance of the K-tree during its construction. For example, the reinsertion of all records can be done every time a new root is created."}, {"heading": "9 Conclusion", "text": "This paper presented, discussed and analyzed an approach to XML mining track, introduced, expanded and analyzed a new representation for linkages, combined it with text to further improve classification performance, and applied the Ktree algorithm for cluster documentation for the first time, the results show that it is well suited for this task, produces high-quality cluster solutions, and delivers superior performance."}], "references": [{"title": "The Wikipedia XML Corpus", "author": ["L. Denoyer", "P. Gallinari"], "venue": "SIGIR Forum", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "K-tree: a height balanced tree structured vector quantizer", "author": ["S. Geva"], "venue": "Proceedings of the 2000 IEEE Signal Processing Society Workshop Neural Networks for Signal Processing X, 2000. 1", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Document clustering based on non-negative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "SIGIR \u201903: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, New York, NY, USA, ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "A hybrid unsupervised approach for document clustering", "author": ["M. Surdeanu", "J. Turmo", "A. Ageno"], "venue": "KDD \u201905: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, New York, NY, USA, ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Ontologies improve text document clustering", "author": ["A. Hotho", "S. Staab", "G. Stumme"], "venue": "Third IEEE International Conference on Data Mining, 2003. ICDM 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "A comparison of document clustering techniques", "author": ["M. Steinbach", "G. Karypis", "V. Kumar"], "venue": "34", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "Text categorization with Support Vector Machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "Text categorization with Support Vector Machines: Learning with many relevant features.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Support vector machine active learning with applications to text classification", "author": ["S. Tong", "D. Koller"], "venue": "Journal of Machine Learning Research 2", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Extended boolean information retrieval", "author": ["G. Salton", "E.A. Fox", "H. Wu"], "venue": "Communications of the ACM 26(11)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1983}, {"title": "Simple, proven approaches to text retrieval", "author": ["S. Robertson", "K. Jones"], "venue": "Update", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "An algorithm for suffix stripping", "author": ["M. Porter"], "venue": "Program: Electronic Library and Information Systems 40(3)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Large Margin Methods for Structured and Interdependent Output Variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research 6", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Authoritative sources in a hyperlinked environment", "author": ["J.M. Kleinberg"], "venue": "J. ACM 46(5)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "The mathematical theory of communication", "author": ["C. Shannon", "W. Weaver"], "venue": "University of Illinois Press", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1949}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SODA \u201907: Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, Philadelphia, PA, USA, Society for Industrial and Applied Mathematics", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Projected Gradient Methods for Nonnegative Matrix Factorization", "author": ["C. Lin"], "venue": "Neural Computation 19(10)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "It is a subset of the XML Wikipedia corpus [1].", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "K-tree is a tree structured clustering algorithm introduced by Geva [2] in the context of signal processing.", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "at SIGIR 2003 [3].", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "Entropy has been used by many researchers [4\u20136] to measure clustering results.", "startOffset": 42, "endOffset": 47}, {"referenceID": 4, "context": "Entropy has been used by many researchers [4\u20136] to measure clustering results.", "startOffset": 42, "endOffset": 47}, {"referenceID": 5, "context": "Entropy has been used by many researchers [4\u20136] to measure clustering results.", "startOffset": 42, "endOffset": 47}, {"referenceID": 6, "context": "Similar approaches have been taken by Joachims [7] and Tong and Koller [8].", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "Similar approaches have been taken by Joachims [7] and Tong and Koller [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "Document content was represented with TF-IDF [9] and BM25 [10].", "startOffset": 45, "endOffset": 48}, {"referenceID": 9, "context": "Document content was represented with TF-IDF [9] and BM25 [10].", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "Stop words were removed and the remaining terms were stemmed using the Porter algorithm [11].", "startOffset": 88, "endOffset": 92}, {"referenceID": 9, "context": "The BM25 tuning parameters were set to the same values as used for TREC [10], K1 = 2 and b = 0.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "SVM [12] was trained with TF-IDF, BM25 and LF-IDF representations of the corpus.", "startOffset": 4, "endOffset": 8}, {"referenceID": 12, "context": "Classifying links in this way corresponds to the idea of hubs and authorities in HITS [13].", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "It is the opposite of information entropy [14].", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "The k-means++ algorithm [15] improves k-means by using the D weighting for seeding.", "startOffset": 24, "endOffset": 28}, {"referenceID": 15, "context": "The projected gradient method was used to solve the NMF problem [16].", "startOffset": 64, "endOffset": 68}], "year": 2010, "abstractText": "This paper describes the approach taken to the XML Mining track at INEX 2008 by a group at the Queensland University of Technology. We introduce the K-tree clustering algorithm in an Information Retrieval context by adapting it for document clustering. Many large scale problems exist in document clustering. K-tree scales well with large inputs due to its low complexity. It offers promising results both in terms of efficiency and quality. Document classification was completed using Support Vector Machines.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}