{"id": "1001.0833", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2010", "title": "Random Indexing K-tree", "abstract": "Random Indexing (RI) K-tree is the combination of two algorithms for clustering. Many large scale problems exist in document clustering. RI K-tree scales well with large inputs due to its low complexity. It also exhibits features that are useful for managing a changing collection. Furthermore, it solves previous issues with sparse document vectors when using K-tree. The algorithms and data structures are defined, explained and motivated. Specific modifications to K-tree are made for use with RI. Experiments have been executed to measure quality. The results indicate that RI K-tree improves document cluster quality over the original K-tree algorithm.", "histories": [["v1", "Wed, 6 Jan 2010 08:03:20 GMT  (154kb)", "https://arxiv.org/abs/1001.0833v1", "8 pages, ADCS 2009"], ["v2", "Tue, 2 Feb 2010 02:46:22 GMT  (135kb)", "http://arxiv.org/abs/1001.0833v2", "8 pages, ADCS 2009; Hyperref and cleveref LaTeX packages conflicted. Removed cleveref"]], "COMMENTS": "8 pages, ADCS 2009", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.DS", "authors": ["christopher m de vries", "lance de vine", "shlomo geva"], "accepted": false, "id": "1001.0833"}, "pdf": {"name": "1001.0833.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["chris@de-vries.id.au", "l.devine@qut.edu.au", "s.geva@qut.edu.au"], "sections": [{"heading": null, "text": "ar Xiv: 100 1.08 33v2 [cs.IR] 2 Feb 201 0keywords random indexing, K-tree, dimensionality reduction, B-tree, search tree, clustering, document clustering, vector quantification, k-mean"}, {"heading": "1 Introduction", "text": "Documents are typically represented in vector space as very sparse high-dimensional vectors, and RI can reduce the dimensionality and conciseness of this representation. Conversely, compressed representation is highly effective when working with K-Tree. The paper focuses on determining the effectiveness of using RI with K-Tree through experimentation and comparative analysis of results.Sections 2 to 6 cover K-Tree, Random Indexing, Document Representation, Experimental Setup, and Experimental Results. The paper ends with a conclusion in Section 7."}, {"heading": "2 K-tree", "text": "K-tree [6, 1] is a height balanced cluster tree. It was first introduced in the context of signal processing by GevaProceedings of the 14th Australasian Document Computing Symposium, Sydney, Australia, on December 4, 2009. Copyright for this article remains with the authors. [10] Due to its low complexity, the algorithm is particularly suitable for clustering large collections. It is a hybrid of the B + tree and k-mean algorithm. The B + tree algorithm is modified to work with multidimensional vectors and k-means to perform node splits in the tree. K-tree is also related to the Tree Structured Vector Quantization (TSVQ). TSVQ splits the data set recursively, in a top-down manner using k-means. TSVQ does not generally generate its efficiency by executing the high costs of this hierarchy."}, {"heading": "2.1 K-tree and Document Clustering", "text": "The K-tree algorithm, due to its low time complexity, is well suited for clustering large collections of documents; the time complexity of the structure of the K-tree is O (n log n), where n is the number of bytes of data to cluster, due to the inherent divide-and-conquer properties of the search tree. De Vries and Geva [5, 6] investigate the runtime performance and quality of the K-tree by comparing the results with other INEX submissions and CLUTO [13]. CLUTO is a popular cluster toolkit used in the information gathering community. K-tree has been compared with k means, including the CLUTO implementation, and provides comparable quality and a significant increase in runtime performance. However, K-tree does not form a hierarchy of clusters and k means. Comparison of the quality of the tree structure is conducted in further research, with the runtime increase of the K tree being most noteworthy when a large number of clusters is required."}, {"heading": "2.2 K-tree Definition", "text": "K-tree forms a nearest search tree over a series of real estimated vectors V in d-dimensional space. The tree nodes N consist of a sequence of (vector, child) length pairs l. The tree order m limits the number of vectors stored in each node to one and m.1 \u2264 l \u2264 m (2) N = < (v1, c1),..., (vl, cl) > (3) The tree consists of two types of nodes. Leaf nodes contain the data vectors inserted into the tree. Internal nodes contain clusters. A cluster vector is the mean value of all data vectors contained in the leaves of all subsequent nodes (i.e. the entire cluster subtree)."}, {"heading": "2.3 Modifications to K-tree", "text": "This modified version is called the \"Modified K Tree,\" and the original K Tree is called the \"Unmodified K Tree.\" All document vectors generated by RI are of unit length in the modified K Tree. Therefore, all centroids are normalized to unit length at all times. K means used for node splits in the K Tree have been modified to use randomized seeding and restart when convergence has not occurred within six iterations. In our experiments, the process always converged quickly; although it is possible to limit the number of restarts, we did not consider this necessary. The original K Tree algorithm does not modify any of the centroids; they are simply the means of the vectors they represent. K means implementation boils down to completing convergence and seed roids by disrupting the global mean. To generate two seeds, the mean is then generated in opposite directions, and the two averages are generated."}, {"heading": "2.4 K-tree Example", "text": "Figures 1 to 3 are K-tree clusters in two dimensions. 1000 points were drawn from a random normal distribution with a mean value of 1.0 and a standard deviation of 0.3. The sequence of the K-tree, m, was 11. The gray dots represent the dataset, the black dots represent the centroids and the lines represent the voronoi tessellation of the centroids. The top level of the tree is Level 1. It is the coarsest cluster formation. In this example, it splits the distribution into three parts. Level 2 is granular and splits the collection into 19 sub-clusters. The individual clusters on Level 2 can only be reached by associating the nearest cluster with a parent cluster on Level 1 of the tree. Level 3 is the lowest level in the tree, where the clusters are divided into the 4th group."}, {"heading": "2.5 Building K-tree", "text": "The K tree is dynamically built when data vectors arrive. Although the tree initially contains a single empty root node at the leaf level, vectors are inserted via a nearest neighbor search that ends at the leaf level. The root of an empty tree is a leaf, so the first m data vectors are stored in the root at which point the node becomes full. When the m + 1 vector arrives, the root is split by k means, where k = 2, all m + 1 vectors are contained in two clusters. The two centroids resulting from k means then become the centroids in a new root. The vectors associated with each centroid are placed in a child node. This conveyance process has created a new root and two leaf nodes in the tree. The tree is now two levels deep. The insertion of a new data vector follows a search for the next neighbor node to find the nearest root center."}, {"heading": "2.6 Sparsity and K-tree", "text": "K-Tree was originally designed to work with dense vectors. If a sparse representation is used, performance deteriorates even though there is significantly less data to process; the clusters at the top levels of the tree are the means of most terms in the collection and are not sparse at all; the algorithm updates the cluster centers along the insertion path in the tree; because document vectors have a very high dimensionality, this becomes a very expensive process; the medoid K-Tree [6] extended the algorithm to use a sparse representation and replace the centering with document examples; this improved runtime performance and reduced memory usage; unfortunately, it has reduced quality by using sparse document vectors; the document examples at the root of the tree were almost orthogonal to new documents that are inserted; the documents were unlikely to result in significant overlaps in word choice; the approach that De Vries and GeINva used in 2008 EX Reduction [5] is a simpler one to select or a feature approach."}, {"heading": "3 Random Indexing", "text": "Random Indexing (RI) [18] is an efficient, scalable and incremental approach to the word-space model. Word-space models use the distribution of terms to create high-dimensional document vectors, the directions of which represent various semantic meanings and contexts. Dimensionality reduction is achieved by projecting the document term occurrence vectors onto the subspace, which is spanned by the vectors with the largest peculiar values in decomposition. This projection is optimal in the sense that it minimizes the deviation between the original matrix and the projected matrix. Random indexing, on the other hand, first generates random context vectors of lower dimensionality and then composes them to produce a pre-occurrence."}, {"heading": "3.1 Random Indexing Definition", "text": "The index vectors consist of randomly distributed + 1 and -1 values in non-zero dimensions. Alternatively, I can point to a random projection matrix. Each row vector in D represents a document, each row vector in I is an index vector, n is the number of documents, t is the number of terms and r is the dimensionality of the reduced matrix. R is the reduced matrix in which each row vector in D represents a document, n is the number of documents, t is the number of terms and r is the dimensionality of the reduced spacing. R is the reduced matrix in which each row vector in D represents a document."}, {"heading": "3.2 Choice of Index Vectors", "text": "Ternary index vectors for RI were introduced by Achlioptas [2] as being well suited for database environments, and the primary concern of sparse index vectors is to reduce time and space complexity. Bingham and Mannila [4] conduct experiments that show that sparse index vectors do not affect the quality of results, which is not the only choice when creating index vectors. Kanerva [12] introduces binary late codes. Table [15] examines holographic reduced representations consisting of dense vectors with floating-point values."}, {"heading": "3.3 Random Indexing Example", "text": "In practice, to construct a document vector, the document vector is first set to zero and then the sparse index vector for each term in the document is added to the document vector. The weight of the added term index vector can be determined by TF-IDF or some other weighting scheme. If all terms are added, the document vector is normalized to unit length. There is no need to explicitly build the random projection matrix in Equation 4 in advance. Random index vectors for each term can be created and stored as they occur for the first time. The fact that each index vector is sparse means that the vectors consume less memory and adhere faster. The effect of this approach is that each document has a specific signature that can be compared with other documents via cosmic similarity."}, {"heading": "3.4 Random Indexing K-tree", "text": "The time complexity of the K-tree depends on the length of the document vectors. Inserting the K-tree incurs two costs: finding the appropriate leaf node for insertion and calling the k-center during the node split. Therefore, it is desirable to operate with a lower dimensional vector representation. Combining RI with K-tree is a good addition. Both algorithms work in online and incremental mode, allowing you to track the distribution of data as it arrives and changes over time. Insertions and deletions of the K-tree provide flexibility in tracking data in volatile and changing collections. In addition, K-tree works best with dense vectors such as are generated by RI."}, {"heading": "4 Document Representation", "text": "It contains 114,366 documents representing a subset of the XML Wikipedia corpus [8]. 15 different categories were provided for the documents, the content of the documents was represented with BM25 [17], stopwords were removed, and the remaining terms were derived using the Porter algorithm [16]. BM25 is determined by the term distributions within each document and the entire collection. BM25 works with concepts similar to TF-IDF, except that there are two tuning parameters. BM25 tuning parameters were set to the same values used for TREC [17], K1 = 2 and b = 0.75. K1 affects the effect of term frequency and b influences document length. Links were represented with LF-IDF [5], resulting in a document-to-link matrix."}, {"heading": "5 Experimental Setup", "text": "Experiments were performed to measure the difference in quality between different configurations of the K tree. Section 2.3 describes the changes to K tree. Table 1 lists all the tested configurations. The following conditions were used to perform the experiments. 1. Each K tree configuration was performed a total of 20 times. 2. The documents were inserted in a different random order each time K tree was built. 3. When RI was used, the index vectors were built statistically independent each time K tree was built. 4. For each K tree erected, 20 times were executed on the codebook vectors to create 15 clusters. 5. All document vectors were unified after performing Dimensionality Reduction. The above conditions resulted in 400 measurements for each K tree configuration being incorrect."}, {"heading": "6 Experimental Results", "text": "Tables 3 to 7 contain results for the K-tree configurations listed in Table 1. Table 2 lists the meaning of the symbols used. Figures 6 and 7 are graphical representations of average micro-purity and entropy. The unmodified K-tree with TF-IDF cultivation and BM25 had unexpected results as in Table 3. Average micro-purity and entropy peaked at 400 dimensions. Implementation of this dimensionality reduction at these lower dimensions was not foreseen. This is an interesting and unexpected result and future experiments must determine whether the phenomenon occurs in different companies. Improvements in micro-purity were tested for significance via t tests. The null hypothesis is that both results come from the same distribution using the same means. In this case, they are not significantly different. If the null hypothesis is rejected, the difference is statistically significant and significant."}, {"heading": "6.1 INEX Results", "text": "The INEX XML Mining Track is a collaborative evaluation forum where research teams improve approaches to supervised and unattended machine learning using XML documents. Participants submit applications and the evaluation results are published later. On average, the RI K tree in Configuration E performs at a level similar to the best results submitted to the INEX 2008 XML Mining Track. The two best results from the track showed a micro-purity of 0.49 and 0.50, which are not averages for the approaches, but the best results found by participants. The RI K tree in Configuration E had a maximum micro-entropy of 0.55, which is 10% higher than the INEX applications."}, {"heading": "7 Conclusion", "text": "The results show that the RI K tree improves the quality of cluster outcomes, even compared to the unexpected outcomes found in the TFIDF cull. Further experiments are needed to determine whether the unexpected effect of the TF-IDF cull at low dimensions is an anomaly or is actually present in many collections. Furthermore, the RI K tree is an efficient and high-quality approach to overcome previous problems with sparse representation when using the K tree. Unfortunately, the combination of BM25 and LF-IDF representations has not improved cluster formation outcomes, as was the case with previous classification results."}], "references": [{"title": "Database-friendly random projections: Johnson-Lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of Computer and System Sciences, Volume 66, Number 4, pages 671\u2013687", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SODA \u201907: Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1027\u20131035, Philadelphia, PA, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["E. Bingham", "H. Mannila"], "venue": "KDD \u201901: Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 245\u2013250, New York, NY, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Document clustering with k-tree", "author": ["C.M. De Vries", "S. Geva"], "venue": "Advances in Focused Retrieval: 7th International Workshop of the Initiative for the Evaluation of XML Retrieval, INEX 2008, Dagstuhl Castle, Germany, December 15-18, 2008. Revised and Selected Papers, pages 420\u2013431", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "K-tree: large scale document clustering", "author": ["C.M. De Vries", "S. Geva"], "venue": "SIGIR \u201909: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 718\u2013 719, New York, NY, USA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science, Volume 41, Number 6, pages 391\u2013407", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1990}, {"title": "The Wikipedia XML Corpus", "author": ["L. Denoyer", "P. Gallinari"], "venue": "SIGIR Forum", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Vector quantization and signal compression", "author": ["A. Gersho", "R.M. Gray"], "venue": "Kluwer Academic Publishers", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "K-tree: a height balanced tree structured vector quantizer", "author": ["S. Geva"], "venue": "Proceedings of the 2000 IEEE Signal Processing Society Workshop Neural Networks for Signal Processing X, 2000., Volume 1, pages 271\u2013280 vol.1", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Contemporary mathematics, Volume 26, Number 189-206, pages 1\u20131", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1984}, {"title": "The spatter code for encoding concepts at many levels", "author": ["P. Kanerva"], "venue": "ICANN94, Proceedings of the International Conference on Artificial Neural Networks", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1994}, {"title": "CLUTO-A Clustering Toolkit", "author": ["G. Karypis"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Least squares quantization in pcm", "author": ["S. Lloyd"], "venue": "Information Theory, IEEE Transactions on, Volume 28,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1982}, {"title": "Distributed representations and nested compositional structure", "author": ["T.A. Plate"], "venue": "Ph.D. thesis", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1994}, {"title": "An algorithm for suffix stripping", "author": ["M.F. Porter"], "venue": "Program: Electronic Library and Information Systems, Volume 40, Number 3, pages 211\u2013218", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Simple", "author": ["S.E. Robertson", "K.S. Jones"], "venue": "proven approaches to text retrieval. Update", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "An introduction to random indexing", "author": ["M. Sahlgren"], "venue": "Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE 2005", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Human behavior and the principle of least effort: An introduction to human ecology", "author": ["G.K. Zipf"], "venue": "Addison- Wesley Press", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1949}], "referenceMentions": [{"referenceID": 4, "context": "K-tree [6, 1] is a height balanced cluster tree.", "startOffset": 7, "endOffset": 13}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "K-tree is also related to Tree Structured Vector Quantization (TSVQ) [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 3, "context": "De Vries and Geva [5, 6] investigate the run-time performance and quality of K-tree by comparing results with other INEX submissions and CLUTO [13].", "startOffset": 18, "endOffset": 24}, {"referenceID": 4, "context": "De Vries and Geva [5, 6] investigate the run-time performance and quality of K-tree by comparing results with other INEX submissions and CLUTO [13].", "startOffset": 18, "endOffset": 24}, {"referenceID": 11, "context": "De Vries and Geva [5, 6] investigate the run-time performance and quality of K-tree by comparing results with other INEX submissions and CLUTO [13].", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "Upon construction of the tree, a nearest neighbour search tree is built in a bottom-up manner by splitting full nodes using k-means [14] where k = 2.", "startOffset": 132, "endOffset": 136}, {"referenceID": 4, "context": "The medoid K-tree [6] extended the algorithm to use a sparse representation and replace centroids with document examples.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "The approach taken by De Vries and Geva at INEX 2008 [5] is a simple approach to dimensionality reduction or feature selection.", "startOffset": 53, "endOffset": 56}, {"referenceID": 17, "context": "This works particularly well with term occurrences due to the Zipf law distribution of terms [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 16, "context": "Random Indexing (RI) [18] is an efficient, scalable and incremental approach to the word space model.", "startOffset": 21, "endOffset": 25}, {"referenceID": 5, "context": "Latent Semantic Analysis (LSA) [7] is a popular word space model.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "Nearly orthogonal vectors can be used and have been found to perform similarly [4].", "startOffset": 79, "endOffset": 82}, {"referenceID": 9, "context": "The Johnson and Linden-Strauss lemma [11] states that if points are projected into a randomly selected subspace of sufficiently high dimensionality, then the distances between the points are approximately preserved.", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "Ternary index vectors for RI were introduced by Achlioptas [2] as being well suited for database environments.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "Bingham and Mannila [4] run experiments indicating", "startOffset": 20, "endOffset": 23}, {"referenceID": 10, "context": "Kanerva [12] introduces binary spatter codes.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "Plate [15] explores Holographic Reduced Representations that consist of dense vectors with floating point values.", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "It contains 114,366 documents that are a subset of the XML Wikipedia corpus [8].", "startOffset": 76, "endOffset": 79}, {"referenceID": 15, "context": "Document content was represented with BM25 [17].", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "Stop words were removed and the remaining terms were stemmed using the Porter algorithm [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "The BM25 tuning parameters were set to the same values as used for TREC [17], K1 = 2 and b = 0.", "startOffset": 72, "endOffset": 76}, {"referenceID": 3, "context": "Links were represented using LF-IDF [5].", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "De Vries and Geva [5] found that normalising link frequencies decreased classification performance.", "startOffset": 18, "endOffset": 21}, {"referenceID": 3, "context": "De Vries and Geva [5] found this to be effective for classification.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "For each K-tree built, k-means++ [3] was run 20 times on the codebook vectors to create 15 clusters.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "The same approach was taken at INEX 2008 by De Vries and Geva [5].", "startOffset": 62, "endOffset": 65}], "year": 2017, "abstractText": "Random Indexing (RI) K-tree is the combination of two algorithms for clustering. Many large scale problems exist in document clustering. RI K-tree scales well with large inputs due to its low complexity. It also exhibits features that are useful for managing a changing collection. Furthermore, it solves previous issues with sparse document vectors when using Ktree. The algorithms and data structures are defined, explained and motivated. Specific modifications to Ktree are made for use with RI. Experiments have been executed to measure quality. The results indicate that RI K-tree improves document cluster quality over the original K-tree algorithm.", "creator": "LaTeX with hyperref package"}}}