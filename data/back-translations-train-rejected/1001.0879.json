{"id": "1001.0879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2010", "title": "Linear Probability Forecasting", "abstract": "Multi-class classification is one of the most important tasks in machine learning. In this paper we consider two online multi-class classification problems: classification by a linear model and by a kernelized model. The quality of predictions is measured by the Brier loss function. We suggest two computationally efficient algorithms to work with these problems and prove theoretical guarantees on their losses. We kernelize one of the algorithms and prove theoretical guarantees on its loss. We perform experiments and compare our algorithms with logistic regression.", "histories": [["v1", "Wed, 6 Jan 2010 12:40:13 GMT  (44kb)", "http://arxiv.org/abs/1001.0879v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fedor zhdanov", "yuri kalnishkan"], "accepted": false, "id": "1001.0879"}, "pdf": {"name": "1001.0879.pdf", "metadata": {"source": "CRF", "title": "Linear Probability Forecasting", "authors": ["Fedor Zhdanov", "Yuri Kalnishkan"], "emails": ["fedor@cs.rhul.ac.uk", "yura@cs.rhul.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 100 1.08 79v1 [cs.LG] Multi-class classification is one of the most important tasks in machine learning. In this paper, we look at two online multi-class classification problems: classification by a linear model and by a kernelized model. The quality of predictions is measured by the Brier loss function. We propose two computationally efficient algorithms to work with these problems and prove theoretical guarantees for their losses. We nuclearize one of the algorithms and prove theoretical guarantees for their loss. We conduct experiments and compare our algorithms with logistic regression."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Framework", "text": "A prediction game contains three components: a room for results, a decision room for results and a loss function. (...) A prediction game contains three components: a room for results, a decision room and a loss function. (...) We are interested in the generalization of the Brier game of Brier (1950), where the result room is a hyperplane in d-dimensional space economics, which contains all results, and for any result we define the losses (...). (...) Prediction results (...). (...) Prediction results (...). (...) Prediction results (...). (...) Prediction results (...)."}, {"heading": "3 Derivation of the algorithms", "text": "In this section, we describe how we apply the aggregation algorithm (AA) proposed in Vovk (1990) to mix experts and make predictions; the algorithm maintains the weights Pt \u2212 1 (d\u03b1) for the experts in each prediction step and updates them through the exponential weight scheme after the actual results are announced: Pt (d\u03b1) = \u03b2 \u03bb (yt)))) Pt \u2212 1 (d\u03b1), \u03b2 (0, 1). (2) Here \u03b2 = e \u2212 \u03b7, where \u03b7 (0, \u221e) is a parameter for the learning rate. This weight update ensures that the experts who predict badly in this step do not get less weight; the weights are then normalised P \u2012 t (d\u03b1) = Pt (d\u03b1) Pt (\u0432).The prediction of the algorithm is a combination of the experts \"predictions. It is proposed in Kivinen and Warmuth (1999) that the expert's prediction is simply the weighted average."}, {"heading": "3.1 Proof of mixability", "text": "In this section, we prove that our game is perfectly miscible and can have a function that can be used to make predictions that are satisfactory (4). It is shown in Theorem \u03b2 1 Vovk and Zhdanov (2008) that the Brier game is perfectly miscible with a finite number of results (0, 1). We must prove that the inequality (4) applies to our experts (1) (who can make predictions outside the probability simplex) and that our result space is simplex (the whole probability is simplex, not just its wells). Lemma 2 describes the first part, but first we must make an additional statement. The following problem shows that any vector from R d can be projected into simplex without increasing the Brier wells. Lemma 2 describes the first part, but we must make an additional statement."}, {"heading": "3.2 Algorithm for multidimensional outcomes", "text": "We set the previous weight distribution P0 over the quantity."}, {"heading": "3.3 Component-wise algorithm", "text": "In this section we derive the component-by-component algorithm. It makes predictions for each component of the result (one at a time) and then combines them in a special way. First, we explain why we should not directly use the algorithm and the theoretical limit proposed in Vovk. In other words, the experts at Vovk do not allow us to take advantage of the fact that at any given moment only one result is possible. They are better suited for the case that each input vector x can belong to many classes at the same time, in the case of classification. In other words, they are centered around the center 1 / 2 of the prediction intervals."}, {"heading": "4 Theoretical bound", "text": "We derive the theoretical limits for the losses of algorithm 1 and a naive component-based algorithm that predicts in the same framework."}, {"heading": "4.1 Component-wise algorithm", "text": "It is easy to prove the following statement (Lemma 1 of Vovk (2001): Lemma 6 \u2212 If the learner follows the aggregating algorithm in a perfectly miscible game, then for each positive integer T each sequence of results is the elength T, and for each initial weight distribution to experts P0 (d\u03b1) it suffers a loss that is satisfactory for each one of them. (7) We proceed from production in T = 0 the inequality is obvious, and for T > 0 we have a loss: LT (AA, P0) \u2264 log\u00df \u03b2LT (AA, P0) -1 (AA, P0). (7) The production in T = 0 the inequality is obvious, and for T > 0 we have: LT (AA, P0) \u2264 LT \u2212 1 (AA, P0)."}, {"heading": "4.2 Linear forecasting", "text": "The theoretical limit for the loss of algorithm 1 is Theorem 3. If we can derive a greater theoretical limit than a greater one (AAR 3) (AAR 2), then for each a > 0, each positive integer number T, each sequence of results of length T and each \u03b1-number Rn (d \u2212 1) mAAR (2a) satisfiesLT (mAAR (2a)) \u2264 LT (\u03b1) + 2a-number \u03b1 22 + n (d \u2212 1) 2 ln (TX2a + 1). (10) Proof. We apply mAAR with the parameter b = 2a. Remember that C = \u2211 Tt = 1 xtx \u2032. After the line of proof of theorem 1 with \u03b7 = 1, we obtain the theoretical limit. We can derive a slightly better theoretical limit: In the determinant of A, we should subtract the second block raw from the first and then subtract the first block column from the second \u2212 fixing n."}, {"heading": "5 Kernelization", "text": "In some cases, the linear model cannot be considered sufficiently rich to describe data well, and a more complicated model is needed. We use a popular trick in computer learning that was first applied to the AAR in Gammerman et al. (2004). We derive an algorithm that competes with all function groups of an RKHS with d \u2212 1 elements."}, {"heading": "5.1 Derivation of the algorithm", "text": "(Definition 1). Take x1,. (xn). A core function is a nonnegative function K: Rn \u00b7 Rn \u00b7 R-1,. (Definition 1). (Definition 1). (Definition 1). (Definition 1). (Definition 1). (Definition 1). (Definition 1). (Definition 1). (Definition 1). (Definition 1). (Definition 1). (Definition 1). (Definition 1). (Definition 1). (Definition). (Definition). (Definition). (Definition 2). (Definition). (Definition 2). (Definition). (Definition). (Definition 2). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition 2). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition 2). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (2). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (2). (Definition). (Definition). (Definition). (Definition). (Definition). (1). (Definition). (Definition). (1). (Definition). (Definition). (1). (Definition). (Definition). (1). (1). (Definition). (1). (Definition). (Definition). (1). (Definition). (1). (1). (Definition). (1). (Definition). (1). (1). ("}, {"heading": "5.2 Theoretical bound for the kernelized algorithm", "text": "To deduce a theoretical limit for the loss of mKAAR, we will use the following matrix determinant identity (= 1).Let B, C are like in Proposition 3, and a is a real number. Then, det (aI + BC) = det (aI + CB).Proof. The proof is by considering a block matrix identity. The main theorem follows from the property of RKHS as a representer theorem (see Schoolhead and Smola, 2002, Theorem 4.2).Theorem 4 (Representer Theorem).Denote of g: [0,] R a strictly monotonically increasing function. Assume X is an arbitrary set, and F is a reproduction kernel space of functions on X with the given kernel K: X2 \u2192 R. Assume we also have a positive integer function c: (X \u00d7 R2).R"}, {"heading": "6 Experiments", "text": "We run our algorithms on six real time series datasets. In the time series we consider it insufficient that there are no signals to the results. However, we can take vectors consisting of previous observations (we will use ten such) and use them as signals. Data set DEC-PKT1 contains an hourly value of all wide ranges of traffic between Digital Equipment Corporation and the rest of the world. Data set LBL-PKT-41 consists of observations of another hour of traffic between the Lawrence Berkeley Laboratory and the rest of the world. We transformed both datasets in such a way that each observation is the number of packets in the corresponding network during a fixed time interval of one second. The other four datasets2 (C4, E5, E8) refer to transport data. Two of them (C9, C11) contain low frequency monthly traffic measures."}, {"heading": "7 Discussion", "text": "Both algorithms have theoretical guarantees for their cumulative losses. One of the algorithms is kernel-based and a theoretical limit has been established for the kernel-based algorithm. We have conducted experiments with linear algorithms and demonstrated that they work relatively well. We have compared them with logistical regression: the benchmark algorithm that gives probability forecasts. Comparison with linear experts in the case where possible results are in a more than 2-dimensional simplex has not been widely considered by other researchers, so the comparison of theoretical limits cannot be performed. Kivinen and Warmuth's work Kivinen and Warmuth (2001) includes the case where the possible results are in a more than 2-dimensional simplex. \u2212 We cannot compare theoretical limits."}, {"heading": "Acknowledgments", "text": "The authors are grateful to Alexey Chernov, Vladimir Vovk and Alex Gammerman for their useful comments and discussions, which have been supported by the EPSRC grant EP / F002998 / 1 and the ASPIDA grant from the Cyprus Research Promotion Foundation."}], "references": [{"title": "Verification of forecasts expressed in terms of probability", "author": ["Glenn W. Brier"], "venue": "Monthly Weather Review,", "citeRegEx": "Brier.,? \\Q1950\\E", "shortCiteRegEx": "Brier.", "year": 1950}, {"title": "Prediction, learning, and games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "J. Comput. System Sci.,", "citeRegEx": "Freund and Schapire.,? \\Q1997\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1997}, {"title": "On-line prediction with kernels and the complexity approximation principle", "author": ["Alexander Gammerman", "Yuri Kalnishkan", "Vladimir Vovk"], "venue": "In UAI,", "citeRegEx": "Gammerman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gammerman et al\\.", "year": 2004}, {"title": "Matrix algebra from a statistician\u2019s perspective", "author": ["David A. Harville"], "venue": null, "citeRegEx": "Harville.,? \\Q1997\\E", "shortCiteRegEx": "Harville.", "year": 1997}, {"title": "Sequential prediction of individual sequences under general loss functions", "author": ["David Haussler", "Jyrki Kivinen", "Manfred K. Warmuth"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Haussler et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Haussler et al\\.", "year": 1998}, {"title": "Ridge regression: Biased estimation for nonorthogonal problems", "author": ["Arthur E. Hoerl", "Robert W. Kennard"], "venue": null, "citeRegEx": "Hoerl and Kennard.,? \\Q2000\\E", "shortCiteRegEx": "Hoerl and Kennard.", "year": 2000}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["Jyrki Kivinen", "Manfred K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1997\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1997}, {"title": "Averaging expert predictions", "author": ["Jyrki Kivinen", "Manfred K. Warmuth"], "venue": "In Computational learning theory (Nordkirchen,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1999\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1999}, {"title": "Relative loss bounds for multidimensional regression problems", "author": ["Jyrki Kivinen", "Manfred K. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Kivinen and Warmuth.,? \\Q2001\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 2001}, {"title": "A finite algorithm for finding the projection of a point onto the canonical simplex of rn", "author": ["C Michelot"], "venue": "J. Optim. Theory Appl.,", "citeRegEx": "Michelot.,? \\Q1986\\E", "shortCiteRegEx": "Michelot.", "year": 1986}, {"title": "Learning with kernels: Support Vector Machines, regularization, optimization, and beyond", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola.,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola.", "year": 2002}, {"title": "Aggregating strategies", "author": ["Vladimir Vovk"], "venue": "In Proceedings of the Third Annual Workshop on Computational Learning Theory,", "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}, {"title": "A game of prediction with expert advice", "author": ["Vladimir Vovk"], "venue": "J. Comput. System Sci.,", "citeRegEx": "Vovk.,? \\Q1998\\E", "shortCiteRegEx": "Vovk.", "year": 1998}, {"title": "Competitive on-line statistics", "author": ["Vladimir Vovk"], "venue": "International Statistical Review,", "citeRegEx": "Vovk.,? \\Q2001\\E", "shortCiteRegEx": "Vovk.", "year": 2001}, {"title": "Prediction with expert advice for the Brier game", "author": ["Vladimir Vovk", "Fedor Zhdanov"], "venue": "Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Vovk and Zhdanov.,? \\Q2008\\E", "shortCiteRegEx": "Vovk and Zhdanov.", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "We consider the square loss: mean square error is one of the benchmark measures for classification algorithms (see Brier, 1950). We use Vovk\u2019s Aggregating Algorithm (a generalization of the Bayesian mixture) to mix functions (as in Aggregating Algorithm Regression, AAR: see Vovk, 2001). This method has previously been applied to the case when possible outcomes lie in a segment of the real line, and so the prediction was one-dimensional. We develop two algorithms to solve the problem of multi-dimensional prediction. The first algorithm applies a variant of AAR to predict each coordinate of the outcome separately, and then combines these predictions in a certain way to get probability prediction. The other algorithm is designed to give probability predictions directly; these are first computationally efficient online regression algorithm designed to solve linear and non-linear multi-class classification problems. We derive theoretical bounds on the losses of both algorithms. We come to an unexpected conclusion that the component-wise algorithm is better than the second one asymptotically, but worse in the beginning of the prediction process. Their performance on benchmark data sets is very similar. One component of the prediction of the second algorithm has the meaning of a remainder. In practice this situation is quite common. For example, in a football match either one team wins or the other, and the remainder is a draw (see Vovk and Zhdanov (2008) for online prediction experiments in football).", "startOffset": 115, "endOffset": 1473}, {"referenceID": 0, "context": "We are interested in the generalisation of the Brier game from Brier (1950) where the space of outcomes \u03a9 = P(\u03a3) is the set of all probability measures on a finite set \u03a3 with d elements, \u0393 := {(\u03b31, .", "startOffset": 47, "endOffset": 76}, {"referenceID": 12, "context": "In this section we describe how we apply the Aggregating Algorithm (AA) proposed in Vovk (1990) to mix experts and make predictions.", "startOffset": 84, "endOffset": 96}, {"referenceID": 7, "context": "It is suggested in Kivinen and Warmuth (1999) that the prediction is simply the weighted average of the experts\u2019 predictions with weights Pt(d\u03b1).", "startOffset": 19, "endOffset": 46}, {"referenceID": 12, "context": "Perfectly mixable games and other types of games are analyzed in Vovk (1998). It is also shown there that for countable (and thus finite) number of experts the AA achieves the best possible theoretical guarantees.", "startOffset": 65, "endOffset": 77}, {"referenceID": 11, "context": "It is shown in Theorem 1 Vovk and Zhdanov (2008) that the Brier game with finite number of outcomes is perfectly mixable iff \u03b7 \u2208 (0, 1].", "startOffset": 25, "endOffset": 49}, {"referenceID": 12, "context": "By Theorem 1 in Vovk and Zhdanov (2008) such prediction exists for any \u03b7 \u2208 (0, 1] (\u03b2 \u2208 [e, 1)).", "startOffset": 16, "endOffset": 40}, {"referenceID": 11, "context": "Such a function is proposed in Vovk and Zhdanov (2008). This is an extension of Lemma 4.", "startOffset": 31, "endOffset": 55}, {"referenceID": 5, "context": "1 from Haussler et al. (1998). Lemma 3.", "startOffset": 7, "endOffset": 30}, {"referenceID": 12, "context": "First we explain why we should not directly use the algorithm and the theoretical bound proposed in Vovk (2001). Vovk\u2019s experts do not allow us to take advantage of the fact that only one outcome is possible to happen at each moment.", "startOffset": 100, "endOffset": 112}, {"referenceID": 12, "context": "Instead of the substitution function from Proposition 1 we use the substitution function suggested in Vovk (2001) for the one-dimensional game:", "startOffset": 102, "endOffset": 114}, {"referenceID": 10, "context": "We use the projection algorithm suggested in Michelot (1986).", "startOffset": 45, "endOffset": 61}, {"referenceID": 12, "context": "It is easy to prove the following statement (Lemma 1 from Vovk (2001)):", "startOffset": 58, "endOffset": 70}, {"referenceID": 12, "context": "Implications similar to the ones in the proof of Lemma 2 from Vovk (2001) lead to the inequality \u03b7 \u2264 2 (B\u2212A) .", "startOffset": 62, "endOffset": 74}, {"referenceID": 3, "context": "We use a popular in computer learning kernel trick, firstly applied to the AAR in Gammerman et al. (2004). We derive an algorithm competing with all sets of functions from an RKHS with d\u2212 1 elements.", "startOffset": 82, "endOffset": 106}, {"referenceID": 0, "context": "We presented new algorithms which give probability predictions in the Brier game. Both algorithms do not involve any numerical integration, and can be easily computed. Both algorithms have theoretical guarantees on their cumulative losses. One of the algorithms is kernelized and a theoretical bound is proven for the kernelized algorithm. We performed experiments with linear algorithms and showed that they perform relatively well. We compared them with the logistic regression: the benchmark algorithm giving probability predictions. Competing with linear experts in the case where possible outcomes lie in a more than 2-dimensional simplex was not widely considered by other researchers, so the comparison of theoretical bounds can not be performed. Kivinen and Warmuth\u2019s work Kivinen and Warmuth (2001) includes the case when the possible outcomes lie in a more than 2-dimensional simplex and their algorithm competes with all logistic regression functions.", "startOffset": 70, "endOffset": 808}, {"referenceID": 0, "context": "We presented new algorithms which give probability predictions in the Brier game. Both algorithms do not involve any numerical integration, and can be easily computed. Both algorithms have theoretical guarantees on their cumulative losses. One of the algorithms is kernelized and a theoretical bound is proven for the kernelized algorithm. We performed experiments with linear algorithms and showed that they perform relatively well. We compared them with the logistic regression: the benchmark algorithm giving probability predictions. Competing with linear experts in the case where possible outcomes lie in a more than 2-dimensional simplex was not widely considered by other researchers, so the comparison of theoretical bounds can not be performed. Kivinen and Warmuth\u2019s work Kivinen and Warmuth (2001) includes the case when the possible outcomes lie in a more than 2-dimensional simplex and their algorithm competes with all logistic regression functions. They use the relative entropy loss function L and get a regret term of the order O( \u221a LT (\u03b1)) which is upper unbounded in the worst case. Their prediction algorithm is not computationally efficient and it is not clear how to extend their results for the case when the predictors lie in an RKHS. We can prove lower bounds for the regret term of the order O( d lnT ) for the case of the linear model (1) using methods similar to ones described in Vovk (2001), and lower bounds for the regret term of the order O( \u221a T ) for the case of RKHS.", "startOffset": 70, "endOffset": 1420}], "year": 2010, "abstractText": "Multi-class classification is one of the most important tasks in machine learning. In this paper we consider two online multi-class classification problems: classification by a linear model and by a kernelized model. The quality of predictions is measured by the Brier loss function. We suggest two computationally efficient algorithms to work with these problems and prove theoretical guarantees on their losses. We kernelize one of the algorithms and prove theoretical guarantees on its loss. We perform experiments and compare our algorithms with logistic regression.", "creator": "LaTeX with hyperref package"}}}