{"id": "1001.0921", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jan-2010", "title": "Graph Quantization", "abstract": "Vector quantization(VQ) is a lossy data compression technique from signal processing, which is restricted to feature vectors and therefore inapplicable for combinatorial structures. This contribution presents a theoretical foundation of graph quantization (GQ) that extends VQ to the domain of attributed graphs. We present the necessary Lloyd-Max conditions for optimality of a graph quantizer and consistency results for optimal GQ design based on empirical distortion measures and stochastic optimization. These results statistically justify existing clustering algorithms in the domain of graphs. The proposed approach provides a template of how to link structural pattern recognition methods other than GQ to statistical pattern recognition.", "histories": [["v1", "Wed, 6 Jan 2010 15:46:03 GMT  (30kb,D)", "http://arxiv.org/abs/1001.0921v1", "24 pages; submitted to CVIU"]], "COMMENTS": "24 pages; submitted to CVIU", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["brijnesh j jain", "klaus obermayer"], "accepted": false, "id": "1001.0921"}, "pdf": {"name": "1001.0921.pdf", "metadata": {"source": "CRF", "title": "Graph Quantization", "authors": ["Brijnesh J. Jain", "Klaus Obermayer"], "emails": ["jbj@cs.tu-berlin.de", "oby@cs.tu-berlin.de"], "sections": [{"heading": "1 Introduction", "text": "The question as to why and why, as to why and why, as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why, as to why and as to why and as to why."}, {"heading": "2 The Problem of Graph Quantizer Design", "text": "This section aims to outline the problem of extending VQ to the quantization of graphs."}, {"heading": "2.1 Attributed Graphs", "text": "First of all, we describe the structures we want to quantify. Let A be a series of attributes, and let \u03b5 \u0441A be a differentiated element designating the null or empty element. An assigned graph is a tuple X = (V, \u03b1) consisting of a finite nonempty set V of vertices and an attribute function \u03b1: V \u00b7 V \u00b7 A. Elements of the assigned graph E = {(i, j) \u0445 V \u00b7 V: i \u00b7 6 and \u03b1 (i, j) 6 = \u03b5} are the edges of X. With GA we denote the set of all assigned graphs with attributes of A. The vertex of an assigned graph X is often referred to as VX and its attribute function as \u03b1X. An alignment of a graph X is a graph X \u2032 with VX \u00b2 and VX \u00b2 VX \u2032 and X \u2032 Y \u00b2 (i, j) we determine a triple density of the pigments (j)."}, {"heading": "2.2 The Graph Edit Distance", "text": "Fundamental to the quantification of data is the concept of distortion. In this section, the Edit graph Distance functions are briefly introduced as our choice of distortion measure. For a more detailed definition of the graph Edit distance, we refer to [2]. In addition, we present an important graph metrics based on a generalization of the concept of the maximum common subgraph, which occurs in various forms as a common choice of proximity measurement [1, 5, 6, 15, 32, 33]. For convenience, we assume that all distances are metrics. Any paired alignment (\u03c6, X \u2032, Y \u2032) can be considered a positive graph. A (X, Y) can be considered as an editing path with cost factor (X, Y)."}, {"heading": "2.3 The Problem of Graph Quantizer Design", "text": "Let (GA, d) be a graph spacing space, where d (\u00b7 | \u00b7) is a graph processing distance. Optimal graph quantization design aims to minimize the expected distortionD (C) = \u0445 GA d (X, Q (X)))) dP (X), where Q: GA \u2192 C is a graph quantizer, C = {Y1,..., Yk} is a code book consisting of k-codegraphs, and P = PGA is a probability measurement defined in a suitable measurable space (GA, GA). In contrast to vector quantization, the following factors complicate the statistically consistent design of an optimal graph quantizer: 1. Graph spacing d (X, Y) is generally non-convex and non-differentiable. 2. Neither a well-defined addition to graphs nor the notion of derivative functions on graphs is difficult to apply in order to overcome these points known as graph betters."}, {"heading": "3 Riemannian Orbifolds", "text": "Orbit folds generalize the notion that multiplicity is locally a quotient of Rn through finite group actions. Consequently, learning orbit folds generalizes learning in Euclidean spaces and Rieman multiplicities. This section presents Rieman orbit folds and their intrinsic metric structure. Evidence of new results is delegated to Section B.1. For all other evidence, refer to [4, 21]."}, {"heading": "3.1 Riemannian Orbifolds", "text": "To keep the treatment simple, we assume that X = Rn is the n-dimensional euclidean vector group that acts on X. However, in a more general context, we can assume that X is a rimannic multiplicity, and that x-X is a finite group of isometries that effectively act on X. The quotient group is a group of X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X"}, {"heading": "3.2 The Riemannian Orbifold of Attributed Graphs", "text": "In this section we show that attributes graphs with points in some Riemannian Orbifolds = less practicable. < A > Effects of the associated graphs arise by looking at equivalence classes of matrices representing the same graph. To identify graphs with points in a Riemannian Orbifold without loss of structural information, some technical assumptions and constraints are necessary to simplify the mathematical treatment. To do this, we leave (GA, d) a graph distance with graphs edit distance d (\u00b7 \u00b7). Then we make the following assumptions: P1 There is a characteristic map: A \u2192 H of attributes in some finite dimensional Euclidean space H and a distance function dH: H \u00b7 R + such that these attributes are distance d."}, {"heading": "3.3 Metric Structures", "text": "We derive from this an intrinsic metric that enables us to do the Riemansche geometry. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &"}, {"heading": "3.4 Orbifold Functions", "text": "Suppose Q = (X) is an orbifold function. An orbifold function is a figure f: (X) = f: (X) = f: (x). The elevator f: (X) is a function f: (X). We say an orbifold function f: (local Lipschitz, differentiable, generalizable) is continuous (local Lipschitz, differentiable, generalizable) in X: X. The definition is independent of the choice of the vector representation projected on X (see Section B.1, Section 1 - Prop. 1 - Prop. 4). For a definition of the generalized differentiable functions and their basic properties, refer to Section A. Example 7. Consider the vector representation projected on X (see Section B.1, Prop. 1 - Prop. 4)."}, {"heading": "3.5 Gradients and Generalized Gradients of Orbifold Functions", "text": "We extend the concept of x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x"}, {"heading": "3.6 Integration on Orbifolds", "text": "Suppose Q = (X, \u0430, \u03c0) is a belt orbit with a singular number SQ. Measurable space is defined by the Borel set B (XB), which is generated by the open quantities of XB. From the orbital set we assume that it is compatible with the local belt dimensions. Furthermore, we demand that the singular set SQ has the measure 0. This is motivated by the following fact: The singular set is covered locally by the finite union of perfectly geodesic submandibles, which measure 0 has relative to the local canonical Riemannic measure. Since the projection on the orbital number is the distance reduction, it is reasonable to ask for an orbital set associated with the orbital number."}, {"heading": "4 Graph Quantization", "text": "In this section, vector quantification is extended to the quantization of graphs."}, {"heading": "4.1 The Basics", "text": "Suppose that Q = (X, \u0430, \u03c0) is a Rieman circular orbit. A graph quantifier of quantity k is a mapping of the formQ: XB \u2192 Cwhere C = {Y1,..., Yk} XB is a finite set, the so-called code book. Elements Yj-C are the code diagrams. The graph quantifier Q divides the input space XB into the disk community regions Rj = {X-XB: Q (X) = Yj} so that their union covers XB. PQ refers to the partition of Q that consists of all k regions Rj. Suppose that J = {1,.., k}. The basic functioning of a vector quantifier Q can be described as the composition Q = dQ-eQ of an encoder eQ: XB = J and a decoder dQ: J \u2192 C. The encoder of each input diagram of a region over the index J."}, {"heading": "4.2 Graph Quantizer Performance", "text": "We measure the performance of a graph quantizer Q using the expected distortion D (Q) = EX [d (X, Q (X))] = nearest graph (X, Q (X))) dP (X), where X-XB is a random variable with probability measure P = PXB representing the observable graphs to be quantified. Expectation EX is taken in relation to a certain probability range (XB, XB, PXB).The quantity d (X, Y) measures the distortion of the random input diagram X and the code diagram Y. Consider here graph distortion measure, which are graph distances. An example is the square-metric distortion, which can be rewritten by an optimal alignment core field (X, Y) = min x-X, y-Y, y-x-y, y-x- with the help of the code buffer C (with the given by the Q and D) = the expected distortion."}, {"heading": "4.3 The Problem of Optimal Graph Quantizer Design", "text": "The problem of the optimal quantization design for graphs is described as follows: Find a code book C specifying the decoder dQ and a partition PQ specifying the encoder eQ to minimize the expected distortion D (Q). The composite mapping Q = dQ-eQ of the resulting encoder and decoder is then an optimal quantizer for graphs. An optimal quantizer for graphs fulfills the following necessary conditions, also known as Lloyd Max conditions: 1. Closest adjacent condition C. In a fixed code book C, a quantizer Q is optimal if the code vector Q (X) of an input pattern X meets the closest adjacent condition ruleQ (X) = argmin Y-C (X, Y) for all X-X and Y-Y connections to be resolved according to a certain rule. Proof this is given in Section B.2, Theorem X2, Centroid Condition."}, {"heading": "4.4 Graph Quantizer Design", "text": "Since the distribution P = PX2 of the observable graphs is generally unknown, the expected distortion D (C) can neither be calculated nor directly minimized. Instead, we have developed an optimal quantization from empirical data. To derive consistency for k-means and easy competitive learning in the domain of graphs, we consider estimators based on empirical distortions and stochastic approximation. To derive consistency for k-means and easy competitive learning in the domain of graphs, we consider estimators based on empirical distortions and stochastic approximation."}, {"heading": "5 Remarks to GQ using the Graph Edit Distance", "text": "A necessary (but not sufficient) condition for the consistency results given in theorems 1 and 2 is that the underlying graph distortion is based locally on Lipschitz. Therefore, both consistency results aim at approximating the expected distortion D (C) based on its empirical meaning C, WD (C) = 1N N N, N = 1 min j, J d (Xi, Yj).As shown in [10], the minimization of the empirical distortion D (C) due to its empirical distortion D (C) can often be meaningless if the underlying graph is problematic with the distance function d (\u00b7 \u00b7 \u00b7) and thus D \u00b7 N (C) if the local distance (C) is continuous."}, {"heading": "6 Conclusion", "text": "This paper proposes a theoretical solid basis for the quantization of graphs, which generalizes the ideas of vector quantizations into the area of ascribed graphs. We presented consistency results for the design of graph quantizers where the underlying graph distances are generalizable. As far as vectors are concerned, estimators based on empirical distortions and stochastic optimization lack statistical consistency of a statistical consistency. If the underlying distortion measure is a discontinuous graph processing distance, we have shown that the Lloyd-Max conditions provide the necessary conditions for the optimality of GQ.The mathematical framework that allows us to derive consistency results are Rieman orbifolds. Identifying graphs with points in a Rieman orbifold provides us with local access to a climatic space, enabling us to recognize a normative pattern again."}, {"heading": "A Generalized Differentiable Functions", "text": "A function f: X \u2192 R is therefore generalized differentiable in the sense of Norkin [27], if there is a multidimensional map that is f: X \u2192 2X in a neighborhood of x, then each accumulation point g of (gi) is a convex and compact set. (3.) For each generalizable set f (x) there is a g-dimensional f (y) with f (y) = f (y) + < g, y \u2212 x > + o (x, g), then each accumulation point g of (gi) is in f (x); 3. for each x-dimensional x there is a g-dimensional f (y) with f (y) + < g, y \u2212 x > o (x, g) where lim (lim)."}, {"heading": "B Proofs", "text": "Let us assume that Q = (X) i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i. \"Let us assume that Q = (X) i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i"}, {"heading": "1. H. Almohamad and S. Duffuaa, \"A linear programming approach for the weighted", "text": "The results of the study show that this is a method based on the weighted minimal supergraph \"Graph Based Representations in Pattern Recognition, Lecture Notes in Lecture Notes in Computer Science, 837.106-118, 1994.\" 3. H. Bunke, P. Foggia, C. Guidobaldi, and M. Vento, Graph Clustering with the weighted minimal supergraph \"Graph Based Representations in Pattern Recognition, Lecture Notes in Computer Science, 2726: 235-246, 2003 4. J.E. Borzellino, Riemannian geometry of orbifolds, PhD thesis, University of California, Los Angelos, 1992. 5. T.S. Caetano, L. Cheng, Q.V. Le, and A.J. Smola,\" Learning graph matching \"International Conference on Computer Vision, p. 1-8, 2007.6."}, {"heading": "28. A. Schenker, M. Last, H. Bunke, and A. Kandel, \"Clustering of web documents", "text": "Using a graph model, \"Web Document Analysis: Challenges and Opportunities, pp. 1-16, 2003. 29. A. Schenker, M. Last, H. Bunke, and A. Kandel, Graph-Theoretic Techniques for Web Content Mining, World Scientific Publishing, 2005. 30. S. Theodoridis and K. Koutroumbas, Pattern Recognition, Elsevier, 2009. 31. A. Torsello and E.R. Hancock,\" Learning shape-classes using a mixture of tree-unions, \"IEEE Transactions on Pattern Analysis and Machine Intelligence, 28 (6): 954-967, 2006. 32. S. Umeyama,\" An eigendecomposition approach to weighted graph matching problems, \"IEEE Transactions on Pattern Analysis and Machine Intelligence, 10 (5): 695- 703, 1988. 33. M. Van Wyk, M. Durrani, and B. Van Wyk,\" A RKHS intermatching on Pattern Pattern Analysis and E88, 2-2, EIntelligence 988, 4-5."}], "references": [{"title": "Duffuaa, \"A linear programming approach for the weighted graph matching problem", "author": ["S.H. Almohamad"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "Messmer, \"Similarity measures for structured representations", "author": ["B.T.H. Bunke"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Graph clustering using the weighted minimum common supergraph\" Graph Based Representations in Pattern Recognition, Lecture", "author": ["H. Bunke", "P. Foggia", "C. Guidobaldi", "M. Vento"], "venue": "Notes in Computer Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Riemannian geometry of orbifolds", "author": ["J.E. Borzellino"], "venue": "PhD thesis,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Learning graph matching", "author": ["T.S. Caetano", "L. Cheng", "Q.V. Le", "A.J. Smola"], "venue": "International Conference on Computer Vision, p", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Balanced graph matching", "author": ["T. Cour", "P. Srinivasan", "J. Shi"], "venue": "NIPS 2006 Conference Proceedings,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Norkin, \"Normalized convergence in stochastic optimization", "author": ["V.I.Y.M. Ermoliev"], "venue": "Annals of Operations Research,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "The minimization of discontinuous functions: mollifier subgradients", "author": ["Y.M. Ermoliev", "V.I. Norkin", "R. Wets"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Norkin, \"On nonsmooth and discontinuous problems of stochastic systems optimization", "author": ["V.I.Y.M. Ermoliev"], "venue": "European Journal of Operational Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Norkin, \"Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization", "author": ["V.I.Y.M. Ermoliev"], "venue": "Cybernetics and Systems Analysis,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Theory and algorithms on the median graph. application to graph-based classification and clustering", "author": ["M. Ferrer"], "venue": "PhD Thesis, Univ. Aut\u2018onoma de Barcelona,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Graph-Based kMeans Clustering: A Comparison of the Set Median versus the Generalized Median Graph", "author": ["M. Ferrer", "E. Valveny", "F. Serratosa", "I. Bardaj\u00ed", "H. Bunke"], "venue": "CAIP 2009 Conference Proceedings,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Vector Quantization and Signal Compression", "author": ["A. Gersho", "R.M. Gray"], "venue": "Kluwer Academic Publishers,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}, {"title": "Graduated Assignment Algorithm for Graph Matching", "author": ["S. Gold", "A. Rangarajan"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Learning with preknowledge: clustering with point and graph matching distance measures", "author": ["S. Gold", "A. Rangarajan", "E. Mjolsness"], "venue": "Neural Computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1996}, {"title": "Self-organizing map for clustering in the graph domain", "author": ["S. G\u00fcnter", "H. Bunke"], "venue": "Pattern Recognition Letters,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "\u00d2A Self-Organizing Map for Adaptive Processing of Structured Data,\u00d3", "author": ["M. Hagenbuchner", "A. Sperduti", "A.C. Tsoi"], "venue": "IEEE Transaction on Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Central Clustering of Attributed Graphs", "author": ["B. Jain", "F. Wysotzki"], "venue": "Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "On the sample mean of graphs", "author": ["B. Jain", "K. Obermayer"], "venue": "IJCNN 2008 Conference Proceedings,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Structure Spaces", "author": ["B. Jain", "K. Obermayer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Accelerating Competitive Learning Graph Quantization", "author": ["B. Jain", "K. Obermayer"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Elkan\u2019s k-Means for Graphs", "author": ["B. Jain", "K. Obermayer"], "venue": "[cs.AI],", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "\u00d2Least squares quantization in PCM\u00d3", "author": ["S.P. Lloyd"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1982}, {"title": "ACM attributed graph clustering for learning classes of images", "author": ["M.A. Lozano", "F. Escolano"], "venue": "Graph Based Representations in Pattern Recognition, Lecture Notes in Computer Science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "Stochastic generalized-differentiable functions in the problem of nonconvex nonsmooth stochastic optimization", "author": ["V.I. Norkin"], "venue": "Cybernetics, 22(6),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1986}, {"title": "Clustering of web documents using a graph model\", Web Document Analysis: Challenges and Opportunities", "author": ["A. Schenker", "M. Last", "H. Bunke", "A. Kandel"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Graph-Theoretic Techniques for Web Content Mining, World", "author": ["A. Schenker", "M. Last", "H. Bunke", "A. Kandel"], "venue": "Scientific Publishing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2005}, {"title": "Learning shape-classes using a mixture of tree-unions", "author": ["A. Torsello", "E.R. Hancock"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "An eigendecomposition approach to weighted graph matching problems", "author": ["S. Umeyama"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1988}, {"title": "A RKHS interpolator-based graph matching algorithm", "author": ["M. Van Wyk", "M. Durrani", "B. Van Wyk"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2002}], "referenceMentions": [{"referenceID": 12, "context": "Vector quantization is a classical technique from signal processing suitable for lossy data compression, density estimation, and prototype-based clustering [7, 14, 30].", "startOffset": 156, "endOffset": 167}, {"referenceID": 22, "context": "The kmeans algorithm is also commonly referred to as the Linde-Buzo-Gray (LBG) algorithm [24] the generalized Lloyd algorithm [25].", "startOffset": 126, "endOffset": 130}, {"referenceID": 14, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 73, "endOffset": 84}, {"referenceID": 15, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 73, "endOffset": 84}, {"referenceID": 16, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 73, "endOffset": 84}, {"referenceID": 17, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 73, "endOffset": 84}, {"referenceID": 18, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 73, "endOffset": 84}, {"referenceID": 20, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 73, "endOffset": 84}, {"referenceID": 10, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 11, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 17, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 18, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 21, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 25, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 26, "context": "Examples include competitive learning algorithms in the domain of graphs [16\u201320, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].", "startOffset": 129, "endOffset": 157}, {"referenceID": 2, "context": "Related clustering method are presented in [3, 26, 31].", "startOffset": 43, "endOffset": 54}, {"referenceID": 23, "context": "Related clustering method are presented in [3, 26, 31].", "startOffset": 43, "endOffset": 54}, {"referenceID": 27, "context": "Related clustering method are presented in [3, 26, 31].", "startOffset": 43, "endOffset": 54}, {"referenceID": 1, "context": "For a more detailed definition of the graph edit distance, we refer to [2].", "startOffset": 71, "endOffset": 74}, {"referenceID": 0, "context": "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].", "startOffset": 200, "endOffset": 221}, {"referenceID": 4, "context": "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].", "startOffset": 200, "endOffset": 221}, {"referenceID": 5, "context": "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].", "startOffset": 200, "endOffset": 221}, {"referenceID": 13, "context": "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].", "startOffset": 200, "endOffset": 221}, {"referenceID": 28, "context": "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].", "startOffset": 200, "endOffset": 221}, {"referenceID": 29, "context": "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].", "startOffset": 200, "endOffset": 221}, {"referenceID": 21, "context": "As shown in [23], d is indeed a metric and can be expressed as a graph edit distance.", "startOffset": 12, "endOffset": 16}, {"referenceID": 3, "context": "For all other proofs we refer to [4, 21].", "startOffset": 33, "endOffset": 40}, {"referenceID": 19, "context": "For all other proofs we refer to [4, 21].", "startOffset": 33, "endOffset": 40}, {"referenceID": 3, "context": "For proofs we refer to [4].", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": "The proof follows from [8] applied to the lift d\u0303 of distortion d.", "startOffset": 23, "endOffset": 26}, {"referenceID": 9, "context": "under mild assumptions (see [11, 27]), we can minimize the expected distortion D(C) according to the following stochastic generalized gradient (SGG) method: yt+1 = yt + \u03b7t (xt \u2212 yt), (4)", "startOffset": 28, "endOffset": 36}, {"referenceID": 24, "context": "under mild assumptions (see [11, 27]), we can minimize the expected distortion D(C) according to the following stochastic generalized gradient (SGG) method: yt+1 = yt + \u03b7t (xt \u2212 yt), (4)", "startOffset": 28, "endOffset": 36}, {"referenceID": 9, "context": "The proof is a direct consequence of Ermoliev and Norkin\u2019s Theorem [11] applied on the lift d\u0303 (\u00b7|\u00b7) of d (\u00b7|\u00b7).", "startOffset": 67, "endOffset": 71}, {"referenceID": 8, "context": "As shown in [10], minimizing the empirical distortion is often meaningless, if the underlying graph edit distance function d (\u00b7|\u00b7) and thus D\u0302N (C) is discontinuous, even if the expectation D(C) may be continuously differentiable.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "For details, we refer to [9].", "startOffset": 25, "endOffset": 28}, {"referenceID": 24, "context": "A function f : X \u2192 R is generalized differentiable at x \u2208 X in the sense of Norkin [27] if there is a multi-valued map \u2202f : X \u2192 2X in a neighborhood of x such that", "startOffset": 83, "endOffset": 87}, {"referenceID": 24, "context": "Generalized differentiable functions have the following properties [27]:", "startOffset": 67, "endOffset": 71}], "year": 2010, "abstractText": "Vector quantization(VQ) is a lossy data compression technique from signal processing, which is restricted to feature vectors and therefore inapplicable for combinatorial structures. This contribution presents a theoretical foundation of graph quantization (GQ) that extends VQ to the domain of attributed graphs. We present the necessary Lloyd-Max conditions for optimality of a graph quantizer and consistency results for optimal GQ design based on empirical distortion measures and stochastic optimization. These results statistically justify existing clustering algorithms in the domain of graphs. The proposed approach provides a template of how to link structural pattern recognition methods other than GQ to statistical pattern recognition.", "creator": "TeX"}}}