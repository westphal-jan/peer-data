{"id": "1001.2267", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jan-2010", "title": "Speech Recognition by Machine, A Review", "abstract": "This paper presents a brief survey on Automatic Speech Recognition and discusses the major themes and advances made in the past 60 years of research, so as to provide a technological perspective and an appreciation of the fundamental progress that has been accomplished in this important area of speech communication. After years of research and development the accuracy of automatic speech recognition remains one of the important research challenges (e.g., variations of the context, speakers, and environment).The design of Speech Recognition system requires careful attentions to the following issues: Definition of various types of speech classes, speech representation, feature extraction techniques, speech classifiers, database and performance evaluation. The problems that are existing in ASR and the various techniques to solve these problems constructed by various research workers have been presented in a chronological order. Hence authors hope that this work shall be a contribution in the area of speech recognition. The objective of this review paper is to summarize and compare some of the well known methods used in various stages of speech recognition system and identify research topic and applications which are at the forefront of this exciting and challenging field.", "histories": [["v1", "Wed, 13 Jan 2010 19:02:18 GMT  (1600kb)", "http://arxiv.org/abs/1001.2267v1", "25 pages IEEE format, International Journal of Computer Science and Information Security, IJCSIS December 2009, ISSN 1947 5500,this http URL"]], "COMMENTS": "25 pages IEEE format, International Journal of Computer Science and Information Security, IJCSIS December 2009, ISSN 1947 5500,this http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["m a anusuya", "s k katti"], "accepted": false, "id": "1001.2267"}, "pdf": {"name": "1001.2267.pdf", "metadata": {"source": "CRF", "title": "Speech Recognition by Machine: A Review", "authors": [], "emails": [], "sections": [{"heading": null, "text": "This paper presents a brief overview of automatic speech recognition and discusses the main topics and advances made in the last 60 years of research to provide a technological perspective and an appreciation of the fundamental progress achieved in this important area of speech communication. After years of research and development, the accuracy of automatic speech recognition remains one of the major research challenges (e.g. variations in context, speaker and environment).The design of the speech recognition system requires careful attention to the following topics: definition of different types of language courses, speech representation, feature extraction techniques, language classifiers, database and performance evaluation. The problems that exist in ASR and the different techniques for solving these problems developed by different researchers have been presented in chronological order. Therefore, the authors hope that this work will contribute to the field of speech recognition. The aim of this paper is to summarize and compare some of the known methods used at different stages of the speech recognition system."}, {"heading": "A. Definition of speech recognition:", "text": "Speech recognition (also known as automatic speech recognition (ASR) or computer speech recognition) is the process of converting a speech signal into a sequence of words using an algorithm implemented as a computer program."}, {"heading": "1.2 Basic Model of Speech Recognition:", "text": "Research in speech processing and communication, for the most part, has been motivated by people's desire to build mechanical models to mimic human verbal communication skills. Language is the most natural form of human communication and speech processing has been one of the most exciting areas of signal processing. Speech recognition technology has enabled the computer to follow human voice commands and understand human languages. The main goal of the speech recognition level is to develop techniques and systems for speech input to machines. Language is the primary means of communication between humans. For reasons ranging from technological curiosity to mechanisms for the mechanical realization of human speech skills to the desire to automate simple tasks requiring human machine interactions and research in automatic speech recognition by machines, P has attracted great attention for sixty years [76]. Based on significant advances in statistical modeling of speech, automated voice recognition systems are now widely used in tasks requiring human data acquisition and automated user interfaces, such as telephone systems, voice recognition systems, voice recognition and automated access to telephone networks."}, {"heading": "1.3 Types of Speech Recognition", "text": "Speech recognition systems can be divided into several distinct classes by describing what types of utterances they can recognize. These classes are classified as follows: Isolated words: Isolated word recognition systems typically require each utterance to be quiet on both sides of the sample window (there is no audio signal); they accept single words or single utterances at once; these systems have \"list / emergency list\" states in which the speaker must wait between utterances (usually during pauses); Continuous language: Continuous speech recognition systems allow the user to speak almost naturally, while the computer determines the content (or, more accurately, \"connected utterances\"). (Basically, it is the computer dictation). Continuous speech recognition systems are among the most difficult because they can use specialized speech recognition methods that can express themselves spontaneously rather than at a natural level."}, {"heading": "1.4 Applications of Speech Recognition:", "text": "Various applications of the speech recognition domain were discussed in the following table 1.Enabling students who are physically handicapped and not be akeyboard to enter text verballyNarrative oriented research, where transcripts are automaticalygenerated. This would reduce the time for manual generation of the transcript and human error. Outside education sectorComputer and video games, Gambling, Precision SurgerySpeech wave form Spoken wordsDomains Domestic sector Oven, Refrigerators, dishwashers and washing machineSpeech wave form Spoken wordsMilitary sector High Performance fighter aircraft, Helicopters, Battle Management, Training Air Trafficers form Spoken words, people with disabilities Speech wave form Spoken wordsArtificial Intelligence sectorRobotics Speech wave form Spoken wordsMedical sector health care, MedicalTranscriptions (digital speech totext) Speech form Spoken wordsGeneral: Automated transcription, Telematics, Air Control Speech form Spoken wordssector in the medical sector."}, {"heading": "1.5 Automatic Speech Recognition system classification:", "text": "The following tree structure underlines the language processing applications. Depending on the selected criterion, automatic speech recognition systems can be classified as shown in Figure 2."}, {"heading": "1.6 Relevant issues of ASR design:", "text": "The most important questions on which recognition accuracy depends are presented in Table 2.Table 2: Relevant ASR designation issues Environment Type of noise; Signal / Noise ratio; Working conditions converter microphone; Telephone channel band amplitude; Distortion; Echo Speaker dependence / independence Gender, age; Physical and mental state Language styles Voice tone (quiet, normal, screamed); Production (isolated words or continuous speech or spontaneous speech) Speed (slow, normal, fast) Vocabulary characteristics of available training data; Specific or general vocabulary; 183 http: / / sites.google.com / site / ijcsis / ISSN 1947-5500"}, {"heading": "2. Approaches to speech recognition:", "text": "There are three main approaches to speech recognition: Acoustic Phonetic Approach Pattern Recognition Approach Artificial Intelligence Approach"}, {"heading": "2.1 Acoustic phonetic approach:", "text": "The earliest approaches to speech recognition were based on finding speech sounds and providing appropriate terms for these sounds. This is the basis of the acoustic phonetic approach (Hemdal and Hughes 1967), which postulates that there are finite, distinctive phonetic units (phonemes) in spoken language and that these units are generally characterized by a series of acoustic properties that manifest themselves in the speech signal over time. Although the acoustic properties of phonetic units are highly variable both with speakers and with adjacent sounds (the so-called co-articulation effect), the acoustic-phonetic approach assumes that the rules that govern variability are simple and can be easily learned by a machine. The first step in the acoustic phonetic approach is a spectral analysis of speech combined with a feature recognition that transforms the spectral measurements into a set of features that describe the distinctive phonetic units."}, {"heading": "2.2 Pattern Recognition approach:", "text": "The pattern matching approach (Itakura 1975; Rabiner 1989; Rabiner and Juang 1993) comprises two essential steps: pattern training and pattern comparison. The main feature of this approach is that it uses a well-formulated mathematical framework and produces uniform pattern representations for reliable pattern comparison from a series of labeled training samples via a formal training algorithm. A pattern representation can take the form of a speech template or statistical model (e.g. a HIDDEN MARKOV MODEL or HMM) and be applied to a sound (smaller than a word), a word or a phrase. At the pattern comparison stage of the approach, a direct comparison between the unknown speech (the language to be recognized) and any possible pattern during the training phase is performed to determine the identity of the unknown according to the quality of the matching of the patterns."}, {"heading": "2.2.1. Template Based Approach:", "text": "The underlying idea is simple: a collection of prototypical speech patterns is stored as reference patterns representing the dictionary of the candidate's words, and recognition occurs by matching an unknown spoken utterance with each of these reference templates and selecting the category of the best matching pattern. Normally, templates are constructed for whole words, with the advantage that errors due to the segmentation or classification of smaller acoustically variable units such as phonemes can be avoided. In turn, each word must have its own complete reference template; template preparation and matching become prohibitively expensive or impractical as the vocabulse size exceeds a few hundred words. A key idea in the template method is to derive a typical sequence of language frames for a pattern (a word) using a mean value process and rely on the use of local spectral distance measurements to compare patterns."}, {"heading": "2.2.2. Stochastic Approach:", "text": "Stochastic modeling [97] involves the use of probability models to deal with uncertain or incomplete information. In speech recognition, uncertainty and incompleteness arise from many sources, such as confusable sounds, loudspeaker variabilities, contextual effects, and homophonic words. Therefore, stochastic models are a particularly suitable approach for speech recognition. Today, the most popular stochastic approach is hidden Markov modeling. A hidden Markov model is characterized by a finite Markov model and a number of output distributions. Transition parameters in the Markov chain models are temporal variabilities, while the parameters in the output distribution model are spectral variabilities. These two types of variables are the essence of speech recognition. Compared to the template-based approach, hidden Markov modeling is more general and has a firmer mathematical basis. A top variance based MM model based on a simple continuity model is likely to be the HM model, although the hidden Markov modeling of knowledge in a more solid mathematical basis is not."}, {"heading": "2.3. Dynamic Time Warping(DTW):", "text": "Dynamic time distortion is an algorithm used to measure the similarity between two sequences, which can vary in time or speed. For example, similarities in walking patterns would be detected, even if the person in one video walks slowly and if he walks faster in another, or even if there are accelerations and delays during the course of an observation. DTW was applied to video, audio and graphics, in fact all data that can be converted into a linear representation can be analyzed with DTW. A well-known application is automatic speech recognition to cope with different speech speeds. In general, DTW is a method that allows a computer to find an optimal match between two predefined sequences (e.g. time series) with certain limitations. Sequences are not \"distorted\" linearly in the time dimension to determine a measure of their similarity independent of certain nonlinear variations in the time dimension. This sequence alignment method is often used in the context of martial."}, {"heading": "2.4. Vector Quantization(VQ):", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country, in which it is a country and in which it is a country."}, {"heading": "2.7. Support Vector Machine(SVM):", "text": "One of the powerful pattern recognition tools that takes a discriminatory approach is an SVM [97]. SVMs use linear and non-linear separating hyperplanes to classify the data. However, since SVMs can only classify fixed-length data, this method cannot easily be applied to tasks involving variable length classification. 186 http: / / sites.google.com / site / ijcsis / ISSN 1947-5500csis data length must be translated to fixed length vectors before SVMs can be used. It is a generalized linear classifier with maximum margin adjustment. This adjustment function provides a regulation that helps the classifier to better generalize. The classifier tends to ignore many of the features. Conventional statistical and neural network methods control the complexity of the model by using a small number of features (the problematic dimensionality or the number of its model spaces), which allows SVM to control the large number of its model dimensions independently."}, {"heading": "2.8. Taxonomy of Speech Recognition:", "text": "Existing speech recognition techniques were schematically illustrated in the following figure 4."}, {"heading": "3. Feature Extraction:", "text": "In fact, most of them are able to survive themselves if they are not able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are not able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are able to survive themselves. Most of them are not able to survive themselves. Most of them are not able to survive themselves. Most of them are able to survive themselves. Most of them are not able to survive themselves. Most of them are able to survive themselves. Most of them are not able to survive themselves."}, {"heading": "4. Classifiers [149]:", "text": "The resulting table of cluster centers is also known as a code book, which can be used to index new vectors by finding the cluster center that comes closest to the new vectors. In the case of speech, there is an extreme case of some vowels that are represented by their formal frequency, the vowels that are represented are as pronounced in the words bot (/ a /) and boot (). Notice that they fall into beautiful groups."}, {"heading": "5. Performance of speech recognition systems:", "text": "Speech recognition system performance is usually specified in terms of accuracy and speed. Accuracy can be measured in terms of performance accuracy, which is normally measured by word recognition rate (WER), while speed is measured by the real-time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).Word Error Rate (WHO): Word Error Rate is a common measure of the performance of a speech recognition or machine translation system. The general difficulty in measuring performance is that the detected word sequence may have a length other than the reference word sequence (supposedly the correct one).The WHO is derived from the Levenshtein distance and operates at the word level instead of the phoneme level. This problem is solved by first aligning the detected word sequence with the reference word sequence (spoken) sequence (the number of words can then be calculated by word string-9), where the number of words (D) may be followed by (the dynamic string-D)."}, {"heading": "6. Literature Survey of speech recognition: ( year vise):", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 1920-1960s:", "text": "Machine recognition emerged in the early 1920s. The first machine to commercially name language to a significant extent, Radio Rex (toys), was manufactured in 1920. [165] Research into the concepts of speech technology began as early as 1936 at Bell Labs. In 1939, Bell Labs demonstrated a speech synthesis machine (which simulates speech) at the New York World's Fair. Bell Labs later abandoned its efforts to develop speech-simulated hearing and recognition based on a false conclusion that artificial intelligence would ultimately be necessary for success. The earliest attempts to develop automated speech recognition systems by machines were made in the 1950s, when various researchers attempted to exploit the basic ideas of acoustic phonetics. In the 1950s, most speech recognition systems explored spectral resonance during the vocal region of each utterance extracted from output signals from an analog filter bank and logical circuits."}, {"heading": "6.2 1960-1970:", "text": "In the 1960s, when computers were not yet fast enough, several specialized hardware was built [6]. However, the decade began with several Japanese laboratories entering the recognition arena and establishing specialized hardware as part of their systems. On the early Japanese system, described by Suzuki and Nakata of the Radio Research Lab in Tokyo, a hardware vowel system was used to select the spoken vowel [7]. An elaborate filter bank spectrum analyzer was used along with the logic that (in a weighted manner) led the outputs of the individual channels of the spectrum analyzer to a vocal decision, and majority decisions were made to select the spoken vowel. Another hardware effort in Japan was the work of Sakai and Doshita of kyoto University in 1962, who incorporated hardware phoneme recognition [7]."}, {"heading": "6.3 1970-1980:", "text": "In the 1970s, speech recognition research reached a number of significant milestones. First, the field of isolated word or discrete speech recognition became a viable and usable technology, based on fundamental studies by Velichko and Zagoruyko in Russia [13], Cakoe and Chiba in Japan [14] and Itakura in the United States. Russian studies helped apply pattern recognition ideas to speech recognition; Japanese research showed how dynamic programming methods could be successfully applied; and Itakura's research showed how the ideas of linear predictive coding (LPC), which were already successfully used in the low bit rate of speech coding, could be extended to speech recognition systems."}, {"heading": "6.4 1980-1990:", "text": "Just as isolated word recognition was a focal point of research in the 1970s, the issue of connected word recognition was a focal point of research in the 1980s, where the goal was to create a robust system capable of recognizing a fluent spoken sequence of words (e.g. the number of words that match in each word), and a wide range of applied algorithms based on the creation of a linked pattern of individual words were developed. Moshey J. Lasry has developed a curious speech recognition system in which the two levels of dynamic programming at Sakoe at the Nippon Electric Corporation (NEC) are applied using a variety of letters and numbers."}, {"heading": "6.5 1990-2000s:", "text": "In fact, it is the case that most of them are able to outdo themselves, and that they are able to outdo themselves, \"he said in an interview with The New York Times:\" I don't think they are able to outdo themselves. \"Indeed,\" I don't think they are able to outdo themselves. \"In the second half of the last decade, in the second half of the last decade, in the second half of the twentieth decade, in the second half of the twentieth decade, in the second half of the twentieth century, in the second half of the twentieth century, in the second half of the twentieth millennium, in the second half of the twentieth century, in the second half of the twentieth century, in the second half of the twentieth century."}, {"heading": "6.6. 2000-2009: a) General:", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "7. Speech Databases:", "text": "Speech databases are more widely used in automatic speech recognition. They are also used in other important applications such as automatic speech synthesis, coding and analysis, including speech recognition and verification. All of these applications require large amounts of re-coded database. Different types of databases used for speech recognition applications are discussed along with their taxonomics.Taxonomy of existing speech databases: The intra-speaker and inter-speaker variability are important parameters for a speech database. However, intra-speaker variability is very important for speech recognition performance. Intraspeaker variation can arise from a variable speech rate, changing emotions or other mental variables and in ambient noise. The variance produced by different speakers is referred to as inter-speaker variance and is caused by the individual variability in voice systems that includes source stimulation, variation tract articulation, lips and / nodal radiation."}, {"heading": "7.1. Resource Management Complete Set 2.0:", "text": "DARPA Resource Management Continuous Speech Corpora (RM) consists of digitized and transcribed language for use in the development and evaluation of Continuous Speech Recognition Systems. There are two main sections, often referred to as RM1 and RM2: RM1 contains three sections, SpeakerDependent (SD) training data, Speaker Independent (SI) training data, and test and evaluation data. RM2 has an additional and larger SD data set, including test material. All RM material consists of read sentences modeled after a resource management task at sea. The entire body contains over 25,000 expressions from more than 160 speakers representing a variety of American dialects. The material was recorded at 16-bit resolution with a Sennheiser HMD-414 headset microphone. All discs comply with the ISO-9660 data format."}, {"heading": "7.1.1. Resource Management SD and SI Training and Test Data (RM1):", "text": "In fact, most of them are able to survive on their own, most of them are not able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own."}, {"heading": "7.2.1. TIMIT and Derivatives:", "text": "The derivatives of TIMIT are: CTIMIT, FFMTIMIT, HTIMIT, NTIMIT, VidTIMIT. They were recorded by playing different recording input devices such as telephone handsets and mobile phones etc. TIMIT and most derivatives are one-on-one sessions and are therefore not optimal for evaluating speaker recognition systems as there is no variability between speakers. VidTIMIT is an exception as it includes video and corresponding audio recordings of 43 subjects. It was recorded in three sessions with about a week delay between each session and can be useful for research involving automatic visual or audiovisual speech recognition or speaker verification."}, {"heading": "7.3 TI46 database:", "text": "The TI46 body was designed and collected by Texas Instruments (TI) and was delivered by 16 speakers, 8 women and 8 men with the designations f1-f8 and m1-m8, respectively, consisting of two words TI-20 and TI-alphabet. The TI-20 vocabulary contains the ten digits from 0 to 9 and ten commands: Enter, Delete, Walk, Help, No, Robot, Stop, Start and Yes. The TI alphabet vocabulary contains the names of the 26 letters of the alphabet from a to Z. For each vocabulary, each speaker produced 10 tokens in a single training session and another two tokens in each of 8 test sessions."}, {"heading": "7.4 SWITCHBOARD:", "text": "Although they support several types of speech and language research, it includes about 2,430 conversations with an average length of 6 minutes; in other words, over 240 hours of recorded and transcribed speech, spoken by over 500 speakers of both sexes. Data is 8 kHz, 8 bit mu-Law encrypted, with the two channels contained in each audio. In addition, it has a number of unique features that contribute to its value."}, {"heading": "7.5. Air Travel Information System(ATIS):", "text": "The ATIS database is often used to evaluate the word performance of automatic speech recognition. ATIS is based on a realistic application environment and is a good simulation of spontaneous conversations."}, {"heading": "8. Summary of the technology progress:", "text": "In the last 60 years, especially in the last three decades, intensive research has been carried out worldwide on speech recognition, spurred by advances in signal processing algorithms, architectures and hardware. The technological progress of the last 60 years can be summarized in Table 7 [137]. Table 7: Summary of technological progress over the last 60 yearsSl. Past Present (new) 1) Template matchingCorpus-based statistical modelling, e.g. HMM and n Grammatics2) Filter bank / spectral resonance Cepstral features, kernel-based function, group delay functions3) Heuristic time normalization DTW / DP-Matching4) Distance-based methods Likelihood-based method5) Maximum probability approach discriminatory approach (e.g. MCE / GPD and MMI6) Local word recognition Continuous speech recognition DTW / DP-Matching4) Distance-based methods Likelihood-based method discriminatory approach (e.g. M6)"}, {"heading": "9. Gap between machine and human speech recognition:", "text": "What we know about the processing of human language is still very limited, and we have yet to experience a complete and worthwhile analysis. In 1994,199 http: / / sites.google.com / site / ijcsis / ISSN 1947-5500Moore [96], the following 20 topics were presented that are important for a greater understanding of the nature of language and the mechanisms of pattern processing in general: How important is the communicative nature of speech? Is human speech communication relevant to human machine communication through language? Language technology or linguistics? (How can we integrate linguistics and technology?) Where is a unified theory of language particular? Is language contradictory? Is it random variability in language normal? Is discretion normal? What is language need? What is good architecture (for language processes)? What are appropriate levels of representation?"}], "references": [{"title": "Furui, 50 years of Progress in speech and Speaker Recognition Research", "author": ["Sadaoki"], "venue": "ECTI Transactions on Computer and Information Technology,Vol.1", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Hidden Markov Models for Speech,  IDA,Princeton, NJ,1980", "author": ["J.Ferguson", "Ed"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1980}, {"title": "Hidden Markov models for speech, IDA, Princeton, NJ,1980", "author": ["J.Ferguson", "Ed"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1980}, {"title": "BYBLOS, the BBN continuous speech recognition system", "author": ["Y.L.Chow", "et.al"], "venue": "Proc.ICASSP, pp.89-92,1987", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1987}, {"title": "Linguistic constraints in hidden Markov model based speech recognition", "author": ["M.Weintraub"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1989}, {"title": "Maximum likelihood linear regression for speaker pp.adaptation of continuous density hidden Markov models, Computer", "author": ["C.J. Leggetter", "P.C. Woodland"], "venue": "Speech and Language,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1995}, {"title": "Hidden Markov model decomposition of speech and noise", "author": ["A.P. Varga", "R.K. Moore"], "venue": "Proc. ICASSP,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1990}, {"title": "A structural Bayes approach to speaker adaptation", "author": ["K. Shinoda", "C.H. Lee"], "venue": "IEEE Trans. Speech and Audio Proc.,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2001}, {"title": "Structural metadata research in the EARS program", "author": ["Y. Liu", "et. al"], "venue": "Proc. ICASSP,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2005}, {"title": "The IBM 2004 conversational transcription", "author": ["H. Soltau", "et. al"], "venue": "Proc. ICASSP,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2005}, {"title": "Recent progress in corpus-based spontaneous speech recognition", "author": ["S. Furui"], "venue": "IEICE Trans. Inf. & Syst., E88-D,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2005}, {"title": "Speech-to-text and speech-to-speech summarization of spontaneous speech", "author": ["S. Furui"], "venue": "IEEE Trans. Speech & Audio Processing,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2004}, {"title": "Statistical confidence measures and their applications", "author": ["C.H. Lee"], "venue": "Proc. ICSP,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2001}, {"title": "Key-phrase verification for flexible speech understanding", "author": ["T. Kawahara"], "venue": "IEEE Trans.Speech & Audio Proc.,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 1998}, {"title": "A level building for connected word recognition", "author": ["C.S. Myers", "L.R. Rabiner"], "venue": "IEEE Trans. Acoustics, Speech, Signal Proc.,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1981}, {"title": "Large Vocabulary Speech Recognition with Multispan statistical Language Models", "author": ["Jerome R. Bellegarda"], "venue": "IEEE Transactions On Speech & Audio Processing,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2000}, {"title": "Utterance Verification In Decoding And Training Procedures", "author": ["Eduardo Lleida Et.al"], "venue": "IEEE Transactions On Speech And Audio Processing,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2000}, {"title": "A Bayesian Predictive Classification Approach to Robust Speech Recognition", "author": ["Qiang Huo", "Chin-Hui Lee"], "venue": "IEEE Transactions On Speech & Audio Processing,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2000}, {"title": "Maximum Likelihood and Minimum Classification Error Factor Analysis for Automatic Speech Recognition", "author": ["Lawrence K. Saul", "Mazin G. Rahim"], "venue": "IEEE Transactions On Speech And Audio Processing,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "During 1950s[1], most of the speech recognition systems investigated spectral resonances during the vowel region of each utterance which were extracted from output signals of an analogue filter bank and logic circuits.", "startOffset": 12, "endOffset": 15}, {"referenceID": 1, "context": "Speech research in the 1980s was characterized by a shift in technology from template based approaches to statistical modeling methods especially the hidden Markov model approach [26,27].", "startOffset": 179, "endOffset": 186}, {"referenceID": 2, "context": "a) Hidden Markov Model(HMM): HMM is one of the key technologies developed in the 1980s, is the hidden Markov model(HMM) approach [28,29,30].", "startOffset": 129, "endOffset": 139}, {"referenceID": 3, "context": "Major research contributions resulted from efforts at CMU(notably the well known SPHINX system)[36], BBN with the BYBLOS system[37], Lincoln Labs[38], SRI[39], MIT[40], and AT&T Bell Labs[41].", "startOffset": 127, "endOffset": 131}, {"referenceID": 4, "context": "Major research contributions resulted from efforts at CMU(notably the well known SPHINX system)[36], BBN with the BYBLOS system[37], Lincoln Labs[38], SRI[39], MIT[40], and AT&T Bell Labs[41].", "startOffset": 145, "endOffset": 149}, {"referenceID": 5, "context": "Major techniques include, the maximum likelihood linear regression (MLLR) [44], the model decomposition [45], parallel model composition (PMC) [46], and the structural maximum a posteriori (SMAP) method [47] for robust speech recognition.", "startOffset": 74, "endOffset": 78}, {"referenceID": 6, "context": "Major techniques include, the maximum likelihood linear regression (MLLR) [44], the model decomposition [45], parallel model composition (PMC) [46], and the structural maximum a posteriori (SMAP) method [47] for robust speech recognition.", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "Major techniques include, the maximum likelihood linear regression (MLLR) [44], the model decomposition [45], parallel model composition (PMC) [46], and the structural maximum a posteriori (SMAP) method [47] for robust speech recognition.", "startOffset": 203, "endOffset": 207}, {"referenceID": 8, "context": "The goal was to make it possible for machines to do a much better job of detecting, extracting, summarizing and translating important information, thus enabling humans to understand what was said by reading transcriptions instead of listening to audio signals [48, 49].", "startOffset": 260, "endOffset": 268}, {"referenceID": 9, "context": "The goal was to make it possible for machines to do a much better job of detecting, extracting, summarizing and translating important information, thus enabling humans to understand what was said by reading transcriptions instead of listening to audio signals [48, 49].", "startOffset": 260, "endOffset": 268}, {"referenceID": 10, "context": "In Japan, a 5-year national project Spontaneous Speech: Corpus and Processing Technology was conducted [50].", "startOffset": 103, "endOffset": 107}, {"referenceID": 11, "context": "These new techniques include flexible acoustic modeling, sentence boundary detection, pronunciation modeling, acoustic as well as language model adaptation, and automatic speech summarization [51].", "startOffset": 192, "endOffset": 196}, {"referenceID": 12, "context": "To further increase the robustness of speech recognition systems, especially for spontaneous speech, utterance verification and confidence measures, are being intensively investigated [52].", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "To detect semantically, significant parts and reject irrelevant portions in spontaneous utterances, a detection based approach has recently been investigated [53].", "startOffset": 158, "endOffset": 162}, {"referenceID": 14, "context": "In order to build acoustic models more sophisticated than conventional HMMs, the dynamic Bayesian network has recently been investigated [54].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "Around 2000, a QBPC[56], systems were developed to find the unknown and mismatch between training and testing conditions.", "startOffset": 19, "endOffset": 23}, {"referenceID": 15, "context": ", have developed Large Vocabulary Speech Recognition with Multi-span Statistical Language Models [55] and the work done in this paper characterizes the behavior of such multi span modeling in actual recognition.", "startOffset": 97, "endOffset": 101}, {"referenceID": 15, "context": "[55], have contributed large vocabulary speech recognition with multispan statistical language model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[56], introduced a set of acoustic modeling and decoding techniques for utterance verification(UV) in HMM based Continuous Speech Recognition .", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[58], discuss regarding HMM models for Automatic speech recognition which rely on high dimension feature vectors for summarizing the short time properties of speech.", "startOffset": 0, "endOffset": 4}], "year": 2009, "abstractText": "This paper presents a brief survey on Automatic Speech Recognition and discusses the major themes and advances made in the past 60 years of research, so as to provide a technological perspective and an appreciation of the fundamental progress that has been accomplished in this important area of speech communication. After years of research and development the accuracy of automatic speech recognition remains one of the important research challenges (eg., variations of the context, speakers, and environment).The design of Speech Recognition system requires careful attentions to the following issues: Definition of various types of speech classes, speech representation, feature extraction techniques, speech classifiers, database and performance evaluation. The problems that are existing in ASR and the various techniques to solve these problems constructed by various research workers have been presented in a chronological order. Hence authors hope that this work shall be a contribution in the area of speech recognition. The objective of this review paper is to summarize and compare some of the well known methods used in various stages of speech recognition system and identify research topic and applications which are at the forefront of this exciting and challenging field.", "creator": "Win2PDF http://www.daneprairie.com"}}}