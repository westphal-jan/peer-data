{"id": "1001.2709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2010", "title": "Kernel machines with two layers and multiple kernel learning", "abstract": "In this paper, the framework of kernel machines with two layers is introduced, generalizing classical kernel methods. The new learning methodology provide a formal connection between computational architectures with multiple layers and the theme of kernel learning in standard regularization methods. First, a representer theorem for two-layer networks is presented, showing that finite linear combinations of kernels on each layer are optimal architectures whenever the corresponding functions solve suitable variational problems in reproducing kernel Hilbert spaces (RKHS). The input-output map expressed by these architectures turns out to be equivalent to a suitable single-layer kernel machines in which the kernel function is also learned from the data. Recently, the so-called multiple kernel learning methods have attracted considerable attention in the machine learning literature. In this paper, multiple kernel learning methods are shown to be specific cases of kernel machines with two layers in which the second layer is linear. Finally, a simple and effective multiple kernel learning method called RLS2 (regularized least squares with two layers) is introduced, and his performances on several learning problems are extensively analyzed. An open source MATLAB toolbox to train and validate RLS2 models with a Graphic User Interface is available.", "histories": [["v1", "Fri, 15 Jan 2010 15:10:39 GMT  (388kb,DS)", "http://arxiv.org/abs/1001.2709v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["francesco dinuzzo"], "accepted": false, "id": "1001.2709"}, "pdf": {"name": "1001.2709.pdf", "metadata": {"source": "CRF", "title": "Kernel machines with two layers and multiple kernel learning", "authors": ["Francesco Dinuzzo"], "emails": ["francesco.dinuzzo@unipv.it."], "sections": [{"heading": "1 Introduction", "text": "In fact, the number of those who are able to study in the USA is much higher than in other countries of the world. In the USA, the number of those who are able to study in the USA is much higher than in other countries of the world. In the USA, the number of those who study in the USA is much higher than in the USA. In the USA, the number of those who study in the USA is much higher than in the USA. In the USA, the number of those who study in the USA is much higher than in the USA. In the USA, the number of those who study in the USA is, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "2 A representer theorem for architectures with", "text": "Two LayersA learning architecture with two layers can be formalized as Map f: X \u2192 Y expressed as a functional composition as in Equation (1). Imagine two RKHS H1 and H2 of vector-weighted functions [31] that are defined via X or Z, with (operatively weighted) core functions K1 and K2, and consider the following problem: Problem 1min f1, f2, H2, H1, H1, H2, H1, H1, H1, f1, H2, H2, H2, H2, H2, H2, H2, H2, H2, H2, H2, H2, H2, 2, K2, K2, K2, K2, H2, H2, H2, H2, H2, H2, H2, H2, 2, K2, K2, K2."}, {"heading": "3 Multiple kernel learning as a kernel machine", "text": "This section demonstrates that multiple learning, consisting of a finite linear combination of cores and the associated predictor, can be interpreted as a specific instance of the kernel architecture with two levels. \u2022 H1 is an RKHS of vector-weighted functions f: X \u2192 RKHS of the matrix-weighted kernel function f: 1 such thatK1 (x1, x2) = diag {K: 1 (x1, x2),., K: 1 (x1, x2)."}, {"heading": "3.1 Linear machines", "text": "In the application of standard kernel methods with high-dimensional input data, the linear kernel on RNK (x1, x2) = x T 1 x2 (9) plays an important role. Optimization algorithms for linear machines are the subject of renewed attention in the literature due to some important experimental findings. First, it turns out that linear models are already flexible enough to be state-of-the-art in application areas such as text document classification, literal disambiguity and drug design. [24] Second, linear machines can be trained with extremely efficient and scalable algorithms [23, 44, 16]. Finally, linear methods can also be used to solve certain non-linear problems (by using non-linear feature cards), ensuring a good trade-off between flexibility and computational convenience."}, {"heading": "4 Regularized least squares with two layers", "text": "\"It's a problem we can solve.\" \"It's a problem we have to solve.\" \"It's a problem we have to solve.\" \"It's a problem we have to solve.\" \"It's a problem we have to solve.\" \"It's a problem we have to solve.\" \"It's a problem we have to solve.\" \"It's a problem we have to solve.\" \"It's a problem we have to solve.\" \"It's a problem we can't solve.\" \"It's a problem we have to solve.\" \"It's a problem we have to solve.\" \"It's a problem we have to solve.\" \"It's a problem we have to solve.\" \"It's a problem we have to solve.\""}, {"heading": "4.1 A Bayesian maximum a posteriori interpretation of RLS2", "text": "The equivalence between regulation problem 2 and multiple kernel learning optimization problem 8 can easily be used to give a Bayesian MAP (maximum a posteriori) interpretation of RLS2. To specify the probabilistic model, we must set a prior distribution over the functional set from X to R and define the data generation model (probability). In the following, N (\u00b5, \u03c32) denotes a real Gaussian distribution with the mean \u00b5 and variance \u03c32, GM (f, K) denotes a Gaussian measurement over the functional set from X to Rm with the mean f and covariance function K and U (val) denote the uniform distribution in Rm over a series of positive finite metrics. Let us leave f: X \u2192 R such a thatf = wTSf1, where f1: X \u2192 Rm is a negative measurement of probability."}, {"heading": "4.2 Linear regularized least squares with two layers", "text": "As described in Section 3.1, if the input set X is a subset of Rm, the linear choice of the base kernel (10) can produce a linear choice of the base kernel (10). (5) First of all, it should be noted that the default regulated smallest squares with the linear kernel (9) on end-dimensional tikhonov regression [51] is also known as rillic regression [22]. (6) The linear version of RLS2 corresponds to a \"scaled\" regression problem in which the optimal scaling is also estimated from the data. (d): = diag {s1d1,. (14) For each fixed d, let n (d) the number of non-zero coefficients in which the optimal scaling is also estimated from the data. (d): = diag {s1d1,."}, {"heading": "4.3 Choice of the scaling and feature/kernel selection", "text": "The ability of RLS2 to choose a meaningful quantity is strongly influenced by scaling S in the second level. In this subsection, we will analyze certain scaling rules associated with popular statistical indices, which are often used as \"filters\" for the selection of characteristics. As the question of scaling requires further investigation, it is not excluded that new rules, which differ from those in this subsection, may better target specific problems. This means that RLS2 tends to select kernels that are highly regulated to maximize the quantity Ak = yTRky, which represents a type of alignment between the kernel Rk and the outputs. Since each scaling is proportional to the scaling factor sk, an effective method is one that makes scaling a meaningful quantification."}, {"heading": "5 Experiments", "text": "This section analyzes the behavior of linear and nonlinear RLS2 in relation to multiple learning problems. In subsection 5.1, an illustrative analysis of linear RLS2 is proposed, the aim of which is to investigate feature selection capabilities and dependence on the regularization parameters of the algorithm in simple experimental environments. RLS2 with nonlinear nuclei is analyzed in subsection 5.2, where a comprehensive benchmark is performed on multiple regression and classification problems from the UCI repository. Finally, in subsection 5.3, a multi-stage classification of microarray data is considered. Calculations are performed in a Matlab environment and the partial problem (problem 7) is solved with an SMO-like (sequential minimum optimization) algorithm. [34] Current implementations have conjugated gradients to solve linear systems and a complex shrinkage technique to reduce the compression of gradients."}, {"heading": "5.1 Linear RLS2: illustrative experiments", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5.2 Non-linear RLS2: regression and classification benchmark", "text": "In this section, the benchmark results are presented on four regression and six classification problems from the UCI repository (Table 2).RLS2 has been extended to 100 random dataset splits with two different training / test ratios: 60 / 40 and 70 / 30. For each dataset split, an approximate regularization path of 30 values is chosen on a logarithmic scale, while the kernel expansion coefficients di are initialized to their optimal values, which are achieved with the previous value of the results. Performance is measured by accuracy for classification and RMSE (root of mean square error) for each dataset split and the value of the following quantities."}, {"heading": "5.3 RLS2: multi-class classification of microarray data", "text": "RLS2 can be applied to multi-class classification problems by solving multiple binary classification problems and combining their results. One possible way to combine binary classifiers is the OVA approach (one against all), in which each class is compared with all others and test labels are assigned to the class that maximizes confidence (the real score) of the corresponding binary classification. Linear RLS2 with OVA has been applied to the 14 cancer data sets [36], a delicate multi-class classification problem whose goal is to distinguish between 14 different cancers based on microarray measurements of 16063 gene expressions. Gene measurements and cancers (labels) are available for 198 patients, with the data sets already divided into a training set of 144 patients and a test set of 54 patients. Another important goal in this issue is to differentiate a small subset of genes relevant to different types of cancer."}, {"heading": "6 Conclusions", "text": "The link between learning with a two-layer network and the problem of learning the kernel has been analysed. While architectures with more than one layer are justified by a representative theorem, an alternative perspective is proposed to look at the problem of kernel learning. Such a perspective makes it clear that these two methods both aim to increase the approximability of standard one-layer methods by using machines that can adapt functions with a variety of shapes when little prior knowledge is available. In particular, it shows that the multi-layer learning system is an important specific case of a more general two-layer architecture. We also introduce RLS2, a new method for conducting multi-layer learning processes based on regularization with the square loss function and alternative optimization. RLS2 shows the latest state of the art on several learning problems, including the multi-class classification of microarray data. An open source collection of MATLAB scripts for RL2 user interface and RL2 graphics is also available at http: / www.rlinem.org and http: / rconquer.org."}, {"heading": "Appendix (proofs)", "text": "The question that arises is: \"What is the answer to the question, what is the answer to the question?\" \"What is the answer?\" \"What is the answer?\" \"What is the answer?\" \"What is the answer?\" \"What is the answer?\" \"What is the answer?\" \"What is the answer?\" \"\" What is the answer? \"\" \"What is the answer?\" \"\" What is the answer? \"\" \"What is the answer?\" \"\" What is the answer? \"\" \"What is the answer?\" \"\" What is the answer? \"\" \"What is the answer?\" \"\" What is the answer? \"\" What is the answer? \"\" What is the answer? \"\" What is the answer? \"What is the answer?\" What is the answer? \"What is the answer?\" What is the answer? \"What is the answer?\" What is the answer? \"What is the answer?\" What is the answer? \"What is the answer?\" What is the answer? \""}], "references": [{"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Akaike"], "venue": "B. N. Petrov and F. Cs\u00e1ki, editors, Second International Symposium on Information Theory. Acad\u00e9miai Kiad\u00f3, Budapest,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1973}, {"title": "A DC-programming algorithm for kernel selection", "author": ["A. Argyriou", "R. Hauser", "C.A. Micchelli", "M. Pontil"], "venue": "ICML \u201906: Proceedings of the 23rd international conference on Machine learning, pages 41\u201348, New York, NY, USA,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning convex combinations of continuously parameterized basic kernels", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil"], "venue": "Proc. Conf. on Learning Theory (COLT\u201905),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "When there is a representer theorem? Vector versus matrix regularizers", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, 10:2507\u20132529,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Theory of reproducing kernels", "author": ["N. Aronszajn"], "venue": "Transactions of the American Mathematical Society, 68(3):337\u2013404,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1950}, {"title": "Consistency of the Group Lasso and multiple kernel learning", "author": ["F.R. Bach"], "venue": "Journal of Machine Learning Research, 9:1179\u20131225,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiple kernel learning, conic duality, and the SMO algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "ICML \u201904: Proceedings of the twenty-first international conference on Machine learning, page 6, New York, NY, USA,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Scaling learning algorithms towards AI", "author": ["Y. Bengio", "Y. LeCun"], "venue": "L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors, Large-Scale Kernel Machines. MIT Press,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Asymptotic analysis of penalized likelihood and related estimators", "author": ["D. Cox", "F. O\u2019 Sullivan"], "venue": "The Annals of Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "Smoothing noisy data with spline functions", "author": ["P. Craven", "G. Wahba"], "venue": "Numerische Mathematik, 31:377\u2013403,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1979}, {"title": "Some properties of regularized kernel methods", "author": ["E. De Vito", "L. Rosasco", "A. Caponnetto", "M. Piana", "A. Verri"], "venue": "Journal of Machine Learning Research, 5:1363\u2013 1390,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "An algebraic characterization of the optimum of regularized kernel methods", "author": ["F. Dinuzzo", "G. De Nicolao"], "venue": "Machine Learning, 74(3):315\u2013345,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "On the representer theorem and equivalent degrees of freedom of SVR", "author": ["F. Dinuzzo", "M. Neve", "G. De Nicolao", "U.P. Gianazza"], "venue": "Journal of Machine Learning Research, 8:2467\u20132495,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "The estimation of prediction error: Covariance penalties and crossvalidation", "author": ["B. Efron"], "venue": "Journal of the American Statistical Association, 99(14):619\u2013632,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research, 9:1871\u20131874,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Multicategory proximal support vector machine classifiers", "author": ["G.M. Fung", "O.L. Mangasarian"], "venue": "Machine Learning, 59(1-2):77\u201397,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Regularization theory and neural networks architectures", "author": ["F. Girosi", "M. Jones", "T. Poggio"], "venue": "Neural Computation, 7(2):219\u2013269,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "Feature Extraction: Foundations and Applications (Studies in Fuzziness and Soft Computing)", "author": ["I. Guyon", "S. Gunn", "M. Nikravesh", "L.A. Zadeh"], "venue": "Springer- Verlag New York, Inc., Secaucus, NJ, USA,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "The Elements of Statistical Learning", "author": ["T.J. Hastie", "R.J. Tibshirani", "J. Friedman"], "venue": "Data Mining, Inference and Prediction. Springer-Verlag, Canada, 2nd edition,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximate maximum a posteriori with Gaussian process priors", "author": ["M. Hengland"], "venue": "Constructive Approximation, 26:205\u2013224,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Ridge regression: biased estimation for nonorthogonal problems", "author": ["A.E. Hoerl", "R. Kennard"], "venue": "Technometrics, 12:55\u201367,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1970}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["C. Hsieh", "K.W. Chang", "C.J. Lin", "S.S. Keerthi", "S. Sundararajan"], "venue": "Proceedings of the Twenty Fifth International Conference on Machine Learning (ICML), pages 408\u2013 415, Helsinki, Finland,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "Proceedings of the ACM Conference on Knowledge Discovery and Data Mining (KDD), pages 217\u2013226, Philadelphia, PA, USA,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning the kernel via convex optimization", "author": ["S.J. Kim", "A. Zymnis", "A. Magnani", "K. Koh", "S. Boyd"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 1997\u20132000, April", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Some results on Tchebycheffian spline functions", "author": ["G. Kimeldorf", "G. Wahba"], "venue": "Journal of Mathematical Analysis and Applications, 33(1):82\u201395,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1971}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G.R.G. Lanckriet", "N. Cristianini", "P. Bartlett", "L. El Ghaoui", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, 5:27\u201372,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Some comments on Cp", "author": ["C. Mallows"], "venue": "Technometrics, 15:661\u2013675,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1973}, {"title": "The support vector machine under test", "author": ["D. Meyer", "F. Leisch", "K. Hornik"], "venue": "Neurocomputing, 55(1-2):169\u2013186,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning the kernel function via regularization", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, 6:1099\u20131125,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "On learning vector-valued functions", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Neural Computation, 17:177\u2013204,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Feature space perspectives for learning the kernel", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Machine Learning, 66:297\u2013319,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning the kernel with hyperkernels", "author": ["C.S. Ong", "A.J. Smola", "R.C. Williamson"], "venue": "Journal of Machine Learning Research, 6:1043\u20131071,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast training of support vector machines using sequential minimal optimization", "author": ["J. Platt"], "venue": "B. Sch\u00f6lkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press, Cambridge (MA),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1998}, {"title": "SimpleMKL", "author": ["A. Rakotomamonjy", "F.R. Bach", "S. Canu", "Y. Grandvalet"], "venue": "Journal of Machine Learning Research, 9:2491\u20132521,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Multiclass cancer diagnosis using tumor gene expression signatures", "author": ["S Ramaswamy", "P Tamayo", "R Rifkin", "S Mukherjee", "C H Yeang", "M Angelo", "C Ladd", "M Reich", "E Latulippe", "J P Mesirov", "T Poggio", "W Gerald", "M Loda", "E S Lander", "T R Golub"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, 98:15149\u201315154,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2001}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "The MIT Press,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Regularized least squares classification", "author": ["R. Rifkin", "G. Yeo", "T. Poggio"], "venue": "Suykens, Horvath, Basu, Micchelli, and Vandewalle, editors, Advances in Learning Theory: Methods, Model and Applications, volume 190 of NATO Science Series III: Computer and Systems Sciences, chapter 7, pages 131\u2013154. VIOS Press, Amsterdam,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": "Princeton University Press, Princeton, NJ, USA,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1970}, {"title": "A generalized representer theorem", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": "Neural Networks and Computational Learning Theory, 81:416\u2013426,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "(Adaptive Computation and Machine Learning). The MIT Press,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2001}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The Annals of Statistics, 6:461\u2013464,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1978}, {"title": "A quantitative theory of immediate visual recognition", "author": ["T. Serre", "G. Kreiman", "M. Kouh", "C. Cadieu", "U. Knoblich", "T. Poggio"], "venue": "Progress in Brain Research, Computational Neuroscience: Theoretical Insights into Brain Function, 165:33\u201356,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2007}, {"title": "PEGASOS: Primal Estimated sub- GrAdient SOlver for Svm", "author": ["S. Shalev-Shwartz", "Y. Singer", "N. Srebro"], "venue": "ICML \u201907: Proceedings of the 24th international conference on Machine learning, pages 807\u2013814, New York, NY, USA,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge University Press,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2004}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research, 7:1531\u20131565,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2006}, {"title": "Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate ii radical prostatectomy treated patients", "author": ["T. Stamey", "J. Kabalin", "J. McNeal", "I. Johnstone", "F. Freiha", "E. Redwine", "N. Yang"], "venue": "Journal of Urology, 16:1076\u20131083,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1989}, {"title": "Estimation of the mean of a multivariate normal distribution", "author": ["C. Stein"], "venue": "The Annals of Statistics, 9:1135\u20131151,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1981}, {"title": "Sparseness of support vector machines", "author": ["I. Steinwart"], "venue": "Journal of Machine Learning Research, 4:1071\u20131105,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2003}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B, 58(1):267\u2013288,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1996}, {"title": "Solutions of Ill Posed Problems", "author": ["A.N. Tikhonov", "V.Y. Arsenin"], "venue": "W. H. Winston, Washington, D. C.,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1977}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "Wiley, New York, NY, USA,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1998}, {"title": "Kernel extrapolation", "author": ["S.V.N. Vishwanathan", "K.M. Borgwardt", "O. Guttman", "A.J. Smola"], "venue": "Neurocomputing, 69(7-9):721\u2013729,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2006}, {"title": "Spline Models for Observational Data", "author": ["G. Wahba"], "venue": "SIAM, Philadelphia, USA,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 1990}, {"title": "Multi-kernel regularized classifiers", "author": ["Q. Wu", "Y. Ying", "D. Zhou"], "venue": "Journal of Complexity, 23(1):108\u2013134,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 53, "context": "Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods.", "startOffset": 187, "endOffset": 211}, {"referenceID": 17, "context": "Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods.", "startOffset": 187, "endOffset": 211}, {"referenceID": 40, "context": "Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods.", "startOffset": 187, "endOffset": 211}, {"referenceID": 51, "context": "Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods.", "startOffset": 187, "endOffset": 211}, {"referenceID": 44, "context": "Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods.", "startOffset": 187, "endOffset": 211}, {"referenceID": 36, "context": "Indeed, the functional analytic point of view is the theoretical core of many successful learning methodologies such as smoothing splines, Gaussian processes, and support vector machines [54, 18, 41, 52, 45, 37], collectively referred to as kernel methods.", "startOffset": 187, "endOffset": 211}, {"referenceID": 4, "context": "These results are usually presented within the theory of RKHS [5], and formalize the intuition that optimal learning machines trained with a finite number of data must be expressed by \u2217Francesco Dinuzzo is with Department of Mathematics, University of Pavia, Pavia, Italy e-mail: francesco.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 39, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 48, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 11, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 30, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 52, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 13, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 3, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 70, "endOffset": 101}, {"referenceID": 25, "context": "Representer theorems have been generalized and analyzed in many forms [10, 40, 49, 12, 31, 53, 14, 4], since their first appearance [26].", "startOffset": 132, "endOffset": 136}, {"referenceID": 7, "context": "Such extension is also suggested by complexity theory of circuits [8] as well as by biological motivated learning models, [43].", "startOffset": 66, "endOffset": 69}, {"referenceID": 42, "context": "Such extension is also suggested by complexity theory of circuits [8] as well as by biological motivated learning models, [43].", "startOffset": 122, "endOffset": 126}, {"referenceID": 6, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 26, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 32, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 29, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 2, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 54, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 31, "context": "In the field of kernel methods, the need for complex hypothesis spaces reflecting \u201cbroad\u201d prior knowledge has led to the idea of learning the kernel from empirical data simultaneously with the predictor [7, 27, 33, 30, 3, 55, 32].", "startOffset": 203, "endOffset": 229}, {"referenceID": 32, "context": "A major extension to the framework of classical kernel methods, based on the concept of hyper-kernels, has been introduced in [33], encompassing many convex kernel learning algorithms.", "startOffset": 126, "endOffset": 130}, {"referenceID": 26, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 24, "endOffset": 32}, {"referenceID": 29, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 24, "endOffset": 32}, {"referenceID": 1, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 88, "endOffset": 106}, {"referenceID": 45, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 88, "endOffset": 106}, {"referenceID": 31, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 88, "endOffset": 106}, {"referenceID": 34, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 88, "endOffset": 106}, {"referenceID": 5, "context": "Since the earlier works [27, 30], many improved optimization schemes have been proposed [2, 46, 32, 35, 6].", "startOffset": 88, "endOffset": 106}, {"referenceID": 45, "context": "Along the line of recent advances in multiple kernel learning [46, 35, 6], it is shown that the involved optimization can be efficiently carried out using a two-step procedure.", "startOffset": 62, "endOffset": 73}, {"referenceID": 34, "context": "Along the line of recent advances in multiple kernel learning [46, 35, 6], it is shown that the involved optimization can be efficiently carried out using a two-step procedure.", "startOffset": 62, "endOffset": 73}, {"referenceID": 5, "context": "Along the line of recent advances in multiple kernel learning [46, 35, 6], it is shown that the involved optimization can be efficiently carried out using a two-step procedure.", "startOffset": 62, "endOffset": 73}, {"referenceID": 30, "context": "Introduce two RKHS H1 and H2 of vector-valued functions [31] defined over X and Z respectively, with (operator-valued) kernel functions K and K, and consider the following problem:", "startOffset": 56, "endOffset": 60}, {"referenceID": 39, "context": "Problem 1 is outside the scope of standard representer theorems [40] due to the presence of the composition (f2 \u25e6 f1).", "startOffset": 64, "endOffset": 68}, {"referenceID": 0, "context": "Introduce the indicator function I of the interval [0, 1] defined as", "startOffset": 51, "endOffset": 57}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Second, linear machines can be trained using extremely efficient and scalable algorithms [23, 44, 16].", "startOffset": 89, "endOffset": 101}, {"referenceID": 43, "context": "Second, linear machines can be trained using extremely efficient and scalable algorithms [23, 44, 16].", "startOffset": 89, "endOffset": 101}, {"referenceID": 15, "context": "Second, linear machines can be trained using extremely efficient and scalable algorithms [23, 44, 16].", "startOffset": 89, "endOffset": 101}, {"referenceID": 49, "context": "From the user\u2019s point of view, linear kernel machines with two layers behave similarly to sparse `1 regularization methods such as the Lasso [50], performing feature selection by varying with continuity a shrinking parameter.", "startOffset": 141, "endOffset": 145}, {"referenceID": 45, "context": "For instance, from the results of the previous section it follows that the two-layer version of standard Support Vector Machines with \u201chinge\u201d loss functions Li(z) = (1\u2212 yiz)+ is equivalent to the SILP (Semi-Infinite Linear Programming) multiple kernel learning problem studied in [46], whose solution can be computed, for instance, by using gradient descent or SimpleMKL [35].", "startOffset": 280, "endOffset": 284}, {"referenceID": 34, "context": "For instance, from the results of the previous section it follows that the two-layer version of standard Support Vector Machines with \u201chinge\u201d loss functions Li(z) = (1\u2212 yiz)+ is equivalent to the SILP (Semi-Infinite Linear Programming) multiple kernel learning problem studied in [46], whose solution can be computed, for instance, by using gradient descent or SimpleMKL [35].", "startOffset": 371, "endOffset": 375}, {"referenceID": 37, "context": "Indeed, generalization performances of regularized least squares classifiers have been shown to be comparable to that of Support Vector Machines on many dataset, see [38, 17] and references therein.", "startOffset": 166, "endOffset": 174}, {"referenceID": 16, "context": "Indeed, generalization performances of regularized least squares classifiers have been shown to be comparable to that of Support Vector Machines on many dataset, see [38, 17] and references therein.", "startOffset": 166, "endOffset": 174}, {"referenceID": 45, "context": "Along the lines of recent developments in multiple kernel learning optimization [46, 35], we propose a two-step minimization procedure that alternates between kernel and predictor optimization.", "startOffset": 80, "endOffset": 88}, {"referenceID": 34, "context": "Along the lines of recent developments in multiple kernel learning optimization [46, 35], we propose a two-step minimization procedure that alternates between kernel and predictor optimization.", "startOffset": 80, "endOffset": 88}, {"referenceID": 20, "context": "Nevertheless, the regularization Problem 2 can be still recovered by understanding MAP estimate as a maximal point of the posterior probability measure, as described in [21].", "startOffset": 169, "endOffset": 173}, {"referenceID": 50, "context": "First of all, recall that standard regularized least squares with the linear kernel (9) boils down to finite-dimensional Tikhonov regularization [51] also known as ridge regression [22]:", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "First of all, recall that standard regularized least squares with the linear kernel (9) boils down to finite-dimensional Tikhonov regularization [51] also known as ridge regression [22]:", "startOffset": 181, "endOffset": 185}, {"referenceID": 14, "context": "[15, 20].", "startOffset": 0, "endOffset": 8}, {"referenceID": 19, "context": "[15, 20].", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "Degrees of freedom is an index more interpretable than the regularization parameter, and can be also used to choose the regularization parameter according to tuning criteria such as Cp [28], AIC [1], BIC [42], GCV [11].", "startOffset": 185, "endOffset": 189}, {"referenceID": 0, "context": "Degrees of freedom is an index more interpretable than the regularization parameter, and can be also used to choose the regularization parameter according to tuning criteria such as Cp [28], AIC [1], BIC [42], GCV [11].", "startOffset": 195, "endOffset": 198}, {"referenceID": 41, "context": "Degrees of freedom is an index more interpretable than the regularization parameter, and can be also used to choose the regularization parameter according to tuning criteria such as Cp [28], AIC [1], BIC [42], GCV [11].", "startOffset": 204, "endOffset": 208}, {"referenceID": 10, "context": "Degrees of freedom is an index more interpretable than the regularization parameter, and can be also used to choose the regularization parameter according to tuning criteria such as Cp [28], AIC [1], BIC [42], GCV [11].", "startOffset": 214, "endOffset": 218}, {"referenceID": 47, "context": "A general expression for the effective degrees of freedom of non-linear kernel regression methods with one layer, based on the SURE (Stein\u2019s Unbiased Risk Estimator) approximation [48] has been recently derived in [13].", "startOffset": 180, "endOffset": 184}, {"referenceID": 12, "context": "A general expression for the effective degrees of freedom of non-linear kernel regression methods with one layer, based on the SURE (Stein\u2019s Unbiased Risk Estimator) approximation [48] has been recently derived in [13].", "startOffset": 214, "endOffset": 218}, {"referenceID": 18, "context": "In this subsection, we analyze certain scaling rules that are connected with popular statistical indices, often used as \u201cfilters\u201d for feature selection [19].", "startOffset": 152, "endOffset": 156}, {"referenceID": 26, "context": "A transductive scaling rule can be obtained by extending the sum to both training and test inputs, namely computing the inverse trace of the overall kernel matrix, as in [27].", "startOffset": 170, "endOffset": 174}, {"referenceID": 33, "context": "Computations are carried out in a Matlab environment and the sub-problem (Problem 7) is solved using an SMO-like (Sequential Minimal Optimization) algorithm [34].", "startOffset": 157, "endOffset": 161}, {"referenceID": 46, "context": "Experiment 2 (Prostate Cancer data) Linear RLS2 is applied to the Prostate Cancer dataset, a regression problem whose goal is to predict the level of prostate-specific antigen on the basis of a number of clinical measures in men who were about to receive a radical prostatectomy [47].", "startOffset": 279, "endOffset": 283}, {"referenceID": 19, "context": "Results for methods other than RLS2 are taken from [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "are used in the textbook [20] to compare different feature selection and shrinkage methods, and have been obtained from the web site http://www-stat.", "startOffset": 25, "endOffset": 29}, {"referenceID": 19, "context": "Following [20], we pick the value of \u03bb corresponding to the least complex model within one standard error of the best validation score.", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "For comparison, Table 1 also reports models and results taken from [20] associated with LS (Least Squares), Best subset regression, Ridge Regression, Lasso regression, PCR (Principal Component Regression), PLS (Partial Least Squares).", "startOffset": 67, "endOffset": 71}, {"referenceID": 34, "context": "[35], the same set of basis kernels for all the datasets has been chosen.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Performances of other kernel learning algorithms on some of these datasets can be found in [27, 33, 35] and references therein.", "startOffset": 91, "endOffset": 103}, {"referenceID": 32, "context": "Performances of other kernel learning algorithms on some of these datasets can be found in [27, 33, 35] and references therein.", "startOffset": 91, "endOffset": 103}, {"referenceID": 34, "context": "Performances of other kernel learning algorithms on some of these datasets can be found in [27, 33, 35] and references therein.", "startOffset": 91, "endOffset": 103}, {"referenceID": 28, "context": "Another benchmark study that might be useful for comparison is [29].", "startOffset": 63, "endOffset": 67}, {"referenceID": 26, "context": "For instance, [27] uses an 80/20 dataset split ratio, [33] uses 60/40, while [35] uses 70/30.", "startOffset": 14, "endOffset": 18}, {"referenceID": 32, "context": "For instance, [27] uses an 80/20 dataset split ratio, [33] uses 60/40, while [35] uses 70/30.", "startOffset": 54, "endOffset": 58}, {"referenceID": 34, "context": "For instance, [27] uses an 80/20 dataset split ratio, [33] uses 60/40, while [35] uses 70/30.", "startOffset": 77, "endOffset": 81}, {"referenceID": 35, "context": "Linear RLS2 with OVA has been applied to the 14 Cancers dataset [36], a delicate multi-class classification problem whose goal is to discriminate between 14 different types of cancer, on the basis of microarray measurements of 16063 gene expressions.", "startOffset": 64, "endOffset": 68}, {"referenceID": 19, "context": "[20] reports several results for these data using a variety of classification methods.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Results for methods other than RLS2 are taken from [20].", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "Table 5 reports the number of test errors and selected genes in correspondence with the value of \u03bb chosen in the validation phase, for RLS2 and other methods from [20].", "startOffset": 163, "endOffset": 167}, {"referenceID": 30, "context": "By standard representer theorems for vector valued functions (see [31] and the remark on monotonicity in [40] after Theorem 1), there exists an optimal f2 in the form", "startOffset": 66, "endOffset": 70}, {"referenceID": 39, "context": "By standard representer theorems for vector valued functions (see [31] and the remark on monotonicity in [40] after Theorem 1), there exists an optimal f2 in the form", "startOffset": 105, "endOffset": 109}, {"referenceID": 38, "context": "where \u2202 is the sub-differential of a convex function [39].", "startOffset": 53, "endOffset": 57}, {"referenceID": 24, "context": "This problem can be seen to be jointly convex in (z,R) using an argument due to [25]: the term zTR\u2020z is a matrix-fractional function (see e.", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "[9], Example 3.", "startOffset": 0, "endOffset": 3}], "year": 2010, "abstractText": "In this paper, the framework of kernel machines with two layers is introduced, generalizing classical kernel methods. The new learning methodology provide a formal connection between computational architectures with multiple layers and the theme of kernel learning in standard regularization methods. First, a representer theorem for two-layer networks is presented, showing that finite linear combinations of kernels on each layer are optimal architectures whenever the corresponding functions solve suitable variational problems in reproducing kernel Hilbert spaces (RKHS). The input-output map expressed by these architectures turns out to be equivalent to a suitable single-layer kernel machines in which the kernel function is also learned from the data. Recently, the so-called multiple kernel learning methods have attracted considerable attention in the machine learning literature. In this paper, multiple kernel learning methods are shown to be specific cases of kernel machines with two layers in which the second layer is linear. Finally, a simple and effective multiple kernel learning method called RLS2 (regularized least squares with two layers) is introduced, and his performances on several learning problems are extensively analyzed. An open source MATLAB toolbox to train and validate RLS2 models with a Graphic User Interface is available.", "creator": "TeX"}}}