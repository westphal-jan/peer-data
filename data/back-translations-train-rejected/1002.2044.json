{"id": "1002.2044", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2010", "title": "On the Stability of Empirical Risk Minimization in the Presence of Multiple Risk Minimizers", "abstract": "Recently Kutin and Niyogi investigated several notions of algorithmic stability--a property of a learning map conceptually similar to continuity--showing that training-stability is sufficient for consistency of Empirical Risk Minimization while distribution-free CV-stability is necessary and sufficient for having finite VC-dimension. This paper concerns a phase transition in the training stability of ERM, conjectured by the same authors. Kutin and Niyogi proved that ERM on finite hypothesis spaces containing a unique risk minimizer has training stability that scales exponentially with sample size, and conjectured that the existence of multiple risk minimizers prevents even super-quadratic convergence. We prove this result for the strictly weaker notion of CV-stability, positively resolving the conjecture.", "histories": [["v1", "Wed, 10 Feb 2010 09:08:56 GMT  (102kb,DS)", "http://arxiv.org/abs/1002.2044v1", "4 pages"]], "COMMENTS": "4 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["benjamin i p rubinstein", "aleksandr simma"], "accepted": false, "id": "1002.2044"}, "pdf": {"name": "1002.2044.pdf", "metadata": {"source": "CRF", "title": "On the Stability of Empirical Risk Minimization in the Presence of Multiple Risk Minimizers", "authors": ["Benjamin I. P. Rubinstein", "Aleksandr Simma"], "emails": ["benr@eecs.berkeley.edu).", "asimma@eecs.berkeley.edu)."], "sections": [{"heading": null, "text": "Index terms - empirical risk minimization, algorithmic stability, threshold phenomena I = multiple sample space with simultaneous miniature size. INTRODUCTIONDEVROYE and Wagner [3] initially investigated the effects of false algorithmic stability on the statistical generalization of learning. Since then, many authors have proposed numerous alternative notions of stability and used them to study the performance of algorithms that cannot easily be analyzed with other techniques [1], [2], [4] - [6]. While the learning ability of a concept class can now be characterized by the admission of a stable learner, the search for natural definitions of stability with such properties is still open [5], [6]. In their general study of algorithmic stability in [5], Kutin and Niyogi observed that empirical risk minimization (ERM) on hypothesis mixes cardinality spaces two that represent a phase transition in achievable stability rates as the number of risk combinations."}, {"heading": "II. PRELIMINARIES", "text": "We follow the setting of [5] closely. X denotes the input space, Y = \u03b2 = \u03b2, 1) the output space, and Z = X \u00b7 Y its output space. D is a distribution on Z. Unless we assume that m is examples according to D. A classifier or hypothesis is a function h: X \u2192 {1, 1}; H denotes a series of classifiers or a hypotheses space. Whenever we refer to a finite H, we assume that no h1, h2 or hypotheses exist, that h1 6 = h2 and h1 (X) = h2 (X) a.s.A Learning algorithm on hypotheses space H is a function m > 0Zm \u2192 H. If A is understood from the context, we write fs = A (s) for s Zm."}, {"heading": "III. FINITE CLASSES WITH MULTIPLE RISK MINIMIZERS", "text": "Lemma 3,1: Let H be a finite hypothesis space with | H | 2 and with risk minimizers satisfying | H (= 2). < ERM on H with respect to the 0-1 loss, in a sample of m examples, is not (0, \u03b4) -CV stable for all cases (m \u2212 1 / 2). Furthermore, it is assumed that fS lies in H with exponentially increasing probability and that the switching occurs frequently within H?. Let = minh H\\ H\\ H (h) \u2212 RD (h1), which exists and is positively bound by the finite cardinality of H. Then by the Union, RD (h)."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Peter Bartlett for his helpful feedback on this research."}], "references": [{"title": "Theory of Classification: a Survey of Recent Advances", "author": ["S. Boucheron", "O. Bousquet", "G. Lugosi"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Stability and Generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Distribution-free performance bounds for potential function rules", "author": ["L. Devroye", "T. Wagner"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1979}, {"title": "Algorithmic stability and sanity-check bounds for leave-one-out cross-validation", "author": ["M. Kearns", "D. Ron"], "venue": "Neural Computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Almost-Everywhere Algorithmic Stability and Generalization Error", "author": ["S. Kutin", "P. Niyogi"], "venue": "Technical report TR-2002-03,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Statistical Learning: stability is sufficient and generalization is necessary and sufficient for consistency of Empirical Risk Minimization", "author": ["S. Mukherjee", "P. Niyogi", "T. Poggio", "R. Rifkin"], "venue": "AI Memo", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}], "referenceMentions": [{"referenceID": 2, "context": "DEVROYE and Wagner [3] first studied the effect of algorithmic stability on the statistical generalization of learning.", "startOffset": 19, "endOffset": 22}, {"referenceID": 0, "context": "Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]\u2013[6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 1, "context": "Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]\u2013[6].", "startOffset": 183, "endOffset": 186}, {"referenceID": 3, "context": "Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]\u2013[6].", "startOffset": 188, "endOffset": 191}, {"referenceID": 5, "context": "Since then many authors have proposed numerous alternate notions of stability and have used them to study the performance of algorithms not easily analyzed with other techniques [1], [2], [4]\u2013[6].", "startOffset": 192, "endOffset": 195}, {"referenceID": 4, "context": "While the learnability of a concept class can now be characterized by the admission of a stable learner, finding natural definitions of stability with such properties is still open [5], [6].", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "While the learnability of a concept class can now be characterized by the admission of a stable learner, finding natural definitions of stability with such properties is still open [5], [6].", "startOffset": 186, "endOffset": 189}, {"referenceID": 4, "context": "As part of their general investigation into algorithmic stability in [5], Kutin and Niyogi observed that Empirical Risk Minimization (ERM) on hypothesis spaces of cardinality two experiences a phase transition in achievable rates of stability as the number of risk minimizers increases from one to two.", "startOffset": 69, "endOffset": 72}, {"referenceID": 4, "context": "PRELIMINARIES We follow the setting of [5] closely.", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "The notion of weak hypothesis stability was first used by Devroye and Wagner in [3] under the name of stability.", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "Kearns and Ron [4] then used the term hypothesis stability for the same concept.", "startOffset": 15, "endOffset": 18}, {"referenceID": 0, "context": "Consider Xn,q \u223c Bin (n, q) for n \u2208 N, q \u2208 [0, 1] and note that for k \u2208 N \u222a {0}", "startOffset": 42, "endOffset": 48}], "year": 2010, "abstractText": "Recently Kutin and Niyogi investigated several notions of algorithmic stability\u2014a property of a learning map conceptually similar to continuity\u2014showing that training-stability is sufficient for consistency of Empirical Risk Minimization while distribution-free CV-stability is necessary and sufficient for having finite VC-dimension. This paper concerns a phase transition in the training stability of ERM, conjectured by the same authors. Kutin and Niyogi proved that ERM on finite hypothesis spaces containing a unique risk minimizer has training stability that scales exponentially with sample size, and conjectured that the existence of multiple risk minimizers prevents even super-quadratic convergence. We prove this result for the strictly weaker notion of CV-stability, positively resolving the conjecture.", "creator": "LaTeX with hyperref package"}}}