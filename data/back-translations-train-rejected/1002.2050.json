{"id": "1002.2050", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Feb-2010", "title": "Intrinsic dimension estimation of data by principal component analysis", "abstract": "Estimating intrinsic dimensionality of data is a classic problem in pattern recognition and statistics. Principal Component Analysis (PCA) is a powerful tool in discovering dimensionality of data sets with a linear structure; it, however, becomes ineffective when data have a nonlinear structure. In this paper, we propose a new PCA-based method to estimate intrinsic dimension of data with nonlinear structures. Our method works by first finding a minimal cover of the data set, then performing PCA locally on each subset in the cover and finally giving the estimation result by checking up the data variance on all small neighborhood regions. The proposed method utilizes the whole data set to estimate its intrinsic dimension and is convenient for incremental learning. In addition, our new PCA procedure can filter out noise in data and converge to a stable estimation with the neighborhood region size increasing. Experiments on synthetic and real world data sets show effectiveness of the proposed method.", "histories": [["v1", "Wed, 10 Feb 2010 10:16:57 GMT  (988kb,S)", "http://arxiv.org/abs/1002.2050v1", "8 pages, submitted for publication"]], "COMMENTS": "8 pages, submitted for publication", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["mingyu fan", "nannan gu", "hong qiao", "bo zhang"], "accepted": false, "id": "1002.2050"}, "pdf": {"name": "1002.2050.pdf", "metadata": {"source": "CRF", "title": "Intrinsic dimension estimation of data by principal component analysis", "authors": ["Mingyu Fan", "Nannan Gu", "Bo Zhang"], "emails": ["fanmingyu@amss.ac.cn,", "b.zhang@amt.ac.cn)", "nan@gmail.com,", "hong.qiao@ia.ac.cn)"], "sections": [{"heading": null, "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "II. PREVIOUS ALGORITHMS ON ID ESTIMATION", "text": "In the past, there were mainly three approaches to estimating the ID of the data: projection, geometric and probable approaches to creating estimation methods. (The projection approach first projects data into a low-dimensional space and then determines the ID by verifying the low-dimensional representation of data.) PCA is a classic prediction method that finds ID by counting the number of significant eigenvalues. (However, traditional PCA only works on data located in a linear subspace but becomes ineffective if the data is distributed in a non-linear way. to overcome this limitation, localPCA [9] and OTPMs PCA [10] have been proposed and can detect the ID of data lying on non-linear manifolds by performing the PCA method locally. The isomap algorithm provides ID of the data by verifying the elbow of residual variance curve [4]. Cheng et al. provided an efficient method of calculating eigenvalues A and eigenvalues in [24]."}, {"heading": "A. PCA-based methods for ID estimation", "text": "Traditional PCA can find a subspace where data projections have maximum variance. In the face of a data set X = {x1, \u00b7 \u00b7, xN} with xi-RD. Let X = [x1, \u00b7 \u00b7 \u00b7, xN] and x = 1N \u2211 N = 1 xi. the covariance matrix of X is given by C = 1NN \u2211 i = 1 (xi \u2212 x) (xi \u2212 x) T. Since C is a positive semi \u00b7 \u00b7 defined matrix, we can assume that \u03bb1 \u2265 2 \u2265 \u00b7 \u00b7 \u2265 N \u2265 0 are the eigenvalues of C with \u03bd1, \u00b7 \u00b7, \u03bdN are the corresponding orthonormal eigenvectors. Since the self-decomposition of 3 matrix C is called C, where D is a diagonal matrix of 0, and vice versa the variance of the variance of Dii = illi, \u00b7 \u00b7 villizable. The eigenvector of 3 matrix C is called C."}, {"heading": "B. Filtering out the noise of data", "text": "There are two challenges for PCA-based ID estimation methods. The first is how to filter out the noise in the data, while the second is how to select the size of the subregions on the manifold. To address these two constraints, we propose the following noise filtering method to efficiently filter out the noise in the data and bring PCA-based methods to convergence. Consider the effect of additive white noise in the data with E (\u00b5) = 0 and var (\u00b5) = 2. The covariance matrix of the noise-corrupted data is C \u2032 = var (X + \u00b5) = C + \u03c32I, where C + \u03c32I is the covariance matrix of the data."}, {"heading": "C. The local region selection method", "text": "The dimensionality of each linear subspace should correspond to the ID of the embedded multiplier. (Therefore, it is possible to estimate the ID of a nonlinear manifold by checking it on the spot.) A cover is referred to a set whose elements are subsets of the record, ensuring that the unification of all subsets in the cover contains the complete data.Definition 3.2 (The Set Cover Problem): Given a universe X of N elements and a collection F of subsets of X, where F = {F1, \u00b7, FN}. Coverage of the cover concerns determining a minimal sub-capture of F covering all data points. Firstly, it can find the minimum number of sub-elements that helps save computing time. Secondly, the result of the ID estimate that uses the entire dataset is more reliable."}, {"heading": "D. The proposed ID estimation algorithm", "text": "We now present the proposed ID estimation algorithm using local PCAs on the minimum target coverage: the C-PCD algorithms summarized below for both batch and incremental data. In many cases, sequential data is collected incrementally, which requires an incremental learning algorithm to verify the change in data structure over time. The incremental C-PCA algorithm is presented as follows. Note 3.2: Our method differs from LocalPCA in many respects [9]. Firstly, the centers and algorithm 2 (The C-PCA algorithm for batch data) are described as Step 1."}, {"heading": "E. Computational complexity analysis", "text": "The calculation complexity of our algorithms is one of the most important questions for their application. The estimation of the batch mode ID can be divided into two parts. In the first part, the calculation of the distance matrix O (N2) takes time, the search for the closest neighbors for each data point takes O (kN2) time, and finding an approximate minimum coverage of X takes O (kN) time. Therefore, the first part O (((K + 1) N2 + kN) runtime. In the second part, the execution of PCA locally requires k3 \u00d7 (N / k) \u2248 O (k2N) runtime. In summary, the total runtime of the batch mode algorithm can be reduced to O (k + 1) N2 + (k2 + k) N). If the proposed method is embedded in a manifold learning algorithm, then the runtime complexity can be reduced to O (k2 + k) N) if the distance matrix and the neighborhood (k2) are already relative."}, {"heading": "IV. EXPERIMENTS", "text": "The proposed algorithm was implemented with the parameters \u03b1 = 10 and \u03b2 = 0.8 for all experiments. In practice, it is stated that the noise contained in the data is of low dimension, with the exception of an additive white noise, which is assumed to be present in every component of the data vectors in RD. In practice, therefore, we only use variances of the first min (10, N \u2212 r + 1) PCs in the noise portion of the data to estimate the variance of noise (see Equation. (3)). Comparison is made between the k-k / 2 NN method [18], the k-NNG method [26], the revised MLE method [20], the C \u2212 PCA method and the L \u2212 PCA method, in which the L-PCA method stands for the C-PCA method, without the method for data noise filtering proposed in Subsection III-B."}, {"heading": "B. Real world data sets", "text": "Our algorithms are compared with the MLE, k-k / 2 NN and k-NNG methods on some benchmark datasets: the Isoface dataset [4], the LLEface dataset [5] and the MNIST '0 and' 1 estimation method. \"The Isoface dataset consists of 698 images of a head with a resolution of 64 \u00d7 64. Some examples of the Isoface dataset are shown in Figure 2 (a). In the experiments, each image is transformed into a 4096-dimensional vector. It can be seen that the Isoface datasets are set under a three-dimensional motion: up, right and down. The isomap algorithm estimates its ID as 3 with the projection approach."}, {"heading": "C. Noisy data sets", "text": "The traditional PCA algorithm is very sensitive to outliers, and the performance of PCA-based algorithms deteriorates rapidly when data points are sparse on a variety such as hand rotation dataset 1. As shown in Figure 6 (a), the hand is under one-dimensional motion so that the data points can be viewed as lying on a one-dimensional curve. The dataset contains 481 image samples, and each sample is a vector in a 512480-dimensional space. Many outliers can be seen due to their low-dimensional embedding by the isomap algorithm (see Figure 6 (b)). ID estimation using different methods is shown in Figure 6 (c). Both k-k / 2 NN and k-NNG methods are sensitive to the choice of neighborhood size and tend to overestimate the PCA database as PCA size increases."}, {"heading": "V. CONCLUSION", "text": "The proposed algorithm is easy to implement and delivers a convergent identity estimate that matches a wide range of neighborhood sizes. It is also suitable for incremental learning. Experiments have shown that the new algorithm has robust performance."}, {"heading": "ACKNOWLEDGMENT", "text": "The work of Mr. Qiao was partially supported by the National Natural Science Foundation (NNSF) of China under grant numbers 60675039 and 60621001 and by the Outstanding Youth Fund of the NNSF of China under grant number 60725310. B. Zhang's work was partially supported by the 863 program of China under grant number 2007AA04Z228, by the 973 program of China under grant number 2007CB311002 and by the NNSF of China under grant number 90820007."}], "references": [{"title": "Characterising experimental time series using local intrinsic dimension", "author": ["T.M. Buzuga", "J.V. Stammb", "G. Pfister"], "venue": "Physics Letters A202 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": "Oxford Univ. Press, Oxford", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "The Elements of Statistical Learning - Data Mining", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "Inference and Prediction, Springer, Berlin", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "V", "author": ["J.B. Tenenbaum"], "venue": "de Sliva, J.C. Landford, A global geometric framework for nonlinear dimensionality reduction, Science 290 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": "Science 290 ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Laplacian eigenmaps for dimensionality reduction and representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation 15 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "The manifold ways of perception", "author": ["H.S. Seung", "D.D. Lee"], "venue": "Science 290 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Principal Component Analysis", "author": ["I.T. Jolliffe"], "venue": "Springer, Berlin", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1989}, {"title": "An algorithm for finding intrinsic dimensionality of data", "author": ["K. Fukunaga", "D.R. Olsen"], "venue": "IEEE Transactions on Computers 20 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1971}, {"title": "Intrinsic dimension estimation with optimally topology preserving maps", "author": ["J. Bruske", "G. Sommer"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 20 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Principal curves", "author": ["T. Hastie", "W. Stuetzle"], "venue": "Journal of the American Statistical Association 84 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1988}, {"title": "Chaos", "author": ["S. Chatterjee", "M.R. Yilmag"], "venue": "fractals and statistics, Statistical Science 7 ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1992}, {"title": "Measuring the strangeness of strange attractors", "author": ["P. Grassberger", "I. Procaccia"], "venue": "Physica D9 ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1983}, {"title": "Intrinsic dimension estimation using packing numbers", "author": ["B. Kegl"], "venue": "Advances in Neural Information Processing Systems 16 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Riemannian manifold learning", "author": ["T. Lin", "H. Zha"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 30 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "An evaluation of intrinsic dimensionality estimators", "author": ["P.J. Verveer", "R.P.W. Duin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 17 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "Intrinsic dimension estimation of manifolds by incising balls", "author": ["M. Fan", "H. Qiao", "B. Zhang"], "venue": "Pattern Recognition 42 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Manifoldadaptive dimension estimation", "author": ["A.M. Farahmand", "C. Szepesvari", "J.Y. Audibert"], "venue": "in: Proceedings of the 24th Annual International Conference on Machine Learning", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Maximum likelihood estimation of intrinsic dimension", "author": ["E. Levina", "P.J. Bickel"], "venue": "Advances in Neural Information Processing Systems 18 ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Comments on \u2019Maximum likelihood estimation of intrinsic dimension\u2019 by E", "author": ["D.J.C. MacKay", "Z. Ghahramani"], "venue": "Levina and P. Bickel, see http://www.inference.phy.cam.ac.uk/mackay/dimension/", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Estimation of intrinsic dimensionality using high-rate vector quantization", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "Advances in Neural Information Processing Systems 19 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Data dimensionality estimation methods: a survey", "author": ["F. Camastra"], "venue": "Pattern Recognition 36 ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Intrinsic dimensionality estimation of submanifolds in R", "author": ["M. Hein", "J.Y. Audibert"], "venue": "in: Proceedings of the 22nd International Conference on Machine Learning (ed. Morgan Kaufmann)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Provable dimension detection using principal component analysis", "author": ["S.W. Cheng", "Y.J. Wang", "Z.Z. Wu"], "venue": "in: Proceedings of the 21th Annual Symposium on Computational Geometry", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Dimension detection via slivers", "author": ["S.W. Cheng", "M.K. Chiu"], "venue": "in: Proceedings of the 19th Annual ACM-SIAM Symposium on Discrete Algorithms", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Geodesic entropic graphs for dimension and entropy estimation in manifold learning", "author": ["J.A. Costa", "A.O. Hero"], "venue": "IEEE Transactions on Signal Processing 52 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Estimating local intrinsic dimension with k-nearest neighbor graphs", "author": ["J.A. Costa", "A.O. Hero"], "venue": "IEEE Transactions on Statistical Signal Processing 30 (23) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Le Cun", "L. Bottou", "Y. Bengio", "H. Patrick"], "venue": "Proceedings of the IEEE 86 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "An intrinsic dimensionality estimator from near-neighbor information", "author": ["K.W. Pettis", "T.A. Bailey", "A.K. Jain", "R.C. Dubes"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 1 ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1979}], "referenceMentions": [{"referenceID": 0, "context": "In time series analysis [1], the domain of attraction of a nonlinear dynamic system has a very complex geometric structure, and study on the geometry of the attraction domain is closely related to the fractal geometry.", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "In neural network design [2], the number of hidden units in the encoding middle layer should be chosen according to the ID of data.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "In classification tasks [3], in order to balance the generalization ability and the empirical risk value, the complexity of the function should also be related to the ID of data.", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "Important manifold learning algorithms include isometric feature mapping (Isomap) [4], locally linear embedding (LLE) [5] and Laplacian eigenmaps (LE) [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 4, "context": "Important manifold learning algorithms include isometric feature mapping (Isomap) [4], locally linear embedding (LLE) [5] and Laplacian eigenmaps (LE) [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 5, "context": "Important manifold learning algorithms include isometric feature mapping (Isomap) [4], locally linear embedding (LLE) [5] and Laplacian eigenmaps (LE) [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 6, "context": "They all assume data to distribute on an intrinsically lowdimensional sub-manifold [7] and reduce the dimensionality of data by investigating the intrinsic structure of data.", "startOffset": 83, "endOffset": 86}, {"referenceID": 8, "context": "The projection approach [9]\u2013[11] finds ID by checking up the lowdimensional embedding of data.", "startOffset": 24, "endOffset": 27}, {"referenceID": 10, "context": "The projection approach [9]\u2013[11] finds ID by checking up the lowdimensional embedding of data.", "startOffset": 28, "endOffset": 32}, {"referenceID": 21, "context": "The geometric method [22] finds ID by investigating the intrinsic geometric structure of data.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "The probabilistic technique [19] builds estimators by making distribution assumptions on data.", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "Our method is compared with the maximum likelihood estimation (MLE) method [19], the manifold adaptive method (which is referred to as the k-k/2 NN method in this paper) [18] and the k-nearest neighbor graph (kNNG) method [26], [27] through experiments.", "startOffset": 75, "endOffset": 79}, {"referenceID": 17, "context": "Our method is compared with the maximum likelihood estimation (MLE) method [19], the manifold adaptive method (which is referred to as the k-k/2 NN method in this paper) [18] and the k-nearest neighbor graph (kNNG) method [26], [27] through experiments.", "startOffset": 170, "endOffset": 174}, {"referenceID": 25, "context": "Our method is compared with the maximum likelihood estimation (MLE) method [19], the manifold adaptive method (which is referred to as the k-k/2 NN method in this paper) [18] and the k-nearest neighbor graph (kNNG) method [26], [27] through experiments.", "startOffset": 222, "endOffset": 226}, {"referenceID": 26, "context": "Our method is compared with the maximum likelihood estimation (MLE) method [19], the manifold adaptive method (which is referred to as the k-k/2 NN method in this paper) [18] and the k-nearest neighbor graph (kNNG) method [26], [27] through experiments.", "startOffset": 228, "endOffset": 232}, {"referenceID": 8, "context": "To overcome this limitation, localPCA [9] and OTPMs PCA [10] have been proposed and can discover the ID of data lying on nonlinear manifolds by performing the PCA method locally.", "startOffset": 38, "endOffset": 41}, {"referenceID": 9, "context": "To overcome this limitation, localPCA [9] and OTPMs PCA [10] have been proposed and can discover the ID of data lying on nonlinear manifolds by performing the PCA method locally.", "startOffset": 56, "endOffset": 60}, {"referenceID": 3, "context": "The Isomap algorithm yields ID of data by inspecting the elbow of residual variance curve [4].", "startOffset": 90, "endOffset": 93}, {"referenceID": 23, "context": "gave an efficient procedure to compute eigenvalues and eigenvectors in PCA [24].", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "For example, the correlation dimension (a kind of fractal dimensions) was used in [13] to estimate the ID, whilst the method of packing numbers was proposed in [14] to find the ID.", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "For example, the correlation dimension (a kind of fractal dimensions) was used in [13] to estimate the ID, whilst the method of packing numbers was proposed in [14] to find the ID.", "startOffset": 160, "endOffset": 164}, {"referenceID": 22, "context": "Other fractalbased methods include the kernel correlation method [23] and the quantization estimator [21].", "startOffset": 65, "endOffset": 69}, {"referenceID": 20, "context": "Other fractalbased methods include the kernel correlation method [23] and the quantization estimator [21].", "startOffset": 101, "endOffset": 105}, {"referenceID": 21, "context": "A good survey on fractal-based methods can be found in [22].", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Lin [15] and Cheng [25] suggested to construct simplices to find the ID, while the nearest neighbor approach uses the distances between data points with their nearest neighbors to build ID estimators such as the estimator proposed by Pettis et al.", "startOffset": 4, "endOffset": 8}, {"referenceID": 24, "context": "Lin [15] and Cheng [25] suggested to construct simplices to find the ID, while the nearest neighbor approach uses the distances between data points with their nearest neighbors to build ID estimators such as the estimator proposed by Pettis et al.", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": "[29], the kNNG method [26], [27] and the incising ball method [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[29], the kNNG method [26], [27] and the incising ball method [17].", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "[29], the kNNG method [26], [27] and the incising ball method [17].", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "[29], the kNNG method [26], [27] and the incising ball method [17].", "startOffset": 62, "endOffset": 66}, {"referenceID": 15, "context": "was made in [16].", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "The MLE-method [19] is a representative method of this approach, whose final global estimator is given by averaging the local estimators:", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "MacKay and Ghahramani [20] pointed out that compared with averaging the local estimators directly, it is more sensible to average their inverses d\u0302 k (xi), i = 1, \u00b7 \u00b7 \u00b7 , N for the maximum likelihood purpose.", "startOffset": 22, "endOffset": 26}, {"referenceID": 6, "context": "More and more real world data are proved to have nonlinear intrinsic structures and may possibly distribute on nonlinear embedding manifolds [7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 16, "context": "Therefore, estimation of embedding ID of data becomes an important problem [17].", "startOffset": 75, "endOffset": 79}, {"referenceID": 8, "context": "Compared with the local region selection algorithm used in [9], our algorithm above has a low time complexity and avoids the supervised process to choose the neighborhood.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "2: Our method is different from the LocalPCA [9] in many aspects.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "local regions are determined simultaneously by using one parameter - the neighborhood size, whilst, in [9], the centers and neighborhood sizes are determined by two parameters.", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "Secondly, our approach finds the subregions by approximating a minimum cover of the data set, while the local-PCA in [9] does not guarantee whether or not the selected subregions cover the whole data set.", "startOffset": 117, "endOffset": 120}, {"referenceID": 17, "context": "Comparison is made among the k-k/2 NN method [18], the k-NNG method [26], the revised MLE (MLE in short) method [20], the C-PCA method and the L-PCA method, where the L-PCA method stands for the C-PCA method without the noise filtering procedure proposed in Subsection III-B.", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "Comparison is made among the k-k/2 NN method [18], the k-NNG method [26], the revised MLE (MLE in short) method [20], the C-PCA method and the L-PCA method, where the L-PCA method stands for the C-PCA method without the noise filtering procedure proposed in Subsection III-B.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "Comparison is made among the k-k/2 NN method [18], the k-NNG method [26], the revised MLE (MLE in short) method [20], the C-PCA method and the L-PCA method, where the L-PCA method stands for the C-PCA method without the noise filtering procedure proposed in Subsection III-B.", "startOffset": 112, "endOffset": 116}, {"referenceID": 3, "context": "Our algorithm is compared with the MLE, k-k/2 NN and k-NNG methods on some benchmark real world data sets: the Isoface data set [4], the LLEface data set [5] and the MNIST \u20190\u2019 and \u20191\u2019 data sets [28].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Our algorithm is compared with the MLE, k-k/2 NN and k-NNG methods on some benchmark real world data sets: the Isoface data set [4], the LLEface data set [5] and the MNIST \u20190\u2019 and \u20191\u2019 data sets [28].", "startOffset": 154, "endOffset": 157}, {"referenceID": 27, "context": "Our algorithm is compared with the MLE, k-k/2 NN and k-NNG methods on some benchmark real world data sets: the Isoface data set [4], the LLEface data set [5] and the MNIST \u20190\u2019 and \u20191\u2019 data sets [28].", "startOffset": 194, "endOffset": 198}, {"referenceID": 3, "context": "In [4], the Isomap algorithm estimated its ID as 3 using the projection approach.", "startOffset": 3, "endOffset": 6}], "year": 2010, "abstractText": "Estimating intrinsic dimensionality of data is a classic problem in pattern recognition and statistics. Principal Component Analysis (PCA) is a powerful tool in discovering dimensionality of data sets with a linear structure; it, however, becomes ineffective when data have a nonlinear structure. In this paper, we propose a new PCA-based method to estimate intrinsic dimension of data with nonlinear structures. Our method works by first finding a minimal cover of the data set, then performing PCA locally on each subset in the cover and finally giving the estimation result by checking up the data variance on all small neighborhood regions. The proposed method utilizes the whole data set to estimate its intrinsic dimension and is convenient for incremental learning. In addition, our new PCA procedure can filter out noise in data and converge to a stable estimation with the neighborhood region size increasing. Experiments on synthetic and real world data sets show effectiveness of the proposed method.", "creator": "LaTeX with hyperref package"}}}