{"id": "1002.3086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2010", "title": "Convergence of Bayesian Control Rule", "abstract": "Recently, new approaches to adaptive control have sought to reformulate the problem as a minimization of a relative entropy criterion to obtain tractable solutions. In particular, it has been shown that minimizing the expected deviation from the causal input-output dependencies of the true plant leads to a new promising stochastic control rule called the Bayesian control rule. This work proves the convergence of the Bayesian control rule under two sufficient assumptions: boundedness, which is an ergodicity condition; and consistency, which is an instantiation of the sure-thing principle.", "histories": [["v1", "Tue, 16 Feb 2010 14:14:59 GMT  (76kb)", "http://arxiv.org/abs/1002.3086v1", "8 pages, 7 figures"]], "COMMENTS": "8 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["pedro a ortega", "daniel a braun"], "accepted": false, "id": "1002.3086"}, "pdf": {"name": "1002.3086.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["peortega@dcc.uchile.cl", "dab54@cam.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 100 2.30 86v1 [cs.AI] 16 Feb 2010Keywords: adaptive behavior, intervention calculation, bayesian control, Kullback-Leibler divergence"}, {"heading": "1. Introduction", "text": "If the behavior of a plant is known in all circumstances, then the designer can choose a controller that generates the desired dynamics. Examples include hitting a target with a cannon in known weather conditions, solving a labyrinth with its map, and controlling a robot arm in a production plant. However, if the behavior of the plant is unknown, the designer faces the problem of adaptive control. Even if the cannon does not have the appropriate measuring instruments, the way out of an unknown labyrinth is clear and the development of an autonomous robot for exploring Mars is very difficult."}, {"heading": "2. Preliminaries", "text": "Exposure is limited to the case of discrete time with discrete stochastic observations and control signals. Let O and A be two finite sets of symbols, the first being the set of inputs (observations) and the second the set of outputs (actions). Actions and observations in time t are referred to as in ao \u2264 a and the abbreviation a \u2264 t: = a1, a2,.. It is assumed that the interaction between the controller and the plant is used to simplify the notation of strings. Symbols are underlined to stick them together, as in ao \u2264 t = a1, a2, o2,. It is assumed that the interaction between the controller and the plant in cycles t = 1, 2,..... where in cycle t the controller the action on and the plant reacts."}, {"heading": "3. Bayesian Control Rule", "text": "A new approach would be to minimize the relative entropy of the controller P with respect to the true controller Pm. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &"}, {"heading": "4. Policy Diagrams", "text": "A political diagram is a useful informal tool for analyzing the effects of control measures on plants. Figure 1 illustrates an example. One can imagine a plant as a collection of states connected by transitions marked by I / O symbols. Figure 1, for example, shows a state where measures are taken that resemble a Venn diagram and where observations are collected that lead to states. In a political diagram, one abstracts from the underlying details of the dynamics of the plant and presents groups of states and transitions that resemble a Venn diagram. Choosing a certain policy in a plant amounts to partially controlling the transitions in the state space, thereby selecting a subset of the dynamics of the plant. Accordingly, a policy is represented by a subset in the state space (surrounded by a directed curve), as shown in Figure 1. Political diagrams that are particularly useful for analyzing the effects of strategies on different hypotheses about the dynamics."}, {"heading": "5. Divergence Processes", "text": "One of the obvious questions to be asked in relation to the Bayesian control rule is whether any assumptions are actually made in order to obtain the results of this process, that is, whether P (at) a (at) a (at) a (at) a (at) a (at) a (at) a (at) a (at) a (at) a (at) a (at) a (at) a (at) a (at) a (at) a (at) a (at) a) a (at) a (at) a (at) a (at) a (at) a (at) a (at) a (c) a (at) c (at) a (c) c (c) c) c (a) c (a) c) c (a) c (a) c) c (a) c (a) c (a) c (a) c) c (a) c (a) c) c (a) c (a) c) c (a) c (a) c) c (c) c) c (a) c) c (c) c) c) c (c) c) c) a (at) a (c) c) c) c) a (c) c) c) c) c) c) a (at) c) c) c) c) c) a (at) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c) c c) c) c) c) c) c"}, {"heading": "6. Boundedness", "text": "In general, a divergence process is very complex: practically all classes of distribution processes that are of interest to the control go far beyond i.i.d. and stationary processes. This increased complexity can endanger the analytical tractability of the divergence process, i.e. that no predictions can be made about its asymptotic behavior. More specifically, if the growth rates of the divergence processes vary too much from realization to realization, then the posterior distribution can vary qualitatively between realizations via the operating modes. Hence, a stability requirement must be imposed in order to limit the class of possible divergence processes to a class that is analytically tractable. In light of this insight, the following property is introduced. A divergence process dt (m) is said to impose divergence processes in the M iff for any possible divergence processes in the M iff."}, {"heading": "7. Core", "text": "If one wants to identify the methods of operation whose posterior probabilities disappear, then it is not enough to describe them as those whose hypothesis does not correspond to the true hypothesis. (...) It is only possible if we apply the P1 policy for a long enough period of time that the controller eventually enters the P2 region and makes false predictions there, thereby postponing his process of divergence as expected. (...) If P1 is carried out only for a short period of time, the controller does not risk accumulating the correct counter evidence for H2. (...) If P1 is carried out only for a short period of time, the controller does not risk disambiguating the region. (...)"}, {"heading": "8. Consistency", "text": "This means that it is unclear whether a multiplexation of policies in time will lead to a disambiguation of the two hypotheses at all. This is not desirable as it could affect the convergence to the right control loads. Thus, it is clear that further restrictions must be imposed on the imaging of the hypotheses. Regarding Figure 7, the following observations can be made: 1. Both modes of operation have policies that are located in region A."}, {"heading": "9. Summary and Conclusions", "text": "The Bayean control rule represents a promising approach to adaptive control based on minimizing the relative entropy of the causal I / O distribution of a mixture controller from the true controller. In this paper, proof is provided of the convergence of the Bayesian control rule to the true controller. Analyzing the asymptotic behavior of a controller system could be perceived as a difficult problem involving the consideration of domain-specific assumptions. Here, it is shown that this is not the case: asymptotic analysis can be recast as a study of simultaneous divergence processes that determine the evolution of rear probabilities over operating modes, thereby abstracting the details of the classes of I / O distributions. In particular, when the set of operating modes is finite, two additional assumptions are sufficient to prove convergence."}], "references": [{"title": "Optimal learning: computational procedures for bayes-adaptive markov decision processes", "author": ["M.O. Duff"], "venue": "PhD thesis,", "citeRegEx": "Duff,? \\Q2002\\E", "shortCiteRegEx": "Duff", "year": 2002}, {"title": "The Minimum Description Length Principle", "author": ["P. Gr\u00fcnwald"], "venue": null, "citeRegEx": "Gr\u00fcnwald,? \\Q2007\\E", "shortCiteRegEx": "Gr\u00fcnwald", "year": 2007}, {"title": "Optimal control as a graphical model inference problem", "author": ["B. Kappen", "V. Gomez", "M. Opper"], "venue": null, "citeRegEx": "Kappen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kappen et al\\.", "year": 2009}, {"title": "A bayesian rule for adaptive control based on causal interventions", "author": ["P.A. Ortega", "D.A. Braun"], "venue": "In Proceedings of the third conference on general artificial intelligence,", "citeRegEx": "Ortega and Braun,? \\Q2010\\E", "shortCiteRegEx": "Ortega and Braun", "year": 2010}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q2000\\E", "shortCiteRegEx": "Pearl", "year": 2000}, {"title": "The Foundations of Statistics", "author": ["L.J. Savage"], "venue": null, "citeRegEx": "Savage,? \\Q1954\\E", "shortCiteRegEx": "Savage", "year": 1954}, {"title": "Linearly solvable markov decision problems", "author": ["E. Todorov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Todorov,? \\Q2006\\E", "shortCiteRegEx": "Todorov", "year": 2006}, {"title": "Efficient computation of optimal actions", "author": ["E. Todorov"], "venue": "Proceedings of the National Academy of Sciences U.S.A.,", "citeRegEx": "Todorov,? \\Q2009\\E", "shortCiteRegEx": "Todorov", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "intractable even for simple toy problems (Duff, 2002).", "startOffset": 41, "endOffset": 53}, {"referenceID": 6, "context": "For example, it has been shown that a large class of optimal control problems can be solved very efficiently if the problem statement is reformulated as the minimization of the deviation of the dynamics of a controlled system from the uncontrolled system (Todorov, 2006; 2009; Kappen et al., 2009).", "startOffset": 255, "endOffset": 297}, {"referenceID": 2, "context": "For example, it has been shown that a large class of optimal control problems can be solved very efficiently if the problem statement is reformulated as the minimization of the deviation of the dynamics of a controlled system from the uncontrolled system (Todorov, 2006; 2009; Kappen et al., 2009).", "startOffset": 255, "endOffset": 297}, {"referenceID": 4, "context": "This result is obtained by using properties of interventions using causal calculus (Pearl, 2000).", "startOffset": 83, "endOffset": 96}, {"referenceID": 5, "context": "Intuitively, this property parallels the well-known sure-thing principle of expected utility theory (Savage, 1954).", "startOffset": 100, "endOffset": 114}], "year": 2010, "abstractText": "Recently, new approaches to adaptive control have sought to reformulate the problem as a minimization of a relative entropy criterion to obtain tractable solutions. In particular, it has been shown that minimizing the expected deviation from the causal input-output dependencies of the true plant leads to a new promising stochastic control rule called the Bayesian control rule. This work proves the convergence of the Bayesian control rule under two sufficient assumptions: boundedness, which is an ergodicity condition; and consistency, which is an instantiation of the surething principle.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}