{"id": "1002.4862", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2010", "title": "Less Regret via Online Conditioning", "abstract": "We analyze and evaluate an online gradient descent algorithm with adaptive per-coordinate adjustment of learning rates. Our algorithm can be thought of as an online version of batch gradient descent with a diagonal preconditioner. This approach leads to regret bounds that are stronger than those of standard online gradient descent for general online convex optimization problems. Experimentally, we show that our algorithm is competitive with state-of-the-art algorithms for large scale machine learning problems.", "histories": [["v1", "Thu, 25 Feb 2010 20:31:05 GMT  (15kb)", "http://arxiv.org/abs/1002.4862v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["matthew streeter", "h brendan mcmahan"], "accepted": false, "id": "1002.4862"}, "pdf": {"name": "1002.4862.pdf", "metadata": {"source": "CRF", "title": "Less Regret via Online Conditioning", "authors": ["Matthew Streeter"], "emails": ["mstreeter@google.com", "mcmahan@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 100 2.48 62v1 [cs.LG] 2 5Fe b20 10We analyze and evaluate an online gradient descent algorithm with adaptive coordinate-specific adaptation of learning rates. Our algorithm can be considered an online version of a stack descent with diagonal preditioner. This approach leads to remorse limits that are stronger than those of the standard online gradient descent for general problems of online gradient optimization. Experimentally, we show that our algorithm competes with state-of-the-art algorithms for large-scale machine learning problems."}, {"heading": "1 Introduction", "text": "In recent years, online algorithms have emerged as state-of-the-art techniques for solving major machine learning problems [2, 13, 16]. In addition to their simplicity and universality, online algorithms are a natural choice for problems where new data is constantly arriving and rapid adaptation is important. Compared to studying convex optimization in batch setting (offline), the study of convex online optimization is relatively new. In light of this, it is not surprising that performance-enhancing techniques, which are well known and widely used in batch setting, do not yet have online analogies. Batch-setting convergence rates in particular can be dramatically improved through the use of preconditioning. However, the online convexsive optimization literature does not offer a comparable method of improving regret (the online analogy of convergence rates)."}, {"heading": "1.1 Background and notation", "text": "In an online optimization problem, we are given a closed, convex feasibility profile (1) (1). In each round, we must select a point xt (2). We then suffer loss ft (xt), where ft is a convex function. At the end of round t, the loss function ft is revealed to us (1). Our regret at the end of the T rounds is the difference between our total loss and that of the best fixed x \u2212 F in Hindsight, i.e., Regret \u2261 T = 1ft (xt) \u2212 min x \u00b2 F {T \u2211 t = 1ft (x)}. Sequential prediction using a generalized linear model is an important special case of online convex optimization. In this case, each xt Rn is a vector of weights where xt, i is assigned the weight that is characteristic i on round t."}, {"heading": "2 Motivations", "text": "It is well known that descent in stack gradients in the presence of so-called canyons is bad, surfaces that curve steeper in some directions than in others. [15] In this section, we present examples that show that descent in stack gradients causes great regret in the online environment when the inclination of the loss function or the size of the possible set varies greatly across the coordinates. These observations motivate the use of coordinated learning rates (which can be imagined as an adaptive diagonal prerequisite)."}, {"heading": "2.1 A motivating application", "text": "Consider the problem of trying to predict, using a general linear model, the likelihood that a user will click on an ad when it appears next to the search results for a particular search query. Imagine that there is only one ad, and we want to predict its click rate for many different searches. In a large search engine, a popular search occurs in orders of magnitude more often than a rare search. In rare searches, it is necessary to use a relatively high learning rate so that the associated trait weights are clearly zero. However, in popular searches, using such a large learning rate causes the trait weights to oscillate wildly, and therefore the predictions of the algorithm will be unstable. Therefore, a drop in chance with a global learning rate cannot work well simultaneously for frequent searches and rare searches. Since rare searches are more numerous than ordinary ones, poor performance in both categories will take considerable time."}, {"heading": "2.2 Tradeoffs in one dimension", "text": "As a simple example, let's assume that the workable amount of regret is [0, D], and the loss function in each round is ft (x) = G-x-x-x, for some small positive points. Then let's assume that ft (x) = \u2212 G, if x < and ft (x) = G, if x >, and the loss function in each round is ft (x) = G-x-x, for some small positive points. Then let's play ft (x) = -G, if x < and ft (x) = G, if x >, and the loss function in each round is ft (x) = G-x \u2212 x, for some small positive points. If the algorithm x1 = 0 plays initially, it will play xt = 0 in odd rounds and xt = G, if even rounds, assuming < G-X = D. Thus, after T rounds, we accept the algorithm as a total loss."}, {"heading": "2.3 A bad example for global learning rates", "text": "We now have a class of convex optimization problems, where the use of a coordinate-independent problem to solve problems to solve problems to solve problems to solve problems to solve problems. Furthermore, we regret to grow at an asymptotically higher rate than with a pro-coordinate learning rate. This result is expressed in the following theory.Theorem 1. There exists a family of convex optimization problems parameterized by their lengths (number of rounds T), where gradient descent leads to regret with a non-increasing global learning rate (T 2 3), whereas gradient descent with a reasonable pro-coordinate learning rate exhibits regret O (T 2).The lower limit given in Theorem 1 does not contradict the previously stated O sum (GD 2 3)."}, {"heading": "3 Improved Regret Bounds using Per-Coordinate Learning Rates", "text": "Zinkevich [17] proved limitations in regretting the on-line gradient descent (choosing xt according to Equation (1)). Building on his analysis, we improve these limitations by adjusting the learning rates on a percoordinate basis. Specifically, we obtain these limitations by constructing the vector yt byyt, i = xt, i \u2212 gt, i\u03b7t, i (2), \u03b7t being a vector of learning rates, one for each coordinate. Then, we play xt = P (yt). We prove limitations for feasible quantities defined by axis-oriented constraints, F = \u00d7 ni = 1 [ai, bi]. Many machine learning problems can be solved with practicable sets of this form, as our experiments are disstrategical.2"}, {"heading": "3.1 A better global learning rate", "text": "We first give an improved regret, which is limited to one gradient, with a global (coordinate-independent) learning rate. In the next subsection, we use this improved regret to prove the desired limits on the other side of the gradient descent with a pro-coordinate learning rate. Zinkevich [17] showed that if we have a gradient descent with a non-increasing sequence of 1, 2,. To guard against the worst case, it is natural to choose our learning rates to minimize this limit. However, this is problematic because in the online setting of gradients g1, g2,.., gT are not known in advance. Perhaps we surprise within a factor of 270, even without having this information."}, {"heading": "3.2 A per-coordinate learning rate", "text": "We can improve the limit given above by using for each coordinate a separate copy of the gradient that uses the learning rate given in the previous section (see algorithm 1). Specifically, we use the update of the equation (2) with regret. (i), its regret is limited by a sum of pro-coordinate limits, where Di = bi \u2212 ai is the diameter of the realizable set along coordinate i. (ii) The following theory makes three important points about the performance of the algorithm 1: (i), its regret is limited by a sum of pro-coordinate limits, each of the same shape as (3); (ii) the choice of the algorithm along coordinate i, i gives a regret that is only a factor of regret if the limit had been optimized, g1, g2, g2."}, {"heading": "4 Additional Improved Regret Bounds", "text": "The approach of bordering general regret in relation to the sum of regret on a number of one-dimensional problems can be used to obtain additional limits of regret that improve on those of the previous work, in the particular case where the realizable set is a hypercube. - The key observation is noted in the following quote. - Let us consider an online optimization problem with the realizable set F = \u00b7 ni = 1 [ai, bi] and loss functions f1, fT. - For each t, let it be (x) a lower limit with the realizable set F = \u00b7 ni = 1 (xi, x) and loss functions f1. - Further, let us assume that ft (xt) = t (xt) for all t (xt) for all t, where {xt} is the sequence of points played by an online algorithm. Let us consider the composite online algorithm formed by operating a one-dimensional algorithm."}, {"heading": "4.1 More general notions of strong convexity", "text": "A function f is H-strongly convex if it considers for all x-y-F that f (y) \u2265 f (x) + female f (x) \u00b7 (y \u2212 x) + H-strongly convex if it occurs, for example, in solving learning problems subject to L2 regularization. Bartlett et al. [1] specify an online convex optimization algorithm whose regret is isO (n \u00b7 min {\u221a T, 1H logT}), where H is the greatest constant, so that every ft is H-strongly convex. We can generalize the concept of strong convectivity as follows: f is strongly convex with respect to the vector ~ H if for allx y (y) \u2265 f (x) + female f (x) \u00b7 (y \u2212 x) + female (yi \u2212 xi) 2. Provided we perform the algorithm of Bartlett algorithms."}, {"heading": "4.2 Tighter bounds in terms of variance", "text": "Hazan and Kale [9] specify a limit of regret regarding the gradient variance of the sequence of gradients. Specifically, their algorithm has remorse O (\u221a nV), where V = \u2211 T t = 1% gt \u2212 \u00b5 \u00b2 and \u00b5 = 1T \u00b2 T = 1 gt, where gt = 2% (xt). By running a separate copy of their algorithm on each coordinate, we can instead obtain a limit of O (\u0445 ni = 1% Vi), where Vi = 1% T = 1 (gt, i \u2212 \u00b5i) 2.To compare the limits, let ~ v \u00b2 Rn be a vector whose ith component is Vi, and let ~ 1% Rn be a vector component whose components are observed every 1st. Note that the Vi = 1% Vi = 1% Vi = 1% Vi = 5% V = 5% V% V. Using the Kauchy-Black inequality n = 1% Vi = 1 \u00b0 ~ \u00b2 Vi = 1 \u00b2 V is substantially better when each coordinate \u00b2 V \u00b2 is separated from the other than each other."}, {"heading": "4.3 Adaptive regret", "text": "One weakness of the standard repentance limits such as those mentioned so far is that they bind performance only in relation to the static optimal solution over all T rounds. In a non-stationary environment, it is desirable to obtain stronger guarantees. For example, let's assume the feasible amount is [0, 1], ft (x) = x for the first T-2 rounds and ft (x) thereafter. Then, an algorithm that plays xt = 0 for all t will have 0 repentance, but its loss in the last T2 rounds is T2 worse than if it played the point x = 1 for those rounds. Indeed, standard repentance minimization algorithms fail in simple examples such as this. Hazan and Seshadhri [10] define adaptive repentance as the maximum over all intervals intervals."}, {"heading": "5 Experimental Evaluation", "text": "In this section, we evaluate the slope of the slope with coordinated learning rates experimentally on various machine learning problems."}, {"heading": "5.1 Online binary classification", "text": "We first compare the performance of the online gradient lineage with that of two current text classification algorithms: the passive-aggressive (PA) algorithm [4] and the confidence-weighted (CW) linear classification [7], which has been proven to perform state-of-the-art in large real-world problems [13]. We used four sentimentality classification datasets (books, dvd, electronics and kitchen), ranging from [6], each with 1000 positive examples and 1000 negative examples, to 3, as well as the scaled versions of the rcv1.binary (677,399 examples) and news20.binary (19,996 examples) datasets from LIBSVM [3], mixing the examples and then using each algorithm to pass through the data, calculating the loss before training on it.For the online gradient lineage algorithms, we set F = \u2212 R, n] which we found to be 100% better for the PA (that we found)."}, {"heading": "5.2 Large-scale logistic regression", "text": "We collected data from a large search engineer4, which consisted of random samples of queries containing a specific formulation, such as \"auto insurance.\" Each data set has a few million examples. We converted this data into an online logistics regression problem with a feature vector for each ad impression, using features based on the text of the ad and the query. The target label is 1 when the ad was clicked, and -1 otherwise. The loss function ft is the sum of the logistic loss, logs (1 + Exp (\u2212 Exp (\u2212 Txt\u03b8t)) and a regulation date L2. We compared the gradient decline based on the global learning rate of \u00a7 3.1 with the gradient drop using the percentage coordinate rate specified in \u00a7 3.2. We scaled the formulas given in these sections by 0.1; this improved performance for both algorithms, but the algorithm showed that the set was different, not the algorithmic one, whereas the set \u2212 1 was a comparative one."}, {"heading": "6 Related Work", "text": "The use of different learning rates for different coordinates has been extensively studied in the community of neural networks, where emphasis has been placed on empirical performance in batch setting, and a large number of algorithms have been developed; see, for example, [12]. These algorithms are not designed to perform well in a contradictory online setting, and for many of them it is easy to construct examples where the algorithm causes great regret. More recently, Hsu et al. [11] provided an algorithm for selecting coordinate-specific learning rates for gradient descent, derive asymptotic convergence rates in batch setting, and present a number of positive experimental outcomes. Confidelity-weighted linear classification [7] and AROW [5] are similar to our algorithm in that they make different-sized adjustments for different coordinates, and in this case, common features are updated less aggressively than rare ones. In contrast to our algorithm, these algorithms are not specific to these algorithms."}], "references": [{"title": "Adaptive online gradient descent", "author": ["Peter L. Bartlett", "Elad Hazan", "Alexander Rakhlin"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "The tradeoffs of large scale learning", "author": ["L\u00e9on Bottou", "Olivier Bousquet"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Online passiveaggressive algorithms", "author": ["Koby Crammer", "Ofer Dekel", "Joseph Keshet", "Shai Shalev-Shwartz", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Adaptive regularization of weight vectors", "author": ["Koby Crammer", "Alex Kulesza", "Mark Drezde"], "venue": "In NIPS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Multi-domain sentiment dataset (v2.0)", "author": ["Mark Dredze"], "venue": "http://www.cs.jhu.edu/~mdredze/datasets/sentiment/,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Confidence-weighted linear classification", "author": ["Mark Drezde", "Koby Crammer", "Fernando Pereira"], "venue": "In ICML,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In COLT,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Efficient learning algorithms for changing environments", "author": ["Elad Hazan", "C. Seshadhri"], "venue": "In ICML,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Periodic step-size adaptation in second-order gradient descent for single-pass on-line structured learning", "author": ["Chun-Nan Hsu", "Han-Shen Huang", "Yu-Ming Chang", "Yuh-Jye Lee"], "venue": "Maching Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Increased rates of convergence through learning rate adaptation", "author": ["Robert A. Jacobs"], "venue": "Neural Networks,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1988}, {"title": "Identifying suspicious URLs: an application of large-scale online learning", "author": ["Justin Ma", "Lawrence K. Saul", "Stefan Savage", "Geoffrey M. Voelker"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Adaptive bound optimization for online convex optimization", "author": ["H. Brendan McMahan", "Matthew Streeter"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Two problems with backpropagation and other steepest-descent learning procedures for networks", "author": ["Richard S. Sutton"], "venue": "In Proc. Eighth Annual Conference of the Cognitive Science Society,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1986}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In ICML,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction In the past few years, online algorithms have emerged as state-of-the-art techniques for solving large-scale machine learning problems [2, 13, 16].", "startOffset": 150, "endOffset": 161}, {"referenceID": 11, "context": "1 Introduction In the past few years, online algorithms have emerged as state-of-the-art techniques for solving large-scale machine learning problems [2, 13, 16].", "startOffset": 150, "endOffset": 161}, {"referenceID": 14, "context": "1 Introduction In the past few years, online algorithms have emerged as state-of-the-art techniques for solving large-scale machine learning problems [2, 13, 16].", "startOffset": 150, "endOffset": 161}, {"referenceID": 13, "context": "2 Motivations It is well-known that batch gradient descent performs poorly in the presence of so-called ravines, surfaces that curve more steeply in some directions than in others [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 15, "context": "Thus, for any choice of \u03b7 there exists a problem where max { D 4\u03b7 ,G\u03b7 T 2 } \u2264 Regret \u2264 D 2 2\u03b7 +G\u03b7 T 2 , where the upper bound is adapted from Zinkevich [17].", "startOffset": 152, "endOffset": 156}, {"referenceID": 0, "context": "2, setting G = 1 and setting the feasible set to [0, 1].", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "On a one-dimensional subproblem with feasible set [0, 1] and gradients of magnitude at most 1, gradient descent using learning rate 1 \u221a s on round s of the subproblem obtains regret O( \u221a S) on a subproblem of length S [17].", "startOffset": 50, "endOffset": 56}, {"referenceID": 15, "context": "On a one-dimensional subproblem with feasible set [0, 1] and gradients of magnitude at most 1, gradient descent using learning rate 1 \u221a s on round s of the subproblem obtains regret O( \u221a S) on a subproblem of length S [17].", "startOffset": 218, "endOffset": 222}, {"referenceID": 15, "context": "3 Improved Regret Bounds using Per-Coordinate Learning Rates Zinkevich [17] proved bounds on the regret of online gradient descent (which chooses xt according to Equation (1)).", "startOffset": 71, "endOffset": 75}, {"referenceID": 15, "context": "Zinkevich [17] showed that if we run gradient descent with a non-increasing sequence \u03b71, \u03b72, .", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "Our techniques can be extended to arbitrary feasible sets using a somewhat different algorithm, but the proofs are signicantly more technical [14].", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "A related result appears in [1], giving improved bounds in the case of strongly convex functions but worse constants than ours in the case of linear functions.", "startOffset": 28, "endOffset": 31}, {"referenceID": 15, "context": "Zinkevich[17] showed that, so long as our algorithm only makes use of \u25bdft(xt), we may assume without loss of generality that ft is linear, and therefore ft(x) = gt \u00b7 x for all x \u2208 F .", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "A similar observation was originally used by Zinkevich [17] to show that any algorithm for online linear optimization can be used for online convex optimization.", "startOffset": 55, "endOffset": 59}, {"referenceID": 0, "context": "For simplicity, when stating these bounds we assume that the feasible set is F = [0, 1] and that the gradients of the loss functions are componentwise upper bounded by 1 (that is, |(\u25bdft(xt))i| \u2264 1 for all t and i).", "startOffset": 81, "endOffset": 87}, {"referenceID": 0, "context": "[1] give an online convex optimization algorithm whose regret is O (", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "2 Tighter bounds in terms of variance Hazan and Kale [9] give a bound on gradient descent\u2019s regret in terms of the variance of the sequence of gradients.", "startOffset": 53, "endOffset": 56}, {"referenceID": 0, "context": "For example, suppose the feasible set is [0, 1], ft(x) = x for the first T 2 rounds and ft(x) = \u2212x thereafter.", "startOffset": 41, "endOffset": 47}, {"referenceID": 8, "context": "Hazan and Seshadhri [10] define adaptive regret as the maximum, over all intervals [T0, T1], of the regret \u2211T1 t=T0 ft(xt) \u2212 minx\u2208F {", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "Holding H constant for simplicity, the adaptive regret bound just stated implies that the algorithm of Hazan and Seshadhri [10] obtains RZ = O((N + 1) log 2 T ), where N is the number of values of t for which zt 6= zt+1 (this follows by summing adaptive regret over the N + 1 intervals where zt is constant).", "startOffset": 123, "endOffset": 127}, {"referenceID": 2, "context": "1 Online binary classification We first compare the performance of online gradient descent with that of two recent algorithms for text classification: the Passive-Aggressive (PA) algorithm [4], and confidence-weighted (CW) linear classification [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 5, "context": "1 Online binary classification We first compare the performance of online gradient descent with that of two recent algorithms for text classification: the Passive-Aggressive (PA) algorithm [4], and confidence-weighted (CW) linear classification [7].", "startOffset": 245, "endOffset": 248}, {"referenceID": 11, "context": "The latter algorithm has been demonstrated to have state-of-the-art performance on large real-world problems [13].", "startOffset": 109, "endOffset": 113}, {"referenceID": 4, "context": "We used four sentiment classification data sets (Books, Dvd, Electronics, and Kitchen), available from [6], each with 1000 positive examples and 1000 negative examples, as well as the scaled versions of the rcv1.", "startOffset": 103, "endOffset": 106}, {"referenceID": 10, "context": "There the focus has been on empirical performance in the batch setting, and a large number of algorithms have been developed; see for example [12].", "startOffset": 142, "endOffset": 146}, {"referenceID": 9, "context": "[11] gave an algorithm for choosing per-coordinate learning rates for gradient descent, derive asymptotic rates of convergence in the batch setting, and present a number of positive experimental results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Confidence-weighted linear classification [7] and AROW [5] are similar to our algorithm in that they make different-sized adjustments for different coordinates, and in that common features are updated less aggressively than rare ones.", "startOffset": 42, "endOffset": 45}, {"referenceID": 3, "context": "Confidence-weighted linear classification [7] and AROW [5] are similar to our algorithm in that they make different-sized adjustments for different coordinates, and in that common features are updated less aggressively than rare ones.", "startOffset": 55, "endOffset": 58}], "year": 2010, "abstractText": "We analyze and evaluate an online gradient descent algorithm with adaptive per-coordinate adjustment of learning rates. Our algorithm can be thought of as an online version of batch gradient descent with a diagonal preconditioner. This approach leads to regret bounds that are stronger than those of standard online gradient descent for general online convex optimization problems. Experimentally, we show that our algorithm is competitive with state-of-the-art algorithms for large scale machine learning problems.", "creator": "LaTeX with hyperref package"}}}