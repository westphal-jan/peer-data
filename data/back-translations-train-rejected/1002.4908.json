{"id": "1002.4908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Feb-2010", "title": "Adaptive Bound Optimization for Online Convex Optimization", "abstract": "We introduce a new online convex optimization algorithm that adaptively chooses its regularization function based on the loss functions observed so far. This is in contrast to previous algorithms that use a fixed regularization function such as L2-squared, and modify it only via a single time-dependent parameter. Our algorithm's regret bounds are worst-case optimal, and for certain realistic classes of loss functions they are much better than existing bounds. These bounds are problem-dependent, which means they can exploit the structure of the actual problem instance. Critically, however, our algorithm does not need to know this structure in advance. Rather, we prove competitive guarantees that show the algorithm provides a bound within a constant factor of the best possible bound (of a certain functional form) in hindsight.", "histories": [["v1", "Fri, 26 Feb 2010 01:36:34 GMT  (63kb,S)", "http://arxiv.org/abs/1002.4908v1", null], ["v2", "Wed, 7 Jul 2010 19:07:16 GMT  (21kb)", "http://arxiv.org/abs/1002.4908v2", "Updates to match final COLT version"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["h brendan mcmahan", "matthew streeter"], "accepted": false, "id": "1002.4908"}, "pdf": {"name": "1002.4908.pdf", "metadata": {"source": "CRF", "title": "Adaptive Bound Optimization for Online Convex Optimization", "authors": ["H. Brendan McMahan", "Matthew Streeter"], "emails": ["mcmahan@google.com", "mstreeter@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 100 2.49 08v1 [cs.LG] 2 6"}, {"heading": "1 Introduction", "text": "We consider online convex optimization to be inevitable in the context of complete information feedback. A closed, limited convex constant BR-Rn is given as input, and in each turn T = 1,.., T we have to select a point xt-F. A convex convex loss function ft is then revealed, and we suffer loss ft (xt). Our regret at the end of the T rounds is reset convex constant T = 1ft (xt) \u2212 min x-FT constant t = 1ft (x). (1) Existing algorithms for online convex optimization are optimal in terms of certain basic quantities. In particular, the online convex constant T-constant reaches a limit of O (DM constant T) \u2212 min x-FT constant t = 1ft (x). (1) Existing algorithms for online convex optimization are optimal in terms of certain basic quantities."}, {"heading": "1.1 Follow the proximally-regularized leader", "text": "We analyze the Regulated Market Leader (FTRL) algorithm, which formrt the regulatory functions of the (x) = 12 (Q1) = 12 (Q2) = 22 (Q2), where Qt is a positive semidefinitive matrix. (2) Unlike other FTRL algorithms, such as Xiao's dual mean method [2009], we center the additional regulation method at the current xt location instead of the point of origin. (2) We call this algorithm the proximally regulated market leader (FTPRL)."}, {"heading": "1.2 The practical importance of adaptive regularization", "text": "In recent years, online algorithms have emerged as the most advanced techniques for solving major machine learning problems (Bottou and Bousquet, 2008, Ma and al., 2009, Zhang, 2004). Two canonical examples of such major learning problems are text classification on large datasets and predicting click-through rates for ads on a search engine. Extremely large learning functions can be viewed online for such problems, but many characteristics occur only rarely, while few problems occur very frequently. Our diagonal adaptation algorithm provides improved boundaries for problems such as theory. As an example, we assume that the ith component of FTT is not significant in each round (xt) (henceforth gt, i) is 1 with a probability i \u2212 \u03b1, and is 0 otherwise for some phenomena [1, 2). Such severely limited distributions are common in text classification applications, where there is a feature for each word."}, {"heading": "1.3 Adaptive algorithms and competitive ratios", "text": "In Section 3, we present specific models for selecting the regulatory matrices Qt for FTPRL and show that these algorithms can be developed within a constant factor for the best post-hoc choice of matrices, namelyinf ~ QTBR (~ Qt, ~ gt) (4), where QT + can develop a series of approved matrices; Sn + is the series of symmetric positive semi-definite matrices, with Sn + the corresponding series of symmetric positive definite matrices. We consider three different options for Q: the series of coordinated constant matrices Qconst = {s \u00b2); the series of non-negative diagonal matrices, Qdiag = {diag = {diag 1,."}, {"heading": "1.4 Notation and technical background", "text": "Similarly, we write Q1: t for a sum of the matrices Qt, and f1: t to denote the function f1: t (x) = \"regret\" t = \"regret\" (x). We write xTy or x \u00b7 y for the inner product between x, y \"Rn.\" The ith entry in a vector x is called \"xi\" R; if we have a sequence of vectors, the ith entry is \"xt,\" i \"R. We use\" f \"(x) to denote the set of subgradients of f, which at x.Recall A\" Sn + + means \"x\" 6 = 0, xTAx > 0. We use the generalized inequality A \"0, if A\" Sn + +, and similarly A \"B,\" if B \"xTAx < xTBx."}, {"heading": "2 Analysis of FTPRL", "text": "In this area, we are able to relate to the question of whether we are able to relate to the question, to what extent we are able to respond to the question, to what extent we are able to respond to the question, to what extent we are able to respond to the question, to what extent we are able to respond to the question, to what extent we are able to respond to the question, to what extent we are able to respond to the question, to what extent we are able to respond to the question, to what extent we are able to respond to the question, to what extent we are able to respond to the question."}, {"heading": "3 Specific Adaptive Algorithms and Competitive Ratios", "text": "Before coming to the specific results, we note several results that will be useful in the arguments below. To prove that adaptive schemes for selecting Qt compete well for the bound optimization problem, we need to compare the limits obtained by the adaptive scheme with the optimal post-hoc limit of Equation (4). Suppose the sequence Q1,.. QT is optimal for Equation (4), and consider the alternative sequence Q \u2032 1 = Q1: T and Q \u2032 t = 0 for t > 1. Starting from the fact that Q1: t Q1: t \u2212 1: t Q \u2212 11: t \u2212 1, it is easy to show that the alternative sequence Q \u2032 1 = Q1 and Q \u2032 t = 0 for t > 1 is always optimal."}, {"heading": "3.1 Adaptive coordinate-constant regularization", "text": "We derive the limits in which Qt is selected from the Qconst set, and show that this algorithm is within a factor 2 of using the best possible constant regularization strength \u03bbI. \u2212 This algorithm reaches a limit of O (DM), where D is the diameter of the feasible region, and M has a limit with respect to these parameters [Abernethy et al., 2008]. Consequence 1. Assumption F has a diameter of L2 D. If we execute FTPRL with diagonal matrices that are such that (Q1: t) ii = \u03b1 \u00b2 t = 2 \u00b0 GtD1In the case where F has 0 width in any direction, the infimum will not be reached by a finite Q, but by a sequence that has 0 penalty bound (on the right) to the components of the gradient in the direction of 0 latitude, we will not exceed the limit of 1 \u00b0 C, with some inputs going in the direction of Q."}, {"heading": "3.2 Adaptive diagonal regularization", "text": "In this section we put and analyze FTPRL-Diag, a specialization of FTPRL that uses regularization matrices of Qdiag (= 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 + 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1 = 1"}, {"heading": "3.3 A post-hoc bound for diagonal regularization on Lp balls", "text": "Suppose the practicable quantity F is a unit of Lp-ball, that is F = {x | q = q = q = q = q = 1). We consider the post-hoc bound optimization problem of equation (11) with Q = Qdiag. Our results are summarized in the following theory: For p > 2, the optimal regulation matrix for BR in Qdiag is not the coordinate constant (i.e., it is not contained in Qconst), except in the degenerated case where Gi + T = 1 g 2 t, i is the same for all i. However, for p \u2264 2, the optimal regulation matrix in Qdiag is always to Qconst.Since F is symmetrical, the optimal post-hoc choice will be in the form of equation (11)."}, {"heading": "3.4 Full matrix regularization on hyperspheres and hyperellipsoids", "text": "In this section we develop an algorithm for the biggest to smallest problem. (...) In this section we develop an algorithm for the biggest to smallest problem. (...) In this section we develop an algorithm for the biggest to smallest problem. (...) In this section we develop an algorithm for the biggest to smallest problem. (...) In this section we develop an algorithm for the biggest to smallest problem. (...) In this section we develop an algorithm for the biggest to smallest problem. (...) We show that we are developed rather as customization schemes specifically for linear transformations of standard balls, it is sufficient (from the point of view of analysis of FTPRL) to consider unit norm balls when appropriate pre-processing is applied. (...) In the same way that pre-conditioning can subgradient speed. (...) We show this approach significantly improved (...)."}, {"heading": "4 Related work", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "5 Conclusions", "text": "In this paper, we analyzed a new algorithm for online convex optimization that incorporates ideas both from the online lineage of subgradients and from the succession of the regulated guide. In our analysis of this algorithm, we show that the learning rates that occur in standard limits can be replaced by positive semi-defined matrices. The additional degrees of freedom that these general learning rates provide provide provide the key to demonstrating better limits of regret. We have characterized the types of workable sets in which this technique can lead to significant gains, and demonstrated that while it is not helpful in the hypersphere, it can have dramatic effects when the workable amount is a hyperrectangle. The diagonal adjustment algorithm that we have introduced can be regarded as a step-by-step optimization of the formula for the final limit of regret. In the case where the workable amount is really a hyperrectangle, this allows us to guarantee our final limit of the persistent problem would have been known from within a small factor."}, {"heading": "A A Proof of the FTRL Bound", "text": "In this section, we offer proof of Lemma 1. The high level structure of our evidence follows Kalai and Vempala's analysis of the consequences of the disrupted leadership algorithm by proving the limits of three quantities: 1. the regret of a hypothetical leadership algorithm (BTL) based on turn t (argmin x x F f1: t (x), 2. the difference between the regret of the BTL and that of the regulated leadership algorithm (BTRL), the playsx x = argmin x x F (r1: t (x) + f1: t (x))) = xt + 3. the difference between the regret of the BTRL and that of the FTRL. As shown in [Kalai and Vempala, 2005], the BTL algorithm has remorse."}, {"heading": "B Proof of Lemma 5", "text": "Proof: The lemmas clearly meet n = 1. Attach n and assume that the lemmas apply to n \u2212 1. Consequently, n \u2211 i = 1xi \u221a \u2211 i j = 1 xj \u2264 2 \u221a \u221a \u221a n \u2212 1 \u2211 i = 1xi + xn \u221a \u0445 n = 1 xi = 2 \u221a Z \u2212 x + x \u221a Z, where we define Z = \u2211 ni = 1 xi and x = xn. The derivative of the right side in relation to x is \u2212 1 \u221a Z \u2212 x + 1 \u221a Z, which is negative for x > 0. Thus, under condition x \u2265 0 the right side is maximized at x = 0 and is therefore at most 2 \u221a Z."}], "references": [{"title": "Optimal strategies and minimax lower bounds for online convex games", "author": ["Jacob Abernethy", "Peter L. Bartlett", "Alexander Rakhlin", "Ambuj Tewari"], "venue": "In COLT,", "citeRegEx": "Abernethy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Abernethy et al\\.", "year": 2008}, {"title": "Adaptive online gradient descent", "author": ["Peter L. Bartlett", "Elad Hazan", "Alexander Rakhlin"], "venue": "In NIPS,", "citeRegEx": "Bartlett et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2008}, {"title": "The tradeoffs of large scale learning", "author": ["L\u00e9on Bottou", "Olivier Bousquet"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bottou and Bousquet.,? \\Q2008\\E", "shortCiteRegEx": "Bottou and Bousquet.", "year": 2008}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Adaptive regularization of weight vectors", "author": ["Koby Crammer", "Alex Kulesza", "Mark Drezde"], "venue": "In NIPS,", "citeRegEx": "Crammer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2009}, {"title": "Proximal regularization for online and batch learning", "author": ["Chuong B. Do", "Quoc V. Le", "Chuan-Sheng Foo"], "venue": "In ICML,", "citeRegEx": "Do et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Do et al\\.", "year": 2009}, {"title": "Confidence-weighted linear classification", "author": ["Mark Drezde", "Koby Crammer", "Fernando Pereira"], "venue": "In ICML,", "citeRegEx": "Drezde et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drezde et al\\.", "year": 2008}, {"title": "Efficient learning using forward-backward splitting", "author": ["John Duchi", "Yoram Singer"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Duchi and Singer.,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer.", "year": 2009}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": null, "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and Systems Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "Identifying suspicious URLs: an application of large-scale online learning", "author": ["Justin Ma", "Lawrence K. Saul", "Stefan Savage", "Geoffrey M. Voelker"], "venue": "In ICML,", "citeRegEx": "Ma et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2009}, {"title": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In ICML,", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Less regret via online conditioning", "author": ["Matthew Streeter", "H. Brendan McMahan"], "venue": "Submitted.,", "citeRegEx": "Streeter and McMahan.,? \\Q2010\\E", "shortCiteRegEx": "Streeter and McMahan.", "year": 2010}, {"title": "Dual averaging method for regularized stochastic learning and online optimization", "author": ["Lin Xiao"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Xiao.,? \\Q2009\\E", "shortCiteRegEx": "Xiao.", "year": 2009}, {"title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "author": ["Tong Zhang"], "venue": "In ICML,", "citeRegEx": "Zhang.,? \\Q2004\\E", "shortCiteRegEx": "Zhang.", "year": 2004}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}, {"title": "Theoretical guarantees for algorithms in multi-agent settings", "author": ["Martin Zinkevich"], "venue": "PhD thesis, Pittsburgh, PA,", "citeRegEx": "Zinkevich.,? \\Q2004\\E", "shortCiteRegEx": "Zinkevich.", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "In particular, although this bound is minimax optimal when the feasible set is a hypersphere [Abernethy et al., 2008], we will see that it can be arbitrarily bad when the feasible set is the hypercube.", "startOffset": 93, "endOffset": 117}, {"referenceID": 4, "context": "This proximal centering of additional regularization is similar in spirit to the optimization solved by online gradient descent (and more generally, online mirror descent, [Cesa-Bianchi and Lugosi, 2006]).", "startOffset": 172, "endOffset": 203}, {"referenceID": 13, "context": "(2) In contrast to other FTRL algorithms, such as the dual averaging method of Xiao [2009], we center the additional regularization at the current feasible point xt rather than at the origin.", "startOffset": 79, "endOffset": 91}, {"referenceID": 13, "context": "In concurrent work [Streeter and McMahan, 2010], we showed that for some problem families, a per-coordinate learning rate for online gradient descent provides asymptotically less regret than even the best non-increasing global learning rate (chosen in hindsight, given the observed loss functions).", "startOffset": 19, "endOffset": 47}, {"referenceID": 3, "context": "For B \u2208 S +, we write B for the square root of B, the unique X \u2208 S + such that XX = B (see, for example, Boyd and Vandenberghe [2004, A.5.2]). We also make use of the fact that any A \u2208 S + can be factored as A = PDP where PP = I and D = diag(\u03bb1, . . . , \u03bbn) where \u03bbi are the eigenvalues of A. Following the arguments of Zinkevich [2003], for the remainder we restrict our attention to linear functions.", "startOffset": 105, "endOffset": 337}, {"referenceID": 10, "context": "The proof is similar to arguments of [Kalai and Vempala, 2005], and is given in Appendix A.", "startOffset": 37, "endOffset": 62}, {"referenceID": 13, "context": "We will use the following Lemma from [Streeter and McMahan, 2010].", "startOffset": 37, "endOffset": 65}, {"referenceID": 0, "context": "This algorithm achieves a bound of O(DM \u221a T ) where D is the diameter of the feasible region and M is a bound on \u2016gt\u20162, matching the best possible bounds in terms of these parameters [Abernethy et al., 2008].", "startOffset": 183, "endOffset": 207}], "year": 2017, "abstractText": "We introduce a new online convex optimization algorithm that adaptively chooses its regularization function based on the loss functions observed so far. This is in contrast to previous algorithms that use a fixed regularization function such as L2-squared, and modify it only via a single time-dependent parameter. Our algorithm\u2019s regret bounds are worst-case optimal, and for certain realistic classes of loss functions they are much better than existing bounds. These bounds are problem-dependent, which means they can exploit the structure of the actual problem instance. Critically, however, our algorithm does not need to know this structure in advance. Rather, we prove competitive guarantees that show the algorithm provides a bound within a constant factor of the best possible bound (of a certain functional form) in hindsight.", "creator": "LaTeX with hyperref package"}}}