{"id": "1003.0516", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2010", "title": "Model Selection with the Loss Rank Principle", "abstract": "A key issue in statistics and machine learning is to automatically select the \"right\" model complexity, e.g., the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials. We suggest a novel principle - the Loss Rank Principle (LoRP) - for model selection in regression and classification. It is based on the loss rank, which counts how many other (fictitious) data would be fitted better. LoRP selects the model that has minimal loss rank. Unlike most penalized maximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the regression functions and the loss function. It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN.", "histories": [["v1", "Tue, 2 Mar 2010 08:21:07 GMT  (41kb)", "http://arxiv.org/abs/1003.0516v1", "31 LaTeX pages, 1 figure"]], "COMMENTS": "31 LaTeX pages, 1 figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marcus hutter", "minh-ngoc tran"], "accepted": false, "id": "1003.0516"}, "pdf": {"name": "1003.0516.pdf", "metadata": {"source": "CRF", "title": "Model Selection with the Loss Rank Principle", "authors": ["Marcus Hutter"], "emails": ["RSISE@ANU", "SML@NICTA,", "marcus@hutter1.net", "ngoctm@nus.edu.sg"], "sections": [{"heading": null, "text": "ar Xiv: 100 3.05 16v1 [cs.LG] contents"}, {"heading": "1 Introduction 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 The Loss Rank Principle 4", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 LoRP for y-Linear Models 8", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Optimality Properties of LoRP for Variable Selection 11", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 Experiments 13", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Comparison to Gaussian Bayesian Linear Regression 17", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Comparison to other Model Selection Schemes 19", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 Loss Functions and their Selection 23", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 Self-Consistent Regression 24", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "10 Nearest Neighbors Classification 26", "text": "11 Conclusion and Outlook 28 References 30KeywordsModel selection, loss strand principle, non-parametric regression, classification, general loss function, k nearest neighbours."}, {"heading": "1 Introduction", "text": "This is an issue where we want to determine the functional relationship between individual countries and individual countries. (This also applies to individual countries). (This applies not only to individual countries, but also to individual countries. \"(This also applies to individual countries). (This also applies to individual countries.) (This does not apply to individual countries.) (This also applies to individual countries.) (This also applies to individual countries.) (This also applies to individual countries.) (This also applies to individual countries.) (This also applies to individual countries.) (This does not apply to individual countries.) (It applies to individual countries.) (This also applies to individual countries.) (This also applies to individual countries.) (This does not apply to individual countries. (This also applies to individual countries.) (It applies to individual countries.\" (It applies to individual countries. \")\" It applies. \"(This also applies to individual countries.\")"}, {"heading": "2 The Loss Rank Principle", "text": "After a brief introduction to the \"true\" values (xi), we write (x). (c) D (x). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c). (c)."}, {"heading": "3 LoRP for y-Linear Models", "text": "In this section we will consider the important class of y-linear regressions with quadratic loss function. \"y-linear regression\" assumes linearity only in y and the dependence on x can be arbitrary. This class is richer than it may appear. It includes the normal linear regression model, which can be efficiently compressed into time O (n3) (theorem 10). For the special case of projective regression, e.g. linear basis of functional regression (example 9), we can even analytically determine the regulation parameters (theorem 11).y-linear regression."}, {"heading": "4 Optimality Properties of LoRP for Variable Se-", "text": "In this section we will discuss some of the theoretical properties of LoRP for variable (also referred to as attribute or attribute) selection.Variable selection is probably the most basic and important topic in linear regression analysis. In the initial phase of regression analysis, a large number of potential covariants is often introduced; one must then select a smaller subset of covariates to adjust / interpret the data. There are two main objectives of variable selection, one is model identification, the other is regression estimation. The former aims to identify the true subset of data, while the latter aims efficiently at the regression function, i.e., the selection of a subset that has the minimum average error loss. Note whether there is a selection or not is still an open question [Yan05, Gru 04]."}, {"heading": "5 Experiments", "text": "In this section we present a simulation study for LoRP, compare it with other methods and also show how LoRP can be used for some specific problems such as the selection of tuning parameters for kNN and spline regression. All experiments are performed using MATLAB software and the source code is freely available at http: / / www.hutter1.net / ai / lorpcode.zip.zip [http: / / www.hutter1.net / ai / lorpcode.zip] where \u03b2 is the vector of coefficients with some zero entries, we assume that \u03b20 = 0, otherwise we can center the reaction vectors y and standardize the design matrix X and exclude \u03b20 from the model."}, {"heading": "6 Comparison to Gaussian Bayesian Linear Re-", "text": "??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "7 Comparison to other Model Selection Schemes", "text": "In this section, we give a brief introduction to the PML for (semi) parametric regression (max.): \"We have the data in d-dimensional models.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" We. \"\" \"\" We. \"\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" We. \".\" \"We.\" \"\" We. \"\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" We. \"\" We. \"We.\" We. \"We.\" We. \"\" We. \"We.\" We. \"We.\" We. \"We.\" \"We.\" We. \"\" We. \"We.\" \"We.\" \"We.\" We. \"\""}, {"heading": "8 Loss Functions and their Selection", "text": "The general additive loss. \"Since V (L) is a linear transformation of this ball with the transformation (I), we can easily generalize to non-square losses (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (I). (................. (....). (....). (....). (..... (....). (....). (....). (..... (....). (....). (. (....). (....). (....). (. (....). (....). (....). (....). (I. (....). (....). (....). (....). (....). (....). (....). (....). (....). (I.). (.....). (....). (.....). (....). (I. (....). (....).). (.....). (I. (....). (....). (.....). (....). (.....). (.....). (.....).). (..... (I. (.....). (.....). (....). (.....).). (..... (.....).).). (I. (.....). (.....). (I.). (I.). (I.). (.........).).). (I.). (......... (I.).). (I.).). (I.).). (................. (I. (I.).)."}, {"heading": "9 Self-Consistent Regression", "text": "So far, we have considered only \"on-data\" regression: LoRP depends only on the data we (possibly) collect based on data (possibly). (...) We have only the data (...) we (...) collect based on data (...). (...) Firstly, this could facilitate the specification of regression functions, secondly, it is a canonical way for interpolation (LoRP cannot distinguish between r, which are identical to D), and thirdly, we show that many standard regressors (kNN, kernel, LBFR) are consistent in the sense that they are canonical. (...) We limit our exposure to linear regression. (...) A linear regressor is entirely determined by the n functions mj (6), but not by the matrix function M (7)."}, {"heading": "10 Nearest Neighbors Classification", "text": "To get more insight into LoRP, we are looking for a case that allows an analytical solution. In general, the determinant detS\u03b1 cannot be calculated analytically, but we consider the d = 1 dimensional case first. We assume that x = (1,2,3,..., n), a circular metric d (xi, xj) = d (i, j) = min {i \u2212 j \u00b2, n \u2212 j \u00b2, and odd k = 1 dimensional case. We assume that we use a circular metric d (xi, xj) = d (i, j) = min {i \u2212 j \u00b2, n \u2212 j \u00b2, and odd k \u00b2. The kNN regression matrixMij = bi \u2212 j = j = 1 k, if d (i, j)."}, {"heading": "11 Conclusion and Outlook", "text": "In fact, it is not that we will be able to find a solution that addresses people's problems, \"he said.\" But it is not that we will engage in a solution. \"He added,\" It is not that we will engage in a solution. \""}, {"heading": "Appendix: List of Abbreviations and Notations", "text": "AIC = Akaike information criterion. BIC = Bayesian information criterion. BMS = Bayesian model selection kNN = k Closest neighbors. LBFR = Linear Basis Function Regression. LoRP = Loss Rank Princion. LRC = Loss Rank Code. MAP = Maximum a Posterior. MDL = Minimum Description Length. ML = Maximum Likelihood. PML = Penalized Maximum Likelihood. D = {(x1, y1),..., (xn, yn)} = Observation data. D = {D} = Set of all possible data. X \u00b7 Y = Observation space. x = (x1,..., xn) = Vector of x-observations, similar y. f: X \u2192 Y = Function dependence between x and y. F = (\"small\") Class of functions f. H = Class of stochastic hypotheses / models."}], "references": [{"title": "Information theory and an extension of the maximum likelihood principle", "author": ["H. Akaike"], "venue": "In Proc. 2nd International Symposium on Information Theory,", "citeRegEx": "Akaike.,? \\Q1973\\E", "shortCiteRegEx": "Akaike.", "year": 1973}, {"title": "The relationship between variable selection and data augmentation and a method for prediction", "author": ["D. Allen"], "venue": null, "citeRegEx": "Allen.,? \\Q1974\\E", "shortCiteRegEx": "Allen.", "year": 1974}, {"title": "Model selection and error estimation", "author": ["P. Bartlett", "S. Boucheron", "G. Lugosi"], "venue": "Machine Learning,", "citeRegEx": "Bartlett et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2002}, {"title": "Testing the order of a model", "author": ["A. Chambaz"], "venue": "Ann. Stat.,", "citeRegEx": "Chambaz.,? \\Q2006\\E", "shortCiteRegEx": "Chambaz.", "year": 2006}, {"title": "Smoothing noisy data with spline functions: estimating the correct degree of smoothing by the methods of generalized cross-validation", "author": ["P. Craven", "G. Wahba"], "venue": "Numerische Mathematik,", "citeRegEx": "Craven and Wahba.,? \\Q1979\\E", "shortCiteRegEx": "Craven and Wahba.", "year": 1979}, {"title": "An Introduction to the Bootstrap", "author": ["B. Efron", "R. Tibshirani"], "venue": null, "citeRegEx": "Efron and Tibshirani.,? \\Q1993\\E", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1993}, {"title": "Tutorial on minimum description length. In Minimum Description Length: recent advances in theory and practice, page Chapters 1 and 2", "author": ["P.D. Gr\u00fcnwald"], "venue": "http://www.cwi.nl/\u223cpdg/ftp/mdlintro.pdf", "citeRegEx": "Gr\u00fcnwald.,? \\Q2004\\E", "shortCiteRegEx": "Gr\u00fcnwald.", "year": 2004}, {"title": "Learning Kernel Classifiers", "author": ["R. Herbrich"], "venue": null, "citeRegEx": "Herbrich.,? \\Q2002\\E", "shortCiteRegEx": "Herbrich.", "year": 2002}, {"title": "Regression and time series model selection in small samples", "author": ["C.M. Hurvich", "C.L. Tsai"], "venue": null, "citeRegEx": "Hurvich and Tsai.,? \\Q1989\\E", "shortCiteRegEx": "Hurvich and Tsai.", "year": 1989}, {"title": "The Elements of Statistical Learning", "author": ["T. Hastie", "R. Tibshirani", "J.H. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "The loss rank principle for model selection", "author": ["M. Hutter"], "venue": "In Proc. 20th Annual Conf. on Learning Theory (COLT\u201907),", "citeRegEx": "Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Hutter.", "year": 2007}, {"title": "Rademacher penalties and structural risk minimization", "author": ["V. Koltchinskii"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "Koltchinskii.,? \\Q2001\\E", "shortCiteRegEx": "Koltchinskii.", "year": 2001}, {"title": "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators", "author": ["P. Liang", "M. Jordan"], "venue": "In Proc. 25th International Conf. on Machine Learning (ICML-2008),", "citeRegEx": "Liang and Jordan.,? \\Q2008\\E", "shortCiteRegEx": "Liang and Jordan.", "year": 2008}, {"title": "Subset Selection in Regression", "author": ["A. Miller"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Miller.,? \\Q2002\\E", "shortCiteRegEx": "Miller.", "year": 2002}, {"title": "Approximation of the determinant of large sparse symmetric positive definite matrices", "author": ["A. Reusken"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Reusken.,? \\Q2002\\E", "shortCiteRegEx": "Reusken.", "year": 2002}, {"title": "Modeling by shortest data", "author": ["J.J. Rissanen"], "venue": "description. Automatica,", "citeRegEx": "Rissanen.,? \\Q1978\\E", "shortCiteRegEx": "Rissanen.", "year": 1978}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Annals of Statistics,", "citeRegEx": "Schwarz.,? \\Q1978\\E", "shortCiteRegEx": "Schwarz.", "year": 1978}, {"title": "An asymptotic theory for linear model selection", "author": ["J. Shao"], "venue": "Statistica Sinica,", "citeRegEx": "Shao.,? \\Q1997\\E", "shortCiteRegEx": "Shao.", "year": 1997}, {"title": "Asymptotic mean efficiency of a selection of regression variables", "author": ["R. Shibata"], "venue": "Annals of the Institute of Statistical Mathematics,", "citeRegEx": "Shibata.,? \\Q1983\\E", "shortCiteRegEx": "Shibata.", "year": 1983}, {"title": "Tuning parameter selectors for the smoothly clipped absolute deviation method", "author": ["H. Wang", "R. Li", "C.L. Tsai"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Extended stochastic complexity and minimax relative loss analysis", "author": ["K. Yamanishi"], "venue": "Proc. 10th International Conference on Algorithmic Learning Theory - ALT\u2019", "citeRegEx": "Yamanishi.,? \\Q1999\\E", "shortCiteRegEx": "Yamanishi.", "year": 1999}, {"title": "Can the strengths of aic and bic be shared? a conflict between model identification and regression estimation", "author": ["Y. Yang"], "venue": null, "citeRegEx": "Yang.,? \\Q2005\\E", "shortCiteRegEx": "Yang.", "year": 2005}], "referenceMentions": [], "year": 2010, "abstractText": "A key issue in statistics and machine learning is to automatically select the \u201cright\u201d model complexity, e.g., the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials. We suggest a novel principle the Loss Rank Principle (LoRP) for model selection in regression and classification. It is based on the loss rank, which counts how many other (fictitious) data would be fitted better. LoRP selects the model that has minimal loss rank. Unlike most penalized maximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the regression functions and the loss function. It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN.", "creator": "LaTeX with hyperref package"}}}