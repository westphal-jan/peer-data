{"id": "1003.0529", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2010", "title": "A Unified Algorithmic Framework for Multi-Dimensional Scaling", "abstract": "In this paper, we propose a unified algorithmic framework for solving many known variants of \\mds. Our algorithm is a simple iterative scheme with guaranteed convergence, and is \\emph{modular}; by changing the internals of a single subroutine in the algorithm, we can switch cost functions and target spaces easily. In addition to the formal guarantees of convergence, our algorithms are accurate; in most cases, they converge to better quality solutions than existing methods, in comparable time. We expect that this framework will be useful for a number of \\mds variants that have not yet been studied.", "histories": [["v1", "Tue, 2 Mar 2010 09:11:44 GMT  (147kb,D)", "https://arxiv.org/abs/1003.0529v1", "18 pages, 7 figures"], ["v2", "Tue, 30 Mar 2010 17:21:53 GMT  (145kb,D)", "http://arxiv.org/abs/1003.0529v2", "18 pages, 7 figures. This version fixes a bug in the proof of Theorem 6.1 (dimensionality reduction for spherical data). The statement of the result remains the same."]], "COMMENTS": "18 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CG cs.CV", "authors": ["arvind agarwal", "jeff m phillips", "suresh venkatasubramanian"], "accepted": false, "id": "1003.0529"}, "pdf": {"name": "1003.0529.pdf", "metadata": {"source": "CRF", "title": "A Unified Algorithmic Framework for Multi-Dimensional Scaling", "authors": ["Arvind Agarwal", "Jeff M. Phillips", "Suresh Venkatasubramanian"], "emails": [], "sections": [{"heading": null, "text": "Our framework extends to embedding high-dimensional dots lying on a sphere into dots on a low-dimensional sphere while maintaining geodesic distances. As a compliment to this result, we also extend the JohnsonLindenstrauss Lemma to include this spherical environment in which projection onto a random O (1 / \"2) log n) -dimensional sphere causes a\" -distortion."}, {"heading": "1 Introduction", "text": "Multidimensional Scaling (MDS) [23, 10, 3] is a widely used method for embedding a general distance matrix in a low-dimensional Euclidean space, which is used both as a pre-processing step for many problems and as a standalone visualization tool. MDS has been studied and used in psychology since the 1930s to visualize and analyze datasets where the only input is a distance matrix. More recently, MDS has become a standard dimension reduction and embedding technique to manage the complexity of handling large high-dimensional datasets [8, 9, 31, 6]. In general, the problem of embedding an arbitrary distance matrix in a fixed-dimensional euclidean space with minimal error is non-convex (due to dimensionality limitation). In addition to the standard formulation [12], many variants of MDS have therefore been proposed based on the change in the basic error function."}, {"heading": "1.1 Our Work", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2 Background and Existing Methods", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3 Definitions", "text": "Let's say D = (di j) is a n \u00b7 n matrix that represents the distances between all pairs of points in a set Y = {y1,.. yn}, although our method does not formally require it. The multidimensional scaling problem takes as input Y, D, and k and requires a mapping \u00b5: Y \u2192 X from Y to a series of points X in a k-dimensional space T. So the difference between the original and resulting distances is minimized. There are many different ways to measure the difference between distances, and these can be done by the following general functionC (X, D) = [x i, x j) \u2212 jErr (f, x) \u2212 j) \u2212 j, where Err measures the discrepancy between the source and the target distances, and f denotes the function that measures the distance in the target area. \u2022 T = Rk, Err () = 2, f (x) Existence is a problem."}, {"heading": "4 Algorithm", "text": "\"We present our cost function as the sum of the costs for each point x i, and so in each and every step of fixing i p.\" We assume that we have an initial embedding in our algorithm. (D) Our experiments suggest that the SVD-based approach [35] is almost always the optimal way to capture the algorithm, and we use it unless it is specified specifically in another way. (D) We can ignore any MDS strategy to obtain initial seeds X. (X, D) for i = 1 to n dox i \u2190 PLACEi (X, D) {these updates x i, X} end foruntil (X, D) < t) {for a fixed threshold} return XPLACECENTER works by using a technique from the block relaxation class of Heuristics. The cost function can be expressed as the sum of the costs for each point x i, and thus in each inner loop."}, {"heading": "4.2 Implementing Recenter", "text": "Up to this point, the description of PLACECENTER and PLACE is generic and does not require specification of Err and f. In fact, the entire domain specificity of the method appears in RECENTER, which solves the minimum sum problem. We will now show how different implementations of RECENTER allow us to solve the various variants of MDS discussed above. < The original MDS: fMDSRecall from Section 3 that the fMDS problem is defined by Err (1 / n), and f (x, x) = different variants of the MDS discussed above. < The minimum of this function is achieved by Err (1 / n)."}, {"heading": "4.2.3 Spherical MDS", "text": "The spherical MDS presents particular challenges for the implementation of the RECENTER problem. First, it is no longer obvious what the definition of x-point should be, since the \"spheres\" surrounding points must also be on the sphere. Second, we consider the case in which Err (\u03b4) = 2 and f (x, x) is given by the geodesic distance on the sphere. Unlike in the case of Rk, we can no longer solve the minimizer of g (x) by calculating the centrification of the given points, since this centrification will generally not be on the sphere, and even the calculation of the centrification followed by a projection on the sphere will not guarantee an optimal calculation. The first problem can easily be solved. Instead of drawing spheres around each x-point, we draw geodesic spheres, which are the number of points at a fixed geodesic distance from x-point."}, {"heading": "4.3 Convergence Proofs", "text": "Here we prove that each step of the PLACECENTER method converges as long as the recursively called procedures decrease the relevant cost functions. Convergence is defined in relation to a cost function, so that an algorithm converges if it decreases in each step until the algorithm ends. Theorem 4.1. If each call to PLACEi (X, D) decreases the cost of PLACEi (X, D, x i), then PLACECENTER (D) converges in relation to C (\u00b7, D). Proof. Let us leave X (PLACEi, D) PLACEi (X, D) x x (X, D).Let us leave X = {x1, x \u2212 j \u2212 1, x, x, i + 1, x, i + 1,. Proof. Let us leave X (PLACEi, xn) PLACEx (X, D) x, x, x, x, x, x, xx."}, {"heading": "5 Experiments", "text": "In this section, the performance of PLACECENTER (PC) is compared with many other cost functions, which we have taken up in order to reduce costs. (...) We do not know what we are supposed to do. (...) We do not know what we are supposed to do. (...) We do not know what we are supposed to do. (...) We do not know what we are supposed to do. (...) We do not know what we are supposed to do. (...) We do not know what we are supposed to do. (...) We do not know what we are supposed to do. (...) We do not know what we are supposed to do. (...). (...). (...). (...). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (We. (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). ().). (). (). (). ().). (). (). (). ().). (). (). (). (). ().). (). (). (). ().).). (. (). ().). (). (). (). ().). (). (). (). ().). (). (). ().). (). (). (). (). (). ().). (). (). ().). (). (). (). ().). (). (). (). ().). (). (). ().). (). (). (). (). ().). (). ().).). (). (). (). (). ().). (). ().). (). ().)."}, {"heading": "5.4 The Spherical MDS Problem", "text": "For the spherical MDS problem, we compare PC against SMACOF-Q, an adjustment of SMACOF Q to restrict data points to a low-dimensional sphere, and a technique by Elad, Keller, and Kimmel [14]. It turns out that the Elad et.al. approach performs poorly throughout compared to both other techniques, and so we do not present it in our reported results. SMACOF-Q basically runs on the original data set, but also adds an additional point p0 in the center of the sphere. The distance d0, i between each other point pi and p0 is set so that all other points are on one sphere, and this restriction is controlled by a weight factor, which implies a stronger emphasis on satisfying this restriction. As the solution produced by this method is not on the sphere, we normalize all points on the sphere for a fair comparison."}, {"heading": "5.5 Summary Of Results", "text": "In summary, these are the main conclusions that can be drawn from this experimental study. First, PC is consistently one of the most powerful methods, regardless of the choice of cost function, type of input, or level of interference of the problem. Occasionally, other methods will approach faster, but generally do not provide a better quality response, and other methods have a much more variable behavior with changing inputs and noise levels."}, {"heading": "6 A JL Lemma for Spherical Data", "text": "In this section, we present a Johnson-Lindenstrauss style that is limited to a low-dimensional sphere for mapping data from a high-dimensional sphere. \"We are looking at a fixed Y sphere that preserves as many points as possible and defines a distance matrix D in which the element di, j represents the geodetic distance between yi and y on S \u00b2 k. We are looking for an embedding of Y in S \u00b2 d that preserves distances in pairs as far as possible. For a fixed Y sphere and a projection of S \u00b2 n (Y) = X \u00b2 S \u00b2 horizontal, say that the X \u00b2 horizontal of Y exists if it is a constant c for all x i, x \u00b2 X \u00b2 X \u00b2 horizontal of Y."}], "references": [{"title": "On embeddings of moving points in Euclidean space", "author": ["P.K. Agarwal", "S. Har-Peled", "H. Yu"], "venue": "Proceedings 23rd Symposium on Computational Geometry,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., 3:993\u20131022,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Modern Multidimensional Scaling", "author": ["I. Borg", "P.J.F. Groenen"], "venue": "Springer,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast approximations for sums of distances, clustering and the Fermat\u2013Weber problem", "author": ["P. Bose", "A. Maheshwari", "P. Morin"], "venue": "Comput. Geom. Theory Appl., 24(3):135\u2013146,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Global convergence of a generalized iterative procedure for the minisum location problem with `p distances", "author": ["J. Brimberg", "R.F. Love"], "venue": "Operations Research, 41(6):1153\u20131163,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1993}, {"title": "Numerical Geometry of Non-Rigid Shapes", "author": ["A.M. Bronstein", "M.M. Bronstein", "R. Kimmel"], "venue": "Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust Euclidean embedding", "author": ["L. Cayton", "S. Dasgupta"], "venue": "ICML \u201906: Proceedings of the 23rd International Conference on Machine Learning, pages 169\u2013176,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Local multidimensional scaling for nonlinear dimension reduction, graph drawing, and proximity analysis", "author": ["L. Chen", "A. Buja"], "venue": "Journal of the Americal Statistical Association, 104:209\u2013219,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Multidimensional Scaling, Second Edition", "author": ["T.F. Cox", "M.A.A. Cox"], "venue": "Chapman & Hall/CRC, September", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "CVPR \u201905: Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 886\u2013893,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Applications of convex analysis to multidimensional scaling", "author": ["J. de Leeuw"], "venue": "Recent Developments in Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1977}, {"title": "Multidimensional scaling using majorization: SMACOF in R", "author": ["J. de Leeuw", "P. Mair"], "venue": "Technical Report 537, UCLA Statistics Preprints Series,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Texture mapping via spherical multi-dimensional scaling", "author": ["A.E. Elad", "Y. Keller", "R. Kimmel"], "venue": "R. Kimmel, N. A. Sochen, and J. Weickert, editors, Scale-Space, volume 3459 of Lecture Notes in Computer Science, pages 443\u2013455. Springer,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "The Geometric Median on Riemannian Manifolds with Application to Robust Atlas Estimation", "author": ["P.T. Fletcher", "S. Venkatasubramanian", "S. Joshi"], "venue": "Neuroimage (invited to special issue), 45(1):S143\u2013S152, March", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Distortion measures for speech processing", "author": ["R. Gray", "A. Buzo", "A. Gray Jr", "Y. Matsuyama"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, 28(4):367\u2013376, Aug", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1980}, {"title": "Low-distortion embeddings of finite metric spaces", "author": ["P. Indyk", "J. Matousek"], "venue": "Handbook of Discrete and Computational Geometry, pages 177\u2013196. CRC Press,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning to Classify Text Using Support Vector Machines \u2013 Methods, Theory, and Algorithms", "author": ["T. Joachims"], "venue": "Kluwer/Springer,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Extensions of Lipschitz mappings into Hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Contemporary Mathematics, 26:189\u2013206,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1984}, {"title": "Riemannian center of mass and mollifier smoothing", "author": ["H. Karcher"], "venue": "Comm. on Pure and Appl. Math., 30:509\u2013 541,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1977}, {"title": "Local convergence in Fermat\u2019s problem", "author": ["I.N. Katz"], "venue": "Mathematical Programming, 6:89\u2013104,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1974}, {"title": "Multidimensional scaling by optimizing goodness of fit to nonmetric hypothesis", "author": ["J.B. Kruskal"], "venue": "Psychometrika, 29:1\u201327,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1964}, {"title": "Multidimensional scaling", "author": ["J.B. Kruskal", "M. Wish"], "venue": "E. M. Uslander, editor, Quantitative Applications in the Social Sciences, volume 11. Sage Publications,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1978}, {"title": "A note on Fermat\u2019s problem", "author": ["H.W. Kuhn"], "venue": "Mathematical Programming, 4:98\u2013107,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1973}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "Int. J. Comput. Vision, 60(2),", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Dimensionality reductions that preserve volumes and distance to affine spaces, and their algorithmic applications", "author": ["A. Magen"], "venue": "Proceedings of the 6th International Workshop on Randomization and Approximation Techniques, Lecture Notes In Computer Science; Vol. 2483,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Inequalities: Theory of Majorization and Its Applications", "author": ["A.W. Marshall", "I. Olkin"], "venue": "Academic Press,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1979}, {"title": "On the convergence of a class of iterative methods for solving the Weber location problem", "author": ["L.M. Ostresh"], "venue": "Operations Research, 26:597\u2013609,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1978}, {"title": "Distributional clustering of English words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 183\u2013190,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1993}, {"title": "Rank reduction of correlation matrices by majorization", "author": ["R. Pietersz", "P.J.F. Groenen"], "venue": "Technical Report 519086, SSRN,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Embedding images in non-flat spaces", "author": ["R. Pless", "I. Simon"], "venue": "Proc. of the International Conference on Imaging Science, Systems, and Technology,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2002}, {"title": "Improved approximation algorithms for large matrices via random projections", "author": ["T. Sarl\u00f3s"], "venue": "Proceedings 47th Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Multidimensional scaling: I", "author": ["W.S. Torgerson"], "venue": "theory and method. Psychometrika, 17:401\u2013419,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1952}, {"title": "Sur le point pour lequel la somme des distances de n points donn\u00e9s est minimum", "author": ["E. Weiszfeld"], "venue": "Tohoku Math. J., 43:355\u2013386,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1937}, {"title": "Discussion of a set of points in terms of their mutual distances", "author": ["G. Young", "A.S. Householder"], "venue": "Psychometrika, 3:19\u201322,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1938}], "referenceMentions": [{"referenceID": 21, "context": "1 Introduction Multidimensional scaling (MDS) [23, 10, 3] is a widely used method for embedding a general distance matrix into a low dimensional Euclidean space, used both as a preprocessing step for many problems, as well as a visualization tool in its own right.", "startOffset": 46, "endOffset": 57}, {"referenceID": 8, "context": "1 Introduction Multidimensional scaling (MDS) [23, 10, 3] is a widely used method for embedding a general distance matrix into a low dimensional Euclidean space, used both as a preprocessing step for many problems, as well as a visualization tool in its own right.", "startOffset": 46, "endOffset": 57}, {"referenceID": 2, "context": "1 Introduction Multidimensional scaling (MDS) [23, 10, 3] is a widely used method for embedding a general distance matrix into a low dimensional Euclidean space, used both as a preprocessing step for many problems, as well as a visualization tool in its own right.", "startOffset": 46, "endOffset": 57}, {"referenceID": 33, "context": "MDS has been studied and used in psychology since the 1930s [35, 33, 22] to help visualize and analyze data sets where the only input is a distance matrix.", "startOffset": 60, "endOffset": 72}, {"referenceID": 31, "context": "MDS has been studied and used in psychology since the 1930s [35, 33, 22] to help visualize and analyze data sets where the only input is a distance matrix.", "startOffset": 60, "endOffset": 72}, {"referenceID": 20, "context": "MDS has been studied and used in psychology since the 1930s [35, 33, 22] to help visualize and analyze data sets where the only input is a distance matrix.", "startOffset": 60, "endOffset": 72}, {"referenceID": 6, "context": "More recently MDS has become a standard dimensionality reduction and embedding technique to manage the complexity of dealing with large high dimensional data sets [8, 9, 31, 6].", "startOffset": 163, "endOffset": 176}, {"referenceID": 7, "context": "More recently MDS has become a standard dimensionality reduction and embedding technique to manage the complexity of dealing with large high dimensional data sets [8, 9, 31, 6].", "startOffset": 163, "endOffset": 176}, {"referenceID": 29, "context": "More recently MDS has become a standard dimensionality reduction and embedding technique to manage the complexity of dealing with large high dimensional data sets [8, 9, 31, 6].", "startOffset": 163, "endOffset": 176}, {"referenceID": 5, "context": "More recently MDS has become a standard dimensionality reduction and embedding technique to manage the complexity of dealing with large high dimensional data sets [8, 9, 31, 6].", "startOffset": 163, "endOffset": 176}, {"referenceID": 10, "context": "Thus, in addition to the standard formulation [12], many variants of MDS have been proposed, based on changing the underlying error function [35, 8].", "startOffset": 46, "endOffset": 50}, {"referenceID": 33, "context": "Thus, in addition to the standard formulation [12], many variants of MDS have been proposed, based on changing the underlying error function [35, 8].", "startOffset": 141, "endOffset": 148}, {"referenceID": 6, "context": "Thus, in addition to the standard formulation [12], many variants of MDS have been proposed, based on changing the underlying error function [35, 8].", "startOffset": 141, "endOffset": 148}, {"referenceID": 11, "context": "a low dimensional sphere), and various heuristics for MDS in this setting have also been proposed [13, 6].", "startOffset": 98, "endOffset": 105}, {"referenceID": 5, "context": "a low dimensional sphere), and various heuristics for MDS in this setting have also been proposed [13, 6].", "startOffset": 98, "endOffset": 105}, {"referenceID": 5, "context": "Spherical MDS has applications in texture mapping and image analysis [6], and is a generalization of the spherical dimensionality reduction problem, where the goal is to map points from a high dimensional sphere onto a lowdimensional sphere.", "startOffset": 69, "endOffset": 72}, {"referenceID": 27, "context": "A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].", "startOffset": 295, "endOffset": 306}, {"referenceID": 16, "context": "A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].", "startOffset": 295, "endOffset": 306}, {"referenceID": 1, "context": "A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].", "startOffset": 295, "endOffset": 306}, {"referenceID": 23, "context": "A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].", "startOffset": 323, "endOffset": 331}, {"referenceID": 9, "context": "A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].", "startOffset": 323, "endOffset": 331}, {"referenceID": 14, "context": "A spherical dimensionality reduction result is an important step to representing high dimensional distributions in a lowerdimensional space of distributions, and will have considerable impact in domains that represent data natively as histograms or distributions, such as in document processing [29, 18, 2], image analysis [25, 11] and speech recognition [16].", "startOffset": 355, "endOffset": 359}, {"referenceID": 8, "context": "There is a general taxonomy of MDS methods [10]; in this paper we will focus primarily the metric and generalized MDS problems.", "startOffset": 43, "endOffset": 47}, {"referenceID": 21, "context": "The traditional formulation of MDS [23] assumes that the distance matrix D arises from points in some ddimensional Euclidean space.", "startOffset": 35, "endOffset": 39}, {"referenceID": 31, "context": "These similarities also arise from many psychology data sets directly [33, 35].", "startOffset": 70, "endOffset": 78}, {"referenceID": 33, "context": "These similarities also arise from many psychology data sets directly [33, 35].", "startOffset": 70, "endOffset": 78}, {"referenceID": 25, "context": "A more general approach called SMACOF that drops the Euclidean assumption uses a technique known as stress majorization [27, 12, 13].", "startOffset": 120, "endOffset": 132}, {"referenceID": 10, "context": "A more general approach called SMACOF that drops the Euclidean assumption uses a technique known as stress majorization [27, 12, 13].", "startOffset": 120, "endOffset": 132}, {"referenceID": 11, "context": "A more general approach called SMACOF that drops the Euclidean assumption uses a technique known as stress majorization [27, 12, 13].", "startOffset": 120, "endOffset": 132}, {"referenceID": 11, "context": "It has been adapted to many other MDS variants as well including restrictions of data to lie on quadratic surfaces and specifically spheres [13].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "Since the sum-of-squares error metric is sensitive to outliers, Cayton and Dasgupta [8] proposed a robust variant based on an `1 error metric.", "startOffset": 84, "endOffset": 87}, {"referenceID": 28, "context": "Among them are majorization methods ([30] and SMACOF-Q [13]), a multiresolution approach due to Elad, Keller, and Kimmel [14] and an approach based on computing the classical MDS and renormalizing [31].", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "Among them are majorization methods ([30] and SMACOF-Q [13]), a multiresolution approach due to Elad, Keller, and Kimmel [14] and an approach based on computing the classical MDS and renormalizing [31].", "startOffset": 55, "endOffset": 59}, {"referenceID": 12, "context": "Among them are majorization methods ([30] and SMACOF-Q [13]), a multiresolution approach due to Elad, Keller, and Kimmel [14] and an approach based on computing the classical MDS and renormalizing [31].", "startOffset": 121, "endOffset": 125}, {"referenceID": 29, "context": "Among them are majorization methods ([30] and SMACOF-Q [13]), a multiresolution approach due to Elad, Keller, and Kimmel [14] and an approach based on computing the classical MDS and renormalizing [31].", "startOffset": 197, "endOffset": 201}, {"referenceID": 17, "context": "The Johnson-Lindenstrauss Lemma [19] states that any collection of n points in a Euclidean space can be embedded in a O((1/\"2) log n) dimensional Euclidean space that preserves all distances within a relative error of \".", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "An exhaustive survey of the different methods for dimensionality reduction is beyond the scope of this paper - the reader is directed to the survey by Indyk and Matousek for more information [17].", "startOffset": 191, "endOffset": 195}, {"referenceID": 0, "context": "Any manifold M with \u201clinearization dimension\u201d k (a measure of its complexity) can be embedded into a O((1/\"2)k log(kn)) dimensional space so that all pairwise Euclidean distances between points on M are distorted by at most a relative (1+ \")factor [1, 32, 26].", "startOffset": 248, "endOffset": 259}, {"referenceID": 30, "context": "Any manifold M with \u201clinearization dimension\u201d k (a measure of its complexity) can be embedded into a O((1/\"2)k log(kn)) dimensional space so that all pairwise Euclidean distances between points on M are distorted by at most a relative (1+ \")factor [1, 32, 26].", "startOffset": 248, "endOffset": 259}, {"referenceID": 24, "context": "Any manifold M with \u201clinearization dimension\u201d k (a measure of its complexity) can be embedded into a O((1/\"2)k log(kn)) dimensional space so that all pairwise Euclidean distances between points on M are distorted by at most a relative (1+ \")factor [1, 32, 26].", "startOffset": 248, "endOffset": 259}, {"referenceID": 24, "context": "The geodesic distance between points on a sphere can be interpreted as the angle between the points in radians, and a result by Magen [26] show that O((1/\"2) log n) dimensions preserve angles to within a relative factor of 1+ p \" (which is weaker than our result preserving the geodesic distance to within a relative factor of (1+ \")).", "startOffset": 134, "endOffset": 138}, {"referenceID": 6, "context": "\u2022 T = Rk,Err(\u03b4) = |\u03b4|, f (x , x \u2032) = \u2016x \u2212 x \u20162: This is a robust variant of MDS called rMDS, first suggested by Cayton and Dasgupta [8].", "startOffset": 132, "endOffset": 135}, {"referenceID": 6, "context": "The actual measure studied by Cayton and Dasgupta [8] is not rMDS.", "startOffset": 50, "endOffset": 53}, {"referenceID": 33, "context": "Our experiments indicate the SVD-based approach [35] is almost always the optimal way to seed the algorithm, and we use it unless specifically indicated otherwise.", "startOffset": 48, "endOffset": 52}, {"referenceID": 32, "context": "Although there is no closed form expression for the 1-median, there are numerous algorithms for solving this problem both exactly [34] and approximately [4].", "startOffset": 130, "endOffset": 134}, {"referenceID": 3, "context": "Although there is no closed form expression for the 1-median, there are numerous algorithms for solving this problem both exactly [34] and approximately [4].", "startOffset": 153, "endOffset": 156}, {"referenceID": 4, "context": "Methods that converge to the global optimum exist for any Err(\u03b4) = |\u03b4|p, p \u2264 2; it is known that if p is sufficiently larger than 2, then convergent methods may not exist [5].", "startOffset": 171, "endOffset": 174}, {"referenceID": 32, "context": "An exact iterative algorithm for solving this problem was given by Weiszfeld [34], and works as follows.", "startOffset": 77, "endOffset": 81}, {"referenceID": 22, "context": "This algorithm is guaranteed to converge to the optimal solution [24, 28], and in most settings converges quadratically [21].", "startOffset": 65, "endOffset": 73}, {"referenceID": 26, "context": "This algorithm is guaranteed to converge to the optimal solution [24, 28], and in most settings converges quadratically [21].", "startOffset": 65, "endOffset": 73}, {"referenceID": 19, "context": "This algorithm is guaranteed to converge to the optimal solution [24, 28], and in most settings converges quadratically [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 4, "context": "If Err(\u03b4) = |\u03b4|p, 1 < p < 2, then an iterative algorithm along the same lines as the Weiszfeld algorithm can be used to minimize g(x) optimally [5].", "startOffset": 144, "endOffset": 147}, {"referenceID": 18, "context": "Karcher [20] proposed an iterative scheme for the geodesic sum-of-squares problem that always converges as long as the points do not span the entire sphere.", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "For this case, we make use of a Weiszfeld-like adaption [15] that again works on general Riemannian manifolds, and on the sphere in particular.", "startOffset": 56, "endOffset": 60}, {"referenceID": 11, "context": "For the fMDS problem the leading algorithm is SMACOF [13]; for the r2MDS problem the leading algorithm is by Cayton and Dasgupta (CD) [8].", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "For the fMDS problem the leading algorithm is SMACOF [13]; for the r2MDS problem the leading algorithm is by Cayton and Dasgupta (CD) [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": "Thus, as suggested by the authors [8], to properly compare the algorithms, we let CD refer to running REE and then projecting the result to a k-dimensional subspace using the SVD technique [35] (our plots show this projection after each step).", "startOffset": 34, "endOffset": 37}, {"referenceID": 33, "context": "Thus, as suggested by the authors [8], to properly compare the algorithms, we let CD refer to running REE and then projecting the result to a k-dimensional subspace using the SVD technique [35] (our plots show this projection after each step).", "startOffset": 189, "endOffset": 193}, {"referenceID": 33, "context": "We also compare with the popular SVD-based method [35], which solves the related cMDS problem based on similarities, by seeding all three iterative techniques with the results of the closed-form SVD-based solution.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "We compare against a version of SMACOF-Q [13] that is designed for data restricted to a low dimensional sphere, specifically for the c-2SMDS measure.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "4 The Spherical MDS Problem For the spherical MDS problem we compare PC against SMACOF-Q, an adaptation of SMACOF to restrict data points to a low-dimensional sphere, and a technique of Elad, Keller and Kimmel [14].", "startOffset": 210, "endOffset": 214}, {"referenceID": 17, "context": "When f (yi , y j) = ||yi \u2212 y j||, and Y \u2208  \u0304 d , then the Johnson-Lindenstrauss (JL) Lemma [19] says that if H \u2282 R \u0304 d is a random k-dimensional linear subspace with k = O((1/\"2) log(n/\u03b4)), then X = \u03c0H(Y ) has \"distortion from Y with probability at least 1\u2212\u03b4.", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "We note that recent results [1] have shown similar results for point on a variety of manifolds (including spheres) where projections preserve Euclidean distances.", "startOffset": 28, "endOffset": 31}, {"referenceID": 24, "context": "Another recent result [26] shows that k = O((1/\"2) log(n/\u03b4)) dimensions preserves p \"-distortion in angles, which is weaker than the following result.", "startOffset": 22, "endOffset": 26}, {"referenceID": 0, "context": "We will show that for x \u2208 [0, 1] and \" \u2208 [0, 0.", "startOffset": 26, "endOffset": 32}], "year": 2010, "abstractText": "In this paper, we propose a unified algorithmic framework for solving many known variants of MDS. Our algorithm is a simple iterative scheme with guaranteed convergence, and is modular; by changing the internals of a single subroutine in the algorithm, we can switch cost functions and target spaces easily. In addition to the formal guarantees of convergence, our algorithms are accurate; in most cases, they converge to better quality solutions than existing methods, in comparable time. We expect that this framework will be useful for a number of MDS variants that have not yet been studied. Our framework extends to embedding high-dimensional points lying on a sphere to points on a lower dimensional sphere, preserving geodesic distances. As a compliment to this result, we also extend the JohnsonLindenstrauss Lemma to this spherical setting, where projecting to a random O((1/\"2) log n)-dimensional sphere causes \"-distortion.", "creator": "LaTeX with hyperref package"}}}