{"id": "1003.0628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Mar-2010", "title": "Linguistic Geometries for Unsupervised Dimensionality Reduction", "abstract": "Text documents are complex high dimensional objects. To effectively visualize such data it is important to reduce its dimensionality and visualize the low dimensional embedding as a 2-D or 3-D scatter plot. In this paper we explore dimensionality reduction methods that draw upon domain knowledge in order to achieve a better low dimensional embedding and visualization of documents. We consider the use of geometries specified manually by an expert, geometries derived automatically from corpus statistics, and geometries computed from linguistic resources.", "histories": [["v1", "Tue, 2 Mar 2010 16:52:32 GMT  (156kb)", "http://arxiv.org/abs/1003.0628v1", "13 pages, 15 figures"]], "COMMENTS": "13 pages, 15 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yi mao", "krishnakumar balasubramanian", "guy lebanon"], "accepted": false, "id": "1003.0628"}, "pdf": {"name": "1003.0628.pdf", "metadata": {"source": "CRF", "title": "Linguistic Geometries for Unsupervised Dimensionality Reduction", "authors": ["Yi Mao", "Krishnakumar Balasubramanian"], "emails": ["yi.mao@cc.gatech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 100 3.06 28v1 [cs.CL]"}, {"heading": "1 Introduction", "text": "It is not as if it is a kind of knowledge that is able to explore the structure of urban space, i.e., documents that refer to similar topics in the vicinity of 2D or 3D space reduction techniques, such as Principal Component Analysis (PCA), locally linear embedding (LLE), or t-distributed stochastic neighbor embedding (t-SNE) [22] take as input a set of feature vectors as bag of words or tf vectors. An obvious disadvantage of such an approach ignores the textual nature of documents and instead looks at the vocabulary of vowels such as vectors such as vectors such as vectors."}, {"heading": "2 Related Work", "text": "Dimensionality reduction, despite its long history, is still an active field of research. By and large, dimensionality reduction methods can be classified as projective or manifold [3]; the first projects data onto a linear subspace (e.g. PCA and canonical correlation analysis), while the second traces a low-dimensional nonlinear multiplicity on which the data is based (e.g. multidimensional scaling, isomaps, laplac eigenmaps, LLE and t-SNE); the use of dimensionality reduction for text documents is investigated by [21], which also describes current homeland security applications; dimensionality reduction is closely related to metric learning. [23] is one of the earliest works focusing on learning metrics of form (1); in particular, they try to learn matrix T in a supervised way by expressing relationships between sample pairs."}, {"heading": "3 Non-Euclidean Geometries", "text": "Dre rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the"}, {"heading": "4 Domain Knowledge", "text": "We consider four different techniques to obtain the transformation matrix H. Each technique approaches in two ways: (1) separately we obtain the stochastic column matrix R, the different words and the diagonal matrix D, which determines the meaning of each word; (2) we estimate the semantic similarity matrix T and dissect it as H H. To ensure that H is a non-negative matrix to be interpretable, non-negative matrix factoring techniques such as those used in [7] can be applied."}, {"heading": "Method A: Manual Specification", "text": "In this method, the user manually specifies the matrices (R, D) based on his assessment of the relationship between the vocabulary words. Specifically, the user first constructs a hierarchical grouping of words that can depend on the current text domain, and then specifies the matrices (R, D) based on the cluster affiliation of the vocabulary. If the user denotes the clusters by C1,.., Cr (a partition of {v1,.,., vn}), the user specifies R by appropriately specifying the values Rij, i = j, vi, vi, vi, Ca, vi, vj, Cb (3). Together, the values between the two determine the merging of words from the same cluster. The value \u03c1ab, a = b captures the semantic matrix between two clusters. This value can either be calculated manually for each cluster pair or automatically determined from the cluster formation hierarchy (R = 4)."}, {"heading": "Method B: Contextual Diffusion", "text": "An alternative technique that works much better is to consider a transformation based on the similarity between the contextual distributions of the vocabulary words. The contextual distribution of the word v is defined as asqv (w) = p (w) appears in x | v (5), where x is a randomly drawn document. In other words, qv is the distribution that determines the words that appear in the context of the word v. A natural measure of similarity between the distributions is the Fisher diffusion core proposed by [13]. Applied to contextual distributions such as in [6] we arrive at the following similarity matrix (where c > 0) T (u, v) = exp (\u2212 c arccos2 (w) qu (w) qv (w))))))).Intuitively, the word u (u) is translated or strewn into v, depending on the geometric diffusion between the distributions of apparent contexts."}, {"heading": "Method D: Word-Net", "text": "The last method we are considering is using Word-Net, a standard linguistic resource, to specify the matrix T in (1), which is similar to the manual specification (Method A) in that it is based on expert knowledge and not on corpus2The Google n-gram dataset contains n-gram counts (n \u2264 5) received by Google based on processing over a trillion words of running text.statistics. Unlike Method A, however, Word-Net is a carefully built resource that contains more precise and comprehensive linguistic information such as synonyms, hyponyms and holonyms. However, unlike Method A, due to its universality, it is at a disadvantage to construct a geometry that is suitable for a particular text domain.We follow [2] to compare the five similarities between words based on Word-Net. In our experiments, we are using Jiang and Conrath's measure [11] (also see [12] c2), cp (2) is considered to be the least likely."}, {"heading": "Convex Combinations", "text": "In addition to the methods A-D, which represent \"pure methods,\" we also consider convex combinations H \u0445 = \u2211 i\u03b1iHi \u03b1i \u2265 0, \u2211 i\u03b1i = 1 (7), where Hi are matrices from methods A-D and \u03b1 is a non-negative weight vector that adds up to 1. Eq.7 allows to combine heterogeneous types of domain knowledge (manually specified as method A and D and automatically derived as method B and C)."}, {"heading": "5 Experiments", "text": "We evaluated methods A-D and the quantitative combination method by experimenting with two sets of data from different areas. The first is the Cornell sensitivity scale Dataset of Movie Reviews [17]. Visualization in this case focuses on the sentiment quantity [18]. For simplicity, we considered only documents with sentiment level 1 (very bad) and 4 (very good). The second text dataset consists of lower-casing, stop-word removal, stemming and selection of the most common 2000 words. Alternative preprocessing is possible, but should not change the results as we focus on comparing alternatives instead of measuring absolute performance. The second text dataset is 20 newsgroups. It consists of newsgroup articles from 20 different newsgroups and is intended to demonstrate visualization. To measure dimensional reduction quality, we show the data as a scatter plot with different data groups (toptiments, combinations with different markers and documents) that belong to our fact."}, {"heading": "H = I 0.6404 0.7465 0.8481 0.8496 0.6559 0.6821 0.6680 0.7410", "text": "We conclude that the combination of heterogeneous domain knowledge can improve the quality of dimensionality reduction for visualization, and that the search for an improved convex combination can be performed without the use of labeled data. Finally, we demonstrate the effect of linguistic geometries on a new dataset consisting of all oral essays appearing in ACL 2001 - 2009. For the purpose of manual specification, we obtain 1545 unique words from paper titles and assign each word affiliation values to the following clusters: morphology / phonology, syntax / parsing, semantics, discourse / dialogue, generation / summary, machine translation, retrievaluation / categorization, and machine learning. The evaluation is from 0 to 2, with 2 being the most relevant. Score information is then used to generate the transformation matrix R. We assign each word a value ranging from 0 to 3 (the greater value, the word)."}, {"heading": "6 Discussion", "text": "The new methods of manual specification, contextual diffusion, Google n-grams and Word-Net generally exceed the original assumption H = I. We emphasize that the baseline H = I is the one currently used in most text visualization systems. PCA and t-SNE's two reduction methods represent a popular classical technique and a recently proposed technique that outperforms other younger competitors (LLE, Isomap, MVU, CCA, Laplacian eigenmaps) Our experiments show that different methods of domain knowledge work best in different situations. Generally, however, the methods of contextual diffusion and Google n-gramm.We also show how the combination of different types of domain knowledge leads to increased effectiveness and that such combinations can be found without the use of labeled data."}], "references": [{"title": "A", "author": ["D. Blei"], "venue": "Ng, , and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993\u20131022", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Semantic distance in wordnet: An experimental", "author": ["A. Budanitsky", "G. Hirst"], "venue": "application-oriented evaluation of five measures. In NAACL Workshop on WordNet and other Lexical Resources", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Dimension reduction: A guided tour", "author": ["C. Burges"], "venue": "Technical Report MSR-TR-2009-2013, Microsoft Research", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Statistical Decision Rules and Optimal Inference", "author": ["N.N. \u010cencov"], "venue": "American Mathematical Society", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1982}, {"title": "A cluster separation measure", "author": ["D.L. Davies", "D.W. Bouldin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 1(4):224\u2013227", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Statistical translation", "author": ["J. Dillon", "Y. Mao", "G. Lebanon", "J. Zhang"], "venue": "heat kernels, and expected distances. In Proc. of the 23rd Conference on Uncertainty in Artificial Intelligence, pages 93\u2013100. AUAI Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "On the equivalence of nonnegative matrix factorization and spectral clustering", "author": ["C. Ding", "X. He", "H.D. Simon"], "venue": "Proc. SIAM Data Mining Conf, pages 606\u2013610", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Pattern classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "Wiley New York", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "Themeriver: Visualizing thematic changes in large document collections", "author": ["S. Havre", "E. Hetzler", "P. Whitney", "L. Nowell"], "venue": "IEEE Transactions on Visualization and Computer Graphics, 8(1)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "TextTiling: Segmenting text into multi-paragraph subtopic passages", "author": ["M.A. Hearst"], "venue": "Computational Linguistics, 23(1):33\u201364", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["J.J. Jiang", "D.W. Conrath"], "venue": "International Conference Research on Computational Linguistics (ROCLING X)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Speech and Language Processing", "author": ["D. Jurafsky", "J.H. Martin"], "venue": "Prentice Hall", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion kernels on statistical manifolds", "author": ["J. Lafferty", "G. Lebanon"], "venue": "Journal of Machine Learning Research, 6:129\u2013163", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Metric learning for text documents", "author": ["G. Lebanon"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(4):497\u2013508", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Sequential document visualization", "author": ["Y. Mao", "J. Dillon", "G. Lebanon"], "venue": "IEEE Transactions on Visualization and Computer Graphics, 13(6):1208\u20131215", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "TextArc: Showing word frequency and distribution in text", "author": ["W.B. Paley"], "venue": "IEEE Symposium on Information Visualization Poster Compendium", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "A sentimental eduction: sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "Proc. of the Association of Computational Linguistics", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Thumbs up?: sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "Proc. of the Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": "Science, 290:2323\u20132326", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "InfoCrystal: A visual tool for information retrieval", "author": ["A. Spoerri"], "venue": "Proc. of IEEE Visualization", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "editors", "author": ["J.J. Thomas", "K.A. Cook"], "venue": "Illuminating the Path: The Research and Development Agenda for Visual Analytics. IEEE Computer Society", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Visualizing data using t-sne", "author": ["L. van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Distance metric learning with applications to clustering with side information", "author": ["E. Xing", "A. Ng", "M. Jordan", "S. Russel"], "venue": "Advances in Neural Information Processing Systems, 15", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "Standard dimensionality reduction methods such as principal component analysis (PCA), locally linear embedding (LLE) [19], or t-distributed stochastic neighbor embedding (t-SNE) [22] take as input a set of feature vectors such as bag of words or tf vectors.", "startOffset": 117, "endOffset": 121}, {"referenceID": 21, "context": "Standard dimensionality reduction methods such as principal component analysis (PCA), locally linear embedding (LLE) [19], or t-distributed stochastic neighbor embedding (t-SNE) [22] take as input a set of feature vectors such as bag of words or tf vectors.", "startOffset": 178, "endOffset": 182}, {"referenceID": 2, "context": "Broadly speaking, dimensionality reduction methods may be classified to projective or manifold based [3].", "startOffset": 101, "endOffset": 104}, {"referenceID": 20, "context": "The use of dimensionality reduction for text documents is surveyed by [21] who also describe current homeland security applications.", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "[23] is one of the earliest papers that focus on learning metrics of the form (1).", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Representative paper on unsupervised metric learning for text documents is [14] which learns a metric on the simplex based on the geometric volume of the data.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist.", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist.", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": "While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist.", "startOffset": 117, "endOffset": 120}, {"referenceID": 15, "context": "While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist.", "startOffset": 122, "endOffset": 126}, {"referenceID": 0, "context": "While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist.", "startOffset": 128, "endOffset": 131}, {"referenceID": 14, "context": "While this is perhaps the most popular and practical text visualization technique, other methods such as [20], [10], [9], [16], [1], [15] exist.", "startOffset": 133, "endOffset": 137}, {"referenceID": 3, "context": "a non-negative matrix whose columns sum to 1 [4] 3", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "To ensure that H is a non-negative matrix for it to be interpretable, non-negativity matrix factorization techniques such as the one in [7] may be applied.", "startOffset": 136, "endOffset": 139}, {"referenceID": 12, "context": "A natural similarity measure between distributions is the Fisher diffusion kernel proposed by [13].", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "Applied to contextual distributions as in [6] we arrive at the following similarity matrix (where c > 0) T (u, v) = exp (", "startOffset": 42, "endOffset": 45}, {"referenceID": 1, "context": "We follow [2] who compare five similarity measures between words based on Word-Net.", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "In our experiments we use Jiang and Conrath\u2019s measure [11] (see also [12]) Tc1,c2 = log p(c1)p(c2) 2p(lcs(c1, c2)) as it was shown to outperform the others.", "startOffset": 54, "endOffset": 58}, {"referenceID": 11, "context": "In our experiments we use Jiang and Conrath\u2019s measure [11] (see also [12]) Tc1,c2 = log p(c1)p(c2) 2p(lcs(c1, c2)) as it was shown to outperform the others.", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "The first is the Cornell sentiment scale dataset of movie reviews [17].", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "The visualization in this case focuses on the sentiment quantity [18].", "startOffset": 65, "endOffset": 69}, {"referenceID": 7, "context": "It equals to trS T SW where SW is the within-cluster scatter matrix, ST = SW +SB is the total scatter matrix, and SB is the between-cluster scatter matrix [8].", "startOffset": 155, "endOffset": 158}, {"referenceID": 4, "context": "(ii) The Davies Bouldin index is an alternative to (i) that is similarly based on the ratio of within-cluster scatter to between-cluster scatter [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 7, "context": "(iv) An alternative to (iii) is to project the embedded data onto a line which is the direction returned by applying Fisher\u2019s linear discriminant analysis [8] to the embedded data.", "startOffset": 155, "endOffset": 158}, {"referenceID": 21, "context": "PCA is a well known classical method while t-SNE [22] is a recently proposed technique shown to outperform LLE, CCA, MVU, Isomap, and Laplacian eigenmaps.", "startOffset": 49, "endOffset": 53}], "year": 2013, "abstractText": "Text documents are complex high dimensional objects. To effectively visualize such data it is important to reduce its dimensionality and visualize the low dimensional embedding as a 2-D or 3-D scatter plot. In this paper we explore dimensionality reduction methods that draw upon domain knowledge in order to achieve a better low dimensional embedding and visualization of documents. We consider the use of geometries specified manually by an expert, geometries derived automatically from corpus statistics, and geometries computed from linguistic resources.", "creator": "LaTeX with hyperref package"}}}