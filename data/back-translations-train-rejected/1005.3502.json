{"id": "1005.3502", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-May-2010", "title": "Using machine learning to make constraint solver implementation decisions", "abstract": "Programs to solve so-called constraint problems are complex pieces of software which require many design decisions to be made more or less arbitrarily by the implementer. These decisions affect the performance of the finished solver significantly. Once a design decision has been made, it cannot easily be reversed, although a different decision may be more appropriate for a particular problem.", "histories": [["v1", "Wed, 19 May 2010 17:53:43 GMT  (29kb,D)", "http://arxiv.org/abs/1005.3502v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["lars kotthoff", "ian gent", "ian miguel"], "accepted": false, "id": "1005.3502"}, "pdf": {"name": "1005.3502.pdf", "metadata": {"source": "CRF", "title": "Using machine learning to make constraint solver implementation decisions", "authors": ["Lars Kotthoff", "Ian Gent"], "emails": ["larsko@cs.st-andrews.ac.uk", "ipg@cs.st-andrews.ac.uk", "ianm@cs.st-andrews.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that the solution to the problem is a series of decision variants, each associated with a domain of potential values. An assignment forms a variable to a value from its domain. Each constraint determines which combinations of values lead to a subset of variables. A solution for such a domain is an assignment to all variables that satisfies all constraints. Solutions are usually found by systematically searching for possible assignments to the variables."}, {"heading": "2 Background", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "3 The benchmark instances and solvers", "text": "We evaluated the performance of the different versions of the alldifferent constraint gacalldiff on two different sets of problem cases. The first was used for learning classifiers, the second only for evaluating the learned classifiers. The set we used for machine learning consisted of 277 benchmark instances from 14 different problem classes. It was selected to include as many instances as possible, regardless of which version of the alldifferent constraint performs best. The set for evaluating the learned classifiers consisted of 1036 instances from 2 different problem classes that were not present in the set we used for machine learning. We chose this set for evaluation because the small number of different problem classes made it suitable for training. Our sources are Lecoutres XCSP repository [15] and our own stock of CSP instances that were not included in the set we used."}, {"heading": "4 Instance attributes and their measurement", "text": "This year, it is time for us to set out to find a solution that paves the way to the future."}, {"heading": "5 Learning a problem classifier", "text": "This year it is more than ever before."}, {"heading": "6 Conclusions and future work", "text": "To facilitate this, we have evaluated the performance of constraint solvers, which represent all decisions on two large groups of problem cases. We have shown that training a group of classifiers without intrinsic knowledge of each individual and combining their decisions can significantly improve performance, rather than always making a standard decision. In particular, our combined classifier is almost as good as the best classifier in the group and much better than the worst classifier. We have finally shown that we can significantly improve the standard decisions proposed in modern literature by using a relatively simple and generic procedure. We provide strong evidence of the overall applicability of a group of classifiers learned in a training set to new, unknown instances. We have identified several problems with the use of machine learning to make constraint programming decisions and successfully resolve them. Our system achieves performance improvements even taking into account the time it takes to calculate the classifiers."}, {"heading": "Acknowledgements", "text": "The authors thank Peter Nightingale for helpful discussions about the various constraints and their implementations and descriptions of the problem features used in the analysis. Chris Jefferson provided additional feature descriptions. We thank Jesse Hoey for useful discussions about machine learning and the anonymous reviewer for their feedback. Reviewer 2 in particular provided helpful hints on relevant machine learning research. Lars Kotthoff is supported by a SICSA fellowship."}], "references": [{"title": "A gender-based genetic algorithm for the automatic configuration of algorithms", "author": ["C. Ans\u00f3tegui", "M. Sellmann", "K. Tierney"], "venue": "CP. pp. 142\u2013157", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Adaptive constraint satisfaction: The quickest first principle", "author": ["J. Borrett", "E. Tsang", "N. Walsh"], "venue": "ECAI. pp. 160\u2013164", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Constraint Processing", "author": ["R. Dechter"], "venue": "Elsevier Science", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "The adaptive constraint engine", "author": ["S. Epstein", "E. Freuder", "R. Wallace", "A. Morozov", "B. Samuels"], "venue": "CP. pp. 525\u2013542", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning when to use lazy learning in constraint solving", "author": ["I. Gent", "C. Jefferson", "L. Kotthoff", "I. Miguel", "N. Moore", "P. Nightingale", "K. Petrie"], "venue": "ECAI", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Minion: A fast scalable constraint solver", "author": ["I. Gent", "C. Jefferson", "I. Miguel"], "venue": "ECAI. pp. 98\u2013102", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Lazy explanations for constraint propagator", "author": ["I. Gent", "I. Miguel", "N. Moore"], "venue": "PADL", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalised arc consistency for the alldifferent constraint: An empirical survey", "author": ["I. Gent", "I. Miguel", "P. Nightingale"], "venue": "Artif. Intell. 172(18), 1973\u20132000", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning techniques for automatic algorithm portfolio selection", "author": ["A. Guerri", "M. Milano"], "venue": "ECAI. pp. 475\u2013479", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "The WEKA data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I. Witten"], "venue": "SIGKDD Explorations 11(1)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Performance prediction and automated tuning of randomized and parametric algorithms", "author": ["F. Hutter", "Y. Hamadi", "H. Hoos", "K. Leyton-Brown"], "venue": "CP. pp. 213\u2013228", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "SATenstein: Automatically building local search SAT solvers from components", "author": ["A. KhudaBukhsh", "L. Xu", "H. Hoos", "K. Leyton-Brown"], "venue": "IJCAI. pp. 517\u2013524", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Constraint solvers: An empirical evaluation of design decisions", "author": ["L. Kotthoff"], "venue": "CIRCA preprint", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Reinforcement learning for algorithm selection", "author": ["M. Lagoudakis", "M. Littman"], "venue": "AAAI/IAAI. p. 1081", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "XCSP benchmarks", "author": ["C. Lecoutre"], "venue": "http://tinyurl.com/y6hpphs", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "A portfolio approach to algorithm selection", "author": ["K. Leyton-Brown", "E. Nudelman", "G. Andrew", "J. McFadden", "Y. Shoham"], "venue": "IJCAI. pp. 1542\u20131543", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Practical graph isomorphism", "author": ["B. McKay"], "venue": "Proc. 10th Manitoba Conf., Winnipeg/Manitoba", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1980}, {"title": "Automatically configuring constraint satisfaction programs: A case study", "author": ["S. Minton"], "venue": "Constraints 1(1/2), 7\u201343", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Using casebased reasoning in an algorithm portfolio for constraint solving", "author": ["E. O\u2019Mahony", "E. Hebrard", "A. Holland", "C. Nugent", "B. O\u2019Sullivan"], "venue": "19th Irish Conference on AI", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "A filtering algorithm for constraints of difference in CSPs", "author": ["J.C. R\u00e9gin"], "venue": "AAAI. pp. 362\u2013367", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1994}, {"title": "The algorithm selection problem", "author": ["J. Rice"], "venue": "Advances in Computers 15, 65\u2013118", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1976}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations", "author": ["I. Witten", "E. Frank"], "venue": "Morgan Kaufmann", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "SATzilla: Portfolio-based algorithm selection for SAT", "author": ["L. Xu", "F. Hutter", "H. Hoos", "K. Leyton-Brown"], "venue": "J. Artif. Intell. Res. (JAIR) 32, 565\u2013606", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 12, "context": "mance of the finished solver significantly [13].", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "A constraint satisfaction problem (CSP [3]) is a set of decision variables, each with an associated domain of potential values, and a set of constraints.", "startOffset": 39, "endOffset": 42}, {"referenceID": 20, "context": "We are addressing an instance of the Algorithm Selection Problem [21], which, given variable performance among a set of algorithms, is to choose the best candidate for a particular problem instance.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Machine learning is an established method of addressing this problem [14, 16].", "startOffset": 69, "endOffset": 77}, {"referenceID": 15, "context": "Machine learning is an established method of addressing this problem [14, 16].", "startOffset": 69, "endOffset": 77}, {"referenceID": 17, "context": "Multi-tac [18] configures a constraint solver for a particular instance distribution.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "The Adaptive Constraint Engine [4] learns search heuristics from training instances.", "startOffset": 31, "endOffset": 34}, {"referenceID": 11, "context": "SATenstein [12] configures stochastic local search solvers for solving SAT problems.", "startOffset": 11, "endOffset": 15}, {"referenceID": 22, "context": "This approach has recently been used with great success in SATzilla [23] and CP Hydra [19].", "startOffset": 68, "endOffset": 72}, {"referenceID": 18, "context": "This approach has recently been used with great success in SATzilla [23] and CP Hydra [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 1, "context": "In earlier work Borrett et al [2] employed a sequential portfolio of constraint solvers.", "startOffset": 30, "endOffset": 33}, {"referenceID": 8, "context": "Guerri and Milano [9] use a decision-tree based technique to select among a portfolio of constraint- and integer-programming based solution methods for the bid evaluation problem.", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "Similarly, Gent et al [5] investigate decision trees to choose whether to use lazy constraint learning [7] or not.", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "Similarly, Gent et al [5] investigate decision trees to choose whether to use lazy constraint learning [7] or not.", "startOffset": 103, "endOffset": 106}, {"referenceID": 10, "context": "Hutter et al [11] apply this method to local search.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "Ansotegui et al [1] employ a genetic algorithm to tune the parameters of both local and systematic SAT solvers.", "startOffset": 16, "endOffset": 19}, {"referenceID": 19, "context": "[20]) consider the constraint as a whole and are able to do more propagation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "For an in-depth survey of the decisions involved, see [8].", "startOffset": 54, "endOffset": 57}, {"referenceID": 14, "context": "Our sources are Lecoutre\u2019s XCSP repository [15] and our own stock of CSP instances.", "startOffset": 43, "endOffset": 47}, {"referenceID": 5, "context": "The reference constraint solver used is Minion [6] version 0.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "We ran the problems with 9 different versions of the alldifferent constraint \u2013 the n\u00e4\u0131ve version which is equivalent to the binary decomposition and 8 different implementations of the more sophisticated version which does more propagation (see [8]).", "startOffset": 244, "endOffset": 247}, {"referenceID": 8, "context": "This attribute has been used with machine learning for a model selection problem in constraint programming [9].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "The width of the ordering is the maximum width over all vertices [3].", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "This can be calculated in polynomial time [3], and is related to some tractability results.", "startOffset": 42, "endOffset": 45}, {"referenceID": 16, "context": "The first stage of the algorithm used by Nauty [17] detects this property.", "startOffset": 47, "endOffset": 51}, {"referenceID": 9, "context": "We used the WEKA [10] machine learning software through the R interface to learn classifiers.", "startOffset": 17, "endOffset": 21}, {"referenceID": 21, "context": "The specific classifiers we used are BayesNet, BFTree, ConjunctiveRule, DecisionTable, FT, HyperPipes, IBk, J48, J48graft, JRip, LADTree, MultilayerPerceptron, NBTree, OneR, PART, RandomForest, RandomTree, REPTree and ZeroR, all of which are described in [22].", "startOffset": 255, "endOffset": 259}, {"referenceID": 21, "context": "To bias the WEKA classifiers towards the instances we care about most, we used the common technique of duplicating instances [22].", "startOffset": 125, "endOffset": 129}, {"referenceID": 21, "context": "In the end, every instance will have been used for both training and testing in different runs [22].", "startOffset": 95, "endOffset": 99}], "year": 2017, "abstractText": "Programs to solve so-called constraint problems are complex pieces of software which require many design decisions to be made more or less arbitrarily by the implementer. These decisions affect the performance of the finished solver significantly [13]. Once a design decision has been made, it cannot easily be reversed, although a different decision may be more appropriate for a particular problem. We investigate using machine learning to make these decisions automatically depending on the problem to solve with the alldifferent constraint as an example. Our system is capable of making non-trivial, multi-level decisions that improve over always making a default choice.", "creator": "TeX"}}}