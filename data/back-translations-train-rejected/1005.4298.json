{"id": "1005.4298", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-May-2010", "title": "Distantly Labeling Data for Large Scale Cross-Document Coreference", "abstract": "Cross-document coreference, the problem of resolving entity mentions across multi-document collections, is crucial to automated knowledge base construction and data mining tasks. However, the scarcity of large labeled data sets has hindered supervised machine learning research for this task. In this paper we develop and demonstrate an approach based on ``distantly-labeling'' a data set from which we can train a discriminative cross-document coreference model. In particular we build a dataset of more than a million people mentions extracted from 3.5 years of New York Times articles, leverage Wikipedia for distant labeling with a generative model (and measure the reliability of such labeling); then we train and evaluate a conditional random field coreference model that has factors on cross-document entities as well as mention-pairs. This coreference model obtains high accuracy in resolving mentions and entities that are not present in the training data, indicating applicability to non-Wikipedia data. Given the large amount of data, our work is also an exercise demonstrating the scalability of our approach.", "histories": [["v1", "Mon, 24 May 2010 10:35:50 GMT  (226kb,DS)", "http://arxiv.org/abs/1005.4298v1", "16 pages, submitted to ECML 2010"]], "COMMENTS": "16 pages, submitted to ECML 2010", "reviews": [], "SUBJECTS": "cs.AI cs.IR cs.LG", "authors": ["sameer singh", "michael wick", "andrew mccallum"], "accepted": false, "id": "1005.4298"}, "pdf": {"name": "1005.4298.pdf", "metadata": {"source": "CRF", "title": "Distantly Labeling Data for Large Scale Cross-Document Coreference", "authors": ["Sameer Singh", "Michael Wick", "Andrew McCallum"], "emails": ["sameer@cs.umass.edu", "mwick@cs.umass.edu", "mccallum@cs.umass.edu"], "sections": [{"heading": null, "text": "Keywords: information extraction, core reference, weak monitoring, structured prediction"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Problem Definition", "text": "The difficulty of the task stems from a large space of hypotheses (exponential in the number of mentions) and from challenges in solving nominal and pronominal mentions to the correctly named mentions. Usually, named mentions within a document are not ambiguous. The problem we are examining in this paper is the problem of cross-document mentions, where the source of mentions is a large collection of documents. The same sources of ambiguity as within the doc correlation do not exist in this scenario either. Typically, the number of mentions and entities is much greater, and for some companies it can be in the millions, which may result in the problem of ambiguity as within the doc correlation."}, {"heading": "3 Distant-Labels for Coreference", "text": "Wikipedia1 pages contain historical and biographical descriptions of a large number of entities that we use as an external source for \"remote labels.\" 1 These pages are used to identify the entities to which strings from a different corpus refer, leading to clustering over those mentions. Our approach to remote labeling for co-references consists of three steps. Given the number of mention strings, the first step is to perform a preliminary co-conference within a document to resolve entities for mentions that occur in the same document. Second, a set of candidates for each mention chain is identified from Wikipedia by using the redirect and disambiguation pages. This set of candidates is reduced by a relevance evaluation function based on a generative model of the article and Wikipedia pages. The steps below will be applied in the context of disambiguating personal names in newspaper labels, but other methods may be described in cross-references."}, {"heading": "3.1 Within-Document Coreference", "text": "In order to reduce the number of available mentions for the cross-sectional function, the co-reference within a document is usually used as a pre-processing step to resolve the entities within the document [1,6,29]. However, the task of resolving proper names within a document is usually straightforward; the inclusion of pronouns and nominal nouns (such as \"he,\" \"she,\" \"the president,\" etc.) makes the inter-doc co-reference much more challenging. As our method of inter-doc co-reference is only used to mention strings that are proper names, it is similar to the one used in [6,27]. A distance function is defined between two mention sets (clusters) that uses hand-tuned weights against characteristics that consider different string matches (e.g. whether the first / last name is the same) and gender matches (one mention contains \"Mr.\" and the other Mrs \"in order to achieve a high level of this doc)."}, {"heading": "3.2 Identifying Candidate Entities", "text": "For each canonical mention string, we provide a set of potential Wikipedia entities to which the mention may refer. 1. To detect these candidates, we use the redirect and term pages used in Wikipedia.Redirect pages to redirect the user to the page with the correct title, since the user query is either a common misspelling (\"Barak Obama\"), an alternative spelling (\"Dick Nixon\"), or refers to multiple entities, one of which is prominent (\"Obama\"). Redirect pages also exist for less common but hard-to-to-identify cases of renaming (\"Sean Combs / Puff Daddy\"), typographical problems (\"Pointcare / Pointcare\"), spacing and enclosure (\"Vangogh / Vincent van Gogh\"), etc. Note that the goal of a redirect page is not always a content page, but can be another redirect page, the other one."}, {"heading": "3.3 Selection Using Multinomials", "text": "Once the candidates for each mention are identified, the method selects which candidate the mention refers to, or whether it refers to any of the Wikipedia entities. To make this decision, we use the content of the candidate's Wikipedia page and the news article to which the mention belongs. A score is calculated between the article and the Wikipedia page, and the candidate with the highest score is selected (or none of the candidates is selected if that score is below a threshold). It is possible to use an alternative approach that only takes into account the local contexts around the sub-mentions, but we are interested in a thematic match between the candidate page and the mention, and the local contexts are a loud signal of this linkage. The task of identifying the best candidate for a mention is presented as a retrievable problem. In information that is retrieved, documents for a given query are asked for the likelihood that the query will be generated from the document."}, {"heading": "3.4 Evaluation", "text": "To evaluate the quality of the labels, we apply our method to the corpus of the New York Times, which has not included New York Times articles for 20 years. Our approach is applied to a subset (January 1, 2004-June 19, 2007) consisting of 308k articles. Stanford Tagger [12] is used to extract the person name string from these articles, resulting in 5 million sub-mentions. High precision within the doc coreference system identifies 2.5 million mentions used in document disambiguification. We use a recent snapshot of Wikipedia3, which consists of 6.2 million pages, of which 2.5 million diversions and 121k are disambiguations. Since the distribution of words in Wikipedia and our corpus can vary significantly, we calculate separate idf. 3 enwiki-20080103-page articles.xmllout of 2.5 million menus, our method does not select the candidate."}, {"heading": "3.5 Dataset Ambiguity", "text": "In this context, it should also be mentioned that the two protagonists who have been active in the United States, in Europe, in the United States and in the United States in recent years are people who have been active in the most diverse areas of life, people who have been active in the most diverse worlds of life: women, men, women, men, men, men, men, women, men, women, women, men, women, men, men, women, women, women, men, women, women, women, men, women, women, women, men, women, women, women, men, women, women, women, women, men, women, women, women, men, women, women, women, women, men, women, women, women, men, women, women, women, men, women, women, men, women, women, women, men, women, women, women, men, women, women, women, men, women, women, women, men, women, women, women, men, women, women, men, women, women, women, men, women, women, women, men, women, women, women, men, women, women, women, men, women, women, women, men, women, women, women, men, women, women, women, men, women, women, women, men, women, women, women, women, men, women, women, women, women, women, women, women, women, women, men, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, men, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women, women,"}, {"heading": "4 Cross-Document Coreference", "text": "Since this conditional, random field-based model consists only of features based on the raw text surrounding the mentions, and does not use Wikipedia monitoring, it can be applied to any text source. In particular, we show that the model is capable of generalizing to a long evaluation set that includes mentions and units that do not occur during training."}, {"heading": "4.1 Model", "text": "We adapt the state of the art within the [9] document reference model to our problem. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "4.2 Features", "text": "Since we want to apply the model to any source of mentions, we rely only on the text stored as observed properties of the mentions (X). As described above, these properties include the mentioning text, the canonical representation of the mention and the word bags extracted from context windows, as well as the sub-mentioning texts. With a mentioning pair < mi, mj > we define the following binary pair characteristics \u03c6 (mi, mj): - Canonical correspondence with the canonical representation of true iff mi is the lowercase one with that of mj. Similarly, we insert a canonical correspondence with the canonical correspondence. - Cosmic spacing characteristics for each type of pouch are measured and quantified in ten letters."}, {"heading": "4.3 Inference", "text": "In this section, we describe a method for scaling inferences to large data sets. In particular, we use a local search method based on the Metropolis Hastings (MH) algorithm with a canopy-based applicant. We briefly describe MH and canopies, then we present our jump function. MH is a Markov chain Monte Carlo method that performs stochastically local changes by making jumps from a suggestion distribution Q depending on the current state that generates a new configuration y. The proposed configuration y is most likely \u03b1 = \u03c0 (y \u00b2) / \u03c0 (y | x) \u00b7 q (y \u00b2) \u00b7 q (y \u00b2) / q \u00b2 (y \u00b2) / q \u00b2 (y \u00b2) / q \u00b2 distribution, where the distribution is encoded by the model (see Eq 1), and q is the probability that the jump to y \u00b2 distributions is proposed. Since we perform a maximum posteriori (MAP) with no latent variables, we can safely ignore the variables contained therein."}, {"heading": "4.4 Learning", "text": "Parameter estimations are performed using SampleRank [34], an extremely efficient stochastic method of gradient ascent that solves a ranking objective function. SampleRank uses the same suggestion distribution as during inference and learns a model whose probabilities correspond to a user-defined ranking function via co-conference configurations. In particular, we learn a model that grades configurations by pairwise accuracy."}, {"heading": "4.5 Experiments", "text": "We present preliminary experiments that show that this is scalable within the document reference model to draw conclusions about a large data set. Specifically, we use our remotely labeled NYT data, which consists of over one million mentions, as described in Section 3.4. This is evenly divided into training and test sets, each containing approximately 550,000 mentions and approximately 90,000 units. We perform ten SampleRank iterations on the training set, with one iteration consisting of 100,000 Metropolis Hastings steps; each iteration takes an average of just 19.6 minutes and the training total less than five hours. We draw conclusions from five million Metropolis Hastings steps on the test data provided. Inference takes 9.5 hours and achieves a paired F1 score of 89.83%. This high score is encouraging for a model trained on our remotely labeled data."}, {"heading": "5 Related Work", "text": "In fact, most people will be able to move to another world in which they are able to live, in which they want to live."}, {"heading": "6 Conclusions", "text": "Motivated by the difficulty of labeling data for large cross-document cores, we propose an approach where large sets of data are automatically labeled with Wikipedia. We applied the method to the New York Times corpus and analyzed the noise and ambiguity in the generated dataset. To enable cross-document cores on this large dataset, we introduced a canopy-based sampling approach for training and inference. The model we trained on this data is versatile in downstream applications, such as search, reputation analysis, trend analysis, and so on. In addition, the model's predictions can be used to provide additional ambiguities and diversions for Wikipedia.There are a number of options for future work. We intend to release the dataset so that the cross-document corpus community can benefit from a large, labeled corpus. Although the current noise level is acceptable, our method can be improved to create less complicated sets, while the more complex ones can be used."}, {"heading": "Acknowledgements", "text": "This work has been supported in part by the Center for Intelligent Information Retrieval, in part by SRI International Subcontract # 27-001338 and ARFL Prime Contract # FA875009-C-0181, and in part by UPenn NSF Medium IIS-0803847. All opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}], "references": [{"title": "Entity-based cross-document coreferencing using the vector space model", "author": ["A. Bagga", "B. Baldwin"], "venue": "International Conference on Computational Linguistics. pp. 79\u201385. Association for Computational Linguistics, Morristown, NJ, USA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "Who is who and what is what: experiments in cross-document coreference", "author": ["A. Baron", "M. Freedman"], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pp. 274\u2013283. Association for Computational Linguistics", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Generalized expectation criteria for bootstrapping extractors using record-text alignment", "author": ["K. Bellare", "A. McCallum"], "venue": "Conference on Empirical Methods on Natural Language Processing (EMNLP). pp. 131\u2013140. Association for Computational Linguistics, Singapore", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Understanding the value of features for coreference resolution", "author": ["E. Bengston", "D. Roth"], "venue": "Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Creating a gold standard for person cross-document coreference resolution in italian news", "author": ["L. Bentivogli", "C. Girardi", "E. Pianta"], "venue": "LREC Workshop on Resources and Evaluation for Identity Matching, Entity Resolution and Entity Management", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Automatic entity disambiguation: Benefits to NER, relation extraction, link analysis, and inference", "author": ["M. Blume"], "venue": "International Conference on Intelligence Analysis (ICIA)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Using encyclopedic knowledge for named entity disambiguation", "author": ["R.C. Bunescu", "M. Pasca"], "venue": "European Chapter of the Association for Computational Linguistics (EACL)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Large-scale named entity disambiguation based on Wikipedia data", "author": ["S. Cucerzan"], "venue": "Empirical Methods in Natural Language Processing (EMNLP). pp. 708\u2013716. Association for Computational Linguistics, Prague, Czech Republic", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "First-order probabilistic models for coreference resolution", "author": ["A. Culotta", "M. Wick", "A. McCallum"], "venue": "North American Chapter of the Association for Computational Linguistics - Human Language Technologies (NAACL HLT)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "A corpus for cross-document co-reference", "author": ["D. Day", "J. Hitzeman", "M. Wick", "K. Crouch", "M. Poesio"], "venue": "International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Using Wikitology for crossdocument entity coreference resolution", "author": ["T. Finin", "Z. Syed", "J. Mayfield", "P. McNamee", "C. Piatko"], "venue": "AAAI Spring Symposium on Learning by Reading and Learning to Read. AAAI Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["J.R. Finkel", "T. Grenager", "C. Manning"], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL). pp. 363\u2013370. Association for Computational Linguistics, Ann Arbor, Michigan", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Computing semantic relatedness using wikipedia-based explicit semantic analysis", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI). pp. 1606\u20131611. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Cross-document coreference on a large scale corpus", "author": ["C.H. Gooi", "J. Allan"], "venue": "Human Language Technologies: Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT). pp. 9\u201316. Association for Computational Linguistics, Boston, Massachusetts, USA", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Unsupervised coreference resolution in a nonparametric bayesian model", "author": ["A. Haghighi", "D. Klein"], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL). pp. 848\u2013855. Association for Computational Linguistics, Prague, Czech Republic", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Named entity disambiguation by leveraging Wikipedia semantic knowledge", "author": ["X. Han", "J. Zhao"], "venue": "Conference on Information and Knowledge Management (CIKM). pp. 215\u2013224. ACM, New York, NY, USA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Interpolated estimation of markov source parameters from sparse data", "author": ["F. Jelinek", "R.L. Mercer"], "venue": "Pattern Recognition in Practice pp. 381\u2013397", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1980}, {"title": "Unsupervised personal name disambiguation", "author": ["G.S. Mann", "D. Yarowsky"], "venue": "Human Language Technologies: Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT). pp. 33\u201340", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Cross-document coreference resolution: A key technology for learning by reading", "author": ["J. Mayfield", "D. Alexander", "B. Dorr", "J. Eisner", "T. Elsayed", "T. Finin", "C. Fink", "M. Freedman", "N. Garera", "P McNamee"], "venue": "AAAI 2009 Spring Symposium on Learning by Reading and Learning to Read", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient clustering of high-dimensional data sets with application to reference matching", "author": ["A. McCallum", "K. Nigam", "L. Ungar"], "venue": "Knowledge Discovery and Data Mining (KDD). pp. 169\u2013178", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "FACTORIE: Probabilistic programming via imperatively defined factor graphs", "author": ["A. McCallum", "K. Schultz", "S. Singh"], "venue": "Neural Information Processing Systems (NIPS)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Machine learning for coreference resolution: From local classification to global ranking", "author": ["V. Ng"], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Weakly supervised learning for cross-document person name disambiguation supported by information extraction", "author": ["C. Niu", "W. Li", "R.K. Srihari"], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL). p. 597. Association for Computational Linguistics, Morristown, NJ, USA", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "An unsupervised language independent method of name discrimination using second order co-occurrence features", "author": ["T. Pedersen", "A. Kulkarni", "R. Angheluta", "Z. Kozareva", "T. Solorio"], "venue": "International Conference on Intelligent Text Processing and Computational Linguistics (CICLing). pp. 208\u2013222", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "A language modeling approach to information retrieval", "author": ["J.M. Ponte", "W.B. Croft"], "venue": "Annual International ACM SIGIR Conference on Research and development in Information Retrieval. pp. 275\u2013281. ACM, New York, NY, USA", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution", "author": ["S.P. Ponzetto", "M. Strube"], "venue": "Human Language Technologies: Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL HLT). pp. 192\u2013199. Association for Computational Linguistics, Morristown, NJ, USA", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Improving cross-document coreference", "author": ["O. Popescu", "C. Girardi", "E. Pianta", "B. Magnini"], "venue": "Journ\u00e9es Internationales d\u2019Analyse statistique des Donn\u00e9es Textuelles 9, 961\u2013969", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Word sense discrimination by clustering contexts in vector and similarity spaces", "author": ["A. Purandare", "T. Pedersen"], "venue": "Conference on Computational Natural Language Learning (CoNLL). pp. 41\u201348. Boston", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Is Hillary Rodham Clinton the president? disambiguating names across documents", "author": ["Y. Ravin", "Z. Kazi"], "venue": "Annual Meeting of the Association for Computational Linguistics (ACL). pp. 9\u201316", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "The New York Times annotated corpus", "author": ["E. Sandhaus"], "venue": "Linguistic Data Consortium", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning hidden markov model structure for information extraction", "author": ["K. Seymore", "A. McCallum", "R. Rosenfeld"], "venue": "AAAI Workshop on Machine Learning for Information Extraction. pp. 37\u201342", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}, {"title": "A general language model for information retrieval", "author": ["F. Song", "W.B. Croft"], "venue": "International Conference on Information and Knowledge Management (CIKM). pp. 316\u2013321. ACM, New York, NY, USA", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1999}, {"title": "WikiRelate! computing semantic relatedness using wikipedia", "author": ["M. Strube", "S.P. Ponzetto"], "venue": "AAAI Conference on Artificial Intelligence. pp. 1419\u20131424. AAAI Press", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Samplerank: Learning preferences from atomic gradients", "author": ["M. Wick", "K. Rohanimanesh", "A. Culotta", "A. McCallum"], "venue": "Neural Information Processing Systems (NIPS), Workshop on Advances in Ranking", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "Coreference is vital for many down-stream semantic analysis and knowledge discovery tasks [6].", "startOffset": 90, "endOffset": 93}, {"referenceID": 21, "context": "In particular, while significant progress has been made in within-document coreference [22,4,9,15] (resolving mentions from inside a single document), the larger problem of cross-document coreference (resolving mentions from across a collection of many documents) has received significantly less attention.", "startOffset": 87, "endOffset": 98}, {"referenceID": 3, "context": "In particular, while significant progress has been made in within-document coreference [22,4,9,15] (resolving mentions from inside a single document), the larger problem of cross-document coreference (resolving mentions from across a collection of many documents) has received significantly less attention.", "startOffset": 87, "endOffset": 98}, {"referenceID": 8, "context": "In particular, while significant progress has been made in within-document coreference [22,4,9,15] (resolving mentions from inside a single document), the larger problem of cross-document coreference (resolving mentions from across a collection of many documents) has received significantly less attention.", "startOffset": 87, "endOffset": 98}, {"referenceID": 14, "context": "In particular, while significant progress has been made in within-document coreference [22,4,9,15] (resolving mentions from inside a single document), the larger problem of cross-document coreference (resolving mentions from across a collection of many documents) has received significantly less attention.", "startOffset": 87, "endOffset": 98}, {"referenceID": 0, "context": "combine a clustering procedure with thresholded distance function over entities [1,14].", "startOffset": 80, "endOffset": 86}, {"referenceID": 13, "context": "combine a clustering procedure with thresholded distance function over entities [1,14].", "startOffset": 80, "endOffset": 86}, {"referenceID": 14, "context": "More recently, generative and non-parametric Bayesian clustering techniques have been proposed as a way to circumvent the need for labeled data [15].", "startOffset": 144, "endOffset": 148}, {"referenceID": 3, "context": "Unfortunately, these unsupervised methods tend to perform considerably worse than supervised methods, making labeled data essential for achieving state-of-the art performance [4].", "startOffset": 175, "endOffset": 178}, {"referenceID": 30, "context": "We process the original data together with this distantlylabeled data [31] to automatically label the original relevant corpus.", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "This is a type of indirect supervision by alignment [3].", "startOffset": 52, "endOffset": 55}, {"referenceID": 19, "context": "We address the challenge of scaling up this model to our massive data set by using a family of Metropolis-Hastings proposal distributions that use canopies [20] to efficiently explore the hypothesis space.", "startOffset": 156, "endOffset": 160}, {"referenceID": 0, "context": "To reduce the number of mentions available for cross-document, within-document coreference is usually applied as a preprocessing step to resolve the entities within the document [1,6,29].", "startOffset": 178, "endOffset": 186}, {"referenceID": 5, "context": "To reduce the number of mentions available for cross-document, within-document coreference is usually applied as a preprocessing step to resolve the entities within the document [1,6,29].", "startOffset": 178, "endOffset": 186}, {"referenceID": 28, "context": "To reduce the number of mentions available for cross-document, within-document coreference is usually applied as a preprocessing step to resolve the entities within the document [1,6,29].", "startOffset": 178, "endOffset": 186}, {"referenceID": 5, "context": "Since our method of within-doc coreference is applied only to mention strings that are proper nouns, it is similar to that used in [6,27].", "startOffset": 131, "endOffset": 137}, {"referenceID": 26, "context": "Since our method of within-doc coreference is applied only to mention strings that are proper nouns, it is similar to that used in [6,27].", "startOffset": 131, "endOffset": 137}, {"referenceID": 8, "context": "The rest of the approach does not rely on a particular choice of within-doc coreference and instead a machine-learned coreference model that also considers pronouns can be used, such as [9].", "startOffset": 186, "endOffset": 189}, {"referenceID": 24, "context": "Smoothing is often introduced as a back-off distribution so that tokens that do not appear in the document have a small positive probability of being generated [25,32].", "startOffset": 160, "endOffset": 167}, {"referenceID": 31, "context": "Smoothing is often introduced as a back-off distribution so that tokens that do not appear in the document have a small positive probability of being generated [25,32].", "startOffset": 160, "endOffset": 167}, {"referenceID": 16, "context": "To account for the back-off distribution, we use uniform multinomial for the global language model (P (t|Mg) = \u03b1), and use linear interpolation (Jelinek-Mercer smoothing) using \u03bb ([17]).", "startOffset": 180, "endOffset": 184}, {"referenceID": 29, "context": "To evaluate the quality of the labels, we apply our method to the New York Times corpus [30] which contains 20 years of New York Times articles.", "startOffset": 88, "endOffset": 92}, {"referenceID": 11, "context": "The Stanford NER tagger [12] is used to extract person-name string from these articles, resulting in 5 million sub-mentions.", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "We adapt the state-of-the-art within document coreference model of [9] to our problem setting.", "startOffset": 67, "endOffset": 70}, {"referenceID": 20, "context": "Our model is implemented as an imperativelydefined factor graph using the FACTORIE probabilistic programming library [21].", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "Since we are performing maximum a posteriori (MAP) with no latent variables, we can safely ignore the ratio containing q [21].", "startOffset": 121, "endOffset": 125}, {"referenceID": 19, "context": "A canopy is a relaxation of a clustering where mentions can refer to more that one entity (in other words the transitivity assumption is not enforced and mentions can be in more than one cluster) [20].", "startOffset": 196, "endOffset": 200}, {"referenceID": 0, "context": "Also, let the notation t \u223c\u03c1 T mean to draw an element t from a set T with probability distribution \u03c1 : T \u2192 [0, 1] s.", "startOffset": 107, "endOffset": 113}, {"referenceID": 33, "context": "Parameter estimation is performed with SampleRank [34], an extremely efficient stochastic gradient-ascent based method that solves a ranking-objective function.", "startOffset": 50, "endOffset": 54}, {"referenceID": 0, "context": "One of the first approaches to cross-document coreference [1] uses a pre-trained within-document step, followed by an idf based scoring function for pairs of contexts for clustering.", "startOffset": 58, "endOffset": 61}, {"referenceID": 28, "context": "[29] extend this work to be more scalable by comparing pairs of context only if the mentions are deemed ambiguous enough using a heuristic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of performing inference [14].", "startOffset": 154, "endOffset": 158}, {"referenceID": 23, "context": "[24] and Purandare & Pedersen [28] integrate second-order co-occurrence of words into the similarity function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[24] and Purandare & Pedersen [28] integrate second-order co-occurrence of words into the similarity function.", "startOffset": 30, "endOffset": 34}, {"referenceID": 5, "context": "A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation for clustering [6,2,27].", "startOffset": 163, "endOffset": 171}, {"referenceID": 1, "context": "A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation for clustering [6,2,27].", "startOffset": 163, "endOffset": 171}, {"referenceID": 26, "context": "A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation for clustering [6,2,27].", "startOffset": 163, "endOffset": 171}, {"referenceID": 17, "context": "Mann & Yarowsky [18] extract biographical facts from the Web, such as birthdate, which are used as features for clustering.", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "[23] incorporate information extraction into the context similarity model, and construct small annotated datasets to learn some of the parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] use additional features that are information extraction based.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Bagga & Baldwin [1] generate a small, highly-ambiguous \u201cjohn smith\u201d dataset.", "startOffset": 16, "endOffset": 19}, {"referenceID": 13, "context": "Gooi & Allan [14] create the ambiguous \u201cPerson-X dataset\u201d that replaces names of different entities to be the same Person-X.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "[23] generate approximate data sets for partial supervision, however the datasets are too small and noisy to be useful for training.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] introduces a small dataset containing high ambiguity (209 names over 709 entities), but the data set is not big enough for large-scale models.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] also introduce a tool and a corpus, however the corpus offers little ambiguity.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Features based on Wikipedia, including categorical and structure, have been used to train supervised models of within-document [26] and cross-document [11,19] coreference.", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "Features based on Wikipedia, including categorical and structure, have been used to train supervised models of within-document [26] and cross-document [11,19] coreference.", "startOffset": 151, "endOffset": 158}, {"referenceID": 18, "context": "Features based on Wikipedia, including categorical and structure, have been used to train supervised models of within-document [26] and cross-document [11,19] coreference.", "startOffset": 151, "endOffset": 158}, {"referenceID": 12, "context": "Similar to our generative model, wikipedia has been used to score similarity between documents [13,16,33].", "startOffset": 95, "endOffset": 105}, {"referenceID": 15, "context": "Similar to our generative model, wikipedia has been used to score similarity between documents [13,16,33].", "startOffset": 95, "endOffset": 105}, {"referenceID": 32, "context": "Similar to our generative model, wikipedia has been used to score similarity between documents [13,16,33].", "startOffset": 95, "endOffset": 105}, {"referenceID": 15, "context": "By making a \u201cone person per document\u201d assumption, Han & Zhao [16] treat document clustering as an approach to unsupervised coreference.", "startOffset": 61, "endOffset": 65}, {"referenceID": 32, "context": "Strube & Ponzetto [33] uses the semantic relatedness of articles as a feature for a supervised coreference model.", "startOffset": 18, "endOffset": 22}, {"referenceID": 6, "context": "Our work is most similar to Bunescu & Pasca [7] and Cucerzan [8] which also use Wikipedia to disambiguate entities in an unsupervised manner.", "startOffset": 44, "endOffset": 47}, {"referenceID": 7, "context": "Our work is most similar to Bunescu & Pasca [7] and Cucerzan [8] which also use Wikipedia to disambiguate entities in an unsupervised manner.", "startOffset": 61, "endOffset": 64}, {"referenceID": 6, "context": "Bunescu & Pasca [7] use content and categorical information to create features for a scoring model that can disambiguate mentions that appear in wikipedia articles.", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "Cucerzan [8] disambiguates mentions in arbitrary data sources, and is evaluated on news articles.", "startOffset": 9, "endOffset": 12}], "year": 2010, "abstractText": "Cross-document coreference, the problem of resolving entity mentions across multi-document collections, is crucial to automated knowledge base construction and data mining tasks. However, the scarcity of large labeled data sets has hindered supervised machine learning research for this task. In this paper we develop and demonstrate an approach based on \u201cdistantly-labeling\u201d a data set from which we can train a discriminative cross-document coreference model. In particular we build a dataset of more than a million people mentions extracted from 3.5 years of New York Times articles, leverage Wikipedia for distant labeling with a generative model (and measure the reliability of such labeling); then we train and evaluate a conditional random field coreference model that has factors on cross-document entities as well as mention-pairs. This coreference model obtains high accuracy in resolving mentions and entities that are not present in the training data, indicating applicability to non-Wikipedia data. Given the large amount of data, our work is also an exercise demonstrating the scalability of our approach.", "creator": "LaTeX with hyperref package"}}}