{"id": "1005.5462", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-May-2010", "title": "On the clustering aspect of nonnegative matrix factorization", "abstract": "This paper provides a theoretical explanation on the clustering aspect of nonnegative matrix factorization (NMF). We prove that even without imposing orthogonality or sparsity constraint on the basis and/or coefficient matrix, NMF still can give clustering results, thus providing a theoretical support for the works of Xu et al. [1] and Kim et al. [2], where the authors showed the superiority of the standard NMF as a clustering method.", "histories": [["v1", "Sat, 29 May 2010 15:27:16 GMT  (58kb)", "https://arxiv.org/abs/1005.5462v1", "4 pages, no figure"], ["v2", "Sat, 12 Jun 2010 10:40:53 GMT  (59kb)", "http://arxiv.org/abs/1005.5462v2", "4 pages, no figure, to appear in ICEIE 2010"]], "COMMENTS": "4 pages, no figure", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["andri mirzal", "masashi furukawa"], "accepted": false, "id": "1005.5462"}, "pdf": {"name": "1005.5462.pdf", "metadata": {"source": "CRF", "title": "On the clustering aspect of nonnegative matrix factorization", "authors": ["Andri Mirzal", "Masashi Furukawa"], "emails": ["andri@complex.eng.hokudai.ac.jp", "mack@complex.eng.hokudai.ac.jp"], "sections": [{"heading": null, "text": "In fact, it is a reactionary, reactionary, reactionary and reactionary group that is able to hold on to power."}, {"heading": "II. CLUSTERING ASPECT OF NMF", "text": "(7) In the \"reactionary\" constellation (7), in the \"reactionary\" constellation (8), in the \"reactionary\" constellation (7), in the \"reactionary\" constellation (8), (8), (8), in the \"reactionary\" constellation (8), in the \"reactionary\" constellation (8), (8), (8), (8), (9), (8), (8), (9), (9), (9), (9, (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9, (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9), (9) (9), (9), (9), (9), (9), (9), (9), (9), (9) (9), (9), (9), (9), (9), (9) (9), (9), (9), (9), (9) (9), (9), (9), (9), (9), (9), (9) (9), (9), (9), (9), (9), (9), (9) (9), (9) (9), (9), (9), (9) (9), (9), (9) (9), (9), (9) (9), (9), (9), (9), (9), (9), (9), (9) (9), (9), (9), (9), (9), (9),"}, {"heading": "A. Orthogonality constraints on both B and C", "text": "The following theorems demonstrate that the introduction of column northogonal constraints for B and row orthogonal constraints for C leads to the simultaneous clustering of similar items and related characteristics. Theorem 1. Minimizing the following objectivity constraints for B, CJa (B, C) = 12, A \u2212 BC (2F (5) s.t. B \u2265 0, C \u2265 0, BTB = I, CCT = Iis equivalent to applying ratio associations for G (ATA) and G (AAT), where ATA and AAT each use the item affinity matrix and the feature affinity matrix to form a simultaneous cluster of similar items and related characteristics.Proof: A \u2212 BC-2F = tr (A \u2212 BC) T (A \u2212 BC)))) T (A \u2212 BC)))) = tr (A \u2212 T (A \u2212 BC), 2CTTA (BTTA + BTI)."}, {"heading": "B. Orthogonality constraints on C", "text": "If the orthogonal function (B, C) = Jb (B) \u2212 r (A \u2212 BB) BB (B \u2212 B) BB (B \u2212 B) B (A \u2212 B) B (A \u2212 B) B (A \u2212 BC) B (A \u2212 BC) B (A \u2212 BC) T (A \u2212 BC) B (A \u2212 BC) B (A \u2212 BC) T (A \u2212 BC) T (A \u2212 BC) T (A \u2212 BC)) = tr (A \u2212 BC) = tr (A \u2212 BTACT + CTBTBC)."}, {"heading": "C. Orthogonality constraints on B", "text": "Theorem 3: The minimization of the following objectives in B, CJc (B, C) = 12 \u0445 A \u2212 BC-2F (28) s.t. B \u2265 0, C \u2265 0, BTB = I corresponds to the application of ratio association to G (AAT) and also leads to the item clustering indicator matrix C, which is approximately row orthogonal. Proof: By following the proof of Theorem 2, the minimization of Jc corresponds to the simultaneous optimization: max Btr (B T AA T) s.t. BTB = I, (29) max Ctr (2CTATAC), and (30) min Ctr (C T CA T AC T).minCtr (CC T CC CC T).31) Gl. 29 corresponds to equivalent to equivalent to equivalent 16 and leads to clustering of related features and the optimization of equivalents 30 and 31 Gl."}, {"heading": "D. No orthogonality constraint on both B and C", "text": "In this section, we prove that the application of the standard NMF to the feature-by-item data matrix ultimately leads to the concurrent feature and item cluster indicator. Theorem 4. Minimizing the following objective indicator B, CJd (B, C) = 12% A \u2212 BC 2F (34) s.t. B \u2265 0, C \u2265 0, leads to the feature cluster indicator Matrix B and the item cluster indicator matrix C, which are approximately column and row-by-row thogonal. (36) By following the evidence for Theorem 2, Jd minimizes concurrent optimization: max B, Ctr (B T AC T) and (35) min B, Ctr (B T BCC T T T T T T T T).By substituting B = ACT and C = BTA into the above equations, we obtain: max Btr (B), max Btr (T T), tr (T).B B (B), max Btr (tr), tr (B) max Btr (Btr).B (Btr."}, {"heading": "III. UNIPARTITE AND DIRECTED GRAPH CASES", "text": "The affinity matrix W, induced by a unipartite (undirected) graph, is a symmetrical matrix, which is a special case of the rectangular affinity matrix A. Therefore, following the discussion in Section II, it can be shown that the standard NMF applied to W leads to the cluster indicator matrix, which is almost orthogonal. Affinity matrix V, induced by a directed graph, is an asymmetrical square matrix. Since columns and rows of V correspond to the same set of vertices with the same order as the cluster problem, V can be replaced by V + VT, which is a symmetrical matrix. Then the standard NMF can be applied to this matrix to obtain the cluster indicator matrix, which is almost orthogonal."}, {"heading": "IV. RELATED WORKS", "text": "Ding et al. [8] provides the theoretical analysis of equivalences between orthogonal NMF and K-mean clusters for both rectangular data matrices and symmetrical matrices. However, since their proofs use the conditions of the zero gradient, the hidden assumptions (setting the Lagrange multipliers to zeros) are not revealed there. Indeed, it can easily be shown that their approach is the KKT conditions applied to the unconventional conversion of Gl. 2. Therefore, there is no guarantee that the minimization of Gl. 2 by using the conditions of the zero gradient will lead to the stationary point located on the non-negative orthant as required by objectivity."}, {"heading": "V. CONCLUSION", "text": "Using the strict CCT optimality conditions, we were able to show that even without explicit orthogonality or austerity limitations, NMF generates an approximate column orthogonal base matrix and a line orthogonal coefficient matrix that leads to simultaneous clustering of features and items. Therefore, this result provides the theoretical explanation for some experimental results showing the performance of the standard NMF as a cluster tool, which is reported to be better than the spectral methods [1] and the K-mean algorithm [2]."}], "references": [{"title": "Document clustering based on non-negative matrix factorization,", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "Proc. ACM SIGIR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Sparse nonnegative matrix factorization for clustering,", "author": ["J. Kim", "H. Park"], "venue": "CSE Technical Reports, Georgia Institute of Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "The relationships among various nonnegative matrix factorization methods for clustering,", "author": ["T. Li", "C. Ding"], "venue": "Proc. ACM 6th Int\u2019l Conf. on Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Convex and Semi-Nonnegative Matrix Factorizations,", "author": ["C. Ding", "T. Li", "M.I. Jordan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Learning the parts of objects by non-negative matrix factorization,", "author": ["D. Lee", "H. Seung"], "venue": "Nature,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Learning spatially localized, parts-based representation,", "author": ["S.Z. Li", "X.W. Hou", "H.J. Zhang", "Q.S. Cheng"], "venue": "Proc. IEEE Comp. Soc. Conf. on Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Non-negative Matrix Factorization with Sparseness Constraints,", "author": ["P.O. Hoyer"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering,", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "Proc. 12th ACM SIGKDD Int\u2019l Conf. on Knowledge Discovery and Data Mining,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Generalized nonnegative matrix approximation with Bregman divergences,", "author": ["I.S. Dhillon", "S. Sra"], "venue": "UTCS Technical Reports, The University of Texas at Austin,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Toward faster nonnegative matrix factorization: A new algorithm and comparisons,", "author": ["J. Kim", "H. Park"], "venue": "Proc. 8th IEEE International Conference on Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Nonnegative matrix factorization based on alternating nonnegativity constrained least squares and active set method,", "author": ["H. Kim", "H. Park"], "venue": "SIAM. J. Matrix Anal. & Appl.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Fast projection-based methods for the least squares nonnegative matrix approximation problem,", "author": ["D. Kim", "S. Sra", "I.S. Dhillon"], "venue": "Stat. Anal. Data Min.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "On the convergence of the block nonlinear Gauss-Seidel method under convex constraints,", "author": ["L. Grippo", "M. Sciandrone"], "venue": "Operation Research Letters,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Weighted Graph Cuts without eigenvectors: A multilevel approach,", "author": ["I.S. Dhillon", "Y. Guan", "B. Kulis"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "More, \u201cProjected gradient methods for linearly constrained problems,", "author": ["J.J.P.H. Calamai"], "venue": "Mathematical Programming,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1987}, {"title": "On the equivalence of nonnegative matrix factorization and spectral clustering,", "author": ["C. Ding", "X. He", "H.D. Simon"], "venue": "Proc. SIAM Data Mining Conference,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "[1] and Kim et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2], that show the superiority of the standard NMF as a clustering method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": ", [3] and [4].", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": ", [3] and [4].", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "The nonnegativity constraints allow only nonsubstractive linear combinations of the basis vectors bk to construct the data vectors an, thus providing the parts-based interpretations as shown in [5], [6], [7].", "startOffset": 194, "endOffset": 197}, {"referenceID": 5, "context": "The nonnegativity constraints allow only nonsubstractive linear combinations of the basis vectors bk to construct the data vectors an, thus providing the parts-based interpretations as shown in [5], [6], [7].", "startOffset": 199, "endOffset": 202}, {"referenceID": 6, "context": "The nonnegativity constraints allow only nonsubstractive linear combinations of the basis vectors bk to construct the data vectors an, thus providing the parts-based interpretations as shown in [5], [6], [7].", "startOffset": 204, "endOffset": 207}, {"referenceID": 0, "context": "[1] showed that NMF outperforms standard spectral methods in finding the document clustering in two text corpora, TDT2 and Reuters.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] showed that NMF and sparse NMF are much more superior methods compared to the K-means algorithm in both a synthetic dataset (which is well separated) and a real dataset (TDT2).", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "If sparsity constraints are imposed to columns of C, the clustering aspect of NMF is intuitive since in the extreme case where there is only one nonzero entry per column, NMF will be equivalent to the K-means algorithm employed to the data vectors an [8], and the sparsity constraints can be thought as the relaxation to the strict orthogonality constraints on rows of C (an equivalent explanation can also be stated for imposing sparsity on rows of B).", "startOffset": 251, "endOffset": 254}, {"referenceID": 0, "context": "[1] and Kim et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2], even without imposing sparsity constraints, NMF still can give very promising clustering results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "So far the best explanation for this remarkable fact is only qualitative: the standard NMF produces nonorthogonal latent semantic directions (the basis vectors) that are more likely to correspond to each of the clusters than those produced by the spectral methods, thus the clustering induced from the latent semantic directions of the standard NMF are better than clustering by the spectral methods [1].", "startOffset": 400, "endOffset": 403}, {"referenceID": 8, "context": "Detailed discussion on the Bregman divergences for NMF can be found in [9].", "startOffset": 71, "endOffset": 74}, {"referenceID": 9, "context": ", alternating NLS with block principal pivoting algorithm [10], active set method [11], and projected quasi-Newton algorithm [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": ", alternating NLS with block principal pivoting algorithm [10], active set method [11], and projected quasi-Newton algorithm [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 11, "context": ", alternating NLS with block principal pivoting algorithm [10], active set method [11], and projected quasi-Newton algorithm [12].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "for two-block case, any limit point of the sequence {B,C}, where t is the updating step, is a stationary point [13].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "16 are the ratio association objectives (see [14] for details on various graph cuts objectives) applied to G(AA) and G(AA) respectively.", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "Now we have box-constraint objectives which are known to behave well and are guaranteed to converge to the stationary point [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 7, "context": "[8] provides the theoretical analysis on the equivalences between orthogonal NMF to K-means clustering for both rectangular data matrices and symmetric matrices.", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Therefore, their results cannot be used to explain the abundant experimental results that show the power of the standard NMF in clustering, latent factors identification, learning the parts of objects, and producing sparse matrices even without explicit sparsity constraint [5].", "startOffset": 274, "endOffset": 277}, {"referenceID": 0, "context": "This result, therefore, gives the theoretical explanation on some experimental results that show the power of the standard NMF as a clustering tool which are reported to be better than the spectral methods [1] and K-means algorithm [2].", "startOffset": 206, "endOffset": 209}, {"referenceID": 1, "context": "This result, therefore, gives the theoretical explanation on some experimental results that show the power of the standard NMF as a clustering tool which are reported to be better than the spectral methods [1] and K-means algorithm [2].", "startOffset": 232, "endOffset": 235}], "year": 2010, "abstractText": "This paper provides a theoretical explanation on the clustering aspect of nonnegative matrix factorization (NMF). We prove that even without imposing orthogonality nor sparsity constraint on the basis and/or coefficient matrix, NMF still can give clustering results, thus providing a theoretical support for many works, e.g., Xu et al. [1] and Kim et al. [2], that show the superiority of the standard NMF as a clustering method. Keywords\u2014bound-constrained optimization, clustering method, non-convex optimization, nonnegative matrix factorization", "creator": "LaTeX with hyperref package"}}}