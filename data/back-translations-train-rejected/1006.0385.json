{"id": "1006.0385", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2010", "title": "Brain-Like Stochastic Search: A Research Challenge and Funding Opportunity", "abstract": "Brain-Like Stochastic Search (BLiSS) refers to this task: given a family of utility functions U(u,A), where u is a vector of parameters or task descriptors, maximize or minimize U with respect to u, using networks (Option Nets) which input A and learn to generate good options u stochastically. This paper discusses why this is crucial to brain-like intelligence (an area funded by NSF) and to many applications, and discusses various possibilities for network design and training. The appendix discusses recent research, relations to work on stochastic optimization in operations research, and relations to engineering-based approaches to understanding neocortex.", "histories": [["v1", "Tue, 1 Jun 2010 18:16:10 GMT  (159kb)", "http://arxiv.org/abs/1006.0385v1", "Plenary talk at IEEE Conference on Evolutionary Computing 1999, extended in 2010 with new appendix"]], "COMMENTS": "Plenary talk at IEEE Conference on Evolutionary Computing 1999, extended in 2010 with new appendix", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["paul j werbos"], "accepted": false, "id": "1006.0385"}, "pdf": {"name": "1006.0385.pdf", "metadata": {"source": "META", "title": "Brain-Like Stochastic Search:", "authors": ["Paul J. Werbos"], "emails": ["pwerbos@nsf.gov"], "sections": [{"heading": null, "text": "* The views expressed here are those of the author, not the official views of the NSF. This task: Given a family of utility functions U (u,) where a vector of parameters or task descriptors exists, maximize or minimize U in relation to u by using networks (option networks) that learn to enter and generate good options u stochastically. This paper discusses why this is critical for brain-like intelligence (an NSF-funded area) and for many applications, and discusses various options for network design and training."}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to leave the country without being able to leave it."}, {"heading": "2 BLiSS and Brain-Like Intelligence", "text": "In fact, most of them will be able to play by the rules that they have set themselves in order to play by the rules that they have played by."}, {"heading": "A fourth \u201cnew\u201d topic is the training of neural nets to", "text": "I would consider the formation of stochastic models of the world as part of the topic of system identification (see e.g. Chapter 13 of White and Sofge, 192). However, since this priority has not received serious attention from the university research community in this formulation, I would add a new topic description to try to encourage unification of the many different strands of work aimed at learning probability distribution functions in different ways, discreetly or continuously or incidentally. I hope that these new priorities can be extended beyond the CNCI program to foster greater collaboration among engineers, computer scientists, and other disciplines in seeking consistent approaches to these tasks."}, {"heading": "3 Training of BLiSS Systems", "text": "There are two simple ways to try to move towards BLiSS systems. One is to start from an existing design, such as the particle swarm approach or Suyken's Fokker Planck machine, and modify it so that it depends on it. If the current design requires you to maintain a population of N choices of u [i] () in order to maintain N networks u [i] (), where i = 1 to N is, then with each iteration, instead of just u [i], trainu [i] () change, based on the same principles. There is clearly a lot of room for experimentation and intuition there.Another option is to use the new training methods (trainingu (, e, T), where e is a vector of random numbers and T is a temperature parameter) that I propose in Werbos (1998b). This has the interesting implication of requiring the brain to adapt to high-tension situations, for example, as the rapid situation of T is required by high-level situations."}, {"heading": "4 Structure of OptionNets", "text": "For example, with the TSP or VLSI design problem, one would want to build networks to input problem descriptors of varying lengths, but normal neural networks contain a fixed number of inputs and outputs! They do not have a rich structure to handle the full range of such problems - although they might be good enough for some preliminary reserves. Likewise, one would expect that an intelligent stochastic search would require the kind of iterative, relaxing approach that a hopfield network (or other recursive network) allows; ordinary feedback networks would likely have very limited capabilities in this regard. Therefore, for maximum performance, research in this area will need to shift relatively quickly to the use of more complex structures or networks. Examples of such networks are the cellular SRN of Pang and Werbos (1998) and the object network design described in the paper \"brain 3\" cited in Werbos (1998), and in slides presented in 1998, the cellular SRN are most familiar with these types of networks (such SAPN)."}], "references": [{"title": "Neural network design for J function approximation in dynamic programming, xxx.lanl.gov/abs/adap-org/9806001", "author": ["Pang", "Werbos"], "venue": null, "citeRegEx": "Pang and Werbos,? \\Q1998\\E", "shortCiteRegEx": "Pang and Werbos", "year": 1998}, {"title": "The Roots of Backpropagation, Wiley", "author": ["P. Werbos"], "venue": null, "citeRegEx": "Werbos,? \\Q1994\\E", "shortCiteRegEx": "Werbos", "year": 1994}], "referenceMentions": [{"referenceID": 1, "context": "In a long paper cited in Werbos (1999), I", "startOffset": 25, "endOffset": 39}, {"referenceID": 1, "context": "u(\uf061, e , T), where e is a vector of random numbers and T is a temperature parameter) which I propose in Werbos (1998b). This has the interesting implication that the parameter T needs to be adjusted over time, by the brain, as a function of circumstances.", "startOffset": 104, "endOffset": 119}, {"referenceID": 0, "context": "Examples of such networks are the cellular SRN of Pang and Werbos (1998) and the Object Net design described in the \u201c3 brain\u201d paper cited in Werbos (1998) and in slides presented At ISAP99.", "startOffset": 50, "endOffset": 73}, {"referenceID": 0, "context": "Examples of such networks are the cellular SRN of Pang and Werbos (1998) and the Object Net design described in the \u201c3 brain\u201d paper cited in Werbos (1998) and in slides presented At ISAP99.", "startOffset": 50, "endOffset": 155}], "year": 2010, "abstractText": "* The views expressed herein are those of the author, not the official views of NSF. Abstract \u2013 Brain-Like Stochastic Search (BLiSS) refers to this task: given a family of utility functions U(u,\uf061), where \uf061 is a vector of parameters or task descriptors, maximize or minimize U with respect to u, using networks (Option Nets) which input \uf061 and learn to generate good options u stochastically. This paper discusses why this is crucial to brain-like intelligence (an area funded by NSF) and to many applications, and discusses various possibilities for network design and training.", "creator": "Microsoft\u00ae Office Word 2007"}}}