{"id": "1006.0475", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2010", "title": "Prediction with Advice of Unknown Number of Experts", "abstract": "In the framework of prediction with expert advice, we consider a recently introduced kind of regret bounds: the bounds that depend on the effective instead of nominal number of experts. In contrast to the NormalHedge bound, which mainly depends on the effective number of experts and also weakly depends on the nominal one, we obtain a bound that does not contain the nominal number of experts at all. We use the defensive forecasting method and introduce an application of defensive forecasting to multivalued supermartingales.", "histories": [["v1", "Wed, 2 Jun 2010 19:41:27 GMT  (81kb,S)", "http://arxiv.org/abs/1006.0475v1", "22 pages; draft version"]], "COMMENTS": "22 pages; draft version", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alexey chernov", "vladimir vovk"], "accepted": false, "id": "1006.0475"}, "pdf": {"name": "1006.0475.pdf", "metadata": {"source": "CRF", "title": "Prediction with Advice of Unknown Number of Experts", "authors": ["Alexey Chernov"], "emails": ["chernov@cs.rhul.ac.uk", "vovk@cs.rhul.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 100 6.04 75v1 [cs.LG] 2 JAs part of the expert prediction, we consider a recently introduced type of repentance limit: the limits that depend on the effective number of experts rather than the nominal number. Unlike the normal hedge limit, which mainly depends on the effective number of experts and also weakly depends on the nominal number, we get a limit that does not include the nominal number of experts at all. We use the method of defensive prediction and introduce an application of defensive prediction to multi-value supermartingals."}, {"heading": "1 Introduction", "text": "We look at the problem of prediction with expert advice (PEA) and its variant, Decision Theory Online Learning (DTOL). Within the PEA framework (see [3] for details, references and historical notes), the difference between cumulative losses of Learner and one of the experts is the regret to that expert and must make his own decision. Then, the environment produces a result and a (real rated) loss is calculated for each decision as a known function of decision and outcome. The difference between cumulative losses of Learner and one of the experts is the regret to that expert. Learner aims to minimize his regret to experts, for each sequence of expert decisions and outcomes.In DTOL, introduced in [8], Learner's decision is a probability distribution over a finite series of actions. Then, each action entails a loss (the vector of losses can be considered the result), and Learner suffers from the loss of all actions (according to the probabilities of his decision)."}, {"heading": "2 Notation and Setup", "text": "The question that arises is whether there can be such a loss at all. (...) The question of whether there can be such a loss at all does not arise. (...) The question of whether there can be such a loss at all does not arise. (...) The question of whether there can be such a loss at all does not arise. (...) The question of whether there can be such a loss at all does not arise. (...) The question of whether there can be such a loss at all. (...) The question of whether there can be such a loss does not arise. (...) The question is not. (...) The question of whether there can be such a loss at all. (...) The question of whether there can be such a loss at all. (...) The question of whether there can be such a loss is not arise. (...) The question is not. (...) The question of whether there can be such a loss. (...) The question of whether there can be such a loss. (...) The question of whether there can be such a loss at all."}, {"heading": "3 Defensive Forecasting and Supermartingales", "text": "This section contains the technical results we need to construct our prediction algorithm, which are used in the evidence, but not in the theory statements and discussions in the next section."}, {"heading": "3.1 Defensive Forecasting", "text": "The general structure of the Defensive Forecasting Algorithm (DFA) is quite simple. In step t we define a function ft: (2), where ft \u2212 1 is the function defined in the previous step, and look for what the decision of the learner is in the previous step, and what the result is in the previous step. Then, this property is announced as the next decision of the learner. The choice of ft can depend on all previous decisions, results and on this step. Expert decisions (for PEA), i.e. ft = F (for PEA), i.e. ft = F (for Nn = 1, g = 1, g = 1, g = 1, g = 1, g = 1, g = 1). After specifying F, we call this strategy of the learner an application of the DFA to F."}, {"heading": "3.2 Multivalued Supermartingales", "text": "Let us define the space of all parameters on the basis of the weak definition as follows: For each measurable function g and any number of parameters g and any number of parameters p (1). Let us be an operator for each sequence e1, p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (1), p (2), p (1), p (1), p (2), p (2), p (2), p), p (2), p), p (2), p), p (2), p), p), p (2), p), p (2), p), p (2), p)."}, {"heading": "3.3 Levin\u2019s Lemma for Multivalued Supermartingales", "text": "The first variant (it is easier) will be used for PEA with finite results. The second variant will be used for DTOL.Lemma 2. Let X be a finite set of finite sets. Let X be a compact subset of RMB. Let q (es) \u00b7 X be a relationship. Denote q (es) is not empty and the set q (es) is convex. If for some real constant C it is true that for each individual group q is closed, then the set q (es) is not empty and the set q (es) is convex. If for some real constant C it is true that for each group P (it) applies that for each individual group g (it)."}, {"heading": "3.4 Hoeffding Supermartingale", "text": "Here we introduce a specific, multilingual supermartingale, or rather a family of supermartingales used for our main results. For technical reasons, our definition of supermartingale St consists of two parts: a function G: P: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p: p"}, {"heading": "4 Loss Bounds", "text": "Let's start with a simple theorem that shows a clean application of DFA theorem 7. If T is known in advance, then the DFA reaches the limit LT \u2264 min nLnT + \u221a 2T lnN (for DTOL with N actions and for PEA with N experts). Let's prove that T = 2 (lnN) / T andft (N-T) = N-N = 11N e\u03b7 (Lt \u2212 1 \u2212 L n-1) \u2212 \u03b72 / 2 \u00b7 e\u03b7 (\u03bb, \u03c9) \u2212 \u03bb (\u03b3nt) \u2212 2 / 2. (11) At each step t, the DFA determines that ft (\u03b3t, \u03c9) \u2212 n-T (T-1 \u2212 1) \u2212 T-T is best bound (T-1 \u2212 1). Such a limit exists due to Lemma 6 in combination with Lemma 3 Dft or TO11T (T) as best (T-1 \u2212 2) and we (T-2) as most strongly bound (T-1 \u2212 1)."}, {"heading": "4.1 Bounds on \u01eb-Quantile Regret", "text": "The next limit is uniform, that is, it applies to every T, and it applies to every T, and it applies to every T, and it applies to every T, and it applies to every T, and it applies to every T, and every T, and every T, and every T is such that at least a fraction of the acts have the loss after step T that is not greater than the loss after step T. Specifically, (12) implies for every T, and for every T, and every T, and every other T, a value that has at least a fraction of the acts. (12) implies for every T, and every T, and every T, and every T, and every T, and every T, and every T is a value. (12) implies for every T, and every T, and every T, and every T, and every T is a value."}, {"heading": "4.2 Discussion of the Bounds", "text": "It is unclear whether the existence of several good (or identical) experts can be exploited in this case. (19) The limits (14) and (17) of the coefficient in T lnN are the same, but the other terms are larger, and even the asymptotics are worse when N is fixed and T \u2192. However, it seems that the bound (19) terms cannot be transferred to the bound (19) terms. (19) The proof of Theorem 2.3 in [3] relies heavily on the loss of only one best expert, and it is unclear whether the existence of several good (or identical) experts can be exploited in this evidence."}, {"heading": "4.3 Internal Regret and Time Selection Functions", "text": "It has been shown in [5] and [7] that the loss limits received from the DFA should only be formalised (instead, all formal weight restrictions are transferred to other experts and sleeping experts); a second expert is a (known) function of the teacher's decision; informally, a second expert explains how learner could (hopefully) improve his performance; sleeping experts (or specialists) will be inactive in some steps by refraining from disclosing their decision (a specialist may decide that the current problem is outside their area of expertise); the learner's regret to a sleeping expert is counted through the steps when the expert is active. Models similar to experts and sleeping experts have been studied (or activated) in DTOL as internal (or wide range) functions of time selection (see [12] for a review); the internal regret compares Learner's loss not with the loss of a fixed action, but with the loss of a form."}, {"heading": "4.4 A Toy Example of a Multiobjective Bound", "text": "In this sub-section, we discuss the probability of less than half the rain, we are only allowed to forecast rain; we have shown how to deal with multiple miscible loss functions, where we combine a miscible loss function (square loss) with a non-miscible loss (absolute loss), let us describe an informal prediction function in which such a combination of loss functions can make sense. We want to predict the probability of rain (\"no rain\") and count the number of errors (the simple prediction game), we need to provide a pair, a probability and a Boolean prediction, and the two components of our prediction must match in the following sense: if we need to predict the probability of rain more than half; if we predict the probability of rain less than half, we need to predict rain."}, {"heading": "Acknowledgements", "text": "This work was supported by EPSRC, grant EP / F002998 / 1. We thank Yura Kalnishkan for the discussion."}, {"heading": "1244 (2008).", "text": "[12] Subhash Khot and Ashok Kumar Ponnuswami. Minimizing Wide Range Regret with Time Selection Functions. In: Rocco A. Servedio and Tong Zhang, editors, 21st Annual Conference on Learning Theory - COLT 2008, Helsinki, Finland, 9-12 July 2008, pp. 81-86, Omnipress, 2008. [13] V. Vovk. A game of prediction of expert advice. Journal of Computer and System Sciences, 56: 153-173, 1998."}], "references": [{"title": "Fixed Point Theory and Applications , volume 141 of Cambridge Tracts in Mathematics", "author": ["R. Agarwal", "M. Meehan", "D. O\u2019Regan"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "From External to Internal Regret", "author": ["Avrim Blum", "Yishay Mansour"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "A Parameter-free Hedging Algorithm", "author": ["Kamalika Chaudhuri", "Yoav Freund", "Daniel Hsu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Supermartingales in prediction with expert advice", "author": ["Alexey Chernov", "Yuri Kalnishkan", "Fedor Zhdanov", "Vladimir Vovk"], "venue": "Proceedings of the Nineteenth International Conference on Algorithmic Learning Theory,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Supermartingales in prediction with expert advice", "author": ["Alexey Chernov", "Yuri Kalnishkan", "Fedor Zhdanov", "Vladimir Vovk"], "venue": "Theoretical Computer Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Prediction with expert evaluators", "author": ["Alexey Chernov", "Vladimir Vovk"], "venue": "Proceedings of the 20th International Conference on Algorithmic Learning Theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Using and combining predictors that specialize", "author": ["Yoav Freund", "Robert E. Schapire", "Yoram Singer", "Manfred K. Warmuth"], "venue": "In Proceedings of the Twenty Ninth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "The weak aggregating algorithm and weak mixability", "author": ["Yuri Kalnishkan", "Michael V. Vyugin"], "venue": "Proc. COLT\u201905,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Minimizing Wide Range Regret with Time Selection Functions", "author": ["Subhash Khot", "Ashok Kumar Ponnuswami"], "venue": "21st Annual Conference on Learning Theory - COLT", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "A game of prediction of expert advice", "author": ["V. Vovk"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}], "referenceMentions": [{"referenceID": 2, "context": "In the PEA framework (see [3] for details, references and historical notes), at each step Learner gets decisions (also called predictions) of several Experts and must make his own decision.", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "In DTOL, introduced in [8], Learner\u2019s decision is a probability distribution on a finite set of actions.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "In this paper we deal with another kind of bound, recently introduced in [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 3, "context": "The following regret bound obtained in [4] for their NormalHedge algorithm holds for this case:", "startOffset": 39, "endOffset": 42}, {"referenceID": 3, "context": "Our bound has a simpler structure, but it is generally incomparable to the (precise) bound for Normal Hedge from [4] (see Subsection 4.", "startOffset": 113, "endOffset": 116}, {"referenceID": 10, "context": "Also our bound can be easily adapted to internal regret (see [12] for definition).", "startOffset": 61, "endOffset": 65}, {"referenceID": 4, "context": "In [5], the DF was used to obtain bounds of the form LT \u2264 cLT + a, where c and a are some constants.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "(This result is of certain independent interest: for example, it helps to get rid of additional Assumption 3 in Theorem 3 in [5].", "startOffset": 125, "endOffset": 128}, {"referenceID": 0, "context": "For any natural N , by \u2206N we denote the standard simplex in R N : \u2206N = {~ p \u2208 [0, 1] | \u2211N n=1 pn = 1}.", "startOffset": 78, "endOffset": 84}, {"referenceID": 0, "context": "Reality announces ~\u03c9t \u2208 [0, 1] .", "startOffset": 24, "endOffset": 30}, {"referenceID": 7, "context": "The decision-theoretic framework for online learning (DTOL) was introduced in [8].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "As defined in [4] (for DTOL), the regret to the top \u01eb-quantile (at step T ) is the value R T such that there are at least \u01ebN actions (the fraction at least \u01eb of all Experts) with R T \u2265 R T .", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": ", pK \u2208 [0, 1] , \u2211K k=1 pk = 1, there exists g \u2208 \u039b such that g(\u03c9) \u2264 \u2211Kk=1 pkgk(\u03c9) for all \u03c9 \u2208 \u03a9.", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "For such games, we assume without loss of generality that \u039b \u2286 [0, 1] (we always can scale the loss function).", "startOffset": 62, "endOffset": 68}, {"referenceID": 0, "context": "For DTOL as a special case of PEA, the outcome space is \u03a9 = [0, 1] , the decision space is \u0393 = \u2206N , and the loss function is \u03bb(~\u03b3, ~\u03c9) = ~\u03b3 \u00b7 ~ \u03c9.", "startOffset": 60, "endOffset": 66}, {"referenceID": 11, "context": "In particular, we can obtain PEA bounds that hold for specific loss functions or classes of loss functions (such as mixable loss functions [13]), and these bounds may be much stronger than the general bounds induced by DTOL.", "startOffset": 139, "endOffset": 143}, {"referenceID": 4, "context": "(3) This definition of supermartingale is equivalent to the one given in [5].", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "Let \u03a9 be [0, 1] .", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": "Since [0, 1] is a compact metric space, the space P([0, 1]) with weak topology is compact too (see, e.", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "Since [0, 1] is a compact metric space, the space P([0, 1]) with weak topology is compact too (see, e.", "startOffset": 52, "endOffset": 58}, {"referenceID": 0, "context": "Assume that g1, g2 \u2208 G\u039b(\u03c0) \u2286 \u039b and \u03b1 \u2208 [0, 1].", "startOffset": 39, "endOffset": 45}, {"referenceID": 0, "context": "For DTOL, the set \u039b = {g \u2208 R[0,1]N | \u2203~ p \u2208 \u2206N\u2200~\u03c9 \u2208 [0, 1] g(\u03c9) = ~ p \u00b7 ~\u03c9} is obviously non-empty and it is compact and convex as a linear image of simplex \u2206N .", "startOffset": 28, "endOffset": 33}, {"referenceID": 0, "context": "For DTOL, the set \u039b = {g \u2208 R[0,1]N | \u2203~ p \u2208 \u2206N\u2200~\u03c9 \u2208 [0, 1] g(\u03c9) = ~ p \u00b7 ~\u03c9} is obviously non-empty and it is compact and convex as a linear image of simplex \u2206N .", "startOffset": 52, "endOffset": 58}, {"referenceID": 0, "context": "Let (\u03a9,\u0393, \u03bb) be a game, the range of \u03bb is included in [0, 1] and G(\u03c0) is defined by (8).", "startOffset": 54, "endOffset": 60}, {"referenceID": 2, "context": "This bound is twice as large as the best bound obtained in [3] (see their Theorems 2.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "2 in [3].", "startOffset": 5, "endOffset": 8}, {"referenceID": 8, "context": "Probably, the first bound for \u01eb-quantile regret was stated (implicitly) in [9].", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "More precisely, that paper considered even more general regret notion: Theorem 1 in [9] gives a bound for PEA with weighed experts under the logarithmic loss of the form", "startOffset": 84, "endOffset": 87}, {"referenceID": 2, "context": "3 in [3] heavily relies on tracking the loss of only one best Expert, and it is unclear whether the existence of several good (or identical) Experts can be exploited in this proof.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "reported in [4] show that algorithms with good best Expert bounds may have rather bad performance when the nominal number of Experts is much greater than the effective number of Experts.", "startOffset": 12, "endOffset": 15}, {"referenceID": 9, "context": "3], see [11].", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "For the Aggregating Algorithm [13] (which can be considered as a special case of the DFA for a certain supermartingale, as shown in [5]), the bound can be trivially adapted to \u01eb-quantile regret: LT \u2264 cLT + a ,", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "For the Aggregating Algorithm [13] (which can be considered as a special case of the DFA for a certain supermartingale, as shown in [5]), the bound can be trivially adapted to \u01eb-quantile regret: LT \u2264 cLT + a ,", "startOffset": 132, "endOffset": 135}, {"referenceID": 3, "context": "The authors of [4] posed an open question whether similar bounds can be obtained if the (effective) number of actions is not known.", "startOffset": 15, "endOffset": 18}, {"referenceID": 4, "context": "3 Internal Regret and Time Selection Functions It was shown in [5] and in [7] that the loss bounds obtained by the DFA can be easily transferred to second-guessing experts and sleeping experts models.", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "3 Internal Regret and Time Selection Functions It was shown in [5] and in [7] that the loss bounds obtained by the DFA can be easily transferred to second-guessing experts and sleeping experts models.", "startOffset": 74, "endOffset": 77}, {"referenceID": 8, "context": "Sleeping experts (or specialists) introduced in [9] may be inactive at some steps, abstaining from announcing their decision (a specialist may decide that the current problem is outside her expertize area).", "startOffset": 48, "endOffset": 51}, {"referenceID": 10, "context": "The models similar to second-guessing experts and sleeping experts were studied in DTOL as internal (or wide range) regret and time selection (or activation) functions respectively (see [12] for a review).", "startOffset": 186, "endOffset": 190}, {"referenceID": 0, "context": "A time selection function attached to a modification rule assigns a scaling factor from [0, 1] to each step.", "startOffset": 88, "endOffset": 94}, {"referenceID": 10, "context": "As has been recently shown [12], an algorithm achieving in DTOL with N action some regret bound with respect to N can be transformed into an algorithm that achieves the same bound with respect to K for K modification rules with attached time selection functions.", "startOffset": 27, "endOffset": 31}, {"referenceID": 10, "context": "We do not apply the general method of [12], but directly modify our supermartingales and proofs.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "The (one-step) regret of Learner\u2019s decision ~\u03b3 \u2208 \u2206N to the modification rule M on the outcome ~\u03c9 \u2208 [0, 1] is ~\u03b3 \u00b7~\u03c9\u2212 (M~\u03b3) \u00b7~\u03c9, where M~\u03b3 is the product of matrix M and vectorcolumn ~\u03b3.", "startOffset": 99, "endOffset": 105}, {"referenceID": 10, "context": "RH,I,f in [12]).", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "Khot and Ponnuswami [12] do not discuss such rules explicitly, but it appears that their method works for them as well (unless we miss some subtlety in the proof).", "startOffset": 20, "endOffset": 24}, {"referenceID": 10, "context": "It is very probable that the method of [12] also transforms a bound in terms of the effective number of actions into a bound in terms of the effective number of modification rules, but we did not check.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "In DTOL with N actions, let us have K modifications rules Mk(t), each assigning a stochastic N \u00d7N matrix to each step t, with attached time selection functions Ik(t) assigning a number from [0, 1].", "startOffset": 190, "endOffset": 196}, {"referenceID": 0, "context": "In DTOL with N actions, let us have K modifications rules Mk(t), each assigning a stochastic N \u00d7N matrix to each step t, with attached time selection functions Ik(t) assigning a number from [0, 1].", "startOffset": 190, "endOffset": 196}, {"referenceID": 0, "context": "To get the loss bound we observe that \u2211T t=1(Ik(t)) 2 \u2264 Tk(T ) since Ik(t) \u2208 [0, 1].", "startOffset": 77, "endOffset": 83}, {"referenceID": 6, "context": "In [7], we showed how to cope with several mixable loss functions.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "Assume that we are given K Experts that give predictions p \u2208 [0, 1] and M Experts that give predictions b \u2208 {0, 1}.", "startOffset": 61, "endOffset": 67}, {"referenceID": 0, "context": "Learner is allowed to give predictions (p, p\u0303) \u2208 [0, 1]\u00d7 [0, 1], with the following restriction: if p < 1/2 then p\u0303 = 0 and if p > 1/2 then p\u0303 = 1.", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "Learner is allowed to give predictions (p, p\u0303) \u2208 [0, 1]\u00d7 [0, 1], with the following restriction: if p < 1/2 then p\u0303 = 0 and if p > 1/2 then p\u0303 = 1.", "startOffset": 57, "endOffset": 63}, {"referenceID": 0, "context": "Let A = {(p, p\u0303) \u2208 [0, 1] | p\u0303 = 0 if p < 1/2 and p\u0303 = 1 if p > 1/2 and } = {(p, 0) | p \u2208 [0, 1/2)} \u222a {(1/2, p\u0303) | p\u0303 \u2208 [0, 1]} \u222a {(p, 1) | p \u2208 (1/2, 1]}.", "startOffset": 19, "endOffset": 25}, {"referenceID": 0, "context": "Let A = {(p, p\u0303) \u2208 [0, 1] | p\u0303 = 0 if p < 1/2 and p\u0303 = 1 if p > 1/2 and } = {(p, 0) | p \u2208 [0, 1/2)} \u222a {(1/2, p\u0303) | p\u0303 \u2208 [0, 1]} \u222a {(p, 1) | p \u2208 (1/2, 1]}.", "startOffset": 120, "endOffset": 126}, {"referenceID": 1, "context": "Consider x \u2208 [0, 2] and two functions", "startOffset": 13, "endOffset": 19}], "year": 2013, "abstractText": "In the framework of prediction with expert advice, we consider a recently introduced kind of regret bounds: the bounds that depend on the effective instead of nominal number of experts. In contrast to the NormalHedge bound, which mainly depends on the effective number of experts and also weakly depends on the nominal one, we obtain a bound that does not contain the nominal number of experts at all. We use the defensive forecasting method and introduce an application of defensive forecasting to multivalued supermartingales.", "creator": "LaTeX with hyperref package"}}}