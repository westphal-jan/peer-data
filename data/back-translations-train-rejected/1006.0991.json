{"id": "1006.0991", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jun-2010", "title": "Variational Program Inference", "abstract": "We introduce a framework for representing a variety of interesting problems as inference over the execution of probabilistic model programs. We represent a \"solution\" to such a problem as a guide program which runs alongside the model program and influences the model program's random choices, leading the model program to sample from a different distribution than from its priors. Ideally the guide program influences the model program to sample from the posteriors given the evidence. We show how the KL- divergence between the true posterior distribution and the distribution induced by the guided model program can be efficiently estimated (up to an additive constant) by sampling multiple executions of the guided model program. In addition, we show how to use the guide program as a proposal distribution in importance sampling to statistically prove lower bounds on the probability of the evidence and on the probability of a hypothesis and the evidence. We can use the quotient of these two bounds as an estimate of the conditional probability of the hypothesis given the evidence. We thus turn the inference problem into a heuristic search for better guide programs.", "histories": [["v1", "Fri, 4 Jun 2010 20:55:04 GMT  (192kb)", "http://arxiv.org/abs/1006.0991v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["georges harik", "noam shazeer"], "accepted": false, "id": "1006.0991"}, "pdf": {"name": "1006.0991.pdf", "metadata": {"source": "CRF", "title": "Variational Program Inference", "authors": ["Georges Harik", "Noam Shazeer", "Harik Shazeer"], "emails": ["georges@gmail.com", "noam@google.com"], "sections": [{"heading": "Problem Specification", "text": "Given the partial observation of a complicated system governed by known or unknown probabilistic rules, we would automatically like to think about the probable state of hidden parts of the system."}, {"heading": "Systems with Known Rules", "text": "We model our system as a program in a general programming language. We call this the model program. We will not know what programming language we are using. We will only insist that it is deterministic, except for a Choose function that takes a probability distribution as argument and derives a random selection from it. Therefore, the model program defines a probability distribution P (x) over the execution path x. If, in the course of an execution path x of the model program, the select function is n times equal with distributions (P1, P2... Pn) and the randomly chosen values (c1, c2.. cn), then the probability of this execution path is isP (x) = i Pi (ci). We are interested in the conditional expectation value E (h) | e), where h is a function of the execution path, and e is proof that we can easily calculate the x."}, {"heading": "Modeling Systems with Unknown Rules", "text": "A system with unknown rules is indeed a special case with known rules, if we generate the unknown rules randomly by known rules. In particular, we include a random program generator in our model program. It turns out that our arbitrary choice of a random program generator is not critical, because there is a better random program generator that has a limited chance of generating it, and so we underestimate the probability that it will generate any program."}, {"heading": "Inference", "text": "By \"conclusion,\" we mean that we want to calculate the expected value E (h (x) | e) of any hypothesis h, which is a function of the execution path x of the model, given the evidence e. (If we want the probability of a binary predicate, we can present this as the expected value of the 0 / 1 indicator function for that predicate. (1) E (h (x) | e) = xP (x | e) h (x) x (x) x (x) P (e) h (x) / P (e) = entrepxP (e | x) h (x) / \u0441xP (x) P (e | x) In general, this is completely impracticable and requires a list of all execution paths of the model. As mentioned above, for a particular execution path x, P (x) and P (e | x) periths are easy to calculate."}, {"heading": "Inference by sampling", "text": "It would be tremendously useful if we could quickly perform a sample from the rear PX | e. If we could, we could estimate E (h (x) | e) by determining an average of h (x) over many execution paths drawn from the rear ones."}, {"heading": "Exact Sampling by Rejection", "text": "We can take slowly from the back samples by repelling them - samples from the precursors PX by running the model program and discarding a sample x with probability (1-P (e | x)). This is generally too slow, since the expected number of attempts to produce a sample increases with the reverse probability of evidence and the evidence for the most interesting problems is most likely highly improbable. Therefore, we will focus on samples from the back samples, for example. Single Point \"Sampling\" (Maximum Likelihood)"}, {"heading": "Search)", "text": "One way to try out the back numbers, for example, is to look heuristically for the most likely single execution path that we can find in the light of the evidence and return it each time. Some advantages of this technique are: 1. It is simple 2. It can provide incremental results 3. The results are measurable 4. They can be easily paralleled between computers and across search heuristics 5. The results can be good for probability distributions that focus on a single execution path. The disadvantage of such a maximum probability search is that it performs poorly when the back distribution is distributed. In our example of the three dice observed to add to 7, for example, there are many possible execution paths, all with identical rear probabilities. This method would select only one and be sure of them. Furthermore, this method discriminates execution paths that cause more random numbers to be thrown, even if the results of these numbers are not used (Sampling program)."}, {"heading": "Inference)", "text": "Our strategy for the sample will be to run the model program but to distort the probability distribution in each of its decisions so that the biased model program instead draws from a distribution approaching the rear PX | e. We do this by means of a leadership program G that runs alongside the model program and influences its random decisions. Each time the main program calls the election command with a previous probability distribution PC, calculates and replaces the leadership program in its own distribution GC, and selection c is instead chosen by GC. If we run the model program in parallel with the leadership program, it will find samples from a leadership-influenced distribution GX over execution paths. Our goal is to find a leadership program G in such a way that GX approaches PX | e. For example, this is Example 1 from above with a good leadership program displayed inline in square brackets < >: 1: = the 1: = 7 (1.. 6, < < 1: 3; 5: 3: 3; 5: 3: 3; 5: 3: 3: 3)."}, {"heading": "Evaluating a Guide Program by Free Energy", "text": "The measurement of similarity that we use is the Kulllbeck-Leibler divergence between GX and PX | e, which is defined as DKL (GX | PX | e), is the Kulllbeck-Leibler divergence between GX and PX | e, which is defined as DKL (GX | PX | e). KL divergence measures how quickly a cautious observer can accumulate evidence that you are starting from GX rather than PX | e. Smaller values of KL divergence indicate more similar distributions. By an amazing trick, KL divergence becomes tractable if we can add the constant term of energy (P (e)). Thus, we define the free energy of G as: F (G, P, e): Variative values of KL divergence indicate other similar distributions."}, {"heading": "Other Utility Metrics for Guide Programs", "text": "In addition to favoring leadership programs with less free energy, we might favor leadership programs that make life easier in other ways - primarily by costing us less computing resources from which to take samples. For example, a leadership program that reproduces only the precursors of the model program and then repels with probability (1-P (e | x)) at the end has optimal free energy, but is very expensive to sample. So we define a utility using leadership programs U (G) = F (G, p, e) + k * (average time to successfully extract a sample from G) + other terms and optimize for that volatility. Here, k is a constant that represents our level of impatience."}, {"heading": "Pros and Cons of Guided Sampling", "text": "The two methods are identical if we limit our search for guides to \"deterministic\" guides, which always provide one-point distributions for each election. As such, Guided Sample shares some of the advantages of searching for maximum probabilities. Specifically, 1. It can provide incremental results 2. The results are measurable. 3. It can be easily paralleled across computers and across search heuristics. On the negative side: 1. Guided Samples are more complicated than searching for maximum probabilities. 2. They require repeated sampling to produce and verify results."}, {"heading": "Importance Sampling", "text": "Suppose we try to calculate the conditional probability of a hypothesis h (x) in the light of the proofs e (instead of just trying PX | e per se). Instead of a 0 > 1 Boolean, we can make h (x) be any positive function and we will try to determine its conditional expectation value. Let's remember the equation 1: (1) E (x) | e) = x P (x) = x (x) P (x). These sums can be very time consuming to calculate exactly (x). This is the quotient of two sums of the form F = x (x) f (x), where f (x) is an easily calculable, non-negative function. These sums can be very time consuming to calculate accurately."}, {"heading": "Extra Guide Choices", "text": "It may sometimes be easier for the guide to specify additional decisions that are not made by the model program. Let's say our model is that a monkey enters a string of a million random characters, and the proof is that the text of this paper occurs somewhere in the string. A good guide would first select a position in the string for this paper to occur. It would then direct the decisions of these characters to contain the characters of this paper and leave the rest randomly. The system described so far prohibits us from counting the probability of choosing the starting position in the calculation G (x). This is because we have no evidence that different values of the additional decisions made by the guide lead to different ways of executing the model program. Thus, we could end up underestimating G (x) and consequently F. To solve this problem, we introduce the idea of model extensions. Before making an additional choice y, the guide program extends the model from a distribution over x to one over x."}, {"heading": "Acknowledgments", "text": "Thanks to Jeremy Bem for introducing the program induction, Sergey Pankov for rehearsing the importance and Noah Goodman, Vikash Mansinghka, Daniel Roy for a discussion about additional possibilities."}], "references": [{"title": "On distribution-free lower confidence limits for the mean of a nonnegative random variable", "author": ["M. Breth", "J. Maritz", "E. Williams"], "venue": null, "citeRegEx": "Breth et al\\.,? \\Q1978\\E", "shortCiteRegEx": "Breth et al\\.", "year": 1978}, {"title": "An Introduction to Variational Methods for Graphical Models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "In Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}], "referenceMentions": [], "year": 2008, "abstractText": "We introduce a framework for representing a variety of interesting problems as inference over the execution of probabilistic model programs. We represent a \u201csolution\u201d to such a problem as a guide program which runs alongside the model program and influences the model program's random choices, leading the model program to sample from a different distribution than from its priors. Ideally the guide program influences the model program to sample from the posteriors given the evidence. We show how the KLdivergence between the true posterior distribution and the distribution induced by the guided model program can be efficiently estimated (up to an additive constant) by sampling multiple executions of the guided model program. In addition, we show how to use the guide program as a proposal distribution in importance sampling to statistically prove lower bounds on the probability of the evidence and on the probability of a hypothesis and the evidence. We can use the quotient of these two bounds as an estimate of the conditional probability of the hypothesis given the evidence. We thus turn the inference problem into a heuristic search for better guide programs. Problem Specification Given partial observations of a complicated system governed by known or unknown probabilistic rules, we would like to automatically reason about the likely state of hidden parts of the system. Systems with Known Rules We model our system as a program in a general purpose programming language. We call this the model program. We will be agnostic to what programming language we are using. We will only insist that it be deterministic except for a choose function which takes a probability distribution as an argument and returns a random choice from it. The model program thus defines a probability distribution P(x) over execution paths x. If over the course of an execution path x of the model program, the choose function is called n times with distributions (P1, P2 ... Pn) and the randomly chosen values are (c1, c2 .. cn) respectively, then the probability of that execution path is P(x) = \u220fi Pi(ci). We are interested in the conditional expected value E(h(x)|e), where h is some function of the execution path, and e is some evidence such that we can easily compute P(e|x) for any x. Our programming language needs to include constructs for specifying P(e|x) and h(x). The model program reports P(e|x) as the product of all calls to a function: evidence. This is a particularly convenient in that the evidence function can be passed boolean values which are interpreted as 0 or 1. For example, we could represent an observation that the grass is wet with the call evidence(grass_wet). If grass_wet is false, P(e|x) is multiplied by 0, and if grass_wet is false, P(e|x) is left unchanged. The value of the hypothesis h(x) is defined as the final value of a global variable *h*. We may not have a hypothesis, and may only be interested in sampling runs of the program given the evidence. In this case, *h* might not be set or used. Example 1: Three fair dice are rolled and it is observed that their sum is 7. What is the probability that the first die rolled was a 5? The following model program (shown in pseudocode) might encode that problem. die1 := choose(uniform(1..6)) die2 := choose(uniform(1..6)) die3 := choose(uniform(1..6))", "creator": "Microsoft Word"}}}