{"id": "1006.1288", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2010", "title": "Regression on fixed-rank positive semidefinite matrices: a Riemannian approach", "abstract": "The paper addresses the problem of learning a regression model parameterized by a fixed-rank positive semidefinite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of fixed-rank positive semidefinite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semidefinite matrix. Good performance is observed on classical benchmarks.", "histories": [["v1", "Mon, 7 Jun 2010 16:20:02 GMT  (173kb,D)", "http://arxiv.org/abs/1006.1288v1", null], ["v2", "Mon, 31 Jan 2011 09:59:44 GMT  (361kb)", "http://arxiv.org/abs/1006.1288v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gilles meyer", "silvere bonnabel", "rodolphe sepulchre"], "accepted": false, "id": "1006.1288"}, "pdf": {"name": "1006.1288.pdf", "metadata": {"source": "CRF", "title": "Regression on fixed-rank positive semidefinite matrices: a Riemannian approach", "authors": ["Gilles Meyer", "Rodolphe Sepulchre"], "emails": ["g.meyer@ulg.ac.be", "silvere.bonnabel@mines-paristech.fr", "r.sepulchre@ulg.ac.be"], "sections": [{"heading": null, "text": "Keywords: linear regression, positive semidefined matrices, low-level approximation, Riemann geometry, gradient-based learning"}, {"heading": "1. Introduction", "text": "A fundamental problem of machine learning is the learning of a distance between data examples. If the distance can be written as a quadratic form (either in data space (Mahalanobis) or in a core functional space (kernel distance), the learning problem is indeed a regression problem on the set of positively defined matrices. The regression problem is converted to the minimization of the non-linear search space, leading to an optimization framework and gradient-based algorithms. The present paper focuses on the nonlinear nature of gradient-based learning, provided that the nonlinear search space is equipped with an appropriate belt-like geometry. Adapting this general framework to Xiv: 100 6.12 88v1 [cs].Lwe design novel learning algorithms on the space of fixed rank positive semidefinite matrices, denked by S + (r, d), where d) the dimensions of learning are in the matrix and in the matrix and in the matrix."}, {"heading": "2. Linear regression on Riemannian spaces", "text": "We look at the following standard regression pairs. (i) We look at the following standard regression problems. (i) We look at the following standard regression pairs. (i) We look at the following (i) data points X = X = X (Rd).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (.W).W (.W).W (I).W (.W).W (I).W (.W).W (.W).W (I).W (.W).W (I).W (I).W (.W).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (I).W (.W).W (I (I).W (I).W (.W).W (I (I).W (.W).W (I (I).W).W (I (I).W (.W).W (I (.W).W (I (.W).W).W (I (.W).W (.W).W (I (.W).W (.W).W (I (.W).W).W (I (.W).W).W (I (.W).W (I (.W).W).W (I (.W).W).W (.W).W (I (I (.W).W).W).W (I (.W).W (I (.W).W).W"}, {"heading": "3. Linear regression on the Grassmann manifold", "text": "As a preliminary step to Section 5, online learning in subspace (Oja, 1992; Crammer, 2006; Warmuth, 2007) is presented in the current structure. Leave X = Y = Rd, and consider the linear modal fundamentals in Rd. The square loss is thenf (U) = \"(y, x) =\" U-Rr \u00b7 d s.t. (UTU = I), the square loss is thenf (U) = \"(y, x) =\" y \"y-y\" -x-x-x. \"(UTx, x, x, x, x, x, x, x). (5) Because the cost (5) is invariant by orthogonal transformation U 7 \u2192 UO (r), where O (r) = St (r, r) y, is the orthogonal group, the search space is indeed a set of equivalent classes [U] s.t."}, {"heading": "4. Linear regression on the cone of positive definite matrices", "text": "G learning a full, definitive matrix is as follows: \"A,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G,\" G \"G,\" G, \"G,\" G, \"G\" G, \"G,\" G, \"G,\" G \"G,\" G, \"G\" G, \"G,\" G, \"G\" G, \"G,\" G, \"G,\" G \"G,\" G \"G,\" G \"G,\" G, \"G,\" G, \"G\" G \"G,\" G \"G,\" G, \"G\" G, \"G,\" G, \"G,\" G, \"G,\" G, \"G\" G, G, \"G, G, G,\" G, \"G,\" G, \"G\" G, \"G,\" G \"G, G, G, G,\" G, \"G,\" G, G, \"G,\" G, \"G,\" G, G, G, \"G,\" G, \"G,\" G, G, G, G, G, \"G,\" G, G, G, G"}, {"heading": "5. Linear regression on fixed-rank positive semidefinite matrices", "text": "We now present the proposed generalizations on fixed positive semidefined matrices."}, {"heading": "5.1 Linear regression with a flat geometry", "text": "The generalization of the results of section 4.1 to the set S + (r, d) is a direct consequence of the factorization W = GGT, G-Rd-r *, where Rd-r * = {G-Rd-r-s.t. rank (G) = r}. The flat quotient geometry of S + (d) 'GL (d) / O (d) is generalized by simply adapting the matrix dimension to the quotient geometry of S + (r, d)' Rd-r-E / O (r), leading to the updates (10) and (11) for matrices Gt-Rd-R. The quotient geometry of S + (r, d) 'Rd-r-E / O (r) is investigated by Journe-e et al. (2010)."}, {"heading": "5.2 Linear regression with a polar geometry", "text": "In contrast, the invariant geometry of S + (d) GL (d) GL (d) GL (d) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL (n) GL) GL (n) GL (n) GL (n) GL (n) GL (n) GL) GL (n) GL (n) GL (n) GL (n) GL (n) GL) GL (n) GL (n) GL (n) GL (n) GL (n) Gn) GL (n) GL (n) Gl (n) Gl (n) Gl (n) Gn) Gl (n) Gl (n) Gn) Gl (n) Gn) Gl (n) Gl (n) Gl (n) Gn) Gl (n) Gn (n) Gn) Gl (n) Gn (n) Gn) Gl (n) Gn) G"}, {"heading": "6. Algorithms", "text": "This section documents the implementation details of the proposed algorithms. Generic pseudocodes are provided in Figure 1 and Table 1 summarizes the computational complexity."}, {"heading": "6.1 From subspace learning to distance learning", "text": "The updated expressions (22) and (21) show that \u03bb, the tuning parameter of the belt metric (18), acts as a weighting factor on the search direction. A correct adjustment of this parameter allows more emphasis to be placed either on learning the sub-space U or on the distance in this sub-space R2. In the case \u03bb = 1, the algorithm only performs sub-space learning. Conversely, in the case \u03bb = 0, the algorithm learns a distance for a fixed sub-space (see section 7.1). Intermediate values of \u03bb interpolate continuously between the sub-space learning problem and the remote learning problem at a fixed range space.An appropriate adjustment of \u03bb is of interest if a good estimate of the sub-space is available (e.g. a sub-space specified by a suitable dimension reduction technique) or if too few observations are available to jointly estimate the sub-space and the distance within this sub-space."}, {"heading": "6.2 Invariance properties", "text": "A nice feature of the proposed algorithms is their invariance in relation to rotations W 7 \u2192 OTWO, EO-O-O (d). This invariance stems from the fact that the chosen metrics are invariant to rotations. A practical consequence is that a rotation of the input matrix X 7 \u2192 OXOT (for example, a whitening of the vectors x 7 \u2192 Ox if X = xxT) does not affect the behaviour of the algorithms. Apart from the fact that the algorithms (21) and (22) are invariant to rotations, they are invariant to the scales W 7 \u2192 \u00b52W with \u00b52-R +. Consequently, a scaling of the input matrix X 7 \u2192 \u00b5X with \u00b5-R will not affect the behaviour of these algorithms."}, {"heading": "6.3 Mini-batch extension of online algorithms", "text": "This is a classic acceleration and stabilization euristics for stochastic gradient algorithms. In the special case p = 1, a simple stochastic gradient descent is restored. Considering the p samples (Xt, 1, yt, 1),..., (Xt, p, yt, p) that arrive at the time t, the abstract update (3) Wt + 1 = RWt (\u2212 st 1p p \u00b2 i = 1 DegW '(y \u0441i, yi)) results in a mini-batch extension of stochastic gradient algorithms."}, {"heading": "6.4 Strategies for choosing the step size", "text": "Here we present strategies for choosing the step size both in the batch and in online cases."}, {"heading": "6.4.1 Batch algorithms", "text": "For batch algorithms, classical traceability methods exist (see Nocedal and Wright, 2006). In this thesis, we use the Armijo step sA, which is defined with each iteration by the conditionf (RWt (\u2212 sA gradf (Wt))) \u2264 f (Wt) + c-Gradf (Wt) and 2Wt (23), where Wt-S + (r, d) is the current iterate, c-S (0, 1), f the empirical costs (2) and RW is the selected retract. In this thesis, we select the specific value c = 0.5 and repeatedly divide it by 2 a certain maximum step size smax until the condition (23) for the considered iteration is met. To reduce the dependence on smax in a particular problem, it is chosen inversely proportional to the norm of the gradient for each iteration, smax = s0-Gradf (Wt)."}, {"heading": "6.4.2 Online algorithms", "text": "For online algorithms, the choice of step size plays a greater role. In this thesis, the step counting plan st is selected as follows: assen = s \u00b5-degree \u00b7 nt0 nt0 + t, (24) where s > 0, n is the number of samples viewed, \u00b5-degree is an estimate of the average gradient standard \u0433gradf (Wt) \u0445 Wt and t0 > 0 controls the gl\u00fchrate of st. During a pre-training phase of our online algorithms, we select a small subset of samples and test the values 2k with k = \u2212 3,..., 3 for s and t0. Values of s and t0, which provide the best decay of the cost function, are selected to process the complete set of samples."}, {"heading": "6.5 Stopping criterion", "text": "Batch algorithms are stopped if the value or relative change in empirical costs f is small enough, or if the relative change in parameter variation is small enough, f (Wt + 1) \u2264 tol, or f (Wt + 1) \u2212 f (Wt) f (Wt) \u2264 tol, or \u0435Gt + 1 \u2212 Gt \u00b2 F \u00b2 Gt \u00b2 F \u2264 tol. (25) We have found that tol = 10 \u2212 5 is a good trade-off between accuracy and convergence time. Online algorithms are executed for a fixed number of epochs (number of passes through the study sample). Typically, a few epochs are sufficient to achieve satisfactory results."}, {"heading": "6.6 Convergence", "text": "Gradient parentage algorithms on matrix manifolds divide the well-characterized convergence properties of their analogues into Rd. Batch algorithms converge linearly to a local minimum of empirical costs depending on the initial condition. Online algorithms converge asymptotically to a local minimum of expected loss. They have intrinsically a much slower convergence rate than batch algorithms, but they generally reduce the expected loss on a large scale faster (Bottou and Bousquet, 2007). However, the main idea is that an inaccurate solution may actually have the same or lower expected cost than a well-optimized on.When learning a matrix W-S + (d), the problem is convex and the proposed algorithms converge to a global minimum of cost function, regardless of the initial condition."}, {"heading": "7. Discussion", "text": "This section presents links to existing work and extensions of the regression model."}, {"heading": "7.1 Closeness-based approaches", "text": "A standardized derivation of the learning algorithms is as follows (Kivinen and Warmuth, 1997): (1) (1) (1) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2)) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2 (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) (2) ("}, {"heading": "7.2 Handling inequalities", "text": "This corresponds to the minimization of the continuously differentiable cost function (W) = \"(y, y) = 12 max (0, \u03c1 (y, y) 2, where \u03c1 = + 1 is required if y, \u2264 y is required, and \u03c1 = \u2212 1 if y, \u0438 y is required."}, {"heading": "7.3 Kernelizing the regression model", "text": "In this paper, we did not consider the nuclearized modular model \u03c6 = Tr (W\u03c6 (x) \u03c6 (x) T), the predictions of which can be extended to new input data \u03c6 (x) in character space F, which can be analyzed by the nonlinear mapping \u03c6: x-X 7 \u2192 \u03c6-x-F. This is potentially a useful extension of the regression model, which could be investigated in the light of recent theoretical results in this area (e.g. Chatpatanasiri et al., 2008)."}, {"heading": "7.4 Connection with multidimensional scaling algorithms", "text": "Given a set of m-scale dissimilarity scales D = {\u03b4ij} m between n data objects, multidimensional scaling algorithms search for an r-dimensional embedding of data objects in a Euclidean spatial representation G-Rn \u00b7 r (Cox and Cox, 2001; Borg and Groenen, 2005). Each G-line is the coordinate of a data object in a Euclidean dimension space r.Multidimensional scaling algorithms based on gradient descent are equivalent to algorithms (11) and (10) if X = (ei \u2212 ej) (ei \u2212 ej) T, where ei \u2212 ej is the unit vector (see Section 8.1), and if the multidimensional scaling reduction criterion is the SSTRESSSSTRESS (G) = (i, j) = (i \u2212 ej) (ei \u2212 ej) T, where ei \u2212 ej is the minimum scale \u2212 if the SALIX is the unit (see Section 8.1 and Si)."}, {"heading": "8. Applications", "text": "Choosing a suitable distance measure is a central issue for many distance-based classification and cluster algorithms, such as the closest neighbouring classifiers, support vector machines or k averages. As this choice is highly problem-dependent, numerous methods have been proposed to learn a distance function directly from data. In this section, we present two important distance learning applications that are compatible with the regression model under consideration and review some relevant literature on the subject."}, {"heading": "8.1 Kernel learning", "text": "In the kernel-based methodology (Shawe-Taylor and Christianini, 2004), the data samples are x1,... xn are first transformed by a nonlinear mapping of data. (The core function is then defined as the dot product between two samples in F. (xi, xj) \u00b7 In practice, the core function is represented by a positive semidefined matrix K. (The core function is then defined as the dot product between two samples in F.) \u00b7 This internal product information is used exclusively to calculate the relevant quantities based on the kernel. (For example, a distance is implicitly defined by a core function.) The core function is defined by any kernel function. (The entries are defined as the separation.)"}, {"heading": "8.2 Mahalanobis distance learning", "text": "In recent years, the number of those who are able to soar has multiplied. (...) In recent years, the number of those who are able to soar has doubled. (...) In recent years, the number of those who are able to soar, to soar. (...) In recent years, the number of those who are able to soar has multiplied. (...) In the last ten years, the number of those who are able to soar, to soar. (...) In the last ten years, the number of those who are able to soar has multiplied. (...) In the last ten years, the number of those who are able to soar has multiplied. (...) In the last ten years, the number of those who are able to soar has multiplied."}, {"heading": "9. Experiments", "text": "In this section, we illustrate the potential of the proposed algorithms using several benchmark experiments. First, the proposed algorithms are evaluated using toy data, and then compared with state-of-the-art kernel learning and Mahalanobis distance learning algorithms on real data sets. Overall, the experiments support that a common estimate of a sub-space and low dimensional distance in this sub-space is a significant advantage of the proposed algorithms over methods that estimate the matrix for a pre-defined sub-space. Table 2 summarizes the various data sets that were taken into account. As a normalization step, the data features are centered and converted to standard deviation."}, {"heading": "9.1 Toy data", "text": "In this section, the proposed algorithms are evaluated on the basis of synthetic regression problems: the data vectors x1,..., xn and the target matrix W * S + (r, d) are generated with the results of a standardized Gaussian distribution N (0, 0.1); the observations are preferred over an additive method to easily control that the observations are after the superposition of noise.Learning the subspace versus fixing the subspace up front. As an illustrative example, we show the difference between two approaches to adapt the data to observations when approaching a target model W * S + (3, 3) with a parameter W * S + (2, 3)."}, {"heading": "9.2 Kernel learning", "text": "In this section, the proposed algorithms are applied to the problem of learning a kernel matrix from pairs of distance limitations between data samples. As already mentioned, we only look at this problem in the transductive environment, i.e., all samples x1,... xn are available in advance and the learned kernel does not generalize to new samples."}, {"heading": "9.2.1 Experimental setup", "text": "After the transformation of the data with the kernel card x 7 \u2192 \u03c6 (x) the purpose is to calculate a kernel matrix with fixed rank (1 \u2212 \u03b1) for identically labeled samples and y \u00b2 ij (1 + \u03b1) for differentiated labeled samples where \u03b1 \u2265 0 is a scaling factor. We investigate both the influence of the amount of page information provided and the influence of the approximation rank and the computing time required by the algorithms."}, {"heading": "9.2.2 Compared methods", "text": "We compare the following methods: 1. Batch algorithms (11) and (22) adapted to handle inequalities (see Section 7.2), 2. The kernel learning algorithm LogDet-KL (Kulis et al., 2009), which learns fixed-range kernel matrices for a given set of distance constraints. 3. The kernel spectral regression algorithm (KSR) by Cai et al. (2007) using a similarity matrix N constructed as follows. Let N be the adjacence matrix of a 5-NN diagram based on the original kernel. We change N according to the available constraints: Nij = 1, if the samples xi and xj belong to the same class (must-link constraint), Nij = 0, if the samples xi and xj do not belong to the same class (cannot-link constraint).4. The maximum unfolding (MVU) algorithm (Weinberger)."}, {"heading": "9.2.3 Results", "text": "The first experiment is reproduced by Tsuda et al. (2005) and Kulis et al. (2009) The goal is to reconstruct the GyrB kernel matrix based on distance constraints. (2009) The goal is to compare the proposed batch methods with the LogDet KL algorithm, which also learns directly from distance constraints. This algorithm is the best performer reported by Kulis et al. (2009) for this experiment. (2009) All algorithms start with the identity matrix, which does not encode domain information. (left) reports classification accuracy as a function of the number of distance constraints provided. (In this complete learning setting, the algorithms are based on polar geometry with the LogDet KL algorithms."}, {"heading": "9.3 Mahalanobis distance learning", "text": "In this section, we deal with the problem of learning from data from a Mahalanobis distance for a supervised classification, and compare our methods with state-of-the-art Mahalanobis learning algorithms."}, {"heading": "9.3.1 Experimental setup", "text": "For the problem under consideration, the purpose is to learn the parameter of a Mahalanobis distance dW (xi, xj) = (xi \u2212 xj) TW (xi \u2212 xj) in such a way that the distance fulfills a given set of constraints as far as possible. As in the work of Davis et al. (2007), we calculate the constraints from the learning set of samples as dW (xi, xj) \u2264 l for equal pairs and dW (xi, xj) \u2265 u for different class pairs. Scalars u and l estimate the 95th and 5th percentiles of the distribution of Mahalanobis distances parameterized by a selected baseline W0. The performance of the distance learned is then quantified by the test error rate of a k-next neighbor classifier based on the distance learned. All experiments use the setting k = 5 and break arbitrarily bonds."}, {"heading": "9.3.2 Compared methods", "text": "We compare the following remote learning algorithms: 1. Batch algorithms (11) and (22), 2. ITML (Davis et al., 2007), 3. LMNN (Weinberger and Saul, 2009), 4. Online algorithms (10) and (21), 5. LEGO (Jain et al., 2008), 6. POLA (Shalev-Shwartz et al., 2004). If some methods require the fine-tuning of a hyperparameter, this is done by a double cross-validation procedure. Both the slip parameter of ITML and the step size of POLA are selected in the value range 10k with k = \u2212 3,..., 3. The step size of LEGO is selected in the same value range for the UCI datasets, and in the value range 10k with k = \u2212 10,... \u2212 5 for the larger datasets isolet and prostate."}, {"heading": "9.3.3 Results", "text": "Reproducing a classic benchmark experiment by Kulis et al. (2009), we show that the proposed batch algorithms compete with the most modern full-fledged Mahalanobis distance learning algorithms on multiple UCI data sets (Figure 7). With the exception of POLA and LMNN, which do not learn from the provided paired constraints, all algorithms process 40c (c \u2212 1) constraints, with c being the number of classes in the data. We choose the Euclidean distance (W0 = I) as the base distance for the initialization of the algorithms. Figure 7 reports thermal results. The two proposed algorithms compete favorably with the other full-fledged distance learning techniques and achieve the minimum average error for 4 of the 5 data sets.Finally, we evaluate the proposed algorithms on higher-value data sets, with the first target values in the lower order (Figure 8)."}, {"heading": "10. Conclusion", "text": "In this thesis, we propose gradient pedigree algorithms to learn a regression model parameterized by a positive semi-defined fixed-rank matrix. Riemann's rich geometry of the set of fixed-rank PSD matrices is exploited by a geometric optimization approach. The resulting algorithms overcome the main difficulties encountered by the previously proposed methods, since they scale to high-dimensional problems, and of course they impose both the rank constraint and the positive definitive property, leaving the range of the matrix free during optimization. We apply the proposed algorithms to the problem of learning a distance function from data when the distance is parameterized by a positive semi-definitive fixed-rank matrix. The good performance of the proposed algorithms is illustrated by several benchmarks."}, {"heading": "Acknowledgments", "text": "This article presents research results of the Belgian network DYSCO (Dynamical Systems, Control, and Optimization), which is funded by the Interuniversity Attraction Poles Programme initiated by the Belgian State. Scientific responsibility lies with the authors. Gilles Meyer is supported as a FRS-FNRS research fellow (Belgian Fund for Scientific Research)."}, {"heading": "Appendix A. Line-search algorithms on matrix manifolds", "text": "It is not the way in which it is about the question, whether it is about the question, whether it is about the question at all, whether it is about the question, whether it is about the question at all, whether it is about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is really about the question, whether it is really about the question, whether it is really about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all about the question, whether it is at all, whether it is at all about the question, whether it is at all, whether it is at all about the question, whether it is at all, whether it is at all, whether it is at all, whether it is at all, whether it is at all, whether it is at all, whether it is at all, whether it is at all, whether it is at all, whether it is at all at all, whether it is at all, whether it is at all at all, whether it is at all, whether it is at all, whether it is at all, whether it is at all, whether it is at all at all, whether it is at all, whether it is at all at all, whether it is at all, whether it is at all at all, whether it is at all, whether it is at all, whether it is at all, whether it is at all at all, whether it is at all, whether it is at all, whether it is at all, whether it is at all at all, whether it is at all, whether it is at all, whether it is at all, whether it is at all, whether it is at all"}], "references": [{"title": "Riemannian geometry of Grassmann manifolds with a view on algorithmic computation", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": "Acta Appl. Math.,", "citeRegEx": "Absil et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2004}, {"title": "Optimization Algorithms on Matrix Manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "Absil et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Absil et al\\.", "year": 2008}, {"title": "On learning rotations", "author": ["R. Arora"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Arora.,? \\Q2009\\E", "shortCiteRegEx": "Arora.", "year": 2009}, {"title": "Geometric means in a novel vector space structure on symmetric positive-definite matrices", "author": ["V. Arsigny", "P. Fillard", "X. Pennec", "N. Ayache"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Arsigny et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Arsigny et al\\.", "year": 2007}, {"title": "University of California, Irvine, School of Information and Computer Sciences", "author": ["A. Asuncion", "D.J. Newman"], "venue": "UCI machine learning repository. http://www.ics.uci.edu/~mlearn/MLRepository.html,", "citeRegEx": "Asuncion and Newman.,? \\Q2007\\E", "shortCiteRegEx": "Asuncion and Newman.", "year": 2007}, {"title": "Predictive low-rank decomposition for kernel methods", "author": ["F. Bach", "M.I. Jordan"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning (ICML),", "citeRegEx": "Bach and Jordan.,? \\Q2005\\E", "shortCiteRegEx": "Bach and Jordan.", "year": 2005}, {"title": "Riemannian metric and geometric mean for positive semidefinite matrices of fixed rank", "author": ["S. Bonnabel", "R. Sepulchre"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Bonnabel and Sepulchre.,? \\Q2009\\E", "shortCiteRegEx": "Bonnabel and Sepulchre.", "year": 2009}, {"title": "Modern Multidimensional Scaling: Theory and Applications", "author": ["I. Borg", "P. Groenen"], "venue": null, "citeRegEx": "Borg and Groenen.,? \\Q2005\\E", "shortCiteRegEx": "Borg and Groenen.", "year": 2005}, {"title": "Online algorithms and stochastic approximations", "author": ["L. Bottou"], "venue": null, "citeRegEx": "Bottou.,? \\Q1998\\E", "shortCiteRegEx": "Bottou.", "year": 1998}, {"title": "Stochastic learning", "author": ["L. Bottou"], "venue": "Advanced Lectures on Machine Learning,", "citeRegEx": "Bottou.,? \\Q2004\\E", "shortCiteRegEx": "Bottou.", "year": 2004}, {"title": "The tradeoffs of large scale learning", "author": ["L. Bottou", "O. Bousquet"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Bottou and Bousquet.,? \\Q2007\\E", "shortCiteRegEx": "Bottou and Bousquet.", "year": 2007}, {"title": "Efficient kernel discriminant analysis via spectral regression", "author": ["D. Cai", "X. He", "J. Han"], "venue": "In Proceedings of the International Conference on Data Mining (ICDM07),", "citeRegEx": "Cai et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cai et al\\.", "year": 2007}, {"title": "On kernelization of supervised mahalanobis distance learners", "author": ["R. Chatpatanasiri", "T. Korsrilabutr", "P. Tangchanachaianan", "B. Kijsirikul"], "venue": "arXiv,", "citeRegEx": "Chatpatanasiri et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Chatpatanasiri et al\\.", "year": 2008}, {"title": "Multidimensional Scaling", "author": ["T.F. Cox", "M.A.A. Cox"], "venue": null, "citeRegEx": "Cox and Cox.,? \\Q2001\\E", "shortCiteRegEx": "Cox and Cox.", "year": 2001}, {"title": "Online tracking of linear subspaces", "author": ["K. Crammer"], "venue": "In Proceedings of 19th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Crammer.,? \\Q2006\\E", "shortCiteRegEx": "Crammer.", "year": 2006}, {"title": "Structured metric learning for high dimensional problems", "author": ["J.V. Davis", "I.S. Dhillon"], "venue": "In Proceedings of the 14th ACM SIGKDD conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Davis and Dhillon.,? \\Q2008\\E", "shortCiteRegEx": "Davis and Dhillon.", "year": 2008}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In Proceedings of the 24th International Conference on Machine Learning (ICML),", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "The geometry of algorithms with orthogonality constraints", "author": ["A. Edelman", "T.A. Arias", "S.T. Smith"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Edelman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Edelman et al\\.", "year": 1998}, {"title": "Analysis on Symmetric Cones", "author": ["J. Faraut", "A. Koranyi"], "venue": null, "citeRegEx": "Faraut and Koranyi.,? \\Q1994\\E", "shortCiteRegEx": "Faraut and Koranyi.", "year": 1994}, {"title": "Efficient SVM training using low-rank kernel representations", "author": ["S. Fine", "K. Scheinberg", "N. Cristianini", "J. Shawe-taylor", "B. Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fine et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fine et al\\.", "year": 2001}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S. Roweis"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Globerson and Roweis.,? \\Q2005\\E", "shortCiteRegEx": "Globerson and Roweis.", "year": 2005}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Goldberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldberger et al\\.", "year": 2004}, {"title": "Matrix Computations", "author": ["G.H. Golub", "C.F. Van Loan"], "venue": null, "citeRegEx": "Golub and Loan.,? \\Q1996\\E", "shortCiteRegEx": "Golub and Loan.", "year": 1996}, {"title": "Online metric learning and fast similarity search", "author": ["P. Jain", "B. Kulis", "I.S. Dhillon", "K. Grauman"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Jain et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2008}, {"title": "Metric and kernel learning using a linear transformation", "author": ["P. Jain", "B. Kulis", "J.V. Davis", "I.S. Dhillon"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2009}, {"title": "Low-rank optimization for semidefinite convex problems", "author": ["M. Journ\u00e9e", "F. Bach", "P.-A. Absil", "R. Sepulchre"], "venue": "SIAM Journal on Matrix Analysis and Applications (in press),", "citeRegEx": "Journ\u00e9e et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Journ\u00e9e et al\\.", "year": 2010}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Journal of Information and Computation,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1997\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1997}, {"title": "Low-rank kernel learning with bregman matrix divergences", "author": ["B. Kulis", "M. Sustik", "I.S. Dhillon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kulis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2009}, {"title": "Learning with idealized kernels", "author": ["J. Kwok", "I. Tsang"], "venue": "Proceedings of the 20th International Conference on Machine learning (ICML),", "citeRegEx": "Kwok and Tsang.,? \\Q2003\\E", "shortCiteRegEx": "Kwok and Tsang.", "year": 2003}, {"title": "Learning the kernel matrix with semidefinite programming", "author": ["G. Lanckriet", "N. Cristianini", "P. Bartlett", "L. El Ghaoui", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lanckriet et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lanckriet et al\\.", "year": 2004}, {"title": "Handwritten digit recognition with a back-propagation network", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Global mapping analysis: Stochastic gradient algorithm in sstress and classical mds stress", "author": ["Y. Matsuda", "K. Yamaguchi"], "venue": "In Proceedings of International Conference on Neural Information Processing,", "citeRegEx": "Matsuda and Yamaguchi.,? \\Q2001\\E", "shortCiteRegEx": "Matsuda and Yamaguchi.", "year": 2001}, {"title": "Numerical Optimization, Second Edition", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "Nocedal and Wright.,? \\Q2006\\E", "shortCiteRegEx": "Nocedal and Wright.", "year": 2006}, {"title": "Principal components, minor components, and linear neural networks", "author": ["E. Oja"], "venue": "Neural Networks,", "citeRegEx": "Oja.,? \\Q1992\\E", "shortCiteRegEx": "Oja.", "year": 1992}, {"title": "Serum proteomic patterns for detection of prostate cancer", "author": ["E.F. Petricoin", "D.K. Ornstein", "C.P. Paweletz", "A.M. Ardekani", "P.S. Hackett", "B.A. Hitt", "A. Velassco", "C. Trucco", "L. Wiegand", "K. Wood", "C.B. Simone", "P.J. Levine", "W.M. Linehan", "M.R. Emmert-Buck", "S.M. Steinberg", "E.C Kohn", "L.A. Liotta"], "venue": "Journal of the National Cancer Institute,", "citeRegEx": "Petricoin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Petricoin et al\\.", "year": 2002}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R. Schapire", "Y. Singer"], "venue": "Machine Learning,", "citeRegEx": "Schapire and Singer.,? \\Q1999\\E", "shortCiteRegEx": "Schapire and Singer.", "year": 1999}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "K.-R. M\u00fcller"], "venue": "Neural Computation,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Online and batch learning of pseudo-metrics", "author": ["S. Shalev-Shwartz", "Y. Singer", "A. Ng"], "venue": "In Proceedings of the 21st International Conference on Machine Learning (ICML),", "citeRegEx": "Shalev.Shwartz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2004}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini.", "year": 2004}, {"title": "Covariance, subspace, and intrinsic cram\u00e9r-rao bounds", "author": ["S.T. Smith"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Smith.,? \\Q2005\\E", "shortCiteRegEx": "Smith.", "year": 2005}, {"title": "Colored maximum variance unfolding", "author": ["L. Song", "A. Smola", "K. Borgwardt", "A. Gretton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Song et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Song et al\\.", "year": 2007}, {"title": "Impact of similarity measures on web-page clustering", "author": ["A. Strehl", "J. Ghosh", "R. Mooney"], "venue": "In Workshop on Artificial Intelligence for Web Search (AAAI),", "citeRegEx": "Strehl et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2000}, {"title": "Large margin component analysis", "author": ["L. Torresani", "K. Lee"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Torresani and Lee.,? \\Q2006\\E", "shortCiteRegEx": "Torresani and Lee.", "year": 2006}, {"title": "Matrix exponentiated gradient updates for on-line learning and bregman projection", "author": ["K. Tsuda", "G. Ratsch", "M. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Tsuda et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsuda et al\\.", "year": 2005}, {"title": "Winnowing subspaces", "author": ["M. Warmuth"], "venue": "Proceedings of the 24th international conference on Machine learning (ICML),", "citeRegEx": "Warmuth.,? \\Q2007\\E", "shortCiteRegEx": "Warmuth.", "year": 2007}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K. Weinberger", "L. Saul"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Weinberger and Saul.,? \\Q2009\\E", "shortCiteRegEx": "Weinberger and Saul.", "year": 2009}, {"title": "Learning a kernel matrix for nonlinear dimensionality reduction", "author": ["K. Weinberger", "F. Sha", "L. Saul"], "venue": "Proceedings of the 21st International Conference on Machine Learning (ICML),", "citeRegEx": "Weinberger et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2004}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S. Russell"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}, {"title": "Distance metric learning: A comprehensive survey", "author": ["L. Yang"], "venue": "Technical report, Michigan State University,", "citeRegEx": "Yang.,? \\Q2006\\E", "shortCiteRegEx": "Yang.", "year": 2006}, {"title": "SimpleNPKL: simple non-parametric kernel learning", "author": ["J. Zhuang", "I. Tsang", "S. Hoi"], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML),", "citeRegEx": "Zhuang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhuang et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al., 2008), we obtain concrete gradient updates that maintain the rank and the positivity of the learned model at each iteration.", "startOffset": 83, "endOffset": 103}, {"referenceID": 47, "context": "This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005).", "startOffset": 247, "endOffset": 294}, {"referenceID": 20, "context": "This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005).", "startOffset": 247, "endOffset": 294}, {"referenceID": 14, "context": "Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al.", "startOffset": 81, "endOffset": 111}, {"referenceID": 44, "context": "Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al.", "startOffset": 81, "endOffset": 111}, {"referenceID": 2, "context": "Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al.", "startOffset": 131, "endOffset": 144}, {"referenceID": 43, "context": "Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al., 2005).", "startOffset": 177, "endOffset": 197}, {"referenceID": 35, "context": "(2005) propose to use the von Neumann divergence, resulting in a generalization of the well-known AdaBoost algorithm (Schapire and Singer, 1999) to positive definite matrices.", "startOffset": 117, "endOffset": 144}, {"referenceID": 19, "context": "Indeed, whereas algorithms based on full-rank matrices scale as O(d3) and require O(d2) storage units, algorithms based on low-rank matrices scale as O(dr2) and require O(dr) storage units (Fine et al., 2001; Bach and Jordan, 2005).", "startOffset": 189, "endOffset": 231}, {"referenceID": 5, "context": "Indeed, whereas algorithms based on full-rank matrices scale as O(d3) and require O(d2) storage units, algorithms based on low-rank matrices scale as O(dr2) and require O(dr) storage units (Fine et al., 2001; Bach and Jordan, 2005).", "startOffset": 189, "endOffset": 231}, {"referenceID": 16, "context": "Our approach makes use of two quotient geometries of the set S+(r, d) that have been recently studied by Journ\u00e9e et al. (2010) and Bonnabel and Sepulchre (2009).", "startOffset": 105, "endOffset": 127}, {"referenceID": 2, "context": "(2010) and Bonnabel and Sepulchre (2009). Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al.", "startOffset": 11, "endOffset": 41}, {"referenceID": 0, "context": "Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al., 2008), we obtain concrete gradient updates that maintain the rank and the positivity of the learned model at each iteration. This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005). Not surprisingly, our approach has close connections with a number of recent contributions on learning algorithms. Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al., 2005). The space of (full-rank) positive definite matrices S+(d) is of particular interest since it coincides with our set of interest in the particular case r = d. The use of Bregman divergences and alternating projection has been recently investigated for learning in S+(d). Tsuda et al. (2005) propose to use the von Neumann divergence, resulting in a generalization of the well-known AdaBoost algorithm (Schapire and Singer, 1999) to positive definite matrices.", "startOffset": 84, "endOffset": 1123}, {"referenceID": 0, "context": "Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al., 2008), we obtain concrete gradient updates that maintain the rank and the positivity of the learned model at each iteration. This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005). Not surprisingly, our approach has close connections with a number of recent contributions on learning algorithms. Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al., 2005). The space of (full-rank) positive definite matrices S+(d) is of particular interest since it coincides with our set of interest in the particular case r = d. The use of Bregman divergences and alternating projection has been recently investigated for learning in S+(d). Tsuda et al. (2005) propose to use the von Neumann divergence, resulting in a generalization of the well-known AdaBoost algorithm (Schapire and Singer, 1999) to positive definite matrices. The use of the so-called LogDet divergence has also been investigated by Davis et al. (2007) in the context of Mahalanobis distance learning.", "startOffset": 84, "endOffset": 1385}, {"referenceID": 0, "context": "Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al., 2008), we obtain concrete gradient updates that maintain the rank and the positivity of the learned model at each iteration. This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005). Not surprisingly, our approach has close connections with a number of recent contributions on learning algorithms. Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al., 2005). The space of (full-rank) positive definite matrices S+(d) is of particular interest since it coincides with our set of interest in the particular case r = d. The use of Bregman divergences and alternating projection has been recently investigated for learning in S+(d). Tsuda et al. (2005) propose to use the von Neumann divergence, resulting in a generalization of the well-known AdaBoost algorithm (Schapire and Singer, 1999) to positive definite matrices. The use of the so-called LogDet divergence has also been investigated by Davis et al. (2007) in the context of Mahalanobis distance learning. More recently, algorithmic work has focused on scalability in terms of dimensionality and data set size. A natural extension of the previous work on positive definite matrices is thus to consider low-rank positive semidefinite matrices. Indeed, whereas algorithms based on full-rank matrices scale as O(d3) and require O(d2) storage units, algorithms based on low-rank matrices scale as O(dr2) and require O(dr) storage units (Fine et al., 2001; Bach and Jordan, 2005). This is a significant complexity reduction as the approximation rank r is typically very small compared to the dimension of the problem d. Extending the work of Tsuda et al. (2005), Kulis et al.", "startOffset": 84, "endOffset": 2085}, {"referenceID": 0, "context": "Making use of a general theory of line-search algorithms in quotient matrix spaces (Absil et al., 2008), we obtain concrete gradient updates that maintain the rank and the positivity of the learned model at each iteration. This is because the update is intrinsically constrained to belong to the nonlinear search space, in contrast to early learning algorithms that neglect the non linear nature of the search space in the update and impose the constraints a posteriori (Xing et al., 2002; Globerson and Roweis, 2005). Not surprisingly, our approach has close connections with a number of recent contributions on learning algorithms. Learning problems over nonlinear matrix spaces include the learning of subspaces (Crammer, 2006; Warmuth, 2007), rotation matrices (Arora, 2009), and positive definite matrices (Tsuda et al., 2005). The space of (full-rank) positive definite matrices S+(d) is of particular interest since it coincides with our set of interest in the particular case r = d. The use of Bregman divergences and alternating projection has been recently investigated for learning in S+(d). Tsuda et al. (2005) propose to use the von Neumann divergence, resulting in a generalization of the well-known AdaBoost algorithm (Schapire and Singer, 1999) to positive definite matrices. The use of the so-called LogDet divergence has also been investigated by Davis et al. (2007) in the context of Mahalanobis distance learning. More recently, algorithmic work has focused on scalability in terms of dimensionality and data set size. A natural extension of the previous work on positive definite matrices is thus to consider low-rank positive semidefinite matrices. Indeed, whereas algorithms based on full-rank matrices scale as O(d3) and require O(d2) storage units, algorithms based on low-rank matrices scale as O(dr2) and require O(dr) storage units (Fine et al., 2001; Bach and Jordan, 2005). This is a significant complexity reduction as the approximation rank r is typically very small compared to the dimension of the problem d. Extending the work of Tsuda et al. (2005), Kulis et al. (2009) recently considered the learning of positive semidefinite matrices.", "startOffset": 84, "endOffset": 2106}, {"referenceID": 9, "context": "Online learning algorithms (Bottou, 2004) consider possibly infinite sets of samples {(Xt, yt)}t\u22651, received one at a time.", "startOffset": 27, "endOffset": 41}, {"referenceID": 0, "context": "Following Absil et al. (2008), an abstract gradient descent algorithm can then be derived based on the update formula", "startOffset": 10, "endOffset": 30}, {"referenceID": 0, "context": "Line-search algorithms in quotient Riemannian spaces are discussed in detail in the book of Absil et al. (2008). For the readers convenience, basic concepts and notations are also provided in Appendix A.", "startOffset": 92, "endOffset": 112}, {"referenceID": 33, "context": "Linear regression on the Grassmann manifold As a preparatory step to Section 5, we review the online subspace learning (Oja, 1992; Crammer, 2006; Warmuth, 2007) in the present framework.", "startOffset": 119, "endOffset": 160}, {"referenceID": 14, "context": "Linear regression on the Grassmann manifold As a preparatory step to Section 5, we review the online subspace learning (Oja, 1992; Crammer, 2006; Warmuth, 2007) in the present framework.", "startOffset": 119, "endOffset": 160}, {"referenceID": 44, "context": "Linear regression on the Grassmann manifold As a preparatory step to Section 5, we review the online subspace learning (Oja, 1992; Crammer, 2006; Warmuth, 2007) in the present framework.", "startOffset": 119, "endOffset": 160}, {"referenceID": 17, "context": "The quotient geometries of Gr(r, d) have been well studied (Edelman et al., 1998; Absil et al., 2004).", "startOffset": 59, "endOffset": 101}, {"referenceID": 0, "context": "The quotient geometries of Gr(r, d) have been well studied (Edelman et al., 1998; Absil et al., 2004).", "startOffset": 59, "endOffset": 101}, {"referenceID": 0, "context": "Following Absil et al. (2004), an alternative convenient retraction in Gr(r, d) is given by", "startOffset": 10, "endOffset": 30}, {"referenceID": 33, "context": "With the formulas (6) and (7) applied to the cost function (5), the abstract update (3) becomes Ut+1 = qf(Ut + st(I\u2212UtUt)xtxt Ut), which is Oja\u2019s update for subspace tracking (Oja, 1992).", "startOffset": 175, "endOffset": 186}, {"referenceID": 18, "context": "2 The affine-invariant metric on S+(d) Because S+(d) ' GL(d)/O(d) is the quotient of two Lie groups, its (reductive) geometric structure can be further exploited (Faraut and Koranyi, 1994).", "startOffset": 162, "endOffset": 188}, {"referenceID": 39, "context": "The affine-invariant geometry of S+(d) has been well studied, in particular in the context of information geometry (Smith, 2005).", "startOffset": 115, "endOffset": 128}, {"referenceID": 39, "context": "This approximation coincides with the Riemannian distance that is induced by the affine-invariant metric (12) (Smith, 2005).", "startOffset": 110, "endOffset": 123}, {"referenceID": 16, "context": "With the alternative retraction (15), the update becomes Wt+1 = Wt \u2212 st(\u0177t \u2212 yt)WtSym(Xt)Wt, which is the update of Davis et al. (2007) based on the LogDet divergence (see Section 7.", "startOffset": 116, "endOffset": 136}, {"referenceID": 3, "context": "This geometry is studied in detail in the paper (Arsigny et al., 2007).", "startOffset": 48, "endOffset": 70}, {"referenceID": 43, "context": "The gradient of this cost function is given by gradf(S) = (\u0177t \u2212 yt)Sym(Xt), and the retraction is RS(s\u03beS) = exp(log W + s\u03beS) The corresponding gradient descent update is Wt+1 = exp(log Wt \u2212 st(\u0177t \u2212 yt)Sym(Xt)), which is the update of Tsuda et al. (2005) based on the von Neumann divergence.", "startOffset": 234, "endOffset": 254}, {"referenceID": 25, "context": "The quotient geometry of S+(r, d) ' Rd\u00d7r \u2217 /O(r) is studied by Journ\u00e9e et al. (2010).", "startOffset": 63, "endOffset": 85}, {"referenceID": 6, "context": "The Riemannian geometry of (16) has been recently studied (Bonnabel and Sepulchre, 2009).", "startOffset": 58, "endOffset": 88}, {"referenceID": 6, "context": "The geodesics do not appear to have a closed form in this geometry, see the paper of Bonnabel and Sepulchre (2009) for details.", "startOffset": 85, "endOffset": 115}, {"referenceID": 10, "context": "They intrinsically have a much slower convergence rate than batch algorithms, but they generally decrease faster the expected loss in the large-scale regime (Bottou and Bousquet, 2007).", "startOffset": 157, "endOffset": 184}, {"referenceID": 0, "context": "For batch algorithms, these statements follow from the convergence theory of line-search algorithms on Riemannian manifolds (see, for example, Absil et al., 2008). For online algorithms, one can prove that the algorithm based on the flat geometry enjoys almost sure asymptotic convergence to a local minimum of the expected cost. In that case, the parameter G belongs to an Euclidean space and the convergence results presented by Bottou (1998) apply directly (see Appendix B for the main ideas of the proof).", "startOffset": 143, "endOffset": 445}, {"referenceID": 0, "context": "For batch algorithms, these statements follow from the convergence theory of line-search algorithms on Riemannian manifolds (see, for example, Absil et al., 2008). For online algorithms, one can prove that the algorithm based on the flat geometry enjoys almost sure asymptotic convergence to a local minimum of the expected cost. In that case, the parameter G belongs to an Euclidean space and the convergence results presented by Bottou (1998) apply directly (see Appendix B for the main ideas of the proof). When the polar parameterization is used, the convergence results presented by Bottou (1998) do not apply directly.", "startOffset": 143, "endOffset": 602}, {"referenceID": 0, "context": "For batch algorithms, these statements follow from the convergence theory of line-search algorithms on Riemannian manifolds (see, for example, Absil et al., 2008). For online algorithms, one can prove that the algorithm based on the flat geometry enjoys almost sure asymptotic convergence to a local minimum of the expected cost. In that case, the parameter G belongs to an Euclidean space and the convergence results presented by Bottou (1998) apply directly (see Appendix B for the main ideas of the proof). When the polar parameterization is used, the convergence results presented by Bottou (1998) do not apply directly. However, we empirically observed that the algorithm always converges to a local minimum of the cost function. Numerical simulations thus suggest that the results of Bottou (1998) on stochastic gradient descent extend to the gradient descent algorithm based on polar parameterization.", "startOffset": 143, "endOffset": 804}, {"referenceID": 26, "context": "1 Closeness-based approaches A standard derivation of learning algorithms is as follows (Kivinen and Warmuth, 1997).", "startOffset": 88, "endOffset": 115}, {"referenceID": 27, "context": "In particular, when \u03bb = 1, the subspace is fixed and one recovers the setup of learning low-rank matrices of fixed range space (Kulis et al., 2009).", "startOffset": 127, "endOffset": 147}, {"referenceID": 27, "context": "Thus the algorithms introduced in the present paper can be viewed as generalizations of the ones presented in (Kulis et al., 2009).", "startOffset": 110, "endOffset": 130}, {"referenceID": 27, "context": "The authors of (Kulis et al., 2009) present the issue of adapting the range space as an open research question.", "startOffset": 15, "endOffset": 35}, {"referenceID": 13, "context": "4 Connection with multidimensional scaling algorithms Given a set of m dissimilarity measures D = {\u03b4ij} between n data objects, multidimensional scaling algorithms search for a r-dimensional embedding of the data objects into an Euclidean space representation G \u2208 Rn\u00d7r (Cox and Cox, 2001; Borg and Groenen, 2005).", "startOffset": 269, "endOffset": 312}, {"referenceID": 7, "context": "4 Connection with multidimensional scaling algorithms Given a set of m dissimilarity measures D = {\u03b4ij} between n data objects, multidimensional scaling algorithms search for a r-dimensional embedding of the data objects into an Euclidean space representation G \u2208 Rn\u00d7r (Cox and Cox, 2001; Borg and Groenen, 2005).", "startOffset": 269, "endOffset": 312}, {"referenceID": 31, "context": "A stochastic gradient descent approach for minimizing the SSTRESS has also been proposed by Matsuda and Yamaguchi (2001). A potential area of future work is the application of the proposed online algorithm (10) for adapting a batch solution to slight modifications of the dissimilarities over time.", "startOffset": 92, "endOffset": 121}, {"referenceID": 38, "context": "1 Kernel learning In kernel-based methods (Shawe-Taylor and Cristianini, 2004), the data samples x1, .", "startOffset": 42, "endOffset": 78}, {"referenceID": 28, "context": "Most of the numerous kernel learning algorithms that have been proposed work in the socalled transductive setting, that is, it is not possible to generalize the learned kernel function to new data samples (Kwok and Tsang, 2003; Lanckriet et al., 2004; Tsuda et al., 2005; Zhuang et al., 2009; Kulis et al., 2009).", "startOffset": 205, "endOffset": 312}, {"referenceID": 29, "context": "Most of the numerous kernel learning algorithms that have been proposed work in the socalled transductive setting, that is, it is not possible to generalize the learned kernel function to new data samples (Kwok and Tsang, 2003; Lanckriet et al., 2004; Tsuda et al., 2005; Zhuang et al., 2009; Kulis et al., 2009).", "startOffset": 205, "endOffset": 312}, {"referenceID": 43, "context": "Most of the numerous kernel learning algorithms that have been proposed work in the socalled transductive setting, that is, it is not possible to generalize the learned kernel function to new data samples (Kwok and Tsang, 2003; Lanckriet et al., 2004; Tsuda et al., 2005; Zhuang et al., 2009; Kulis et al., 2009).", "startOffset": 205, "endOffset": 312}, {"referenceID": 49, "context": "Most of the numerous kernel learning algorithms that have been proposed work in the socalled transductive setting, that is, it is not possible to generalize the learned kernel function to new data samples (Kwok and Tsang, 2003; Lanckriet et al., 2004; Tsuda et al., 2005; Zhuang et al., 2009; Kulis et al., 2009).", "startOffset": 205, "endOffset": 312}, {"referenceID": 27, "context": "Most of the numerous kernel learning algorithms that have been proposed work in the socalled transductive setting, that is, it is not possible to generalize the learned kernel function to new data samples (Kwok and Tsang, 2003; Lanckriet et al., 2004; Tsuda et al., 2005; Zhuang et al., 2009; Kulis et al., 2009).", "startOffset": 205, "endOffset": 312}, {"referenceID": 12, "context": "Recently, algorithms have been proposed to learn a kernel function that can be extended to new points (Chatpatanasiri et al., 2008; Jain et al., 2009).", "startOffset": 102, "endOffset": 150}, {"referenceID": 24, "context": "Recently, algorithms have been proposed to learn a kernel function that can be extended to new points (Chatpatanasiri et al., 2008; Jain et al., 2009).", "startOffset": 102, "endOffset": 150}, {"referenceID": 36, "context": "Very popular unsupervised algorithms in that context are kernel principal component analysis (Sch\u00f6lkopf et al., 1998) and multidimensional scaling (Cox and Cox, 2001; Borg and Groenen, 2005).", "startOffset": 93, "endOffset": 117}, {"referenceID": 13, "context": ", 1998) and multidimensional scaling (Cox and Cox, 2001; Borg and Groenen, 2005).", "startOffset": 37, "endOffset": 80}, {"referenceID": 7, "context": ", 1998) and multidimensional scaling (Cox and Cox, 2001; Borg and Groenen, 2005).", "startOffset": 37, "endOffset": 80}, {"referenceID": 46, "context": "Other kernel learning techniques include the maximum variance unfolding algorithm (Weinberger et al., 2004) and its semisupervised version (Song et al.", "startOffset": 82, "endOffset": 107}, {"referenceID": 40, "context": ", 2004) and its semisupervised version (Song et al., 2007), and the kernel spectral regression framework (Cai et al.", "startOffset": 39, "endOffset": 58}, {"referenceID": 11, "context": ", 2007), and the kernel spectral regression framework (Cai et al., 2007) which encompasses many reduction criterion (for example, linear discriminant analysis (LDA), locality preserving projection (LPP), neighborhood preserving embedding (NPE)).", "startOffset": 54, "endOffset": 72}, {"referenceID": 48, "context": "See the survey of (Yang, 2006) for a more complete state-of-the-art in this area.", "startOffset": 18, "endOffset": 30}, {"referenceID": 47, "context": "The first proposed methods have been based on successive projections onto a set of large margin constraints (Xing et al., 2002; Shalev-Shwartz et al., 2004).", "startOffset": 108, "endOffset": 156}, {"referenceID": 37, "context": "The first proposed methods have been based on successive projections onto a set of large margin constraints (Xing et al., 2002; Shalev-Shwartz et al., 2004).", "startOffset": 108, "endOffset": 156}, {"referenceID": 21, "context": "A simpler objective is pursued by the algorithms that optimize the Mahalanobis distance for the specific k-nearest neighbor classifier (Goldberger et al., 2004; Torresani and Lee, 2006; Weinberger and Saul, 2009).", "startOffset": 135, "endOffset": 212}, {"referenceID": 42, "context": "A simpler objective is pursued by the algorithms that optimize the Mahalanobis distance for the specific k-nearest neighbor classifier (Goldberger et al., 2004; Torresani and Lee, 2006; Weinberger and Saul, 2009).", "startOffset": 135, "endOffset": 212}, {"referenceID": 45, "context": "A simpler objective is pursued by the algorithms that optimize the Mahalanobis distance for the specific k-nearest neighbor classifier (Goldberger et al., 2004; Torresani and Lee, 2006; Weinberger and Saul, 2009).", "startOffset": 135, "endOffset": 212}, {"referenceID": 16, "context": "Both batch (Davis et al., 2007) and online (Jain et al.", "startOffset": 11, "endOffset": 31}, {"referenceID": 23, "context": ", 2007) and online (Jain et al., 2008) formulations have been proposed for learning full-rank matrices.", "startOffset": 19, "endOffset": 38}, {"referenceID": 15, "context": "Low-rank matrices have also been considered with Bregman divergences but only when the range space of the matrix is fixed in the first place (Davis and Dhillon, 2008; Kulis et al., 2009).", "startOffset": 141, "endOffset": 186}, {"referenceID": 27, "context": "Low-rank matrices have also been considered with Bregman divergences but only when the range space of the matrix is fixed in the first place (Davis and Dhillon, 2008; Kulis et al., 2009).", "startOffset": 141, "endOffset": 186}, {"referenceID": 18, "context": "The method proposed by Globerson and Roweis (2005) seeks a Mahalanobis matrix that maximizes the between classes distance while forcing to zero the within classes distance.", "startOffset": 23, "endOffset": 51}, {"referenceID": 40, "context": "Data Set Samples Features Classes Reference GyrB 52 3 Tsuda et al. (2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 54, "endOffset": 74}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 23, "endOffset": 50}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 23, "endOffset": 92}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 23, "endOffset": 139}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 23, "endOffset": 188}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 23, "endOffset": 228}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al.", "startOffset": 23, "endOffset": 273}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al. (1989) Isolet 7,797 617 26 Asuncion and Newman (2007) Prostate 322 15,154 2 Petricoin et al.", "startOffset": 23, "endOffset": 311}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al. (1989) Isolet 7,797 617 26 Asuncion and Newman (2007) Prostate 322 15,154 2 Petricoin et al.", "startOffset": 23, "endOffset": 358}, {"referenceID": 4, "context": "(2005) Digits 300 16 3 Asuncion and Newman (2007) Wine 178 13 13 Asuncion and Newman (2007) Ionosphere 351 33 2 Asuncion and Newman (2007) Balance Scale 625 4 3 Asuncion and Newman (2007) Iris 150 4 3 Asuncion and Newman (2007) Soybean 532 35 17 Asuncion and Newman (2007) USPS 2,007 256 10 LeCun et al. (1989) Isolet 7,797 617 26 Asuncion and Newman (2007) Prostate 322 15,154 2 Petricoin et al. (2002)", "startOffset": 23, "endOffset": 404}, {"referenceID": 27, "context": ", 2004), LogDet-KL (Kulis et al., 2009) and LEGO (Jain et al.", "startOffset": 19, "endOffset": 39}, {"referenceID": 23, "context": ", 2009) and LEGO (Jain et al., 2008) have been implemented on our own.", "startOffset": 17, "endOffset": 36}, {"referenceID": 40, "context": "The implementations of algorithms MVU3, KSR4, LMNN5 and ITML6 have been rendered publicly available by Weinberger et al. (2004), Cai et al.", "startOffset": 103, "endOffset": 128}, {"referenceID": 11, "context": "(2004), Cai et al. (2007), Weinberger and Saul (2009) and Davis et al.", "startOffset": 8, "endOffset": 26}, {"referenceID": 11, "context": "(2004), Cai et al. (2007), Weinberger and Saul (2009) and Davis et al.", "startOffset": 8, "endOffset": 54}, {"referenceID": 11, "context": "(2004), Cai et al. (2007), Weinberger and Saul (2009) and Davis et al. (2007) respectively.", "startOffset": 8, "endOffset": 78}, {"referenceID": 15, "context": "Recent methods compute that subspace of reduced dimension using principal component analysis (Davis and Dhillon, 2008; Weinberger and Saul, 2009), that is, a subspace that captures a maximal amount of variance in the data.", "startOffset": 93, "endOffset": 145}, {"referenceID": 45, "context": "Recent methods compute that subspace of reduced dimension using principal component analysis (Davis and Dhillon, 2008; Weinberger and Saul, 2009), that is, a subspace that captures a maximal amount of variance in the data.", "startOffset": 93, "endOffset": 145}, {"referenceID": 41, "context": "The quality of the clustering is measured by the normalized mutual information (NMI) shared between the random variables of cluster indicators C and true class labels T (Strehl et al., 2000),", "startOffset": 169, "endOffset": 190}, {"referenceID": 27, "context": "The kernel learning algorithm LogDet-KL (Kulis et al., 2009) which learn kernel matrices of fixed range space for a given set of distance constraints.", "startOffset": 40, "endOffset": 60}, {"referenceID": 11, "context": "The kernel spectral regression (KSR) algorithm of Cai et al. (2007) using a similarity matrix N constructed as follows.", "startOffset": 50, "endOffset": 68}, {"referenceID": 46, "context": "The Maximum Variance Unfolding (MVU) algorithm (Weinberger et al., 2004),", "startOffset": 47, "endOffset": 72}, {"referenceID": 36, "context": "The Kernel PCA algorithm (Sch\u00f6lkopf et al., 1998).", "startOffset": 25, "endOffset": 49}, {"referenceID": 42, "context": "The first experiment is reproduced from Tsuda et al. (2005) and Kulis et al.", "startOffset": 40, "endOffset": 60}, {"referenceID": 27, "context": "(2005) and Kulis et al. (2009). The goal is to reconstruct the GyrB kernel matrix based on distance constraints only.", "startOffset": 11, "endOffset": 31}, {"referenceID": 27, "context": "(2005) and Kulis et al. (2009). The goal is to reconstruct the GyrB kernel matrix based on distance constraints only. This matrix contains information about the proteins of three bacteria species. The distance constraints are randomly generated from the original kernel matrix with \u03b1 = 0. We compare the proposed batch methods with the LogDet-KL algorithm, the only competing algorithm that also learns directly from distance constraints. This algorithm is the best performer reported by Kulis et al. (2009) for this experiment.", "startOffset": 11, "endOffset": 508}, {"referenceID": 27, "context": "(2005) and Kulis et al. (2009). The goal is to reconstruct the GyrB kernel matrix based on distance constraints only. This matrix contains information about the proteins of three bacteria species. The distance constraints are randomly generated from the original kernel matrix with \u03b1 = 0. We compare the proposed batch methods with the LogDet-KL algorithm, the only competing algorithm that also learns directly from distance constraints. This algorithm is the best performer reported by Kulis et al. (2009) for this experiment. All algorithms start from the identity matrix that do not encode any domain information. Figure 5 (left) reports the k-NN classification accuracy as a function of the number of distance constraints provided. In this full-rank learning setting, the algorithm based on the polar geometry compete with the LogDet-KL algorithm. The convergence time of the algorithm based on the polar geometry is however much faster (0.15 seconds versus 58 seconds for LogDet-KL when learning 1000 constraints). The algorithm based on the flat geometry has inferior performance when too few constraints are provided. This is because in the kernel learning setting, updates of this algorithm only involve the rows and columns that correspond to the set of points for which constraints are provided. It may thus result in a partial update of the kernel matrix entries. This issue disappears as the number of provided constraints increases. The second experiment is reproduced from Kulis et al. (2009). It aims at improving an existing low-rank kernel using limited information about class labels.", "startOffset": 11, "endOffset": 1508}, {"referenceID": 27, "context": "(2005) and Kulis et al. (2009). The goal is to reconstruct the GyrB kernel matrix based on distance constraints only. This matrix contains information about the proteins of three bacteria species. The distance constraints are randomly generated from the original kernel matrix with \u03b1 = 0. We compare the proposed batch methods with the LogDet-KL algorithm, the only competing algorithm that also learns directly from distance constraints. This algorithm is the best performer reported by Kulis et al. (2009) for this experiment. All algorithms start from the identity matrix that do not encode any domain information. Figure 5 (left) reports the k-NN classification accuracy as a function of the number of distance constraints provided. In this full-rank learning setting, the algorithm based on the polar geometry compete with the LogDet-KL algorithm. The convergence time of the algorithm based on the polar geometry is however much faster (0.15 seconds versus 58 seconds for LogDet-KL when learning 1000 constraints). The algorithm based on the flat geometry has inferior performance when too few constraints are provided. This is because in the kernel learning setting, updates of this algorithm only involve the rows and columns that correspond to the set of points for which constraints are provided. It may thus result in a partial update of the kernel matrix entries. This issue disappears as the number of provided constraints increases. The second experiment is reproduced from Kulis et al. (2009). It aims at improving an existing low-rank kernel using limited information about class labels. A rank-16 kernel matrix is computed for clustering a database of 300 handwritten digits randomly sampled from the 3, 8 and 9 digits of the Digits dataset (since we could not find out the specific samples that have been selected by Kulis et al. (2009), we made our own samples selection).", "startOffset": 11, "endOffset": 1855}, {"referenceID": 16, "context": "As in the paper of Davis et al. (2007), we generate the constraints from the learning set of samples as dW(xi,xj) \u2264 l for same-class pairs and dW(xi,xj) \u2265 u for different-class pairs.", "startOffset": 19, "endOffset": 39}, {"referenceID": 16, "context": "ITML (Davis et al., 2007),", "startOffset": 5, "endOffset": 25}, {"referenceID": 45, "context": "LMNN (Weinberger and Saul, 2009),", "startOffset": 5, "endOffset": 32}, {"referenceID": 23, "context": "LEGO (Jain et al., 2008),", "startOffset": 5, "endOffset": 24}, {"referenceID": 37, "context": "POLA (Shalev-Shwartz et al., 2004).", "startOffset": 5, "endOffset": 34}, {"referenceID": 27, "context": "3 Results Reproducing a classical benchmark experiment from Kulis et al. (2009), we demonstrate that the proposed batch algorithms compete with state-of-the-art full-rank Mahalanobis distance learning algorithms on several UCI datasets (Figure 7).", "startOffset": 60, "endOffset": 80}, {"referenceID": 8, "context": "Convergence proof of algorithm (10) Bottou (1998) reviews the mathematical tools required to prove almost sure convergence, that is asymptotic convergence with probability one, of stochastic gradient algorithms.", "startOffset": 36, "endOffset": 50}, {"referenceID": 8, "context": "Technical details are provided in the paper of Bottou (1998).", "startOffset": 47, "endOffset": 61}], "year": 2017, "abstractText": "The paper addresses the problem of learning a regression model parameterized by a fixedrank positive semidefinite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of fixed-rank positive semidefinite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semidefinite matrix. Good performance is observed on classical benchmarks.", "creator": "LaTeX with hyperref package"}}}