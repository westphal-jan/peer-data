{"id": "1006.1434", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2010", "title": "Computing by Means of Physics-Based Optical Neural Networks", "abstract": "We report recent research on computing with biology-based neural network models by means of physics-based opto-electronic hardware. New technology provides opportunities for very-high-speed computation and uncovers problems obstructing the wide-spread use of this new capability. The Computation Modeling community may be able to offer solutions to these cross-boundary research problems.", "histories": [["v1", "Tue, 8 Jun 2010 01:17:00 GMT  (119kb,D)", "http://arxiv.org/abs/1006.1434v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["a steven younger", "emmett redd"], "accepted": false, "id": "1006.1434"}, "pdf": {"name": "1006.1434.pdf", "metadata": {"source": "CRF", "title": "Computing by Means of Physics-Based Optical Neural Networks", "authors": ["Emmett Redd", "A. Steven Younger"], "emails": ["steveyounger@missouristate.edu", "emmettredd@missouristate.edu"], "sections": [{"heading": null, "text": "S. B. Cooper, E. Kashefi, P. Panangaden (eds.): Developments in Computational Models (DCM 2010) EPTCS 26, 2010, pp. 159-167, doi: 10.4204 / EPTCS.26.15c \u00a9 A. Steven Younger & Emmett Redd This work is published under the Creative Commons Attribution License.Computing by Means of Physics-Based Optical Neural NetworksA. Steven Younger Center for Applied Science and EngineeringMissouri State University, USA steveyounger @ missouristate.eduEmmett Redd Department of Physics, Astronomy, and Materials ScienceMissouri State University, USA emmettredd @ missouristate.eduWe report on recent research results on computing with biology-based neural network models using physics-based optoelectronic hardware."}, {"heading": "1 Introduction", "text": "About two decades ago, Optical Computing and Optical Neural Networks were the subject of intense research interest. [1] [2] [3] They were considered possible solutions to the expected limitations of Moore's Law. However, several problems, such as slow learning speed, high component costs, and pushing back the boundaries of Moore's Law by more conventional technologies, caused a decline in interest. New technologies are enabling the development of the next generation of optical computing devices. This new technology is largely driven by the explosion of optical communication. [4] [5] [6] We expect interest in optical computing and optical neural networks to increase, especially since the boundaries of Moore's Law appear to have been reached by conventional chip-making technology. [7] The new optical systems will enable us to evolve faster than we believe."}, {"heading": "2 Optical Neural Network Hardware", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Optical Neural Network Prototype", "text": "The prototype is based on the relatively old concept of a Stanford Matrix Multiplier [8] modified to efficiently implement FWL-NNs (Fig. 1).The Stanford design multiplies a matrix of M by N with a vector argument of 1 to N, resulting in a vector output of M to 1. Electronic input signals are converted into light beams by a horizontal array of laser sources (left)."}, {"heading": "2.2 Optical Signal Emitters (Laser Arrays)", "text": "Perhaps the most significant new technology is the development of low-cost, small, and very fast laser arrays. The horizontal blue bar at the bottom left of Figure 1 shows a linear arrangement of 12 Vertical Cavity Surface Emitting Lasers (VSCELs), each of which can be modulated independently at 10 GHz. The lasers are mounted on a 3 mm long rectangular die that can generate either binary or (usually 8 bits) analog optical signals. The beam from each laser is about as bright as a standard laser pointer. Each array has a potential bandwidth of 120 gigabytes / second. (This performance represents, as the estimates below, a best-case that is rarely achieved in a real device for cost reasons.) In a prototype device that we are developing, a total of 60 individual lasers are used."}, {"heading": "2.3 Optical Signal Detection and Opto-Electronics", "text": "As with the sources, high-speed photodetectors have made great strides in recent years. Figure 1 below right shows a linear array of 16 detectors based on silicon PiN technology that has lowered costs and increased speed. In addition, cost-effective operational amplifiers required to amplify and process the detectors \"electrical performance have reached speeds in the 10 GHz range. Opto-Electronics\" electronics side has made similar strides in recent years. To the far left of Figure 1 shows a Virtex-4 Field Programmable Gate Array (FPGA) development board. FPGAs allow easy creation of massively parallel digital logic components. The FPGA configuration is specified by a hardware description language, such as VHDL. This FPGA generates all 60 laser driver signals in our prototype device at the same time. A high-end FPGA (Virtex-6) can easily generate simultaneous driver drivers for laser detectors for VHDL 256 and 256."}, {"heading": "2.4 Spatial Light Modulators (SLMs)", "text": "The basic computational elements of most optical computers are SLMs. They are manufactured in a variety of shapes and technologies, from liquid crystals and digital microscopic mirrors to 35mm films. Although advances have been made in SLMs (for example, millions of pixel devices are common), it has not been as dramatic as with emitters and detectors. Low methods have slow refresh rates (or are unchangeable), and expensive devices are not much faster. The maximum speed for most of these devices is only about 1kHz. We have addressed the problem of thought / learning speed by developing a method that shifts learning into the fast signals and leaves the synaptic weights firmly in place. We will discuss this in more detail below. Physics tells us that light passing through a region on the SLM is attenuated (reduced) by an amount proportional to the grayscale of the region."}, {"heading": "2.5 The Stanford Matrix Multiplier", "text": "The design of the Stanford Optical Matrix Multiplier was motivated by the physics of the phenomena. However, slow thinking requires, for example, the speed and relative ease of summing up optical signals through cylindrical lenses. However, current mathematical models struggle to master some of the inherent features of the process, such as its hybrid analog / digital texture. Light from the laser field crosses a combination of lenses that divide each beam into a thin vertical line that falls on the SLM. The beams cover the width of the SLM as a series of vertical lines (only one is shown in Fig. 1). Each vertical line falls on horizontal segments that parallel (analog) multiply the matrix product by attenuating the light beams as they pass through the SLM. Next, light concentrates in the horizontal direction on an amplification model."}, {"heading": "2.6 Optical Encoding of Neural Signals", "text": "The method of encoding or displaying these signals is an important issue, especially in hardware-based systems. In biology, the signals are mainly in the form of pulses. We know that the average pulse rate is important for the behavior of the network. However, in a digital computer, these pulse rates are often represented by a floating-point number, variability of interpulse timing, and the activation history of neurons. Most artificial neural models only use pulse rates to encode neuronal activations. In a digital computer, these pulse rates are often represented by a floating-point number. In optical systems, this rate must be encoded in an analog signal. There are a variety of ways to optically encode activations. Brightness or intensity of the laser light is probably the fastest, but it requires relatively complex interface and drive electronics. The precision of the signal optical unit is limited to a certain number of bits (the most electrical signals) (the method is narrow, 12 to the most)."}, {"heading": "3 Fixed-Weight Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 What is it and how does it work?", "text": "The (fixed) synaptic weights of the FWL-NN implement a learning algorithm that adapts the recurring signals to their correct values. Instead of encoding a particular mapping, the synaptic weights of a FWL-NN encode how to learn any mapping (within a large, perhaps infinite set of possible mappings).Corrective feedback retains the value of the recurring signals during learning.Fixed-weight learning is somewhat analogous to biological memory, where information storage also includes a persistent pattern of activated neurons, which can be implemented through feedback. FWL-NNs can be implemented by many means, such as pure software, digital electronic hardware or analog electronic hardware. FWL-NNs are a particularly good complement to the capabilities provided by optical networks."}, {"heading": "NN Layers Neurons Synapses Encoding", "text": "In 1991, Cotter and Conwell [10] introduced the Fixed-Weight Learning Theorem and proved it: Given a neural network topology (which learns by changing weights) and the associated learning algorithm, an equivalent FWL-NN exists. Any image that can be learned by changing the weights of the original network can be learned from the FWL-NN without changing any synaptic weights. Note that the FWL-NN is necessarily recurring, even if the corresponding original change-weight network does not exist.The Fixed-Weight Learning Theorem is a proof of existence based on Universal Approximation. It has not specified how to generate the FWL-NN. Several methods have been used by us and other researchers [4] [5] [6] to derive the fixed synaptic weights. We have based on up-to-date optical hardware algorithms, designed several WL-NL based on the backup algorithms and propagation."}, {"heading": "3.2 Training the Fixed-Weights", "text": "As mentioned above, the synaptic weights in a FWL-NN encode a learning algorithm (e.g. back propagation). The (fixed) synaptic weights that implement the learning algorithm are derived from a process we call subnetwork method. Firstly, we break down the learning algorithm to be encoded (e.g. back propagation) into smaller subtasks. Secondly, we train small upstream subnetworks to perform these subtasks. (Some subnetworks are simple enough to design manually.) Finally, we integrate the subnetworks together to form the full FWL-NN. During the integration process, care must be taken to provide the correct signal delays for the various subnetworks. To this end, we performed the integration manually, which was time consuming and error prone."}, {"heading": "4 Hardware-Based Experimental Results", "text": "We have designed and tested several FWL-NNs that use almost any type of encoding method (intensity, pulse-based training), and it is the only method we can use to learn synaptic multiplications. However, synaptic multiplications are the vast majority of computations for a neural network, and they are usually the most problematic part of optical calculations.All of these networks are presented below the learning line, where the network is constantly adjusting its mapping information. However, this is not a limitation inherent in FWL-NN. Three of these networks are described in Table 1. The number of layers, neurons and synapses are given, and the encoding column indicates the way signal encoding is activated that the net credits are almost any encoding method."}, {"heading": "5 Building/Verifying Hypercomputational Hardware", "text": "The hardware described above is a mixture of analog and digital components, and we had a long-term interest in getting more computing power onto the optical path. [11] In the context of this hypercalculation, the realization of a true Universal Approximation Neural Network (UANN) could be successful. [12] However, this is real recursive hardware that contains noise, which ultimately limits precision. Although Siegelman [11] claims that linear precision is sufficient, noise could thwart the possibility of building a P / Poly calculator and limiting the machine to BPP / Log. [13] Noise could also prevent us from being able to produce a true UANN. In both cases, our expertise lies in building hardware. We seek collaboration with computer modelers to get help with several activities."}], "references": [{"title": "Optical Neural Computers", "author": ["Yaser S. Abu-Mostafa", "Demetri Psaltis"], "venue": "Scientific American ,Volume", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "Operational Parameters of an Opto-Electronic Neural Network Employing Fixed-Planar Holographic Interconnects", "author": ["P.E. Keller", "A.F"], "venue": "Gmitro", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "Parallel N4 weighted optical interconnections", "author": ["H. John Caulfield"], "venue": "Applied Optics.Vol. 26, Issue", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "Fixed-Weight Learning Neural Networks on Optical Hardware", "author": ["A. Steven Younger", "Emmett Redd"], "venue": "International Joint Conference on Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Learning at the Speed of Light: A New Type of Optical Neural Network", "author": ["A.S. Younger", "E. Redd"], "venue": "Optical Super Computing", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Adaptive Behavior with Fixed Weights in RNN: An Overview", "author": ["Danil V. Prokhorov", "Lee A. Feldkamp", "Ivan Yu", "Tyukin"], "venue": "International Joint Conference on Neural Networks", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Introduction to Fourier Optics", "author": ["Goodman", "Joseph W"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1988}, {"title": "FPGA-Based Stochastic Neural Networks Implementation", "author": ["B.L.S.L. Bade"], "venue": "IEEE FPGAs for Custom Computing Machines Workshop,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Learning algorithms and fixed dynamics", "author": ["N.E. Cotter", "P. R Conwell"], "venue": "Proceedings of the International Conference on Neural Networks 91,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1991}, {"title": "Neural Networks and Analog Computation: Beyond the Turing Limit", "author": ["Hava T. Siegelmann"], "venue": "Birkhauser. ISBN 0-8176-3949-7,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Neural Networks, Approximation Theory, and Finite Precision Computation", "author": ["Jonathon Wray", "Gary G.R"], "venue": "Green", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Neural and Super-Turing Computing", "author": ["Hava T. Siegelmann"], "venue": "Minds and Machines,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "[1][2][3] They were seen as possible solutions to the expected limits of Moore \u0301 s Law.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1][2][3] They were seen as possible solutions to the expected limits of Moore \u0301 s Law.", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "[1][2][3] They were seen as possible solutions to the expected limits of Moore \u0301 s Law.", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "[4][5][6] We expect that interest in optical computing and optical neural networks will increase, especially since the Moore \u0301 s Law limit seems to have been reached by conventional chip-making technology.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4][5][6] We expect that interest in optical computing and optical neural networks will increase, especially since the Moore \u0301 s Law limit seems to have been reached by conventional chip-making technology.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "[4][5][6] We expect that interest in optical computing and optical neural networks will increase, especially since the Moore \u0301 s Law limit seems to have been reached by conventional chip-making technology.", "startOffset": 6, "endOffset": 9}, {"referenceID": 6, "context": "The prototype is based on the relatively old concept of a Stanford Matrix Multiplier,[8] modified for efficient implementation of FWL-NNs (Fig.", "startOffset": 85, "endOffset": 88}, {"referenceID": 7, "context": "[9] When two statistically independent signals are combined with an AND operation, the", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "In 1991, Cotter and Conwell [10] presented and proved the Fixed-Weight Learning Theorem: Given a neural network topology (which learns by changing weights) and its attendant learning algorithm, there exists an equivalent FWL-NN.", "startOffset": 28, "endOffset": 32}, {"referenceID": 3, "context": "Several methods have been used by us and other researchers [4][5][6] to derive the fixed synaptic weights.", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "Several methods have been used by us and other researchers [4][5][6] to derive the fixed synaptic weights.", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "Several methods have been used by us and other researchers [4][5][6] to derive the fixed synaptic weights.", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "We designed and tested several FWL-NNs on an Optical Hardware Test Bench, described in [4][5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "We designed and tested several FWL-NNs on an Optical Hardware Test Bench, described in [4][5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 9, "context": "[11] Related to this hypercomputation may be the realization of a true Universal Approximation neural network (UANN).", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] However, this is real, physical hardware which contains noise that will ultimately limit precision.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Although Siegelman [11] claims linear precision suffices, noise may thwart being able to build a P/poly computation machine and limit the machine to BPP/log.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "[13] Noise may also thwart being able to make a true UANN.", "startOffset": 0, "endOffset": 4}], "year": 2010, "abstractText": null, "creator": "LaTeX with hyperref package"}}}