{"id": "1006.2743", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2010", "title": "Global Optimization for Value Function Approximation", "abstract": "Existing value function approximation methods have been successfully used in many applications, but they often lack useful a priori error bounds. We propose a new approximate bilinear programming formulation of value function approximation, which employs global optimization. The formulation provides strong a priori guarantees on both robust and expected policy loss by minimizing specific norms of the Bellman residual. Solving a bilinear program optimally is NP-hard, but this is unavoidable because the Bellman-residual minimization itself is NP-hard. We describe and analyze both optimal and approximate algorithms for solving bilinear programs. The analysis shows that this algorithm offers a convergent generalization of approximate policy iteration. We also briefly analyze the behavior of bilinear programming algorithms under incomplete samples. Finally, we demonstrate that the proposed approach can consistently minimize the Bellman residual on simple benchmark problems.", "histories": [["v1", "Mon, 14 Jun 2010 15:38:41 GMT  (550kb,D)", "http://arxiv.org/abs/1006.2743v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["marek petrik", "shlomo zilberstein"], "accepted": false, "id": "1006.2743"}, "pdf": {"name": "1006.2743.pdf", "metadata": {"source": "CRF", "title": "Global Optimization for Value Function Approximation Global Optimization for Value Function Approximation", "authors": ["Marek Petrik"], "emails": ["petrik@cs.umass.edu", "shlomo@cs.umass.edu"], "sections": [{"heading": null, "text": "Keywords: Value Function Approximation, Markov Decision Processes, Reinforcement Learning, Approximate Dynamic Programming"}, {"heading": "1. Motivation", "text": "It is widely accepted that large MDPs can only approximately be solved, but the commonly used alignment methods can be divided into three broad categories: 1) Political search, which explores a limited space of all policies; 2) Approximate dynamic programming, which seeks a limited space of value functions; and 3) Approximate linear programming, which approaches the solution with a linear program. While all these methods have achieved impressive results in many areas of application, they have significant limitations. Political search methods rely on local search in a limited political space. Politics can be presented, for example, as a finite-state controller (Stanley and Miikkulainen, 2004) or as a greedy policy related to an approximate value function (Szita and Lorincz, 2006)."}, {"heading": "2. Framework and Notation", "text": "This section formally defines the framework and notation we use. We also define Markov decision-making processes and the associated approximation errors. Markov decision-making processes come in many variants based on the objective function that is optimized. This work focuses on the infinite horizon of discounted MDPs, which are defined as a sequential function. Definition 1 (e.g. (Puterman, 2005): Markov decision-making process is a tuple (S, P, r, \u03b1, \u03b1). Here, S is a finite series of states, A is a finite series of actions, P: S \u00b7 A \u00b7 S 7 \u2192 [0, 1] is the transition function (P, a, s \u2032 s) is the probability of transferring to a state s \"given action a), and r: S \u00b7 A \u00b7 R + is a reward function. The initial distribution is: S 7 \u2192 S 7 \u2192 [0, 1], so that there is an finite action."}, {"heading": "3. Value Function Approximation", "text": "This section describes the basic methods for approximating values. The MDPs used in practical applications are often too large for the optimal policy to be calculated accurately. In these cases, we first calculate an approximate value function and then take the approximate value approximation in relation to it. The quality of such a policy can be characterised by its value function, which is v\u03c0 in one of the following two main modes. Definition 5 (policy loss) is a policy calculated from the approximation of the value function. \"The expected policy loss measures the expected loss of \u03c0, which is defined as follows:\" V \"-\" V \"-\" V \"-\" D \"-\" D \"-\" D. \"-\" - \"D\" - D \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - \"-\" - D. \"-\" - \"- D.\" - \"- D.\" - \"- D.\" - \"-\" - D. \"-\" - D. \"-\" - D. \"-\" - D. \"-\" - \"- D.\" - D. \"-\" - D. \"-\" - \"- D.\" - D. \"- D.\" - \"- D.\" - D. \"- D.\" - D. \"-\" - D. \"-\" - D. \"- D.\" - D. \"- D.\" - D. \"-\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - D. \"- D.\" - \"- D.\" - D. \"- D.\" - \"- D.\" - D. \"-\" - D. \"- D.\" - \"- D.\" - D. \"- D.\" - D. \"-\" - \"-\" - \"- D.\" - D. \"-\" - D. \"- D.\" - \"-\" - D. \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-"}, {"heading": "4. Bilinear Program Formulations", "text": "This section shows how to formulate value adjustment as a divisible bilinear program. In this section we refer only to three approximate programs. Bilinear programs are a generalization of linear programs with an additional bilinear term in the objective function. A divisible bilinear program consists of two linear programs with independent limitations and are relatively easy to solve and analyze. Definition 12 (divisible bilinear program) A divisible bilinear program in the normal form is defined as follows: min w, x y, zsT1w + rT2 y + s T 2 zs.t. A1x + B1w = b1 A2y + B2z = b2 w, x bilinear program 0 y, z 0 (BP-m) The objective of the bilinear program (BP-m) is called f (w, x, y, z). We separate the variables using a vertical line and the constraints using different columns to emphasize the invisible nature of the program."}, {"heading": "4.1 Robust Policy Loss", "text": "The solution of the robust bilinear program (ABP-L) meets the L standard of the Bellman residual function (Lv-Lv-L1). This minimization can be formulated as follows: \"All variables are vectors except\" L, \"which is a scalar. Matrix A represents constraints identical to the constraints in (ALP-L1). The variables correspond to all governmental pairs of shareholders. These variables represent the residuals that are minimized. This formulation provides the following guarantees. Theorem 13. Given the assumption that any optimal solution (ALP-L1) is the optimal solution."}, {"heading": "4.2 Expected Policy Loss", "text": "This section describes bilinear programs that reduce the expected political loss for a particular initial distribution. (...) The initial distribution can be used to derive narrower limits for political loss. (...) We describe two formulas. (...) They each minimize an L formula and a weighted L1 norm for the Bellman residual. The expected political loss can be minimized by solving the following bilinear formula. (...) This formulation is identical to the bilinear program (...). (...) B\u03c0 = 1 Av \u2212 b. (...) The expected political loss can be minimized by solving the following bilinear formula. (...)"}, {"heading": "4.3 Hybrid Formulation", "text": "It is easy to show that this standard represents the c-weighted L1 standard of the largest components of the vector. As such, it is more robust than the pure L1 standard, but not as sensitive as the L standard."}, {"heading": "5. Solving Bilinear Programs", "text": "This section describes methods for solving approximate bilinear programs until the solution becomes practicable. Bilinear programs can easily be assigned to other global optimization problems, such as the Mixed Integer Linear Programs (Horst and Tuy, 1996). We focus on a simple iterative algorithm for solving bilinear programs, which also serve as the basis for many optimal algorithms. Solving a bilinear program is a NP-complete problem (Bennett and Mangasarian, 1992). Membership in NP follows from the finite number of basic realizable solutions of individual linear programs, each of which can be verified in polynomial time. NP hardness is demonstrated by a reduction in the SAT problem. There are two essential approaches to solving bilinear programs, which solves a relativization of the bilinear program. Solving the relaxed problem represents a lower limit to the optimal solution."}, {"heading": "6. Sampling Guarantees", "text": "In most practical cases, the number of samples is too large to be enumerated explicitly. Although the value function is limited to be representable, the problem cannot be solved. The usual procedure is to capture a limited number of samples and then show how to use them. The simplest samples are defined as the following. One-step simple samples are defined as: The simple samples are considered guarantees of solution quality with incomplete samples.The simplest samples are defined as: The simple samples are defined as: the number of samples (s, a), (s1), the complete distribution of samples."}, {"heading": "7. Discussion and Related ADP Methods", "text": "These sections describe the relationships between the use of approximately bilinear programs and two closely related approaches that differ in terms of the appropriation problems of individual countries. < < < < < < < < # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "8. Experimental Results", "text": "In this section, we validate the approach by applying it to simple amplification learning benchmark problems. Emphasis is placed on the theoretical properties and the experiments are deliberately designed to avoid interactions between the approach to formulation and the approximate solution methods. As Theorem 34 shows, OAPI, the very simple approximate algorithm for ABP, can also be factored in just like existing methods on MDPs.ABP is an off-policy approach method, such as LSPI (Lagoudakis and Parr, 2003) or ALP. This means that the samples can be collected independently of the control policy. It is necessary that several measures are sampled for each state to allow the selection of different policys.First, we demonstrate and analyze the characteristics of ABP on a simple chain of problems with 200 states where the transitions move to the right or left (2 actions) by a step with a centered Gaussian noise standard deviation 3."}, {"heading": "9. Conclusion and Future Work", "text": "ABP provides the optimal approximation of Bellman residual limits, although formulated in terms of transitive-realizable value functions. We also showed that there is no asymptotically simpler formulation, since finding the closest value function and solving a bilinear program are both NP-complete problems. Finally, the formulation leads to the development of OAPI, a new convergent form of API that monotonously improves the objective value function. While we discussed only approximate solutions of ABP, a deeper study of bilinear solvers can make optimal solutions feasible. ABPs have a small number of essential variables (which determine the value function) and a large number of limitations that can be used by the solvers (Petrik and Zilberstein, 2007). L-error-bound understanding provides good formulations, but it guarantees that a similar formulation in practice can result in minimization."}, {"heading": "Appendix A. Proofs", "text": "The basic properties of the Bellman operator, which we often use without reference, are the following. Let P be a stochastic matrix. Let P be a stochastic matrix. Then, both the linear operators P and (I \u2212 \u03b3P) \u2212 1 are monotonous: x \u00b2 Py x \u00b2 y \u00b2 (I \u2212 \u03b3P) \u2212 1y for all x and y.Lemma 4. Transitive feasibility functions form an optimal value function. If v \u00b2 K () is a -transitive value function, it is a -transitive value function, thenv \u00b2 v \u00b2. Let P \u00b2 and r \u00b2 I form the transition matrix and the reward of politics."}], "references": [{"title": "Learning vehicular dynamics, with application to modeling helicopters", "author": ["Pieter Abbeel", "Varun Ganapathi", "Andrew Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Abbeel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2006}, {"title": "A price-directed approach to stochastic inventory/routing", "author": ["Daniel Adelman"], "venue": "Operations Research,", "citeRegEx": "Adelman.,? \\Q2004\\E", "shortCiteRegEx": "Adelman.", "year": 2004}, {"title": "Dynamic Programming", "author": ["Richard Bellman"], "venue": null, "citeRegEx": "Bellman.,? \\Q1957\\E", "shortCiteRegEx": "Bellman.", "year": 1957}, {"title": "Bilinear separation of two sets in n-space", "author": ["Kristin P. Bennett", "Olvi L. Mangasarian"], "venue": "Technical report,", "citeRegEx": "Bennett and Mangasarian.,? \\Q1992\\E", "shortCiteRegEx": "Bennett and Mangasarian.", "year": 1992}, {"title": "Temporal differences-based policy iteration and applications in neuro-dynamic programming", "author": ["Dimitri P. Bertsekas", "Sergey Ioffe"], "venue": "Technical Report LIDS-P-2349,", "citeRegEx": "Bertsekas and Ioffe.,? \\Q1997\\E", "shortCiteRegEx": "Bertsekas and Ioffe.", "year": 1997}, {"title": "Bidimensional packing by bilinear programming", "author": ["Alberto Carpara", "Michele Monaci"], "venue": "Mathematical Programming Series A,", "citeRegEx": "Carpara and Monaci.,? \\Q2009\\E", "shortCiteRegEx": "Carpara and Monaci.", "year": 2009}, {"title": "The Linear Programming Approach to Approximate Dynamic Programming: Theory and Application", "author": ["Daniela P. de Farias"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "Farias.,? \\Q2002\\E", "shortCiteRegEx": "Farias.", "year": 2002}, {"title": "The linear programming approach to approximate dynamic programming", "author": ["Daniela P. de Farias", "Ben van Roy"], "venue": "Operations Research,", "citeRegEx": "Farias and Roy.,? \\Q2003\\E", "shortCiteRegEx": "Farias and Roy.", "year": 2003}, {"title": "On constraint sampling in the linear programming approach to approximate dynamic programming", "author": ["Daniela Pucci de Farias", "Benjamin van Roy"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Farias and Roy.,? \\Q2004\\E", "shortCiteRegEx": "Farias and Roy.", "year": 2004}, {"title": "Efficient solution algorithms for factored MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Ronald Parr", "Shobha Venkataraman"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Guestrin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2003}, {"title": "Global optimization: Deterministic approaches", "author": ["Reiner Horst", "Hoang Tuy"], "venue": null, "citeRegEx": "Horst and Tuy.,? \\Q1996\\E", "shortCiteRegEx": "Horst and Tuy.", "year": 1996}, {"title": "Least-squares policy iteration", "author": ["Michail G. Lagoudakis", "Ronald Parr"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lagoudakis and Parr.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr.", "year": 2003}, {"title": "The linear complementarity problem as a separable bilinear program", "author": ["Olvi L. Mangasarian"], "venue": "Journal of Global Optimization,", "citeRegEx": "Mangasarian.,? \\Q1995\\E", "shortCiteRegEx": "Mangasarian.", "year": 1995}, {"title": "Error bounds for approximate policy iteration", "author": ["Remi Munos"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Munos.,? \\Q2003\\E", "shortCiteRegEx": "Munos.", "year": 2003}, {"title": "Anytime coordination using separable bilinear programs", "author": ["Marek Petrik", "Shlomo Zilberstein"], "venue": "In Conference on Artificial Intelligence,", "citeRegEx": "Petrik and Zilberstein.,? \\Q2007\\E", "shortCiteRegEx": "Petrik and Zilberstein.", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Lorincz, 2006) and helicopter control (Abbeel et al., 2006).", "startOffset": 38, "endOffset": 59}, {"referenceID": 4, "context": "Approximate dynamic programming (ADP) methods iteratively approximate the value function (Bertsekas and Ioffe, 1997; Powell, 2007; Sutton and Barto, 1998).", "startOffset": 89, "endOffset": 154}, {"referenceID": 4, "context": "The approximation error bounds are usually expressed in terms of the worst-case approximation of the value function over all policies (Bertsekas and Ioffe, 1997).", "startOffset": 134, "endOffset": 161}, {"referenceID": 13, "context": "While there exist some L2-based bounds (Munos, 2003), they require values that are difficult to obtain.", "startOffset": 39, "endOffset": 52}, {"referenceID": 1, "context": "ALP has been previously used in a wide variety of settings (Adelman, 2004; de Farias and van Roy, 2004; Guestrin et al., 2003).", "startOffset": 59, "endOffset": 126}, {"referenceID": 9, "context": "ALP has been previously used in a wide variety of settings (Adelman, 2004; de Farias and van Roy, 2004; Guestrin et al., 2003).", "startOffset": 59, "endOffset": 126}, {"referenceID": 4, "context": "Many methods that compute a value function based on a given set of features have been developed, such as neural networks and genetic algorithms (Bertsekas and Ioffe, 1997).", "startOffset": 144, "endOffset": 171}, {"referenceID": 11, "context": "The two most commonly used methods \u2014 Bellman residual approximation and least-squares approximation (Lagoudakis and Parr, 2003) \u2014 minimize the L2 norm of the Bellman residual.", "startOffset": 100, "endOffset": 127}, {"referenceID": 2, "context": "The ideas of approximate value iteration could be traced to Bellman (1957), which was followed by many additional research efforts (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Powell, 2007).", "startOffset": 60, "endOffset": 75}, {"referenceID": 10, "context": "Bilinear programs can be easily mapped to other global optimization problems, such as mixed integer linear programs (Horst and Tuy, 1996).", "startOffset": 116, "endOffset": 137}, {"referenceID": 3, "context": "Solving a bilinear program is an NP-complete problem (Bennett and Mangasarian, 1992).", "startOffset": 53, "endOffset": 84}, {"referenceID": 5, "context": "The relaxation of the bilinear program is typically either a linear or semi-definite program (Carpara and Monaci, 2009).", "startOffset": 93, "endOffset": 119}, {"referenceID": 10, "context": "The bilinear program formulation is then refined \u2014 using concavity cuts (Horst and Tuy, 1996) \u2014 to eliminate previously computed feasible solutions and solved again.", "startOffset": 72, "endOffset": 93}, {"referenceID": 12, "context": "While algorithm 2 is not guaranteed to find an optimal solution, its empirical performance is often remarkably good (Mangasarian, 1995).", "startOffset": 116, "endOffset": 135}, {"referenceID": 3, "context": "(Bennett and Mangasarian, 1992)).", "startOffset": 0, "endOffset": 31}, {"referenceID": 9, "context": "Discussion and Related ADP Methods This section describes connections between approximate bilinear programming and two closely related approximate dynamic programming methods: ALP, and L\u221e-API, which are commonly used to solve factored MDPs (Guestrin et al., 2003).", "startOffset": 240, "endOffset": 263}, {"referenceID": 9, "context": "Approximate bilinear programming can also improve on API with L\u221e minimization (L\u221e-API for short), which is a leading method for solving factored MDPs (Guestrin et al., 2003).", "startOffset": 150, "endOffset": 173}, {"referenceID": 9, "context": "Minimizing the L\u221e approximation error is theoretically preferable, since it is compatible with the existing bounds on policy loss (Guestrin et al., 2003).", "startOffset": 130, "endOffset": 153}, {"referenceID": 13, "context": "The bounds on value function approximation in API are typically (Munos, 2003):", "startOffset": 64, "endOffset": 77}, {"referenceID": 9, "context": "Theorem 34 also explains why API provides better solutions than ALP, as observed in (Guestrin et al., 2003).", "startOffset": 84, "endOffset": 107}, {"referenceID": 11, "context": "ABP is an off-policy approximation method, like LSPI (Lagoudakis and Parr, 2003) or ALP.", "startOffset": 53, "endOffset": 80}, {"referenceID": 14, "context": "ABPs have a small number of essential variables (that determine the value function) and a large number of constraints, which can be leveraged by the solvers (Petrik and Zilberstein, 2007).", "startOffset": 157, "endOffset": 187}], "year": 2010, "abstractText": "Existing value function approximation methods have been successfully used in many applications, but they often lack useful a priori error bounds. We propose a new approximate bilinear programming formulation of value function approximation, which employs global optimization. The formulation provides strong a priori guarantees on both robust and expected policy loss by minimizing specific norms of the Bellman residual. Solving a bilinear program optimally is NP-hard, but this is unavoidable because the Bellman-residual minimization itself is NP-hard. We describe and analyze both optimal and approximate algorithms for solving bilinear programs. The analysis shows that this algorithm offers a convergent generalization of approximate policy iteration. We also briefly analyze the behavior of bilinear programming algorithms under incomplete samples. Finally, we demonstrate that the proposed approach can consistently minimize the Bellman residual on simple benchmark problems.", "creator": "LaTeX with hyperref package"}}}