{"id": "1007.1282", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jul-2010", "title": "A note on sample complexity of learning binary output neural networks under fixed input distributions", "abstract": "We show that the learning sample complexity of a sigmoidal neural network constructed by Sontag (1992) required to achieve a given misclassification error under a fixed purely atomic distribution can grow arbitrarily fast: for any prescribed rate of growth there is an input distribution having this rate as the sample complexity, and the bound is asymptotically tight. The rate can be superexponential, a non-recursive function, etc. We further observe that Sontag's ANN is not Glivenko-Cantelli under any input distribution having a non-atomic part.", "histories": [["v1", "Thu, 8 Jul 2010 03:58:25 GMT  (72kb)", "http://arxiv.org/abs/1007.1282v1", "6 pages, latex in IEEE conference proceedings format"]], "COMMENTS": "6 pages, latex in IEEE conference proceedings format", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vladimir pestov"], "accepted": false, "id": "1007.1282"}, "pdf": {"name": "1007.1282.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["vpest283@uottawa.ca"], "sections": [{"heading": null, "text": "The question is whether the first part of the open problem 12.6 from the book of Vidyasagar is a complex problem [11] (this problem already appears in the original version of 1997). \"How can we reconcile the fact that in distributional-free learning every learnable concept class can also be learned\" polynomically, \"whereas this could not be the case in the fixed distribution class. [...] There are only two possibilities: 1. C has an infinite VC dimension, in which C is not learnable at all. 2. C has a limited VC dimension, in which C is not learnable."}, {"heading": "II. GLIVENKO\u2013CANTELLI CLASSES AND LEARNABILITY", "text": "A. PAC Learnability and total boundednessBenedek and Itai [2] had proven that a concept class C is learnable under a single probability distribution \u00b5 if and only if C is totally bounded in the L1 (\u00b5) -distance. Here we remember their absence. Theorem 2.1 (Theorem 4.8 in [2]; Theorem 6.3 in [11]: Suppose C is a concept class, \u03b5 > 0, and that B1,., Bk is an \u03b5 / 2 cover for C. Then the minimal empirical risk algorithm is PAC in terms of accuracy. In particular, the example complexity of PAC Learning C to accuracy."}, {"heading": "III. ALL RATES OF SAMPLE COMPLEXITY ARE POSSIBLE", "text": "Theorem 3.1: Let C be a concept class which shatters every finite subset of some infinite set =. Let (\u03b5k), \u03b5k \u2193 0 be a sequence of positive reals converting to zero, and let f: R + be a non-decreasing function growing at least linearly: f (x) = \u03b5k (x). Then there is a probability measurement \u00b5 = \u00b5 (\u03b5k), f) on the input domain. (2) Furthermore, the above estimate is essentially narrow in the sense that the sample complexityn (\u03b5k) = O (f (1\u03b5k) + log."}, {"heading": "IV. CONCLUSION", "text": "Prompted by a question embedded in the problem 12.6 of Vidyasagar [11], we have shown that all growth rates of the sample compleixity are possible for distribution-dependent learning, especially all through Sontag's binary output feed forward sigmoidal neuronal network. Now, Vidyasagar continues as follows: \"I would like to have an\" intrinsic \"explanation for why, in non-distribution-learning, every learnable concept class is also forced to be polynomically learnable. Next, how far can this line of argumentation be\" advanced \"? Suppose P is a family of probabilities that contains a bullet in the total metric capability of variation. From Theorem 8.8, it follows that every concept class that can be learnt in relation to P-learnable must also be polynomically learnable to the subject (because C must have a finite VCdimension). Is it possible to identify other such classes of probabilities to the test subjects? The following problem is the following from our point of view: the problem is the correct for the conditions in front of the test subjects."}, {"heading": "ACKNOWLEDGMENTS", "text": "The author is grateful to the anonymous arbitrators, in particular for the hint [5], [6] and Ilijas Farah for the hint [4]."}], "references": [{"title": "A sufficient condition for polynomial distribution-dependent learnability", "author": ["M. Anthony", "J. Shawe-Taylor"], "venue": "Discrete Applied Math. 77 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Learnability with respect to fixed distributions", "author": ["G.M. Benedek", "A. Itai"], "venue": "Theor. Comp. Sci. 86 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1991}, {"title": "Uniform Central Limit Theorems", "author": ["R.M. Dudley"], "venue": "Cambridge Studies in Advanced Mathematics, 63, Cambridge University Press, Cambridge ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1999}, {"title": "Two measure properties of Cartesian product sets", "author": ["H.G. Eggleston"], "venue": "Quart. J. Math. Oxford (2) 5 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1954}, {"title": "Learning with Recurrent Neural Networks", "author": ["B. Hammer"], "venue": "Dissertation, Universit\u00e4t Osnabr\u00fcck", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "On the learnability of recursive data", "author": ["B. Hammer"], "venue": "Mathematics of Control Signals and Systems, 12 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "PAC learnability of a concept class under nonatomic measures: a problem by Vidyasagar", "author": ["V. Pestov"], "venue": "to appear in Proc. 21st Conf. on Algorithmic Learning Theory ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Feedforward nets for interpolation and classification", "author": ["E.D. Sontag"], "venue": "J. Comp. Systems Sci 45(1) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1992}, {"title": "The Glivenko\u2013Cantelli problem", "author": ["M. Talagrand"], "venue": "Ann. Probab. 15 ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1987}, {"title": "The Glivenko-Cantelli problem", "author": ["M. Talagrand"], "venue": "ten years later, J. Theoret. Probab. 9 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning and Generalization", "author": ["M. Vidyasagar"], "venue": "with Applications to Neural Networks, 2nd Ed., Springer-Verlag", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 10, "context": "6 from Vidyasagar\u2019s book [11] (this problem appears already in the original 1997 version).", "startOffset": 25, "endOffset": 29}, {"referenceID": 1, "context": "\u201d In fact, the existence of a concept class whose sample complexity grows exponentially in 1/\u03b5 under a given fixed input distribution was already shown in 1991 by Benedek and Itai [2] (Theorem 3.", "startOffset": 180, "endOffset": 183}, {"referenceID": 4, "context": "thesis [5] (Example 4.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "also [6].", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "For example, a wellknown sigmoidal feed-forward neural network of infinite VC dimension constructed by Sontag [8] has this property.", "startOffset": 110, "endOffset": 113}, {"referenceID": 8, "context": "It follows from Talagrand\u2019s theory of witness of irregularity [9], [10] that N is not Glivenko\u2013Cantelli with regard to any measure having a non-atomic part.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": "It follows from Talagrand\u2019s theory of witness of irregularity [9], [10] that N is not Glivenko\u2013Cantelli with regard to any measure having a non-atomic part.", "startOffset": 67, "endOffset": 71}, {"referenceID": 1, "context": "Benedek and Itai [2] had proved that a concept class C is PAC learnable under a single probability distribution \u03bc if and only if C is totally bounded in the L(\u03bc)-distance.", "startOffset": 17, "endOffset": 20}, {"referenceID": 1, "context": "8 in [2]; Theorem 6.", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "3 in [11]): Suppose C is a concept class, \u03b5 > 0, and that B1, .", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "2 ([11], Lemma 7.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "8 in [2]; Theorem 6.", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "6 in [11]): Suppose C is a given concept class, and let \u03b5 > 0 be specified.", "startOffset": 5, "endOffset": 9}, {"referenceID": 10, "context": "For the most comprehensive presentation of PAC learnability under a single distribution, see [11], Ch.", "startOffset": 93, "endOffset": 97}, {"referenceID": 2, "context": "A function class F on a domain (a standard Borel space) \u03a9 is Glivenko\u2013Cantelli with regard to a probability distribution \u03bc ([3], Ch.", "startOffset": 124, "endOffset": 127}, {"referenceID": 10, "context": "3), or else has the property of uniform convergence of empirical means (UCEM property) [11], if for each \u03b5 > 0", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "We find it instructive to give a different proof, replying in passing to a remark of Vidyasagar [11], p.", "startOffset": 96, "endOffset": 100}, {"referenceID": 9, "context": "The answer is yes, as is (implicitely) stated in [10] (p.", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "Talagrand [9], [10] had characterized uniform Glivenko\u2013 Cantelli function classes with regard to a single distribution in terms of shattering.", "startOffset": 10, "endOffset": 13}, {"referenceID": 9, "context": "Talagrand [9], [10] had characterized uniform Glivenko\u2013 Cantelli function classes with regard to a single distribution in terms of shattering.", "startOffset": 15, "endOffset": 19}, {"referenceID": 8, "context": "5 (Talagrand [9], Th.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "The corollary is easy to prove directly, without using subtle results of Talagrand, and the result was observed (independently) in 1991 and investigated in detail by Benedek and Itai ([2], Theorem 3.", "startOffset": 184, "endOffset": 187}, {"referenceID": 7, "context": "Figure 1 recalls a well-known example of a sigmoidal neural network N constructed by Sontag [8], pp.", "startOffset": 92, "endOffset": 95}, {"referenceID": 10, "context": "also [11], page 389, where the top diagram in Figure 1 is borrowed from.", "startOffset": 5, "endOffset": 9}, {"referenceID": 7, "context": "7 ([8], pp.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "This is a consequence of Eggleston\u2019s theorem [4]: If A is a measurable, Lebesgue-positive subset of the unit square, then there is a measurable positive set B and a perfect set C such", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "[2], p.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "385, note (2), or [11], p.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "6 in [11], p.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "This is the set left of the closed unit interval [0, 1] after first deleting the middle third (1/3, 2/3), then deleting the middle thirds of the two remaining intervals, (1/9, 2/9) and (7/9, 8/9), and continuing to delete the middle thirds ad infimum.", "startOffset": 49, "endOffset": 55}, {"referenceID": 6, "context": "2 exactly as it was done in [7], proof of Theorem 3, in order to conclude that C is not totally bounded in the L(\u03bd)-distance.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "However, by replacing the domain \u03a9 with \u03a9 \u00d7 [0, 1], every concept C \u2208 C with C \u00d7 [0, 1], and \u03bc with the product \u03bc\u2297 \u03bb, where \u03bb is the uniform (Lebesgue) measure on the interval, one can \u201ctranslate\u201d every example as above into an example of learning under a non-atomic probability distribution.", "startOffset": 44, "endOffset": 50}, {"referenceID": 0, "context": "However, by replacing the domain \u03a9 with \u03a9 \u00d7 [0, 1], every concept C \u2208 C with C \u00d7 [0, 1], and \u03bc with the product \u03bc\u2297 \u03bb, where \u03bb is the uniform (Lebesgue) measure on the interval, one can \u201ctranslate\u201d every example as above into an example of learning under a non-atomic probability distribution.", "startOffset": 81, "endOffset": 87}, {"referenceID": 10, "context": "6 of Vidyasagar [11], we have shown that all rates of sample compleixity growth are possible for distributiondependent learning, in particular all are realized by binary output feed-forward sigmoidal neural network of Sontag.", "startOffset": 16, "endOffset": 20}, {"referenceID": 1, "context": "Some sufficient conditions can be found in [2], [1], but none of them is also necessary.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "Some sufficient conditions can be found in [2], [1], but none of them is also necessary.", "startOffset": 48, "endOffset": 51}, {"referenceID": 8, "context": "The \u201cright\u201d condition will be strictly intermediate between the witness of irregularity [9], [10] and the VC dimension modulo countable sets [7].", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "The \u201cright\u201d condition will be strictly intermediate between the witness of irregularity [9], [10] and the VC dimension modulo countable sets [7].", "startOffset": 93, "endOffset": 97}, {"referenceID": 6, "context": "The \u201cright\u201d condition will be strictly intermediate between the witness of irregularity [9], [10] and the VC dimension modulo countable sets [7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "The author is grateful to the anonymous referees, in particular for pointing out the references [5], [6], and to Ilijas Farah for pointing out the reference [4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "The author is grateful to the anonymous referees, in particular for pointing out the references [5], [6], and to Ilijas Farah for pointing out the reference [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 3, "context": "The author is grateful to the anonymous referees, in particular for pointing out the references [5], [6], and to Ilijas Farah for pointing out the reference [4].", "startOffset": 157, "endOffset": 160}], "year": 2013, "abstractText": "We show that the learning sample complexity of a sigmoidal neural network constructed by Sontag (1992) required to achieve a given misclassification error under a fixed purely atomic distribution can grow arbitrarily fast: for any prescribed rate of growth there is an input distribution having this rate as the sample complexity, and the bound is asymptotically tight. The rate can be superexponential, a nonrecursive function, etc. We further observe that Sontag\u2019s ANN is not Glivenko\u2013Cantelli under any input distribution having a non-atomic part. Keywords-PAC learnability, fixed distribution learning, sample complexity, infinite VC dimension, witness of irregularity, Sontag\u2019s ANN, precompactness.", "creator": "LaTeX with hyperref package"}}}