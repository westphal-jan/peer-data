{"id": "1007.1766", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jul-2010", "title": "An svm multiclassifier approach to land cover mapping", "abstract": "From the advent of the application of satellite imagery to land cover mapping, one of the growing areas of research interest has been in the area of image classification. Image classifiers are algorithms used to extract land cover information from satellite imagery. Most of the initial research has focussed on the development and application of algorithms to better existing and emerging classifiers. In this paper, a paradigm shift is proposed whereby a committee of classifiers is used to determine the final classification output. Two of the key components of an ensemble system are that there should be diversity among the classifiers and that there should be a mechanism through which the results are combined. In this paper, the members of the ensemble system include: Linear SVM, Gaussian SVM and Quadratic SVM. The final output was determined through a simple majority vote of the individual classifiers. From the results obtained it was observed that the final derived map generated by an ensemble system can potentially improve on the results derived from the individual classifiers making up the ensemble system. The ensemble system classification accuracy was, in this case, better than the linear and quadratic SVM result. It was however less than that of the RBF SVM. Areas for further research could focus on improving the diversity of the ensemble system used in this research.", "histories": [["v1", "Sun, 11 Jul 2010 09:36:07 GMT  (729kb)", "http://arxiv.org/abs/1007.1766v1", "ASPRS 2008 Annual Conference Portland, Oregon. April 28 - May 2, 2008"]], "COMMENTS": "ASPRS 2008 Annual Conference Portland, Oregon. April 28 - May 2, 2008", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gidudu anthony", "hulley gregg", "marwala tshilidzi"], "accepted": false, "id": "1007.1766"}, "pdf": {"name": "1007.1766.pdf", "metadata": {"source": "CRF", "title": "AN SVM MULTICLASSIFIER APPROACH TO LAND COVER MAPPING", "authors": [], "emails": ["Anthony.Gidudu@wits.ac.za,", "greghul@icon.co.za,", "Tshilidzi.Marwala@wits.ac.za"], "sections": [{"heading": null, "text": "ASPRS 2008 Annual Conference Portland, Oregon \u2666 April 28 - May 2, 2008"}, {"heading": "INTRODUCTION", "text": "It is one of the means by which land coverage classes can be extracted from various disciplines such as easterliness, precision, disaster management, etc., is an operational principle through the use of algorithms called operation classifiers. Image classifiers can be used in areas where the different classes are directly searched for adequate boundaries between them (Keuchel, 2003) This is different from traditional classifiers such as the maximum probability and minimum distance from the resources classifiers, which fall under the category of parametric classifiers. Interest in researching new and emerging classifiers comes from the importance of land coverage information on various disciplines such as forestry, precision, disaster management, disaster management, disaster management, disaster management, disaster management, disaster management, disaster management, etc."}, {"heading": "ENSEMBLE CLASSIFIERS", "text": "The idea behind the ensemble systems is that the final classification result depends on a pool of classifiers. Importantly, for the creation of an ensemble system, each individual classifier must be unique in the way it creates decision boundaries (Polikar, 2006). The term used in ensemble systems is that there must be diversity in the ensemble system. The rational basis for ensuring diversity is that each classifier makes a different mistake, and that the strategic combination of these classifiers can reduce the overall error (Parikh and Polikar, 2007). Another central aspect of ensemble systems is the use of different data sets to train different classifiers, the use of different training parameters for different classifiers, the use of different types of classifiers, and the use of different characteristics for the different classifiers (Polikar, 2006)."}, {"heading": "METHODOLOGY", "text": "The study area for this research was taken from a Landsat scene from 2001 (series 171 and series 60) and relates to Kampala, the capital of Uganda. Interested land cover classes were water, built-up areas, mental swamps, light swamps and other vegetation. IDRISI Andes were used for preliminary processing such as identification of training samples and then exported to MATLAB (version 7) for SVM classification. Content of the ensemble system was: linear, RBF and a square SVM. Since the decision limits for these classifiers are different, it was assumed that they would provide the required diversity. Final classification results were determined by a majority decision of each classifier. Classification results were imported into IDRISI to perform geo referencing, GIS integration, derivation of land cover maps and accuracy assessments.ASPRS 2008 Annual Portland Conference Oregon, April 28 - May 2, 2008"}, {"heading": "RESULTS, DISCUSSIONS AND CONCLUSIONS", "text": "In the presentation of these results, a comparison is made between the derived maps of the individual SVMs and the ensemble system. Figure 2 shows the derived map after using the linear classifier. Similar land cover maps were derived for the RBF and the square SVMs and are shown in Figure 3 and 4 respectively. The most important observation in this map is the presence of mixed pixels characteristic of linear SVM classifiers. Second, the derived map from the ensemble system is less susceptible to mixed pixels. Figure 5 shows the result of the ensemble system. From the derived maps, it appears that the ensemble system has considerably increased the visual appeal of the linear SVM. Second, the derived map from the ensemble system resembles the RBF and the square SVM. To further estimate these results, error matrices of the total cap were calculated."}], "references": [{"title": "An Introduction to Support Vector Machines: And Other Kernel-based", "author": ["N. Christianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "Christianini and Shawe.Taylor,? \\Q2000\\E", "shortCiteRegEx": "Christianini and Shawe.Taylor", "year": 2000}, {"title": "Image classification using SVMs: One-against-one vs one-againstall", "author": ["A. Gidudu", "G. Hulley", "T. Marwala"], "venue": "In Proceedings of the 28 Asian Conference of Remote Sensing, Kuala Lumpur, Malaysia,", "citeRegEx": "Gidudu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gidudu et al\\.", "year": 2007}, {"title": "An assessment of support vector machines for land cover", "author": ["C. Huang", "L.S. Davis", "J.R.G. Townshed"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2002}, {"title": "Text categorization with support vector machines\u2014learning with many relevant features", "author": ["T. Joachims"], "venue": "In Proceedings of the 10 European Conference on Machine Learning,", "citeRegEx": "Joachims,? \\Q1998\\E", "shortCiteRegEx": "Joachims", "year": 1998}, {"title": "Automatic land cover analysis for Tenerife", "author": ["J. Keuchela", "S. Naumanna", "M. Heilera", "A. Siegmund"], "venue": null, "citeRegEx": "Keuchela et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Keuchela et al\\.", "year": 2003}, {"title": "Support vector classifiers for land cover classification, In Proceedings of the 6 Annual International Conference, Map India", "author": ["Mahesh P", "P.M. Mather"], "venue": "New Delhi, India,", "citeRegEx": "P. and Mather,? \\Q2003\\E", "shortCiteRegEx": "P. and Mather", "year": 2003}, {"title": "An ensemble-based incremental learning approach to data", "author": ["D. Parikh", "R. Polikar"], "venue": null, "citeRegEx": "Parikh and Polikar,? \\Q2007\\E", "shortCiteRegEx": "Parikh and Polikar", "year": 2007}, {"title": "Ensemble based systems in decision making, IEEE Circuits and Systems Magazine, pp", "author": ["R. Polikar"], "venue": null, "citeRegEx": "Polikar,? \\Q2006\\E", "shortCiteRegEx": "Polikar", "year": 2006}, {"title": "The Nature of Statistical Learning Theory, New York: Springer-Verlag", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1995\\E", "shortCiteRegEx": "Vapnik", "year": 1995}], "referenceMentions": [{"referenceID": 8, "context": "Having started off as a Statistical Learning Theory (Vapnik, 1995), SVMs have continued to be used in machine vision fields such as character, handwriting digit and text recognition (Vapnik, 1995; Joachims, 1998).", "startOffset": 52, "endOffset": 66}, {"referenceID": 8, "context": "Having started off as a Statistical Learning Theory (Vapnik, 1995), SVMs have continued to be used in machine vision fields such as character, handwriting digit and text recognition (Vapnik, 1995; Joachims, 1998).", "startOffset": 182, "endOffset": 212}, {"referenceID": 3, "context": "Having started off as a Statistical Learning Theory (Vapnik, 1995), SVMs have continued to be used in machine vision fields such as character, handwriting digit and text recognition (Vapnik, 1995; Joachims, 1998).", "startOffset": 182, "endOffset": 212}, {"referenceID": 8, "context": "SVMs control this through the principle of Structural Risk Minimization (Vapnik, 1995).", "startOffset": 72, "endOffset": 86}, {"referenceID": 3, "context": "Having started off as a Statistical Learning Theory (Vapnik, 1995), SVMs have continued to be used in machine vision fields such as character, handwriting digit and text recognition (Vapnik, 1995; Joachims, 1998). More recently, their application to land cover mapping has been vigorously explored (Huang et al, 2002; Mahesh and Mather, 2003, Gidudu et al, 2007). Like other supervised classifiers, training data is a prerequisite to define the decision boundaries within the feature space, based upon which classification decision rules are made. For SVMs, this decision boundary is a linear discriminant placed midway between the classes of interest. Unfortunately, land cover classes when projected to the input space are rarely linearly separable. SVMs handle such datasets by nonlinearly projecting the training data in the input space to a feature space of higher (infinite) dimension by use of a kernel function. This results in the previously nonlinear datasets becoming linearly separable. Placing a linear discriminant in this high (infinite) dimension will be equivalent to placing a non linear discriminant in the previous input space. Some examples of functions (also called kernels) used to this effect include: polynomial, gaussian (more commonly referred to as radial basis functions) and sigmoid functions. Each function has parameters which have to be determined prior to classification and they are usually determined through a cross validation process. Operating in high dimension potentially renders the risk of overfitting in the input space possible. SVMs control this through the principle of Structural Risk Minimization (Vapnik, 1995). The empirical risk of misclassification is controlled by maximizing the margin between the training data and the decision boundary (Mashao, 2004). In practice this criterion is softened to the minimization of a cost factor involving both the complexity of the classifier and the degree to which marginal points are misclassified, and the tradeoff between these factors is managed through a margin of error parameter (usually designated C). Like the respective function parameters, this C parameter is tuned through cross-validation procedures (Mashao, 2004). Some of the classical literature relating to SVMs can be found in Vapnik (1995), Campbell (2000) and Christianini (2002).", "startOffset": 197, "endOffset": 2301}, {"referenceID": 3, "context": "Having started off as a Statistical Learning Theory (Vapnik, 1995), SVMs have continued to be used in machine vision fields such as character, handwriting digit and text recognition (Vapnik, 1995; Joachims, 1998). More recently, their application to land cover mapping has been vigorously explored (Huang et al, 2002; Mahesh and Mather, 2003, Gidudu et al, 2007). Like other supervised classifiers, training data is a prerequisite to define the decision boundaries within the feature space, based upon which classification decision rules are made. For SVMs, this decision boundary is a linear discriminant placed midway between the classes of interest. Unfortunately, land cover classes when projected to the input space are rarely linearly separable. SVMs handle such datasets by nonlinearly projecting the training data in the input space to a feature space of higher (infinite) dimension by use of a kernel function. This results in the previously nonlinear datasets becoming linearly separable. Placing a linear discriminant in this high (infinite) dimension will be equivalent to placing a non linear discriminant in the previous input space. Some examples of functions (also called kernels) used to this effect include: polynomial, gaussian (more commonly referred to as radial basis functions) and sigmoid functions. Each function has parameters which have to be determined prior to classification and they are usually determined through a cross validation process. Operating in high dimension potentially renders the risk of overfitting in the input space possible. SVMs control this through the principle of Structural Risk Minimization (Vapnik, 1995). The empirical risk of misclassification is controlled by maximizing the margin between the training data and the decision boundary (Mashao, 2004). In practice this criterion is softened to the minimization of a cost factor involving both the complexity of the classifier and the degree to which marginal points are misclassified, and the tradeoff between these factors is managed through a margin of error parameter (usually designated C). Like the respective function parameters, this C parameter is tuned through cross-validation procedures (Mashao, 2004). Some of the classical literature relating to SVMs can be found in Vapnik (1995), Campbell (2000) and Christianini (2002).", "startOffset": 197, "endOffset": 2318}, {"referenceID": 3, "context": "Having started off as a Statistical Learning Theory (Vapnik, 1995), SVMs have continued to be used in machine vision fields such as character, handwriting digit and text recognition (Vapnik, 1995; Joachims, 1998). More recently, their application to land cover mapping has been vigorously explored (Huang et al, 2002; Mahesh and Mather, 2003, Gidudu et al, 2007). Like other supervised classifiers, training data is a prerequisite to define the decision boundaries within the feature space, based upon which classification decision rules are made. For SVMs, this decision boundary is a linear discriminant placed midway between the classes of interest. Unfortunately, land cover classes when projected to the input space are rarely linearly separable. SVMs handle such datasets by nonlinearly projecting the training data in the input space to a feature space of higher (infinite) dimension by use of a kernel function. This results in the previously nonlinear datasets becoming linearly separable. Placing a linear discriminant in this high (infinite) dimension will be equivalent to placing a non linear discriminant in the previous input space. Some examples of functions (also called kernels) used to this effect include: polynomial, gaussian (more commonly referred to as radial basis functions) and sigmoid functions. Each function has parameters which have to be determined prior to classification and they are usually determined through a cross validation process. Operating in high dimension potentially renders the risk of overfitting in the input space possible. SVMs control this through the principle of Structural Risk Minimization (Vapnik, 1995). The empirical risk of misclassification is controlled by maximizing the margin between the training data and the decision boundary (Mashao, 2004). In practice this criterion is softened to the minimization of a cost factor involving both the complexity of the classifier and the degree to which marginal points are misclassified, and the tradeoff between these factors is managed through a margin of error parameter (usually designated C). Like the respective function parameters, this C parameter is tuned through cross-validation procedures (Mashao, 2004). Some of the classical literature relating to SVMs can be found in Vapnik (1995), Campbell (2000) and Christianini (2002).", "startOffset": 197, "endOffset": 2342}, {"referenceID": 7, "context": "Of importance to the generation of an ensemble system is that each individual classifier must be unique in how it generates decision boundaries (Polikar, 2006).", "startOffset": 144, "endOffset": 159}, {"referenceID": 6, "context": "The rational behind ensuring diversity is that each classifier will make a different error, and strategically combining these classifiers can reduce the total error (Parikh and Polikar, 2007).", "startOffset": 165, "endOffset": 191}, {"referenceID": 7, "context": "Some of the ways through which diversity can be ensured include: using different datasets to train different classifiers, using different training parameters for different classifiers, using different types of classifiers and using different features for the different classifiers (Polikar, 2006).", "startOffset": 281, "endOffset": 296}, {"referenceID": 6, "context": "The rational behind ensuring diversity is that each classifier will make a different error, and strategically combining these classifiers can reduce the total error (Parikh and Polikar, 2007). Some of the ways through which diversity can be ensured include: using different datasets to train different classifiers, using different training parameters for different classifiers, using different types of classifiers and using different features for the different classifiers (Polikar, 2006). Another key aspect about ensemble systems is how to combine the results from the individual classifiers. Two examples of common combination rules include simple majority and weighted majority A more theoretical treatise on ensemble systems can be found in Polikar (2006). Figure 1 gives a graphical illustration of an ensemble system.", "startOffset": 166, "endOffset": 762}, {"referenceID": 7, "context": "Polikar (2006) posits that whereas the ensemble system results may not beat the", "startOffset": 0, "endOffset": 15}], "year": 2008, "abstractText": "From the advent of the application of satellite imagery to land cover mapping, one of the growing areas of research interest has been in the area of image classification. Image classifiers are algorithms used to extract land cover information from satellite imagery. Most of the initial research has focussed on the development and application of algorithms to better existing and emerging classifiers. In this paper, a paradigm shift is proposed whereby a \u2018committee\u2019 of classifiers is used to determine the final classification output. Two of the key components of an ensemble system are that there should be diversity among the classifiers and that there should be a mechanism through which the results are combined. In this paper, the members of the ensemble system include: Linear SVM, Gaussian (Radial Basis Function) SVM and Quadratic SVM. The final output was determined through a simple majority vote of the individual classifiers. From the results obtained it was observed that the final derived map generated by an ensemble system can potentially improve on the results derived from the individual classifiers making up the ensemble system. The ensemble system classification accuracy was, in this case, better than the linear and quadratic SVM result. It was however less than that of the RBF SVM. Areas for further research could focus on improving the diversity of the ensemble system used in this research.", "creator": "PScript5.dll Version 5.2.2"}}}