{"id": "1012.0084", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Dec-2010", "title": "Survey on Various Gesture Recognition Techniques for Interfacing Machines Based on Ambient Intelligence", "abstract": "Gesture recognition is mainly apprehensive on analyzing the functionality of human wits. The main goal of gesture recognition is to create a system which can recognize specific human gestures and use them to convey information or for device control. Hand gestures provide a separate complementary modality to speech for expressing ones ideas. Information associated with hand gestures in a conversation is degree,discourse structure, spatial and temporal structure. The approaches present can be mainly divided into Data-Glove Based and Vision Based approaches. An important face feature point is the nose tip. Since nose is the highest protruding point from the face. Besides that, it is not affected by facial expressions.Another important function of the nose is that it is able to indicate the head pose. Knowledge of the nose location will enable us to align an unknown 3D face with those in a face database. Eye detection is divided into eye position detection and eye contour detection. Existing works in eye detection can be classified into two major categories: traditional image-based passive approaches and the active IR based approaches. The former uses intensity and shape of eyes for detection and the latter works on the assumption that eyes have a reflection under near IR illumination and produce bright/dark pupil effect. The traditional methods can be broadly classified into three categories: template based methods,appearance based methods and feature based methods. The purpose of this paper is to compare various human Gesture recognition systems for interfacing machines directly to human wits without any corporeal media in an ambient environment.", "histories": [["v1", "Wed, 1 Dec 2010 02:54:24 GMT  (310kb)", "http://arxiv.org/abs/1012.0084v1", "12 PAGES"]], "COMMENTS": "12 PAGES", "reviews": [], "SUBJECTS": "cs.AI cs.CV cs.HC cs.RO", "authors": ["harshith c", "karthik r shastry", "manoj ravindran", "m v v n s srikanth", "naveen lakshmikhanth"], "accepted": false, "id": "1012.0084"}, "pdf": {"name": "1012.0084.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Manoj Ravindran"], "emails": ["@gmail.com", "naveen.lakshmikhanth@gmail.com"], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijcses.2010.1203 31 Gesture Recognition is based primarily on analyzing the functionality of human intelligence. The main goal of gesture recognition is to create a system that can recognize specific human gestures and use them to convey information or control devices. Hand gestures offer a separate complementary modality to speech to express one's own ideas. Information associated with hand gestures in a conversation is degree, discourse structure, spatial and temporal structure. Current approaches can be divided mainly into data glove-based and vision-based approaches. An important facial feature is the tip of the nose. As the nose is the highest protruding point from the face, it is not affected by facial expressions. Another important function of the nose is that it is able to display the head posture. Knowing the nose position allows us to match an unknown 3D face with those in a facial database. Eye recognition is divided into eye contact."}, {"heading": "1. INTRODUCTION", "text": "The information associated with hand gestures in a conversation uses sensor devices to digitize hand and finger movements into multiparametric data. The additional sensors make it easy to capture hand configuration and movement. However, the devices are quite expensive and bring users a lot of cumbersome experience. In contrast, vision-based methods require only one camera, allowing for natural interaction between people and computers without the need for additional devices. These systems tend to complement biological vision by describing artificial vision systems implemented in software and / or hardware. This is a difficult problem as these systems must be invariant in the background, lighting insensitive, person and camera independent to achieve real-time performance."}, {"heading": "2. RELATED WORK", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3. HAND DETECTION AND RECOGNITION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Hidden Markov Models", "text": "This method (Hidden Markov Model [1]) deals with the dynamic aspects of gestures. Gestures are extracted from a sequence of video images by centering the skin-color spots corresponding to the hand into a body-color space. The goal is to recognize two classes of gestures: deictic and symbolic. The image is filtered using a quick reference table of skin-color pixels in the YUV color space. After filtering, skin-color pixels are grouped into splotches. Splotches are statistical objects based on the location (x, y) and the colormetry (Y, U, V) of the skin-color pixels to determine homogeneous areas. A skin-color pixel belongs to the block that has the same location and the same color component. Deitic gestures show movements to the left (right) of the body space and symbolic gestures are supposed to execute commands (gripping, clicking, flicking on the shoulder)."}, {"heading": "3.2 YUV Colour Space and CAMSHIFT Algorithm", "text": "This method is concerned with the recognition of hand gestures. It is done in the following five steps. 1. A digital camera records a video stream of hand gestures. 2. All frames are taken into account and then the color segmentation of the YUV color space takes place. The YUV color system is used to separate chrominance and intensity. The symbol Y indicates the intensity while UV chrominant components are defined. 3. Now the hand is separated by means of the CAMSHIFT [2] algorithm. Since the hand is the largest connected region, we can segment the hand from the body. 4. Then the position of the hand center is calculated in each frame. This is done by first calculating the zero point and initial moments and then calculating the centrifuge based on this information. 5. Now the different center points are joined together to form a trajectory. This trajectory indicates the path of the hand movement and thus hand tracking is determined."}, {"heading": "3.3 Using Time Flight Camera", "text": "This approach uses X and Y projections of the image and optional depth features for gesture classification; the system uses a 3D time sensor (TOF) [3] [4], which has the great advantage of simplifying hand segmentation; the gestures used in the system have good separation potential along the two image axes; therefore, the projections of the hand on the X and Y axes are used as features for classification; the region of the arm is discarded because it contains no useful information for classification and because of strong differences between people; and depth features are included to distinguish certain gestures: gestures with the same projection but different orientation; the algorithm can be divided into five steps: 1. Segmentation of the hand and arm by distance values: hand and arm are segmented by an iterative seed filling algorithm. 2. Determination of the bounding field: The segmented area is determined on the x- and 3. axis to determine the extraction and the axis."}, {"heading": "3.4 Na\u00efve Bayes\u2019 Classifier", "text": "This method is an effective and fast method for static hand gesture recognition, based on the classification of the various gestures based on geometric invariants obtained from image data after segmentation; unlike many other recognition methods, this method is not dependent on skin color. Gestures are extracted from each image of the video, with static background, and segmented by dynamic extraction of background pixels according to the histogram of each image. Gestures are classified using a weighted K-Nearest Neighbor algorithm, which is combined with a naive Bayes [5] approach to estimate the probability of each gesture type. When tested in the JAST Human Robot dialog system, this method correctly classified more than 93% of the gestures. This algorithm consists of three main steps. The first step consists of segmenting and labeling the objects of interest and extracting the geometric inventories from them."}, {"heading": "3.5 Vision Based Hand Gesture Recognition", "text": "In visual hand gesture recognition [6], the movement of the hand is recorded using a video camera (s), and this input video is broken down into a series of features using individual frames; the hands are isolated from other body parts and other background objects; the isolated hands are detected for different postures; since gestures are nothing more than a sequence of hand postures connected by continuous movements, a recognition device can be trained against possible grammar; hand gestures can be specified as a set of hand postures in different compositions, just as phrases can be constructed by words; the recognised gestures can be used to advance a variety of applications; the visual hand postures and gesture recognition approaches (i) 3D hand model-based approach (ii) Appearance-based approach"}, {"heading": "3.5.1 3D Hand Model Based Approach", "text": "Three-dimensional approaches based on the hand-held 3D model rely on the hand-held 3D kinematic model with significant DOFs and try to estimate the hand parameters by comparing the input images with the possible 2D appearance projected by the hand-held 3D model. Such an approach is ideal for realistic interactions in virtual environments. This approach has several disadvantages that have prevented it from being used in real life. Firstly, the initial parameters for each image must be close to the solution, otherwise there is a risk that the approach will find a suboptimal solution (i.e. local minima). Secondly, the adjustment process is also sensitive to noise (e.g. lens deviations, sensor noise) in the imaging process. Finally, the approach cannot cope with the inevitable self-occurrence of the hand."}, {"heading": "3.5.2 Appearance Based Approach", "text": "This method uses image functions to model the visual appearance of the hand and to compare these parameters with the extracted image functions from video input. In general, optical approaches have the advantage of real-time performance due to the simpler 2D image functions used. In recent years, there has been a number of research efforts on optical methods. A simple and simple approach that is commonly used is to search for skin-coloured regions in the image. Although very popular, this has some drawbacks such as skin colour recognition being very sensitive to light conditions. While practical and efficient methods for skin colour recognition under controlled (and known) lighting exist, the problem of learning a flexible skin model and adapting it over time is a challenge. This only works if we assume that there is no other skin like objects in the scene. Another approach is to use the space to provide an efficient representation of a large number of high-dimensional dots with a small set of base vectors."}, {"heading": "4. NOSE DETECTION AND RECOGNITION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Using Geometric Features and FFT", "text": "In this method, the generalized eccentricities \u03b50 and \u03b52 are calculated for each pixel and assigned to the characteristic space. In the characteristic space, noses are marked by a delimiting field that is learned from a set of labeled training data. In classification, it is said that a certain pixel belongs to a nose tip if and only if it is mapped into this delimiting field in the characteristic space. In the FFT-based approach, the background is set to a constant value (the maximum value that appears in the foreground); in the NFFT approach [7], the background pixels were simply discarded. An adaptive threshold for the amplitude range is used to segment the foreground object. Figure 1 represents the distinction between the six surface types - pit, tip, saddle, valley, ridge and plane within the characteristic space spanned by \u03b50 and \u03b52."}, {"heading": "4.2 Using Hough Transform", "text": "This method (Hough Transform [8]) extracts the points of view close to the found sphere and enlarges the extracted cloud by a factor of 10 to center the sphere on the nose. The mean of the normal vectors of each point of view is evaluated with the aim of obtaining information about the face orientation in three-dimensional space. The center of the sphere is then moved by the evaluated direction and proportional to the beam value of the sphere. In this way, we collapse the entire sphere into a point that represents the tip of the nose. The point that is reached must not be between the apex of the examined face. Thus, the final step is to explore the point closest to the previously extracted tip of the nose. This apex represents the real tip of the nose."}, {"heading": "4.3 Effective Energy Calculation", "text": "Since the nose is the protruding area at the front of the face, this method uses edge detection techniques [9] to identify potential candidates for the tip of the nose by examining the adjacent pixels. Then, the tip of the nose is identified by potential candidates based on their effective energy, mean and variance values obtained from the Principal Component Analysis Technique, which has the ability to locate the tip of the nose in both frontal and non-frontal faces."}, {"heading": "4.4 Structural and Holistic Features", "text": "Then the template matching method is applied on the basis of the correlation [10] and the point at which the correlation between the stencil and the sub-image is the maximum is defined as the tip of the nose. However, template matching is a time-consuming process, and to reduce the search time, the eye line is determined first. The ideal eye line is the horizontal line that runs through the pupil of the eye. To locate the eye line, the upper half of the image is looked at and then the horizontal projection of the Sobel gradient is calculated, and then the t-horizontal lines with the highest lines are selected. From these t-lines, the middle line represents the approximate eye line. The template matching process starts at the eye line and continues, say, up to half the height of the original image."}, {"heading": "4.5 Colour Data Model", "text": "The human facial features such as the tip of the nose and the edge of the nose were identified using ashading technology on the colour data in the image. However, the highly sensitive nature of the nose region to light made the system disadvantageous."}, {"heading": "4.6. Template Based Method", "text": "This approach uses the convex shape of the nose to identify the nose region, calculating the local search area used as the template, scanning the template through the image, identifying the pixel and one with maximum correlation, and refining the position of the best match using the Evident Based Convolution filter [11]."}, {"heading": "5. EYE DETECTION AND TRACKING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Template Based Method", "text": "In the template-based methods [12], a generic eye model based on the shape of the eyes is first designed, which is then pixel by pixel mapped to the entire face with an eye template. Although these methods can accurately recognize eyes, they are usually time consuming. To improve accuracy, these methods must match pixel by pixel with an eye template. If we are not then aware of the size of the eyes, we must repeat the matching process with eye templates of different sizes, i.e., the template matching must be performed several times. In fact, this method first detects the two rough eye regions in the face using a feature-based method. Therefore, the following template matching is performed only in these two regions, which are much smaller than the entire face. The solution to improve the efficiency of the template matching method focuses on two points - the reduction of the area in the face for the template matching and the shortening of the times of this type of matching, once the effectiveness of the eye matching of the two regions can be assessed based on the size of the eye algorithm, in addition to the effectiveness of the two regions."}, {"heading": "5.2 Appearance Based Method", "text": "Appearance-based methods [13] recognize eyes by their photometric appearance. These methods usually have to collect a large amount of training data that depicts the eyes of different individuals under different illumination conditions. These data are used to train a classifier, and recognition is done by classification."}, {"heading": "5.3 Pixel-Pattern-Based Texture Feature (PPBTF)", "text": "PPBTF [14] is proposed first. It is constructed from a pattern card. A grayscale image is first converted into a pattern card, in which edge and background pixels are classified by matching with a given set of M pattern templates B. For each pixel (x, y) in a grayscale image I zi is the inner product of its S \u00d7 S adjacent block b with the pattern templates. Then, the pixel (x, y) in the pattern card P is assigned a number k, so that z k = max (z1, z2,..., zm).Therefore, the pixel values in a pattern card represent the pattern classes of pixels in the original grayscale image. The feature model is as follows: Suppose that the number of patterns is M, then the pixel value P (x, y) in the pattern card P is specified in a range of [1, M] the characteristics are specified in each one pixel (x)."}, {"heading": "5.4 Cascade Classifier", "text": "The key finding is that smaller and therefore more efficient classifiers can be constructed that discard many of the negative subwindows while detecting almost all positive instances. Simpler classifiers are used to discard the majority of the subwindows before more complex classifiers are needed to achieve low false positive rates. The phase of the cascade is constructed by ad boost. (Figure 2) After detecting the cascade classifier, some errors may occur, such as eyebrows, mouth, larger or smaller eyes, to exclude these patches, another method of classifying the eye and non-eye spots emerging from the cascade classifier is necessary, and this is done using PPBTF."}, {"heading": "5.5 Principal Component Analysis", "text": "Pattern templates represent the spatial characteristics of an image and reflect that the value of a pixel depends on those of its neighbors. The process of creating the templates is as follows: First, for a given image, an S \u00d7 S image block is referred to as a neighboring vector, and N numbers of S \u00d7 S image blocks are generated randomly. If the image numbers are N, and a (S \u00b7 S) \u00d7 (M \u00b7 N) matrix A would result, then PCA [16] is calculated using matrix A. After PCA analysis, eigenvectors are obtained and some of the eigenvectors are selected as a template. The first one that corresponds to the largest eigenvalue is a Gaussian lowpass filter, and the others are derived filters. Except for the first basic function, the others can be used as gradient filters for pattern matching. When transforming a grayscale image (24 \u00d7 12) eye into a pattern card, each pixel of the index of the pattern corresponding best to a neighboring pattern block is assigned to a pattern."}, {"heading": "5.6 Ada boost and SVM Classifier", "text": "A subset of features is selected at each step by Ada Boost; the selected feature is uncorrelated with the output of the previous features. As the selected features are smaller than the overall features, the speed is improved. SVM classifiers are trained on the features selected by AdaBoost. During the operation, a 5x cross-validation scheme is used. The best SVM [17] parameters are obtained by a method called raster search. RBF core function is used for the SVM. The main parameters of the SVM are Penalty C and Sigma (parameters of the RBF kernel). To improve the speed, both are selected in the experiment between 25 and 220. The face image is inserted into the cascade classifier using rectangular features, and the result parameters of the classifier are scaled and converted into a sample card using PCA base functions to form the features that are classified by Ad-Boost and SVM classifiers."}, {"heading": "5.7 Colour Based Detection", "text": "The additional phase consists of flexible thresholds and geometric tests. Flexible thresholds make the generation of candidates more careful and geometric tests allow the selection of suitable candidates as eyes. The main idea of EyeMapC is based on two separate eye maps [18] from the facial image, EyeMapC from the chrominance components and EyeMapL from the luminance component. These two maps are then combined to form a single eye map. Equation 2 The basic idea of EyeMapC is based on the properties of the eyes in the YCbCr color space, which show that eye regions have a high Cb and a low Cr. This formula is designed to brighten pixels with high Cb and low Cr values. Equation 2 As the eyes usually contain both dark and bright pixels in the luminance component, these eye regions have a high Cb and low Cr value. This formula is designed so that they contain mycexels and low Cr values."}, {"heading": "6. CONCLUSION", "text": "Based on the observations on hand recognition and tracking, we can conclude that the use of YUV skin color segmentation followed by the CAMSHIFT algorithm will help in effective detection and tracking, as the centric values can easily be determined by calculating the moments at each point, later we could combine Hidden Markov training for further applications. It is better compared to the Time-Flight Camera, where you have to find the boundary field and then apply the Iterative Seed Fill algorithm. For nose detection and tracking systems, the results obtained using Edge Detection Techniques were comparatively better, and it also recognizes the nose for faces at different angles. The success rate for this method is about 93% for frontal surfaces and about 68% for non-frontal surfaces. This shows that this particular method is suitable to be implemented in an automatic 3D face recognition system. For eye detection and tracking analysis, it would be efficient to introduce a combination of eye color and eye texture later on, using a larger method of 9%."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We would like to thank our parents for the blessing they bestowed on us and for the continuous support we received. We would also like to thank our project mentor Mr. T. Gireesh Kumar and our department head Mr. K. Gangadharan for their invaluable support throughout the work."}], "references": [{"title": "Hand gesture recognition using a real-time tracking method and hidden Markov models", "author": ["Chih-Ming Fu et.al"], "venue": "Science Direct \u2013 Image and Vision Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "et.al, \u201cMultimodal Approach to Human-Face Detection and Tracking", "author": ["P Vadakkepat"], "venue": "Industrial Electronics, IEEE Transactions on Issue Date: March 2008, Volume: 55 Issue:3,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Gesture recognition with a time-of-flight camera, Dynamic 3D Imaging", "author": ["E. Kollorz", "J. Hornegger", "A. Barke"], "venue": "International Journal of Intelligent Systems Technologies and Applications Issue: Volume", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "A facial feature tracker for humancomputer interaction based on 3D TOF cameras\u201d, Dynamic 3D Imaging (Workshop in conjunction with DAGM 2007)", "author": ["M. B \u0308ohme", "M. Haker", "T. Martinetz", "E. Barth"], "venue": "International Journal of Computer Science & Engineering Survey (IJCSES) Vol.1,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Using a Na\u00efve Bayes Classifier based on K-Nearest Neighbors with Distance Weighting for Static Hand-Gesture Recognition in a Human-Robot Dialog System", "author": ["Pujan Ziaie et.al"], "venue": "Advances in Computer Science and Engineering Communications in Computer and Information Science,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "et.al, \u201cVision Based Hand Gesture Recognition", "author": ["Pragati Garg"], "venue": "World Academy of Science, Engineering and Technology,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "et.al, \u201cScale-invariant range features for time-of-flight camera applications", "author": ["M Haker"], "venue": "Computer Vision and Pattern Recognition Workshops,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Extending Hough Transform to a Points\u2019 Cloud for 3D-Face Nose-Tip Detection\u201d, Advanced Intelligent Computing Theories and Applications", "author": ["Vitoantonio B et.al"], "venue": "With Aspects of Artificial Intelligence Lecture Notes in Computer Science,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Nose Tip Detection on a Three-Dimensional Face Range Image Invariant to Head Pose", "author": ["Wei Jen Chew et.al"], "venue": "International MultiConference of Engineers and Computer Scientists 2009 Vol I IMECS 2009, March", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Hybrid Face Recognition Method", "author": ["Mandal", "S et.al", "\u201dA"], "venue": "Based on Structural and Holistic Features\u201d,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "et.al,\u201dFast Anisotropic Gauss Filtering", "author": ["Jan-Mark Geusebroek"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "et.al,\u201dA robust algorithm for eye detection on gray intensity face without spectacles", "author": ["Kun Peng"], "venue": "Journal of Computer Science  Technology,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "et.al,\u201dA feature and appearance based method for eye detection on gray intensity face images", "author": ["V Kith"], "venue": "Computer Engineering & Systems, ICCES 2008, International Conference,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Eye detection based on rectangle features and pixel-pattern-based texture features", "author": ["Huchuan Lu et.al"], "venue": "Intelligent Signal Processing and Communication Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "et.al, \u201cThe Performance of the Haar Cascade Classifiers Applied to the Face and Eyes Detection", "author": ["Adam Schmidt"], "venue": "Advances in Soft Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Principal Component Analysis of Eye-Tracking Data during Visual Perception of Human Faces in Adults and Children with Autism", "author": ["M. Ben Mlouka et.al"], "venue": "International Conference on Advancements of Medicine and Health Care through Technology IFMBE Proceedings,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Precise Eye Location by Adaboost and SVM Techniques\u201d, Advances in Neural Networks", "author": ["Xusheng Tang"], "venue": "ISNN", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Visual tracking of high DOF articulated structures: An application to human hand tracking", "author": ["JM Rehg", "T Kanade"], "venue": "Proc. European Conference on Computer Vision,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Capturing natural hand Articulation\u201d.In", "author": ["L.J.Y.Y. Wu", "T.S. Huang"], "venue": "Proc. 8th Int. Conf. on Computer Vision,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}], "referenceMentions": [{"referenceID": 11, "context": "In the template based methods, a generic eye model, based on the eye shape, was designed at first [12].", "startOffset": 98, "endOffset": 102}, {"referenceID": 11, "context": "In other words, profiting from possibility of evaluating the size of eyes, this algorithm performs the template matching just once [12].", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "This data is used to train some classifier, and detection is achieved by classification [15] [17].", "startOffset": 88, "endOffset": 92}, {"referenceID": 16, "context": "This data is used to train some classifier, and detection is achieved by classification [15] [17].", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "One of the earliest model based approaches to the problem of bare hand tracking was proposed by Rehg and Kanade [19].", "startOffset": 112, "endOffset": 116}, {"referenceID": 18, "context": "In [20] a model-based method for capturing articulated hand motion is presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "1 Hidden Markov Models This method (Hidden Markov Model [1]) deals with the dynamic aspects of gestures.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Now the hand is separated using CAMSHIFT [2] algorithm .", "startOffset": 41, "endOffset": 44}, {"referenceID": 2, "context": "The system uses a 3-D time-of-flight (TOF) [3] [4] sensor which has the big advantage of simplifying hand segmentation.", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "The system uses a 3-D time-of-flight (TOF) [3] [4] sensor which has the big advantage of simplifying hand segmentation.", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "Gestures are classified using a weighted K-Nearest Neighbors Algorithm which is combined with a Na\u00efve Bayes [5] approach to estimate the probability of each gesture type.", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "5 Vision Based Hand Gesture Recognition In vision based hand gesture recognition system [6], the movement of the hand is recorded by video camera(s).", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "For FFT based approach the background was set to a constant values (the max value appearing in the foreground); for NFFT [7] approach the background pixels was simply discarded.", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "2 Using Hough Transform In this method (Hough Transform [8]), the face vertexes in proximity of the found sphere are extracted, and magnifying the extracted cloud of a factor of 10, in order to center the sphere on the nose.", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "3 Effective Energy Calculation Since the Nose is the protruding area on the facial front, this method uses Edge Detection techniques [9] to identify potential nose tip candidates by investigating the neighbouring pixels.", "startOffset": 133, "endOffset": 136}, {"referenceID": 9, "context": "Then, the template matching technique based on correlation [10] is employed and the point, at which the correlation between the template and the sub-image is the maximum, is defined as nose tip.", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "The position of best match was refined using Evident Based Convolution Filter [11].", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "1 Template Based Method In the template based methods [12], a generic eye model, based on the eye shape, is designed at first.", "startOffset": 54, "endOffset": 58}, {"referenceID": 12, "context": "2 Appearance Based Method Appearance based methods [13] detect eyes based on their photometric appearance.", "startOffset": 51, "endOffset": 55}, {"referenceID": 13, "context": "3 Pixel-Pattern-Based Texture Feature (PPBTF) PPBTF [14] is firstly proposed in.", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "4 Cascade Classifier Cascade of classifier [15] achieves increased detection performance while radically reducing computation time.", "startOffset": 43, "endOffset": 47}, {"referenceID": 15, "context": "If the image numbers are N, and an (S \u00d7 S) \u00d7 (M \u00d7 N) matrix A would come out, then PCA [16] will be computed using the matrix A.", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "6 Ada boost and SVM Classifier First Ada boost [17] is used to select features instead of being a classifier.", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "The best SVM [17] parameters are gained by a method called grid search.", "startOffset": 13, "endOffset": 17}], "year": 2010, "abstractText": "Gesture recognition is mainly apprehensive on analyzing the functionality of human wits. The main goal of gesture recognition is to create a system which can recognize specific human gestures and use them to convey information or for device control. Hand gestures provide a separate complementary modality to speech for expressing ones ideas. Information associated with hand gestures in a conversation is degree, discourse structure, spatial and temporal structure. The approaches present can be mainly divided into Data-Glove Based and Vision Based approaches. An important face feature point is the nose tip. Since nose is the highest protruding point from the face. Besides that, it is not affected by facial expressions. Another important function of the nose is that it is able to indicate the head pose. Knowledge of the nose location will enable us to align an unknown 3D face with those in a face database. Eye detection is divided into eye position detection and eye contour detection. Existing works in eye detection can be classified into two major categories: traditional image-based passive approaches and the active IR based approaches. The former uses intensity and shape of eyes for detection and the latter works on the assumption that eyes have a reflection under near IR illumination and produce bright/dark pupil effect. The traditional methods can be broadly classified into three categories: template based methods, appearance based methods and feature based methods. The purpose of this paper is to compare various human Gesture recognition systems for interfacing machines directly to human wits without any corporeal media in an ambient environment.", "creator": "PScript5.dll Version 5.2.2"}}}