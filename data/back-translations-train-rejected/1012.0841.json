{"id": "1012.0841", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2010", "title": "Automated Query Learning with Wikipedia and Genetic Programming", "abstract": "Most of the existing information retrieval systems are based on bag of words model and are not equipped with common world knowledge. Work has been done towards improving the efficiency of such systems by using intelligent algorithms to generate search queries, however, not much research has been done in the direction of incorporating human-and-society level knowledge in the queries. This paper is one of the first attempts where such information is incorporated into the search queries using Wikipedia semantics. The paper presents an essential shift from conventional token based queries to concept based queries, leading to an enhanced efficiency of information retrieval systems. To efficiently handle the automated query learning problem, we propose Wikipedia-based Evolutionary Semantics (Wiki-ES) framework where concept based queries are learnt using a co-evolving evolutionary procedure. Learning concept based queries using an intelligent evolutionary procedure yields significant improvement in performance which is shown through an extensive study using Reuters newswire documents. Comparison of the proposed framework is performed with other information retrieval systems. Concept based approach has also been implemented on other information retrieval systems to justify the effectiveness of a transition from token based queries to concept based queries.", "histories": [["v1", "Fri, 3 Dec 2010 20:53:36 GMT  (160kb)", "http://arxiv.org/abs/1012.0841v1", "44 pages"]], "COMMENTS": "44 pages", "reviews": [], "SUBJECTS": "cs.AI cs.IR cs.LG cs.NE", "authors": ["pekka malo", "pyry siitari", "ankur sinha"], "accepted": false, "id": "1012.0841"}, "pdf": {"name": "1012.0841.pdf", "metadata": {"source": "CRF", "title": "Automated Query Learning with Wikipedia and Genetic Programming", "authors": ["Pekka Malo", "Pyry Siitari", "Ankur Sinha"], "emails": ["pekka.malo@aalto.fi", "pyry.siitari@aalto.fi", "ankur.sinha@aalto.fi"], "sections": [{"heading": null, "text": "ar Xiv: 101 2.08 41v1 [cs.AI] 3Most of the existing information retrieval systems are based on a bag of word models and are not equipped with general world knowledge. Work has been done to improve the efficiency of such systems by using intelligent algorithms to generate queries, but not much research has been done to date on the inclusion of knowledge at the human and societal level in queries. This paper is one of the first attempts to incorporate such information into queries using Wikipedia semantics. It represents a significant shift from conventional token-based queries to concept-based queries, which lead to improved efficiency of information retrieval systems. In order to solve the automated query problem efficiently, we propose a Wikipedia-based framework of evolutionary semantics (Wiki-ES), in which concept-based queries are learned using a jointly developed evolutionary process."}, {"heading": "1. Introduction", "text": "A central challenge in building expert systems for sectarian information (IR) is the provision of shared knowledge to the world. Just as Hendler and Feigenbaum concisely [16] say, in order to build a system with \"significant levels of computational intelligence,\" we need significant bodies of knowledge in knowledge bases. That is, if a system is expected to understand general semantics in text and data, then it should have access to the extensive background knowledge that humans use in interpreting concepts (units of knowledge) and their dependencies. Of course, statistical methods and the processing of natural language can be used to extract text or data, but the ability of collections of texts to convey people and society at the level of semantics is fairly limited. Currently, there is an ongoing search for new ways of integrating semantic knowledge into documents without time-consuming technology."}, {"heading": "2. Contributions", "text": "The main contributions of the paper are summarised in the following points:"}, {"heading": "2.1. Use of Wikipedia semantics in query learning", "text": "When a set of documents on a particular topic is to be retrieved from a database, it is customary for a user to generate a query composed of tokens. This query is used to determine the relevance of documents in a database by performing a search for the tokens in those documents. However, when analyzing the problem from the user's perspective, it is recognized that the user is not only interested in the documents that contain the exactly matching tokens, but rather is looking for all those documents that contain the concept represented by the token. This provides a motivation to work on creating queries that are composed of concepts rather than tokens. Queries that consist of concepts contain a broad knowledge at the human and social level that provides a better representation of the topic being searched for. In this essay, we use Wikipedia semantics to convey the concept behind a token. There is no previous study of the knowledge of the authors using the Wikipedia semantics to determine the meaning of this token has been constructed in order to relate to the token's effectiveness."}, {"heading": "2.2. Development of a co-evolving GP", "text": "Generating an accurate query for a search is often an iterative and tedious task to complete. However, if a set of documents is available, with each document marked as relevant or irrelevant, the task of query generation can be completely avoided by directing the documents to an intelligent algorithm. Based on the relevance or irrelevance of the training materials, a concept-based query can be learned through the algorithm, thereby saving the user from a monotonous task. The paper contributes to the development of an evolving evolutionary algorithm that specializes in generating concept-based queries for document queries. The algorithm takes a set of training materials as input. Each document in the training set is marked as relevant or irrelevant by the user, based on which the algorithm produces concept-based queries. The result of the algorithm is not a single query, but a set of queries that are most suitable by means of a coordination function, and the object is most suitable for multiple use."}, {"heading": "2.3. Comparison of existing methodologies for Information Retrieval", "text": "In addition to the proposed framework, other frameworks on a hundred different topics were evaluated and the results presented; the concept-based approach of Query Construction was also applied to the existing frameworks and significant improvements in results were achieved for all methodologies; detailed evaluation results were presented for the proposed framework over its nearest competitor; and extensive simulations found that the proposed framework outperformed other common methods."}, {"heading": "3. Prelude: Wikipedia semantics and IQBE", "text": "To give an idea of the wealth of semantic information Wikipedia has to offer and how this information can be used in query learning, we briefly discuss the latest innovations that use Wikipedia's link structure to create cost-effective measures of the relationship between concepts and topics. In this section, we also summarize recent developments in automated query learning, in particular the work inspired by evolution-based genetic algorithms and the IQBE paradigm of Smith and Smith [42] and Chen et al. [5]."}, {"heading": "3.1. Wikipedia as a semantic knowledge-resource", "text": "Research on ontology-based knowledge models has been motivated in large part by its ability to provide unique definitions of concepts, their relationships, and characteristics that together create a unified description of a given area. Access to such structured information in machine-readable form has provided standardized ways to exchange common knowledge, thus enabling its efficient reuse in applications. Despite these advantages, the use of ontologies is limited due to the high development costs that are inevitable in manually generated knowledge resources, and there is also the difficulty of keeping resources up to date, especially when looking at multiple areas. As is well known [43, 25], even the most comprehensive ontologies, such as cyclic ontology, have limited and patchy coverage, so there is an urgent need to find less costly ways to describe concepts and their dependencies."}, {"heading": "3.1.1. Wikipedia-concept", "text": "Despite the fact that Wikipedia does not really meet the criteria of being an ontology, a closer look at its structure reveals many similarities [17]. By interpreting Wikipedia's articles as concepts and considering the general link structure - including redirects, hyperlinks and category links - as relationships, it is justified to argue that Wikipedia is the largest semantic network available today. [25] Wikipedia provides a solid middle ground between ontologies and classical thesauri \"by offering a rare blend of scale and structure.\" In fact, recent developments suggest a number of ways in which Wikipedia can be used for extractontological knowledge; for example, we see the Yago ontology of Suchanek et al. [45] and WikiNet of Nastase et al. [34] The primary feature that makes Wikipedia significantly richer in sectontological knowledge is its inner structure."}, {"heading": "3.1.2. Measuring semantic relatedness", "text": "Although the approaches to measuring conceptuality still exhibit relatively high correlation with people, which we have essentially applied as a yardstick for this unit of measurement, the approaches to measuring conceptuality are based on the way corpora or WordNet is fairly durable (McHale) and the approaches to measuring conceptuality (McHale), which are based on the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they and they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they are in the way they and they are in the way they are in the way they are in the way they are"}, {"heading": "3.2. Query learning problem", "text": "In fact, most of them are able to determine for themselves what they want and what they want to do."}, {"heading": "3.3. IQBE - Inductive Query By Example", "text": "The idea behind the IQBE paradigm, however, is very similar in principle to relevance feedback; both require explicit relevance statements from the user to guide the retrieval process. In IQBE, the user provides the system with a collection of sample documents (positive / negative examples) from which an algorithm learns the terms and the Boolean operators that join them, so that the query received best represents the information needs of the user. Instead of iteratively modifying an existing query, the system performs a single run to generate a fresh query. Once the learned query is available, it can run on any arbitrary Boolean information system."}, {"heading": "4. Wiki-ES: Learning semantic queries with Wikipedia", "text": "In this section, we present the Wiki-ES framework (Wikipedia-based Evolutionary Semantics) for automated query learning, which is based on the paradigm of genetic programming (GP), which is an effective tool in artificial intelligence to perform program induction. In GP, the idea is to use the principles of evolutionary computation to intelligently search the space of possible computer programs to find an individual who is most suitable for solving the problem at hand. Indeed, one could say that the purpose is to get the machine to generate a solution to the problem without being explicitly programmed [19]. In our case, for example, we want the Wiki-ES system to learn a program (i.e. a query) that will restore a high number of relevant documents while keeping the irrelevant documents away. The learning process is driven by the evolutionary pressure that guarantees that only fit candidates will survive among all the potential candidates."}, {"heading": "4.1. Wiki-ES framework overview", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "4.2. Wikipedia-based document model", "text": "In the context of Wiki-ES, each document is represented by a collection of Wikipedia concepts, which are identified from their contents. The approach is based on those of Milne and al. [30] and Medelyan et al. [26], where a two-step classification is used to identify those terms in the document that should function as Wikipedia concepts. To explain the reasons for this modification, the model used here extends to the Wikification process, by dividing the concepts found into two categories, general Wikipedia concepts and concepts called Wikipedia entity entity entity entity entity terms. To explain the reasons for this modification, we consider, for example, a named entity \"Goldman Sachs\" and a general concept \"investment banking.\" To say that a certain document Goldman Sachs concepts and concepts entity concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-are concepts-concepts-concepts-other concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-are-concepts-concepts-concepts-concepts-Wikipedia-investment banking-concepts-concepts-concepts-concepts-concepts-concepts-concepts-are-concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-are-concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-concepts-Wikipedia-Wikipedia-other concepts-concepts-concepts-concepts-concepts-are-concepts-concepts-concepts-concepts-concepts-concepts-Wikipedia-concepts-are-concepts-Wikipedia-concepts-concepts-Wikipedia-concepts-Wikipedia"}, {"heading": "4.3. Query model: structure and matching of Wiki-ES rules", "text": "As mentioned in Section 4.1, each Wiki-ES rule can be considered a composition of a number of queries, and the Wiki-ES rule has a basic structure that is essentially different from the ordinary Boolean query. To give a more accurate picture, we formalize the definition of the Wiki-ES rule as a voting system, in which several concept-based queries are selected for a vote and the weighted sum of their votes is considered the relevance of a document. The presentation of the Wiki-ES model is structured as follows: First, we define the wiki queries used as building blocks in the Wiki-ES rule (Section 4.3.1), then, in Section 4.3.2, we introduce a fitness measure to evaluate the quality of individual queries and discuss how a voting system can be used to combine the results of multiple wiki queries to generate a Wiki-ES rule. Section 4.4 summarizes the Wiki-ES learning problem."}, {"heading": "4.3.1. Building blocks of Wiki-ES rules", "text": "In order to distinguish these from the usual terminology-based questions, we refer to the way in which the answer to the question depends on the answer to the question, whether the answer to the question depends on the answer to the question, whether the answer to the question depends on the answer to the question, whether the answer to the question depends on the answer to the question, whether the answer to the question depends on the answer to the question, whether the answer to the question deviates from the answer to the question, whether the answer to the question is divergent from the answer to the question, whether the answer to the question is divergent from the question, whether the answer to the question is divergent from the answer to the question, whether the answer to the question is divergent from the answer to the answer to the question is divergent."}, {"heading": "4.3.2. Wiki-ES rule", "text": "In this sense, we define two additional functions: (i) a fitness function to measure the quality of individual wiki queries; and (ii) a voting function to summarize the results of a wiki query into a single measurement. (q) Definition 4.3.4 (Fitness of the wiki query) Let's leave Q voting function for a wiki query q-Q is defined as the figure, F: (q, Dt) 7 \u2192 c figure [0, 1], which corresponds to the F score within a given set of evaluation documents: F (q, Dt) = 2P (q, Dt) R (q, Dt) P (q, Dt) + R (q, Dt)."}, {"heading": "4.4. Wiki-ES as an optimization problem", "text": "As discussed in Section 3.3, the query learning task can be viewed as a major optimization problem, where the search space consists of all sorts of queries that can be submitted to the IRS. However, rather than considering optimization over the space of permitted wiki queries, we turn the query learning task into the problem of finding an optimal wiki-ES rule that maximizes the F score in terms of the given collection of training materials."}, {"heading": "Q\u0304 denote the space of Wiki-ES rules. The learning problem is given by", "text": "Q: (q, Dt) 7 \u2192 c [0, 1] is the Wiki-ES fitness function corresponding to the F score in the training document Dt; see definition 4.3.4. The reason for defining the learning problem in the sense of the Wiki-ES rules instead of wiki queries arises for the following reasons: The first is the multimodality of the user's relevance function. As Tamine et al. [46] has shown, the relevant documents corresponding to the same topic can be distributed in different regions of the document space and thus have very different profiles. This implies that in order to restore the relevant documents it is necessary to examine the document space in a number of directions at the same time. Therefore, given the definition of a Wiki-ES rule as a choice system, the relevant documents corresponding to the same topic may be a natural solution to the multimodality problem."}, {"heading": "5. Wiki-ES GP-algorithm", "text": "The proposed GP algorithm aims to generate more suitable queries using a mechanism inspired by biological evolution. [35] The approach is based on population, with each individual representing a wiki query. The idea behind the technique is that environmental pressure causes natural selection in a particular population of individuals, which leads to an increase in the fitness of the population. Once the genetic representation of a query and fitness function is defined, the algorithm initiates a population of random queries. The population of wiki queries is then improved by repeated use of Selection, Crossover, Mutation and Replacement. To ensure sufficient diversity and reduce the risk of overmatch of the training set, the population is developed in a number of codeveloping subpopulations. The wiki ES rules are then formed by collecting the most fit individuals from each subpopulation to map the set of queries, the rest of which follows the voting function."}, {"heading": "5.1. Genetic Representation", "text": "Each query is expressed as a syntax tree, with the nodes acting as Boolean operators and the terminals as concepts.The query in the figure consists of four concepts, {w1, w2, w3, w4}, and the basic Boolean operators, {AND, OR, NOT}.The tree represents a Boolean expression (w1, w2).Such a query leads to the selection of those documents from the library that contain either the concepts w1 and w2 or the concept w3, but not w4. Each tree has a depth that represents the size of a tree.The depth of a tree is the number of branches traversed to reach the deepest terminal.The tree in Figure 4 has w4 as root, the deepest depth of a tree, and the depth of the node is 0."}, {"heading": "5.2. Population Initialization", "text": "As in any evolutionary algorithm, the initial population subjects are randomly generated in genetic programming; the maximum depth (dmax) that an individual can have is given as input; a number d is randomly selected from the set {1, 2, 3., dmax}; the selected number becomes the depth of the tree (individual) to be initialized; starting from the root node, an operator is randomly selected from the set O = {AND, OR, NOT} and placed on the node. If the node turns out to be AND or OR, two subnodes are created; otherwise, a single subnode is created; the procedure is repeated for each of the subnodes and the tree size grows. At a depth of d \u2212 1, a terminal should be selected to stop the growth of the tree. Therefore, a random selection is made from the set W0 = {w1, w2,..., wk} and the concept is placed on the terminal."}, {"heading": "5.3. Fitness Assignment", "text": "As already mentioned, the set W0 = {w1, w2,.., wk} is generated by scanning the training documents and selecting the most relevant concepts that provide a good representation of the training set. Once a random query with members of the group W0 and the basic Boolean operators has been compiled, the query can be evaluated by verification with the training set. The Boolean query is applied to each document in the training set, and the query predicts the document as relevant or irrelevant. The number of correct relevant or irrelevant predictions results in suitability for the query. The algorithm searches for the queries that provide the maximum number of correct predictions. Often, there is a possibility that more than one query produces the same results and therefore has the same fitness."}, {"heading": "5.4. Producing New Queries", "text": "The crossover method is performed by randomly selecting a crossover point in each parent tree. Once the crossover points are selected, the offspring are generated by swapping the subtree that is rooted at the crossover point of a parent with the subtree that is rooted at the crossover point of the other parent. Figure 6 shows two parents and the crossover operation. The subtrees to be swapped are shown shaded in the figure. Swapping the two shaded subtrees generates the offspring. As soon as the crossover operation is performed and the offspring are generated, they undergo a mutation operation. A point mutation operation is not used Parent 2Parent 1, in which each node operation is considered an NOR, and with a certain probability, the primitive stored at the node is replaced by another mutation."}, {"heading": "5.5. Algorithm Description", "text": "The proposed algorithm follows the framework of a general evolutionary algorithm. Instead of having a single population, the algorithm maintains several subpopulations that interact with each other during the optimization run. The algorithm terminates when the prescribed number of generations is completed. At the end of the optimization run, the algorithm provides elites from each of the subpopulations as final solutions. These elites are supposed to represent different niches in the search space. Each elite represents a wiki query that participates in the formation of a wiki-ES rule. Multiple queries are accepted as solutions from the algorithm, as we do not want to rely on a single query."}, {"heading": "5.6. Formation of Wiki-ES rules", "text": "As already mentioned, the proposed GP algorithm generates multiple queries as output. If the number of subpopulations in the algorithm is selected as M, then the number of final queries is also the number M. Given a document, each query suggests that it is either relevant or irrelevant. However, we do not want to rely on a single query, but want to take a weighted contribution from each of the queries before making a final decision. Let each query by Qi: i: i: i: i: 1, 2., M} and the fitness associated with it represent Fi: i: i: i: i: 1, 2., M} For each given document d, when we need to decide whether it is relevant or irrelevant, the output of each query is presented as Bi: i: i: i: 1, 2., M: with Bi being either 0 or 1. Now, a weighted contribution of the query is taken into account in the following metric: \u00b5 = Mi = If this document is considered as Mi = 0.5, then the contribution is weighted as Mi = 1."}, {"heading": "6. Experiment and results", "text": "To demonstrate the benefits of using Wiki-ES rules, we evaluate the system based on the topics in the TREC-11 corpus. The experiment is structured as follows: First, we describe the data set in Section 6.1, followed by a report on the software components used to implement the Wiki-ES system in Section 6.2; the parameter structure of the GP algorithm is outlined in Section 6.3; and the results of comparing Wiki-ES with competing algorithms are presented in Section 6.4. In particular, we demonstrate the benefits of using Wikipedia concepts for query learning by comparing the performance of Wiki-ES with a corresponding term-based model."}, {"heading": "6.1. Data", "text": "The documents contained in TREC-11 Corpus are Reuters RCV1 news stories from 1996-1997, but the data is divided into a training kit (items from 1996-08-20 to 1996-09-30) and a test kit (rest of the collection), and the training and test kit is further divided into 100 topic-specific sub-sets. All 100 TREC-11 topics (numbered R101-R200) are used in the experiment. In this paper, only the initial training data is used, while the relevance statements available for adaptive learning are not used. Also, none of the information in the separately available topic description file is used. Given that query learning techniques are highly dependent on the quality and quantity of training data, it is worth taking a closer look at the available data for the 100 TREC-11 topics. Figure 9 shows two histograms that represent the number of training and assessment documents for each topic."}, {"heading": "6.2. System description", "text": "The system used in the experiment was implemented with Java software on the GATE platform, which provides tools for standard documentation pre-processing tasks; the other software components used in implementing and evaluating the Wiki-ES framework are described as follows: \u2022 Wikipedia model: The Wikipedia-based content model was created using the WikipediaMiner published by Milne et al. [31], which was modified accordingly and integrated into our framework. \u2022 NER: The task to detect the aforementioned entities was implemented using a Conditional Random Field (CRF) classifier proposed by Finkel et al. [27]. \u2022 Classifiers: The classifiers (SVM and C4.5), which were implemented in the Java package [using benchmarks]."}, {"heading": "6.3. Parameter setting", "text": "The GP method used in this experiment has the usual genetic programming parameters such as population size, crossover probability, probability of mutation, etc. In Table 2, in addition to the general GP parameters, we have used 15 as the maximum size for the terminal set when building query trees. That is, when building the queries, the maximum number of different Wikipedia concepts that could occur in a single wiki query has been set to 15. The selection of Wikipedia concepts for each topic was made by selecting those that occur most frequently in the relevant training materials."}, {"heading": "6.4. Results", "text": "In this section we present the results of two experiments carried out using TREC-11 data. The first experiment discussed in Section 6.4.1 investigates the importance of using Wikipedia concepts in Wiki-ES rules by comparing them with the results obtained by executing the same algorithm with the sack-of-words document model. By using the sack-of-words profile in the competing model, we get an effective comparison with the established IQBE paradigm. The second experiment presented in Section 6.4.2 evaluates the benefits of the Wiki-ES model compared to the known classification models based on support vector machines (SVM) and the decision tree algorithm C4.5."}, {"heading": "6.4.1. Experiment 1: Effect of Wikipedia semantics", "text": "This year, it is only a matter of time before a solution is found, until an agreement is reached."}, {"heading": "6.4.2. Experiment 2: Comparison with classification models", "text": "The purpose of the second experiment is to compare the performance of the Wiki-ES model with two well-known classification algorithms, SVM and C4.5. To also assess the impact of feature selection, benchmark algorithms are trained with both token-based (bag-of-words) document representations and wiki-based document models. Results are summarized in Table 4, which shows the key performance metrics for each of the 5 models. A general comparison of the models suggests that the Wiki-ES framework consistently exceeds its F-score benchmarks, and the primary cause of the performance advantage appears to be improved memory of Wiki-ES rules."}, {"heading": "7. Conclusions", "text": "The purpose of any automated query system is to help the user define a query that finds the elements relevant to his topic. There are plenty of studies in this direction that have been discussed in the work. We have also discussed that traditional query systems lack the information contained in a word or symbol at the concept level. Some studies have enabled the use of intelligent systems, but it has been difficult for them to significantly improve the performance of the query system for information, which suggests that all of these query systems are inherently lacking an important feature, since they do not use the concept-based information that prevents improvement beyond a certain point. The proposal to access concept-level information via Wikipedia is a simple and fast technique for injecting the information at the human and social level into an information retrieval system. Wikipedia is a free and universally available database of information that is frequently updated by the Wikipedia community."}], "references": [{"title": "Java-ML: A Machine Learning Library", "author": ["T. Abeel", "Y. de Peer", "Y. Saeys"], "venue": "Journal of Machine Learning Research", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Genetic Approach to Query Space Exploration", "author": ["M. Boughanem", "C. Chrisment", "L. Tamine"], "venue": "Information Retrieval", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "On Using Genetic Algorithms for Multimodal Relevance Optimization in Information Retrieval", "author": ["M. Boughanem", "C. Chrisment", "L. Tamine"], "venue": "Journal of the American Society for Information Science and Technology", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Using genetic algorithms to evolve a population of topical queries", "author": ["R. Cecchini", "C. Lorenzetti", "A. Maguitman"], "venue": "Information Processing and Management", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "A machine learning approach to inductive query by example: An experiment using relevance feedback, ID3, genetic algorithms, and simulated annealing", "author": ["H. Chen", "G. Shankaranarayanan", "L. She", "A. Iyer"], "venue": "Journal of the American Society for Information Science", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "The Google similarity distance", "author": ["R. Cilibrasi", "P. Vitanyi"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "A review on the application of evolutionary computation to information retrieval", "author": ["O. Cord\u00f3n", "E. Herrera-Viedma", "C. L\u00f3pez-Pujalte", "M. Luque", "C. Zarco"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Improving the learning of Boolean queries by means of a multiobjective IQBE evolutionary algorithm", "author": ["O. Cord\u00f3n", "E. Herrera-Viedma", "M. Luque"], "venue": "Information Processing and Management", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["J. Finkel", "T. Grenader", "C. Manning"], "venue": "in: Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "Y. Gabrilovich", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "ACM Transactions on Information Systems", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Overcoming the brittleness bottleneck using Wikipedia", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "in: Proc. National Conference on Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "in: Proc. IJCAI-07", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Wikipedia-based Semantic Interpretation for Natural Language Processing", "author": ["E. Gabrilovich", "S. Markovitch"], "venue": "Journal of Artificial Intelligence Research 34,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "An extension on \u201dStatistical comparisons of classifiers over multiple data sets\u201d for all pairwise comparisons", "author": ["S. Gar\u0107\u0131a", "F. Herrera"], "venue": "Journal of Machine Learning Research", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "The WEKA Data Mining Software: An Update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I. Witten"], "venue": "SIGKDD Explorations", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Knowledge Is Power: The Semantic Web Vision, in: Web Intelligence: Research and Development", "author": ["J. Hendler", "E. Feigenbaum"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Harvesting Wiki consensus - using Wikipedia entries as ontology elements, in: Proceedings of the First International Workshop: SemWiki\u201906-FromWiki to Semantics", "author": ["M. Hepp", "D. Bachlechner", "K. Siorpaes"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Applying genetic algorithms to query optimization in document retrieval", "author": ["J. Horng", "C. Yeh"], "venue": "Information Processing and Management", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Genetic programming: On the programming of computers by means of natural selection", "author": ["J. Koza"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "Genetic algorithm for query optimization in information retrieval: relevance feedback", "author": ["D. Kraft", "F. Petry", "B. Buckes", "T. Sadasivan"], "venue": "Genetic Algorithms and Fuzzy Logic Systems", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "A Study of the Use of Multi-Objective Evolutionary Algorithms to Learn Boolean Queries: A Comparative Study", "author": ["A. L\u00f3pez-Herrera", "E. Herrera-Viedma", "F. Herrera"], "venue": "Journal of the American Society for Information Science and Technology", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Applying multi-objective evolutionary algorithms to the automatic learning of extended Boolean queries in fuzzy ordinal linguistic information retrieval systems", "author": ["A. L\u00f3pez-Herrera", "E. Herrera-Viedma", "F. Herrera"], "venue": "Fuzzy Sets and Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "A Comparison of WordNet and Roget\u2019s Taxonomy for Measuring Semantic Similarity", "author": ["M. McHale"], "venue": "in: Proc. of COLING/ACL Workshop 41  on Usage of WordNet in Natural Language Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Augmenting Domain-Specific Thesauri with Knowledge from Wikipedia", "author": ["O. Medelyan", "D. Milne"], "venue": "in: Proceedings of the New Zealand", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Mining meaning from Wikipedia", "author": ["O. Medelyan", "D. Milne", "C. Legg", "I. Witten"], "venue": "International Journal of Human-Computer Studies", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Topic Indexing with Wikipedia", "author": ["O. Medelyan", "I. Witten", "D. Milne"], "venue": "in: Proceedings of the AAAI 2008 Workshop on Wikipedia and Artificial Intelligence (WIKIAI", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "JGAP - Java Genetic Algorithms and Genetic Programming Package", "author": ["K. Meffert"], "venue": "Technical Report. URL: http://jgap.sf.net", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Wikify!: linking documents to encyclopedic knowledge", "author": ["R. Mihalcea", "A. Csomai"], "venue": "in: Proc. CIKM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Computing Semantic Relatedness using Wikipedia Link Structure", "author": ["D. Milne"], "venue": "in: Proceedings of the New Zealand Computer Science Research Student Conference", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Learning to Link with Wikipedia", "author": ["D. Milne", "I. Witten"], "venue": "in: Proc. CIKM", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "An Open-Source Toolkit for Mining Wikipedia", "author": ["D. Milne", "I. Witten"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}, {"title": "A knowledge-based search engine powered by Wikipedia", "author": ["D. Milne", "I. Witten", "D. Nichols"], "venue": "in: Proceedings of the 16th ACM Conference on Information and Knowledge Management CIKM\u201907,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Decoding Wikipedia Categories for Knowledge Acquisition", "author": ["V. Nastase", "M. Strube"], "venue": "in: Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2008}, {"title": "Wikinet: A very large scale multi-lingual concept network, in: Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC\u201910)", "author": ["V. Nastase", "M. Strube", "B. Boerschinger", "C. Zirn", "A. Elghafari"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "A field guide to genetic programming. Published via http://lulu.com and freely available at http://www.gp-field-guide.org.uk", "author": ["R. Poli", "W. Langdon", "N. McPhee"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Exploiting Semantic Role Labeling", "author": ["S. Ponzetto", "M. Strube"], "venue": "Wordnet and Wikipedia,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2006}, {"title": "An API for Measuring the Relatedness of Words in Wikipedia", "author": ["S. Ponzetto", "M. Strube"], "venue": "in: Proceedings of the ACL 2007 Demo and Poster Sessions,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2007}, {"title": "Knowledge Derived From Wikipedia For Computing Semantic Relatedness", "author": ["S. Ponzetto", "M. Strube"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "Information retrieval. Butterworths", "author": ["C. van Rijsbergen"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1979}, {"title": "Relevance feedback in information retrieval, in: The SMART Retrieval System", "author": ["J. Rocchio"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1971}, {"title": "Improving retrieval performance by relevance feedback", "author": ["G. Salton", "C. Buckley"], "venue": "Journal of the American Society for Information Science", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1990}, {"title": "The use of genetic programming to build boolean queries for text retrieval through relevance feedback", "author": ["M. Smith"], "venue": "Journal of Information Science", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1997}, {"title": "The Challenge of Knowledge Soup", "author": ["J. Sowa"], "venue": "Research Trends in Science, Technology and Mathematics Education", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2004}, {"title": "WikiRelate! Computing semantic relatedness using Wikipedia", "author": ["M. Strube", "S. Ponzetto"], "venue": "in: Proceedings of the 21st AAAI conference on artificial intelligence", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2006}, {"title": "Yago: A large ontology from wikipedia and wordnet", "author": ["F. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "Elsevier Journal of Web Semantics", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2008}, {"title": "Multiple query evaluation based on an enhanced genetic algorithm", "author": ["L. Tamine", "C. Chrisment", "M. Boughanem"], "venue": "Information Processing and Management", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2003}, {"title": "Query modifications using genetic algorithms in vector space models", "author": ["J. Yang", "R. Korfhage"], "venue": "International Journal of Expert Systems", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1994}], "referenceMentions": [{"referenceID": 15, "context": "As succinctly put by Hendler and Feigenbaum [16], in order to build any system with \u201csignificant levels of computational intelligence, we need significant bodies of knowledge in knowledge bases\u201d.", "startOffset": 44, "endOffset": 48}, {"referenceID": 41, "context": "The proposed system extends the Inductive Query By Example (IQBE) paradigm of Smith and Smith [42] and Chen et al.", "startOffset": 94, "endOffset": 98}, {"referenceID": 4, "context": "[5] by incorporating human-level semantics using Wikipedia.", "startOffset": 0, "endOffset": 3}, {"referenceID": 38, "context": "This transition from an ordinary boolean query [39] to a semantified query is necessary for integrating human and society-level semantic information into the information retrieval (IR) system.", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "[7, 8], Gar\u0107\u0131a and Herrera [14], and L\u00f3pez-Herrera et al.", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "[7, 8], Gar\u0107\u0131a and Herrera [14], and L\u00f3pez-Herrera et al.", "startOffset": 0, "endOffset": 6}, {"referenceID": 13, "context": "[7, 8], Gar\u0107\u0131a and Herrera [14], and L\u00f3pez-Herrera et al.", "startOffset": 27, "endOffset": 31}, {"referenceID": 21, "context": "[22, 21].", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[22, 21].", "startOffset": 0, "endOffset": 8}, {"referenceID": 41, "context": "In particular, we consider the work inspired by evolution-based genetic algorithms, and the IQBE paradigm of Smith and Smith [42] and Chen et al.", "startOffset": 125, "endOffset": 129}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 42, "context": "As it is commonly known [43, 25], even the most extensive ontologies, such as the Cyc ontology, have limited and patchy coverage.", "startOffset": 24, "endOffset": 32}, {"referenceID": 24, "context": "As it is commonly known [43, 25], even the most extensive ontologies, such as the Cyc ontology, have limited and patchy coverage.", "startOffset": 24, "endOffset": 32}, {"referenceID": 24, "context": "[25] for a comprehensive review.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "As pioneering research in this field, we acknowledge the work done by Ponzetto and Strube [36, 38, 37, 44], Gabrilovich and Markovich [11, 12, 13], Milne et al.", "startOffset": 90, "endOffset": 106}, {"referenceID": 37, "context": "As pioneering research in this field, we acknowledge the work done by Ponzetto and Strube [36, 38, 37, 44], Gabrilovich and Markovich [11, 12, 13], Milne et al.", "startOffset": 90, "endOffset": 106}, {"referenceID": 36, "context": "As pioneering research in this field, we acknowledge the work done by Ponzetto and Strube [36, 38, 37, 44], Gabrilovich and Markovich [11, 12, 13], Milne et al.", "startOffset": 90, "endOffset": 106}, {"referenceID": 43, "context": "As pioneering research in this field, we acknowledge the work done by Ponzetto and Strube [36, 38, 37, 44], Gabrilovich and Markovich [11, 12, 13], Milne et al.", "startOffset": 90, "endOffset": 106}, {"referenceID": 10, "context": "As pioneering research in this field, we acknowledge the work done by Ponzetto and Strube [36, 38, 37, 44], Gabrilovich and Markovich [11, 12, 13], Milne et al.", "startOffset": 134, "endOffset": 146}, {"referenceID": 11, "context": "As pioneering research in this field, we acknowledge the work done by Ponzetto and Strube [36, 38, 37, 44], Gabrilovich and Markovich [11, 12, 13], Milne et al.", "startOffset": 134, "endOffset": 146}, {"referenceID": 12, "context": "As pioneering research in this field, we acknowledge the work done by Ponzetto and Strube [36, 38, 37, 44], Gabrilovich and Markovich [11, 12, 13], Milne et al.", "startOffset": 134, "endOffset": 146}, {"referenceID": 28, "context": "[29, 32, 30, 31], Medelyan et al.", "startOffset": 0, "endOffset": 16}, {"referenceID": 31, "context": "[29, 32, 30, 31], Medelyan et al.", "startOffset": 0, "endOffset": 16}, {"referenceID": 29, "context": "[29, 32, 30, 31], Medelyan et al.", "startOffset": 0, "endOffset": 16}, {"referenceID": 30, "context": "[29, 32, 30, 31], Medelyan et al.", "startOffset": 0, "endOffset": 16}, {"referenceID": 25, "context": "[26, 24], Nastase et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 23, "context": "[26, 24], Nastase et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 32, "context": "[33, 34], and Mihalcea and Csomai [28], who have examined different ways of using Wikipedia to compute semantic relatedness between concepts and perform automated cross-referencing of documents.", "startOffset": 0, "endOffset": 8}, {"referenceID": 33, "context": "[33, 34], and Mihalcea and Csomai [28], who have examined different ways of using Wikipedia to compute semantic relatedness between concepts and perform automated cross-referencing of documents.", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "[33, 34], and Mihalcea and Csomai [28], who have examined different ways of using Wikipedia to compute semantic relatedness between concepts and perform automated cross-referencing of documents.", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "Wikipedia-concept In spite of the fact that Wikipedia does not really fulfill the criteria of being an ontology, a closer look at its structure reveals many similarities [17].", "startOffset": 170, "endOffset": 174}, {"referenceID": 24, "context": "[25], Wikipedia provides a solid middle-ground between ontologies and classical thesauri \u201cby offering a rare mix of scale and structure\u201d.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[45] and WikiNet by Nastase et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "In the literature, this is commonly referred to as the wikification task [28] or automatic topic-linking problem [30, 26].", "startOffset": 73, "endOffset": 77}, {"referenceID": 29, "context": "In the literature, this is commonly referred to as the wikification task [28] or automatic topic-linking problem [30, 26].", "startOffset": 113, "endOffset": 121}, {"referenceID": 25, "context": "In the literature, this is commonly referred to as the wikification task [28] or automatic topic-linking problem [30, 26].", "startOffset": 113, "endOffset": 121}, {"referenceID": 22, "context": "Measuring semantic relatedness Although approaches to measuring conceptual relatedness based on corpora or WordNet have been around for quite long (McHale [23] and Finkelstein et al.", "startOffset": 155, "endOffset": 159}, {"referenceID": 9, "context": "[10]), the use of Wikipedia as a source of background knowledge is a relatively new idea.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "The first step in this direction was taken by Strube and Ponzetto [44], who proposed their WikiRelate-technique that modified existing measures to better work with Wikipedia.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "This was soon followed by the paper of Gabrilovich and Markovitch [12], who suggested explicit semantic analysis (ESA) to define a highly accurate similarity measure using the full text of all Wikipedia articles.", "startOffset": 66, "endOffset": 70}, {"referenceID": 28, "context": "[29, 30], where only the internal link structure of Wikipedia is used to define relatedness.", "startOffset": 0, "endOffset": 8}, {"referenceID": 29, "context": "[29, 30], where only the internal link structure of Wikipedia is used to define relatedness.", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "The relatedness measure essentially corresponds to the Normalized Google Distance inspired by Cilibrasi and Vitanyi [6]", "startOffset": 116, "endOffset": 119}, {"referenceID": 28, "context": "[29, 30])).", "startOffset": 0, "endOffset": 8}, {"referenceID": 29, "context": "[29, 30])).", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "The link structure -based concept-relatedness measure, link-rel : W \u00d7W \u2192 [0, 1] , is then given by", "startOffset": 73, "endOffset": 79}, {"referenceID": 0, "context": "The Wikipediabased document-term -relatedness measure, d-rel : W \u00d7 D \u2192 [0, 1], is given by d-rel(w, d) = max{link-rel(w, w\u0304) : w\u0304 \u2208 \u039b(d)}", "startOffset": 71, "endOffset": 77}, {"referenceID": 6, "context": "[7], these can roughly be categorized into three baskets: (1) term learning; (2) weight learning; and (3) query-structure learning.", "startOffset": 0, "endOffset": 3}, {"referenceID": 40, "context": "Salton and Buckley [41] and Rocchio [40], Yang and Korfhage [47], Horng and Yeh [18], and Boughanem et al.", "startOffset": 19, "endOffset": 23}, {"referenceID": 39, "context": "Salton and Buckley [41] and Rocchio [40], Yang and Korfhage [47], Horng and Yeh [18], and Boughanem et al.", "startOffset": 36, "endOffset": 40}, {"referenceID": 46, "context": "Salton and Buckley [41] and Rocchio [40], Yang and Korfhage [47], Horng and Yeh [18], and Boughanem et al.", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "Salton and Buckley [41] and Rocchio [40], Yang and Korfhage [47], Horng and Yeh [18], and Boughanem et al.", "startOffset": 80, "endOffset": 84}, {"referenceID": 1, "context": "[2, 3].", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[2, 3].", "startOffset": 0, "endOffset": 6}, {"referenceID": 6, "context": "[7, 8], L\u00f3pez-Herrera et al.", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "[7, 8], L\u00f3pez-Herrera et al.", "startOffset": 0, "endOffset": 6}, {"referenceID": 21, "context": "[22, 21] and their references.", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[22, 21] and their references.", "startOffset": 0, "endOffset": 8}, {"referenceID": 41, "context": "1 is the Inductive Query By Example (IQBE) framework originated by Smith and Smith [42] and Chen et al.", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22] and Figure 2 for descriptions of a general IQBE system.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] and Smith and Smith [42], genetic programming [19] has gained ground as a robust choice for query learning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[20] and Smith and Smith [42], genetic programming [19] has gained ground as a robust choice for query learning.", "startOffset": 25, "endOffset": 29}, {"referenceID": 18, "context": "[20] and Smith and Smith [42], genetic programming [19] has gained ground as a robust choice for query learning.", "startOffset": 51, "endOffset": 55}, {"referenceID": 7, "context": "[8] and L\u00f3pez-Herrera et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22, 21].", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[22, 21].", "startOffset": 0, "endOffset": 8}, {"referenceID": 45, "context": "[46], the popularity of evolutionary algorithms is largely explained by their implicit parallelism which allows them to search different regions of the solution space simultaneously.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "Whereas classical relevance feedback methods, such as Rocchio [40], perform poorly if the initial query fails to retrieve relevant documents.", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "The probabilistic exploration induced by evolutionary algorithms permits them to search unexplored areas independent of the initial query [4].", "startOffset": 138, "endOffset": 141}, {"referenceID": 18, "context": "In effect, one could say that the purpose is to get the machine to generate a solution to the problem without being explicitly programmed [19].", "startOffset": 138, "endOffset": 142}, {"referenceID": 29, "context": "[30] andMedelyan et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[26], where a two-stage classifier is utilised to recognize those terms in the document which should act as Wikipedia-concepts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30], where a sequence of two classifiers is run to detect which terms should be linked to Wikipedia and to which Wikipedia-concepts do they correspond.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The fitness-function for a Wiki-query q \u2208 Q is defined as the mapping, F : (q,Dt) 7\u2192 c \u2208 [0, 1], which corresponds to the F-score within a given set of evaluation documents Dt \u2282 D:", "startOffset": 89, "endOffset": 95}, {"referenceID": 0, "context": "A voting function \u03bcA : D \u2192 [0, 1] is given by", "startOffset": 27, "endOffset": 33}, {"referenceID": 0, "context": "where F : (q\u0304, Dt) 7\u2192 c \u2208 [0, 1] is the Wiki-ES fitness function, which corresponds to the F-score within the training document set Dt; see Definition 4.", "startOffset": 26, "endOffset": 32}, {"referenceID": 45, "context": "[46], the relevant documents corresponding to the same topic can be dispersed into different regions of the document space, and thereby have quite different profiles.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "The aim of the proposed GP algorithm is to generate better fit queries using a mechanism inspired by biological evolution [35].", "startOffset": 122, "endOffset": 126}, {"referenceID": 30, "context": "[31], which was suitably modified and integrated into our framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "5) used as benchmarks in the experiment, where implemented using Weka [15] through Java-ML [1] package.", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "5) used as benchmarks in the experiment, where implemented using Weka [15] through Java-ML [1] package.", "startOffset": 91, "endOffset": 94}], "year": 2010, "abstractText": "Most of the existing information retrieval systems are based on bag of words model and are not equipped with common world knowledge. Work has been done towards improving the efficiency of such systems by using intelligent algorithms to generate search queries, however, not much research has been done in the direction of incorporating human-and-society level knowledge in the queries. This paper is one of the first attempts where such information is incorporated into the search queries using Wikipedia semantics. The paper presents an essential shift from conventional token based queries to concept based queries, leading to an enhanced efficiency of information retrieval systems. To efficiently handle the automated query learning problem, we propose Wikipedia-based Evolutionary Semantics (Wiki-ES) framework where concept based queries are learnt using a co-evolving evolutionary procedure. Learning concept based queries using an intelligent evolutionary procedure yields significant improvement in performance which is shown through an extensive study using Reuters newswire documents. Comparison of the proposed framework is performed with other information retrieval systems. Concept based approach has also been implemented on other information retrieval systems to justify the effectiveness of a transition from token based queries to concept based queries.", "creator": "LaTeX with hyperref package"}}}