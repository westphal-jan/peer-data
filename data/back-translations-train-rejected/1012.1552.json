{"id": "1012.1552", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2010", "title": "Bridging the Gap between Reinforcement Learning and Knowledge Representation: A Logical Off- and On-Policy Framework", "abstract": "Knowledge Representation is important issue in reinforcement learning. In this paper, we bridge the gap between reinforcement learning and knowledge representation, by providing a rich knowledge representation framework, based on normal logic programs with answer set semantics, that is capable of solving model-free reinforcement learning problems for more complex do-mains and exploits the domain-specific knowledge. We prove the correctness of our approach. We show that the complexity of finding an offline and online policy for a model-free reinforcement learning problem in our approach is NP-complete. Moreover, we show that any model-free reinforcement learning problem in MDP environment can be encoded as a SAT problem. The importance of that is model-free reinforcement", "histories": [["v1", "Tue, 7 Dec 2010 16:57:54 GMT  (676kb)", "http://arxiv.org/abs/1012.1552v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.LO", "authors": ["emad saad"], "accepted": false, "id": "1012.1552"}, "pdf": {"name": "1012.1552.pdf", "metadata": {"source": "CRF", "title": "Bridging the Gap between Reinforcement Learning and Knowledge Representation: A Logical Off- and On-Policy Framework", "authors": ["Emad Saad"], "emails": ["saad.e@gust.edu.kw"], "sections": [{"heading": null, "text": "In this paper, we build a bridge between reinforcement learning and knowledge representation by providing a comprehensive framework for knowledge representation, based on normal logic programs with response semantics, capable of solving model-free reinforcement problems in more complex areas and exploiting domain-specific knowledge. We prove the correctness of our approach. We show that the complexity of finding an offline and online policy for a model-free reinforcement learning problem is NP-complete in our approach. Furthermore, we show that any model-free reinforcement problem can be encoded as a SAT problem in MDP environments."}, {"heading": "1 Introduction", "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is not a country, but a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which it is a country, in which is a country, in which is a country, in which it is a country, in which is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which"}, {"heading": "2 Preliminaries", "text": "As in the underlying assumptions of the original Q-Learning and SARSA, the later results in this article assume that the MDPs considered are deterministic. Normal logic programs [7] as well as Q-Learning [4] and SARSA [17] are reviewed in this section."}, {"heading": "2.1 Normal Logic Programs", "text": "A Herbrand interpretation is a subset of the Herbrand base. A normal logic program is a finite set of rules of FormWhere are atoms and is the negation as failure. A normal logic program is grounded if there are no variables in one of its rules. A normal logic program should be a normal logic program and be a Herbrand interpretation, then we say that the above rule is fulfilled by iff, when and for some, A Herbrand model is a Herbrand interpretation that fulfills every rule. A Herbrand interpretation of a normal logic program should be an answer to whether the minimal Herbrand model (in terms of the specified inclusion) of reduction is where"}, {"heading": "2.2 Q-learning and SARSA", "text": "The Q value, in the face of a policy (a mapping of states to actions), is defined as the expected sum of discounted rewards resulting from the execution of actions in a state, and then the policy follows thereafter. Given, an optimal policy can be determined by determining the optimal actions in each state where it is optimal in a state, i.e., and is executable in. An episode is an exploration of the environment, which is a sequence of states in which the reward occurs, the form in which each remedy that an agent has executed and in which he receives a reward executes. An episode is a sequence of states, which is a sequence of states in which the reward occurs."}, {"heading": "3 Action Language", "text": "This section develops the syntax and semantics of the action language, which allows the representation of model-free amplification problems, which extends the action language [8]."}, {"heading": "3.1 Language syntax", "text": "A flowing letter is either a flowing letter, the negation of. A flowing formula is a combination of flowing words of form, where there are flowing dictionaries. Sometimes we abuse the notation and refer to a flowing formula as a series of flowing dictionaries (). An action theory is a tuple of form, where a sentence of form (3) is a set of sentences from (4-6) and is a discount factor as follows: Where there is a flowing literal formula, are conjunctive flowing formulas, is a real number in. Sentence (3) represents the set of possible initial states. Statement (4) states that an action is executable in any state in which any variable that also occurs."}, {"heading": "3.2 Semantics", "text": "We say that a set of literals is contained in if, otherwise it does not hold. We say that a set of literals fulfills an indirect effect of the action of the form (5) if it belongs incrementally to what is contained or not contained in the form. Let's be an action theory in and a series of literals. Then, the smallest set of literals that contains all indirect effects of actions and satisfies. A state is a complete and consistent series of literals that defines all indirect effects of actions in. Definition 1 Let an action theory in, a state be a sentence in. Then, the state that results from the execution of actions in, gifts that are executable in the state in which the reward is well defined by calculation. An episode in is an expression of the form where for each definition 2 can be a preference in. Then, the state that can be executed out of the calculation, which can be executed in the way that."}, {"heading": "4 Off- and On-Policy Model-free Reinforcement Learning Using Answer Set Programming", "text": "This year, the number of Indonesian deaths in the US is many times higher than the number of deaths in the US."}, {"heading": "5 Correctness", "text": "In fact, we will be able to find a solution with which we can identify, \"he told the German Press Agency in an interview.\" I am very satisfied, \"he said,\" but it is not yet as if we are able to find a solution. \""}, {"heading": "6 Conclusions and Related Work", "text": "We have described a high action language called \"dynamic programming,\" which allows the representation of model-free amplification problems in MDP environments. Furthermore, we have introduced online and offline a logical framework for model-free amplification learning, by linking model-free amplification learning in the MDP environment with normal logic programs that are able to represent and argue MDPs and actions with probabilistic effects. Translating from a action theory into a normal logic program builds on similar translations described in [24, 18, 19] The literature is rich in action languages that are able to represent and argue these MDPs and actions with probabilistic effects that include [1, 2, 6, 9, 12] The main difference between these languages and is that the factored characterization of MDP for model-free amplification learning is possible. Many approaches to the solution of learning from MDP are presented as optimal for both the MDP policy and the MDP policy for the first level."}], "references": [{"title": "Reasoning about actions in a probabilistic setting", "author": ["C. Baral", "N. Tran", "L.C. Tuan"], "venue": "AAAI, ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Decision-theoretic planning: structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "Journal of AI Research, 11(1), ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Symbolic dynamic programming for first-order mdps", "author": ["C. Boutilier", "R. Reiter", "B. Price"], "venue": "17th IJCAI, ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning from delayed rewards", "author": ["Christopher C. Watkins"], "venue": "Ph.D. dissertation, University of Cambridge,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "Improving elevator performance using reinforcement learning", "author": ["R. Crites", "A. Barto"], "venue": "Advances in Neural Information Processing, ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "probabilistic reasoning about actions in nonmonotonic causal theories", "author": ["T. Eiter", "T. Lukasiewicz"], "venue": "19th Conference on UAI, ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "The stable model semantics for logic programming", "author": ["M. Gelfond", "V. Lifschitz"], "venue": "ICSLP. MIT Pres, ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1988}, {"title": "Action languages", "author": ["M. Gelfond", "V. Lifschitz"], "venue": "Electronic Transactions on AI, 3(16), 193\u2013210, ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Reasoning about actions with sensing under qualitative and probabilistic uncertainty", "author": ["L. Iocchi", "T. Lukasiewicz", "D. Nardi", "R. Rosati"], "venue": "16th European Conference on Artificial Intelligence , ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Reinforcement learning: A survey", "author": ["L. Kaelbling", "M. Littman", "A. Moore"], "venue": "JAIR, 4, 237\u2013285, ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Logical markov decision programs and the convergence of logical td(\u03bb)", "author": ["K. Kersting", "L. De Raedt"], "venue": "14th International Conference on Inductive Logic Programming, ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "An algorithm for probabilistic planning", "author": ["N. Kushmeric", "S. Hanks", "D. Weld"], "venue": "Artificial Intelligenc , 76(1-2), 239\u2013286, ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1995}, {"title": "Assat: Computing answer sets of a logic program by sat solvers", "author": ["F. Lin", "Y. Zhao"], "venue": "Artificial Intelligence , 157(1-2), 115\u2013137, ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2004}, {"title": "The computational com-plexity of probabilistic planning", "author": ["M. Littman", "J. Goldsmith", "M. Mundhenk"], "venue": "Journal of Artificial Intelligence Re-search, 9, 1\u201336, ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Maxplan: A new approach to probabilistic planning", "author": ["S. Majercik", "M. Littman"], "venue": "Fourth International Conference on Artificial Intelligence Planning, pp. 86\u201393, ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Contingent planning under uncertainty via stochastic satisfiability", "author": ["S. Majercik", "M. Littman"], "venue": "Artificial Intelligence , 147(1-2), 119\u2013 162, ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Online q-learning using connectionist systems", "author": ["G. Rummery", "M. Niranjan"], "venue": "Technical report, CUED/F-INFENG/TR166, Cambridge University, ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "Probabilistic planning in hybrid probabilistic logic programs", "author": ["E. Saad"], "venue": "1st International Conference on Scalable Uncertainty Management, ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "A logical framework to reinforcement learning using hybrid probabilistic logic programs", "author": ["E. Saad"], "venue": "Second International Conference on Scalable Uncertainty Management, ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "On the relationship between hybrid probabilistic logic programs and stochastic satisfiability", "author": ["E. Saad"], "venue": "Second International Conference on Scalable Uncertainty Management, ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic planning with imperfect sensing actions using hybrid probabilistic logic programs", "author": ["E. Saad"], "venue": "Third International Conference on Scalable Uncertainty Management, ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Probabilistic reasoning by sat solvers", "author": ["E. Saad"], "venue": "Tenth ECSQARU, ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "A new approach to hybrid probabilistic logic programs", "author": ["E. Saad", "E. Pontelli"], "venue": "Annals of Mathematics and Artificial Intelligence , 48(3- 4), 187\u2013243, ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "Domain-dependent knowl-edge in answer set planning", "author": ["T. Son", "C. Baral", "T. Nam", "S. McIlraith"], "venue": "ACM Transactions on Computational Logic, 7(4), 613\u2013657, ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 3, "context": "of the agent\u2019s limited computational abilities to consider all states systematically [4].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "Therefore, Q-learning [4] and SARSA [17] are proposed as model-free reinforcement learning algorithms that learn", "startOffset": 22, "endOffset": 25}, {"referenceID": 16, "context": "Therefore, Q-learning [4] and SARSA [17] are proposed as model-free reinforcement learning algorithms that learn", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "Moreover, dynamic programming methods use primitive representation of states and actions and do not exploit domain-specific knowledge of the problem domain, in addition they solve MDP with relatively small domain sizes [16].", "startOffset": 219, "endOffset": 223}, {"referenceID": 18, "context": "A logical framework to model-based reinforcement learning has been proposed in [19] that overcomes the representational limitations of dynamic programming methods and capable of representing domain specific knowledge.", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "The framework in [19] is based on the integration of model-based reinforcement learning in MDP environment with normal hybrid probabilistic logic programs with probabilistic answer set semantics [23] that allows representing and reasoning about a variety of fundamental probabilistic reasoning problems including probabilistic planning [18], contingent probabilistic planning [21], the most probable explanation in belief networks, and the most likely trajectory [20].", "startOffset": 17, "endOffset": 21}, {"referenceID": 22, "context": "The framework in [19] is based on the integration of model-based reinforcement learning in MDP environment with normal hybrid probabilistic logic programs with probabilistic answer set semantics [23] that allows representing and reasoning about a variety of fundamental probabilistic reasoning problems including probabilistic planning [18], contingent probabilistic planning [21], the most probable explanation in belief networks, and the most likely trajectory [20].", "startOffset": 195, "endOffset": 199}, {"referenceID": 17, "context": "The framework in [19] is based on the integration of model-based reinforcement learning in MDP environment with normal hybrid probabilistic logic programs with probabilistic answer set semantics [23] that allows representing and reasoning about a variety of fundamental probabilistic reasoning problems including probabilistic planning [18], contingent probabilistic planning [21], the most probable explanation in belief networks, and the most likely trajectory [20].", "startOffset": 336, "endOffset": 340}, {"referenceID": 20, "context": "The framework in [19] is based on the integration of model-based reinforcement learning in MDP environment with normal hybrid probabilistic logic programs with probabilistic answer set semantics [23] that allows representing and reasoning about a variety of fundamental probabilistic reasoning problems including probabilistic planning [18], contingent probabilistic planning [21], the most probable explanation in belief networks, and the most likely trajectory [20].", "startOffset": 376, "endOffset": 380}, {"referenceID": 19, "context": "The framework in [19] is based on the integration of model-based reinforcement learning in MDP environment with normal hybrid probabilistic logic programs with probabilistic answer set semantics [23] that allows representing and reasoning about a variety of fundamental probabilistic reasoning problems including probabilistic planning [18], contingent probabilistic planning [21], the most probable explanation in belief networks, and the most likely trajectory [20].", "startOffset": 463, "endOffset": 467}, {"referenceID": 18, "context": "[19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Normal logic programs [7] and Q-learning [4] and SARSA [17] are reviewed in this section.", "startOffset": 22, "endOffset": 25}, {"referenceID": 3, "context": "Normal logic programs [7] and Q-learning [4] and SARSA [17] are reviewed in this section.", "startOffset": 41, "endOffset": 44}, {"referenceID": 16, "context": "Normal logic programs [7] and Q-learning [4] and SARSA [17] are reviewed in this section.", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "Under the same convergence assumptions as in Q-learning, SARSA [17] has been developed as an online model-free reinforcement learning algorithm, that learns optimal Q-function while exploring the environment.", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "ment learning problems, which extends the action language [8].", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "Example 1 Consider an elevator of n-story building domain adapted from [5] that is represented by an action theory, is described by (7) ( is a particular value in ) and is represented by (8)-(14).", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "Notice that, unlike [4], by using (15) and (16), Q-learning can be computed online during the exploration of the environment as well as offline.", "startOffset": 20, "endOffset": 23}, {"referenceID": 23, "context": "This translation follows some related translations described in [24, 18, 19].", "startOffset": 64, "endOffset": 76}, {"referenceID": 17, "context": "This translation follows some related translations described in [24, 18, 19].", "startOffset": 64, "endOffset": 76}, {"referenceID": 18, "context": "This translation follows some related translations described in [24, 18, 19].", "startOffset": 64, "endOffset": 76}, {"referenceID": 12, "context": "are equivalent to the answer sets of [13].", "startOffset": 37, "endOffset": 41}, {"referenceID": 21, "context": "The transformation step from nor-mal logic program encoding of a model-free reinforcement learning problem into SAT can be avoided, by encoding a modelfree reinforcement learning problem directly into SAT [22].", "startOffset": 205, "endOffset": 209}, {"referenceID": 13, "context": "The flat representation of reinforcement learning problem domains is the explicit enumeration of world states [14].", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "Hence, Theorem 5 follows directly from Theorem 4 [14].", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "The translation from an action theory in into a normal logic program builds on similar translations described in [24, 18, 19].", "startOffset": 113, "endOffset": 125}, {"referenceID": 17, "context": "The translation from an action theory in into a normal logic program builds on similar translations described in [24, 18, 19].", "startOffset": 113, "endOffset": 125}, {"referenceID": 18, "context": "The translation from an action theory in into a normal logic program builds on similar translations described in [24, 18, 19].", "startOffset": 113, "endOffset": 125}, {"referenceID": 0, "context": "The literature is rich with action languages that are capable of represent-ing and reasoning about MDPs and actions with probabilistic effects, which include [1, 2, 6, 9, 12].", "startOffset": 158, "endOffset": 174}, {"referenceID": 1, "context": "The literature is rich with action languages that are capable of represent-ing and reasoning about MDPs and actions with probabilistic effects, which include [1, 2, 6, 9, 12].", "startOffset": 158, "endOffset": 174}, {"referenceID": 5, "context": "The literature is rich with action languages that are capable of represent-ing and reasoning about MDPs and actions with probabilistic effects, which include [1, 2, 6, 9, 12].", "startOffset": 158, "endOffset": 174}, {"referenceID": 8, "context": "The literature is rich with action languages that are capable of represent-ing and reasoning about MDPs and actions with probabilistic effects, which include [1, 2, 6, 9, 12].", "startOffset": 158, "endOffset": 174}, {"referenceID": 11, "context": "The literature is rich with action languages that are capable of represent-ing and reasoning about MDPs and actions with probabilistic effects, which include [1, 2, 6, 9, 12].", "startOffset": 158, "endOffset": 174}, {"referenceID": 9, "context": "These approaches can be classified into two main categories of approaches; dynamic programming approaches and the search-based approaches (a detailed survey on these approaches can be found in [10, 2]).", "startOffset": 193, "endOffset": 200}, {"referenceID": 1, "context": "These approaches can be classified into two main categories of approaches; dynamic programming approaches and the search-based approaches (a detailed survey on these approaches can be found in [10, 2]).", "startOffset": 193, "endOffset": 200}, {"referenceID": 14, "context": "A logic based approach for solving MDP, for probabilistic plan-ning, has been presented in [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "The approach of [15] converts MDP specification of a probabilistic planning problem into a stochastic satisfiability problem and solving the stochastic satisfiability problem instead.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "First-order logic representation of MDP for model-based reinforcement learning has been described in [11] based on first-order logic programs without nonmonotonic negations.", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "Similar to the firstorder representation of MDP in [11], allows objects and relations.", "startOffset": 51, "endOffset": 55}, {"referenceID": 10, "context": "However, unlike , [11] finds policies in the abstract level.", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "expressive first-order representation of MDP than [11] has been presented in [3] that is a probabilistic extension to Reiter\u2019s situation calculus.", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "expressive first-order representation of MDP than [11] has been presented in [3] that is a probabilistic extension to Reiter\u2019s situation calculus.", "startOffset": 77, "endOffset": 80}, {"referenceID": 10, "context": "Although more expressive, it is more complex than [11].", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "Unlike the logical model-based reinforcement learning frame-work of [19] that uses normal hybrid probabilistic logic programs to encode model-based reinforcement learning problems, normal logic program with answer set semantics is used to encode model-free reinforcement learning problems.", "startOffset": 68, "endOffset": 72}], "year": 2010, "abstractText": "Knowledge Representation is important issue in reinforcement learning. In this paper, we bridge the gap between reinforcement learning and knowledge representation, by providing a rich knowledge representation framework, based on normal logic programs with answer set semantics, that is capable of solving model-free reinforcement learning problems for more complex domains and exploits the domain-specific knowledge. We prove the correctness of our approach. We show that the complexity of finding an offline and online policy for a model-free reinforcement learning problem in our approach is NP-complete. Moreover, we show that any model-free reinforcement learning problem in MDP environment can be encoded as a SAT problem. The importance of that is model-free reinforcement learning problems can be now solved as SAT problems.", "creator": "Microsoft\u00ae Office Word 2007"}}}