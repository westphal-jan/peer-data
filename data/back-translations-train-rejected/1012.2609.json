{"id": "1012.2609", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Dec-2010", "title": "Inverse-Category-Frequency based supervised term weighting scheme for text categorization", "abstract": "Unsupervised term weighting schemes, borrowed from information retrieval field, have been widely used for text categorization and the most famous one is tf.idf. The intuition behind idf seems less reasonable for TC task than IR task. In this paper, we introduce inverse category frequency into supervised term weighting schemes and propose a novel icf-based method. The method combines icf and relevance frequency (rf) to weight terms in training dataset. Our experiments have shown that icf-based supervised term weighting scheme is superior to tf.rf and prob-based supervised term weighting schemes and tf.idf based on two widely used datasets, i.e., the unbalanced Reuters-21578 corpus and the balanced 20 Newsgroup corpus. We also present the detailed evaluations of each category of the two datasets among the four term weighting schemes on precision, recall and F1 measure.", "histories": [["v1", "Mon, 13 Dec 2010 01:22:36 GMT  (256kb)", "http://arxiv.org/abs/1012.2609v1", "this is a paper about a new supervised term weighting scheme"], ["v2", "Tue, 14 Dec 2010 09:26:49 GMT  (230kb)", "http://arxiv.org/abs/1012.2609v2", "this is a paper about a new supervised term weighting scheme"], ["v3", "Sat, 24 Dec 2011 02:34:31 GMT  (0kb,I)", "http://arxiv.org/abs/1012.2609v3", "The paper is withdrawn"], ["v4", "Wed, 6 Jun 2012 03:29:13 GMT  (348kb)", "http://arxiv.org/abs/1012.2609v4", "18 pages"]], "COMMENTS": "this is a paper about a new supervised term weighting scheme", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["deqing wang", "hui zhang"], "accepted": false, "id": "1012.2609"}, "pdf": {"name": "1012.2609.pdf", "metadata": {"source": "CRF", "title": "Inverse category frequency based supervised term weighting scheme for text categorization", "authors": ["Deqing Wang", "Hui Zhang", "Wenjun Wu"], "emails": ["dqwang@nlsde.buaa.edu.cn", "hzhang@nlsde.buaa.edu.cn", "wwj@nlsde.buaa.edu.cn"], "sections": [{"heading": null, "text": "The intuition underlying idf seems less useful for TC tasks than for IR tasks. In this paper, we introduce inverse category frequency into monitored term weight schemes and propose a novel icf-based methodology that combines icf and relevance frequency (rf) with weight terms in the training dataset. Our experiments have shown that icf-based monitored term weight schemes tf.rf and prob-based monitored term weight schemes and tf.idf are superior based on two widely used datasets, namely the unbalanced Reuters-21578 corpus and the balanced 20 newsgroup corpus. In addition, we present the detailed ratings of each category of the two datasets under the four term weight schemes based on precision, recall and F1 measure. Keywords: Unmonitored term weight schemes; monitored text category; text weight categories; inverses; text categories;"}, {"heading": "1. Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Motivation", "text": "In fact, it is not the case that it is a way of thinking, but a way of thinking. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "1.2 Related work", "text": "In recent years, TC has been widely used in many fields of application, such as spam email classification, question categorization, and online news classification; many TC techniques have been researched in literature, such as centroid-based classifiers, kNN (Li, Lu, & Yu, 2004), Na\u00efve Bayes (McCallum & Nigam, 1998), decision tree (Quinlan, 1993), and support vector machines (Cortes & Vapnik, 1995); as this paper focuses on text representation, readers interested in TC techniques can find details from Sebastiani (2002). Text representation comes from the IR field; the famous model of text representation is tf.idf, which has had great success in IR."}, {"heading": "2. Overview of term weighting schemes", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Unsupervised term weighting schemes", "text": "The intuition of idf in IR is that a term that occurs in many documents is not a good discriminator and should give less weight than one that occurs in a few documents, and the measures are a heuristic implementation of this intuition (Robertson, 2004). Debole & Sebastiani (2003) concluded three assumptions about tf.idf model, i.e. (\u0456) idf assumption: rare terms are no less important than common terms; (ii) tf assumption: multiple occurrences of a term in a document are no less important than individual occurrences; (iii) normalization assumption: long documents are no more important than short documents for the same quantity of term matching. According to these assumptions, researchers have proposed many variants of the tf.idf model."}, {"heading": "2.2 Supervised term weighting schemes", "text": "The researchers have pointed out that tf.idf may not be the best text representation for TC tasks. Therefore, Debole & Sebastiani (2003) is a proposal for the reviewed term weighting programs. They have introduced term (property) into term weighting programs and have considered the phases of term weighting as an activity of supervised learning in which information about membership in training documents is used in categories. As later studies have shown that the use of metrics (such as information gain, profit ratio, and so on) to weight term is no better than traditional tf.idf models (Liu, Loh, and Sun, 2009), we will not give a detailed discussion of this method to readers who are interested in the method. Debole & Sebastiani (2003).Recently, two new reviewed term weighting programs have been proposed, d.rf and prob-based measures introduced."}, {"heading": "3. A novel ICF-based supervised term weighting scheme", "text": "In this thesis, we introduce inverse category frequency (icf) into supervised term weight scheme for telecom tasks. Two concepts are defined as category frequency (cf): the number of categories in which term ti occurs. Inverse category frequency (icf): The formula of ICF is similar to idf and it is expressed as a factor in their question categorization problem. Meanwhile, Guan, Zhou, & Guo (2009) have adopted ICF to construct centric vector of category for centered classifiers and their class feature centroid classifiers."}, {"heading": "4. Data corpora and benchmark methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Reuters-21578 (unbalanced corpus) & 20Newsgroup (balanced corpus)", "text": "Reuters-21578: The Reuters-21578 dataset \u2020 is a widely used benchmarking collection. Our dataset is based on the version of Trinity College Dublin that changed documents from the original SGML format to the XML format. In accordance with the ModApte split, we received a collection of 52 categories after removing blank documents and documents with more than one class name. 6532 training documents and 2568 test documents remain. To compare the results of Lan et al. (2009), we select the 10 largest categories. Training materials and test documents in the 10 best categories are listed in Table 3. One edition of the Reuters corpus is the distribution problem of the distorted categories. Among the 10 most common categories, 43% of the total categories (earn) are accounted for. http: / / ronaldo.cs.tcd.ie / esslli07 / sw / step01.tgzset, while the rarest category (coffee) contains only 99 training texts containing 1.5%. \""}, {"heading": "4.2 Classifier: Support Vector Machines", "text": "We choose the most advanced algorithm, i.e. the SVM-based classifier, as our default classifier, because SVM performs almost top-notch among the widely used classification algorithms (Joachims, 1997; Sebastiani, 2002; Lan et al., 2009). Lan et al. also adopted kNN, but their experiments showed that kNN is inferior to SVM at both the macro- and micro-F1 levels. We use the radial base function (RBF) as the core function of the SVM classifier implemented in the LibSVM package (Chang, & Lin, 2001), while Lan et al. (2009) adopted the linear kernel. Our experimental results have shown that the RBF kernel works better than the linear kernel. In each experiment, we use the Grid.py (a python script) to determine the optimal core parameters of C and Rg."}, {"heading": "4.3 Performance measures", "text": "The two parameters are popular performance parameters for TC tasks. As a measure of effectiveness that combines the contributions of p and r, we use the well-known Formula 1 function (Lewis, 1995), defined as 1 (7). Normally, Formula 1 is estimated in two ways, i.e. by Formula 1 macro averaging (macro-F1) and Formula 1 micro averaging (micro-F1). Macro-F1 gives all categories the same weight and is therefore mainly influenced by Formula 1 of rare categories for distorted Reuters-25718 corpus. On the contrary, micro-F1 is dominated by the performance of common categories for distorted Reuters-25718 corpus. Therefore, the macro-F1 and micro-F1 categories of Reuters-25718 can lead to very different results. Due to the balance of 20 newsgroups, the macro-F1 and micro-F1 of 20 newsgroups are quite similar."}, {"heading": "5. Results and discussions", "text": "We have set up 80 groups of experiments (40 groups of binary classification experiments for Reuters-21578 top 10 categories corpus and 40 groups of binary classification experiments for 20 newsgroup corpus) to test the validation of the icf-based, monitored term weighting scheme. For each category, we use \"local guidelines\" to construct training sets and test sets. In each experiment, a selected category c is marked as a positive category, and the remaining categories in the training corpus are combined. http: / / kdd.ics.uci.edu / databases / 20newsgroup together as a negative category."}, {"heading": "5.1 Overall performance", "text": "Model model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model-model"}, {"heading": "5.2 Results on Reuters-21578", "text": "To give a more detailed view of the top 10 categories on Reuters-21578, Table 4 compares the results based on precision, recall and F1 values. However, we can conclude that icf-based SWT and tf.rf SWT exceed the prob-based SWT on F1 measure because only F1 value of the raw sugar category of the prob-based models is very close to that of tf.rf and icf-based systems, and the remaining F1 values of the prob-based monitored term weighting schemes are all inferior to tf.rf and icf-based. Meanwhile, since Reuters-21578 top-10 categories corpus is unbalanced, we expect that prob-based term weighting schemes could achieve better performance on F1. However, we do not have expectations. There are two same categories (i.e. raw and ship categories) in our corpus, and the Wu and Sun categories are used, but it is difficult to compare them on Sun & Sun."}, {"heading": "5.3 Results on 20 Newsgroup", "text": "Since the prob-based, monitored weighting scheme has the same shape as tf.rf when the corpus is balanced and has partial results (we randomly selected six categories from 20 newsgroups and constructed six groups of experiments with prob-based SWT), we have shown that its performance is worse than that of tf.rf. In the meantime, we do not make comparisons between icf-based and tf.idf on balanced 20 newsgroup corpus, because tf.rf consistently performs best according to the experiments by Lan et al. (2009). They have many comparative results between tf.rf and tf.idf. Here, we only give the detailed comparison between icf-based SWT and tf.rf SWT on balanced 20 newsgroup corpus, as shown in Table 5. We can find that the F1 values between tf.rf and tf.rf-based SWT are very closely related to this precedent, with this phenomenon being the 19th SWT."}, {"heading": "5.4 Discussions", "text": "Through the above experiments, we can conclude that our icf-based and tf.rf-monitored terminology weighting schemes have improved the performance of text categorization compared to tf.idf and prob-based. However, the traditional tf.idf also has its own superiority and outperforms some monitored terminology weighting schemes (e.g. prob-based, tf-based and tf.ig).We should point out that the above conclusions are made in combination with SVM classifiers with respect to macro-F1 and micro-F1 and other controlled settings. Lan et al. (2009) have not presented detailed results for each category, here we present the results for precision, memory and F1 measurement, and express some discussions about these results that later researchers will provide detailed data when focusing on monitored terminology weighting schemes on Reuters-21578 corpus and 20 newsgroup corpus."}, {"heading": "6. Conclusion and future work", "text": "Compared to unsupervised terminology weighting, supervised terminology weighting for TC tasks has become an important research point. Known category information of terms can be applied to text categorization, this paper combines inverse category frequency and relevance frequency (rf is proposed by Lan et al., 2009) and proposes a novel icf-based supervised terminology weighting scheme. Introduction of the icf can match term weighting scheme that occurs in many categories. Our experimental results and extensive comparisons based on two common corporations, namely the distorted Reuters-21578 and the balanced 20 newsgroup, have shown that the icf-based supervised terminology weighting scheme performs better than tf.rf and prob-based two supervised terminology weighting schemes and a traditional tf.idf terminology weighting scheme will perform better than tf.rf and prob-based two supervised terminology weighting schemes and a traditional SVM weighting scheme."}, {"heading": "Acknowledgment", "text": "We thank Guan Hu et al. for distributing their software to help us analyze the 20 newsgroup corpus. Source code of their software is available at http: / / epcc.sjtu.edu.cn / ~ hzhou / research / cfc. References Chang, C. C., & Lin, C. J. (2001). LIBM: A Library for Support Vector Machines, http: / / www.csie.ntu.edu.tw / cjlin / libsvm.Cortes, C., & Vapnik, V. (1995). Support-Vector Networks. Machine Learning, 20, 273-297.Debole, F., Sebastiani, F. (2003). Supervised term weighting for automated text categorization. In Proceedings of the 2003 ACMsymposium on Applied Computing (pp. 784-788). Melbourne, Florida, USA.Guan, H., Zhou, J., Guo, M. (2009)."}, {"heading": "24 (5), 513\u2013523.", "text": "Sebastiani, F. (2002). Machine learning in automated text categorization. ACM Computing Surveys (CSUR), 34 (1), 1-47.Xue, X. B., Zhou, Z. H. (2009). Distributional Features for Text Categorization. IEEE Transactions on Knowledge and DataEngineering, 21 (3), 428-442.Yang, Y., Liu, X. (1999). A Re-examination of text categorization methods. In Proc. of Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval (SIGIR), ACM Press New York, NY, USA, 42-49. Zipf, G. K. (1949)."}], "references": [{"title": "LIBSVM: A Library for Support Vector Machines, http://www.csie.ntu.edu.tw/ cjlin/libsvm", "author": ["C.C. Chang", "C.J. Lin"], "venue": "Support-Vector Networks. Machine Learning,", "citeRegEx": "Chang and Lin,? \\Q2001\\E", "shortCiteRegEx": "Chang and Lin", "year": 2001}, {"title": "A Statistical Interpretation of Term Specificity and Its Application in Retrieval", "author": ["K.S. Jones"], "venue": "J. Documentation,", "citeRegEx": "Jones,? \\Q2004\\E", "shortCiteRegEx": "Jones", "year": 2004}, {"title": "Text categorization with support vector machines \u2013 How to represent texts in input space", "author": ["E. Leopold", "J. Kindermann"], "venue": "Machine Learning,", "citeRegEx": "Leopold and Kindermann,? \\Q2002\\E", "shortCiteRegEx": "Leopold and Kindermann", "year": 2002}, {"title": "frequency approach and mutual information algorithm", "author": ["X. J", "W.Y. Liu"], "venue": "PROGRESS IN NATURAL SCIENCE,", "citeRegEx": "J. and Liu,? \\Q1993\\E", "shortCiteRegEx": "J. and Liu", "year": 1993}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["G. Salton", "C. Buckley"], "venue": "Information Processing & Management,", "citeRegEx": "Salton and Buckley,? \\Q1988\\E", "shortCiteRegEx": "Salton and Buckley", "year": 1988}, {"title": "A re-examination of text categorization methods", "author": ["Y. Yang", "X. Liu"], "venue": "In Proc. of Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval(SIGIR),", "citeRegEx": "Yang and Liu,? \\Q1999\\E", "shortCiteRegEx": "Yang and Liu", "year": 1999}], "referenceMentions": [{"referenceID": 1, "context": "idf (term frequency and inverse document frequency) proposed by Jones (Jones, 1972; Jones, 2004).", "startOffset": 70, "endOffset": 96}, {"referenceID": 1, "context": "The term weighting schemes often affect the effectiveness of TC, Leopold and Kindermann (2002) pointed out that the performance of SVM classifiers is dominated by text representation schemes, rather than kernel functions.", "startOffset": 65, "endOffset": 95}, {"referenceID": 1, "context": "idf (term frequency and inverse document frequency) proposed by Jones (Jones, 1972; Jones, 2004). Robertson (2004) tried to present the theoretical justifications of both idf and tf.", "startOffset": 64, "endOffset": 115}], "year": 2010, "abstractText": "Unsupervised term weighting schemes, borrowed from information retrieval field, have been widely used for text categorization and the most famous one is tf.idf. The intuition behind idf seems less reasonable for TC task than IR task. In this paper, we introduce inverse category frequency into supervised term weighting schemes and propose a novel icf-based method. The method combines icf and relevance frequency (rf) to weight terms in training dataset. Our experiments have shown that icf-based supervised term weighting scheme is superior to tf.rf and prob-based supervised term weighting schemes and tf.idf based on two widely used datasets, i.e., the unbalanced Reuters-21578 corpus and the balanced 20 Newsgroup corpus. We also present the detailed evaluations of each category of the two datasets among the four term weighting schemes on precision, recall and F1 measure.", "creator": "PScript5.dll Version 5.2"}}}