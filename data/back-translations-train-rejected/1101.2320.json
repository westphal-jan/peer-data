{"id": "1101.2320", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Jan-2011", "title": "Review and Evaluation of Feature Selection Algorithms in Synthetic Problems", "abstract": "The main purpose of Feature Subset Selection is to find a reduced subset of attributes from a data set described by a feature set. The task of a feature selection algorithm (FSA) is to provide with a computational solution motivated by a certain definition of relevance or by a reliable evaluation measure. In this paper several fundamental algorithms are studied to assess their performance in a controlled experimental scenario. A measure to evaluate FSAs is devised that computes the degree of matching between the output given by a FSA and the known optimal solutions. An extensive experimental study on synthetic problems is carried out to assess the behaviour of the algorithms in terms of solution accuracy and size as a function of the relevance, irrelevance, redundancy and size of the data samples. The controlled experimental conditions facilitate the derivation of better-supported and meaningful conclusions.", "histories": [["v1", "Wed, 12 Jan 2011 10:49:51 GMT  (238kb)", "http://arxiv.org/abs/1101.2320v1", "13 pages, 3 figures"]], "COMMENTS": "13 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["l a belanche", "f f gonz\\'alez"], "accepted": false, "id": "1101.2320"}, "pdf": {"name": "1101.2320.pdf", "metadata": {"source": "CRF", "title": "Feature Selection Algorithms in Synthetic Problems", "authors": ["L.A. Belanche"], "emails": ["belanche@lsi.upc.edu", "fgonzalez@lsi.upc.edu"], "sections": [{"heading": null, "text": "ar Xiv: 110 1.23 20v1 [cs.AKeywords: Feature Selection Algorithms; Empirical Evaluations; Attributes Relevance and Redundancy."}, {"heading": "1 INTRODUCTION", "text": "The main advantage of correct selection is the improvement of the inductive learner, either in terms of learning speed, generalization capacity or simplicity of the induced model. On the other hand, there is the scientific benefit associated with a lower number of characteristics: a reduced measurement of costs and hopefully a better understanding of the area. A feature selection algorithm (FSA) is a computerized solution that should be guided by a certain definition of subset relevance, although this definition is implicit or pursued in a loose sense. This is because the relevance of a feature can have multiple definitions depending on the exact goal being sought (Caruana and Freitag, 1994). Therefore, there is a need to rely on common sense, which makes it possible to adequately decide which algorithm to use (or not)."}, {"heading": "2 MOTIVATION AND RELATED WORK", "text": "In fact, most of them will be able to play by the rules they have set themselves in order to play by the rules."}, {"heading": "3 THE FEATURE SELECTION PROBLEM", "text": "The continuous feature selection problem (also called feature weighing) refers to the selection of a subset of features that collectively maximize a given metric in relation to its theoretical relevance, which can be done directly as many FSAs do (Almuallim and Dietterich, 1991; Caruana and Freitag, 1994; Hall, 1999), or the definition of an intersection in the output of Continuous Problem Solving. Although both types can be seen in a uniform way (the latter case corresponds to the allocation of weights in {0, 1}), these are entirely different problems that reflect different design goals."}, {"heading": "4 FEATURE SUBSET SELECTION ALGORITHMS", "text": "The relationship between an FSA and the inductive learning method used to infer a model can take three main forms: filters, wrappers, or embedded, which we call the mode: Embedded Mode: The inducer has its own FSA (either explicitly or implicitly); the methods for inducing logical conjunctions (Vere, 1975; Winston, 1975), decision trees, or artificial neural networks are examples of this embedding; Filter Mode: If the feature selection takes place before the introductory step, the former can be considered a filter (of unuseful features); in a general sense, it can be considered a special case of the embedded mode in which the feature selection is used as pre-processing; the filter mode is then independent of the inducer that evaluates the model after the selection process; wrapper mode: Here the relationship is taken inversely: The FSA uses the learning algorithm as a subroutine."}, {"heading": "4.1 Algorithm LVF", "text": "Lvf (Las Vegas Filter) (Liu and Setiono, 1998a) is a type 2 algorithm that repeatedly generates random subsets and calculates the consistency of the sample: an inconsistency in X and S is defined as two instances in S that are equal if one considers only the attributes in X and belongs to the different classes. The goal is to find the minimum subset of attributes that leads to inconsistencies in S. The inconsistency number of an instance A \"S\" is defined as: ICX \"(A) = X\" (A) \u2212 max \"X\" k \"(A) (1), where X\" A \"is the number of instances in S equal to A if one uses only the attributes in X\" and X \"k\" (A). The inconsistency rate of a substance in S \"and X\" k \"(A) is the number of instances in S of class A, with X\" k \"being max\" (A) (A) being used."}, {"heading": "4.2 Algorithm LVI", "text": "Lvi (Las Vegas incremental) is also a type 2 algorithm and an evolution of Lvi. It is based on the reasoning that it is not necessary to use the entire sample S for input: max \u2212 the maximum number of cases in which a too small share of the savings of J \u2212 eva luat i on measure S (X) \u2212 a sample S descr ibed by X, | X | = n Output: L \u2212 a l equ i va l en s o l u t o s foundL: = [] / L s t o r e s equa l y good s s Best: = X / / I n i t i a l i a l e bes s o l u n J0: = J (S (X))) / / minimal al low value o f J r epeat e: l imesX \u2032 l imesX (L) (Best) i f J (S (X)."}, {"heading": "4.3 Algorithm RELIEF", "text": "Relief (Kira and Rendell, 1992) is a general type algorithm that works exclusively in filter mode = J = 1. The algorithm randomly selects an instance and finds its near-hit and its near-miss. The former is the closest instance to me of all instances in a different class, but the latter is the closest instance to me of all instances in a different class. The underlying idea is that the more it separates me and its near-miss, the more relevant a feature is to me, and the least it separates me from its near-miss. The result is a weighted version of the original feature set. The algorithm for two classes is described as an algorithm f. If the costs are only the quantities, the algorithm can be used to simulate a type-1 scenario by iteratively verifying the input: the maximum \u2212 number of inputs, or the maximum \u2212 number of inputs."}, {"heading": "4.4 Algorithms SFG/SBG", "text": "These two are classic general algorithms that can operate in filter or wrapper mode. Sfg (Sequential Forward Generation) adds iterative features to an initial subset and tries to improve a measure J, always taking into account the features already selected. Consequently, an ordered list can also be obtained. Sbg (Sequential Backward Generation) is the reverse counterpart. They are jointly described as Algorithm 4. If the number of features is low, Doak (1992) reports that Sbg tends to perform better than Sfg, most likely because Sbg evaluates the contribution of all features from the outset. Furthermore, Aha and Bankert (1995) point out that Sfg is preferable when the number of relevant features is (famously) small; otherwise, Sbg should be used. Interestingly, it has also been reported that Sbg does not always perform better than Sbducg, contrary to the conclusions in Doak (1992)."}, {"heading": "4.5 Algorithms SFFG/SFBG", "text": "Sffg (Sequential Floating Forward Generation) (Pudil et al., 1994) is an exponential cost algorithm that works sequentially and performs a forward step followed by a variable (and possibly zero) number of backward directional ones. Essentially, a feature is first added unconditionally and then removed as long as the subsets produced are the best of their respective quantities.The algorithm (described in Algorithm 5 as a flowchart) is so-called because it has the property to float a potentially good solution of the specified size. Its backward-facing counterpart Sfbg performs a backward step followed by zero or more forward steps. These two algorithms have proven to be very effective in some situations (Jain and Zongker, 1997) and are among the most popular \"Xars.\" Their major drawbacks are the computer-related costs, which cannot be affordable if the input of J (S) and the deviation of 1%!"}, {"heading": "4.6 Algorithm QBB", "text": "The Qbb (Quick Branch and Bound) algorithm (Dash and Liu, 1998) (described as Algorithm 7) is a type 1. In fact, it is a hybrid algorithm composed of Lvf and Fig. Abb originates in Branch & Bound (Narendra and Fukunaga, 1977), an optimal search algorithm. Given a user-defined threshold \u03b2, the search stops at any node whose rating is lower than \u03b2, thus truncating efferent branches. Abb (Automatic Branch & Bound) (Liu et al., 1998) is a variant whose limits lie in the inconsistency rate of the data when using the full set of attributes (Algorithm 6). Qbb's basic idea is to find good starting points for Fb. Abb is expected to be able to efficiently explore the remaining search space."}, {"heading": "5 EMPIRICAL EVALUATION OF FSAs", "text": "The main question arising from a selection of experimental designs is: What are the aspects that we would like to base an FSA solution on in a given dataset? Certainly, a good algorithm is one that maintains a balanced trade-off between small-format and competing solutions. To assess these two problems simultaneously, it is a difficult un-input: S (X) - an example that is characterized by X (X)."}, {"heading": "5.1 Evaluation of performance", "text": "In this section, we derive a measure to grasp the degree to which a solution achieved by an FSA is defined by selecting the individual features, which behaves like a similarity s: P (X) \u00b7 P (X) \u00b7 [0, 1], between subsets of X in the data analysis sense (Chandon and Pinson, 1981), where s (X1, X2) > s (X1, X3) indicates that X2 is more similar to the totality of features than X3, and the satisfaction s (X1, X2) = X1 and s (X2, X2). Let us show the totality of features divided into X = XR XI (XR), where XR, XI, XR (XR) are the subsets of relevant, irrelevant and redundant features of X."}, {"heading": "6 EXPERIMENTAL EVALUATION", "text": "In the following sections we explain the experimental methodology and quantify the different parameters of the experiments. The basic idea is to generate sample data sets using synthetic functions f with known relevant characteristics. These data sets (of different sizes) are corrupted with irrelevant and / or redundant characteristics and passed to the various FSAs in order to obtain a hypothesis H. The divergence between the defined function f and the obtained hypothesis H is evaluated using the score criterion (with B = 1)."}, {"heading": "6.1 Description of the FSAs used", "text": "Up to ten FSAs were used in the experiments: E-Sfg, Qbb, Lvf, Lvi, C-Sbg, Relief, Sfbg, Sffg, WSbg, and W-Sfg. The E-Sfg, W-Sfg algorithms are versions of Sfg with entropy or the accuracy of a C4.5 coil, respectively. C-Sbg, W-Sbg algorithms are versions of Sbg with consistency or the accuracy of a C4.5 coil. Since Relief and E-Sfg produce an ordered list of xi characteristics according to their weight wi, an automatic filter criterion is necessary to transform each solution into a subset of characteristics. The procedure for determining a suitable intersection is simple: First, the weights are sorted in descending order (with the greatest weight corresponding to the most relevant characteristic)."}, {"heading": "6.2 Implementations of data families", "text": "A total of twelve families of data sets have been created, covering three different problems and four cases of b = 3 b = 3 b = 3 b = 3 b = 3 b = 3 b = 3 b = 3 b = 4 b = 3 b = 3 b = 4 b = 4 b = 4 b = 1 b = 1 b = 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 2 x 0 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 1 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x 2 x"}, {"heading": "6.3 Experimental setup", "text": "The experiments are divided into three main groups: the first group examines the relationship between irrelevance and relevance; the second group examines the relationship between redundancy and relevance; and the last group examines the effect of different sample sizes. Each group uses three problem families (parity, disjunction, and GMonks), each with four different instances, with the number of relevant characteristics NR varying as stated: Relevance: The different numbers NR vary for each problem as follows: {4, 8, 16, 32} (for parity), {5, 10, 15, 20} (for disjunction), and {6, 12, 18, 24} (for monks). Irrelevance: In these experiments, NI runs from zero to twice the value of NR. Specifically: NI: (k \u00b7 NR) / p, k = 5 = N\u03b1c = relevance."}, {"heading": "6.4 Discussion of the results", "text": "In recent years, it has become clear that most of them are people who are not able to position themselves in public. (...) Most of them are able to position themselves in public. (...) Most of them are not able to position themselves in public. (...) Most of them are not able to position themselves in public. (...) Most of them are able to position themselves in public. (...) Most of them are able to position themselves in public. (...) Most of them are not able to position themselves in public. (...) Most of them are not able to position themselves in public. (...) Most of them are able to position themselves in public. (...)"}, {"heading": "6.5 General considerations", "text": "The results show that solving problems that have arisen in the past is not only a problem, but also a problem that needs to be solved in the present. (...) Given that we are able to find a solution that is tailored to people's needs, we have to be prepared for the fact that it is a problem. (...) Given that it is a problem that has arisen in the past, we have to be able to solve it. \"(...)"}, {"heading": "2 0.869 0.601 24 14", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 0.884 0.588 19 10", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 0.858 0.609 22 13", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 0.876 0.730 30 19", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 0.875 0.475 5 0", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 0.872 0.456 8 6", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 0.880 0.412 5 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 0.881 0.630 14 4", "text": "The same experiment can be used to show the variability of the results depending on the data sample. It turns out that the number of relevant and redundant as well as irrelevant characteristics depends very much on the sample. A look at the selected precise characteristics shows that they are very different solutions (a fact also indicated by the score), which nonetheless give a similar rating by the inductor. Given the incremental character of W-Sfg, it can be deduced that improvements in the classifier were achieved by adding completely irrelevant characteristics."}, {"heading": "7 CONCLUSIONS", "text": "Finally, the task of a feature selection algorithm (FSA) is to provide a computational solution to the feature selection problem motivated by a specific definition of relevance, or at least a performance evaluation measure. This algorithm should also be increasingly reliable in sample size and pursue the solution of a clearly defined optimization goal. The many algorithms proposed in the literature are based on very different principles and follow these recommendations, if any. In this research, several basic algorithms have been studied to evaluate their performance in a controlled experimental scenario. A measure for evaluating FSAs has been developed that calculates the degree of agreement between the performance given by an FSA and the known optimal solution. This measure takes into account the details of relevance, irrelevance, redundancy, and size of synthetic datasets. Our results illustrate the pitfalls in dependence on a single algorithm and sample solution, especially if the structure of the sample size is deficient."}], "references": [{"title": "A Comparative Evaluation of Sequential Feature Selection Algorithms", "author": ["D.W. Aha", "R.L. Bankert"], "venue": "In Proc. of the 5th International Workshop on Artificial Intelligence and Statistics ,", "citeRegEx": "Aha and Bankert,? \\Q1995\\E", "shortCiteRegEx": "Aha and Bankert", "year": 1995}, {"title": "Learning with Many Irrelevant Features", "author": ["H. Almuallim", "T.G. Dietterich"], "venue": "In Proc. of the 9th National Conference on Artificial Intelligence,", "citeRegEx": "Almuallim and Dietterich,? \\Q1991\\E", "shortCiteRegEx": "Almuallim and Dietterich", "year": 1991}, {"title": "Use of Distance Measures, Information Measures and Error Bounds in Fuature Evaluation", "author": ["M. Ben-Bassat"], "venue": "Handbook of Statistics ,", "citeRegEx": "Ben.Bassat,? \\Q1982\\E", "shortCiteRegEx": "Ben.Bassat", "year": 1982}, {"title": "Feature Selection from Huge Feature Sets", "author": ["J. Bins", "B. Draper"], "venue": "In International Conference on Computer Vision,", "citeRegEx": "Bins and Draper,? \\Q2001\\E", "shortCiteRegEx": "Bins and Draper", "year": 2001}, {"title": "Random Forests", "author": ["L. Breiman"], "venue": "Machine Learning,", "citeRegEx": "Breiman,? \\Q2001\\E", "shortCiteRegEx": "Breiman", "year": 2001}, {"title": "Greedy Attribute Selection", "author": ["R.A. Caruana", "D. Freitag"], "venue": "In Proc. of the 11th International Conference on Machine Learning,", "citeRegEx": "Caruana and Freitag,? \\Q1994\\E", "shortCiteRegEx": "Caruana and Freitag", "year": 1994}, {"title": "Analyse Typologique", "author": ["S. Chandon", "L. Pinson"], "venue": null, "citeRegEx": "Chandon and Pinson,? \\Q1981\\E", "shortCiteRegEx": "Chandon and Pinson", "year": 1981}, {"title": "Hybrid Search of Feature Subsets", "author": ["M. Dash", "H. Liu"], "venue": "Proc. of the 15th Pacific Rim International Conference on AI ,", "citeRegEx": "Dash and Liu,? \\Q1998\\E", "shortCiteRegEx": "Dash and Liu", "year": 1998}, {"title": "Feature Selection for Classification", "author": ["M. Dash", "H. Liu", "H. Motoda"], "venue": "Intelligence Data Analysis: An International Journal", "citeRegEx": "Dash et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dash et al\\.", "year": 1997}, {"title": "Consistency Based Feature Selection", "author": ["M. Dash", "H. Liu", "H. Motoda"], "venue": "In Pacific\u2013Asia Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Dash et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Dash et al\\.", "year": 2000}, {"title": "An Evaluation of Feature Selection Methods and their Application to Computer Security", "author": ["J. Doak"], "venue": "Technical Report CSE\u201392\u201318,", "citeRegEx": "Doak,? \\Q1992\\E", "shortCiteRegEx": "Doak", "year": 1992}, {"title": "Correlation\u2013based Feature Selection for Machine Learning", "author": ["M.A. Hall"], "venue": "PhD thesis,", "citeRegEx": "Hall,? \\Q1999\\E", "shortCiteRegEx": "Hall", "year": 1999}, {"title": "Feature Selection: Evaluation, Application, and Small Sample Performance", "author": ["A.K. Jain", "D. Zongker"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Jain and Zongker,? \\Q1997\\E", "shortCiteRegEx": "Jain and Zongker", "year": 1997}, {"title": "Irrelevant Features and the Subset Selection Problem", "author": ["G.H. John", "R. Kohavi", "K. Pfleger"], "venue": "In Proc. of the 11th International Conference on Machine Learning,", "citeRegEx": "John et al\\.,? \\Q1994\\E", "shortCiteRegEx": "John et al\\.", "year": 1994}, {"title": "A Practical Approach to Feature Selection", "author": ["K. Kira", "L. Rendell"], "venue": "In Proc. of the 9th International Conference on Machine Learning,", "citeRegEx": "Kira and Rendell,? \\Q1992\\E", "shortCiteRegEx": "Kira and Rendell", "year": 1992}, {"title": "Estimating Attributes: Analysis and Extensions of Relief", "author": ["I. Kononenko"], "venue": "In Proc. of the European Conference on Machine Learning,", "citeRegEx": "Kononenko,? \\Q1994\\E", "shortCiteRegEx": "Kononenko", "year": 1994}, {"title": "A Comparative Evaluation of medium and large\u2013scale Feature Selectors for Pattern Classifiers", "author": ["M. Kudo", "J. Sklansky"], "venue": "In Proc. of the 1st International Workshop on Statistical Techniques in Pattern Recognition,", "citeRegEx": "Kudo and Sklansky,? \\Q1997\\E", "shortCiteRegEx": "Kudo and Sklansky", "year": 1997}, {"title": "Feature Selection for Knowledge Discovery and Data Mining", "author": ["H. Liu", "H. Motoda"], "venue": null, "citeRegEx": "Liu and Motoda,? \\Q1998\\E", "shortCiteRegEx": "Liu and Motoda", "year": 1998}, {"title": "A Monotonic Measure for Optimal Feature Selection", "author": ["H. Liu", "H. Motoda", "M. Dash"], "venue": "In Proc. of the European Conference on Machine Learning,", "citeRegEx": "Liu et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Liu et al\\.", "year": 1998}, {"title": "Incremental feature selection", "author": ["H. Liu", "R. Setiono"], "venue": "Applied Intelligence,", "citeRegEx": "Liu and Setiono,? \\Q1998\\E", "shortCiteRegEx": "Liu and Setiono", "year": 1998}, {"title": "Scalable Feature Selection for Large Sized Databases", "author": ["H. Liu", "R. Setiono"], "venue": "In Proc. of the 4th World Congress on Expert Systems ,", "citeRegEx": "Liu and Setiono,? \\Q1998\\E", "shortCiteRegEx": "Liu and Setiono", "year": 1998}, {"title": "A Branch and Bound Algorithm for Feature Subset Selection", "author": ["P. Narendra", "K. Fukunaga"], "venue": "IEEE Transactions on Computer ,", "citeRegEx": "Narendra and Fukunaga,? \\Q1977\\E", "shortCiteRegEx": "Narendra and Fukunaga", "year": 1977}, {"title": "Floating Search Methods in Feature Selection", "author": ["P. Pudil", "J. Novovicov\u00e1", "J. Kittler"], "venue": "Pattern Recognition", "citeRegEx": "Pudil et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Pudil et al\\.", "year": 1994}, {"title": "Overfitting in Making Comparisons Between Variable Selection Methods", "author": ["J. Reunanen"], "venue": "J. of Machine Learning Research,", "citeRegEx": "Reunanen,? \\Q2003\\E", "shortCiteRegEx": "Reunanen", "year": 2003}, {"title": "The MONK\u2019s Problems: A Performance Comparison of Different Learning Algorithms", "author": ["Thrun", "S. e"], "venue": "Technical Report CS-91-197,", "citeRegEx": "Thrun and e.,? \\Q1991\\E", "shortCiteRegEx": "Thrun and e.", "year": 1991}, {"title": "Induction of Concepts in the Predicate Calculus", "author": ["S.A. Vere"], "venue": "In Proc. of the 4th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Vere,? \\Q1975\\E", "shortCiteRegEx": "Vere", "year": 1975}, {"title": "Learning Structural Descriptions from Examples", "author": ["P.H. Winston"], "venue": "The Psychology of Computer Vision. McGrawHill", "citeRegEx": "Winston,? \\Q1975\\E", "shortCiteRegEx": "Winston", "year": 1975}], "referenceMentions": [{"referenceID": 5, "context": "This is so because, from the inductive learning perspective, the relevance of a feature may have several definitions depending on the precise objective that is looked for (Caruana and Freitag, 1994).", "startOffset": 171, "endOffset": 198}, {"referenceID": 23, "context": "In this sense, it is our hypothesis that FSAs are very affected by finite sample sizes, which distort reliable assessments of subset relevance, even in the presence of a very sophisticated search algorithm (Reunanen, 2003).", "startOffset": 206, "endOffset": 222}, {"referenceID": 2, "context": "This problem is aggravated when using filter measures, since in this case the relation to true generalization ability (as expressed by the Bayes error) can be very loose (Ben-Bassat, 1982).", "startOffset": 170, "endOffset": 188}, {"referenceID": 0, "context": "Previous experimental work on feature selection algorithms for comparative purposes include Aha and Bankert (1995), Doak (1992), Jain and Zongker (1997), Kudo and Sklansky (1997) and Liu and Setiono (1998b).", "startOffset": 92, "endOffset": 115}, {"referenceID": 0, "context": "Previous experimental work on feature selection algorithms for comparative purposes include Aha and Bankert (1995), Doak (1992), Jain and Zongker (1997), Kudo and Sklansky (1997) and Liu and Setiono (1998b).", "startOffset": 92, "endOffset": 128}, {"referenceID": 0, "context": "Previous experimental work on feature selection algorithms for comparative purposes include Aha and Bankert (1995), Doak (1992), Jain and Zongker (1997), Kudo and Sklansky (1997) and Liu and Setiono (1998b).", "startOffset": 92, "endOffset": 153}, {"referenceID": 0, "context": "Previous experimental work on feature selection algorithms for comparative purposes include Aha and Bankert (1995), Doak (1992), Jain and Zongker (1997), Kudo and Sklansky (1997) and Liu and Setiono (1998b).", "startOffset": 92, "endOffset": 179}, {"referenceID": 0, "context": "Previous experimental work on feature selection algorithms for comparative purposes include Aha and Bankert (1995), Doak (1992), Jain and Zongker (1997), Kudo and Sklansky (1997) and Liu and Setiono (1998b). Some of these studies use artificially generated data sets, like the widespread Parity, Led or Monks problems (Thrun, 1991).", "startOffset": 92, "endOffset": 207}, {"referenceID": 1, "context": "This can be carried out directly, as many FSAs do (Almuallim and Dietterich, 1991; Caruana and Freitag, 1994; Hall, 1999), or setting a cut-point in the output of the continuous problem solution.", "startOffset": 50, "endOffset": 121}, {"referenceID": 5, "context": "This can be carried out directly, as many FSAs do (Almuallim and Dietterich, 1991; Caruana and Freitag, 1994; Hall, 1999), or setting a cut-point in the output of the continuous problem solution.", "startOffset": 50, "endOffset": 121}, {"referenceID": 11, "context": "This can be carried out directly, as many FSAs do (Almuallim and Dietterich, 1991; Caruana and Freitag, 1994; Hall, 1999), or setting a cut-point in the output of the continuous problem solution.", "startOffset": 50, "endOffset": 121}, {"referenceID": 25, "context": "The methods to induce logical conjunctions (Vere, 1975; Winston, 1975), decision trees or artificial neural networks are examples of this embedding.", "startOffset": 43, "endOffset": 70}, {"referenceID": 26, "context": "The methods to induce logical conjunctions (Vere, 1975; Winston, 1975), decision trees or artificial neural networks are examples of this embedding.", "startOffset": 43, "endOffset": 70}, {"referenceID": 13, "context": "Wrapper Mode: Here the relationship is taken the other way around: the FSA uses the learning algorithm as a subroutine (John et al., 1994).", "startOffset": 119, "endOffset": 138}, {"referenceID": 8, "context": "It has been found to be particularly efficient for data sets having redundant features (Dash et al., 1997).", "startOffset": 87, "endOffset": 106}, {"referenceID": 7, "context": "Arguably its main advantage may be that it quickly reduces the number of features in the initial stages with certain confidence (Dash and Liu, 1998; Dash et al., 2000); however, many poor solution subsets are analyzed, wasting computing resources.", "startOffset": 128, "endOffset": 167}, {"referenceID": 9, "context": "Arguably its main advantage may be that it quickly reduces the number of features in the initial stages with certain confidence (Dash and Liu, 1998; Dash et al., 2000); however, many poor solution subsets are analyzed, wasting computing resources.", "startOffset": 128, "endOffset": 167}, {"referenceID": 17, "context": "In Liu and Motoda (1998) it is reported experimentally that Lvi adequately chooses relevant features, but may fail for noisy data sets, in which case the algorithm it is shown to consider irrelevant features.", "startOffset": 3, "endOffset": 25}, {"referenceID": 14, "context": "Relief (Kira and Rendell, 1992) is a general-type algorithm that works exclusively in filter mode.", "startOffset": 7, "endOffset": 31}, {"referenceID": 8, "context": "The algorithm has been found to choose correlated features instead of relevant features (Dash et al., 1997), and therefore the optimal subset can be far from assured (Kira and Rendell, 1992).", "startOffset": 88, "endOffset": 107}, {"referenceID": 14, "context": ", 1997), and therefore the optimal subset can be far from assured (Kira and Rendell, 1992).", "startOffset": 66, "endOffset": 90}, {"referenceID": 15, "context": "Some variants have been proposed to account for several classes (Kononenko, 1994), where the k more similar instances are selected and their averages computed.", "startOffset": 64, "endOffset": 81}, {"referenceID": 9, "context": "When the number of features is small, Doak (1992) reported that Sbg tends to show better performance than Sfg, most likely because Sbg evaluates the contribution of all features from the onset.", "startOffset": 38, "endOffset": 50}, {"referenceID": 0, "context": "In addition, Aha and Bankert (1995) points out that Sfg is preferable when the number of relevant features is (known to be) small; otherwise Sbg should be used.", "startOffset": 13, "endOffset": 36}, {"referenceID": 0, "context": "In addition, Aha and Bankert (1995) points out that Sfg is preferable when the number of relevant features is (known to be) small; otherwise Sbg should be used. Interestingly, it was also reported that Sbg did not always have better performance than Sfg, contrary to the conclusions in Doak (1992). Besides, Sfg is faster in practice.", "startOffset": 13, "endOffset": 298}, {"referenceID": 22, "context": "Sffg (Sequential Floating Forward Generation) (Pudil et al., 1994) is an exponential cost algorithm that operates in a sequential fashion, performing a forward step followed by a variable (and possibly null) number of backward ones.", "startOffset": 46, "endOffset": 66}, {"referenceID": 12, "context": "These two algorithms have been found to be very effective in some situations (Jain and Zongker, 1997), and are among the most popular nowadays.", "startOffset": 77, "endOffset": 101}, {"referenceID": 3, "context": "of features nears the hundred (Bins and Draper, 2001) and the need to fix the size of the final desired subset.", "startOffset": 30, "endOffset": 53}, {"referenceID": 7, "context": "The Qbb (Quick Branch and Bound) algorithm (Dash and Liu, 1998) (described as Algorithm 7) is a type 1 algorithm.", "startOffset": 43, "endOffset": 63}, {"referenceID": 21, "context": "The origin of Abb is in Branch & Bound (Narendra and Fukunaga, 1977), an optimal search algorithm.", "startOffset": 39, "endOffset": 68}, {"referenceID": 18, "context": "Abb (Automatic Branch & Bound) (Liu et al., 1998) is a variant having its bound as the inconsistency rate of the data when the full set of features is used (Algorithm 6).", "startOffset": 31, "endOffset": 49}, {"referenceID": 6, "context": "This criterion behaves as a similarity s : P(X)\u00d7 P(X) \u2192 [0, 1], between subsets of X in the data analysis sense (Chandon and Pinson, 1981), where s(X1, X2) > s(X1, X3) indicates that X2 is more similar to X1 than X3, and satisfying s(X1, X2) = 1 \u21d0\u21d2 X1 = X2 and s(X1, X2) = s(X2, X1).", "startOffset": 112, "endOffset": 138}, {"referenceID": 12, "context": "of dimensionality (Jain and Zongker, 1997).", "startOffset": 18, "endOffset": 42}, {"referenceID": 4, "context": "this vein, the use of resampling techniques like Random Forests (Breiman, 2001) is strongly recommended.", "startOffset": 64, "endOffset": 79}], "year": 2011, "abstractText": "The main purpose of Feature Subset Selection is to find a reduced subset of attributes from a data set described by a feature set. The task of a feature selection algorithm (FSA) is to provide with a computational solution motivated by a certain definition of relevance or by a reliable evaluation measure. In this paper several fundamental algorithms are studied to assess their performance in a controlled experimental scenario. A measure to evaluate FSAs is devised that computes the degree of matching between the output given by a FSA and the known optimal solutions. An extensive experimental study on synthetic problems is carried out to assess the behaviour of the algorithms in terms of solution accuracy and size as a function of the relevance, irrelevance, redundancy and size of the data samples. The controlled experimental conditions facilitate the derivation of better-supported and meaningful conclusions.", "creator": "LaTeX with hyperref package"}}}