{"id": "1101.5632", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jan-2011", "title": "Active Markov Information-Theoretic Path Planning for Robotic Environmental Sensing", "abstract": "Recent research in multi-robot exploration and mapping has focused on sampling environmental fields, which are typically modeled using the Gaussian process (GP). Existing information-theoretic exploration strategies for learning GP-based environmental field maps adopt the non-Markovian problem structure and consequently scale poorly with the length of history of observations. Hence, it becomes computationally impractical to use these strategies for in situ, real-time active sampling. To ease this computational burden, this paper presents a Markov-based approach to efficient information-theoretic path planning for active sampling of GP-based fields. We analyze the time complexity of solving the Markov-based path planning problem, and demonstrate analytically that it scales better than that of deriving the non-Markovian strategies with increasing length of planning horizon. For a class of exploration tasks called the transect sampling task, we provide theoretical guarantees on the active sampling performance of our Markov-based policy, from which ideal environmental field conditions and sampling task settings can be established to limit its performance degradation due to violation of the Markov assumption. Empirical evaluation on real-world temperature and plankton density field data shows that our Markov-based policy can generally achieve active sampling performance comparable to that of the widely-used non-Markovian greedy policies under less favorable realistic field conditions and task settings while enjoying significant computational gain over them.", "histories": [["v1", "Fri, 28 Jan 2011 21:27:31 GMT  (663kb,D)", "http://arxiv.org/abs/1101.5632v1", "10th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2011), Extended version with proofs, 11 pages"]], "COMMENTS": "10th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2011), Extended version with proofs, 11 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.MA cs.RO", "authors": ["kian hsiang low", "john m dolan", "pradeep khosla"], "accepted": false, "id": "1101.5632"}, "pdf": {"name": "1101.5632.pdf", "metadata": {"source": "CRF", "title": "Active Markov Information-Theoretic Path Planning for Robotic Environmental Sensing", "authors": ["Kian Hsiang Low", "John M. Dolan", "Pradeep Khosla"], "emails": ["lowkh@comp.nus.edu.sg", "jmd@cs.cmu.edu,", "pkk@ece.cmu.edu"], "sections": [{"heading": "Categories and Subject Descriptors", "text": "G.3 [Probability and Statistics]: Markov Processes, Stochastic Processes; I.2.8 [Problem Solving, Control Methods and Search]: Dynamic Programming; I.2.9 [Robotics]: Autonomous Vehicles"}, {"heading": "General Terms", "text": "algorithms, power, experiment, theory"}, {"heading": "Keywords", "text": "Multi-robot exploration and mapping, adaptive sampling, active learning, Gaussian process, non-myopic path planningCite as: Active Markov Information-Theoretic Path Planning for Robotic Environmental Sensing, Kian Hsiang Low, John M. Dolan, and Pradeep Khosla, Proc. of 10th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2011), Tumer, Yolum, Sonenberg and Stone (eds.), May, 2-6, 2011, Taipei, Taiwan, S. XXX-XXX. Copyright c \u00a9 2011, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved."}, {"heading": "1. INTRODUCTION", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they"}, {"heading": "2. TRANSECT SAMPLING TASK", "text": "A temperature field is spatially distributed over a 25 m x 150 m long transect, which is divided into a 5 m x 30 m grid of sample locations, each comprising 30 columns. Robots are forced to simultaneously explore one column after another, from the left to the right column of the transect, so that each robot tries out one location per column for a total of 30 locations. Robots are forced to simultaneously explore one column after another, from the left to the right column."}, {"heading": "3. NON-MARKOVIAN PATH PLANNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Notations and Preliminaries", "text": "The columns of the transect are indexed in increasing order from left to right, with the left column indexed with \"0.\" Each planning phase is associated with a column from which each robot in the team selects and performs observations (i.e., consisting of a pair of locations and their measurement). Let's say k the number of robots in the team. At each stage i, the k robot team then collects from column i a total of k observations, which are denoted by a vector pair xi of k positions and zxi of the corresponding measurements. Let's say x0: i and zx0: i vectors that include the history of the robot locations and the corresponding measurements over stages 0 to i (i.e. concatenation of Zx0, x1, Zxxi, to 0 and zxi associated with the measurements)."}, {"heading": "3.2 Gaussian Process-Based Environmental Field", "text": "U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \"U\" U \""}, {"heading": "4. MARKOV-BASED PATH PLANNING", "text": "The Markov property assumes that the measurements Zxi + 1, which are to be observed next in stage i + 1, depend only on the current measurements Zxi in stage i and conditionally independent of the previous measurements Zx0: i \u2212 1, which are observed in stages 0 to i \u2212 1. That is, f (zxi + 1 | zx0: i) = f (zxi + 1 | zxi) for all zx0, zx1,..., zxi + 1. Consequently, H [Zxi + 1 | Zx0: i] (6) can be approximated by H [Zxi + 1 | Zxi]. It is therefore easy to apply the Markov assumption to iMASP (7), resulting in the following dynamic programming equations for the Markov-based planning problem: V (xi) = i (xi) = max."}, {"heading": "4.1 Time Complexity: Analysis & Comparison", "text": "Theorem 1. Leave A 4 = A (x0) =.. = A (xt). Deriving the Markov-based policy \u03c0 (12) for the transect sample task takes O (| A | 2 (t + k4)) time. Note that | A | = rCk = O (rk), where r is the number of sampling points per column and k \u2264 r, as in Section 2. Although | A | is exponential in the number k of robots, r in a transect is likely to be small, which prevents | A | from becoming too large. In contrast, deriving the iMASP-based policy \u03c0 (8) O (| A | tt2k4) takes time. Deriving greedy strategies \u03c0G (9) and \u03c0M (10) each cause O (| A | t4k3 + | A | 2tk4) an advantage over the other strategies based on Markov is not necessarily possible."}, {"heading": "4.2 Performance Guarantees", "text": "This key finding stems from our intuition that if the horizontal spatial correlation becomes small, the use of the previous metrics for route planning is unlikely to improve active sampling performance in a transectoral sampling task, thus favoring the Markov-based policy. Although this intuition is easy to support with formal theoretical results and their corresponding results (appendix A), it turns out that the sampling task is not trivial, as shown below. Recall the Markov assumption that H [Zxi + 1] (6) is to be approximated by H [Zxi + 1]. This prompts us to first consider the difference of these posterior entropies resulting from Markov ownership: H [Zxi + 1] is to be approximated by H [Zxi + 1]."}, {"heading": "5. EXPERIMENTS AND DISCUSSION", "text": "In Section 4.2, we highlighted the practical implications of our main theoretical result (i.e., theory 5), which defines various environmental conditions and sampling tasks to limit the performance deterioration of Markov-based strategies, but this result does not indicate whether \u03c0 strategies perform well (or not) under \"seemingly\" less favorable field conditions and tasks that do not collectively fulfill their sufficient state, including large spatial correlations, less noisy, high-intensity fields, small grid discretization width, long planning horizon (i.e., many cross-sectional columns), and a large number of robots. Thus, this section empirically evaluates the active sampling performance and time efficiency of \u03c0 strategies on two real-world datasets detailed under such field conditions and task settings: (a) the temperature field data of Panther Hollow Lake in Pittsburgh, PA, covering 25 m at 150 m and 16 m, June 2009 and 316 months of data (Bucht)."}, {"heading": "5.1 Performance Metrics", "text": "The measures tested are evaluated using the two metrics proposed in [7], which quantify the mapping uncertainty of the unobserved areas of the field differently (a) The ENT metric measures the rear common entropy H [Zx0: t + 1 | Zx0: t + 1] of the field measurements Zx0: t + 1 at unobserved locations x0: t + 1, whereby the vector encompassing the locations of domain U is not selected in the locations selected by the policy x0: t + 1. Smaller ENT measures imply lower mapping uncertainty; (b) The ERR metric measures the mean squared relative error rate."}, {"heading": "5.2 Temperature Field Data", "text": "We will first examine how different spatial correlations (i.e., different length scales) of the temperature field affect the ENT (\u03c0) and ERR scales of the assessed policies, and the temperature field will be divided into a 5 \u00d7 30 grid of sampling locations, as shown in Figures 1 and 2d. Horizontal and / or vertical length scales of the original field (i.e., field 4 in Fig. 2d) will be reduced to produce modified fields 1, 2, and 3 (Fig. 2a, 2b, and 2c); we will correct these reduced length scales while measuring the remaining hyperparameters (i.e. signal and noise deviations) by MLE.Table 1 shows the results of the mean ENT and ERR performance of the tested policies (i.e., averaged across all possible launch robot locations) with different length scales and number of robots."}, {"heading": "5.3 Plankton Density Field Data", "text": "Figure 3 illustrates the plankton density field discredited into an 8 \u00d7 45 grid. Table 3 shows the results of the mean ENT (\u03c0) and ERR (\u03c0) performance of tested strategies with different numbers of robots. The observations are as follows: \u03c0 can achieve the same ENT (\u03c0) and ERR (\u03c0) performance as those of \u03c0G and superior ENT (\u03c0) performance over those of \u03c0M because small horizontal and large vertical correlations favor equations as explained in Section 5.2. By increasing the number of robots (i.e. k > 2) \u03c0 can achieve an ERR performance comparable to that of \u03c0M. Table 4 shows the results of the mean ENT (\u03c0) and ERR performance of tested strategies after increasing the resolution to 16 \u00d7 89 grid; the resulting grid disk width and planning horizon are comparable to those of \u03c0M. Table 4 shows the results of the mean ENT (\u03c0) and ERR performance of tested strategies after increasing the resolution to 16 \u00d7 89 grid; the resulting grid disk width and planning horizon are comparable to those of approximately \u03c0 and the performance is less than 0.5 times longer than the ENT and vice versa."}, {"heading": "5.4 Incurred Policy Time", "text": "Figure 4 shows the time needed to derive the tested strategies for the derivation of temperature and plankton density fields with different number of robots and network resolutions. It can be stated that the time needed to derive \u03c0 start robot sites is shorter by more than 1 or 4 orders of magnitude. It is important to point out that Figure 4 indicates the average time needed to derive \u03c0G and \u03c0M over all possible start robot derivative sites. Therefore, if the start robot sites are unknown, the induced time for the derivative \u03c0G and \u03c0M must be increased by the RCK multiple. In contrast, the induced time refers to all possible start robot derivative derivative sites. These observations show a considerable calculation gain of \u03c0 over the derivative 3: Comparison of the ENT derivative derivative derivative derivative derivative sites."}, {"heading": "6. CONCLUSION", "text": "This paper describes an efficient Markov-based information theory pathplanner for active sampling of GP-based environmental fields. We have provided theoretical guarantees for the active sampling performance of our Markov-based strategies for the Transect sampling task, from which ideal environmental conditions (i.e. small horizontal spatial correlation and noisy, less intensive fields) and sampling tasks (i.e. large network discretization width and short planning horizon) can be determined to limit their performance degradation. Empirically, we have shown that we can generally achieve active sampling performance comparable to the widespread non-Markovian greedy policy."}, {"heading": "7. REFERENCES", "text": "[1] G. H. Golub and C.-F. Van Loan. MatrixComputations. Johns Hopkins Univ. Press, 3rd edition, 1996. [2] R. Korf. Real-time heuristic search. Artif. L. Intell., 42 (2-3): 189-211, 1990. [3] A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies. JMLR, 9: 235-284, 2008. [4] N. E. Leonard, D. Paley, F. Lekien, R. Sepulchre, D. M. Fratantoni, and R. Davis. Collective motion, sensor networks and ocean sampling. Proc. IEEE, 95 (1): 48-74, 2007. [5] K. H. Low. Multi-Robot Adaptive Exploration and Mapping for Environmental Sensing Applications."}, {"heading": "APPENDIX", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. PROOFS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof Sketch of Theorem 1", "text": "For each vector xi of current robot locations, the time required to assess rear entropy H [Z\u03c4 (xi, ai) | Zxi] (i.e., using cholesky factorization) across all possible actions occurs across all possible vectors of current robot locations in each column. Thus, we do not need to recalculate these rear entropies for each column, since the entropies evaluated for one column replicate across different columns. This computational saving is based on the Markov assumption and the problem structure of the transect sample task. Multiplying the optimal values from time to 0 takes O | A | 2t the problem."}, {"heading": "A.2 Proof of Lemma 2", "text": "Imagine that the constellation in which the constellation takes place cannot be reconciled with the constellation in which the constellation between the constellations is reflected. (...) The constellation in which the constellation between the two constellations is reflected cannot be reconciled. (...) The constellation in which the constellation between the two constellations is reflected cannot be reconciled. (...) The constellation in which the constellation between the two constellations is reflected cannot be reconciled. (...) The constellation in which the constellation between the two constellations is reflected cannot be reconciled. (...) The constellation in which the constellation between the two constellations is reflected cannot be reconciled. (...) The constellation in which the constellation between the two constellations is reflected cannot be reconciled. (...)"}, {"heading": "A.3 Proof of Theorem 4", "text": "The proof by induction to i that V xi + xi xi xi (s) for i = t,.., 0.Base case (i = t): By Lemma 3, H [Zxt + 1 | Zx0: t] \u2264 H [Zxt + 1 | Zxt + 1 | Zxt] \u2264 H [Zxt + 1 | Zxt + 1 | Zxt] \u2264 H [Zxt + 1 | Zxt] \u2264 H [Zxt + 1 | Zxt] \u2264 H [Zxt + 1 | Zxt] + 1 \u21d2 max at i (t) + 1 \u21d2 A (xt) H [Zxt + 1 | Zxt + 1: t] \u2264 H [Zxt + 1 \u2264 H] \u2264 H [Zxt + 1 | Zxt + 1 | Zxt + 1 \u00b2 s (t) \u21d2 V (Zxt + 1: t) \u0432i (Zxt) \u0432i (V: t) \u2012 V (Zxt + 1: xi t) \u2012 t \u2012 V (Zxt + 1: xi + 1 Zxt + 1: xt) \u2264 H (xi + 1: xt) \u2264 H (Zxt + 1: x) \u2264 H (xi + 1: t)."}, {"heading": "A.4 Proof of Theorem 5", "text": "The following Lemma is required for this proof: Lemma 7. V \u00b2 i (xi) \u2264 V \u00b2 i (xi) \u2012 i \u00b2 (x0: i) + \u2211 t = i \u00b2 (s) for i = 0,.., t.The proof for the above Lemma is in Appendix A.6. Proof by induction to i that V \u00b2 i (x0: i) \u2264 V \u00b2 i (x0: i) + \u2211 t (t) for i = t., 0.Base case (i = t): V \u00b2 t (x0: t) \u2264 V \u00b2 t (xt) \u2264 V \u00b2 t (x0: t) + \u0445 (t).The first inequality results from Theorem 4. The second inequality results from Lemma 7. Therefore, the base case is true. The inductive case: Suppose thatV \u00b2 i + 1 (x0: i + 1) \u2264 V xi xi xi \u00b2 s (t) follows Theorem 4. The second inequality results from Lemma 7. The second inequality results from Lemma, the base case is true."}, {"heading": "A.5 Proof Sketch of Lemma 6", "text": "Definition x [m] i as m-th component of the vector xi of robotlocations for m = 1,., k. Let x [1: m] i stands for a vector containing the first m-components of xi (i.e., concatenation of x [1] i,., x [m] i).I [Zxi + 1; Zx0: i \u2212 1 | Zxi] = H [Zxi + 1 | Zxi] \u2212 H [Zxi + 1 | Zx0: i] = k [m] m = 1 (H [m] i + 1 (xi, x [1: m \u2212 1] i + 1) \u2212 H [Z x [m] \u2212 i + 1 | Z (x0: i), x [1: m \u2212 1] i, x [Lemm \u2212 1] i + 1: i + 1 k [m] i: 1 x: i [m]], x: 1 x [m] i: 1 [m], x [m]."}, {"heading": "A.6 Proof of Lemma 7", "text": "The proof by induction on i that V, xi, xi, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i,"}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "Recent research in multi-robot exploration and mapping has<lb>focused on sampling environmental fields, which are typi-<lb>cally modeled using the Gaussian process (GP). Existing<lb>information-theoretic exploration strategies for learning GP-<lb>based environmental field maps adopt the non-Markovian<lb>problem structure and consequently scale poorly with the<lb>length of history of observations. Hence, it becomes compu-<lb>tationally impractical to use these strategies for in situ, real-<lb>time active sampling. To ease this computational burden,<lb>this paper presents a Markov-based approach to efficient<lb>information-theoretic path planning for active sampling of<lb>GP-based fields. We analyze the time complexity of solving<lb>the Markov-based path planning problem, and demonstrate<lb>analytically that it scales better than that of deriving the<lb>non-Markovian strategies with increasing length of planning<lb>horizon. For a class of exploration tasks called the transect<lb>sampling task, we provide theoretical guarantees on the ac-<lb>tive sampling performance of our Markov-based policy, from<lb>which ideal environmental field conditions and sampling task<lb>settings can be established to limit its performance degrada-<lb>tion due to violation of the Markov assumption. Empirical<lb>evaluation on real-world temperature and plankton density<lb>field data shows that our Markov-based policy can generally<lb>achieve active sampling performance comparable to that of<lb>the widely-used non-Markovian greedy policies under less<lb>favorable realistic field conditions and task settings while<lb>enjoying significant computational gain over them.", "creator": "LaTeX with hyperref package"}}}