{"id": "1102.2670", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2011", "title": "Online Least Squares Estimation with Self-Normalized Processes: An Application to Bandit Problems", "abstract": "The analysis of online least squares estimation is at the heart of many stochastic sequential decision making problems. We employ tools from the self-normalized processes to provide a simple and self-contained proof of a tail bound of a vector-valued martingale. We use the bound to construct a new tighter confidence sets for the least squares estimate.", "histories": [["v1", "Mon, 14 Feb 2011 04:06:31 GMT  (22kb)", "http://arxiv.org/abs/1102.2670v1", "Submitted to the 24th Annual Conference on Learning Theory (COLT 2011)"]], "COMMENTS": "Submitted to the 24th Annual Conference on Learning Theory (COLT 2011)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yasin abbasi-yadkori", "david pal", "csaba szepesvari"], "accepted": false, "id": "1102.2670"}, "pdf": {"name": "1102.2670.pdf", "metadata": {"source": "CRF", "title": "Online Least Squares Estimation with Self-Normalized Processes: An Application to Bandit Problems\u2217", "authors": ["Yasin Abbasi-Yadkori", "Csaba Szepesv\u00e1ri"], "emails": ["abbasiya@cs.ualberta.ca", "dpal@cs.ualberta.ca", "szepesva@cs.ualberta.ca"], "sections": [{"heading": null, "text": "ar Xiv: 110 2.26 70v1 [cs.AI] Feb 14, 2011We apply the confidence sets to several online decision problems, such as the multi-armed and linear parameterized bandit problems. The confidence sets are potentially applicable to other problems, such as dormant bandits, generalized linear bandits and other linear control problems. We improve the regret limit of the Upper Confidence Bound (UCB) algorithm by Auer et al. (2002) and show that its regret is highly likely to be a problem-dependent constant. In the case of linear bandits (Dani et al., 2008) we improve the problem depending on the dimension and number of time steps. Furthermore, contrary to the previous result, we prove that our limit applies to small sample sizes, and at the same time the limit is improved by a logarithmic factor and the constant is improved."}, {"heading": "1 Introduction", "text": "The least quadratic method is a cornerstone of statistics and machine learning. It is used as the main component of many stochastic sequential decision problems, such as multi-armed bandit, linear bandit and other linear control problems. However, the analysis of the least quadratic processes in these online settings is not trivial, since the correlations between the data points are treated in detail. Fortunately, there is a link between the online least quadratic estimation and the range of self-normalized processes. The study of the least quadratic processes has a long history going back to student and is covered in detail by de la Pen. (2009) Using these tools, we provide evidence of the deviation for vector-rated martinales. A less general version of the limit can already be found in de la Pen. (2004, 2009) In addition, our evidence, based on the method of blending, is new, simpler and self-determined."}, {"heading": "1.1 Notation", "text": "For a positive definitive matrix A, Rd, and the weighted inner product x Ay = < x, y > A. We use \u03bbmin (A) to specify the minimum eigenvalue of the positive definitive matrix A. We use A-0 to specify that A is positively defined, while we use A-0 to specify that it is positively semidefined. The same notation is used to specify the partial order of the Loewner matrices. We will use ei to specify the vector of the i-th unit, i.e. for all j 6 = i, eij = 0, and eii = 1."}, {"heading": "2 Vector-Valued Martingale Tail Inequalities", "text": "Let us assume (Fk; k) that with probability the eigenvalues of V are greater than the values of V >. Let us (Fk; k) be a filtration, (mk; k) be a Rd-weighted stochastic process adapted to (Fk) (Fk), (\u03b7k; k) be a real rated martingal difference process adapted to (Fk). Let us suppose that there are conditional sub-Gaussians in the sense that there is any R > 0, that there is any R > 0 that exists for any R, k-1, E [exp (\u03b32R22) a.s. (1) Consider the martingaleSt = t (1) such a R > 0 (2) and the matrix-weighted processesVt = t = t."}, {"heading": "3 Optional Skipping", "text": "(Consider the case if d = 1, mk = \u03b5k = 0, 1}, i.e. the case of an optional skip process. (Then use again V = I = 1, V = 1 + \u2211 t = 1 \u03b5k \u2212 1 def = 1 + Nt and thus the expression under investigation becomes \"St.\" (Let's use again V = I = 1 \u03b5k \u2212 1\u03b7k | 1 + Nt.) We also have \"Mouvelog det\" (V t) = \"t.\" k = 1log. \"(1 + \u03b5k \u2212 11 + Nk). (Let's use\" Mouvelog det. \"). (Let's use\" Mouvelog. \"). (V t) =\" t. \"(1 + Nt.). (Let's use.\"). (Let's use. \""}, {"heading": "4 The Multi-Armed Bandit Problem", "text": "Now let's turn our attention to the multi-layered bandit problem. Let's specify the expected reward of action i = > regret (1 > regret) and the number of times we have played action i until time t, and the number of rewards we will receive through action i until time t. Let's specify Ni, t the number of times we have played action i until time t, and X, t the average of rewards received through action i until time t. From (11) until time t we will receive the reward / K instead of trust, and a union bound by actions, we have the following trust intervals, which are likely to be held at least 1 \u2212 g."}, {"heading": "5 Application to Least Squares Estimation and Linear Bandit Problem", "text": "In this section, we first apply Theorem 3 to derive confidence intervals for the least quadratic estimation, in which the covariant process is an arbitrary process, and then use these confidence intervals to improve the confidence intervals of Dani and others. (2008) For the linear bandit problem, our assumption with respect to the data is as follows: \"Assumption A1\" (FI) is a filtration, (x1, y1),. (xt, yt) is a sequence of random variables via Rd \u00b7 R that xi is measurable, and yi is \"Fi + 1-measurable\" (i = 1, 2,.) Suppose that there is such a sequence that E [yi | Fi] = x-i \"i\" i \"i\" i \"i\" i \"i.\""}, {"heading": "5.1 The Linear Bandit Problem", "text": "Let us now turn our attention to the linear bandit problem. Let us assume that the actions are in D'Rd and for all x'D, x '2 \u2264 L. Let us assume that the reward of the action is x'D in the form of x'er (x) = x'er (and x'en) x'er (\u2212 1, 1]. Let us define regret according to R (T) = T'er (ppi x'en), where x'er (x'en) is the optimal action (x'en). Let us define regret according to R'en (T) = T'en (T) = = 1 (ppi x'en). Let us define trust according to T'en. (21) Where it is the optimal action."}, {"heading": "5.2 Saving Computation", "text": "The rule of action (22) is NP-hard in general (Dani et al., 2008). In this section, we show that we essentially only have to solve this problem O (log t) times up to a certain time and therefore have to do calculations. \u2212 \u2212 \u2212 Setpoint 2 achieves this goal by changing its policy only if the volume of confidence sets is halved and still has almost the same regrets as for Algorithm 1.Theorem. \u2212 Setpoint 1 \u2212 Setpoint 1 \u2212 Setpoint 1 \u2212 Setpoint 1 \u2212 Setpoint 1 \u2212 Setpoint of the Linear Bandit algorithm shown in Table 2. \u2212 Setpoint 2 (T) \u2264 Setpoint 2Td Log (\u03bb + TL / d) (\u03bb1 / 2S + R Log 2 / D Log 1 / D Log 1). \u2212 Setpoint 1 / D Log (T / d)."}, {"heading": "5.3 Problem Dependent Bound (\u2206 > 0)", "text": "Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log Log"}], "references": [{"title": "Active learning in heteroscedastic noise", "author": ["A. Antos", "V. Grover", "Cs. Szepesv\u00e1ri"], "venue": "Theoretical Computer Science,", "citeRegEx": "Antos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Antos et al\\.", "year": 2010}, {"title": "Online optimization in X-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz", "Cs. Szepesv\u00e1ri"], "venue": "Learning Research,", "citeRegEx": "Bubeck et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2003}, {"title": "Stochastic linear optimization under bandit feedback", "author": ["V. Dani", "T.P. Hayes", "S.M. Kakade"], "venue": "Annals of Probability,", "citeRegEx": "Dani et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2006}, {"title": "Self-normalized processes: Limit theory and Statistical Applications", "author": ["V.H. de la Pe\u00f1a", "T.L. Lai", "Q.-M. Shao"], "venue": "The Annals of Probability,", "citeRegEx": "Pe\u00f1a et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pe\u00f1a et al\\.", "year": 2009}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q2008\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 2008}, {"title": "Strong consistency of least squares estimates in multiple regression", "author": ["T.L. Lai", "H. Robbins", "C.Z. Wei"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Lai et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Lai et al\\.", "year": 1979}], "referenceMentions": [{"referenceID": 2, "context": "In the case of linear bandits (Dani et al., 2008), we improve the problem dependent bound in the dimension and number of time steps. Furthermore, as opposed to the previous result, we prove that our bound holds for small sample sizes, and at the same time the worst case bound is improved by a logarithmic factor and the constant is improved. 1 Introduction The least squares method forms a cornerstone of statistics and machine learning. It is used as the main component of many stochastic sequential decision problems, such as multi-armed bandit, linear bandits, and other linear control problems. However, the analysis of least squares in these online settings is non-trivial because of the correlations between data points. Fortunately, there is a connection between online least squares estimation and the area of self-normalized processes. Study of self-normalized processes has a long history that goes back to Student and is treated in detail in recent book by de la Pe\u00f1a et al. (2009). Using these tools we provide a proof of a bound on the deviation for vector-valued martingales.", "startOffset": 31, "endOffset": 994}, {"referenceID": 2, "context": "In the case of linear bandits (Dani et al., 2008), we improve the problem dependent bound in the dimension and number of time steps. Furthermore, as opposed to the previous result, we prove that our bound holds for small sample sizes, and at the same time the worst case bound is improved by a logarithmic factor and the constant is improved. 1 Introduction The least squares method forms a cornerstone of statistics and machine learning. It is used as the main component of many stochastic sequential decision problems, such as multi-armed bandit, linear bandits, and other linear control problems. However, the analysis of least squares in these online settings is non-trivial because of the correlations between data points. Fortunately, there is a connection between online least squares estimation and the area of self-normalized processes. Study of self-normalized processes has a long history that goes back to Student and is treated in detail in recent book by de la Pe\u00f1a et al. (2009). Using these tools we provide a proof of a bound on the deviation for vector-valued martingales. A less general version of the bound can be found already in de la Pe\u00f1a et al. (2004, 2009). Additionally our proof, based on the method of mixtures, is new, simpler and self-contained. The bound improves the previous bound of Rusmevichientong and Tsitsiklis (2010) and it is applicable to virtually any online least squares problem.", "startOffset": 31, "endOffset": 1356}, {"referenceID": 2, "context": "In the case of linear bandits (Dani et al., 2008), we improve the problem dependent bound in the dimension and number of time steps. Furthermore, as opposed to the previous result, we prove that our bound holds for small sample sizes, and at the same time the worst case bound is improved by a logarithmic factor and the constant is improved. 1 Introduction The least squares method forms a cornerstone of statistics and machine learning. It is used as the main component of many stochastic sequential decision problems, such as multi-armed bandit, linear bandits, and other linear control problems. However, the analysis of least squares in these online settings is non-trivial because of the correlations between data points. Fortunately, there is a connection between online least squares estimation and the area of self-normalized processes. Study of self-normalized processes has a long history that goes back to Student and is treated in detail in recent book by de la Pe\u00f1a et al. (2009). Using these tools we provide a proof of a bound on the deviation for vector-valued martingales. A less general version of the bound can be found already in de la Pe\u00f1a et al. (2004, 2009). Additionally our proof, based on the method of mixtures, is new, simpler and self-contained. The bound improves the previous bound of Rusmevichientong and Tsitsiklis (2010) and it is applicable to virtually any online least squares problem. The bound that we derive, gives immediately rise to tight confidence sets for the online least squares estimate that can replace the confidence sets in existing algorithms. In particular, the confidence sets can be used in the UCB algorithm for the multi-armed bandit problem, the ConfidenceBall algorithm of Dani et al. (2008) for the linear bandit problem, and LinRel algorithm of Auer (2003) for the associative reinforcement learning problem.", "startOffset": 31, "endOffset": 1752}, {"referenceID": 2, "context": "In the case of linear bandits (Dani et al., 2008), we improve the problem dependent bound in the dimension and number of time steps. Furthermore, as opposed to the previous result, we prove that our bound holds for small sample sizes, and at the same time the worst case bound is improved by a logarithmic factor and the constant is improved. 1 Introduction The least squares method forms a cornerstone of statistics and machine learning. It is used as the main component of many stochastic sequential decision problems, such as multi-armed bandit, linear bandits, and other linear control problems. However, the analysis of least squares in these online settings is non-trivial because of the correlations between data points. Fortunately, there is a connection between online least squares estimation and the area of self-normalized processes. Study of self-normalized processes has a long history that goes back to Student and is treated in detail in recent book by de la Pe\u00f1a et al. (2009). Using these tools we provide a proof of a bound on the deviation for vector-valued martingales. A less general version of the bound can be found already in de la Pe\u00f1a et al. (2004, 2009). Additionally our proof, based on the method of mixtures, is new, simpler and self-contained. The bound improves the previous bound of Rusmevichientong and Tsitsiklis (2010) and it is applicable to virtually any online least squares problem. The bound that we derive, gives immediately rise to tight confidence sets for the online least squares estimate that can replace the confidence sets in existing algorithms. In particular, the confidence sets can be used in the UCB algorithm for the multi-armed bandit problem, the ConfidenceBall algorithm of Dani et al. (2008) for the linear bandit problem, and LinRel algorithm of Auer (2003) for the associative reinforcement learning problem.", "startOffset": 31, "endOffset": 1819}, {"referenceID": 2, "context": "In the case of linear bandits (Dani et al., 2008), we improve the problem dependent bound in the dimension and number of time steps. Furthermore, as opposed to the previous result, we prove that our bound holds for small sample sizes, and at the same time the worst case bound is improved by a logarithmic factor and the constant is improved. 1 Introduction The least squares method forms a cornerstone of statistics and machine learning. It is used as the main component of many stochastic sequential decision problems, such as multi-armed bandit, linear bandits, and other linear control problems. However, the analysis of least squares in these online settings is non-trivial because of the correlations between data points. Fortunately, there is a connection between online least squares estimation and the area of self-normalized processes. Study of self-normalized processes has a long history that goes back to Student and is treated in detail in recent book by de la Pe\u00f1a et al. (2009). Using these tools we provide a proof of a bound on the deviation for vector-valued martingales. A less general version of the bound can be found already in de la Pe\u00f1a et al. (2004, 2009). Additionally our proof, based on the method of mixtures, is new, simpler and self-contained. The bound improves the previous bound of Rusmevichientong and Tsitsiklis (2010) and it is applicable to virtually any online least squares problem. The bound that we derive, gives immediately rise to tight confidence sets for the online least squares estimate that can replace the confidence sets in existing algorithms. In particular, the confidence sets can be used in the UCB algorithm for the multi-armed bandit problem, the ConfidenceBall algorithm of Dani et al. (2008) for the linear bandit problem, and LinRel algorithm of Auer (2003) for the associative reinforcement learning problem. We show that this leads to improved performance of these algorithms. Our hope is that the new confidence sets can be used to improve the performance of other similar linear decision problems. The multi-armed bandit problem, introduced by Robbins (1952), is a game between the learner and the environment.", "startOffset": 31, "endOffset": 2124}, {"referenceID": 3, "context": "Lai and Robbins (1985) prove a ( \u2211 i6=i\u2217 1/D(pj, pi\u2217) \u2212 o(1)) logT lower bound on the expected regret of any algorithm, where T is the number of time steps, pi\u2217 and pi are the reward distributions of the optimal arm and arm i respectively, and D is the KL-divergence.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "Lai and Robbins (1985) prove a ( \u2211 i6=i\u2217 1/D(pj, pi\u2217) \u2212 o(1)) logT lower bound on the expected regret of any algorithm, where T is the number of time steps, pi\u2217 and pi are the reward distributions of the optimal arm and arm i respectively, and D is the KL-divergence. Auer et al. (2002) designed the UCB algorithm and proved a finite-time logarithmic bound on its regret.", "startOffset": 0, "endOffset": 287}, {"referenceID": 3, "context": "Lai and Robbins (1985) prove a ( \u2211 i6=i\u2217 1/D(pj, pi\u2217) \u2212 o(1)) logT lower bound on the expected regret of any algorithm, where T is the number of time steps, pi\u2217 and pi are the reward distributions of the optimal arm and arm i respectively, and D is the KL-divergence. Auer et al. (2002) designed the UCB algorithm and proved a finite-time logarithmic bound on its regret. He used Hoeffding\u2019s inequality to construct confidence intervals and obtained a O((K log T )/\u2206) bound on the expected regret, where \u2206 is the difference between the expected rewards of the best and the second best action. We modify UCB so that it uses our new confidence sets and we show a stronger result. Namely, we show that with probability 1 \u2212 \u03b4, the regret of the modified algorithm is O(K log(1/\u03b4)/\u2206). Seemingly, this result contradicts the lower bound of Lai and Robbins (1985), however our algorithm depends on \u03b4 which it receives as an input.", "startOffset": 0, "endOffset": 857}, {"referenceID": 2, "context": "Dani et al. (2008) proposed theConfidenceBall algorithm and showed that its regret is at mostO(d log(T ) \u221a T log(T/\u03b4)) with probability at most 1 \u2212 \u03b4.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Dani et al. (2008) proposed theConfidenceBall algorithm and showed that its regret is at mostO(d log(T ) \u221a T log(T/\u03b4)) with probability at most 1 \u2212 \u03b4. We modify their algorithm so that it uses our new confidence sets and we show that its regret is at most O(d log(T ) \u221a T + \u221a dT log(T/\u03b4)). Additionally, constants in our bound are smaller, and our bound holds for all T \u2265 1, as opposed the previous one which holds only for sufficiently large T . Dani et al. (2008) prove also a problem dependent regret bound.", "startOffset": 0, "endOffset": 466}, {"referenceID": 2, "context": "Most of this argument can be extracted from the paper of Dani et al. (2008). However, the idea goes back at least to Lai et al.", "startOffset": 57, "endOffset": 76}, {"referenceID": 2, "context": "Most of this argument can be extracted from the paper of Dani et al. (2008). However, the idea goes back at least to Lai et al. (1979), Lai and Wei (1982) (a similar argument is used around Theorem 11.", "startOffset": 57, "endOffset": 135}, {"referenceID": 2, "context": "Most of this argument can be extracted from the paper of Dani et al. (2008). However, the idea goes back at least to Lai et al. (1979), Lai and Wei (1982) (a similar argument is used around Theorem 11.", "startOffset": 57, "endOffset": 155}, {"referenceID": 2, "context": "Most of this argument can be extracted from the paper of Dani et al. (2008). However, the idea goes back at least to Lai et al. (1979), Lai and Wei (1982) (a similar argument is used around Theorem 11.7 in the book by Cesa-Bianchi and Lugosi (2006)).", "startOffset": 57, "endOffset": 249}, {"referenceID": 2, "context": "Most of this argument can be extracted from the paper of Dani et al. (2008). However, the idea goes back at least to Lai et al. (1979), Lai and Wei (1982) (a similar argument is used around Theorem 11.7 in the book by Cesa-Bianchi and Lugosi (2006)). Note that Lemmas B.9\u2013B.11 of Rusmevichientong and Tsitsiklis (2010) also give a bound on \u2211t k=1 \u2016mk\u22121\u2016 2 V \u22121 k\u22121 , with an essentially identical argument.", "startOffset": 57, "endOffset": 319}, {"referenceID": 2, "context": "Most of this argument can be extracted from the paper of Dani et al. (2008). However, the idea goes back at least to Lai et al. (1979), Lai and Wei (1982) (a similar argument is used around Theorem 11.7 in the book by Cesa-Bianchi and Lugosi (2006)). Note that Lemmas B.9\u2013B.11 of Rusmevichientong and Tsitsiklis (2010) also give a bound on \u2211t k=1 \u2016mk\u22121\u2016 2 V \u22121 k\u22121 , with an essentially identical argument. Alternatively, one can use the bounding technique of Auer (2003) (see the proof of Lemma 13 there on pages 412\u2013413) to derive a bound like \u2211t k=1 \u2016mk\u22121\u2016 2 V \u22121 k\u22121 \u2264 Cd log t for a suitable chosen constant C > 0.", "startOffset": 57, "endOffset": 472}, {"referenceID": 3, "context": "Note that the log(t) factor cannot be removed, as shown by Problem 3, page 203 in the book by de la Pe\u00f1a et al. (2009). 3 Optional Skipping Consider the case when d = 1, mk = \u03b5k \u2208 {0, 1}, i.", "startOffset": 100, "endOffset": 119}, {"referenceID": 1, "context": ", the paper of Bubeck et al. (2008)), we would get, for any 0 < \u03b4 < 1, t \u2265 2, with probability 1\u2212 \u03b4, \u2200s \u2208 {0, .", "startOffset": 15, "endOffset": 36}, {"referenceID": 1, "context": "in the paper of Bubeck et al. (2008) to improve the computational complexity of the HOO algorithm.", "startOffset": 16, "endOffset": 37}, {"referenceID": 1, "context": "in the paper of Bubeck et al. (2008) to improve the computational complexity of the HOO algorithm. Also, the coefficient of the leading term in (11) under the square root is 1, whereas in (12) it is 2. Instead of a union bound, it is possible to use a \u201cpeeling device\u201d to replace the conservative log t factor in the above bound by essentially log log t. This is done e.g. in Garivier and Moulines (2008) in their Theorem 22.", "startOffset": 16, "endOffset": 405}, {"referenceID": 0, "context": "By using Lemma 8 of Antos et al. (2010), we get that Ni,s \u2264 3 + 16 \u2206i log 2K \u2206i\u03b4 .", "startOffset": 20, "endOffset": 40}, {"referenceID": 3, "context": "Lai and Robbins (1985) prove that for any suboptimal arm j, E [Ni,t] \u2265 log t D(pj , p\u2217) , where, p\u2217 and pj are the reward density of the optimal arm and arm j respectively, and D is the KD-divergence.", "startOffset": 0, "endOffset": 23}, {"referenceID": 2, "context": "5 Application to Least Squares Estimation and Linear Bandit Problem In this section we first apply Theorem 3 to derive confidence intervals for least-squares estimation, where the covariate process is an arbitrary process and then use these confidence intervals to improve the regret bound of Dani et al. (2008) for the linear bandit problem.", "startOffset": 293, "endOffset": 312}, {"referenceID": 2, "context": "The above bound could be compared with a similar bound of Dani et al. (2008) whose bound, under identical conditions, states that (with appropriate initialization) with probability 1\u2212\u03b4, for all t large enough, \u2225", "startOffset": 58, "endOffset": 77}, {"referenceID": 2, "context": "Consider the ConfidenceBall algorithm of Dani et al. (2008). We use the confidence intervals (21) and change the action selection rule accordingly.", "startOffset": 41, "endOffset": 60}, {"referenceID": 2, "context": "3 Problem Dependent Bound (\u2206 > 0) Let \u2206 be as defined in (Dani et al., 2008). In this section we assume that \u2206 > 0. This includes the case when the action set is a polytope. First we state a matrix perturbation theorem from Stewart and Sun (1990) that will be used later.", "startOffset": 58, "endOffset": 247}, {"referenceID": 2, "context": "3 Problem Dependent Bound (\u2206 > 0) Let \u2206 be as defined in (Dani et al., 2008). In this section we assume that \u2206 > 0. This includes the case when the action set is a polytope. First we state a matrix perturbation theorem from Stewart and Sun (1990) that will be used later. Theorem 16 (Stewart and Sun (1990), Corollary 4.", "startOffset": 58, "endOffset": 307}], "year": 2011, "abstractText": "We improve the regret bound of the Upper Confidence Bound (UCB) algorithm of Auer et al. (2002) and show that its regret is with high-probability a problem dependent constant. In the case of linear bandits (Dani et al., 2008), we improve the problem dependent bound in the dimension and number of time steps. Furthermore, as opposed to the previous result, we prove that our bound holds for small sample sizes, and at the same time the worst case bound is improved by a logarithmic factor and the constant is improved.", "creator": "LaTeX with hyperref package"}}}