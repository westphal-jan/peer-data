{"id": "1102.2808", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2011", "title": "Transductive Ordinal Regression", "abstract": "Ordinal regression is commonly formulated as a multi-class problem with ordinal constraints. The challenge of designing accurate classifiers for ordinal regression generally increases with the number of classes involved, due to the large number of labeled patterns that are needed. The availability of ordinal class labels, however, are often costly to calibrate or difficult to obtain. Unlabeled patterns, on the other hand, often exist in much greater abundance and are freely available. To take benefits from the abundance of unlabeled patterns, we present a novel transductive learning paradigm for ordinal regression in this paper, namely Transductive Ordinal Regression (TOR). The key challenge of the present study lies in the precise estimation of both the ordinal class label of the unlabeled data and the decision functions of the ordinal classes, simultaneously. The core elements of the proposed TOR include an objective function that caters to several commonly used loss functions casted in transductive settings, for general ordinal regression. A label swapping scheme that facilitates a strict monotonic decrease in the objective function value is also introduced. Extensive numerical studies on commonly used benchmark datasets including the real world sentiment prediction problem are presented to showcase the characteristics and efficacies of the proposed transductive ordinal regression. Further, comparisons to recent state-of-the-art ordinal regression methods demonstrate the introduced transductive learning paradigm for ordinal regression led to the robust and improved performance.", "histories": [["v1", "Mon, 14 Feb 2011 15:53:06 GMT  (981kb)", "https://arxiv.org/abs/1102.2808v1", null], ["v2", "Tue, 15 Feb 2011 12:46:46 GMT  (890kb,S)", "http://arxiv.org/abs/1102.2808v2", null], ["v3", "Thu, 30 Aug 2012 02:23:16 GMT  (1309kb)", "http://arxiv.org/abs/1102.2808v3", null], ["v4", "Fri, 31 Aug 2012 02:54:05 GMT  (1313kb,S)", "http://arxiv.org/abs/1102.2808v4", null], ["v5", "Mon, 3 Sep 2012 02:17:30 GMT  (1311kb,S)", "http://arxiv.org/abs/1102.2808v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chun-wei seah", "ivor w tsang", "yew-soon ong"], "accepted": false, "id": "1102.2808"}, "pdf": {"name": "1102.2808.pdf", "metadata": {"source": "CRF", "title": "Transductive Ordinal Regression", "authors": ["Chun-Wei Seah", "Ivor W. Tsang", "Yew-Soon Ong"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is the case that most of them are able to survive themselves, and that they are able to survive themselves, \"he said.\" But it is not the case that they can survive themselves. \"In fact,\" it is not the case that they are able to survive themselves, but that they are able to survive themselves. \"In the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, and in the second half of the 20th century."}, {"heading": "II. REVIEW OF ORDINAL REGRESSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Notation", "text": "Throughout the rest of this work, a high sentence T denotes the transposition of a vector or a matrix. In view of n samples described: (xi, y1), (x2, y2),..., (xn, yn) in the dataset, where xi-p represents the ith sample with the order class designation yi, {1, 2,..., K}. Let us also consider a threshold model, as shown in Figure 1, where a K order class problem K \u2212 1 has ordered thresholds: \u03b81 < \u04452 <. < \u0445K \u2212 1. Therefore, a sample x is classified as class i if the predictive output h (x) = wTx is in the range of \u03b8i \u2212 1 < h (x) \u2264 \u0445i, where w p and \u04450 = \u2212 \u0438 and \u0445K = \u0438 are typically assumed."}, {"heading": "B. Ordinal Regression as an Extended Binary Classification Model", "text": "Ordinary regression using a threshold model generally looks at the problem of the extended binary classification [?] of the form: x k i = (xi, ek), x p + K \u2212 1, yki = 1 \u2212 2I [yi \u2264 k], (1) for k = 1, 2,..., K \u2212 1. Here ek K \u2212 1 denotes a vector with the kest element as a value 1 and the rest of the elements as a value zero, and I [\u00b7] denotes an indicator function that returns 1 if the predicate holds, otherwise a zero is returned. Essentially, each sample described in the original dataset is duplicated K \u2212 1 times, and the kest copy is completed with ek and assigned to the transformed problem with a binary label yki. A binary classifier with a weight vector w = (w, \u2212 \u03b8) is used to compress T = (SMp + K \u2212 1, (2), then an IS is learned to predict an IS = = optimik, T \u00b7 w = \u2212."}, {"heading": "III. TRANSDUCTIVE ORDINAL REGRESSION", "text": "In this section we introduce the essential components of the proposed TOR algorithm for the ordinary regression. < p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p p? p? p? p? p? p? p p? p p? p p? p p p p? p? p p p? p? p p p p? p? p? p p p? p? p p p p? p p p p p p? p? p p p p? p p p? p p p? p p p? p p p p? p? p p p p? p? p p p p p? p p p p? p? p p p p? p p p p p? p? p p p? p p p p p? p p p p p p? p p p? p p? p p p p p p? p p p p p p? p? p p p p? p p p p p? p? p p p p p? p? p p p? p? p p p p p p? p p p p p p p? p? p p p p p? p p p? p p p p p p p p p p p? p p p p p p p p p? p p p p p p p p p p? p p p p? p"}, {"heading": "A. Pseudo-labels of Unlabeled Data Initialization", "text": "The initialization phase of the TOR focuses on the assignment of the first pseudo-labels to the unlabeled data. By using a large margin criterion, the optimization problem can lead to trivial solutions, for example, classifying all unlabeled data with positive labels [?], [?]. Common practice in transductive learning is to impose certain class ratio restrictions on the final labels of the unlabeled data (e.g. assuming a balanced class distribution), where such an assumption has been shown to alleviate the problem of unbalanced output distribution and improve predictive performance [?]. Based on this criterion, the pseudo-labels of the unlabeled data are limited to the class distribution of the labeled data. Specifically, the limitations are implicitly addressed by the process of first training a supervised OR classifier on available labeled data and then the unlabeled data are assigned to the labeled class according to the labeled data."}, {"heading": "B. Transductive Learning by Label Swapping", "text": "After the initialization phase to define the structural risk of (4), the minimization of (4) is continued with a 2-step swap procedure, the first of which involves the fixation of the y function to solve h and \u03b8. Next, both the derived h and the derived h function are fixed in turn to find suitable y functions to minimize the objective (4). Definition 1. Loss function is defined with the following properties: 1), 2), 2), 2), 3), 4), 5 (2), 5 (2), 5 (2), 5 (2), 5 (2), 5 (2), 5 (2), 5 (2), 5 (2), 5 (2), 5), 5 (2), 5), 6 (2), 6 (2), 6 (2), 6 (6 (), 7 (), 7 (), 7 (), 7 (), 7 (), 8 (), 8 (), 8 (), (), 8 (), 8 (), 8 (8, 8, 8, 8 (8), 8, 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (), 8 (8)."}, {"heading": "C. Control Parameters", "text": "C1 and C2 denote the control parameters of the proposed TOR, which are described in detail in algorithm 1. C1 specifically regulates the trade-off between error classification errors on the labeled samples and model complexity. In the same way, C2 regulates the trade-off for the unlabeled samples. C1 denotes a user-specific parameter, while C2 heuristically is derived in TOR. Typically, C2 is initialized with a low value and gradually increased to C1, in the sense of [?]. This is a common heuristic strategy used to reduce the possibility of premature convergence and to get stuck in a poor approximate solution when assigning the labels of the unlabeled data. Note that when C2 tends toward zero, the algorithm becomes a typical supervised learning problem. Therefore, increasing C2 gradually transforms the problem of ordinal regression toward TOR. When the criterion weight of the label pair is reached, the label class for the label pair, the label pair is defined as the label weight for the label pair."}, {"heading": "IV. GENERALIZING THE FAMILY OF BINARY LOSS", "text": "FUNCTIONS IN TORIn this section, we generalize a family of existing binary functions for use as a potential loss function in TOR. In particular, Section IV-A defines how K \u2212 1 binary functions can be used as a loss function in TOR. Subsequently, Section IV-B presents an instantiation of TOR with hinge loss. Next, Section IV-C discusses replacing the label of TOR with K-ordinal problem."}, {"heading": "A. Superimposing extended binary functions as the loss function of TOR", "text": "Using the representation in the extended binary classification, binary loss functions defined to fulfill the properties of Def. 1 (via superimposed K \u2212 1 binary loss functions) are defined as follows: (\u2212 K \u2212 \u2212 K = 1) Each binary loss has the following properties: Definition 3. Binary loss function is defined as follows: 1) a > 0 (\u2212 a) > Property is defined: 1 (a), 2). Each binary loss function has the following properties: Definition 3. Binary loss function is defined."}, {"heading": "B. An Instantiation of TOR using Hinge loss", "text": "As already mentioned in Section IV-A, our proposed framework can take into account several commonly used loss functions, the Def. 3 to minimize the structural risk of (4). Here we can illustrate an instance of TOR based on extended binary loss, as it is commonly used in SVM, and Def. 3: For a certain marked risk, we can be derived as: max {0, 1 \u2212 yi}, and using the extended binary classification model (7), in which both the term augmented xki and the extended binary loss function yki () are derived for a certain threshold as: max {0, 1 \u2212 yki (w) -yki (7), in which both the term augmented xki and w (1) are defined as well as (2), respectively. From (7), the ordinary loss function yi (), the superimposed function yi ()."}, {"heading": "C. Discussion of label swapping for K ordinal class problem", "text": "Sentence 2 for TOR is a generalization of the K-ordinary class problem, so the sentence also applies to the binary class problems described in [?]. However, the TSVM in [?] cannot elegantly handle ordinal classification problems. For example, data {x, y = 3} in a 5-class problem can be extended to binary data by (1) as {(x, e1), 1}, {(x, e2), 1}, {(x, e3), \u2212 1}, {(x, e4), \u2212 1} in a 5-class problem. However, replacing it with another data vector can cause the data set to violate the ordinal properties defined in (1) (e.g. {(x, e1), \u2212 1}, {(x, e2), 1}, {(x, e3), \u2212 1}, {(x, e4), \u2212 1}, and {(x, e1)."}, {"heading": "V. EXPERIMENTS", "text": "In this section, we examine the effectiveness of several state-of-the-art regression algorithms and the proposed transductive regression described in Table I, using a set of benchmark data sets and the task of predicting sentiment. Since existing regression models can only handle marked data, the comparison to three state-of-the-art algorithms trained with marked data is also taken into account in the present study (namely RED-SVM3 using (9), SVOR-EXC4 and SVOR-IMC4). To investigate the effects of cluster acceptance on unmarked data, the comparison with the multi-class transductive SVM (M-TSVM) [?] is considered as cordable by using a multiclass training paradigm. In the experimental study, the MTSVM is considered to be labeled as well as unlabeled data and incorporating the coordinated data."}, {"heading": "A. Experimental Setup", "text": "For each data set, the labeled data is randomly divided into different sizes (100, 150, 200, 250, 300, 350 and 400). s3http: / / www.work.caltech.edu / \u0445 htlin / program / libsvm / # ordinal 4http: / / www.gatsby.ucl.ac.uk / \u0445 chuwei / svor.htm then denotes the sample size of each data set described in Tables III and IV, s \u2212 400. To report statistically significant results on the unlabeled data, the average test results of 20 independent realizations are determined using a quintuple cross-validation procedure using log10C1 (\u2212 3, \u2212 2, \u2212 1, 0, 1, 1, 2, 3, 4, 5}. To statistically significantly evaluate the unlabeled data, the average test results of 20 independent realizations are presented."}, {"heading": "B. Benchmark data sets", "text": "Four commonly used benchmark datasets (Abalone, Bank, California and Census) for ordinal regression problems are considered in this study. Statistics of these benchmark datasets are summarized in Table III. These datasets have a quantization level of K = 5. For all algorithms, we consider the perceptron nucleus [?], which is defined as \u2206 p \u2212 | x \u2212 x \u2032 | | 2, where p denotes a constant. As discussed in [?], the SVM perceptron nucleus can be used to construct an infinite interplay of classifiers over perceptrons. In other words, the resulting SVM classifier using the perceptron nucleus corresponds to a neural network with a hidden layer containing infinite hidden neurons. Furthermore, on the basis of the Karush Kuhn Tucker (KKT) conditions, the dual dataset can be modified."}, {"heading": "C. Synthetic data set", "text": "A synthetic dataset with different degrees of cluster acceptance is created on the basis of our generator described in algorithm 3 to measure the performances of the transductive TOR against the non-transduction RED-SVM. Algorithm 3 Synthetic Data Set Generator 1: Inputs: y [1,.. K], where K is the number of ordinal classes, p is a parameter to control the strength of the cluster acceptance 2: for int d = 1; d \u2264 2000 (K + 2); d + + do 3: if d [2000 (y \u2212 1), 2000 (y + 2)] then 4: if rand () < 0.01 then 5: xd = rand () 6: otherwise 7: xd = 0 8: end if 9: else10: if rand () < 0.01p then 11: xd = rand (), 12: otherwise 13: xd = x x: end if 15: end for 17: return xall holds the cluster acceptance if each class is defined by a specific function."}, {"heading": "D. Sentiment data sets", "text": "The data sets for the sentiment prediction7 by definition [?] were generated by Amazon.com and cover four categories of product ratings: book, DVDs, electronics, and kitchen appliances. The ratings consist of five order labels ranging from 1 to 5. A higher rating means better evaluation feedback. The details regarding the sample and the characteristic size of the sentiment data sets are in Table IV. In the experimental study, we further processed the data sets by removing all stop words, normalizing and deriving each feature. Finally, each feature of a review is represented by its respective tf-idf value. The inner product of two evaluations is defined using cosmic similarity, with linear core used in the experiments.6The perctron kernel was reported as a competitive data core for the Gaussian kernel [?], but one advantage of the Percepon kernel is higher compression efficiency than in some cases [?] 7."}, {"heading": "VI. DISCUSSIONS ON EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Results on Benchmark and synthetic Datasets", "text": "The results of the study show that people in the United States, Europe, Asia, Asia, Asia, Africa, Asia, Asia, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa"}, {"heading": "B. Results on Real World Sentiment Datasets", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we have presented a novel transductive learning paradigm for ordinal regression, namely Transductive Ordinal Regression (TOR). To our knowledge, the present work serves as the first attempt to address the general ordinal regression problem in a transductive environment for a family of ordinal loss functions, supporting the family of ordinal loss functions including hinge loss, logistic loss and laplastic loss. A proposed label-swapping scheme is also introduced to ensure a strictly monotonous decrease in the objective value of the transductive ordinal function. Based on the experimental results, it has been reported that TOR achieves significant improvements in accuracy over all other algorithms considered through the use of cluster adoption on the unlabeled data and ordal constraints to maximize differentiation between successive classes of ordinal regression. In situations where few data are available, TOR clearly serves as an indispensable tool."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "Ordinal regression is commonly formulated as a multi-class problem with ordinal constraints. The challenge of designing accurate classifiers for ordinal regression generally increases with the number of classes involved, due to the large number of labeled patterns that are needed. The availability of ordinal class labels, however, is often costly to calibrate or difficult to obtain. Unlabeled patterns, on the other hand, often exist in much greater abundance and are freely available. To take benefits from the abundance of unlabeled patterns, we present a novel transductive learning paradigm for ordinal regression in this paper, namely Transductive Ordinal Regression (TOR). The key challenge of the present study lies in the precise estimation of both the ordinal class label of the unlabeled data and the decision functions of the ordinal classes, simultaneously. The core elements of the proposed TOR include an objective function that caters to several commonly used loss functions casted in transductive settings, for general ordinal regression. A label swapping scheme that facilitates a strictly monotonic decrease in the objective function value is also introduced. Extensive numerical studies on commonly used benchmark datasets including the real world sentiment prediction problem are then presented to showcase the characteristics and efficacies of the proposed transductive ordinal regression. Further, comparisons to recent state-of-the-art ordinal regression methods demonstrate the introduced transductive learning paradigm for ordinal regression led to the robust and improved performance.", "creator": "LaTeX with hyperref package"}}}