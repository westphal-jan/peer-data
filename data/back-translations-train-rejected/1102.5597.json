{"id": "1102.5597", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2011", "title": "Fast and Faster: A Comparison of Two Streamed Matrix Decomposition Algorithms", "abstract": "With the explosion of the size of digital dataset, the limiting factor for decomposition algorithms is the \\emph{number of passes} over the input, as the input is often stored out-of-core or even off-site. Moreover, we're only interested in algorithms that operate in \\emph{constant memory} w.r.t. to the input size, so that arbitrarily large input can be processed. In this paper, we present a practical comparison of two such algorithms: a distributed method that operates in a single pass over the input vs. a streamed two-pass stochastic algorithm. The experiments track the effect of distributed computing, oversampling and memory trade-offs on the accuracy and performance of the two algorithms. To ensure meaningful results, we choose the input to be a real dataset, namely the whole of the English Wikipedia, in the application settings of Latent Semantic Analysis.", "histories": [["v1", "Mon, 28 Feb 2011 05:26:58 GMT  (159kb)", "http://arxiv.org/abs/1102.5597v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["radim \\v{r}eh{\\r{u}}\\v{r}ek"], "accepted": false, "id": "1102.5597"}, "pdf": {"name": "1102.5597.pdf", "metadata": {"source": "CRF", "title": "Fast and Faster: A Comparison of Two Streamed Matrix Decomposition Algorithms", "authors": ["Radim \u0158eh\u016f\u0159ek"], "emails": ["radimrehurek@seznam.cz"], "sections": [{"heading": null, "text": "ar Xiv: 110 2.55 97v1 [cs.NA] 2 8Fe b"}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them will be able to move to a different world in which they are able than to another world in which they are able to, in which they live, in which they live."}, {"heading": "1.1 Stochastic two-pass algorithm, P2", "text": "The stochastic single-pass algorithm, as described in [Halko et al., 2009], is unsuitable for large-scale decompositions because it requires O (nk + mk) memory to calculate. We can reduce this to a manageable O (mk), i.e. independent of the size of the input current n, at the expense of two passes over the input matrix instead of one2. This is achieved by two optimizations: 1) the sample matrix is built piece by piece from the current, rather than a direct matrix multiplication, and 2) the final dense decomposition is performed on a smaller k \u00b7 k eigenproblem BBT instead of the complete k \u00b7 n matrix B. These two \"tricks\" allow us to calculate decomposition in constant memory by processing the observations successively, or preferably in chunks as large as they fit into the core memory."}, {"heading": "1.2 One-pass algorithm, P1", "text": "Transmitted one-pass algorithms are fundamentally different from the two-pass algorithm described above (or any other multipass algorithm) in that they manage to keep their memory requirements constant, allowing us to process infinite input streams. In environments where input cannot be stored permanently, this may be the only option. In [R] ehu r ek, 2010], I describe such an algorithm. It works by splitting chunks of document in the core, possibly on different machines, and efficiently merging these dense partial decompositions into one. The partial decomposition algorithm at the core is considered a \"black box\" and selected as Douglas Rohdes SVDLIBC. The coarse parallelism of this algorithm makes it suitable for distributing the calculation over a cluster of raw material computers connected by a high-latency network."}, {"heading": "2 Experiments", "text": "We will compare the algorithms on an implicit 100,000 x 3,199,665 sparse matrix with 0.5 billion unequal entries (0.15% density) representing the entire English Wikipedia3, truncating the vocabulary (number of characters) to the 100,000 most common word types.4 In all the experiments, the number of eigenfactors requested is arbitrarily reduced to k = 400.The experiments were carried out with three 2.0 GHz Intel Xeon workstations with 4GB of RAM connected via Ethernet to a single network segment; the machines were not dedicated; due to the large number of experiments, we were only able to perform each experiment twice."}, {"heading": "2.1 Oversampling", "text": "In this series of experiments, we examine the relative accuracy of the three algorithms. P2 has two parameters that influence accuracy: the oversampling factor l and the number of power siterations q. In the P1 and P12 pass-in algorithms, we improve accuracy by requiring additional factors l in intermediate calculations, which are shortened at the very end of the decomposition. Figure 1 summarizes both the relative accuracy and runtime performance of the algorithms in multiple selections of l and q. We see that although all methods are very accurate for the largest factors, without the accuracy decreasing rapidly, this is especially true for the P2 algorithm, where oversampling does not help and performance siterations are definitely required. \"Soil truth\" decomposition is unknown, so we cannot give absolute errors."}, {"heading": "2.2 Chunk size", "text": "The P1 and P12 single-pass algorithms work in pieces of document that fit into the core memory. A natural question is what effect the size of these pieces will have on performance and accuracy; for smaller pieces, the algorithm will use less memory; for larger pieces, it will perform fewer merges, so we could expect better performance; this intuition is quantified in Figure 2, where accuracy and performance results for pieces of 10,000, 20,000 and 40,000 documents are listed. We see that piece sizes in this range have little impact on accuracy, and that performance gradually improves with increasing piece size. This acceleration is inversely proportional to the efficiency of the decomposition algorithm: with a hypothetical zero-cost merge algorithm, there would be no improvement at all, and runtime would be strictly dominated by the cost of core decomposition operations. On the other hand, a very expensive merge routine would imply a linear relationship."}, {"heading": "2.3 Input stream order", "text": "In the Wikipedia input stream, the observations are presented in lexicographic order - the observation corresponding to the Wikipedia entry on anarchy precedes the entry on the Bible, which comes before the censorship, etc. Of course, this order is anything but random, so we are naturally interested in how it affects the resulting decomposition of the single-pass algorithms (the two-pass algorithm is structurally order-agnostic).To test this, we randomly mixed the input flow and restarted the experiments on P1. Ideally, the results should be identical no matter how we mute the input flow. The results in Figure 3 show that this is not the case: Singular values derived from the mixed runs are significantly different from those from the original, alphabetically ordered sequence. This probably shows that the one-pass truncated scheme has some difficulty adapting to the gradual drift of the sub-space."}, {"heading": "2.4 Distributed computing", "text": "The two single-pass algorithms P1 and P12 are suitable for easy parallelization. In Figure 4, we evaluate them on a cluster of 1, 2 and 4 computing nodes. The scaling behavior is linear in the number of machines, since virtually no communication takes place except for sending the input data and capturing the results. As with cluster size, the choice of cluster size does not have much impact on accuracy. The P2 algorithm can also be distributed, but is already dominated by the cost of data access in its q + 2 passes. Routing data around the network does not give a boost in performance, so we leave the results out in the mapping. We note that the distribution of P2 would make sense on condition that the data is already distributed to the computing nodes, perhaps via a distributed file system."}, {"heading": "3 Conclusion", "text": "We presented a streamed version of a two-pass stochastic own decomposition algorithm and compared it to two streamed one-pass algorithms, one of which is a novel distributed one-pass algorithm. Comparison was performed in the context of latent semantic analysis, on a corpus of 3.2 million documents encompassing English Wikipedia. On a single 2GHz machine, we achieved decomposition times of 4 hours and 42 minutes for the one-pass P12 algorithm and 3 hours and 6 minutes for the stochastic multipass algorithm. Without power iterations and with a reduced amount of oversampling, we recorded even lower times, but at the expense of a serious loss of accuracy. On a cluster of four computing nodes on three physical machines, the single pass P12 was completed in 1 hour and 41 minutes. We observed that the lightning-fast stochastic algorithm suffers from serious accuracy problems, which is most costly by increasing the number of passes (the 2009)."}, {"heading": "Acknowledgments", "text": "This study was partially supported by the LC536 grant from MS-MT C-R."}, {"heading": "A Streamed Stochastic Eigen Decomposition", "text": "Algorithm 1: 2-Pass Stochastic Decomposition in Constant Memory with Streamed InputInput: m \u00b7 n input matrix A, presented as a stream of observation chunks A = [C1, C2,..., CC]. Truncation factor k. Oversampling factor l. Number of power siterations q. Output: U, S2 spectral decomposition of A (i.e, US2UT = AAT) cut to the k greatest factors. Data: Intermediate matrices require O (k + l) memory; in particular, the algorithm avoids materialization of O (n) or O (m2) matrices. / / Construct the m \u00d7 (k + l) sample matrix Y = AO, in a pass over the input Sm (k +) Smatrict Sum the first swamp (CiOi for Ci in A); / / / each Oi is a random | Ci."}, {"heading": "B Wikipedia LSA Topics", "text": "The first ten places in the \"Top Ten\" list are occupied by the United States, followed by the United Kingdom, France, Great Britain, Great Britain, Great Britain, France, Great Britain, the United Kingdom, the United States, Great Britain, France, the United States, the United Kingdom, France, France, France, the United States, the United Kingdom, the United Kingdom, France, France, the United States, the United States, the United States, France, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, France, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, France, the United States, the United States, the United States, the United States, the United States, France, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, France, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States, the United States of America, the United States, the United States, the United States, the United States, the United States of America, the United States, the United States, the United States, the United States of America, the United States, the United States, the United States, the United States, the United States of America, the United States, the United States, the United States, the United States of America, the United States of America, the United States, the United States, the United States of America, the United States"}], "references": [{"title": "Tracking a few extreme singular values and vectors in signal processing", "author": ["Comon", "Golub", "P. 1990] Comon", "G. Golub"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Comon et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Comon et al\\.", "year": 1990}, {"title": "Generalized hebbian algorithm for incremental Latent Semantic Analysis", "author": ["Gorrell", "Webb", "G. 2005] Gorrell", "B. Webb"], "venue": "In Ninth European Conference on Speech Communication and Technology", "citeRegEx": "Gorrell et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Gorrell et al\\.", "year": 2005}, {"title": "Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions", "author": ["Halko et al", "N. 2009] Halko", "P. Martinsson", "J. Tropp"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Sequential Karhunen\u2013Loeve basis extraction and its application to images", "author": ["Levy", "Lindenbaum", "A. 2000] Levy", "M. Lindenbaum"], "venue": "IEEE Transactions on Image processing,", "citeRegEx": "Levy et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2000}, {"title": "On updating problems in Latent Semantic Indexing", "author": ["Zha", "Simon", "H. 1999] Zha", "H. Simon"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Zha et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Zha et al\\.", "year": 1999}], "referenceMentions": [], "year": 2016, "abstractText": "With the explosion of the size of digital dataset, the limiting factor for decomposition algorithms is the number of passes over the input, as the input is often stored out-of-core or even off-site. Moreover, we\u2019re only interested in algorithms that operate in constant memory w.r.t. to the input size, so that arbitrarily large input can be processed. In this paper, we present a practical comparison of two such algorithms: a distributed method that operates in a single pass over the input vs. a streamed two-pass stochastic algorithm. The experiments track the effect of distributed computing, oversampling and memory trade-offs on the accuracy and performance of the two algorithms. To ensure meaningful results, we choose the input to be a real dataset, namely the whole of the English Wikipedia, in the application settings of Latent Semantic Analysis.", "creator": "LaTeX with hyperref package"}}}