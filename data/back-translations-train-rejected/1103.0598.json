{"id": "1103.0598", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2011", "title": "Learning transformed product distributions", "abstract": "We consider the problem of learning an unknown product distribution $X$ over $\\{0,1\\}^n$ using samples $f(X)$ where $f$ is a \\emph{known} transformation function. Each choice of a transformation function $f$ specifies a learning problem in this framework.", "histories": [["v1", "Thu, 3 Mar 2011 02:46:51 GMT  (41kb)", "http://arxiv.org/abs/1103.0598v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["constantinos daskalakis", "ilias diakonikolas", "rocco a servedio"], "accepted": false, "id": "1103.0598"}, "pdf": {"name": "1103.0598.pdf", "metadata": {"source": "CRF", "title": "Learning transformed product distributions", "authors": ["Constantinos Daskalakis", "Ilias Diakonikolas", "Rocco A. Servedio"], "emails": ["costis@csail.mit.edu", "ilias@cs.berkeley.edu", "rocco@cs.columbia.edu"], "sections": [{"heading": null, "text": "ar Xiv: 110 3.05 98v1 [cs.LG] 3 Mar 201 1Information-theoretical arguments show that for each transformation function f the corresponding learning problem can be solved precisely with the help of a generic algorithm whose runtime can be exponentially in n. We show that this learning problem is mathematically insoluble even with constant and rather simple transformation functions. Furthermore, the sample complexity limit given above is almost optimal for the general problem, since we specify a simple, explicit linear transformation function f (x) = w \u00b7 x with integer weights wi \u2264 n and prove that the corresponding learning problem requires suitable examples. As our most important positive result, we give a highly efficient algorithm for learning a sum of independent unknown Bernoulli random variables corresponding to the transformation function f (x) = 1 xi. Our algorithm learns to achieve accuracy in poly (n) time by also giving a surprising poly (1) which is efficient."}, {"heading": "1 Introduction", "text": "We look at the problem of learning an unknown product distribution transformed according to a known function f. This is a simple and natural learning problem, but one that does not seem to have been explicitly systematically examined from the perspective of mathematical learning theory. Specifically, in this thesis we limit our model to the natural case where the input distribution is a product distribution via the Boolean cube {0, 1} n. In this learning scenario the learner receives samples from the random variable f (X), where X = (X1,., Xn) is a vector of the independent 0 / 1 Bernoulli random variable Xi whose expectations are unknown to the learner. We write p = (p1,., CCn) n to denote E [X], and point to p as the target vector of probabilities; we will sometimes write f (p) to denote the above-described random variable f (X)."}, {"heading": "1.1 Motivation, examples, and connection to prior work", "text": "Our motivation to consider this model is twofold. First, we believe that it is so simple and natural that it could justify a direct access to the study for its own sake. Second, we believe that it provides a useful perspective on modeling probability distributions in environments where the underlying source of randomness is not directly accessible to the learner. In many environments, we can understand a phenomenon (in the physical world, in a market, etc.) in which the available observations can be seen as the result of a transformation f applied to an underlying random source X; learning an exact approximation of the distribution of f (X) is a natural goal in such an environment. (The limitation to X imposed in this paper - that it is a product of independent Bernoulli random variables - admittedly represents an idealized scenario, but it is a natural starting point for theoretical studies) is plausible as the consequence of such a situation being well known in our market."}, {"heading": "1.2 Our results and techniques", "text": "We find a number of positive and negative results for this learning problem, both for general functions and for certain transformation functions of interest. (Most of these works focus on the case where the transformation function f (X) is simply a real random variable, i.e. we have to estimate a transformation similar to the algorithm of Devroye and Lugosi (2001) for selecting a density, we show (Theorem 6) that the transformation function f (p) n of all f-transformed product distributions n, then there is a generic learning algorithm for the f-transformed product distributions, we show (Theorem 6) that the space {f (p)} p [0,1] p [2] p [2] p [4] p [4] p [5] p [5] p [5] p [5] p [5] p] p [5] p] p [5] p p [5] p p [5] p p p p [5] p [5] p [5] p [5] p [5] p p [5] p [5] p [5] p p [5] p [5] p [5] p p [5] p [5] p [5] p [5] p [5] p [5] p [5] p [5] p [5] p [5] p [5] p [5] p [5] p [5] p] p [5] p [5] p [5] p [5] p] p [5] p [5] p [5] p [5] p] p [5] p [5] p [5] p] p [5] p] p [5] p] p [5] p [5] p [5] p] p [5] p] p [5] p [5] p [5] p [5] p] p [5] p] p [5] p [5] p [5] p] p [5] p [5] p [5] p] p [5] p [5] p] p] p [5] p] p [5] p] p] p [5] p] p [5] p [5] p] p [5] p] p] p] p] p ["}, {"heading": "2 Preliminaries", "text": "Remember that the total variation distance between two distributions P and Q over a finite domain D is dTV (P, Q): = (1 / 2) \u00b7 \u2211 \u03b1 D | P (\u03b1) \u2212 Q (\u03b1) |. Similarly, if X and Y are two random variables extending over a finite set, their total variation distance dTV (X, Y) is defined as the total variation distance between their distributions. Another idea of the distance between distributions / random variables that we use is the Kolmogorov distance. For two distributions P and Q, which are based on R, their Kolmogorov distance dK (P, Q): = supx, P (\u2212 \u221e, x] \u2212 Q (\u2212 \u221e, x]) |.2A simple reduction to distinguish a fair coin from a biased coin shows that each learning algorithm requires a specific distribution for this problem."}, {"heading": "3 A generic algorithm for any f and some lower bounds", "text": "In this section, we specify a simple generic algorithm to solve the learning problem of transformed product distribution for each transformation function f, and cite some lower limits that show that even for rather simple functions f, the time and sample complexity of the generic algorithm can be essentially the best possible."}, {"heading": "3.1 A generic algorithm", "text": "The key ingredient in the generic algorithm is the following: Theorem 6 Devric a function f: {0, 1} n, where each range is set. Suppose there is a high-level idea behind Theorem 6 is as follows: for a pair of distributions Q1, Q2, Q2, S, we define a contest between Q1 and Q2, which takes a sample from the target distribution f (X) and either a crown of Q1, Q2 as the winner of the contest or calls the contest a draw. Let Q'S provide a cover for f (S) of the cardinality N = N (n, L). The algorithm conducts a tournament between each pair of distributions of Q and returns a distribution Q that has never been achieved."}, {"heading": "3.2 Learning transformed product distributions can be computationally hard", "text": "Although Theorem 1 shows that any learning problem f within our framework can be solved with the complexity of the O sample, it is of course to be expected that some learning problems can be mathematically difficult. We confirm this intuition by determining a NP hardness result for a particular function f, which is calculated by an explicit degree2 polynomial. We show that if there is a poly (n) age algorithm for the transformed learning problem of product distribution for this f, even for learning with constant accuracy, then NP BPP. Let us remember Theorem 2: Theorem 2 Suppose NP * BPP. Then there is an explicit grade 2 polynomial f, so that there is no polynomial age algorithm that solves the transformed learning problem of product distribution for f to the accuracy of 1 / 3."}, {"heading": "3.3 Linear transformation functions can require \u2126(n) samples", "text": "We now show that even with a simple linear transformation function f (x) = w \u00b7 x with low integer weights, it may be impossible to significantly improve the complexity of the O (n) sample of the generic algorithm from theorem 1. Let us remember Theorem 3: Theorem 3 (lower limit of sample complexity) Repair each even k \u2264 n and leave f (x) = \u2211 ki = k / 2 + 1 ixi. Let L be some learning algorithm that provides a hypotheses vector p so that dTV (f (p), f (p))) \u2264 1 / 40 with a probability of at least e \u2212 o (k)."}, {"heading": "4 Learning an unknown sum of Bernoullis from poly(1/\u01eb) samples", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Learning with respect to Kolmogorov distance", "text": "Make X any random variable based on n-tv appear as optimal., n) We write FX and fX, respectively, to determine the cumulative distribution and probability density of X.Let Z1,., n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv, n-tv."}, {"heading": "4.2 From Kolmogorov distance to total variation distance", "text": "The algorithms of the previous subsection learn the target sum of the Bernoullis in relation to the total number of Bernoullis in relation to the total number of Bernoullis."}, {"heading": "4.2.1 Handling sparse form distributions in the cover.", "text": "The following two assertions (whose proofs we move in Section 4.3) state that if dTV (X, Y) is large, the value of Y can be calculated directly in time poly (1 / 2), the value of Y can be calculated directly in time poly (1 / 2), and the following two assertions (whose proofs we move in Section 4.3) state that if dTV (X, Y) is large, the value of Y is calculated directly in time poly (1 / 3) that Y."}, {"heading": "4.2.2 Handling Binomial form distributions in the cover.", "text": "We can use the \"test\" for the sparse points in the coverage, but it may be that the target detection X = \u03b2 \u03b2 = \u03b2 = \u03b2 \u03b2-neighbor in the coverage does not accept a sparse point in the coverage. We must develop a method that similarly filters the points of the heavy binomial form in the coverage, so that we do not eliminate a sparse \"point\" in the coverage, while at the same time not allowing a sparse \"point\" in the coverage. Since X does not have a sparse \"neighbor\" in the coverage, it follows from theorem 9 that there is a collection \"X,\" so that there is a collection \"X\" i \"in k\" (sparse) -heavy binomial form, so that Xi is \"i\" within the entire variation distance. \"We show that the total variation distance is essentially within a constant factor of Kolmogorov distances for two collections of random variables in the heavy binomial form."}, {"heading": "4.2.3 Finishing the Proof of Theorem 10", "text": "We can assume that \"c\" is the constant hidden in the O (\u00b7) notation in the statement of \"\u03b2\" 17. We can assume that \"c\" is smaller than any fixed constant, and therefore that it fulfills \"i\" and \"c\" equally. We will now describe the algorithm promised in \"Theorem 10.\" The algorithm takes as its input the estimate \"F\" X \"(which, as described at the beginning of the evidence, can be obtained from the samples of\" X \"by means of\" Theorem 7. \"Algorithm 1. Calculate the lid\" S \"defined in\" Theorem 9.2. \"If any Y\" S \"\u03b2\" in the sparse form passes the \"test, there is such a\" Y \"and\" Halt. \"Otherwise, any Y\" S \"\u03b2 in the k\" heavy binomical form consists of the \"\u03b2\" -heavy \"-binomical\" test, \"so that the lid\" of the \"test\" and \"lid\" of the \"theory\" follows \"X.\""}, {"heading": "4.3 Proof of Claims 11 and 12", "text": "Remember that Y (z) + Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z \u00b7 Z \u00b7 Z (z) \u00b7 Z) \u00b7 Z (z) (z) \u00b7 Z) (z) (Z) (Z) (Z) (Z) \u00b7 Z) \u00b7 Z (z) \u00b7 Z) (z) \u00b7 Z (z) \u00b7 Z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z) \u00b7 Z (z) (z) \u00b7 Z) \u00b7 Z (z) \u00b7 Z (z) (z) \u00b7 Z) \u00b7 Z (z) \u00b7 Z (z) (z) \u00b7 Z) (z) \u00b7 Z (z) \u00b7 Z (z) (z) \u00b7 Z (z) \u00b7 Z (z) (z) \u00b7 Z (z) (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z) (z) \u00b7 Z (z) \u00b7 Z (z) \u00b7 Z (z (z)."}, {"heading": "4.4 Proof of Lemma 15", "text": "Recall lemmas 15 Let X = # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "6 Conclusion and open problems", "text": "In fact, it is a purely problem that can be solved by a single algorithm: there is an algorithm that learns the sums of Bernoullis, that deduces the samples of Poly (1 / 2) and the runs of Poly (1 / 3)."}, {"heading": "A Proof of Observation 1.1: AND-gate functions", "text": "Let p = (p1,.., pn) be the unknown target vector of the probabilities. By using poly (n, 1 / 2) random samples of f (X), it is easy to obtain upper and lower limits 0 \u2264 Pi, \u2212 < Pi, + \u2264 1 in such a way that Pi, + \u2212 Pi, \u2212 \u2264 n and with a probability of at least 9 / 10 each i [m] Pi, \u2212 \u2264 Pi \u2264 Pi, +.For each i [m] we have that the function fi (x) is such that Pi = \u0445i-S ipi and hence the Log-i values are different."}, {"heading": "B Proof of Theorem 6: A tournament between distributions in a cover", "text": "Suppose there is such coverage for f (S) of size N = N (n, \u03b4). Then there is an algorithm that uses O (\u03b4 \u2212 2 log N) samples and solves the f -transformed product distribution problem to accuracy 6. (F) Proof: Let P (S) be the input distribution fed into circuit f. We will describe an algorithm that given m = O (Q \u2212 2 log N) independent samples s = {si} mi = 1 of f (P), finds a distribution Q \u00b2 s that dTV (f (P), f (Q) with a probability of at least 9 / 10.Recall that the high-level idea of proof is as follows."}, {"heading": "C Learning transformed product distributions can be computationally hard", "text": "so it is not as if there is a solution to the problem of transformed product distribution for f / 3.proof: Function f is quite simple. It lasts m = n. 2 + n bits of input (w, s) = (w, s) = (w 1,1,. w1,. w2,1,. w2, n.,.,. w2,. w2,. w2,. w. w2,. w2,. w2,. w2,. w2,. w2,. w2,. w.,. w2,. w.,. w. w. w. w. w. w. w. w. w. w. w. w. w. n. n. n. n. n. n. n., n. n. n., n. n., n. n. n., n. n. n., n. n., n. n. n., n. n. n., n. n. n., n. n. n.. n..., w. w., w. w., w. w. w., w. w., w. n............., n..............., n...."}, {"heading": "E A Simple Proof of the DKW Inequality", "text": "Recall the framework of the DKW inequality: X is any variable set to {0, 1,., n} (Z1,.., Zk are independent copies of X, and Z () i is defined as 1Zi \u2264, for all 0,.., n and i = (1 / 2) (1 / 2) (1 / 2) (1 / 8) (1 / 2 / 2) (1 /. \u2212 Z (0 \u2212 Z (1 /) \u2212 Z (1 /) \u2212 \u2212) \u2212 \u2212 \u2212) \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 Z (()) \u2212 \u2212 \u2212 (). \u2212 \u2212) (1 / n) (1 / n) \u2212 \u2212) (). \u2212 Z (1 / n) \u2212 n ()."}], "references": [{"title": "Nearly tight bounds on the learnability of evolution", "author": ["Andris Ambainis", "Richard Desper", "Martin Farach", "Sampath Kannan"], "venue": "In FOCS,", "citeRegEx": "Ambainis et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Ambainis et al\\.", "year": 1997}, {"title": "Estimating a density under order restrictions: Nonasymptotic minimax risk", "author": ["L. Birg\u00e9"], "venue": "Annals of Statistics,", "citeRegEx": "Birg\u00e9.,? \\Q1987\\E", "shortCiteRegEx": "Birg\u00e9.", "year": 1987}, {"title": "Estimation of unimodal densities without smoothness assumptions", "author": ["L. Birg\u00e9"], "venue": "Annals of Statistics,", "citeRegEx": "Birg\u00e9.,? \\Q1997\\E", "shortCiteRegEx": "Birg\u00e9.", "year": 1997}, {"title": "Evolutionary trees can be learned in polynomial time in the two state general Markov model", "author": ["M. Cryan", "L. Goldberg", "P. Goldberg"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Cryan et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Cryan et al\\.", "year": 2002}, {"title": "An efficient ptas for two-strategy anonymous games", "author": ["Constantinos Daskalakis"], "venue": "WINE 2008; full version in CoRR,", "citeRegEx": "Daskalakis.,? \\Q2008\\E", "shortCiteRegEx": "Daskalakis.", "year": 2008}, {"title": "On oblivious ptas\u2019s for nash equilibrium", "author": ["Constantinos Daskalakis", "Christos H. Papadimitriou"], "venue": "STOC 2009; full version in,", "citeRegEx": "Daskalakis and Papadimitriou.,? \\Q2011\\E", "shortCiteRegEx": "Daskalakis and Papadimitriou.", "year": 2011}, {"title": "A universally acceptable smoothing factor for kernel density estimation", "author": ["L. Devroye", "G. Lugosi"], "venue": "Annals of Statistics,", "citeRegEx": "Devroye and Lugosi.,? \\Q1996\\E", "shortCiteRegEx": "Devroye and Lugosi.", "year": 1996}, {"title": "Nonasymptotic universal smoothing factors, kernel complexity and Yatracos classes", "author": ["L. Devroye", "G. Lugosi"], "venue": "Annals of Statistics,", "citeRegEx": "Devroye and Lugosi.,? \\Q1996\\E", "shortCiteRegEx": "Devroye and Lugosi.", "year": 1996}, {"title": "Combinatorial methods in density estimation", "author": ["L. Devroye", "G. Lugosi"], "venue": null, "citeRegEx": "Devroye and Lugosi.,? \\Q2001\\E", "shortCiteRegEx": "Devroye and Lugosi.", "year": 2001}, {"title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator", "author": ["A. Dvoretzky", "J. Kiefer", "J. Wolfowitz"], "venue": "Ann. Mathematical Statistics,", "citeRegEx": "Dvoretzky et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Dvoretzky et al\\.", "year": 1956}, {"title": "Efficient algorithms for inverting evolution", "author": ["Martin Farach", "Sampath Kannan"], "venue": "J. ACM,", "citeRegEx": "Farach and Kannan.,? \\Q1999\\E", "shortCiteRegEx": "Farach and Kannan.", "year": 1999}, {"title": "Learning mixtures of product distributions over discrete domains", "author": ["Jon Feldman", "Ryan O\u2019Donnell", "Rocco A. Servedio"], "venue": "SIAM J. Comput.,", "citeRegEx": "Feldman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2008}, {"title": "Estimating a mixture of two product distributions", "author": ["Y. Freund", "Y. Mansour"], "venue": "In Proceedings of the Twelfth Annual Conference on Computational Learning Theory,", "citeRegEx": "Freund and Mansour.,? \\Q1999\\E", "shortCiteRegEx": "Freund and Mansour.", "year": 1999}, {"title": "Polya. Inequalities", "author": ["G.H. Hardy", "J.E. Littlewood"], "venue": null, "citeRegEx": "Hardy et al\\.,? \\Q1934\\E", "shortCiteRegEx": "Hardy et al\\.", "year": 1934}, {"title": "On the learnability of discrete distributions", "author": ["M. Kearns", "Y. Mansour", "D. Ron", "R. Rubinfeld", "R. Schapire", "L. Sellie"], "venue": "In Proceedings of the 26th Symposium on Theory of Computing,", "citeRegEx": "Kearns et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1994}, {"title": "Some Results for Discrete Unimodality", "author": ["J. Keilson", "H. Gerber"], "venue": "J. American Statistical Association,", "citeRegEx": "Keilson and Gerber.,? \\Q1971\\E", "shortCiteRegEx": "Keilson and Gerber.", "year": 1971}, {"title": "The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality", "author": ["P. Massart"], "venue": "Annals of Probability,", "citeRegEx": "Massart.,? \\Q1990\\E", "shortCiteRegEx": "Massart.", "year": 1990}, {"title": "Evaluation may be easier than generation", "author": ["M. Naor"], "venue": "In Proceedings of the 28th Symposium on Theory of Computing (STOC),", "citeRegEx": "Naor.,? \\Q1996\\E", "shortCiteRegEx": "Naor.", "year": 1996}, {"title": "The unreasonable effectiveness of martingales", "author": ["Y. Peres"], "venue": "Plenary talk at SODA,", "citeRegEx": "Peres.,? \\Q2009\\E", "shortCiteRegEx": "Peres.", "year": 2009}, {"title": "Translated Poisson Approximation Using Exchangeable Pair Couplings", "author": ["A. R\u00f6llin"], "venue": "ArXiV Report,", "citeRegEx": "R\u00f6llin.,? \\Q2006\\E", "shortCiteRegEx": "R\u00f6llin.", "year": 2006}, {"title": "Rates of convergence of minimum distance estimators and Kolmogorov\u2019s entropy", "author": ["Y.G. Yatracos"], "venue": "Annals of Statistics,", "citeRegEx": "Yatracos.,? \\Q1985\\E", "shortCiteRegEx": "Yatracos.", "year": 1985}], "referenceMentions": [{"referenceID": 8, "context": "As a second example, we point out that the transformed product distribution learning model is broad enough to encompass the problem of learning an unknown mixture of k product distributions over {0, 1}n that was considered by Freund and Mansour (1999), Cryan et al.", "startOffset": 226, "endOffset": 252}, {"referenceID": 2, "context": "As a second example, we point out that the transformed product distribution learning model is broad enough to encompass the problem of learning an unknown mixture of k product distributions over {0, 1}n that was considered by Freund and Mansour (1999), Cryan et al. (2002), Feldman et al.", "startOffset": 253, "endOffset": 273}, {"referenceID": 2, "context": "As a second example, we point out that the transformed product distribution learning model is broad enough to encompass the problem of learning an unknown mixture of k product distributions over {0, 1}n that was considered by Freund and Mansour (1999), Cryan et al. (2002), Feldman et al. (2008). For simplicity we describe the case k = 2: there are unknown product distributions p, q over {0, 1}n and unknown mixing weights \u03c0p, \u03c0q = 1 \u2212 \u03c0p.", "startOffset": 253, "endOffset": 296}, {"referenceID": 2, "context": "As a second example, we point out that the transformed product distribution learning model is broad enough to encompass the problem of learning an unknown mixture of k product distributions over {0, 1}n that was considered by Freund and Mansour (1999), Cryan et al. (2002), Feldman et al. (2008). For simplicity we describe the case k = 2: there are unknown product distributions p, q over {0, 1}n and unknown mixing weights \u03c0p, \u03c0q = 1 \u2212 \u03c0p. The learner is given independent draws from the mixture distribution (each draw is independently taken from p with probability \u03c0p and from q with probability \u03c0q), and must output hypothesis product distributions p\u0302, q\u0302 and hypothesis mixing weights \u03c0\u0302p, \u03c0\u0302q. This problem is easily seen to be equivalent to the transformed product distribution learning problem for the function f : {0, 1}2n+1 \u2192 {0, 1}n which is such that on input (z, x1, . . . , xn, y1, . . . , yn) \u2208 {0, 1}2n+1 the i-th bit of f \u2019s output is zxi + (1 \u2212 z)yi. It is easy to see that if the target vector of probabilities for f is (\u03c0p, p1, . . . , pn, q1, . . .qn) then samples of f are distributed exactly according to the mixture of product distributions, and finding a good hypothesis vector in [0, 1]2n+1 amounts to finding a hypothesis mixing weight \u03c0\u0302p and hypothesis product distributions p\u0302, q\u0302 as required in the original \u201clearning mixtures of product distributions\u201d problem. Connection to prior work: The transformed product distribution learning model is related to the PACstyle model of learning discrete probability distributions that was introduced by Kearns et al. (1994) and studied in several subsequent works of Naor (1996), Ambainis et al.", "startOffset": 253, "endOffset": 1596}, {"referenceID": 2, "context": "As a second example, we point out that the transformed product distribution learning model is broad enough to encompass the problem of learning an unknown mixture of k product distributions over {0, 1}n that was considered by Freund and Mansour (1999), Cryan et al. (2002), Feldman et al. (2008). For simplicity we describe the case k = 2: there are unknown product distributions p, q over {0, 1}n and unknown mixing weights \u03c0p, \u03c0q = 1 \u2212 \u03c0p. The learner is given independent draws from the mixture distribution (each draw is independently taken from p with probability \u03c0p and from q with probability \u03c0q), and must output hypothesis product distributions p\u0302, q\u0302 and hypothesis mixing weights \u03c0\u0302p, \u03c0\u0302q. This problem is easily seen to be equivalent to the transformed product distribution learning problem for the function f : {0, 1}2n+1 \u2192 {0, 1}n which is such that on input (z, x1, . . . , xn, y1, . . . , yn) \u2208 {0, 1}2n+1 the i-th bit of f \u2019s output is zxi + (1 \u2212 z)yi. It is easy to see that if the target vector of probabilities for f is (\u03c0p, p1, . . . , pn, q1, . . .qn) then samples of f are distributed exactly according to the mixture of product distributions, and finding a good hypothesis vector in [0, 1]2n+1 amounts to finding a hypothesis mixing weight \u03c0\u0302p and hypothesis product distributions p\u0302, q\u0302 as required in the original \u201clearning mixtures of product distributions\u201d problem. Connection to prior work: The transformed product distribution learning model is related to the PACstyle model of learning discrete probability distributions that was introduced by Kearns et al. (1994) and studied in several subsequent works of Naor (1996), Ambainis et al.", "startOffset": 253, "endOffset": 1651}, {"referenceID": 0, "context": "(1994) and studied in several subsequent works of Naor (1996), Ambainis et al. (1997), Farach and Kannan (1999), Freund and Mansour (1999), Cryan et al.", "startOffset": 63, "endOffset": 86}, {"referenceID": 0, "context": "(1994) and studied in several subsequent works of Naor (1996), Ambainis et al. (1997), Farach and Kannan (1999), Freund and Mansour (1999), Cryan et al.", "startOffset": 63, "endOffset": 112}, {"referenceID": 0, "context": "(1994) and studied in several subsequent works of Naor (1996), Ambainis et al. (1997), Farach and Kannan (1999), Freund and Mansour (1999), Cryan et al.", "startOffset": 63, "endOffset": 139}, {"referenceID": 0, "context": "(1994) and studied in several subsequent works of Naor (1996), Ambainis et al. (1997), Farach and Kannan (1999), Freund and Mansour (1999), Cryan et al. (2002), Feldman et al.", "startOffset": 63, "endOffset": 160}, {"referenceID": 0, "context": "(1994) and studied in several subsequent works of Naor (1996), Ambainis et al. (1997), Farach and Kannan (1999), Freund and Mansour (1999), Cryan et al. (2002), Feldman et al. (2008). In the Kearns et al.", "startOffset": 63, "endOffset": 183}, {"referenceID": 0, "context": "(1994) and studied in several subsequent works of Naor (1996), Ambainis et al. (1997), Farach and Kannan (1999), Freund and Mansour (1999), Cryan et al. (2002), Feldman et al. (2008). In the Kearns et al. (1994) framework a learning problem is defined by a class C of Boolean circuits, and an instance of the problem corresponds to the choice of a specific (unknown to the learner) target circuit C \u2208 C.", "startOffset": 63, "endOffset": 212}, {"referenceID": 0, "context": "(1994) and studied in several subsequent works of Naor (1996), Ambainis et al. (1997), Farach and Kannan (1999), Freund and Mansour (1999), Cryan et al. (2002), Feldman et al. (2008). In the Kearns et al. (1994) framework a learning problem is defined by a class C of Boolean circuits, and an instance of the problem corresponds to the choice of a specific (unknown to the learner) target circuit C \u2208 C. The learner is given samples from C(X) where X is a uniform random string from {0, 1}m, and the learner must with high probability output a hypothesis circuit C such that the random variable C(X) is \u01eb-close to C(X) (in KL-divergence). Strictly speaking the transformed product distribution learning model may be viewed as a special case of the Kearns et al model. This is done by considering a circuit class C that has a circuit C = Cp for each possible product distribution p over {0, 1}n; the circuit Cp first transforms the uniform distribution over {0, 1}m to the product distribution p over {0, 1}n and then applies the transformation function f to the output of p. However, learning problems C of this sort do not seem to have been previously considered in the Kearns et al. (1994) model, and we feel it is more natural to view our model as dual in spirit to the earlier model.", "startOffset": 63, "endOffset": 1192}, {"referenceID": 0, "context": "(1994) and studied in several subsequent works of Naor (1996), Ambainis et al. (1997), Farach and Kannan (1999), Freund and Mansour (1999), Cryan et al. (2002), Feldman et al. (2008). In the Kearns et al. (1994) framework a learning problem is defined by a class C of Boolean circuits, and an instance of the problem corresponds to the choice of a specific (unknown to the learner) target circuit C \u2208 C. The learner is given samples from C(X) where X is a uniform random string from {0, 1}m, and the learner must with high probability output a hypothesis circuit C such that the random variable C(X) is \u01eb-close to C(X) (in KL-divergence). Strictly speaking the transformed product distribution learning model may be viewed as a special case of the Kearns et al model. This is done by considering a circuit class C that has a circuit C = Cp for each possible product distribution p over {0, 1}n; the circuit Cp first transforms the uniform distribution over {0, 1}m to the product distribution p over {0, 1}n and then applies the transformation function f to the output of p. However, learning problems C of this sort do not seem to have been previously considered in the Kearns et al. (1994) model, and we feel it is more natural to view our model as dual in spirit to the earlier model. In Kearns et al. (1994) the learner\u2019s task is to infer an unknown transformation (the circuit C) into which are fed n-bit strings that are known to be distributed uniformly.", "startOffset": 63, "endOffset": 1312}, {"referenceID": 6, "context": "By an approach similar to the algorithm of Devroye and Lugosi (2001) for choosing a density estimate, we show (Theorem 6) that if the space { f (p)}p\u2208[0,1]n of all f -transformed product distributions has an \u01eb-cover of size N, then there is a generic learning algorithm for the f -transformed product distribution problem that uses O((log N)/\u01eb2) samples.", "startOffset": 43, "endOffset": 69}, {"referenceID": 6, "context": "While the algorithm itself is simple, its analysis relies on a fundamental result from probability theory, known as the Dvoretzky-Kiefer-Wolfowitz inequality (Dvoretzky et al. (1956)), which may be viewed as a special case of the fundamental Vapnik-Chervonenkis theorem (see Chapter 3 of Devroye and Lugosi (2001)).", "startOffset": 159, "endOffset": 183}, {"referenceID": 6, "context": "(1956)), which may be viewed as a special case of the fundamental Vapnik-Chervonenkis theorem (see Chapter 3 of Devroye and Lugosi (2001)).", "startOffset": 112, "endOffset": 138}, {"referenceID": 6, "context": "(1956)), which may be viewed as a special case of the fundamental Vapnik-Chervonenkis theorem (see Chapter 3 of Devroye and Lugosi (2001)). In Appendix E we give a self-contained proof of the DKW inequality using elementary techniques (martingales and the method of bounded differences) and an interesting trick that goes back to Kolmogorov (see Peres (2009)); this proof is significantly different from the proofs we know of in the probability literature (see Dvoretzky et al.", "startOffset": 112, "endOffset": 359}, {"referenceID": 6, "context": "(1956)), which may be viewed as a special case of the fundamental Vapnik-Chervonenkis theorem (see Chapter 3 of Devroye and Lugosi (2001)). In Appendix E we give a self-contained proof of the DKW inequality using elementary techniques (martingales and the method of bounded differences) and an interesting trick that goes back to Kolmogorov (see Peres (2009)); this proof is significantly different from the proofs we know of in the probability literature (see Dvoretzky et al. (1956), Massart (1990), and Chapter 3 of Devroye and Lugosi (2001)).", "startOffset": 112, "endOffset": 485}, {"referenceID": 6, "context": "(1956)), which may be viewed as a special case of the fundamental Vapnik-Chervonenkis theorem (see Chapter 3 of Devroye and Lugosi (2001)). In Appendix E we give a self-contained proof of the DKW inequality using elementary techniques (martingales and the method of bounded differences) and an interesting trick that goes back to Kolmogorov (see Peres (2009)); this proof is significantly different from the proofs we know of in the probability literature (see Dvoretzky et al. (1956), Massart (1990), and Chapter 3 of Devroye and Lugosi (2001)).", "startOffset": 112, "endOffset": 501}, {"referenceID": 6, "context": "(1956)), which may be viewed as a special case of the fundamental Vapnik-Chervonenkis theorem (see Chapter 3 of Devroye and Lugosi (2001)). In Appendix E we give a self-contained proof of the DKW inequality using elementary techniques (martingales and the method of bounded differences) and an interesting trick that goes back to Kolmogorov (see Peres (2009)); this proof is significantly different from the proofs we know of in the probability literature (see Dvoretzky et al. (1956), Massart (1990), and Chapter 3 of Devroye and Lugosi (2001)).", "startOffset": 112, "endOffset": 545}, {"referenceID": 11, "context": "Hardy et al. (1934) section 2.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Nevertheless, we extend the Kolmogorov distance learning algorithm to total variation distance via a delicate algorithm that exploits the detailed structure of a small \u01eb-cover (Daskalakis and Papadimitriou (2011), Daskalakis (2008)) of the space of all distributions that are sums of independent Bernoulli random variables (see Theorem 9).", "startOffset": 177, "endOffset": 213}, {"referenceID": 4, "context": "Nevertheless, we extend the Kolmogorov distance learning algorithm to total variation distance via a delicate algorithm that exploits the detailed structure of a small \u01eb-cover (Daskalakis and Papadimitriou (2011), Daskalakis (2008)) of the space of all distributions that are sums of independent Bernoulli random variables (see Theorem 9).", "startOffset": 177, "endOffset": 232}, {"referenceID": 4, "context": "Nevertheless, we extend the Kolmogorov distance learning algorithm to total variation distance via a delicate algorithm that exploits the detailed structure of a small \u01eb-cover (Daskalakis and Papadimitriou (2011), Daskalakis (2008)) of the space of all distributions that are sums of independent Bernoulli random variables (see Theorem 9). Interestingly, this becomes feasible by establishing an analog of the aforementioned argument by Newton to a class of distributions used in the cover that are called heavy (see Lemma 15). This in turn relies on probabilistic approximation results via translated Poisson distributions (see Definition 18, R\u00f6llin (2006)).", "startOffset": 177, "endOffset": 658}, {"referenceID": 1, "context": "This lets us apply a powerful algorithm due to Birg\u00e9 (1997) that can learn any unimodal distribution to accuracy \u01eb using O(log n)/\u01eb3 samples.", "startOffset": 47, "endOffset": 60}, {"referenceID": 6, "context": "\u201d) This basic approach of running a tournament between distributions in an \u03b4-cover is quite similar to the algorithm of Devroye and Lugosi for choosing a density estimate (see Devroye and Lugosi (1996a,b) and Chapters 6 and 7 of Devroye and Lugosi (2001)), which in turn built closely on the work of Yatracos (1985).", "startOffset": 120, "endOffset": 255}, {"referenceID": 6, "context": "\u201d) This basic approach of running a tournament between distributions in an \u03b4-cover is quite similar to the algorithm of Devroye and Lugosi for choosing a density estimate (see Devroye and Lugosi (1996a,b) and Chapters 6 and 7 of Devroye and Lugosi (2001)), which in turn built closely on the work of Yatracos (1985). Our algorithm achieves essentially the same bounds as these earlier approaches but there are some small differences.", "startOffset": 120, "endOffset": 316}, {"referenceID": 9, "context": "The Dvoretzky-Kiefer-Wolfowitz inequality (Dvoretzky et al. (1956), Massart (1990)) confirms this, and in fact shows that a surprisingly small value of k \u2013 independent of n \u2013 suffices.", "startOffset": 43, "endOffset": 67}, {"referenceID": 9, "context": "The Dvoretzky-Kiefer-Wolfowitz inequality (Dvoretzky et al. (1956), Massart (1990)) confirms this, and in fact shows that a surprisingly small value of k \u2013 independent of n \u2013 suffices.", "startOffset": 43, "endOffset": 83}, {"referenceID": 18, "context": "In Appendix E we give a self-contained proof of the theorem using elementary techniques (martingales and the method of bounded differences) and an interesting trick that goes back to Kolmogorov (see Peres (2009)).", "startOffset": 199, "endOffset": 212}, {"referenceID": 4, "context": "The following is Theorem 9 of the full version of (Daskalakis and Papadimitriou (2011)):", "startOffset": 51, "endOffset": 87}, {"referenceID": 4, "context": "(We remark that Daskalakis (2008) establishes the same theorem, except that the size of the cover given there, as well as the time needed to produce it, are n3 \u00b7O(1/\u01eb)+n \u00b7 ( 1 \u01eb )O(1/\u01eb2) .", "startOffset": 16, "endOffset": 34}, {"referenceID": 19, "context": "Definition 18 (R\u00f6llin (2006)) We say that an integer random variable Y has a translated Poisson distribution with paremeters \u03bc and \u03c32 and write L(Y) = T P(\u03bc, \u03c32) if L(Y \u2212 \u230a\u03bc \u2212 \u03c32\u230b) = Poisson(\u03c32 + {\u03bc \u2212 \u03c32}), where {\u03bc \u2212 \u03c32} represents the fractional part of \u03bc \u2212 \u03c32.", "startOffset": 15, "endOffset": 29}, {"referenceID": 4, "context": "Given the above, and following Daskalakis (2008) (see Section 6.", "startOffset": 31, "endOffset": 49}], "year": 2011, "abstractText": "We consider the problem of learning an unknown product distribution X over {0, 1}n using samples f (X) where f is a known transformation function. Each choice of a transformation function f specifies a learning problem in this framework. Information-theoretic arguments show that for every transformation function f the corresponding learning problem can be solved to accuracy \u01eb, using \u00d5(n/\u01eb2) examples, by a generic algorithm whose running time may be exponential in n. We show that this learning problem can be computationally intractable even for constant \u01eb and rather simple transformation functions. Moreover, the above sample complexity bound is nearly optimal for the general problem, as we give a simple explicit linear transformation function f (x) = w \u00b7 x with integer weights wi \u2264 n and prove that the corresponding learning problem requires \u03a9(n) samples. As our main positive result we give a highly efficient algorithm for learning a sum of independent unknown Bernoulli random variables, corresponding to the transformation function f (x) = \u2211n i=1 xi. Our algorithm learns to \u01eb-accuracy in poly(n) time, using a surprising poly(1/\u01eb) number of samples that is independent of n. We also give an efficient algorithm that uses log n \u00b7 poly(1/\u01eb) samples but has running time that is only poly(log n, 1/\u01eb).", "creator": "LaTeX with hyperref package"}}}