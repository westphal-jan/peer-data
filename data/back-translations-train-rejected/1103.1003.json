{"id": "1103.1003", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2011", "title": "Teraflop-scale Incremental Machine Learning", "abstract": "We propose a long-term memory design for artificial general intelligence based on Solomonoff's incremental machine learning methods. We use R5RS Scheme and its standard library with a few omissions as the reference machine. We introduce a Levin Search variant based on Stochastic Context Free Grammar together with four synergistic update algorithms that use the same grammar as a guiding probability distribution of programs. The update algorithms include adjusting production probabilities, re-using previous solutions, learning programming idioms and discovery of frequent subprograms. Experiments with two training sequences demonstrate that our approach to incremental learning is effective.", "histories": [["v1", "Sat, 5 Mar 2011 03:41:30 GMT  (35kb,S)", "http://arxiv.org/abs/1103.1003v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eray \\\"ozkural"], "accepted": false, "id": "1103.1003"}, "pdf": {"name": "1103.1003.pdf", "metadata": {"source": "CRF", "title": "Teraflop-scale Incremental Machine Learning", "authors": ["Eray \u00d6zkural"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 110 3.10 03v1 [cs.AI] 5M ar2 01"}, {"heading": "1 Introduction", "text": "The field of Artificial General Intelligence (AGI) has received considerable attention from researchers over the past decade as computing capacity moves toward human scale. Many promising theoretical proposals have been made [1,2,3] and practical general-purpose programs have been demonstrated (for example [4,5]). We understand the requirements of an AGI system much better than before, so we believe it is now time to start building a full AGI system. Therefore, we set out early to identify and overcome the experimental challenges. Teramachine is our prototype for implementing an AGI system in the O'Caml language, as a candidate for Solomonoff's \"Phase 1 Machine,\" which he proposed as the basis for a powerful AGI system [6]."}, {"heading": "2 Incremental learning: Heuristic Algorithmic Memory", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "3 Scheme as the reference machine", "text": "For a general learning system, we need a general programming system that can handle a wide variety of data structures and make it possible to write complex programs of any kind. Whereas FORTH produces rather impressive results, we chose R5RS because it is a simple but general programming language. R5RS contains a reasonably large standard library that does not have major handicaps compared to Solomonoff and whose implementations are necessary to have a proper fluctuation recursion."}, {"heading": "4 Using a stochastic CFG in Levin Search", "text": "In many AGI systems, a variant or extension of Levin Search [10] is used to find solutions. Solomonoff's incremental machine learning also uses Levin Search as a basic search algorithm to find solutions [1]. In our system, we use the stochastic grammatical guidance function for the mass of probability (pmf) for the search process as well. A stochastic CFG is a CFG that is extended by a probability value for each production. For each head, the probabilities of the productions of this head must add up to one, obvious. We can expand our Levin search procedure to work with a stochastic CFG that assigns probabilities to each sentence in the language. To do this, we need two things: first, a generational logic for individual sentences, and second, a search strategy to enumerate the sentences that correspond to the termination condition of LSearch [4]."}, {"heading": "5 Stochastic CFG updates", "text": "The most critical part of our design is updating the stochastic CFG so that the solutions found in a training sequence are more likely to be found in the search for follow-up problems. We propose four synergistic update algorithms for HAM. Our SCFG structure expands the usual productions with production processes that generate productions dynamically."}, {"heading": "5.1 Modifying production probabilities", "text": "The simplest way of updating is to modify the probabilities, because new solutions are added to the solution corpus. However, for this, the search algorithm must provide the derivative that led to the solution (which we do), or the solution must be analyzed with the same grammar. Then, the probability for each production A \u2192 \u03b2 in the solution corpus can easily be determined by the ratio of the frequency of productions A \u2192 \u03b2 in the solution corpus to the frequency of productions in the corpus with a head of A. Production processes are of course excluded from updating, as they may be variable. However, we cannot simply write the probabilities calculated in this way about the original probabilities, since there will be few solutions at first, and most probabilities will be zero. We use exponential smoothing to solve this problem."}, {"heading": "5.2 Re-using previous solutions", "text": "In the course of a training sequence, the solutions can be fully absorbed by adding the solutions to the grammar. In the case of Scheme, there could be many possible implementations. The simplest design is to add all the solutions to the library of the Scheme interpreter, add a non-terminal previous solution to the grammar, and then expand the previous solution with the syntax to call the new solution. We assume that this syntax is provided in the problem definition. We add new solutions as follows, the new solution among other previous solutions receives a probability of \u03b3 in the hope that this solution will soon be used again, and then the probabilities of the old productions of the previous solution are normalized, so that they result in a sum of 0.5. If it is impossible or difficult to add the solutions to the Scheme interpreter as in our case, then all the solutions can be added to the definition of blocks."}, {"heading": "5.3 Learning programming idioms", "text": "Programmers not only learn concrete solutions to problems, but also learn abstract programs or program schemes. One way to formalize this is by learning sentence shapes. If we can extract suitable sentence shapes, we can add them to the grammar. We construct the derivative tree from the most left-hand derivative, with an obvious algorithm that we will omit. The current abstraction algorithm starts with the derivative subtrees that are rooted in the current solution for each expression. For each derivative subtree, we cut the leaves from bottom to top. At each pruning step, an abstract expression is output. The pruning algorithm works as follows: the tree [node <: S: > [node <: B: > [leaf bb]] [node <: A: > leaf a: > [leaf < leaf a:] [node <: <::: B: [leaf a: < level: < a: b] is a] level."}, {"heading": "5.4 Frequent sub-program mining", "text": "Frequent sub-programs in the detachment corpus, i.e. sub-programs that occur with a frequency above a certain support threshold, can be added as alternative productions to the frequently occurring non-terminal expression in the Scheme grammar. If, for example, the detachment corpus contains several (lambda (x y) (* x y) sub-programs, the frequent sub-program removal would detect this and we can add it as an alternative expression to the Scheme grammar. We would like to find all frequent sub-programs that occur twice or more often, so that we can increase the probability of such sub-tree removal programs accordingly. First, we interpret the problem of finding frequent sub-tree removal programs as a syntactical problem ignoring semantic equivalences between sub-tree removal programs."}, {"heading": "6 Experiments", "text": "In fact, it is the case that we will be able to play by the rules that applied in the past."}, {"heading": "7 Conclusion", "text": "We have described in detail a stochastic CFG-based incremental machine learning system. We have presented our realization of Solomonoff's Phase 1 machine and our heuristic algorithmic memory (HAM). We have adapted the R5RS scheme as a universal reference computer to our system. SCFG is used in parallel in LSearch to calculate a priori probabilities and efficiently generate programs to avoid syntactically faulty programs. We derive sentences that are derived furthest left. We use parallel DFS algorithms to enumerate candidate programs. We have specialized in productions for number literature, variable bindings and variable references. We have proposed four update algorithms for incremental machine learning, all of which have been implemented and proved to be quite efficient. The effectiveness of our update elogics has been demonstrated by experiments in a short and a long training sequence, yet to be fully demonstrated."}], "references": [{"title": "A system for incremental learning based on algorithmic probability", "author": ["R.J. Solomonoff"], "venue": "Proceedings of the Sixth Israeli Conference on Artificial Intelligence, Tel Aviv, Israel", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "The fastest and shortest algorithm for all well-defined problems", "author": ["M. Hutter"], "venue": "International Journal of Foundations of Computer Science 13(3)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Ultimate cognition \u00e0 la G\u00f6del", "author": ["J. Schmidhuber"], "venue": "Cognitive Computation 1(2)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimal ordered problem solver", "author": ["J. Schmidhuber"], "venue": "Machine Learning 54", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Clustering by compression", "author": ["P.V.R. Cilibrasi"], "venue": "Technical report, CWI", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Progress in incremental machine learning", "author": ["R.J. Solomonoff"], "venue": "NIPS Workshop on Universal Learning Algorithms and Optimal Search.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Algorithmic probability: Theory and applications", "author": ["R.J. Solomonoff"], "venue": "In Dehmer, M., Emmert-Streib, F., eds.: Information Theory and Statistical Learning, Springer Science+Business Media, N.Y.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Shifting inductive bias with successstory algorithm, adaptive levin search, and incremental self-improvement", "author": ["J. Schmidhuber", "J. Zhao", "M. Wiering"], "venue": "Machine Learning 28(1)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Revised5 report on the algorithmic language scheme", "author": ["Richard Kelsey", "J.R. William Clinger"], "venue": "Higher-Order and Symbolic Computation 11(1)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Universal sequential search problems", "author": ["L.A. Levin"], "venue": "Problems of Information Transmission 9(3)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1973}, {"title": "Introduction to Automata Theory, Languages, and Computation", "author": ["John E. Hopcroft", "J.U. Rajeev Motwani"], "venue": "Second edn. Addison Wesley", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Tagging english text with a probabilistic model", "author": ["B. Merialdo"], "venue": "Computational Linguistics 20", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "Algorithmic probability, heuristic programming and agi", "author": ["R.J. Solomonoff"], "venue": "Third Conference on Artificial General Intelligence.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Three kinds of probabilistic induction: Universal distributions and convergence theorems", "author": ["R.J. Solomonoff"], "venue": "The Computer Journal 51(5)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Universal algorithmic intelligence: A mathematical top\u2192down approach", "author": ["M. Hutter"], "venue": "In Goertzel, B., Pennachin, C., eds.: Artificial General Intelligence. Cognitive Technologies. Springer, Berlin", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Scalable estimation-of-distribution program evolution", "author": ["M. Looks"], "venue": "Proceedings of the 9th annual conference on Genetic and evolutionary computation.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Many promising theoretical proposals have been put forward [1,2,3] and practical general-purpose programs have been demonstrated (for instance [4,5]).", "startOffset": 59, "endOffset": 66}, {"referenceID": 1, "context": "Many promising theoretical proposals have been put forward [1,2,3] and practical general-purpose programs have been demonstrated (for instance [4,5]).", "startOffset": 59, "endOffset": 66}, {"referenceID": 2, "context": "Many promising theoretical proposals have been put forward [1,2,3] and practical general-purpose programs have been demonstrated (for instance [4,5]).", "startOffset": 59, "endOffset": 66}, {"referenceID": 3, "context": "Many promising theoretical proposals have been put forward [1,2,3] and practical general-purpose programs have been demonstrated (for instance [4,5]).", "startOffset": 143, "endOffset": 148}, {"referenceID": 4, "context": "Many promising theoretical proposals have been put forward [1,2,3] and practical general-purpose programs have been demonstrated (for instance [4,5]).", "startOffset": 143, "endOffset": 148}, {"referenceID": 5, "context": "Teramachine is our prototype implementation of an AGI system in the O\u2019Caml language, as a candidate for Solomonoff\u2019s \u201cPhase 1 machine\u201d that he proposed to use as the basis of a powerful AGI system [6].", "startOffset": 197, "endOffset": 200}, {"referenceID": 3, "context": "The reader is referred to [4,6,7] for a background on general-purpose incremental machine learning.", "startOffset": 26, "endOffset": 33}, {"referenceID": 5, "context": "The reader is referred to [4,6,7] for a background on general-purpose incremental machine learning.", "startOffset": 26, "endOffset": 33}, {"referenceID": 6, "context": "The reader is referred to [4,6,7] for a background on general-purpose incremental machine learning.", "startOffset": 26, "endOffset": 33}, {"referenceID": 3, "context": "The present system may also be viewed as an extension of OOPS [4], in similar vein to Adaptive Levin Search [8].", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "The present system may also be viewed as an extension of OOPS [4], in similar vein to Adaptive Levin Search [8].", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "The Conceptual Jump Size (CJS) = ti/pi of a solution si is roughly the time required to find a particular solution si that runs in ti time with a priori probability pi [6].", "startOffset": 168, "endOffset": 171}, {"referenceID": 3, "context": "While FORTH has yielded rather impressive results [4], we have chosen R5RS Scheme on the grounds that it is a simple yet general purpose high-level programming language.", "startOffset": 50, "endOffset": 53}, {"referenceID": 8, "context": "It is an improvement over LISP in that it is statically scoped and its implementations are required to have proper tail recursion; it is defined precisely in a standards document [9].", "startOffset": 179, "endOffset": 182}, {"referenceID": 6, "context": "We do not think that Scheme has any major handicaps compared to Solomonoff\u2019s AZ system [7].", "startOffset": 87, "endOffset": 90}, {"referenceID": 9, "context": "In many AGI systems, a variant or extension of Levin Search [10] is used for finding solutions.", "startOffset": 60, "endOffset": 64}, {"referenceID": 0, "context": "Solomonoff\u2019s incremental machine learning also uses Levin Search as the basic search algorithm to find solutions [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 3, "context": "For this, we need two things, first a generation logic for individual sentences, and second a search strategy to enumerate the sentences that meet the termination condition of LSearch [4].", "startOffset": 184, "endOffset": 187}, {"referenceID": 11, "context": "See [12] for the application of smoothing in a similar problem.", "startOffset": 4, "endOffset": 8}, {"referenceID": 5, "context": "Other methods like Laplace\u2019s rule may be used to avoid zero probabilities [6].", "startOffset": 74, "endOffset": 77}, {"referenceID": 3, "context": ", a program\u2019s probability is the product of the probabilities of the instructions it contains[4]), and the probability update in OOPS is dynamically caused by the bump instruction, the probability distribution is not stored long-term as in teramachine.", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "For each problem, we have a sequence of input and output pairs, and we approximate operator induction as described by Solomonoff [6,14].", "startOffset": 129, "endOffset": 135}, {"referenceID": 13, "context": "For each problem, we have a sequence of input and output pairs, and we approximate operator induction as described by Solomonoff [6,14].", "startOffset": 129, "endOffset": 135}, {"referenceID": 2, "context": "In the future, we plan to implement the Phase 2 of Solomonoff\u2019s Alpha system, and attempt integrating our AGI kernel to other AGI proposals such as the G\u00f6del Machine [3], AIXI [15], as well as adapting new program search algorithms such as HSearch [2], and MOSES[16].", "startOffset": 166, "endOffset": 169}, {"referenceID": 14, "context": "In the future, we plan to implement the Phase 2 of Solomonoff\u2019s Alpha system, and attempt integrating our AGI kernel to other AGI proposals such as the G\u00f6del Machine [3], AIXI [15], as well as adapting new program search algorithms such as HSearch [2], and MOSES[16].", "startOffset": 176, "endOffset": 180}, {"referenceID": 1, "context": "In the future, we plan to implement the Phase 2 of Solomonoff\u2019s Alpha system, and attempt integrating our AGI kernel to other AGI proposals such as the G\u00f6del Machine [3], AIXI [15], as well as adapting new program search algorithms such as HSearch [2], and MOSES[16].", "startOffset": 248, "endOffset": 251}, {"referenceID": 15, "context": "In the future, we plan to implement the Phase 2 of Solomonoff\u2019s Alpha system, and attempt integrating our AGI kernel to other AGI proposals such as the G\u00f6del Machine [3], AIXI [15], as well as adapting new program search algorithms such as HSearch [2], and MOSES[16].", "startOffset": 262, "endOffset": 266}], "year": 2011, "abstractText": "We propose a long-term memory design for artificial general intelligence based on Solomonoff\u2019s incremental machine learning methods. We use R5RS Scheme and its standard library with a few omissions as the reference machine. We introduce a Levin Search variant based on stochastic Context Free Grammar together with four synergistic update algorithms that use the same grammar as a guiding probability distribution of programs. The update algorithms include adjusting production probabilities, re-using previous solutions, learning programming idioms and discovery of frequent subprograms. Experiments with two training sequences demonstrate that our approach to incremental learning is ef-", "creator": "LaTeX with hyperref package"}}}