{"id": "1103.1013", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Mar-2011", "title": "A Feature Selection Method for Multivariate Performance Measures", "abstract": "Feature selection with specific multivariate performance measures is the key to the success of many applications such as information retrieval. In this paper, we propose a feature selection method for multivariate performance measures. The proposed method forms an optimization problem with exponential size of both feature groups and label configurations for a given dataset. To address this problem, a two-layer cutting plane algorithm is proposed. The outer layer performs group feature generation; while the inner layer learns the label configuration for multivariate performance measures. Comprehensive experiments on large-scale and high-dimensional real world datasets show that the proposed method can significantly outperform $l_1$-SVM and SVM-RFE when choosing a small subset of features, and achieve significantly improved performances over SVM$^{perf}$ in terms of $F_1$-score. It also learns a sparse yet effective decision rule for multivariate performance measures.", "histories": [["v1", "Sat, 5 Mar 2011 07:10:41 GMT  (73kb)", "https://arxiv.org/abs/1103.1013v1", null], ["v2", "Sat, 4 May 2013 14:48:06 GMT  (97kb)", "http://arxiv.org/abs/1103.1013v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qi mao", "ivor w tsang"], "accepted": false, "id": "1103.1013"}, "pdf": {"name": "1103.1013.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["IVOR W. TSANG"], "emails": ["QMAO1@ntu.edu.sg.", "IvorTsang@ntu.edu.sg."], "sections": [{"heading": null, "text": "ar Xiv: 110 3.10 13v2 [cs.LG] 4 May 2"}, {"heading": "1. INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2. SVM FOR MULTIVARIATE PERFORMANCE MEASURE", "text": "Considering a training sample of input-output pairs (xi, yi) and X-Y for i = 1,.., n, drawn from a fixed but unknown probability distribution with X-Rm and Y-Rm, the learning problem is treated as a multivariate predictive problem by defining the hypotheses h: X \u2192 Y, which represent a multiple x-X of n characteristic vectors x = (x1,.., xn) to a multiple y of n labels y = (y1,.., yn), where X = X \u00d7., X and Y xi (\u2212 1, + 1} n. The linear discrimination function of SVMperf is defined as (1) hw (x) = argmax y \"f\" (x, y \") = argmax y.\""}, {"heading": "3. GENERALIZED SPARSE REGULARIZER", "text": "In this paper, we focus on the minimization of regulated empirical loss as (3) = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "4. FEATURE SELECTION FOR MULTIVARIATE PERFORMANCE MEASURES", "text": "In order to optimize the multivariate loss functions and at the same time to learn a sparse representation of properties, we propose to solve the following convex problem via d and (w,) via p = 2, min d \u2212 D min w = 1 | wj | 2dj + Cements (5) s.t.: wT 1nn \u00b2 i = 1 (yi \u2212 y \u00b2) xi \u00b2 (y, y \u00b2) \u2212 \u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d"}, {"heading": "5. TWO-LAYER CUTTING PLANE ALGORITHM", "text": "In this section we propose a two-layer intersection algorithm to solve problem (6) efficiently and effectively, but the two levels, namely the generation of group attributes and the selection of group attributes, are described in sections 5.1 and 5.2. However, the two-layer intersection algorithm is described in sections 5.3 and 5.4.5.1. by naming S (\u03b1, d) = 12 x \u00b2 y \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s Problem d y \u00b2 s \u00b2 s \u00b2 s (Problem (6) it turns out that bemin d \u00b2 s \u00b2 s max \u00b2 s \u00b2 s \u00b2 s (\u03b1, d).Since the domains D and A \u00b2 s \u00b2 s \u00b2 s are not empty, the function S (\u03b1 \u00b2 s) is closed and convex \u00b2 s for all d \u00b2 s."}, {"heading": "6. RELATIONS TO EXISTING METHODS", "text": "In this section, we will discuss the relationships between our proposed method for multilateral loss (5) and related experiments (1). (1) However, it can be easily adapted to the general framework (4). (2) It can be easy to adapt to the general framework (4). (2) It is not that we have problems (5) in the particular case of B = 1, dj = 1, dj = 1, dj = 1, dj = 1, dj = 1, m \u00b2 in the real domain. (2) We observe that D = 1, if B = 1, we have problems (5) in the specific case of B = 1 up to the following equivalent optimization problems, min w = 1, wj = 1, wj = 2 + C2, s.t."}, {"heading": "7. MULTIPLE INSTANCE LEARNING FOR MULTIVARIATE PERFORMANCE MEASURES", "text": "We have expanded the proposed framework by optimizing multicultural performance benchmarks for feature selection in Section 4. In this section, we expand this approach to solving multiple content learning problems that have been used to solve a variety of learning problems, e.g. to optimize task-specific performance metrics, e.g. task definition [35], text categorization [20], but it is rarely optimized for multicultural performance metrics in literature. In this learning scenario, a pocket is represented by a set of instances in which each instance is represented by a feature vector, and the classification criterion for a learning method in image retrievalisms has been formally introduced."}, {"heading": "8. EXPERIMENTS", "text": "In this section, we conduct extensive experiments to evaluate the performance of our proposed method and the status-of-the-art selection methods: 1) SVM-RFE [9]; 2) l1SVM; 3) FGM [26]; 4) l1-bmrm-F11, which is l1 regulates SVM for optimizing F1 score1http: / / users.anu.edu.au / \"chteo / BMRM.htmlby bundle method [27]. SVM-RFE and FGM use Liblinear Software 2 as QP solvers for their SVM sub-problems. For l1-SVM, we also use Liblinear Software, which implements the state of the art l1-SVM algorithm [33]. In addition to comparing for 0-1 loss, we also perform experiments on F1 measurements."}, {"heading": "9. CONCLUSION", "text": "In this paper, we propose a generalized sparse regulator for feature selection and a unified framework for feature selection for general loss functions. To solve the resulting optimization problem, a two-layer cutting-plane algorithm was proposed; the convergence characteristics of the proposed algorithm are investigated; links to a variety of modern methods for feature selection are discussed in detail; a variety of analyses by comparison with the different methods for feature selection show that the proposed method is superior to others; experimental results show that the proposed method is comparable to FGM and SVM-RFE and better than l1 models in the task of feature selection and exceeds SVM in multivariate performance measures for full features."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported by Singapore A * star under Grant SERC 112 280 4005Appendices"}, {"heading": "A. PROOF OF PROPOSITION 1", "text": "Since the loss concept \"Y\" (y, \"y\") = 0 for all y \"Y\" = \"Dual\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem\" = \"Dual Problem.\""}, {"heading": "B. PROOF OF THEOREM 2", "text": "Considering the problem \u03b1 = max d \u2264 D Fd (\u03b1) or min \u03b1 \u0445 A, \u03b3: \u03b3 \u2265 Fd (\u03b1), \u0394d \u0445 D, (30) we have the equivalent optimization problem asmax \u03b1,.., dk} after k-iterations. In the kest iteration, the most violated constraint dk + 1 is found with respect to \u03b1k, so that Fdk + 1 (\u03b1k) = maxd = D-iteration after k-iterations usually yields a formal solution. Therefore, we can get two sequences {\u03b3k} and {\u03b3k} like the optimal k = max. theories k = maxd-iterations after k-iterations."}], "references": [{"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": "NIPS,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Optimization with sparsity-inducing penalties", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Foundations and Trends in Machine Learning, 4:1\u2013106,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Multiple kernel learning, conic duality, and the SMO algorithm", "author": ["F.R. Bach", "G.R.G. Lanckriet", "M.I. Jordan"], "venue": "ICML,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Convex Analysis and Nonlinear Optimization", "author": ["J.M. Borwein", "A.S. Lewis"], "venue": "Springer,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Convex Optimization", "author": ["S. Boyd", "L. Vandenberghe"], "venue": "Cambridge University Press, Cambridge, UK.,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "MILES: Multiple-instance learning via embedded instance selection", "author": ["Y. Chen", "J. Bi", "J.Z. Wang"], "venue": "TPAMI, 28:1931\u20131947,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Solving the multiple instance problem with axisparallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-Perez"], "venue": "Artificial Intelligence, 89:31\u201371,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "A feature selection newton method for support vector machine classification", "author": ["G.M. Fung", "O.L. Mangasarian"], "venue": "Computational Optimization and Applications, 28:185\u2013202,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyou", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine Learning, 46:389\u2013422,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Convex Analysis and Minimization Algorithms", "author": ["J.B. Hiriart-Urruty", "C. Lemarechal"], "venue": "Springer-Verlag,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "A support vector method for multivariate performance measures", "author": ["T. Joachims"], "venue": "ICML,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "SIGKDD,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Cutting-plane training of structural SVMs", "author": ["T. Joachims", "T. Finley", "C.J. Yu"], "venue": "Machine Learning, 77:27\u2013 59,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "The cutting plane algorithm for solving convex programs", "author": ["J.E. Kelley"], "venue": "Journal of the Society for Industrial and Applied Mathematics, 8(4):703\u2013712,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1960}, {"title": "Embedded methods", "author": ["T.N. Lal", "O. Chapelle", "J. Weston", "A. Elisseeff"], "venue": "I. Guyon, S. Gunn, M. Nikravesh, and L. A. Zadeh, editors, Feature Extraction: Foundations and Applications, Studies in Fuzziness and Soft Computing, number 207, pages 137\u2013165. Springer,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Direct optimization of ranking measures", "author": ["Q.V. Le", "A. Smola"], "venue": "JMLR, 1:1\u201348,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "A risk ratio comparison of l0 and l1 penalized regressions", "author": ["D. Lin", "D.P. Foster", "L.H. Ungar"], "venue": "Technical report, University of Pennsylvania,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse logistic regression with lp penalty for biomarker identification", "author": ["Z. Liu", "F. Jiang", "G. Tian", "S. Wang", "F. Sato", "S.J. Meltzer", "M. Tan"], "venue": "Statistical Applications in Genetics and Molecular Biology, 6(1),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimizing performance measures for feature selection", "author": ["Q. Mao", "I.W. Tsang"], "venue": "ICDM,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple-instance learning for natural scene classification", "author": ["O. Maron", "A.L. Ratan"], "venue": "ICML,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Optimizing f-measure with support vector machines", "author": ["D.R. Musicant", "V. Kumar", "A. Ozgur"], "venue": "Proceedings of the 16th International Florida Artificial Intelligence Research Society Conference,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2003}, {"title": "Cutting-set methods for robust convex optimization with pessimizing oracles", "author": ["A. Mutapcic", "S. Boyd"], "venue": "Optimization Methods & Software, 24(3):381406,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Feature selection, l1 vs", "author": ["A.Y. Ng"], "venue": "l2 regularization, and rotational invariance. In ICML,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "SimpleMKL", "author": ["A. Rakotomamonjy", "F.R. Bach", "Y. Grandvalet", "S. Canu"], "venue": "JMLR, 3:1439\u20131461,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Large scale multiple kernel learning", "author": ["S. Sonnenburg", "G. R\u00e4tsch", "C. Sch\u00e4fer", "B. Scholk\u00f6pf"], "venue": "JMLR, 7,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning sparse SVM for feature selection on very high dimensional datasets", "author": ["M. Tan", "L. Wang", "I.W. Tsang"], "venue": "ICML,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Bundle methods for regularized risk minimization", "author": ["C.H. Teo", "S.V.N. Vishwanathan", "A. Smola", "Quoc V. Le"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altum"], "venue": "JMLR, 6:1453\u20131484,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning to rank by optimizing ndcg measure", "author": ["H. Valizadengan", "R. Jin", "R. Zhang", "J. Mao"], "venue": "NIPS,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Use of the zero-norm with linear models and kernel methods", "author": ["J. Weston", "A. Elisseeff", "B. Scholk\u00f6pf"], "venue": "JMLR, 3:1439\u20131461,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "An extended level method for efficient multiple kernel learning", "author": ["Z. Xu", "R. Jin", "I. King", "M.R. Lyu"], "venue": "NIPS,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Non-monotonic feature selection", "author": ["Z. Xu", "R. Jin", "J. Ye", "Michael R. Lyu", "I. King"], "venue": "In ICML,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "A comparison of optimization methods and software for large-scale l1-regularized linear classification", "author": ["G.-X. Yuan", "K.-W. Chang", "C.-J. Hsieh", "C.-J. Lin"], "venue": "JMLR, 11:3183\u20133234,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "A support vector method for optimizing average precision", "author": ["Y. Yue", "T. Finley", "F. Radlinski", "T. Joachims"], "venue": "SIGIR,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Content-based image retrieval using multiple-instance learning", "author": ["Q. Zhang", "S.A. Goldman", "W. Yu", "J. Fritts"], "venue": "ICML,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "Analysis of multi-stage convex relaxation for sparse regularization", "author": ["T. Zhang"], "venue": "JMLR, 11:1081\u20131107, Mar", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Smoothing multivariate performance measures", "author": ["X. Zhang", "A. Saha", "S.V.N. Vishwanathan"], "venue": "UAI,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-instance multi-label learning with application to scene classification", "author": ["Z.-H. Zhou", "M.-L. Zhang"], "venue": "NIPS,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "1-norm support vector machine", "author": ["J. Zhu", "S. Rossett", "T. Hastie", "R. Tibshirani"], "venue": "NIPS,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiclass multiple kernel learning", "author": ["A. Zien", "C.S. Ong"], "venue": "ICML,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 10, "context": "Hence, the error rate is considered as a poor criterion for the problems with highly skewed class distributions [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "Instead of optimizing the error rate, Support Vector Machine for multivariate performance measures (SVM ) [11] was proposed to directly optimize the losses based on a variety of multivariate performance measures.", "startOffset": 106, "endOffset": 110}, {"referenceID": 36, "context": "A smoothing version of SVM [37] was proposed to accelerate the convergence of the optimization problem specially designed", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "Structural SVMs are considered as the general framework for optimizing a variety of loss functions [27, 13, 28].", "startOffset": 99, "endOffset": 111}, {"referenceID": 12, "context": "Structural SVMs are considered as the general framework for optimizing a variety of loss functions [27, 13, 28].", "startOffset": 99, "endOffset": 111}, {"referenceID": 27, "context": "Structural SVMs are considered as the general framework for optimizing a variety of loss functions [27, 13, 28].", "startOffset": 99, "endOffset": 111}, {"referenceID": 20, "context": "Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.", "startOffset": 81, "endOffset": 85}, {"referenceID": 28, "context": "Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.", "startOffset": 154, "endOffset": 158}, {"referenceID": 15, "context": "Other works optimize specific multivariate performance measures, such as F-score [21], normalize discount cumulative gain (NDCG) [29], ordinal regression [12], ranking loss [16] and so on.", "startOffset": 173, "endOffset": 177}, {"referenceID": 14, "context": "To alleviate these issues, one can resort to embedded feature selection methods [15], which can be categorized into the following two major directions.", "startOffset": 80, "endOffset": 84}, {"referenceID": 38, "context": "SVM, logistic regression) with l1-norm \u2016w\u20161 [39, 8, 23].", "startOffset": 44, "endOffset": 55}, {"referenceID": 7, "context": "SVM, logistic regression) with l1-norm \u2016w\u20161 [39, 8, 23].", "startOffset": 44, "endOffset": 55}, {"referenceID": 22, "context": "SVM, logistic regression) with l1-norm \u2016w\u20161 [39, 8, 23].", "startOffset": 44, "endOffset": 55}, {"referenceID": 32, "context": "A thorough study to compare several recently developed l1regularized algorithms has been conducted in [33].", "startOffset": 102, "endOffset": 106}, {"referenceID": 29, "context": "To achieve a sparser solution, the Approximation of the zeRO norm Minimization (AROM) was proposed [30] to optimize l0 models.", "startOffset": 99, "endOffset": 103}, {"referenceID": 17, "context": "However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].", "startOffset": 28, "endOffset": 32}, {"referenceID": 16, "context": "However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].", "startOffset": 57, "endOffset": 65}, {"referenceID": 35, "context": "However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].", "startOffset": 57, "endOffset": 65}, {"referenceID": 17, "context": "However, the recent results [18] and theoretical studies [17, 36] have showed that lp models (where p < 1) even with a local optimal solution can achieve better prediction performance than convex l1 models, which are asymptotically biased [18].", "startOffset": 239, "endOffset": 243}, {"referenceID": 8, "context": "Another way is to sort the weights of a SVM classifier and remove the smallest weights iteratively, which is known as SVM with Recursive Feature Elimination (SVM-RFE) [9].", "startOffset": 167, "endOffset": 170}, {"referenceID": 31, "context": "However, as discussed in [32], such nested \u201cmonotonic\u201d feature selection scheme leads to suboptimal performance.", "startOffset": 25, "endOffset": 29}, {"referenceID": 31, "context": "Non-monotonic feature selection (NMMKL) [32] has been proposed to solve this problem, but each feature corresponding to one kernel makes NMMKL infeasible for high-dimensional problems.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "[26] proposed Feature Generating Machine (FGM), which shows great scalability to non-monotonic feature selection on large-scale and very high-dimensional datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.", "startOffset": 45, "endOffset": 64}, {"referenceID": 29, "context": "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.", "startOffset": 45, "endOffset": 64}, {"referenceID": 8, "context": "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.", "startOffset": 45, "endOffset": 64}, {"referenceID": 31, "context": "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.", "startOffset": 45, "endOffset": 64}, {"referenceID": 25, "context": "The aforementioned feature selection methods [33, 30, 9, 32, 26] are usually designed for optimizing classification error only.", "startOffset": 45, "endOffset": 64}, {"referenceID": 10, "context": "Due to exponential number of constraints brought by non-smooth multivariate loss functions [11, 13] and exponential number of feature subset combinations [26], the resultant optimization problem is very challenging for high-dimensional data.", "startOffset": 91, "endOffset": 99}, {"referenceID": 12, "context": "Due to exponential number of constraints brought by non-smooth multivariate loss functions [11, 13] and exponential number of feature subset combinations [26], the resultant optimization problem is very challenging for high-dimensional data.", "startOffset": 91, "endOffset": 99}, {"referenceID": 25, "context": "Due to exponential number of constraints brought by non-smooth multivariate loss functions [11, 13] and exponential number of feature subset combinations [26], the resultant optimization problem is very challenging for high-dimensional data.", "startOffset": 154, "endOffset": 158}, {"referenceID": 18, "context": "This paper is an extension of our preliminary work [19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "\u2022 Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.", "startOffset": 91, "endOffset": 94}, {"referenceID": 31, "context": "\u2022 Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.", "startOffset": 102, "endOffset": 106}, {"referenceID": 32, "context": "\u2022 Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.", "startOffset": 115, "endOffset": 119}, {"referenceID": 29, "context": "\u2022 Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.", "startOffset": 128, "endOffset": 132}, {"referenceID": 25, "context": "\u2022 Connections to a variety of the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26] are discussed in details.", "startOffset": 141, "endOffset": 145}, {"referenceID": 32, "context": "By comparing with these methods, the advantages of our proposed methods are summarized as follows: (1) The tradeoff parameter C in l1 SVM [33] is too sensitive to be tuned properly since it controls both margin loss and the sparsity of w.", "startOffset": 138, "endOffset": 142}, {"referenceID": 2, "context": "This separation makes parameter tuning for our methods much easier than those of SKM [3] and l1 SVM.", "startOffset": 85, "endOffset": 88}, {"referenceID": 31, "context": "(2) NMMKL [32] uses the similar parameter separation strategy, but it is intractable for this method to handle high-dimensional datasets, let alone optimize multivariate losses.", "startOffset": 10, "endOffset": 14}, {"referenceID": 25, "context": "(3) FGM [26] is a special case of the propose framework when optimizing square hinge loss with indicator variables in integer domain.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": "\u2022 Recall that Multiple-Instance Learning via Embedded instance Selection (MILES) [6], which transforms multiple instance learning (MIL) into a feature selection problem by embedding bags into an instance-based feature space and selecting the most important features, achieves state-of-the-art performance for multiple instance learning problems.", "startOffset": 81, "endOffset": 84}, {"referenceID": 27, "context": "Structural SVMs [28, 13] are proposed to solve the corresponding soft-margin case by 1-slack variable formula as,", "startOffset": 16, "endOffset": 24}, {"referenceID": 12, "context": "Structural SVMs [28, 13] are proposed to solve the corresponding soft-margin case by 1-slack variable formula as,", "startOffset": 16, "endOffset": 24}, {"referenceID": 10, "context": "As shown in [11], optimizing the learning model subject to one specific multivariate measure can really boost the performance of this measure.", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "This regularizer is similar to SimpleMKL [24] with each feature corresponding to one kernel, but SimpleMKL is a special case of D with B = 1, which also can be interpreted by the quadratic variational formulation of l1 norm [2].", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "This regularizer is similar to SimpleMKL [24] with each feature corresponding to one kernel, but SimpleMKL is a special case of D with B = 1, which also can be interpreted by the quadratic variational formulation of l1 norm [2].", "startOffset": 224, "endOffset": 227}, {"referenceID": 3, "context": "Since domains D and A are nonempty, the function S(\u03b1,d) is closed and convex for all d \u2208 D given any \u03b1 \u2208 A, and the function S(\u03b1,d) is closed and concave for all \u03b1 \u2208 A given any d \u2208 D, the saddle-point property: mind\u2208D max\u03b1\u2208A S(\u03b1, d) = max\u03b1\u2208A mind\u2208D S(\u03b1, d) holds [4].", "startOffset": 264, "endOffset": 267}, {"referenceID": 13, "context": "Cutting plane algorithm [14] could be used here to solve this problem.", "startOffset": 24, "endOffset": 28}, {"referenceID": 21, "context": "As from [22], such cutting plane algorithm can converge to a robust optimal solution within tens of iterations with the exact worst-case analysis.", "startOffset": 8, "endOffset": 12}, {"referenceID": 24, "context": "In this case, state-of-the-art multiple kernel learning algorithms [25, 24, 31] do not work any more.", "startOffset": 67, "endOffset": 79}, {"referenceID": 23, "context": "In this case, state-of-the-art multiple kernel learning algorithms [25, 24, 31] do not work any more.", "startOffset": 67, "endOffset": 79}, {"referenceID": 30, "context": "In this case, state-of-the-art multiple kernel learning algorithms [25, 24, 31] do not work any more.", "startOffset": 67, "endOffset": 79}, {"referenceID": 26, "context": "Then we can apply the bundle method [27] to solve this primal problem.", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "Since Remp(w) is a convex function, its subgradient exists everywhere in its domain [10].", "startOffset": 84, "endOffset": 88}, {"referenceID": 26, "context": "Following the bundle method [27], the criterion for selecting the next point w is to solve the following problem,", "startOffset": 28, "endOffset": 32}, {"referenceID": 26, "context": "The convergence proof in [27] does not apply in this case as the Fenchel dual of \u03a9(w) fails to satisfy the strong convexity assumption if K > 1.", "startOffset": 25, "endOffset": 29}, {"referenceID": 26, "context": "As K = 1, Algorithm 1 is exactly the bundle method [27].", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "When K \u2265 2, we can adapt the proof of Theorem 5 in [13] for the following convergence results.", "startOffset": 51, "endOffset": 55}, {"referenceID": 12, "context": "We adapt the proof of Theorem 5 in [13], and sketch the necessary changes corresponding to Problem (29).", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "The Lemma 2 in [13] states that a line search starting at \u03b1 along an ascent direction \u03b7 with maximum step-size C > 0 improves the objective by at least max0\u2264\u03b2\u2264C { \u0398d(\u03b1+ \u03b2\u03b7) \u2212 \u0398d(\u03b1) } \u2265 12 min { C, \u2202\u0398d(\u03b1) T \u03b7 \u03b7T Qd\u03b7 } \u2202\u0398d(\u03b1) T \u03b7.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "We can see that it is a special case of [13] if T = 1.", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "According to Theorem 5 in [13], for a newly added constraint \u0177 and some \u03b3d > 0, we can obtain \u2202\u0398d(\u03b1) \u03b7 = \u03b3d by setting the ascent direction \u03b7\u0177 = 1 for the newly added \u0177 and \u03b7y = \u2212 1 C\u03b1y for the others.", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "Following the remaining derivation in [13], the overall bound results are obtained.", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "Remark 1: Problem (15) is similar to Support Kernel Machine (SKM) [3] in which the multiple Gaussian kernels are built on random subsets of features, with varying widths.", "startOffset": 66, "endOffset": 69}, {"referenceID": 25, "context": "In terms of the convergence proof of FGM in [26] and Theorem 1, we can obtain the following theorem to illustrate the approximation with an \u01eb-optimal solution to the original problem.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "Fortunately, there are a series of structured loss functions, such as Area Under ROC (AUC), Average Precision (AP), ranking and contingency table scores and other measures listed in [11, 34, 27], which can be implemented efficiently in our algorithms.", "startOffset": 182, "endOffset": 194}, {"referenceID": 33, "context": "Fortunately, there are a series of structured loss functions, such as Area Under ROC (AUC), Average Precision (AP), ranking and contingency table scores and other measures listed in [11, 34, 27], which can be implemented efficiently in our algorithms.", "startOffset": 182, "endOffset": 194}, {"referenceID": 26, "context": "Fortunately, there are a series of structured loss functions, such as Area Under ROC (AUC), Average Precision (AP), ranking and contingency table scores and other measures listed in [11, 34, 27], which can be implemented efficiently in our algorithms.", "startOffset": 182, "endOffset": 194}, {"referenceID": 10, "context": "In this paper, we only use several multivariate performance measures based on contingency table as the showcases and their finding y could be solved in time complexity O(n) [11].", "startOffset": 173, "endOffset": 177}, {"referenceID": 10, "context": "Then, Algorithm 2 in [11] can be directly applied.", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].", "startOffset": 200, "endOffset": 203}, {"referenceID": 31, "context": "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].", "startOffset": 211, "endOffset": 215}, {"referenceID": 32, "context": "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].", "startOffset": 224, "endOffset": 228}, {"referenceID": 29, "context": "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].", "startOffset": 237, "endOffset": 241}, {"referenceID": 25, "context": "RELATIONS TO EXISTING METHODS In this section, we will discuss the relationships between our proposed method for multivariate loss (5) and the state-of-the-art feature selection methods including SKM [3], NMMKL [32], l1-SVM [33], l0-SVM [30] and FGM [26].", "startOffset": 250, "endOffset": 254}, {"referenceID": 23, "context": "According to [24], we transform Problem (5) in the special case of B = 1 to the following equivalent optimization problem,", "startOffset": 13, "endOffset": 17}, {"referenceID": 2, "context": "SKM [3] attempts to obtain the sparsity of w by penalizing the square of a weighted block l1-norm ( \u2211k j=1 \u03b3j ||wj ||2) where k is the number of groups and wj is the weight vector for the features in the jth group.", "startOffset": 4, "endOffset": 7}, {"referenceID": 32, "context": "Minimizing the square of the l1-norm is very similar to l1-norm SVM [33] by setting \u03a9(w) = ||w||1 with the non-negative (convex) loss function.", "startOffset": 68, "endOffset": 72}, {"referenceID": 31, "context": "This strategy is also used in NMMKL [32], but one feature corresponding to one base kernel makes NMMKL intractable for highdimensional problems.", "startOffset": 36, "endOffset": 40}, {"referenceID": 39, "context": "According to the work [40], we can reformulate Problem (20) as an equivalent optimization problem", "startOffset": 22, "endOffset": 26}, {"referenceID": 25, "context": "It is not trivial to perform the extension of FGM to optimize multivariate loss because original FGM method [26] cannot directly apply to solve the exponential number of constraints.", "startOffset": 108, "endOffset": 112}, {"referenceID": 6, "context": ", drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature.", "startOffset": 27, "endOffset": 30}, {"referenceID": 34, "context": ", drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature.", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": ", drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature.", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": ", drug activity prediction [7], image retrieval [35], natural scene classification [20] and text categorization [1], but it is seldom optimized for multivariate performance measures in the literature.", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": "Multi-instance learning was formally introduced in the context of drug activity prediction [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 5, "context": "By defining a similarity measure between a bag and an instance, Multiple-Instance Learning via Embedded instance Selection (MILES) [6] successfully transforms multiple instance learning into a feature selection problem by embedding bags into an instance-based feature space and selecting the most important features.", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "Following the notations in [6], we denote ith positive bags as Bi = {xi,j} n + i j=1 which consists of n + i instances x + i,j , j = 1, .", "startOffset": 27, "endOffset": 30}, {"referenceID": 8, "context": "EXPERIMENTS In this Section, we conduct extensive experiments to evaluate the performance of our proposed method and state-of-the-art feature selection methods: 1) SVM-RFE [9]; 2) l1SVM; 3) FGM [26]; 4) l1-bmrm-F1, which is l1 regularized SVM for optimizing F1 score 1http://users.", "startOffset": 172, "endOffset": 175}, {"referenceID": 25, "context": "EXPERIMENTS In this Section, we conduct extensive experiments to evaluate the performance of our proposed method and state-of-the-art feature selection methods: 1) SVM-RFE [9]; 2) l1SVM; 3) FGM [26]; 4) l1-bmrm-F1, which is l1 regularized SVM for optimizing F1 score 1http://users.", "startOffset": 194, "endOffset": 198}, {"referenceID": 26, "context": "by bundle method [27].", "startOffset": 17, "endOffset": 21}, {"referenceID": 32, "context": "For l1-SVM, we also use Liblinear software, which implements the state-of-the-art l1-SVM algorithm [33].", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "Furthermore, several specific measures on the contingency table are investigated on Text datasets by comparing with SVM [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "Removing inactive constraints from the working set [13] in the inner layer is employed for speedup the QCQP problem.", "startOffset": 51, "endOffset": 55}, {"referenceID": 1, "context": "1, 1, 10, 100]\u00d7 n and B in [2, 5, 10, 50, 100, 150, 200, 250].", "startOffset": 27, "endOffset": 61}, {"referenceID": 4, "context": "1, 1, 10, 100]\u00d7 n and B in [2, 5, 10, 50, 100, 150, 200, 250].", "startOffset": 27, "endOffset": 61}, {"referenceID": 9, "context": "1, 1, 10, 100]\u00d7 n and B in [2, 5, 10, 50, 100, 150, 200, 250].", "startOffset": 27, "endOffset": 61}, {"referenceID": 9, "context": "binary dataset, we cannot report the training time for l1-bmrm-F1 since l1-bmrm-F1 cannot terminate after more than two days with the maximum iteration 1000 and parameter \u03bb \u2208 [10, 10] due to the extremely high dimensionality.", "startOffset": 175, "endOffset": 183}, {"referenceID": 9, "context": "binary dataset, we cannot report the training time for l1-bmrm-F1 since l1-bmrm-F1 cannot terminate after more than two days with the maximum iteration 1000 and parameter \u03bb \u2208 [10, 10] due to the extremely high dimensionality.", "startOffset": 175, "endOffset": 183}, {"referenceID": 26, "context": "after the maximum iteration, which is consistent with the discussion in Appendix C of [27] that bundle method with l1 regularizer cannot guarantee the convergence.", "startOffset": 86, "endOffset": 90}, {"referenceID": 10, "context": "Since [11] has proven that SVMmulti with Hamming loss, namely\u2206Err(y, y) = 2(b+c), is the same as SVM.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "Both datasets are used in [26], and they are already split into training and testing sets.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "This is the same as [26].", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "The setting for budget parameter B = [2, 5, 10, 50, 100, 150, 200, 250] for News20.", "startOffset": 37, "endOffset": 71}, {"referenceID": 4, "context": "The setting for budget parameter B = [2, 5, 10, 50, 100, 150, 200, 250] for News20.", "startOffset": 37, "endOffset": 71}, {"referenceID": 9, "context": "The setting for budget parameter B = [2, 5, 10, 50, 100, 150, 200, 250] for News20.", "startOffset": 37, "endOffset": 71}, {"referenceID": 1, "context": "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 4, "context": "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 9, "context": "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 19, "context": "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 29, "context": "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 39, "context": "binary, and B = [2, 5, 10, 20, 30, 40, 50, 60] for URL1.", "startOffset": 16, "endOffset": 46}, {"referenceID": 25, "context": "The elimination scheme of features for SVM-RFE method can be referred to [26].", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "1 of [6] to construct a dense and high-dimensional dataset on a preprocessed image data .", "startOffset": 5, "endOffset": 8}, {"referenceID": 37, "context": "This dataset is used in [38] for multi-instance learning.", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": "Each image is represented as a bag of nine instances generated by the SBN method [20].", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "After that, following [6], the natural scene image retrieval problem turns out to be a feature selection task to select relevant embedded instances for prediction.", "startOffset": 22, "endOffset": 25}, {"referenceID": 10, "context": "Following [11], we use the same notation SVMmulti for different multivariate performance measures.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": ", 2] as in [11].", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "Following the recipe of [5], the self-dual cone \u2016vt\u20162 \u2264 \u03b7t, \u2200t = 1, .", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "According to the property of self-dual cone [3], we can obtain the primal solution from its dual as wt = \u2212\u03bctvt = \u03bct \u2211 y \u03b1y\u2032a t y where \u03bcj is the dual variable of the j quadratic constraint such that \u2211m j=1 \u03bcj = 1, \u03bcj \u2208 R+, \u2200j = 1, .", "startOffset": 44, "endOffset": 47}, {"referenceID": 25, "context": "This turns out to be the same problem of FGM [26].", "startOffset": 45, "endOffset": 49}, {"referenceID": 25, "context": "For self-completeness, we give the theorem as follows, Theorem 3 ([26]).", "startOffset": 66, "endOffset": 70}], "year": 2013, "abstractText": "Feature selection with specific multivariate performance measures is the key to the success of many applications, such as image retrieval and text classification. The existing feature selection methods are usually designed for classification error. In this paper, we propose a generalized sparse regularizer. Based on the proposed regularizer, we present a unified feature selection framework for general loss functions. In particular, we study the novel feature selection paradigm by optimizing multivariate performance measures. The resultant formulation is a challenging problem for high-dimensional data. Hence, a twolayer cutting plane algorithm is proposed to solve this problem, and the convergence is presented. In addition, we adapt the proposed method to optimize multivariate measures for multiple instance learning problems. The analyses by comparing with the state-ofthe-art feature selection methods show that the proposed method is superior to others. Extensive experiments on large-scale and high-dimensional real world datasets show that the proposed method outperforms l1-SVM and SVM-RFE when choosing a small subset of features, and achieves significantly improved performances over SVM in terms of F1-score.", "creator": "LaTeX with hyperref package"}}}