{"id": "1103.4204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2011", "title": "Parallel Online Learning", "abstract": "In this work we study parallelization of online learning, a core primitive in machine learning. In a parallel environment all known approaches for parallel online learning lead to delayed updates, where the model is updated using out-of-date information. In the worst case, or when examples are temporally correlated, delay can have a very adverse effect on the learning algorithm. Here, we analyze and present preliminary empirical results on a set of learning architectures based on a feature sharding approach that present various tradeoffs between delay, degree of parallelism, representation power and empirical performance.", "histories": [["v1", "Tue, 22 Mar 2011 04:54:35 GMT  (97kb,D)", "http://arxiv.org/abs/1103.4204v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel hsu", "nikos karampatziakis", "john langford", "alex smola"], "accepted": false, "id": "1103.4204"}, "pdf": {"name": "1103.4204.pdf", "metadata": {"source": "CRF", "title": "Parallel Online Learning", "authors": ["Daniel Hsu", "Nikos Karampatziakis", "John Langford", "Alex J. Smola"], "emails": [], "sections": [{"heading": "Parallel Online Learning", "text": "It is not as if this is a basic approach to machine learning by greedily updating or subtracting the characteristics of a misclassified instance repeatedly. More generally, typical methods calculate the gradient of the prediction in relation to the loss of the weight vector and then update the system according to the negative gradient. This basic approach has many variations and extensions, as well as at least two names. In neural network literature, this approach is often referred to as \"stochastic gradient,\" whereas in learning theory it is typically referred to as \"gradient.\""}, {"heading": "Tree Architectures", "text": "Our strategy is to apply feature sharding to multiple nodes, each of which updates its parameters online, as a single learning algorithm would do. The problem now is that we have n independent predictors that each use only a subset of attributes (where n is the number of attributes splitters), not a single predictor that uses all attributes. We agree on this in the following way: (i) We require each of these nodes to calculate a prediction and pass it to a master node after it has received each new instance (but before updating its parameters); and (ii) we use the master node to treat these n predictions as attributes from which the master node learns to predict the label symmetrically."}, {"heading": "Convergence Time vs Representation Power", "text": "That is, it is not as if it is a kind and manner, as if it is in the position it sees itself in, in the position it sees itself in, in the position it is in, in the position it is in 3 (1) 3 (1) 3 (1) 3 (1) 4 (2) 4 (0) 4 (0) 3 (0) 3 (0) 3 (0) 3 (0) 3 (0) (1) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) 0 (0) 0 (0) 0 (0) 0 (0) 0 (0) 0 (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0 (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0 (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0 (0) (0) (0) (0) (0) (0) (0) (0) (0) (0) (0 (0) (0) (0) (0) (0) (0) (0) (0 (0) (0) (0) (0) (0"}], "references": [{"title": "Random Features for Large-Scale Kernel Machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In: Platt, J.C., Koller, D., Singer, Y., and Roweis, S. (eds), Advances in Neural Information Processing Systems 20. Cambridge, MA: MIT Press.", "citeRegEx": "Rahimi and Recht,? 2008", "shortCiteRegEx": "Rahimi and Recht", "year": 2008}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review, 65(6), 386\u2013408.", "citeRegEx": "Rosenblatt,? 1958", "shortCiteRegEx": "Rosenblatt", "year": 1958}, {"title": "Learning Internal Representations by Error Propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Chap. 8, pages 318\u2013362 of: Parallel Distributed Processing. Cambridge, MA: MIT Press.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Pegasos: Primal Estimated sub-GrAdient Solver for SVM", "author": ["Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan."], "venue": "In: Proc. Intl. Conf. Machine Learning.", "citeRegEx": "Shalev.Shwartz et al\\.,? 2007", "shortCiteRegEx": "Shalev.Shwartz et al\\.", "year": 2007}, {"title": "Hash Kernels", "author": ["Shi", "Qinfeng", "Petterson", "James", "Dror", "Gideon", "Langford", "John", "Smola", "Alex", "Strehl", "Alex", "S.V.N. Vishwanathan"], "venue": "In: Welling, Max, and van Dyk, David (eds), Proc. Intl. Workshop on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics.", "citeRegEx": "Shi et al\\.,? 2009", "shortCiteRegEx": "Shi et al\\.", "year": 2009}, {"title": "Bundle Methods for Regularized Risk Minimization", "author": ["Teo", "Choon Hui", "S.V.N. Vishwanthan", "Smola", "Alex J.", "Le", "Quoc V."], "venue": "J. Mach. Learn. Res. Submitted in February 2009.", "citeRegEx": "Teo et al\\.,? 2009", "shortCiteRegEx": "Teo et al\\.", "year": 2009}, {"title": "Feature Hashing for Large Scale Multitask Learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Attenberg", "J. Langford", "A.J. Smola"], "venue": "In: Bottou, L., and Littman, M. (eds), International Conference on Machine Learning.", "citeRegEx": "Weinberger et al\\.,? 2009", "shortCiteRegEx": "Weinberger et al\\.", "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "A canonical example of this is provided by the perceptron algorithm (Rosenblatt, 1958) which modifies a weight vector by adding or subtracting the features of a misclassified instance.", "startOffset": 68, "endOffset": 86}, {"referenceID": 2, "context": "For the training of complex nonlinear prediction systems, the stochastic gradient descent approach was described long ago and has been standard practice for at least two decades (Bryson and Ho, 1969; Rumelhart et al., 1986; Amari, 1967).", "startOffset": 178, "endOffset": 236}, {"referenceID": 0, "context": "For instance, we may use the random kitchen sink features (Rahimi and Recht, 2008) to obtain prediction performance comparable with Gaussian RBF kernel classes.", "startOffset": 58, "endOffset": 82}, {"referenceID": 4, "context": "Note that VW can additionally reduce the dimensionality of each instance using feature hashing (Shi et al., 2009; Weinberger et al., 2009) , which is essential when the (expanded) feature space is large, perhaps even exceeding memory size.", "startOffset": 95, "endOffset": 138}, {"referenceID": 6, "context": "Note that VW can additionally reduce the dimensionality of each instance using feature hashing (Shi et al., 2009; Weinberger et al., 2009) , which is essential when the (expanded) feature space is large, perhaps even exceeding memory size.", "startOffset": 95, "endOffset": 138}, {"referenceID": 5, "context": "Contrast this with, say, bundle methods (Teo et al., 2009) which use the gradients to construct a global approximation of the loss.", "startOffset": 40, "endOffset": 58}], "year": 2011, "abstractText": null, "creator": "LaTeX with hyperref package"}}}