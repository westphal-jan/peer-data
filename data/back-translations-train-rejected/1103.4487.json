{"id": "1103.4487", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2011", "title": "Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs", "abstract": "The competitive MNIST handwritten digit recognition benchmark has a long history of broken records since 1998. The most recent substantial improvement by others dates back 7 years (error rate 0.4%) . Recently we were able to significantly improve this result, using graphics cards to greatly speed up training of simple but deep MLPs, which achieved 0.35%, outperforming all the previous more complex methods. Here we report another substantial improvement: 0.31% obtained using a committee of MLPs.", "histories": [["v1", "Wed, 23 Mar 2011 10:38:50 GMT  (110kb,D)", "http://arxiv.org/abs/1103.4487v1", "9 pages, 4 figures, 3 tables"]], "COMMENTS": "9 pages, 4 figures, 3 tables", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE", "authors": ["dan c cire\\c{s}an", "ueli meier", "luca m gambardella", "j\\\"urgen schmidhuber"], "accepted": false, "id": "1103.4487"}, "pdf": {"name": "1103.4487.pdf", "metadata": {"source": "CRF", "title": "Handwritten Digit Recognition with a Committee of Deep Neural Nets on GPUs", "authors": ["Dan C. Cire\u015fan", "Ueli Meier", "Luca M. Gambardella", "J\u00fcrgen Schmidhuber"], "emails": [], "sections": [{"heading": null, "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction", "text": "More than a decade ago, Multilayer Perceptrons or MLPs (Werbos, 1974; LeCun, 1985; Rumelhart et al., 1986) were among the first classifiers to be tested on the now famous MNIST handwritten digit recognition benchmark, most of which had few layers or few artificial neurons (units) per layer (LeCun et al., 1998), but apparently these were the largest viable MLPs at the time, formed when CPU cores were at least 20 times slower than today. A newer MLP with a single hidden layer of 800 units reached 0.70% error (Simard et al., 2003), but the most recent significant improvement by others occurred in 2003 (Simard et al., 2003) (error rate 0.4%), which we achieved with a revolutionary neural network (CNN) using novel elastic image deformations."}, {"heading": "2 Distorting images to get more training instances", "text": "MNIST consists of two sets of data, one for training (60,000 frames) and one for testing (10,000 frames). Many studies divide the training into two groups, consisting of 50,000 frames for training and 10,000 for validation. However, so far, the best results on MNIST have been achieved by deforming training images, greatly increasing their number. This allows the training of networks with many weights, making them insensitive to variability in the class. Our network is also trained on numerous slightly deformed images that are generated continuously online, so we can use the entire undeformed training for validation without wasting training images. Pixel intensities on the original image scale range from 0 (background) to 255 (maximum foreground intensity). 28 \u00d7 28 pixels per image are mapped to real values pixels."}, {"heading": "3 Forming a committee", "text": "The deviation of the aspect ratio for different digits is quite large, and we normalize the width of the bounding field in a range of 10 to 20 pixels with a step size of 2 pixels before training for all digits. Normalization of the original MNIST training data results in 6 normalized training sets. In total, we conduct experiments with seven different data sets (6 normalized and the original MNIST). The training procedure of a network is summarized in Figure 2. Each network is trained separately on normalized or original data. Normalization is performed for all digits in the training set before training (normalization level). During each training, the digits are distorted (Sec. 2)."}, {"heading": "4 Experiments on GPUs", "text": "All simulations were performed on a computer with a Core i7 920 2.66 GHz processor, 12GB RAM and a GTX 480 graphics card. GPU speeds up the deformation routine by a factor of 10 (only elastic deformations are GPU-optimized); forward propagation (FP) and BP routines by a factor of 50. We select the trained MLP with the lowest validation error and evaluate it on the MNIST testset.Our GPU implementation of the MLP framework is explained by Ciresan et al. (2010).We use the architecture (841 neurons on the input layer, five hidden layers with 2500, 2000, 1500, 1000 and 500 neurons and 10 outputs), which has a very low error rate of 0.35% on the MNIST framework. We train six additional networks with the same architecture on pre-processed (normalized) data, and build a net already by calculating the individual size of the IST, or the result is the size of the processed NT."}, {"heading": "5 Conclusion", "text": "Current GPUs are more than 50 times faster than standard microprocessors when it comes to training large and deep neural networks with online back-propagation (weight update rates up to 7.5 x 109 / s and more than 1015 per trained network).In the competitive MNIST handwriting benchmark, GPU-based floating-point committees for neural networks (each with a different preprocessor motivated by observed aspect ratio deviations and oblique handwritten digits) outperform all previously published methods, including complex methods that include specialized architectures, unattended pre-training, combinations of machine learning classifiers, and so on, to avoid an overhaul, training sets of sufficient size are achieved through appropriately distorted images."}, {"heading": "Acknowledgment", "text": "This work was partly financed by the Swiss Commission for Technology and Innovation (CTI), Project No. 9688.1 IFF: Intelligent Fill in Form."}], "references": [{"title": "Pattern Recognition and Machine Learning", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q2006\\E", "shortCiteRegEx": "Bishop.", "year": 2006}, {"title": "Combining multiple classifiers for faster optical character recognition. In Document Analysis Systems VII, pages 358\u2013367", "author": ["Kumar Chellapilla", "Michael Shilman", "Patrice Simard"], "venue": null, "citeRegEx": "Chellapilla et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chellapilla et al\\.", "year": 2006}, {"title": "Deep big simple neural nets for handwritten digit recognition", "author": ["Dan C. Ciresan", "Ueli Meier", "Luca M. Gambardella", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Ciresan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ciresan et al\\.", "year": 2010}, {"title": "Une proc\u00e9dure d\u2019apprentissage pour r\u00e9seau a seuil asymmetrique (a learning scheme for asymmetric threshold networks)", "author": ["Yann LeCun"], "venue": "In Proceedings of Cognitiva", "citeRegEx": "LeCun.,? \\Q1985\\E", "shortCiteRegEx": "LeCun.", "year": 1985}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["Marc\u2019Aurelio Ranzato", "Christopher Poultney", "Sumit Chopra", "Yann LeCun"], "venue": "In J. Platt et al., editor, Advances in Neural Information Processing Systems (NIPS", "citeRegEx": "Ranzato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2006}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "author": ["Marc\u2019Aurelio Ranzato", "Fu Jie Huang", "Y-Lan Boureau", "Yann LeCun"], "venue": "In Proc. Computer Vision and Pattern Recognition Conference", "citeRegEx": "Ranzato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "Geoffrey E. Hinton", "Ronald. J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["Patrice Y. Simard", "Dave. Steinkraus", "John C. Platt"], "venue": "In Seventh International Conference on Document Analysis and Recognition,", "citeRegEx": "Simard et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Simard et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "More than a decade ago, Multilayer Perceptrons or MLPs (Werbos, 1974; LeCun, 1985; Rumelhart et al., 1986) were among the first classifiers tested on the now famous MNIST handwritten digit recognition benchmark.", "startOffset": 55, "endOffset": 106}, {"referenceID": 7, "context": "More than a decade ago, Multilayer Perceptrons or MLPs (Werbos, 1974; LeCun, 1985; Rumelhart et al., 1986) were among the first classifiers tested on the now famous MNIST handwritten digit recognition benchmark.", "startOffset": 55, "endOffset": 106}, {"referenceID": 4, "context": "Most had few layers or few artificial neurons (units) per layer (LeCun et al., 1998), but apparently back then these were the biggest feasible MLPs, trained when CPU cores were at least 20 times slower than today.", "startOffset": 64, "endOffset": 84}, {"referenceID": 8, "context": "70% error (Simard et al., 2003).", "startOffset": 10, "endOffset": 31}, {"referenceID": 8, "context": "The latest substantial improvement by others occurred in 2003 (Simard et al., 2003) (error rate 0.", "startOffset": 62, "endOffset": 83}, {"referenceID": 2, "context": "35% using graphics cards (GPUs) to greatly speed up training of plain but deep MLPs (Ciresan et al., 2010).", "startOffset": 84, "endOffset": 106}, {"referenceID": 0, "context": "Our goal is to produce a group of classifiers whose errors on various parts of the training set differ as much as possible (Bishop, 2006).", "startOffset": 123, "endOffset": 137}, {"referenceID": 0, "context": "In the context of handwritten recognition, Chellapilla et al. (2006) already showed how a combination of various classifiers can be trained more quickly than a single classifier yielding the same error rate.", "startOffset": 43, "endOffset": 69}, {"referenceID": 8, "context": "\u2022 \u03c3 and \u03b1: for elastic distortions emulating uncontrolled oscillations of hand muscles (Simard et al., 2003).", "startOffset": 87, "endOffset": 108}, {"referenceID": 2, "context": "Our GPU implementation of the MLP framework is explained by Ciresan et al. (2010). We use the architecture (841 neurons on the input layer, five hidden layers containing 2500, 2000, 1500, 1000 and 500 neurons, and 10 outputs) that has a very low 0.", "startOffset": 60, "endOffset": 82}], "year": 2011, "abstractText": "The competitive MNIST handwritten digit recognition benchmark has a long history of broken records since 1998. The most recent substantial improvement by others dates back 7 years (error rate 0.4%) . Recently we were able to significantly improve this result, using graphics cards to greatly speed up training of simple but deep MLPs, which achieved 0.35%, outperforming all the previous more complex methods. Here we report another substantial improvement: 0.31% obtained using a committee of MLPs.", "creator": "LaTeX with hyperref package"}}}