{"id": "1104.0651", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Apr-2011", "title": "Meaningful Clustered Forest: an Automatic and Robust Clustering Algorithm", "abstract": "We propose a new clustering method that can be regarded as a numerical method to compute the proximity gestalt. The method analyzes edge length statistics in the MST of the dataset and provides an a contrario cluster detection criterion. The approach is fully parametric on the chosen distance and can detect arbitrarily shaped clusters. The method is also automatic, in the sense that only a single parameter is left to the user. This parameter has an intuitive interpretation as it controls the expected number of false detections. We show that the iterative application of our method can (1) provide robustness to noise and (2) solve a masking phenomenon in which a highly populated and salient cluster dominates the scene and inhibits the detection of less-populated, but still salient, clusters.", "histories": [["v1", "Mon, 4 Apr 2011 19:04:25 GMT  (6279kb,D)", "https://arxiv.org/abs/1104.0651v1", null], ["v2", "Thu, 16 Jun 2011 22:30:03 GMT  (6280kb,D)", "http://arxiv.org/abs/1104.0651v2", null], ["v3", "Tue, 19 Jul 2011 14:39:35 GMT  (6280kb,D)", "http://arxiv.org/abs/1104.0651v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mariano tepper", "pablo mus\\'e", "r\\'es almansa"], "accepted": false, "id": "1104.0651"}, "pdf": {"name": "1104.0651.pdf", "metadata": {"source": "META", "title": "Meaningful Clustered Forest: an Automatic and Robust Clustering Algorithm", "authors": ["Mariano Tepper", "Pablo Mus\u00e9", "Andr\u00e9s Almansa"], "emails": ["mtepper@dc.uba.ar", "pmuse@fing.edu.uy", "andres.almansa@telecom-paristech.fr"], "sections": [{"heading": null, "text": "Keywords and phrases: clusters, minimum spanning tree, a contrary detection."}, {"heading": "1. Introduction", "text": "In a sense, this is due only to the fact that the term cluster cannot be trivially separated from the context. Consequently, in practice, there are different authors who provide different definitions, which are usually derived from the algorithm used, rather than the oppositional structure. Unfortunately, the lack of a uniform definition is responsible for the fact that it is difficult to find a unifying cluster theory. A plethora of methods for evaluating or classifying clustering algorithms has been developed, some of them with very interesting results. To1ar Xiv: 110 4.06 51v3 [cs.Lcite a few [7, 20, 21]. For a broad perspective on clustering techniques, we refer the reader to the excellent overview of methods recently reported by Jain."}, {"heading": "2. A New Clustering Method: Proximal Meaningful Forest", "text": "We now propose a new method to find clusters in graphs that are independent of their shape and dimension. First, we create a weighted undirected graph G = (X, E), where X is a set of characteristics in a metric space (M, d) and the weighting function \u03c9 is defined with respect to the corresponding distance function\u03c9 ((vi, vj)) = d (xi, xj). (1)"}, {"heading": "2.1. The Minimum Spanning Tree", "text": "Informally, the Minimum Spanning Tree (MST) of an undirected, weighted graph is the tree that covers all vertices with minimum total edge cost (max.).In view of a metric space (M, d) and a feature set X M, we will present by G = (X, E) the undirected chart in which E = X \u00b7 X and the feature function of the chart \"X.\" A very important and classic property of the MST is that a hierarchy of point groups can be built by it.Notation 1. Let T = (X, ET) the minimum spanning tree by X. For a group of points C \u00b2 X, we denoteE (C) = {vi, vj \u00b2 C \u00b2."}, {"heading": "2.2. Proximal Meaningful Forest", "text": "First, let's observe that the edge length distribution of an MST of a cluster point configuration differs significantly from that of a non-clustered point in the light of the observed results (Figure 4). As a general idea, by knowing how to model non-clustered data, clustered data could be detected by measuring some kind of dissimilarity between the two factors. To achieve such a characterization, we need only two elements: (1) a naive model and (2) a measurement made on structures that can potentially be detected. The naive model is a probabilistic model that describes typical configurations in which no structure is present, and we will describe it in detail in Section 2.3. We will now focus on setting up a coherent measurement for MST-based clustering. Specifically, we are looking for the probability of the occurrence of the data under the background."}, {"heading": "2.3. The background model", "text": "The distribution Pr (\u03c9max (C) < \u03c9max (C) | \u03c9max (CF) \u0445 \u03c9max (CF) is a priori not known. Furthermore, to our knowledge there is no analytical expression for the cumulative edge distribution under H0 for the MST [17]. We estimate this distribution by performing Monte Carlo simulations of the background process. However, this estimate would involve extremely high computing costs.We assume that the edge lengths in the cluster, provided that \u03c9max (CF) \u03c9max (CF) \u03c9max (CF) are mutually independent and identically distributed. Let us be a random variable with this common distribution: The spectrum of the spectral data is as wide as the spectrum of spectroscopy."}, {"heading": "2.4. Eliminating redundancy", "text": "While each meaningful cluster is relevant on its own, the entire set of meaningful components generally has a high degree of redundancy. (A meaningful component C1 can, of course, contain another meaningful component C2. [5] This question can be answered by comparing NFA (C1) and NFA (C2) with definition 5. Of course, the group with the smallest NFA must be preferred. Classically, the following rule would be used for all meaningful clusters C1, C2 if C2, C1 and C2 would eliminate Arg max (NFA (C1), NFA (C2)) to perform the pruning of the meaningful components. Unfortunately, it leads to a phenomenon described in [5], where it was called collateral elimination. A very meaningful component can hide another meaningful component, as in Figure 5.The inclusive hierarchy provides an alternative scheme for truncating the meaningful components."}, {"heading": "3. Experiments on Synthetic examples", "text": "The data consists of 950 dots evenly distributed in the square of the unit, and 50 dots added manually around the positions (0.4, 0.4) and (0.7, 0.7).The figure shows the result of a numerical method that includes the above NFA. The distribution of the soil is chosen to be uniform in [0, 1] 2. Both visible clusters are found and their NFAs are respectively 10 \u2212 15 and 10 \u2212 8. Such low numbers can hardly be the result of the opportunities.The case of the mixture of Gaussians shown in Figure 8 provides an interesting example. On the tails, points are obviously more sparing and the distance to adjacent points increases. As we are looking for narrow components, the tail could be discarded depending on the complex components that we are different depending on the complex components."}, {"heading": "4. Handling MST Instability", "text": "This year, it has come to the point that it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We're able to get upset, \"he said,\" but we're not going to do it. \""}, {"heading": "5. The Masking Challenge", "text": "In 2009, Jain [18] stated that no available algorithm could hide the dataset in Figure 14b and the result in Figure 14c. (D) The dataset would be interesting if the MCF group could hide a phenomenon similar to that in the Voronoi diagrams. (D) It is a complete process in which the MCF group in the Voronoi diagrams is emptied at points in the MCF group and filled with dots distributed according to the background model. (D) In the fourth iteration, no MCF group is detected, thus stopping the algorithm group.Algorithm 3: \"It is a point that includes the F group of non-background dots. 1: F:\" It is a group of Voronoi diagrams that joins the background. \"(D):\" It is a point at which the MCF group does not line up. \"(Group) It is a point in the background."}, {"heading": "6. Three-dimensional point clouds", "text": "We tested the proposed algorithm with three-dimensional point clouds. We placed two point clouds in different positions in the same scene, thus creating two scenes in Figures 15 and 16. In both cases, an evenly distributed noise was artificially added, the skeletal hand and the rabbit consisting of 3274 and 3595, respectively. In Figure 15, 3031 noise points were added for a total of 9900 points. In Figure 15, 7031 noise points were added to a total of 13900 points, and both shapes were positioned closer together and in such a way that there is no linear separation between them. In both cases, the result is corrected. in Figure 15, the MCF is split, but the stabilization process discussed in Section 4 corrects the problem. In Figure 15, the same phenomenon is possible, but this realization of the noise process does not occur."}, {"heading": "7. Final Remarks", "text": "In this paper, we propose a new cluster method, which can be considered a numerical method for calculating the proximity shape. The proposed method is based on the analysis of the edge spacing in the MST of the dataset. The direct consequence is that our approach is completely parametric based on the chosen distance. The proposed method presents several novelties over other MST-based formulations. Some formulations prefer compact clusters because they extract their cluster recognition rule from characteristics that are not inherent in the MST. Our method focuses only on the length of the MST edges; therefore, it does not present such a preference. Other formulations analyze the data on a fixed local scale, thus introducing a new method parameter. We have shown from examples that these local methods can fail when the input data have clusters of different sizes and densities. In these examples, our method works well without the need to introduce additional parameters."}, {"heading": "Appendix A: Proof of Proposition 1", "text": "The proof is based on the following classic lemma.Lemma = E (E (E (E) = variable)."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the financial support of CNES (R & T Echantillonnage Irregulier DCT / SI / MO - 2010.001.4673), FREEDOM (ANR07-JCJC-0048-01), Callisto (ANR-09-CORD-003), ECOS Sud U06E01, ARFITEC (07 MATRH) and STIC Amsud (11STIC-01 - MMVPSCV) and the Uruguayan Agency for Research and Innovation (ANII) within the framework of the promotion PR-POS-2008-003."}], "references": [{"title": "Vanishing point detection without any a priori information", "author": ["A. Almansa", "A. Desolneux", "S. Vamech"], "venue": "Transactions on Pattern Analysis and Machine Intelligence, 25(4):502\u2013507,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Voronoi diagrams - a survey of a fundamental geometric data structure", "author": ["F. Aurenhammer"], "venue": "ACM Computing Surveys, 23(3):345\u2013405, September", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1991}, {"title": "An automatic shape independent clustering technique", "author": ["S. Bandyopadhyay"], "venue": "Pattern Recognition, 37(1):33\u201345, January", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "On a Minimal Spanning Tree Approach in the Cluster Validation Problem", "author": ["Z. Barzily", "Z. Volkovich", "B. Akteke \u00d6zt\u00fcrk", "G.W. Weber"], "venue": "Informatica, 20(2):187\u2013202,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "A Unified Framework for Detecting Groups and Application to Shape Recognition", "author": ["F. Cao", "J. Delon", "A. Desolneux", "P. Mus\u00e9", "F. Sur"], "venue": "Journal of Mathematical Imaging and Vision, 27(2):91\u2013119, February", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Extracting Meaningful Curves from Images", "author": ["F. Cao", "P. Mus\u00e9", "F. Sur"], "venue": "J. Math. Imaging Vis., 22(2-3):159\u2013181,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Characterization, Stability and Convergence of Hierarchical Clustering methods", "author": ["G. Carlsson", "F. M\u00e9moli"], "venue": "Journal of Machine Learning Research, 11:1425\u20131470,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Mean shift: a robust approach toward feature space analysis", "author": ["D. Comaniciu", "P. Meer"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(5):603\u2013619, August", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Introduction to Algorithms", "author": ["T. Cormen", "C. Leiserson", "R. Rivest", "C. Stein"], "venue": "McGraw-Hill Science / Engineering / Math, 2nd edition, December", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "From Gestalt Theory to Image Analysis, volume 34", "author": ["A. Desolneux", "L. Moisan", "J.M. Morel"], "venue": "Springer-Verlag,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "The Perceptual Organization of Point Constellations", "author": ["M. Dry", "D. Navarro", "K. Preiss", "M. Lee"], "venue": "Annual Meeting of the Cognitive Science Society,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient Graph-Based Image Segmentation", "author": ["P. Felzenszwalb", "D. Huttenlocher"], "venue": "International Journal of Computer Vision, 59(2):167\u2013181, September", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Introduction to Statistical Pattern Recognition", "author": ["K. Fukunaga"], "venue": "Academic Press, second edition,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "The estimation of the gradient of a density function, with applications in pattern recognition", "author": ["K. Fukunaga", "L. Hostetler"], "venue": "IEEE Transactions on Information Theory, 21(1):32\u201340, January", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1975}, {"title": "LSD: A Fast Line Segment Detector with a False Detection Control", "author": ["R. Grompone von Gioi", "J. Jakubowicz", "J.M. Morel", "G. Randall"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "The Elements of Statistical Learning, Second Edition: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "Springer Series in Statistics. Springer, 2nd edition, February", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "A test of randomness based on the minimal spanning tree", "author": ["Richard Hoffman", "Anil K. Jain"], "venue": "Pattern Recognition Letters,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1983}, {"title": "Data clustering: 50 years beyond K-means", "author": ["A.K. Jain"], "venue": "Pattern Recognition Letters, 31(8):651\u2013666, September", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Uniformity testing using minimal spanning tree", "author": ["A.K. Jain", "Xiaowei Xu", "Tin K. Ho", "Fan Xiao"], "venue": "In International Conference on Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "On clusterings: Good, bad and  M", "author": ["R. Kannan", "S. Vempala", "A. Vetta"], "venue": "Tepper et al./Meaningful Clustered Forest  29 spectral. Journal of the ACM, 51(3):497\u2013515, May", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "An impossibility theorem for clustering", "author": ["J. Kleinberg"], "venue": "S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems, pages 446\u2013453. MIT Press,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "A Statistical Approach to the Matching of Local Features", "author": ["J. Rabin", "J. Delon", "Y. Gousseau"], "venue": "SIAM Journal on Imaging Sciences, 2(3):931\u2013958,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatically finding clusters in normalized cuts", "author": ["M. Tepper", "P. Mus\u00e9", "A. Almansa", "M. Mejail"], "venue": "Pattern Recognition, 44(7):1372\u20131386, July", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Boruvka Meets Nearest Neighbors", "author": ["M. Tepper", "P. Mus\u00e9", "A. Almansa", "M. Mejail"], "venue": "Technical report, Departamento de Computaci\u00f3n, Facultad de Ciencias Exactas y Naturales, Universidad de Buenos Aires, Argentina, April", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Laws of organization in perceptual forms, pages 71\u201388", "author": ["M. Wertheimer"], "venue": "Routledge and Kegan Paul,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1938}, {"title": "Graph-Theoretical Methods for Detecting and Describing Gestalt Clusters", "author": ["C.T. Zahn"], "venue": "Transactions on Computers, C-20(1):68\u201386,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1971}], "referenceMentions": [{"referenceID": 6, "context": "cite a few [7, 20, 21].", "startOffset": 11, "endOffset": 22}, {"referenceID": 19, "context": "cite a few [7, 20, 21].", "startOffset": 11, "endOffset": 22}, {"referenceID": 20, "context": "cite a few [7, 20, 21].", "startOffset": 11, "endOffset": 22}, {"referenceID": 17, "context": "For a broad perspective on clustering techniques, we refer the reader to the excellent overview of clustering methods recently reported by Jain [18].", "startOffset": 144, "endOffset": 148}, {"referenceID": 24, "context": "Based on psychophysical experiments using simple 2D figures, the Gestalt school studied the perceptual organization, and identified a set of rules that govern human perception [25].", "startOffset": 176, "endOffset": 180}, {"referenceID": 25, "context": "The conceptual grounds on which our work is based were laid by Zahn in a seminal paper from 1971 [26].", "startOffset": 97, "endOffset": 101}, {"referenceID": 8, "context": "(The MST is the tree structure induced by the distance dm [9].", "startOffset": 58, "endOffset": 61}, {"referenceID": 10, "context": "Reproduced from [11].", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "[11] supported this choice.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "From a theoretical point of view, Carlsson and M\u00e9moli [7] proved very recently that the single-link hierarchy (i.", "startOffset": 54, "endOffset": 57}, {"referenceID": 25, "context": "Zahn [26] suggested to cluster a feature set by eliminating the inconsistent edges in the minimum spanning tree.", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "For example, Felzenszwalb and Huttenlocher [12] and Bandyopadhyay [3] make use of the MST and RNG respectively.", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "For example, Felzenszwalb and Huttenlocher [12] and Bandyopadhyay [3] make use of the MST and RNG respectively.", "startOffset": 66, "endOffset": 69}, {"referenceID": 23, "context": "[24] proposed an efficient method to compute the MST on metric datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5, 12]), thus permitting to treat large datasets.", "startOffset": 0, "endOffset": 7}, {"referenceID": 11, "context": "[5, 12]), thus permitting to treat large datasets.", "startOffset": 0, "endOffset": 7}, {"referenceID": 9, "context": "developed a detection theory which seeks to provide a quantitative assessment of gestalts [10].", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "This theory is often referred as Computational Gestalt Theory and it has been successfully applied to numerous gestalts and detection problems [6, 15, 22].", "startOffset": 143, "endOffset": 154}, {"referenceID": 14, "context": "This theory is often referred as Computational Gestalt Theory and it has been successfully applied to numerous gestalts and detection problems [6, 15, 22].", "startOffset": 143, "endOffset": 154}, {"referenceID": 21, "context": "This theory is often referred as Computational Gestalt Theory and it has been successfully applied to numerous gestalts and detection problems [6, 15, 22].", "startOffset": 143, "endOffset": 154}, {"referenceID": 9, "context": "analyzed the proximity gestalt, proposing a clustering algorithm [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "[10] suffers from some problems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "Last, two phenomena called collateral elimination and faulty union in [5] occur when an extremely exceptional cluster hides or masks other less but still exceptional ones.", "startOffset": 70, "endOffset": 73}, {"referenceID": 4, "context": "[5] continued this line of research extending the clustering algorithm to higher dimensions and corrected the collateral elimination and faulty union issues, by introducing what they called indivisibility criterion.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] introducing a bias for small rectangles (since they are more densely sampled).", "startOffset": 0, "endOffset": 3}, {"referenceID": 22, "context": "[23] introduced the concept of graph-based a contrario clustering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "a point must be similar to all points in the cluster to at least one point in the cluster k-means single-link algorithm [13] Cao et al.", "startOffset": 120, "endOffset": 124}, {"referenceID": 4, "context": "[5] Mean Shift [8] Tepper et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[5] Mean Shift [8] Tepper et al.", "startOffset": 15, "endOffset": 18}, {"referenceID": 22, "context": "[23] Felzenszwalb and Huttenlocher [12] Table 1 Conceptually there are two different ways to form a cluster.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[23] Felzenszwalb and Huttenlocher [12] Table 1 Conceptually there are two different ways to form a cluster.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "With the objective of finding a suitable partition and to the best of our knowledge, Felzenszwalb and Huttenlocher [12] were the first to compare \u03c9max(CF ) with \u03c9max(C1) and \u03c9max(C2), with an additional correction factor \u03c4 .", "startOffset": 115, "endOffset": 119}, {"referenceID": 16, "context": "In 1983, following the same rationale Hoffman and Jain [17] proposed a similar idea: to perform a test of randomness.", "startOffset": 55, "endOffset": 59}, {"referenceID": 18, "context": "[19] further refined this work, by using heuristic computations to separate the dataset into two or more subsets which were then tested using a two sample test statistic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "recently continued this line of work [4].", "startOffset": 37, "endOffset": 40}, {"referenceID": 3, "context": "non-elongated) and equally sized clusters [4].", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "The NFA has a more intuitive meaning than the PFA, since it is an upper bound on the expectation of the number of false detections [10].", "startOffset": 131, "endOffset": 135}, {"referenceID": 8, "context": "The components are mainly determined by the sorted sequence of the edges from the original graph; this follows directly from Kruskal\u2019s algorithm [9].", "startOffset": 145, "endOffset": 148}, {"referenceID": 16, "context": "Moreover, up to our knowledge there is no analytical expression for the cumulative edge distribution under H0 for the MST [17].", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "This explains why naive classifiers, such as the Naive Bayes classifier, despite their simplicity, can often outperform more sophisticated classification methods [16].", "startOffset": 162, "endOffset": 166}, {"referenceID": 16, "context": "Hoffman and Jain [17] point out that the sampling window for the background point process is usually unknown for a given dataset.", "startOffset": 17, "endOffset": 21}, {"referenceID": 4, "context": "While each meaningful cluster is relevant by itself, the whole set of meaningful components exhibits, in general, high redundancy: a meaningful component C1 can contain another meaningful component C2 [5].", "startOffset": 201, "endOffset": 204}, {"referenceID": 4, "context": "Unfortunately, it leads to a phenomenon described in [5], where it was called collateral elimination.", "startOffset": 53, "endOffset": 56}, {"referenceID": 9, "context": "[10], which states that", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "in Figure 2 [6].", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "background distribution is chosen to be uniform in [0, 1].", "startOffset": 51, "endOffset": 57}, {"referenceID": 7, "context": "Our results are compared with Felzenszwalb and Huttenlocher\u2019 algorithm (denoted by FH in the following) and with Mean Shift [8, 14].", "startOffset": 124, "endOffset": 131}, {"referenceID": 13, "context": "Our results are compared with Felzenszwalb and Huttenlocher\u2019 algorithm (denoted by FH in the following) and with Mean Shift [8, 14].", "startOffset": 124, "endOffset": 131}, {"referenceID": 7, "context": "Clusters are determined by what Comaniciu and Meer call \u201cbassins of attraction\u201d [8]: points are assigned to a local maximum following an ascendent path along the density gradient .", "startOffset": 80, "endOffset": 83}, {"referenceID": 1, "context": "This noise filling procedure can be achieved by using the Voronoi diagram [2] of the original point set.", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "Notice that this procedure is actually generic since the Voronoi tesselation can be generalized to higher dimensional metric spaces [2].", "startOffset": 132, "endOffset": 135}, {"referenceID": 17, "context": "In 2009, Jain [18] stated that no available algorithm could cluster the dataset in Figure 14b and obtain the result in Figure 14c.", "startOffset": 14, "endOffset": 18}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}], "year": 2011, "abstractText": "We propose a new clustering technique that can be regarded as a numerical method to compute the proximity gestalt. The method analyzes edge length statistics in the MST of the dataset and provides an a contrario cluster detection criterion. The approach is fully parametric on the chosen distance and can detect arbitrarily shaped clusters. The method is also automatic, in the sense that only a single parameter is left to the user. This parameter has an intuitive interpretation as it controls the expected number of false detections. We show that the iterative application of our method can (1) provide robustness to noise and (2) solve a masking phenomenon in which a highly populated and salient cluster dominates the scene and inhibits the detection of less-populated, but still salient, clusters.", "creator": "LaTeX with hyperref package"}}}