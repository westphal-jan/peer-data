{"id": "1104.3250", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2011", "title": "Adding noise to the input of a model trained with a regularized objective", "abstract": "Regularization is a well studied problem in the context of neural networks. It is usually used to improve the generalization performance when the number of input samples is relatively small or heavily contaminated with noise. The regularization of a parametric model can be achieved in different manners some of which are early stopping (Morgan and Bourlard, 1990), weight decay, output smoothing that are used to avoid overfitting during the training of the considered model. From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters (Krogh and Hertz, 1991). Using Bishop's approximation (Bishop, 1995) of the objective function when a restricted type of noise is added to the input of a parametric function, we derive the higher order terms of the Taylor expansion and analyze the coefficients of the regularization terms induced by the noisy input. In particular we study the effect of penalizing the Hessian of the mapping function with respect to the input in terms of generalization performance. We also show how we can control independently this coefficient by explicitly penalizing the Jacobian of the mapping function on corrupted inputs.", "histories": [["v1", "Sat, 16 Apr 2011 18:09:13 GMT  (39kb,D)", "http://arxiv.org/abs/1104.3250v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["salah rifai", "xavier glorot", "yoshua bengio", "pascal vincent"], "accepted": false, "id": "1104.3250"}, "pdf": {"name": "1104.3250.pdf", "metadata": {"source": "CRF", "title": "Adding noise to the input of a model trained with a regularized objective", "authors": ["Salah Rifai", "Xavier Glorot", "Yoshua Bengio"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Regularization of a parametric model can be achieved in various ways, some of which are early stops (Morgan and Bourlard, 1990), weight breakdown or output smoothing, and are used to avoid overadjustment during the training of the model under consideration. From a Bayesian perspective, many regularization techniques correspond to the imposition of certain prior distributions of model parameters (Krogh and Hertz, 1991).In this paper, we propose a novel approach to achieve a regularization that combines noise in input and explicit output smoothing by regulating the L2 standard of the Jacobic mapping function with respect to input. Bishop (1995) has proven that the Thear Xiv: 110 4.32 50v1 [cs.AI] 16 r 2two approaches are essentially equivalent assumptions under certain objective assumptions that we achieve with a weak approximation function of our secondary view."}, {"heading": "2 Definitions", "text": "To improve legibility, most of our analyses refer only to vectors and matrices, with the exception of Section 6, for which it was not possible to avoid the use of tensor objects. Also, our analysis assumes that the output of the model is scalar, which will prevent the use of tensors for the lower terms of Taylor expansion. We will use the following notations: \u2022 <. >: internal product, \u2022 tensor product, \u2022 Jf (x), Hf (x), T (n) f (x): respectively the Jacobic, Hessian and n-th derivatives of f with respect to the vector x.We will consider the following points: Dn = {zi = (xi, yi)."}, {"heading": "3 Penalty term induced by noisy input", "text": "Bishop (1995) has already shown that aligning the parameters of a model with corrupt inputs is asymptotically equivalent to minimizing the true error function when, at the same time, the level of corruption is reduced to zero, since the number of corrupt inputs tends to infinity. He has also shown that adding noise in the input is equivalent to minimizing another objective function that includes one or more punitive terms, and he uses a Taylor expansion to derive an analytical approximation of the penalty caused by noise. If we use the above assumption, we can write: Cnoisy (\u03b8) = Cclean (\u03b8) + \u03c6 (8), where it is the punitive term. Substitution (7) and (2) in (8), we express the punitive term as being (Rachel) = 1n n n n n n nn nn n. i [... L (z) Nzi, \u03b8) Nzi, 2 (z) vedz \u2212 L (dz) as the german (L)."}, {"heading": "4 Non regularized objective", "text": "We define the following error function: Lclean (zi, \u03b8) = HLclean (zi, \u03b8) = HLclean (zi, \u03b8) = HLclean (zi, \u03b8) | without loss of generality to assume that F is a scalar function1, and that the sound is added to the input value x, we consider only the Hesse of the loss function in relation to x: HLclean (x) = Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes x: Bytes: Bytes: Bytes: Byx: Bytes: Bytes x: Bytes: Bytes x: Bytes x, Bytes: Bytes: Bytes x: Bytes: Bytes: Bytes x: Bytes."}, {"heading": "5 Our regularized objective", "text": "As the results of the previous section suggest, adding noise to the input of the objective function results in an undesirable term that could disrupt the goal of smoothing out the output of our function. We propose to overcome this difficulty by adding only noise to the input of the objective function of the Jacobian-JF. (This would avoid the unforeseeable effect of additional undesirable terms.) We define the error function that we have previously used by adding the L2 standard of the Jacobian term F in relation to x: Lreg + noise (zi, \u03b8) = F (xi, \u03b8) -yi + JF (x) -yF (x) -yF (x) -yF (x) We now calculate the Hessian standard of the first term, since the noise is only added to the input of the regulation term, and use the approximation of the F (xi, \u03b8) -yi standard of F (x)."}, {"heading": "6 Higher order terms of the Taylor expansion", "text": "In this section we are interested in the higher terms of cost approximation, then we find it convenient to use the following formalism: If TnL (z, \u03b5) denotes the n-th derivative of L in relation to z, then: TnL (z, \u03b5) = 1n! (1) (in \u03b5i1,..., \u03b5inT n i1,..., in (z) where Tn is a tensor of the order n andTni1,..., in (z) = nL (z). (in \u03b5i1,..., applying this formalism we can use the fourth derivative of the order as: T 4L (z, \u03b5) = 124 \u0445 i, j, k, l \u03b5i\u03b5jk\u03b5lT 4 ijkl (z). Using the two assumptions (z) (in relation to the sound distribution) we can know that the third derivative of the approximation is x."}, {"heading": "7 Comparison", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Noise added to the input of the objective function", "text": "Now that we have an approximation of higher order when noise is added to the input of the function, we can compare the order of magnitude of the coefficients penalizing the Hessian HF, note that in this case the Hessian term appears in the fourth order of the Taylor expansion of the cost function, while we only need an approximation of second order when adding a regularization term evaluated on an erroneous input. We can describe the approximation of the cost function without regulation in the fourth order as follows: Lnoise (z) \u2248 Lclean (z) + \u03c3 22! Tr (HLclean (z) + \u03c34 4 4! Tr (T 4Lclean (z)) Lnoise (z) = 0 F (xi, \u03b8) \u2212 yi \u0445 2 + \u03c32 | | | JF (x) | 2F + \u03c344 | | HF (x) 2F + R (20), where the number i of overlap corresponds to the term of Taylor expansion caused by the noise."}, {"heading": "7.2 Noise added to the input of the Jacobian of the objective function", "text": "In this case, we only need to approximate the cost function up to the second order of the Taylor expansion: Lreg + noise (zi, \u03b8) = HF (xi, \u03b8) \u2212 yi-2 + \u03bb | | JF (x) | | 2F + 2\u03bb\u03c32 | | HF (x) | | 2 F + R (21)"}, {"heading": "8 Experimental results", "text": "We tried to perform several experiments to compare the effect of regulation and noise with each other, for this task we used the well-known MNIST (LeCun et al., 1998), MNIST binarized and the USPS database. Surprisingly, we were able to achieve results close to those achieved with unattended pretraining (Erhan et al., 2010). MNIST is composed of 70k handwritten digits represented by a vector of pixels. It is divided into 50k for the training set, 10k for each of the validation and test kits, the range of features have been rescaled within [0, 1]. MNIST binary is divided exactly the same way as asMNIST, the only difference is that the intensity of the pixels are higher than 2552, where the others have been set to 0. USPS dataset consists of a set of training images set 2007, with a 7and 291 validation set."}, {"heading": "9 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Constraining the solution space", "text": "When optimizing a non-convex function with an infinite amount of local minimum, it is not clear which of these provides an adequate generalization performance, the concept of overfitting clearly illustrates this point. The proposed regularizer tries to avoid this scheme by flattening the mapping function over the training examples, inducing a local invariance in the mapping space to an infinitesimal variance in the input space. Figure 2 shows that the models learned with the regularization term are more robust compared to noise on the input. Geometrically, the added regularization forces the model to be a Lipschitz function or a shrinking map around the training examples imposing the constraint F (x + \u03b5) \u2248 F (x)."}, {"heading": "9.2 Smoothing away from the training points", "text": "To illustrate this point, one can imagine that at the top of a Dirac function, the norm of the Jacobin function is zero and infinite around it. Although this situation is not possible in the context of neural networks due to their smooth activation function, with sufficient capacity, we could approach this solution if we did not add additional constraints, one of which would be the adjustment of the locality of flatness as a hyperparameter of the model. It requires the calculation of higher terms of the mapping function in order to regulate the magnitude of its norms. As was discussed earlier, it is computationally expansive to calculate the norm of high-order derivatives exponentially, as their number of components increases exponentially, instead, our approach proposes an approximation of the Hessian term that allows you to control both the Jacobin and the Hessian norms at the same time."}, {"heading": "9.3 The other terms induced by the noise", "text": "As we have seen in Eq.17, the added noise not only results in a penalty for the standard of successive derivatives of the mapping function, and it is somewhat unclear how these terms behave during the gradient descent, since they are not forced to be positive. In a monitored environment, due to the low dimensionality of the target points, it is empirically possible to bring these terms to zero, whereas in a multidimensional regression task such as the reconstruction goal of an auto encoder, it is often impossible to achieve a \"near\" minimization of the costs from zero with a first-order optimization such as a stochastic gradient descent. The reader should note that the approximation of the noisy costs is valid when the number of corrupted inputs tends to infinity, although this is never the case in practice. It would be interesting to increase an estimate of the difference between the notions caused by noise and the real values of the notion of the dependence on the number of corrupted samples."}, {"heading": "10 Conclusion", "text": "We have shown how a better generalization performance can be achieved by using a regularization term that adds a marginal computational effort compared to the conventional approach. Furthermore, by extending the Taylor cost function, we have shown that by adding noise to the regularization input, we are able to punish the norm of higher order derivatives of the model with greater magnitude without having to calculate it explicitly, which would obviously be mathematically prohibitive. Initial results indicate that various parametric models would clearly benefit from this approach in terms of predicting output sample points. It would be interesting to investigate how this regularization approach would behave if used with non-parametric models such as Gauss mixtures."}], "references": [{"title": "The effects of adding noise during backpropagation training on a generalization performance", "author": ["G. An"], "venue": "Neural Comput.,", "citeRegEx": "An,? \\Q1996\\E", "shortCiteRegEx": "An", "year": 1996}, {"title": "Training with noise is equivalent to Tikhonov regularization", "author": ["C.M. Bishop"], "venue": "Neural Computation,", "citeRegEx": "Bishop,? \\Q1995\\E", "shortCiteRegEx": "Bishop", "year": 1995}, {"title": "Calculation of the smoothing spline with weighted roughness measure", "author": ["C. De Boor"], "venue": "In Mathematical Models and Methods in Applied Sciences,", "citeRegEx": "Boor,? \\Q1998\\E", "shortCiteRegEx": "Boor", "year": 1998}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "Manzagol", "P.-A", "P. Vincent", "S. Bengio"], "venue": null, "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "Deep sparse rectifier neural networks. Deep Learning and Unsupervised Feature Learning Workshop \u2014 NIPS \u201910", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": null, "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Nonparametric Regression and Generalized Linear Models", "author": ["Green", "B.W.S. Bernard W Silverman"], "venue": null, "citeRegEx": "Green and Silverman,? \\Q1993\\E", "shortCiteRegEx": "Green and Silverman", "year": 1993}, {"title": "A simple weight decay can improve generalization", "author": ["A. Krogh", "J.A. Hertz"], "venue": null, "citeRegEx": "Krogh and Hertz,? \\Q1991\\E", "shortCiteRegEx": "Krogh and Hertz", "year": 1991}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE ,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Generalization and parameter estimation in feedforward", "author": ["N. Morgan", "H. Bourlard"], "venue": null, "citeRegEx": "Morgan and Bourlard,? \\Q1990\\E", "shortCiteRegEx": "Morgan and Bourlard", "year": 1990}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Denver", "V. CO. Morgan Kaufmann. Nair", "G.E. Hinton"], "venue": null, "citeRegEx": "Denver et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Denver et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "The regularization of a parametric model can be achieved in different manners some of which are early stopping (Morgan and Bourlard, 1990), weight decay, output smoothing that are used to avoid overfitting during the training of the considered model.", "startOffset": 111, "endOffset": 138}, {"referenceID": 6, "context": "From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters (Krogh and Hertz, 1991).", "startOffset": 133, "endOffset": 156}, {"referenceID": 1, "context": "Using Bishop\u2019s approximation (Bishop, 1995) of the objective function when a restricted type of noise is added to the input of a parametric function, we derive the higher order terms of the Taylor expansion and analyze the coefficients of the regularization terms induced by the noisy input.", "startOffset": 29, "endOffset": 43}, {"referenceID": 8, "context": "The regularization of a parametric model can be achieved in different manners some of which are early stopping (Morgan and Bourlard, 1990), weight decay or output smoothing, and are used to avoid overfitting during the training of the considered model.", "startOffset": 111, "endOffset": 138}, {"referenceID": 6, "context": "From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters (Krogh and Hertz, 1991).", "startOffset": 133, "endOffset": 156}, {"referenceID": 1, "context": "Bishop (1995) has proved that the", "startOffset": 0, "endOffset": 14}, {"referenceID": 0, "context": "Using (a), we can write (An, 1996): \u2200i, \u222b \u03b5i\u03c8(\u03b5)d\u03b5 = 0 (4)", "startOffset": 24, "endOffset": 34}, {"referenceID": 1, "context": "3 Penalty term induced by noisy input Bishop (1995) already showed that tuning the parameters of a model with corrupted inputs is asymptotically equivalent to minimizing the true error function when simultaneously decreasing the level of corruption to zero as the number of corrupted inputs tends to infinity.", "startOffset": 38, "endOffset": 52}, {"referenceID": 1, "context": "All the above results are already well established in the literature of noise injection (Bishop, 1995; An, 1996).", "startOffset": 88, "endOffset": 112}, {"referenceID": 0, "context": "All the above results are already well established in the literature of noise injection (Bishop, 1995; An, 1996).", "startOffset": 88, "endOffset": 112}, {"referenceID": 7, "context": "8 Experimental results We have tried several experiments in order to benchmark the effect of regularization and noise combined, for this task we used the well known MNIST (LeCun et al., 1998), MNIST binarised and the USPS database.", "startOffset": 171, "endOffset": 191}, {"referenceID": 3, "context": "Surprisingly, we were able to achieve results close to those obtained with unsupervised pretraining (Erhan et al., 2010).", "startOffset": 100, "endOffset": 120}, {"referenceID": 4, "context": "max(0, x)) (Nair and Hinton, 2010; Glorot et al., 2010) as non-linearity in the hidden layer, surprisingly they seemed to benefit less from the added noise to the input than from the regularization term alone, they achieved a test classification performance of 4.", "startOffset": 11, "endOffset": 55}], "year": 2011, "abstractText": "Regularization is a well studied problem in the context of neural networks. It is usually used to improve the generalization performance when the number of input samples is relatively small or heavily contaminated with noise. The regularization of a parametric model can be achieved in different manners some of which are early stopping (Morgan and Bourlard, 1990), weight decay, output smoothing that are used to avoid overfitting during the training of the considered model. From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters (Krogh and Hertz, 1991). Using Bishop\u2019s approximation (Bishop, 1995) of the objective function when a restricted type of noise is added to the input of a parametric function, we derive the higher order terms of the Taylor expansion and analyze the coefficients of the regularization terms induced by the noisy input. In particular we study the effect of penalizing the Hessian of the mapping function with respect to the input in terms of generalization performance. We also show how we can control independently this coefficient by explicitly penalizing the Jacobian of the mapping function on corrupted inputs.", "creator": "LaTeX with hyperref package"}}}