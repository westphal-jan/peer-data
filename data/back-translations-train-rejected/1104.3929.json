{"id": "1104.3929", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2011", "title": "Understanding Exhaustive Pattern Learning", "abstract": "Pattern learning in an important problem in Natural Language Processing (NLP). Some exhaustive pattern learning (EPL) methods (Bod, 1992) were proved to be flawed (Johnson, 2002), while similar algorithms (Och and Ney, 2004) showed great advantages on other tasks, such as machine translation. In this article, we first formalize EPL, and then show that the probability given by an EPL model is constant-factor approximation of the probability given by an ensemble method that integrates exponential number of models obtained with various segmentations of the training data. This work for the first time provides theoretical justification for the widely used EPL algorithm in NLP, which was previously viewed as a flawed heuristic method. Better understanding of EPL may lead to improved pattern learning algorithms in future.", "histories": [["v1", "Wed, 20 Apr 2011 02:49:59 GMT  (26kb)", "http://arxiv.org/abs/1104.3929v1", "15 pages, 3 figures"]], "COMMENTS": "15 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["libin shen"], "accepted": false, "id": "1104.3929"}, "pdf": {"name": "1104.3929.pdf", "metadata": {"source": "CRF", "title": "Understanding Exhaustive Pattern Learning", "authors": ["Libin Shen"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 110 4.39 29v1 [cs.AI] 20 Apr 2"}, {"heading": "1. Introduction", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "2. Formalizing Exhaustive Pattern Learning", "text": "To formalize the basic idea of the EPL, we hereby introduce a task called Monotonous Translation. Analysis of this task can be extended to other pattern learning problems. Subsequently, we define the segmentation of training data and introduce EPL grammar, which will be used later in Section 3, Theoretical Justification of the EPL."}, {"heading": "2.1 Monotonic Translation", "text": "Monotonous translation is defined as: The input x-X is a word sequence x1x2... xi in the source language; the monotonous translation of x is y-Y, a word sequence y1y2... yi, of equal length in the target language, where yj is the translation of xj, 1 \u2264 j \u2264 i. In short, the monotonous translation is a simplified version of machine translation. There is no reordering, insertion or deletion of words. In this way, we ignore the effects of aligning word levels to focus our efforts on studying building blocks. We leave the inclusion of alignments to future work. In fact, we can simply consider alignments as constraints on building blocks. Monotonous translation is already generic enough to model many NLP tasks such as labeling and chunking."}, {"heading": "2.2 Training Data Segmentation and MLE Grammars", "text": "Without losing the generalization, we assume that the training data D contains a single pair of word strings, xD and yD, which could both be very long. Let's leave xD = x1x2... xn, and yD = y1y2... yn. The source word xi is oriented toward the target word yi. However, let's leave the length of the word strings | D | = n. Figure 1 shows a simple example of the training data. Here, we assume that there is a hidden segmentation of the training data that xD and yD segments into tokens. A token consists of a word string, either on the source or the target, and it contains at least one fictitious word. As for the monotonous translation, the source page and the target page share the same topology of segmentation. Tokens are the building blocks of the statistical model to be presented, which means that the parameters for the model token are defined."}, {"heading": "2.3 Exhaustive Pattern Learning for Monotonic Translation", "text": "We now present an EPL solution. We follow the widely used heuristic method to generate a grammar by applying different segmentations at the same time. (D) We build a heuristic grammar GD by counting all possible pairs of symbols (u, v) with the most d words on each page where d. \"(D, v) is a given parameterPr (u, v\") # (u, v) Zd where # (u, v) is the number of pairs of characters in (u, v) appears in D, 1 andZd \"(u, v\") # (u, v \"d\") # (u, d \") = (D) is the number of character pairs encrypted in (u, v). (D, v) is the number of pairs of characters in (u, v) in D, v.\""}, {"heading": "3. Theoretical Justification of Exhaustive Pattern Learning", "text": "In this section we first present an ensemble model and a prior distribution of segmentation, then we show the theorem of approximation and present conclusions on conditional probabilities and tree structures."}, {"heading": "3.1 An Ensemble Model", "text": "Let D be the training data of | D | words. Let D be an arbitrary symbol segmentation on D, where s is unknown to us. D and s give us the opportunity to obtain a model / grammar GD with maximum probability. Thus, we can calculate the common probability of (uj, vj) given grammar GD, Pr (uj, vj | GD). There are potentially exponential numbers of different segments for D. Here, we use an ensemble method to sum up over all possible segmentations. This method would provide a desirable coverage and variety of translation rules to learn from the training data. For each segmentation s, we have a fixed previous probability Pr (s), which we will show in Section 3.2. Thus, we define the ensemble probability L (uj, vj) as the following method: L (uj, vj, Pr) is a fixed prior probability of Pr (Pr)."}, {"heading": "3.2 Prior Distribution of Segmentation", "text": "Now we define a probability model to generate segmentation. s = < s1, s2,..., s | D | \u2212 1 > is a vector of | D | \u2212 1 independent Bernoulli variables. si represents whether xi and xi + 1 belong to separate tokens. 1 means yes and 0 means no. All individual separating variables have the same distribution, Pq (si = 0) = q and Pq (si = 1) = 1 \u2212 q, for a given real value q, 0 \u2264 q \u2264 1. Since L (ua, v) now depends on q, we rewrite it as Lq (ua, v). Based on the definition, Lemma 1 immediately follows, which will be used later. Lemma 1 For each pair of strings (u, v), the probability is that an appearance of (u, v) in D is marked exactly by s as u, q | u | \u2212 1 (1 \u2212 q) 2."}, {"heading": "3.3 Theorem of Approximation", "text": "We assume that the following two assumptions are true for each pair of tokens (uj, vj). Assumption 1 Any two of the mj phenomena in D are neither overlapping nor consecutive. We assume that this assumption is necessary for the calculation of E [mj, s, vj). We assume that we appear for the calculation of E [mj, s], s, s, s, s, s, s, s, vj, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s, s,"}, {"heading": "3.4 Corollaries on Conditional Probabilities", "text": "Theorem 2 refers to the common distribution of symbol pairs. In previous work of using EPL = = q = q = q = q = q probabilities, the same probabilities were used, for example, as P (u | v) and P (v | u). Starting from Theorem 2, we can easily get the following sequences for the conditional probabilities. Suppose the assumptions 1 and 2 apply to a particular pair of tokens (uj, vj), then we have the following consequences for the conditional probabilities. Pr (uj, GD, d) / Lq (uj, vj). Suppose the assumption u Lq (u, vj) applies to a particular pair of tokens (uj, vj) = 1, where q = d / (d + 1)."}, {"heading": "3.5 Extension to Tree Structures", "text": "Now we try to extend theorem 2 to the string-to-tree grammar. First, we define a previous q distribution to tree segmentation. We assign a Bernoulli variable to each tree node, which represents the probability that we will separate the tree at that node, i.e., with probabilities of 1 \u2212 q we choose to separate each node. Let (uj, vj) be a string-tree pair, where uj is a source string and vj is a target tree. Let tj be the number of words in uj, and let nj be the number of non-terminals in uj, where tj + nj \u2264 d and \u2211 tj is the length of the input set, | x |. Thus, the probability that an occurrence of (uj, vj) in D is tokenized exactly as qtj \u2212 1 (1 \u2212 q) nj \u2212 vj + 1.j is the length of the input set, with methods similar to vj that we can show that strings in D are better structured than vj)."}, {"heading": "4. Discussion", "text": "In this section, we will focus on three facts that need further explanation."}, {"heading": "4.1 On the Use of Assumption 2", "text": "In the proofs of Lemmas 4 and 5, assumption 2 is used only in the very last steps. Therefore, we could establish the upper and lower boundaries of the ratio without assumption 2 by linking inequalities (12) and (13) in appendices A and B, respectively."}, {"heading": "4.2 On the Ensemble Probability", "text": "The ensemble probability in (4) can be seen as a simplification of a Bayesian model in (9). L (uj, vj) = Pr (uj, vj | D) = \u2211 G (D) Pr (uj, vj | G) Pr (G | D) (9) In (9) we marginalize all possible symbolic grammars G from D, G (D). Furthermore, we approximate the posterior probability of G with point estimation. Therefore, Pr (G | D, s) = 1 if and only if G is the MLE grammar of D, which means that the entire mass of distribution is assigned to GD, the MLE grammar for D and s. We also assume that s is independent of D. Thus, Pr (G | D) = 1 if G is the MLE grammar of D, which means that the entire mass of distribution is assigned to GD, the LE grammar for MLE-D."}, {"heading": "4.3 On the DOP Model", "text": "The EPL method examined in this article may be based on Data Oriented Parsing (DOP) by Bod (1992). What is special about DOP is that the DOP model exhaustively uses overlapping tree trunks of different sizes as building blocks of statistical tree grammatics.Within our framework, we can use uj for each pair (uj, vj) to represent the input text, and vj to represent its tree structure, so it would be similar to the string-to-tree model in Section 3.5. The common probability of (uj, vj) stands for the unigram probability Pr (Treelet).However, the original DOP estimator (DOP1) differs significantly from our monotonic translation model. Conditional probability in DOP1 is defined as Pr (treelet | subroot-label), so there is no obvious way to model DOP1 with monotonous translation. Therefore, the theoretical justification of DOP1 is still an open problem."}, {"heading": "5. Conclusion", "text": "In this article, we formalized Exhaustive Pattern Learning (EPL), which is widely used in grammar induction in the NLP. We demonstrated that using heuristic grammar in the EPL is equivalent to using an ensemble method to deal with the uncertainty of building blocks of statistical models. A better understanding of EPL could lead to improved pattern learning algorithms in the future. This work will influence research in various areas of natural language processing, including machine translation, analysis, sequence classification, etc. EPL can also be applied to other areas of research outside the NLP."}, {"heading": "Acknowledgments", "text": "This work was inspired by an insightful discussion with Scott Miller, Rich Schwartz and Spyros Matsoukas when the author was at BBN Technologies. Reviewers of ACL 2010, CoNLL 2010, EMNLP 2010, ICML 2011, CLJ and JMLR helped sharpen the focus of this work."}, {"heading": "Appendix A. Proof for Lemma 4", "text": "Lq (uj, vj) = Es \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 1 (1 \u2212 q, s) E [1 | I s0 (I) |] {independence from mj, s and Is0 (I)} = E [mj, s] E [1 \u2212 q, s] (1 \u2212 q) (| I | \u2212 mj) (1 \u2212 q) (1 \u2212 q) (I | \u2212 mj) {Lemma 8} \u2264 E [mj, s] (1 \u2212 q) (| I s0 (I) |] = E [mj, s] (1 \u2212 q) (1 \u2212 q) (1 \u2212 q) (I | \u2212 mj) (1 \u2212 q, s) (1 \u2212 mj, s) (1 \u2212 mj) (1 \u2212 mj, s) (1 \u2212 mjq | uj) 2 (1 \u2212 d) (1 \u2212 d) (1 \u2212 d) (1 \u2212 d) (1 \u2212 d) (1 \u2212 d) (1 \u2212 d) (1 \u2212 d) (1 \u2212 d) (1 \u2212 \u2212 d) (1 \u2212 f)."}, {"heading": "Appendix B. Proof for Lemma 5", "text": "Lq (uj, vj) = Es \u2212 j \u2212 j, s | Ds |] qqq | qn. (5) | sqm | sqm (5) | sqm (I) | sqm (I) | sqm (I) | sqm (I) | sqm (I) | sqm (I) | sqm (I) = sqm (I), sqm (I), sqm (I), sqm (I), sqm (I), sqm (I), sqm (I), sqm (I), sqm (I), sqm (I), sqm (I), sqm (I), sqm (I), sqm (I), sqm (1), sqm (I), sqm (I)."}, {"heading": "Appendix C. Lemma 8 and its Proof", "text": "Lemma 8 Let X be a random variable of the binomial distribution B (n, 1 \u2212 q), then E [1X + 1] = 1 \u2212 qn + 1 (1 \u2212 q) (n + 1) E [1X + 1] = n \u2211 k = 01k + 1n! k! (n \u2212 k)! qn \u2212 k (1 \u2212 q) k = 1 (1 \u2212 q) (n + 1) n \u2211 k = 0 (n + 1)! (k + 1)! (n \u2212 k)! qn \u2212 k (1 \u2212 q) k + 1 = 1 (1 \u2212 q) (n + 1) (((q + 1 \u2212 q) n + 1 \u2212 qn + 1) = 1 \u2212 qn + 1 (1 \u2212 q) (n + 1)"}], "references": [{"title": "A gibbs sampler for phrasal synchronous grammar induction", "author": ["Phil Blunsom", "Trevor Cohn", "Chris Dyer", "Miles Osborne"], "venue": "In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Blunsom et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Blunsom et al\\.", "year": 2009}, {"title": "A computational model of language performance: Data Oriented Parsing", "author": ["Rens Bod"], "venue": "In Proc. of COLING92,", "citeRegEx": "Bod.,? \\Q1992\\E", "shortCiteRegEx": "Bod.", "year": 1992}, {"title": "Non-projective parsing for statistical machine translation", "author": ["Xavier Carreras", "Michael Collins"], "venue": "In Proceedings of the 2009 Conference of Empirical Methods in Natural Language Processing,", "citeRegEx": "Carreras and Collins.,? \\Q2009\\E", "shortCiteRegEx": "Carreras and Collins.", "year": 2009}, {"title": "A hierarchical phrase-based model for statistical machine translation", "author": ["David Chiang"], "venue": "In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Chiang.,? \\Q2005\\E", "shortCiteRegEx": "Chiang.", "year": 2005}, {"title": "A Bayesian model of syntax-directed tree to string grammar induction", "author": ["Trevor Cohn", "Phil Blunsom"], "venue": "In Proceedings of the 2009 Conference of Empirical Methods in Natural Language Processing,", "citeRegEx": "Cohn and Blunsom.,? \\Q2009\\E", "shortCiteRegEx": "Cohn and Blunsom.", "year": 2009}, {"title": "Inducing compact but accurate tree-substitution grammars", "author": ["Trevor Cohn", "Sharon Goldwater", "Phil Blunsom"], "venue": "In Proceedings of the 2009 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Cohn et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2009}, {"title": "Natural language processing blog: Teaching machine translation", "author": ["Hal Daum\u00e9 III"], "venue": null, "citeRegEx": "III.,? \\Q2008\\E", "shortCiteRegEx": "III.", "year": 2008}, {"title": "Learning as search optimization: Approximate large margin methods for structured prediction", "author": ["Hal Daum\u00e9 III", "Daniel Marcu"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "III and Marcu.,? \\Q2005\\E", "shortCiteRegEx": "III and Marcu.", "year": 2005}, {"title": "Why generative phrase models underperform surface heuristics", "author": ["John DeNero", "Dan Gillick", "James Zhang", "Dan Klein"], "venue": "In Proceedings of the Workshop on Statistical Machine Translation,", "citeRegEx": "DeNero et al\\.,? \\Q2006\\E", "shortCiteRegEx": "DeNero et al\\.", "year": 2006}, {"title": "Sampling alignment structure under a Bayesian translation model", "author": ["John DeNero", "Alexandre Bouchard-C\u00f4t\u00e9", "Dan Klein"], "venue": "In Proceedings of the 2008 Conference of Empirical Methods in Natural Language Processing,", "citeRegEx": "DeNero et al\\.,? \\Q2008\\E", "shortCiteRegEx": "DeNero et al\\.", "year": 2008}, {"title": "Scalable inference and training of context-rich syntactic models", "author": ["Michel Galley", "Jonathan Graehl", "Kevin Knight", "Daniel Marcu", "Steve DeNeefe", "Wei Wang", "Ignacio Thayer"], "venue": "COLINGACL", "citeRegEx": "Galley et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2006}, {"title": "The DOP estimation method is biased and inconsistent", "author": ["Mark Johnson"], "venue": "Computational Linguistics,", "citeRegEx": "Johnson.,? \\Q2002\\E", "shortCiteRegEx": "Johnson.", "year": 2002}, {"title": "Statistical phrase based translation", "author": ["Philipp Koehn", "Franz J. Och", "Daniel Marcu"], "venue": "In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "The alignment template approach to statistical machine translation", "author": ["Franz J. Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "Och and Ney.,? \\Q2004\\E", "shortCiteRegEx": "Och and Ney.", "year": 2004}, {"title": "Bayesian learning of a tree substitution grammar", "author": ["Matt Post", "Daniel Gildea"], "venue": "In Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Post and Gildea.,? \\Q2009\\E", "shortCiteRegEx": "Post and Gildea.", "year": 2009}, {"title": "On the statistical consistency of dop estimators", "author": ["Detlef Prescher", "Remko Scha", "Khalil Sima\u2019an", "Andreas Zollmann"], "venue": "In Proceedings of the 14th Meeting of Computational Linguistics in the Netherlands,", "citeRegEx": "Prescher et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Prescher et al\\.", "year": 2004}, {"title": "Dependency treelet translation: Syntactically informed phrasal SMT", "author": ["Chris Quirk", "Arul Menezes", "Colin Cherry"], "venue": "In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Quirk et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Quirk et al\\.", "year": 2005}, {"title": "A new string-to-dependency machine translation algorithm with a target dependency language model", "author": ["Libin Shen", "Jinxi Xu", "Ralph Weischedel"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Shen et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2008}, {"title": "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars", "author": ["Luke Zettlemoyer", "Michael Collins"], "venue": null, "citeRegEx": "Zettlemoyer and Collins.,? \\Q2005\\E", "shortCiteRegEx": "Zettlemoyer and Collins.", "year": 2005}], "referenceMentions": [{"referenceID": 1, "context": "Some exhaustive pattern learning (EPL) methods Bod (1992) were proved to be flawed Johnson (2002), while similar algorithms Och and Ney (2004) showed great advantages on other tasks, such as machine translation.", "startOffset": 47, "endOffset": 58}, {"referenceID": 1, "context": "Some exhaustive pattern learning (EPL) methods Bod (1992) were proved to be flawed Johnson (2002), while similar algorithms Och and Ney (2004) showed great advantages on other tasks, such as machine translation.", "startOffset": 47, "endOffset": 98}, {"referenceID": 1, "context": "Some exhaustive pattern learning (EPL) methods Bod (1992) were proved to be flawed Johnson (2002), while similar algorithms Och and Ney (2004) showed great advantages on other tasks, such as machine translation.", "startOffset": 47, "endOffset": 143}, {"referenceID": 5, "context": "Johnson (2002); Prescher et al.", "startOffset": 0, "endOffset": 15}, {"referenceID": 5, "context": "Johnson (2002); Prescher et al. (2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent.", "startOffset": 0, "endOffset": 39}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent.", "startOffset": 18, "endOffset": 29}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al.", "startOffset": 18, "endOffset": 271}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al.", "startOffset": 18, "endOffset": 291}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al.", "startOffset": 18, "endOffset": 306}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al.", "startOffset": 18, "endOffset": 327}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al.", "startOffset": 18, "endOffset": 349}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al. (2008); Carreras and Collins (2009), to name a few of them.", "startOffset": 18, "endOffset": 369}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al. (2008); Carreras and Collins (2009), to name a few of them.", "startOffset": 18, "endOffset": 398}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al. (2008); Carreras and Collins (2009), to name a few of them. Similar heuristic methods have also been used in many other pattern learning tasks, for example, like semantic parsing as in Zettlemoyer and Collins (2005) and chunking as in Daum\u00e9 III and Marcu (2005) in an implicit way.", "startOffset": 18, "endOffset": 578}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al. (2008); Carreras and Collins (2009), to name a few of them. Similar heuristic methods have also been used in many other pattern learning tasks, for example, like semantic parsing as in Zettlemoyer and Collins (2005) and chunking as in Daum\u00e9 III and Marcu (2005) in an implicit way.", "startOffset": 18, "endOffset": 624}, {"referenceID": 1, "context": "(2004) showed the Bod (1992)\u2019s data oriented parsing (DOP) algorithm is biased and inconsistent. In the MT field, almost all the statistical MT models proposed in recent years rely on similar heuristic methods to extract translation grammars, such as Koehn et al. (2003); Och and Ney (2004); Chiang (2005); Quirk et al. (2005); Galley et al. (2006); Shen et al. (2008); Carreras and Collins (2009), to name a few of them. Similar heuristic methods have also been used in many other pattern learning tasks, for example, like semantic parsing as in Zettlemoyer and Collins (2005) and chunking as in Daum\u00e9 III and Marcu (2005) in an implicit way. In all these heuristic algorithms, one needs to extract overlapping structures from training data in an exhaustive way. Therefore, in the article, we call them exhaustive pattern learning (EPL) methods. The use of EPL methods is intended to cope with the uncertainty of building blocks used in statistical models. As far as MT is concerned, Koehn et al. (2003) found that it was better to define a translation model on phrases than on words, but there was no obvious way to define what phrases", "startOffset": 18, "endOffset": 1005}, {"referenceID": 4, "context": "DeNero et al. (2006) observed that exhaustive pattern learning outperforms generative models with fixed building blocks.", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "DeNero et al. (2006) observed that exhaustive pattern learning outperforms generative models with fixed building blocks. In EPL algorithms, one needs to collect statistics of overlapping structures from training data, so that they are not valid generative models. Thus, the EPL algorithms for grammar induction were viewed as heuristic methods DeNero et al. (2006); Daum\u00e9 III (2008).", "startOffset": 0, "endOffset": 365}, {"referenceID": 3, "context": "(2006); Daum\u00e9 III (2008). Recently, DeNero et al.", "startOffset": 14, "endOffset": 25}, {"referenceID": 3, "context": "(2006); Daum\u00e9 III (2008). Recently, DeNero et al. (2008); Blunsom et al.", "startOffset": 14, "endOffset": 57}, {"referenceID": 0, "context": "(2008); Blunsom et al. (2009); Cohn and Blunsom (2009); Cohn et al.", "startOffset": 8, "endOffset": 30}, {"referenceID": 0, "context": "(2008); Blunsom et al. (2009); Cohn and Blunsom (2009); Cohn et al.", "startOffset": 8, "endOffset": 55}, {"referenceID": 0, "context": "(2008); Blunsom et al. (2009); Cohn and Blunsom (2009); Cohn et al. (2009); Post and Gildea (2009) investigated various sampling methods for grammar induction, which were believed to be more principled than EPL.", "startOffset": 8, "endOffset": 75}, {"referenceID": 0, "context": "(2008); Blunsom et al. (2009); Cohn and Blunsom (2009); Cohn et al. (2009); Post and Gildea (2009) investigated various sampling methods for grammar induction, which were believed to be more principled than EPL.", "startOffset": 8, "endOffset": 99}, {"referenceID": 8, "context": "Having the model probabilities fixed in this way could avoid over-fitting of the training data DeNero et al. (2006). In decoding, we search for the best hypothesis v\u0302 given training data D and input x as follows.", "startOffset": 95, "endOffset": 116}, {"referenceID": 1, "context": "3 On the DOP Model The EPL method investigated in this article may date back to Data Oriented Parsing (DOP) by Bod (1992). What is special with DOP is that the DOP model uses overlapping treelets of various sizes in an exhaustive way as building blocks of a statistical tree grammar.", "startOffset": 111, "endOffset": 122}], "year": 2013, "abstractText": "Pattern learning in an important problem in Natural Language Processing (NLP). Some exhaustive pattern learning (EPL) methods Bod (1992) were proved to be flawed Johnson (2002), while similar algorithms Och and Ney (2004) showed great advantages on other tasks, such as machine translation. In this article, we first formalize EPL, and then show that the probability given by an EPL model is constant-factor approximation of the probability given by an ensemble method that integrates exponential number of models obtained with various segmentations of the training data. This work for the first time provides theoretical justification for the widely used EPL algorithm in NLP, which was previously viewed as a flawed heuristic method. Better understanding of EPL may lead to improved pattern learning algorithms in future.", "creator": "LaTeX with hyperref package"}}}