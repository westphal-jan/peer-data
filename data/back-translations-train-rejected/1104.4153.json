{"id": "1104.4153", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Apr-2011", "title": "Learning invariant features through local space contraction", "abstract": "We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pre-training.", "histories": [["v1", "Thu, 21 Apr 2011 01:39:25 GMT  (96kb,D)", "http://arxiv.org/abs/1104.4153v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["salah rifai", "xavier muller", "xavier glorot", "gregoire mesnil", "yoshua bengio", "pascal vincent"], "accepted": false, "id": "1104.4153"}, "pdf": {"name": "1104.4153.pdf", "metadata": {"source": "CRF", "title": "Learning invariant features through local space contraction", "authors": ["Salah Rifai", "Xavier Muller", "Xavier Glorot", "Gr\u00e9goire Mesnil", "Yoshua Bengio", "Pascal Vincent"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this year it has come to the point where one sees oneself in a position to create a new world, in which people are able to create a new world, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 How to extract robust features", "text": "Most successful modern approaches to building deep networks begin by initializing each layer in turn, using a local unattended learning technique to extract potentially useful features for the next layer. If used as feature extractors in this way, both RBMs and different tastes of auto-encoders result in a non-linear feature extractor of exactly the same shape: a linear mapping followed by a sigmoid non-linearity2. From this perspective, however, these algorithms are various unattended techniques for learning the parameters of such a shape. It is not yet fully understood what properties of such mappings contribute to superior classification performance (for classifiers initialized with the features produced). It has been argued that mappings that produce sparse representation are promoted, inspiring several variants of sparse auto-encoding."}, {"heading": "3 Auto-encoders variants", "text": "In its simplest form, an auto encoder (AE) consists of two parts, an encoder and a decoder. It was introduced in the late 1980s to reconstruct the original representation of the encoder by minimizing a cost function. More specifically, if the encoder activation functions are linear and the number of hidden units at the input2This corresponds to the encoder part of the traditional auto encoder network and its regulated variants. In RBMs, the conditional expectation of the hidden layer giving the visible layer is exactly the same form.3Using the now customary additional limitations of the encoder and common (transposed) weights, which is a mere contraction of the visible layer layer."}, {"heading": "4 Contracting auto-encoders (CAE)", "text": ", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\", \",\" \",\" \",\" \",\", \",\" \",\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \"\", \",\" \",\", \",\" \",\", \",\" \",\", \"\", \",\", \"\" \",\" \",\" \",\" \",\", \",\" \",\" \"\", \"\", \"\", \"\", \",\" \",\" \",\", \"\" \",\", \"\" \"\", \",\" \",\" \"\" \",\", \"\" \",\", \"\" \",\" \"\", \"\" \",\", \"\", \"\" \"\", \",\" \",\" \",\" \"\" \",\", \",\" \"\", \",\", \"\", \",\" \"\" \"\", \",\" \"\", \",\", \"\", \"\", \"\" \",\" \",\" \"\" \"\", \"\" \",\" \"\" \",\", \",\" \"\" \"\" \",\" \",\", \",\" \",\" \"\", \"\" \"\", \"\", \"\" \",\" \"\" \",\", \"\" \",\" \",\" \",\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \",\" \",\" \"\" \",\", \"\" \"\", \"\", \",\" \"\" \"\" \"\" \",\" \"\", \"\" \""}, {"heading": "4.1 Analytical link between denoising auto-encoders and contracting auto-encoders", "text": "If the noise used in the PCS is Gaussian, its effect can be analytically approximated with an additional penalty term on a standard autocoding cost function Bishop (1995) and Kingma and LeCun (2010). So let's define L (\u03b8, x) as the loss function of the auto encoder. We can write the expected cost of our loss in such a way that we can write the expected cost of our losses in such a way that empirical costs are centered on our samples by displaying our probabilty distribution as a set of dirac functions: Cclean (\u03b8) =.L (x). Empirical costs can be written in such a way that we express our probabilty distribution as a set of dirac functions centered on our samples: Cclean (\u03b8)."}, {"heading": "5 Experiments and results", "text": "This year it's at the stage where it will be able to put itself at the top, \"he said."}, {"heading": "5.1 Classification performance", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1 MNIST and CIFAR-bw", "text": "Our first series of experiments focuses on the MNIST and CIFAR-bw datasets. We compare the classification performance obtained through a neural network with a hidden layer of 1000 units initialized with each of the observed unattended algorithms. In each case, we selected the value of the hyperparameters (such as the strength of regulation), which, after supervised fine tuning, yielded the best classification performance on the validation set. We then calculated the final classification error rate on the test set. Using the parameters obtained after unattended pre-training (before fine tuning), we also calculated the average value of the encoder contraction with Jf (x) and F on the validation set, as well as a measurement of the average fraction of saturated units per example 7. These results are presented in Table 1. We see that the local contradiction measurement (the average dating of Jf) is strongly compared to the pre-graded model of the three."}, {"heading": "5.1.2 CIFAR-10", "text": "The pipeline of pre-processing steps used here is similar?. We randomly extracted 160,000 patches 8 \u00d7 8 from the 10,000 first images of CIFAR-10. For each patch, we subtract the mean and divide by the standard deviation (local contrast normalization), then a PCA is applied to these patches. The first 2 components (corresponding to the black patches) are dropped, but we keep the next 80 first components (above 192). To build the last training set of patches, we project these patches onto the PCA components, perform whitening, i.e. divide them by eigenvalues, and perform a logistical function to map them to [0, 1]. A contracting auto-encoder with a number of hidden units, this technique is applied to these patches using HID-50, 100, 200, 400} minimizing cross-entropy reconstruction error and increasing the gradient with a stoic regulator."}, {"heading": "5.2 Closer examination of the contraction", "text": "To better understand the characteristic extractor generated by each algorithm in terms of its contractive properties, we used the following analytical tools: What happens on the spot: Let's look at the singular values of the Jacobian. A high-dimensional Jacobian contains directional information: The amount of contraction is generally not equal in all directions. This can be investigated by performing a singular value calculation of Jf. We calculated the average singular value spectrum of the Jacobian using the quantity of validation specified for the above models. The results are presented in Figure 2 and are presented in Section 5.3.What happens further away: Contraction curves. The Frobenius standard of the Jacobian measures at any point x the contraction of the mapping locally at that point. Intuitively, the contraction caused by the proposed penalty term can be measured beyond the immediate training examples by the ratio of the distances between two points in their original (input) space and their distance."}, {"heading": "5.3 Discussion: Local Space Contraction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "6 Conclusion", "text": "In this paper, we try to answer the following question: What constitutes a good representation? In addition to being useful for a specific task we can measure, or to which we can steer a representation, this paper highlights the advantages of locally invariant representations in many directions of changing the raw input. This idea is implemented by a penalty based on the Frobenius norm of the Jacobic matrix of the encoder mapping, which compresses the representation. The paper also introduces empirical measures of robustness and inventory, based on the contraction ratio of the mapping learned, at different distances and in different directions around the training examples. We suspect that this reveals the diverse structure learned by the model, and we find (by looking at the singular value spectrum of the mapping) that the contracting auto-encoder detects low-dimensional inconsistencies. Furthermore, experiments on many data sets suggest that automated encoding always contributes to a better representation of the auto-encoding or the penalty."}], "references": [{"title": "Neural networks and principal component analysis: Learning from examples without local minima", "author": ["P. Baldi", "K. Hornik"], "venue": "Neural Networks, 2, 53\u201358.", "citeRegEx": "Baldi and Hornik,? 1989", "shortCiteRegEx": "Baldi and Hornik", "year": 1989}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning , 2(1), 1\u2013127. Also published as a book. Now Publishers, 2009.", "citeRegEx": "Bengio,? 2009", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Greedy layerwise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "NIPS 19 , pages 153\u2013160. MIT Press.", "citeRegEx": "Bengio et al\\.,? 2007", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Training with noise is equivalent to Tikhonov regularization", "author": ["C.M. Bishop"], "venue": "Neural Computation, 7(1), 108\u2013116.", "citeRegEx": "Bishop,? 1995", "shortCiteRegEx": "Bishop", "year": 1995}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxe", "A. Ng"], "venue": "NIPS\u201909 , pages 646\u2013654.", "citeRegEx": "Goodfellow et al\\.,? 2009", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2009}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation, 18, 1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Estimation of non-normalized statistical models using score matching", "author": ["A. Hyv\u00e4rinen"], "venue": "Journal of Machine Learning Research, 6, 695\u2013709.", "citeRegEx": "Hyv\u00e4rinen,? 2005", "shortCiteRegEx": "Hyv\u00e4rinen", "year": 2005}, {"title": "Nonlinear autoassociation is not equivalent to PCA", "author": ["N. Japkowicz", "S.J. Hanson", "M.A. Gluck"], "venue": "Neural Computation, 12(3), 531\u2013545.", "citeRegEx": "Japkowicz et al\\.,? 2000", "shortCiteRegEx": "Japkowicz et al\\.", "year": 2000}, {"title": "What is the best multi-stage architecture for object recognition? In Proc", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "International Conference on Computer Vision (ICCV\u201909). IEEE.", "citeRegEx": "Jarrett et al\\.,? 2009", "shortCiteRegEx": "Jarrett et al\\.", "year": 2009}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "Technical report, Computational and Biological Learning Lab, Courant Institute, NYU. Tech Report CBLL-TR-2008-12-01.", "citeRegEx": "Kavukcuoglu et al\\.,? 2008", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2008}, {"title": "Learning invariant features through topographic filter maps", "author": ["K. Kavukcuoglu", "M. Ranzato", "R. Fergus", "Y. LeCun"], "venue": "Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR\u201909). IEEE.", "citeRegEx": "Kavukcuoglu et al\\.,? 2009", "shortCiteRegEx": "Kavukcuoglu et al\\.", "year": 2009}, {"title": "Regularized estimation of image statistics by score matching", "author": ["D. Kingma", "Y. LeCun"], "venue": "J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23 , pages 1126\u20131134.", "citeRegEx": "Kingma and LeCun,? 2010", "shortCiteRegEx": "Kingma and LeCun", "year": 2010}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Technical report, University of Toronto.", "citeRegEx": "Krizhevsky and Hinton,? 2009", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "An empirical evaluation of deep architectures on problems with many factors of variation", "author": ["H. Larochelle", "D. Erhan", "A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "Z. Ghahramani, editor, ICML 2007 , pages 473\u2013480. ACM.", "citeRegEx": "Larochelle et al\\.,? 2007", "shortCiteRegEx": "Larochelle et al\\.", "year": 2007}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A. Ng"], "venue": "NIPS\u201907 , pages 873\u2013880. MIT Press, Cambridge, MA.", "citeRegEx": "Lee et al\\.,? 2008", "shortCiteRegEx": "Lee et al\\.", "year": 2008}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "L. Bottou and M. Littman, editors, ICML 2009 . ACM, Montreal (Qc), Canada.", "citeRegEx": "Lee et al\\.,? 2009", "shortCiteRegEx": "Lee et al\\.", "year": 2009}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by V1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision Research,", "citeRegEx": "Olshausen and Field,? \\Q1997\\E", "shortCiteRegEx": "Olshausen and Field", "year": 1997}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "NIPS\u201906 .", "citeRegEx": "Ranzato et al\\.,? 2007", "shortCiteRegEx": "Ranzato et al\\.", "year": 2007}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323, 533\u2013536.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "Manzagol", "P.-A."], "venue": "ICML 2008 .", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A."], "venue": "JMLR, 11(3371\u20133408).", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "Deep learning via semisupervised embedding", "author": ["J. Weston", "F. Ratle", "R. Collobert"], "venue": "W. W. Cohen, A. McCallum, and S. T. Roweis, editors, ICML 2008 , pages 1168\u20131175, New York, NY, USA. ACM.", "citeRegEx": "Weston et al\\.,? 2008", "shortCiteRegEx": "Weston et al\\.", "year": 2008}, {"title": "Deconvolutional networks", "author": ["M. Zeiler", "D. Krishnan", "G. Taylor", "R. Fergus"], "venue": "Proc. of the 23rd IEEE Computer Scociety Conference on Computer Vision and Pattern Recognition (CVPR). 14", "citeRegEx": "Zeiler et al\\.,? 2010", "shortCiteRegEx": "Zeiler et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "See Bengio (2009) for a recent review of Deep Learning algorithms.", "startOffset": 4, "endOffset": 18}, {"referenceID": 2, "context": "Many experiments show that deeper models can thus yield lower classification error (Bengio et al., 2007; Jarrett et al., 2009; Vincent et al., 2008).", "startOffset": 83, "endOffset": 148}, {"referenceID": 8, "context": "Many experiments show that deeper models can thus yield lower classification error (Bengio et al., 2007; Jarrett et al., 2009; Vincent et al., 2008).", "startOffset": 83, "endOffset": 148}, {"referenceID": 19, "context": "Many experiments show that deeper models can thus yield lower classification error (Bengio et al., 2007; Jarrett et al., 2009; Vincent et al., 2008).", "startOffset": 83, "endOffset": 148}, {"referenceID": 4, "context": "Previous work has shown that deep learners can discover representations whose features are invariant to some of the factors of variation in the input (Goodfellow et al., 2009).", "startOffset": 150, "endOffset": 175}, {"referenceID": 2, "context": "remains to be done to understand the characteristics and theoretical advantages of the representations learned by a Restricted Boltzmann Machine Hinton et al. (2006), an auto-encoder Bengio et al.", "startOffset": 145, "endOffset": 166}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al.", "startOffset": 24, "endOffset": 45}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al.", "startOffset": 24, "endOffset": 87}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al.", "startOffset": 24, "endOffset": 110}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al.", "startOffset": 24, "endOffset": 137}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al.", "startOffset": 24, "endOffset": 159}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al. (2008). All of these produce a non-linear representation which, unlike that of PCA or ICA, can be stacked (composed) to yield deeper levels of representation.", "startOffset": 24, "endOffset": 210}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al. (2008). All of these produce a non-linear representation which, unlike that of PCA or ICA, can be stacked (composed) to yield deeper levels of representation. It has also been observed empirically Lee et al. (2009) that the deeper levels often capture more abstract features (such as parts of objects) defined in terms of less abstract ones (such as sub-parts of objects or low-level visual features like edges), and that these features are generally more invariant Goodfellow et al.", "startOffset": 24, "endOffset": 418}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al. (2008). All of these produce a non-linear representation which, unlike that of PCA or ICA, can be stacked (composed) to yield deeper levels of representation. It has also been observed empirically Lee et al. (2009) that the deeper levels often capture more abstract features (such as parts of objects) defined in terms of less abstract ones (such as sub-parts of objects or low-level visual features like edges), and that these features are generally more invariant Goodfellow et al. (2009) to changes in the known factors of variation in the data (such as geometric transformations in the case of images).", "startOffset": 24, "endOffset": 694}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al. (2008). All of these produce a non-linear representation which, unlike that of PCA or ICA, can be stacked (composed) to yield deeper levels of representation. It has also been observed empirically Lee et al. (2009) that the deeper levels often capture more abstract features (such as parts of objects) defined in terms of less abstract ones (such as sub-parts of objects or low-level visual features like edges), and that these features are generally more invariant Goodfellow et al. (2009) to changes in the known factors of variation in the data (such as geometric transformations in the case of images). A simple approach, used here, to empirically verify that the learned representations are useful, is to use them to initialize a classifier (such as a multi-layer neural network), and measure classification error. Many experiments show that deeper models can thus yield lower classification error (Bengio et al., 2007; Jarrett et al., 2009; Vincent et al., 2008). Contribution. What principles should guide the learning of such intermediate representations? They should capture as much as possible of the information in each given example, when that example is likely under the underlying generating distribution. That is what auto-encoders Vincent et al. (2008) and sparse coding aim to achieve when minimizing reconstruction error.", "startOffset": 24, "endOffset": 1472}, {"referenceID": 1, "context": "(2006), an auto-encoder Bengio et al. (2007), sparse coding Olshausen and Field (1997); Ranzato et al. (2007); Kavukcuoglu et al. (2009); Zeiler et al. (2010), or semi-supervised embedding Weston et al. (2008). All of these produce a non-linear representation which, unlike that of PCA or ICA, can be stacked (composed) to yield deeper levels of representation. It has also been observed empirically Lee et al. (2009) that the deeper levels often capture more abstract features (such as parts of objects) defined in terms of less abstract ones (such as sub-parts of objects or low-level visual features like edges), and that these features are generally more invariant Goodfellow et al. (2009) to changes in the known factors of variation in the data (such as geometric transformations in the case of images). A simple approach, used here, to empirically verify that the learned representations are useful, is to use them to initialize a classifier (such as a multi-layer neural network), and measure classification error. Many experiments show that deeper models can thus yield lower classification error (Bengio et al., 2007; Jarrett et al., 2009; Vincent et al., 2008). Contribution. What principles should guide the learning of such intermediate representations? They should capture as much as possible of the information in each given example, when that example is likely under the underlying generating distribution. That is what auto-encoders Vincent et al. (2008) and sparse coding aim to achieve when minimizing reconstruction error. We would also like these representations to be useful in characterizing the input distribution, and that is what is achieved by directly optimizing a generative model\u2019s likelihood (such as RBMs), or a proxy, such as Score Matching Hyv\u00e4rinen (2005). In this paper, we introduce a penalty term that could be added to either of the above contexts, which encourages the intermediate representation to be robust to small changes of the input around the training examples.", "startOffset": 24, "endOffset": 1791}, {"referenceID": 17, "context": "It was introduced in the late 80\u2019s Rumelhart et al. (1986); Baldi and Hornik (1989) as a technique for dimensionality reduction, where the output of the encoder represents the reduced representation and where the decoder is tuned to reconstruct the initial input from the encoder\u2019s representation through the minimization of a cost function.", "startOffset": 35, "endOffset": 59}, {"referenceID": 0, "context": "(1986); Baldi and Hornik (1989) as a technique for dimensionality reduction, where the output of the encoder represents the reduced representation and where the decoder is tuned to reconstruct the initial input from the encoder\u2019s representation through the minimization of a cost function.", "startOffset": 8, "endOffset": 32}, {"referenceID": 7, "context": "With the use of non-linear activation functions an AE can however be expected to learn more useful feature-detectors than what can be obtained with a simple PCA (Japkowicz et al., 2000).", "startOffset": 161, "endOffset": 185}, {"referenceID": 0, "context": "dimension (hence forming a bottleneck), it has been shown that the learnt parameters of the encoder are a subspace of the principal components of the input space Baldi and Hornik (1989). With the use of non-linear activation functions an AE can however be expected to learn more useful feature-detectors than what can be obtained with a simple PCA (Japkowicz et al.", "startOffset": 162, "endOffset": 186}, {"referenceID": 9, "context": "this viewpoint, several techniques have been developed to encourage sparsity of the representation (Kavukcuoglu et al., 2008; Lee et al., 2008).", "startOffset": 99, "endOffset": 143}, {"referenceID": 14, "context": "this viewpoint, several techniques have been developed to encourage sparsity of the representation (Kavukcuoglu et al., 2008; Lee et al., 2008).", "startOffset": 99, "endOffset": 143}, {"referenceID": 19, "context": "Robustness to input perturbations was also one of the motivation of the denoising auto-encoder, as stated in Vincent et al. (2010). The CAE and the DAE differ however in the following ways:", "startOffset": 109, "endOffset": 131}, {"referenceID": 3, "context": "If the noise used in the DAE is Gaussian, its effect can be approximated analytically with an added penality term on a standard autoencoder cost functionBishop (1995) and Kingma and LeCun (2010).", "startOffset": 153, "endOffset": 167}, {"referenceID": 3, "context": "If the noise used in the DAE is Gaussian, its effect can be approximated analytically with an added penality term on a standard autoencoder cost functionBishop (1995) and Kingma and LeCun (2010). Let us define L(\u03b8, x) as the loss function of the auto-encoder.", "startOffset": 153, "endOffset": 195}, {"referenceID": 12, "context": "We have tested our approach on a benchmark of image classification problems, namely: \u2022 CIFAR-10: the image-classification task (32 \u00d7 32 \u00d7 3 channels RGB) (Krizhevsky and Hinton, 2009).", "startOffset": 154, "endOffset": 183}, {"referenceID": 12, "context": "We have tested our approach on a benchmark of image classification problems, namely: \u2022 CIFAR-10: the image-classification task (32 \u00d7 32 \u00d7 3 channels RGB) (Krizhevsky and Hinton, 2009). \u2022 CIFAR-bw: a gray-scale version of the original CIFAR-10. The gray-scale versions were obtained with a color weighting of 0.3 for red, 0.59 for green and 0.11 for blue. \u2022 MNIST: the well-known digit classification problem (28 \u00d7 28 gray-scale pixel values scaled to [0,1]). It has 50000 examples for training, 10000 for validation, and 10000 for test. Six harder digit recognition problems used in the benchmark of Larochelle et al. (2007). They were derived by adding extra factors of variation to MNIST digits.", "startOffset": 155, "endOffset": 625}, {"referenceID": 13, "context": "Two artificial shape classification problems from the benchmark of Larochelle et al. (2007): \u2022 rect: Discriminate between tall and wide rectangles (white on black).", "startOffset": 67, "endOffset": 92}, {"referenceID": 13, "context": "Results given in Table 2 compare the performance of stacked CAEs on the benchmark problems of Larochelle et al. (2007) to the three-layer models reported in Vincent et al.", "startOffset": 94, "endOffset": 119}, {"referenceID": 13, "context": "Results given in Table 2 compare the performance of stacked CAEs on the benchmark problems of Larochelle et al. (2007) to the three-layer models reported in Vincent et al. (2010). Stacking a second layer CAE on top of a first layer appears to significantly improves performance, thus demonstrating their usefulness for building deep networks.", "startOffset": 94, "endOffset": 179}], "year": 2011, "abstractText": "We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pre-training.", "creator": "LaTeX with hyperref package"}}}