{"id": "1104.5059", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Apr-2011", "title": "Reducing Commitment to Tasks with Off-Policy Hierarchical Reinforcement Learning", "abstract": "In experimenting with off-policy temporal difference (TD) methods in hierarchical reinforcement learning (HRL) systems, we have observed unwanted on-policy learning under reproducible conditions. Here we present modifications to several TD methods that prevent unintentional on-policy learning from occurring. These modifications create a tension between exploration and learning. Traditional TD methods require commitment to finishing subtasks without exploration in order to update Q-values for early actions with high probability. One-step intra-option learning and temporal second difference traces (TSDT) do not suffer from this limitation. We demonstrate that our HRL system is efficient without commitment to completion of subtasks in a cliff-walking domain, contrary to a widespread claim in the literature that it is critical for efficiency of learning. Furthermore, decreasing commitment as exploration progresses is shown to improve both online performance and the resultant policy in the taxicab domain, opening a new avenue for research into when it is more beneficial to continue with the current subtask or to replan.", "histories": [["v1", "Wed, 27 Apr 2011 00:58:52 GMT  (283kb,S)", "http://arxiv.org/abs/1104.5059v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mitchell keith bloch"], "accepted": false, "id": "1104.5059"}, "pdf": {"name": "1104.5059.pdf", "metadata": {"source": "META", "title": "Reducing Commitment to Tasks with Off-Policy Hierarchical Reinforcement Learning", "authors": ["Mitchell Keith Bloch"], "emails": ["bazald@umich.edu"], "sections": [{"heading": null, "text": "ar Xiv: 110 4.50 59v1 [cs.LG] 2 7A pr2 01"}, {"heading": "1 Introduction and Background", "text": "Hierarchical Reinforcement Learning (HRL) is an established solution to attacking the curse of dimensionality. Second, there is the possibility of increasing state abstraction at any point in the decision-making process. Third, available measures can be ignored in some sub-problems, thereby reducing the complexity of learning individual decisions. Finally, an agent can divide sub-tasks between parts of a problem, allowing an agent to exploit regularities in behavior demanded by the environment. Non-political time differences (TD) allow an agent to learn about a policy that is different from the one being pursued. This allows an agent to reliably learn from a wider variety of exploration strategies. We have observed that non-political TD methods can lead to unintended policy updates in the context of HRL. An example is discussed in detail in Section 2."}, {"heading": "1.1 Hierarchical Reinforcement Learning", "text": "Here we briefly discuss Hierarchical Semi-Markov Q-Learning (HSMQ), a close relative of the HRL system Q presented in Section 3; updating all objectives, a technique to improve the efficiency of learning; and non-hierarchical or survey hierarchical execution of tasks, a technique to improve the quality of a learned hierarchical policy. The design of HSMQ (Dietterich 2000b) keeps the objectives of the tasks truly independent and sacrifices guarantees for achieving hierarchical optimality. Each task is only about achieving its goal as efficiently as possible, assuming an episodic structure for all tasks. Rewards that occur after a task do not affect learning within the task. HSMQ instead guarantees convergence to a recursively optimal policy, meaning that the hierarchy can be converted to a policy that is best in view of the constraints imposed by the hierarchy. Kaelbling (1993) an actualized knowledge that improves all objectives of an agent."}, {"heading": "2 Difficulty Learning Off-Policy", "text": "It is not easy to develop a flat agent to learn the domain as shown in Figure 1, but developing a hierarchy should not create additional difficulties if it turns out that the trivial hierarchical affirmation of learning (HRL) agent shown in Figure 2 causes problems if the task is presented as a flat agent to learn the domain, but developing a hierarchy should not cause additional difficulties. However, it turns out that the trivial hierarchical affirmation of learning (HRL) agent shown in Figure 2 causes problems for traditional off-policy difference methods. The agent shown in Figure 1 simply selects action A, B, or C and then finishes. Learning with a fixed exploration strategy (for example epsilon-greedy) and selecting a non-greedy action 10% of the time is guaranteed to adapt the optimal hierarchy to the optimal policy."}, {"heading": "3 Off-Policy Hierarchical Reinforcement Learning (OPHRL)", "text": "The execution of agent learning with OPHRL, as described in Algorithm 1, takes the form of a traditional hierarchical reinforcement learning algorithm, but does not perform hierarchically or selectively during learning. At each step of execution, there is no obligation at any level of the hierarchy to continue the same sub-task performed in the previous step. As a result, single-step intra-option learning and time traces of the second difference (TSDT) result in certain advantages over Watkins \"Q (\u03bb) as in Section 2. OPHRL also supports arbitrary reject and transformation functions per task, allowing a solution of the hierarchical credit allocation problem. OPHRL will approach the true value function for a task, regardless of whether exploration is diminished at all, as long as the exploration policy is not hungry for all governmental shareholder pairs."}, {"heading": "3.1 Not Committing to Tasks", "text": "It has even been suggested that the obligation to complete all tasks an agent selects is necessary to provide hierarchical reinforcement learning to provide advantages over flat reinforcement learning (Ryan 2004a). However, it is worth doing OPHRL 1 Official Hierarchical Reinforcement Tasks (OPHRL) if OPHRL (root) rejects every step and TransformReward tasks are task specific. Ensure: Q initializes tasks arbitrarily, e.g. Qi (s, a) exploitative tasks i \u2212 s tasks i, s tasks i, s tasks i-i, a functions OPHRHRL (task i) 1: function OPHRL (task i) 2: serve s 3: select one of Ai (s) {not staring} 4: if isTask i (a)."}, {"heading": "3.2 Credit Assignment Problem", "text": "There are cases where rewards do not apply to certain levels of hierarchy. It could be that a sub-task has not learned that certain actions are legal only in a subset of state space. It could be that an over-task has been wrongly planned and a high negative reward is obtained through no fault of the given task. Dietterich (1998) dealt with the hierarchical problem of credit allocation by transforming the reward function. However, these rewards must be flatly rejected when using single-step intra-option learning or TSDT. A 0 reward does not affect the sum calculated at the end of a task, but it can cause instability when using TD methods that are updated in an immediate local fashion. It is still useful to apply a transformation to the reward before making a backup for a given task. It is possible to rule out some cases where recursive optimality does not lead to a hierarchical one, such as in 200a."}, {"heading": "3.3 Gated Temporal Second Difference Traces", "text": "As described in Section 2, it is important to avoid rewards from exploration when trying to learn outside politics. As it turns out, TSDT is a method of time difference that is almost ideally suited to operating under this constraint. However, it can benefit from further modifications. A key observation that gives TSDT its power is that actions that appear explorative when performed may later turn out to be the best choice. A non-greedy choice may later turn out to be suboptimal, enabling the flow of information to be turned on. The same problem occurs when detecting exploration in subtasks. In fact, TSDT suffers from the problem that an action that appears optimal in a subtask may later turn out to be suboptimal. An entry may persist in the track after it turns out that the subtask has been explored at that time. This problem disappears when subtasks converge to their optimal value functions, but this could pose a serious problem for non-episodic or partial tasks."}, {"heading": "4 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Cliff-Walking", "text": "We are investigating a 100x2 cliff walking domain - a longer version of the domain shown in Figure 4. Four deterministic pull actions can be attempted from any of the 199 non-terminal states. All actions result in a reward of \u2212 1 with the exception of terminal actions, which bring 200 for success and \u2212 200 for failure. Agent A flat reinforcement learning agent simply decides between the four pull actions from each state. We have constructed a hierarchical agent, represented in Figure 5, who chooses between a sub-task that attempts to solve the traditional cliff walking task by getting to the lower right corner, and a sub-task that attempts to finish by jumping off the cliff. Given the terminal rewards of 200 and \u2212 200, the solution of the traditional task is always preferred by an optimal policy. The problem then is that the agent efficiently learns to solve both the traditional cliff walking task and that solving the traditional cliff walking task is always better than the jump task."}, {"heading": "4.2 The Taxicab Domain", "text": "In the taxi center (Dietterich 1998), an agent is assigned the task of picking up a passenger and getting him to his destination in as few steps as possible. The environment is a 5x5 network world. There are four cells that serve as possible starting points and possible destinations for the passenger. There is a gas station near the center of the map. In addition, there are six impassable walls (or 26 that enclose the walls of the map). There are seven actions that are always available to an agent. Trying to move north automatically causes the taxi to move a cell in that direction, provided there is no wall in the way the action is moved and the taxi remains in place."}, {"heading": "5 Discussion and Future Directions", "text": "Solutions for Q-Learning and Watkins \"Q (\u03bb) are not ideal and require the abandonment of potential learning, but single-level intra-option learning and time traces of the second difference solve the problem more gracefully. We have shown that it is possible for hierarchical reinforcement learning systems to learn efficiently and without obligation, contrary to the literature's assertion that engagement should be crucial for efficient learning. Furthermore, we have shown that engagement reduction can actually contribute to faster learning of time difference methods. The approach we examined for engagement reduction is somewhat ad hoc. It would be interesting to examine more complex exploration strategies that can determine whether engagement is valuable or not at a particular point in time, rather than assuming that engagement is most valuable at the beginning of exploration."}, {"heading": "Acknowledgments", "text": "I would like to thank Professor John Laird and the University of Michigan for their support. I would like to thank the Soar Group, which includes Professor John Laird, Jon Voigt, Nate Derbinsky, Nick Gorski, Justin Li, Bob Marinier, Shiwali Mohan, Miller Tinkerhess, Yongjia Wang, Sam Wintermute, Joseph Xu and Mark Yong, who have helped me refine the presentation of these ideas."}], "references": [{"title": "Temporal second difference traces", "author": ["M.K. Bloch"], "venue": "arXiv:cs.LG/1104.4664.", "citeRegEx": "Bloch,? 2011", "shortCiteRegEx": "Bloch", "year": 2011}, {"title": "The MAXQ method for hierarchical reinforcement learning", "author": ["T.G. Dietterich"], "venue": "ICML, 118\u2013126.", "citeRegEx": "Dietterich,? 1998", "shortCiteRegEx": "Dietterich", "year": 1998}, {"title": "Hierarchical reinforcement learning with the MAXQ value function decomposition", "author": ["T.G. Dietterich"], "venue": "J. Artif. Intell. Res. (JAIR) 13:227\u2013303.", "citeRegEx": "Dietterich,? 2000a", "shortCiteRegEx": "Dietterich", "year": 2000}, {"title": "An overview of MAXQ hierarchical reinforcement learning", "author": ["T.G. Dietterich"], "venue": "SARA, 26\u201344.", "citeRegEx": "Dietterich,? 2000b", "shortCiteRegEx": "Dietterich", "year": 2000}, {"title": "Learning to achieve goals", "author": ["L.P. Kaelbling"], "venue": "IJCAI, 1094\u20131099.", "citeRegEx": "Kaelbling,? 1993", "shortCiteRegEx": "Kaelbling", "year": 1993}, {"title": "Handbook of Learning and Approximate Dynamic Programming", "author": ["M. Ryan"], "venue": "Series on Computational Intelligence. IEEE Press. chapter 8, Hierarchical Decision Making. Edited by Jennie Si, Andrew G. Barto, Warren B. Powell, and Donald Wunsch II.", "citeRegEx": "Ryan,? 2004a", "shortCiteRegEx": "Ryan", "year": 2004}, {"title": "Hierarchical reinforcement learning: a hybrid approach", "author": ["M.R.K. Ryan"], "venue": "Ph.D. Dissertation, New South Wales, Australia, Australia. Supervisor-Sammut, Claude.", "citeRegEx": "Ryan,? 2004b", "shortCiteRegEx": "Ryan", "year": 2004}, {"title": "Intra-option learning about temporally abstract actions", "author": ["R.S. Sutton", "D. Precup"], "venue": "Proceedings of the Fifteenth International Conference on Machine Learning, 556\u2013 564. Morgan Kaufman.", "citeRegEx": "Sutton and Precup,? 1998", "shortCiteRegEx": "Sutton and Precup", "year": 1998}, {"title": "Improved switching among temporally abstract actions", "author": ["R.S. Sutton", "S.P. Singh", "D. Precup", "B. Ravindran"], "venue": "NIPS, 1066\u20131072.", "citeRegEx": "Sutton et al\\.,? 1998", "shortCiteRegEx": "Sutton et al\\.", "year": 1998}, {"title": "Between MDPs and Semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S.P. Singh"], "venue": "Artif. Intell. 112(1-2):181\u2013 211.", "citeRegEx": "Sutton et al\\.,? 1999", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}], "referenceMentions": [{"referenceID": 4, "context": "In exploring solutions to this new problem, we challenge the widespread claim that committing to completion of tasks is a critical aspect of what gives HRL an advantage over flat reinforcement learning (Kaelbling 1993; Dietterich 1998; 2000a; Ryan 2004b; 2004a).", "startOffset": 202, "endOffset": 261}, {"referenceID": 1, "context": "In exploring solutions to this new problem, we challenge the widespread claim that committing to completion of tasks is a critical aspect of what gives HRL an advantage over flat reinforcement learning (Kaelbling 1993; Dietterich 1998; 2000a; Ryan 2004b; 2004a).", "startOffset": 202, "endOffset": 261}, {"referenceID": 6, "context": "In exploring solutions to this new problem, we challenge the widespread claim that committing to completion of tasks is a critical aspect of what gives HRL an advantage over flat reinforcement learning (Kaelbling 1993; Dietterich 1998; 2000a; Ryan 2004b; 2004a).", "startOffset": 202, "endOffset": 261}, {"referenceID": 0, "context": "Using corrected TD methods, and particularly a gated version of temporal second difference traces (TSDT) (Bloch 2011), we demonstrate that it is possible to learn efficiently with no commitment to completing tasks.", "startOffset": 105, "endOffset": 117}, {"referenceID": 3, "context": "The design of HSMQ (Dietterich 2000b) keeps the goals of tasks truly independent, sacrificing guarantees of achieving hierarchical optimality.", "startOffset": 19, "endOffset": 37}, {"referenceID": 1, "context": "Dietterich (1998) introduced a subset of all-goals updating, all-states updating, in which only goals that the agent is trying to achieve are updated.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "Both Kaelbling (1993) and Dietterich (1998) discuss non-hierarchical or polling execution of tasks.", "startOffset": 5, "endOffset": 22}, {"referenceID": 1, "context": "Both Kaelbling (1993) and Dietterich (1998) discuss non-hierarchical or polling execution of tasks.", "startOffset": 26, "endOffset": 44}, {"referenceID": 1, "context": "Both Kaelbling (1993) and Dietterich (1998) discuss non-hierarchical or polling execution of tasks. However, they describe them as techniques for improving the performance of learned policies. Actually learning while executing non-hierarchically is not permitted. We demonstrate a system capable of learning while executing non-hierarchically in section 3. Sutton et al. (1998) presents a simple proof for why executing non-hierarchically must result in a policy that is at least as good as the original.", "startOffset": 26, "endOffset": 378}, {"referenceID": 7, "context": "One-Step Intra-Option Learning Sutton and Precup (Sutton and Precup 1998; Sutton, Precup, and Singh 1999) introduced one-step intra-option learning.", "startOffset": 49, "endOffset": 105}, {"referenceID": 0, "context": "Temporal Second Difference Traces Bloch (2011) introduced temporal second difference traces (TSDT), an alternative to Watkins\u2019 Q(\u03bb) with a number of advantages.", "startOffset": 34, "endOffset": 47}, {"referenceID": 1, "context": "Taking big steps of exploration has been cited as a significant advantage of hierarchical reinforcement learning (Dietterich 1998).", "startOffset": 113, "endOffset": 130}, {"referenceID": 5, "context": "It has even been suggested that committing to completing all tasks that an agent chooses to begin is necessary for hierarchical reinforcement learning to offer advantages over flat reinforcement learning (Ryan 2004a).", "startOffset": 204, "endOffset": 216}, {"referenceID": 5, "context": "noting that Ryan (2004a) acknowledges that it is conceivable that an algorithm without a requirement of commitment could be developed.", "startOffset": 12, "endOffset": 25}, {"referenceID": 1, "context": "Dietterich (1998) addressed the hierarchical credit assignment problem by transforming the reward function.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": "One can eliminate some cases where recursive optimality does not imply hierarchical optimality, as outlined in Dietterich (2000a). Additionally, by increasing the reward for successful termination, it is possible to allow a greedy policy to guarantee convergence to an optimal policy in some cases in which it may otherwise get stuck in a local minimum.", "startOffset": 111, "endOffset": 130}, {"referenceID": 5, "context": "Additionally, as identified by Ryan (2004b), hierarchies can be constructed such that a subset of the state space is never explored within a given task if supertasks are never acceptable in those states.", "startOffset": 31, "endOffset": 44}, {"referenceID": 1, "context": "In the taxicab domain (Dietterich 1998), an agent is tasked with the problem of picking up a passenger and delivering him to his destination in as few steps as possible.", "startOffset": 22, "endOffset": 39}, {"referenceID": 1, "context": "Agents The hierarchy depicted in figure 8 is used with a few modifications from the version for HSMQ/MAXQ (Dietterich 1998).", "startOffset": 106, "endOffset": 123}, {"referenceID": 1, "context": "Figure 8: Hierarchical agent for the taxicab domain, as described in (Dietterich 1998).", "startOffset": 69, "endOffset": 86}], "year": 2014, "abstractText": "In experimenting with off-policy temporal difference (TD) methods in hierarchical reinforcement learning (HRL) systems, we have observed unwanted on-policy learning under reproducible conditions. Here we present modifications to several TD methods that prevent unintentional on-policy learning from occurring. These modifications create a tension between exploration and learning. Traditional TD methods require commitment to finishing subtasks without exploration in order to update Q-values for early actions with high probability. One-step intra-option learning and temporal second difference traces (TSDT) do not suffer from this limitation. We demonstrate that our HRL system is efficient without commitment to completion of subtasks in a cliff-walking domain, contrary to a widespread claim in the literature that it is critical for efficiency of learning. Furthermore, decreasing commitment as exploration progresses is shown to improve both online performance and the resultant policy in the taxicab domain, opening a new avenue for research into when it is more beneficial to continue with the current subtask or to replan.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}