{"id": "1105.0857", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-May-2011", "title": "Domain Adaptation: Overfitting and Small Sample Statistics", "abstract": "We study the prevalent problem when a test distribution differs from the training distribution. We consider a setting where our training set consists of a small number of sample domains, but where we have many samples in each domain. Our goal is to generalize to a new domain. For example, we may want to learn a similarity function using only certain classes of objects, but we desire that this similarity function be applicable to object classes not present in our training sample (e.g. we might seek to learn that \"dogs are similar to dogs\" even though images of dogs were absent from our training set). Our theoretical analysis shows that we can select many more features than domains while avoiding overfitting by utilizing data-dependent variance properties. We present a greedy feature selection algorithm based on using T-statistics. Our experiments validate this theory showing that our T-statistic based greedy feature selection is more robust at avoiding overfitting than the classical greedy procedure.", "histories": [["v1", "Wed, 4 May 2011 15:50:44 GMT  (294kb)", "http://arxiv.org/abs/1105.0857v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dean foster", "sham kakade", "ruslan salakhutdinov"], "accepted": false, "id": "1105.0857"}, "pdf": {"name": "1105.0857.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["foster@wharton.upenn.edu", "skakade@wharton.upenn.edu", "rsalakhu@mit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 110 5.08 57v1 [cs.LG]"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Setting", "text": "A key idea in our setting is that we consider a distribution over domains that we call Pr [D] (it is possible that there may be an infinite number of domains). Condition for a domain D = d is that the distribution over input / output pairs is Pr [(X, Y) | D = d], where our inputs are X-Rp. As usual, these inputs could represent a high-dimensional attribute space. The goal is to find a weight vector that minimizes the square error averaged across both instances and domains. Specifically, the error we want to minimize is: L (w) = EDEX, Y [(Y \u2212 w \u00b7 X) 2 | D], where the inner and outer expectations are above (X, Y) and D, accordingly."}, {"heading": "3 Feature Selection and Small Sample Statistics", "text": "The na\u00efve, greedy method is to add traits that maximize our training set error, which, as we showed in the introduction, can perform remarkably poorly. Instead, we provide a theory that, when we add a trait, actually improves our performance."}, {"heading": "3.1 Adding a Single Feature", "text": "\"We first examine the question of whether a single trait improves the zero prediction of a repetitive 0.\" It is natural to base our theory on unbiased estimates, since we often have the most robust statistical tests for these estimates. Consider a trait Xi normalized to E [X2i] = 1. However, the optimal weight on this trait is that our task is to find a trait that defines Xi and the weight vector in such a way that we have confidence that we are closer to E [XiY] than 0 (weight 0 is the zero prediction). The natural unbiased estimate for w \"i\" is simple: \"XiY\" i = 1nn. \""}, {"heading": "3.2 Subset Selection", "text": "We are merely looking for the lowest error solution for all subsets of, say, size q is susceptible to overmatch q. Instead, we are trying to take empirical variance into account when searching for subsets of characteristics. We are now providing a data-dependent boundary that shows that empirical variance can be used for a much sharper boundary. (In the next subsection, we will discuss a greedy method for this search.) Given some characteristics S of size q, let X-q be an orthonormal basis for this subspace (e.g. E [X-iX-j] is 0 if i-j and 1 if i-j. Note that we can put S into this foundation because we have assumed knowledge of E [XX-iq]. Again, the best weight vector for this subspace is covariance [\u00b5S] i = E [X-iY]."}, {"heading": "3.3 Practice: The T-Greedy Algorithm", "text": "In practice, the natural method is to \"greedily\" select a feature, instead of looking for all the subsets that normally consist of finding the feature that reduces the error most. Instead, we introduce the \"greedy\" algorithm, a \"stage-by-stage\" method of adding the feature that has the highest T statistic (e.g., the goal is to add a feature in which we have the most confidence that the true error will be improved); there are a variety of greedy regression procedures, such as \"incremental,\" \"staggered,\" etc. We now present a stage-by-by-taking into account covariances with our residual error (Y \u2212 w \u00b7 X); suppose that our current weight vector is w (on our current feature list); for each Xi attribute, we calculate the empirical mean; and the variance: \"we\" i = 1nn \"is the model k = 1E.\""}, {"heading": "4 Experimental Results", "text": "The MNIST dataset contains 60,000 training images and 10,000 test images of ten handwritten digits (0 to 9) with 28 x 28 pixels. In all experiments, we used 10,000 digits (1,000 per class) for training and 10,000 digits for the test. Instead of using raw pixel values, each image was represented by 2,000 real properties extracted using a deep faith network [13]. As with the MNIST dataset, we used 10,000 images (1,000 per class) for training and 10,000 images for the test. Each image was also represented by 2,000 real properties extracted using a deep faith network. As with the MNIST dataset, we used 10,000 images (1,000 per class) for the test and 10,000 images for the test. Each image was represented by 2,000 real properties extracted using a deep faith network [18]."}, {"heading": "4.1 MNIST (2 vs. other)", "text": "In our first experiment, which is shown as the leftmost representation in Fig. 2, we tested the ability of the proposed solid graph to generalize to a new target domain: the detection of the digit \"2\" versus the new, previously invisible digit \"9.\" To this end, we created eight source domains: {\"2\" vs \"0\"},..., {\"2 vs\" 8 \"}, where each domain contained a balanced set of 2000 designated training domains - 2.\" Our new target domain (as a test set) {\"2\" vs \"9\"} also contained a balanced set of 2000 readings. 2Remember that our key assumption is that the sampled domains are independent and that the source domains are known.Figure 2, the most pronounced graph, shows an evolution of the range under ROC (AUROC), which is both for greedy (red curves) and for blue curves (T) algorithms."}, {"heading": "4.2 Learning similarity function", "text": "We are now looking at a more challenging task of learning a similarity function between two images. A good similarity function can give an insight into how high-dimensional data is organized and how to significantly improve the performance of many machine learning algorithms based on arithmetic similarity metrics. Our goal is to learn a similarity function that not only works well for objects that are part of the training set, but also works well for new objects that we may never have seen before: a widely studied problem known as \"zero-shot\" learning. Figure 3 shows that the generalization error of the greedy algorithm differs rapidly on the source and target domains, while the T greedy is able to select up to 25 reliable features that help us generalize well to the new target domain. Figure 3 shows more performance results when generalizing a comparison function to different target domains occurs."}, {"heading": "5 Discussion", "text": "All the experiments show that the T-greedy algorithm has a better match between the training AUROC and testing AUROC. Curves begin with training and testing AUROC curves with approximately the same value. This is particularly noticeable in the averaged curves shown in Fig. 4. Even if you only look at the training curves, you can get a good estimate of the generalization performance. As expected, there will eventually be agreement as the training AUROC continues to improve while testing AUROC decreases, but even then it is possible to get a grip on our method (e.g. when we need to stop). One option is simply to keep another domain for cross-validation and go through it. Alternatively, we can use properties of T statistics to get a handle when we stop (e.g. when the T statistics behave like a coincidence)."}, {"heading": "A Appendix", "text": "Let's start with Theorem 2 (also: Theorem 2.15 in [10] q = 1 = 1 = 1 = 1 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2: 2 = 2 = 2 = 2 = 2 = 2: 2 = 2 = 2 = 2 = 2 = 2 = 2: 2 - 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2: 2 = 2 = 2 = 2 = 2 = 2: 2 = 2 = 2 = 2 = 2: 2 - 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2: 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 - = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = - = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = - = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = 2 = = 2 ="}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "We study the prevalent problem when a test distribution differs from the training dis-<lb>tribution. We consider a setting where our training set consists of a small number of<lb>sample domains, but where we have many samples in each domain. Our goal is to<lb>generalize to a new domain. For example, we may want to learn a similarity function<lb>using only certain classes of objects, but we desire that this similarity function be<lb>applicable to object classes not present in our training sample (e.g. we might seek to<lb>learn that \u201cdogs are similar to dogs\u201d even though images of dogs were absent from our<lb>training set). Our theoretical analysis shows that we can select many more features<lb>than domains while avoiding overfitting by utilizing data-dependent variance proper-<lb>ties. We present a greedy feature selection algorithm based on using T -statistics. Our<lb>experiments validate this theory showing that our T -statistic based greedy feature<lb>selection is more robust at avoiding overfitting than the classical greedy procedure.", "creator": "LaTeX with hyperref package"}}}