{"id": "1105.0974", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-May-2011", "title": "GANC: Greedy Agglomerative Normalized Cut", "abstract": "This paper describes a graph clustering algorithm that aims to minimize the normalized cut criterion and has a model order selection procedure. The performance of the proposed algorithm is comparable to spectral approaches in terms of minimizing normalized cut. However, unlike spectral approaches, the proposed algorithm scales to graphs with millions of nodes and edges. The algorithm consists of three components that are processed sequentially: a greedy agglomerative hierarchical clustering procedure, model order selection, and a local refinement.", "histories": [["v1", "Thu, 5 May 2011 04:55:53 GMT  (659kb)", "http://arxiv.org/abs/1105.0974v1", "Submitted to Pattern Recognition. 27 pages, 5 figures"]], "COMMENTS": "Submitted to Pattern Recognition. 27 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["seyed salim tabatabaei", "mark coates", "michael rabbat"], "accepted": false, "id": "1105.0974"}, "pdf": {"name": "1105.0974.pdf", "metadata": {"source": "CRF", "title": "GANC: Greedy Agglomerative Normalized Cut", "authors": ["Seyed Salim Tabatabaei", "Michael Rabbat"], "emails": ["seyed.s.tabatabaei@mail.mcgill.ca.", "mark.coates@mcgill.ca", "michael.rabbat@mcgill.ca"], "sections": [{"heading": null, "text": "ar Xiv: 110 5.09 74v1 [cs.AI] 5M ay2 011This paper describes a graph cluster algorithm that aims to minimize the normalized intersection criterion and has a method for selecting the model sequence. The performance of the proposed algorithm is comparable to spectral approaches in terms of minimizing the normalized intersection. However, unlike spectral approaches, the proposed algorithm scales to graphs with millions of nodes and edges. The algorithm consists of three components that are processed sequentially: a greedy agglomerative hierarchical cluster method, the selection of the model sequence and local refinement. For a graph of n-nodes and O (n) edges, the computer-aided complexity of the O (n log2 n) algorithm is a significant improvement over the O (n3) method of the complexity of the spectral methods, the scale experiments proposed on the scale, the scale experiments on the proposed networks, and the scale effectiveness of the proposed to demonstrate the:"}, {"heading": "1. Introduction", "text": "There are three components of cluster work: (i) the selection of a criterion that measures the composition of the individual candidates, and (ii) the determination of an algorithm that searches for the optimal cluster work. (i) the selection of a criterion that measures the composition of the individual candidates. (ii) the search for an optimal cluster can be used for both the selection of the number of clusters and the selection of modularity. (6) The modularity of the individual criteria is based on the minimum description length [7].There are no universal performance criteria, nor suitable criteria."}, {"heading": "2. Related Work", "text": "The identification of clusters in graphs and networks receives a lot of attention. In our review, we focus on a representative set of algorithms that minimize normalized cross-section with or without spectral decomposition, and review existing methods for cluster number selection."}, {"heading": "2.1. Optimization of Normalized Cut", "text": "Interest in the techniques was renewed in the 1990s, when Pothen et al. described an algorithm for bi-partitioning with the Fiedler vector [15]. Hendrickson et al. and Karypis et al. contributed several levels to a more efficient spectral partitioning [16, 17]. Normalized cut metrics were introduced by Shi and Malik in [9]. They demonstrated how the bipartitioning task, with the aim of minimizing the normalized section, could be defused to construct a generalized eigenvalue problem and related to spectral partitioning. The eigenvector, which corresponds to the second smallest eigenvalue of the graph, identifies the real values (the real values of the eigenvalue) in the partitioning."}, {"heading": "2.2. Model Order Selection", "text": "The question that has arisen in the last few years in the USA is whether this is an attempt that has taken place in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA,"}, {"heading": "3. Problem Formulation", "text": "We assume that the edge weights w (u, v) = w (u, u) \u2265 0 are not negative and symmetrical, i.e., the greater the weight, the more similar the nodes are. We also allow self-weights, w (u, u) \u0445 E, the weight w (u, v) \u2265 0 is indicative of the similarity between nodes u and v; that is, the greater the weight, the more similar the nodes are. We also allow self-weights, w (u, u) \u0445E, the weight w (u) \u2265 0,4In this work we assume that we obtain the graph on which we want to perform clusters. We do not address the problem of creating a graph from data that arises when applying the graph cluster methods to general data sets; see, for example, vnosters of standardized data sets."}, {"heading": "4. GANC: Greedy Agglomerative Normalized Cut", "text": "In this section, we describe a greedy algorithm for building an agglomerative cluster based on a diagram. Although we do not give any guarantees for its accuracy, the algorithm is fast on sparse diagrams and performs excellently on a variety of examples, as in Section 5. In addition, GANC has a criterion for selecting the model sequence and does not need to be assigned the number of clusters a priori. It consists of three steps: agglomerative clustering, model sequence selection and refinement."}, {"heading": "4.1. Agglomerative Clustering", "text": "In the following discussion it should be noted that the number of clusters in the level k of the hierarchy k. At the level k of the hierarchy, using the given partition, Ck, two clusters are merged to form a new partition, Ck \u2212 1. The two clusters that are merged are chosen so that the normalized association of the level k \u2212 1 maximized.5First, Cn is a function that merges maps into unique clusters, 1,., n. The degree of each node u, d (u), is calculated for each edge, (u), the improvement in the normalized association by its contraction to get Cn \u2212 1, is stored in Cn \u2212 1 (u, v) = 5We note that a similar greedy merger of algorithms is implied in Shi and Malik."}, {"heading": "4.2. Model Order Selection", "text": "Many of the clustering algorithms require the number of clusters provided a priori to the algorithm. However, in practical situations, such information is often not available, which makes deciding on the number of clusters a problem in itself. It is worth noting that the number of steps (k) that the NAssoc (Ck) maximizes does not necessarily correspond to a significant number of clusters. 6 Here, we propose a simple but effective approach to selecting the model order. Let C \u0445 k = argmaxCkNAssoc (Ck) designate the partition that maximizes the normalized assocation across all partitions from V to k clusters."}, {"heading": "4.3. Refinement", "text": "This is also the case with the old nodes. (u = u) If the nodes (u = v = v) (u = v = v) (u = v = v) (v = v (u = v) v (v = v) v (u = v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v v (v) v v v v v) v (v) v v (v) v (v) v (v) v v v) v v v (v) v v v v (v) v v (v) v) v v (v) v v (v) v (v) v (v) v (v) v (v) v) v (v) v v (v) v (v) v (v) v) v (v) v (v) v (v) v (v) v (v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v) v (v) v (v) v (v) v) v (v) v (v) v (v) v (v) v v (v) v (v) v v v (v) v v (v) v v (v) v (v) v (v) v v (v) v (v) v (v v) v (v v v) v (v (v) v (v) v v (v) v (v) v (v) v v (v) v (v) v v (v) v (v) v v) v (v) v (v) v (v) v v (v) v v) v (v) v) v (v v) v v v (v) v v (v) v (v) v) v v v (v) v) v (v) v (v) v (v) v) v v v) v v v v"}, {"heading": "4.4. Implementation and Computational Complexity of GANC", "text": "We take a similar approach [57] when implementing the agglomerative clustering approach of GANC (1). (Max-heaps and balanced binary trees are used to the agglomerative clustering procedure, where h is the height of the generated dendrogram and m is the number of edges with non-zero weight (see [57] for details).The model order selection step of the algorithm needs O (n) computational requirements are much less than those of the methods analyzing the eigenvalueAlgorithm 1 GANC: Greedy Agglomerative Normalized CutGREEDY AGGLOMERATION1: Build the initial, and nAssoc (Cn)."}, {"heading": "5. Experimental Results", "text": "In this section we offer a comparison of the performance and runtime of our proposed algorithm GANC with a selection of the most modern graph cluster algorithms from the literature, the implementations of which have been downloaded from the corresponding author websites. We experimented on an Intel 3.0 GHz Core 2 Quad CPU with 8 GB RAM and Ubuntu 9.10 operating system."}, {"heading": "5.1. Comparing Algorithms", "text": "We compare the following algorithms: Shi and Malik (recursive NCut) [9]; Meila and Shi (k-way NCut) [18]; Ng, Jordan and Wei\u00df [19]; Dhillon, Gaun and Kulis [11]; Rosvall and Bergstrom [7]; and Blondel et al. [56] The first four algorithms focus on maximizing NAssoc. Although they do not directly address our criterion of interest, the other algorithms are included because they are scalable and represent the state-of-the-art in graphics. The cluster algorithms discussed focus on maximizing NAssoc [18], [19] are not scalable because they include self-composition. Dhillon, Guan and Kulis proposed an algorithm that seeks to maximize normalized associations without requiring any own compositions."}, {"heading": "5.2. Synthetic Graphs", "text": "First, we analyze performance using synthetic graphs for which true cluster behavior is known. We use benchmark graphs developed by Lancichinetti, Fortunato, and Radicchi [61] (LFR graphs), which are random graphs based on the planted partition model [62]. Each node is assigned to one of the k clusters. If edges are added to the graph, the probability of the edge being between nodes of the same cluster is 1 \u2212 \u00b5, and the probability of the edge connecting nodes from different clusters is \u00b5. LFR benchmarks have heterogeneous cluster sizes with user-defined lower and upper limits, cmin and cmax, respectively. In addition, node degrees are limited upwards by dmax, and the average node degree is denoted by davg. As the number of edges decreases, edges are increasingly displayed intra-specific cluster sizes, making partitioning task easier."}, {"heading": "5.2.1. Maximizing normalized association", "text": "Figure 1 examines how algorithms perform in terms of maximizing NAssoc for 1,000-node LFR graphs. To observe how algorithms behave on graphs with heterogeneous clusters, we vary cluster sizes between 20 and 50 nodes.8 For each value of \u00b5, 100 graph realizations are generated.The value of NAssoc is divided by k to obtain a value between 0 and 1 that does not significantly improve the average NAssoc per cluster.The algorithms work almost identically with the exception of the Dhillon et al. Algorithm [11]. The refinement step of GANC results in a significant improvement in the value of NAssoc. However, Dhillon's local search does not significantly improve NAssoc's algorithm. Note that the Blondel et algorithm results in higher values of NAssoc because a lower number of clusters than the true number of NAssoc (NAssoc / NAssoc) is only possible if a number of nodes is selected."}, {"heading": "5.2.2. Comparing to the planted partitions", "text": "The advantage of researching performance on synthetic graphs is that a soil-truth partitioning is available. Since the LFR benchmarks are based on the planted partition model, there is the added advantage that NAssoc is an appropriate criterion for adoption. Apart from some possible small errors due to the randomness inherent in the construction of the benchmark graphs, the soil-truth partitioning will be a maximum of NAssoc8In the real networks we are looking at in this paper, the clusters will be of limited size, regardless of the network size [60]; for example, the Dunbar number suggests an upper limit of 150 nodes for clusters in a social network [63].For a given value of k, we use the Jaccard index [64] defined for two partitions, X and Y, asJI = a (a + b + c), where a, b, and c are the total pair of Y that are both assigned nodes."}, {"heading": "5.3. Model order selection: the curvature metric", "text": "This example highlights the difference in behavior compared to the modularity metric used by modularity maximization algorithms. [56] An example used by Good et al. [59] to illustrate the resolution limit is the ring of 24 clicks, each of which has 5 nodes and is connected to its adjacent clique by a single edge. However, the value of modularity is maximized by a 12-cluster partition that merges pairs of clicks. A more natural cluster formation is to view each clique as an individual cluster. Figure 2b shows the curvature graph in the case of cliques. In addition to the peak of curvature, which is correctly located at 24 clusters, it is interesting to note that the other local peaks of the curvature are significant."}, {"heading": "5.4. Real networks", "text": "In this section, we will examine the behavior of the GANC and compare it with the other algorithms for graphs that represent real networks. First, we will examine the behavior for small networks that contain knowledge about the Ground Truth Partition. Then, we will experiment with large networks, which will allow us to assess the scalability of GANC."}, {"heading": "5.4.1. Small networks", "text": "In recent years, the number of unemployed has multiplied, the number of unemployed has multiplied, the number of unemployed has doubled and the number of unemployed has tripled."}, {"heading": "5.4.2. Cortical Networks", "text": "The nodes correspond to small regions of the human cerebral cortex and the edges correspond to cortical axonal paths. Networks are designed for five patients (extraction is performed twice for patient A. First, we perform a comparison of competing algorithms, then we discuss the cluster results of GANC. When applying the Rosvall Bergstrom and Blondel et algorithms, the algorithms used do not have the freedom to choose the number of clusters. Therefore, we repeat the experiment twice to make a meaningful objective comparison in relation to NAssoc. Table 3 lists the average NAssoc of all clustering algorithms and patients. In the first subtable, k is selected for the model order selected by the Rosvall Bergstrom algorithm."}, {"heading": "5.4.3. Larger networks", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "5.5. Case Study: US Patent Citation Graph", "text": "As a case study, we consider the undirected version of the citation chart published by the National Bureau of Economic Research [70, 71]. Patents are divided into 6 broad technological categories. A more refined classification leads to 36 subcategories. We use the designation of each patent (category or subcategory) in addition to NAssoc and term to perform a comparison. The original graph is not connected, but the largest connected component contains more than 99.7% of the nodes and all edges, so we focus on the largest connected component, which contains 3,764,117 nodes and 16,511,740 edges, with a maximum node degree of 793."}, {"heading": "5.5.1. Clustering Runtime", "text": "The Rosvall Bergstrom algorithm [7] was terminated after more than 30 hours without a solution being found. GANC takes 77 minutes to create the complete hierarchy. After the hierarchy is created, each flat partitioning including refinement takes less than 35 seconds. The algorithm by Blondel et al. [56] takes 8 minutes. The Dhillon-Guan-Kulis algorithm [11] takes 72 seconds for k = 57 and increases when k is increased."}, {"heading": "5.5.2. Maximization of Normalized Association", "text": "In order to have a fair comparison with NAssoc, we apply k = 57 to the number of clusters of Blondel et al. The values of NAssoc / k for GANC, Dhillon-GuanKulis and Blondel et al. are 0.964, 0.859 and 0.855, respectively. The values of NAssoc for the individual clusters are shown in Figure 4b, which illustrates the clear superiority of GANC."}, {"heading": "5.5.3. Extraction of True Clusters and Absence of Large Well-Defined Clusters", "text": "We use the categories and sub-categories to classify the nodes in the patent quotation curve. However, we use k = 57 for the Blondel et al. and Dhillon-GuanKulis algorithms, and k = 52 for GANC (the next peak of the curvature curve), and the clusters are sorted by their homogeneity portion in Figure5a. The figure shows the superiority of GANC in extracting nodes from the same categories. when subcategories are used for evaluation, the superiority of GANC becomes more pronounced. Figure 5a alone does not provide a fair comparison, as each singleton would have a homogeneity portion of 1. Figure 5b plots the cluster cluster as a cluster."}, {"heading": "6. Conclusion", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}], "references": [{"title": "Social network analysis: Methods and applications", "author": ["S. Wasserman", "K. Faust"], "venue": "Cambridge Univ. Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Functional cartography of complex metabolic networks", "author": ["R. Guimera", "L. Amaral"], "venue": "Nature 433 (7028) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient algorithms for accurate hierarchical clustering of huge datasets: tackling the entire protein space", "author": ["Y. Loewenstein", "E. Portugaly", "M. Fromer", "M. Linial"], "venue": "Bioinformatics 24 (13) ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Revealing modular architecture of human brain structural networks by using cortical thickness from MRI", "author": ["Z. Chen", "Y. He", "P. Rosa-Neto", "J. Germann", "A. Evans"], "venue": "Cerebral Cortex 18 (10) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Spectral Clustering and Label Fusion For 3D Tissue Classification: Sensitivity and Consistency Analysis", "author": ["W. Crum"], "venue": "in: Proc. Med. Im. Underst. Anal., Dundee, Scotland", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Community structure in social and biological networks", "author": ["M. Girvan", "M.E.J. Newman"], "venue": "Proc. Natl. Acad. Sci. 99 (12) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Maps of random walks on complex networks reveal community structure", "author": ["M. Rosvall", "C.T. Bergstrom"], "venue": "Proc. Natl. Acad. Sci. 105 (4) ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion maps and coarse-graining: a unified framework for dimensionality reduction", "author": ["S. Lafon", "A. Lee"], "venue": "graph partitioning, and data set parameterization, IEEE Trans. Patt. Anal. Mach. Intel. 28 (9) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Trans. Patt. Anal. Mach. Intel. 22 (8) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Trans. Patt. Anal. Mach. Intel. 26 (2) ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Weighted graph cuts without eigenvectors: a multilevel approach", "author": ["I. Dhillon", "Y. Guan", "B. Kulis"], "venue": "IEEE Trans. Patt. Anal. Mach. Intel. 29 (11) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Network community discovery: solving modularity clustering via normalized cut", "author": ["L. Yu", "C. Ding"], "venue": "in: Proc. ACM Wkshp Mining and Learn. with Graphs", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Approximate counting", "author": ["A. Sinclair", "M. Jerrum"], "venue": "uniform generation and rapidly mixing Markov chains, Info. Comp. 82 (1) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1989}, {"title": "Algorithms for partitioning of graphs and computer logic based on eigenvectors of connection matrices", "author": ["W. Donath", "A. Hoffman"], "venue": "IBM Tech. Disc. Bull. 15 (3) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1972}, {"title": "Partitioning sparse matrices with eigenvectors of graphs", "author": ["A. Pothen", "H. Simon", "K.-P. Liou"], "venue": "SIAM J. Matrix Anal. App. 11 (1) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1990}, {"title": "A multilevel algorithm for partitioning graphs", "author": ["B. Hendrickson", "R. Leland"], "venue": "in: Proc. ACM Int. Conf. Supercomp., Barcelona, Spain", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1995}, {"title": "A fast and high quality multilevel scheme for partitioning irregular graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "SIAM J. Sci. Comp. 20 (1) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "A random walks view of spectral segmentation", "author": ["M. Meila", "J. Shi"], "venue": "in: Proc. Int. Wkshp Art. Intel. Stat., Key West, FL, USA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A. Ng", "M. Jordan", "Y. Weiss"], "venue": "in: Proc. Adv. Neur. Inf. Proc. Sys., Vancouver, BC, Canada", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast approximate spectral clustering", "author": ["D. Yan", "L. Huang", "M.I. Jordan"], "venue": "in: Proc. ACM Int. Conf. Knowl. Disc. Data Mining, Paris, France", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast multiscale image segmentation", "author": ["E. Sharon", "A. Brandt", "R. Basri"], "venue": "in: Proc. IEEE Conf. Comp. Vis. Patt. Recog., Hilton Head, SC, USA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "A min-max cut algorithm for graph partitioning and data clustering", "author": ["C. Ding", "X. He", "H. Zha", "M. Gu", "H. Simon"], "venue": "in: Proc. IEEE Int. Conf. Data Mining, San Jose, CA, USA", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Segmentation and boundary detection using multiscale intensity measurements", "author": ["E. Sharon", "A. Brandt", "R. Basri"], "venue": "in: Proc. IEEE Conf. Comp. Vis. Patt. Recog., Kauai, HI, USA", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2001}, {"title": "An integrated segmentation and classification approach applied to multiple sclerosis analysis", "author": ["A. Akselrod-Ballin", "M. Galun", "R. Basri", "A. Brandt", "M. Gomori", "M. Filippi", "P. Valsasina"], "venue": "in: Proc. IEEE Conf. Comp. Vis. Patt. Recog., New York, NY, USA", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient multilevel brain tumor segmentation with integrated bayesian model classification", "author": ["J. Corso", "E. Sharon", "S. Dube", "S. El-Saden", "U. Sinha", "A. Yuille"], "venue": "IEEE Trans. Med. Im. 27 (5) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "An examination of procedures for determining the number of clusters in a data set", "author": ["G. Milligan", "M. Cooper"], "venue": "Psychometrika 50 (2) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1985}, {"title": "On clustering validation techniques", "author": ["M. Halkidi", "Y. Batistakis", "M. Vazirgiannis"], "venue": "J. Intel. Inf. Sys. 17 (2) ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "A dendrite method for cluster analysis", "author": ["T. Cali\u0144ski", "J. Harabasz"], "venue": "Comm. Stat. Theo. Meth. 3 (1) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1974}, {"title": "A general statistical framework for assessing categorical clustering in free recall", "author": ["L. Hubert", "J. Levin"], "venue": "Psych. Bull. 83 (6) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1976}, {"title": "Measuring the power of hierarchical cluster analysis", "author": ["F. Baker", "L. Hubert"], "venue": "J. American Stat. Assoc. ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1975}, {"title": "Cubic Clustering Criterion", "author": ["W. Sarle"], "venue": "Tech. rep., Cary, NC: SAS Inst. Inc ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1983}, {"title": "Estimating the number of clusters in a data set via the gap statistic", "author": ["R. Tibshirani", "G. Walther", "T. Hastie"], "venue": "J. Royal Stat. Soc.: Series B (Stat. Meth.) 63 (2) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2001}, {"title": "Graph clustering using distance-k cliques", "author": ["J. Edachery", "A. Sen", "F. Brandenburg"], "venue": "in: Graph Drawing, Vol. 1731 of Lecture Notes in Comp. Science, Springer Berlin / Heidelberg", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1999}, {"title": "Interpreting and assessing the results of cluster analyses", "author": ["R. Gnanadesikan", "J. Kettenring", "J. Landwehr"], "venue": "Bull. Int. Stat. Inst. 47 (2) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1977}, {"title": "A criterion for determining the number of groups in a data set using sum-of-squares clustering", "author": ["W. Krzanowski", "Y. Lai"], "venue": "Biometrics 44 (1) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1988}, {"title": "Automatic cluster stopping with criterion functions and the gap statistic", "author": ["T. Pedersen", "A. Kulkarni"], "venue": "in: Proc. Conf. North American Chap. Assoc. Comp. Ling. Human Lang. Tech., New York City, NY, USA", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "A test for clusters", "author": ["S. Arnold"], "venue": "J. Market. Res. 16 (4) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1979}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Ann. Stat. 6 (2) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1978}, {"title": "A new look at the statistical model identification", "author": ["H. Akaike"], "venue": "IEEE Trans. Auto. Control 19 (6) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1974}, {"title": "Pattern clustering by multivariate mixture analysis", "author": ["J. Wolfe"], "venue": "Multivar. Behav. Res. 5 (3) ", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1970}, {"title": "X-means: Extending K-means with Effcient Estimation of the Number of Clusters", "author": ["D. Pelleg", "A. Moore"], "venue": "in: Proc. Int. Conf. Mach. Learn., Stanford, CA, USA", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2000}, {"title": "Uncovering latent structure in valued graphs: a variational approach", "author": ["M. Mariadassou", "S. Robin", "C. Vacher"], "venue": "Ann. App. Stat. 4 (2) ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian methods for graph clustering", "author": ["P. Latouche", "E. Birmel", "C. Ambroise"], "venue": "Tech. rep., Laboratoire Statistique et Gnome, UMR CNRS 8071-INRA 1152-UEVE, 91000 Evry, France ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "Bayesian methods for graph clustering", "author": ["P. Latouche", "E. Birmel", "C. Ambroise"], "venue": "in: Adv. Data Anal. Data Hand. Bus. Intel.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Stochastic image segmentation by typical cuts", "author": ["Y. Gdalyahu", "D. Weinshall", "M. Werman"], "venue": "in: Proc. IEEE Comp. Soc. Conf. Comp. Vis. Patt. Recog., Fort Collins, CO, USA", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1999}, {"title": "On clustering using random walks", "author": ["D. Harel", "Y. Koren"], "venue": "in: Proc. Conf. Found. Soft. Tech. Theor. Comp. Sci., Bangalore, India", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2001}, {"title": "Spectral methods for automatic multiscale data clustering", "author": ["A. Azran", "Z. Ghaharamani"], "venue": "in: Proc. IEEE Conf. Comp. Vis. Patt. Recog., New York, NY, USA", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2006}, {"title": "Self-tuning spectral clustering", "author": ["L. Zelnik-Manor", "P. Perona"], "venue": "in: Proc. Adv. Neur. Inf. Proc. Sys., Vancouver, BC, Canada", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2004}, {"title": "A stability based method for discovering structure in clustered data", "author": ["A. Ben-Hur", "A. Elisseeff", "I. Guyon"], "venue": "in: Pac. Symp. Biocomp., Lihue, HI, USA", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2002}, {"title": "Stability-based validation of clustering solutions", "author": ["T. Lange", "V. Roth", "M. Braun", "J. Buhmann"], "venue": "Neural Comp. 16 (6) ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2004}, {"title": "A sober look at clustering stability", "author": ["S. Ben-David", "U. Von Luxburg", "D. P\u00e1l"], "venue": "Learning Theory ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2006}, {"title": "Cluster stability scores for microarray data in cancer studies", "author": ["M. Smolkin", "D. Ghosh"], "venue": "BMC Bioinformatics 4 (1) ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2003}, {"title": "Fitting a graph to vector data", "author": ["S.I. Daitch", "J.A. Kelner", "D.A. Spielman"], "venue": "in: Proc. ACM Int. Conf. Mach. Learn., Montreal, QC, Canada", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "Influence of graph construction on graph-based clustering measures", "author": ["M. Maier", "U. Von Luxburg", "M. Hein"], "venue": "in: Proc. Adv. Neur. Inf. Proc. Sys., Vancouver, BC, Canada", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast unfolding of communities in large networks", "author": ["V. Blondel", "J. Guillaume", "R. Lambiotte", "E. Lefebvre"], "venue": "J. Stat. Mech.: Theor. Exp. 2008 ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2008}, {"title": "Finding community structure in very large networks", "author": ["A. Clauset", "M. Newman", "C. Moore"], "venue": "Phys. Rev. E 70 (6) ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2004}, {"title": "C++ Standard Template Library", "author": ["P. Plauger", "M. Lee", "D. Musser", "A. Stepanov"], "venue": "Prentice Hall PTR Upper Saddle River, NJ, USA", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2000}, {"title": "Performance of modularity maximization in practical contexts", "author": ["B.H. Good", "Y.-A. de Montjoye", "A. Clauset"], "venue": "Phys. Rev. E", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2010}, {"title": "Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communities", "author": ["A. Lancichinetti", "S. Fortunato"], "venue": "Phys. Rev. E 80 (1) ", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithms for graph partitioning on the planted partition model", "author": ["A. Condon", "R.M. Karp"], "venue": "Rand. Struct. Alg. 18 (2) ", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2001}, {"title": "Grooming", "author": ["R. Dunbar"], "venue": "gossip, and the evolution of language, Harvard Univ. Press, MA, USA", "citeRegEx": "63", "shortCiteRegEx": null, "year": 1998}, {"title": "Comparing classifications: an evaluation of several coefficients of partition agreement", "author": ["M. Downton", "T. Brennan"], "venue": "in: Proc. Meet. Class. Soc., Boulder, CO, USA", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1980}, {"title": "An information flow model for conflict and fission in small groups", "author": ["W. Zachary"], "venue": "J. Anthrop. Res. 33 (4) ", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1977}, {"title": "Mapping the structural core of human cerebral cortex", "author": ["P. Hagmann", "L. Cammoun", "X. Gigandet", "R. Meuli", "C. Honey", "V. Wedeen", "O. Sporns"], "venue": "PLoS Biol. 6 (7) ", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2008}, {"title": "The structure of scientific collaboration networks", "author": ["M. Newman"], "venue": "Proc. Natl. Acad. Sci. 98 (2) ", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2001}, {"title": "The dynamics of viral marketing", "author": ["J. Leskovec", "L.A. Adamic", "B.A. Huberman"], "venue": "in: Proc. Int. Conf. Elect. Commerce, Ann Arbor, MI, USA", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2006}, {"title": "Graphs over time: densification laws", "author": ["J. Leskovec", "J. Kleinberg", "C. Faloutsos"], "venue": "shrinking diameters and possible explanations, in: Proc. ACM SIGKDD Int. Conf. Knowl. Disc. Data Mining, 26  ACM, Chicago, Illinois, USA", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2005}, {"title": "A connectivity based clustering algorithm with application to VLSI circuit partitioning", "author": ["J. Li", "L. Behjat"], "venue": "IEEE Trans. Circ. Sys. II: Exp. Briefs 53 (5) ", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2006}, {"title": "Clustering by weighted cuts in directed graphs", "author": ["M. Meila", "W. Pentney"], "venue": "in: Proc. SIAM Int. Conf. Data Mining, Minneapolis, MN, USA", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "It has been used for many years to study social networks [1] and continues to be employed in the field of sociology to explore social interactions.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "More recently it has been employed in the study of biochemical networks [2, 3], biological neural networks [4, 5], and transport and communication networks.", "startOffset": 72, "endOffset": 78}, {"referenceID": 2, "context": "More recently it has been employed in the study of biochemical networks [2, 3], biological neural networks [4, 5], and transport and communication networks.", "startOffset": 72, "endOffset": 78}, {"referenceID": 3, "context": "More recently it has been employed in the study of biochemical networks [2, 3], biological neural networks [4, 5], and transport and communication networks.", "startOffset": 107, "endOffset": 113}, {"referenceID": 4, "context": "More recently it has been employed in the study of biochemical networks [2, 3], biological neural networks [4, 5], and transport and communication networks.", "startOffset": 107, "endOffset": 113}, {"referenceID": 5, "context": ", modularity [6] or information-theoretic criteria based on the minimum description length [7].", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": ", modularity [6] or information-theoretic criteria based on the minimum description length [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "On the other hand, clustering algorithms based on Markov random walks [7, 8] also value indirect connections and network flow.", "startOffset": 70, "endOffset": 76}, {"referenceID": 7, "context": "On the other hand, clustering algorithms based on Markov random walks [7, 8] also value indirect connections and network flow.", "startOffset": 70, "endOffset": 76}, {"referenceID": 8, "context": "In this paper we select the normalized cut criterion [9], which simultaneously encourages intra-cluster similarity while penalizing inter-cluster similarities.", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312].", "startOffset": 96, "endOffset": 105}, {"referenceID": 7, "context": "Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312].", "startOffset": 96, "endOffset": 105}, {"referenceID": 8, "context": "Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312].", "startOffset": 96, "endOffset": 105}, {"referenceID": 9, "context": "Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312].", "startOffset": 96, "endOffset": 105}, {"referenceID": 10, "context": "Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312].", "startOffset": 96, "endOffset": 105}, {"referenceID": 11, "context": "Methods based on this criterion have been employed successfully in a wide range of applications [5, 8\u201312].", "startOffset": 96, "endOffset": 105}, {"referenceID": 12, "context": "The normalized cut criterion is related to the conductance of the underlying graph [13].", "startOffset": 83, "endOffset": 87}, {"referenceID": 8, "context": "The normalized cut metric was introduced by Shi and Malik in [9] to address this shortcoming.", "startOffset": 61, "endOffset": 64}, {"referenceID": 8, "context": "Minimizing normalized cut is an NP-complete problem [9].", "startOffset": 52, "endOffset": 55}, {"referenceID": 8, "context": "In order to identify a partitioning of n nodes to k clusters, some techniques perform recursive bipartitioning [9], and thus require the repeated identification of two eigenvectors.", "startOffset": 111, "endOffset": 114}, {"referenceID": 13, "context": "Spectral partitioning of graphs was first proposed by Donath and Hoffman in the 1970\u2019s [14].", "startOffset": 87, "endOffset": 91}, {"referenceID": 14, "context": "described an algorithm for bi-partitioning using the Fiedler vector [15].", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "contributed with multilevel algorithms for more efficient spectral partitioning [16, 17].", "startOffset": 80, "endOffset": 88}, {"referenceID": 16, "context": "contributed with multilevel algorithms for more efficient spectral partitioning [16, 17].", "startOffset": 80, "endOffset": 88}, {"referenceID": 8, "context": "The normalized cut metric was introduced by Shi and Malik in [9].", "startOffset": 61, "endOffset": 64}, {"referenceID": 17, "context": "In [18], Meila and Shi proposed an algorithm that calculates k eigenvectors (thereby associating k real values with each node in the graph) and then uses a clustering algorithm, such as k-means, to do the partitioning in R.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "observed in [19] that the algorithm in [18] is susceptible to failure when there is substantial variation in the degree of connectivity between clusters.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "observed in [19] that the algorithm in [18] is susceptible to failure when there is substantial variation in the degree of connectivity between clusters.", "startOffset": 39, "endOffset": 43}, {"referenceID": 9, "context": "described a procedure that uses the Nystr\u00f6m method to reduce the complexity of the eigenvalue problem [10].", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "recently described an algorithm for fast approximate spectral clustering [20], but the focus is not on clustering for graphs (rather it addresses real-valued feature vectors).", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "introduced a much faster algorithm for minimizing normalized cut in [11]; the graph is first greedily coarsened, then the coarsened graph is partitioned using a region growing procedure [17] and finally weighted kernel k-means clustering is applied to each partition to refine the clustering.", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "introduced a much faster algorithm for minimizing normalized cut in [11]; the graph is first greedily coarsened, then the coarsened graph is partitioned using a region growing procedure [17] and finally weighted kernel k-means clustering is applied to each partition to refine the clustering.", "startOffset": 186, "endOffset": 190}, {"referenceID": 20, "context": "proposed a scalable hierarchical clustering using ratio association (the normalization term is the number of nodes in clusters) [21].", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "[22], Sharon et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23], Akselrod-Ballin et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24], and Corso et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] that use the sum of internal weights of clusters as the normalization term.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 41, "endOffset": 45}, {"referenceID": 26, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 95, "endOffset": 99}, {"referenceID": 28, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 109, "endOffset": 113}, {"referenceID": 29, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 132, "endOffset": 136}, {"referenceID": 30, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 169, "endOffset": 173}, {"referenceID": 25, "context": "The majority of the methods discussed in [26] and [27] are of this type; Calinski and Harabasz [28], C-index [29], Baker and Hubert [30], and cubic clustering criterion [31] are among the most effective ones [26].", "startOffset": 208, "endOffset": 212}, {"referenceID": 31, "context": "[32] define F (k) as the gap, that is the difference of the average pairwise distance of the data points of the clustering at k level and the expected value of the same measure of some reference model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "This is similar to modularity [6] (discussed later) in the sense that it compares the clustering results to a reference model.", "startOffset": 30, "endOffset": 33}, {"referenceID": 32, "context": "Possible distance metrics include the shortest path [33] or the diffusion distance [8]; however the shortest path is very sensitive to noise and the calculation of the diffusion distance requires eigendecomposition.", "startOffset": 52, "endOffset": 56}, {"referenceID": 7, "context": "Possible distance metrics include the shortest path [33] or the diffusion distance [8]; however the shortest path is very sensitive to noise and the calculation of the diffusion distance requires eigendecomposition.", "startOffset": 83, "endOffset": 86}, {"referenceID": 33, "context": "Hence, the value of F (k) corresponding to two or more choices of k are examined to quantify the significance of a given level of a hierarchical clustering [34\u201336].", "startOffset": 156, "endOffset": 163}, {"referenceID": 34, "context": "Hence, the value of F (k) corresponding to two or more choices of k are examined to quantify the significance of a given level of a hierarchical clustering [34\u201336].", "startOffset": 156, "endOffset": 163}, {"referenceID": 35, "context": "Hence, the value of F (k) corresponding to two or more choices of k are examined to quantify the significance of a given level of a hierarchical clustering [34\u201336].", "startOffset": 156, "endOffset": 163}, {"referenceID": 33, "context": "[34] propose the fraction F (k)/F (k\u2212 1), Arnold [37] use the value of F (k)\u2212F (k\u2212 1), Krzanowski and Lai [35] employ the fraction |F (k) \u2212 F (k \u2212 1)|/|F (k + 1) \u2212 F (k)|, and Pederson and Kulkarni [36] suggest using the fraction 2\u00d7F (k)/ (F (k \u2212 1) + F (k + 1)).", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[34] propose the fraction F (k)/F (k\u2212 1), Arnold [37] use the value of F (k)\u2212F (k\u2212 1), Krzanowski and Lai [35] employ the fraction |F (k) \u2212 F (k \u2212 1)|/|F (k + 1) \u2212 F (k)|, and Pederson and Kulkarni [36] suggest using the fraction 2\u00d7F (k)/ (F (k \u2212 1) + F (k + 1)).", "startOffset": 49, "endOffset": 53}, {"referenceID": 34, "context": "[34] propose the fraction F (k)/F (k\u2212 1), Arnold [37] use the value of F (k)\u2212F (k\u2212 1), Krzanowski and Lai [35] employ the fraction |F (k) \u2212 F (k \u2212 1)|/|F (k + 1) \u2212 F (k)|, and Pederson and Kulkarni [36] suggest using the fraction 2\u00d7F (k)/ (F (k \u2212 1) + F (k + 1)).", "startOffset": 106, "endOffset": 110}, {"referenceID": 35, "context": "[34] propose the fraction F (k)/F (k\u2212 1), Arnold [37] use the value of F (k)\u2212F (k\u2212 1), Krzanowski and Lai [35] employ the fraction |F (k) \u2212 F (k \u2212 1)|/|F (k + 1) \u2212 F (k)|, and Pederson and Kulkarni [36] suggest using the fraction 2\u00d7F (k)/ (F (k \u2212 1) + F (k + 1)).", "startOffset": 198, "endOffset": 202}, {"referenceID": 34, "context": "However the approaches in [35, 36] are potentially susceptible to noise due to the division; small perturbations in the weights could lead to dramatic changes in the selected model order.", "startOffset": 26, "endOffset": 34}, {"referenceID": 35, "context": "However the approaches in [35, 36] are potentially susceptible to noise due to the division; small perturbations in the weights could lead to dramatic changes in the selected model order.", "startOffset": 26, "endOffset": 34}, {"referenceID": 37, "context": "This allows the application of model selection techniques based on concepts such as the Bayesian InformationCriterion (BIC) [38], and Akaike Information Criterion [39].", "startOffset": 124, "endOffset": 128}, {"referenceID": 38, "context": "This allows the application of model selection techniques based on concepts such as the Bayesian InformationCriterion (BIC) [38], and Akaike Information Criterion [39].", "startOffset": 163, "endOffset": 167}, {"referenceID": 39, "context": "An example is the requirement in [40, 41] that the input data are normally distributed (after projection of the graph into a real space).", "startOffset": 33, "endOffset": 41}, {"referenceID": 40, "context": "An example is the requirement in [40, 41] that the input data are normally distributed (after projection of the graph into a real space).", "startOffset": 33, "endOffset": 41}, {"referenceID": 41, "context": "The more general methods based on mixture models do not scale well to very large graphs; even the recent approaches have only been applied to graphs with a few thousand nodes [42\u201344].", "startOffset": 175, "endOffset": 182}, {"referenceID": 42, "context": "The more general methods based on mixture models do not scale well to very large graphs; even the recent approaches have only been applied to graphs with a few thousand nodes [42\u201344].", "startOffset": 175, "endOffset": 182}, {"referenceID": 43, "context": "The more general methods based on mixture models do not scale well to very large graphs; even the recent approaches have only been applied to graphs with a few thousand nodes [42\u201344].", "startOffset": 175, "endOffset": 182}, {"referenceID": 44, "context": "Some heuristics are based on the sizes of the clusters that are merged at different levels of the clustering hierarchy [45, 46].", "startOffset": 119, "endOffset": 127}, {"referenceID": 45, "context": "Some heuristics are based on the sizes of the clusters that are merged at different levels of the clustering hierarchy [45, 46].", "startOffset": 119, "endOffset": 127}, {"referenceID": 45, "context": "The authors in [46] suggest that when two clusters with large number of nodes are merged, a significant amount of detail is lost; hence such an instance is potentially where a hierarchical clustering algorithm should stop.", "startOffset": 15, "endOffset": 19}, {"referenceID": 44, "context": "of [45] propose a similar approach.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "A well-known and effective method of selecting the number of clusters is to examine the eigenvalues of the Laplacian of the graph that is to be clustered [19, 47, 48].", "startOffset": 154, "endOffset": 166}, {"referenceID": 46, "context": "A well-known and effective method of selecting the number of clusters is to examine the eigenvalues of the Laplacian of the graph that is to be clustered [19, 47, 48].", "startOffset": 154, "endOffset": 166}, {"referenceID": 18, "context": "A large eigengap at k \u2217 is the case in which spectral algorithms using the Laplacian perform most successfully [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 47, "context": "A more robust criterion is proposed by Zelnik-Manor and Perona [49] that uses eigenvectors instead.", "startOffset": 63, "endOffset": 67}, {"referenceID": 48, "context": "The perturbation could be in terms of noise [50], sampling subsets from the input [51, 52], or random projection of a high dimensional data into a lower dimensional space [53].", "startOffset": 44, "endOffset": 48}, {"referenceID": 49, "context": "The perturbation could be in terms of noise [50], sampling subsets from the input [51, 52], or random projection of a high dimensional data into a lower dimensional space [53].", "startOffset": 82, "endOffset": 90}, {"referenceID": 50, "context": "The perturbation could be in terms of noise [50], sampling subsets from the input [51, 52], or random projection of a high dimensional data into a lower dimensional space [53].", "startOffset": 82, "endOffset": 90}, {"referenceID": 51, "context": "The perturbation could be in terms of noise [50], sampling subsets from the input [51, 52], or random projection of a high dimensional data into a lower dimensional space [53].", "startOffset": 171, "endOffset": 175}, {"referenceID": 50, "context": "[52] warn against using stability analysis in this context, and they suggest that this family of model order selection techniques is not suitable for selecting the number of clusters in general.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Two more recently proposed methods to select the number of clusters are based on optimization of the quality metrics of modularity [6] and description length [7].", "startOffset": 131, "endOffset": 134}, {"referenceID": 6, "context": "Two more recently proposed methods to select the number of clusters are based on optimization of the quality metrics of modularity [6] and description length [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 52, "context": ", [48, 54, 55].", "startOffset": 2, "endOffset": 14}, {"referenceID": 53, "context": ", [48, 54, 55].", "startOffset": 2, "endOffset": 14}, {"referenceID": 8, "context": "For a fixed number of clusters, k, we measure the quality of the partition via the normalized cut metric [9], defined as follows.", "startOffset": 105, "endOffset": 108}, {"referenceID": 8, "context": "We note that a similar greedy merging algorithm is alluded to by Shi and Malik in [9], however they also suggest first projecting each node into R using the first k eigenvectors of the graph Laplacian, and running an algorithm such as k-means to obtain an initial clustering.", "startOffset": 82, "endOffset": 85}, {"referenceID": 54, "context": "A similar approach is taken in [56], but groups of nodes are moved from a cluster to another instead of individual nodes.", "startOffset": 31, "endOffset": 35}, {"referenceID": 55, "context": "We take a similar approach to [57] when implementing the agglomerative clustering procedure of GANC.", "startOffset": 30, "endOffset": 34}, {"referenceID": 55, "context": "This leads to the complexity of O(mh log(n)) for the agglomerative clustering procedure, where h is the height of the generated dendrogram and m is the number of edges with non-zero weight (see [57] for details).", "startOffset": 194, "endOffset": 198}, {"referenceID": 47, "context": ", [49] and [47]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 46, "context": ", [49] and [47]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 56, "context": "Every row of B is stored in a map (C++ STL implementation of the map data structure is used [58]).", "startOffset": 92, "endOffset": 96}, {"referenceID": 8, "context": "We compare to the following algorithms: Shi and Malik (recursive NCut) [9]; Meila and Shi (k-way NCut) [18]; Ng, Jordan, and Weiss [19]; Dhillon, Gaun, and Kulis [11]; Rosvall and Bergstrom [7]; and Blondel et al.", "startOffset": 71, "endOffset": 74}, {"referenceID": 17, "context": "We compare to the following algorithms: Shi and Malik (recursive NCut) [9]; Meila and Shi (k-way NCut) [18]; Ng, Jordan, and Weiss [19]; Dhillon, Gaun, and Kulis [11]; Rosvall and Bergstrom [7]; and Blondel et al.", "startOffset": 103, "endOffset": 107}, {"referenceID": 18, "context": "We compare to the following algorithms: Shi and Malik (recursive NCut) [9]; Meila and Shi (k-way NCut) [18]; Ng, Jordan, and Weiss [19]; Dhillon, Gaun, and Kulis [11]; Rosvall and Bergstrom [7]; and Blondel et al.", "startOffset": 131, "endOffset": 135}, {"referenceID": 10, "context": "We compare to the following algorithms: Shi and Malik (recursive NCut) [9]; Meila and Shi (k-way NCut) [18]; Ng, Jordan, and Weiss [19]; Dhillon, Gaun, and Kulis [11]; Rosvall and Bergstrom [7]; and Blondel et al.", "startOffset": 162, "endOffset": 166}, {"referenceID": 6, "context": "We compare to the following algorithms: Shi and Malik (recursive NCut) [9]; Meila and Shi (k-way NCut) [18]; Ng, Jordan, and Weiss [19]; Dhillon, Gaun, and Kulis [11]; Rosvall and Bergstrom [7]; and Blondel et al.", "startOffset": 190, "endOffset": 193}, {"referenceID": 54, "context": "[56].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "The discussed clustering algorithms ([9],[18],[19]) are not scalable as they include eigendecomposition.", "startOffset": 37, "endOffset": 40}, {"referenceID": 17, "context": "The discussed clustering algorithms ([9],[18],[19]) are not scalable as they include eigendecomposition.", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "The discussed clustering algorithms ([9],[18],[19]) are not scalable as they include eigendecomposition.", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "Dhillon, Guan, and Kulis proposed an algorithm that strives to maximize normalized association without requiring any eigendecomposition [11].", "startOffset": 136, "endOffset": 140}, {"referenceID": 6, "context": "Minimization of the criterion proposed by Rosvall and Bergstrom [7] results in a coarsegrained representation of the information flow through the network.", "startOffset": 64, "endOffset": 67}, {"referenceID": 54, "context": "targets maximizing modularity [56].", "startOffset": 30, "endOffset": 34}, {"referenceID": 57, "context": "Modularity provides a valuable metric of the connectedness of clusters, but a number of authors have demonstrated that it suffers from a resolution limit when used to select the number of clusters [59].", "startOffset": 197, "endOffset": 201}, {"referenceID": 11, "context": "By normalizing the summation by d(Ci) as suggested in [12], the resolution limit phenomena is resolved; but we have", "startOffset": 54, "endOffset": 58}, {"referenceID": 58, "context": "We use benchmark graphs developed by Lancichinetti, Fortunato, and Radicchi [61] (LFR graphs).", "startOffset": 76, "endOffset": 80}, {"referenceID": 59, "context": "These random graphs are designed based on the planted partition model [62].", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "The Meila-Shi algorithm performs at least as well as the other spectral clustering algorithms (Ng-Jordan-Weiss [19] and Shi-Malik [9]) and hence we only display the Meila-Shi results.", "startOffset": 111, "endOffset": 115}, {"referenceID": 8, "context": "The Meila-Shi algorithm performs at least as well as the other spectral clustering algorithms (Ng-Jordan-Weiss [19] and Shi-Malik [9]) and hence we only display the Meila-Shi results.", "startOffset": 130, "endOffset": 133}, {"referenceID": 10, "context": "algorithm [11].", "startOffset": 10, "endOffset": 14}, {"referenceID": 60, "context": "In the real-life networks that we consider in this paper, the clusters have been observed to be of a limited size, regardless of the network size [60]; for example the Dunbar number suggests an upper limit of 150 nodes for clusters in a social network [63].", "startOffset": 252, "endOffset": 256}, {"referenceID": 61, "context": "For comparing two partitions on the same graph, we use the Jaccard index [64] which for two partitions, X and Y , is defined as", "startOffset": 73, "endOffset": 77}, {"referenceID": 54, "context": "This example highlights the difference in behavior compared to the modularity metric used by modularity maximization algorithms [56].", "startOffset": 128, "endOffset": 132}, {"referenceID": 57, "context": "[59], to illustrate the resolution limit is the ring of 24 cliques, each of which has 5 nodes and is connected to its neighboring clique by a single edge.", "startOffset": 0, "endOffset": 4}, {"referenceID": 62, "context": "We conduct experiments using the Zachary karate club network [65], the football network [6], and the political books network.", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "We conduct experiments using the Zachary karate club network [65], the football network [6], and the political books network.", "startOffset": 88, "endOffset": 91}, {"referenceID": 6, "context": "Rosvall\u2019s algorithm uses a description length metric to select the number of clusters [7]; Blondel\u2019s algorithm chooses the partitioning that maximizes the modularity [56].", "startOffset": 86, "endOffset": 89}, {"referenceID": 54, "context": "Rosvall\u2019s algorithm uses a description length metric to select the number of clusters [7]; Blondel\u2019s algorithm chooses the partitioning that maximizes the modularity [56].", "startOffset": 166, "endOffset": 170}, {"referenceID": 63, "context": "Here we study the networks presented in [66] which are weighted graphs, each with 998 nodes.", "startOffset": 40, "endOffset": 44}, {"referenceID": 63, "context": "[66]; Hagmann et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 63, "context": "The authors of [66] also used modularity maximization and found six modules, two of which included nodes from both of the hemispheres (central clusters).", "startOffset": 15, "endOffset": 19}, {"referenceID": 64, "context": "We apply our algorithm to four networks with different natures: Cond-Mat (a collaboration network) [67], Googleweb (a web graph) [60], Amazon (a product co-purchasing network)[68], and ASSkitter (an autonomous system graph)[69].", "startOffset": 99, "endOffset": 103}, {"referenceID": 65, "context": "We apply our algorithm to four networks with different natures: Cond-Mat (a collaboration network) [67], Googleweb (a web graph) [60], Amazon (a product co-purchasing network)[68], and ASSkitter (an autonomous system graph)[69].", "startOffset": 175, "endOffset": 179}, {"referenceID": 66, "context": "We apply our algorithm to four networks with different natures: Cond-Mat (a collaboration network) [67], Googleweb (a web graph) [60], Amazon (a product co-purchasing network)[68], and ASSkitter (an autonomous system graph)[69].", "startOffset": 223, "endOffset": 227}, {"referenceID": 6, "context": "The Rosvall-Bergstrom algorithm [7] was terminated after more than 30 hours without converging to a solution.", "startOffset": 32, "endOffset": 35}, {"referenceID": 54, "context": "[56] takes 8 minutes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The Dhillon-Guan-Kulis algorithm [11] takes 72 seconds for k = 57 and increases as k is increased.", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "This behavior of the Dhillon-GuanKulis algorithm is due to the underlying region-growing procedure it adopts from Metis [17].", "startOffset": 120, "endOffset": 124}, {"referenceID": 67, "context": "This is more suitable for VLSI applications for instance [72].", "startOffset": 57, "endOffset": 61}, {"referenceID": 68, "context": "An extension of GANC can be developed by adopting the generalized normalized cut criterion [73].", "startOffset": 91, "endOffset": 95}], "year": 2011, "abstractText": "This paper describes a graph clustering algorithm that aims to minimize the normalized cut criterion and has a model order selection procedure. The performance of the proposed algorithm is comparable to spectral approaches in terms of minimizing normalized cut. However, unlike spectral approaches, the proposed algorithm scales to graphs with millions of nodes and edges. The algorithm consists of three components that are processed sequentially: a greedy agglomerative hierarchical clustering procedure, model order selection, and a local refinement. For a graph of n nodes and O(n) edges, the computational complexity of the algorithm is O(n log n), a major improvement over the O(n) complexity of spectral methods. Experiments are performed on real and synthetic networks to demonstrate the scalability of the proposed approach, the effectiveness of the model order selection procedure, and the performance of the proposed algorithm in terms of minimizing the normalized cut metric.", "creator": "LaTeX with hyperref package"}}}