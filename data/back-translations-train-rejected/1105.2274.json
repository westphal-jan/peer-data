{"id": "1105.2274", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-May-2011", "title": "Data-Distributed Weighted Majority and Online Mirror Descent", "abstract": "In this paper, we focus on the question of the extent to which online learning can benefit from distributed computing. We focus on the setting in which $N$ agents online-learn cooperatively, where each agent only has access to its own data. We propose a generic data-distributed online learning meta-algorithm. We then introduce the Distributed Weighted Majority and Distributed Online Mirror Descent algorithms, as special cases. We show, using both theoretical analysis and experiments, that compared to a single agent: given the same computation time, these distributed algorithms achieve smaller generalization errors; and given the same generalization errors, they can be $N$ times faster.", "histories": [["v1", "Wed, 11 May 2011 18:59:13 GMT  (863kb)", "http://arxiv.org/abs/1105.2274v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["hua ouyang", "alexander gray"], "accepted": false, "id": "1105.2274"}, "pdf": {"name": "1105.2274.pdf", "metadata": {"source": "META", "title": "Data-Distributed Weighted Majority and Online Mirror Descent", "authors": [], "emails": ["houyang@cc.gatech.edu", "agray@cc.gatech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 110 5.22 74v1 [cs.LG] 1 1M ay"}, {"heading": "1. Introduction", "text": "An intelligent agent in this system can learn from two sources: examples from the environment and information from other agents. One way to ask the question answered by the Data-Distributed Online Learning (DDOL) programs we have introduced can informally be described as follows: within a networked network of learning agents, even though an agent receives only m samples of input data, can he be made to act as if he had received M > m samples? Here, performance is measured by generalization skills (predictive error or regret for the online setting). In other words, to what extent can an agent make fewer generalization errors by uti-preliminary work."}, {"heading": "1.1. Related Work", "text": "In an empirical study (Delalleau & Bengio, 2007), the authors proposed solving a trade-off between batch and stochastic gradient descent by using averaged mini-batches of size 10 \u0445 100. In (Mann et al., 2009), an averaging scheme was proposed to solve a batch-regulated problem of conditional maximum entropy optimization, in which the distributed parameters of each agent are averaged in the final phase. In (Duchi et al., 2010), a distributed dual averaging descent algorithm was proposed to minimize convex empirical risks through decentralized networks, and in (Johansson et al., 2009) an incremental subgradient method using a Markov chain."}, {"heading": "2. Setup and DDOL Meta-Algorithm", "text": "In this thesis, we assume that each agent only has access to a portion of the data locally and communicates with other agents. Suppose we have N learning agents. In round t, the ith learning agent Ai: i = 1,..., N receives an example xti-RD from the environment and makes a prediction yti. The environment then reveals the correct response lti corresponding to x t i and the agent suffers a loss L (xti, y t i, l t i). The parameter set of an agent Ai at a given time t is wti-W. Each agent is a vertex of a connected graph G = (A, E). There will be a bidirectional communication between Ai and Aj if they are neighbors (connected by an edge eij). Ai has Ni \u2212 1 neighbors. The generic meta algorithm for Data Distributed Online Learning (DDOL) is very simple: Each Ai works according to the following update: an algorithm for online learning (D1) and an answer for DI (2)."}, {"heading": "3. Distributed Weighted Majority", "text": "The following re-algorithms 2 DWM-I: agent Ai 1: Initialize all weights w0i1:, w 0 iPof P shared experts for agent Ai., w 0 iPof P shared experts for agent Ai., w 0 iPof P shared experts for agent Ai., w 0 iPof P shared experts in the same way as any other agents do as any other agents as a).Alg. 2 is named Distributed re-algorithm 2 DWM-I: agent Ai 1: Initialize all weights w0i1,., w 0 iPof P shared experts for agent Ai to 1.2: for agent = 1,."}, {"heading": "4. Distributed Online Mirror Descent", "text": "In this section, we expand the idea of distributed online learning on online convex optimization (OCO) problems (OCO = individual T = 1997). < W is an online variant of convex optimization that is ubiquitous in many machine learning problems, such as support vector machines, logistical regression, and sparse signal reconstruction tasks. < W is each of these learning algorithms a convex loss function that needs to be minimized. \u2212 W is a repetitive game between an algorithm A and the environment. In each round, A selects a strategy wt-W and the environment reveals a convex function ft. Here, we assume that all convex functions share the same feasible set W. The goal of A is to minimize the difference between the accumulation (wt) and the best strategies it can play in hindsight. This difference is generally known as external regret, defined as below.4.1."}, {"heading": "Distributed OGD", "text": "Assuming the following, an agent Ai must solve the minimization minz \u0445 Ni = 1 [\u03b7t < gti, z \u2212 wti > + 12 \u0445 z \u2212 wti \u0445 22], which results in a simple DOGD updating rule + 1i \u2190 1NiNi \u0445 j = 1 (wtj \u2212 \u03b7tgtj). (9)"}, {"heading": "Distributed OEG", "text": "The unnormalized relative entropie (u, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v, v,"}, {"heading": "5. Experimental Study", "text": "In this section, several sets of online classification experiments are used to evaluate the theories and proposed distributed online learning algorithms. Three real binary class datasets 1 from different application areas are used. Table 1 summarizes these datasets and the parameters used in Section 5.2. To simulate the behavior of multi-agents, we use Pthreads (POSIX threads) for multi-step programming, where each thread is an agent, and they communicate with each other via shared memory. Barriers are used for synchronization. All exper-1www.csie.ntu.edu.tw / ~ cjlin / libsvmtools / datasets / iments are performed on a workstation with a 4-core 2.7GHz Intel Core 2 quad CPU."}, {"heading": "5.1. Distributed Weighted Majority", "text": "To evaluate the proposed DWM algorithms, the simplest decision stumps are selected as experts, and all experts are trained offline. We randomly select P \u2264 D dimensions. Within each dimension, 200 probes are placed evenly between the minimum and maximum values of that dimension. However, the probe with the minimum training error over the entire dataset is selected as the decision threshold. In all subsequent weighted majority experiments, we select the tightening factor \u03b1 = 0.9. The first set of experiments reports the behavior of DWM-I and DWM-A from the point of view of the individual agent. Each agent shares the same P = 4 experts and communicates with all others. Fig. 1 and 2 show the cumulative number of false predictions made by each thread as a function of the number of samples accessed by a single agent, comparing 1, 2, 3 and 4 agents. Each plot in a subfigure represents an agent. It is clear that Ai makes less errors as he receives more information from his neighbors."}, {"heading": "5.2. Distributed Online Mirror Descent", "text": "In this section, several online classification experiments are performed using the proposed DOGD and DOEG algorithms = > ltate = otherwise. For DOGD, we choose the L2 regulated instance hinge loss function as our convex objective function: ft (w) = Cmax {0, 1 \u2212 ltwTxt} + 0, w \u00b2 -22 / 2.For DOEG, we use ft (w) = max {0, 1 \u2212 ltwTxt} and W = {w: w \u00b2 1 \u2264 1 \u2264 S}. Since the update rule (10) cannot change the characters of wt, we use a similar trick likeEG \u00b1 proposed in (Kivinen & Warmuth, 1997), i.e. leave w = w + \u2212 w \u2212 w \u2212 w \u00b2 -0, w \u2212 0. Since we cannot compare the generalization capacities between these two algorithms, the generalization capacity in all the following experiments is that the parameters are selected by DOxt without further matching the DOxt and DOEG parameters according to 1."}, {"heading": "6. Conclusions and Future Work", "text": "As concrete examples, two sets of distributed algorithms have been derived: one is for the distributed weighted majority, the other for distributed convex online optimization, and their effectiveness is supported by both analysis and experiments.The analysis shows that DWM can have a 1 / N lower upper error limit for N agents than when using a single agent, and from a social point of view, the limit of the total number of errors made by all N agents is also much lower than when using 1 agent while processing the same number of examples at the same time. This suggests that DWM reaches the same level of generalization errors as WM, but is N times faster than when using a single agent.The average individual regret for DOMD algorithms is also much lower than for OMD, although it is not 1 / N lower than for DWM. In the worst case, the limit of social regret is higher than when using a single agent."}, {"heading": "Dekel, O., Gilad-Bachrach, R., Shamir, O., and Xiao,", "text": "L. Optimal distributed online prediction using minibatches. In the NIPS 2010 Workshop on Learning on Cores, Clusters and Clouds, 2010. Delalleau, O. and Bengio, Y. Parallel stochastic gradient descent. In CIAR Summer School, 2007."}, {"heading": "Duchi, J., Agarwal, A., and Wainwright, M. Distributed", "text": "Dual averaging in networks. In NIPS, 2010."}, {"heading": "Hazan, Elad, Agarwal, Amit, Kalai, Adam, and Kale,", "text": "Satyen. Logarithmic regret algorithms for online convex optimization. In COLT, 2006."}, {"heading": "Johansson, Bjorn, Rabi, Maben, and Johansson, Mikael.", "text": "A randomized, incremental method for distributed optimization in networked systems. SIAM J. Optim., 20 (3): 1157-1170, 2009."}, {"heading": "Kivinen, Jyrki and Warmuth, Manfred K. Exponentiated", "text": "Information and Computation, (132): 1-63, 1997.Langford, John, Smola, Alexander J., and Zinkevich, Martin. Slow learners are fast. arXiv Submitted, 2009.Littlestone, Nick. Fast learning when irrelevant attributes abound: A new algorithm with linear threshold. Machine learning, 2: 285-318, 1988."}, {"heading": "Littlestone, Nick and Warmuth, Manfred K. The weighted", "text": "In Foundations of Computer Science, 30th Annual Symposium on, pp. 256-261, 1989."}, {"heading": "Mann, G., McDonald, R., Mohri, M., Silberman, N., and", "text": "Walker, D. D. Efcient Large-scale distributed training of conditional maximum entropy models. In NIPS, 2009.Mesterharm, Chris. Online learning with delayed label feedback. In: Algorithmic Learning Theory, pp. 399-413, 2005.Nedic, Angelia and Ozdaglar, Asuman. Distributed subgradient methods for multi-agent optimization. IEEE Trans. on Automatic Control, 54 (1): 48-61, 2009.Nemirovsky, A. S. and Yudin, D. B. Problem complexity and method efficiency in optimization. John Wiley & Sons, 1983.Zinkevich, Martin. Online convex programming and generalized infinitesimal gradient ascendancy. In ICML, 2003."}], "references": [{"title": "Optimal distributed online prediction using minibatches", "author": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "venue": "In NIPS 2010 Workshop on Learning on Cores, Clusters and Clouds,", "citeRegEx": "Dekel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Dekel et al\\.", "year": 2010}, {"title": "Parallel stochastic gradient descent", "author": ["O. Delalleau", "Y. Bengio"], "venue": "In CIAR Summer School,", "citeRegEx": "Delalleau and Bengio,? \\Q2007\\E", "shortCiteRegEx": "Delalleau and Bengio", "year": 2007}, {"title": "Distributed dual averaging in networks", "author": ["J. Duchi", "A. Agarwal", "M. Wainwright"], "venue": "In NIPS,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["Hazan", "Elad", "Agarwal", "Amit", "Kalai", "Adam", "Kale", "Satyen"], "venue": "In COLT,", "citeRegEx": "Hazan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2006}, {"title": "A randomized incremental subgradient method for distributed optimization in networked systems", "author": ["Johansson", "Bjorn", "Rabi", "Maben", "Mikael"], "venue": "SIAM J. Optim.,", "citeRegEx": "Johansson et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Johansson et al\\.", "year": 2009}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["Kivinen", "Jyrki", "Warmuth", "Manfred K"], "venue": "Information and Computation,", "citeRegEx": "Kivinen et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kivinen et al\\.", "year": 1997}, {"title": "Slow learners are fast", "author": ["Langford", "John", "Smola", "Alexander J", "Zinkevich", "Martin"], "venue": "arXiv Submitted,", "citeRegEx": "Langford et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Langford et al\\.", "year": 2009}, {"title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", "author": ["Littlestone", "Nick"], "venue": "Machine Learning,", "citeRegEx": "Littlestone and Nick.,? \\Q1988\\E", "shortCiteRegEx": "Littlestone and Nick.", "year": 1988}, {"title": "The weighted majority algorithm", "author": ["Littlestone", "Nick", "Warmuth", "Manfred K"], "venue": "In Foundations of Computer Science, 30th Annual Symposium on,", "citeRegEx": "Littlestone et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Littlestone et al\\.", "year": 1989}, {"title": "Efcient large-scale distributed training of conditional maximum entropy models", "author": ["G. Mann", "R. McDonald", "M. Mohri", "N. Silberman", "D.D. Walker"], "venue": "In NIPS,", "citeRegEx": "Mann et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mann et al\\.", "year": 2009}, {"title": "Online learning with delayed label feedback", "author": ["Mesterharm", "Chris"], "venue": "In Algorithmic Learning Theory, pp", "citeRegEx": "Mesterharm and Chris.,? \\Q2005\\E", "shortCiteRegEx": "Mesterharm and Chris.", "year": 2005}, {"title": "Distributed subgradient methods for multi-agent optimization", "author": ["Nedic", "Angelia", "Ozdaglar", "Asuman"], "venue": "IEEE Trans. on Automatic Control,", "citeRegEx": "Nedic et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Nedic et al\\.", "year": 2009}, {"title": "Problem Complexity and Method Efficiency in Optimization", "author": ["A.S. Nemirovsky", "D.B. Yudin"], "venue": null, "citeRegEx": "Nemirovsky and Yudin,? \\Q1983\\E", "shortCiteRegEx": "Nemirovsky and Yudin", "year": 1983}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Zinkevich", "Martin"], "venue": "In ICML,", "citeRegEx": "Zinkevich and Martin.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich and Martin.", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "A parameter averaging scheme was proposed in (Mann et al., 2009) to solve a batch regularized conditional max entropy optimization problem, where the distributed parameters from each agent is averaged in the final stage.", "startOffset": 45, "endOffset": 64}, {"referenceID": 4, "context": "(Nedic & Ozdaglar, 2009) and an incremental subgradient method using a Markov chain was proposed in (Johansson et al., 2009).", "startOffset": 100, "endOffset": 124}, {"referenceID": 2, "context": "In (Duchi et al., 2010), a distributed dual averaging algorithm was proposed for minimizing convex empirical risks via decentralized networks.", "startOffset": 3, "endOffset": 23}, {"referenceID": 0, "context": "The same idea of averaged subgradients was extended to centralized online learning settings in (Dekel et al., 2010).", "startOffset": 95, "endOffset": 115}, {"referenceID": 3, "context": "We will present the online mirror descent (OMD) framework which generalize many OCO algorithms such as online subgradient descent (Zinkevich, 2003), Winnow (Littlestone, 1988), online exponentiated gradient (Kivinen & Warmuth, 1997), online Newton\u2019s method (Hazan et al., 2006).", "startOffset": 257, "endOffset": 277}], "year": 2011, "abstractText": "In this paper, we focus on the question of the extent to which online learning can benefit from distributed computing. We focus on the setting in whichN agents online-learn cooperatively, where each agent only has access to its own data. We propose a generic datadistributed online learning meta-algorithm. We then introduce the Distributed Weighted Majority and Distributed Online Mirror Descent algorithms, as special cases. We show, using both theoretical analysis and experiments, that compared to a single agent: given the same computation time, these distributed algorithms achieve smaller generalization errors; and given the same generalization errors, they can be N times faster.", "creator": "LaTeX with hyperref package"}}}