{"id": "1105.4318", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2011", "title": "Correction of Noisy Sentences using a Monolingual Corpus", "abstract": "Correction of Noisy Natural Language Text is an important and well studied problem in Natural Language Processing. It has a number of applications in domains like Statistical Machine Translation, Second Language Learning and Natural Language Generation. In this work, we consider some statistical techniques for Text Correction. We define the classes of errors commonly found in text and describe algorithms to correct them. The data has been taken from a poorly trained Machine Translation system. The algorithms use only a language model in the target language in order to correct the sentences. We use phrase based correction methods in both the algorithms. The phrases are replaced and combined to give us the final corrected sentence. We also present the methods to model different kinds of errors, in addition to results of the working of the algorithms on the test set. We show that one of the approaches fail to achieve the desired goal, whereas the other succeeds well. In the end, we analyze the possible reasons for such a trend in performance.", "histories": [["v1", "Sun, 22 May 2011 09:01:38 GMT  (99kb,D)", "http://arxiv.org/abs/1105.4318v1", "67 pages, 2 figures, 4 tables, 2 algorithms"]], "COMMENTS": "67 pages, 2 figures, 4 tables, 2 algorithms", "reviews": [], "SUBJECTS": "cs.DL cs.AI", "authors": ["diptesh chatterhee"], "accepted": false, "id": "1105.4318"}, "pdf": {"name": "1105.4318.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Arnab Sengupta", "Rahul Sarkar"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 110 5.43 18v1 [cs.DL] 2 2M ay2 011Correction of Noisy Sentences using aMonolingual CorpusThesis, filed in partial compliance with the requirements for the degree of Master of TechnologyinComputer Science and Engineeringby Diptesh ChatterjeeRoll No. 06CS3031Under the supervision of Dr. Sudeshna SarkarDepartment of Computer Science and EngineeringIndian Institute of Technology, KharagpurWest Bengal, India, 721302"}, {"heading": "CERTIFICATE", "text": "This is to confirm that the dissertation entitled \"Correction of NoisySentences using a Monolingual Corpus\" by Diptesh Chatterjee (Roll No. 06CS3031) is submitted in partial fulfilment of the requirements for the conferral of the degree of Master of Technology in Computer Science and Engineering at the Indian Institute of Technology, Kharagpur, India. It is a faithful documentation of the work he has done at the Institute of Informatics and Engineering, IIT Kharagpur, under Myguide and supervision."}, {"heading": "Prof. Sudeshna Sarkar", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Department of Computer Science and Engineering", "text": "IIT Kharagpur"}, {"heading": "ACKNOWLEDGEMENT", "text": "I would like to express my sincere gratitude to Prof. Sudeshna Sarkar for being available to me as a consultant and giving me the opportunity to research in the field of my interest. I am indebted to her throughout my project work in the Department of Computer Scienceand Engineering at IIT Kharagpur. I am truly grateful to my friends Arnab Sengupta, Rahul Sarkar, KoushikHembram and Ashish Jhunjhunwala who encouraged me at every stage of my project and were there for me every time the chips were on the ground. I also thank Mr. Sanjay Chatterjee for providing some data to test my algorithm. I would also like to take this opportunity to express my sincere gratitude to my parents and family members who have been an endless source of encouragement for me."}, {"heading": "DECLARATION", "text": "I certify that, 1. The thesis contained in the dissertation is original and was carried out by myself under the general supervision of my Superiors.2. The work has not been submitted to any other Institute for any study or diploma. 3. I have followed the Institute's guidelines in writing theses. 4. I have adhered to the norms and guidelines of the Institute's Code of Ethical Conduct. 5. Whenever I have used materials (data, theoretical analyses and texts) from other sources, I have given them due recognition by quoting them in the text of the dissertation and their details in the Reference. 6. Whenever I have quoted written materials from other sources, I have put them in quotation marks and duly appreciated the sources by quoting them and providing the necessary details in the Reference. Diptesh Chatterjee"}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "List of Figures iii", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "List of Tables iv", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "List of Algorithms v", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 1", "text": "1.1 Language modeling.......................... 21.2 Machine translation decoding............."}, {"heading": "2 Previous Work and Problem Definition 7", "text": "2.1 Literature review.............................. 72.2 Troubleshooting.............................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3 Fixed Length Phrase-based Correction Model 19", "text": "3.1 Algorithm.................................................................................................................."}, {"heading": "4 The Dynamic Programming based method 24", "text": "4.1 Motivation............................ 24i4.2 Algorithm.................................. 254.3. Computer calculation of phrase replacement.............. 294.3.1 Levenshtein distance between individual words.................................................. 314.3.2 Synset distance in Wordswords.............."}, {"heading": "5 Conclusion 50", "text": "Bibliography 56"}, {"heading": "List of Figures", "text": "1.1 Noisy Channel Model of Statistical Machine Translation... 44.1 An illustration of how the dynamic programming algorithm for N = 5....................... 28iii"}, {"heading": "List of Tables", "text": "224.1 The distance functions used for experiments....... 414.2 Summary of the results for all the distance functions used... 424.3 Results of the experiments with twice translated sentences from the FIRE corpus............... 43iv."}, {"heading": "List of Algorithms", "text": "1 Fixed-length phrases algorithm for calculating the best substitution.................................... 202 Correction algorithm based on dynamic programming...... 27v"}, {"heading": "Abstract", "text": "The correction of loud texts of natural language is an important and well-studied problem in the processing of natural language. It has a number of applications in areas such as statistical machine translation, second language learning and natural language generation. In this thesis, we will consider some statistical techniques for text correction. We define the common error classes in the text and describe algorithms for their correction. The data comes from a poorly trained machine translation system. The algorithms use only one language model in the target language to correct the sentences. We use phrase correction methods in both algorithms. The phrases are replaced and combined to give us the final corrected sentence. We use the methods for modeling various types of errors, in addition to the results of the work of the algorithms on the test sentence. We show that one of the approaches does not achieve the desired goal, while the other is successful.At the end, we will analyze the possible reasons for such a development in 1. chapter."}, {"heading": "Introduction", "text": "One of the biggest challenges in processing natural language (NLP) is generating correct sentences of natural language, and the degree of correctness is measured by how correct the sentence is syntactically and semantically. The branch of the NLP that deals with this task is known as the Natural Language Generation. However, Natural Language is like an infinite labyrinth of ambiguities, through which traveling seems to be a fairly monumental task. So far, we do not have a satisfactory computer model of the human brain - one that can explain how people really learn, how knowledge is stored, and how it is that we can retrieve information extremely quickly [1]. The algorithms and models used in processing natural languages are nothing more than a collection of mere approximations that work well on a very select amount of data. Thus, it is quite obvious that such imperfect models are unable to generate sentences that exhibit the quality of sentences actually composed by humans."}, {"heading": "1.1 Language Modeling", "text": "In fact, a language model that is calculated using a corpus of a particular language is usually the case that the larger and more diverse corpus, the language model that is calculated, is better. The above definition defines an n-gram language model in which n is known as the order of the model. A language model is in fact a Markov chain, because the probability of a word occurring depends on its preceding words. These probability values are used to evaluate coherent sequences of words. In the face of a sequence of M-words S = s2, s2,. sM that is assigned to it by the language model: PLM (S) = j P (sj | sj \u2212 2, explicitly Wen)."}, {"heading": "1.2 Decoding in Machine Translation", "text": "This year it is more than ever before."}, {"heading": "Previous Work and Problem", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Definition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Literature Review", "text": "It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. It is. Most people who stand up for the rights of women and men stand up for the rights of men and women. (...) It is not. It is. It is. \"It is.\" (...) It is. \"It is.\" It is. \"It is.\" (...) It is. \"It is.\" It is. \"(...) It is.\" It is. \"(...) It is.\" It is. (...) It is. \"(...) It is.\" (...) It is. (...) It is. (...) It is. It is. (...) It is. It is. It is. (...) It is. It is. (...) It is. It is. It is. (...) It is. It is. It is. It is. It is. (...) It is. It is. It is. It is. It is. It is. (...) It is. It is. It is. It is. It is. It is. It is. It is. It is. (...). It is. It is. It is. It is."}, {"heading": "2.2 The Problem Statement", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "2.3 Evaluation Metrics", "text": "The main purpose of this work is to improve the accuracy of a sentence generated by a language model. Since a language model helps to measure the fluidity of a text, the most important quantity to measure is fluidity. Although fluidity is not a sufficient measure to measure how good a piece of text is, this is because it is possible to string a series of significant parts together to form a sentence. Such a sentence receives a good score according to the language model, but does not count as a good sentence in the language. Consequently, we should also measure the correctness of the text. Correctness can be measured by how close a particular piece of text is to a human-written piece of text. This is done by comparing the machine-generated text with a standard reference text written by people. To measure the correctness of sentences quantitatively, we need numerical methods to assess the fluctuation and correctness of the sentences generated."}, {"heading": "2.3.1 Perplexity", "text": "Perplexity helps determine how likely a given sentence is according to a language model. In other words, it determines how phrases are ordered by a fluent speaker of the language. Here, the sequence of phrases is the key factor, because perplexity does not take into account any syntactical or semantic rules that could regulate the formation of a correct sentence in any language. Perplexity is a purely information-theoretical measure. We define the average logprob per word as follows: Average logprob (Si \u2212 N \u2212 N) = \u2212 l \u00b2 l = N logPLM (Si | Si \u2212 N \u2212 S2,..) Average logprob per word that defines a probability distribution PLM. Let's assume that LM is a N logPLM (Si \u2212 N \u2212 Si \u2212 N) formal value if the average logprob per word is defined as follows: Average logprob (LP) = l \u00b2 l \u00b2 PLM = (SN \u2212 N) logM."}, {"heading": "2.3.2 BLEU Score", "text": "The fact is that it is able to come out on top in the way that it is and in the way that it is, in the way that it is."}, {"heading": "2.4 System Overview", "text": "The entire experimental procedure can be divided into the following components: 1. Obtaining noisy sentences from natural language; 2. Constructing the language model; 3. Obtaining the best replacement for a noisy input sentence. From the number of aforementioned sources from which noisy data could be made available, we used poor machine translation in our experiments; to obtain the noisy data, we used a System Statistical Machine17Translation (SMT) trained on a very small amount of data; from this small amount of data, the required language model for the translation system was derived; this data was transmitted to GIZA + + [38] to learn the translation model; the language model was created using the language modeling tool SRILM [39]; the translation model and the language model were then fed to MOSES [40], which translated the sentences from the source language into the target language. These translated sentences are the loud sentences, which we try to correct in the next step."}, {"heading": "Fixed Length Phrase-based", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Correction Model", "text": "Finding the optimal phrase segmentation is the most important part of our task. In the first approach, we try to use phrases of fixed length for this purpose. The replacement mechanism we follow is local to the phrases themselves, and the substitution of one phrase does not affect the substitution of another phrase in any way."}, {"heading": "3.1 Algorithm", "text": "The first approach is the fixed-length phrase-based approach. In this approach, we divide the sentence into a set of fixed-length phrases and calculate the best replacement for each phrase with overlapping n-grams. The algorithm then performs a set of local substitutions on each phrase. Let's take an example of how this algorithm works: Let the phrase be limited to P = p1p2. Let's get the language model in order = 4 First, let's look at the subphrase p1p2p3 p4. Let's get the sentences for this phrase to the point: 191. R1 = x2xxxx3x2 p2 p2. R2 = y2y4First, let's replace the subphrase p1. We get P's for x1x1x2x3p7. Next, let's make a recursive call to the function with P."}, {"heading": "3.2 Experiments and Results", "text": "For our experiments, we used English as the target language. As already mentioned, due to the lack of data from other areas, we used data from 21 poor machine translation systems to test our algorithms. To test the performance of this approach, we used the Europarl [41] corpus. We used 1000 sentences from the English-German bittext of Europarl to train the machine translation system. 100 English sentences comprised the test sentence. To create the language model, we used the entire English part of the Europarl corpus. For the perplexity calculation, we used the SRILM perplexity calculation module. For the experimentation, we used a 4 gram language model, i.e. n = 4. The length of the phrase size was chosen to 7. The results obtained from the evaluation of the 100 sentences in the test sentence will be shown in Table 3.1.We can see that this algorithm does not perform well. In fact, we do not see any change in the EU sentence size or the fact that the sentences do not change in the amount of the fact."}, {"heading": "3.3 Analysis", "text": "An analysis of the steps of this algorithm reveals the fact that this algorithm is likely to perform too many substitutions, thereby actually trying to increase the noise in the sentences. Also, people usually do not try to make changes in sentences in this way. Corrections are made to phrases. A sentence is taken as a unit, and some changes 22 are made in that sentence, so that the resulting phrase resembles the actual phrase in some way, but the errors of the original phrase are eliminated. Moreover, this algorithm does not deal with interphratic correlations, for which a conditional probability should be used between phrases. All of these factors contribute to the failure of the algorithm. Another factor that does not support the use of this algorithm is the time complexity. For each phrase, the algorithm takes time, which is exponentially in the number of replacement candidates. To understand the temporal complexity of the algorithm, let us use the phrase replacement method as a graph."}, {"heading": "The Dynamic Programming", "text": ""}, {"heading": "4.1 Motivation", "text": "The failure of the fixed length of the phrase approach led us to conclude that this approach is only trying to increase the amount of errors already present in the sentences by trying to make too many substitutions. So, we assume that an approach that takes a phrase as a unit and corrects a phrase in one step after identifying the errors could be a good way. Also, the previous approach did not have a function that explicitly models all the previously mentioned errors. Consequently, in this approach, we also need to modify the function used to select the substitutions for the phrases. Another problem with the previous approach is its exponential time complexity and the fact that it uses up a lot of memory due to its recursive nature. We need to find a more efficient way to select the best phrase segmentation of a sentence. We also want to allow variable length phrases. All this can be achieved by a simple recursive formulation of the problem we have as follows a sentence like S = 1 supplement."}, {"heading": "4.2 Algorithm", "text": "It is a sentence that can be divided into an O (N2) number of contiguous phrases. Each phrase can begin at any Si and end at any Sj that meets the condition 1 \u2264 i \u2264 j. For this purpose, we designate a phrase that starts at Si and ends at Sj as Pij. Let the language model of LM be called. First, the algorithm calculates a list of substitutions for each Pij from LM. For this purpose, a function is called FIND BEST SUB (). It takes a phrase P and returns a list of the best replacement phrases for P from LM. For now, we treat this function as a black field. This function is detailed in next25section.Let the list of top-k substitutes for Pij."}, {"heading": "4.3 Computing the Phrase Substitutes", "text": "This function takes every single argument (the phrase p) and returns a list of 2-tuples of the form (q, s) where q is a candidate substitute for p and q has a score of s according to the language model that is used. As mentioned in the previous section, naive implementation of this function by means of linear search and a distance function can lead to very high time complexity. So, efficient implementation is essential. This problem can be modeled as information that is used as a task. We can treat each phrase in the language model as a document and consider the replacement candidate as the query. Our goal is to find out which documents best match the query and also have high scores to calculate the best list, we follow a 2-step process: 1. From all phrases in the language model, we calculate the top T phrases that correspond to the replacement candidate."}, {"heading": "4.3.1 Levenshtein distance between individual words", "text": "In fact, it is such that it is an \"imperfect,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \"incomplete,\" \",\" \"incomplete,\" \",\" \"incomplete,\" \"\", \"\" \",\" \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\" \"incomplete,\" \"\" \"\", \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\" \",\" \"\" \"\" \"\" \",\" \"\" \"\" \"\", \"\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\" \",\" \"\" \",\" \"\" \"\" \"\", \"\" \"\" \"\" \",\" \"\" \"\" \"\", \"\""}, {"heading": "4.3.2 Synset Distance in Wordnet", "text": "Almost all languages have an extremely extensive vocabulary. As a result, it is almost impossible for a single person to remember all the words of a language. It is not uncommon for words to be misused. For example, it is often the case that a word is used in a sentence, while one of its synonyms or a closely related word must be used in its place. Consider the following sentence:"}, {"heading": "He was a one game surprise.", "text": "A more correct and semantically appropriate sentence, however, would be:"}, {"heading": "He was a one game wonder.", "text": "To calculate this metric, we use the WordNet [45], a lexical database for the English language, which groups English words into sets of synonyms called synsets, provides short, generic definitions, and records the various semantic relationships between these synonym sentences. It has a dual purpose: to create a combination of dictionary and thesaurus that is more intuitive to use, and to support automatic text analysis and artificial intelligence applications. The database and software tools have been published under a BSD license and are freely downloadable and usable, and the database can also be searched online. Each synthesis in the WordNet consists of words with similar semantics, or in short, synonyms."}, {"heading": "4.3.3 Word order based metrics", "text": "This year it has come to the point where it will be able to put itself at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said."}, {"heading": "4.4 A Representative Example", "text": "Consider the following sentence: 36the europe extreme right in is characterized by its 2. and its use of migration as differences of the issue of.We can see that there are a number of errors in this sentence. For example: 1. The formulation europ extreme right should become european extreme right of europe.2. The word in should not be moved to the right or replaced by anything else. Let's see how the algorithm works on this sentence. First, it calls the function FIND BEST SUB () to calculate the best substitutions for all possible phrases. We have used the 5 best substitutions in this experiment. A snapshot of the substitution of some of the more significant phrases is given together with the results."}, {"heading": "4.5 Experiments and Results", "text": "In fact, it is as if most of us are able to go in search of a solution that they have got to grips with. (...) It is not as if they are able to find a solution. (...) It is not as if they are able to find a solution. (...) It is as if they are able to find a solution. (...) It is as if they are able to find a solution. (...) It is as if they are able to find a solution. (...) It is as if they are able to find a solution. \"(...) It is as if they are able to find a solution.\" (...) \"It is as if they are able to find a solution.\""}, {"heading": "4.6 Analysis", "text": "This year it is more than ever before."}, {"heading": "Conclusion", "text": "In this paper, we present two algorithms for the correction of noisy sentences, one of which performs relatively well while the other fails completely. Indeed, this approach formulates the problem as a decoding task and uses a model similar to decoding-based models to make corrections to the text. This model is capable of correcting a number of errors and increasing the frequency of sentences used. We have also proposed a number of applications of this algorithm, such as the correction of machine translations and domain-specific correction of machine translations, and as an example, we have examined machine translations and increased the frequency of sentences used."}], "references": [{"title": "Computational models of the brain: from structure to function", "author": ["M. Breakspear", "V. Jirsa", "G. Deco"], "venue": "Neuroimage, 3(52)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Foundations of Sttistical Natural Language Processing", "author": ["Christopher D. Manning", "Hinrich Schutze"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "The mathematic of statistical machine translation: Parameter estimation", "author": ["Peter F. Brown", "Stephen Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Minimum error rate training in statistical machine translation", "author": ["Franz Josef Och"], "venue": "In ACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Statistical phrasebased translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu"], "venue": "In HLT-NAACL,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Greedy decoding for statistical machine translation in almost linear time", "author": ["Ulrich Germann"], "venue": "In HLT-NAACL,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Pharaoh: A beam search decoder for phrase-based statistical machine translation models", "author": ["Philipp Koehn"], "venue": "In AMTA,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2004}, {"title": "Generation of word graphs in statistical machine translation", "author": ["N. Ueffing", "F.J. Och", "H. Ney"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "A decoder for syntax-based statistical mt", "author": ["Kenji Yamada", "Kevin Knight"], "venue": "In ACL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Hierarchical phrase based translation", "author": ["David Chiang"], "venue": "Computational Linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Techniques for automatically correcting words in text", "author": ["Karen Kukich"], "venue": "ACM Comput. Surv.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Finding approximate matches in large lexicons", "author": ["Justin Zobel", "Philip Dart"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Hierarchically coded lexicon with variants", "author": ["Fran\u00e7ois de Bertrand de Beuvron", "Philippe Trigano"], "venue": "IJPRAI, 9(1):145\u2013165,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction", "author": ["Kemal Oflazer"], "venue": "Computational Linguistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1996}, {"title": "The string-to-string correction problem", "author": ["Robert A. Wagner", "Michael J. Fischer"], "venue": "J. ACM,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1974}, {"title": "Fast string correction with levenshtein", "author": ["Klaus U. Schulz", "Stoyan Mihov"], "venue": "automata. IJDAR,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Binary codes capable of correcting deletions, insertions, and reversals", "author": ["Vladimir I. Levenshtein"], "venue": "Technical Report", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1966}, {"title": "Language independent text correction using finite state automata", "author": ["Ahmed Hassan", "Sara Noeman", "Hany Hassan"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Applying winnow to contextsensitive spelling correction", "author": ["Andrew R. Golding", "Dan Roth"], "venue": "CoRR, cmp-lg/9607024,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1996}, {"title": "Automatic rule acquisition for spelling correction", "author": ["Lidia Mangu", "Eric Brill"], "venue": "In ICML,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "Proceedings of the Twelfth National Conference on Artificial Intelligense", "author": ["K. Knight", "I. Chander"], "venue": "pages 779\u2013784", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1994}, {"title": "A comparison of merging strategies for translation of german compounds", "author": ["Sara Stymne"], "venue": "In EACL (Student Research", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Using a grammar checker for evaluation and postprocessing of statistical machine translation", "author": ["Sara Stymne", "Lars Ahrenberg"], "venue": "In LREC\u201910,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Improving a statistical mt system with automatically learned rewrite patterns", "author": ["Fei Xia", "Michael McCord"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Statistical machine translation with scarce resources using morpho-syntactic information", "author": ["Sonja Niessen", "Hermann Ney"], "venue": "Comput. Linguist.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "Clause restructuring for statistical machine translation", "author": ["Michael Collins", "Philipp Koehn", "Ivona Kucerova"], "venue": "In ACL,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "Chinese syntactic reordering for statistical machine translation", "author": ["Chao Wang", "Michael Collins", "Philipp Koehn"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Using statistical techniques and web search to correct esl errors", "author": ["Michael Gamon", "Claudia Leacock", "Chris Brockett", "William B. Dolan", "Jianfeng Gao", "Dmitriy Belenko", "Alexandre Klementiev"], "venue": "CALICO Journal,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "From spelling correction to text cleaning - using context information", "author": ["Martin Schierle", "Sascha Schulz", "Markus Ackermann"], "venue": "In GfKl,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "Speech and Language Processing (2nd Edition) (Prentice Hall Series in Artificial Intelligence)", "author": ["Daniel Jurafsky", "James H. Martin"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In ACL,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2002}, {"title": "Error analysis of the written english essays of secondary school students in malaysia: A case", "author": ["Saadiyah Darus", "Kaladevi Subramaniam"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Franz Josef Och", "Hermann Ney"], "venue": "Computational Linguistics,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Srilm \u2013 an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "volume 2, pages 901\u2013904, Denver", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2002}, {"title": "Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ondrej Bojar", "Alexandra Constantin", "Evan Herbst. Moses"], "venue": "In ACL,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2007}, {"title": "Programming languages and their compilers: Preliminary notes", "author": ["John Cocke"], "venue": "Courant Institute of Mathematical Sciences,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1969}, {"title": "Recognition and parsing of context-free languages in time n", "author": ["Daniel H. Younger"], "venue": "Information and Control,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1967}, {"title": "Wordnet: A lexical database for english", "author": ["George A. Miller"], "venue": "Commun. ACM,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "So far, we have no satisfactory computational model of the human brain - one that can explain how humans truly learn, how knowledge is stored and how is it that we can retrieve information extremely fast [1].", "startOffset": 204, "endOffset": 207}, {"referenceID": 1, "context": "Mathematically, a Language Model [2] is expressed as a probability distribution as follows: P (Wi|Wi\u22121Wi\u22122 .", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "Another method used to compute scores for unknown n-grams is known as smoothing [4].", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "The fundamental principle used to model SMT is the noisy channel model [5].", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "The SMT system can be trained on this parallel corpus using techniques such as the IBM models [5] and Minimum Error Rate Training [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 4, "context": "The SMT system can be trained on this parallel corpus using techniques such as the IBM models [5] and Minimum Error Rate Training [7].", "startOffset": 130, "endOffset": 133}, {"referenceID": 3, "context": "decoding [5].", "startOffset": 9, "endOffset": 12}, {"referenceID": 5, "context": "In phrase-based translation models [8], the decoding process involves the following steps:", "startOffset": 35, "endOffset": 38}, {"referenceID": 6, "context": "Greedy Decoder [9]: It starts with the gloss for words and improves the probability by taking the currently cheapest path.", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "Beam Search based decoder [10]: Such a decoder starts translating from left to right and maintains a list of partial hypotheses.", "startOffset": 26, "endOffset": 30}, {"referenceID": 8, "context": "Word Graph based Decoding [11]: Such a decoder uses a search graph over the phrases and uses hypothesis recombination to output the kbest list of top translations.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "String to Tree model [12]: This is also known as parsing based decoding.", "startOffset": 21, "endOffset": 25}, {"referenceID": 10, "context": "It also uses a technique called Cube Pruning [13] to prune the extremely large hypothesis space.", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "For example, Kukich(1992) [15] , Zobel and Dart(1995) [16], De Beuvron and Trinago(1995) [17] proposed similarity keys methods.", "startOffset": 26, "endOffset": 30}, {"referenceID": 12, "context": "For example, Kukich(1992) [15] , Zobel and Dart(1995) [16], De Beuvron and Trinago(1995) [17] proposed similarity keys methods.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "For example, Kukich(1992) [15] , Zobel and Dart(1995) [16], De Beuvron and Trinago(1995) [17] proposed similarity keys methods.", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "Oflazer (1996) [18] suggested a method where all words in a dictionary are treated as a regular language over an alphabet of letters.", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "For each misspelt word, an exhaustive traversal of the dictionary automaton is initiated using a variant of the Wagner-Fisher algorithm [19] to control the traversal of the dictionary.", "startOffset": 136, "endOffset": 140}, {"referenceID": 16, "context": "Schultz and Mihov (2002) [20] present a variant of this approach where the dictionary is also represented as a finite state automaton.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "The acceptor accepts all words that are at a Levenshtein distance [21] k from the input word.", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "al(2008) [22] propose an approach where they assume that the dictionary is represented as a deterministic finite state automaton.", "startOffset": 9, "endOffset": 13}, {"referenceID": 19, "context": "Using a predefined confusion set is a common approach in this task, for example Golding and Roth (1996) [23] and Mangu and Brill (1997) [24].", "startOffset": 104, "endOffset": 108}, {"referenceID": 20, "context": "Using a predefined confusion set is a common approach in this task, for example Golding and Roth (1996) [23] and Mangu and Brill (1997) [24].", "startOffset": 136, "endOffset": 140}, {"referenceID": 21, "context": "But, such methods often target specific phenomena, like correcting English determiners (Knight and Chander, 1994) [25], merging German compounds (Stymne, 2009) [26], or applying word substitution (Elming, 2006) [27].", "startOffset": 114, "endOffset": 118}, {"referenceID": 22, "context": "But, such methods often target specific phenomena, like correcting English determiners (Knight and Chander, 1994) [25], merging German compounds (Stymne, 2009) [26], or applying word substitution (Elming, 2006) [27].", "startOffset": 160, "endOffset": 164}, {"referenceID": 23, "context": "al (2010) [28] also proposed another method", "startOffset": 10, "endOffset": 14}, {"referenceID": 24, "context": "Xia and McCord (2004) [29] describe a method for French, where reordering rules that operate on context-free productions are acquired automatically.", "startOffset": 22, "endOffset": 26}, {"referenceID": 25, "context": "Niessen and Ney (2004) [30] describe an approach for translation from German to English that combines verbs with associated particles, and also reorders questions.", "startOffset": 23, "endOffset": 27}, {"referenceID": 26, "context": "(2005) [31] also describe an approach for German for reordering of German clauses, which have quite different orders from English clauses.", "startOffset": 7, "endOffset": 11}, {"referenceID": 27, "context": "(2007) [32] describe a method of Chinese syntactic reordering for SMT systems.", "startOffset": 7, "endOffset": 11}, {"referenceID": 28, "context": "al (2009) [33] present a system for identifying and correcting English as Second Language (ESL) errors.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "(2007) [34] take spelling correction one step further and use it for text cleaning.", "startOffset": 7, "endOffset": 11}, {"referenceID": 30, "context": "An important thing to note here is that sentences are characterized by two basic characteristics [35]:", "startOffset": 97, "endOffset": 101}, {"referenceID": 31, "context": "Faithfulness : Given by one or more metric like the BLEU score [36]", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Some of the commonly observed errors in written English are [37]:", "startOffset": 60, "endOffset": 64}, {"referenceID": 33, "context": "This data was fed to GIZA++ [38] to learn the translation model.", "startOffset": 28, "endOffset": 32}, {"referenceID": 34, "context": "The language model was created using the SRILM language modeling toolkit [39].", "startOffset": 73, "endOffset": 77}, {"referenceID": 35, "context": "The translation model and the language model were then fed to MOSES [40] decoder which translated the candidate source language sentences into target language sentences.", "startOffset": 68, "endOffset": 72}, {"referenceID": 36, "context": "This formulation is, in fact, identical to the formulation of the famed Cocke-Younger-Kasami (CYK) algorithm [42][43][44] for parsing of context-free grammars.", "startOffset": 109, "endOffset": 113}, {"referenceID": 37, "context": "This formulation is, in fact, identical to the formulation of the famed Cocke-Younger-Kasami (CYK) algorithm [42][43][44] for parsing of context-free grammars.", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "Levenshtein distance [21] between individual words (f1), which models the orthographic errors.", "startOffset": 21, "endOffset": 25}, {"referenceID": 38, "context": "In order to compute this metric, we use the Wordnet [45].", "startOffset": 52, "endOffset": 56}], "year": 2016, "abstractText": "Correction of Noisy Natural Language Text is an important and well studied problem in Natural Language Processing. It has a number of applications in domains like Statistical Machine Translation, Second Language Learning and Natural Language Generation. In this work, we consider some statistical techniques for Text Correction. We define the classes of errors commonly found in text and describe algorithms to correct them. The data has been taken from a poorly trained Machine Translation system. The algorithms use only a language model in the target language in order to correct the sentences. We use phrase based correction methods in both the algorithms. The phrases are replaced and combined to give us the final corrected sentence. We also present the methods to model different kinds of errors, in addition to results of the working of the algorithms on the test set. We show that one of the approaches fail to achieve the desired goal, whereas the other succeeds well. In the end, we analyze the possible reasons for such a trend in performance. Chapter", "creator": "LaTeX with hyperref package"}}}