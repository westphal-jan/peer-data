{"id": "1105.5196", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-May-2011", "title": "Large-Scale Music Annotation and Retrieval: Learning to Rank in Joint Semantic Spaces", "abstract": "Music prediction tasks range from predicting tags given a song or clip of audio, predicting the name of the artist, or predicting related songs given a song, clip, artist name or tag. That is, we are interested in every semantic relationship between the different musical concepts in our database. In realistically sized databases, the number of songs is measured in the hundreds of thousands or more, and the number of artists in the tens of thousands or more, providing a considerable challenge to standard machine learning techniques. In this work, we propose a method that scales to such datasets which attempts to capture the semantic similarities between the database items by modeling audio, artist names, and tags in a single low-dimensional semantic space. This choice of space is learnt by optimizing the set of prediction tasks of interest jointly using multi-task learning. Our method both outperforms baseline methods and, in comparison to them, is faster and consumes less memory. We then demonstrate how our method learns an interpretable model, where the semantic space captures well the similarities of interest.", "histories": [["v1", "Thu, 26 May 2011 03:41:47 GMT  (52kb,S)", "http://arxiv.org/abs/1105.5196v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jason weston", "samy bengio", "philippe hamel"], "accepted": false, "id": "1105.5196"}, "pdf": {"name": "1105.5196.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Philippe Hamel", "Jason Weston", "Samy Bengio"], "emails": ["jweston@google.com", "bengio@google.com", "hamelphi@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 110 5.51 96v1 [cs.LG] 2 6M ay"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Music Annotation and Retrieval Tasks", "text": "Task definitions: In this work, we focus on solving the following annotations and retrieval tasks: 1. Artist Prediction: In the case of a song or audio clip (which is not visible during training), return a ranking of the likely performers who played it. 2. Song Prediction: In the case of an artist's name, return a ranking of songs (which are not visible during training) that were likely performed by that artist. 4. Similar songs: In the case of a song or audio clip (which is not visible during training), return a ranking of songs that are positive (e.g. rock, guitar, fast,...) that best describe the song. Day Prediction: In the case of a song or audio clip (which is not visible during training), return a ranking of songs that are positive (e.g. rock, fast,...) that best describe the song. Rating: If a ranking of literature is returned, we are interested in this."}, {"heading": "3 Semantic Embedding Model for Music Understanding", "text": "The basic idea in our model is that songs, artists, and keywords attributed to music can all be considered together by learning a single model to grasp the semantics and thus the relationships between these musical concepts. Our method assumes that these semantic relationships can be modelled in a dimension d attribute space in which musical concepts (songs, artists, or keywords) are represented as coordinate vectors; the similarity between two concepts is measured by the Dot product between their two vector representations; the vectors are learned to induce similarities that are relevant (i.e., the precision @ k metric for those in Section 2. For a particular artist, indexed by the Dot product between their two vector representations)."}, {"heading": "4 Training the Semantic Embedding Model", "text": "During the training, our goal is to learn the parameters of our model, which provide a good ranking performance on the training set, using precision in the k-measure (with the general goal that this also applies to our test data, of course)."}, {"heading": "4.1 Multi-Task Training", "text": "Suppose we define the objective function for a given task as \u2211 i irr (f (xi), yi), where x is the set of input examples, and y is the set of objectives for these examples, and irr is a loss function that measures the quality of a given precedence (the exact form of this function is discussed in Section 4.2). In the case of the tag prediction task, we want to minimize the function \u0445 i irr (f TP (si), ti), and for the artist prediction task, we want to minimize the function \u2211 i irr (f AP (si), ai). To multitask these two, we simply consider the (unweighted) sum of the two objectives: errAP + TP (D) = m \u00b2 i = 1err (fAP (si), ai) + m \u00b2 i = 1err (fTP (si), ti).We optimize this function by chastostic progression [4]. This boils down to repeating the following task:"}, {"heading": "4.2 Loss Functions", "text": "We consider two loss functions, the standard margin ranking loss and the newly introduced WARP (Weighted Approximately Ranking Pairwise) loss (1).AUC Margin Ranking Loss A standard loss function commonly used for recovery is the margin ranking criterion [5, 6], in particular it has been considered for text embedding models (1).Let's assume that the input x and output y (which can be replaced by artists, songs or tags, depending on the task) is the loss: errAUC (D) = m, i = 1 [1].yi [1]."}, {"heading": "5 Related Approaches", "text": "The task is to give people the opportunity to develop."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Datasets", "text": "TagATune Dataset The TagATune dataset consists of a set of 30-second annotated clips. Each clip is provided with one or more descriptors or tags that represent concepts that can be associated with the given clip, including negative concepts (no voice, no classical music, no drums, etc.).The annotations of the dataset were collected using a web-based game.Details of how the data was collected are described in [22].The TagATune dataset was used in the MIREX 2009 Audio Tag Classification Competition [23].To compare our results with the MIREX 2009 participants, we used the same set of tags and the same turn / test split as in the Contest. Big-Data Dataset We had access to a large proprietary database of tracks and artists from which we took a subset of prediction data."}, {"heading": "6.2 Audio Feature Representation", "text": "In this paper, we focus on learning algorithms, not representations. We used the well-known Mel Frequency Cepstral Coefficient (MFCC) representation. MFCCs use the deconvolution of source / filter from the ceptral transformation and the perceptual compression of spectra from the Mel pitch scale. MFCCs have been widely used in the speech recognition community for many years [24] and are also the de facto basic feature used in music modeling (see for example [25]). In particular, the MFCCs are known to provide an adequate representation of musical timbre [26]. In this paper, 13 MFCCs were extracted every 10 ms over a hammer window of 25ms, and first and second derivatives were summarized, for a total of 39 features. We then calculated a dictionary of d = 2000 typical MFCC vectors across the training set (using K means)."}, {"heading": "6.3 Baselines", "text": "We compare our proposed approach with the following baselines: man-versus-rest large margin classifiers (one-vs-rest) of the form fi (x) = w'i x trained with the margin perceptron algorithm, which provides similar results to support vector machines [28]. The loss function for the tag prediction in this case is: m \u2211 i = 1 | T | \u2211 j = 1max (0, 1 \u2212 \u03c6 (ti, j) fi (ai))), where \u03c6 (t \u2032, j) = 1 if j \u2032 t \u2032 t, and \u2212 1 otherwise. For the similar song task, we compare with cosmic similarity in the feature space, a classical information retrieval baseline [29], in the TagATune dataset we compare all participants of the MIREX 2009 competition [23]. We will compare the performance of the different models to use the cosmic similarity in the feature space to retrieve a classical information baseline [29]."}, {"heading": "6.4 Results", "text": "The results of comparing all methods of predicting the TagATune data are summarized in Table 2. Muslse surpasses the one-on-one baseline that we have used with the same functions, as well as the competitors on the TagATune dataset. The results of selecting different embedding dimensions d for Muslse are listed in Table 5 and show that performance is relatively stable, although we see minor improvements for larger d. We give a more detailed analysis of the results, including time and space requirements in subsequent sections. AUC on WARP loss compared to embedding models that perform either WARP or AUC optimization for different dimensions and function types."}, {"heading": "7 Conclusions", "text": "We have introduced a music comment and retrieval model that works by learning multiple tasks together by mapping units of different types (audio, artist names and tags) into a single, flat space in which they all live. This seems to offer a number of advantages, in particular: (i) semantic similarities between all entity types are learned in the embedding space, (ii) by multitasking all tasks for which we have the same embedding space, accuracy improves for all tasks, (iii) optimizing (approximately) precision at k leads to improved performance, (iv) because the model has a low capacity, making it more difficult to fit the tail of the distribution (where data is sparse), (v) the model is also fast at test time and has low memory consumption. Our resulting model has evolved well compared to baseline on two sets of data and is scalable enough to use it in a real system."}, {"heading": "8 Acknowledgements", "text": "We thank Doug Eck, Ryan Rifkin and Tom Walters for providing us with the big data set and extracting the relevant features from it."}], "references": [{"title": "Large scale image annotation: Learning to rank with joint word-image embeddings", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "European conference on Machine Learning.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological) 58(1)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Multitask Learning", "author": ["R. Caruana"], "venue": "Machine Learning 28(1)", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "A stochastic approximation method", "author": ["H. Robbins", "S. Monro"], "venue": "Annals of Mathematical Statistics 22", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1951}, {"title": "A discriminative kernel-based model to rank images from text queries", "author": ["D. Grangier", "S. Bengio"], "venue": "Transactions on Pattern Analysis and Machine Intelligence 30(8)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Kernel methods for multi-labelled classification and categorical regression problems", "author": ["A. Elisseeff", "J. Weston"], "venue": "Advances in neural information processing systems 14", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Polynomial semantic indexing", "author": ["B. Bai", "J. Weston", "D. Grangier", "R. Collobert", "K. Sadamasa", "Y. Qi", "C. Cortes", "M. Mohri"], "venue": "Advances in Neural Information Processing Systems (NIPS 2009).", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Ranking with ordered weighted pairwise classification", "author": ["N. Usunier", "D. Buffoni", "P. Gallinari"], "venue": "In Bottou, L., Littman, M., eds.: Proceedings of the 26th International Conference on Machine Learning, Montreal, Omnipress", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning features from music audio with deep belief networks", "author": ["P. Hamel", "D. Eck"], "venue": "ISMIR.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning to tag from open vocabulary labels", "author": ["E. Law", "B. Settles", "T. Mitchell"], "venue": "ECML.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic tagging of audio: The stateof-the-art", "author": ["T. Bertin-Mahieux", "D. Eck", "M. Mandel"], "venue": "In Wang, W., ed.: Machine Audition: Principles, Algorithms and Systems. IGI Publishing", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "A Large-Scale Evaluation of Acoustic and Subjective MusicSimilarity Measures", "author": ["A. Berenzweig"], "venue": "Computer Music Journal 28(2)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "The quest for ground truth in musical artist similarity", "author": ["D.P.W. Ellis", "B. Whitman", "A. Berenzweig", "S. Lawrence"], "venue": "ISMIR.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "On the evaluation of perceptual similarity measures for music", "author": ["E. Pampalk", "S. Dixon", "G. Widmer"], "venue": "Intl. Conf. on Digital Audio Effects.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Improvements of audio-based music similarity and genre classificaton", "author": ["E. Pampalk", "A. Flexer", "G. Widmer"], "venue": "ISMIR.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Generating transparent, steerable recommendations from textual descriptions of items", "author": ["S.J. Green", "P. Lamere", "J. Alexander", "F. Maillet", "S. Kirk", "J. Holt", "J. Bourque", "X.W. Mak"], "venue": "RecSys.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Anchor space for classification and similarity measurement of music", "author": ["A. Berenzweig", "D. Ellis", "S. Lawrence"], "venue": "ICME", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning similarity in heterogeneous data", "author": ["B. McFee", "G. Lanckriet"], "venue": "MIR \u201910: Proceedings of the international conference on Multimedia information retrieval, New York, NY, USA, ACM", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "JMLR 6", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Unlabeled Data Improves Word Prediction", "author": ["N. Loeff", "A. Farhadi", "I. Endres", "D. Forsyth"], "venue": "ICCV \u201909", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "Proc. Computer Vision and Pattern Recognition Conference.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Input-agreement: A new mechanism for data collection using human computation games", "author": ["E. Law", "L. von Ahn"], "venue": "CHI.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Evaluation of algorithms using games: the case of music tagging", "author": ["E. Law", "K. West", "M. Mandel", "M. Bay", "J.S. Downie"], "venue": "Proceedings of the 10th International Conference on Music Information Retrieval (ISMIR).", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Fundamentals of Speech Recognition", "author": ["L.R. Rabiner", "B.H. Juang"], "venue": "Prentice-Hall", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1993}, {"title": "Content-based retrieval of music and audio", "author": ["J.T. Foote"], "venue": "SPIE.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Perceptual distance in timbre space", "author": ["H. Terasawa", "M. Slaney", "J. Berger"], "venue": "Proceedings of the International Conference on Auditory Display (ICAD05).", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Sound retrieval and ranking using sparse auditory representations", "author": ["R.F. Lyon", "M. Rehn", "S. Bengio", "T.C. Walters", "G. Chechik"], "venue": "Neural Computation 22(9)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Y. Freund", "R.E. Schapire"], "venue": "In Shavlik, J., ed.: Machine Learning: Proceedings of the Fifteenth International Conference, San Francisco, CA, Morgan Kaufmann", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1998}, {"title": "Modern information retrieval", "author": ["R. Baeza-Yates", "B Ribeiro-Neto"], "venue": "AddisonWesley Harlow, England", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1999}, {"title": "Marsyas submissions to MIREX 2009", "author": ["G. Tzanetakis"], "venue": "MIREX 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiple-instance learning for music information retrieval", "author": ["M. Mandel", "D. Ellis"], "venue": "Proc. Intl. Symp. Music Information Retrieval.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Mirex special tagatune evaluation submission", "author": ["P.A. Manzagol", "S. Bengio"], "venue": "MIREX 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "On the Use of Anti-WordModels for Audio Music Annotation and Retrieval", "author": ["Z.S. Chen", "J.S.R. Jang"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing 17(8)", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "In order to do that, we use a recently developed embedding algorithm [1], which was applied to a vision task, and extend it to perform multi-tasking (and apply it to the music annotation and retrieval domain).", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "using the hyperparameter C which will act as a regularizer in a similar way as used in lasso [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "As we shall see, it is possible to learn the parameters A, T and V of our model jointly to perform well on all our tasks, which is referred to as multi-task learning [3].", "startOffset": 166, "endOffset": 169}, {"referenceID": 3, "context": "We will optimize this function by stochastic gradient descent [4].", "startOffset": 62, "endOffset": 65}, {"referenceID": 2, "context": "This amounts to iteratively repeating the following procedure [3]:", "startOffset": 62, "endOffset": 65}, {"referenceID": 0, "context": "We consider two loss functions, the standard margin ranking loss and the newly introduced WARP (Weighted Approximately Ranked Pairwise) Loss [1].", "startOffset": 141, "endOffset": 144}, {"referenceID": 4, "context": "AUC Margin Ranking Loss A standard loss function that is often using for retrieval is the margin ranking criterion [5, 6], in particular it was used for text embedding models in [7].", "startOffset": 115, "endOffset": 121}, {"referenceID": 5, "context": "AUC Margin Ranking Loss A standard loss function that is often using for retrieval is the margin ranking criterion [5, 6], in particular it was used for text embedding models in [7].", "startOffset": 115, "endOffset": 121}, {"referenceID": 6, "context": "AUC Margin Ranking Loss A standard loss function that is often using for retrieval is the margin ranking criterion [5, 6], in particular it was used for text embedding models in [7].", "startOffset": 178, "endOffset": 181}, {"referenceID": 7, "context": "This type of ranking error functions was recently developed in [8], and then used in an image annotation application in [1].", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "This type of ranking error functions was recently developed in [8], and then used in an image annotation application in [1].", "startOffset": 120, "endOffset": 123}, {"referenceID": 7, "context": "This is useful when one wants to optimize precision at k for a variety of different values of k at once [8].", "startOffset": 104, "endOffset": 107}, {"referenceID": 0, "context": "We will optimize this function by stochastic gradient descent following the authors of [1], that is samples are drawn at random, and a gradient step is made for that sample.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "averaging their ranking scores, f i (x) = f 1 i (x) + f 2 i (x) + f 3 i (x) for a given label i one can obtain improved results, as has been shown in [1] on vision tasks.", "startOffset": 150, "endOffset": 153}, {"referenceID": 8, "context": "The features can also be learnt directly from the audio [9].", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "Often, the machine learning algorithm attempts to model the semantic relations between the tags [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 10, "context": "A more extensive review of the automatic tagging of audio is presented in [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "This issue is addressed in [12] and [13].", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "This issue is addressed in [12] and [13].", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "Despite this, a number of music similarity systems rely only on acoustic features [14, 15].", "startOffset": 82, "endOffset": 90}, {"referenceID": 14, "context": "Despite this, a number of music similarity systems rely only on acoustic features [14, 15].", "startOffset": 82, "endOffset": 90}, {"referenceID": 15, "context": "Several systems such as [16, 17] combine audio content with meta-data.", "startOffset": 24, "endOffset": 32}, {"referenceID": 16, "context": "Several systems such as [16, 17] combine audio content with meta-data.", "startOffset": 24, "endOffset": 32}, {"referenceID": 17, "context": "One way to do this is to embed songs or artists in a Euclidean space using metric learning [18].", "startOffset": 91, "endOffset": 95}, {"referenceID": 6, "context": "We should also note that other related work (outside of the music domain) includes learning embeddings for supervised document ranking [7], semi-supervised multi-task learning [19, 20] and for vision tasks [21, 1].", "startOffset": 135, "endOffset": 138}, {"referenceID": 18, "context": "We should also note that other related work (outside of the music domain) includes learning embeddings for supervised document ranking [7], semi-supervised multi-task learning [19, 20] and for vision tasks [21, 1].", "startOffset": 176, "endOffset": 184}, {"referenceID": 19, "context": "We should also note that other related work (outside of the music domain) includes learning embeddings for supervised document ranking [7], semi-supervised multi-task learning [19, 20] and for vision tasks [21, 1].", "startOffset": 176, "endOffset": 184}, {"referenceID": 20, "context": "We should also note that other related work (outside of the music domain) includes learning embeddings for supervised document ranking [7], semi-supervised multi-task learning [19, 20] and for vision tasks [21, 1].", "startOffset": 206, "endOffset": 213}, {"referenceID": 0, "context": "We should also note that other related work (outside of the music domain) includes learning embeddings for supervised document ranking [7], semi-supervised multi-task learning [19, 20] and for vision tasks [21, 1].", "startOffset": 206, "endOffset": 213}, {"referenceID": 21, "context": "Details of how the data was collected are described in [22].", "startOffset": 55, "endOffset": 59}, {"referenceID": 22, "context": "The TagATune dataset was used in the MIREX 2009 contest on audio tag classification [23].", "startOffset": 84, "endOffset": 88}, {"referenceID": 23, "context": "years [24] and are also the de facto baseline feature used in music modeling (see for instance [25]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 24, "context": "years [24] and are also the de facto baseline feature used in music modeling (see for instance [25]).", "startOffset": 95, "endOffset": 99}, {"referenceID": 25, "context": "In particular, the MFCCs are known to offer a reasonable representation of the musical timbre [26].", "startOffset": 94, "endOffset": 98}, {"referenceID": 26, "context": "They have been used successfully in audio retrieval tasks [27].", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "We compare our proposed approach to the following baselines: one-versus-rest large margin classifiers (one-vs-rest) of the form fi(x) = w \u22a4 i x trained using the margin perceptron algorithm, which gives similar results to support vector machines [28].", "startOffset": 246, "endOffset": 250}, {"referenceID": 28, "context": "For the similar song task we compare to using cosine similarity in the feature space, a classical information retrieval baseline [29].", "startOffset": 129, "endOffset": 133}, {"referenceID": 22, "context": "Additionally, on the TagATune dataset we compare to all the entrants of the MIREX 2009 competition [23].", "startOffset": 99, "endOffset": 103}, {"referenceID": 29, "context": "We present here the results of the four best contestants: Marsyas [30], Mandel [31], Manzagol [32] and Zhi [33].", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "We present here the results of the four best contestants: Marsyas [30], Mandel [31], Manzagol [32] and Zhi [33].", "startOffset": 79, "endOffset": 83}, {"referenceID": 31, "context": "We present here the results of the four best contestants: Marsyas [30], Mandel [31], Manzagol [32] and Zhi [33].", "startOffset": 94, "endOffset": 98}, {"referenceID": 32, "context": "We present here the results of the four best contestants: Marsyas [30], Mandel [31], Manzagol [32] and Zhi [33].", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "Manzagol uses vector quantization and applies an algorithm called PAMIR (passive-aggressive model for image retrieval) [5].", "startOffset": 119, "endOffset": 122}], "year": 2011, "abstractText": "Music prediction tasks range from predicting tags given a song or clip of audio, predicting the name of the artist, or predicting related songs given a song, clip, artist name or tag. That is, we are interested in every semantic relationship between the different musical concepts in our database. In realistically sized databases, the number of songs is measured in the hundreds of thousands or more, and the number of artists in the tens of thousands or more, providing a considerable challenge to standard machine learning techniques. In this work, we propose a method that scales to such datasets which attempts to capture the semantic similarities between the database items by modeling audio, artist names, and tags in a single low-dimensional semantic space. This choice of space is learnt by optimizing the set of prediction tasks of interest jointly using multi-task learning. Our method both outperforms baseline methods and, in comparison to them, is faster and consumes less memory. We then demonstrate how our method learns an interpretable model, where the semantic space captures well the similarities of interest.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}