{"id": "1105.5460", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2011", "title": "Decision-Theoretic Planning: Structural Assumptions and Computational Leverage", "abstract": "Planning under uncertainty is a central problem in the study of automated sequential decision making, and has been addressed by researchers in many different fields, including AI planning, decision analysis, operations research, control theory and economics. While the assumptions and perspectives adopted in these areas often differ in substantial ways, many planning problems of interest to researchers in these fields can be modeled as Markov decision processes (MDPs) and analyzed using the techniques of decision theory. This paper presents an overview and synthesis of MDP-related methods, showing how they provide a unifying framework for modeling many classes of planning problems studied in AI. It also describes structural properties of MDPs that, when exhibited by particular classes of problems, can be exploited in the construction of optimal or approximately optimal policies or plans. Planning problems commonly possess structure in the reward and value functions used to describe performance criteria, in the functions used to describe state transitions and observations, and in the relationships among features used to describe states, actions, rewards, and observations. Specialized representations, and algorithms employing these representations, can achieve computational leverage by exploiting these various forms of structure. Certain AI techniques -- in particular those based on the use of structured, intensional representations -- can be viewed in this way. This paper surveys several types of representations for both classical and decision-theoretic planning problems, and planning algorithms that exploit these representations in a number of different ways to ease the computational burden of constructing policies or plans. It focuses primarily on abstraction, aggregation and decomposition techniques based on AI-style representations.", "histories": [["v1", "Fri, 27 May 2011 01:53:02 GMT  (371kb)", "http://arxiv.org/abs/1105.5460v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["c boutilier", "t dean", "s hanks"], "accepted": false, "id": "1105.5460"}, "pdf": {"name": "1105.5460.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Craig Boutilier"], "emails": ["cebly@cs.ubc.ca", "tld@cs.brown.edu", "hanks@cs.washington.edu"], "sections": [{"heading": null, "text": "It is a central problem in the investigation of automated sequential decision-making, and is used by researchers in many areas: includingAI planning, decision-making, decision-making, decision-making, decision-making, decision-making, decision-making, decision-making, decision-making, decision-making, decision-making, decision-making, decision-making, decision-making, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision, decision,"}, {"heading": "Boutilier, Dean, & Hanks", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times. \"\" I don't think they are able to change the world, \"he said in an interview with the\" New York Times. \"\" I don't think they will be able to change the world, \"he said in an interview with the\" New York Times. \"\" I don't think they will be able to change the world. \""}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "The reason for this is that there has been a massive increase in unemployment in the United States. (...) In the United States, there has been an increase in unemployment. (...) In the United States, there has been an increase in unemployment. (...) In the United States, there has been an increase in unemployment. (...) In the United States, there has been an increase in unemployment. (...) In the United States, there has been an increase in unemployment. (...) In the United States, there has been an increase in unemployment. (...) In the United States, there has been an increase in unemployment. (...) In the United States, there has been an increase in unemployment. (...) In the United States, there has been an increase in unemployment. (...) In the United States, there has been an increase in unemployment. (...)"}, {"heading": "Boutilier, Dean, & Hanks", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "Boutilier, Dean, & Hanks", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "This year, more than ever, it will be able to retaliate."}, {"heading": "Boutilier, Dean, & Hanks", "text": "P (RHC) t + 1MRHM RHMRHC RHCCR CRT TLoc LocMO L C M HO 0.1 0 0.9 L 0.1 0 0 0.1 0 0 H 0 0.9 0.1t f 0 0.2 f 0 0.8 f t 1.0 f 0 1.0tRHCP (Loc) t + 1P (CR) t + 1 CRtLoc tTime t + 1Figure 13: A factored 2TBN for the Markov chain induced by counterclockwise (shown with selected CPTs).the variables M, CR, and Tidy, respectively the dynamics of Loc (and the other variables) can only be described."}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "This year it is more than ever before."}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "In fact, it is as if we are the world, as if we are the world, as if we are the world. (...) In fact, it is as if we are able to change the world. (...) It is as if we are able to change the world. (...) It is as if we are able to change the world. (...) It is as if we are able to change the world. (...) It is as if we are able to change the world of the world. (...) It is as if we are able to change the world of the world. (...) It is as if we are able to change the world of the world. (...) It is as if we are the world of the world, as if we are the world. (...) It is as if we are able to change the world of the world."}, {"heading": "Boutilier, Dean, & Hanks", "text": "This year, it is at the top of the list as never before."}, {"heading": "Boutilier, Dean, & Hanks", "text": "+ Loc (L) nil 0.9 0.1 + Loc (C) nil 0.9 0.1 + Loc (M) nil 0.9 0.1 + Loc (H) nil 0.9 0.1 + Loc (O) -RHC -RHM + Loc (O) -RHC + Loc (O) -RHM -RHC nil 0.135 0.63 0.015 0.07LocO L C MHFigure 18: A PSO representation of a simple CClk action. Context can be kept in any state, the transition distribution for the action in any state is iseasible. Figure 17 provides a graphical representation of the PSO for the DelC action (represented as 2TBN in Figure 16). The three contexts: RHC, RHC ^ Loc (O) and RHC ^ RHC ^ RC2 are different."}, {"heading": "ArrM", "text": "This year it is more than ever before."}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "This year it is more than ever before in the history of the city."}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to hide, and that we will be able, that we will be able to achieve our goals."}, {"heading": "Boutilier, Dean, & Hanks", "text": "In fact, it is the case that most of them are able to move into a different world, in which they are able to integrate themselves, in which they are able to live, in which they are able to live, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they"}, {"heading": "Boutilier, Dean, & Hanks", "text": "The question of whether and in what form and in what form they will be able to establish themselves in the USA arises not only in the USA itself, but also in other countries, such as the USA, but also in the USA, in Europe and the USA. (...) Also in other countries, such as the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "A B C A B C A B CA B C A B C A B C B C A B C A B C A B C A B C A B C C C Uniform Nonuniform5.3 2.9 5.3 9.0"}, {"heading": "Exact Approximate", "text": "It is not the first time that a country has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a way that it has evolved in a certain algorithm."}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": ""}, {"heading": "RHC", "text": ""}, {"heading": "GetC PUM DelC DelM", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "Boutilier, Dean, & Hanks", "text": "In fact, the fact is that most of them are able to move, in the way in which they are able to move, in the way in which they are able to move, in the way in which they are able, in the way in which they are able to move, in the way in which they are able, in the way in which they are able to act, in the way in which they are able to act, in the way in which they are able to move, in the way in which they are able to act, in the way in which they are able to act, in the way in which they are able to act, in the way in which they are able to act, in the way in which they are able to move, in the way in which they are able to act."}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "In fact, it is the case that most of them are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to break the rules. \"(...)"}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "The initial partition shown in Figure 31 (a) is the same in terms of immediate rewards. We say that all states in a particular start cluster behave the same in relation to a particular target cluster if the probability of landing in the target cluster is the same for all states in the start cluster. This property is not satisfactory for starting cluster CR and target cluster CR in Figure 31 (a), so we divide the cluster referred to as CR to obtain the model in Figure 31 (b). Now, ownership is the same for all states in the start cluster, and the model in Figure 31 (b) is the property of all pairs of clusters, and the model in Figure 31 (b) is the minimum model."}, {"heading": "Boutilier, Dean, & Hanks", "text": "In fact, most of them are able to survive on their own, without putting themselves in danger."}, {"heading": "Boutilier, Dean, & Hanks", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own."}, {"heading": "Decision-Theoretic Planning: Structural Assumptions", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}], "references": [{"title": "Optimal control of Markov decision processes with incomplete state", "author": ["K.J. Astr\u007fom"], "venue": null, "citeRegEx": "Astr\u007fom,? \\Q1965\\E", "shortCiteRegEx": "Astr\u007fom", "year": 1965}, {"title": "Structured solution methods for non", "author": ["F. Bacchus", "C. Boutilier", "A. Grove"], "venue": null, "citeRegEx": "Bacchus et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 1997}, {"title": "Using temporal logic to control search", "author": ["F. Bacchus", "F. Kabanza"], "venue": null, "citeRegEx": "Bacchus and Kabanza,? \\Q1995\\E", "shortCiteRegEx": "Bacchus and Kabanza", "year": 1995}, {"title": "Making forward chaining relevant", "author": ["F. Bacchus", "Y.W. Teh"], "venue": null, "citeRegEx": "Bacchus and Teh,? \\Q1998\\E", "shortCiteRegEx": "Bacchus and Teh", "year": 1998}, {"title": "Algebraic decision diagrams and their applications", "author": ["F."], "venue": "International Con-", "citeRegEx": "F.,? 1993", "shortCiteRegEx": "F.", "year": 1993}, {"title": "Nonmonotonic reasoning in the framework of the situation calculus", "author": ["A.B. Baker"], "venue": null, "citeRegEx": "Baker,? \\Q1991\\E", "shortCiteRegEx": "Baker", "year": 1991}, {"title": "Learning to act using real-time dynamic", "author": ["A.G. Barto", "S.J. Bradtke", "S.P. Singh"], "venue": null, "citeRegEx": "Barto et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1995}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": "Princeton University Press, Princeton, NJ.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Adaptive aggregation for in nite horizon", "author": ["D.P. Bertsekas", "D.A. Castanon"], "venue": null, "citeRegEx": "Bertsekas and Castanon,? \\Q1989\\E", "shortCiteRegEx": "Bertsekas and Castanon", "year": 1989}, {"title": "Dynamic Programming", "author": ["D.P. Bertsekas"], "venue": "Prentice-Hall, Englewood Cli s, NJ.", "citeRegEx": "Bertsekas,? 1987", "shortCiteRegEx": "Bertsekas", "year": 1987}, {"title": "Discrete dynamic programming", "author": ["D. Blackwell"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Blackwell,? 1962", "shortCiteRegEx": "Blackwell", "year": 1962}, {"title": "Fast planning through graph analysis", "author": ["A.L. Blum", "M.L. Furst"], "venue": null, "citeRegEx": "Blum and Furst,? \\Q1995\\E", "shortCiteRegEx": "Blum and Furst", "year": 1995}, {"title": "Learning sorting and decision trees with POMDPs", "author": ["B. Bonet", "H. Ge ner"], "venue": null, "citeRegEx": "Bonet and ner,? \\Q1998\\E", "shortCiteRegEx": "Bonet and ner", "year": 1998}, {"title": "A robust and fast action selection mechanism", "author": ["B. Bonet", "G. Loerincs", "H. Ge ner"], "venue": null, "citeRegEx": "Bonet et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bonet et al\\.", "year": 1997}, {"title": "Correlated action e ects in decision theoretic regression", "author": ["C. Boutilier"], "venue": "Proceed-", "citeRegEx": "Boutilier,? 1997", "shortCiteRegEx": "Boutilier", "year": 1997}, {"title": "Prioritized goal decomposition", "author": ["C. Boutilier", "R.I. Brafman", "C. Geib"], "venue": null, "citeRegEx": "Boutilier et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1997}, {"title": "Structured reachability analysis", "author": ["C. Boutilier", "R.I. Brafman", "C. Geib"], "venue": null, "citeRegEx": "Boutilier et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1998}, {"title": "Using abstractions for decision-theoretic planning", "author": ["C. Boutilier", "R. Dearden"], "venue": null, "citeRegEx": "Boutilier and Dearden,? \\Q1994\\E", "shortCiteRegEx": "Boutilier and Dearden", "year": 1994}, {"title": "Approximating value trees in structured dynamic", "author": ["C. Boutilier", "R. Dearden"], "venue": null, "citeRegEx": "Boutilier and Dearden,? \\Q1996\\E", "shortCiteRegEx": "Boutilier and Dearden", "year": 1996}, {"title": "Exploiting structure in policy", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": null, "citeRegEx": "Boutilier et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1995}, {"title": "Stochastic dynamic programming", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": null, "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "The frame problem and Bayesian network action", "author": ["C. Boutilier", "M. Goldszmidt"], "venue": null, "citeRegEx": "Boutilier and Goldszmidt,? \\Q1996\\E", "shortCiteRegEx": "Boutilier and Goldszmidt", "year": 1996}, {"title": "Computing optimal policies for partially observable", "author": ["C. Boutilier", "D. Poole"], "venue": null, "citeRegEx": "Boutilier and Poole,? \\Q1996\\E", "shortCiteRegEx": "Boutilier and Poole", "year": 1996}, {"title": "Process-oriented planning and average-reward", "author": ["C. Boutilier", "M.L. Puterman"], "venue": null, "citeRegEx": "Boutilier and Puterman,? \\Q1995\\E", "shortCiteRegEx": "Boutilier and Puterman", "year": 1995}, {"title": "A heuristic variable-grid solution method for POMDPs", "author": ["R.I. Brafman"], "venue": "Pro-", "citeRegEx": "Brafman,? 1997", "shortCiteRegEx": "Brafman", "year": 1997}, {"title": "Graph-based algorithms for boolean function manipulation", "author": ["R.E. Bryant"], "venue": "IEEE", "citeRegEx": "Bryant,? 1986", "shortCiteRegEx": "Bryant", "year": 1986}, {"title": "The computational complexity of propositional STRIPS planning", "author": ["T. Bylander"], "venue": null, "citeRegEx": "Bylander,? \\Q1994\\E", "shortCiteRegEx": "Bylander", "year": 1994}, {"title": "Linear stochastic systems", "author": ["P.E. Caines"], "venue": "Wiley, New York.", "citeRegEx": "Caines,? 1988", "shortCiteRegEx": "Caines", "year": 1988}, {"title": "Acting optimally in partially", "author": ["A.R. Cassandra", "L.P. Kaelbling", "M.L. Littman"], "venue": null, "citeRegEx": "Cassandra et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1994}, {"title": "Incremental pruning: A", "author": ["A.R. Cassandra", "M.L. Littman", "N.L. Zhang"], "venue": null, "citeRegEx": "Cassandra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1997}, {"title": "Planning for conjunctive goals", "author": ["D. Chapman"], "venue": "Arti cial Intelligence, 32 (3), 333{377.", "citeRegEx": "Chapman,? 1987", "shortCiteRegEx": "Chapman", "year": 1987}, {"title": "Input generalization in delayed reinforcement", "author": ["D. Chapman", "L.P. Kaelbling"], "venue": null, "citeRegEx": "Chapman and Kaelbling,? \\Q1991\\E", "shortCiteRegEx": "Chapman and Kaelbling", "year": 1991}, {"title": "Decomposition principle for dynamic programs", "author": ["G. Dantzig", "P. Wolfe"], "venue": null, "citeRegEx": "Dantzig and Wolfe,? \\Q1960\\E", "shortCiteRegEx": "Dantzig and Wolfe", "year": 1960}, {"title": "Model minimization in Markov decision processes", "author": ["T. Dean", "R. Givan"], "venue": null, "citeRegEx": "Dean and Givan,? \\Q1997\\E", "shortCiteRegEx": "Dean and Givan", "year": 1997}, {"title": "Solving planning problems with large state and", "author": ["T. Dean", "R. Givan", "Kim", "K.-E"], "venue": null, "citeRegEx": "Dean et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1998}, {"title": "Model reduction techniques for computing", "author": ["T. Dean", "R. Givan", "S. Leach"], "venue": null, "citeRegEx": "Dean et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1997}, {"title": "Planning with deadlines", "author": ["T. Dean", "L. Kaelbling", "J. Kirman", "A. Nicholson"], "venue": null, "citeRegEx": "Dean et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1993}, {"title": "Planning under time", "author": ["T. Dean", "L. Kaelbling", "J. Kirman", "A. Nicholson"], "venue": null, "citeRegEx": "Dean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1995}, {"title": "A model for reasoning about persistence and causation", "author": ["T. Dean", "K. Kanazawa"], "venue": null, "citeRegEx": "Dean and Kanazawa,? \\Q1989\\E", "shortCiteRegEx": "Dean and Kanazawa", "year": 1989}, {"title": "Decomposition techniques for planning in stochastic do", "author": ["T. Dean", "Lin", "S.-H"], "venue": null, "citeRegEx": "Dean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1995}, {"title": "Planning and Control", "author": ["T. Dean", "M. Wellman"], "venue": null, "citeRegEx": "Dean and Wellman,? \\Q1991\\E", "shortCiteRegEx": "Dean and Wellman", "year": 1991}, {"title": "Integrating planning and execution in stochastic", "author": ["R. Dearden", "C. Boutilier"], "venue": null, "citeRegEx": "Dearden and Boutilier,? \\Q1994\\E", "shortCiteRegEx": "Dearden and Boutilier", "year": 1994}, {"title": "Abstraction and approximate decision theoretic plan", "author": ["R. Dearden", "C. Boutilier"], "venue": null, "citeRegEx": "Dearden and Boutilier,? \\Q1997\\E", "shortCiteRegEx": "Dearden and Boutilier", "year": 1997}, {"title": "Bucket elimination: A unifying framework for probabilistic inference", "author": ["R. Dechter"], "venue": null, "citeRegEx": "Dechter,? \\Q1996\\E", "shortCiteRegEx": "Dechter", "year": 1996}, {"title": "Mini-buckets: A general scheme for generating approximations", "author": ["R. Dechter"], "venue": null, "citeRegEx": "Dechter,? \\Q1997\\E", "shortCiteRegEx": "Dechter", "year": 1997}, {"title": "Sur un probl", "author": ["F. D'Epenoux"], "venue": null, "citeRegEx": "D.Epenoux,? \\Q1963\\E", "shortCiteRegEx": "D.Epenoux", "year": 1963}, {"title": "Explanation-based learning and reinforcement", "author": ["T.G. Dietterich", "N.S. Flann"], "venue": null, "citeRegEx": "Dietterich and Flann,? \\Q1995\\E", "shortCiteRegEx": "Dietterich and Flann", "year": 1995}, {"title": "A probabilistic model of action", "author": ["D. Draper", "S. Hanks", "D. Weld"], "venue": null, "citeRegEx": "Draper et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Draper et al\\.", "year": 1994}, {"title": "Probabilistic planning with information", "author": ["D. Draper", "S. Hanks", "D. Weld"], "venue": null, "citeRegEx": "Draper et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Draper et al\\.", "year": 1994}, {"title": "Learning and executing generalized robot plans", "author": ["R. Fikes", "P. Hart", "N. Nilsson"], "venue": null, "citeRegEx": "Fikes et al\\.,? \\Q1972\\E", "shortCiteRegEx": "Fikes et al\\.", "year": 1972}, {"title": "STRIPS: A new approach to the application of theorem", "author": ["R. Fikes", "N.J. Nilsson"], "venue": null, "citeRegEx": "Fikes and Nilsson,? \\Q1971\\E", "shortCiteRegEx": "Fikes and Nilsson", "year": 1971}, {"title": "Exploiting Constraints in Design Synthesis", "author": ["J. Finger"], "venue": "Ph.D. thesis, Stanford Uni-", "citeRegEx": "Finger,? 1986", "shortCiteRegEx": "Finger", "year": 1986}, {"title": "Algorithm 97 (shortest path)", "author": ["R.W. Floyd"], "venue": "Communications of the ACM, 5 (6),", "citeRegEx": "Floyd,? 1962", "shortCiteRegEx": "Floyd", "year": 1962}, {"title": "An algorithm for identifying the ergodic subchains and", "author": ["B.L. Fox", "D.M. Landi"], "venue": null, "citeRegEx": "Fox and Landi,? \\Q1968\\E", "shortCiteRegEx": "Fox and Landi", "year": 1968}, {"title": "Decision Theory", "author": ["S. French"], "venue": "Halsted Press, New York.", "citeRegEx": "French,? 1986", "shortCiteRegEx": "French", "year": 1986}, {"title": "Advances in probabilistic reasoning", "author": ["D. Geiger", "D. Heckerman"], "venue": null, "citeRegEx": "Geiger and Heckerman,? \\Q1991\\E", "shortCiteRegEx": "Geiger and Heckerman", "year": 1991}, {"title": "Model minimization, regression, and propositional STRIPS", "author": ["R. Givan", "T. Dean"], "venue": null, "citeRegEx": "Givan and Dean,? \\Q1997\\E", "shortCiteRegEx": "Givan and Dean", "year": 1997}, {"title": "Bounded-parameter Markov decision processes", "author": ["R. Givan", "S. Leach", "T. Dean"], "venue": null, "citeRegEx": "Givan et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Givan et al\\.", "year": 1997}, {"title": "Representing uncertainty in simple planners", "author": ["R.P. Goldman", "M.S. Boddy"], "venue": null, "citeRegEx": "Goldman and Boddy,? \\Q1994\\E", "shortCiteRegEx": "Goldman and Boddy", "year": 1994}, {"title": "Abstracting probabilistic actions", "author": ["P. Haddawy", "A. Doan"], "venue": null, "citeRegEx": "Haddawy and Doan,? \\Q1994\\E", "shortCiteRegEx": "Haddawy and Doan", "year": 1994}, {"title": "Utility Models for Goal-Directed Decision-Theoretic", "author": ["P. Haddawy", "S. Hanks"], "venue": null, "citeRegEx": "Haddawy and Hanks,? \\Q1998\\E", "shortCiteRegEx": "Haddawy and Hanks", "year": 1998}, {"title": "Decision-theoretic re nement planning using inheri", "author": ["P. Haddawy", "M. Suwandi"], "venue": null, "citeRegEx": "Haddawy and Suwandi,? \\Q1994\\E", "shortCiteRegEx": "Haddawy and Suwandi", "year": 1994}, {"title": "Projecting plans for uncertain worlds", "author": ["S. Hanks"], "venue": "Ph.D. thesis 756, Yale University,", "citeRegEx": "Hanks,? 1990", "shortCiteRegEx": "Hanks", "year": 1990}, {"title": "Modeling a dynamic and uncertain world I: Symbolic", "author": ["S. Hanks", "D.V. McDermott"], "venue": null, "citeRegEx": "Hanks and McDermott,? \\Q1994\\E", "shortCiteRegEx": "Hanks and McDermott", "year": 1994}, {"title": "Decision Theoretic Planning", "author": ["S. Hanks", "S. Russell", "M. Wellman"], "venue": null, "citeRegEx": "Hanks et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Hanks et al\\.", "year": 1994}, {"title": "Heuristic search in cyclic AND/OR graphs", "author": ["E.A. Hansen", "S. Zilberstein"], "venue": null, "citeRegEx": "Hansen and Zilberstein,? \\Q1998\\E", "shortCiteRegEx": "Hansen and Zilberstein", "year": 1998}, {"title": "A heuristic variable-grid solution method for POMDPs", "author": ["M. Hauskrecht"], "venue": "Pro-", "citeRegEx": "Hauskrecht,? 1997", "shortCiteRegEx": "Hauskrecht", "year": 1997}, {"title": "Planning and Control in Stochastic Domains with Imperfect", "author": ["M. Hauskrecht"], "venue": null, "citeRegEx": "Hauskrecht,? \\Q1998\\E", "shortCiteRegEx": "Hauskrecht", "year": 1998}, {"title": "SPUDD: Stochastic planning", "author": ["J. Hoey", "R. St-Aubin", "A. Hu", "C. Boutilier"], "venue": null, "citeRegEx": "Hoey et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hoey et al\\.", "year": 1999}, {"title": "Dynamic Programming and Markov Processes", "author": ["R.A. Howard"], "venue": "MIT Press, Cam-", "citeRegEx": "Howard,? 1960", "shortCiteRegEx": "Howard", "year": 1960}, {"title": "Re nement planning as a unifying framework for plan synthesis", "author": ["S. Kambhampati"], "venue": null, "citeRegEx": "Kambhampati,? \\Q1997\\E", "shortCiteRegEx": "Kambhampati", "year": 1997}, {"title": "A sparse sampling algorithm for near", "author": ["M. Kearns", "Y. Mansour", "A.Y. Ng"], "venue": null, "citeRegEx": "Kearns et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "Decisions with Multiple Objectives: Preferences", "author": ["R.L. Keeney", "H. Rai a"], "venue": null, "citeRegEx": "Keeney and a,? \\Q1976\\E", "shortCiteRegEx": "Keeney and a", "year": 1976}, {"title": "A computational scheme for reasoning in dynamic probabilistic net", "author": ["U. Kjaerul"], "venue": null, "citeRegEx": "Kjaerul,? \\Q1992\\E", "shortCiteRegEx": "Kjaerul", "year": 1992}, {"title": "Generating Abstraction Hierarchies: An Automated Approach", "author": ["C.A. Knoblock"], "venue": null, "citeRegEx": "Knoblock,? \\Q1993\\E", "shortCiteRegEx": "Knoblock", "year": 1993}, {"title": "Characterizing abstraction hier", "author": ["C.A. Knoblock", "J.D. Tenenberg", "Q. Yang"], "venue": null, "citeRegEx": "Knoblock et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Knoblock et al\\.", "year": 1991}, {"title": "Optimal probabilistic and decision-theoretic planning using Markovian", "author": ["S. Koenig"], "venue": null, "citeRegEx": "Koenig,? \\Q1991\\E", "shortCiteRegEx": "Koenig", "year": 1991}, {"title": "Real-time search in nondeterministic domains", "author": ["S. Koenig", "R. Simmons"], "venue": null, "citeRegEx": "Koenig and Simmons,? \\Q1995\\E", "shortCiteRegEx": "Koenig and Simmons", "year": 1995}, {"title": "Macro-operators: A weak method for learning", "author": ["R. Korf"], "venue": "Arti cial Intelligence, 26,", "citeRegEx": "Korf,? 1985", "shortCiteRegEx": "Korf", "year": 1985}, {"title": "Real-time heuristic search", "author": ["R.E. Korf"], "venue": "Arti cial Intelligence, 42, 189{211.", "citeRegEx": "Korf,? 1990", "shortCiteRegEx": "Korf", "year": 1990}, {"title": "An Algorithm for Probabilistic Planning", "author": ["N. Kushmerick", "S. Hanks", "D. Weld"], "venue": null, "citeRegEx": "Kushmerick et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kushmerick et al\\.", "year": 1995}, {"title": "Decomposition of systems governed by Markov", "author": ["H.J. Kushner", "Chen", "C.-H"], "venue": null, "citeRegEx": "Kushner et al\\.,? \\Q1974\\E", "shortCiteRegEx": "Kushner et al\\.", "year": 1974}, {"title": "Online minimization of transition systems", "author": ["D. Lee", "M. Yannakakis"], "venue": null, "citeRegEx": "Lee and Yannakakis,? \\Q1992\\E", "shortCiteRegEx": "Lee and Yannakakis", "year": 1992}, {"title": "State constraints revisited", "author": ["F. Lin", "R. Reiter"], "venue": "Journal of Logic and Computation,", "citeRegEx": "Lin and Reiter,? \\Q1994\\E", "shortCiteRegEx": "Lin and Reiter", "year": 1994}, {"title": "Exploiting Structure for Planning and Control", "author": ["Lin", "S.-H."], "venue": "Ph.D. thesis, Department", "citeRegEx": "Lin and S..H.,? 1997", "shortCiteRegEx": "Lin and S..H.", "year": 1997}, {"title": "Generating optimal policies for high-level plans", "author": ["Lin", "S.-H", "T. Dean"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Lin et al\\.", "year": 1995}, {"title": "Probabilistic propositional planning: Representations and complex", "author": ["M.L. Littman"], "venue": null, "citeRegEx": "Littman,? \\Q1997\\E", "shortCiteRegEx": "Littman", "year": 1997}, {"title": "On the complexity of solving", "author": ["M.L. Littman", "T.L. Dean", "L.P. Kaelbling"], "venue": null, "citeRegEx": "Littman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1995}, {"title": "Algorithms for sequential decision making", "author": ["M.L. Littman"], "venue": "Ph.D. thesis CS{96{09,", "citeRegEx": "Littman,? 1996", "shortCiteRegEx": "Littman", "year": 1996}, {"title": "Computationally feasible bounds for partially observed", "author": ["W.S. Lovejoy"], "venue": null, "citeRegEx": "Lovejoy,? \\Q1991\\E", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "A survey of algorithmic methods for partially observed", "author": ["W.S. Lovejoy"], "venue": null, "citeRegEx": "Lovejoy,? \\Q1991\\E", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "Introduction to Linear and Nonlinear Programming", "author": ["D.G. Luenberger"], "venue": "Addison-", "citeRegEx": "Luenberger,? 1973", "shortCiteRegEx": "Luenberger", "year": 1973}, {"title": "Introduction to Dynamic Systems: Theory, Models and Applica", "author": ["D.G. Luenberger"], "venue": null, "citeRegEx": "Luenberger,? \\Q1979\\E", "shortCiteRegEx": "Luenberger", "year": 1979}, {"title": "On the undecidability of probabilistic planning", "author": ["O. Madani", "A. Condon", "S. Hanks"], "venue": null, "citeRegEx": "Madani et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Madani et al\\.", "year": 1999}, {"title": "To discount or not to discount in reinforcement learning: A case", "author": ["S. Mahadevan"], "venue": null, "citeRegEx": "Mahadevan,? \\Q1994\\E", "shortCiteRegEx": "Mahadevan", "year": 1994}, {"title": "Systematic nonlinear planning", "author": ["D. McAllester", "D. Rosenblitt"], "venue": null, "citeRegEx": "McAllester and Rosenblitt,? \\Q1991\\E", "shortCiteRegEx": "McAllester and Rosenblitt", "year": 1991}, {"title": "Instance-based utile distinctions for reinforcement learning", "author": ["R.A. McCallum"], "venue": null, "citeRegEx": "McCallum,? \\Q1995\\E", "shortCiteRegEx": "McCallum", "year": 1995}, {"title": "Some philosophical problems from the standpoint", "author": ["J. McCarthy", "P.J. Hayes"], "venue": null, "citeRegEx": "McCarthy and Hayes,? \\Q1969\\E", "shortCiteRegEx": "McCarthy and Hayes", "year": 1969}, {"title": "The parti-game algorithm for variable resolution", "author": ["A.W. Moore", "C.G. Atkeson"], "venue": null, "citeRegEx": "Moore and Atkeson,? \\Q1995\\E", "shortCiteRegEx": "Moore and Atkeson", "year": 1995}, {"title": "The complexity of Markov chain decision", "author": ["C.H. Papadimitriou", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "Flexible decomposition algorithms for weakly coupled Markov decision", "author": ["R. Parr"], "venue": null, "citeRegEx": "Parr,? \\Q1998\\E", "shortCiteRegEx": "Parr", "year": 1998}, {"title": "Approximating optimal policies for partially observable", "author": ["R. Parr", "S. Russell"], "venue": null, "citeRegEx": "Parr and Russell,? \\Q1995\\E", "shortCiteRegEx": "Parr and Russell", "year": 1995}, {"title": "Reinforcement learning with hierarchies of machines", "author": ["R. Parr", "S. Russell"], "venue": null, "citeRegEx": "Parr and Russell,? \\Q1998\\E", "shortCiteRegEx": "Parr and Russell", "year": 1998}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "ADL: Exploring the middle ground between STRIPS and the situa", "author": ["E. Pednault"], "venue": null, "citeRegEx": "Pednault,? \\Q1989\\E", "shortCiteRegEx": "Pednault", "year": 1989}, {"title": "UCPOP: A sound, complete, partial order planner", "author": ["J.S. Penberthy", "D.S. Weld"], "venue": null, "citeRegEx": "Penberthy and Weld,? \\Q1992\\E", "shortCiteRegEx": "Penberthy and Weld", "year": 1992}, {"title": "Conditional Nonlinear Planning", "author": ["M. Peot", "D. Smith"], "venue": "In Proceedings of the First", "citeRegEx": "Peot and Smith,? \\Q1992\\E", "shortCiteRegEx": "Peot and Smith", "year": 1992}, {"title": "Control knowledge to improve plan quality", "author": ["M.A. Perez", "J.G. Carbonell"], "venue": null, "citeRegEx": "Perez and Carbonell,? \\Q1994\\E", "shortCiteRegEx": "Perez and Carbonell", "year": 1994}, {"title": "Exploiting the rule structure for decision making within the independent", "author": ["D. Poole"], "venue": null, "citeRegEx": "Poole,? \\Q1995\\E", "shortCiteRegEx": "Poole", "year": 1995}, {"title": "The independent choice logic for modelling multiple agents under", "author": ["D. Poole"], "venue": null, "citeRegEx": "Poole,? \\Q1997\\E", "shortCiteRegEx": "Poole", "year": 1997}, {"title": "Probabilistic partial evaluation: Exploiting rule structure in probabilistic", "author": ["D. Poole"], "venue": null, "citeRegEx": "Poole,? \\Q1997\\E", "shortCiteRegEx": "Poole", "year": 1997}, {"title": "Context-speci c approximation in probabilistic inference", "author": ["D. Poole"], "venue": "Proceedings", "citeRegEx": "Poole,? 1998", "shortCiteRegEx": "Poole", "year": 1998}, {"title": "Theoretical results on reinforcement learning", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": null, "citeRegEx": "Precup et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Precup et al\\.", "year": 1998}, {"title": "CASSANDRA: Planning for contingencies", "author": ["L. Pryor", "G. Collins"], "venue": null, "citeRegEx": "Pryor and Collins,? \\Q1993\\E", "shortCiteRegEx": "Pryor and Collins", "year": 1993}, {"title": "Markov Decision Processes", "author": ["M.L. Puterman"], "venue": "John Wiley & Sons, New York.", "citeRegEx": "Puterman,? 1994", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Modi ed policy iteration algorithms for discounted", "author": ["M.L. Puterman", "M. Shin"], "venue": null, "citeRegEx": "Puterman and Shin,? \\Q1978\\E", "shortCiteRegEx": "Puterman and Shin", "year": 1978}, {"title": "Multichain Markov decision processes with a", "author": ["K.W. Ross", "R. Varadarajan"], "venue": null, "citeRegEx": "Ross and Varadarajan,? \\Q1991\\E", "shortCiteRegEx": "Ross and Varadarajan", "year": 1991}, {"title": "Planning in a hierarchy of abstraction spaces", "author": ["E.D. Sacerdoti"], "venue": "Arti cial Intelligence,", "citeRegEx": "Sacerdoti,? 1974", "shortCiteRegEx": "Sacerdoti", "year": 1974}, {"title": "The nonlinear nature of plans", "author": ["E.D. Sacerdoti"], "venue": "Proceedings of the Fourth", "citeRegEx": "Sacerdoti,? 1975", "shortCiteRegEx": "Sacerdoti", "year": 1975}, {"title": "Universal plans for reactive robots in unpredictable environments", "author": ["M.J. Schoppers"], "venue": null, "citeRegEx": "Schoppers,? \\Q1987\\E", "shortCiteRegEx": "Schoppers", "year": 1987}, {"title": "A reinforcement learning method for maximizing undiscounted", "author": ["A. Schwartz"], "venue": null, "citeRegEx": "Schwartz,? \\Q1993\\E", "shortCiteRegEx": "Schwartz", "year": 1993}, {"title": "Evaluating in uence diagrams", "author": ["R.D. Shachter"], "venue": "Operations Research, 33 (6), 871{", "citeRegEx": "Shachter,? 1986", "shortCiteRegEx": "Shachter", "year": 1986}, {"title": "The role of relevance in explanation I: Irrelevance", "author": ["S.E. Shimony"], "venue": null, "citeRegEx": "Shimony,? \\Q1993\\E", "shortCiteRegEx": "Shimony", "year": 1993}, {"title": "Probabilistic robot navigation in partially observable", "author": ["R. Simmons", "S. Koenig"], "venue": null, "citeRegEx": "Simmons and Koenig,? \\Q1995\\E", "shortCiteRegEx": "Simmons and Koenig", "year": 1995}, {"title": "How to dynamically merge Markov decision processes", "author": ["S.P. Singh", "D. Cohn"], "venue": null, "citeRegEx": "Singh and Cohn,? \\Q1998\\E", "shortCiteRegEx": "Singh and Cohn", "year": 1998}, {"title": "Reinforcement learning with soft state", "author": ["S.P. Singh", "T. Jaakkola", "M.I. Jordan"], "venue": null, "citeRegEx": "Singh et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1994}, {"title": "The optimal control of partially observable", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": null, "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Postponing threats in partial-order planning", "author": ["D. Smith", "M. Peot"], "venue": null, "citeRegEx": "Smith and Peot,? \\Q1993\\E", "shortCiteRegEx": "Smith and Peot", "year": 1993}, {"title": "The optimal control of partially observable Markov processes over", "author": ["E.J. Sondik"], "venue": null, "citeRegEx": "Sondik,? \\Q1978\\E", "shortCiteRegEx": "Sondik", "year": 1978}, {"title": "Team-partitioned, opaque-transition reinforcement learning", "author": ["P. Stone", "M. Veloso"], "venue": null, "citeRegEx": "Stone and Veloso,? \\Q1999\\E", "shortCiteRegEx": "Stone and Veloso", "year": 1999}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R.S. Sutton"], "venue": "In", "citeRegEx": "Sutton,? 1995", "shortCiteRegEx": "Sutton", "year": 1995}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Control strategies for a stochastic planner", "author": ["J. Tash", "S. Russell"], "venue": null, "citeRegEx": "Tash and Russell,? \\Q1994\\E", "shortCiteRegEx": "Tash and Russell", "year": 1994}, {"title": "Dynamic programming and in uence diagrams", "author": ["J.A. Tatman", "R.D. Shachter"], "venue": null, "citeRegEx": "Tatman and Shachter,? \\Q1990\\E", "shortCiteRegEx": "Tatman and Shachter", "year": 1990}, {"title": "TD-Gammon, a self-teaching backgammon program, achieves master", "author": ["G.J. Tesauro"], "venue": null, "citeRegEx": "Tesauro,? \\Q1994\\E", "shortCiteRegEx": "Tesauro", "year": 1994}, {"title": "A probabilistic approach to concurrent mapping", "author": ["S. Thrun", "D. Fox", "W. Burgard"], "venue": null, "citeRegEx": "Thrun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 1998}, {"title": "Finding structure in reinforcement learning", "author": ["S. Thrun", "A. Schwartz"], "venue": null, "citeRegEx": "Thrun and Schwartz,? \\Q1995\\E", "shortCiteRegEx": "Thrun and Schwartz", "year": 1995}, {"title": "Generating conditional plans and programs", "author": ["D. Warren"], "venue": "Proceedings of AISB", "citeRegEx": "Warren,? 1976", "shortCiteRegEx": "Warren", "year": 1976}, {"title": "An introduction to least commitment planning", "author": ["D.S. Weld"], "venue": "AI Magazine, Winter", "citeRegEx": "Weld,? 1994", "shortCiteRegEx": "Weld", "year": 1994}, {"title": "Solutions procedures for partially observed", "author": ["C.C. White III", "W.T. Scherer"], "venue": null, "citeRegEx": "III and Scherer,? \\Q1989\\E", "shortCiteRegEx": "III and Scherer", "year": 1989}, {"title": "A value-directed approach to planning", "author": ["M. Williamson"], "venue": "Ph.D. thesis 96{06{03,", "citeRegEx": "Williamson,? 1996", "shortCiteRegEx": "Williamson", "year": 1996}, {"title": "Optimal planning with a goal-directed utility model", "author": ["M. Williamson", "S. Hanks"], "venue": null, "citeRegEx": "Williamson and Hanks,? \\Q1994\\E", "shortCiteRegEx": "Williamson and Hanks", "year": 1994}, {"title": "Arti cial Intelligence, Third Edition", "author": ["P.H. Winston"], "venue": "Addison-Wesley, Reading,", "citeRegEx": "Winston,? 1992", "shortCiteRegEx": "Winston", "year": 1992}, {"title": "Intelligent Planning : A Decomposition and Abstraction Based Approach", "author": ["Q. Yang"], "venue": null, "citeRegEx": "Yang,? \\Q1998\\E", "shortCiteRegEx": "Yang", "year": 1998}, {"title": "A model approximation scheme for planning", "author": ["N.L. Zhang", "W. Liu"], "venue": null, "citeRegEx": "Zhang and Liu,? \\Q1997\\E", "shortCiteRegEx": "Zhang and Liu", "year": 1997}, {"title": "Exploiting causal independence in Bayesian network", "author": ["N.L. Zhang", "D. Poole"], "venue": null, "citeRegEx": "Zhang and Poole,? \\Q1996\\E", "shortCiteRegEx": "Zhang and Poole", "year": 1996}], "referenceMentions": [{"referenceID": 4, "context": "Journal of Arti cial Intelligence Research 11 (1999) 1{94 Submitted 09/98; published 07/99 Decision-Theoretic Planning: Structural Assumptions and Computational Leverage Craig Boutilier cebly@cs.", "startOffset": 9, "endOffset": 53}, {"referenceID": 76, "context": "Much recent research on DTP has explicitly adopted the MDP framework as an underlying model (Barto, Bradtke, & Singh, 1995; Boutilier & Dearden, 1994; Boutilier, Dearden, & Goldszmidt, 1995; Dean, Kaelbling, Kirman, & Nicholson, 1993; Koenig, 1991; Simmons & Koenig, 1995; Tash & Russell, 1994), allowing the adaptation of existing results and algorithms for solving MDPs (e.", "startOffset": 92, "endOffset": 294}, {"referenceID": 114, "context": "See (Puterman, 1994) for a discussion of in nite and continuous-state problems.", "startOffset": 4, "endOffset": 20}, {"referenceID": 114, "context": "While we do not deal with such topics here, there is a considerable literature in the OR community on continuous-time Markov decision processes (Puterman, 1994).", "startOffset": 144, "endOffset": 160}, {"referenceID": 92, "context": "In control theory, this is called conversion to state form (Luenberger, 1979).", "startOffset": 59, "endOffset": 77}, {"referenceID": 91, "context": "See (Luenberger, 1973) for a more precise de nition of time-separability.", "startOffset": 4, "endOffset": 22}, {"referenceID": 7, "context": "Commonly, our aim is to maximize the total expected reward associated with a course of action; we therefore de ne the ( nite-horizon) value of any length T history h as (Bellman, 1957): V (h) = T 1 Xt=0fR(st) C(st; at)g+R(sT ) An in nite-horizon problem, on the other hand, requires that the agent's performance be evaluated over an in nite trajectory.", "startOffset": 169, "endOffset": 184}, {"referenceID": 7, "context": "The value function for an expected total discounted reward problem is de ned as follows (Bellman, 1957; Howard, 1960): V (h) = 1 Xt=0 t(R(st) C(st; at)) where is a xed discount rate (0 < 1).", "startOffset": 88, "endOffset": 117}, {"referenceID": 69, "context": "The value function for an expected total discounted reward problem is de ned as follows (Bellman, 1957; Howard, 1960): V (h) = 1 Xt=0 t(R(st) C(st; at)) where is a xed discount rate (0 < 1).", "startOffset": 88, "endOffset": 117}, {"referenceID": 114, "context": "Boutilier, Dean, & Hanks Re nements of this criterion have also been proposed (Puterman, 1994).", "startOffset": 78, "endOffset": 94}, {"referenceID": 9, "context": "As long as the goal can be reached with certainty, this situation can be formulated as an in nite-horizon problem where total reward is bounded for any desired trajectory (Bertsekas, 1987; Puterman, 1994).", "startOffset": 171, "endOffset": 204}, {"referenceID": 114, "context": "As long as the goal can be reached with certainty, this situation can be formulated as an in nite-horizon problem where total reward is bounded for any desired trajectory (Bertsekas, 1987; Puterman, 1994).", "startOffset": 171, "endOffset": 204}, {"referenceID": 9, "context": "Recent texts on FOMDPs include (Bertsekas, 1987) and (Puterman, 1994).", "startOffset": 31, "endOffset": 48}, {"referenceID": 114, "context": "Recent texts on FOMDPs include (Bertsekas, 1987) and (Puterman, 1994).", "startOffset": 53, "endOffset": 69}, {"referenceID": 10, "context": "Average reward optimality has also received attention in this literature (Blackwell, 1962; Howard, 1960; Puterman, 1994).", "startOffset": 73, "endOffset": 120}, {"referenceID": 69, "context": "Average reward optimality has also received attention in this literature (Blackwell, 1962; Howard, 1960; Puterman, 1994).", "startOffset": 73, "endOffset": 120}, {"referenceID": 114, "context": "Average reward optimality has also received attention in this literature (Blackwell, 1962; Howard, 1960; Puterman, 1994).", "startOffset": 73, "endOffset": 120}, {"referenceID": 6, "context": "In the AI literature, discounted or total reward models have been most popular as well (Barto et al., 1995; Dearden & Boutilier, 1997; Dean, Kaelbling, Kirman, & Nicholson, 1995; Koenig, 1991), though the average-reward criterion has been proposed as more suitable for modeling AI planning problems (Boutilier & Puterman, 1995; Mahadevan, 1994; Schwartz, 1993).", "startOffset": 87, "endOffset": 192}, {"referenceID": 76, "context": "In the AI literature, discounted or total reward models have been most popular as well (Barto et al., 1995; Dearden & Boutilier, 1997; Dean, Kaelbling, Kirman, & Nicholson, 1995; Koenig, 1991), though the average-reward criterion has been proposed as more suitable for modeling AI planning problems (Boutilier & Puterman, 1995; Mahadevan, 1994; Schwartz, 1993).", "startOffset": 87, "endOffset": 192}, {"referenceID": 94, "context": ", 1995; Dearden & Boutilier, 1997; Dean, Kaelbling, Kirman, & Nicholson, 1995; Koenig, 1991), though the average-reward criterion has been proposed as more suitable for modeling AI planning problems (Boutilier & Puterman, 1995; Mahadevan, 1994; Schwartz, 1993).", "startOffset": 199, "endOffset": 260}, {"referenceID": 120, "context": ", 1995; Dearden & Boutilier, 1997; Dean, Kaelbling, Kirman, & Nicholson, 1995; Koenig, 1991), though the average-reward criterion has been proposed as more suitable for modeling AI planning problems (Boutilier & Puterman, 1995; Mahadevan, 1994; Schwartz, 1993).", "startOffset": 199, "endOffset": 260}, {"referenceID": 0, "context": "POMDPs have been widely studied in OR and control theory (Astr\u007fom, 1965; Lovejoy, 1991b; Smallwood & Sondik, 1973; Sondik, 1978), and have drawn increasing attention in AI circles (Cassandra, Kaelbling, & Littman, 1994; Hauskrecht, 1998; Littman, 1996; Parr & Russell, 1995; Simmons & Koenig, 1995; Thrun, Fox, & Burgard, 1998; Zhang & Liu, 1997).", "startOffset": 57, "endOffset": 128}, {"referenceID": 128, "context": "POMDPs have been widely studied in OR and control theory (Astr\u007fom, 1965; Lovejoy, 1991b; Smallwood & Sondik, 1973; Sondik, 1978), and have drawn increasing attention in AI circles (Cassandra, Kaelbling, & Littman, 1994; Hauskrecht, 1998; Littman, 1996; Parr & Russell, 1995; Simmons & Koenig, 1995; Thrun, Fox, & Burgard, 1998; Zhang & Liu, 1997).", "startOffset": 57, "endOffset": 128}, {"referenceID": 67, "context": "POMDPs have been widely studied in OR and control theory (Astr\u007fom, 1965; Lovejoy, 1991b; Smallwood & Sondik, 1973; Sondik, 1978), and have drawn increasing attention in AI circles (Cassandra, Kaelbling, & Littman, 1994; Hauskrecht, 1998; Littman, 1996; Parr & Russell, 1995; Simmons & Koenig, 1995; Thrun, Fox, & Burgard, 1998; Zhang & Liu, 1997).", "startOffset": 180, "endOffset": 346}, {"referenceID": 88, "context": "POMDPs have been widely studied in OR and control theory (Astr\u007fom, 1965; Lovejoy, 1991b; Smallwood & Sondik, 1973; Sondik, 1978), and have drawn increasing attention in AI circles (Cassandra, Kaelbling, & Littman, 1994; Hauskrecht, 1998; Littman, 1996; Parr & Russell, 1995; Simmons & Koenig, 1995; Thrun, Fox, & Burgard, 1998; Zhang & Liu, 1997).", "startOffset": 180, "endOffset": 346}, {"referenceID": 121, "context": "In uence diagrams (Howard & Matheson, 1984; Shachter, 1986) are a popular model for decision making in AI and are, in fact, a structured representational method for POMDPs (see Section 4.", "startOffset": 18, "endOffset": 59}, {"referenceID": 3, "context": "This provides a criterion for evaluating potential solutions to planning problems. 2.10.2 Common Planning Problems We can use this general framework to classify various problems commonly studied in the planning and decision-making literature. In each case below, we note the modeling assumptions that de ne the problem class. Planning Problems in the OR/Decision Sciences Tradition Fully Observable Markov Decision Processes (FOMDPs) | There is an extremely large body of research studying FOMDPs, and we present the basic algorithmic techniques in some detail in the next section. The most commonly used formulation of FOMDPs assumes full observability and stationarity, and uses as its optimality criterion the maximization of expected total reward over a nite horizon, maximization of expected total discounted reward over an in nite horizon, or minimization of the expected cost to a goal state. FOMDPs were introduced by Bellman (1957) and have been studied in depth in the elds of decision analysis and OR, including the seminal work of Howard (1960).", "startOffset": 26, "endOffset": 941}, {"referenceID": 3, "context": "This provides a criterion for evaluating potential solutions to planning problems. 2.10.2 Common Planning Problems We can use this general framework to classify various problems commonly studied in the planning and decision-making literature. In each case below, we note the modeling assumptions that de ne the problem class. Planning Problems in the OR/Decision Sciences Tradition Fully Observable Markov Decision Processes (FOMDPs) | There is an extremely large body of research studying FOMDPs, and we present the basic algorithmic techniques in some detail in the next section. The most commonly used formulation of FOMDPs assumes full observability and stationarity, and uses as its optimality criterion the maximization of expected total reward over a nite horizon, maximization of expected total discounted reward over an in nite horizon, or minimization of the expected cost to a goal state. FOMDPs were introduced by Bellman (1957) and have been studied in depth in the elds of decision analysis and OR, including the seminal work of Howard (1960). Recent texts on FOMDPs include (Bertsekas, 1987) and (Puterman, 1994).", "startOffset": 26, "endOffset": 1057}, {"referenceID": 4, "context": "The other important assumptions are non-observability and that value is determined by reaching a goal state: any plan that leads to a goal state is preferred to any that does not. Often there is a preference for shorter plans; this can be represented by using a discount factor to \\encourage\" faster goal achievement or by assigning a cost to actions. Reward is associated only with transitions to goal states, which are absorbing. Action costs are typically ignored, except as noted above. In classical models it is usually assumed that the initial state is known with certainty. This contrasts with the general speci cation of MDPs above, which does not assume knowledge of or even distributional information about the initial state. Policies are de ned to be applicable no matter what state (or distribution over states) one nds oneself in|action choices are de ned for every possible state or history. Knowledge of the initial state and determinism allow optimal straight-line plans to be constructed, with no loss in value associated with non-observability, but unpredictable exogenous events and uncertain action e ects cannot be modeled consistently if these assumptions are adopted. For an overview of early classical planning research and the variety of approaches adopted, see (Allen, Hendler, & Tate, 1990) as well as Yang's (1998) recent text.", "startOffset": 151, "endOffset": 1343}, {"referenceID": 4, "context": "The other important assumptions are non-observability and that value is determined by reaching a goal state: any plan that leads to a goal state is preferred to any that does not. Often there is a preference for shorter plans; this can be represented by using a discount factor to \\encourage\" faster goal achievement or by assigning a cost to actions. Reward is associated only with transitions to goal states, which are absorbing. Action costs are typically ignored, except as noted above. In classical models it is usually assumed that the initial state is known with certainty. This contrasts with the general speci cation of MDPs above, which does not assume knowledge of or even distributional information about the initial state. Policies are de ned to be applicable no matter what state (or distribution over states) one nds oneself in|action choices are de ned for every possible state or history. Knowledge of the initial state and determinism allow optimal straight-line plans to be constructed, with no loss in value associated with non-observability, but unpredictable exogenous events and uncertain action e ects cannot be modeled consistently if these assumptions are adopted. For an overview of early classical planning research and the variety of approaches adopted, see (Allen, Hendler, & Tate, 1990) as well as Yang's (1998) recent text. Optimal Deterministic Planning | A separate body of work retains the classical assumptions of complete information and determinism, but tries to recast the planning problem as an optimization that relaxes the implicit assumption of \\achieve the goal at all costs.\" At the same time, these methods use the same sort of representations and algorithms applied to satis cing planning. Haddawy and Hanks (1998) present a multi-attribute utility model for planners that keeps the explicit information about the initial state and goals, but allows preferences to be stated about the partial satisfaction of the goals as well as the cost of the resources consumed in satisfying them.", "startOffset": 151, "endOffset": 1762}, {"referenceID": 4, "context": "The other important assumptions are non-observability and that value is determined by reaching a goal state: any plan that leads to a goal state is preferred to any that does not. Often there is a preference for shorter plans; this can be represented by using a discount factor to \\encourage\" faster goal achievement or by assigning a cost to actions. Reward is associated only with transitions to goal states, which are absorbing. Action costs are typically ignored, except as noted above. In classical models it is usually assumed that the initial state is known with certainty. This contrasts with the general speci cation of MDPs above, which does not assume knowledge of or even distributional information about the initial state. Policies are de ned to be applicable no matter what state (or distribution over states) one nds oneself in|action choices are de ned for every possible state or history. Knowledge of the initial state and determinism allow optimal straight-line plans to be constructed, with no loss in value associated with non-observability, but unpredictable exogenous events and uncertain action e ects cannot be modeled consistently if these assumptions are adopted. For an overview of early classical planning research and the variety of approaches adopted, see (Allen, Hendler, & Tate, 1990) as well as Yang's (1998) recent text. Optimal Deterministic Planning | A separate body of work retains the classical assumptions of complete information and determinism, but tries to recast the planning problem as an optimization that relaxes the implicit assumption of \\achieve the goal at all costs.\" At the same time, these methods use the same sort of representations and algorithms applied to satis cing planning. Haddawy and Hanks (1998) present a multi-attribute utility model for planners that keeps the explicit information about the initial state and goals, but allows preferences to be stated about the partial satisfaction of the goals as well as the cost of the resources consumed in satisfying them. The model also allows the expression of preferences over phenomena like temporal deadlines and maintenance intervals that are di cult to capture using a time-separable additive value function. Williamson (1996) (see also Williamson & Hanks, 1994).", "startOffset": 151, "endOffset": 2243}, {"referenceID": 4, "context": "The other important assumptions are non-observability and that value is determined by reaching a goal state: any plan that leads to a goal state is preferred to any that does not. Often there is a preference for shorter plans; this can be represented by using a discount factor to \\encourage\" faster goal achievement or by assigning a cost to actions. Reward is associated only with transitions to goal states, which are absorbing. Action costs are typically ignored, except as noted above. In classical models it is usually assumed that the initial state is known with certainty. This contrasts with the general speci cation of MDPs above, which does not assume knowledge of or even distributional information about the initial state. Policies are de ned to be applicable no matter what state (or distribution over states) one nds oneself in|action choices are de ned for every possible state or history. Knowledge of the initial state and determinism allow optimal straight-line plans to be constructed, with no loss in value associated with non-observability, but unpredictable exogenous events and uncertain action e ects cannot be modeled consistently if these assumptions are adopted. For an overview of early classical planning research and the variety of approaches adopted, see (Allen, Hendler, & Tate, 1990) as well as Yang's (1998) recent text. Optimal Deterministic Planning | A separate body of work retains the classical assumptions of complete information and determinism, but tries to recast the planning problem as an optimization that relaxes the implicit assumption of \\achieve the goal at all costs.\" At the same time, these methods use the same sort of representations and algorithms applied to satis cing planning. Haddawy and Hanks (1998) present a multi-attribute utility model for planners that keeps the explicit information about the initial state and goals, but allows preferences to be stated about the partial satisfaction of the goals as well as the cost of the resources consumed in satisfying them. The model also allows the expression of preferences over phenomena like temporal deadlines and maintenance intervals that are di cult to capture using a time-separable additive value function. Williamson (1996) (see also Williamson & Hanks, 1994). implements this model by extending a classical planning algorithm to solve the resulting optimization problem. Haddawy and Suwandi (1994) also implement this model in a complete decision-theoretic framework.", "startOffset": 151, "endOffset": 2418}, {"referenceID": 4, "context": "The other important assumptions are non-observability and that value is determined by reaching a goal state: any plan that leads to a goal state is preferred to any that does not. Often there is a preference for shorter plans; this can be represented by using a discount factor to \\encourage\" faster goal achievement or by assigning a cost to actions. Reward is associated only with transitions to goal states, which are absorbing. Action costs are typically ignored, except as noted above. In classical models it is usually assumed that the initial state is known with certainty. This contrasts with the general speci cation of MDPs above, which does not assume knowledge of or even distributional information about the initial state. Policies are de ned to be applicable no matter what state (or distribution over states) one nds oneself in|action choices are de ned for every possible state or history. Knowledge of the initial state and determinism allow optimal straight-line plans to be constructed, with no loss in value associated with non-observability, but unpredictable exogenous events and uncertain action e ects cannot be modeled consistently if these assumptions are adopted. For an overview of early classical planning research and the variety of approaches adopted, see (Allen, Hendler, & Tate, 1990) as well as Yang's (1998) recent text. Optimal Deterministic Planning | A separate body of work retains the classical assumptions of complete information and determinism, but tries to recast the planning problem as an optimization that relaxes the implicit assumption of \\achieve the goal at all costs.\" At the same time, these methods use the same sort of representations and algorithms applied to satis cing planning. Haddawy and Hanks (1998) present a multi-attribute utility model for planners that keeps the explicit information about the initial state and goals, but allows preferences to be stated about the partial satisfaction of the goals as well as the cost of the resources consumed in satisfying them. The model also allows the expression of preferences over phenomena like temporal deadlines and maintenance intervals that are di cult to capture using a time-separable additive value function. Williamson (1996) (see also Williamson & Hanks, 1994). implements this model by extending a classical planning algorithm to solve the resulting optimization problem. Haddawy and Suwandi (1994) also implement this model in a complete decision-theoretic framework. Their model of planning, re nement planning, di ers somewhat from the generative model discussed in this paper. In their model the set of all possible plans is pre-stored in an abstraction hierarchy, and the problem solver's job is to nd in the hierarchy the optimal choice of concrete actions for a particular problem. Perez and Carbonell's (1994) work also incorporates cost information into the classical planning framework, but maintains the split between a classical satis cing planner and additional cost information provided in the utility model.", "startOffset": 151, "endOffset": 2837}, {"referenceID": 80, "context": "Probabilistic Planning Without Feedback | A direct probabilistic extension of the classical planning problem can be stated as follows (Kushmerick et al., 1995): take as input (a) a probability distribution over initial states, (b) stochastic actions (explicit or implicit transition matrices), (c) a set of goal states, and (d) a probability success threshold .", "startOffset": 134, "endOffset": 159}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions Conditional Deterministic Planning | The classical planning assumption of omniscience can be relaxed somewhat by allowing the state of some aspects of the world to be unknown. The agent is thus in a situation where it is certain that the system is one of a particular set of states, but does not know which one. Unknown truth values can be included in the initial state speci cation, and taking actions can cause a proposition to become unknown as well. Actions can provide the agent with information while the plan is being executed: conditional planners introduce the idea of actions providing runtime information about the prevailing state, distinguishing between an action that makes proposition P true and an action that will tell the agent whether P is true when the action is executed. An action can have both causal and informational e ects, simultaneously changing the world and reporting on the value of one or more propositions. This second sort of information is not useful at planning time except that it allows steps in the plan to be executed conditionally, depending on the runtime information provided by prior information-producing steps. The value of such actions lies in the fact that di erent courses of action may be appropriate under di erent conditions|these informational e ects allow runtime selection of actions based on the observations produced, much like the general POMDP model. Examples of conditional planners in the classical framework include early work by Warren (1976) and the more recent CNLP (Peot & Smith, 1992), Cassandra (Pryor & Collins, 1993), Plynth (Goldman & Boddy, 1994), and UWL (Etzioni, Hanks, Weld, Draper, Lesh, & Williamson, 1992) systems.", "startOffset": 124, "endOffset": 1558}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions Conditional Deterministic Planning | The classical planning assumption of omniscience can be relaxed somewhat by allowing the state of some aspects of the world to be unknown. The agent is thus in a situation where it is certain that the system is one of a particular set of states, but does not know which one. Unknown truth values can be included in the initial state speci cation, and taking actions can cause a proposition to become unknown as well. Actions can provide the agent with information while the plan is being executed: conditional planners introduce the idea of actions providing runtime information about the prevailing state, distinguishing between an action that makes proposition P true and an action that will tell the agent whether P is true when the action is executed. An action can have both causal and informational e ects, simultaneously changing the world and reporting on the value of one or more propositions. This second sort of information is not useful at planning time except that it allows steps in the plan to be executed conditionally, depending on the runtime information provided by prior information-producing steps. The value of such actions lies in the fact that di erent courses of action may be appropriate under di erent conditions|these informational e ects allow runtime selection of actions based on the observations produced, much like the general POMDP model. Examples of conditional planners in the classical framework include early work by Warren (1976) and the more recent CNLP (Peot & Smith, 1992), Cassandra (Pryor & Collins, 1993), Plynth (Goldman & Boddy, 1994), and UWL (Etzioni, Hanks, Weld, Draper, Lesh, & Williamson, 1992) systems. Probabilistic Planning Without Feedback | A direct probabilistic extension of the classical planning problem can be stated as follows (Kushmerick et al., 1995): take as input (a) a probability distribution over initial states, (b) stochastic actions (explicit or implicit transition matrices), (c) a set of goal states, and (d) a probability success threshold . The objective is to produce a plan that reaches any goal state with probability at least , given the initial state distribution. No provision is made for execution-time observation, thus straight-line plans are the only form of policy possible. This is a restricted case of the in nite-horizon NOMDP problem, one in which actions incur no cost and goal states o er positive reward and are absorbing. It is also a special case in that the objective is to nd a satis cing policy rather than an optimal one. Probabilistic Planning With Feedback | Draper et al. (1994a) have proposed an extension of the probabilistic planning problem in which actions provide feedback, using exactly the observation model described in Section 2.", "startOffset": 124, "endOffset": 2674}, {"referenceID": 0, "context": "As mentioned above, a POMDP can be viewed as a FOMDP with an in nite state space consisting of probability distributions over S, each distribution representing the agent's state of belief at a point in time (Astr\u007fom, 1965; Smallwood & Sondik, 1973).", "startOffset": 207, "endOffset": 248}, {"referenceID": 0, "context": "As mentioned above, a POMDP can be viewed as a FOMDP with an in nite state space consisting of probability distributions over S, each distribution representing the agent's state of belief at a point in time (Astr\u007fom, 1965; Smallwood & Sondik, 1973). The problem of nding an optimal policy for a POMDP with the objective of maximizing expected total reward or expected total discounted reward over a nite horizon T has been shown to be exponentially hard both in jSj and in T (Papadimitriou & Tsitsiklis, 1987). The problem of nding a policy that maximizes or approximately maximizes the expected discounted total reward over an in nite horizon is shown to be undecidable (Madani, Condon, & Hanks, 1999). Even restricted cases of the POMDP problem are computationally di cult in the worst case. Littman (1996) considers the special case of boolean rewards: determining whether there is an in nite-horizon policy with nonzero total reward given that the rewards associated with all states are non-negative.", "startOffset": 208, "endOffset": 809}, {"referenceID": 0, "context": "As mentioned above, a POMDP can be viewed as a FOMDP with an in nite state space consisting of probability distributions over S, each distribution representing the agent's state of belief at a point in time (Astr\u007fom, 1965; Smallwood & Sondik, 1973). The problem of nding an optimal policy for a POMDP with the objective of maximizing expected total reward or expected total discounted reward over a nite horizon T has been shown to be exponentially hard both in jSj and in T (Papadimitriou & Tsitsiklis, 1987). The problem of nding a policy that maximizes or approximately maximizes the expected discounted total reward over an in nite horizon is shown to be undecidable (Madani, Condon, & Hanks, 1999). Even restricted cases of the POMDP problem are computationally di cult in the worst case. Littman (1996) considers the special case of boolean rewards: determining whether there is an in nite-horizon policy with nonzero total reward given that the rewards associated with all states are non-negative. He shows that the problem is EXPTIME-complete if the transitions are stochastic, and PSPACE-hard if the transitions are deterministic. Deterministic Planning Recall that the classical planning problem is de ned quite di erently from the MDP problems above: the agent has no ability to observe the state but has perfect predictive powers, knowing the initial state and the e ects of all actions with certainty. In addition, rewards come only from reaching a goal state, and any plan that achieves the goal su ces. Planning problems are typically de ned in terms of a set P of boolean features or propositions: a complete assignment of truth values to features describes exactly one state, and a partial assignment of truth values describes a set of states. A set of propositions P induces a state space of size 2jPj. Thus, the space required to represent a planning problem using a feature-based representation can be exponentially smaller than that required by a at representation for the same problem (see Section 4). The ability to represent planning problems compactly has a dramatic impact on worstcase complexity. Bylander (1994) shows that the deterministic planning problem without observation is PSPACE-complete.", "startOffset": 208, "endOffset": 2140}, {"referenceID": 93, "context": "Even the simplest problem in probabilistic planning|one that admits no observability|is undecidable at worst (Madani et al., 1999).", "startOffset": 109, "endOffset": 130}, {"referenceID": 7, "context": "Bellman's principle of optimality (Bellman, 1957) forms the basis of the stochastic dynamic programming algorithms used to solve MDPs, establishing the following relationship between the optimal value function at tth stage and the optimal value function at the previous stage: V t (s) = R(s) + max a2A fC(a) +X s02S Pr(s0ja; s)V t 1(s0)g (2) 3.", "startOffset": 34, "endOffset": 49}, {"referenceID": 4, "context": "1 Dynamic Programming Approaches Suppose we are given a fully-observable MDP with a time-separable, additive value function. In other words, we are given the state space S, action space A, a transition matrix Pr(s0js; a) for each action a, a reward function R, and a cost function C. We start with the problem of nding the policy that maximizes expected total reward for some xed, nite-horizon T . Suppose we are given a policy such that (s; t) is the action to be performed by the agent in state s with t stages remaining to act (for 0 t T ).18 Bellman (1957) shows that the expected value of such a policy at any state can be computed using the set of t-stage-to-go value functions V t .", "startOffset": 56, "endOffset": 561}, {"referenceID": 94, "context": "This is by far the most commonly studied problem in the literature, though it is argued in (Boutilier & Puterman, 1995; Mahadevan, 1994; Schwartz, 1993) that such problems are often best modeled using average reward per stage as the optimality criterion.", "startOffset": 91, "endOffset": 152}, {"referenceID": 120, "context": "This is by far the most commonly studied problem in the literature, though it is argued in (Boutilier & Puterman, 1995; Mahadevan, 1994; Schwartz, 1993) that such problems are often best modeled using average reward per stage as the optimality criterion.", "startOffset": 91, "endOffset": 152}, {"referenceID": 114, "context": "For a discussion of average reward optimality and its many variants and re nements, see (Puterman, 1994).", "startOffset": 88, "endOffset": 104}, {"referenceID": 114, "context": "For a discussion of stopping criteria and convergence of the algorithm, see (Puterman, 1994).", "startOffset": 76, "endOffset": 92}, {"referenceID": 4, "context": "There is no xed time horizon associated with these tasks|they are to be performed as the need arises. Such problems are best modeled as in nite-horizon problems. We consider here the problem of building a policy that maximizes the discounted sum of expected rewards over an in nite horizon.19 Howard (1960) showed that there always exists an optimal stationary policy for such problems.", "startOffset": 75, "endOffset": 307}, {"referenceID": 114, "context": "However, the algorithm converges to an optimal policy at least linearly and under certain conditions converges superlinearly or quadratically (Puterman, 1994).", "startOffset": 142, "endOffset": 158}, {"referenceID": 114, "context": "With some tuning of the value of t used at each iteration, modi ed policy iteration can work extremely well in practice (Puterman, 1994).", "startOffset": 120, "endOffset": 136}, {"referenceID": 9, "context": "For a nice discussion of asynchronous dynamic programming, see (Bertsekas, 1987; Bertsekas & Tsitsiklis, 1996).", "startOffset": 63, "endOffset": 110}, {"referenceID": 87, "context": "See (Littman et al., 1995) for a discussion of the complexity of the algorithm.", "startOffset": 4, "endOffset": 26}, {"referenceID": 114, "context": "See (Puterman, 1994) for details.", "startOffset": 4, "endOffset": 20}, {"referenceID": 0, "context": "While history-dependence precludes dynamic programming, the observable history can be summarized adequately with a probability distribution over S (Astr\u007fom, 1965), and policies can be computed as a function of these distributions, or belief states.", "startOffset": 147, "endOffset": 162}, {"referenceID": 128, "context": "A key observation of Sondik (Smallwood & Sondik, 1973; Sondik, 1978) is that when one views a POMDP with a time-separable value function by taking the state space to be the set of probability distributions over S, one obtains a fully observable MDP that can be solved by dynamic programming.", "startOffset": 28, "endOffset": 68}, {"referenceID": 128, "context": "Boutilier, Dean, & Hanks that the state space for this FOMDP is an N -dimensional continuous space,23 and special techniques must be used to solve it (Smallwood & Sondik, 1973; Sondik, 1978).", "startOffset": 150, "endOffset": 190}, {"referenceID": 28, "context": "We do not explore these techniques here, but note that they are currently practical only for very small problems (Cassandra et al., 1994; Cassandra, Littman, & Zhang, 1997; Littman, 1996; Lovejoy, 1991b).", "startOffset": 113, "endOffset": 203}, {"referenceID": 88, "context": "We do not explore these techniques here, but note that they are currently practical only for very small problems (Cassandra et al., 1994; Cassandra, Littman, & Zhang, 1997; Littman, 1996; Lovejoy, 1991b).", "startOffset": 113, "endOffset": 203}, {"referenceID": 24, "context": "A number of approximation methods, developed both in OR (Lovejoy, 1991a; White III & Scherer, 1989) and AI (Brafman, 1997; Hauskrecht, 1997; Parr & Russell, 1995; Zhang & Liu, 1997), can be used to increase the range of solvable problems, but even these techniques are presently of limited practical value.", "startOffset": 107, "endOffset": 181}, {"referenceID": 66, "context": "A number of approximation methods, developed both in OR (Lovejoy, 1991a; White III & Scherer, 1989) and AI (Brafman, 1997; Hauskrecht, 1997; Parr & Russell, 1995; Zhang & Liu, 1997), can be used to increase the range of solvable problems, but even these techniques are presently of limited practical value.", "startOffset": 107, "endOffset": 181}, {"referenceID": 52, "context": "Decision-Theoretic Planning: Structural Assumptions is equivalent to using the Floyd-Warshall algorithm to nd a minimum-cost path through a weighted graph (Floyd, 1962).", "startOffset": 155, "endOffset": 168}, {"referenceID": 54, "context": "A decision tree rooted at sinit is constructed in much the same way as a search tree for a deterministic planning problem (French, 1986).", "startOffset": 122, "endOffset": 136}, {"referenceID": 1, "context": "See Bacchus et al. (1995, 1998) for some recent work that makes the case for progression with good search control, and Bonet et al. (1997) who argue that progression in deterministic planning is useful when integrating planning and execution.", "startOffset": 4, "endOffset": 139}, {"referenceID": 79, "context": "One way around this di culty is the use of real time search (Korf, 1990).", "startOffset": 60, "endOffset": 72}, {"referenceID": 6, "context": "In particular, real-time dynamic programming, or RTDP, has been proposed in (Barto et al., 1995) as a way of approximately solving large MDPs in an online fashion.", "startOffset": 76, "endOffset": 96}, {"referenceID": 13, "context": "This technique is also investigated in (Bonet et al., 1997; Dearden & Boutilier, 1994, 1997; Koenig & Simmons, 1995), and is closely tied to Korf's (1990) LRTA* algorithm.", "startOffset": 39, "endOffset": 116}, {"referenceID": 4, "context": "Furthermore, in nite-horizon problems pose the di culty of determining a su ciently deep tree. One way around this di culty is the use of real time search (Korf, 1990). In particular, real-time dynamic programming, or RTDP, has been proposed in (Barto et al., 1995) as a way of approximately solving large MDPs in an online fashion. One can interleave search with execution of an approximately optimal policy using a form of RTDP similar to decisiontree evaluation as follows. Imagine the agent nds itself in a particular state sinit. It can then build a partial search tree to some depth, perhaps uniformly or perhaps with some branches expanded more deeply than others. Partial tree construction may be halted due to time pressure or due to an assessment by the agent that further expansion of the tree may not be fruitful. When a decision to act must be made, the rollback procedure is applied to this partial, possibly unevenly expanded decision tree. Reward values can be used to evaluate the leaves of the tree, but this may o er an inaccurate picture of the value of nodes higher in the tree. Heuristic information can be used to estimate the long-term value of states labeling leaves. As with value iteration, the deeper the tree, the more accurate the estimated value at the root (generally speaking) for a xed heuristic. We will see in Section 5 that structured representations of MDPs can provide a means to construct such heuristics (Dearden & Boutilier, 1994, 1997). Speci cally, with admissible heuristics or upper and lower bounds on the true values of leaf nodes in the tree, methods such as A* or branch-and-bound search can be used. A key advantage of integrating search with execution is that the actual outcome of the action taken can be used to prune from the tree the branches rooted at the unrealized outcomes. The subtree rooted at the realized state can then be expanded further to make the next action choice. The algorithm of Hansen and Zilberstein (1998) can be viewed as a variant of these methods in which stationary policies (i.", "startOffset": 0, "endOffset": 1983}, {"referenceID": 4, "context": "Furthermore, in nite-horizon problems pose the di culty of determining a su ciently deep tree. One way around this di culty is the use of real time search (Korf, 1990). In particular, real-time dynamic programming, or RTDP, has been proposed in (Barto et al., 1995) as a way of approximately solving large MDPs in an online fashion. One can interleave search with execution of an approximately optimal policy using a form of RTDP similar to decisiontree evaluation as follows. Imagine the agent nds itself in a particular state sinit. It can then build a partial search tree to some depth, perhaps uniformly or perhaps with some branches expanded more deeply than others. Partial tree construction may be halted due to time pressure or due to an assessment by the agent that further expansion of the tree may not be fruitful. When a decision to act must be made, the rollback procedure is applied to this partial, possibly unevenly expanded decision tree. Reward values can be used to evaluate the leaves of the tree, but this may o er an inaccurate picture of the value of nodes higher in the tree. Heuristic information can be used to estimate the long-term value of states labeling leaves. As with value iteration, the deeper the tree, the more accurate the estimated value at the root (generally speaking) for a xed heuristic. We will see in Section 5 that structured representations of MDPs can provide a means to construct such heuristics (Dearden & Boutilier, 1994, 1997). Speci cally, with admissible heuristics or upper and lower bounds on the true values of leaf nodes in the tree, methods such as A* or branch-and-bound search can be used. A key advantage of integrating search with execution is that the actual outcome of the action taken can be used to prune from the tree the branches rooted at the unrealized outcomes. The subtree rooted at the realized state can then be expanded further to make the next action choice. The algorithm of Hansen and Zilberstein (1998) can be viewed as a variant of these methods in which stationary policies (i.e., state-action mappings) can be extracted during the search process. RTDP is formulated by Barto et al. (1995) more generally as a form of online, asynchronous value iteration.", "startOffset": 0, "endOffset": 2172}, {"referenceID": 4, "context": "Furthermore, in nite-horizon problems pose the di culty of determining a su ciently deep tree. One way around this di culty is the use of real time search (Korf, 1990). In particular, real-time dynamic programming, or RTDP, has been proposed in (Barto et al., 1995) as a way of approximately solving large MDPs in an online fashion. One can interleave search with execution of an approximately optimal policy using a form of RTDP similar to decisiontree evaluation as follows. Imagine the agent nds itself in a particular state sinit. It can then build a partial search tree to some depth, perhaps uniformly or perhaps with some branches expanded more deeply than others. Partial tree construction may be halted due to time pressure or due to an assessment by the agent that further expansion of the tree may not be fruitful. When a decision to act must be made, the rollback procedure is applied to this partial, possibly unevenly expanded decision tree. Reward values can be used to evaluate the leaves of the tree, but this may o er an inaccurate picture of the value of nodes higher in the tree. Heuristic information can be used to estimate the long-term value of states labeling leaves. As with value iteration, the deeper the tree, the more accurate the estimated value at the root (generally speaking) for a xed heuristic. We will see in Section 5 that structured representations of MDPs can provide a means to construct such heuristics (Dearden & Boutilier, 1994, 1997). Speci cally, with admissible heuristics or upper and lower bounds on the true values of leaf nodes in the tree, methods such as A* or branch-and-bound search can be used. A key advantage of integrating search with execution is that the actual outcome of the action taken can be used to prune from the tree the branches rooted at the unrealized outcomes. The subtree rooted at the realized state can then be expanded further to make the next action choice. The algorithm of Hansen and Zilberstein (1998) can be viewed as a variant of these methods in which stationary policies (i.e., state-action mappings) can be extracted during the search process. RTDP is formulated by Barto et al. (1995) more generally as a form of online, asynchronous value iteration. Speci cally, the values \\rolled backed\" can be cached and used as improved heuristic estimates of the value function at the states in question. This technique is also investigated in (Bonet et al., 1997; Dearden & Boutilier, 1994, 1997; Koenig & Simmons, 1995), and is closely tied to Korf's (1990) LRTA* algorithm.", "startOffset": 0, "endOffset": 2537}, {"referenceID": 103, "context": "Decision-Theoretic Planning: Structural Assumptions A Bayesian network (Pearl, 1988) is a representational framework for compactly representing a probability distribution in factored form.", "startOffset": 71, "endOffset": 84}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions A Bayesian network (Pearl, 1988) is a representational framework for compactly representing a probability distribution in factored form. Although these networks have most typically been used to represent atemporal problem domains, we can apply the same techniques to represent Markov chains, encoding the chain's transition probabilities in the network structure (Dean & Kanazawa, 1989). Formally, a Bayes net is a directed acyclic graph in which vertices correspond to random variables and an edge between two variables indicates a direct probabilistic dependency between them. A network so constructed also re ects implicit independencies among the variables. The network must be quanti ed by specifying a probability for each variable (vertex) conditioned on all possible values of its immediate parents in the graph. In addition, the network must include a marginal distribution: an unconditional probability for each vertex that has no parents. This quanti cation is captured by associating a conditional probability table (CPT) with each variable in the network. Together with the independence assumptions de ned by the graph, this quanti cation de nes a unique joint distribution over the variables in the network. The probability of any event over this space can then be computed using algorithms that exploit the independencies represented within the graphical structure. We refer to Pearl (1988) for details.", "startOffset": 107, "endOffset": 1458}, {"referenceID": 5, "context": "Such correlations can be thought of as rami cations (Baker, 1991; Finger, 1986; Lin & Reiter, 1994).", "startOffset": 52, "endOffset": 99}, {"referenceID": 51, "context": "Such correlations can be thought of as rami cations (Baker, 1991; Finger, 1986; Lin & Reiter, 1994).", "startOffset": 52, "endOffset": 99}, {"referenceID": 19, "context": "30 Decision-tree and decision-graph representations are used to represent actions in fully observable MDPs in (Boutilier et al., 1995; Hoey, St-Aubin, Hu, & Boutilier, 1999) and 30.", "startOffset": 110, "endOffset": 173}, {"referenceID": 108, "context": "Decision-Theoretic Planning: Structural Assumptions are described in detail in (Poole, 1995; Boutilier & Goldszmidt, 1996).", "startOffset": 79, "endOffset": 122}, {"referenceID": 62, "context": "A probabilistic Strips operator (PSO) (Hanks, 1990; Hanks & McDermott, 1994; Kushmerick et al., 1995) extends the Strips representation in two ways.", "startOffset": 38, "endOffset": 101}, {"referenceID": 80, "context": "A probabilistic Strips operator (PSO) (Hanks, 1990; Hanks & McDermott, 1994; Kushmerick et al., 1995) extends the Strips representation in two ways.", "startOffset": 38, "endOffset": 101}, {"referenceID": 122, "context": "The fact that certain direct dependencies among variables in a Bayes net are rendered irrelevant under speci c variable assignments has been studied more generally in the guise of context-speci c independence (Boutilier, Friedman, Goldszmidt, & Koller, 1996); see (Geiger & Heckerman, 1991; Shimony, 1993) for related notions.", "startOffset": 264, "endOffset": 305}, {"referenceID": 104, "context": "The conditional nature of e ects is also a feature of a deterministic extension of Strips known as ADL (Pednault, 1989).", "startOffset": 103, "endOffset": 119}, {"referenceID": 4, "context": "Boutilier, Dean, & Hanks form with synchronic arcs, as described in Section 4.1. Essentially, correlated e ects are \\compiled\" into explicit outcomes in PSOs. Recent results by Littman (1997) have shown that simple 2TBNs and PSOs can both be used to represent any action represented as a 2TBN without an exponential blowup in representation size.", "startOffset": 25, "endOffset": 192}, {"referenceID": 121, "context": "3 In uence Diagrams In uence diagrams (Howard & Matheson, 1984; Shachter, 1986) extend Bayesian networks to include special decision nodes to represent action choices, and value nodes to represent the e ect of action choice on a value function.", "startOffset": 38, "endOffset": 79}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions action networks shown in Figure 23(a) and (b). Action a1 makes Y true with probability 0:9 if X is true (having no e ect otherwise), while a2 makes Y true if Z is true. Combining these actions in a single network in the obvious way produces the in uence diagram shown in Figure 23(c). Notice that Y now has four parent nodes, inheriting the union of all its parents in the individual networks (plus the action node) and requiring a CPT with 16 entries for actions a1 and a2 together with eight additional entries for each action that does not a ect Y . The individual networks re ect the fact that Y depends on X only when a1 is performed and on Z only when a2 is performed. This fact is lost in the naively constructed in uence diagram. However, structured CPTs can be used to recapture this independence and compactness of representation: the tree of Figure 23(d) captures the distribution much more concisely, requiring only eight entries. This structured representation also allows us concisely to express that Y persists under all other actions. In large domains, we expect variables to generally be una ected by a substantial number of (perhaps most) actions, thus requiring representations such as this for in uence diagrams. See (Boutilier & Goldszmidt, 1996) for a deeper discussion of this issue and its relationship to the frame problem. While we provide no distributional information over the action choice, it is not hard to see that a 2TBN with an explicit decision node can be used to represent the Markov chain induced by a particular policy in a very natural way. Speci cally, by adding arcs from state variables at time t to the decision node, the value of the decision node (i.e., the choice of action at that point) can be dictated by the prevailing state.38 4.3 In uence Diagrams In uence diagrams (Howard & Matheson, 1984; Shachter, 1986) extend Bayesian networks to include special decision nodes to represent action choices, and value nodes to represent the e ect of action choice on a value function. The presence of decision nodes means that action choice is treated as a variable under the decision maker's control. Value nodes treat reward as a variable in uenced (usually deterministically) by certain state variables. In uence diagrams have not typically been associated with the schematic representation of stationary systems, instead being used as a tool for decision analysts where the sequential decision problem is carefully handcrafted. This more generic use of in uence diagrams has been discussed by Tatman and Shachter (1990). In any event, there is no theory of plan construction associated with in uence diagrams: the choice of all possible actions at each stage must be explicitly encoded in the model.", "startOffset": 77, "endOffset": 2617}, {"referenceID": 19, "context": "Although in the worst case the CRT will take exponential space to store, in many cases the reward function exhibits structure, allowing it to be represented compactly using decision trees or graphs (Boutilier et al., 1995), Strips-like tables (Boutilier & Dearden, 1994), or logical rules (Poole, 1995, 1997a).", "startOffset": 198, "endOffset": 222}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions nonstationary or history-dependent rewards and are often used to represent value functions for nite-horizon problems. Although in the worst case the CRT will take exponential space to store, in many cases the reward function exhibits structure, allowing it to be represented compactly using decision trees or graphs (Boutilier et al., 1995), Strips-like tables (Boutilier & Dearden, 1994), or logical rules (Poole, 1995, 1997a). Figure 24 shows a fragment of one possible decision-tree representation for the reward function used in the running example. The independence assumptions studied in multiattribute utility theory (Keeney & Rai a, 1976) provide yet another way in which reward functions can be represented compactly. If we assume that the component attributes of the reward function make independent contributions to a state's total reward, the individual contributions can be combined functionally. For instance, we might imagine penalizing states where CR holds with a (partial) reward of 3, penalizing situations where there is undelivered mail (M _ RHM) with 2, and penalizing untidiness T (i) with i 4 (i.e., in proportion to how untidy things are). The reward for any state can then be determined simply by adding the individual penalties associated with each feature. The individual component rewards along with the combination function constitute a compact representation of the reward function. The tree fragment in Figure 24, which re ects the additive independent structure just described, is considerably more complex than a representation that de nes the (independent) rewards for individual propositions separately. The use of additive reward functions for MDPs is considered in (Boutilier, Brafman, & Geib, 1997; Meuleau, Hauskrecht, Kim, Peshkin, Kaelbling, Dean, & Boutilier, 1998; Singh & Cohn, 1998). Another example of structured rewards is the goal structure studied in classical planning. Goals are generally speci ed by a single proposition (or a set of literals) to be achieved. As such, they can generally be represented very compactly. Haddawy and Hanks (1998) explore generalizations of goal-oriented models that permit extensions such as partial goal satisfaction, yet still admit compact representations.", "startOffset": 104, "endOffset": 2149}, {"referenceID": 19, "context": "Decision trees have also been used for policies and value functions (Boutilier et al., 1995; Chapman & Kaelbling, 1991).", "startOffset": 68, "endOffset": 119}, {"referenceID": 4, "context": "GetC Cclk M PUM M PUM Cclk Cclk Cclk HRM DelM Cclk Figure 25: A tree representation of a policy. actions and reward functions can be used to represent policies and value functions as well. Here we focus on stationary policies and value functions for FOMDPs, for which any logical function representation may be used. For example, Schoppers (1987) uses a Strips-style representation for universal plans, which are deterministic, plan-like policies.", "startOffset": 51, "endOffset": 347}, {"referenceID": 4, "context": "3 Figure 26: Di erent forms of state space abstraction. cluster agree on this characteristic. A non-exact abstraction is called approximate. This is illustrated schematically in Figure 26: the exact abstraction groups together states that agree on the value assigned to them by a value function, while the approximate abstraction allows states to be grouped together that di er in value. The extent to which these states di er is often used as a measure of the quality of an approximate abstraction. A third dimension is adaptivity. Technically, this is a property not of an abstraction itself, but of how abstractions are used by a particular algorithm. An adaptive abstraction technique is one in which the abstraction can change during the course of computation, while a xed abstraction scheme groups together states once and for all (again, see Figure 26). For example, one can imagine using an abstraction in the representation of a value function V k, then revising this abstraction to represent V k+1 more accurately. Abstraction and aggregation techniques have been studied in the OR literature on MDPs. Bertsekas and Castanon (1989) develop an adaptive aggregation (as opposed to abstraction) technique.", "startOffset": 2, "endOffset": 1142}, {"referenceID": 4, "context": "3 Figure 26: Di erent forms of state space abstraction. cluster agree on this characteristic. A non-exact abstraction is called approximate. This is illustrated schematically in Figure 26: the exact abstraction groups together states that agree on the value assigned to them by a value function, while the approximate abstraction allows states to be grouped together that di er in value. The extent to which these states di er is often used as a measure of the quality of an approximate abstraction. A third dimension is adaptivity. Technically, this is a property not of an abstraction itself, but of how abstractions are used by a particular algorithm. An adaptive abstraction technique is one in which the abstraction can change during the course of computation, while a xed abstraction scheme groups together states once and for all (again, see Figure 26). For example, one can imagine using an abstraction in the representation of a value function V k, then revising this abstraction to represent V k+1 more accurately. Abstraction and aggregation techniques have been studied in the OR literature on MDPs. Bertsekas and Castanon (1989) develop an adaptive aggregation (as opposed to abstraction) technique. The proposed method operates on at state spaces, however, and therefore does not exploit implicit structure in the state space itself. An adaptive, uniform abstraction method is proposed by Schweitzer et al. (1985) for solving stochastic queuing models.", "startOffset": 2, "endOffset": 1428}, {"referenceID": 119, "context": "This use of dynamic programming using Strips action descriptions forms the basic idea of Schoppers's universal planning method (Schoppers, 1987).", "startOffset": 127, "endOffset": 144}, {"referenceID": 30, "context": "Another general technique for solving classical planning problems is partial order planning (POP) (Chapman, 1987; Sacerdoti, 1975), embodied in such popular planning algorithms as SNLP (McAllester & Rosenblitt, 1991) and UCPOP (Penberthy & Weld, 1992).", "startOffset": 98, "endOffset": 130}, {"referenceID": 118, "context": "Another general technique for solving classical planning problems is partial order planning (POP) (Chapman, 1987; Sacerdoti, 1975), embodied in such popular planning algorithms as SNLP (McAllester & Rosenblitt, 1991) and UCPOP (Penberthy & Weld, 1992).", "startOffset": 98, "endOffset": 130}, {"referenceID": 4, "context": "Goal S S S S 4 3 2 1 Figure 27: An example of goal regression. regression to compute the (largest) set of states such that, after executing , all subgoals are satis ed. In particular, the state space is repartitioned into two abstract states: SGi+1 and SGi+1. In this way, the abstraction mechanism implemented by goal regression should be considered adaptive. This can be viewed as an (i + 1)-stage value function: any state satisfying SGi+1 can reach a goal state in i+1 steps using the action sequence that produced SGi+1.41 The regression process can be stopped when the initial state is a member of the abstract state SGi+1. Figure 27 illustrates the repartitioning of the state space into the di erent regions SGi+1 for each of the steps in the example above. While regression produces a compact representation of something like a value function (as in our discussion of deterministic, goal-based dynamic programming in Section 3.2), the analogy is not exact in that the regions produced by regression record only the property of goal reachability contingent on a particular choice of action or action sequence. Standard dynamic programming methods can be implemented in a structured way by simply noticing that a number of di erent regions can be produced at the ith iteration by considering all actions that can be regressed at that stage. The union of all of these regressions form the states that have positive values in Vi, thus making the representation of the i-stage-to-go value function exact. Notice that each iteration is now more costly, since regression through all actions must be attempted, but this approach obviates the need for backtracking and can ensure that a shortest plan is found. Standard regression does not provide such guarantees without commitment to a particular search strategy (e.g., breadthrst). This use of dynamic programming using Strips action descriptions forms the basic idea of Schoppers's universal planning method (Schoppers, 1987). Another general technique for solving classical planning problems is partial order planning (POP) (Chapman, 1987; Sacerdoti, 1975), embodied in such popular planning algorithms as SNLP (McAllester & Rosenblitt, 1991) and UCPOP (Penberthy & Weld, 1992).42 The main motivation for the least-commitment approach comes from the realization that regression techniques are incrementally building a plan from the end to the beginning (in the temporal dimension). Thus, each iteration must commit to inserting a step last in the plan. In many cases it can be determined that a particular step must appear somewhere in the plan, but not necessarily as the last step in the plan; and, indeed, in many cases the step 41. It is not the case, however, that states in SGi+1 cannot reach the goal region in i + 1 steps. It is only the case that they cannot do so using the speci c sequence of actions chosen so far. 42. This type of planning is also sometimes called nonlinear or least-commitment planning. See Weld's (1994) survey for a nice overview.", "startOffset": 21, "endOffset": 2991}, {"referenceID": 70, "context": "See (Kambhampati, 1997) for a framework that uni es various approaches to solving classical plan-generation problems.", "startOffset": 4, "endOffset": 23}, {"referenceID": 80, "context": "While techniques relying on regression have been studied extensively in the deterministic setting, they have only recently been applied to probabilistic unobservable (Kushmerick et al., 1995) and partially observable (Draper, Hanks, & Weld, 1994b) domains.", "startOffset": 166, "endOffset": 191}, {"referenceID": 14, "context": "For FOMDPs, approaches of this type are taken in (Boutilier, 1997; Boutilier & Dearden, 1996; Boutilier et al., 1995; Boutilier, Dearden, & Goldszmidt, 1999; Dietterich & Flann, 1995; Hoey et al., 1999).", "startOffset": 49, "endOffset": 202}, {"referenceID": 19, "context": "For FOMDPs, approaches of this type are taken in (Boutilier, 1997; Boutilier & Dearden, 1996; Boutilier et al., 1995; Boutilier, Dearden, & Goldszmidt, 1999; Dietterich & Flann, 1995; Hoey et al., 1999).", "startOffset": 49, "endOffset": 202}, {"referenceID": 68, "context": "For FOMDPs, approaches of this type are taken in (Boutilier, 1997; Boutilier & Dearden, 1996; Boutilier et al., 1995; Boutilier, Dearden, & Goldszmidt, 1999; Dietterich & Flann, 1995; Hoey et al., 1999).", "startOffset": 49, "endOffset": 202}, {"referenceID": 19, "context": "By piecing together the regions produced for the di erent labels in the description of V0, we can construct a set of regions such that each state in a given region: (a) transitions (under action ) to a particular part of V0 with identical probability; and hence (b) has identical expected future value (Boutilier et al., 1995).", "startOffset": 302, "endOffset": 326}, {"referenceID": 4, "context": "V Figure 29: An iteration of decision-theoretic regression. Step 1 produces the portion of the tree with dashed lines, while Step 2 produces the portion with dotted lines. determine whether the future value is 0 or 9:0). The probability with which Z becomes true is given by the tree representing the CPT for node Z. In Step 2 in Figure 29, the conditions in that CPT are conjoined to the conditions required for predicting Y 's probability (by \\grafting\" the tree for Z to the tree for Y given by the rst step). This grafting is slightly di erent at each of the three leaves of the tree for Y : (a) the full tree for Z is attached to the leaf X = t; (b) the tree for Z is simpli ed where it is attached to to the leaf X = f ^ Y = f by removal of the redundant test on variable Y ; (c) notice that there is no need to attach the tree for Z to the leaf X = f ^ Y = t, since a makes Y true with probability 1 under those conditions (and Z is relevant to the determination of V 0 only when Y is false). At each of the leaves of the newly formed tree we have both Pr(Y ) and Pr(Z). Each of these joint distributions over Y and Z (the e ect of a and these variables is independent by the semantics of the network) tells us the probability of having Y and Z true with zero stages to go given that the conditions labeling the appropriate branch of the tree hold with one stage to go. In other words, the new tree uniquely determines, for any state with one stage remaining, the probability of making any of the conditions labeling the branches of V 0 true. The computation of expected future value obtained by performing a with one stage to go can then be placed at the leaves of this tree by taking expectation over the values at the leaves of V 0. 2 The new set of regions produced this way describes the function Q 1 , where Q 1 (s) is the value associated with performing at state s with one stage to go and acting optimally thereafter. These functions (for each action ) can be pieced together (i.e., \\maxed\"|see Section 3.1) to determine V1. Of course, the process can be repeated some number of times to produce Vn for some suitable n, as well as the optimal policy with respect to Vn. This basic technique can be used in a number of di erent ways. Dietterich and Flann (1995) propose ideas similar to these, but restrict attention to MDPs with goal regions 65", "startOffset": 2, "endOffset": 2277}, {"referenceID": 14, "context": ", synchronic arcs in the 2TBNs) in (Boutilier, 1997).", "startOffset": 35, "endOffset": 52}, {"referenceID": 20, "context": "In (Boutilier et al., 1999), results on a series of abstract process-planning examples are reported, and the scheme is shown to be very useful, especially for larger problems.", "startOffset": 3, "endOffset": 27}, {"referenceID": 20, "context": "What might be viewed as best- and worst-case behavior is also described in (Boutilier et al., 1999).", "startOffset": 75, "endOffset": 99}, {"referenceID": 68, "context": "In (Hoey et al., 1999), a similar algorithm is described that uses algebraic decision diagrams (ADDs) (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi, 1993) rather than trees.", "startOffset": 3, "endOffset": 22}, {"referenceID": 25, "context": "ADDs are a simple generalization of boolean decision diagrams (BDDs) (Bryant, 1986) that allow terminal nodes to be labeled with real values instead of just boolean values.", "startOffset": 69, "endOffset": 83}, {"referenceID": 68, "context": "Initial results provided in (Hoey et al., 1999) are encouraging, showing considerable savings over tree-based algorithms on the same problems.", "startOffset": 28, "endOffset": 47}, {"referenceID": 4, "context": "(1995) develop a version of modi ed policy iteration to produce tree-structured policies and value functions, while Boutilier and Dearden (1996) develop the version of value iteration described above.", "startOffset": 26, "endOffset": 145}, {"referenceID": 4, "context": "(1995) develop a version of modi ed policy iteration to produce tree-structured policies and value functions, while Boutilier and Dearden (1996) develop the version of value iteration described above. These algorithms are extended to deal with correlations in action e ects (i.e., synchronic arcs in the 2TBNs) in (Boutilier, 1997). These abstraction schemes can be categorized as nonuniform, exact and adaptive. The utility of such exact abstraction techniques has not been tested on real-world problems to date. In (Boutilier et al., 1999), results on a series of abstract process-planning examples are reported, and the scheme is shown to be very useful, especially for larger problems. For example, in one speci c problem with 1.7 million states, the tree representation of the value function has only 40,000 leaves, indicating a tremendous amount of regularity in the value function. Schemes like this exploit such regularity to solve problems more quickly (in this example, in much less than half the time required by modi ed policy iteration) and with much lower memory demands. However, these schemes do involve substantial overhead in tree construction, and for smaller problems with little regularity, the overhead is not repaid in time savings (simple vector-matrix representations methods are faster), though they still generally provide substantial memory savings. What might be viewed as best- and worst-case behavior is also described in (Boutilier et al., 1999). In a series of \\linear\" examples (i.e., problems with value functions that can be represented with trees whose size is linear in the number of problem variables), the tree-based scheme solves problems many orders of magnitude faster than classical state-based techniques. In contrast, problems with exponentially-many distinct values are also tested (i.e., with a distinct value at each state): here tree-construction methods are required to construct a complete decision tree in addition to performing the same number of expected value and maximization computations as classical methods. In this worst case, tree-construction overhead makes the algorithm run about 100 times slower than standard modi ed policy iteration. In (Hoey et al., 1999), a similar algorithm is described that uses algebraic decision diagrams (ADDs) (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi, 1993) rather than trees. ADDs are a simple generalization of boolean decision diagrams (BDDs) (Bryant, 1986) that allow terminal nodes to be labeled with real values instead of just boolean values. Essentially, ADD-based algorithms are similar to the tree-based algorithms except that isomorphic subtrees can be shared. This lets ADDs provide more compact representations of certain types of value functions. Highly optimized ADD manipulation and evaluation software developed in the veri cation community can also be applied to solving MDPs. Initial results provided in (Hoey et al., 1999) are encouraging, showing considerable savings over tree-based algorithms on the same problems. For example, the ADD algorithm applied to the 1.7-million-state example described above revealed the value function to have only 178 distinct values (cf. the 40,000 tree leaves required) and produced an ADD description of the value function with less than 2200 internal nodes. It also solved the same problem in seven minutes, about 40 times faster than earlier reported timing results using decision trees (though some of this improvement was due to the use of optimized ADD software packages). Similar results obtain with other problems (problems of up to 268 million states 44. Dietterich and Flann (1995) also describe their work in the context of reinforcement learning rather than as a method for solving MDPs directly.", "startOffset": 26, "endOffset": 3655}, {"referenceID": 121, "context": "Methods for solving in uence diagrams (Shachter, 1986) exploit structure in a natural way; Tatman and Shachter (1990) explore the connection between in uence diagrams and FOMDPs and the relationship between in uence diagram solution techniques and dynamic programming.", "startOffset": 38, "endOffset": 54}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions were solved in about four hours). Most encouraging is the fact that on the worst-case (exponential) examples, the overhead associated with using ADDs|compared to classical, vector-based methods|is much less than with trees (about a factor of 20 compared to \\ at\" modi ed policy iteration with 12 state variables), and lessens as problems become larger. Like tree-based algorithms, these methods have yet to be applied to real-world problems. With these exact abstraction schemes it is clear that, while in some examples the resulting policies and value functions may be compact, in others the set of regions may get very large (even reaching the level of individual states Boutilier et al., 1995), thus precluding any computational savings. Boutilier and Dearden (1996) develop an approximation scheme that exploits the tree-structured nature of the value functions produced.", "startOffset": 73, "endOffset": 822}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions were solved in about four hours). Most encouraging is the fact that on the worst-case (exponential) examples, the overhead associated with using ADDs|compared to classical, vector-based methods|is much less than with trees (about a factor of 20 compared to \\ at\" modi ed policy iteration with 12 state variables), and lessens as problems become larger. Like tree-based algorithms, these methods have yet to be applied to real-world problems. With these exact abstraction schemes it is clear that, while in some examples the resulting policies and value functions may be compact, in others the set of regions may get very large (even reaching the level of individual states Boutilier et al., 1995), thus precluding any computational savings. Boutilier and Dearden (1996) develop an approximation scheme that exploits the tree-structured nature of the value functions produced. At each stage k, the value function Vk can be pruned to produce a smaller, less accurate tree that approximates Vk. Speci cally, approximate value functions are represented using trees whose leaves are labeled with an upper and lower bound on the value function in that region; decisiontheoretic regression is performed on these bounds. Certain subtrees of the value tree can be pruned when leaves of the subtree are very close in value or when the tree is too large given computational constraints. This scheme is nonuniform, approximate and adaptive. This approximation scheme can be tailored to provide (roughly) the most accurate value function of a given maximum tree size, or the smallest value function (with respect to tree size) of some given minimum accuracy. Results reported in (Boutilier & Dearden, 1996) show that approximation on a small set of examples (including the worst-case examples for tree-based algorithms) allows substantial reduction in computational cost. For instance, in a 10-variable worst-case example, a small amount of pruning introduced an average error of only 0.5% but reduced computation time by a factor of 50. More aggressive pruning tends to increase error and decrease computation time very rapidly; making appropriate tradeo s in these two dimensions is still to be addressed. This method too remains to be tested and evaluated on realistic problems. Structured representations and solution algorithms can be applied to problems other than FOMDPs. Methods for solving in uence diagrams (Shachter, 1986) exploit structure in a natural way; Tatman and Shachter (1990) explore the connection between in uence diagrams and FOMDPs and the relationship between in uence diagram solution techniques and dynamic programming.", "startOffset": 73, "endOffset": 2536}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions were solved in about four hours). Most encouraging is the fact that on the worst-case (exponential) examples, the overhead associated with using ADDs|compared to classical, vector-based methods|is much less than with trees (about a factor of 20 compared to \\ at\" modi ed policy iteration with 12 state variables), and lessens as problems become larger. Like tree-based algorithms, these methods have yet to be applied to real-world problems. With these exact abstraction schemes it is clear that, while in some examples the resulting policies and value functions may be compact, in others the set of regions may get very large (even reaching the level of individual states Boutilier et al., 1995), thus precluding any computational savings. Boutilier and Dearden (1996) develop an approximation scheme that exploits the tree-structured nature of the value functions produced. At each stage k, the value function Vk can be pruned to produce a smaller, less accurate tree that approximates Vk. Speci cally, approximate value functions are represented using trees whose leaves are labeled with an upper and lower bound on the value function in that region; decisiontheoretic regression is performed on these bounds. Certain subtrees of the value tree can be pruned when leaves of the subtree are very close in value or when the tree is too large given computational constraints. This scheme is nonuniform, approximate and adaptive. This approximation scheme can be tailored to provide (roughly) the most accurate value function of a given maximum tree size, or the smallest value function (with respect to tree size) of some given minimum accuracy. Results reported in (Boutilier & Dearden, 1996) show that approximation on a small set of examples (including the worst-case examples for tree-based algorithms) allows substantial reduction in computational cost. For instance, in a 10-variable worst-case example, a small amount of pruning introduced an average error of only 0.5% but reduced computation time by a factor of 50. More aggressive pruning tends to increase error and decrease computation time very rapidly; making appropriate tradeo s in these two dimensions is still to be addressed. This method too remains to be tested and evaluated on realistic problems. Structured representations and solution algorithms can be applied to problems other than FOMDPs. Methods for solving in uence diagrams (Shachter, 1986) exploit structure in a natural way; Tatman and Shachter (1990) explore the connection between in uence diagrams and FOMDPs and the relationship between in uence diagram solution techniques and dynamic programming. Boutilier and Poole (1996) show how classic history-independent methods for solving POMDPs, based on conversion to a FOMDP with belief states, can exploit the types of structured representations described here.", "startOffset": 73, "endOffset": 2714}, {"referenceID": 117, "context": "This approach has been adopted in classical planning in hierarchical or abstraction-based planners, pioneered by Sacerdoti's AbStrips system (Sacerdoti, 1974).", "startOffset": 141, "endOffset": 158}, {"referenceID": 4, "context": "A similar form of abstraction is studied by Knoblock (1993) (see also Knoblock, Tenenberg, & Yang, 1991).", "startOffset": 10, "endOffset": 60}, {"referenceID": 15, "context": "However, a series of such abstractions can be used that take into account objectives of decreasing importance, and the a posteriori most valuable objectives can be dealt with once risk and controllability are taken into account (Boutilier et al., 1997).", "startOffset": 228, "endOffset": 252}, {"referenceID": 4, "context": "Boutilier, Dean, & Hanks We can think of the dynamic programming techniques that rely on structured representations discussed earlier as operating on a reduced model without ever explicitly constructing that model. In some cases, building the reduced model once and for all may be appropriate; in other cases, one might save considerable e ort by explicitly constructing only those parts of the reduced model that are absolutely necessary. There are some potential computational problems with the model-minimization techniques sketched above. A small minimal model may exist, but it may be hard to nd. Instead, we might look for a reduced model that is easier to nd but not necessarily minimal. This too could fail, in which case we might look for a model small enough to be useful but only approximately equivalent to the original factored model. We have to be careful what we mean by \\approximate,\" but intuitively two MDPs are approximately equivalent if the corresponding optimal value functions are within some small factor of one another. In order to be practical, MDP model reduction schemes operate directly on the implicit or factored representation of the original MDP. Lee and Yannakakis (1992) call this online model minimization.", "startOffset": 39, "endOffset": 1206}, {"referenceID": 16, "context": "General k-ary constraints of this type are considered in (Boutilier et al., 1998).", "startOffset": 57, "endOffset": 81}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions Several schemes have been proposed in the AI literature for exploiting such reachability constraints, apart from the usual forward- or backward-search approaches. Peot and Smith (1993) introduce the operator graph, a structure computed prior to problem solving that caches reachability relationships among propositions.", "startOffset": 108, "endOffset": 237}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions Several schemes have been proposed in the AI literature for exploiting such reachability constraints, apart from the usual forward- or backward-search approaches. Peot and Smith (1993) introduce the operator graph, a structure computed prior to problem solving that caches reachability relationships among propositions. The graph can be consulted during the planning process in deciding which actions to insert into the plan and how to resolve threats. The GraphPlan algorithm of Blum and Furst (1995) attempts to blend considerations of both forward and backward reachability in a deterministic planning context.", "startOffset": 108, "endOffset": 554}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions Several schemes have been proposed in the AI literature for exploiting such reachability constraints, apart from the usual forward- or backward-search approaches. Peot and Smith (1993) introduce the operator graph, a structure computed prior to problem solving that caches reachability relationships among propositions. The graph can be consulted during the planning process in deciding which actions to insert into the plan and how to resolve threats. The GraphPlan algorithm of Blum and Furst (1995) attempts to blend considerations of both forward and backward reachability in a deterministic planning context. One of the di culties with regression is that we may regress the goal region through a sequence of operators only to nd ourselves in a region that cannot be reached from the initial state. In Figure 32(a), for example, not all states in region R may be reachable from the initial state. GraphPlan constructs a variant of the operator graph called the planning graph, in which certain forward reachability constraints are posted. Regression is then implemented as usual, but if the current subgoal set violates the forward reachability constraints at any point, this subgoal set is abandoned and the regression search backtracks. Conceptually, one might think of GraphPlan as constructing a forward search tree through state space with the initial state as the root, then doing a backward search from the goal region backward through this tree. Of course, the process is not state-based: instead, constraints on the possible variable values that can hold simultaneously at di erent planning stages are recorded, and regression is used to search backward through the planning graph. In a sense, GraphPlan can be viewed as constructing an abstraction in which forward-reachable states are distinguished from unreachable states at each planning stage, and using this distinction among abstract states quickly to identify infeasible regression paths. Note, however, the GraphPlan approximates this distinction by overestimating the set of reachable states. Overestimation (as opposed to underestimation) ensures that the regression search space contains all legitimate plans. Reachability has also been exploited in the solution of more general MDPs. Dean et al. (1995) propose an envelope method for solving \\goal-based\" MDPs approximately.", "startOffset": 108, "endOffset": 2331}, {"referenceID": 16, "context": "Just as reachability constraints can be used to prune regression paths in deterministic domains, they can be used to prune value function and policy estimates generated by decision-theoretic regression and abstraction algorithms (Boutilier et al., 1998).", "startOffset": 229, "endOffset": 253}, {"referenceID": 16, "context": "The results reported in (Boutilier et al., 1998) are limited to a single process-planning domain, but show that reachability analysis together with abstraction can provide substantial reductions in the size of the e ective MDP that must be solved, at least in some domains.", "startOffset": 24, "endOffset": 48}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions This observation leads to the following suggestion for optimal policy construction:49 we solve the subprocesses consisting of the recurrent classes for the MDPs; we then remove these states from the MDP, forming a reduced MDP consisting only of the transient states. We then break the reduced MDP into its recurrent classes and solve these independently. The key to doing this e ectively is to use the value function for the original recurrent states (computed in solving the independent subproblems in Step 1) to take into account transitions out of the recurrent classes in the reduced MDP. Figure 32(c) shows an MDP broken into the classes that might be constructed this way. In the original MDP, classes C and E are recurrent and can be solved independently. Once removed from the MDP, class D is recurrent in the reduced MDP. It can, of course, be solved without reference to classes A and B, but does rely on the value of the states that it transitions to in class E. However, the value function for E is available for this purpose, and can be used to solve for D as if D consisted only of jDj states. With this in hand, B can then be solved, and nally A can be solved. Lin and Dean (1995) provide a version of this type of decomposition that also employs a factored representation.", "startOffset": 82, "endOffset": 1248}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions This observation leads to the following suggestion for optimal policy construction:49 we solve the subprocesses consisting of the recurrent classes for the MDPs; we then remove these states from the MDP, forming a reduced MDP consisting only of the transient states. We then break the reduced MDP into its recurrent classes and solve these independently. The key to doing this e ectively is to use the value function for the original recurrent states (computed in solving the independent subproblems in Step 1) to take into account transitions out of the recurrent classes in the reduced MDP. Figure 32(c) shows an MDP broken into the classes that might be constructed this way. In the original MDP, classes C and E are recurrent and can be solved independently. Once removed from the MDP, class D is recurrent in the reduced MDP. It can, of course, be solved without reference to classes A and B, but does rely on the value of the states that it transitions to in class E. However, the value function for E is available for this purpose, and can be used to solve for D as if D consisted only of jDj states. With this in hand, B can then be solved, and nally A can be solved. Lin and Dean (1995) provide a version of this type of decomposition that also employs a factored representation. The factored representation allows dimensionality reduction in di erent state subspaces by aggregating states that di er only in the values of the irrelevant variables in their subspaces. A key to such a decomposition is the discovery of the recurrent classes of an MDP. Puterman (1994) suggests an adaptation of the Fox-Landi algorithm (Fox & Landi, 1968) for discovering the structure of Markov chains that is O(N2) (recall N = jSj).", "startOffset": 82, "endOffset": 1628}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions This observation leads to the following suggestion for optimal policy construction:49 we solve the subprocesses consisting of the recurrent classes for the MDPs; we then remove these states from the MDP, forming a reduced MDP consisting only of the transient states. We then break the reduced MDP into its recurrent classes and solve these independently. The key to doing this e ectively is to use the value function for the original recurrent states (computed in solving the independent subproblems in Step 1) to take into account transitions out of the recurrent classes in the reduced MDP. Figure 32(c) shows an MDP broken into the classes that might be constructed this way. In the original MDP, classes C and E are recurrent and can be solved independently. Once removed from the MDP, class D is recurrent in the reduced MDP. It can, of course, be solved without reference to classes A and B, but does rely on the value of the states that it transitions to in class E. However, the value function for E is available for this purpose, and can be used to solve for D as if D consisted only of jDj states. With this in hand, B can then be solved, and nally A can be solved. Lin and Dean (1995) provide a version of this type of decomposition that also employs a factored representation. The factored representation allows dimensionality reduction in di erent state subspaces by aggregating states that di er only in the values of the irrelevant variables in their subspaces. A key to such a decomposition is the discovery of the recurrent classes of an MDP. Puterman (1994) suggests an adaptation of the Fox-Landi algorithm (Fox & Landi, 1968) for discovering the structure of Markov chains that is O(N2) (recall N = jSj).50 To alleviate the di culties of algorithms that work with an explicit state-based representation, Boutilier and Puterman (1995) propose a variant of the algorithm that works with a factored 2TBN representation.", "startOffset": 82, "endOffset": 1906}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions This observation leads to the following suggestion for optimal policy construction:49 we solve the subprocesses consisting of the recurrent classes for the MDPs; we then remove these states from the MDP, forming a reduced MDP consisting only of the transient states. We then break the reduced MDP into its recurrent classes and solve these independently. The key to doing this e ectively is to use the value function for the original recurrent states (computed in solving the independent subproblems in Step 1) to take into account transitions out of the recurrent classes in the reduced MDP. Figure 32(c) shows an MDP broken into the classes that might be constructed this way. In the original MDP, classes C and E are recurrent and can be solved independently. Once removed from the MDP, class D is recurrent in the reduced MDP. It can, of course, be solved without reference to classes A and B, but does rely on the value of the states that it transitions to in class E. However, the value function for E is available for this purpose, and can be used to solve for D as if D consisted only of jDj states. With this in hand, B can then be solved, and nally A can be solved. Lin and Dean (1995) provide a version of this type of decomposition that also employs a factored representation. The factored representation allows dimensionality reduction in di erent state subspaces by aggregating states that di er only in the values of the irrelevant variables in their subspaces. A key to such a decomposition is the discovery of the recurrent classes of an MDP. Puterman (1994) suggests an adaptation of the Fox-Landi algorithm (Fox & Landi, 1968) for discovering the structure of Markov chains that is O(N2) (recall N = jSj).50 To alleviate the di culties of algorithms that work with an explicit state-based representation, Boutilier and Puterman (1995) propose a variant of the algorithm that works with a factored 2TBN representation. One di culty with this form of decomposition is its reliance on strongly independent subproblems (i.e., recurrent classes) within the MDP. Others have explored exact and approximate techniques that work under less restrictive assumptions. One simple method of approximation is to construct \\approximately recurrent classes.\" In Figure 32(c) we might imagine that C and E are nearly independent in the sense that all transitions between them are very low-probability or high-cost. Treating them as independent might lead to approximately optimal policies whose error can be bounded. If the solutions to C and E interact strongly enough that the solutions should not be constructed completely independently, a di erent approach to solving the decomposed problem can be taken. If we have the optimal value function for E then, as pointed out, we can calculate the optimal value function for D. The rst thing to note is that we don't need to know the value function for all of the states in E, just the value of every state in E that is reachable from some state in D in a single step. The set of all states outside D reachable in a single step from a state inside D is referred to as the states in the periphery of D. The values of the states in the intersection of E and the periphery of D summarize the value of exiting D and ending up in E. We refer to the set of all states that are in the periphery of some block as the kernel of the MDP. All of the di erent blocks interact with one another through states in the kernel. 49. Ross and Varadarajan (1991) make a related suggestion for solving average-reward problems.", "startOffset": 82, "endOffset": 3545}, {"referenceID": 142, "context": "The OR literature is replete with such methods for both linear and nonlinear systems of equations (Winston, 1992).", "startOffset": 98, "endOffset": 113}, {"referenceID": 45, "context": "It is possible to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994).", "startOffset": 55, "endOffset": 88}, {"referenceID": 114, "context": "It is possible to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994).", "startOffset": 55, "endOffset": 88}, {"referenceID": 4, "context": "5 Spatial features often provide a natural dimension along which to decompose a domain. In our running example, the location of the robot might be used to decompose the state space into blocks of states, one block for each of the possible locations. Figure 33 shows such a decomposition superimposed over the state-transition diagram for the MDP. States in the kernel are shaded and might correspond to the entrances and exits of locations. The star-shaped topology, induced by the kernel decomposition used in (Kushner & Chen, 1974) and (Dean & Lin, 1995), is illustrated in Figure 34. In Figure 33, the hallway location is not explicitly represented. This simpli cation may be reasonable if the hallway is only a conduit for moving from one room to another; in this case the function of the hallway is accounted for in the dynamics governing states in the kernel. Figures 33 and 34 are idealized in that, given the full set of features in our running example, the kernel would contain many more states. 2 One technique for computing the optimal policy for the entire MDP involves repeatedly solving the MDPs corresponding to the individual blocks. The techniques works as follows: initially, we guess the value of every state in the kernel.51 Given a current estimate for the values of the kernel states, we solve the component MDPs; this solution produces a new estimate for the states in the kernel. We adjust the values of the states in the kernel by considering the di erence between the current and the new estimates and iterate until this di erence is negligible. This iterative method for solving a decomposed MDP is a special case of the Lagrangian method for nding the extrema of a function. The OR literature is replete with such methods for both linear and nonlinear systems of equations (Winston, 1992). It is possible to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994). Dantzig and Wolfe (1960) developed a method of decomposing a system of equations involving a very large number of variables into a set of smaller systems of equations interacting through a set of coupling variables (variables shared by more two or more blocks).", "startOffset": 10, "endOffset": 1933}, {"referenceID": 4, "context": "5 Spatial features often provide a natural dimension along which to decompose a domain. In our running example, the location of the robot might be used to decompose the state space into blocks of states, one block for each of the possible locations. Figure 33 shows such a decomposition superimposed over the state-transition diagram for the MDP. States in the kernel are shaded and might correspond to the entrances and exits of locations. The star-shaped topology, induced by the kernel decomposition used in (Kushner & Chen, 1974) and (Dean & Lin, 1995), is illustrated in Figure 34. In Figure 33, the hallway location is not explicitly represented. This simpli cation may be reasonable if the hallway is only a conduit for moving from one room to another; in this case the function of the hallway is accounted for in the dynamics governing states in the kernel. Figures 33 and 34 are idealized in that, given the full set of features in our running example, the kernel would contain many more states. 2 One technique for computing the optimal policy for the entire MDP involves repeatedly solving the MDPs corresponding to the individual blocks. The techniques works as follows: initially, we guess the value of every state in the kernel.51 Given a current estimate for the values of the kernel states, we solve the component MDPs; this solution produces a new estimate for the states in the kernel. We adjust the values of the states in the kernel by considering the di erence between the current and the new estimates and iterate until this di erence is negligible. This iterative method for solving a decomposed MDP is a special case of the Lagrangian method for nding the extrema of a function. The OR literature is replete with such methods for both linear and nonlinear systems of equations (Winston, 1992). It is possible to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994). Dantzig and Wolfe (1960) developed a method of decomposing a system of equations involving a very large number of variables into a set of smaller systems of equations interacting through a set of coupling variables (variables shared by more two or more blocks). In the Dantzig-Wolfe decomposition method, the original, very large system of equations is solved by iteratively solving the smaller systems and adjusting the coupling variables on each iteration until no further adjustment is required. In the linear programming formulation of an MDP, the values of the states are encoded as variables. Kushner and Chen (1974) exploit the fact that MDPs can be modeled as linear programs by using the Dantzig-Wolfe decomposition method to solve MDPs involving a large number of states.", "startOffset": 10, "endOffset": 2531}, {"referenceID": 4, "context": "5 Spatial features often provide a natural dimension along which to decompose a domain. In our running example, the location of the robot might be used to decompose the state space into blocks of states, one block for each of the possible locations. Figure 33 shows such a decomposition superimposed over the state-transition diagram for the MDP. States in the kernel are shaded and might correspond to the entrances and exits of locations. The star-shaped topology, induced by the kernel decomposition used in (Kushner & Chen, 1974) and (Dean & Lin, 1995), is illustrated in Figure 34. In Figure 33, the hallway location is not explicitly represented. This simpli cation may be reasonable if the hallway is only a conduit for moving from one room to another; in this case the function of the hallway is accounted for in the dynamics governing states in the kernel. Figures 33 and 34 are idealized in that, given the full set of features in our running example, the kernel would contain many more states. 2 One technique for computing the optimal policy for the entire MDP involves repeatedly solving the MDPs corresponding to the individual blocks. The techniques works as follows: initially, we guess the value of every state in the kernel.51 Given a current estimate for the values of the kernel states, we solve the component MDPs; this solution produces a new estimate for the states in the kernel. We adjust the values of the states in the kernel by considering the di erence between the current and the new estimates and iterate until this di erence is negligible. This iterative method for solving a decomposed MDP is a special case of the Lagrangian method for nding the extrema of a function. The OR literature is replete with such methods for both linear and nonlinear systems of equations (Winston, 1992). It is possible to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994). Dantzig and Wolfe (1960) developed a method of decomposing a system of equations involving a very large number of variables into a set of smaller systems of equations interacting through a set of coupling variables (variables shared by more two or more blocks). In the Dantzig-Wolfe decomposition method, the original, very large system of equations is solved by iteratively solving the smaller systems and adjusting the coupling variables on each iteration until no further adjustment is required. In the linear programming formulation of an MDP, the values of the states are encoded as variables. Kushner and Chen (1974) exploit the fact that MDPs can be modeled as linear programs by using the Dantzig-Wolfe decomposition method to solve MDPs involving a large number of states. Dean and Lin (1995) describe a general framework for solving decomposed MDPs pointing to the work of Kushner and Chen as a special case, but neither work addresses the issue of where the decompositions come from.", "startOffset": 10, "endOffset": 2710}, {"referenceID": 4, "context": "5 Spatial features often provide a natural dimension along which to decompose a domain. In our running example, the location of the robot might be used to decompose the state space into blocks of states, one block for each of the possible locations. Figure 33 shows such a decomposition superimposed over the state-transition diagram for the MDP. States in the kernel are shaded and might correspond to the entrances and exits of locations. The star-shaped topology, induced by the kernel decomposition used in (Kushner & Chen, 1974) and (Dean & Lin, 1995), is illustrated in Figure 34. In Figure 33, the hallway location is not explicitly represented. This simpli cation may be reasonable if the hallway is only a conduit for moving from one room to another; in this case the function of the hallway is accounted for in the dynamics governing states in the kernel. Figures 33 and 34 are idealized in that, given the full set of features in our running example, the kernel would contain many more states. 2 One technique for computing the optimal policy for the entire MDP involves repeatedly solving the MDPs corresponding to the individual blocks. The techniques works as follows: initially, we guess the value of every state in the kernel.51 Given a current estimate for the values of the kernel states, we solve the component MDPs; this solution produces a new estimate for the states in the kernel. We adjust the values of the states in the kernel by considering the di erence between the current and the new estimates and iterate until this di erence is negligible. This iterative method for solving a decomposed MDP is a special case of the Lagrangian method for nding the extrema of a function. The OR literature is replete with such methods for both linear and nonlinear systems of equations (Winston, 1992). It is possible to formulate an MDP as a linear program (D'Epenoux, 1963; Puterman, 1994). Dantzig and Wolfe (1960) developed a method of decomposing a system of equations involving a very large number of variables into a set of smaller systems of equations interacting through a set of coupling variables (variables shared by more two or more blocks). In the Dantzig-Wolfe decomposition method, the original, very large system of equations is solved by iteratively solving the smaller systems and adjusting the coupling variables on each iteration until no further adjustment is required. In the linear programming formulation of an MDP, the values of the states are encoded as variables. Kushner and Chen (1974) exploit the fact that MDPs can be modeled as linear programs by using the Dantzig-Wolfe decomposition method to solve MDPs involving a large number of states. Dean and Lin (1995) describe a general framework for solving decomposed MDPs pointing to the work of Kushner and Chen as a special case, but neither work addresses the issue of where the decompositions come from. Dean et al. (1995) investigate methods for decomposing the state space into two blocks: those reachable in k steps or fewer and those not reachable in k steps (see the discussion of reachability above).", "startOffset": 10, "endOffset": 2922}, {"referenceID": 130, "context": "Related to this form of decomposition is the development of macro operators for MDPs (Sutton, 1995).", "startOffset": 85, "endOffset": 99}, {"referenceID": 78, "context": "Macros have a long history in classical planning and problem solving (Fikes, Hart, & Nilsson, 1972; Korf, 1985), but only recently have they been generalized to MDPs (Hauskrecht, Meuleau, Kaelbling, Dean, & Boutilier, 1998; Parr, 1998; Parr & Russell, 1998; Precup, Sutton, & Singh, 1998; Stone & Veloso, 1999; Sutton, 1995; Thrun & Schwartz, 1995).", "startOffset": 69, "endOffset": 111}, {"referenceID": 100, "context": "Macros have a long history in classical planning and problem solving (Fikes, Hart, & Nilsson, 1972; Korf, 1985), but only recently have they been generalized to MDPs (Hauskrecht, Meuleau, Kaelbling, Dean, & Boutilier, 1998; Parr, 1998; Parr & Russell, 1998; Precup, Sutton, & Singh, 1998; Stone & Veloso, 1999; Sutton, 1995; Thrun & Schwartz, 1995).", "startOffset": 166, "endOffset": 348}, {"referenceID": 130, "context": "Macros have a long history in classical planning and problem solving (Fikes, Hart, & Nilsson, 1972; Korf, 1985), but only recently have they been generalized to MDPs (Hauskrecht, Meuleau, Kaelbling, Dean, & Boutilier, 1998; Parr, 1998; Parr & Russell, 1998; Precup, Sutton, & Singh, 1998; Stone & Veloso, 1999; Sutton, 1995; Thrun & Schwartz, 1995).", "startOffset": 166, "endOffset": 348}, {"referenceID": 130, "context": "In (Sutton, 1995; Precup et al., 1998), macros are treated as temporally-abstract actions and models are de ned by which a macro can be treated as if it were a single action and used in policy or value iteration (along with concrete actions).", "startOffset": 3, "endOffset": 38}, {"referenceID": 112, "context": "In (Sutton, 1995; Precup et al., 1998), macros are treated as temporally-abstract actions and models are de ned by which a macro can be treated as if it were a single action and used in policy or value iteration (along with concrete actions).", "startOffset": 3, "endOffset": 38}, {"referenceID": 100, "context": "In (Hauskrecht et al., 1998; Parr, 1998; Parr & Russell, 1998), these models are exploited in a hierarchical fashion, with a high-level MDP consisting only of states lying on the boundaries of blocks, and macros the only \\actions\" that can be chosen at these states.", "startOffset": 3, "endOffset": 62}, {"referenceID": 100, "context": "The issue of macro generation| constructing a set of macros guaranteed to provide the exibility to select close to optimal global behavior|is addressed in (Hauskrecht et al., 1998; Parr, 1998).", "startOffset": 155, "endOffset": 192}, {"referenceID": 4, "context": "(1995) discuss methods for solving MDPs in time-critical problems by trading o quality against time. We have ignored the issue of how to obtain decompositions that expedite our calculations. Ideally, each component of the decomposition would yield to simpli cation via aggregation and abstraction, reducing the dimensionality in each component and thereby avoiding explicit enumeration of all the states. Lin (1997) presents methods for exploiting structure for certain special cases in which the communicating structure is revealed by a domain expert.", "startOffset": 23, "endOffset": 416}, {"referenceID": 4, "context": "MDP1 MDP2 MDP3 a a a Figure 35: Parallel problem decomposition. a a ects the state of each subprocess. Intuitively, an action is suitable for execution in the original MDP at some state if it is reasonably good in each of the sub-MDPs. Generally, the sub-MDPs form either a product or join decomposition of the original state space (contrast this with the union decompositions of state space determined by serial decompositions): the state space is formed by taking the cross product of the sub-MDP state spaces, or the join if certain states in the subprocesses cannot be linked. The subprocesses may have identical action spaces (as in Figure 35), or each may have its own action space, with the global action choice being factored into a choice for each subprocess. In the latter case, the sub-MDPs may be completely independent, in which case the (global) MDP can be solved exponentially faster. A more challenging problem arises when there are constraints on the legal action combinations. For example, if the actions in the subprocesses each require certain shared resources, interactions in the global choice may arise. In a parallel MDP decomposition, we wish to solve the sub-MDPs and use the policies or value functions generated to help construct an optimal or approximately optimal solution to the original MDP, highlighting the need to nd appropriate decompositions for MDPs and to develop suitable merging techniques. Recent parallel decomposition methods have all involved decomposing an MDP into subprocesses suitable for distinct objectives. Since reward functions often deal with multiple objectives, each associated with an independent reward, and whose rewards can be summed to determine a global reward, this is often a very natural way to decompose MDPs. Thus, ideas from multiattribute utility theory can be seen to play a role in the solution of MDPs. Boutilier et al. (1997) decompose an MDP speci ed using 2TBNs and an additive reward function using the abstraction technique described in Section 5.", "startOffset": 21, "endOffset": 1900}, {"referenceID": 15, "context": "It seems likely that such approaches could take further advantage of automatic MDP decomposition algorithms such as that of (Boutilier et al., 1997), where factored representations explicitly play a part.", "startOffset": 124, "endOffset": 148}, {"referenceID": 4, "context": "Note that each sub-MDP has identical sets of actions. If the action space is large, the branching factor of the search process may be prohibitive. Singh and Cohn (1998) also deal with parallel decomposition, though they assume the global MDP is speci ed explicitly as a set of parallel MDPs, thus generating decompositions of a global MDP is not at issue.", "startOffset": 43, "endOffset": 169}, {"referenceID": 4, "context": "Note that each sub-MDP has identical sets of actions. If the action space is large, the branching factor of the search process may be prohibitive. Singh and Cohn (1998) also deal with parallel decomposition, though they assume the global MDP is speci ed explicitly as a set of parallel MDPs, thus generating decompositions of a global MDP is not at issue. The global MDP is given by the cross product of the state and action spaces of these sub-MDPs and the reward functions are summed. However, constraints on the feasible action combinations couple the solutions of these sub-MDPs. To solve the global MDP, the sum of the sub-MDP value functions is used as an upper bound on the optimal global value function, while the maximum of these (at any global state) is used as a lower bound. These bounds then form the basis of an action-elimination procedure in a value-iteration algorithm for solving the global MDP.53 Unfortunately, value iteration is run over the explicit state space of the global MDP. Since the action space is also a cross product, this is a potential computational bottleneck for value iteration, as well. Meuleau et al. (1998) use parallel decomposition to approximate the solution of stochastic resource allocation problems with very large state and action spaces.", "startOffset": 43, "endOffset": 1148}, {"referenceID": 4, "context": "Note that each sub-MDP has identical sets of actions. If the action space is large, the branching factor of the search process may be prohibitive. Singh and Cohn (1998) also deal with parallel decomposition, though they assume the global MDP is speci ed explicitly as a set of parallel MDPs, thus generating decompositions of a global MDP is not at issue. The global MDP is given by the cross product of the state and action spaces of these sub-MDPs and the reward functions are summed. However, constraints on the feasible action combinations couple the solutions of these sub-MDPs. To solve the global MDP, the sum of the sub-MDP value functions is used as an upper bound on the optimal global value function, while the maximum of these (at any global state) is used as a lower bound. These bounds then form the basis of an action-elimination procedure in a value-iteration algorithm for solving the global MDP.53 Unfortunately, value iteration is run over the explicit state space of the global MDP. Since the action space is also a cross product, this is a potential computational bottleneck for value iteration, as well. Meuleau et al. (1998) use parallel decomposition to approximate the solution of stochastic resource allocation problems with very large state and action spaces. Much like Singh and Cohn (1998), an MDP is speci ed in terms of a number of independent MDPs, each involving a distinct objective, whose action choices are linked through shared resource constraints.", "startOffset": 43, "endOffset": 1319}, {"referenceID": 4, "context": "Note that each sub-MDP has identical sets of actions. If the action space is large, the branching factor of the search process may be prohibitive. Singh and Cohn (1998) also deal with parallel decomposition, though they assume the global MDP is speci ed explicitly as a set of parallel MDPs, thus generating decompositions of a global MDP is not at issue. The global MDP is given by the cross product of the state and action spaces of these sub-MDPs and the reward functions are summed. However, constraints on the feasible action combinations couple the solutions of these sub-MDPs. To solve the global MDP, the sum of the sub-MDP value functions is used as an upper bound on the optimal global value function, while the maximum of these (at any global state) is used as a lower bound. These bounds then form the basis of an action-elimination procedure in a value-iteration algorithm for solving the global MDP.53 Unfortunately, value iteration is run over the explicit state space of the global MDP. Since the action space is also a cross product, this is a potential computational bottleneck for value iteration, as well. Meuleau et al. (1998) use parallel decomposition to approximate the solution of stochastic resource allocation problems with very large state and action spaces. Much like Singh and Cohn (1998), an MDP is speci ed in terms of a number of independent MDPs, each involving a distinct objective, whose action choices are linked through shared resource constraints. The value functions for the individual MDPs are constructed o ine and then used in set of online action-selection procedures. Unlike many of the approximation procedures we have discussed, this approach makes no attempt to construct a policy explicitly (and is similar to real-time search or RTDP in this respect) nor to construct the value function explicitly. This method has been applied to very large MDPs, with state spaces of size 21000 and actions spaces that are even larger, and can solve such problems in roughly half an hour. The solutions produced are approximate, but the size of the problem precludes exact solution; so good estimates of solution quality are hard to derive. However, when the same method is applied to smaller problems of the same nature whose exact solution can be computed, the approximations have very high quality (Meuleau et al., 1998). While able to solve very large MDPs (with large, but factored, state and action spaces), the model relies on somewhat restrictive assumptions about the nature of the local value functions that ensure good solution quality. However, the basic approach appears to be generalizable, and o ers great promise for solving very large factored MDPs. The algorithms in both (Singh & Cohn, 1998) and (Meuleau et al., 1998) can be seen to rely at least implicitly on structured MDP representations involving almost independent subprocesses. It seems likely that such approaches could take further advantage of automatic MDP decomposition algorithms such as that of (Boutilier et al., 1997), where factored representations explicitly play a part. 5.4 Summary We have seen a number of ways in which intensional representations can be exploited to solve MDPs e ectively without enumeration of the state space. These include techniques for abstraction of MDPs, including those based on relevance analysis, goal regression and decision-theoretic regression; techniques relying on reachability analysis and serial decomposition; and methods for parallel MDP decomposition exploiting the multiattribute nature 53. Singh and Cohn (1998) also incorporate methods for removing unreachable states during value iteration.", "startOffset": 43, "endOffset": 3578}, {"referenceID": 43, "context": "2 can be interpreted as a form of variable elimination (Dechter, 1996; Zhang & Poole, 1996).", "startOffset": 55, "endOffset": 91}, {"referenceID": 44, "context": "Approximation schemes based on variable elimination (Dechter, 1997; Poole, 1998) may also be related to certain of the approximation methods developed for MDPs.", "startOffset": 52, "endOffset": 80}, {"referenceID": 111, "context": "Approximation schemes based on variable elimination (Dechter, 1997; Poole, 1998) may also be related to certain of the approximation methods developed for MDPs.", "startOffset": 52, "endOffset": 80}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions of reward functions. Many of these methods can, in fortunate circumstances, o er exponential reduction is solution time and space required to represent a policy and value function; but none come with guarantees of such reductions except in certain special cases. While most of the methods described provide approximate solutions (often with error bounds provided), some of them o er optimality guarantees in general, and most can provide optimal solutions under suitable assumptions. One avenue that has not been explored in detail is the relationship between the structured solution methods developed for MDPs described above and techniques used for solving Bayesian networks. Since many of the algorithms discussed in this section rely on the structure inherent in the 2TBN representation of the MDP, it is natural to ask whether they embody some of the intuitions that underlie solution algorithms for Bayes nets, and thus whether the solution techniques for Bayes nets can be (directly or indirectly) applied to MDPs in ways that give rise to algorithms similar to those discussed here. This remains an open question at this point, but undoubtedly some strong ties exist. Tatman and Shachter (1990) have explored the connections between in uence diagrams and MDPs.", "startOffset": 53, "endOffset": 1255}, {"referenceID": 4, "context": "Decision-Theoretic Planning: Structural Assumptions of reward functions. Many of these methods can, in fortunate circumstances, o er exponential reduction is solution time and space required to represent a policy and value function; but none come with guarantees of such reductions except in certain special cases. While most of the methods described provide approximate solutions (often with error bounds provided), some of them o er optimality guarantees in general, and most can provide optimal solutions under suitable assumptions. One avenue that has not been explored in detail is the relationship between the structured solution methods developed for MDPs described above and techniques used for solving Bayesian networks. Since many of the algorithms discussed in this section rely on the structure inherent in the 2TBN representation of the MDP, it is natural to ask whether they embody some of the intuitions that underlie solution algorithms for Bayes nets, and thus whether the solution techniques for Bayes nets can be (directly or indirectly) applied to MDPs in ways that give rise to algorithms similar to those discussed here. This remains an open question at this point, but undoubtedly some strong ties exist. Tatman and Shachter (1990) have explored the connections between in uence diagrams and MDPs. Kjaerul (1992) has investigated computational considerations involved in applying join tree methods for reasoning tasks such as monitoring and prediction in temporal Bayes nets.", "startOffset": 53, "endOffset": 1336}], "year": 2011, "abstractText": null, "creator": "dvipsk 5.58f Copyright 1986, 1994 Radical Eye Software"}}}