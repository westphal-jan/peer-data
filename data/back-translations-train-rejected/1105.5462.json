{"id": "1105.5462", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2011", "title": "Variational Probabilistic Inference and the QMR-DT Network", "abstract": "We describe a variational approximation method for efficient inference in large-scale probabilistic models. Variational methods are deterministic procedures that provide approximations to marginal and conditional probabilities of interest. They provide alternatives to approximate inference methods based on stochastic sampling or search. We describe a variational approach to the problem of diagnostic inference in the `Quick Medical Reference' (QMR) network. The QMR network is a large-scale probabilistic graphical model built on statistical and expert knowledge. Exact probabilistic inference is infeasible in this model for all but a small set of cases. We evaluate our variational inference algorithm on a large set of diagnostic test cases, comparing the algorithm to a state-of-the-art stochastic sampling method.", "histories": [["v1", "Fri, 27 May 2011 01:53:36 GMT  (166kb)", "http://arxiv.org/abs/1105.5462v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["t s jaakkola", "m i jordan"], "accepted": false, "id": "1105.5462"}, "pdf": {"name": "1105.5462.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Tommi S. Jaakkola"], "emails": ["tommi@ai.mit.edu", "jordan@cs.berkeley.edu"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before the first two countries join the EU."}], "references": [{"title": "Incremental probabilistic inference", "author": ["B. D'Ambrosio"], "venue": "Proceedings of the Ninth", "citeRegEx": "D.Ambrosio,? 1993", "shortCiteRegEx": "D.Ambrosio", "year": 1993}, {"title": "Symbolic probabilistic inference in large BN20 networks", "author": ["B. D'Ambrosio"], "venue": "Pro-", "citeRegEx": "D.Ambrosio,? 1994", "shortCiteRegEx": "D.Ambrosio", "year": 1994}, {"title": "NESTOR: A computer-based medical diagnostic aid that integrates", "author": ["G. Cooper"], "venue": null, "citeRegEx": "Cooper,? \\Q1985\\E", "shortCiteRegEx": "Cooper", "year": 1985}, {"title": "The computational complexity of probabilistic inference using Bayesian", "author": ["G. Cooper"], "venue": null, "citeRegEx": "Cooper,? \\Q1990\\E", "shortCiteRegEx": "Cooper", "year": 1990}, {"title": "Reformulating inference problems through selective", "author": ["P. Dagum", "E. Horvitz"], "venue": null, "citeRegEx": "Dagum and Horvitz,? \\Q1992\\E", "shortCiteRegEx": "Dagum and Horvitz", "year": 1992}, {"title": "A Bayesian analysis of simulation algorithms for inference", "author": ["P. Dagum", "E. Horvitz"], "venue": null, "citeRegEx": "Dagum and Horvitz,? \\Q1993\\E", "shortCiteRegEx": "Dagum and Horvitz", "year": 1993}, {"title": "Approximate probabilistic reasoning in Bayesian belief", "author": ["P. Dagum", "M. Luby"], "venue": null, "citeRegEx": "Dagum and Luby,? \\Q1993\\E", "shortCiteRegEx": "Dagum and Luby", "year": 1993}, {"title": "Mini-buckets: A general scheme of generating approximations in auto", "author": ["R. Dechter"], "venue": null, "citeRegEx": "Dechter,? \\Q1997\\E", "shortCiteRegEx": "Dechter", "year": 1997}, {"title": "Bucket elimination: A unifying framework for probabilistic inference", "author": ["R. Dechter"], "venue": null, "citeRegEx": "Dechter,? \\Q1998\\E", "shortCiteRegEx": "Dechter", "year": 1998}, {"title": "Maximum likelihood from incomplete data", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": null, "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Localized partial evaluation of belief networks", "author": ["D. Draper", "S. Hanks"], "venue": null, "citeRegEx": "Draper and Hanks,? \\Q1994\\E", "shortCiteRegEx": "Draper and Hanks", "year": 1994}, {"title": "Weighting and integrating evidence for stochastic sim", "author": ["R. Fung", "K.C. Chang"], "venue": null, "citeRegEx": "Fung and Chang,? \\Q1990\\E", "shortCiteRegEx": "Fung and Chang", "year": 1990}, {"title": "Sampling-based approaches to calculating marginal Den", "author": ["A. Gelfand", "A. Smith"], "venue": null, "citeRegEx": "Gelfand and Smith,? \\Q1990\\E", "shortCiteRegEx": "Gelfand and Smith", "year": 1990}, {"title": "A tractable inference algorithm for diagnosing multiple diseases", "author": ["D. Heckerman"], "venue": "In", "citeRegEx": "Heckerman,? 1989", "shortCiteRegEx": "Heckerman", "year": 1989}, {"title": "Search-based methods to bound diagnostic probabilities in very large", "author": ["M. Henrion"], "venue": null, "citeRegEx": "Henrion,? \\Q1991\\E", "shortCiteRegEx": "Henrion", "year": 1991}, {"title": "Bounded conditioning: Flexible inference", "author": ["Horvitz", "H.E. Suermondt", "G. Cooper"], "venue": null, "citeRegEx": "Horvitz et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Horvitz et al\\.", "year": 1989}, {"title": "Variational methods for inference and learning in graphical models", "author": ["T. Jaakkola"], "venue": null, "citeRegEx": "Jaakkola,? \\Q1997\\E", "shortCiteRegEx": "Jaakkola", "year": 1997}, {"title": "Recursive algorithms for approximating probabilities", "author": ["T. Jaakkola", "M. Jordan"], "venue": null, "citeRegEx": "Jaakkola and Jordan,? \\Q1996\\E", "shortCiteRegEx": "Jaakkola and Jordan", "year": 1996}, {"title": "Blocking-Gibbs sampling in very large", "author": ["C.S. Jensen", "A. Kong", "U. Kj rul"], "venue": null, "citeRegEx": "Jensen et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Jensen et al\\.", "year": 1995}, {"title": "Introduction to Bayesian networks", "author": ["F. Jensen"], "venue": "New York: Springer.", "citeRegEx": "Jensen,? 1996", "shortCiteRegEx": "Jensen", "year": 1996}, {"title": "Local computations with probabilities on", "author": ["S. Lauritzen", "D. Spiegelhalter"], "venue": null, "citeRegEx": "Lauritzen and Spiegelhalter,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "Introduction to Monte Carlo methods", "author": ["D.J.C. MacKay"], "venue": "M. I. Jordan (Ed.),", "citeRegEx": "MacKay,? 1998", "shortCiteRegEx": "MacKay", "year": 1998}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": "San Mateo, CA: Morgan", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "A probabilistic causal model for diagnostic problem solving", "author": ["Y. Peng", "J. Reggia"], "venue": null, "citeRegEx": "Peng and Reggia,? \\Q1987\\E", "shortCiteRegEx": "Peng and Reggia", "year": 1987}, {"title": "Probabilistic partial evaluation: Exploiting rule structure in probabilistic", "author": ["D. Poole"], "venue": null, "citeRegEx": "Poole,? \\Q1997\\E", "shortCiteRegEx": "Poole", "year": 1997}, {"title": "Convex Analysis", "author": ["R. Rockafellar"], "venue": "Princeton University Press.", "citeRegEx": "Rockafellar,? 1972", "shortCiteRegEx": "Rockafellar", "year": 1972}, {"title": "Simulation approaches to general probabilistic inference", "author": ["R.D. Shachter", "M. Peot"], "venue": null, "citeRegEx": "Shachter and Peot,? \\Q1990\\E", "shortCiteRegEx": "Shachter and Peot", "year": 1990}, {"title": "Valuation-based systems for Bayesian decision analysis", "author": ["P.P. Shenoy"], "venue": "Operations", "citeRegEx": "Shenoy,? 1992", "shortCiteRegEx": "Shenoy", "year": 1992}, {"title": "An empirical analysis of likelihood { weighting simulation", "author": ["M. Shwe", "G. Cooper"], "venue": null, "citeRegEx": "Shwe and Cooper,? \\Q1991\\E", "shortCiteRegEx": "Shwe and Cooper", "year": 1991}], "referenceMentions": [{"referenceID": 22, "context": "Beyond the signi cant representational advantages of probability theory, including guarantees of consistency and a naturalness at combining diverse sources of knowledge (Pearl, 1988), the discovery of general exact inference algorithms has been principally responsible for the rapid growth in probabilistic AI (see, e.", "startOffset": 169, "endOffset": 182}, {"referenceID": 22, "context": "Beyond the signi cant representational advantages of probability theory, including guarantees of consistency and a naturalness at combining diverse sources of knowledge (Pearl, 1988), the discovery of general exact inference algorithms has been principally responsible for the rapid growth in probabilistic AI (see, e.g., Lauritzen & Spiegelhalter, 1988; Pearl, 1988; Shenoy, 1992).", "startOffset": 310, "endOffset": 381}, {"referenceID": 27, "context": "Beyond the signi cant representational advantages of probability theory, including guarantees of consistency and a naturalness at combining diverse sources of knowledge (Pearl, 1988), the discovery of general exact inference algorithms has been principally responsible for the rapid growth in probabilistic AI (see, e.g., Lauritzen & Spiegelhalter, 1988; Pearl, 1988; Shenoy, 1992).", "startOffset": 310, "endOffset": 381}, {"referenceID": 3, "context": "From this point of view, it is perhaps not surprising that exact inference is NP-hard (Cooper, 1990).", "startOffset": 86, "endOffset": 100}, {"referenceID": 2, "context": "From this point of view, it is perhaps not surprising that exact inference is NP-hard (Cooper, 1990). In this paper we discuss the inference problem for a particular large-scale graphical model, the Quick Medical Reference (QMR) model.1 The QMR model consists of a combination of statistical and expert knowledge for approximately 600 signi cant diseases and approximately 4000 ndings. In the probabilistic formulation of the model (the QMR-DT), the diseases and the ndings are arranged in a bi-partite graph, and the diagnosis problem is to infer a probability distribution for the diseases given a subset of ndings. Given that each nding is generally relevant to a wide variety of diseases, the graph underlying the QMR-DT is dense, re ecting high-order stochastic dependencies. The computational complexity of treating these dependencies exactly can be characterized in terms of the size of the maximal clique of the \\moralized\" graph (see, e.g., Dechter, 1998; Lauritzen & Spiegelhalter, 1988). In particular, the running time is exponential in this measure of size. For the QMR-DT, considering the standardized \\clinocopathologic conference\" (CPC) cases that we discuss below, we nd that the median size of the maximal clique of the moralized graph is 151.5 nodes. This rules out the use of general exact algorithms for the QMR-DT. The general algorithms do not take advantage of the particular parametric form of the probability distributions at the nodes of the graph, and it is conceivable that additional factorizations might be found that take advantage of the particular choice made by the QMR-DT. Such a factorization was in fact found by Heckerman (1989); his \\Quickscore algorithm\" provides an exact inference algorithm that is tailored to the QMR-DT.", "startOffset": 87, "endOffset": 1668}, {"referenceID": 2, "context": "From this point of view, it is perhaps not surprising that exact inference is NP-hard (Cooper, 1990). In this paper we discuss the inference problem for a particular large-scale graphical model, the Quick Medical Reference (QMR) model.1 The QMR model consists of a combination of statistical and expert knowledge for approximately 600 signi cant diseases and approximately 4000 ndings. In the probabilistic formulation of the model (the QMR-DT), the diseases and the ndings are arranged in a bi-partite graph, and the diagnosis problem is to infer a probability distribution for the diseases given a subset of ndings. Given that each nding is generally relevant to a wide variety of diseases, the graph underlying the QMR-DT is dense, re ecting high-order stochastic dependencies. The computational complexity of treating these dependencies exactly can be characterized in terms of the size of the maximal clique of the \\moralized\" graph (see, e.g., Dechter, 1998; Lauritzen & Spiegelhalter, 1988). In particular, the running time is exponential in this measure of size. For the QMR-DT, considering the standardized \\clinocopathologic conference\" (CPC) cases that we discuss below, we nd that the median size of the maximal clique of the moralized graph is 151.5 nodes. This rules out the use of general exact algorithms for the QMR-DT. The general algorithms do not take advantage of the particular parametric form of the probability distributions at the nodes of the graph, and it is conceivable that additional factorizations might be found that take advantage of the particular choice made by the QMR-DT. Such a factorization was in fact found by Heckerman (1989); his \\Quickscore algorithm\" provides an exact inference algorithm that is tailored to the QMR-DT. Unfortunately, however, the run time of the algorithm is still exponential in the number of positive ndings. For the CPC cases, we estimate that the algorithm would require an average of 50 years to solve the inference problem on current computers. Faced with the apparent infeasibility of exact inference for large-scale models such as the QMR-DT, many researchers have investigated approximation methods. One general approach to developing approximate algorithms is to perform exact inference, but to do so partially. One can consider partial sets of node instantiations, partial sets of hypotheses, and partial sets of nodes. This point of view has led to the development of algorithms for approximate inference based on heuristic search. Another approach to developing approximation algorithms is to exploit averaging phenomena in dense graphs. In particular, laws of large numbers tell us that sums of random variables can behave simply, converging to predictable numerical results. Thus, there may be no need to perform sums explicitly, either exactly or partially. This point of view leads to the variational approach to approximate inference. Finally, yet another approach to approximate inference is based on stochastic sampling. One can sample from simpli ed distributions and in so doing obtain information about a more complex distribution of interest. We discuss each of these methods in turn. Horvitz, Suermondt and Cooper (1991) have developed a partial evaluation algorithm known as \\bounded conditioning\" that works by considering partial sets of node instan1.", "startOffset": 87, "endOffset": 3210}, {"referenceID": 2, "context": "From this point of view, it is perhaps not surprising that exact inference is NP-hard (Cooper, 1990). In this paper we discuss the inference problem for a particular large-scale graphical model, the Quick Medical Reference (QMR) model.1 The QMR model consists of a combination of statistical and expert knowledge for approximately 600 signi cant diseases and approximately 4000 ndings. In the probabilistic formulation of the model (the QMR-DT), the diseases and the ndings are arranged in a bi-partite graph, and the diagnosis problem is to infer a probability distribution for the diseases given a subset of ndings. Given that each nding is generally relevant to a wide variety of diseases, the graph underlying the QMR-DT is dense, re ecting high-order stochastic dependencies. The computational complexity of treating these dependencies exactly can be characterized in terms of the size of the maximal clique of the \\moralized\" graph (see, e.g., Dechter, 1998; Lauritzen & Spiegelhalter, 1988). In particular, the running time is exponential in this measure of size. For the QMR-DT, considering the standardized \\clinocopathologic conference\" (CPC) cases that we discuss below, we nd that the median size of the maximal clique of the moralized graph is 151.5 nodes. This rules out the use of general exact algorithms for the QMR-DT. The general algorithms do not take advantage of the particular parametric form of the probability distributions at the nodes of the graph, and it is conceivable that additional factorizations might be found that take advantage of the particular choice made by the QMR-DT. Such a factorization was in fact found by Heckerman (1989); his \\Quickscore algorithm\" provides an exact inference algorithm that is tailored to the QMR-DT. Unfortunately, however, the run time of the algorithm is still exponential in the number of positive ndings. For the CPC cases, we estimate that the algorithm would require an average of 50 years to solve the inference problem on current computers. Faced with the apparent infeasibility of exact inference for large-scale models such as the QMR-DT, many researchers have investigated approximation methods. One general approach to developing approximate algorithms is to perform exact inference, but to do so partially. One can consider partial sets of node instantiations, partial sets of hypotheses, and partial sets of nodes. This point of view has led to the development of algorithms for approximate inference based on heuristic search. Another approach to developing approximation algorithms is to exploit averaging phenomena in dense graphs. In particular, laws of large numbers tell us that sums of random variables can behave simply, converging to predictable numerical results. Thus, there may be no need to perform sums explicitly, either exactly or partially. This point of view leads to the variational approach to approximate inference. Finally, yet another approach to approximate inference is based on stochastic sampling. One can sample from simpli ed distributions and in so doing obtain information about a more complex distribution of interest. We discuss each of these methods in turn. Horvitz, Suermondt and Cooper (1991) have developed a partial evaluation algorithm known as \\bounded conditioning\" that works by considering partial sets of node instan1. The acronym \\QMR-DT\" that we use in this paper refers to the \\decision-theoretic\" reformulation of the QMR by Shwe, et al. (1991). Shwe, et al.", "startOffset": 87, "endOffset": 3474}, {"referenceID": 22, "context": "E cient exact algorithms exist for singly-connected graphs (Pearl, 1988).", "startOffset": 59, "endOffset": 72}, {"referenceID": 2, "context": "Another approach to approximate inference is provided by \\search-based\" methods, which consider node instantiations across the entire graph (Cooper, 1985; Henrion, 1991; Peng & Reggia, 1987).", "startOffset": 140, "endOffset": 190}, {"referenceID": 14, "context": "Another approach to approximate inference is provided by \\search-based\" methods, which consider node instantiations across the entire graph (Cooper, 1985; Henrion, 1991; Peng & Reggia, 1987).", "startOffset": 140, "endOffset": 190}, {"referenceID": 0, "context": "Another approach to approximate inference is provided by \\search-based\" methods, which consider node instantiations across the entire graph (Cooper, 1985; Henrion, 1991; Peng & Reggia, 1987). The general hope in these methods is that a relatively small fraction of the (exponentially many) node instantiations contains a majority of the probability mass, and that by exploring the high probability instantiations (and bounding the unexplored probability mass) one can obtain reasonable bounds on posterior probabilities. The QMRDT search space is huge, containing approximately 2600 disease hypotheses. If, however, one only considers cases with a small number of diseases, and if the hypotheses involving a small number of diseases contain most of the high probability posteriors, then it may be possible to search a signi cant fraction of the relevant portions of the hypothesis space. Henrion (1991) was in fact able to run a search-based algorithm on the QMR-DT inference problem, for a set of cases characterized by a small number of diseases.", "startOffset": 141, "endOffset": 903}, {"referenceID": 0, "context": "Another approach to approximate inference is provided by \\search-based\" methods, which consider node instantiations across the entire graph (Cooper, 1985; Henrion, 1991; Peng & Reggia, 1987). The general hope in these methods is that a relatively small fraction of the (exponentially many) node instantiations contains a majority of the probability mass, and that by exploring the high probability instantiations (and bounding the unexplored probability mass) one can obtain reasonable bounds on posterior probabilities. The QMRDT search space is huge, containing approximately 2600 disease hypotheses. If, however, one only considers cases with a small number of diseases, and if the hypotheses involving a small number of diseases contain most of the high probability posteriors, then it may be possible to search a signi cant fraction of the relevant portions of the hypothesis space. Henrion (1991) was in fact able to run a search-based algorithm on the QMR-DT inference problem, for a set of cases characterized by a small number of diseases. These were cases, however, for which the exact Quickscore algorithm is e cient. The more general corpus of CPC cases that we discuss in the current paper is not characterized by a small number of diseases per case. In general, even if we impose the assumption that patients have a limited number N of diseases, we cannot assume a priori that the model will show a sharp cuto in posterior probability after disease N . Finally, in high-dimensional search problems it is often necessary to allow paths that are not limited to the target hypothesis subspace; in particular, one would like to be able to arrive at a hypothesis containing few diseases by pruning hypotheses containing additional diseases (Peng & Reggia, 1987). Imposing such a limitation can lead to failure of the search. More recent partial evaluation methods include the \\localized partial evaluation\" method of Draper and Hanks (1994), the \\incremental SPI\" algorithm of D'Ambrosio (1993), the \\probabilistic partial evaluation\" method of Poole (1997), and the \\mini-buckets\" algorithm of Dechter (1997).", "startOffset": 141, "endOffset": 1950}, {"referenceID": 0, "context": "More recent partial evaluation methods include the \\localized partial evaluation\" method of Draper and Hanks (1994), the \\incremental SPI\" algorithm of D'Ambrosio (1993), the \\probabilistic partial evaluation\" method of Poole (1997), and the \\mini-buckets\" algorithm of Dechter (1997).", "startOffset": 152, "endOffset": 170}, {"referenceID": 0, "context": "More recent partial evaluation methods include the \\localized partial evaluation\" method of Draper and Hanks (1994), the \\incremental SPI\" algorithm of D'Ambrosio (1993), the \\probabilistic partial evaluation\" method of Poole (1997), and the \\mini-buckets\" algorithm of Dechter (1997).", "startOffset": 152, "endOffset": 233}, {"referenceID": 0, "context": "More recent partial evaluation methods include the \\localized partial evaluation\" method of Draper and Hanks (1994), the \\incremental SPI\" algorithm of D'Ambrosio (1993), the \\probabilistic partial evaluation\" method of Poole (1997), and the \\mini-buckets\" algorithm of Dechter (1997). The former algorithm considers partial sets of nodes, and the latter three consider partial evaluations of the sums that emerge during an exact inference run.", "startOffset": 152, "endOffset": 285}, {"referenceID": 0, "context": "More recent partial evaluation methods include the \\localized partial evaluation\" method of Draper and Hanks (1994), the \\incremental SPI\" algorithm of D'Ambrosio (1993), the \\probabilistic partial evaluation\" method of Poole (1997), and the \\mini-buckets\" algorithm of Dechter (1997). The former algorithm considers partial sets of nodes, and the latter three consider partial evaluations of the sums that emerge during an exact inference run. These are all promising methods, but like the other partial evaluation methods it is yet not clear if they restrict the exponential growth in complexity in ways that yield realistic accuracy/time tradeo s in large-scale models such as the QMR-DT.2 Variational methods provide an alternative approach to approximate inference. They are similar in spirit to partial evaluation methods (in particular the incremental SPI and mini-buckets algorithms), in that they aim to avoid performing sums over exponentially 2. D'Ambrosio (1994) reports \\mixed\" results using incremental SPI on the QMR-DT, for a somewhat more di cult set of cases than Heckerman (1989) and Henrion (1991), but still with a restricted number of positive ndings.", "startOffset": 152, "endOffset": 975}, {"referenceID": 0, "context": "More recent partial evaluation methods include the \\localized partial evaluation\" method of Draper and Hanks (1994), the \\incremental SPI\" algorithm of D'Ambrosio (1993), the \\probabilistic partial evaluation\" method of Poole (1997), and the \\mini-buckets\" algorithm of Dechter (1997). The former algorithm considers partial sets of nodes, and the latter three consider partial evaluations of the sums that emerge during an exact inference run. These are all promising methods, but like the other partial evaluation methods it is yet not clear if they restrict the exponential growth in complexity in ways that yield realistic accuracy/time tradeo s in large-scale models such as the QMR-DT.2 Variational methods provide an alternative approach to approximate inference. They are similar in spirit to partial evaluation methods (in particular the incremental SPI and mini-buckets algorithms), in that they aim to avoid performing sums over exponentially 2. D'Ambrosio (1994) reports \\mixed\" results using incremental SPI on the QMR-DT, for a somewhat more di cult set of cases than Heckerman (1989) and Henrion (1991), but still with a restricted number of positive ndings.", "startOffset": 152, "endOffset": 1099}, {"referenceID": 0, "context": "More recent partial evaluation methods include the \\localized partial evaluation\" method of Draper and Hanks (1994), the \\incremental SPI\" algorithm of D'Ambrosio (1993), the \\probabilistic partial evaluation\" method of Poole (1997), and the \\mini-buckets\" algorithm of Dechter (1997). The former algorithm considers partial sets of nodes, and the latter three consider partial evaluations of the sums that emerge during an exact inference run. These are all promising methods, but like the other partial evaluation methods it is yet not clear if they restrict the exponential growth in complexity in ways that yield realistic accuracy/time tradeo s in large-scale models such as the QMR-DT.2 Variational methods provide an alternative approach to approximate inference. They are similar in spirit to partial evaluation methods (in particular the incremental SPI and mini-buckets algorithms), in that they aim to avoid performing sums over exponentially 2. D'Ambrosio (1994) reports \\mixed\" results using incremental SPI on the QMR-DT, for a somewhat more di cult set of cases than Heckerman (1989) and Henrion (1991), but still with a restricted number of positive ndings.", "startOffset": 152, "endOffset": 1118}, {"referenceID": 21, "context": "Stochastic sampling is a large family, including techniques such as rejection sampling, importance sampling, and Markov chain Monte Carlo methods (MacKay, 1998).", "startOffset": 146, "endOffset": 160}, {"referenceID": 2, "context": "In particular, Shwe and Cooper (1991) proposed a stochastic sampling method known as \\likelihood-weighted sampling\" for the QMR-DT model.", "startOffset": 24, "endOffset": 38}, {"referenceID": 2, "context": "In particular, Shwe and Cooper (1991) proposed a stochastic sampling method known as \\likelihood-weighted sampling\" for the QMR-DT model. Their results are the most promising results to date for inference for the QMR-DT|they were able to produce reasonably accurate approximations in reasonable time for two of the di cult CPC cases. We consider the Shwe and Cooper algorithm later in this paper; in particular we compare the algorithm empirically to our variational algorithm across the entire corpus of CPC cases. Although it is important to compare approximation methods, it should be emphasized at the outset that we do not think that the goal should be to identify a single champion approximate inference technique. Rather, di erent methods exploit di erent structural features of large-scale probability models, and we expect that optimal solutions will involve a combination of methods. We return to this point in the discussion section, where we consider various promising hybrids of approximate and exact inference algorithms. The general problem of approximate inference is NP-hard (Dagum & Luby, 1993) and this provides additional reason to doubt the existence of a single champion approximate inference technique. We think it important to stress, however, that this hardness result, together with Cooper's (1990) hardness result for exact inference cited above, should not be taken to suggest that exact inference and approximate inference are \\equally hard.", "startOffset": 24, "endOffset": 1325}, {"referenceID": 22, "context": "The conditional probabilities P (fijd) are represented by the \\noisy-OR model\" (Pearl, 1988): P (fi = 0jd) = P (fi = 0jL) Y j2 i P (fi = 0jdj) (2) = (1 qi0) Y j2 i(1 qij)dj (3) e i0 Pj2 i ijdj ; (4) where i is the set of diseases that are parents of the nding fi in the QMR graph, qij = P (fi = 1jdj = 1) is the probability that the disease j, if present, could alone cause the nding to have a positive outcome, and qi0 = P (fi = 1jL) is the \\leak\" probability, i.", "startOffset": 79, "endOffset": 92}, {"referenceID": 13, "context": "In the worst case the exact calculation of posterior probabilities is exponentially costly in the number of positive ndings (Heckerman, 1989; D'Ambrosio, 1994).", "startOffset": 124, "endOffset": 159}, {"referenceID": 1, "context": "In the worst case the exact calculation of posterior probabilities is exponentially costly in the number of positive ndings (Heckerman, 1989; D'Ambrosio, 1994).", "startOffset": 124, "endOffset": 159}, {"referenceID": 1, "context": "While algorithms exist that attempt to nd and exploit factorizations in this expression, based on the particular pattern of observed evidence (cf. Heckerman, 1989; D'Ambrosio, 1994), these algorithms are limited to roughly 20 positive ndings on current computers.", "startOffset": 142, "endOffset": 181}, {"referenceID": 13, "context": "We used Heckerman's \\Quickscore\" algorithm (Heckerman 1989)|an algorithm tailored to the QMR-DT architecture|to perform these exact calculations.", "startOffset": 43, "endOffset": 59}, {"referenceID": 2, "context": "We also present comparisons with the likelihood-weighted sampler of Shwe and Cooper (1991). In the second section we present results for the remaining, intractable CPC cases.", "startOffset": 77, "endOffset": 91}, {"referenceID": 2, "context": "A current state-of-the-art algorithm for the QMR-DT is the enhanced version of likelihoodweighted sampling proposed by Shwe and Cooper (1991). Likelihood-weighted sampling is a stochastic sampling method proposed by Fung and Chang (1990) and Shachter and Peot (1990).", "startOffset": 128, "endOffset": 142}, {"referenceID": 2, "context": "A current state-of-the-art algorithm for the QMR-DT is the enhanced version of likelihoodweighted sampling proposed by Shwe and Cooper (1991). Likelihood-weighted sampling is a stochastic sampling method proposed by Fung and Chang (1990) and Shachter and Peot (1990).", "startOffset": 128, "endOffset": 238}, {"referenceID": 2, "context": "A current state-of-the-art algorithm for the QMR-DT is the enhanced version of likelihoodweighted sampling proposed by Shwe and Cooper (1991). Likelihood-weighted sampling is a stochastic sampling method proposed by Fung and Chang (1990) and Shachter and Peot (1990). Likelihood-weighted sampling is basically a simple forward sampling method that weights samples by their likelihoods.", "startOffset": 128, "endOffset": 267}, {"referenceID": 2, "context": "A current state-of-the-art algorithm for the QMR-DT is the enhanced version of likelihoodweighted sampling proposed by Shwe and Cooper (1991). Likelihood-weighted sampling is a stochastic sampling method proposed by Fung and Chang (1990) and Shachter and Peot (1990). Likelihood-weighted sampling is basically a simple forward sampling method that weights samples by their likelihoods. It can be enhanced and improved by utilizing \\selfimportance sampling\" (see Shachter & Peot, 1990), a version of importance sampling in which the importance sampling distribution is continually updated to re ect the current estimated posterior distribution. Middleton et al. (1990) utilized likelihood-weighted sampling with self-importance sampling (as well as a heuristic initialization scheme known as \\iterative tabular Bayes\") for the QMR-DT model and found that it did not work satisfactorily.", "startOffset": 128, "endOffset": 668}, {"referenceID": 2, "context": "A current state-of-the-art algorithm for the QMR-DT is the enhanced version of likelihoodweighted sampling proposed by Shwe and Cooper (1991). Likelihood-weighted sampling is a stochastic sampling method proposed by Fung and Chang (1990) and Shachter and Peot (1990). Likelihood-weighted sampling is basically a simple forward sampling method that weights samples by their likelihoods. It can be enhanced and improved by utilizing \\selfimportance sampling\" (see Shachter & Peot, 1990), a version of importance sampling in which the importance sampling distribution is continually updated to re ect the current estimated posterior distribution. Middleton et al. (1990) utilized likelihood-weighted sampling with self-importance sampling (as well as a heuristic initialization scheme known as \\iterative tabular Bayes\") for the QMR-DT model and found that it did not work satisfactorily. Subsequent work by Shwe and Cooper (1991), however, used an additional enhancement to the algorithm known as `Markov blanket scoring\" (see Shachter & Peot, 1990), which distributes fractions of samples to the positive and negative values of a node in proportion to the probability of these values conditioned on the Markov blanket of the node.", "startOffset": 128, "endOffset": 928}, {"referenceID": 22, "context": "We also investigated Gibbs sampling (Pearl, 1988).", "startOffset": 36, "endOffset": 49}, {"referenceID": 13, "context": "In particular, recall that we use the Quickscore algorithm (Heckerman, 1989) to handle the exact calculations within the framework of our variational algorithm.", "startOffset": 59, "endOffset": 76}, {"referenceID": 16, "context": "Finally, for diseases in which the bounds are loose there are also perturbation methods available (Jaakkola, 1997) that can help to validate the approximations for these diseases.", "startOffset": 98, "endOffset": 114}, {"referenceID": 16, "context": "It is also the case that the type of transformations that we have exploited in the QMR-DT setting extend to a larger class of dependence relations based on generalized linear models (Jaakkola, 1997).", "startOffset": 182, "endOffset": 198}, {"referenceID": 2, "context": "In particular, search-based methods (Cooper, 1985; Peng & Reggia, 1987, Henrion, 1991) and variational methods both yield bounds on probabilities, and, as we have indicated in the introduction, they seem to exploit di erent aspects of the structure of complex probability distributions.", "startOffset": 36, "endOffset": 86}, {"referenceID": 2, "context": "We also compared the variational algorithm to a state-of-the-art algorithm for the QMRDT, the likelihood-weighted sampler of Shwe and Cooper (1991). We found that the variational algorithm outperformed the likelihood-weighted sampler both for the tractable cases and for the full corpus.", "startOffset": 134, "endOffset": 148}, {"referenceID": 2, "context": "We also compared the variational algorithm to a state-of-the-art algorithm for the QMRDT, the likelihood-weighted sampler of Shwe and Cooper (1991). We found that the variational algorithm outperformed the likelihood-weighted sampler both for the tractable cases and for the full corpus. In particular, for a xed accuracy requirement the variational algorithm was signi cantly faster (cf. Figure 5), and for a xed time allotment the variational algorithm was signi cantly more accurate (cf. Figure 8 and Figure 11). Our results were less satisfactory for the interval bounds on the posterior marginals. Across the full CPC corpus we found that for approximately one third of the disease the bounds were tight but for half of the diseases the bounds were vacuous. A major impediment to obtaining tighter bounds appears to lie not in the variational approximation per se but rather in the exact subroutine, and we are investigating exact methods with improved numerical properties. Although we have focused in detail on the QMR-DT model in this paper, it is worth noting that the variational probabilistic inference methodology is considerably more general. Speci cally, the methods that we have described here are not limited to the bi-partite graphical structure of the QMR-DT model, nor is it necessary to employ noisy-OR nodes (Jaakkola & Jordan, 1996). It is also the case that the type of transformations that we have exploited in the QMR-DT setting extend to a larger class of dependence relations based on generalized linear models (Jaakkola, 1997). Finally, for a review of applications of variational methods to a variety of other graphical model architectures, see Jordan, et al. (1998). A promising direction for future research appears to be in the integration of various kinds of approximate and exact methods (see, e.", "startOffset": 134, "endOffset": 1696}, {"referenceID": 13, "context": "We also want to thank David Heckerman for suggesting that we attack QMR-DT with variational methods, and for providing helpful counsel along the way. Appendix A. Duality The upper and lower bounds for individual conditional probability distributions that form the basis of our variational method are based on the \\dual\" or \\conjugate\" representations of convex functions. We present a brief description of convex duality in this appendix, and refer the reader to Rockafellar (1970) for a more extensive treatment.", "startOffset": 28, "endOffset": 482}], "year": 2011, "abstractText": null, "creator": "dvipsk 5.58f Copyright 1986, 1994 Radical Eye Software"}}}