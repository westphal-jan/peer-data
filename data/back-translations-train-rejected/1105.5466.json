{"id": "1105.5466", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2011", "title": "Issues in Stacked Generalization", "abstract": "Stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been considered to be a `black art' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We find that best results are obtained when the higher-level model combines the confidence (and not just the predictions) of the lower-level ones. We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging.", "histories": [["v1", "Fri, 27 May 2011 01:54:47 GMT  (104kb)", "http://arxiv.org/abs/1105.5466v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["k m ting", "i h witten"], "accepted": false, "id": "1105.5466"}, "pdf": {"name": "1105.5466.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Kai Ming Ting", "Ian H. Witten"], "emails": ["kmting@deakin.edu.au", "ihw@cs.waikato.ac.nz"], "sections": [{"heading": null, "text": "This year, it will be able to fix and fix the mentioned bugs."}], "references": [{"title": "Instance-Based Learning Algorithms", "author": ["D.W. Aha", "M.K.D. Kibler"], "venue": null, "citeRegEx": "Aha and Kibler,? \\Q1991\\E", "shortCiteRegEx": "Aha and Kibler", "year": 1991}, {"title": "UCI Repository of machine learning databases", "author": ["C. Blake", "E. Keogh", "C.J. Merz"], "venue": null, "citeRegEx": "Blake et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Blake et al\\.", "year": 1998}, {"title": "Stacked Regressions", "author": ["L. Breiman"], "venue": "Machine Learning, Vol. 24, pp. 49-64.", "citeRegEx": "Breiman,? 1996a", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Bagging Predictors", "author": ["L. Breiman"], "venue": "Machine Learning, Vol. 24, No. 2, pp. 123-140.", "citeRegEx": "Breiman,? 1996b", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Bias, Variance, and Arcing Classi ers", "author": ["L. Breiman"], "venue": "Technical Report 460. De-", "citeRegEx": "Breiman,? 1996c", "shortCiteRegEx": "Breiman", "year": 1996}, {"title": "Estimating Probabilities: A Crucial Task in Machine Learning", "author": ["B. Cestnik"], "venue": "In", "citeRegEx": "Cestnik,? 1990", "shortCiteRegEx": "Cestnik", "year": 1990}, {"title": "A Comparative Evaluation of Voting and Meta-learning", "author": ["P.K. Chan", "S.J"], "venue": "Stolfo", "citeRegEx": "Chan and S.J.,? \\Q1995\\E", "shortCiteRegEx": "Chan and S.J.", "year": 1995}, {"title": "A Weighted Nearest Neighbor Algorithm for Learning", "author": ["S Cost", "S. Salzberg"], "venue": null, "citeRegEx": "Cost and Salzberg,? \\Q1993\\E", "shortCiteRegEx": "Cost and Salzberg", "year": 1993}, {"title": "A Comparative Evaluation of Combiner", "author": ["D.W. Fan", "S.J.P.K. Chan"], "venue": "Stolfo", "citeRegEx": "Fan and Chan,? \\Q1996\\E", "shortCiteRegEx": "Fan and Chan", "year": 1996}, {"title": "Neural Network Ensembles", "author": ["L.K. Hansen", "P. Salamon"], "venue": "IEEE Transactions", "citeRegEx": "Hansen and Salamon,? \\Q1990\\E", "shortCiteRegEx": "Hansen and Salamon", "year": 1990}, {"title": "Decision Combination in Multiple Classi er", "author": ["T.K. Ho", "S.N.J.J. Hull"], "venue": null, "citeRegEx": "Ho and Hull,? \\Q1994\\E", "shortCiteRegEx": "Ho and Hull", "year": 1994}, {"title": "Methods of Combining Experts' Probability Assessments", "author": ["R.A. Jacobs"], "venue": "Neural", "citeRegEx": "Jacobs,? 1995", "shortCiteRegEx": "Jacobs", "year": 1995}, {"title": "Hierachical Mixtures of Experts and the EM", "author": ["R.A. Jacobs"], "venue": "M.I. Jordan", "citeRegEx": "Jacobs,? \\Q1994\\E", "shortCiteRegEx": "Jacobs", "year": 1994}, {"title": "Error Estimation by Series Association for Neural Network", "author": ["K. Kim", "E.B"], "venue": null, "citeRegEx": "Kim and E.B.,? \\Q1995\\E", "shortCiteRegEx": "Kim and E.B.", "year": 1995}, {"title": "Neural Network Ensembles, Cross Validation, and Active", "author": ["A. Krogh"], "venue": "J. Vedelsby", "citeRegEx": "Krogh,? \\Q1995\\E", "shortCiteRegEx": "Krogh", "year": 1995}, {"title": "Multiple Decision Trees", "author": ["S. Kwok", "C. Carter"], "venue": "Uncertainty in Arti cial Intel-", "citeRegEx": "Kwok and Carter,? \\Q1990\\E", "shortCiteRegEx": "Kwok and Carter", "year": 1990}, {"title": "Combining Estimates in Regression and Classi", "author": ["M. LeBlanc", "R. Tibshirani"], "venue": null, "citeRegEx": "LeBlanc and Tibshirani,? \\Q1993\\E", "shortCiteRegEx": "LeBlanc and Tibshirani", "year": 1993}, {"title": "On Voting Ensembles of Classi ers (extended abstract)", "author": ["O. Matan"], "venue": "Proceedings", "citeRegEx": "Matan,? 1996", "shortCiteRegEx": "Matan", "year": 1996}, {"title": "Generalized Linear Models", "author": ["P. McCullagh", "J.A"], "venue": null, "citeRegEx": "McCullagh and J.A.,? \\Q1983\\E", "shortCiteRegEx": "McCullagh and J.A.", "year": 1983}, {"title": "Dynamic Learning Bias Selection", "author": ["C.J. Merz"], "venue": "Proceedings of the Fifth In-", "citeRegEx": "Merz,? 1995", "shortCiteRegEx": "Merz", "year": 1995}, {"title": "On Pruning and Averaging Decision Trees", "author": ["J.J. Oliver", "D.J"], "venue": "Hand", "citeRegEx": "Oliver and D.J.,? \\Q1995\\E", "shortCiteRegEx": "Oliver and D.J.", "year": 1995}, {"title": "When Networks Disagree: Ensemble Methods", "author": ["M.P. Perrone"], "venue": "L.N. Cooper", "citeRegEx": "Perrone,? \\Q1993\\E", "shortCiteRegEx": "Perrone", "year": 1993}, {"title": "Program for machine learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "Quinlan,? \\Q1993\\E", "shortCiteRegEx": "Quinlan", "year": 1993}, {"title": "The Strength of Weak Learnability", "author": ["R.E. Schapire"], "venue": "Machine Learning, 5, pp.", "citeRegEx": "Schapire,? 1990", "shortCiteRegEx": "Schapire", "year": 1990}, {"title": "Boosting the margin: A new", "author": ["R.E. Schapire", "Y. Freund", "P. Bartlett", "W.S"], "venue": null, "citeRegEx": "Schapire et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schapire et al\\.", "year": 1997}, {"title": "Stacked Density Estimation", "author": ["P. Smyth", "D. Wolpert"], "venue": "Advances in Neural Infor-", "citeRegEx": "Smyth and Wolpert,? \\Q1997\\E", "shortCiteRegEx": "Smyth and Wolpert", "year": 1997}, {"title": "The Characterisation of Predictive Accuracy and Decision Combina", "author": ["K.M. Ting"], "venue": null, "citeRegEx": "Ting,? \\Q1996\\E", "shortCiteRegEx": "Ting", "year": 1996}, {"title": "Stacking Bagged and Dagged Models", "author": ["K.M. Ting"], "venue": "I.H. Witten", "citeRegEx": "Ting,? \\Q1997\\E", "shortCiteRegEx": "Ting", "year": 1997}, {"title": "Computer Systems That Learns", "author": ["Weiss S. M", "C.A. Kulikowski"], "venue": null, "citeRegEx": "M. and Kulikowski,? \\Q1991\\E", "shortCiteRegEx": "M. and Kulikowski", "year": 1991}, {"title": "Stacked Generalization", "author": ["D.H. Wolpert"], "venue": "Neural Networks, Vol. 5, pp. 241-259,", "citeRegEx": "Wolpert,? 1992", "shortCiteRegEx": "Wolpert", "year": 1992}, {"title": "Hybrid System for Protein Secondary", "author": ["X. Zhang", "D.L.J.P. Mesirov"], "venue": "Waltz", "citeRegEx": "Zhang and Mesirov,? \\Q1992\\E", "shortCiteRegEx": "Zhang and Mesirov", "year": 1992}], "referenceMentions": [{"referenceID": 29, "context": "Introduction Stacked generalization is a way of combining multiple models that have been learned for a classi cation task (Wolpert, 1992), which has also been used for regression (Breiman, 1996a) and even unsupervised learning (Smyth & Wolpert, 1997).", "startOffset": 122, "endOffset": 137}, {"referenceID": 2, "context": "Introduction Stacked generalization is a way of combining multiple models that have been learned for a classi cation task (Wolpert, 1992), which has also been used for regression (Breiman, 1996a) and even unsupervised learning (Smyth & Wolpert, 1997).", "startOffset": 179, "endOffset": 195}, {"referenceID": 2, "context": "Introduction Stacked generalization is a way of combining multiple models that have been learned for a classi cation task (Wolpert, 1992), which has also been used for regression (Breiman, 1996a) and even unsupervised learning (Smyth & Wolpert, 1997). Typically, di erent learning algorithms learn di erent models for the task at hand, and in the most common form of stacking the rst step is to collect the output of each model into a new set of data. For each instance in the original training set, this data set represents every model's prediction of that instance's class, along with its true classi cation. During this step, care is taken to ensure that the models are formed from a batch of training data that does not include the instance in question, in just the same way as ordinary cross-validation. The new data are treated as the data for another learning problem, and in the second step a learning algorithm is employed to solve this problem. In Wolpert's terminology, the original data and the models constructed for them in the rst step are referred to as level-0 data and level-0 models, respectively, while the set of cross-validated data and the second-stage learning algorithm are referred to as level-1 data and the level-1 generalizer. In this paper, we show how to make stacked generalization work for classi cation tasks by addressing two crucial issues which Wolpert (1992) originally described as `black art' and have not been resolved since.", "startOffset": 180, "endOffset": 1397}, {"referenceID": 2, "context": "Introduction Stacked generalization is a way of combining multiple models that have been learned for a classi cation task (Wolpert, 1992), which has also been used for regression (Breiman, 1996a) and even unsupervised learning (Smyth & Wolpert, 1997). Typically, di erent learning algorithms learn di erent models for the task at hand, and in the most common form of stacking the rst step is to collect the output of each model into a new set of data. For each instance in the original training set, this data set represents every model's prediction of that instance's class, along with its true classi cation. During this step, care is taken to ensure that the models are formed from a batch of training data that does not include the instance in question, in just the same way as ordinary cross-validation. The new data are treated as the data for another learning problem, and in the second step a learning algorithm is employed to solve this problem. In Wolpert's terminology, the original data and the models constructed for them in the rst step are referred to as level-0 data and level-0 models, respectively, while the set of cross-validated data and the second-stage learning algorithm are referred to as level-1 data and the level-1 generalizer. In this paper, we show how to make stacked generalization work for classi cation tasks by addressing two crucial issues which Wolpert (1992) originally described as `black art' and have not been resolved since. The two issues are (i) the type of attributes that should be used to form level-1 data, and (ii) the type of level-1 generalizer in order to get improved accuracy using the stacked generalization method. Breiman (1996a) demonstrated the success of stacked generalization in the setting of ordinary regression.", "startOffset": 180, "endOffset": 1687}, {"referenceID": 2, "context": "But instead of selecting the single model that works best as judged by (for example) cross-validation, Breiman used the di erent level0 regressors' output values for each member of the training set to form level-1 data. Then he used least-squares linear regression, under the constraint that all regression coe cients be non-negative, as the level-1 generalizer. The non-negativity constraint turned out to be crucial to guarantee that the predictive accuracy would be better than that achieved by selecting the single best predictor. Here we show how stacked generalization can be made to work reliably in classi cation tasks. We do this by using the output class probabilities generated by level-0 models to form level-1 data. Then for the level-1 generalizer we use a version of least squares linear regression adapted for classi cation tasks. We nd the use of class probabilities to be crucial for the successful application of stacked generalization in classi cation tasks. However, the non-negativity constraints found necessary by Breiman in regression are found to be irrelevant to improved predictive accuracy in our classi cation situation. In Section 2, we formally introduce the technique of stacked generalization and describe pertinent details of each learning algorithm used in our experiments. Section 3 describes the results of stacking three di erent types of learning algorithms. Section 4 compares stacked generalization with arcing and bagging, two recent methods that employ sampling techniques to modify the data distribution in order to produce multiple models from a single learning algorithm. The following section describes related work, and the paper ends with a summary of our conclusions. 2. Stacked Generalization Given a data set L = f(yn; xn); n = 1; : : : ; Ng, where yn is the class value and xn is a vector representing the attribute values of the nth instance, randomly split the data into J almost equal parts L1; : : : ;LJ . De ne Lj and L( j) = L Lj to be the test and training sets for the jth fold of a J -fold cross-validation. Given K learning algorithms, which we call level-0 generalizers, invoke the kth algorithm on the data in the training set L( j) to induce a model M( j) k , for k = 1; : : : ;K. These are called level-0 models. For each instance xn in Lj, the test set for the jth cross-validation fold, let zkn denote the prediction of the model M( j) k on xn. At the end of the entire cross-validation process, the data set assembled from the outputs of the K models is LCV = f(yn; z1n; : : : ; zKn); n = 1; : : : ; Ng: These are the level-1 data. Use some learning algorithm that we call the level-1 generalizer to derive from these data a model ~ M for y as a function of (z1; : : : ; zK). This is the level-1 model. Figure 1 illustrates the cross-validation process. To complete the training process, the nal level-0 models Mk, k = 1; : : : ;K, are derived using all the data in L. Now let us consider the classi cation process, which uses the modelsMk, k = 1; : : : ;K, in conjunction with ~ M. Given a new instance, models Mk produce a vector (z1; : : : ; zK). This vector is input to the level-1 model ~ M, whose output is the nal classi cation result for that instance. This completes the stacked generalization method as proposed by Wolpert (1992), and also used by Breiman (1996a) and LeBlanc & Tibshirani (1993).", "startOffset": 103, "endOffset": 3311}, {"referenceID": 2, "context": "But instead of selecting the single model that works best as judged by (for example) cross-validation, Breiman used the di erent level0 regressors' output values for each member of the training set to form level-1 data. Then he used least-squares linear regression, under the constraint that all regression coe cients be non-negative, as the level-1 generalizer. The non-negativity constraint turned out to be crucial to guarantee that the predictive accuracy would be better than that achieved by selecting the single best predictor. Here we show how stacked generalization can be made to work reliably in classi cation tasks. We do this by using the output class probabilities generated by level-0 models to form level-1 data. Then for the level-1 generalizer we use a version of least squares linear regression adapted for classi cation tasks. We nd the use of class probabilities to be crucial for the successful application of stacked generalization in classi cation tasks. However, the non-negativity constraints found necessary by Breiman in regression are found to be irrelevant to improved predictive accuracy in our classi cation situation. In Section 2, we formally introduce the technique of stacked generalization and describe pertinent details of each learning algorithm used in our experiments. Section 3 describes the results of stacking three di erent types of learning algorithms. Section 4 compares stacked generalization with arcing and bagging, two recent methods that employ sampling techniques to modify the data distribution in order to produce multiple models from a single learning algorithm. The following section describes related work, and the paper ends with a summary of our conclusions. 2. Stacked Generalization Given a data set L = f(yn; xn); n = 1; : : : ; Ng, where yn is the class value and xn is a vector representing the attribute values of the nth instance, randomly split the data into J almost equal parts L1; : : : ;LJ . De ne Lj and L( j) = L Lj to be the test and training sets for the jth fold of a J -fold cross-validation. Given K learning algorithms, which we call level-0 generalizers, invoke the kth algorithm on the data in the training set L( j) to induce a model M( j) k , for k = 1; : : : ;K. These are called level-0 models. For each instance xn in Lj, the test set for the jth cross-validation fold, let zkn denote the prediction of the model M( j) k on xn. At the end of the entire cross-validation process, the data set assembled from the outputs of the K models is LCV = f(yn; z1n; : : : ; zKn); n = 1; : : : ; Ng: These are the level-1 data. Use some learning algorithm that we call the level-1 generalizer to derive from these data a model ~ M for y as a function of (z1; : : : ; zK). This is the level-1 model. Figure 1 illustrates the cross-validation process. To complete the training process, the nal level-0 models Mk, k = 1; : : : ;K, are derived using all the data in L. Now let us consider the classi cation process, which uses the modelsMk, k = 1; : : : ;K, in conjunction with ~ M. Given a new instance, models Mk produce a vector (z1; : : : ; zK). This vector is input to the level-1 model ~ M, whose output is the nal classi cation result for that instance. This completes the stacked generalization method as proposed by Wolpert (1992), and also used by Breiman (1996a) and LeBlanc & Tibshirani (1993).", "startOffset": 103, "endOffset": 3345}, {"referenceID": 2, "context": "But instead of selecting the single model that works best as judged by (for example) cross-validation, Breiman used the di erent level0 regressors' output values for each member of the training set to form level-1 data. Then he used least-squares linear regression, under the constraint that all regression coe cients be non-negative, as the level-1 generalizer. The non-negativity constraint turned out to be crucial to guarantee that the predictive accuracy would be better than that achieved by selecting the single best predictor. Here we show how stacked generalization can be made to work reliably in classi cation tasks. We do this by using the output class probabilities generated by level-0 models to form level-1 data. Then for the level-1 generalizer we use a version of least squares linear regression adapted for classi cation tasks. We nd the use of class probabilities to be crucial for the successful application of stacked generalization in classi cation tasks. However, the non-negativity constraints found necessary by Breiman in regression are found to be irrelevant to improved predictive accuracy in our classi cation situation. In Section 2, we formally introduce the technique of stacked generalization and describe pertinent details of each learning algorithm used in our experiments. Section 3 describes the results of stacking three di erent types of learning algorithms. Section 4 compares stacked generalization with arcing and bagging, two recent methods that employ sampling techniques to modify the data distribution in order to produce multiple models from a single learning algorithm. The following section describes related work, and the paper ends with a summary of our conclusions. 2. Stacked Generalization Given a data set L = f(yn; xn); n = 1; : : : ; Ng, where yn is the class value and xn is a vector representing the attribute values of the nth instance, randomly split the data into J almost equal parts L1; : : : ;LJ . De ne Lj and L( j) = L Lj to be the test and training sets for the jth fold of a J -fold cross-validation. Given K learning algorithms, which we call level-0 generalizers, invoke the kth algorithm on the data in the training set L( j) to induce a model M( j) k , for k = 1; : : : ;K. These are called level-0 models. For each instance xn in Lj, the test set for the jth cross-validation fold, let zkn denote the prediction of the model M( j) k on xn. At the end of the entire cross-validation process, the data set assembled from the outputs of the K models is LCV = f(yn; z1n; : : : ; zKn); n = 1; : : : ; Ng: These are the level-1 data. Use some learning algorithm that we call the level-1 generalizer to derive from these data a model ~ M for y as a function of (z1; : : : ; zK). This is the level-1 model. Figure 1 illustrates the cross-validation process. To complete the training process, the nal level-0 models Mk, k = 1; : : : ;K, are derived using all the data in L. Now let us consider the classi cation process, which uses the modelsMk, k = 1; : : : ;K, in conjunction with ~ M. Given a new instance, models Mk produce a vector (z1; : : : ; zK). This vector is input to the level-1 model ~ M, whose output is the nal classi cation result for that instance. This completes the stacked generalization method as proposed by Wolpert (1992), and also used by Breiman (1996a) and LeBlanc & Tibshirani (1993). 272", "startOffset": 103, "endOffset": 3377}, {"referenceID": 22, "context": "5, a decision tree learning algorithm (Quinlan, 1993); NB, a re-implementation of a Naive Bayesian classi er (Cestnik, 1990); and IB1, a variant of a lazy learning algorithm (Aha, Kibler & Albert, 1991) which employs the p-nearest-neighbor method using a modi ed value-di erence metric for nominal and binary attributes (Cost & Salzberg, 1993).", "startOffset": 38, "endOffset": 53}, {"referenceID": 5, "context": "5, a decision tree learning algorithm (Quinlan, 1993); NB, a re-implementation of a Naive Bayesian classi er (Cestnik, 1990); and IB1, a variant of a lazy learning algorithm (Aha, Kibler & Albert, 1991) which employs the p-nearest-neighbor method using a modi ed value-di erence metric for nominal and binary attributes (Cost & Salzberg, 1993).", "startOffset": 109, "endOffset": 124}, {"referenceID": 2, "context": "MLR is an adaptation of a least-squares linear regression algorithm that Breiman (1996a) used in regression settings.", "startOffset": 73, "endOffset": 89}, {"referenceID": 2, "context": "MLR is an adaptation of a least-squares linear regression algorithm that Breiman (1996a) used in regression settings. Any classi cation problem with real-valued attributes can be transformed into a multi-response regression problem. If the original classi cation problem has I classes, it is converted into I separate regression problems, where the problem for class ` has instances with responses equal to one when they have class ` and zero otherwise. The input to MLR is level-1 data, and we need to consider the situation for the model ~ M0, where the attributes are probabilities, separately from that for the model ~ M, where 1. A large p value is used following Wolpert's (1992) advice that \\: : : it is reasonable that `relatively global, smooth : : : ' level-1 generalizers should perform well.", "startOffset": 73, "endOffset": 686}, {"referenceID": 2, "context": "Choose the linear regression coe cients f k`g to minimize Xj X (yn;xn)2Lj(yn Xk k`P ( j) k` (xn))2: The coe cients f k`g are constrained to be non-negative, following Breiman's (1996a) discovery that this is necessary for the successful application of stacked generalization to regression problems.", "startOffset": 167, "endOffset": 185}, {"referenceID": 2, "context": "Choose the linear regression coe cients f k`g to minimize Xj X (yn;xn)2Lj(yn Xk k`P ( j) k` (xn))2: The coe cients f k`g are constrained to be non-negative, following Breiman's (1996a) discovery that this is necessary for the successful application of stacked generalization to regression problems. The non-negative-coe cient least-squares algorithm described by Lawson & Hanson (1995) is employed here to derive the linear regression for each class.", "startOffset": 167, "endOffset": 386}, {"referenceID": 2, "context": "2 Are Non-negativity Constraints Necessary? Both Breiman (1996a) and LeBlanc & Tibshirani (1993) use the stacked generalization method in a regression setting and report that it is necessary to constrain the regression coe cients to be non-negative in order to guarantee that stacked regression improves predictive accuracy.", "startOffset": 49, "endOffset": 65}, {"referenceID": 2, "context": "2 Are Non-negativity Constraints Necessary? Both Breiman (1996a) and LeBlanc & Tibshirani (1993) use the stacked generalization method in a regression setting and report that it is necessary to constrain the regression coe cients to be non-negative in order to guarantee that stacked regression improves predictive accuracy.", "startOffset": 49, "endOffset": 97}, {"referenceID": 17, "context": "Although small values of #SE are a necessary condition for majority vote to rival BestCV, they are not a su cient condition|see Matan (1996) for an example.", "startOffset": 128, "endOffset": 141}, {"referenceID": 2, "context": "It is worth mentioning a method that averages Pi(x) for each i over all level-0 models, yielding Pi(x), and then predicts class \u00ce for which P\u00ce(x) > Pi(x) for all i 6= \u00ce : According to Breiman (1996b), this method produces an error rate almost identical to that of majority vote.", "startOffset": 184, "endOffset": 200}, {"referenceID": 11, "context": "Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule.", "startOffset": 197, "endOffset": 211}, {"referenceID": 20, "context": "Ting & Witten (1997) show one possible way of incorporating bagged models with level-1 learning, employing MLR instead of voting.", "startOffset": 0, "endOffset": 21}, {"referenceID": 20, "context": "Ting & Witten (1997) show one possible way of incorporating bagged models with level-1 learning, employing MLR instead of voting. In this implementation, L is used as a test set for each of the bagged models to derive level-1 data rather than the cross-validated data. This is viable because each bootstrap sample leaves out about 37% of the examples. Ting & Witten (1997) show that bag-stacking almost always has higher predictive accuracy than bagging models derived from either C4.", "startOffset": 0, "endOffset": 373}, {"referenceID": 2, "context": "Note that the only di erence here is whether an adaptive level-1 model or a simple majority vote is employed According to Breiman (1996b; 1996c), arcing and bagging can only improve the predictive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithm is one for which small perturbations in the training set can produce large changes in the derived model. Decision trees and neural networks are unstable; NB and IB1 are stable. Stacking works with both. While MLR is the most successful candidate for level-1 learning that we have found, other algorithms might work equally well. One candidate is neural networks. However, we have experimented with back-propagation neural networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the nettalk dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule. 5. Related Work Our analysis of stacked generalization was motivated by that of Breiman (1996a), discussed earlier, and LeBlanc & Tibshirani (1993).", "startOffset": 122, "endOffset": 1382}, {"referenceID": 2, "context": "Note that the only di erence here is whether an adaptive level-1 model or a simple majority vote is employed According to Breiman (1996b; 1996c), arcing and bagging can only improve the predictive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithm is one for which small perturbations in the training set can produce large changes in the derived model. Decision trees and neural networks are unstable; NB and IB1 are stable. Stacking works with both. While MLR is the most successful candidate for level-1 learning that we have found, other algorithms might work equally well. One candidate is neural networks. However, we have experimented with back-propagation neural networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the nettalk dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule. 5. Related Work Our analysis of stacked generalization was motivated by that of Breiman (1996a), discussed earlier, and LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine the stacking of a linear discriminant and a nearest neighbor classi er and show that, for one arti cial dataset, a method similar to MLR performs better with non-negativity constraints than without.", "startOffset": 122, "endOffset": 1434}, {"referenceID": 2, "context": "Note that the only di erence here is whether an adaptive level-1 model or a simple majority vote is employed According to Breiman (1996b; 1996c), arcing and bagging can only improve the predictive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithm is one for which small perturbations in the training set can produce large changes in the derived model. Decision trees and neural networks are unstable; NB and IB1 are stable. Stacking works with both. While MLR is the most successful candidate for level-1 learning that we have found, other algorithms might work equally well. One candidate is neural networks. However, we have experimented with back-propagation neural networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the nettalk dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule. 5. Related Work Our analysis of stacked generalization was motivated by that of Breiman (1996a), discussed earlier, and LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine the stacking of a linear discriminant and a nearest neighbor classi er and show that, for one arti cial dataset, a method similar to MLR performs better with non-negativity constraints than without.", "startOffset": 122, "endOffset": 1463}, {"referenceID": 2, "context": "Note that the only di erence here is whether an adaptive level-1 model or a simple majority vote is employed According to Breiman (1996b; 1996c), arcing and bagging can only improve the predictive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithm is one for which small perturbations in the training set can produce large changes in the derived model. Decision trees and neural networks are unstable; NB and IB1 are stable. Stacking works with both. While MLR is the most successful candidate for level-1 learning that we have found, other algorithms might work equally well. One candidate is neural networks. However, we have experimented with back-propagation neural networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the nettalk dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule. 5. Related Work Our analysis of stacked generalization was motivated by that of Breiman (1996a), discussed earlier, and LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine the stacking of a linear discriminant and a nearest neighbor classi er and show that, for one arti cial dataset, a method similar to MLR performs better with non-negativity constraints than without. Our results in Section 3.2 show that these constraints are irrelevant to MLR's predictive accuracy in the classi cation situation. LeBlanc & Tibshirani (1993) and Ting & Witten (1997) use a version of MLR that employs all class probabilities from each level-0 model to induce each linear regression.", "startOffset": 122, "endOffset": 1829}, {"referenceID": 2, "context": "Note that the only di erence here is whether an adaptive level-1 model or a simple majority vote is employed According to Breiman (1996b; 1996c), arcing and bagging can only improve the predictive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithm is one for which small perturbations in the training set can produce large changes in the derived model. Decision trees and neural networks are unstable; NB and IB1 are stable. Stacking works with both. While MLR is the most successful candidate for level-1 learning that we have found, other algorithms might work equally well. One candidate is neural networks. However, we have experimented with back-propagation neural networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the nettalk dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule. 5. Related Work Our analysis of stacked generalization was motivated by that of Breiman (1996a), discussed earlier, and LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine the stacking of a linear discriminant and a nearest neighbor classi er and show that, for one arti cial dataset, a method similar to MLR performs better with non-negativity constraints than without. Our results in Section 3.2 show that these constraints are irrelevant to MLR's predictive accuracy in the classi cation situation. LeBlanc & Tibshirani (1993) and Ting & Witten (1997) use a version of MLR that employs all class probabilities from each level-0 model to induce each linear regression.", "startOffset": 122, "endOffset": 1854}, {"referenceID": 2, "context": "Note that the only di erence here is whether an adaptive level-1 model or a simple majority vote is employed According to Breiman (1996b; 1996c), arcing and bagging can only improve the predictive accuracy of learning algorithms that are `unstable.'5 An unstable learning algorithm is one for which small perturbations in the training set can produce large changes in the derived model. Decision trees and neural networks are unstable; NB and IB1 are stable. Stacking works with both. While MLR is the most successful candidate for level-1 learning that we have found, other algorithms might work equally well. One candidate is neural networks. However, we have experimented with back-propagation neural networks for this purpose and found that they have a much slower learning rate than MLR. For example, MLR only took 2.9 seconds as compare to 4790 seconds for the neural network in the nettalk dataset; while both have the same error rate. Other possible candidates are the multinomial logit model (Jordan & Jacobs, 1994), which is a special case of generalized linear models (McCullagh & Nelder, 1983), and the supra Bayesian procedure (Jacobs, 1995) which treats the level-0 models' con dence as data that may be combined with prior distribution of level-0 models via Bayes' rule. 5. Related Work Our analysis of stacked generalization was motivated by that of Breiman (1996a), discussed earlier, and LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine the stacking of a linear discriminant and a nearest neighbor classi er and show that, for one arti cial dataset, a method similar to MLR performs better with non-negativity constraints than without. Our results in Section 3.2 show that these constraints are irrelevant to MLR's predictive accuracy in the classi cation situation. LeBlanc & Tibshirani (1993) and Ting & Witten (1997) use a version of MLR that employs all class probabilities from each level-0 model to induce each linear regression. In this case, the linear regression for class ` is LR`(x) = K Xk I Xi ki`Pki(x): This implementation requires the tting of KI parameters, as compared to K parameters for the version used in this paper (see the corresponding formula in Section 2.2). Both 5. Schapire, R.E., Y. Freund, P. Bartlett, & W.S. Lee (1997) provide an alternative explanation for the e ectiveness of arcing and bagging.", "startOffset": 122, "endOffset": 2285}], "year": 2011, "abstractText": null, "creator": "dvips 5.58 Copyright 1986, 1994 Radical Eye Software"}}}