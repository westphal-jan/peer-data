{"id": "1105.5592", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-May-2011", "title": "Kernel Belief Propagation", "abstract": "We propose a nonparametric generalization of belief propagation, Kernel Belief Propagation (KBP), for pairwise Markov random fields. Messages are represented as functions in a reproducing kernel Hilbert space (RKHS), and message updates are simple linear operations in the RKHS. KBP makes none of the assumptions commonly required in classical BP algorithms: the variables need not arise from a finite domain or a Gaussian distribution, nor must their relations take any particular parametric form. Rather, the relations between variables are represented implicitly, and are learned nonparametrically from training data. KBP has the advantage that it may be used on any domain where kernels are defined (Rd, strings, groups), even where explicit parametric models are not known, or closed form expressions for the BP updates do not exist. The computational cost of message updates in KBP is polynomial in the training data size. We also propose a constant time approximate message update procedure by representing messages using a small number of basis functions. In experiments, we apply KBP to image denoising, depth prediction from still images, and protein configuration prediction: KBP is faster than competing classical and nonparametric approaches (by orders of magnitude, in some cases), while providing significantly more accurate results.", "histories": [["v1", "Fri, 27 May 2011 15:56:11 GMT  (303kb,D)", "http://arxiv.org/abs/1105.5592v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["le song", "arthur gretton", "danny bickson", "yucheng low", "carlos guestrin"], "accepted": false, "id": "1105.5592"}, "pdf": {"name": "1105.5592.pdf", "metadata": {"source": "CRF", "title": "Kernel Belief Propagation", "authors": ["Le Song", "Arthur Gretton", "Danny Bickson", "Yucheng Low", "Carlos Guestrin"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country"}, {"heading": "2 Markov Random Fields And Belief Propagation", "text": "We begin with a brief introduction to paired Markov problems (MRFs) and the idea that this is a way of applying it in practice. (...) Each node is associated with a random variable. (...) Each node is associated with a random variation of Xs. (...) Each node is associated with a random variation of Xs. (...) Each node is associated with a random variation of Xs. (...) Each node is associated with a random variation of Xs. (...) Each node is associated with a random variation of Xs. (...) Each node is associated with a random variation of Xs. (...) Each node is associated with a random variation of Xs. (...) Each node is associated with a random variation of Xs."}, {"heading": "3 Properties of Belief Propagation", "text": "It is a question of whether it is at all possible for people who live in the EU to come to the EU. (...) It is a question of whether they want to stay in the EU or not. (...) It is a question of whether they want to stay in the EU. (...) It is a question of whether they want to stay in the EU. (...) It is a question of whether they want to stay in the EU. (...) It is a question of whether they want to stay in the EU. (...) It is a question of whether they want to stay in the EU. (...) It is a question of whether they want to stay in the EU. (...) It is a question of whether they want to stay in the EU. (...) It is a question of whether they want to stay in the EU. (...) It is a question of whether they want to stay in the EU."}, {"heading": "4 Kernel Belief Propagation", "text": "We are developing a novel kerelization of faith propagation based on Hilbert Space conditional distribution embedding (Song et al., 2009), which generalizes an earlier kernel algorithm to draw exact conclusions about trees (Song et al., 2010). As is to be expected, the kernel implementation of BP updates in (4) is almost identical to the earlier tree algorithm, the main difference being that we are now looking at graphs with loops and iterating to convergence (rather than getting an exact solution in a single pass), which has a significant impact on implementation: The earlier solution of Song et al. is polynomic in sample size, which was not a problem for the smaller trees considered by Song et al., but expensive for the large, loopy graphical models we address in our experiments. We are shifting the problem of the efficient annulation of the efficient implementation we present in the BP implemented section in 2010, where BP implemented a new message strategy is included in the SBP section 5."}, {"heading": "4.1 Message Representations", "text": "We start with a description of the properties of a message mut (xs), since it is located in the reproducing Hilbert space (RKHS) F of functions in the separable metric space X (Aronszajn, 1950; Scholkopf & Smola, 2002). As we will see, the advantage of this assumption is that the update procedure in the RKHS can be expressed as a linear operation and leads to new messages, which are also RKHS functions. RKHS F is defined in the sense of a clearly positively defined core k (xs, x \u2032 s) with the reproduction property < mts (\u00b7), k (xs, \u00b7) > F = mts (xs, \u00b7), where k () indi-cates that a core argument is fixed at xs. Thus, we can consider the evaluation of messages mts at any point xs, xs, as a linear operation in F: we call k (xs, s) the universal functions of the core at xs are fixed."}, {"heading": "4.2 Kernel BP Message Updates", "text": "Next, we define a representation for news updates, assuming that messages are RKHS functions."}, {"heading": "4.3 Learning Kernel Graphical Models", "text": "(x1t),. (x1t),. (x1t),. (x1t),. (x1t),. (x1s),. (x1s),. (x1t),. (x1t),. (x1t),. (x),. (x1s),. (x1s),. (x1s),. (x1t),. (xxs),. (xxxs),. (x),. (x1s),. (x1s),. (x1s),. (x1s),. (x1t),. (x2t),. (xxxs),. (xx.),. (xxs),. (xx.),. (x.),. (x. \"(x.). (x. (x.). (x.). (x. (x.). (x.),. (x. (x.). (x.),. (x. (x.),. (xxx.),. (x. (xxxxs),. (x.),. (x. (xxx.),. (xx.),. (xx.),. (xxxxx.),. (xxx. (.),. (xxxx.),. (x. (.),. (xx.),. (xxxxx. (.),. (. (x.),. (x. (xx.),. (x.),. (xx. (x.),. (x.),. (x. (. (x.),. (x.),. (x. (. (x.),. (x.),. (x. (x. (.),. (x.),. (x.),. (x.),. (x. (.),. (x. (.. (.). (.). (x. (.). (x...). (x..."}, {"heading": "5 Constant Time Message Updates", "text": "In this section we will formulate a more computationally efficient alternative to the full update in (12). Our goal is to reduce the computing cost for each update to O ('2dmax) wo'm. We need a one-time pre-processing step that is linear inm. This efficient method of message delivery makes Kernel BP practicable even for very large graphical models and / or training sets."}, {"heading": "5.1 Approximating Feature Matrices", "text": "The main idea of the pre-processing step is to approximate messages in the RKHS with a few informative basic functions and to estimate these basic functions based on data. This is achieved by approximating the characteristic matrix as a weighted combination of a subset of its columns, which means that the characteristic matrix in which I: = {i1,..., i. \"is chosen by taking the columns of the residence corresponding to the indices in I. \u2212 Likewise, we approach the dimensions of the JWs, assuming simplicity. We can therefore approximate the kernel matrices as low-level factorizations, i.e., K \u2248 W > t KIIIWt and L = > s LJWs, with KII: > J = > Q = equal and Q = equal."}, {"heading": "5.2 Approximating Tensor Features", "text": "The approximation method applied directly to these results only results in a linear temporal approximation algorithm: this can be seen by replacing the kernel matrices in (12) with their low rank approximations. In order to achieve a constant approximate update, our strategy is to go one step further: In addition to approximating the kernel matrices, we are moving closer to the matrix of the tensor product features in Equation (11) by further approximating the proposed approach to the tensor product features in Equation (W't). Crucially, the individual kernel matrix approximations do not take the subsequent tensor product of these messages into account. In contrast, our proposed approach also approaches the tensor product directly. The computational advantage of a direct tensor approximation approach is significant in practice (a comparison between an exact kernel and its constant BP and its time)."}, {"heading": "5.3 Constant Time Approximate Updates", "text": "We now calculate the news updates based on the various low-ranking approximations. The incoming messages and conditional embed operators become \"u\" s mut \"u\" s \u03a6IWt\u03b2ut, \"(13) U\" X \"t\" i \"s.\" (14), where Wts: \"W\" t (W \"t\" s LJWs + \"s\" s \"s.\" \u2212 1W \"s\" s. \"If we fix the messages as\" mut \"i\" s \"\" \"(Wt\u03b2t.\"), we can fix the news updates for mts \"s\" (xs) asmts \"s\" (xs) \"s\" s \"s\" s. \"(15), where the message difference between\" W \"(Wt\u03b2t.\"), \".\" (Wt\u03b2ut. \""}, {"heading": "6 Gaussian Mixture And Particle BP", "text": "We briefly consider two state-of-the-art approaches to non-parametric density propagation: Gaussian Mixture BP (Sudderth et al., 2003) and Particle BP (Ihler & McAllester, 2009). Contrary to our approach, we must provide these algorithms with a conditional density estimate P? (Xt | Xs) in advance to calculate conditional expectation in (4). For Gaussian Mixture BP, this conditional density must take the form of a mixture of Gaussians. We describe how we learn conditional density from the data and then show how the two algorithms use it for inferencing. A direct approach to estimating conditional density P? (Xt | Xs) would be to take the ratio of common empirical density to marginal empirical density."}, {"heading": "7 Experiments", "text": "The first two have subsequently been shaken by a series of inconsistencies, reflected in the way they have occurred in the past: in the way they have occurred in the past, how they have occurred in the past, how they have occurred in the past, how they have occurred in the past, how they have occurred in the past, how they have occurred in the past, how they have occurred in the past, how they have occurred in the past, how they have occurred in the past."}, {"heading": "8 Conclusions and Further Work", "text": "Kernel BP performs learning and conclusions from challenging graphical models with structured and continuous random variables and is more precise and much faster than previous non-parametric BP algorithms. A possible extension of this work would be to kernel the tree-weighted propagation of belief (Wainwright et al., 2003). The convergence of kernel BP is another challenging topic for future work (Ihler et al., 2005).Recognition: We thank Alex Ihler for the Gaussian mix of BP codes and helpful discussions. LS is supported by a Stephenie and Ray Lane Fellowship. This research was also supported by ARO MURI W911NF0710287, ARO MURI W911NF0810242, NSF Mundo Supplement IIS-0803333, NSF Nets-NBD CNS-0721591 and ONR MURI W911NF0710287, a detailed description of this Soll- / Soll- / Soll- / Soll- / Soll-"}, {"heading": "1 Gaussian Mixture and Particle BP", "text": "We describe two competing approaches to a mixture of Gaussians, P (u) b (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (e) e (n) e) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e) e (e) e (e) e (e) e (e) e (e) e (e) e (e) e (e) e (e) e) e (n) e (n) e (n) e) e (n) e (n) e (n) e (n) e (n) e) e (n) e (n) e (n) e (n) e (n) e (n) e) e (n) e (n) e (n) e (n) e (n) e (n) e (n) e) e (n (n) e) e (e) e (n (n) e) e (n) e) e (e) e (e (e) e) e (e (e) e) e (e) e (n) e (e) e (e) e (e) e) e (e (e) e) e) e (n) e (e (e) e (n) e) e (n) e (e) e (e (e) e (e) e) e (e (e (e) e) e) e) e) e (e (e) e (e) e (e) e) e (n) e) e (e (e) e (e) e (e) e (e) e) e (e) e) e (n) e) e (n) e (n) e (n) e (n) e (e (e) e) e"}, {"heading": "2 Settings for Discrete and Particle BP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Depth Reconstruction from 2-D Images", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1.1 Discrete BP", "text": "The log depth was discredited into 30 containers and edge parameters were selected to achieve locally consistent loopy BP margins using the technique described in Wainwright et al. (2003). Empirically, finer discretizations did not improve the resulting accuracy, but the runtime significantly. We used the splash planning by Gonz\u00e1lez et al. (2009) as it offered the lowest runtime of all the dates tested."}, {"heading": "2.1.2 Particle BP", "text": "In theory, results comparable to the Kernel BP method were achievable, but in practice, the extremely high cost of the resampling phase on large models meant that only a small number of particles could be maintained if a reasonable runtime was to be achieved with our evaluation rate of 274 images. Ultimately, we opted for a configuration that allowed us to complete the evaluation in about two machine days on an 8-core Intel Nehalem machine, allowing conclusions to be drawn for each evaluation image to be computed 10 minutes in parallel. For each image, we performed 100 iterations of a simple linear sweep schedule, using 20 particles per message and re-sampling all 10 iterations. Each resampling phase ran MCMC for a maximum of 10 steps per particle. We also implemented acceleration tricks where low-weight particles (< 1E \u2212 7 after normalization) were ignored during the transfer quality during the transmission process without the message being ignored during the transition quality."}, {"heading": "2.2 Synthetic Image Denoising", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 Discrete BP", "text": "To simplify the assessment, we allowed a certain amount of \"oracle information\" by comparing the degrees of discretization during inference with the color planes in the basic image. Here, we evaluated combined gradient / IPF + BP methods to learn the edge parameters. We found that gradient / IPF performed well when there were few colors in the image, but did not converge when the number of colors in the hundreds increased, in part due to the instability of BP and the large number of free parameters in the edge potential. Therefore, edge potentials were re-selected using the technique described in Wainwright et al. (2003), which performed quite well empirically, as can be seen in Figure 1 (c) (main document)."}, {"heading": "2.2.2 Particle BP", "text": "The long runtime of the Particle Belief Propagation made accuracy evaluation more difficult. As before, we adjusted the Particle BP parameters so that in two machine days we could draw conclusions about the evaluation set of 110 images, which allowed about 25 minutes per evaluation frame. We performed 100 iterations of 30 particles per message, sampling all 15 iterations anew. In each repetition phase, MCMC ran for a maximum of 10 steps per particle."}, {"heading": "3 Effects of Approximate Message Updates", "text": "In this section, we will study how different levels of feature approximation errors affect the speed of the core BP and the resulting performance. Our experimental setup was the image described in Section 5.1 of the main document. We note that the calculation cost of our constant error message is O (\"2dmax\"), where we reverse the approximation error. This is a significant runtime improvement over naively applying a low kernel matrix approximation, which only results in a linear time update with computational costs O (\"mdmax\"). In this experiment, we varied the feature approximation over three levels, namely 10 \u2212 10 \u2212 3, and compared to the performance of the constant time."}, {"heading": "4 Predicting Paper Categories", "text": "In this case, the simple approach of learning for each category is very costly, for both one and the other. (The other side of the world) We are dealing with the other side. (The other side of the world) We are dealing with a variety of categories ranging from 1 to 10; there were a total of 367 different categories in our datasets. (The other side of the world) We are dealing with the simplicity of more than 85% of the papers that have less than 10 shortcuts. The maximum number of shortcuts was 450.Paper category prediction is a multiple problem with a large output space. The output is very sparse: the number of non-zeros entries is in this case, the simple one-against-all category predictions for each category can become prohibitive. (The output is very sparse: the label vectors have only a small number of non-zeros.) In this case, the simple one-against-all category predictions for each category can become unaffordable."}, {"heading": "5 Local Marginal Consistency Condition When Learning With BP", "text": "In this section, we show that we do not know certain boundary consistency conditions (29) and (30) below. (As we will see, these result from the fact that we have a Bethe-free energy approximation in relation to our model and the shape of the fixed point equations that define the minimum of Bethe-free energy, not from a number of references (for example, Yedidia et al., 2001, 2005; Wainwright & Jordan, 2008; Koller & Friedman, 2009), Butis in a form specifically tailored to our case, since we are neither in a discrete domain nor with exponential families.The parameters of a paired Markov random field (MRF) can be learned by comparing the log liquidity of the model P with the actual distribution of the model P? Denote the model byP (X): = 1Z (s)."}, {"heading": "6 BP Inference Using Learned Potentials", "text": "The inference problem in pairs of MRFs is the calculation of the marginal or log partition function for the model with learned potentials. The results of this algorithm are a set of beliefs that can be used for obtaining the MAP mapping of the corresponding variables.The BP message update (with the learned potentials) ismts (Xs) = \u03b1 (Xs) Xs, Xt), Xmus (Xt), Xxt (32) and at each iteration the beliefs can be calculated using the current messages."}, {"heading": "7 A Note on Kernelization of Gaussian BP", "text": "In this section we will look at the problem of defining a common Gaussian model (= >) caused by a kernel. (>) We will follow Bickson (2008) in our presentation of the original Gaussian setting. We will show that adopting a Gaussian in an infinite attribute space leads to challenges in interpreting and estimating the model. (44) In the case of the Gaussian, the probability density takes the form P (X), in which the Gaussian function takes the form P (X). (\u2212 12) The Gaussian function takes the form P (X)."}, {"heading": "8 Message Error Incurred by the Additional Feature Approximation", "text": "S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-"}], "references": [{"title": "Theory of reproducing kernels", "author": ["N. Aronszajn"], "venue": "Trans. Amer. Math. Soc., 68, 337\u2013404.", "citeRegEx": "Aronszajn,? 1950", "shortCiteRegEx": "Aronszajn", "year": 1950}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bach and Jordan,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan", "year": 2002}, {"title": "Fault identification via non-parametric belief propagation", "author": ["D. Bickson", "D. Baron", "A. Ihler", "H. Avissar", "D. Dolev"], "venue": "IEEE Transactions on Signal Processing. ISSN 1053-587X", "citeRegEx": "Bickson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bickson et al\\.", "year": 2011}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces", "author": ["K. Fukumizu", "F.R. Bach", "M.I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Fukumizu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2004}, {"title": "Characteristic kernels on groups and semigroups", "author": ["K. Fukumizu", "B. Sriperumbudur", "A. Gretton", "B. Schoelkopf"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Fukumizu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Fukumizu et al\\.", "year": 2009}, {"title": "Particle belief propagation", "author": ["A. Ihler", "D. McAllester"], "venue": "In AISTATS", "citeRegEx": "Ihler and McAllester,? \\Q2009\\E", "shortCiteRegEx": "Ihler and McAllester", "year": 2009}, {"title": "Loopy belief propagation: Convergence and effects of message errors", "author": ["A.T. Ihler", "J.W. Fisher III", "A.S. Willsky"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Ihler et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ihler et al\\.", "year": 2005}, {"title": "Efficient multiscale sampling from products of gaussian mixtures", "author": ["E.T. Ihler", "E.B. Sudderth", "W.T. Freeman", "A.S. Willsky"], "venue": null, "citeRegEx": "Ihler et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ihler et al\\.", "year": 2003}, {"title": "Protein secondary structure prediction based on position-specific scoring matrices", "author": ["D.T. Jones"], "venue": "J. Mol. Biol., 292, 195\u2013202.", "citeRegEx": "Jones,? 1999", "shortCiteRegEx": "Jones", "year": 1999}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "GraphLab: A new parallel framework for machine learning", "author": ["Y. Low", "J. Gonzalez", "A. Kyrola", "D. Bickson", "C. Guestrin", "J.M. Hellerstein"], "venue": "In Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Low et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Low et al\\.", "year": 2010}, {"title": "Turbo decoding as an instance of Pearl\u2019s belief propagation algorithm. J-SAC", "author": ["R. McEliece", "D. MacKay", "J. Cheng"], "venue": null, "citeRegEx": "McEliece et al\\.,? \\Q1998\\E", "shortCiteRegEx": "McEliece et al\\.", "year": 1998}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": null, "citeRegEx": "Murphy et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 1999}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufman.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Make3d: Learning 3d scene structure from a single still image", "author": ["A. Saxena", "M. Sun", "A.Y. Ng"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "Saxena et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Saxena et al\\.", "year": 2009}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": null, "citeRegEx": "Sch\u00f6lkopf and Smola,? \\Q2002\\E", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2002}, {"title": "Kernel Methods in Computational Biology", "author": ["B. Sch\u00f6lkopf", "K. Tsuda", "Vert", "J.-P"], "venue": null, "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2004}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini", "year": 2004}, {"title": "Nonparametric tree graphical models", "author": ["L. Song", "A. Gretton", "C. Guestrin"], "venue": "In 13th Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Song et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Song et al\\.", "year": 2010}, {"title": "Hilbert space embeddings of conditional distributions", "author": ["L. Song", "J. Huang", "A. Smola", "K. Fukumizu"], "venue": "In Proc. Intl. Conf. Machine Learning", "citeRegEx": "Song et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Song et al\\.", "year": 2009}, {"title": "On the influence of the kernel on the consistency of support vector machines", "author": ["I. Steinwart"], "venue": "J. Mach. Learn. Res., 2, 67\u201393.", "citeRegEx": "Steinwart,? 2001", "shortCiteRegEx": "Steinwart", "year": 2001}, {"title": "Nonparametric belief propagation", "author": ["E. Sudderth", "A. Ihler", "W. Freeman", "A. Willsky"], "venue": "In CVPR", "citeRegEx": "Sudderth et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sudderth et al\\.", "year": 2003}, {"title": "Conditional density estimation via least-squares density ratio estimation", "author": ["M. Sugiyama", "I. Takeuchi", "T. Suzuki", "T. Kanamori", "H. Hachiya", "D. Okanohara"], "venue": null, "citeRegEx": "Sugiyama et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2010}, {"title": "Tree-reweighted belief propagation and approximate ML estimation by pseudo-moment matching", "author": ["M. Wainwright", "T. Jaakkola", "A. Willsky"], "venue": "In 9th Workshop on Artificial Intelligence and Statistics", "citeRegEx": "Wainwright et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2003}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Correctness of belief propagation in Gaussian graphical models of arbitrary topology", "author": ["Y. Weiss", "W.T. Freeman"], "venue": "Neural Computation,", "citeRegEx": "Weiss and Freeman,? \\Q2001\\E", "shortCiteRegEx": "Weiss and Freeman", "year": 2001}, {"title": "Scattered Data Approximation", "author": ["H. Wendland"], "venue": "Cambridge, UK: Cambridge University Press.", "citeRegEx": "Wendland,? 2005", "shortCiteRegEx": "Wendland", "year": 2005}, {"title": "Approximate inference and protein-folding", "author": ["C. Yanover", "Y. Weiss"], "venue": "In NIPS,", "citeRegEx": "Yanover and Weiss,? \\Q2002\\E", "shortCiteRegEx": "Yanover and Weiss", "year": 2002}, {"title": "Generalized belief propagation", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Yedidia et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2001}, {"title": "Nonlinear Programming", "author": ["D.P. Bertsekas"], "venue": "Belmont, MA: Athena Scientific, second edn.", "citeRegEx": "Bertsekas,? 1999", "shortCiteRegEx": "Bertsekas", "year": 1999}, {"title": "Gaussian Belief Propagation: Theory and Application", "author": ["D. Bickson"], "venue": "Ph.D. thesis, The Hebrew University of Jerusalem.", "citeRegEx": "Bickson,? 2008", "shortCiteRegEx": "Bickson", "year": 2008}, {"title": "Residual splash for optimally parallelizing belief propagation", "author": ["J. Gonzalez", "Y. Low", "C. Guestrin"], "venue": null, "citeRegEx": "Gonzalez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gonzalez et al\\.", "year": 2009}, {"title": "Multi-label prediction via compressed sensing", "author": ["D. Hsu", "S.M. Kakade", "J. Langford", "T. Zhang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "KDE Matlab ToolBox", "author": ["A. Ihler"], "venue": "http://www.ics.uci.edu/ ihler/code/.", "citeRegEx": "Ihler,? 2003", "shortCiteRegEx": "Ihler", "year": 2003}, {"title": "Particle belief propagation", "author": ["A. Ihler", "D. McAllester"], "venue": "In AISTATS,", "citeRegEx": "Ihler and McAllester,? \\Q2009\\E", "shortCiteRegEx": "Ihler and McAllester", "year": 2009}, {"title": "Efficient multiscale sampling from products of gaussian mixtures", "author": ["E.T. Ihler", "E.B. Sudderth", "W.T. Freeman", "A.S. Willsky"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ihler et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Ihler et al\\.", "year": 2003}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Nonparametric belief propagation", "author": ["E. Sudderth", "A. Ihler", "W. Freeman", "A. Willsky"], "venue": "In CVPR", "citeRegEx": "Sudderth et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sudderth et al\\.", "year": 2003}, {"title": "Conditional density estimation via least-squares density ratio estimation", "author": ["M. Sugiyama", "I. Takeuchi", "T. Suzuki", "T. Kanamori", "H. Hachiya", "D. Okanohara"], "venue": null, "citeRegEx": "Sugiyama et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sugiyama et al\\.", "year": 2010}, {"title": "Tree-reweighted belief propagation and approximate ML estimation by pseudo-moment matching", "author": ["M. Wainwright", "T. Jaakkola", "A. Willsky"], "venue": "In 9th Workshop on Artificial Intelligence and Statistics", "citeRegEx": "Wainwright et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Wainwright et al\\.", "year": 2003}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan", "year": 2008}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J. Yedidia", "W. Freeman", "Y. Weiss"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}, {"title": "Generalized belief propagation", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Yedidia et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 21, "context": "1 Introduction Belief propagation is an inference algorithm for graphical models that has been widely and successfully applied in a great variety of domains, including vision (Sudderth et al., 2003), protein folding (Yanover & Weiss, 2002), and turbo decoding (McEliece et al.", "startOffset": 175, "endOffset": 198}, {"referenceID": 11, "context": ", 2003), protein folding (Yanover & Weiss, 2002), and turbo decoding (McEliece et al., 1998).", "startOffset": 69, "endOffset": 92}, {"referenceID": 16, "context": "Our approach applies not only to continuous-valued non-Gaussian variables, but also generalizes to strings and graphs (Sch\u00f6lkopf et al., 2004), groups (Fukumizu et al.", "startOffset": 118, "endOffset": 142}, {"referenceID": 4, "context": ", 2004), groups (Fukumizu et al., 2009), compact manifolds (Wendland, 2005, Chapter 17), and other domains on which kernels may be defined.", "startOffset": 16, "endOffset": 39}, {"referenceID": 15, "context": "This extends earlier work of Song et al. (2010) on inference for trees to the case of graphs with loops.", "startOffset": 29, "endOffset": 48}, {"referenceID": 21, "context": "Sudderth et al. (2003) proposed an approximate belief propagation algorithm for pairwise Markov random fields, where the parametric forms of the node and edge potentials are supplied in advance, and the messages are approximated as mixtures of Gaussians: we refer to this approach as Gaussian Mixture BP (this method was introduced as \u201cnonparametric BP\u201d, but it is in fact a Gaussian mixture approach).", "startOffset": 0, "endOffset": 23}, {"referenceID": 21, "context": "Sudderth et al. (2003) proposed an approximate belief propagation algorithm for pairwise Markov random fields, where the parametric forms of the node and edge potentials are supplied in advance, and the messages are approximated as mixtures of Gaussians: we refer to this approach as Gaussian Mixture BP (this method was introduced as \u201cnonparametric BP\u201d, but it is in fact a Gaussian mixture approach). Instead of mixtures of Gaussians, Ihler & McAllester (2009) used particles to approximate the messages, resulting in the Particle BP algorithm.", "startOffset": 0, "endOffset": 463}, {"referenceID": 13, "context": "Belief Propagation (BP) is an iterative algorithm for performing inference in MRFs (Pearl, 1988).", "startOffset": 83, "endOffset": 96}, {"referenceID": 12, "context": "In many applications, however, the resulting loopy BP algorithm exhibits excellent empirical performance (Murphy et al., 1999).", "startOffset": 105, "endOffset": 126}, {"referenceID": 28, "context": "Several theoretical studies have also provided insight into the approximations made by loopy BP, partially justifying its application to graphs with cycles (Wainwright & Jordan, 2008; Yedidia et al., 2001).", "startOffset": 156, "endOffset": 205}, {"referenceID": 19, "context": "4 Kernel Belief Propagation We develop a novel kernelization of belief propagation, based on Hilbert space embeddings of conditional distributions (Song et al., 2009), which generalizes an earlier kernel algorithm for exact inference on trees (Song et al.", "startOffset": 147, "endOffset": 166}, {"referenceID": 18, "context": ", 2009), which generalizes an earlier kernel algorithm for exact inference on trees (Song et al., 2010).", "startOffset": 84, "endOffset": 103}, {"referenceID": 18, "context": "In the present section, we will provide a detailed derivation of kernel BP in accordance with Song et al. (2010). While the immediate purpose is to make the paper selfcontained, there are two further important reasons: to provide the background necessary in understanding our efficient kernel BP updates in Section 5, and to demonstrate how kernel BP differs from the competing Gaussian mixture and particle based BP approaches in Section 6 (which was not addressed in earlier work on kernel tree graphical models).", "startOffset": 94, "endOffset": 113}, {"referenceID": 0, "context": "1 Message Representations We begin with a description of the properties of a message mut(xt), given it is in the reproducing kernel Hilbert space (RKHS) F of functions on the separable metric space X (Aronszajn, 1950; Sch\u00f6lkopf & Smola, 2002).", "startOffset": 200, "endOffset": 242}, {"referenceID": 16, "context": "An expression for the conditional distribution embedding was proposed by Song et al. (2009). We describe this expression by analogy with the conditioning operation for a Gaussian random vector z \u223c N (0, C), where we partition z = (z> 1 , z > 2 ) > such that z1 \u2208 R and z2 \u2208 R \u2032 .", "startOffset": 73, "endOffset": 92}, {"referenceID": 3, "context": "Following Fukumizu et al. (2004), we define the covariance operator CXsXt which allows us to compute the expectation of the product of function f(Xs) and g(Xt), i.", "startOffset": 10, "endOffset": 33}, {"referenceID": 18, "context": "cannot be performed), the belief can instead be expressed as a conditional embedding operator (Song et al., 2010).", "startOffset": 94, "endOffset": 113}, {"referenceID": 18, "context": "On this basis, Song et al. (2009) propose a direct regularized estimate of the conditional embedding operators from the data.", "startOffset": 15, "endOffset": 34}, {"referenceID": 21, "context": "6 Gaussian Mixture And Particle BP We briefly review two state-of-the-art approaches to nonparametric belief propagation: Gaussian Mixture BP (Sudderth et al., 2003) and Particle BP (Ihler & McAllester, 2009).", "startOffset": 142, "endOffset": 165}, {"referenceID": 22, "context": "We propose instead to learn P(Xt|Xs) directly from training data following Sugiyama et al. (2010), who provide an estimate in the form of a mixture of Gaussians (see Section 1 of the Appendix for details).", "startOffset": 75, "endOffset": 98}, {"referenceID": 2, "context": "An overview of approximation approaches can be found in Bickson et al. (2011); we used an efficient KD-tree method of Ihler et al.", "startOffset": 56, "endOffset": 78}, {"referenceID": 2, "context": "An overview of approximation approaches can be found in Bickson et al. (2011); we used an efficient KD-tree method of Ihler et al. (2003) for performing the approximation step.", "startOffset": 56, "endOffset": 138}, {"referenceID": 10, "context": "The first two were image denoising and depth prediction problems, where we show that kernel BP is superior to discrete, Gaussian mixture and particle BP in both speed and accuracy, using a GraphLab implementation of each (Low et al., 2010).", "startOffset": 221, "endOffset": 239}, {"referenceID": 14, "context": "We used a set of 274 images taken on the Stanford campus, including both indoor and outdoor scenes (Saxena et al., 2009).", "startOffset": 99, "endOffset": 120}, {"referenceID": 14, "context": "We note that the error of kernel BP is slightly better than the results of pointwise MRF reported in Saxena et al. (2009).", "startOffset": 101, "endOffset": 122}, {"referenceID": 8, "context": "We first ran PSI-BLAST to generate the sequence profile (a 20 dimensional feature for each amino acid position), and then used this profile as features for predicting the folding structure (Jones, 1999).", "startOffset": 189, "endOffset": 202}, {"referenceID": 23, "context": "A possible extension to this work would be to kernelize tree-reweighted belief propagation (Wainwright et al., 2003).", "startOffset": 91, "endOffset": 116}, {"referenceID": 6, "context": "The convergence of kernel BP is a further challenging topic for future work (Ihler et al., 2005).", "startOffset": 76, "endOffset": 96}, {"referenceID": 21, "context": "We describe two competing approaches for nonparametric belief propagation: Gaussian mixture BP, originally known as non-parametric BP (Sudderth et al., 2003), and particle BP (Ihler & McAllester, 2009).", "startOffset": 134, "endOffset": 157}, {"referenceID": 22, "context": "In learning the edge potentials, we turn to Sugiyama et al. (2010), who provide a least-squares estimate of a conditional density in the form of a mixture of Gaussians,", "startOffset": 44, "endOffset": 67}, {"referenceID": 6, "context": "In our implementation, we used the more efficient multiscale KD-tree sampling method of Ihler et al. (2003). We converted the Matlab Mex implementation of Ihler (2003) to C++, and used GraphLab to execute sampling in parallel with up to 16 cores.", "startOffset": 88, "endOffset": 108}, {"referenceID": 6, "context": "In our implementation, we used the more efficient multiscale KD-tree sampling method of Ihler et al. (2003). We converted the Matlab Mex implementation of Ihler (2003) to C++, and used GraphLab to execute sampling in parallel with up to 16 cores.", "startOffset": 88, "endOffset": 168}, {"referenceID": 23, "context": "The log-depth was discretized into 30 bins, and edge parameters were selected to achieve locally consistent Loopy BP marginals using the technique described in Wainwright et al. (2003). Empirically, finer discretizations did not improve resultant accuracy, but increased runtime significantly.", "startOffset": 160, "endOffset": 185}, {"referenceID": 23, "context": "The log-depth was discretized into 30 bins, and edge parameters were selected to achieve locally consistent Loopy BP marginals using the technique described in Wainwright et al. (2003). Empirically, finer discretizations did not improve resultant accuracy, but increased runtime significantly. We used the Splash scheduling of Gonzalez et al. (2009) since it provided the lowest runtime among all tested schedulings.", "startOffset": 160, "endOffset": 350}, {"referenceID": 23, "context": "Therefore once again, edge potentials were selected using the technique described in Wainwright et al. (2003). This performed quite well empirically, as seen in Figure 1(c) (main document).", "startOffset": 85, "endOffset": 110}, {"referenceID": 32, "context": "Recently, Hsu et al. (2009) proposed to solve this problem using compressed sensing techniques: high dimensional sparse category labels are first compressed to lower dimensional real vectors using a random projection, and regressors are learned for these real vectors.", "startOffset": 10, "endOffset": 28}, {"referenceID": 28, "context": "Yedidia et al. (2001) showed that the fixed point of F (and therefore the global minimum {bst, bs}) must satisfy the relations bst(Xs, Xt) = \u03b1\u03a8st(Xs, Xt)\u03a8s(Xs)\u03a8t(Xt) \u220f", "startOffset": 0, "endOffset": 22}, {"referenceID": 30, "context": "We follow Bickson (2008) in our presentation of the original Gaussian BP setting.", "startOffset": 10, "endOffset": 25}], "year": 2011, "abstractText": "We propose a nonparametric generalization of belief propagation, Kernel Belief Propagation (KBP), for pairwise Markov random fields. Messages are represented as functions in a reproducing kernel Hilbert space (RKHS), and message updates are simple linear operations in the RKHS. KBP makes none of the assumptions commonly required in classical BP algorithms: the variables need not arise from a finite domain or a Gaussian distribution, nor must their relations take any particular parametric form. Rather, the relations between variables are represented implicitly, and are learned nonparametrically from training data. KBP has the advantage that it may be used on any domain where kernels are defined (R, strings, groups), even where explicit parametric models are not known, or closed form expressions for the BP updates do not exist. The computational cost of message updates in KBP is polynomial in the training data size. We also propose a constant time approximate message update procedure by representing messages using a small number of basis functions. In experiments, we apply KBP to image denoising, depth prediction from still images, and protein configuration prediction: KBP is faster than competing classical and nonparametric approaches (by orders of magnitude, in some cases), while providing significantly more accurate results.", "creator": "LaTeX with hyperref package"}}}