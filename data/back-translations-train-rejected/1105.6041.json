{"id": "1105.6041", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2011", "title": "The Perceptron with Dynamic Margin", "abstract": "The classical perceptron rule provides a varying upper bound on the maximum margin, namely the length of the current weight vector divided by the total number of updates up to that time. Requiring that the perceptron updates its internal state whenever the normalized margin of a pattern is found not to exceed a certain fraction of this dynamic upper bound we construct a new approximate maximum margin classifier called the perceptron with dynamic margin (PDM). We demonstrate that PDM converges in a finite number of steps and derive an upper bound on them. We also compare experimentally PDM with other perceptron-like algorithms and support vector machines on hard margin tasks involving linear kernels which are equivalent to 2-norm soft margin.", "histories": [["v1", "Mon, 30 May 2011 17:02:09 GMT  (59kb)", "http://arxiv.org/abs/1105.6041v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["constantinos panagiotakopoulos", "petroula tsampouka"], "accepted": false, "id": "1105.6041"}, "pdf": {"name": "1105.6041.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 110 5.60 41v1 [cs.LG] 3 0M ayKeywords: online learning, classification, maximum margin."}, {"heading": "1 Introduction", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2 Motivation of the Algorithm", "text": "Let's consider a linear, detachable workout. (xk, lk) This pattern is a pattern that we do in terms of the way we do it in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and in terms of the way we do it, and the way we do it, and the way we do it in terms of the way we"}, {"heading": "4 Efficient Implementation", "text": "In order to reduce the calculation costs associated with executing PDM, we extend the procedure of [14, 13] and construct a three-part, nested sequence of reduced \"active sets\" of data points. As we pass through the complete dataset once, the (largest) first level of active patterns is formed by the points of the complete dataset that are satisfactory at \u00b7 yk \u2264 c1 (1 \u2212.2) when the second level of active sets is once formed by the first level of active sets from the points that are satisfactory at \u00b7 yk \u2264 c1 (1 \u2212.2), and the third level of active sets includes the points that are satisfactory at \u00b7 yk \u2264 2 (1 \u2212.2) when we pass through the first level of active sets from the points that are satisfactory at \u00b7 yk \u00b2."}, {"heading": "5 Experimental Evaluation", "text": "We compare PDM with several other major margins (with which we are able to achieve a rapid approximation to the \"optimal\" hyper-plane).We compare PDM with several other major margins (with which we place the \"optimal\" hyper-plane into space).We compare PDM with several major margins (with which we include the \"optimal\" hyper-plane into space).We compare PDM with other margins (with which we place the \"inseparable\" margins into space).We have put a margins into space in which we place the \"inseparable\" margins into space, in which we introduce the \"insoluble\" margins into space. \"We have a margins of at least one margins into the space of the\" insoluble margins. \"Moreover, employment is based on the well-known margins.\""}, {"heading": "6 Conclusions", "text": "We introduced the Perceptron with Dynamic Margin (PDM), a new approximate maximum margin classifier that uses the classic Perceptron update, demonstrated its convergence in a finite number of steps, and derived an upper limit from it. PDM uses the required accuracy as the only input parameter. In addition, it is a strictly online algorithm in the sense that it decides whether to perform an update only on the basis of its current state, regardless of whether the pattern presented to it when repeating the data set has been encountered before. Certainly, this does not apply to linear SVMs. Our experimental results suggest that PDM is the fastest large margin classifier enjoying the above two very desirable properties."}, {"heading": "1. Blum, A.: Lectures on machine learning theory. Carnegie Mellon University, USA.", "text": "Available at http: / / www.cs.cmu.edu / avrim / ML09 / lect0126.pdf 2. Christianini, N., Shawe-Taylor, J.: An introduction to support vector machines (2000) Cambridge, UK: Cambridge University Press 3. Duda, R.O., Hart, P.E.: Pattern Classification and Scene Analysis (1973) Wiley 4. Freund, Y., Shapire, R.E.: Large margin classification using the perceptron algorithm. Machine Learning 37 (3) (1999) Rep.: 277-296 5. Gentile, C.: A new approximate maximum margin classification algorithm. Journal of Machine Learning Research 2 (2001) 213-242 6. Joachims, T.: Making large-scale SVM learning practice. In Advances in kernel methods-support vector learning (1999)."}, {"heading": "18. Tsampouka, P., Shawe-Taylor, J.: Analysis of generic perceptron-like large margin", "text": "ECML (2005) 750-758 19. Tsampouka, P., Shawe-Taylor, J.: Constant rate approximate maximum margin algorithms. ECML (2006) 437-448 20. Tsampouka, P., Shawe-Taylor, J.: Constant rate approximate maximum margin algorithms with rules control by the number of errors. ICML (2007) 903-910 21. Vapnik, V.: Statistical learning theory (1998) Wiley"}], "references": [{"title": "An introduction to support vector machines", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Pattern classsification and scene analysis", "author": ["R.O. Duda", "P.E. Hart"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1973}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Y. Freund", "R.E. Shapire"], "venue": "Machine Learning 37(3)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "A new approximate maximal margin classification algorithm", "author": ["C. Gentile"], "venue": "Journal of Machine Learning Research 2", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "In Advances in kernel methods-support vector learning", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "KDD", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["Hsieh", "C.-J.", "Chang", "K.-W.", "Lin", "C.-J.", "S.S. Keerthi", "S. Sundararajan"], "venue": "ICML", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Online learning of maximum p-norm margin classifiers", "author": ["K. Ishibashi", "K. Hatano", "M. Takeda"], "venue": "COLT", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning algorithms with optimal stability in neural networks", "author": ["W. Krauth", "M. M\u00e9zard"], "venue": "Journal of Physics A20", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1987}, {"title": "The relaxed online maximummargin algorithm", "author": ["Y. Li", "P. Long"], "venue": "Machine Learning, 46(1-3)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "On convergence proofs on perceptrons", "author": ["A.B.J. Novikoff"], "venue": "In Proc. Symp. Math. Theory Automata, Vol. 12", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1962}, {"title": "The margin perceptron with unlearning", "author": ["C. Panagiotakopoulos", "P. Tsampouka"], "venue": "ICML", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "The margitron: A generalized perceptron with margin", "author": ["C. Panagiotakopoulos", "P. Tsampouka"], "venue": "IEEE Transactions on Neural Networks 22(3)", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["J.C. Platt"], "venue": "Microsoft Res. Redmond WA, Tech. Rep. MSR-TR-98-14", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review, 65(6)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1958}, {"title": "Perceptron-like large margin classifiers", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "Tech. Rep., ECS, University of Southampton, UK", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Analysis of generic perceptron-like large margin classifiers", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "ECML", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Constant rate approximate maximum margin algorithms", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "ECML", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Approximate maximum margin algorithms with rules controlled by the number of mistakes", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Statistical learning theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}], "referenceMentions": [{"referenceID": 19, "context": "It is a common belief that learning machines able to produce solution hyperplanes with large margins exhibit greater generalization ability [21] and this justifies the enormous interest in Support Vector Machines (SVMs) [21, 2].", "startOffset": 140, "endOffset": 144}, {"referenceID": 19, "context": "It is a common belief that learning machines able to produce solution hyperplanes with large margins exhibit greater generalization ability [21] and this justifies the enormous interest in Support Vector Machines (SVMs) [21, 2].", "startOffset": 220, "endOffset": 227}, {"referenceID": 0, "context": "It is a common belief that learning machines able to produce solution hyperplanes with large margins exhibit greater generalization ability [21] and this justifies the enormous interest in Support Vector Machines (SVMs) [21, 2].", "startOffset": 220, "endOffset": 227}, {"referenceID": 13, "context": "To overcome this problem decomposition methods [15, 6] were developed that apply optimization only to a subset of the training set.", "startOffset": 47, "endOffset": 54}, {"referenceID": 4, "context": "To overcome this problem decomposition methods [15, 6] were developed that apply optimization only to a subset of the training set.", "startOffset": 47, "endOffset": 54}, {"referenceID": 5, "context": "Recently, the so-called linear SVMs [7, 8, 13] made their appearance.", "startOffset": 36, "endOffset": 46}, {"referenceID": 6, "context": "Recently, the so-called linear SVMs [7, 8, 13] made their appearance.", "startOffset": 36, "endOffset": 46}, {"referenceID": 11, "context": "Recently, the so-called linear SVMs [7, 8, 13] made their appearance.", "startOffset": 36, "endOffset": 46}, {"referenceID": 14, "context": "Such algorithms are mostly based on the perceptron [16, 12], the simplest online learning algorithm for binary linear classification.", "startOffset": 51, "endOffset": 59}, {"referenceID": 10, "context": "Such algorithms are mostly based on the perceptron [16, 12], the simplest online learning algorithm for binary linear classification.", "startOffset": 51, "endOffset": 59}, {"referenceID": 1, "context": "The first algorithm of that kind is the perceptron with margin [3] which is much older than SVMs.", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "It is an immediate extension of the perceptron which provably achieves solutions with only up to 1/2 of the maximum margin [10].", "startOffset": 123, "endOffset": 127}, {"referenceID": 9, "context": "Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20].", "startOffset": 30, "endOffset": 34}, {"referenceID": 3, "context": "Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20].", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20].", "startOffset": 53, "endOffset": 57}, {"referenceID": 18, "context": "Such algorithms include ROMMA [11], ALMA [5], CRAMMA [19] and MICRA [20].", "startOffset": 68, "endOffset": 72}, {"referenceID": 12, "context": "Very recently, the same goal was accomplished by a generalized perceptron with margin, the margitron [14].", "startOffset": 101, "endOffset": 105}, {"referenceID": 15, "context": "The most straightforward way of obtaining large margin solutions through a perceptron is by requiring that the weight vector be updated every time the example presented to the algorithm has (normalized) margin which does not exceed a predefined value [17, 18, 1].", "startOffset": 251, "endOffset": 262}, {"referenceID": 16, "context": "The most straightforward way of obtaining large margin solutions through a perceptron is by requiring that the weight vector be updated every time the example presented to the algorithm has (normalized) margin which does not exceed a predefined value [17, 18, 1].", "startOffset": 251, "endOffset": 262}, {"referenceID": 12, "context": "In an earlier work [14] we noticed that the upper bound \u2016at\u2016 /t on the maximum margin, with \u2016at\u2016 being the length of the weight vector and t the number of updates, that comes as an immediate consequence of the perceptron update rule is very accurate and tends to improve as the algorithm achieves larger margins.", "startOffset": 19, "endOffset": 23}, {"referenceID": 19, "context": "This training set may either be the original dataset or the result of a mapping into a feature space of higher dimensionality [21, 2].", "startOffset": 126, "endOffset": 133}, {"referenceID": 0, "context": "This training set may either be the original dataset or the result of a mapping into a feature space of higher dimensionality [21, 2].", "startOffset": 126, "endOffset": 133}, {"referenceID": 2, "context": "Actually, there is a very well-known construction [4] making linear separability always possible, which amounts to the adoption of the 2-norm soft margin.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "by extending xk to [xk, \u03c1], we construct an embedding of our data into the socalled augmented space [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 15, "context": "As \u03c1 \u2192 \u221e, R/\u03c1 \u2192 1 and, consequently, \u03b3d \u2192 \u03b3 [17, 18].", "startOffset": 44, "endOffset": 52}, {"referenceID": 16, "context": "As \u03c1 \u2192 \u221e, R/\u03c1 \u2192 1 and, consequently, \u03b3d \u2192 \u03b3 [17, 18].", "startOffset": 44, "endOffset": 52}, {"referenceID": 10, "context": "Taking the inner product of (2) with the optimal direction u and using (1) we get u \u00b7 at+1 \u2212 u \u00b7 at = u \u00b7 yk \u2265 \u03b3d a repeated application of which gives [12] \u2016at\u2016 \u2265 u \u00b7 at \u2265 \u03b3dt .", "startOffset": 152, "endOffset": 156}, {"referenceID": 15, "context": "The perceptron algorithm with fixed margin condition (PFM) is known to converge in a finite number of updates to an \u01eb-accurate approximation of the maximum directional margin hyperplane [17, 18, 1].", "startOffset": 186, "endOffset": 197}, {"referenceID": 16, "context": "The perceptron algorithm with fixed margin condition (PFM) is known to converge in a finite number of updates to an \u01eb-accurate approximation of the maximum directional margin hyperplane [17, 18, 1].", "startOffset": 186, "endOffset": 197}, {"referenceID": 15, "context": "This has already been shown for PFM [17, 18, 1] but no general \u01eb-dependent bound in closed form has been derived.", "startOffset": 36, "endOffset": 47}, {"referenceID": 16, "context": "This has already been shown for PFM [17, 18, 1] but no general \u01eb-dependent bound in closed form has been derived.", "startOffset": 36, "endOffset": 47}, {"referenceID": 12, "context": "To reduce the computational cost involved in running PDM, we extend the procedure of [14, 13] and construct a three-member nested sequence of reduced \u201cactive sets\u201d of data points.", "startOffset": 85, "endOffset": 93}, {"referenceID": 11, "context": "To reduce the computational cost involved in running PDM, we extend the procedure of [14, 13] and construct a three-member nested sequence of reduced \u201cactive sets\u201d of data points.", "startOffset": 85, "endOffset": 93}, {"referenceID": 12, "context": "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14, 13] once a data point is presented to the algorithm.", "startOffset": 134, "endOffset": 142}, {"referenceID": 11, "context": "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14, 13] once a data point is presented to the algorithm.", "startOffset": 134, "endOffset": 142}, {"referenceID": 2, "context": "For linearly separable data the feature space is the initial instance space whereas for inseparable data (which is the case here) a space extended by as many dimensions as the instances is considered where each instance is placed at a distance \u2206 from the origin in the corresponding dimension [4].", "startOffset": 293, "endOffset": 296}, {"referenceID": 0, "context": "equivalence between the hard margin optimization in the extended space and the soft margin optimization in the initial instance space with objective function \u2016w\u2016 +\u2206 \u2211 i\u03be\u0304 2 i involving the weight vector w and the 2-norm of the slacks \u03be\u0304i [2].", "startOffset": 238, "endOffset": 241}, {"referenceID": 13, "context": "The datasets we used for training are: the Adult (m = 32561 instances, n = 123 attributes) and Web (m = 49749, n = 300) UCI datasets as compiled by Platt [15], the training set of the KDD04 Physics dataset (m = 50000, n = 70 after removing the 8 columns containing missing features) obtainable from http://kodiak.", "startOffset": 154, "endOffset": 158}, {"referenceID": 7, "context": "From the class of perceptron-like algorithms we have chosen (aggressive) ROMMA which is much faster than ALMA in the light of the results presented in [9, 14].", "startOffset": 151, "endOffset": 158}, {"referenceID": 12, "context": "From the class of perceptron-like algorithms we have chosen (aggressive) ROMMA which is much faster than ALMA in the light of the results presented in [9, 14].", "startOffset": 151, "endOffset": 158}, {"referenceID": 5, "context": "Decomposition SVMs are represented by SVM [7] which, apart from being one of the fastest algorithms of this class, has the additional advantage of making very efficient use of memory, thereby making possible the training on very large datasets.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "Finally, from the more recent class of linear SVMs we have included in our study the dual coordinate descent (DCD) algorithm [8] and the margin perceptron with unlearning (MPU) [13].", "startOffset": 125, "endOffset": 128}, {"referenceID": 11, "context": "Finally, from the more recent class of linear SVMs we have included in our study the dual coordinate descent (DCD) algorithm [8] and the margin perceptron with unlearning (MPU) [13].", "startOffset": 177, "endOffset": 181}, {"referenceID": 7, "context": "The absence of publicly available implementations for ROMMA necessitated the writing of our own code in C++ employing the mechanism of active sets proposed in [9] and incorporating a mechanism of permutations performed at the beginning of a full epoch.", "startOffset": 159, "endOffset": 162}, {"referenceID": 11, "context": "For MPU the implementation followed closely [13] with active set parameters c\u0304 = 1.", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "From the results of [14] the fastest algorithm of this class is the margitron which has strong before-run guarantees and a very good after-run estimate of the achieved accuracy through (5).", "startOffset": 20, "endOffset": 24}], "year": 2013, "abstractText": "The classical perceptron rule provides a varying upper bound on the maximum margin, namely the length of the current weight vector divided by the total number of updates up to that time. Requiring that the perceptron updates its internal state whenever the normalized margin of a pattern is found not to exceed a certain fraction of this dynamic upper bound we construct a new approximate maximum margin classifier called the perceptron with dynamic margin (PDM). We demonstrate that PDM converges in a finite number of steps and derive an upper bound on them. We also compare experimentally PDM with other perceptron-like algorithms and support vector machines on hard margin tasks involving linear kernels which are equivalent to 2-norm soft margin.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}