{"id": "1105.6162", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2011", "title": "A statistical learning algorithm for word segmentation", "abstract": "In natural speech, the speaker does not pause between words, yet a human listener somehow perceives this continuous stream of phonemes as a series of distinct words. The detection of boundaries between spoken words is an instance of a general capability of the human neocortex to remember and to recognize recurring sequences. This paper describes a computer algorithm that is designed to solve the problem of locating word boundaries in blocks of English text from which the spaces have been removed. This problem avoids the complexities of processing speech but requires similar capabilities for detecting recurring sequences. The algorithm that is described in this paper relies entirely on statistical relationships between letters in the input stream to infer the locations of word boundaries. The source code for a C++ version of this algorithm is presented in an appendix.", "histories": [["v1", "Tue, 31 May 2011 05:03:06 GMT  (732kb)", "http://arxiv.org/abs/1105.6162v1", "28 pages, 5 figures"], ["v2", "Sun, 26 Jun 2011 22:52:32 GMT  (745kb)", "http://arxiv.org/abs/1105.6162v2", "30 pages, 5 figures"]], "COMMENTS": "28 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jerry r van aken"], "accepted": false, "id": "1105.6162"}, "pdf": {"name": "1105.6162.pdf", "metadata": {"source": "CRF", "title": "A Statistical Learning Algorithm for Word Segmentation", "authors": ["Jerry R. Van Aken"], "emails": [], "sections": [{"heading": null, "text": "KEYWORDS: Recognition of word boundaries, sequence memory, text segmentation, temporal pattern recognition, word fragmentation, viterbi algorithm. In natural language, spoken words merge into a continuous stream of sounds, but humans perceive language as a sequence of different words. How the human neocortex performs this feat is an open question. Even experienced listeners seem to rely to some extent on prosodic cues in language - timing, pauses, stress and changes in intonation - to separate words and phrases. Is prosody the primary mechanism through which humans learn to identify individual words? Perhaps prosody merely supports a more basic learning mechanism based on the statistical properties of continuous speech to identify individual words. (See Cutler [3], Kuhl [8] and Saffran, Aslin & Newport [9] to design something similar if possible."}, {"heading": "Problem Statement", "text": "In fact, most of us will be able to play by the rules that they have imposed on themselves, and they will be able to understand the rules that they have imposed on themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "Relationship to Prior Work", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "Sequence Memory", "text": "In fact, the number of people who are in the city is very high. (...) The number of people who are in the city is very high. (...) The number of people who are in the city is very high. (...) The number of people who are in the city is very high. (...) The number of people who are in the city is very high. (...) The number of people who are in the city is very high. (...) The number of people who are in the city is very high. (...) The number of people who are in the city is very high. (...) The number of people who are in the city is very high. \""}, {"heading": "Word Boundary Detection", "text": "This year, it has come to the point where it can only take a year for a solution to be found and a solution to be found."}, {"heading": "Transition Probabilities", "text": "To calculate the probability of a path through an event window, the algorithm must first calculate the probabilities of each sequence-to-sequence transitions in the path = = previously described. As already discussed when an instance of sequence X in the instance stream is immediately followed by the letter x, the two possible transitions from this sequence are the equinox transition to sequence X '= X + x (where the plus sign indicates concatenation) and the new word transition to the one-word sequence X' = 0 + x, where X 'is the next sequence in the path after X, and 0 is the zero sequence of sequence X' = X + x (where the plus sign indicates concatenation) and the new word transition to the one-word sequence X 'x', where X'member of the X sequence is the next sequence in the path after X, and 0 the zero sequence of the sequence X' must be the sequence X sequence sequence sequence sequence X, therefore X sequence sequence sequence sequence X must be the same sequence sequence sequence sequence X."}, {"heading": "Two-Level Scoring System", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "Discussion", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to break the rules they have set themselves."}, {"heading": "Acknowledgment", "text": "A careful reading of an early manuscript by Dana M. Van Aken led to a number of improvements in the presentation of the concepts in this essay."}, {"heading": "Appendix A: The SEQUENCE Structure", "text": "The SEQUENCE structure represents a sequence of characters. Each sequence stored in the sequence memory is represented by a SEQUENCE structure, which is defined as: typedef struct _ SEQUENCE {struct _ SEQUENCE * _ link; struct _ SEQUENCE * _ prevSeq; struct {BYTE _ event; BYTE _ length; WORD _ allocNum;} _ info; DWORD _ createCount; DWORD _ outCount; DWORD _ inCount; DWORD _ succCount; float _ accumScores;} SEQUENCE;"}, {"heading": "Members", "text": "linkA pointer to the next SEQUENCE structure in a linked list. A hash table contains all sequences in the sequence store. Each bucket in the hash table contains a linked list of sequences.prevSeqA pointer to the SEQUENCE structure representing the previous sequence in sequence storage.eventSpecifies the number of characters in the sequence represented by this SEQUENCE structure. the other characters in this sequence are contained in the previous sequence referenced by the prevSeq membership.lengthSpecifies the number of characters in the sequence represented by this sequence structure. allocNumSpeciunt specifies the assignment number assigned to this structure. Each created instance of the SEQUENCE structure is assigned an assignment number to uniquely identify the instance of the instances. Countatecrefied characters received at the time the specified number of the accumulation was received."}, {"heading": "Appendix B: Program Listing", "text": "\"It's a very complex issue,\" he says, \"but it's not over yet.\""}], "references": [{"title": "Hashing Rehashed", "author": ["Binstock", "Andrew"], "venue": "Dr. Dobb's Journal, 4(2), April 1996.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1996}, {"title": "An Efficient, Probabilistically Sound Algorithm for Segmentation and Word Discovery", "author": ["Brent", "Michael"], "venue": "Machine Learning, 34, 71-106.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 0}, {"title": "Prosody and the Word Boundary Problem", "author": ["Cutler", "Anne"], "venue": "Signal to Syntax: Bootstrapping from Speech to Grammar in Early Acquisition, ed. James Morgan & Kathering Demuth, Lawrence Erlbaum Associates, 1996, 87-99.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Finding Structure in Time", "author": ["Elman", "Jeffrey"], "venue": "Cognitive Science, 14, 1990, 179-211.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "The Viterbi Algorithm", "author": ["Forney", "David"], "venue": "Proceedings of the IEEE, 61(3), March 1973, 268-278.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1973}, {"title": "Mechanisms and Constraints in Word Segmentation", "author": ["Gambell", "Timothy", "Charles Yang"], "venue": "Manuscript, Yale University, 2005, 31 pages.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Segmentation and Morphology", "author": ["Goldsmith", "John"], "venue": "The Handbook of Computational Linguistics and Natural Language Processing, ed. Alexander Clark, et al, Blackwell Publishing, 2010, 364-393.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Early Language Acquisition: Cracking the Speech Code", "author": ["Kuhl", "Patricia"], "venue": "Nature Reviews, 5, November 2004, 831-842.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Statistical Learning by 8-Month- Old Infants", "author": ["Saffran", "Jenny", "Richard Aslin", "Elissa Newport"], "venue": "Science, 274, December 1996, 1926-1928.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Error Bounds for Convolutional Codes and an Asymptotically Optimum Decoding Algorithm", "author": ["Viterbi", "Andrew"], "venue": "IEEE Transactions on Information Theory, 13(2), April 1967, 260-269.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1967}], "referenceMentions": [{"referenceID": 2, "context": "(See Cutler [3], Kuhl [8], and Saffran, Aslin & Newport [9].", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "(See Cutler [3], Kuhl [8], and Saffran, Aslin & Newport [9].", "startOffset": 22, "endOffset": 25}, {"referenceID": 8, "context": "(See Cutler [3], Kuhl [8], and Saffran, Aslin & Newport [9].", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "Brent [2] provides an overview of word-segmentation algorithms that are primarily published in the speech-processing literature.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "Gambell & Yang [6] provide a performance comparison many of these algorithms.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "Goldsmith [7] provides a recent overview of wordsegmentation algorithms that are primarily from the text-processing literature.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Elman [4] trained a simple recurrent network (SRN) to predict the next symbol in an input stream based on the symbols that immediately precede this symbol.", "startOffset": 6, "endOffset": 9}, {"referenceID": 8, "context": "Saffran, Aslin & Newport [9] studied language learning in infants, and showed that statistical relationships between phonemes in speech are an important source of information about word boundaries.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "Brent [2] proposed that the accuracy of word boundary predictions can be improved by using mutual information between phonemes instead of transitional probabilities.", "startOffset": 6, "endOffset": 9}, {"referenceID": 4, "context": "The Viterbi algorithm [5][10] provides a useful model of the kind of processing that the wordsegmentation algorithm requires to simultaneously evaluate multiple competing paths.", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "The Viterbi algorithm [5][10] provides a useful model of the kind of processing that the wordsegmentation algorithm requires to simultaneously evaluate multiple competing paths.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "The GetHashIndex function in the preceding program uses the ELF hash algorithm, as described by Binstock [1].", "startOffset": 105, "endOffset": 108}], "year": 2011, "abstractText": "In natural speech, the speaker does not pause between words, yet a human listener somehow perceives this continuous stream of phonemes as a series of distinct words. The detection of boundaries between spoken words is an instance of a general capability of the human neocortex to remember and to recognize recurring sequences. This paper describes a computer algorithm that is designed to solve the problem of locating word boundaries in blocks of English text from which the spaces have been removed. This problem avoids the complexities of processing speech but requires similar capabilities for detecting recurring sequences. The algorithm that is described in this paper relies entirely on statistical relationships between letters in the input stream to infer the locations of word boundaries. The source code for a C++ version of this algorithm is presented in an appendix.", "creator": "Microsoft\u00ae Office Word 2007"}}}