{"id": "1106.0357", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2011", "title": "Learning Hierarchical Sparse Representations using Iterative Dictionary Learning and Dimension Reduction", "abstract": "This paper introduces an elemental building block which combines Dictionary Learning and Dimension Reduction (DRDL). We show how this foundational element can be used to iteratively construct a Hierarchical Sparse Representation (HSR) of a sensory stream. We compare our approach to existing models showing the generality of our simple prescription. We then perform preliminary experiments using this framework, illustrating with the example of an object recognition task using standard datasets. This work introduces the very first steps towards an integrated framework for designing and analyzing various computational tasks from learning to attention to action. The ultimate goal is building a mathematically rigorous, integrated theory of intelligence.", "histories": [["v1", "Thu, 2 Jun 2011 02:31:04 GMT  (438kb,D)", "http://arxiv.org/abs/1106.0357v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["mohamad tarifi", "meera sitharam", "jeffery ho"], "accepted": false, "id": "1106.0357"}, "pdf": {"name": "1106.0357.pdf", "metadata": {"source": "CRF", "title": "Learning Hierarchical Sparse Representations using Iterative Dictionary Learning and Dimension Reduction", "authors": ["Mohamad Tarifi", "Meera Sitharam", "Jeffery Ho"], "emails": [], "sections": [{"heading": null, "text": "This year, it has come to the point that there is only one time that there is such a process, in which there is such a process."}, {"heading": "1 DRDL Circuit Element", "text": "Our circuit element is a simple concatenation of a Dictionary Learning (DL) step followed by a Dimension Reduction (DR) step. Using an over-complete dictionary increases the dimension of the data, but now that the data is sparse, we can use compressed sensing to achieve a dimension reduction."}, {"heading": "1.1 Dictionary Learning", "text": "Dictionary Learning gets a sparse representation through learning functions where the data can be written in sparse linear combinations.Definition 1. Faced with an input set X = [x0... xm], xi-Rd, Dictionary Learning finds D = [v0.. vn] and \u03b8 = [\u03b8i.. \u03b8m], so xi = Dictionary Factorization (SNMF).An optimization version of Dictionary Learning can be written as the L0 norm or sparseness. If all input from the Dictionary is classified as non-negative, we get the Sparse-Non-Negative Matrix Factorization (SNMF).An optimization version of Dictionary Learning can be used as: min D-Rd, n-Maximum ximin the Dictionary Method."}, {"heading": "1.2 Dimension Reduction", "text": "The DL step learns a representation of the input that lives in a high dimension, but we can get a low-dimensional representation, because Traffici in the orthonormal standard basis of the dimension n is now readily regarded as spar.Definition 2. A linear operator A has the limited isometry property (RIP) for s, iff s, as follows: (1 \u2212 \u03b4s): (1 \u2212 \u03b4s): (1 \u2212 \u03b4s: (2): (2): (2): (2): (2 \u2212): (2 \u2212): (2 \u2212): (2 \u2212): (2 \u2212): (2 \u2212): (2 \u2212): (2 \u2212): (2 \u2212): (2 \u2212): (2 \u2212): (2): (2 \u2212:::: (2): (2): (2): (2): (2):: (2): (2): (2): (2 \u2212::): (2): (2): (2 \u2212): (2): (2): (2): (2): (2): (2): (2): (2):): (2:): (2:): (2:): (2:): (2:): (2:): (2:): (2:): (2:): (2: (2:): (2:): (2:): (2: (2:): (2:): (2: (2:): (2:): (2: (2:): (2: (2:): (2:): (2: (2:): (2:): (2: (2:): (2: (2:): (2:): (2: (2:): (2:): (2: (2:): (2:): (2:): (2: (2:): (2: (2:): (2: (2:): (2: (2:): (2: (2:): (2:):): (2: (2:): (2"}, {"heading": "1.3 Illustrating the model with simple examples", "text": "Figure 2 illustrates a particular distribution that is sparse in the 3 drawn vectors s = 1. Dictionary D1 is (1 0 1 0 1 1) The columns of the learned dictionary correspond to the drawn vectors, and the data is simply expressed as a vector with a coefficient corresponding to the inner product with the nearest vector and zero otherwise. This results in ans = 1 sparse vector in dimension 3. We can then apply a dimension reduction that preserves the distances between these representations. The representations correspond to the standard basis in d = 3. The best dimension reduction to d = 2 is then simply the projection of the representations perpendicular to the plane (1, 1, 1, 1)."}, {"heading": "1.4 Relation between DR and DL", "text": "To illustrate their relationship, we rewrite the two problems with the same variable names. These variables are only relevant for this section, and the two problems can be defined as follows: 1. DL asks for D and {x1... xm}, given {y1... ym}, for Dxi = yi, so that the dimension of Yi is minimized for a fixed dimension of Yi; 2. DR asks for D and {y1..... ym}, given {x1... xm}, for Dxi = yi, so that the dimension of Yi is minimized for a fixed dimension of Yi. In practice, both problems use the L1 approximation as a substitute for the L0 optimization. This leads to the following observation: The inversion of a DRDL is a DRDL. This means that the space of mappings / functions of our model is the same as this property will be useful for inclusion."}, {"heading": "1.5 Discussion of Trade-offs in DRDL", "text": "DRDL can be thought of as a storage system (\"memory pocket\") or as a method of reducing the size of data that can be expressed sparsely in a dictionary. A parameter compromise exists between n, the number of columns in D, and s, the sparse representation. On the one hand, we find that the DR step brings the data into the O (slog (n) dimension. So, if we want to maximize the reduction in the dimension, increasing n by increasing it to a constant performance k is comparable to multiplying s by k. This means that we increase the number of columns in the dictionary much more than the sparity. On the other hand, increasing the number of columns in D forces the columns to a high correlation. This will become problematic for the base pursuit vector selection. This trade-off underscores the importance of studying dictionary learning approaches and vector selection that can go beyond current results in highly coherent dictionaries."}, {"heading": "2 A Hierarchical Sparse Representation", "text": "If we start from a hierarchical architecture that models the topographical organization of the visual cortex, a single DRDL element can be factorized and expressed as a tree of simpler DRDL elements. With this architecture, we can learn a hierarchical, sparse representation by iterating DRDL elements."}, {"heading": "2.1 Assumptions of Generative Model", "text": "Our models assume that data is generated by a hierarchy of spatio-temporal invariants. At each level i, each node in the generative model is assumed to consist of a small number of characteristics si. It is generated by recursive decompression of the pattern from parent node to child node. This input is subsequently fed into the learning algorithm. In this paper, we assume that both the topology of the generative model and the spatio-temporal extent of each node are known. Discussions of algorithms for learning the topology and internal dimensions are reserved for future work. Let's consider a simple data stream consisting of a spatio-temporal sequence of a generative model defined above. Figure 1 shows a potential learning hierarchy. For simple visual problems, we can consider all dictionaries within a level to be the same. In this paper, processing is carried out only from the bottom up of the hierarchy."}, {"heading": "2.2 Learning Algorithm", "text": "Divide the spatio-temporal signal xi recursively to get a tree that represents the known topographical hierarchy of spatio-temporal blocks. Let x0i.j be the jth block at level 0. Then, by starting a lower part of the tree, do: 1. Learn a dictionary Dj, k, in which the spatio-temporal data x k i, j can be sparsely represented. This creates a vector of weights \u03b8i, j, k.2. Replace the dimensionality reduction to the sparse representation to ui, j, k = A\u03b8i, j, k.3. Generate xk + 1i, j by concatenating vectors ui, l, k for all l that are child of j at level k in the tree. Replace k = k + 1. And now j extends across elements of the plane k. If k is even smaller than the depth of the tree, go to Step 1.Note that it is reasonable to assume that all areas of the computer can be limited to 1."}, {"heading": "2.3 Representation Inference", "text": "For new data points, representation in analogy to the learning algorithm is achieved by recursively dividing the spatio-temporal signal to obtain a tree representing the well-known topographic hierarchy of spatio-temporal blocks. Of course, representation is derived by iterative application of vector selection and compressed scanning. To select vectors, we use a common variation technology called Basis Pursuit De-Noising (BPDN), which minimizes the relationships between the dictionaries D.C = max k, l (DTD) k, l (4). This technique delivers optimal results when the difference between the dictionaries \u03b8 0 < 12 + 12C. (3) Where C is the coherence of the dictionary D.C = max k, l (DTD) k, l (4) This is a limitation in practice, as it is desirable to have highly coherent dictionaries."}, {"heading": "2.4 Mapping between HSR and current models", "text": "We abstracted this model from the need for conceptual simplicity and generality. Several models inspired by the neocortex have been proposed, many of which have similar characteristics. We present some here and compare them with our model.H-Max is replaced by Basis Pursuit for Sparse Approximation with Template Matching. H-Max uses random templates from the data, while HSR uses Dictionary Learning.The \"max\" operation in H-Max can be understood in terms of the operation of economical coding, which generates local competition between features that represent small discrepancies in the data.Alternatively, the \"max\" operation can be seen as a limited form of dimension reduction. HTM can be mapped to HSR by viewing time as another dimension of space. In this way, limited variable order markov chains can be described as a sparse approximation of spatiotemporal vectors, which is a conditional reduction of time by mapping HTM to another dimension of space."}, {"heading": "2.5 What does HSR offer beyond current models?", "text": "One advantage of our approach is to provide invertibility without dimensionality blow-up for hierarchically sparse data. Models that fall within the framework of Bouvrie et al lose information when you move the hierarchy upwards due to the pooling process. This becomes problematic when models are expanded to include feedback. Furthermore, this loss of information forces an algorithm designer to specify for which inventories a particular model must be selected (such as translational inventory in H-max). On the other hand, invertible models such as HTM suffer from dimensionality when the number of traits learned at a certain level is greater than the input dimension at that level - as is usually the case. Dimensionality reduction achieves both a saving of computational resources and better noise resistance by avoiding overpassing.Dictionary learning represents the data by sparse combinations of common words, which may be motivated by a better spreadout of 1 or a better spreadout of 1."}, {"heading": "2.6 Discussion of Trade-offs in HSR", "text": "Informally, the hierarchy is useful to reduce the sample complexity and dimensionality of the problem. Consider, for example, the simplified case of binary {0, 1} coefficients and translation invariance (such as vision). A generative HSR model of two layers generates (m2 s2) patterns. Learning this with a single layer of HSR would be learning a dictionary of m2 columns and s2 sparity using | X | samples in dimension d. With two layers, we have the first layer learning a dictionary of size m1 and sparity s1 using the dimension dk, the second layer learning a dictionary of size m2 columns and s2 with | X | samples in dimension k \u0445 O (s1 \u0445 logm1) < d. In fact, by adding a layer we have divided the problem into two simpler problems in terms of dimensionality and sample complexity. A more formal future and more complete work will be presented in the form."}, {"heading": "3 Experiments", "text": "In this section, we will discuss preliminary numerical experiments performed with DRDL and HSR on basic standard machine learning datasets. We applied our model to the MNIST and COIL datasets and then used the representation as a step to feature extraction for a classification algorithm such as Support Vector Machines (SVM) or k-Nearest Neighbors (kNN). In practice, other earlier assumptions can be incorporated into our model as discussed in the appendix."}, {"heading": "3.1 MNIST Results", "text": "We applied our model to all pairs of the MNIST dataset. For the RIP step, we tested random matrices and sparsely random matrices, and tested the effectiveness of the approach by reconstructing the training data with basic tracking. We used only two layers. After learning the characteristics, we applied a k-NN with k = 3. We refrained from optimizing the initial parameters, as we expect our model to work off the bar. For one layer of DRDL, we got a 1.24% error rate with a standard deviation of 0.011. With two layers, we achieved an error of 2.01% and a standard deviation of 0.016."}, {"heading": "3.2 COIL Results", "text": "We applied our model to all pairs of COIL-30 (that is, a subset of 30 objects out of the total 100 objects in the COIL dataset). The dataset consists of 72 images for each class. We used only 4 labeled images per class for training, and these are taken at the same angles (0, 90, 180, 270). We used the same procedure as before to determine and verify the RIP matrix. We applied a single layer of DRDRL, then trained a k-NN with k = 3. We also waived the adjustment of the initial parameters. We achieved an average error of 12.2%."}, {"heading": "4 Discussion", "text": "We have introduced a novel formulation of an elementary building block that could serve as a bottom-up building block within the common cortical algorithm. This model leads to several interesting theoretical questions. In the appendix, we show how additional earlier assumptions about the generative model can be expressed within our integrated framework. In addition, this framework can also be extended to include feedback, attention, action, complementary learning and the role of time. In future work, we will examine these questions in detail, propose alternative dictionary learning algorithms, dimension reduction techniques, and incorporate feedback."}, {"heading": "Acknowledgements", "text": "We benefited from discussions with several neuroscientists, in particular we would like to thank Dr. Linda Hermer."}, {"heading": "Appendix: Expressing Prior Assumptions", "text": "Additional prior information can be encoded separately at different levels in the hierarchical topology and in the steps Dictionary Learning and Dimension Reduction. One type of knowledge is through invariances specified on the data. Invariances can be considered as transformations that map our data to themselves. Specifically, a set of points X under a transformation I iff I (X) = Xinvariant. These assumptions can be encoded as follows:"}], "references": [{"title": "Emergence of simple cell receptive field properties by learning a sparse code for natural", "author": ["Bruno a Olshausen", "David J Field"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Computer Science and Artificial Intelligence Laboratory Technical Report Generalization and Properties of the Neural Response Generalization and Properties of the Neural Response", "author": ["Jake Bouvrie", "Tomaso Poggio", "Lorenzo Rosasco", "Steve Smale", "Andre Wibisono"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "What and where: A Bayesian inference theory of attention", "author": ["Sharat S. Chikkerur", "Thomas Serre", "Cheston Tan", "Tomaso Poggio"], "venue": "Vision Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "An integrated model of visual attention using shape-based features", "author": ["Sharat S. Chikkerur", "Cheston Tan", "Thomas Serre", "Tomaso Poggio"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "On the Prospects for Building a Working Model of the Visual Cortex", "author": ["Thomas Dean", "Glenn Carroll", "Richard Washington"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Hierarchical models in the brain", "author": ["Karl J Friston"], "venue": "PLoS computational biology,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Reinforcement learning or active inference", "author": ["Karl J Friston", "Jean Daunizeau", "Stefan J Kiebel"], "venue": "PloS one,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Action and behavior: a free-energy formulation", "author": ["Karl J Friston", "Jean Daunizeau", "James Kilner", "Stefan J Kiebel"], "venue": "Biological cybernetics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Action understanding and active inference", "author": ["Karl J Friston", "J\u00e9r\u00e9mie Mattout", "James Kilner"], "venue": "Biological cybernetics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Towards a mathematical theory of cortical micro-circuits", "author": ["Dileep George", "Jeff Hawkins"], "venue": "PLoS computational biology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Sparse Recovery Using Sparse Matrices", "author": ["Anna Gilbert", "Piotr Indyk"], "venue": "Proceedings of the IEEE,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Hierarchical Bayesian inference in the visual cortex", "author": ["Tai Sing Lee", "David Mumford"], "venue": "Journal of the Optical Society of America. A, Optics, image science, and vision,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Perceptual Neuroscience: The Cerebral Cortex", "author": ["Vernon B. Mountcastle"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by V1", "author": ["Bruno A Olshausen", "D J Field"], "venue": "Vision research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Hierarchical models of object recognition in cortex", "author": ["M Riesenhuber", "Tomaso Poggio"], "venue": "Nature neuroscience,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "A quantitative theory of immediate visual recognition", "author": ["Thomas Serre", "Gabriel Kreiman", "Minjoon Kouh", "Charles Cadieu", "Ulf Knoblich", "Tomaso Poggio"], "venue": "Brain, 165:33\u201356,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "A feedforward architecture accounts for rapid categorization", "author": ["Thomas Serre", "Aude Oliva", "Tomaso Poggio"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Robust object recognition with cortex-like mechanisms", "author": ["Thomas Serre", "Lior Wolf", "Stanley Bileschi", "Maximilian Riesenhuber", "Tomaso Poggio"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "An integrated view of Intelligence has been proptosed by Karl Friston based on free-energy [13, 11, 8, 9, 10, 12].", "startOffset": 91, "endOffset": 113}, {"referenceID": 6, "context": "An integrated view of Intelligence has been proptosed by Karl Friston based on free-energy [13, 11, 8, 9, 10, 12].", "startOffset": 91, "endOffset": 113}, {"referenceID": 7, "context": "An integrated view of Intelligence has been proptosed by Karl Friston based on free-energy [13, 11, 8, 9, 10, 12].", "startOffset": 91, "endOffset": 113}, {"referenceID": 8, "context": "An integrated view of Intelligence has been proptosed by Karl Friston based on free-energy [13, 11, 8, 9, 10, 12].", "startOffset": 91, "endOffset": 113}, {"referenceID": 12, "context": "1 Background Literature In A Glance Speculation on a cortical micro-circuit element dates back to Mountcastle\u2019s observation that a cortical column may serve as an algorithmic building block of the neocortex [18].", "startOffset": 207, "endOffset": 211}, {"referenceID": 11, "context": "Later work by Lee and Mumford [16], Hawkins and George [14] attempted further investigation of this process.", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "Later work by Lee and Mumford [16], Hawkins and George [14] attempted further investigation of this process.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Work by Poggio, Serre, et al [20, 21, 22, 23], Dean [6, 7], discuss a hierarchical topology.", "startOffset": 29, "endOffset": 45}, {"referenceID": 15, "context": "Work by Poggio, Serre, et al [20, 21, 22, 23], Dean [6, 7], discuss a hierarchical topology.", "startOffset": 29, "endOffset": 45}, {"referenceID": 16, "context": "Work by Poggio, Serre, et al [20, 21, 22, 23], Dean [6, 7], discuss a hierarchical topology.", "startOffset": 29, "endOffset": 45}, {"referenceID": 17, "context": "Work by Poggio, Serre, et al [20, 21, 22, 23], Dean [6, 7], discuss a hierarchical topology.", "startOffset": 29, "endOffset": 45}, {"referenceID": 4, "context": "Work by Poggio, Serre, et al [20, 21, 22, 23], Dean [6, 7], discuss a hierarchical topology.", "startOffset": 52, "endOffset": 58}, {"referenceID": 13, "context": "Work on modeling early stages of sensory processing by Olshausen [19, 1] using sparse coding produced results that account for the observed receptive fields in early visual processing.", "startOffset": 65, "endOffset": 72}, {"referenceID": 0, "context": "Work on modeling early stages of sensory processing by Olshausen [19, 1] using sparse coding produced results that account for the observed receptive fields in early visual processing.", "startOffset": 65, "endOffset": 72}, {"referenceID": 14, "context": "It is basically a hierarchical succession of template matching and a max-operations, corresponding to simple and complex cells respectively [20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 1, "context": "Bouvrie et al [3, 2] introduced a generalization of hierarchical architectures centered around a foundational element involving two steps, Filtering and Pooling.", "startOffset": 14, "endOffset": 20}, {"referenceID": 5, "context": "Friston proposed Hierarchical Dynamic Models (HDM) which are similar to the above mentioned architectures but framed in a control theoretic framework operating in continuous time [8].", "startOffset": 179, "endOffset": 182}, {"referenceID": 3, "context": "Feedback processes, mediating action and attention, can be incorporated into this model, similar to work by Chikkerur et al [5, 4], and more generically to a theory by Friston [10, 9].", "startOffset": 124, "endOffset": 130}, {"referenceID": 2, "context": "Feedback processes, mediating action and attention, can be incorporated into this model, similar to work by Chikkerur et al [5, 4], and more generically to a theory by Friston [10, 9].", "startOffset": 124, "endOffset": 130}, {"referenceID": 7, "context": "Feedback processes, mediating action and attention, can be incorporated into this model, similar to work by Chikkerur et al [5, 4], and more generically to a theory by Friston [10, 9].", "startOffset": 176, "endOffset": 183}, {"referenceID": 6, "context": "Feedback processes, mediating action and attention, can be incorporated into this model, similar to work by Chikkerur et al [5, 4], and more generically to a theory by Friston [10, 9].", "startOffset": 176, "endOffset": 183}, {"referenceID": 10, "context": "Alternatively RIP matrices can be obtained using sparse random matrices [15].", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "This type of regularization is intuitive and is well motivated by neuroscience [19, 1] and the organization of complex systems.", "startOffset": 79, "endOffset": 86}, {"referenceID": 0, "context": "This type of regularization is intuitive and is well motivated by neuroscience [19, 1] and the organization of complex systems.", "startOffset": 79, "endOffset": 86}], "year": 2011, "abstractText": "This paper introduces an elemental building block which combines Dictionary Learning and Dimension Reduction (DRDL). We show how this foundational element can be used to iteratively construct a Hierarchical Sparse Representation (HSR) of a sensory stream. We compare our approach to existing models showing the generality of our simple prescription. We then perform preliminary experiments using this framework, illustrating with the example of an object recognition task using standard datasets. This work introduces the very first steps towards an integrated framework for designing and analyzing various computational tasks from learning to attention to action. The ultimate goal is building a mathematically rigorous, integrated theory of intelligence.", "creator": "LaTeX with hyperref package"}}}