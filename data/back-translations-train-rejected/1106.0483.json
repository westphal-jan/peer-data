{"id": "1106.0483", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2011", "title": "Learning unbelievable marginal probabilities", "abstract": "Loopy belief propagation performs approximate inference on graphical models with loops. One might hope to compensate for the approximation by adjusting model parameters. Learning algorithms for this purpose have been explored previously, and the claim has been made that every set of locally consistent marginals can arise from belief propagation run on a graphical model. On the contrary, here we show that many probability distributions have marginals that cannot be reached by belief propagation using any set of model parameters or any learning algorithm. We call such marginals `unbelievable.' This problem occurs whenever the Hessian of the Bethe free energy is not positive-definite at the target marginals. All learning algorithms for belief propagation necessarily fail in these cases, producing beliefs or sets of beliefs that may even be worse than the pre-learning approximation. We then show that averaging inaccurate beliefs, each obtained from belief propagation using model parameters perturbed about some learned mean values, can achieve the unbelievable marginals.", "histories": [["v1", "Thu, 2 Jun 2011 18:48:59 GMT  (2159kb,D)", "http://arxiv.org/abs/1106.0483v1", "10 pages, 3 figures, submitted to NIPS*2011"]], "COMMENTS": "10 pages, 3 figures, submitted to NIPS*2011", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["xaq pitkow", "yashar ahmadian", "ken d miller"], "accepted": false, "id": "1106.0483"}, "pdf": {"name": "1106.0483.pdf", "metadata": {"source": "CRF", "title": "Learning unbelievable marginal probabilities", "authors": ["Xaq Pitkow", "Yashar Ahmadian"], "emails": ["xaq@post.harvard.edu", "ya2005@columbia.edu", "ken@neurotheory.columbia.edu"], "sections": [{"heading": "1 Introduction", "text": "Calculation of marginal probabilities for a graphical model generally requires summing up over exponentially many states (taken as a whole) and is NP-hard [1]. To work around this problem, a variety of approximate methods have been used. A popular technique is faith propagation (BP), especially the sum product rule, which is a message-carrying algorithm for performing conclusions on a graphical model [2]. Although exact and efficient methods are applied to trees, it is only an approximation when applied to graphical models with loopholes. However, a natural question is whether one can compensate for the inadequacies of the approximation by appropriately specifying the model parameters. In this paper, we prove that some sets of marginal values simply cannot be achieved by faith propagation. In these cases, we provide a new algorithm that can achieve much better results by using an ensemble of parameters instead of an individual instance of a probability distribution (we want to obtain an individual probability x from a given set of probabilities)."}, {"heading": "2 Learning in Belief Propagation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Belief propagation", "text": "The summary product algorithm for the propagation of belief on the basis of a graphical model with energy function (2) uses the following equations [4]: mi \u2192 (xi) Vististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististististist"}, {"heading": "2.2 Bethe free energy", "text": "Despite its limitations, BP is empirically functioning well under many circumstances. Some theoretical justifications for the propagation of loopy beliefs have resulted in evidence that its stable fixed points are local minima of free energy [6, 7]. Free energies are important variables in machine learning because the Kullback-Leibler divergence between the data and model distributions can be expressed in terms of free energy, so that models can be optimized accordingly by minimizing free energies. Given an energy function E (x) from (2), the free energy of the distribution Q (x) isF [Q] = U [Q] \u2212 S [Q] \u2212 xi (5), where U is the average energy of the districts U [Q] = E (x) Q (x) Q (x) = \u2212 square energy of the free energy of the distribution Q (\u03b1), the free energy of the Gibbs distributions Q [Q] = square free, square."}, {"heading": "2.3 Pseudo-moment matching", "text": "We would now like to correct the deficiencies in the propagation of belief by determining the parameters \u03b2 \u03b2 \u03b2 q q q, so that BP produces beliefs b that correspond to the true marginals p of the target distribution p (x). Since the fixed points of BP are stationary points of F \u03b2 [6], one can simply try to find parameters that generate a stationary point in the pseudomarginal space p (x), which is a necessary condition for BP to reach a fixed point there. \u2212 Simply evaluate the gradient p, set it to zero, and solve it for it. Note that in principle this gradient could be used to directly minimize the free energy of Bethe, but F \u03b2 [q] is a complicated function of q that cannot normally be analytically minimized [8]. In contrast, we use it to solve the parameters needed to move the beliefs to a destination. This is much easier because the free energy of Bethe moment is described as \"the paramedo-learning approach.\""}, {"heading": "2.4 Unbelievable marginals", "text": "In this section, we show that the reversal is also true: There are some distributions whose marginals cannot be realized as beliefs for any kind of coupling. In these cases, existing methods of learning often lead to bad results, sometimes even worse, than not doing any learning at all. This is surprising given the assertions to the contrary: [9,5] State in which faith propagation after pseudo-moment matching can always reach a fixed point that reproduces the target marginals. While BP technically has such fixed points, they are not always stable and thus may not be attainable by doing faith propagation. Definition 1. A set of marginals is \"incredible\" if faith propagation cannot converge to them for any set of parameters. For faith propagation to converge to the goal - namely, the marginals p - a zero-gradient is not enough: free energy must also be a minimum."}, {"heading": "2.5 Bethe wake-sleep algorithm", "text": "If pseudo-moment matching is not able to reproduce incredible margins, an alternative is to use a gradient descending method for learning (\u03b2 \u03b2 \u03b2 q-points = \u03b2 \u03b2 \u03b2-points), which is painful for the wake-sleep algorithm used to train Boltzmann machines [13]. The original rule can be considered a gradient descent of the Kullback body1 Even this is not sufficient, but it is necessary. Divergence between the target P (x) and the graphical model Q (1), DKL [P | Q] = \u2211 s P (x) Q (x) Q (x) Q (x) Q (x) = F [P] \u2212 F [Q] (17), where F is the Gibbs free energy (5) using the energy function (2). Here we use a new cost function, the \"Bethe divergence\" D\u03b2 | b | b], by replacing these free energies with the free energies."}, {"heading": "2.6 Ensemble belief propagation", "text": "When the Bethe-wake-sleep algorithm attempts to learn incredible marginals, the parameters and beliefs do not reach a fixed point, but vary further over time (Figure 2A, B). However, when the learning reaches a balance, then the temporal average of the beliefs equals the incredible principle of marginality. Theorem 2. When the Bethe-wake-sleep algorithm reaches an equilibrium by definition, incredible marginalities are achieved by the propagation of beliefs fixed points averaged over the equilibrium ensemble of parameters.Evidence. In equilibrium, the time average of the parameter changes is by definition zero, < < p = 0. Substitution of the Bethe-wake-sleep equation when fixed points are averaged over the equilibrium ensemble of parameters.Proof: The time average of the parameter changes is by definition zero, < < >; p;"}, {"heading": "3 Experiments", "text": "The experiments in this section focus on the Ising model: N binary variables, s \u00b2 q = q q q = q = q q q, with factors containing individual variables xi and pairs xi, xj. The energy function is E (x) = \u2212 sp. i hixi \u2212 \u2211 (ij) Jijxixj. Then the sufficient statistics are the various first and second moments, xi and xixj, and the natural parameters are hi, Jij. We use this model both for the target distributions and for the model. We parameterise pseudomarginals as {q + i, q + ij}, where q + i = qi (xi = + 1) and q + ij = qij (xi = xj = 1). The remaining probabilities are linear functions of these values. Positivity constraints and local consistency constraints then appear as 0."}, {"heading": "4 Discussion", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "Acknowledgments", "text": "The authors thank Greg Wayne for helpful conversations."}], "references": [{"title": "The computational complexity of probabilistic inference using bayesian belief networks", "author": ["G Cooper"], "venue": "Artificial intelligence", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1990}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J Pearl"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F Kschischang", "B Frey", "H Loeliger"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Pattern recognition and machine learning", "author": ["C Bishop"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Graphical models, exponential families, and variational inference", "author": ["M Wainwright", "M Jordan"], "venue": "Foundations and Trends in Machine Learning", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Generalized belief propagation", "author": ["JS Yedidia", "WT Freeman", "Y Weiss"], "venue": "IN NIPS", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Stable fixed points of loopy belief propagation are minima of the Bethe free energy", "author": ["T Heskes"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Belief optimization for binary networks: A stable alternative to loopy belief propagation", "author": ["M Welling", "Y Teh"], "venue": "Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Tree-reweighted belief propagation algorithms and approximate ML estimation by pseudo-moment matching", "author": ["MJ Wainwright", "TS Jaakkola", "AS Willsky"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Approximate inference in Boltzmann machines", "author": ["M Welling", "Y Teh"], "venue": "Artificial Intelligence", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Learning in markov random fields: An empirical study", "author": ["S Parise", "M Welling"], "venue": "Joint Statistical Meeting", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Loopy belief propagation, Bethe free energy and graph zeta function", "author": ["Y Watanabe", "K Fukumizu"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Analyzing cooperative computation", "author": ["G Hinton", "T Sejnowski"], "venue": "Proceedings of the Fifth Annual Cognitive Science", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1983}, {"title": "Learning in markov random fields with contrastive free energies", "author": ["M Welling", "C Sutton"], "venue": "In Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics. Society for Artificial Intelligence and Statistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J Yedidia", "W Freeman", "Y Weiss"], "venue": "IEEE Transactions on Information Theory", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "On the properties of the Bethe approximation and loopy belief propagation on binary networks. Journal of Statistical Mechanics: Theory and Experiment : P11012", "author": ["J Mooij", "H Kappen"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Validity estimates for loopy belief propagation on binary real-world networks", "author": ["J Mooij", "H Kappen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "On the uniqueness of loopy belief propagation fixed points", "author": ["T Heskes"], "venue": "Neural Computation", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J Lafferty", "A McCallum", "F Pereira"], "venue": "Proceedings of the 18th International Conference on Machine", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Cortical circuitry implementing graphical models", "author": ["S Litvak", "S Ullman"], "venue": "Neural Computation", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Belief propagation in networks of spiking neurons", "author": ["A Steimer", "W Maass", "R Douglas"], "venue": "Neural Computation", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "The neurodynamics of belief propagation on binary markov random fields", "author": ["T Ott", "R Stoop"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Implementing belief propagation in neural circuits", "author": ["A Shon", "R Rao"], "venue": "Neurocomputing 65\u201366:", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Towards a mathematical theory of cortical micro-circuits. PLoS computational biology", "author": ["D George", "J Hawkins"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Calculating marginal probabilities for a graphical model generally requires summing over exponentially many states, and is NP-hard in general [1].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "One popular technique is belief propagation (BP), in particular the sumproduct rule, which is a message-passing algorithm for performing inference on a graphical model [2].", "startOffset": 168, "endOffset": 171}, {"referenceID": 2, "context": "Here, \u03b1 indexes sets of interacting variables (factors in the factor graph [3]), and x\u03b1 is a subset of variables whose interaction is characterized by a vector of sufficient statistics \u03c6\u03b1(x\u03b1) and corresponding natural parameters \u03b8\u03b1.", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "The sum-product algorithm for belief propagation on a graphical model with energy function (2) uses the following equations [4]: mi\u2192\u03b1(xi) \u221d \u220f", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "While they are guaranteed to be locally consistent, \u2211 x\u03b1\\xi b\u03b1(x\u03b1) = bi(xi), they are not necessarily globally consistent: There may not exist a single joint distribution B(x) of which the beliefs are the marginals [5].", "startOffset": 215, "endOffset": 218}, {"referenceID": 5, "context": "Some theoretical justification for loopy belief propagation emerged with proofs that its stable fixed points are local minima of the Bethe free energy [6, 7].", "startOffset": 151, "endOffset": 157}, {"referenceID": 6, "context": "Some theoretical justification for loopy belief propagation emerged with proofs that its stable fixed points are local minima of the Bethe free energy [6, 7].", "startOffset": 151, "endOffset": 157}, {"referenceID": 5, "context": "in which the average energy U is exact, but the true entropy S is replaced by an approximation, the Bethe entropy S , which is a sum over the factor and node entropies [6]: S [Q] = \u2211", "startOffset": 168, "endOffset": 171}, {"referenceID": 7, "context": "Nonetheless, the Bethe free energy is often close enough to the Gibbs free energy that its minima approximate the true marginals [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 5, "context": "Since stable fixed points of BP are minima of the Bethe free energy [6, 7], this helped explain why belief propagation is often so successful.", "startOffset": 68, "endOffset": 74}, {"referenceID": 6, "context": "Since stable fixed points of BP are minima of the Bethe free energy [6, 7], this helped explain why belief propagation is often so successful.", "startOffset": 68, "endOffset": 74}, {"referenceID": 4, "context": "Pseudomarginal space is the convex set [5] of all q that satisfy the positivity and local consistency constraints,", "startOffset": 39, "endOffset": 42}, {"referenceID": 5, "context": "Since the fixed points of BP are stationary points of F \u03b2 [6], one may simply try to find parameters \u03b8 that produce a stationary point in pseudomarginal space at p, which is a necessary condition for BP to reach a fixed point there.", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "Note that in principle this gradient could be used to directly minimize the Bethe free energy, but F \u03b2 [q] is a complicated function of q that usually cannot be minimized analytically [8].", "startOffset": 184, "endOffset": 187}, {"referenceID": 8, "context": "This approach to learning parameters has been described as \u2018pseudo-moment matching\u2019 [9, 10, 11].", "startOffset": 84, "endOffset": 95}, {"referenceID": 9, "context": "This approach to learning parameters has been described as \u2018pseudo-moment matching\u2019 [9, 10, 11].", "startOffset": 84, "endOffset": 95}, {"referenceID": 10, "context": "This approach to learning parameters has been described as \u2018pseudo-moment matching\u2019 [9, 10, 11].", "startOffset": 84, "endOffset": 95}, {"referenceID": 4, "context": "One example is the expectation parameters \u03b7\u03b1 = \u2211 x\u03b1 q\u03b1(x\u03b1)\u03c6\u03b1(x\u03b1) [5], giving the energy simply as U = \u2212\u03b8 \u00b7 \u03b7.", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "This is surprising in view of claims to the contrary: [9, 5] state that belief propagation run after pseudo-moment matching can always reach a fixed point that reproduces the target marginals.", "startOffset": 54, "endOffset": 60}, {"referenceID": 4, "context": "This is surprising in view of claims to the contrary: [9, 5] state that belief propagation run after pseudo-moment matching can always reach a fixed point that reproduces the target marginals.", "startOffset": 54, "endOffset": 60}, {"referenceID": 6, "context": "For belief propagation to converge to the target \u2014 namely, the marginals p \u2014 a zero gradient is not sufficient: The Bethe free energy must also be a local minimum [7].", "startOffset": 163, "endOffset": 166}, {"referenceID": 6, "context": "Stable fixed points of BP occur only at local minima of the Bethe free energy [7], and so BP cannot reproduce the marginals p for any parameters.", "startOffset": 78, "endOffset": 81}, {"referenceID": 11, "context": "Graphical models with multinomial or gaussian variables and at least two loops always have some pseudomarginals for which the Hessian is not positive definite [12].", "startOffset": 159, "endOffset": 163}, {"referenceID": 11, "context": "On the other hand, all marginals with sufficiently small correlations are believable because they are guaranteed to have a positive-definite Bethe Hessian [12].", "startOffset": 155, "endOffset": 159}, {"referenceID": 12, "context": "When pseudo-moment matching fails to reproduce unbelievable marginals, an alternative is to use a gradient descent procedure for learning, analagous to the wake-sleep algorithm used to train Boltzmann machines [13].", "startOffset": 210, "endOffset": 214}, {"referenceID": 13, "context": "Here we use a new cost function, the \u2018Bethe divergence\u2019 D\u03b2 [p||b], by replacing these free energies by Bethe free energies [14] evaluated at the true marginals p and at the beliefs b obtained from BP fixed points, D\u03b2 [p||b] = F \u03b2 [p]\u2212 F \u03b2 [b] (18) We use gradient descent to optimize this cost, with gradient dD\u03b2 d\u03b8 = \u2202D\u03b2 \u2202\u03b8 + \u2202D\u03b2 \u2202b \u2202b \u2202\u03b8 (19)", "startOffset": 123, "endOffset": 127}, {"referenceID": 7, "context": "We parameterize pseudomarginals as {q i , q ij }where q i = qi(xi = +1) and q ij = qij(xi = xj = +1) [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 14, "context": "If all the interactions are finite, then the inequality constraints are not active [15].", "startOffset": 83, "endOffset": 87}, {"referenceID": 15, "context": "Each run of BP used exponential temporal message damping of 5 time steps [16], m = am + (1 \u2212 a)mundamped with a = e\u22121/5.", "startOffset": 73, "endOffset": 77}, {"referenceID": 16, "context": "For instance, the Hessian reveals that the Ising model\u2019s paramagnetic state becomes unstable in BP for large enough couplings [17].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "For another example, when the Hessian is positive definite throughout pseudomarginal space, then the Bethe free energy is convex and thus BP has a unique fixed point [18].", "startOffset": 166, "endOffset": 170}, {"referenceID": 18, "context": "An especially clear application of eBP is to discriminative models like Conditional Random Fields [19].", "startOffset": 98, "endOffset": 102}, {"referenceID": 19, "context": "When inference is hard, neural computations may resort to approximations, perhaps including belief propagation [20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 131}, {"referenceID": 20, "context": "When inference is hard, neural computations may resort to approximations, perhaps including belief propagation [20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 131}, {"referenceID": 21, "context": "When inference is hard, neural computations may resort to approximations, perhaps including belief propagation [20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 131}, {"referenceID": 22, "context": "When inference is hard, neural computations may resort to approximations, perhaps including belief propagation [20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 131}, {"referenceID": 23, "context": "When inference is hard, neural computations may resort to approximations, perhaps including belief propagation [20, 21, 22, 23, 24].", "startOffset": 111, "endOffset": 131}], "year": 2011, "abstractText": "Loopy belief propagation performs approximate inference on graphical models with loops. One might hope to compensate for the approximation by adjusting model parameters. Learning algorithms for this purpose have been explored previously, and the claim has been made that every set of locally consistent marginals can arise from belief propagation run on a graphical model. On the contrary, here we show that many probability distributions have marginals that cannot be reached by belief propagation using any set of model parameters or any learning algorithm. We call such marginals \u2018unbelievable.\u2019 This problem occurs whenever the Hessian of the Bethe free energy is not positive-definite at the target marginals. All learning algorithms for belief propagation necessarily fail in these cases, producing beliefs or sets of beliefs that may even be worse than the pre-learning approximation. We then show that averaging inaccurate beliefs, each obtained from belief propagation using model parameters perturbed about some learned mean values, can achieve the unbelievable marginals.", "creator": "LaTeX with hyperref package"}}}