{"id": "1106.0665", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2011", "title": "Infinite-Horizon Policy-Gradient Estimation", "abstract": "Gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods. In this paper we introduce GPOMDP, a simulation-based algorithm for generating a biased estimate of the gradient of the average reward in Partially Observable Markov Decision Processes POMDPs controlled by parameterized stochastic policies. A similar algorithm was proposed by (Kimura et al. 1995). The algorithm's chief advantages are that it requires storage of only twice the number of policy parameters, uses one free beta (which has a natural interpretation in terms of bias-variance trade-off), and requires no knowledge of the underlying state. We prove convergence of GPOMDP, and show how the correct choice of the parameter beta is related to the mixing time of the controlled POMDP. We briefly describe extensions of GPOMDP to controlled Markov chains, continuous state, observation and control spaces, multiple-agents, higher-order derivatives, and a version for training stochastic policies with internal states. In a companion paper (Baxter et al., this volume) we show how the gradient estimates generated by GPOMDP can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure to find local optima of the average reward.", "histories": [["v1", "Fri, 3 Jun 2011 14:52:01 GMT  (119kb)", "http://arxiv.org/abs/1106.0665v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["p l bartlett", "j baxter"], "accepted": false, "id": "1106.0665"}, "pdf": {"name": "1106.0665.pdf", "metadata": {"source": "CRF", "title": "Infinite-Horizon Policy-Gradient Estimation", "authors": ["Jonathan Baxter", "Peter L. Bartlett"], "emails": ["JBAXTER@WHIZBANG.COM", "BARTLETT@BARNHILLTECHNOLOGIES.COM"], "sections": [{"heading": null, "text": "In this paper we present GPOMDP, a simulation-based algorithm for generating a distorted estimate of the gradient of average reward in partially observable Markov decision processes (POMDPs) controlled by parametrized stochastic strategies. A similar algorithm was proposed by Kimura, Yamamura and Kobayashi (1995) and does not require knowledge of the underlying state. The main advantage of the algorithm is that it stores only twice the number of political parameters, uses a free parameter 2 [0; 1) (which has a natural interpretation regarding bias-variance-trade-off) and does not require knowledge of the underlying state. We demonstrate the convergence of GPOMDP and show how the correct choice of the parameter is related to the mixing time of the controlled POMDP."}, {"heading": "1. Introduction", "text": "Dynamic programming is the method of choice for solving problems of decision-making under uncertainty (Bertsekas, 1995). However, the application of dynamic programming becomes problematic in large or infinite state spaces, in situations where system dynamics are unknown, or when the state is only partially observed. In such cases, one looks for approximate techniques based on simulation and for parametric representations concerning either value creation or politics."}, {"heading": "1.1 A Brief History of Policy-Gradient Algorithms", "text": "In the case of large-scale problems or problems where the system dynamics are unknown, the performance gap becomes q q q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q) q (q) q (q) q (q) q (q) q (q) q (q) q (q) q) q (q) q) q q (q) q (q) q) q (q) q (q) q) q (q) q) q (q) q (q) q) q (q) q) q (q) q) q (q) q) c (c c) q (c) q c) q (c c) q (c) q c) q (c) q (c) q (c) q (c) q (c) q) q (c) q (c) q), q (c) q (c) q), q (c), q (c) q (c) q) q (c) q (c) q (c) q), q (c) q (c), q (c) q (c) q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q (c), q"}, {"heading": "1.1.1 UNBIASED ESTIMATES OF THE PERFORMANCE GRADIENT FOR REGENERATIVE PROCESSES", "text": "Extensions of the probelihood-ratio method to regenerative processes (including Markov Decision Processes = 1 average) = 1 period (1983) were given by Glynn (1986, 1990), Glynn and L'Ecuyer (1995) and Reiman and Weiss (1986, 1989), and indeward for episodic Partially Observable Markov Decision Processes (POMDPs) reached by Williams (1992), who brought the REINFORCE algorithm2. In this case, the i.i.d. examples X of the previous section are sequences of states X0;:; XT (random length) encountered between visits of some named recursive states i, or sequences of states from some initial state. rq (; X) = q (; X) can be written as sumrq (; X)."}, {"heading": "1.1.2 BIASED ESTIMATES OF THE PERFORMANCE GRADIENT", "text": "It seems difficult to obtain unbiased assessments of gradients without access to a particular state."}, {"heading": "1.2 Our Contribution", "text": "We describe GPOMDP, a general algorithm based on (11) for generating a distorted estimate of the power gradient r () in general POMDP, controlled by parameterized stochastic guidelines. Here () denotes the average policy reward with parameters 2 RK. GPOMDP is not based on access to an underlying recurring state. Writing r () for the expectation of the estimate produced by GPOMDP, we show that Limm! 1r () = r () and quantitative that r () is close to the true gradient, which is 1 = (1) exceeds the mixing time of the Markov chain induced by the POMDP6. As shown in the abridged estimate above, the target conflict prevents the setting arbitrarily close to 1 from increasing the variance of the algorithm's algorithm with the parameter values."}, {"heading": "2. The Reinforcement Learning Problem", "text": "We model amplification of learning as Markov decision process (MDP) with a finite state spaceS = f1;::; ng, and a stochastic matrix 7 P = [pij give the probability of transition from statei to state j. each state i has an associated reward 8 r (i). The matrix P belongs to a parameterized class of stochastic matrices, P: = fP (): 2 RK g. Denote the Markov chain corresponds toP () of M (). We assume that these Markov chains and rewards fulfill the following assumptions: Assumption 1. Each P () 2 P has a unique stationary distribution ();:; (; n) 0 Satisfaction of the balance equations 0 () P ()."}, {"heading": "3. Computing the Gradient of the Average Reward", "text": "In fact, it is that it is a matter of a way in which it comes to a satisfaction, in which it comes to a satisfaction. () It is not that it comes to a satisfaction. () It is that it comes to a satisfaction. () It is not that it comes to a satisfaction. () It is that it comes to a satisfaction. () It is that it comes to a satisfaction. () It is that it comes to a satisfaction. () It is that it comes to a satisfaction. () It is that it comes to a satisfaction. () It is that it comes to a satisfaction. () It is that it comes to a satisfaction. () It is that it comes to a satisfaction. () It is that it comes to a satisfaction."}, {"heading": "4. Approximating the Gradient in Parameterized Markov Chains", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "5. Estimating the Gradient in Parameterized Markov Chains", "text": "Algorithm 1 introduces MCG (Markov Chain Gradient), an algorithm for estimating the approximate gradient r from a single online example path X0; X1;: from the Markov chain M ().MCG requires only 2K reals to be stored, where K is the dimension of the parameter space (Xt): K parameters for the calculation paths zt, and K parameters for the gradient estimation t. Note that after T time steps T the average of r (Xt) zt, T = 1T 1Xzt T 1Xzt = 0 ztr stationary (Xt): Algorithm 1 The MCG (Markov Chain Gradient) algorithm1: Given: Parameter 2 RK. Parameterized class of stochastic matrices P = fP (): 2 RK g Satisfaction Assumptions 3 and 1. 2 [0]."}, {"heading": "6. Estimating the Gradient in Partially Observable Markov Decision Processes", "text": "In this section, we will consider the specific case of P (), which starts from a parameterized class of randomized policies that control a partially observable Markov decision-making process (POMDP). The \"partially observable\" qualification means that we assume that these policies have access to an observation process that depends on the state, but in general they cannot see the state. Specifically, we assume that there are N controls U = f1;:; Ng and M observations Y = f1;:;:; Mg. Each u 2 U determines a stochastic matrix P (u) that does not depend on the parameters. For each state i 2 S, an observation Y 2 Y is generated independently of a probability distribution (i) over observations in Y."}, {"heading": "6.1 Control dependent rewards", "text": "There are many circumstances in which the rewards themselves depend on the controls u. For example, some controls can consume more energy than others, so we may want to add a penalty term to the reward function to save energy. The easiest way to deal with this is to define for each state i the expected reward r (i) by r (i) = EY (i) EU (; Y) r (U; i); (36) and then redefine J in relation to r: J (; i): = limN! 1E \"NXt = 0 t r (Xt) X0 = i #; (37), where the expectation for all trajectories X0; X1;::. The performance differential is then r = 0 r + 0r r; which can be approximated by r = 0 rP J + r; due to the fact that J replaces the Bellman equations (20) with r."}, {"heading": "6.2 Parameter dependent rewards", "text": "It is possible to modify the GPOMDP if the rewards themselves depend directly on it. In this case, the fifth line of the GPOMDP is implemented by t + 1 = t + 1t + 1 [r (; Xt + 1) zt + 1 + rr (; Xt + 1) t ': (38) Again, the convergence and approximation theories are implemented if rr (; i) is uniformly limited. Parameter-dependent rewards have been considered by Glynn (1990), Marbach and Tsitsiklis (1998), as well as Baird and Moore (1999). In particular, Baird and Moore (1999) showed how appropriate decisions by r (; i) lead to a combination of value and policy search or \"VAPS.\" For example, if ~ J (; i) is an approximate value function, then the policy of Xr (; Xt; Xt; Xt 1) will lead to a certain value."}, {"heading": "6.3 Extensions to infinite state, observation, and control spaces", "text": "The proof of convergence for algorithm 2 is based on finite state (S), observation (Y) and control (U) spaces. However, it should be clear that without modification algorithm 2 can immediately be applied to POMDPs with payable or uncountable infinite S and Y (U). All this changes if U is a subset of RN, then (y;) becomes a probability function on U (y;) becomes the density on u (y). If U and Y are subsets of euclidean space (but S is a finite proposition), theorem 5 can be extended to show that the estimates generated by this algorithm are almost certainly r."}, {"heading": "7. New Results", "text": "Since the first version of this paper, we have added some new settings to GPOMDP and also demonstrated some new properties of the algorithm. In this section, we will briefly outline these results."}, {"heading": "7.1 Multiple Agents", "text": "Instead of a single agent generating actions according to (; y), we assume that we have several agents = 1;:::; na, who all receive the same reward signal r (Xt) (they can, for example, cooperate to solve the same task), and who generate their own actions according to a policy ui (i; yi), then GPOMDP can be applied to the collective POMDP obtained by concatenating the observations, controls and parameters to individual vectors y = y1;::; yna, u = u1;::; una or = 1;:; na. A simple calculation shows that the gradient estimate generated by GPOMDP in the collective case is exactly the same as that obtained by applying GPOMDP to each agent independently."}, {"heading": "7.2 Policies with internal states", "text": "So far, we have only reactive or memory-less policies where the measures mentioned are only a function of current observation. GPOMDP is easily able to cover the case of policies based on endless observations."}, {"heading": "7.4 Bias and Variance Bounds", "text": "Theorem 3 provides a limit on the bias of r () in relation to r (), which applies when the underlying Markov chain has different eigenvalues. We have extended this result to any Markov chain (Bartlett & Baxter, 2001). However, the additional generality comes at a price, since the latter limit includes the number of states in the chain, whereas Theorem 3 does not. The same paper also provides evidence that the deviation of the GPOMDP scale is 1 = (1) 2, which is a formal justification for the interpretation of bias / variance trading."}, {"heading": "8. Conclusion", "text": "We presented a general algorithm (MCG) for calculating arbitrarily precise approximations to the gradient of the average reward in a parameterized Markov chain. If the transition matrix of the chain has unique eigenvalues, it was shown that the accuracy of the approximation is controlled by the size of the subdominant eigenvalue j 2j. We showed how the algorithm could be modified to apply to partially observable Markov decision processes controlled by parameterized stochastic strategies, with discrete and continuous control, observation and state spaces (GPOMDP). For the finite state case, we demonstrated convergence with probability 1 of both algorithms. We briefly described extensions to multi-agent problems, internal state strategies, derivatives of higher order, generalizations of bias on chains with ambiguous eigenvalues, and a new result of variance. There are many avenues for further research outcomes to follow as the MDP results presented here."}, {"heading": "Acknowledgements", "text": "This work was supported by the Australian Research Council and benefited from the comments of several anonymous speakers. Most of these studies were conducted while the authors were at the Research School of Information Sciences and Engineering at the Australian National University."}, {"heading": "Appendix A. A Simple Example of Policy Degradation in Value-Function Learning", "text": "It has long been known that this does not necessarily lead to improved political performance from the new value function. We include this appendix because it shows that this phenomenon can occur in the simplest possible system, a two-state MDP, and also provides some geometric intuitions for which the phenomenon arises. Consider the Two-State Markov Decision Process (MDP) in Figure 1. There are two Controlsu1; u2 with the corresponding transition probability MatricesP (u1) = 13 2313; P (u2) = 23 1323 13; so that u1 always takes the system to state 2 with probability 2 = 3, regardless of the initial state (and therefore to state 1 with probability 1), and u2 does the opposite. Since state 2 has a reward of 0, the optimal policy is always a reward of 0."}, {"heading": "Appendix B. Proof of Theorem 6", "text": "The proof needs the following topological problem. For definitions see, for example, (Dudley, 1989, pp. 24-25).Lemma 7. Let (X; T) be a topological space that is Hausdorff, separable, and first-countable. Let B be the Borel -algebra generated by T: then the measuable space (X; B) has a sequenceS1;::: B of sets that is a true conditions: 1. Each Si is a partition of X (that is, X = SfS 2 Sig and any distinct elements of Si have empty intersection).2 For all x 2 X, fxg 2 B and 1\\ i = 1fS 2 Si: x 2 Sg: Proof. Since X is separable, it has a countable dense subset S = fx1; x2;: There is X."}], "references": [{"title": "Policy-gradient learning of controllers with internal state", "author": ["D. Aberdeen", "J. Baxter"], "venue": null, "citeRegEx": "Aberdeen and Baxter,? \\Q2001\\E", "shortCiteRegEx": "Aberdeen and Baxter", "year": 2001}, {"title": "Gradient descent for general reinforcement learning", "author": ["L. Baird", "A. Moore"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Baird and Moore,? \\Q1999\\E", "shortCiteRegEx": "Baird and Moore", "year": 1999}, {"title": "Hebbian synaptic modifications in spiking neurons that learn", "author": ["P.L. Bartlett", "J. Baxter"], "venue": "Tech. rep., Research School of Information Sciences and Engineering,", "citeRegEx": "Bartlett and Baxter,? \\Q1999\\E", "shortCiteRegEx": "Bartlett and Baxter", "year": 1999}, {"title": "Estimation and approximation bounds for gradient-based reinforcement learning", "author": ["P.L. Bartlett", "J. Baxter"], "venue": "Journal of Computer and Systems Sciences,", "citeRegEx": "Bartlett and Baxter,? \\Q2001\\E", "shortCiteRegEx": "Bartlett and Baxter", "year": 2001}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A.G. Barto", "R.S. Sutton", "C.W. Anderson"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Barto et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1983}, {"title": "Experiments with infinite-horizon, policy-gradient estimation", "author": ["J. Baxter", "P.L. Bartlett", "L. Weaver"], "venue": "Journal of Artificial Intelligence Research. To appear", "citeRegEx": "Baxter et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Baxter et al\\.", "year": 2001}, {"title": "Learning to play chess using temporal-differences", "author": ["J. Baxter", "A. Tridgell", "L. Weaver"], "venue": "Machine Learning,", "citeRegEx": "Baxter et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Baxter et al\\.", "year": 2000}, {"title": "Dynamic Programming and Optimal Control, Vol II", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific.", "citeRegEx": "Bertsekas,? 1995", "shortCiteRegEx": "Bertsekas", "year": 1995}, {"title": "Probability", "author": ["L. Breiman"], "venue": "Addison-Wesley.", "citeRegEx": "Breiman,? 1966", "shortCiteRegEx": "Breiman", "year": 1966}, {"title": "Algorithms for Sensitivity Analysis of Markov Chains Through Potentials and Perturbation Realization", "author": ["Cao", "X.-R", "Wan", "Y.-W"], "venue": "IEEE Transactions on Control Systems Technology,", "citeRegEx": "Cao et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Cao et al\\.", "year": 1998}, {"title": "Real Analysis and Probability", "author": ["R.M. Dudley"], "venue": "Wadsworth & Brooks/Cole, Belmont, California.", "citeRegEx": "Dudley,? 1989", "shortCiteRegEx": "Dudley", "year": 1989}, {"title": "Stochastic approximation for monte-carlo optimization", "author": ["P.W. Glynn"], "venue": "Proceedings of the 1986 Winter Simulation Conference, pp. 356\u2013365.", "citeRegEx": "Glynn,? 1986", "shortCiteRegEx": "Glynn", "year": 1986}, {"title": "Likelihood ratio gradient estimation for stochastic systems", "author": ["P.W. Glynn"], "venue": "Communications of the ACM, 33, 75\u201384.", "citeRegEx": "Glynn,? 1990", "shortCiteRegEx": "Glynn", "year": 1990}, {"title": "Likelihood ratio gradient estimation for regenerative stochastic recursions", "author": ["P.W. Glynn", "P. L\u2018Ecuyer"], "venue": "Advances in Applied Probability,", "citeRegEx": "Glynn and L.Ecuyer,? \\Q1995\\E", "shortCiteRegEx": "Glynn and L.Ecuyer", "year": 1995}, {"title": "Perturbation Analysis of Discrete Event Dynamic Systems", "author": ["Ho", "Y.-C", "Cao", "X.-R"], "venue": null, "citeRegEx": "Ho et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Ho et al\\.", "year": 1991}, {"title": "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems", "author": ["T. Jaakkola", "S.P. Singh", "M.I. Jordan"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Jaakkola et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1995}, {"title": "An analysis of actor/critic algorithms using eligibility traces: Reinforcement learning with imperfect value functions", "author": ["H. Kimura", "S. Kobayashi"], "venue": "In Fifteenth International Conference on Machine Learning,", "citeRegEx": "Kimura and Kobayashi,? \\Q1998\\E", "shortCiteRegEx": "Kimura and Kobayashi", "year": 1998}, {"title": "Reinforcement learning for continuous action using stochastic gradient ascent", "author": ["H. Kimura", "S. Kobayashi"], "venue": "In Intelligent Autonomous Systems", "citeRegEx": "Kimura and Kobayashi,? \\Q1998\\E", "shortCiteRegEx": "Kimura and Kobayashi", "year": 1998}, {"title": "Reinforcement learning in POMDPs with function approximation", "author": ["H. Kimura", "K. Miyazaki", "S. Kobayashi"], "venue": "Proceedings of the Fourteenth International Conference on Machine Learning", "citeRegEx": "Kimura et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kimura et al\\.", "year": 1997}, {"title": "Reinforcement learning by stochastic hill climbing on discounted reward", "author": ["H. Kimura", "M. Yamamura", "S. Kobayashi"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning", "citeRegEx": "Kimura et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kimura et al\\.", "year": 1995}, {"title": "Actor-Critic Algorithms", "author": ["V.R. Konda", "J.N. Tsitsiklis"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Konda and Tsitsiklis,? \\Q2000\\E", "shortCiteRegEx": "Konda and Tsitsiklis", "year": 2000}, {"title": "The Theory of Matrices", "author": ["P. Lancaster", "M. Tismenetsky"], "venue": null, "citeRegEx": "Lancaster and Tismenetsky,? \\Q1985\\E", "shortCiteRegEx": "Lancaster and Tismenetsky", "year": 1985}, {"title": "Simulation-Based Optimization of Markov Reward Processes", "author": ["P. Marbach", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "Marbach and Tsitsiklis,? \\Q1998\\E", "shortCiteRegEx": "Marbach and Tsitsiklis", "year": 1998}, {"title": "Off-policy policy search", "author": ["N. Meuleau", "L. Peshkin", "L.P. Kaelbling", "Kim", "K.-E"], "venue": "Tech. rep.,", "citeRegEx": "Meuleau et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 2000}, {"title": "Learning finite-state controllers for partially observable environments", "author": ["N. Meuleau", "L. Peshkin", "Kim", "K.-E", "L.P. Kaelbling"], "venue": "In Proceedings of the Fifteenth International Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Meuleau et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 1999}, {"title": "Learning to cooperate via policy search", "author": ["L. Peshkin", "Kim", "K.-E", "N. Meuleau", "L.P. Kaelbling"], "venue": "In Proceedings of the Sixteenth International Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Peshkin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Peshkin et al\\.", "year": 2000}, {"title": "Sensitivity analysis via likelihood ratios", "author": ["M.I. Reiman", "A. Weiss"], "venue": "In Proceedings of the 1986 Winter Simulation Conference", "citeRegEx": "Reiman and Weiss,? \\Q1986\\E", "shortCiteRegEx": "Reiman and Weiss", "year": 1986}, {"title": "Sensitivity analysis for simulations via likelihood ratios", "author": ["M.I. Reiman", "A. Weiss"], "venue": "Operations Research,", "citeRegEx": "Reiman and Weiss,? \\Q1989\\E", "shortCiteRegEx": "Reiman and Weiss", "year": 1989}, {"title": "Some Problems in Monte Carlo Optimization", "author": ["R.Y. Rubinstein"], "venue": "Ph.D. thesis.", "citeRegEx": "Rubinstein,? 1969", "shortCiteRegEx": "Rubinstein", "year": 1969}, {"title": "How to optimize complex stochastic systems from a single sample path by the score function method", "author": ["R.Y. Rubinstein"], "venue": "Annals of Operations Research, 27, 175\u2013211.", "citeRegEx": "Rubinstein,? 1991", "shortCiteRegEx": "Rubinstein", "year": 1991}, {"title": "Decomposable score function estimators for sensitivity analysis and optimization of queueing networks", "author": ["R.Y. Rubinstein"], "venue": "Annals of Operations Research, 39, 195\u2013229.", "citeRegEx": "Rubinstein,? 1992", "shortCiteRegEx": "Rubinstein", "year": 1992}, {"title": "Some Studies in Machine Learning Using the Game of Checkers", "author": ["A.L. Samuel"], "venue": "IBM Journal of Research and Development, 3, 210\u2013229.", "citeRegEx": "Samuel,? 1959", "shortCiteRegEx": "Samuel", "year": 1959}, {"title": "Integral, Measure and Derivative: A Unified Approach", "author": ["G.E. Shilov", "B.L. Gurevich"], "venue": null, "citeRegEx": "Shilov and Gurevich,? \\Q1966\\E", "shortCiteRegEx": "Shilov and Gurevich", "year": 1966}, {"title": "Learning Without State-Estimation in Partially Observable Markovian Decision Processes", "author": ["S.P. Singh", "T. Jaakkola", "M.I. Jordan"], "venue": "In Proceedings of the Eleventh International Conference on Machine Learning", "citeRegEx": "Singh et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1994}, {"title": "Reinforcement learning for dynamic channel allocation in cellular telephone systems", "author": ["S. Singh", "D. Bertsekas"], "venue": "In Advances in Neural Information Processing Systems: Proceedings of the 1996 Conference,", "citeRegEx": "Singh and Bertsekas,? \\Q1997\\E", "shortCiteRegEx": "Singh and Bertsekas", "year": 1997}, {"title": "The optimal control of partially observable Markov decision processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "The optimal control of partially observable Markov decision processes over the infinite horizon: Discounted costs", "author": ["E.J. Sondik"], "venue": "Operations Research, 26.", "citeRegEx": "Sondik,? 1978", "shortCiteRegEx": "Sondik", "year": 1978}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation", "author": ["R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "A multi-agent, policy-gradient approach to network routing", "author": ["N. Tao", "J. Baxter", "L. Weaver"], "venue": null, "citeRegEx": "Tao et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Tao et al\\.", "year": 2001}, {"title": "Practical Issues in Temporal Difference Learning", "author": ["G. Tesauro"], "venue": "Machine Learning, 8, 257\u2013 278.", "citeRegEx": "Tesauro,? 1992", "shortCiteRegEx": "Tesauro", "year": 1992}, {"title": "TD-Gammon, a self-teaching backgammon program, achieves master-level play", "author": ["G. Tesauro"], "venue": "Neural Computation, 6, 215\u2013219.", "citeRegEx": "Tesauro,? 1994", "shortCiteRegEx": "Tesauro", "year": 1994}, {"title": "An Analysis of Temporal Difference Learning with Function Approximation", "author": ["J.N. Tsitsikilis", "B. Van-Roy"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Tsitsikilis and Van.Roy,? \\Q1997\\E", "shortCiteRegEx": "Tsitsikilis and Van.Roy", "year": 1997}, {"title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning", "author": ["R.J. Williams"], "venue": "Machine Learning, 8, 229\u2013256.", "citeRegEx": "Williams,? 1992", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "A reinforcement learning approach to job-shop scheduling", "author": ["W. Zhang", "T. Dietterich"], "venue": "In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Zhang and Dietterich,? \\Q1995\\E", "shortCiteRegEx": "Zhang and Dietterich", "year": 1995}], "referenceMentions": [{"referenceID": 7, "context": "Introduction Dynamic Programming is the method of choice for solving problems of decision making under uncertainty (Bertsekas, 1995).", "startOffset": 115, "endOffset": 132}, {"referenceID": 31, "context": "This approach has yielded some remarkable empirical successes in a number of different domains, including learning to play checkers (Samuel, 1959), backgammon (Tesauro, 1992, 1994), and chess (Baxter, Tridgell, & Weaver, 2000), job-shop scheduling (Zhang & Dietterich, 1995) and dynamic channel allocation (Singh & Bertsekas, 1997).", "startOffset": 132, "endOffset": 146}, {"referenceID": 28, "context": "The technique is called the score function or likelihood ratio method and appears to have been first proposed in the sixties (Aleksandrov, Sysoyev, & Shemeneva, 1968; Rubinstein, 1969) for computing performance gradients in i.", "startOffset": 125, "endOffset": 184}, {"referenceID": 11, "context": "1 UNBIASED ESTIMATES OF THE PERFORMANCE GRADIENT FOR REGENERATIVE PROCESSES Extensions of the likelihood-ratio method to regenerative processes (including Markov Decision Processes or MDPs) were given by Glynn (1986, 1990), Glynn and L\u2018Ecuyer (1995) and Reiman and Weiss (1986, 1989), and independently for episodic Partially Observable Markov Decision Processes (POMDPs) by Williams (1992), who introduced the REINFORCE algorithm2.", "startOffset": 204, "endOffset": 250}, {"referenceID": 11, "context": "1 UNBIASED ESTIMATES OF THE PERFORMANCE GRADIENT FOR REGENERATIVE PROCESSES Extensions of the likelihood-ratio method to regenerative processes (including Markov Decision Processes or MDPs) were given by Glynn (1986, 1990), Glynn and L\u2018Ecuyer (1995) and Reiman and Weiss (1986, 1989), and independently for episodic Partially Observable Markov Decision Processes (POMDPs) by Williams (1992), who introduced the REINFORCE algorithm2.", "startOffset": 204, "endOffset": 391}, {"referenceID": 4, "context": "This terminology is used in Barto et al. (1983).", "startOffset": 28, "endOffset": 48}, {"referenceID": 43, "context": "As Williams (1992) pointed out, a further simplification is possible in the case that rT = r(X0; : : : ;XT ) is a sum of scalar rewards r(Xt; t) depending on the state and possibly the time t since the starting state (such as r(Xt; t) = r(Xt), or r(Xt; t) = tr(Xt) as above).", "startOffset": 3, "endOffset": 19}, {"referenceID": 25, "context": "1 and the variants above have been extended to cover multiple agents (Peshkin et al., 2000), policies with internal state (Meuleau et al.", "startOffset": 69, "endOffset": 91}, {"referenceID": 24, "context": ", 2000), policies with internal state (Meuleau et al., 1999), and importance sampling methods (Meuleau et al.", "startOffset": 38, "endOffset": 60}, {"referenceID": 23, "context": ", 1999), and importance sampling methods (Meuleau et al., 2000).", "startOffset": 41, "endOffset": 63}, {"referenceID": 16, "context": "Marbach and Tsitsiklis (1998) provide the only convergence proof that we know of, albeit for a slightly different update of the form t+1 = t + t [r(Xt; s) \u0011\u0302( t)\u2104 zt, where \u0011\u0302( t) is a moving estimate of the expected performance, and is also updated on-line (this update was first suggested in the context of POMDPs by Jaakkola et al.", "startOffset": 0, "endOffset": 30}, {"referenceID": 13, "context": "Marbach and Tsitsiklis (1998) provide the only convergence proof that we know of, albeit for a slightly different update of the form t+1 = t + t [r(Xt; s) \u0011\u0302( t)\u2104 zt, where \u0011\u0302( t) is a moving estimate of the expected performance, and is also updated on-line (this update was first suggested in the context of POMDPs by Jaakkola et al. (1995)).", "startOffset": 319, "endOffset": 342}, {"referenceID": 13, "context": "Marbach and Tsitsiklis (1998) provide the only convergence proof that we know of, albeit for a slightly different update of the form t+1 = t + t [r(Xt; s) \u0011\u0302( t)\u2104 zt, where \u0011\u0302( t) is a moving estimate of the expected performance, and is also updated on-line (this update was first suggested in the context of POMDPs by Jaakkola et al. (1995)). Marbach and Tsitsiklis (1998) also considered the case of -dependent rewards (recall the discussion after (3)), as did Baird and Moore (1999) with their \u201cVAPS\u201d algorithm (Value And Policy Search).", "startOffset": 319, "endOffset": 374}, {"referenceID": 1, "context": "Marbach and Tsitsiklis (1998) also considered the case of -dependent rewards (recall the discussion after (3)), as did Baird and Moore (1999) with their \u201cVAPS\u201d algorithm (Value And Policy Search).", "startOffset": 119, "endOffset": 142}, {"referenceID": 1, "context": "Marbach and Tsitsiklis (1998) also considered the case of -dependent rewards (recall the discussion after (3)), as did Baird and Moore (1999) with their \u201cVAPS\u201d algorithm (Value And Policy Search). This last paper contains an interesting insight: through suitable choices of the performance function r(X0; : : : ;XT ; ), one can combine policy-gradient search with approximate value function methods. The resulting algorithms can be viewed as actor-critic techniques in the spirit of Barto et al. (1983); the policy is the actor and the value function is the critic.", "startOffset": 119, "endOffset": 503}, {"referenceID": 1, "context": "Marbach and Tsitsiklis (1998) also considered the case of -dependent rewards (recall the discussion after (3)), as did Baird and Moore (1999) with their \u201cVAPS\u201d algorithm (Value And Policy Search). This last paper contains an interesting insight: through suitable choices of the performance function r(X0; : : : ;XT ; ), one can combine policy-gradient search with approximate value function methods. The resulting algorithms can be viewed as actor-critic techniques in the spirit of Barto et al. (1983); the policy is the actor and the value function is the critic. The primary motivation is to reduce variance in the policy-gradient estimates. Experimental evidence for this phenomenon has been presented by a number of authors, including Barto et al. (1983), Kimura and Kobayashi (1998a), and Baird and Moore (1999).", "startOffset": 119, "endOffset": 760}, {"referenceID": 1, "context": "Marbach and Tsitsiklis (1998) also considered the case of -dependent rewards (recall the discussion after (3)), as did Baird and Moore (1999) with their \u201cVAPS\u201d algorithm (Value And Policy Search). This last paper contains an interesting insight: through suitable choices of the performance function r(X0; : : : ;XT ; ), one can combine policy-gradient search with approximate value function methods. The resulting algorithms can be viewed as actor-critic techniques in the spirit of Barto et al. (1983); the policy is the actor and the value function is the critic. The primary motivation is to reduce variance in the policy-gradient estimates. Experimental evidence for this phenomenon has been presented by a number of authors, including Barto et al. (1983), Kimura and Kobayashi (1998a), and Baird and Moore (1999).", "startOffset": 119, "endOffset": 790}, {"referenceID": 1, "context": "Marbach and Tsitsiklis (1998) also considered the case of -dependent rewards (recall the discussion after (3)), as did Baird and Moore (1999) with their \u201cVAPS\u201d algorithm (Value And Policy Search). This last paper contains an interesting insight: through suitable choices of the performance function r(X0; : : : ;XT ; ), one can combine policy-gradient search with approximate value function methods. The resulting algorithms can be viewed as actor-critic techniques in the spirit of Barto et al. (1983); the policy is the actor and the value function is the critic. The primary motivation is to reduce variance in the policy-gradient estimates. Experimental evidence for this phenomenon has been presented by a number of authors, including Barto et al. (1983), Kimura and Kobayashi (1998a), and Baird and Moore (1999). More recent work on this subject includes that of Sutton et al.", "startOffset": 119, "endOffset": 818}, {"referenceID": 1, "context": "Marbach and Tsitsiklis (1998) also considered the case of -dependent rewards (recall the discussion after (3)), as did Baird and Moore (1999) with their \u201cVAPS\u201d algorithm (Value And Policy Search). This last paper contains an interesting insight: through suitable choices of the performance function r(X0; : : : ;XT ; ), one can combine policy-gradient search with approximate value function methods. The resulting algorithms can be viewed as actor-critic techniques in the spirit of Barto et al. (1983); the policy is the actor and the value function is the critic. The primary motivation is to reduce variance in the policy-gradient estimates. Experimental evidence for this phenomenon has been presented by a number of authors, including Barto et al. (1983), Kimura and Kobayashi (1998a), and Baird and Moore (1999). More recent work on this subject includes that of Sutton et al. (2000) and Konda and Tsitsiklis (2000).", "startOffset": 119, "endOffset": 890}, {"referenceID": 1, "context": "Marbach and Tsitsiklis (1998) also considered the case of -dependent rewards (recall the discussion after (3)), as did Baird and Moore (1999) with their \u201cVAPS\u201d algorithm (Value And Policy Search). This last paper contains an interesting insight: through suitable choices of the performance function r(X0; : : : ;XT ; ), one can combine policy-gradient search with approximate value function methods. The resulting algorithms can be viewed as actor-critic techniques in the spirit of Barto et al. (1983); the policy is the actor and the value function is the critic. The primary motivation is to reduce variance in the policy-gradient estimates. Experimental evidence for this phenomenon has been presented by a number of authors, including Barto et al. (1983), Kimura and Kobayashi (1998a), and Baird and Moore (1999). More recent work on this subject includes that of Sutton et al. (2000) and Konda and Tsitsiklis (2000). We discuss the use of VAPS-style updates further in Section 6.", "startOffset": 119, "endOffset": 922}, {"referenceID": 1, "context": "Marbach and Tsitsiklis (1998) also considered the case of -dependent rewards (recall the discussion after (3)), as did Baird and Moore (1999) with their \u201cVAPS\u201d algorithm (Value And Policy Search). This last paper contains an interesting insight: through suitable choices of the performance function r(X0; : : : ;XT ; ), one can combine policy-gradient search with approximate value function methods. The resulting algorithms can be viewed as actor-critic techniques in the spirit of Barto et al. (1983); the policy is the actor and the value function is the critic. The primary motivation is to reduce variance in the policy-gradient estimates. Experimental evidence for this phenomenon has been presented by a number of authors, including Barto et al. (1983), Kimura and Kobayashi (1998a), and Baird and Moore (1999). More recent work on this subject includes that of Sutton et al. (2000) and Konda and Tsitsiklis (2000). We discuss the use of VAPS-style updates further in Section 6.2. So far we have not addressed the question of how the parameterized state-transition probabilities pXtXt+1( ) arise. Of course, they could simply be generated by parameterizing the matrix of transition probabilities directly. Alternatively, in the case of MDPs or POMDPs, state transitions are typically generated by feeding an observation Yt that depends stochastically on the state Xt into a parameterized stochastic policy, which selects a control Ut at random from a set of available controls (approximate value-function based approaches that generate controls stochastically via some form of lookahead also fall into this category). The distribution over successor states pXtXt+1(Ut) is then a fixed function of the control. If we denote the probability of control ut given parameters and observation yt by ut( ; yt), then all of the above discussion carries through with rpXtXt+1( )=pXtXt+1( ) replaced by r Ut( ; Yt)= Ut( ; Yt). In that case, Algorithm 1.1 is precisely Williams\u2019 REINFORCE algorithm. Algorithm 1.1 and the variants above have been extended to cover multiple agents (Peshkin et al., 2000), policies with internal state (Meuleau et al., 1999), and importance sampling methods (Meuleau et al., 2000). We also refer the reader to the work of Rubinstein and Shapiro (1993) and Rubinstein and Melamed (1998) for in-depth analysis of the application of the likelihood-ratio method to Discrete-Event Systems (DES), in particular networks of queues.", "startOffset": 119, "endOffset": 2279}, {"referenceID": 1, "context": "Marbach and Tsitsiklis (1998) also considered the case of -dependent rewards (recall the discussion after (3)), as did Baird and Moore (1999) with their \u201cVAPS\u201d algorithm (Value And Policy Search). This last paper contains an interesting insight: through suitable choices of the performance function r(X0; : : : ;XT ; ), one can combine policy-gradient search with approximate value function methods. The resulting algorithms can be viewed as actor-critic techniques in the spirit of Barto et al. (1983); the policy is the actor and the value function is the critic. The primary motivation is to reduce variance in the policy-gradient estimates. Experimental evidence for this phenomenon has been presented by a number of authors, including Barto et al. (1983), Kimura and Kobayashi (1998a), and Baird and Moore (1999). More recent work on this subject includes that of Sutton et al. (2000) and Konda and Tsitsiklis (2000). We discuss the use of VAPS-style updates further in Section 6.2. So far we have not addressed the question of how the parameterized state-transition probabilities pXtXt+1( ) arise. Of course, they could simply be generated by parameterizing the matrix of transition probabilities directly. Alternatively, in the case of MDPs or POMDPs, state transitions are typically generated by feeding an observation Yt that depends stochastically on the state Xt into a parameterized stochastic policy, which selects a control Ut at random from a set of available controls (approximate value-function based approaches that generate controls stochastically via some form of lookahead also fall into this category). The distribution over successor states pXtXt+1(Ut) is then a fixed function of the control. If we denote the probability of control ut given parameters and observation yt by ut( ; yt), then all of the above discussion carries through with rpXtXt+1( )=pXtXt+1( ) replaced by r Ut( ; Yt)= Ut( ; Yt). In that case, Algorithm 1.1 is precisely Williams\u2019 REINFORCE algorithm. Algorithm 1.1 and the variants above have been extended to cover multiple agents (Peshkin et al., 2000), policies with internal state (Meuleau et al., 1999), and importance sampling methods (Meuleau et al., 2000). We also refer the reader to the work of Rubinstein and Shapiro (1993) and Rubinstein and Melamed (1998) for in-depth analysis of the application of the likelihood-ratio method to Discrete-Event Systems (DES), in particular networks of queues.", "startOffset": 119, "endOffset": 2313}, {"referenceID": 1, "context": "Marbach and Tsitsiklis (1998) also considered the case of -dependent rewards (recall the discussion after (3)), as did Baird and Moore (1999) with their \u201cVAPS\u201d algorithm (Value And Policy Search). This last paper contains an interesting insight: through suitable choices of the performance function r(X0; : : : ;XT ; ), one can combine policy-gradient search with approximate value function methods. The resulting algorithms can be viewed as actor-critic techniques in the spirit of Barto et al. (1983); the policy is the actor and the value function is the critic. The primary motivation is to reduce variance in the policy-gradient estimates. Experimental evidence for this phenomenon has been presented by a number of authors, including Barto et al. (1983), Kimura and Kobayashi (1998a), and Baird and Moore (1999). More recent work on this subject includes that of Sutton et al. (2000) and Konda and Tsitsiklis (2000). We discuss the use of VAPS-style updates further in Section 6.2. So far we have not addressed the question of how the parameterized state-transition probabilities pXtXt+1( ) arise. Of course, they could simply be generated by parameterizing the matrix of transition probabilities directly. Alternatively, in the case of MDPs or POMDPs, state transitions are typically generated by feeding an observation Yt that depends stochastically on the state Xt into a parameterized stochastic policy, which selects a control Ut at random from a set of available controls (approximate value-function based approaches that generate controls stochastically via some form of lookahead also fall into this category). The distribution over successor states pXtXt+1(Ut) is then a fixed function of the control. If we denote the probability of control ut given parameters and observation yt by ut( ; yt), then all of the above discussion carries through with rpXtXt+1( )=pXtXt+1( ) replaced by r Ut( ; Yt)= Ut( ; Yt). In that case, Algorithm 1.1 is precisely Williams\u2019 REINFORCE algorithm. Algorithm 1.1 and the variants above have been extended to cover multiple agents (Peshkin et al., 2000), policies with internal state (Meuleau et al., 1999), and importance sampling methods (Meuleau et al., 2000). We also refer the reader to the work of Rubinstein and Shapiro (1993) and Rubinstein and Melamed (1998) for in-depth analysis of the application of the likelihood-ratio method to Discrete-Event Systems (DES), in particular networks of queues. Also worth mentioning is the large literature on Infinitesimal Perturbation Analysis (IPA), which seeks a similar goal of estimating performance gradients, but operates under more restrictive assumptions than the likelihoodratio approach; see, for example, Ho and Cao (1991).", "startOffset": 119, "endOffset": 2727}, {"referenceID": 12, "context": "The first method takes as a starting point the formula5 for the eligibility trace at time t: zt = t 1 Xs=0 rpXsXs+1( ) pXsXs+1( ) and simply truncates it at some (fixed, not random) number of terms n looking backwards (Glynn, 1990; Rubinstein, 1991, 1992; Cao & Wan, 1998): zt(n) := t 1 X s=t n rpXsXs+1( ) pXsXs+1( ) : (7)", "startOffset": 218, "endOffset": 272}, {"referenceID": 16, "context": "(1995, 1997) and for continuous control by Kimura and Kobayashi (1998b). In fact the use of (r(Xt) b) in place of r(Xt) does not affect the expectation of the estimates of the algorithm (although judicious choice of the reward baseline b can reduce the variance of the estimates).", "startOffset": 43, "endOffset": 72}, {"referenceID": 16, "context": "(1995, 1997) and for continuous control by Kimura and Kobayashi (1998b). In fact the use of (r(Xt) b) in place of r(Xt) does not affect the expectation of the estimates of the algorithm (although judicious choice of the reward baseline b can reduce the variance of the estimates). While the algorithm presented by Kimura et al. (1995) provides estimates of the expectation under the stationary distribution of the gradient of the discounted reward, we will show that these are in fact biased estimates of the gradient of the expected discounted reward.", "startOffset": 43, "endOffset": 335}, {"referenceID": 16, "context": "(1995, 1997) and for continuous control by Kimura and Kobayashi (1998b). In fact the use of (r(Xt) b) in place of r(Xt) does not affect the expectation of the estimates of the algorithm (although judicious choice of the reward baseline b can reduce the variance of the estimates). While the algorithm presented by Kimura et al. (1995) provides estimates of the expectation under the stationary distribution of the gradient of the discounted reward, we will show that these are in fact biased estimates of the gradient of the expected discounted reward. This arises because the stationary distribution itself depends on the parameters. A similar estimate to (11) was also proposed by Marbach and Tsitsiklis (1998), but this time with r(Xt)zt( ) replaced by (r(Xt) \u0011\u0302( ))zt( ), where \u0011\u0302( ) is an estimate of the average reward, and with zt zeroed on visits to an identifiable recurrent state.", "startOffset": 43, "endOffset": 713}, {"referenceID": 5, "context": "In a companion paper we show how the gradient estimates produced by GPOMDP can be used to perform gradient ascent on the average reward ( ) (Baxter et al., 2001).", "startOffset": 140, "endOffset": 161}, {"referenceID": 2, "context": "Better estimates of the bias and variance of GPOMDP may be found in Bartlett and Baxter (2001), for more general Markov chains than those treated here, and for more refined notions of the mixing time.", "startOffset": 68, "endOffset": 95}, {"referenceID": 5, "context": "However, when we consider gradient-ascent algorithms Baxter et al. (2001), this assumption becomes more restrictive since it guarantees that the recurrence class cannot change as the parameters are adjusted.", "startOffset": 53, "endOffset": 74}, {"referenceID": 7, "context": "where r = [r(1); : : : ; r(n)\u21040 (Bertsekas, 1995).", "startOffset": 32, "endOffset": 49}, {"referenceID": 7, "context": "Observe that J satisfies the Bellman equations: J = r + PJ : (20) (Bertsekas, 1995).", "startOffset": 66, "endOffset": 83}, {"referenceID": 18, "context": "In fact, Theorem 1 in (Kimura et al., 1997) shows that the gradient estimates of the algorithm presented in that paper converge to (1 ) 0rJ .", "startOffset": 22, "endOffset": 43}, {"referenceID": 18, "context": "In fact, Theorem 1 in (Kimura et al., 1997) shows that the gradient estimates of the algorithm presented in that paper converge to (1 ) 0rJ . By the Bellman equations (20), this is equal to (1 ) ( 0rPJ + 0rJ ), which implies (1 ) 0rJ = 0rPJ . Thus the algorithm of Kimura et al. (1997) also estimates the second term in the expression for r ( ) given by (19).", "startOffset": 23, "endOffset": 286}, {"referenceID": 18, "context": "In fact, Theorem 1 in (Kimura et al., 1997) shows that the gradient estimates of the algorithm presented in that paper converge to (1 ) 0rJ . By the Bellman equations (20), this is equal to (1 ) ( 0rPJ + 0rJ ), which implies (1 ) 0rJ = 0rPJ . Thus the algorithm of Kimura et al. (1997) also estimates the second term in the expression for r ( ) given by (19). It is important to note that 0rJ 6= r [ 0J \u2104\u2014the two quantities disagree by the first term in (19). This arises because the the stationary distribution itself depends on the parameters. Hence, the algorithm of Kimura et al. (1997) does not estimate the gradient of the expected discounted reward.", "startOffset": 23, "endOffset": 591}, {"referenceID": 18, "context": "GPOMDP is essentially the algorithm proposed by Kimura et al. (1997) without the reward baseline.", "startOffset": 48, "endOffset": 69}, {"referenceID": 4, "context": "This kind of update is known as an actor-critic algorithm (Barto et al., 1983), with the policy playing the role of the actor, and the value function playing the role of the critic.", "startOffset": 58, "endOffset": 78}, {"referenceID": 9, "context": "Parameter-dependent rewards have been considered by Glynn (1990), Marbach and Tsitsiklis (1998), and Baird and Moore (1999).", "startOffset": 52, "endOffset": 65}, {"referenceID": 9, "context": "Parameter-dependent rewards have been considered by Glynn (1990), Marbach and Tsitsiklis (1998), and Baird and Moore (1999).", "startOffset": 52, "endOffset": 96}, {"referenceID": 1, "context": "Parameter-dependent rewards have been considered by Glynn (1990), Marbach and Tsitsiklis (1998), and Baird and Moore (1999). In particular, Baird and Moore (1999) showed how suitable choices of r( ; i) lead to a combination of value and policy search, or \u201cVAPS\u201d.", "startOffset": 101, "endOffset": 124}, {"referenceID": 1, "context": "Parameter-dependent rewards have been considered by Glynn (1990), Marbach and Tsitsiklis (1998), and Baird and Moore (1999). In particular, Baird and Moore (1999) showed how suitable choices of r( ; i) lead to a combination of value and policy search, or \u201cVAPS\u201d.", "startOffset": 101, "endOffset": 163}, {"referenceID": 10, "context": "(For definitions see, for example, (Dudley, 1989).", "startOffset": 35, "endOffset": 49}, {"referenceID": 25, "context": "For similar observations in the context of REINFORCE and VAPS, see Peshkin et al. (2000). This algorithm gives a biologically plausible synaptic weight-update rule when applied to networks of spiking neurons in which the neurons are regarded as independent agents (Bartlett & Baxter, 1999), and has shown some promise in a network routing application (Tao, Baxter, & Weaver, 2001).", "startOffset": 67, "endOffset": 89}, {"referenceID": 36, "context": "Fortunately, the observation history may be summarized in the form of a belief state (the current distribution over states), which is itself updated based only upon the current observation, and knowledge of which is sufficient for optimal behaviour (Smallwood & Sondik, 1973; Sondik, 1978).", "startOffset": 249, "endOffset": 289}, {"referenceID": 0, "context": "An extension of GPOMDP to policies with parameterized internal belief states is described by Aberdeen and Baxter (2001), similar in spirit to the extension of VAPS and REINFORCE described by Meuleau et al.", "startOffset": 93, "endOffset": 120}, {"referenceID": 0, "context": "An extension of GPOMDP to policies with parameterized internal belief states is described by Aberdeen and Baxter (2001), similar in spirit to the extension of VAPS and REINFORCE described by Meuleau et al. (1999).", "startOffset": 93, "endOffset": 213}, {"referenceID": 5, "context": "In the companion paper (Baxter et al., 2001), we present experimental results showing rapid convergence of the estimates generated by GPOMDP to the true gradient r .", "startOffset": 23, "endOffset": 44}], "year": 2011, "abstractText": "Gradient-based approaches to direct policy search in reinforcement learning have received much recent attention as a means to solve problems of partial observability and to avoid some of the problems associated with policy degradation in value-function methods. In this paper we introduce GPOMDP, a simulation-based algorithm for generating a biased estimate of the gradient of the average reward in Partially Observable Markov Decision Processes (POMDPs) controlled by parameterized stochastic policies. A similar algorithm was proposed by Kimura, Yamamura, and Kobayashi (1995). The algorithm\u2019s chief advantages are that it requires storage of only twice the number of policy parameters, uses one free parameter 2 [0; 1) (which has a natural interpretation in terms of bias-variance trade-off), and requires no knowledge of the underlying state. We prove convergence of GPOMDP, and show how the correct choice of the parameter is related to the mixing time of the controlledPOMDP. We briefly describe extensions ofGPOMDP to controlled Markov chains, continuous state, observation and control spaces, multiple-agents, higher-order derivatives, and a version for training stochastic policies with internal states. In a companion paper (Baxter, Bartlett, & Weaver, 2001) we show how the gradient estimates generated by GPOMDP can be used in both a traditional stochastic gradient algorithm and a conjugate-gradient procedure to find local optima of the average reward.", "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}