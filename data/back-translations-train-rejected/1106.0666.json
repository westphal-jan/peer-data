{"id": "1106.0666", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2011", "title": "Experiments with Infinite-Horizon, Policy-Gradient Estimation", "abstract": "In this paper, we present algorithms that perform gradient ascent of the average reward in a partially observable Markov decision process (POMDP). These algorithms are based on GPOMDP, an algorithm introduced in a companion paper (Baxter and Bartlett, this volume), which computes biased estimates of the performance gradient in POMDPs. The algorithm's chief advantages are that it uses only one free parameter beta, which has a natural interpretation in terms of bias-variance trade-off, it requires no knowledge of the underlying state, and it can be applied to infinite state, control and observation spaces. We show how the gradient estimates produced by GPOMDP can be used to perform gradient ascent, both with a traditional stochastic-gradient algorithm, and with an algorithm based on conjugate-gradients that utilizes gradient information to bracket maxima in line searches. Experimental results are presented illustrating both the theoretical results of (Baxter and Bartlett, this volume) on a toy problem, and practical aspects of the algorithms on a number of more realistic problems.", "histories": [["v1", "Fri, 3 Jun 2011 14:52:26 GMT  (146kb)", "http://arxiv.org/abs/1106.0666v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["p l bartlett", "j baxter", "l weaver"], "accepted": false, "id": "1106.0666"}, "pdf": {"name": "1106.0666.pdf", "metadata": {"source": "CRF", "title": "Experiments with Infinite-Horizon, Policy-Gradient Estimation", "authors": ["Jonathan Baxter", "Peter L. Bartlett"], "emails": ["JBAXTER@WHIZBANG.COM", "BARTLETT@BARNHILLTECHNOLOGIES.COM", "LEX.WEAVER@ANU.EDU.AU"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves if they do not follow the rules. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "4. Stochastic Gradient Ascent Algorithms", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "5. Experiments", "text": "In this section, we present several sets of experimental results. During this section, in which we refer to CONJPOMDP 1998, we mean CONJPOMDP with GPOMDP as its GRAD argumentation.In the first series of experiments, we consider a system in which a controller is used to select actions for a 3-state Markov Decision Process (MDP).For this system, we are able to calculate the true gradient precisely using the matrix equation () = 0 () rP () + e 0 () 1 r; (3) where P () is the transition matrix of the underlying Markov chain with the controller parameters pointing to, 0 () the stationary distribution according to P () (written as a row vector), e 0 () is the square matrix in which each row is the stationary distribution, and r is the (column) vector of rewards (see Baxlett function) (we have a problem with beard tray)."}, {"heading": "5.1 A three-state MDP", "text": "In this section, we consider a three-dimensional MDP in which there is a choice between two actions1 and a2 in each state. Table 1 shows the transition probabilities depending on the states and actions. Each state x has a corresponding two-dimensional characteristic vector (x) = (1 (x); 2 (x) and reward vector (x), which is detailed in Table 2. Clearly, the optimal policy is to always select the action that leads to state C with the highest probability, which means from Table 1 to always select action a2. This rather strange choice of characteristic vectors for the states ensures that a linear value function is ensured in these characteristics and, using TD (1) - while respecting the optimal policy - a sub-optimal one-stage predictive policy is implemented (see Weaver & Baxter, 1999) for proof)."}, {"heading": "5.1.1 TRAINING A CONTROLLER", "text": "Our goal is to learn a stochastic controller for this system that implements an optimal (or near-optimal) policy. Given a parameter vector = (1; 2; 3; 4), we generate a policy as follows: For each state x, s1 (x): = 1 1 (x) + 2 (x) s2 (x): = 3 1 (x) + 4 2 (x): Then, the probability of selecting action a1 in state x is given by a1 (x) = es1 (x) es1 (x) es1 (x) es1 (x) es1 (x) + es2 (x); while the probability of selecting action a2 is given by a2 (x) = es2 (x) es1 (x) = 1 a1 (x): The ratio ai (x) that the algorithms 1 and 4 (x) require is determined by, r a1 (x) (2) (2) (2) (2) and 2 (x) = 2 (x) (x) (x)."}, {"heading": "5.1.2 GRADIENT ESTIMATES", "text": "In fact, most of them will be able to hold their own, and they will be able to hold their own, \"he said in an interview with The New York Times."}, {"heading": "5.2 Puck World", "text": "This section describes experiments in which CONJPOMDP and OLPOMDP were used to train 1-hidden-layer neural network controllers that navigate a small puck through a two-dimensional world."}, {"heading": "5.2.1 THE WORLD", "text": "The puck had no internal dynamics (i.e. rotation), collisions with the boundaries of the region were inelastic with a (tunable) restitution coefficient e (set to 0: 9 for the experiments reported here), the puck was controlled by applying a force of 5 units in the positive or negative x-direction and a force of 5 units in the positive or negative y-direction, giving a total of four different controls, the control could be changed every 1 = 10 of a second, and the simulator operated with a granularity of 1 = 100 of a second. The puck also had a retarding force due to the aerodynamic drag of 0: 005 velocity 2. There was no friction between the puck and the ground, and the puck was given a reward of 1 = 10 of a second at each decision point."}, {"heading": "5.2.2 THE CONTROLLER", "text": "A single-layer neural network with six input nodes, eight hidden nodes, and four output nodes was used to generate a probabilistic policy similar to that of the controller in the example of the three-stage Markov chain of the previous section. Four of the inputs were adjusted to the raw x and y positions and velocities of the puck in the current time step, the other two were the differences between the x and y positions of the puck and the x and y positions of the target, respectively. Position inputs were scaled so that they were between 1 and 1, while speed inputs were scaled so that a speed of 10 units per second was mapped to a value of 1. Hidden nodes computed a tanh quashing function, while the output nodes were linear. Each hidden and output node had the usual additional offset parameter. The four output nodes were exposed and then normalized, as in the example of the marker chain, to generate a probability distribution of 5 over the direction of the control node (4 were randomly selected)."}, {"heading": "5.2.3 CONJUGATE GRADIENT ASCENT", "text": "We trained the neural networker with the gradients it generated. After some experiments, we decided to use 0: 95 and T = 1; 000 as the parameters CONJPOMDP. GSEARCH used the same value and scheme in section 5.1.3 to determine the number of iterations with which GPOMDP would work. That is, the weights grew very early in the simulation, but in the exposed nodes, there was a tendency for the network."}, {"heading": "5.3 Call Admission Control", "text": "In this section we report on the results of experiments in which CONJPOMDP was applied to the task of training a controller for the call recording problem dealt with by Marbach (1998, Chapter 7)."}, {"heading": "5.3.1 THE PROBLEM", "text": "The call access control problem dealt with by Marbach (1998, Chapter 7) is the situation in which a telecommunications provider wishes to sell the bandwidth of a communication connection to its customers in order to maximize the long-term average remuneration.Specifically, this is a hold-up problem. There are three different types of calls, each with its own call response rate (1), (2), (3), bandwidth demand b (1), b (2), b (3) and average wait time h (1), h (2), h (3).The arrivals are poisson distributed, while the wait times are distributed exponentially.The call has a maximum bandwidth of 10 units. If a call arrives and there is enough available bandwidth, the service provider can choose whether to accept or reject the call (if there is insufficient available bandwidth), the call is always refused. If a call is accepted from 1998 to 2004, the maximum service provider receives (where the reward is 3)."}, {"heading": "5.3.2 THE CONTROLLER", "text": "The above-mentioned reasons for this development are primarily to be found in the fact that the two candidates are candidates who are able to decide to stand."}, {"heading": "5.4 Mountainous Puck World", "text": "As shown in Figure 16, the task is to drive a car to the top of a one-dimensional hill. The car is not strong enough to accelerate directly up the hill against gravity, so any successful controller must learn to \"swing\" back and forth until it builds enough speed to climb the hill. In this section, we describe a variation of the Mountaincar problem using the puck world example from Section 5.2. In relation to Figure 17, the task is to navigate a puck from a valley to a plateau at the northern end of the valley. As with the mountain car task, the puck does not have enough power to accelerate directly up the hill, and must therefore learn to swing to climb out of the valley."}, {"heading": "5.4.1 THE WORLD", "text": "The world dimensions, physics, puck dynamics, and control were identical to the flat puck world described in Section 5.2, except that the puck was subject to a constant gravitational force of 10 units, the maximum thrust allowed was 3 units (instead of 5), and the height of the world varied as follows: Height (x; y) = 8 <: 15 if y < 25 or y > 757: 5 1 os (y2 50) 25 otherwise: With only 3 units of thrust, a unit spit cannot accelerate directly out of the valley. Every 120 (simulated) seconds, the puck was initialized at zero velocity on the valley floor, with a random x position. The puck was not rewarded either in the valley or on the southern plateau, and a reward of 100 s2 on the northern plateau, where s was the speed of the puck."}, {"heading": "5.4.2 THE CONTROLLER", "text": "After some experimentation, we found that a neural network controller could be reliably trained to navigate to the northern plateau or stay there on the northern plateau, but it was difficult to combine the two in one controller (which is not so surprising since the two tasks are quite different). To solve this problem, we trained a \"switched\" neural network controller: the puck used a controller when in the valley and on the southern plateau, and then switched to a second neural network controller while on the northern plateau. Both controllers were single-layer neural networks with nine input nodes, five hidden nodes, and four output nodes. The nine inputs were the normalized ([1; 1-rated) x, y, and z puck positions, the normalized x, y, and z locations relative to the center of the northern wall, and the x, y and z positions."}, {"heading": "5.4.3 CONJUGATE GRADIENT ASCENT", "text": "The switched neural network controller was designed using the same scheme discussed in Section 5.2.3, except this time the discounting factor was automatically set to 0: 98.A diagram of the average reward of the neural network controller, while the average reward of the neural network controller was automatically estimated in Figure 18 as a function of the number of iterations of PODP. The graph is an average of over 100 independent runs, with the neural network controller whose performance is near optimal after about 40 million total iterations of the puck world used. Although this number may seem quite high to represent it in any perspective note that a random neural network controller takes about 10,000 iterations to reach the northern plateau."}, {"heading": "6. Conclusion", "text": "This paper shows how to use the performance gradient estimates of the GPOMDP algorithm (Baxter & Bartlett, 2001) to optimize the average reward of parameterized POMDPs. We described both a traditional \"on-line\" stochastic gradient algorithm and an \"off-line\" approach based on the use of GSEARCH, a robust line search algorithm that uses gradient estimates to maintain the maximum. Offline approach was found to be good for four fairly clear problems: optimizing a controller for a three-state MDP, optimizing a neural network controller for navigating a puck around a two-dimensional world, optimizing a controller for a call authorization problem, and optimizing a neural network controller for a three-state MDP."}, {"heading": "Acknowledgements", "text": "This work was supported by the Australian Research Council and benefited from the comments of several anonymous speakers. Most of the research was conducted while the first and second authors were at the Research School of Information Sciences and Engineering at the Australian National University."}], "references": [{"title": "Policy-gradient learning of controllers with internal state", "author": ["D. Aberdeen", "J. Baxter"], "venue": null, "citeRegEx": "Aberdeen and Baxter,? \\Q2001\\E", "shortCiteRegEx": "Aberdeen and Baxter", "year": 2001}, {"title": "Gradient descent for general reinforcement learning", "author": ["L. Baird", "A. Moore"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Baird and Moore,? \\Q1999\\E", "shortCiteRegEx": "Baird and Moore", "year": 1999}, {"title": "Hebbian synaptic modifications in spiking neurons that learn", "author": ["P.L. Bartlett", "J. Baxter"], "venue": "Tech. rep., Research School of Information Sciences and Engineering,", "citeRegEx": "Bartlett and Baxter,? \\Q1999\\E", "shortCiteRegEx": "Bartlett and Baxter", "year": 1999}, {"title": "Estimation and approximation bounds for gradient-based reinforcement learning", "author": ["P.L. Bartlett", "J. Baxter"], "venue": "In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory,", "citeRegEx": "Bartlett and Baxter,? \\Q2000\\E", "shortCiteRegEx": "Bartlett and Baxter", "year": 2000}, {"title": "Stochastic optimization of controlled partially observable markov decision processes", "author": ["P.L. Bartlett", "J. Baxter"], "venue": "In Proceedings of the 39th IEEE Conference on Decision and Control (CDC00)", "citeRegEx": "Bartlett and Baxter,? \\Q2000\\E", "shortCiteRegEx": "Bartlett and Baxter", "year": 2000}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A.G. Barto", "R.S. Sutton", "C.W. Anderson"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Barto et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1983}, {"title": "Reinforcement learning in POMDPs via direct gradient ascent", "author": ["J. Baxter", "P.L. Bartlett"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning", "citeRegEx": "Baxter and Bartlett,? \\Q2000\\E", "shortCiteRegEx": "Baxter and Bartlett", "year": 2000}, {"title": "Infinite-horizon policy-gradient estimation", "author": ["J. Baxter", "P.L. Bartlett"], "venue": "Journal of Artificial Intelligence Research. To appear", "citeRegEx": "Baxter and Bartlett,? \\Q2001\\E", "shortCiteRegEx": "Baxter and Bartlett", "year": 2001}, {"title": "Learning to play chess using temporal-differences", "author": ["J. Baxter", "A. Tridgell", "L. Weaver"], "venue": "Machine Learning,", "citeRegEx": "Baxter et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Baxter et al\\.", "year": 2000}, {"title": "Perturbation Realization, Potentials, and Sensitivity Analysis of Markov Processes", "author": ["Cao", "X.-R", "Chen", "H.-F"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Cao et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cao et al\\.", "year": 1997}, {"title": "Algorithms for Sensitivity Analysis of Markov Chains Through Potentials and Perturbation Realization", "author": ["Cao", "X.-R", "Wan", "Y.-W"], "venue": "IEEE Transactions on Control Systems Technology,", "citeRegEx": "Cao et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Cao et al\\.", "year": 1998}, {"title": "Feedforward Neural Network Methodology", "author": ["T.L. Fine"], "venue": null, "citeRegEx": "Fine,? \\Q1999\\E", "shortCiteRegEx": "Fine", "year": 1999}, {"title": "Smooth Perturbation Derivative Estimation for Markov Chains", "author": ["M.C. Fu", "J. Hu"], "venue": "Operations Research Letters,", "citeRegEx": "Fu and Hu,? \\Q1994\\E", "shortCiteRegEx": "Fu and Hu", "year": 1994}, {"title": "Stochastic approximation for monte-carlo optimization", "author": ["P.W. Glynn"], "venue": "In Proceedings of the 1986 Winter Simulation Conference,", "citeRegEx": "Glynn,? \\Q1986\\E", "shortCiteRegEx": "Glynn", "year": 1986}, {"title": "An analysis of actor/critic algorithms using eligibility traces: Reinforcement learning with imperfect value functions", "author": ["H. Kimura", "S. Kobayashi"], "venue": "In Fifteenth International Conference on Machine Learning,", "citeRegEx": "Kimura and Kobayashi,? \\Q1998\\E", "shortCiteRegEx": "Kimura and Kobayashi", "year": 1998}, {"title": "Reinforcement learning in POMDPs with function approximation", "author": ["H. Kimura", "K. Miyazaki", "S. Kobayashi"], "venue": "Proceedings of the Fourteenth International Conference on Machine Learning", "citeRegEx": "Kimura et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Kimura et al\\.", "year": 1997}, {"title": "Reinforcement learning by stochastic hill climbing on discounted reward", "author": ["H. Kimura", "M. Yamamura", "S. Kobayashi"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning", "citeRegEx": "Kimura et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kimura et al\\.", "year": 1995}, {"title": "Actor-Critic Algorithms", "author": ["V.R. Konda", "J.N. Tsitsiklis"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Konda and Tsitsiklis,? \\Q2000\\E", "shortCiteRegEx": "Konda and Tsitsiklis", "year": 2000}, {"title": "Simulation-Based Methods for Markov Decision Processes", "author": ["P. Marbach"], "venue": "Ph.D. thesis,", "citeRegEx": "Marbach,? \\Q1998\\E", "shortCiteRegEx": "Marbach", "year": 1998}, {"title": "Simulation-Based Optimization of Markov Reward Processes", "author": ["P. Marbach", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "Marbach and Tsitsiklis,? \\Q1998\\E", "shortCiteRegEx": "Marbach and Tsitsiklis", "year": 1998}, {"title": "Some Studies in Machine Learning Using the Game of Checkers", "author": ["A.L. Samuel"], "venue": "IBM Journal of Research and Development,", "citeRegEx": "Samuel,? \\Q1959\\E", "shortCiteRegEx": "Samuel", "year": 1959}, {"title": "Learning Without State-Estimation in Partially Observable Markovian Decision Processes", "author": ["S.P. Singh", "T. Jaakkola", "M.I. Jordan"], "venue": "In Proceedings of the Eleventh International Conference on Machine Learning", "citeRegEx": "Singh et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1994}, {"title": "Reinforcement learning for dynamic channel allocation in cellular telephone systems", "author": ["S. Singh", "D. Bertsekas"], "venue": "In Advances in Neural Information Processing Systems: Proceedings of the 1996 Conference,", "citeRegEx": "Singh and Bertsekas,? \\Q1997\\E", "shortCiteRegEx": "Singh and Bertsekas", "year": 1997}, {"title": "Reinforcement learning with soft state aggregation", "author": ["S. Singh", "T. Jaakkola", "M. Jordan"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Singh et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Singh et al\\.", "year": 1995}, {"title": "Learning to Predict by the Method of Temporal Differences", "author": ["R. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation", "author": ["R.S. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": "In Neural Information Processing Systems", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "A multi-agent, policy-gradient approach to network routing", "author": ["N. Tao", "J. Baxter", "L. Weaver"], "venue": null, "citeRegEx": "Tao et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Tao et al\\.", "year": 2001}, {"title": "Practical Issues in Temporal Difference Learning", "author": ["G. Tesauro"], "venue": "Machine Learning,", "citeRegEx": "Tesauro,? \\Q1992\\E", "shortCiteRegEx": "Tesauro", "year": 1992}, {"title": "TD-Gammon, a self-teaching backgammon program, achieves master-level play", "author": ["G. Tesauro"], "venue": "Neural Computation,", "citeRegEx": "Tesauro,? \\Q1994\\E", "shortCiteRegEx": "Tesauro", "year": 1994}, {"title": "Reinforcement learning from state and temporal differences", "author": ["L. Weaver", "J. Baxter"], "venue": null, "citeRegEx": "Weaver and Baxter,? \\Q1999\\E", "shortCiteRegEx": "Weaver and Baxter", "year": 1999}, {"title": "Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams,? \\Q1992\\E", "shortCiteRegEx": "Williams", "year": 1992}, {"title": "A reinforcement learning approach to job-shop scheduling", "author": ["W. Zhang", "T. Dietterich"], "venue": "In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Zhang and Dietterich,? \\Q1995\\E", "shortCiteRegEx": "Zhang and Dietterich", "year": 1995}], "referenceMentions": [{"referenceID": 6, "context": "Experimental results are presented illustrating both the theoretical results of Baxter and Bartlett (2001) on a toy problem, and practical aspects of the algorithms on a number of more realistic problems.", "startOffset": 80, "endOffset": 107}, {"referenceID": 31, "context": "This question has been addressed in a large body of previous work (Barto, Sutton, & Anderson, 1983; Williams, 1992; Glynn, 1986; Cao & Chen, 1997; Cao & Wan, 1998; Fu & Hu, 1994; Singh, Jaakkola, & Jordan, 1994, 1995; Marbach & Tsitsiklis, 1998; Marbach, 1998; Baird & Moore, 1999; Rubinstein & Melamed, 1998; Kimura, Yamamura, & Kobayashi, 1995; Kimura, Miyazaki, & Kobayashi, 1997).", "startOffset": 66, "endOffset": 383}, {"referenceID": 13, "context": "This question has been addressed in a large body of previous work (Barto, Sutton, & Anderson, 1983; Williams, 1992; Glynn, 1986; Cao & Chen, 1997; Cao & Wan, 1998; Fu & Hu, 1994; Singh, Jaakkola, & Jordan, 1994, 1995; Marbach & Tsitsiklis, 1998; Marbach, 1998; Baird & Moore, 1999; Rubinstein & Melamed, 1998; Kimura, Yamamura, & Kobayashi, 1995; Kimura, Miyazaki, & Kobayashi, 1997).", "startOffset": 66, "endOffset": 383}, {"referenceID": 18, "context": "This question has been addressed in a large body of previous work (Barto, Sutton, & Anderson, 1983; Williams, 1992; Glynn, 1986; Cao & Chen, 1997; Cao & Wan, 1998; Fu & Hu, 1994; Singh, Jaakkola, & Jordan, 1994, 1995; Marbach & Tsitsiklis, 1998; Marbach, 1998; Baird & Moore, 1999; Rubinstein & Melamed, 1998; Kimura, Yamamura, & Kobayashi, 1995; Kimura, Miyazaki, & Kobayashi, 1997).", "startOffset": 66, "endOffset": 383}, {"referenceID": 15, "context": "This algorithm is essentially the algorithm proposed in (Kimura et al., 1997).", "startOffset": 56, "endOffset": 77}, {"referenceID": 18, "context": "In the third experiment, we use the off-line algorithm to train a controller for the call admission queueing problem treated in (Marbach, 1998).", "startOffset": 128, "endOffset": 143}, {"referenceID": 18, "context": "In this case near-optimal solutions are found within about 2000 iterations of the underlying queue, 1-2 orders of magnitude faster than the experiments reported in (Marbach, 1998) with on-line (stochastic-gradient) algorithms.", "startOffset": 164, "endOffset": 179}, {"referenceID": 6, "context": "Although the results in Baxter and Bartlett (2001) only guarantee convergence of GPOMDP in the case of finite S (but continuous U and Y), the algorithm can be applied regardless of the nature of S so we do not restrict the cardinality of S , U or Y .", "startOffset": 24, "endOffset": 51}, {"referenceID": 0, "context": "Aberdeen and Baxter (2001) have extended GPOMDP and the techniques of the present paper to controllers with internal state.", "startOffset": 0, "endOffset": 27}, {"referenceID": 2, "context": "See Bartlett and Baxter (2000b) for a proof that OLPOMDP converges to the vicinity of a local maximum of ( ).", "startOffset": 4, "endOffset": 32}, {"referenceID": 24, "context": "training a linear value-function for this system using TD(1) (Sutton, 1988), which can be shown to converge to a value function whose one-step lookahead policy is suboptimal (Weaver & Baxter, 1999).", "startOffset": 61, "endOffset": 75}, {"referenceID": 18, "context": "In the third set of experiments we use CONJPOMDP to optimize the admission thresholds for the call-admission problem considered in (Marbach, 1998).", "startOffset": 131, "endOffset": 146}, {"referenceID": 18, "context": "With these settings, the optimal policy (found by dynamic programming by Marbach (1998)) is to always accept calls of type 2 and 3 (assuming sufficient available bandwidth) and to accept calls of type 1 if the available", "startOffset": 73, "endOffset": 88}, {"referenceID": 18, "context": "This is the class of controllers studied by Marbach (1998).", "startOffset": 44, "endOffset": 59}, {"referenceID": 18, "context": "This is probably due to a discrepancy in the way the state transitions are counted, which was not clear from the discussion in (Marbach, 1998).", "startOffset": 127, "endOffset": 142}, {"referenceID": 18, "context": "The controller was always started from the same parameter setting = (8; 8; 8) (as was done by Marbach (1998)).", "startOffset": 94, "endOffset": 109}, {"referenceID": 18, "context": "The controller was always started from the same parameter setting = (8; 8; 8) (as was done by Marbach (1998)). The value of this initial policy is 0:691. The graph of the average reward of the final controller produced by CONJPOMDP as a function of the total number of iterations of the queue is shown in Figure 12. A performance of 0:784 was reliably achieved with less than 2000 iterations of the queue. Note that the optimal policy is not achievable with this controller class since it is incapable of implementing any threshold policy other than the \u201calways accept\u201d and \u201calways reject\u201d policies. Although not provably optimal, a parameter setting of 1 7:5 and any suitably large values of 2 and 3 generates something close to the optimal policy within the controller class, with an average reward of 0:8. Figure 13 shows the probability of accepting a call of each type under this policy (with 2 = 3 = 15), as a function of the available bandwidth. The controllers produced by CONJPOMDPwith = 0:0 and sufficiently large T are essentially \u201calways accept\u201d controllers with an average reward of 0:784, within 2% of the optimum achievable in the class. To produce policies even nearer to the optimal policy in performance, CONJPOMDP must keep 1 close to its starting value of 8, and hence the gradient estimate T = ( 1; 2; 3) 5. There is some discrepancy between our average rewards and those quoted by Marbach (1998). This is probably due to a discrepancy in the way the state transitions are counted, which was not clear from the discussion in (Marbach, 1998).", "startOffset": 94, "endOffset": 1418}, {"referenceID": 5, "context": "With this in mind an interesting avenue for further research is Actor-Critic algorithms (Barto et al., 1983; Baird & Moore, 1999; Kimura & Kobayashi, 1998; Konda & Tsitsiklis, 2000; Sutton, McAllester, Singh, & Mansour, 2000) in which one attempts to combine the fast convergence of value-functions with the theoretical guarantees of policy-gradient approaches.", "startOffset": 88, "endOffset": 225}], "year": 2011, "abstractText": "In this paper, we present algorithms that perform gradient ascent of the average reward in a partially observable Markov decision process (POMDP). These algorithms are based on GPOMDP, an algorithm introduced in a companion paper (Baxter & Bartlett, 2001), which computes biased estimates of the performance gradient in POMDPs. The algorithm\u2019s chief advantages are that it uses only one free parameter 2 [0; 1), which has a natural interpretation in terms of bias-variance trade-off, it requires no knowledge of the underlying state, and it can be applied to infinite state, control and observation spaces. We show how the gradient estimates produced by GPOMDP can be used to perform gradient ascent, both with a traditional stochastic-gradient algorithm, and with an algorithm based on conjugate-gradients that utilizes gradient information to bracket maxima in line searches. Experimental results are presented illustrating both the theoretical results of Baxter and Bartlett (2001) on a toy problem, and practical aspects of the algorithms on a number of more realistic problems.", "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}