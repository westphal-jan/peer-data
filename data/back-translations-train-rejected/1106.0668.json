{"id": "1106.0668", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2011", "title": "An Analysis of Reduced Error Pruning", "abstract": "Top-down induction of decision trees has been observed to suffer from the inadequate functioning of the pruning phase. In particular, it is known that the size of the resulting tree grows linearly with the sample size, even though the accuracy of the tree does not improve. Reduced Error Pruning is an algorithm that has been used as a representative technique in attempts to explain the problems of decision tree learning. In this paper we present analyses of Reduced Error Pruning in three different settings. First we study the basic algorithmic properties of the method, properties that hold independent of the input decision tree and pruning examples. Then we examine a situation that intuitively should lead to the subtree under consideration to be replaced by a leaf node, one in which the class label and attribute values of the pruning examples are independent of each other. This analysis is conducted under two different assumptions. The general analysis shows that the pruning probability of a node fitting pure noise is bounded by a function that decreases exponentially as the size of the tree grows. In a specific analysis we assume that the examples are distributed uniformly to the tree. This assumption lets us approximate the number of subtrees that are pruned because they do not receive any pruning examples. This paper clarifies the different variants of the Reduced Error Pruning algorithm, brings new insight to its algorithmic properties, analyses the algorithm with less imposed assumptions than before, and includes the previously overlooked empty subtrees to the analysis.", "histories": [["v1", "Fri, 3 Jun 2011 14:53:10 GMT  (125kb)", "http://arxiv.org/abs/1106.0668v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["t elomaa", "m kaariainen"], "accepted": false, "id": "1106.0668"}, "pdf": {"name": "1106.0668.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Tapio Elomaa", "elomaa s.helsinki.fi", "Matti K\u00e4\u00e4ri\u00e4inen"], "emails": [], "sections": [{"heading": "An Analysis of Redu ed Error Pruning", "text": "Tapio Elomaa elomaa s.helsinki.fi Matti K\u00e4\u00e4ri\u00e4inen matti.kaariainen s.helsinki.fi Department of Computer S ien e P. O. Box 26 (Teollisuuskatu 23) FIN-00014 University of Helsinki, FinlandAbstra tTop-down indu tion of the tree is seen to su er from the inadequate fun tioning of the pruning phase. In part, it is known that the size of the resulting tree grows linearly with the sample size, even if the quality of the tree does not improve. Redu ed Error Pruning is an algorithm that has been used as a representative hnique in attempts to solve the problems of the de ision tree learning.In this paper, we present analyses of Redu ed Error Pruning pruning in three di erent settings. First, we examine the basi algorithmi properties of the method, properties that are independent of the input de ision tree explique."}, {"heading": "1. Introdu tion", "text": "This year it is so far that it can only take a few days to get a result."}, {"heading": "2. Redu ed Error Pruning Algorithm", "text": "Although rep seems to be a very simple, almost trivial algorithm for pruning, there are many different algorithms running under the same name. There is no consensus on whether rep is a bottom-up algorithm or an iterative method, nor is it obvious whether the training set or the pruning set is used to decipher the labels of the leaves resulting from the pruning."}, {"heading": "2.1 High-Level Control", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide eeisrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeBnln ni rf\u00fc the eeisrgneeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "2.2 Leaf Labeling", "text": "Another reason for this is the fact that the new leaves are not most of them, but a version of the algorithm in which the new leaves receive the majority of the pruning examples. Oates and Jensen have motivated their explanatory examples by empirical observation that there are very few labels in which the labels of the leaf are divided in both directions. Oates and Jensen have motivated their explanatory examples by empirical observation that the labels of the labels are very small."}, {"heading": "2.3 Empty Subtrees", "text": "Sin e rep uses various examples to create and prune a de ision tree, it is possible that some parts of the tree may not receive examples at the pruning stage. So, of course, parts of the de ision tree can be pruned with a single leaf without hanging the number of laser errors the tree makes on the pruning examples. In other words, sub-trees that do not receive pruning examples are always pruned. Quinlan (1987) already noted that those parts of the original tree that respond to rarer spheres not shown in the pruning set can be removed."}, {"heading": "De isionTree REP( De isionTree T, ExampleArray S )", "text": "{for (i = 0 to S. length-1) classify (T, S [i '); Intuitively, it is not lear, which is the most well-founded strategy for dealing with empty subtrees, those that do not give examples. On the one hand, they receive support from the training set, which is usually more numerous than the tree pruning set, but on the other hand, the fact that no pruning example responds to these parts of the tree would justify the conclusion that these parts of the tree pruning set were built by the characteristics of the training data. In the event of repetition, mainly with the preference for smaller tree pruning sets in other ways, the latter view is adopted. The problem of empty subtrees can only be attributed to the problem of small Disjun in ma hine learning algorithms (Holte, A ker, & Porter, 1989). A small Disjun passes over only a small number of training examples."}, {"heading": "2.4 The Analyzed Pruning Algorithm", "text": "As already mentioned, the ontrol strategy of the algorithm is bottom-up processing with a single sweep. First, a top-down traversal guides the trimming examples through the tree to the corresponding leaves. On the way, the refuseniks of the nodes are updated. Of course, in a bottom-up traversal, the trimming operations caused by the lasso errors are exeutted. The error is determined by the node-down values. In the bottom-up traversal, the ea h node is only labeled on e. the trimmed leaves are labeled by the majority of the tree pruning sets (see Table 1)."}, {"heading": "3. Previous Work", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "5.1 Probability Theoreti al Preliminaries", "text": "We denote the probability of an event E of PrfEg and its expiration of EE. A dis rete (integer weighted) random variable X should be binomically distributed with the parameters n and p, denoted by X B (n; p), ifPrfX = k = nk! pk (1 p) n k; k = 0; 1;:; n: IfX B (n; p), then its exposed value or mean is EX = np, varian e varX = np (1 p), and default deviation = p np (1 p).An Indi variable is a random variable that takes only the values 0 and 1. An Indi variable of the variable I is used to denote the original e or non-original e of an event."}, {"heading": "5.2 Bounding the Pruning Probability of a Tree", "text": "In fact, it is as if it were a reactionary, but not a reactionary, reactionary, reactionary and reactionary, reactionary and reactionary, \"he says.\" We have to be able to be, \"he says.\" We have to be able to be able to be, \"he says.\" We have to be able to be able to be, \"he says.\" We have to be in the position we are in, \"he says."}, {"heading": "5.3 Impli ations of the Analysis", "text": "It has been empiri ally observed that the size of the de ision tree is distributed linearly with the training set size (x) even when the trees are pruned (Catlett, 1991; Oates & Jensen, 1997, 1998).The above analysis gives us a way to explain this behavior, but we have only one example that if there is no relationship between the attribute values and the let tag of an example, the size of the tree that perfe tly ts, 0 and 1 the training data depends on the size of the sample.Our setting is as simple as a being. We have only one real attribute x and the let attribute y, whose value is independent of that of x. As before, y has two possible values, 0 and 1. The tree is built with binary splits of a numeri al value range; i.e., propositions of type x < r are mapped to the internal leaves of the tree."}, {"heading": "5.4 Limitations of the Analysis", "text": "In fact, the majority of people who see themselves in a position are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "6. Pruning Probability Under Uniform Distribution", "text": "rE \"s tis,\" so sasd re rf\u00fc ide rf\u00fc rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rf\u00fc the rf\u00fc the rfu the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rfu"}, {"heading": "6.1 An Illustration of the Upper Bound", "text": "Figure 3 shows the upper limit of the pruning probability of a tree with 100 safe knots when 500 pruning examples are used. The value of the parameter varies from 0 to 2 and p varies from 0.5 to 1. We observe that the number responding to the upper limit remains 0 if the upper limit is not too distorted and if the parameter does not have a very small value. If the probability of an example with a positive label meets the value of 0.75 or the value of approa hes 0, the upper limit of the limbs remains very steep. At least on the side of the parameter, this is due to the inexorable approach to the extreme values. If the probability p that an example has a positive label, hes 1 is the error that falls to 0 by a single positive leaf."}, {"heading": "6.2 On the Exa tness of the Approximation", "text": "The deviation from R is from Theorem 7: Prf jR ERj g 2 exp2 (k 1 = 2) k2E2R!: For Q we do not yet have a similar result. In this deviation we ask ourselves the question whether we should change the definition of the lips hitz ondition.De nition Let f: D1Dm! IR be a real-rated fun to m: m hite. The fun f is said to satisfy the lip hitz ondition when x12 D1:::; xm2 Dm, any i 2 f1:::; mg, and any yi2 Di: x1;:: xi 1;::: xi 1; xi 1::::: xm (xm2 Dm: any i 2 f1::: xi)."}, {"heading": "7. Related Work", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrrrrrrrrrrr rrrrrrrrrrrrrrrr rrrrrrrrrrrrrr rrrrrrrrrrrrr rrrrrrrrrr rrrrrrrrrrrrr rrrrrrrrrrrrrrr rrrrrrrrrrrrrrrrrrrrrrrrrrrrrr rrrr rrrrr"}, {"heading": "8. Con lusion", "text": "This year it is more than ever before."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "Top-down indu tion of de ision trees has been observed to su er from the inadequate fun tioning of the pruning phase. In parti ular, it is known that the size of the resulting tree grows linearly with the sample size, even though the a ura y of the tree does not improve. Redu ed Error Pruning is an algorithm that has been used as a representative te hnique in attempts to explain the problems of de ision tree learning. In this paper we present analyses of Redu ed Error Pruning in three di erent settings. First we study the basi algorithmi properties of the method, properties that hold independent of the input de ision tree and pruning examples. Then we examine a situation that intuitively should lead to the subtree under onsideration to be repla ed by a leaf node, one in whi h the lass label and attribute values of the pruning examples are independent of ea h other. This analysis is ondu ted under two di erent assumptions. The general analysis shows that the pruning probability of a node tting pure noise is bounded by a fun tion that de reases exponentially as the size of the tree grows. In a spe i analysis we assume that the examples are distributed uniformly to the tree. This assumption lets us approximate the number of subtrees that are pruned be ause they do not re eive any pruning examples. This paper lari es the di erent variants of the Redu ed Error Pruning algorithm, brings new insight to its algorithmi properties, analyses the algorithm with less imposed assumptions than before, and in ludes the previously overlooked empty subtrees to the analysis.", "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}