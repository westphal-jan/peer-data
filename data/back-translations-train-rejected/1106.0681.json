{"id": "1106.0681", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2011", "title": "Accelerating Reinforcement Learning through Implicit Imitation", "abstract": "Imitation can be viewed as a means of enhancing learning in multiagent environments. It augments an agent's ability to learn useful behaviors by making intelligent use of the knowledge implicit in behaviors demonstrated by cooperative teachers or other more experienced agents. We propose and study a formal model of implicit imitation that can accelerate reinforcement learning dramatically in certain cases. Roughly, by observing a mentor, a reinforcement-learning agent can extract information about its own capabilities in, and the relative value of, unvisited parts of the state space. We study two specific instantiations of this model, one in which the learning agent and the mentor have identical abilities, and one designed to deal with agents and mentors with different action sets. We illustrate the benefits of implicit imitation by integrating it with prioritized sweeping, and demonstrating improved performance and convergence through observation of single and multiple mentors. Though we make some stringent assumptions regarding observability and possible interactions, we briefly comment on extensions of the model that relax these restricitions.", "histories": [["v1", "Fri, 3 Jun 2011 14:57:02 GMT  (526kb)", "http://arxiv.org/abs/1106.0681v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["c boutilier", "b price"], "accepted": false, "id": "1106.0681"}, "pdf": {"name": "1106.0681.pdf", "metadata": {"source": "CRF", "title": "Accelerating Reinforcement Learning through Implicit Imitation", "authors": ["Bob Price", "Craig Boutilier"], "emails": ["price@cs.ubc.ca", "cebly@cs.toronto.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, people who are able to realize themselves are also able to put themselves in a situation in which they put themselves in a situation in which they put themselves in a situation in which they put themselves in a situation in which they put themselves in a situation in which they put themselves in a situation in which they put themselves in a situation in which they put themselves in a situation in which they put themselves in a situation in which they put themselves in a situation in which they put themselves in a situation in which they entangled themselves in another."}, {"heading": "2. Reinforcement Learning", "text": "Our goal is to provide a formal model of implicit imitation, where an agent can learn how to act optimally by combining his or her own experience with his or her observations of the behavior of a knowledgeable mentor. Before doing so, we will describe in this section the standard model of reinforcement learning used in artificial intelligence. Our model builds on this single agent view of learning, starting with the review of the Markov decision-making processes, which represent a model of sequential decision-making under uncertainty, and then moving on to the description of reinforcement learning, with an emphasis on model-based methods."}, {"heading": "2.1 Markov Decision Processes", "text": "Markov Decision Processes (MDPs) have proven to be very useful in modeling stochastic sequential decision problems (Hankas 1987) and have been widely used in decision theory to model areas where the actions of an agent have uncertain effects, although they do not present any particular complications and the agent may have multiple, possibly contradictory objectives. In this section we describe the basic MDP model and consider a classic solution method. We do not consider the cost of action in our formulation of MDPs, although these do not present any particular complications. Finally, we make the assumption of full observability. Partially observable MDPs (POMDPs) (Cassandra, Kaelbling, 1994; Lovejoy, 1991; Probe & Probe, 1973) are much more computationally demanding than fully observable MDPs. Our model is based on a fully observable model, although some of the POMDPs mentioned in the final section are."}, {"heading": "2.2 Model-based Reinforcement Learning", "text": "One difficulty in using MDPs is that constructing an optimal policy requires the agent to know the exact transition probabilities Pr and reward model R. In the specification of a decision problem, these requirements, in particular the detailed specification of the dynamics of the domain, can place an undue burden on the agent's designer. Reinforcement Learning can be seen as a solution to an MDP in which the full details of the model, especially Pr and R, are not known to the agent. Instead, the agent learns how to act optimally through experience with his environment. We offer a brief overview of the enhanced learning in this section (with an emphasis on model-based approaches).For further details, please refer to the texts of Sutton and Barto (1998) and Bertsekas and Tsitsiklis (1996), and the survey of Kaelbling, Littman and Moore (1996).In the general model, we assume that an agent controls an MDP."}, {"heading": "3. A Formal Framework for Implicit Imitation", "text": "In order to model the influence that a mentor-agent can have on the decision-making or learning behavior of an observer, we need to expand the single-agent decision model of MDPs to take into account the actions and objectives of multiple actors. In this section, we introduce a formal framework for the study of implicit imitation, starting with the introduction of a general model for stochastic games (Shapley, 1953; Myerson, 1991), and then putting various assumptions and limitations on this general model that allow us to focus on the key aspects of implicit imitation. We note that the framework proposed here is useful for studying other forms of knowledge transfer in multiagent systems, and briefly point out various extensions of the framework that would allow implicit imitation and other forms of knowledge transfer in more general contexts."}, {"heading": "3.1 Non-Interacting Stochastic Games", "text": "It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) In fact it is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. It is. It is. (...). It is. It is. It is. (... It is. (...). It is. It is. It is. (...). It is. It is. It is. It is. It is. (...). It is. It is. It is. (...). It is. It is. It is. (...). It is. It is. It is. It is. It is. (...). It is. (...). It is. It is. It is. It is. It is. (...). It is. It is. It is. It is. It is. It is. It is. It is. (). It is. It is. It is. It is. It is. It is. It is. It is. (). It is. (). It is. It is. (). It is. It is. (). It is. It is. It is"}, {"heading": "3.2 Implicit Imitation", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "4. Implicit Imitation in Homogeneous Settings", "text": "We begin by describing the implicit imitation in homogeneous action scenarios - the expansion to heterogeneous scenarios builds on the insights developed in this section. We develop a technique called implicit imitation, through which a mentor's observations can be used to accelerate the learning of amplification. First, we define the homogeneous setting. Then, we develop the implicit imitation algorithm. Finally, we show how implicit imitation affects a number of simple problems that illustrate the role of the various mechanisms we describe."}, {"heading": "4.1 Homogeneous Actions", "text": "The homogeneous framework of action is defined as follows: We assume a single mentor m and an observer o, with the individual MDPs Mm = < S, Am, Prm, Rm > and Mo = < S, Ao, Pro, Ro > sharing the same state space (more precisely, we assume a trivial isomorphic mapping that allows us to identify their local states); we also assume that the mentor carries out a certain stationary policy \u0432m. We will often treat this policy as deterministic, but most of our remarks also apply to stochastic strategies; let us determine the support Supp (\u03c0m, s) for measures that require a non-zero percent probability of action on the part of the state. We assume that the observer has the same capabilities as the mentor in the following sense:"}, {"heading": "4.2 The Implicit Imitation Algorithm", "text": "The implicit imitation algorithm can be understood in terms of its components. First, we extract action models from a mentor, then we integrate this information into the observer's own estimations by supplementing the usual Bellman backup with mentor action models. A confidence-testing process ensures that we only use this enhanced model when the observer's model of the mentor is more reliable than the observer's model of his own behavior. Also, we extract occupancy information from the mentor trajectory observations to focus the observer's computational effort (to some extent) on specific parts of the state. Finally, we expand our action selection process to select actions that explore the high-quality regions detected by the mentor. The rest of this section focuses on each of these processes and how they fit together."}, {"heading": "4.2.1 Model Extraction", "text": "The information available to the observer in his quest for optimal action can be divided into two categories: First, with each action he takes, he receives an experiential pair < s, a, r, t >; in fact, we often ignore the collected reward r, since we assume that the reward function R is known in advance. As with standard model-based learning, any such experience can be used to update his own transition model Pro (s, a, \u00b7); second, with each mentor transition, the observer receives an experiential pair < s, t >. Again, the observer has no direct access to the actions taken by the mentor, but only the induced state transition. Suppose the mentor implements a deterministic, stationary policy approach, indicating the mentor's choice of action in state s."}, {"heading": "4.2.2 Augmented Bellman Backups", "text": "In fact, it is so that it is a matter of a way in which people are able to determine for themselves what they want and what they do not want. (...) It is so that people are able to decide whether they want it or not. (...) It is not so that people are able to decide whether they want it or not. (...) \"It is so that they do not want it. (...)\" (...) \"(...)\" (\") It is so that they do not want it. (...)\" (...) \"(...)\" (... \")\" (... \"(...)\" (... \")\" (... \")\" ((...) \"(()\") (((...) \"() ((...)\" () (()) (()) (()) (()) (()) () () () () () () () ()) (()) () () () () () () () ()) () () () () () () () () () () () () ()) () () () () () ()) () () () () () ()) () () ()) () () () () () () () () ()) () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()"}, {"heading": "4.2.3 Model Confidence", "text": "If the mentor's Markov chain is not ergodic, or if the mixed rate7 is sufficiently low, the mentor may visit a particular state relatively infrequently. An estimated mentor transition model, which corresponds to a state rarely (or never) visited by the mentor, can provide a very misleading estimate - based on the small sample or previous estimate of the mentor chain - of the value of the mentor's (unknown) action, and since the mentor's policies are not under the observer's control, this misleading value may persist for an extended period of time. As the augmented Bellman equation does not take into account the relative reliability of the mentor and observer models, the value of such a state can be overestimated. 8 This is the observer being tricked into overestimating the (unknown) action of the mentor, and consequently we overestimate the value of the state. To overcome this, we integrate an estimate of model trust into our augmented backups."}, {"heading": "4.2.4 Focusing", "text": "The extended Bellman backups improve the accuracy of the observer model. A second way an observer can use his / her observations of the mentor is to draw attention to the states visited by the mentor. In a model-based approach, the specific focusing mecha-10 will be used. Ideally, we would like to take into account not only the uncertainty of the model in the current state, but also the uncertainty of future states (Meuleau & Bourgine, 1999). If the mentor tends to visit interesting regions of space (e.g. if he shares a certain reward structure with the observer), then the significant values supported by the states visited by the mentor will restrict the observer's exploration into these regions. Second, the computing effort will focus on parts of the state space where the estimated model P-rm (t, hm), precise changes in the model (where the value of the observer is valued higher) and where the change of the observer is likely."}, {"heading": "4.2.5 Action Selection", "text": "The integration of exploration techniques into the policy of action selection is important for any strengthening of the learning algorithm to ensure convergence (unfortunately, in implicit imitation it plays a second, crucial role in helping the agent to use the information extracted from the mentor. Therefore, our improved convergence results are based on the greedy quality of the exploration strategy to tilt an observer towards the more highly rated paths that the mentor has shown. For expediency, we have applied the greedy action selection method, using an exploration rate that decreases over time. We could easily have used other semi-greedy methods such as Boltzmann Exploration. In the presence of a mentor, the greedy action selection becomes more complex. The observer examines his own actions in the usual manner and observes a best action that has a corresponding value Vo (s). A value is also calculated for the mentor's action."}, {"heading": "4.2.6 Model Extraction in Specific Reinforcement Learning Algorithms", "text": "In fact, it is so that most of them are able to survive themselves by placing themselves in the center of attention. (...) In fact, it is so that they are able to place themselves in the center. (...) It is so that they are able to place themselves in the center. (...) It is as if they are able to place themselves in the center. (...) \"(...)\" It is not so that they are able to place themselves in the center. (...) \"(...)\" (...) \"(...)\" (...) \"((...)\" ((...) \"(((()) (() (() () () () () () () () () () () () () () () () () () () () () ()) (()) (()) (()) (()) (()) (()) (()) (() ()) (() ()) (()) () () () () () () () () () () () () () () () () () () ()) () () () () () ()) () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () (() (() () () (() (() () (() (() (() ((() () (() ((() () ((() (() (() () () ((() ((() (() (() ((() (() (((() () (() ((() (() ((() ((() ((() (() ((("}, {"heading": "4.2.7 Extensions", "text": "The implicit imitation model can be easily expanded to extract model information from multiple mentors by mixing and matching the parts taken from each mentor to achieve good results by searching for the group of mentors they know in each state in order to find the mentor with the highest appreciation, and then comparing the appreciation of the \"best\" mentor with the observer's own appreciation by means of the trust test described above. Formally, the algorithm is expressed by the multi-augmented Bellman equation: V (s) = Ro (s) + GP (max a) {p) {p) SPro (s, a, t) V (t) V (s) rewards (s), max m) SPrm (s) V (t)}}} (11), with M (s) being the group of candidate mentors."}, {"heading": "4.3 Empirical Demonstrations", "text": "The following empirical tests involve the extraction of models and our focusing mechanism into prioritized overall experiences. The results illustrate the types of problems and scenarios in which implicit imitation can bring advantages to a reinforcement agent. In each of the experiments, an expert mentor is introduced into the experiment to serve as a model for the observer. In any case, the mentor follows a policy of low exploration and proximity (in the order of 0.01), which results in the mentor's trajectories being within a \"cluster\" surrounding optimal trajectories (reflecting good if not optimal trajectories). Even with a low level of exploration and proximity to the environment, mentors generally do not cover the entire state space, so confidence tests are important. Priority flight experiments are used in all of these experiments to determine the optimal number of backups per observed or experienced field."}, {"heading": "4.3.1 Experiment 1: The Imitation Effect", "text": "In our first experiment, we compare the performance of an observer using model extraction and an expert mentor with the performance of a control agent using independent reinforcement learning. Considering the uniformity of this grid world and the lack of interim rewards, no trust test is required. Both agents try to learn a policy that maximizes the discounted yield in a 10 x 10 grid world, starting in the upper left corner and looking for a target with a value of 1.0 in the lower right corner. In reaching the target, the agents each take a similar, if not identical, path, as the mentors are trained with a greedy strategy that values one path slightly higher than the rest. Action dynamics are loud, with the \"intended\" direction realized 90% of the time, and one of the other directions that are otherwise included (uniform)."}, {"heading": "4.3.2 Experiment 2: Scaling and Noise", "text": "The next experiment illustrates the sensitivity of the imitation to the size of the state room and the action sound level. Again, the observer uses model extraction, but not trust testing. In Figure 4, we plot the delta curves (i.e. performance differences between observers and control agents) for the \"base scenario\" just described, the \"scale\" scenario, in which the state room size is increased by 69 percent (to a 13 x 13 grid), and the \"poke\" scenario, in which the sound level is increased to 40 percent (the results are calculated on average over ten passes).The total gain represented by the area below the curves for the observer, and the prioritized sweeper that does not emulate, increases with the state room size. This reflects Whitehead's (1991a) observation that the exploration requirements for network worlds can quickly increase with the state room size, but the optimal path length can only increase with the help of the mentor."}, {"heading": "4.3.3 Experiment 3: Confidence Testing", "text": "The trust mechanism proposed in the previous section can prevent the observer from being deceived by misleading privileges about the mentor's chances of transition. To demonstrate the role of the trust mechanism in implicit imitation, we designed an experiment based on the scenario presented in Figure 5. Again, the task of the agent is to navigate from the upper left corner to the lower right corner of a 10-fold trust to achieve a reward of + 1. We have designed a pathological scenario in which the islands of high reward (+ 5) are surrounded by obstacles. As the observer's priors reflect eight connectivity and are uniform, the highly rated cells in the middle of each island are considered attainable."}, {"heading": "4.3.4 Experiment 4: Qualitative Difficulty", "text": "The next experiment shows how the potential gain of imitation can increase with the (qualitative) difficulty of the problem. The observer uses both model extraction and trust tests, whereby trust tests will not play a significant role in this. 13 In the \"labyrinth\" scenario, we present obstacles to increase the difficulty of the learning problem. The labyrinth is built on a 25 x 25 grid (Figure 7), with 286 obstacles complicating the agent's journey from the left to the lower right corner.The optimal solution takes the form of a meandering 133-step path, with distracting paths (up to a length of 22) branching off the solution path that require frequent tracing. The discount factor is 0.98. At 10 percent noise, the optimal target achievement rate is about six targets per 1000 steps.From the chart in Figure 8 (with results averaged over ten runs), we see that at this point of 1000, the control agent is only one point out of 1000."}, {"heading": "4.3.5 Experiment 5: Improving Suboptimal Policies by Imitation", "text": "The extended safeguard rule does not require that the reward structure of the mentor and the observer be identical. There are many useful scenarios in which the rewards are unequal, but the value functions and strategies share a common structure. In this experiment, we present an interesting scenario in which it is relatively easy to find a suboptimal solution, but difficult to find the optimal solution. However, once the observer has found this suboptimal path, he is able to use his observations of the mentor to see that there is a shortcut that significantly shortens the path to the goal. The structure of the scenario is Figure 9. The suboptimal solution is along the path from location 1 to the \"scenic route\" to location 2 and on to the destination 3. The mentor takes the vertical path from location 4 to location 5 by the shortcut. 14 To prevent the use of the shortcut by the inexperienced agent, it is accompanied by cells (marked \"*) that immediately bounce back from the observer to the initial state."}, {"heading": "4.3.6 Experiment 6: Multiple Mentors", "text": "The final experiment illustrates how model extraction can be easily expanded so that the observer can extract models from multiple mentors and exploit the most valuable parts of each. In Figure 11, the learner must move from starting position 1 to finishing position 4. In this experiment, two experts with different starting and finishing states serve as potential mentors. A mentor who moves from position 3 to position 5 repeatedly along the dotted line, while a second mentor leaves from position 2 and ends at position 4 along the dotted line. In this experiment, the observer must 14. A mentor who moves from 5 to 4 would not provide guidance without first knowing that the actions are reversible. Combine the information from the examples of the two mentors with independent explorations of their own to solve the problem. In Figure 12, we see that the observer successfully assembles these sources of information to learn faster than the control agent (the results are averaged over 10 passes)."}, {"heading": "5. Implicit Imitation in Heterogeneous Settings", "text": "If the homogeneity assumption is violated, the implicit imitation framework described above can lead to a dramatic slowdown in the student's rate of convergence and, in some cases, the student being stuck in a small neighborhood of the state space. Especially, if the student is unable to make the same state transition (or transition with the same probability) as the mentor in a particular state, he can drastically overestimate the value of that state. Excessive appreciation causes the student to repeatedly return to that state, even though his research never leads to a feasible action reaching the overestimated value. There is no mechanism to eliminate the influence of the Markov chain of the mentor on valuations - the observer can be extremely (and correctly) confident in his observations of the mentor model. The problem lies in the fact that the extended Bellman safeguard is justified by the assumption that the observer can duplicate any mentor action."}, {"heading": "5.1 Feasibility Testing", "text": "In such heterogeneous settings, we can prevent lock-up and poor convergence by using an explicit action feasibility check: before performing an extended backup, the observer will test whether the mentor's action is actually \"different\" from any of his actions to s, given his current estimated models. If this is the case, the extended backup is suppressed and a standard backup is used to update the value of the action. However, the decision is binary, but we could object to a smoother decision that measures the extent to which the mentor's action can be duplicated."}, {"heading": "5.3 Empirical Demonstrations", "text": "In this section, we demonstrate empirically the benefits of feasibility tests and k-step repairs, and show how the techniques can be used to overcome both differences in action between actors and small local differences in state-space topology. Issues here have been specifically selected to demonstrate the necessity and benefits of both feasibility tests and k-step repairs."}, {"heading": "5.3.1 Experiment 1: Necessity of Feasibility Testing", "text": "In this scenario, all parties must pull together to find a solution."}, {"heading": "5.3.2 Experiment 2: Changes to State Space", "text": "In fact, most of them are able to move to another world, in which they are able to move to another world, in which they are able to move, and in which they are able to move to another world, in which they are able to move, in which they are able to move."}, {"heading": "6. Applicability", "text": "In this section, we will examine how the assumptions and mechanisms we have outlined in the previous sections determine the types of problems that are suitable for implicit imitation. Repair, however, does not imply a short-term negative return. We have already identified a number of assumptions where implicit imitation is applicable - some assumptions under which other models of imitation or teaching cannot be applied in this scenario, and some assumptions that limit the applicability of our model, including: the lack of explicit communication between mentors and observers; independent objectives for mentors and observers; full observation of mentors by implication; inobservability of mentors; actions that go beyond the applicability of our model (although we consider them to be indefinitely applicable)."}, {"heading": "6.1 Predicting Performance", "text": "In this section, we examine two questions: firstly, since implicit imitation is applicable, when can implicit imitation cause an actor to come up with a suboptimal solution; and secondly, how will the performance of implicit imitation vary with the structural characteristics of the domains to which it is intended to apply? We show how an analysis of the internal structure of state space can be used to motivate a metric that predicts (roughly) implicit imitation performance; and we conclude with an analysis of how the problem space can be understood in terms of different regions that play different roles within an imitation context. In the implicit imitation model, we use observations from other actors to improve the observer's knowledge of its environment and then rely on a reasonable exploration policy to exploit this additional knowledge. A clear understanding of how environmental knowledge affects exploration is therefore central to understanding how implicit imitation works in a state."}, {"heading": "6.1.1 The Imitation Regions Framework", "text": "This year it is more than ever before."}, {"heading": "6.1.2 Cross regional textures", "text": "In our analysis of how model information affects imitation performance, we found that regions connected by precise action models enabled an observer to use mentor observations to learn about the most promising direction for exploration. We therefore see that any set of mentor observations will be more useful if they are focused on a connected region and less useful if they are distributed across the state in unconnected components. We are fortunate in fully observable environments that mentor observations tend to capture continuous paths and thus provide continuous regions of extended states. In partially observable environments, occlusion and noise may reduce the value of mentor observations in the absence of a model for predicting mentor states, as the effects of heterogeneity, whether due to differences in activity capacity in the mentor due to differences in action patterns or due to differences in connectivity in the environment, are not possible."}, {"heading": "6.2 The Fracture Metric", "text": "We try to characterize connectivity in the form of a metric. Since differences in reward structure, environmental dynamics, and the action models that affect connectivity would all manifest themselves as differences in policy between mentor and observer, we have designed a metric based on differences in the optimal policy of agents. We call this metric fracture essentially, it calculates the average minimum distance from a state where a mentor and observer disagree on a policy where mentor and observer agree on policy. This metric roughly captures the difficulty of observers in profitably exploiting mentors or observations to reduce their exploratory landscapes. Formally, we let the optimal policy of the mentor and the role of the observer be in the policy of the observer. Let S be the state space and the series of disputed states where the mentor and observer have different optimal policies."}, {"heading": "6.3 Suboptimality and Bias", "text": "It is worth asking when this will have a positive effect on the performance of the observer. The short answer is that a mentor pursuing an optimal policy for an observer will lead an observer to explore in the vicinity of the optimal policy, and this will generally prompt the observer to find the optimal solution. A more detailed response will require explicitly looking at exploration in the area of enhanced learning. In theory, such a greedy exploration policy with an appropriate rate of decay will eventually lead implicit imitators to agree on the same optimal solution as their unsupported counterparts. In practice, however, the exploration rate will typically decline faster to improve the early exploitation of the mentor. Given the practical but theoretically unsound exploration rates, an observer can opt for a mentor strategy that is feasible but not optimal."}, {"heading": "6.4 Specific Applications", "text": "In networks used for competitive purposes such as commerce, implicit imitation by an RL agent can be used to inform oneself about purchasing strategies or information filtering strategies of other agents in order to improve one's own behavior.In the controller, implicit imitation could be used to transfer knowledge from an existing trained controller that has already adapted to its customers to a new learning controller with a completely different architecture.Many modern products such as elevator controllers (Crites & Barto, 1998), cell traffic routers (Singh & Bertsekas, 1997) and fuel injection systems use adaptive controllers to optimize the performance of a system for specific user projects.In modernizing the technology of the underlying system, it is quite possible that sensors, actuators and the internal representation of the new system are incompatible with the old system."}, {"heading": "7. Extensions", "text": "The implicit imitation model presented above assumes certain restrictive assumptions regarding the structure of the decision problem to be solved (e.g. full observability, knowledge of the reward function, separate state and scope for action).While these simplistic assumptions supported the detailed development of the model, we believe that the basic intuitions and much of the technical development can be extended to richer problem classes. In this section, we propose several possible extensions, each of which offers a very interesting path for future research."}, {"heading": "7.1 Unknown Reward Functions", "text": "Our current paradigm assumes that the observer knows his or her own reward function. This assumption is in line with the view of the reward function as a form of automatic programming. However, we can loosen this limitation if we assume that there is some ability to generalize the observed rewards. Suppose that the expected reward can be expressed in the form of a probability distribution over the characteristics of the observer's state, Pr | f (so). In the model-based RL, this distribution can be learned by the actor through his or her own experience. If the same characteristics can be applied to the state of the mentor, then the observer can also use what he or she has learned through the reward distribution to estimate the expected reward for mentor states. This expands the paradigm to areas where rewards are unknown, but preserves the observer's ability to evaluate mentor experiences on his or's \"own terms.\" Imitation techniques that Russell would have developed around the year 2000 assumption that the mentor's function would remain identical to the mentor's function)."}, {"heading": "7.2 Interaction of agents", "text": "While we look at the general model of imitation in the context of stochastic games, limiting the model to non-interactive games essentially means that the standard problems associated with multi-agent interactions do not occur. Of course, there are many tasks that require interactions between actors; in such cases, implicit imitation offers the potential to accelerate learning. A general solution would undoubtedly require integrating imitation into more general models of multi-agent RL based on stochastic or Markov games (Littman, 1994; Hu & Wellman, 1998; Bowling & Veloso, 2001). This would undoubtedly be quite a challenging but rewarding effort. To give a simple example, in simple coordination problems (e.g., two mobile actors trying to evade each other while performing related tasks), we could imagine an imitator learning from a mentor reversing the roles of their roles as he considers how the observed state transition is influenced by their collective action."}, {"heading": "7.3 Partially Observable Domains", "text": "Extending this model to partially observable areas is crucial, as it is unrealistic in many situations to assume that a learner can constantly monitor a mentor's activities; the central idea of implicit imitation is to extract model information from the mentor's observations rather than duplicate the mentor's behavior, which means that the mentor's internal beliefs and policies are not (directly) relevant to the learner. We adopt a somewhat behavioral attitude and concern ourselves only with what the mentor's observed behaviors tell us about the possibilities inherent in the environment; the observer must maintain a state of belief about the mentor's current state, but this can be done with the same prized world model that the observer uses to update his own beliefs. Preliminary studies of such a model suggest that dealing with partial observability is practicable."}, {"heading": "7.4 Continuous and Model-Free Learning", "text": "In many realistic areas, continuous attributes and large spaces of state and action prohibit the use of explicit tabular Q-based representations (Q).Reinforcement learning in these areas is typically modified to use function approximation systems to estimate the Q function at points where there is no direct evidence.However, two important approaches are parameter-based models (e.g. neural networks) (Bertsekas & Tsitsiklis, 1996) and memory-based approaches (Atkeson, Moore, & Schaal, 1997).In both approaches, model-free learning is generally used, whereby the agent maintains a value function but uses the environment as an implicit model to perform backups provided by environmental observations.A direct approach to implicit imitation in a continuous setting would use a model-free learning paradigm (Watkins & Dayan, 1992)."}, {"heading": "8. Related Work", "text": "There are a number of reasons why people who are able to survive themselves survive themselves. There are a number of reasons why people are able to survive themselves. There are many reasons why people are able to survive themselves. There are many reasons why people are able to survive themselves. There are many reasons why people are able to survive themselves. There are people who are able to survive themselves. There are people who are able to survive themselves. There are people who are able to survive themselves. There are people who are able to survive themselves. There are people who are able to survive themselves. There are people who are able to survive themselves. There are people who are able to survive themselves."}, {"heading": "9. Concluding Remarks", "text": "We have a formal and principled approach to imitation, called implicit imitation. Although stochastic problems where explicit forms of communication are not possible, the underlying model-based framework combined with model-based extraction provide an alternative to other imitation and learning-by-observation systems. Our new approach uses a model to calculate the actions of an imitator without the observer accurately duplicating the actions of the mentor. We have shown implicit imitation to provide significant transfer capabilities to multiple test problems where it proves robust to integrate subqualifications of multiple mentors, and to provide benefits that increase with the difficulty of the problem."}, {"heading": "Acknowledgments", "text": "Thanks to the anonymous speakers for their suggestions and comments on earlier versions of this work and to Michael Littman for editorial suggestions. Price was supported by the NCE IRISIII Project BAC. Boutilier was supported by NSERC Research Grant OGP0121843 and the NCE IRIS-III Project BAC. Some parts of this paper were presented in: \"Implicit Imitation in Reinforcement Learning,\" Proceedings of the Sixteenth International Conference on Machine Learning (ICML-99), Bled, Slovenia, pp. 325-334 (1999) and \"Imitation and Reinforcement Learning in Agents with Heterogeneous Actions,\" Proceedings Fourteenth Biennial Conference of the Canadian Society for Computational Studies of Intelligence (AI 2001), Ottawa, pp. 111-120 (2001)."}], "references": [{"title": "Learning how to do things with imitation", "author": ["A. Alissandrakis", "C.L. Nehaniv", "K. Dautenhahn"], "venue": "AAAI Fall Symposium on Learning How to Do Things,", "citeRegEx": "Alissandrakis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Alissandrakis et al\\.", "year": 2000}, {"title": "Robot learning from demonstration", "author": ["C.G. Atkeson", "S. Schaal"], "venue": "In Proceedings of the Fourteenth International Conference on Machine Learning,", "citeRegEx": "Atkeson and Schaal,? \\Q1997\\E", "shortCiteRegEx": "Atkeson and Schaal", "year": 1997}, {"title": "Locally weighted learning for control", "author": ["C.G. Atkeson", "A.W. Moore", "S. Schaal"], "venue": "Artificial Intelligence Review,", "citeRegEx": "Atkeson et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Atkeson et al\\.", "year": 1997}, {"title": "Robot see, robot do: An overview of robot imitation", "author": ["P. Bakker", "Y. Kuniyoshi"], "venue": "In AISB96 Workshop on Learning in Robots and Animals,", "citeRegEx": "Bakker and Kuniyoshi,? \\Q1996\\E", "shortCiteRegEx": "Bakker and Kuniyoshi", "year": 1996}, {"title": "Dynamic Programming", "author": ["R.E. Bellman"], "venue": "Princeton University Press, Princeton.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Dynamic Programming: Deterministic and Stochastic Models", "author": ["D.P. Bertsekas"], "venue": "Prentice-Hall, Englewood Cliffs.", "citeRegEx": "Bertsekas,? 1987", "shortCiteRegEx": "Bertsekas", "year": 1987}, {"title": "Learning to communicate through imitation in autonomous robots", "author": ["A. Billard", "G. Hayes"], "venue": "In Proceedings of The Seventh International Conference on Artificial Neural Networks,", "citeRegEx": "Billard and Hayes,? \\Q1997\\E", "shortCiteRegEx": "Billard and Hayes", "year": 1997}, {"title": "Drama, a connectionist architecturefor control and learning in autonomous robots", "author": ["A. Billard", "G. Hayes"], "venue": "Adaptive Behavior Journal,", "citeRegEx": "Billard and Hayes,? \\Q1999\\E", "shortCiteRegEx": "Billard and Hayes", "year": 1999}, {"title": "Imitation skills as a means to enhance learning of a synthetic proto-language in an autonomous robot", "author": ["A. Billard", "G. Hayes", "K. Dautenhahn"], "venue": "In Proceedings of the AISB\u201999 Symposium on Imitation in Animals and Artifacts,", "citeRegEx": "Billard et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Billard et al\\.", "year": 1999}, {"title": "Sequential optimality and coordination in multiagent systems", "author": ["C. Boutilier"], "venue": "Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, pp. 478\u2013485 Stockholm.", "citeRegEx": "Boutilier,? 1999", "shortCiteRegEx": "Boutilier", "year": 1999}, {"title": "Decision theoretic planning: Structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Rational and convergent learning in stochastic games", "author": ["M. Bowling", "M. Veloso"], "venue": "In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Bowling and Veloso,? \\Q2001\\E", "shortCiteRegEx": "Bowling and Veloso", "year": 2001}, {"title": "Imitation as social exchange between humans and robot", "author": ["C. Breazeal"], "venue": "Proceedings of the AISB\u201999 Symposium on Imitation in Animals and Artifacts, pp. 96\u2013104 Edinburgh.", "citeRegEx": "Breazeal,? 1999", "shortCiteRegEx": "Breazeal", "year": 1999}, {"title": "Learning by imitation: a hierarchical approach", "author": ["R.W. Byrne", "A.E. Russon"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "Byrne and Russon,? \\Q1998\\E", "shortCiteRegEx": "Byrne and Russon", "year": 1998}, {"title": "Imitating human performances to automatically generate expressive jazz ballads", "author": ["D. Ca\u00f1amero", "J.L. Arcos", "R.L. de Mantaras"], "venue": "In Proceedings of the AISB\u201999 Symposium on Imitation in Animals and Artifacts,", "citeRegEx": "Ca\u00f1amero et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ca\u00f1amero et al\\.", "year": 1999}, {"title": "Acting optimally in partially observable stochastic domains", "author": ["A.R. Cassandra", "L.P. Kaelbling", "M.L. Littman"], "venue": "In Proceedings of the Twelfth National Conference on Artificial Intelligence,", "citeRegEx": "Cassandra et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1994}, {"title": "Intelligent social learning", "author": ["R. Conte"], "venue": "Proceedings of the AISB\u201900 Symposium on Starting from Society: the Applications of Social Analogies to Computational Systems Birmingham.", "citeRegEx": "Conte,? 2000", "shortCiteRegEx": "Conte", "year": 2000}, {"title": "Elevator group control using multiple reinforcement learning", "author": ["R. Crites", "A.G. Barto"], "venue": "agents. Machine-Learning,", "citeRegEx": "Crites and Barto,? \\Q1998\\E", "shortCiteRegEx": "Crites and Barto", "year": 1998}, {"title": "Model minimization in Markov decision processes", "author": ["T. Dean", "R. Givan"], "venue": "In Proceedings of the Fourteenth National Conference on Artificial Intelligence,", "citeRegEx": "Dean and Givan,? \\Q1997\\E", "shortCiteRegEx": "Dean and Givan", "year": 1997}, {"title": "Abstraction and approximate decision theoretic planning", "author": ["R. Dearden", "C. Boutilier"], "venue": "Artificial Intelligence,", "citeRegEx": "Dearden and Boutilier,? \\Q1997\\E", "shortCiteRegEx": "Dearden and Boutilier", "year": 1997}, {"title": "Model-based bayesian exploration", "author": ["R. Dearden", "N. Friedman", "D. Andre"], "venue": "In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Dearden et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1999}, {"title": "Probability and statistics", "author": ["M.H. DeGroot"], "venue": "Addison-Wesley, Reading, MA.", "citeRegEx": "DeGroot,? 1975", "shortCiteRegEx": "DeGroot", "year": 1975}, {"title": "Do robots ape", "author": ["J. Demiris", "G. Hayes"], "venue": "In Proceedings of the AAAI Fall Symposium on Socially Intelligent Agents,", "citeRegEx": "Demiris and Hayes,? \\Q1997\\E", "shortCiteRegEx": "Demiris and Hayes", "year": 1997}, {"title": "Active and passive routes to imitation", "author": ["J. Demiris", "G. Hayes"], "venue": "In Proceedings of the AISB\u201999 Symposium on Imitation in Animals and Artifacts,", "citeRegEx": "Demiris and Hayes,? \\Q1999\\E", "shortCiteRegEx": "Demiris and Hayes", "year": 1999}, {"title": "Observational learning in octopus vulgaris", "author": ["G. Fiorito", "P. Scotto"], "venue": null, "citeRegEx": "Fiorito and Scotto,? \\Q1992\\E", "shortCiteRegEx": "Fiorito and Scotto", "year": 1992}, {"title": "Practical reinforcement learning in continuous domains", "author": ["J. Forbes", "D. Andre"], "venue": "Tech. rep. UCB/CSD-00-1109,", "citeRegEx": "Forbes and Andre,? \\Q2000\\E", "shortCiteRegEx": "Forbes and Andre", "year": 2000}, {"title": "Robot programming by demonstration (RPD): Support the induction by human interaction", "author": ["H. Friedrich", "S. Munch", "R. Dillmann", "S. Bocionek", "M. Sassin"], "venue": "Machine Learning,", "citeRegEx": "Friedrich et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Friedrich et al\\.", "year": 1996}, {"title": "Algebraic Structure Theory of Sequential Machines. PrenticeHall, Englewood Cliffs", "author": ["J. Hartmanis", "R.E. Stearns"], "venue": null, "citeRegEx": "Hartmanis and Stearns,? \\Q1966\\E", "shortCiteRegEx": "Hartmanis and Stearns", "year": 1966}, {"title": "Multiagent reinforcement learning: Theoretical framework and an algorithm", "author": ["J. Hu", "M.P. Wellman"], "venue": "In Proceedings of the Fifthteenth International Conference on Machine Learning,", "citeRegEx": "Hu and Wellman,? \\Q1998\\E", "shortCiteRegEx": "Hu and Wellman", "year": 1998}, {"title": "Learning in Embedded Systems", "author": ["L.P. Kaelbling"], "venue": "MIT Press, Cambridge,MA.", "citeRegEx": "Kaelbling,? 1993", "shortCiteRegEx": "Kaelbling", "year": 1993}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "In Proceedings of the Fifthteenth International Conference on Machine Learning,", "citeRegEx": "Kearns and Singh,? \\Q1998\\E", "shortCiteRegEx": "Kearns and Singh", "year": 1998}, {"title": "Learning by watching: Extracting reusable task knowledge from visual observation of human performance", "author": ["Y. Kuniyoshi", "M. Inaba", "H. Inoue"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "Kuniyoshi et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kuniyoshi et al\\.", "year": 1994}, {"title": "Online miminization of transition systems", "author": ["D. Lee", "M. Yannakakis"], "venue": "In Proceedings of the 24th Annual ACM Symposium on the Theory of Computing", "citeRegEx": "Lee and Yannakakis,? \\Q1992\\E", "shortCiteRegEx": "Lee and Yannakakis", "year": 1992}, {"title": "Mondrian: A teachable graphical editor", "author": ["H. Lieberman"], "venue": "Cypher, A. (Ed.), Watch What I Do: Programming by Demonstration, pp. 340\u2013358. MIT Press, Cambridge, MA.", "citeRegEx": "Lieberman,? 1993", "shortCiteRegEx": "Lieberman", "year": 1993}, {"title": "Self-improvement based on reinforcement learning, planning and teaching", "author": ["Lin", "L.-J."], "venue": "Machine Learning: Proceedings of the Eighth International Workshop (ML91), 8, 323\u201327.", "citeRegEx": "Lin and L..J.,? 1991", "shortCiteRegEx": "Lin and L..J.", "year": 1991}, {"title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "author": ["Lin", "L.-J."], "venue": "Machine Learning, 8, 293\u2013321.", "citeRegEx": "Lin and L..J.,? 1992", "shortCiteRegEx": "Lin and L..J.", "year": 1992}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["M.L. Littman"], "venue": "Proceedings of the Eleventh International Conference on Machine Learning, pp. 157\u2013163 New Brunswick, NJ.", "citeRegEx": "Littman,? 1994", "shortCiteRegEx": "Littman", "year": 1994}, {"title": "A survey of algorithmic methods for partially observed Markov decision processes", "author": ["W.S. Lovejoy"], "venue": "Annals of Operations Research, 28, 47\u201366.", "citeRegEx": "Lovejoy,? 1991", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "Using communication to reduce locality in distributed multi-agent learning", "author": ["M.J. Mataric"], "venue": "Journal Experimental and Theoretical Artificial Intelligence, 10 (3), 357\u2013369.", "citeRegEx": "Mataric,? 1998", "shortCiteRegEx": "Mataric", "year": 1998}, {"title": "Behaviour-based primitives for articulated control", "author": ["M.J. Matari\u0107", "M. Williamson", "J. Demiris", "A. Mohan"], "venue": "Fifth International conference on simulation of adaptive behavior SAB\u201998,", "citeRegEx": "Matari\u0107 et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Matari\u0107 et al\\.", "year": 1998}, {"title": "Exploration of multi-state environments: Local mesures and back-propagation of uncertainty", "author": ["N. Meuleau", "P. Bourgine"], "venue": "Machine Learning,", "citeRegEx": "Meuleau and Bourgine,? \\Q1999\\E", "shortCiteRegEx": "Meuleau and Bourgine", "year": 1999}, {"title": "A comparison of the Bonferroni and Scheff\u00e9 bounds", "author": ["J. Mi", "A.R. Sampson"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "Mi and Sampson,? \\Q1993\\E", "shortCiteRegEx": "Mi and Sampson", "year": 1993}, {"title": "Knowledge, learning and machine intelligence", "author": ["D. Michie"], "venue": "Sterling, L. (Ed.), Intelligent Systems. Plenum Press, New York.", "citeRegEx": "Michie,? 1993", "shortCiteRegEx": "Michie", "year": 1993}, {"title": "LEAP: A learning apprentice for VLSI design", "author": ["T.M. Mitchell", "S. Mahadevan", "L. Steinberg"], "venue": "In Proceedings of the Ninth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Mitchell et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 1985}, {"title": "Prioritized sweeping: Reinforcement learning with less data and less real time", "author": ["A.W. Moore", "C.G. Atkeson"], "venue": "Machine Learning,", "citeRegEx": "Moore and Atkeson,? \\Q1993\\E", "shortCiteRegEx": "Moore and Atkeson", "year": 1993}, {"title": "Game Theory: Analysis of Conflict", "author": ["R.B. Myerson"], "venue": "Harvard University Press, Cambridge.", "citeRegEx": "Myerson,? 1991", "shortCiteRegEx": "Myerson", "year": 1991}, {"title": "Mapping between dissimilar bodies: Affordances and the algebraic foundations of imitation", "author": ["C. Nehaniv", "K. Dautenhahn"], "venue": "In Proceedings of the Seventh European Workshop on Learning Robots,", "citeRegEx": "Nehaniv and Dautenhahn,? \\Q1998\\E", "shortCiteRegEx": "Nehaniv and Dautenhahn", "year": 1998}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A.Y. Ng", "S. Russell"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning,", "citeRegEx": "Ng and Russell,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell", "year": 2000}, {"title": "Is it really imitation? a review of simple mechanisms in social information gathering", "author": ["J. Noble", "P.M. Todd"], "venue": "In Proceedings of the AISB\u201999 Symposium on Imitation in Animals and Artifacts,", "citeRegEx": "Noble and Todd,? \\Q1999\\E", "shortCiteRegEx": "Noble and Todd", "year": 1999}, {"title": "Cultural transmission of communications systems: Comparing observational and reinforcement learning models", "author": ["M. Oliphant"], "venue": "Proceedings of the AISB\u201999 Symposium on Imitation in Animals and Artifacts, pp. 47\u201354 Edinburgh.", "citeRegEx": "Oliphant,? 1999", "shortCiteRegEx": "Oliphant", "year": 1999}, {"title": "A Bayesian approach to imitation in reinforcement learning", "author": ["B. Price", "C. Boutilier"], "venue": "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence Acapulco", "citeRegEx": "Price and Boutilier,? \\Q2003\\E", "shortCiteRegEx": "Price and Boutilier", "year": 2003}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": "John Wiley and Sons, Inc., New York.", "citeRegEx": "Puterman,? 1994", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Imitation in free-ranging rehabilitant orangutans (pongopygmaeus)", "author": ["A. Russon", "B. Galdikas"], "venue": "Journal of Comparative Psychology,", "citeRegEx": "Russon and Galdikas,? \\Q1993\\E", "shortCiteRegEx": "Russon and Galdikas", "year": 1993}, {"title": "Learning to fly", "author": ["C. Sammut", "S. Hurst", "D. Kedzier", "D. Michie"], "venue": "In Proceedings of the Ninth International Conference on Machine Learning,", "citeRegEx": "Sammut et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Sammut et al\\.", "year": 1992}, {"title": "Knowing what to imitate and knowing when you succeed", "author": ["B. Scassellati"], "venue": "Proceedings of the AISB\u201999 Symposium on Imitation in Animals and Artifacts, pp. 105\u2013113 Edinburgh.", "citeRegEx": "Scassellati,? 1999", "shortCiteRegEx": "Scassellati", "year": 1999}, {"title": "Why experimentation can be better than perfect guidance", "author": ["T. Scheffer", "R. Greiner", "C. Darken"], "venue": "In Proceedings of the Fourteenth International Conference on Machine Learning,", "citeRegEx": "Scheffer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Scheffer et al\\.", "year": 1997}, {"title": "Multivariate Observations", "author": ["G.A.F. Seber"], "venue": "Wiley, New York.", "citeRegEx": "Seber,? 1984", "shortCiteRegEx": "Seber", "year": 1984}, {"title": "Stochastic games", "author": ["L.S. Shapley"], "venue": "Proceedings of the National Academy of Sciences, 39, 327\u2013332.", "citeRegEx": "Shapley,? 1953", "shortCiteRegEx": "Shapley", "year": 1953}, {"title": "Reinforcement learning for dynamic channel allocation in cellular telephone systems", "author": ["S.P. Singh", "D. Bertsekas"], "venue": "In Advances in Neural information processing systems,", "citeRegEx": "Singh and Bertsekas,? \\Q1997\\E", "shortCiteRegEx": "Singh and Bertsekas", "year": 1997}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "Skill reconstruction as induction of LQ controllers with subgoals", "author": ["D. \u0160uc", "I. Bratko"], "venue": "In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "\u0160uc and Bratko,? \\Q1997\\E", "shortCiteRegEx": "\u0160uc and Bratko", "year": 1997}, {"title": "Learning to predict by the method of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning, 3, 9\u201344.", "citeRegEx": "Sutton,? 1988", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Multi-agent reinforcement learning: Independent vs", "author": ["M. Tan"], "venue": "cooperative agents. In ICML93, pp. 330\u201337.", "citeRegEx": "Tan,? 1993", "shortCiteRegEx": "Tan", "year": 1993}, {"title": "Reconstruction human skill with machine learning", "author": ["T. Urbancic", "I. Bratko"], "venue": "In Eleventh European Conference on Artificial Intelligence,", "citeRegEx": "Urbancic and Bratko,? \\Q1994\\E", "shortCiteRegEx": "Urbancic and Bratko", "year": 1994}, {"title": "Two kinds of training information for evaluation function learning", "author": ["P.E. Utgoff", "J.A. Clouse"], "venue": "In Proceedings of the Ninth National Conference on Artificial Intelligence,", "citeRegEx": "Utgoff and Clouse,? \\Q1991\\E", "shortCiteRegEx": "Utgoff and Clouse", "year": 1991}, {"title": "Learning hierarchical performance knowledge by observation", "author": ["M. van Lent", "J. Laird"], "venue": "In Proceedings of the Sixteenth International Conference on Machine Learning,", "citeRegEx": "Lent and Laird,? \\Q1999\\E", "shortCiteRegEx": "Lent and Laird", "year": 1999}, {"title": "Do monkeys ape", "author": ["E. Visalberghi", "D. Fragazy"], "venue": null, "citeRegEx": "Visalberghi and Fragazy,? \\Q1990\\E", "shortCiteRegEx": "Visalberghi and Fragazy", "year": 1990}, {"title": "Complexity analysis of cooperative mechanisms in reinforcement learning", "author": ["S.D. Whitehead"], "venue": "Proceedings of the Ninth National Conference on Artificial Intelligence, pp. 607\u2013613 Anaheim.", "citeRegEx": "Whitehead,? 1991a", "shortCiteRegEx": "Whitehead", "year": 1991}, {"title": "Complexity and cooperation in q-learning", "author": ["S.D. Whitehead"], "venue": "Machine Learning. Proceedings of the Eighth International Workshop (ML91), pp. 363\u2013367.", "citeRegEx": "Whitehead,? 1991b", "shortCiteRegEx": "Whitehead", "year": 1991}], "referenceMentions": [{"referenceID": 37, "context": "When agents are viewed as independently trying to achieve their own ends, interesting issues in the interaction of agent policies (Littman, 1994) must be resolved (e.", "startOffset": 130, "endOffset": 145}, {"referenceID": 64, "context": "However, the fact that agents may share information for mutual gain (Tan, 1993) or distribute their search for optimal policies and communicate reinforcement signals to one another (Mataric, 1998) offers intriguing possibilities for accelerating reinforcement learning and enhancing agent performance.", "startOffset": 68, "endOffset": 79}, {"referenceID": 39, "context": "However, the fact that agents may share information for mutual gain (Tan, 1993) or distribute their search for optimal policies and communicate reinforcement signals to one another (Mataric, 1998) offers intriguing possibilities for accelerating reinforcement learning and enhancing agent performance.", "startOffset": 181, "endOffset": 196}, {"referenceID": 69, "context": "This type of learning can be brought about through explicit teaching or demonstration (Atkeson & Schaal, 1997; Lin, 1992; Whitehead, 1991a), by sharing of privileged information (Mataric, 1998), or through an explicit cognitive representation of imitation (Bakker & Kuniyoshi, 1996).", "startOffset": 86, "endOffset": 139}, {"referenceID": 39, "context": "This type of learning can be brought about through explicit teaching or demonstration (Atkeson & Schaal, 1997; Lin, 1992; Whitehead, 1991a), by sharing of privileged information (Mataric, 1998), or through an explicit cognitive representation of imitation (Bakker & Kuniyoshi, 1996).", "startOffset": 178, "endOffset": 193}, {"referenceID": 44, "context": "We illustrate its effectiveness empirically by incorporating it into Moore and Atkeson\u2019s (1993) prioritized sweeping algorithm.", "startOffset": 69, "endOffset": 96}, {"referenceID": 38, "context": "Partially observable MDPs (POMDPs) (Cassandra, Kaelbling, & Littman, 1994; Lovejoy, 1991; Smallwood & Sondik, 1973) are much more computationally demanding than fully observable MDPs.", "startOffset": 35, "endOffset": 115}, {"referenceID": 5, "context": "We refer the reader to Bertsekas (1987); Boutilier, Dean and Hanks (1999); and Puterman (1994) for further material on MDPs.", "startOffset": 23, "endOffset": 40}, {"referenceID": 5, "context": "We refer the reader to Bertsekas (1987); Boutilier, Dean and Hanks (1999); and Puterman (1994) for further material on MDPs.", "startOffset": 23, "endOffset": 74}, {"referenceID": 5, "context": "We refer the reader to Bertsekas (1987); Boutilier, Dean and Hanks (1999); and Puterman (1994) for further material on MDPs.", "startOffset": 23, "endOffset": 95}, {"referenceID": 52, "context": "We are guaranteed that such optimal (stationary) policies exist in our setting (Puterman, 1994).", "startOffset": 79, "endOffset": 95}, {"referenceID": 4, "context": "Value iteration (Bellman, 1957) is a simple iterative approximation algorithm for optimal policy construction.", "startOffset": 16, "endOffset": 31}, {"referenceID": 52, "context": ", its value is within \u03b5 of V ) (Puterman, 1994).", "startOffset": 31, "endOffset": 47}, {"referenceID": 59, "context": "For further details, please refer to the texts of Sutton and Barto (1998) and Bertsekas and Tsitsiklis (1996), and the survey of Kaelbling, Littman and Moore (1996).", "startOffset": 50, "endOffset": 74}, {"referenceID": 5, "context": "For further details, please refer to the texts of Sutton and Barto (1998) and Bertsekas and Tsitsiklis (1996), and the survey of Kaelbling, Littman and Moore (1996).", "startOffset": 78, "endOffset": 110}, {"referenceID": 5, "context": "For further details, please refer to the texts of Sutton and Barto (1998) and Bertsekas and Tsitsiklis (1996), and the survey of Kaelbling, Littman and Moore (1996).", "startOffset": 78, "endOffset": 165}, {"referenceID": 21, "context": "For instance, we might assume a Dirichlet (Generalized Beta) distribution (DeGroot, 1975) with parameters n(s, a, t) associated with each possible successor state t.", "startOffset": 74, "endOffset": 89}, {"referenceID": 4, "context": "Alternatively, one could use computational effort judiciously to apply Bellman backups only at those states whose values (or Q-values) are likely to change the most given a change in the model. Moore and Atkeson\u2019s (1993) prioritized sweeping algorithm does just this.", "startOffset": 71, "endOffset": 221}, {"referenceID": 29, "context": "An early approximation of this scheme can be found in the interval estimation method (Kaelbling, 1993).", "startOffset": 85, "endOffset": 102}, {"referenceID": 20, "context": "Bayesian methods have also been used to calculate the expected value of information to be gained from exploration (Meuleau & Bourgine, 1999; Dearden et al., 1999).", "startOffset": 114, "endOffset": 162}, {"referenceID": 62, "context": "For example, TD-methods (Sutton, 1988) and Q-learning (Watkins & Dayan, 1992) have both proven to be among the more popular methods for reinforcement learning.", "startOffset": 24, "endOffset": 38}, {"referenceID": 58, "context": "We begin by introducing a general model for stochastic games (Shapley, 1953; Myerson, 1991), and then impose various assumptions and restrictions on this general model that allow us to focus on the key aspects of implicit imitation.", "startOffset": 61, "endOffset": 91}, {"referenceID": 46, "context": "We begin by introducing a general model for stochastic games (Shapley, 1953; Myerson, 1991), and then impose various assumptions and restrictions on this general model that allow us to focus on the key aspects of implicit imitation.", "startOffset": 61, "endOffset": 91}, {"referenceID": 46, "context": "Though Shapley\u2019s (1953) original formulation of stochastic games involved a zero-sum (fully competitive) assumption, various generalizations of the model have been proposed allowing for arbitrary relationships between agents\u2019 utility functions (Myerson, 1991).", "startOffset": 244, "endOffset": 259}, {"referenceID": 57, "context": "Though Shapley\u2019s (1953) original formulation of stochastic games involved a zero-sum (fully competitive) assumption, various generalizations of the model have been proposed allowing for arbitrary relationships between agents\u2019 utility functions (Myerson, 1991).", "startOffset": 7, "endOffset": 24}, {"referenceID": 9, "context": "For example, see the fully cooperative multiagent MDP model proposed by Boutilier (1999).", "startOffset": 72, "endOffset": 89}, {"referenceID": 70, "context": "Our model is also distinct from models of explicit teaching (Lin, 1992; Whitehead, 1991b): we do not assume that the mentor has any incentive to move through its environment in a way that explicitly guides the learner to explore its own environment and action space more effectively.", "startOffset": 60, "endOffset": 89}, {"referenceID": 21, "context": "For both the mentor\u2019s Markov chain and the observer\u2019s action transitions, we assume a Dirichlet prior over the parameters of each of these multinomial distributions (DeGroot, 1975).", "startOffset": 165, "endOffset": 180}, {"referenceID": 28, "context": "terest of simplicity we have employed an approximate method for combining information sources inspired by Kaelbling\u2019s (1993) interval estimation method.", "startOffset": 106, "endOffset": 125}, {"referenceID": 69, "context": "This reflects Whitehead\u2019s (1991a) observation that for grid worlds, exploration requirements can increase quickly with state space size, but that the optimal path length increases only linearly.", "startOffset": 14, "endOffset": 34}, {"referenceID": 57, "context": "We deal with the multivariate complications by performing the Bonferroni test (Seber, 1984), which has been shown to give good results in practice (Mi & Sampson, 1993), is efficient to compute, and is known to be robust to dependence between variables.", "startOffset": 78, "endOffset": 91}, {"referenceID": 31, "context": "Our partitioning of states into explored, blind and augmented regions bears some resemblance to Kearns and Singh\u2019s (1998) partitioning of state space into known and unknown regions.", "startOffset": 96, "endOffset": 122}, {"referenceID": 54, "context": "Research within the behavioral cloning paradigm has investigated transfer in applications such as piloting aircraft (Sammut et al., 1992) and controlling loading cranes (\u0160uc & Bratko, 1997).", "startOffset": 116, "endOffset": 137}, {"referenceID": 37, "context": "A general solution requires the integration of imitation into more general models for multiagent RL based on stochastic or Markov games (Littman, 1994; Hu & Wellman, 1998; Bowling & Veloso, 2001).", "startOffset": 136, "endOffset": 195}, {"referenceID": 20, "context": "These updates are based on a Bayesian formulation of implicit imitation which is, in turn, based on Bayesian RL (Dearden et al., 1999).", "startOffset": 112, "endOffset": 134}, {"referenceID": 2, "context": "Memorybased approaches, such as locally-weighted regression (Atkeson et al., 1997), not only provide estimates for functions at points previously unvisited, they also maintain the evidence", "startOffset": 60, "endOffset": 82}, {"referenceID": 16, "context": "A number of researchers have pointed out, however, that social facilitation can take many forms (Conte, 2000; Noble & Todd, 1999).", "startOffset": 96, "endOffset": 129}, {"referenceID": 16, "context": "A number of researchers have pointed out, however, that social facilitation can take many forms (Conte, 2000; Noble & Todd, 1999). For instance, a mentor\u2019s attention to an object can draw an observer\u2019s attention to it and thereby lead the observer to manipulate the object independently of the model provided by the mentor. \u201cTrue imitation\u201d is therefore typically defined in a more restrictive fashion. Visalberghi and Fragazy (1990) cite Mitchell\u2019s definition:", "startOffset": 97, "endOffset": 434}, {"referenceID": 54, "context": "Highly-trained mentors following an optimal policy with small coverage of the state space yield less reliable clones than those that make more mistakes (Sammut et al., 1992).", "startOffset": 152, "endOffset": 173}, {"referenceID": 54, "context": "Finally, it has been observed that successful clones would often outperform the original mentor due to the \u201ccleanup effect\u201d (Sammut et al., 1992).", "startOffset": 124, "endOffset": 145}, {"referenceID": 43, "context": "One of the original goals of behavioral cloning (Michie, 1993) was to extract knowledge from humans to speed up the design of controllers.", "startOffset": 48, "endOffset": 62}, {"referenceID": 54, "context": "Early behavioral cloning research took advantage of supervised learning techniques such as decision trees (Sammut et al., 1992).", "startOffset": 106, "endOffset": 127}, {"referenceID": 44, "context": "Learning apprentice systems (Mitchell et al., 1985) also attempted to extract useful knowledge by watching users, but the goal of apprentices is not to independently solve problems.", "startOffset": 28, "endOffset": 51}, {"referenceID": 34, "context": "Learning apprentices are closely related to programming by demonstration systems (Lieberman, 1993).", "startOffset": 81, "endOffset": 98}, {"referenceID": 32, "context": "more sophisticated techniques to extract actions from visual perceptions and abstract these actions for future use (Kuniyoshi et al., 1994).", "startOffset": 115, "endOffset": 139}, {"referenceID": 64, "context": "An early precursor to imitation can be found in work on sharing of perceptions between agents (Tan, 1993).", "startOffset": 94, "endOffset": 105}, {"referenceID": 69, "context": "Closer to imitation is the idea of replaying the perceptions and actions of one agent for a second agent (Lin, 1991; Whitehead, 1991a).", "startOffset": 105, "endOffset": 134}, {"referenceID": 56, "context": "Friedrich, Munch, Dillmann, Bocionek, & Sassin, 1996), aeration plants (Scheffer et al., 1997), and container loading cranes (\u0160uc & Bratko, 1997; Urbancic & Bratko, 1994).", "startOffset": 71, "endOffset": 94}, {"referenceID": 69, "context": "Imitation learning has also been applied to acceleration of generic reinforcement learning (Lin, 1991; Whitehead, 1991a).", "startOffset": 91, "endOffset": 120}, {"referenceID": 12, "context": "Less traditional applications include transfer of musical style (Ca\u00f1amero, Arcos, & de Mantaras, 1999) and the support of a social atmosphere (Billard, Hayes, & Dautenhahn, 1999; Breazeal, 1999; Scassellati, 1999).", "startOffset": 142, "endOffset": 213}, {"referenceID": 55, "context": "Less traditional applications include transfer of musical style (Ca\u00f1amero, Arcos, & de Mantaras, 1999) and the support of a social atmosphere (Billard, Hayes, & Dautenhahn, 1999; Breazeal, 1999; Scassellati, 1999).", "startOffset": 142, "endOffset": 213}, {"referenceID": 8, "context": "Imitation has also been investigated as a route to language acquisition and transmission (Billard et al., 1999; Oliphant, 1999).", "startOffset": 89, "endOffset": 127}, {"referenceID": 50, "context": "Imitation has also been investigated as a route to language acquisition and transmission (Billard et al., 1999; Oliphant, 1999).", "startOffset": 89, "endOffset": 127}, {"referenceID": 3, "context": "Bakker and Kuniyoshi (1996) describe a number of these.", "startOffset": 0, "endOffset": 28}, {"referenceID": 9, "context": "Boutilier was supported by NSERC Research Grant OGP0121843, and the NCE IRIS-III Project BAC. Some parts of this paper were presented in \u201cImplicit Imitation in Reinforcement Learning,\u201d Proceedings of the Sixteenth International Conference on Machine Learning (ICML-99), Bled, Slovenia, pp.325\u2013334 (1999) and \u201dImitation and Reinforcement Learning in Agents with Heterogeneous Actions,\u201d Proceedings Fourteenth Biennial Conference of the Canadian Society for Computational Studies of Intelligence (AI 2001), Ottawa, pp.", "startOffset": 0, "endOffset": 304}, {"referenceID": 9, "context": "Boutilier was supported by NSERC Research Grant OGP0121843, and the NCE IRIS-III Project BAC. Some parts of this paper were presented in \u201cImplicit Imitation in Reinforcement Learning,\u201d Proceedings of the Sixteenth International Conference on Machine Learning (ICML-99), Bled, Slovenia, pp.325\u2013334 (1999) and \u201dImitation and Reinforcement Learning in Agents with Heterogeneous Actions,\u201d Proceedings Fourteenth Biennial Conference of the Canadian Society for Computational Studies of Intelligence (AI 2001), Ottawa, pp.111\u2013120 (2001).", "startOffset": 0, "endOffset": 531}], "year": 2011, "abstractText": "Imitation can be viewed as a means of enhancing learning in multiagent environments. It augments an agent\u2019s ability to learn useful behaviors by making intelligent use of the knowledge implicit in behaviors demonstrated by cooperative teachers or other more experienced agents. We propose and study a formal model of implicit imitation that can accelerate reinforcement learning dramatically in certain cases. Roughly, by observing a mentor, a reinforcement-learning agent can extract information about its own capabilities in, and the relative value of, unvisited parts of the state space. We study two specific instantiations of this model, one in which the learning agent and the mentor have identical abilities, and one designed to deal with agents and mentors with different action sets. We illustrate the benefits of implicit imitation by integrating it with prioritized sweeping, and demonstrating improved performance and convergence through observation of single and multiple mentors. Though we make some stringent assumptions regarding observability and possible interactions, we briefly comment on extensions of the model that relax these restricitions.", "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}