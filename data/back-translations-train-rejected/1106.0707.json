{"id": "1106.0707", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2011", "title": "Efficient Reinforcement Learning Using Recursive Least-Squares Methods", "abstract": "The recursive least-squares (RLS) algorithm is one of the most well-known algorithms used in adaptive filtering, system identification and adaptive control. Its popularity is mainly due to its fast convergence speed, which is considered to be optimal in practice. In this paper, RLS methods are used to solve reinforcement learning problems, where two new reinforcement learning algorithms using linear value function approximators are proposed and analyzed. The two algorithms are called RLS-TD(lambda) and Fast-AHC (Fast Adaptive Heuristic Critic), respectively. RLS-TD(lambda) can be viewed as the extension of RLS-TD(0) from lambda=0 to general lambda within interval [0,1], so it is a multi-step temporal-difference (TD) learning algorithm using RLS methods. The convergence with probability one and the limit of convergence of RLS-TD(lambda) are proved for ergodic Markov chains. Compared to the existing LS-TD(lambda) algorithm, RLS-TD(lambda) has advantages in computation and is more suitable for online learning. The effectiveness of RLS-TD(lambda) is analyzed and verified by learning prediction experiments of Markov chains with a wide range of parameter settings. The Fast-AHC algorithm is derived by applying the proposed RLS-TD(lambda) algorithm in the critic network of the adaptive heuristic critic method. Unlike conventional AHC algorithm, Fast-AHC makes use of RLS methods to improve the learning-prediction efficiency in the critic. Learning control experiments of the cart-pole balancing and the acrobot swing-up problems are conducted to compare the data efficiency of Fast-AHC with conventional AHC. From the experimental results, it is shown that the data efficiency of learning control can also be improved by using RLS methods in the learning-prediction process of the critic. The performance of Fast-AHC is also compared with that of the AHC method using LS-TD(lambda). Furthermore, it is demonstrated in the experiments that different initial values of the variance matrix in RLS-TD(lambda) are required to get better performance not only in learning prediction but also in learning control. The experimental results are analyzed based on the existing theoretical work on the transient phase of forgetting factor RLS methods.", "histories": [["v1", "Fri, 3 Jun 2011 16:44:06 GMT  (339kb)", "http://arxiv.org/abs/1106.0707v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["h he", "d hu", "x xu"], "accepted": false, "id": "1106.0707"}, "pdf": {"name": "1106.0707.pdf", "metadata": {"source": "CRF", "title": "Efficient Reinforcement Learning Using Recursive Least-Squares Methods", "authors": ["Xin Xu", "Han-gen He", "Dewen Hu"], "emails": ["XUXIN_MAIL@263.NET", "HEHANGEN@CS.HN.CN", "DWHU@NUDT.EDU.CN"], "sections": [{"heading": null, "text": "The recursive least-squares (RLS) algorithm is one of the most well-known algorithms used in adaptive filtering, system identification and adaptive control. Its popularity is mainly due to its fast convergence rate, which is considered optimal in practice. In this paper, RLS methods are used to solve amplification problems, where two new reinforcement learning algorithms are proposed and analyzed using linear value approximation methods.The two algorithms are called RLS-TD () and Fast-AHC (Fast Adaptive Heuristic Critic), or RLS-TD (), respectively, can be considered an extension of RLS-TD (0) to 0 1, so that it is a multi-level temporal difference (TD) to learning algorithms using RLS methods."}, {"heading": "1. Introduction", "text": "In recent years, enhanced learning (RL) has become an active area of research not only in the field of machine learning, but also in control engineering, operations research and robotics (Kaelbling et al., 1996; Bertsekas, et al., 1996; Sutton and Barto, 1998; Lin, 1992). It is a computerized approach to understand and automate targeted learning and decisions without relying on exemplary supervision or complete models of the environment. In RL, an agent is placed in an initially unknown environment and receives only evaluative feedback from the environment. The ultimate goal of RL is to learn a strategy for selecting actions, such as the expected sum of discounted rewards is maximized.Since many problems in the real world are sequential decision-making processes with delayed evaluative feedback, research in RL is focused on theory and algorithms of learning."}, {"heading": "2. Previous Work on Linear Temporal-Difference Algorithms", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves."}, {"heading": "3. The RLS-TD( ) Algorithm", "text": "For the Markov chain discussed above, when linear function approximators are used, the lowest methods of quantum estimation of (11) have the following objective function.211) () (TttTtt XbWXAJ (19), where nt nn t RXbRXA) (,) (are defined as (12) and (13), each is a Euclid standard and n is the number of basic functions. In LS-TD (), where the least square estimates of the weight vector W are calculated according to the following equation.) () ()) () () (TttTttTTTDLS XbXAbAW (20) whereTtt T tT xxzXAA 0 0) () () ())) (21) TtttTT tXbb 00 (22)."}, {"heading": "4. Learning Prediction Experiments on Markov Chains", "text": "In this section, an illustrative example is given to show the effectiveness of the proposed RLS-TD () algorithm. [LS] algorithm are each 8.4 states. Furthermore, algorithmic performance is examined under the influence of the initializing constant. The example is a finite state absorbing Markov chain called the Hop World Problem (Boyan, 1999). As shown in Figure 1, the Hop World problem is a 13 state Markov chain with an absorbing status. In Figure 1, state 12 is the initial state for each path and state 0 the absorbing state. Each non-absorbing state has two possible state transitions with a transition probability of 0.5. Each state transition has reward -3 with the exception of the transition from state 1 to state 0, which has a reward of -2. The true value function for the state i (0 i 12) is -2i.To apply linear differential algorithmic algorithms to the set of value problems, or one of four is selected."}, {"heading": "5. The Fast-AHC Algorithm and Two Learning Control Experiments", "text": "In this section, the Fast-AHC algorithm is proposed as a learning predictor for solving learning control problems based on the above results. Two learning control experiments will be performed to demonstrate the efficiency of Fast-AHC."}, {"heading": "5.1 The Fast-AHC Algorithm", "text": "The ultimate answer to the question of the why and the why is the question of the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why, the why and the why and the why?"}, {"heading": "5.2 Learning Control Experiments on The Cart-Pole Balancing Problem", "text": "In fact, it is such that one is able to establish oneself in a country in which most people are able to go to another world, in which they are able to go to another world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in, in which they live, in which they live, in which they live, in which they live, live, in which they live, in which they live, live, live, live, in which they live"}, {"heading": "5.3 Learning Control Experiments of The Acrobot", "text": "In this connection also the fact is to be called that it concerns with the two different types, which are in the most different expressions. (...) In this case it is so that the individual expressions of the expressions of the expressions of the expressions of the expressions, the expressions of the expressions, the expressions of the expressions and the expressions of the expressions of the expressions, the expressions of the expressions and the expressions of the expressions of the expressions, the expressions of the expressions, the expressions and the expressions of the expressions of the expressions, the expressions and the expressions of the expressions of the expressions, the expressions of the expressions of the expressions, the expressions of the expressions, the expressions of the expressions of the expressions, the expressions of the expressions and the"}, {"heading": "5.4 Analysis of The Experimental Results", "text": "Based on the above experimental results, it can be concluded that by using the RLS-TD () algorithm in the Critics Network, the Fast AHC algorithm can achieve better performance than traditional AHC algorithms, where fewer trials or data are required to arrive at a near-optimal policy. As is known, one difficulty in applying RL methods is their slow convergence, especially in cases where learning data is difficult to generate. For the Fast AHC algorithm, although more computation per step is required, it will not be a serious problem if the number of linear state characteristics is small. In all of our learning control experiments, hash techniques are used to reduce the state characteristics in CMAC networks, so that the computation of Fast AHC methods can be reduced to an economic quantity. Nevertheless, if the state of the characteristics is large, conventional AHC methods are preferred."}, {"heading": "6. Conclusions and Future Work", "text": "Two new reinforcement learning algorithms using RLS methods, referred to as RLS-TD () and Fast-AHC respectively, are proposed in this paper. RLS-TD () can be used to solve learning forecast problems more efficiently than conventional linear TD () algorithms. Convergence with probability 1 is also proven for RLS-TD () and the limit of convergence is also analyzed. Experimental results on learning forecast problems show that the RLS-TD () algorithm is superior to conventional TD algorithms in data efficiency, and it also eliminates the design problem of step sizes in linear TD () algorithms. RLS-TD () may be used as an extension of RLS-TD (0) to general 0 < 1 Although the effect of RLS-TD on the convergence speed is not significant, the use of RLS-TD algorithms will not be significant (in some cases)."}, {"heading": "Acknowledgements", "text": "This work is supported by the National Natural Science Foundation of China under Grants60075020, 60171003 and the China University Key Teacher's Fellowship. We would like to thank the anonymous reviewer and co-editor Michael L. Littman for their insights and constructive criticism, which have helped to significantly improve the work."}, {"heading": "Appendix A. Derivation of the RLS-TD( ) Algorithm", "text": "For the derivation of Rtttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt"}, {"heading": "Appendix B. Proof of Theorem 1", "text": "In order to investigate the steady property of the Markov chain defined in section 3, we construct a stationary process as follows: Let {xt} be a Markov chain that develops according to the transition matrix P and is already in its steady state, which means that Pr {xt = i} = (i) for all i and t. In view of any example path of the Markov chain, let us define tt xz () (72) Then}, {1 tttt zxxX is a stationary process identical to the one discussed in (Tsitsiklisand Roy, 1997). Let D define an N diagonal matrix with diagonal entries (1), (2),..., (N) where N is the cardinality of the state space X. Then, Lemma 2 can be derived as the following (Tsitsiklisand Roy, 1997). (Tsitsiklis and Roy, 1997) XxxT [TxT] are equal to Dx.1 and DxT (following DxT-DR.1)."}, {"heading": "Appendix C. Some details of the coding structures of CMAC networks", "text": "In the following discussion, the coding structures of the CMAC networks in the CMAC networks are presented.] 12,12 [,] deg / 50, deg / 50 [ss] 4,2,4.2 [x,] 1,1 [xFor the critical network, C = 4 and (61) are used and the total memory size is 100. (2) CMAC coding structures in the acrobots (50) and (51) are used and the total memory size is 30. In the simulation, the angles of], [and the angular velocities of] 4.4 [1,] -0.5 [2], CMAC coding structures are used in the acrobot swing-up problems: In the simulation, the angles of 0, [and the angular velocities of] 4.4 [1,] 9.9 [2], the numbers of the acrobot swing-up problem are equal: In the simulation, the numbers for the acrobot swing-up range} (the angle velocities {1 = 4) and the acrobat velocities {1 = 1 are limited by the acrobat velocities (the acrobat velocity = 1)."}, {"heading": "16, 185-202.", "text": "Tsitsiklis J.N. & Roy B.V. (1994): Feature-based methods for large-scale dynamic programming.Neural Computation. 6 (6), 1185-1201.Tsitsiklis J.N. & Roy B.V. (1997): An Analysis of Learning Time Differences with Functional Approximation. IEEE Transactions on Automatic Control. 42 (5), 674-690.Watkins C.J.C.H. & Dayan P. (1992): Q-Learning. Machine Learning. 8, 279-292.Young P. (1984): Recursive Estimation and Time-Series Analysis. Springer-Verlag."}], "references": [{"title": "Neurodynamic Programming. Belmont, Mass.: Athena Scientific", "author": ["D.P. Bertsekas"], "venue": "Berenji H.R. & Khedkar P", "citeRegEx": "Bertsekas,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas", "year": 1996}, {"title": "The convergence of TD", "author": ["Dayan P"], "venue": "Machine Learning,", "citeRegEx": "P..1992..,? \\Q1994\\E", "shortCiteRegEx": "P..1992..", "year": 1994}, {"title": "Convergence of the RLS and LMS adaptive filters", "author": ["1097-1110. Eweda E", "O. Macchi"], "venue": "IEEE Trans. Circuits and Systems,", "citeRegEx": "E. and Macchi,? \\Q1987\\E", "shortCiteRegEx": "E. and Macchi", "year": 1987}, {"title": "On the convergence of stochastic iterative dynamic programming algorithms", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "RLS algorithms. In Proc. of the IEEE International Conference on Acoustics, Speech and Signal Processing. Jaakkola T.,", "citeRegEx": "Kaelbling et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1994}, {"title": "Self-improving reactive agents based reinforcement learning, planning and teaching", "author": ["L.J. Lin"], "venue": "Machine Learning,", "citeRegEx": "Lin,? \\Q1992\\E", "shortCiteRegEx": "Lin", "year": 1992}, {"title": "Analysis of recursive stochastic algorithm", "author": ["L. Ljung"], "venue": "IEEE. Transactions on Automatic Control,", "citeRegEx": "Ljung,? \\Q1977\\E", "shortCiteRegEx": "Ljung", "year": 1977}, {"title": "Study of the transient phase of the forgetting factor RLS", "author": ["G.V. Moustakides"], "venue": "brain-model problem. Ph.D. Thesis,", "citeRegEx": "Moustakides,? \\Q1997\\E", "shortCiteRegEx": "Moustakides", "year": 1997}, {"title": "Convergence results for singlestep on-policy reinforcement-learning algorithms", "author": ["Singh S.P", "T. Jaakkola", "M.L. Littman", "C. Szepesvari"], "venue": "Machine Learning,", "citeRegEx": "S.P. et al\\.,? \\Q2000\\E", "shortCiteRegEx": "S.P. et al\\.", "year": 2000}, {"title": "Asynchronous stochastic approximation and Q-learning", "author": ["J.N. Tsitsiklis"], "venue": "Machine Learning,", "citeRegEx": "Tsitsiklis,? \\Q1994\\E", "shortCiteRegEx": "Tsitsiklis", "year": 1994}, {"title": "An analysis of temporal difference learning with function approximation", "author": ["J.N. Tsitsiklis"], "venue": "IEEE Transactions on Automatic Control", "citeRegEx": "Tsitsiklis,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis", "year": 1997}], "referenceMentions": [{"referenceID": 8, "context": "Therefore, the theory and algorithms for multi-step learning prediction become an important topic in RL and much research work has been done in the literature (Sutton, 1988; Tsitsiklis and Roy, 1997). Among the proposed multi-step learning prediction methods, temporal-difference (TD) learning (Sutton, 1988) is one of the most popular methods. It was studied and applied in the early research of machine learning, including the celebrated checkers-playing program (Minsky, 1954; Samuel, 1959). In 1988, Sutton presented the first formal description of temporal- difference methods and the TD( ) algorithm (Sutton,1988). Convergence results are established for tabular temporal-difference learning algorithms where the cardinality of tunable parameters is the same as that of the state space (Sutton, 1988; Watkins,et al.,1992; Dayan,et al., 1994; Jaakkola, et al.,1994). Since many real-world applications have large or infinite state space, value function approximation (VFA) methods need to be used in those cases. When combined with nonlinear value function approximators, TD( ) can not guarantee convergence and several results regarding divergence have been reported in the literature (Tsitsiklis and Roy,1997). For TD( ) with linear function approximators, also called linear TD( ) algorithms, several convergence proofs have been presented. Dayan (1992) showed the convergence in the mean for linear TD( ) algorithms with arbitrary 1 0 .", "startOffset": 174, "endOffset": 1362}, {"referenceID": 8, "context": "Therefore, the theory and algorithms for multi-step learning prediction become an important topic in RL and much research work has been done in the literature (Sutton, 1988; Tsitsiklis and Roy, 1997). Among the proposed multi-step learning prediction methods, temporal-difference (TD) learning (Sutton, 1988) is one of the most popular methods. It was studied and applied in the early research of machine learning, including the celebrated checkers-playing program (Minsky, 1954; Samuel, 1959). In 1988, Sutton presented the first formal description of temporal- difference methods and the TD( ) algorithm (Sutton,1988). Convergence results are established for tabular temporal-difference learning algorithms where the cardinality of tunable parameters is the same as that of the state space (Sutton, 1988; Watkins,et al.,1992; Dayan,et al., 1994; Jaakkola, et al.,1994). Since many real-world applications have large or infinite state space, value function approximation (VFA) methods need to be used in those cases. When combined with nonlinear value function approximators, TD( ) can not guarantee convergence and several results regarding divergence have been reported in the literature (Tsitsiklis and Roy,1997). For TD( ) with linear function approximators, also called linear TD( ) algorithms, several convergence proofs have been presented. Dayan (1992) showed the convergence in the mean for linear TD( ) algorithms with arbitrary 1 0 . Tsitsiklis and Roy (1994) proved the convergence for a special class of TD learning algorithms, known as TD(0), while in Tsitsiklis and Roy (1997), they extended the early results to general linear TD( ) case and proved the convergence with probability one.", "startOffset": 174, "endOffset": 1472}, {"referenceID": 8, "context": "Therefore, the theory and algorithms for multi-step learning prediction become an important topic in RL and much research work has been done in the literature (Sutton, 1988; Tsitsiklis and Roy, 1997). Among the proposed multi-step learning prediction methods, temporal-difference (TD) learning (Sutton, 1988) is one of the most popular methods. It was studied and applied in the early research of machine learning, including the celebrated checkers-playing program (Minsky, 1954; Samuel, 1959). In 1988, Sutton presented the first formal description of temporal- difference methods and the TD( ) algorithm (Sutton,1988). Convergence results are established for tabular temporal-difference learning algorithms where the cardinality of tunable parameters is the same as that of the state space (Sutton, 1988; Watkins,et al.,1992; Dayan,et al., 1994; Jaakkola, et al.,1994). Since many real-world applications have large or infinite state space, value function approximation (VFA) methods need to be used in those cases. When combined with nonlinear value function approximators, TD( ) can not guarantee convergence and several results regarding divergence have been reported in the literature (Tsitsiklis and Roy,1997). For TD( ) with linear function approximators, also called linear TD( ) algorithms, several convergence proofs have been presented. Dayan (1992) showed the convergence in the mean for linear TD( ) algorithms with arbitrary 1 0 . Tsitsiklis and Roy (1994) proved the convergence for a special class of TD learning algorithms, known as TD(0), while in Tsitsiklis and Roy (1997), they extended the early results to general linear TD( ) case and proved the convergence with probability one.", "startOffset": 174, "endOffset": 1593}, {"referenceID": 6, "context": "It is observed that the rate of convergence is influenced by the initialization of the variance matrix, which is a phenomenon investigated theoretically in adaptive filtering (Moustakides, 1997; Haykin, 1996).", "startOffset": 175, "endOffset": 208}, {"referenceID": 8, "context": "In Tsitsiklis and Roy (1997), the above linear TD( ) algorithm is proved to converge with probability 1 under certain assumptions and the limit of convergence W* is also derived, which satisfies the following equation.", "startOffset": 3, "endOffset": 29}, {"referenceID": 8, "context": "Under similar assumptions as in Tsitsiklis and Roy (1997), we will prove that the proposed RLS-TD( ) algorithm converges with probability one.", "startOffset": 32, "endOffset": 58}, {"referenceID": 8, "context": "Assumptions 1\u20134 are almost the same as those for the linear TD( ) algorithms discussed in Tsitsiklis and Roy (1997) except that in Assumption 1, ergodic Markov chains are considered.", "startOffset": 90, "endOffset": 116}, {"referenceID": 6, "context": "In practice, it is observed that there is a variable performance of RLS as a function of the initialization of (Moustakides, 1997).", "startOffset": 111, "endOffset": 130}, {"referenceID": 6, "context": "As discussed by Tsitsiklis and Roy (1997), the above theorem shows that the distance of the limiting function W* from the true value function V* is bounded and the smallest bound of approximation error can be obtained when =1.", "startOffset": 16, "endOffset": 42}, {"referenceID": 6, "context": "Moustakides (1997) provided a theoretical analysis on the relation between the algorithmic performance of RLS and the initialization of .", "startOffset": 0, "endOffset": 19}, {"referenceID": 6, "context": "We may refer this phenomenon to the low SNR case of the forgetting factor RLS studied in Moustakides (1997). For the Hop-World problem, the stochastic state transitions could introduce high equation residuals ) ( ) ( t t X b W X A in (19), which corresponds to the additive noise with large variance, i.", "startOffset": 89, "endOffset": 108}, {"referenceID": 6, "context": "These results may be explained by the theoretical analysis on the transient phase of the forgetting factor RLS (Moustakides,1997). According to the theory in Moustakides (1997), larger values of are needed for better performance in the cases of low SNR while smaller values are preferable for fast convergence in the cases of high and medium SNR.", "startOffset": 112, "endOffset": 177}, {"referenceID": 6, "context": "These results may be explained by the theoretical analysis on the transient phase of the forgetting factor RLS (Moustakides,1997). According to the theory in Moustakides (1997), larger values of are needed for better performance in the cases of low SNR while smaller values are preferable for fast convergence in the cases of high and medium SNR. So different values of must be selected for faster convergence of RLS-TD( ) in different cases. Especially, in some cases, such as the high SNR case discussed in Moustakides (1997), RLS methods with small values of can obtain a very fast speed of convergence.", "startOffset": 112, "endOffset": 528}, {"referenceID": 4, "context": "(1992) and Lin, et al.(1994), AHC methods with continuous outputs are applied to the cart-pole balancing problem.", "startOffset": 11, "endOffset": 29}, {"referenceID": 6, "context": "Thus this problem may be referred to the low SNR case in Moustakides (1997), where large values of are preferable for best convergence rate of RLS methods.", "startOffset": 57, "endOffset": 76}, {"referenceID": 6, "context": "These two different properties of Fast-AHC may be referred to the different SNR cases for RLS methods (Moustakides,1997). A thorough theoretical analysis on this problem is an interesting topic for future research. In our experiments, the performance of the AHC method using LS-TD( ) is also tested. As has been studied in Section 4, when the initializing constant is large, the performance of RLS-TD( ) and LS-TD( ) does not differ much. So the performance of AHC using LS-TD( ) is similar to that of Fast-AHC with large values of . As studied in Moustakides (1997), the RLS method can converge much faster than other adaptive filtering methods if the environment is stationary and the initializing constant is selected appropriately.", "startOffset": 103, "endOffset": 567}, {"referenceID": 6, "context": "287 filtering and the theoretical results in Moustakides (1997) provide some basis for the explanations of our results.", "startOffset": 45, "endOffset": 64}, {"referenceID": 6, "context": "287 filtering and the theoretical results in Moustakides (1997) provide some basis for the explanations of our results. A complete investigation of this problem is our ongoing work. The idea of using RLS-TD( ) in the critic network may be applied to other reinforcement learning methods with actor-critic architectures. In Konda and Tsitsiklis (1998), a new actor-critic algorithm using linear function approximators is proposed and the convergence under certain conditions is proved.", "startOffset": 45, "endOffset": 351}], "year": 2011, "abstractText": "The recursive least-squares (RLS) algorithm is one of the most well-known algorithms used in adaptive filtering, system identification and adaptive control. Its popularity is mainly due to its fast convergence speed, which is considered to be optimal in practice. In this paper, RLS methods are used to solve reinforcement learning problems, where two new reinforcement learning algorithms using linear value function approximators are proposed and analyzed. The two algorithms are called RLS-TD( ) and Fast-AHC (Fast Adaptive Heuristic Critic), respectively. RLS-TD( ) can be viewed as the extension of RLS-TD(0) from =0 to general 0 1, so it is a multi-step temporal-difference (TD) learning algorithm using RLS methods. The convergence with probability one and the limit of convergence of RLS-TD( ) are proved for ergodic Markov chains. Compared to the existing LS-TD( ) algorithm, RLS-TD( ) has advantages in computation and is more suitable for online learning. The effectiveness of RLS-TD( ) is analyzed and verified by learning prediction experiments of Markov chains with a wide range of parameter settings. The Fast-AHC algorithm is derived by applying the proposed RLS-TD( ) algorithm in the critic network of the adaptive heuristic critic method. Unlike conventional AHC algorithm, Fast-AHC makes use of RLS methods to improve the learning-prediction efficiency in the critic. Learning control experiments of the cart-pole balancing and the acrobot swing-up problems are conducted to compare the data efficiency of Fast-AHC with conventional AHC. From the experimental results, it is shown that the data efficiency of learning control can also be improved by using RLS methods in the learning-prediction process of the critic. The performance of Fast-AHC is also compared with that of the AHC method using LS-TD( ). Furthermore, it is demonstrated in the experiments that different initial values of the variance matrix in RLS-TD( ) are required to get better performance not only in learning prediction but also in learning control. The experimental results are analyzed based on the existing theoretical work on the transient phase of forgetting factor RLS methods.", "creator": "(PScript5.dll Version 5.2)"}}}