{"id": "1106.1684", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Jun-2011", "title": "Max-Margin Stacking and Sparse Regularization for Linear Classifier Combination and Selection", "abstract": "The main principle of stacked generalization (or Stacking) is using a second-level generalizer to combine the outputs of base classifiers in an ensemble. In this paper, we investigate different combination types under the stacking framework; namely weighted sum (WS), class-dependent weighted sum (CWS) and linear stacked generalization (LSG). For learning the weights, we propose using regularized empirical risk minimization with the hinge loss. In addition, we propose using group sparsity for regularization to facilitate classifier selection. We performed experiments using two different ensemble setups with differing diversities on 8 real-world datasets. Results show the power of regularized learning with the hinge loss function. Using sparse regularization, we are able to reduce the number of selected classifiers of the diverse ensemble without sacrificing accuracy. With the non-diverse ensembles, we even gain accuracy on average by using sparse regularization.", "histories": [["v1", "Wed, 8 Jun 2011 23:03:47 GMT  (277kb,D)", "http://arxiv.org/abs/1106.1684v1", "8 pages, 3 figures, 6 tables, journal"]], "COMMENTS": "8 pages, 3 figures, 6 tables, journal", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mehmet umut sen", "hakan erdogan"], "accepted": false, "id": "1106.1684"}, "pdf": {"name": "1106.1684.pdf", "metadata": {"source": "CRF", "title": "Max-Margin Stacking and Sparse Regularization for Linear Classifier Combination and Selection", "authors": ["Mehmet Umut Sen", "Hakan Erdogan"], "emails": ["umutsen@sabanciuniv.edu,", "haerdogan@sabanciuniv.edu."], "sections": [{"heading": null, "text": "Index Terms - Classification Combination, Classification Selection, Regulated Empirical Risk Minimization, Hinge Loss, Group Sparseness"}, {"heading": "1 INTRODUCTION", "text": "It is time for EU member states to set out to find a solution that paves the way to the European Union."}, {"heading": "2 STACKED GENERALIZATION", "text": "In 1992, a novel approach, known as stacked generalization or stacking, was introduced. [4] The basic idea is to apply a metal level generator (or level 1 generator) to the outputs of basic classifiers (or level 0 classifiers). To train the level 1 classifier, we need the confidence values (level 1 data) of the training data, but training the combinator with the same data instances used to train the basic classifiers will overequip the database and ultimately lead to poor generalization performance. Stacking solves this problem by an ingenious cross-validation method (internal resume) in which the training data is split into k-parts and each part of the data is tested with the base classifiers trained with the other k-1 parts of the data."}, {"heading": "3 COMBINATION TYPES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Problem Formulation", "text": "Let pnm be the trailing score of class n obtained by the classifier m for any data instance. Let pm = [p 1 m, p 2 m,.., p N m] T, where N is the number of classes and M is the number of classifiers. Outputs of the combiner are N different scores representing the level of support for each class. Let rn be the combined score of class n and leave r = [r1,.. rN] T where N is the number of classes and M is the number of classifiers. Outputs of the combiner are N different scores representing the level of support for each class. Let rn be the combined score of class n and leave r = [rN] T; then in general, the combiner is defined as a function g: RMN \u2192 RN so that r = g (f) is the number of training data instances containing the number of training data."}, {"heading": "3.2 Linear Combination Types", "text": "In this section we describe and analyze three combination types, namely the weighted sum rule (WS), the class-dependent weighted sum rule (CWS) and the linearly stacked generalization (LSG), where LSG is already defined in (2)."}, {"heading": "3.2.1 Weighted Sum Rule", "text": "In this type of combination, each classifier is given a weight, so there are completely different weights for M. If we convert the weight of the classifier m, then the final value of class n is estimated as follows: rn = M \u2211 m = 1 ump n m = u T fn, n = 1,..., N, (3) where fn contains the values of class n: fn = [pn1,..., p n M] T and u = [u1,..., uM] T. For the framework specified in (2), the WS combination can be obtained by having b = 0 and W being the concatenation of constant diagonal matrices: W = [u1IN |.. | uM IN], (4) where IN is the N-N identity matrix. We expect higher weights for stronger base classifiers after we have learned the weights from the database."}, {"heading": "3.2.2 Class-Dependent Weighted Sum Rule", "text": "If vnm is the weight of the classifier m for class n, then the final state of class n is estimated as follows: rn = M \u2211 m = 1 vnmp n = v T n f n, n = 1,.., (5) where vn = [vn1,..., vnM] T. There are MN parameters in a CWS combinator. For the framework specified in (2), the CWS combination can be obtained by having b = 0 and W being the concatenation of diagonal matrices; but unlike WS, diagonals are not constant: W = [W1 | W2 |.... | WM], (6) where Wm and RN \u00b7 N are diagonal for m = 1,.., M."}, {"heading": "3.2.3 Linear Stacked Generalization", "text": "This type of combination is the most common form of monitored linear combinations and is already defined in (2).With LSG, the value of class n is estimated as follows: rn = wTn f + bn, n = 1,..., N, (7), where wn-RMN is the n-th row of W and bn is the n-th element of b. LSG can be interpreted as giving the outputs of the base classifiers to a linear multi-class classifier as a new set of characteristics.This type of combination can result in agreement with the database and provide a lower accuracy than WS and CWS combinations if there is insufficient data. From this point of view, WS and CWS combinations can be treated as regulated versions of LSG. A crucial disadvantage of LSG is that the number of parameters to be learned is MN2 + N, which will result in a long training time. There is not a single superior of these three combinations, as the best results will be selected in (4)."}, {"heading": "4 LEARNING THE COMBINER", "text": "In this context, learning is formulated as an unrestricted minimization problem and the objective function consists of a summation of the empirical risk function via data instances and a regularization function. Empirical risk is obtained as the sum of the \"loss values\" from each sample (max.). Various decisions of the loss functions and regularization functions correspond with different classifiers. The use of the hinge loss function with l2 standard regularization corresponds to the support of vector machines (SVM). Studies have shown that the hinge loss function provides much better classification performance compared to the least square functions (LS) in general. Previous classification combination literature uses LS-loss function [6], [8], [13], which is suboptimal compared to the hinge losses we promote and use in this essay. The use of leastquares with l2 regularization corresponds least to the application we support."}, {"heading": "5 SPARSE REGULARIZATION", "text": "In this section, we define a number of regulation functions to enforce weight economy, so that the resulting combinator does not use all the basic classifiers that lead to a shorter test time. This method can be considered a selection algorithm for classifiers, but here classifiers are automatically selected and we cannot determine the number of selected classifiers in advance, but we can reduce this number by increasing the weight of the regulation function (\u03bb) and vice versa. In a sparse regulation \u03bb has two main effects on the resulting combinator. First, it determines how much the combinator should fit into the data. A reduction of \u03bb leads to a better adaptation of the training data and a too strong reduction of this function leads to an overload, as it increases too much, which prevents the combinator from learning from the data and dramatically decreasing the accuracy. Second, as already mentioned, it will determine the number of selected classifiers."}, {"heading": "5.1 Regularization with the l1 Norm", "text": "For CWS and LSG, where the combinator consists of matrices, we can concatenate the weights in a vector and take the l1 standard or equivalent. We have the following sparse regulatory functions for WS, CWS and LSG: RWS (u) = | u | | 1, (13) RCWS (V) = | V | | 1.1 = N \u2211 n = 1 | | vn | 1, (14) RLSG (W) = | | 1.1 = N = 1 | | | 1. (15) If all weights of a classifier are equal to zero, this classifier is eliminated and we do not need to use this basic classifier for a test instance, so that testing will proceed more quickly. However, the problem that this 1 standard is not selectable for all MS and 5 weight classes can be overcome next."}, {"heading": "5.2 Regularization with Group Sparsity", "text": "We define another set of regulation functions embedded by group parity for LSG and CWS to force classifier selection. The main principle of group parity is to set all elements belonging to a group completely to zero. Grouping of elements occurs before learning. In the classifier combination, the back values obtained from each basic classifier form a group. The following regulation function yields a group parity for LSG: RLSG (W) = M x m = 1 | Wm | | F. (16) For CWS, we use the following regulation: RCWS (V) = | V | | 1.2 = M x m = 1 | | vm | 2, (17) where vm is the highest row of V, so that it contains the weights of classifier m. After the learning process, the elements of vm for each m are either all zero or all non-zero. This results in a better performance than the 1 regulation we describe in the next section 7 for automatic classification."}, {"heading": "6 EXPERIMENTAL SETUPS", "text": "We have conducted extensive experiments in eight real data sets from the UCI repository [23]. For a summary of the properties of the data sets see Table 1. In order to obtain statistically significant results, we applied a 5x2 cross-validation [24] based on 5 iterations of 2x cross-validation (CV). In this method, the data for each CV is randomly divided into two stacks to train and test, resulting in a total of 10 stacks for each database. We constructed two ensembles that differ in their diversity. In the first group, we construct 10 different subsets randomly containing 80% of the original data. Subsequently, 13 different classifiers are formed with each subset, resulting in a total of 130 base classifiers. We used PR tools [25] and Libsvm toolkit [26] to determine the base classifiers. These 13 different classifiers are: Normal density based on standard classifiers, next-level classifiers, next-level classifiers, next-level classifiers."}, {"heading": "7 RESULTS", "text": "First, we examine the performance of regulated learning of weights with the loss of hinges compared to the conventional least square losses. [13] and the multireacting linear regression method, which does not include regularization. [6] It should be noted that the results here and in [13], [6] are not directly comparable, because the structure of the ensembles is different. Error rates of these three different learning algorithms for WS, CWS and LSG are shown in Table 2. Results for the simple sum rule, which is equivalent to the use of equal weights in WS, are also given in the column entitled EW. The first entries in the boxes are the mean error rates above 5 \u00d7 2 CV stacks and the second entries are the standard deviations. For five datasets, the lowest error means are obtained with the hinge loss function and for two datasets with the least quarterly function."}, {"heading": "8 CONCLUSION", "text": "In this paper, we proposed using hinge loss function with regulation to learn the parameters (or weights) of linear combinators in stacked generalization. We are able to achieve better accuracy in hinge loss function than conventional estimation of weights with smallest squares. The results also indicate the importance of regularized learning of weights. We also proposed a l1 \u2212 l2 standardization (or group sparseness) to obtain a reduced number of basic classifiers, thus shortening the test time. Results indicate that we can use a smaller number of basic classifiers, with a small sacrifice in the accuracy of the diverse ensemble. We show that the l1 \u2212 l2 regulation exceeds the l1 regulation in terms of both accuracy and number of selected classifiers."}, {"heading": "9 ACKNOWLEDGMENTS", "text": "This research is supported by the Turkish Council of Science and Technology (TUBITAK) under the Programme for the Support of Scientific and Technological Research (Code 1001) under project number 107E015 entitled \"Novel Approaches in Audio Visual Speech Recognition.\""}], "references": [{"title": "Ensemble methods in machine learning,", "author": ["T.G. Dietterich"], "venue": "in International Workshop on Multiple Classifier Systems. Springer- Verlag,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Ensemble based systems in decision making,", "author": ["R. Polikar"], "venue": "Ieee Circuits And Systems Magazine,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Combining Pattern Classifiers: Methods and Algorithms", "author": ["L.I. Kuncheva"], "venue": "Wiley-Interscience", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Stacked generalization,", "author": ["D.H. Wolpert"], "venue": "Neural Netw.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Combining the results of several neural network classifiers,", "author": ["G. Rogova"], "venue": "Neural Netw.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Issues in stacked generalization,", "author": ["K.M. Ting", "I.H. Witten"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Combining estimates in regression and classification,", "author": ["M. LeBlanc", "R. Tibshirani"], "venue": "Journal of the American Statistical Association, Tech. Rep.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "Stacking bagged and dagged models,", "author": ["K.M. Ting", "I.H. Witten"], "venue": "Proc. 14th International Conference on Machine Learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "How to make stacking better and faster while also taking care of an unknown weakness,", "author": ["A.K. Seewald"], "venue": "Proceedings of the Nineteenth International Conference on Machine Learning, ser. ICML \u201902", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Using correspondence analysis to combine classifiers,", "author": ["C.J. Merz", "S. Stolfo"], "venue": "Machine Learning. Kluwer Academic Publishers,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "and D", "author": ["J. Sill", "G. Tak\u00e1cs", "L. Mackey"], "venue": "Lin, \u201cFeature-weighted linear stacking,\u201d CoRR, vol. abs/0911.0460", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Regularized linear models in stacked generalization,", "author": ["S. Reid", "G. Grudic"], "venue": "Proceedings of the 8th International Workshop on Multiple Classifier Systems, ser. MCS \u201909. Berlin, Heidelberg: Springer-Verlag,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Deroski, \u201cCombining multiple models with meta decision trees,\u201d in Principles of Data Mining and Knowledge Discovery, ser", "author": ["S.L. Todorovski"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Ga-stacking: Evolutionary stacked generalization,", "author": ["A. Ledezma", "R. Aler", "A. Sanchis", "D. Borrajo"], "venue": "Intell. Data Anal.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "and X", "author": ["B. Tang", "Q. Chen", "X. Wang"], "venue": "Wang, \u201cReranking for stacking ensemble learning,\u201d in Proceedings of the 17th international conference on Neural information processing: theory and algorithms - Volume Part I, ser. ICONIP\u201910. Berlin, Heidelberg: Springer-Verlag", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimal linear combination of neural networks for improving classification performance,", "author": ["N. Ueda"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 22,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "and B", "author": ["Y. Lecun", "S. Chopra", "R. Hadsell", "F.J. Huang", "G. Bakir", "T. Hofman", "B. Schlkopf", "A. Smola"], "venue": "T. (eds, \u201cA tutorial on energy-based learning,\u201d in Predicting Structured Data. MIT Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Sparse ensembles using weighted combination methods based on linear programming,", "author": ["L. Zhang", "W.-D. Zhou"], "venue": "Pattern Recognition,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "A unifying framework for learning the linear combiners for classifier ensembles,", "author": ["H. Erdogan", "M. Sen"], "venue": "in Pattern Recognition (ICPR),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Least squares support vector machine classifiers,", "author": ["J. Suykens", "J. Vandewalle"], "venue": "Neural Processing Letters,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Ultraconservative online algorithms for multiclass problems,", "author": ["K. Crammer", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "UCI machine learning repository,", "author": ["A. Asuncion", "D. Newman"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Approximate statistical tests for comparing supervised classification learning algorithms,", "author": ["T.G. Dietterich"], "venue": "Neural Computation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "PRTools4.1, A Matlab Toolbox for Pattern Recognition, 2007, delft University of Technology", "author": ["R.P.W.J.P. D", "P.P.E. P", "D. d. R", "D.M.J. T", "S. V"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "LIBSVM: a library for support vector machines, 2001, software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "CVX: Matlab software for disciplined convex programming, version 1.21,", "author": ["M. Grant", "S. Boyd"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Statistical comparisons of classifiers over multiple data sets,", "author": ["J. Dem\u0161ar"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Among all theoretical and practical reasons to prefer using ensembles, which are categorized as statistical, computational and representational in [1], the most important ones are the statistical reasons.", "startOffset": 147, "endOffset": 150}, {"referenceID": 0, "context": "Some other reasons besides statistical reasons can be found in [1], [2].", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": "Some other reasons besides statistical reasons can be found in [1], [2].", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "They can be interpreted as confidences in the suggested labels or estimates of the posterior probabilities for the classes [3].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 93, "endOffset": 96}, {"referenceID": 4, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 129, "endOffset": 132}, {"referenceID": 3, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 211, "endOffset": 214}, {"referenceID": 5, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 216, "endOffset": 219}, {"referenceID": 6, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 221, "endOffset": 224}, {"referenceID": 7, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 226, "endOffset": 229}, {"referenceID": 8, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 231, "endOffset": 234}, {"referenceID": 9, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 236, "endOffset": 240}, {"referenceID": 10, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 248, "endOffset": 252}, {"referenceID": 11, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 254, "endOffset": 258}, {"referenceID": 12, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 260, "endOffset": 264}, {"referenceID": 13, "context": "Among trainable combiners, such as stacked generalization (Stacking) [4], Decision Templates [3] and Dempster-Shafer Combination [5]; stacked generalization is deeply investigated and analyzed in the literature [4], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15].", "startOffset": 266, "endOffset": 270}, {"referenceID": 3, "context": "The idea of Stacking is to use the confidence scores that are obtained from base classifiers as attributes in a new training set with the original class labels and training a meta-classifier (This classifier is called level-1 generalizer in [4]) with this new dataset.", "startOffset": 241, "endOffset": 244}, {"referenceID": 3, "context": "the class predictions of the base classifiers [4].", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "Ting & Witten used confidence scores of base classifiers as input features and improved stacking\u2019s performance [6], [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 7, "context": "Ting & Witten used confidence scores of base classifiers as input features and improved stacking\u2019s performance [6], [8].", "startOffset": 116, "endOffset": 119}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Sill, incorporated meta-features with the posterior scores of base classifiers to improve accuracy [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "Ledezma, used genetic algorithms to search for good Stacking configurations [15].", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "Tang, re-ranked all possible class labels according to the scores and obtained a learner which outperforms all base classifiers [16].", "startOffset": 128, "endOffset": 132}, {"referenceID": 15, "context": ") Given a combination type, how should we learn the parameters of the combiner? For the former problem, Ueda [17] defined three linear combination types namely type-1, type-2 and type-3.", "startOffset": 109, "endOffset": 113}, {"referenceID": 6, "context": "In [7], [8], LSG is used and CWS combination is proposed in [6].", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "In [7], [8], LSG is used and CWS combination is proposed in [6].", "startOffset": 8, "endOffset": 11}, {"referenceID": 5, "context": "In [7], [8], LSG is used and CWS combination is proposed in [6].", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "For the latter problem, Ting & Witten proposed a multi-response linear regression algorithm for learning the weights [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 15, "context": "Ueda in [17] proposed using minimum classification error (MCE) criterion for estimating optimal weights, which increased the accuracies.", "startOffset": 8, "endOffset": 12}, {"referenceID": 11, "context": "Reid & Grudic in [13] regularized the standard linear least squares estimation of the weights with CWS and improved the performance of stacking.", "startOffset": 17, "endOffset": 21}, {"referenceID": 16, "context": "We work with the regularized empirical risk minimization framework [18] and use the hinge loss function with l2 regularization, which corresponds to the support vector machines (SVM).", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "Another issue, recently addressed in [19], is combination with a sparse weight vector so that we do not use all of the ensemble.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "Zhang formulated this problem as a linear programming problem for only the WS combination type [19].", "startOffset": 95, "endOffset": 99}, {"referenceID": 11, "context": "Reid used l1 norm regularization for CWS combination [13].", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "A novel approach has been introduced in 1992 known as stacked generalization or stacking [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Wolpert combined only the class predictions with this framework, Ting & Witten improved the performance of stacking by combining continuous valued predictions [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 15, "context": "This type of combination is the most general form of linear combiners and called type-3 combination in [17].", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "These types are categorized as class-conscious combinations in [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 18, "context": "There is not a single superior one among these three combination types since results are shown to be data dependent [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "We use the regularized empirical risk minimization (RERM) framework [18] for learning the weights.", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "Earlier classifier combination literature uses LS loss function [6], [8], [13], which is suboptimal as compared to the hinge loss that we promote and use in this paper.", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "Earlier classifier combination literature uses LS loss function [6], [8], [13], which is suboptimal as compared to the hinge loss that we promote and use in this paper.", "startOffset": 69, "endOffset": 72}, {"referenceID": 11, "context": "Earlier classifier combination literature uses LS loss function [6], [8], [13], which is suboptimal as compared to the hinge loss that we promote and use in this paper.", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "Using leastsquares with l2 regularization is equivalent to applying least-squares support vector machine (LS-SVM) [21].", "startOffset": 114, "endOffset": 118}, {"referenceID": 20, "context": "We use an adaptation of SVM in multiclass problems defined in [22].", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "Equation (8) is given for LSG but it can be modified for other types of combinations using the unifying framework described in [20].", "startOffset": 127, "endOffset": 131}, {"referenceID": 21, "context": "We have performed extensive experiments in eight realworld datasets from the UCI repository [23].", "startOffset": 92, "endOffset": 96}, {"referenceID": 22, "context": "In order to obtain statistically significant results, we applied 5x2 cross-validation [24] which is based on 5 iterations of 2-fold cross-validation (CV).", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "We used PR-Tools [25] and Libsvm toolbox [26] for obtaining the base classifiers.", "startOffset": 17, "endOffset": 21}, {"referenceID": 24, "context": "We used PR-Tools [25] and Libsvm toolbox [26] for obtaining the base classifiers.", "startOffset": 41, "endOffset": 45}, {"referenceID": 25, "context": "For the minimization of the objective functions, we used the CVX-toolbox [27].", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "05 [28].", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "First, we investigate the performance of regularized learning of the weights with the hinge loss compared to the conventional least squares loss [13] and the multiresponse linear regression method which does not contain regularization [6] with the diverse ensemble setup described in section 6.", "startOffset": 145, "endOffset": 149}, {"referenceID": 5, "context": "First, we investigate the performance of regularized learning of the weights with the hinge loss compared to the conventional least squares loss [13] and the multiresponse linear regression method which does not contain regularization [6] with the diverse ensemble setup described in section 6.", "startOffset": 235, "endOffset": 238}, {"referenceID": 11, "context": "It should be noted that results shown here and in [13], [6] are not directly comparable since construction of the ensembles is different.", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "It should be noted that results shown here and in [13], [6] are not directly comparable since construction of the ensembles is different.", "startOffset": 56, "endOffset": 59}, {"referenceID": 5, "context": "It should be noted that in [6], 3 base classifiers are used and here we use", "startOffset": 27, "endOffset": 30}, {"referenceID": 26, "context": "According to the pairwise Wilcoxon signed-ranks test [28], hinge loss function outperforms least squares loss function at one-tailed significant level \u03b1 = 0.", "startOffset": 53, "endOffset": 57}], "year": 2011, "abstractText": "The main principle of stacked generalization (or Stacking) is using a second-level generalizer to combine the outputs of base classifiers in an ensemble. In this paper, we investigate different combination types under the stacking framework; namely weighted sum (WS), class-dependent weighted sum (CWS) and linear stacked generalization (LSG). For learning the weights, we propose using regularized empirical risk minimization with the hinge loss. In addition, we propose using group sparsity for regularization to facilitate classifier selection. We performed experiments using two different ensemble setups with differing diversities on 8 real-world datasets. Results show the power of regularized learning with the hinge loss function. Using sparse regularization, we are able to reduce the number of selected classifiers of the diverse ensemble without sacrificing accuracy. With the non-diverse ensembles, we even gain accuracy on average by using sparse regularization.", "creator": "LaTeX with hyperref package"}}}