{"id": "1106.1797", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2011", "title": "Parameter Learning of Logic Programs for Symbolic-Statistical Modeling", "abstract": "We propose a logical/mathematical framework for statistical parameter learning of parameterized logic programs, i.e. definite clause programs containing probabilistic facts with a parameterized distribution. It extends the traditional least Herbrand model semantics in logic programming to distribution semantics, possible world semantics with a probability distribution which is unconditionally applicable to arbitrary logic programs including ones for HMMs, PCFGs and Bayesian networks. We also propose a new EM algorithm, the graphical EM algorithm, that runs for a class of parameterized logic programs representing sequential decision processes where each decision is exclusive and independent. It runs on a new data structure called support graphs describing the logical relationship between observations and their explanations, and learns parameters by computing inside and outside probability generalized for logic programs. The complexity analysis shows that when combined with OLDT search for all explanations for observations, the graphical EM algorithm, despite its generality, has the same time complexity as existing EM algorithms, i.e. the Baum-Welch algorithm for HMMs, the Inside-Outside algorithm for PCFGs, and the one for singly connected Bayesian networks that have been developed independently in each research field. Learning experiments with PCFGs using two corpora of moderate size indicate that the graphical EM algorithm can significantly outperform the Inside-Outside algorithm.", "histories": [["v1", "Thu, 9 Jun 2011 13:13:03 GMT  (225kb)", "http://arxiv.org/abs/1106.1797v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["t sato", "y kameya"], "accepted": false, "id": "1106.1797"}, "pdf": {"name": "1106.1797.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Taisuke Sato", "Yoshitaka Kameya"], "emails": ["sato@mi.cs.titech.ac.jp", "kame@mi.cs.titech.ac.jp"], "sections": [{"heading": null, "text": "We propose a logical-mathematical framework for learning statistical parameters of parameterized logic programs, i.e. de nite clause programs that contain probabilistic facts with parameterized distribution. It extends the traditional semantics of the least Herbrand model in logic programming to distribution semantics, a possible world semantics with a probability distribution that is unconditionally applicable to any logic program, including those for HMMs, PCFGs, and Bayesian networks. We also propose a new EM algorithm, the graphical EM algorithm that runs for a class of parameterized logic programs that represent sequential decision processes in which each decision is exclusive and independent. It runs on a new data structure called support graphs that describe the logical relationship between observations and their explanations, and learns parameters by generalizing within and outside the probability algorithm for network programs that exist in the OLT search algorithms independent of each other, the combination of PCM algorithms for PCM complexity analysis with PCM that are independent of each other."}, {"heading": "1. Introduction", "text": "Parameter learning is common in various elds from neural networks to reinforcement learning to statistics. It is used to optimize systems for their best performance, be it classical or statistical models. In contrast to these numerical systems, which are described by mathematical formulas, symbolic systems, typically programs, do not seem to be accessible to any kind of parameter learning. In fact, there is little literature on parameter learning of programs. This paper is an attempt to integrate parameter learning into computer programs, for two reasons. Theoretically, we would like to add the ability of learning to computer programs that the authors believe is a necessary step in building intelligent systems. Practically, it expands the class of probability distributions that go beyond traditionally applied numerical systems that are available for modeling complex phenomena such as genes inheritance, consumer behavior, natural language processing, and so on.c 2001 AI Access Foundation and Kaufmann Publishers. All rights reserved."}, {"heading": "2. Preliminaries", "text": "Since our subject overlaps logic programming and EM learning, which are quite different in nature, we separate preparations."}, {"heading": "2.1 Logic Programming and OLDT", "text": "In logic programming, a DB program is a set of de nite clauses8and the execution is searchfor an SLD refutation of a specific target G. The top-down interpreter recursively selects the7. Pseudo-PCSGs (probabilistic context sensitive grammars) are a context-sensitive extension of PCFGsproposed by Charniak and Carroll (1994). sc-BN is an abbreviation for a single connected Bayesian network (Pearl, 1988).8. We do not deal with general logical programs in this paper. Next goal and unfolds it (Tamaki & Sato, 1984) into subtargets using a non-deterministically chosen clause. The calculated result by the SLD refutation, i.e. a solution, is a response substitution (variable binding), so DB'G. 9 Usually there is more than one refutation for G, and the search space for all rebuttons."}, {"heading": "2.2 EM Learning", "text": "Imagine a random sample x1;:; xTof size T on a random variable X drawn from a distribution P (X = x j) lgorithzed by unknown, is watched by ML estimation as the MLE (maximum probability) of, i.e. as the maximizer of the likelihood Q1 i TP (xij).Things get much more di cult when data are incomplete. Think of a probabilistic relationship between non-observable cause X and observable e ect Y as one between diseases and symptoms in medicine and assus that Y does not uniquely determine the cause X (xij).Then Y is incomplete in the sense that Y does not carry enough to observatory X. Let P (X = x; Y j j) be a parameterized joint distribution over X and Y."}, {"heading": "3. Distribution Semantics", "text": "In this section we present parameterized logic programs and their declarative semantics. Basically, we start with a sentence F of probable facts (atoms) and a sentence R of non-uniform de nite clauses. The sample from F determines a sentence F 0 of real atoms, and the Herbrand model of F 0 [R determines the truth value of each atom in DB = F [R. Therefore, each atom can be considered a random variable, assuming 1 (true) or 0 (false). In the following, we formalize this process and construct the underlying probability space for the denotation of the DB."}, {"heading": "3.1 Basic Distribution P", "text": "F & A = F & A = 1 = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A = 1; D & A; D & A; D & A; D & A; D & A; D & A; A & A; A; A & A; A; A & A; A; A; A & A; A; A; A & A; A; A & A; A; A & A; A; A & A; A; A & A; A; A & A; A; A & A; A; A & A; A; A & A; A; A & A; A & A; A; A & A; A & A; A & A; A & A; A & A; A & A; A & A & A; A & A & A & A; A & A & A; A & A & A; A & A & A; A & A & A & A & A; A & A; A & A & A & A; A; A & A & A; A & A & A; A & A; A & A & A & A; A; A & A & A; A & A & A; A & A & A & A; A & A & A; A & A; A & A & A; A & A & A; A & A & A & A; A & A & A; A & A; A & A; A & A & A & A; A & A & A; A & A & A; A & A & A; A & A & A; A & A; A & A & A; A & A; A & A & A & A; A & A & A; A & A; A & A; A & A; A & A & A & A &"}, {"heading": "3.2 Extending P", "text": "Fto PDB In this section we extend PDB to a probability measure PDB about the possible worlds for L, i.e. the amount of all possible truth assignments to terrestrial atoms in L by the minimum."}, {"heading": "13. This naming of P", "text": "F, although it is a probability variable, is partially caused by the observation that it behaves like a DB in its common distribution = DB = 1984; PF (A1 = x1; A2 = x2;::) for a vector hA1 random in nite; A2;: iof which P (n) F (A 1 = x 1;:: A n = x n) (n = 1; 2;::::) are marginal distributions. Another reason is intuition. These considerations are also valid for PDBde ned in the next subsection as well. 14. Here clauses are not necessarily grounded: Herbrand Model (Lloyd, 1984; Doets, 1994). Before we go on, however, we need a few notations. For an atom A, de ne A x of (Ax = A if x = 1Ax =: A if x =: Herbrand Interpretation 2Fof F. It makes some atoms true and others false."}, {"heading": "15. I de nes, mutually, a Herbrand interpretation such that a ground atom A is true if and only if A 2 I.", "text": "A Herbrand model of a program is a Herbrand interpretation that renders truthful every ground instance of each clause in the program. 16. Note that this enumeration also lists ground atoms in F. This is the name of the program DB = F [R w.r.t. PFto be PDB. De ned de-notational semantics of parameterized logic programs is referred to above as distribution semantics. As already mentioned, we consider PDB as a kind of in-nite joint distribution PDB (A1 = x1; A2 = x2;::). The mathematical properties of the PDBs are listed in the appendix, where our semantics are likely to be an extension of the standard semantics of the smallest model semantics in the logic of possible world semantics."}, {"heading": "3.3 Programs as Distributions", "text": "Distribution semantics considers parameterized logic programs as expressions of distributions. Traditionally, distributions are expressed through the use of mathematical formulas, but the use of programs as (discrete) distributions gives us much more freedom and flexibility than mathematical formulas in the construction of distributions because they have recursion and arbitrary composition. Specifically, a program can contain many random variables as probabilistic atoms by recursion, and thus describe stochastic processes that are potentially contained in an insignificant number of random variables such as Markov chains and derivatives in PCFGs (Manning & Sch utze, 1999). 17Programs also allow us to permanently express complicated constraints on distributions such as the sum of the events of alphabets a or b in an HMM output sequence."}, {"heading": "4. Graphical EM Algorithm", "text": "According to the previous section, a parameterized logic program DB = F [R in arst-order language L = 1984 q = 1 with a parameterized basic distribution PF (j) via the Herbrandian interpretations of terrestrial atoms in F speci es a parameterized distribution PDB (j) via the Herbrand interpretations for L. In this section, we will develop step by step an efficient EM algorithm for the parameter learning of parameterized logic programs by interpreting PDBas a distribution via the observable and unobservable events. The new EM algorithm is called a graphical EM algorithm. It is applicable to arbitrary logic programs that meet certain later described conditions, provided that the basic distribution is a direct product of multiple random switches, which is a minor complication of the binary ones introduced in Section 3.1.1. From this section, we assume that DB consists of ordinary, quantifiable clauses (which contain)."}, {"heading": "4.1 Motivating Example", "text": "First, we look at distribution semantics based on a concrete example. Consider the following program DBb = Fb [Rbin Figure 1], which models how one's blood group is determined by blood group genes likely to be inherited from parents.19The first four clauses in Rbstats determine a blood group by genotype, i.e. a pair of blood group genes a, b and o. For example, btype (\"A\"): - (gtype (a, a); gtype (a, o); gtype (o, a)) states that blood group A is if its genotype is ha; ai, ha; oi or ho; ai. These are propositional rules. Successful clauses lay down general rules relating to logical variables. The fifth proposition states that regardless of the values of X and Y event type gtype (X, Y) (one has genotype hX; Yi) is caused by two events (X, Gw), Gen (choice)."}, {"heading": "18. This de nition of a support set di ers from the one used by Sato (1995) and Kameya and Sato (2000).", "text": "If we implicitly emphasize the procedural reading of logic programs = gene (gene, gene, gene) = msb = msb; msb = msb; msb = msb; msb = msb; msb = msw; msw = b (msb & Shapiro, 1986). Thus stands; for\\ or, \"\\ and\": -\\ implied by \"father.\" Strings starting with a capital letter are (universally quanti ed) variables, but quoted such as \"A.\" The underscore is an anonymous variabbreviation. 20. msw is an abbreviation of\\ multi-ary random switch \"and msw (;) expresses a probable choice of pious alternatives. Within the framework of statistical abduction, msw atoms are constructed abductible from whichever explanations as a conjunction. Genetic knowledge that the choice of G occurs by chance and is expressed from fa; b; og by specifying a common distribution of Fbas, Fwt (Fwt)."}, {"heading": "4.2 Four Simplifying Conditions", "text": "This is not generally the case. As our primary interest is learning, in particular learning parameterized logical programs, we can now focus on determining which property of a program makes calculating probability as simple as DBb, thereby hoping to construct a probabilistic computational model. To answer this question precisely, let us formulate the entire modeling process. Suppose that there are symbolic-statistical phenomena such as gene inheritance for which we hope to construct a probabilistic computational model. We specify a target predicate p, whose basic atom p (s) represents our observation of the phenomena. Then, explaining the empirical distribution of p, we write a parameterized logical program DB = F [R with a fundamental distribution of PFwith parameters that can reproduce all observable patterns of p (s)."}, {"heading": "4.3 Modeling Principle", "text": "Up to this point, we have introduced four conditions, the uniqueness condition, the exclusivity condition, the nite support condition and the distribution condition, to simplify the probability calculation. The last condition is easy to fulfill. We only assume Fmswtogether with Pmsw. So, from here, we always assume that Fmswhas is a parameterized distribution Pmswintroduced in the previous subsection. Unfortunately, the others are not automatically saturated. However, according to our modeling experience, it is only mild di Kult to satisfy the uniqueness condition and the exclusivity condition, as long as we principally obey the following modeling condition. Modeling principle: DB = Fmsw [R describes a sequential decision process (modulo auxiliary compilations) that clearly produces an observable atom G 2 obs (DB) condition, where every decision is expressed by some msw atom condition, we have to write a Gw-22B condition, which means that in programming we have to specify a 22B condition."}, {"heading": "4.4 Four Conditions Revisited", "text": "In this section, we discuss how to loosen the four simplistic conditions introduced in paragraph 4.2 for the purpose of exible modeling. First, we examine the uniqueness condition taking into account its crucial role in adapting the EM algorithm to our semantics. However, the uniqueness condition guarantees that there is a (multiple) mapping of explanations for observations so that the EM algorithm is applicable (Dempster et al., 1977). However, it is possible to loosen the uniqueness condition while justifying the application of the EM algorithm. We proceed from the MAR condition (lack of random state) introduced by Rubin (1976), which is a statistical condition for how complete data (explanation) becomes incomplete data (observation) and is usually assumed implicitly or explicitly in statistics (see Appendix B)."}, {"heading": "22. Decisions made in the process are a nite subset of F", "text": "The question is whether the two cases are a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case, which is a case."}, {"heading": "4.5 Naive Approach to EM Learning", "text": "In this section we derive a concrete EM algorithm for parametrized logic programs DB = DB x) (msv) msw (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (msv) s (sv) s (sv) s (sv) s (sv) s (s) s (sv) s (s) s (sv) s (sv) s (sv) s (s) s (sv) s (s) s (sv) s (s) s (sv) s (sv) s (s) s (sv) s (sv) s (s) s (sv) s (sv) s (s) s (sv) s) s (sv) s (s) s (sv) s (s) s (s) s (s) s (sv) s (s) s (s) s (sv) s (sv) s (s) s (s) s (s) s (s) s (s) s (s (s) s (s) s (sv) s (s) s (s (sv) s) s (s (s) s (s) s (s) sv) s (s (s (s) sv) s (s (s) s (sv) s (s (s) s (sv) s (s) s (s (s) s (sv) s (s (s) s (s) s (s (s) s (sv) s (sv) s (s (s) s (sv) s (s (s) s) s (s (s) s (s (s (s) s (s)"}, {"heading": "4.6 Inside Probability and Outside Probability for Logic Programs", "text": "In this subsection we generalize the concept of internal probability and external probability (Baker, 1979; Lari & Young, 1990) to logic programs. Important calculations in Learn naive (DB, G) are therefore those of two terms in line 6, PDB (Gtj) and PS2DB (Gt) Pmsw (S j) i; v (S). Calculation-conditional redundancy lurks in the naive calculation of both terms. We show it using an example. Suppose there is an interpretation program DBp = Fp [Rpwhere Fp = fa; b; d; < dg ^ e ^ g g g g g g g g g g g g g h h: (10) Here f is an observable atom. Let's assume that a c = Fp [Rpwhere Fp = fa; b; b; b; c; < dg ^ e; g g g g g g g g g g g g g g g g g g g g g g g g g g g h m: c: (10). Here f is an atom. Let's an observable program DBp; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c; c"}, {"heading": "4.7 OLDT Search", "text": "To calculate probabilities inside and outside (15) or (17) and (18) recursively, we need a tabulation mechanism for the structural division of partial explanations at the programming level."}, {"heading": "26. Because of the independence assumption on body atoms, W", "text": "k; j (1 h K; 1 j ih) and A are independent. Therefore, the DB (A ^ Wh; j) @ (A) = @ (A) (Wh; j) @ (A) = (Wh; j): between partial goals. We will henceforth deal with programs DB, in which a covered table (DB) of table predicates is explained in advance. A ground atom, which contains a table predicate, is called a table atom. The purpose of table atoms is to store their support sets and to eliminate the need for recomputation, namely, to construct hierarchically organized explanations consisting of the table atoms and the msw atoms. Let DB = Fmsw [R be a parameterized logical program, which fulfills the ninth support condition and the uniquency condition. Also G1; G2;; GTbe a random sample of observable atoms in (DB)."}, {"heading": "27. Pre x \\t-\" is an abbreviation of \\tabled-\".", "text": "This year, it is so far that it will be able to retaliate, \"he said.\" We have never waited so long to get this far, \"he said."}, {"heading": "4.8 Support Graphs", "text": "In retrospect, we only need to calculate a hierarchical system of t-declarations, which is essentially a Boolean combination of primitive events (msw atoms) and composite events (table-atoms), and as such can be represented more intuitively as a graph. Therefore, and to better visualize our learning algorithm, we are introducing a new data structure called a support graph, although the new EM algorithm itself is described exclusively by the hierarchical system of t-declarations in the next subsection. As illustrated in Figure 7 (a), the support graph for Gtis consists of a graphical representation of the hierarchical system of t-declarations DB = h t 0; t 1;:; t Kti (t 0 = G t) for G t in (20).It consists of completely ordered unconnected subgraphs, each of which is labeled with the corresponding table tkint DB (0 k K t)."}, {"heading": "4.9 Graphical EM Algorithm", "text": "We describe here an e-learning algorithm called a graphical EM algorithm: Q = DB Q (DB 8) introduced by Kameya and Sato (2000), which runs on support graphs. Suppose we already have a random sample G = G1;::; GTof observable atoms (DB naized supportgraphs for Gt (1 t T), i.e. hierarchical systems of t-declarations that meet the acyclicsupport condition, the t-exclusivity condition and the independent condition, were successfully developed from a parameterized DB logic program that meets the uniqueness condition and the nite support condition.The graphical EM algorithms re nes Learn-naive (DB, G) by introducing two subroutines, get-inside-probs (DB, G) to calculate internal probabilities and get expectations (DB, G)."}, {"heading": "5. Complexity", "text": "In this section, we analyze the temporal complexity of the graphical EM algorithm, which is applied to various symbolic-statistical frameworks such as HMMs, PCFGs, pseudo-PCSGs, and Bayesian networks. Results show that the graphical EM algorithm competes with these specialized EM algorithms, which have been developed independently in each area of research."}, {"heading": "5.1 Basic Property", "text": "In fact, most of us will be able to move to another world where we are able to integrate, \"he said in an interview with the German Press Agency."}, {"heading": "5.2 HMMs", "text": "It is about the question of whether and in what way the people of the EU will be able to integrate into the EU. (...) It is about the question of whether the EU will be able to integrate into the EU. (...) It is about the question of how far the EU is able to integrate into the EU. (...) It is about the question of how far the EU will be able to integrate into the EU. (...) It is about the question of how far the EU is able to integrate into the EU. (...) It is about the question of how far the EU will integrate into the EU. (...) It is about the EU. (...) It is about. (...) It is about the EU. (...) It is about. (...) It is about the EU. (...) It is about. (...) It is about. (...) It is about. (...) It is about. (...) It is about. (... It is about. (...) It is about. (... It is about. (...) It is about. (... It is about. (...) It is about. (it is about. (...) It is about. (it is about) It is about. (it is about. (...) It is about."}, {"heading": "5.3 PCFGs", "text": "Dre rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the"}, {"heading": "44. L here is not a Prolog variable but a constant denoting the sentence length.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "45. q is a table predicate.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "46. It can be inductively proved that T", "text": "(1) d + 1contains each calculated q (i, d0, d00) (0 q L 3; d + 1 d0 < d00L; 1 i N; d00d0L d 2).computed) in O (1) time. Since all clauses are grounded, such an execution creates only a single child node. We enumerate h (1) d, the number of nodes in T (1) d, but not in T (k) d + 1 (1 k N). From Figure 10 we see h (1) d = O (N d) 2). 47 h (k) d (2 k) d) the number of nodes inT (k) d + 1 is not included in T (1 k N). It is estimated as O (N 2) d = O (L d 2)."}, {"heading": "48. The number of nodes in T", "text": "This year it is more than ever before."}, {"heading": "5.4 Pseudo PCSGs", "text": "PCFGs can be improved by making context-sensitive decisions, and one of these attempts is pseudo-PCSGs (pseudo probabilistic context-sensitive grammars), where a rule is likely to be chosen depending on both the non-terminal to be extended and the higher-level non-terminal (Charniak & Carroll, 1994).A pseudo-PCSG is easily programmable.We add an extra argument to the predicate q '(I, D0, D2, C0, L0-L2) in Figure 11 and replace msw (I, C0, [J, K]) with msw ([N, I], C0, [J, K]]).Since the (linkest) derivation of a sentence from a pseudo-PCSG is still a pseudo-pseudo-sequential decision process supplemented by the modified program, the graphical EM algorithm will be applied to pseudo-pseudo-1 pseudo-1 pseudo-G; pseudo-1 pseudo-1 pseudo-observed rules of the MG;"}, {"heading": "5.5 Bayesian Networks", "text": "A relationship between Cause C and its e ect E is often probable like that between diseases and symptoms, and as such it is mathematically understood as the conditional probability P (E = e j C = c) of e ect e in the face of Cause c; however, what we want to know is the reverse, i.e. the probability of a candidate causing c given proofs e, i.e. P (C = c j E = e), that of Bayes' theorem as P (E = e j C = c) P (C = e j C = c0) P (C = c0). Bayesian networks are a representative / computational framework representing this type of probable inference (Pearl, 1988; Castillo et al, 1997).A Bayesian network is a graphical representation of a common distribution P (X1 = x1)."}, {"heading": "58. Since distribution semantics is based on the least model semantics, and because unfold/fold transforma-", "text": "The transformation applied to parameterized logic programs (Tamaki & Sato, 1984) preserves the least Herbrand model of the transformed program, the unfolding / folding of the transformation applied to parameterized logic programs preserves the designation of the transformed program. 59. DBG is further transformed for the OLDT interpreter to collect msw atoms, as in the case of the HMM program. The OLDT search for X is limited from above, by constant O (val (U1) val (U2) val (U3) val (X))) in the case of Figure 15. Consequently, the OLDT time is proportional to the number of nodes in the original diagram G (and also the size of the supporting diagram), provided that the number of edges connecting to a node and the values of a random variable are limited from above."}, {"heading": "V the set of nodes, and DB", "text": "Suppose access to the table can be done in O (1) time. Then, if there are T observations, the time complexity is O (jV jD) for an observed value u of a random variable U using DBGisO (jV j), and so the time complexity required to compute a marginal distribution for an individually connected Bayesian network by a standard algorithm (Pearl, 1988; Castillo et al., 1997), and also the time complexity is O (jV jD).O (jV j) is the time complexity required to compute a marginal distribution for an individually connected Bayesian network by a standard algorithm."}, {"heading": "60. When a marginal distribution of P", "text": "If more than one variable is required, we can construct a similar program that calculates marginal probabilities in O (jV j) time by adding extra arguments that provide other proofs, or by embedding other proofs in the program.61 We verify the Ve conditions with DBGin Figure 13. The uniqueness condition is obvious because sampling always clearly generates a sampled value for each random variable. the ninth support condition is met because there is only a nite number of random variables and their values. The acyclic support condition is directly due to the proliferation of Bayean networks. The t exclusivity condition and the independent condition are easy to verify."}, {"heading": "5.6 Modeling Language PRISM", "text": "We have developed a symbolic-statistical modeling part that presents PRISM since 1995 (http: / / mi.cs.titech.jp / prism /) as an implementation of distributional semantics (Sato, 1995; Sato & Kameya, 1997; Sato, 1998). Language is designed to model complex symbolic-statistical phenomena, such as discourse interpretations in natural language processing and inheritance of genes that interact with social rules. As a programming language, it looks like an extension of prologue with new built-in predicates that include the msw predicate and other special predicates for manipulating msw atoms and their parameters. A PRISM program consists of three parts, one for modeling and one for utilities."}, {"heading": "6. Learning Experiments", "text": "After analyzing the complexity of the graphical EM algorithm for popular symbolic probabilistic models in the previous section, we will consider the actual behavior of the graphical EM algorithm with real data in this section. We conducted learning experiments with PCFGs using two corpora with contrasting characters, and compared the performance of the graphical EM algorithm with that of the inside-outside algorithm in terms of time per iteration (= time for updating parameters). Results indicate that the graphical EM algorithm can exceed the inside-outside algorithm by orders of magnitude. Detalis are reported by Sato, Kameya, Abe and Shirai (2001)."}, {"heading": "6.1 The Inside-Outside Algorithm", "text": "The Inside-Outside algorithm was proposed by Baker (1979) as a generalization of the Tree-Welch algorithm to PCFGs. The algorithm was developed to estimate parameters for a CFG grammar in the Chomsky normal form, which contains rules expressed by numbers such as i! j; k (1 i; j; k N for N-Nonterminals, where 1 is a starting symbol). Suppose an input clause w1;::; wLis is given. In each iteration, it first calculates inner probabilities e (s; t; i) = P (i) ws;:; wt) and then calculates outside the probabilities f (s; t; i) = P (S) w1;:; ws 1i wt + 1;:."}, {"heading": "6.2 Learning Experiments Using Two Corpora", "text": "We report here on the parameter learning of existing PCFGs using two corpora of moderate size and compare the graphical EM algorithm with the inside-outside algorithm in terms of time per iteration. As already mentioned, support graphs, inputs to the Garphic EM algorithm, were generated by a parser, i.e. an MSLR parser. 62 All measurements were made on a 296MHz Sun UltraSPARC-II with 2GB of memory under Solaris 2.6 and the threshold for increasing the log probability of input sets was set to 10 6 as a stop criterion for the EM algorithms. In the experiments we used ATR corpus and EDR corpus (each converted into a POS (part of the language) tagged corpus), which are similarly large (about 10,000), but contrasting in their characters, sentence length and ambiguity of their grammatics."}, {"heading": "62. MSLR parser is a Tomita (Generalized LR) parser developed by Tanaka-Tokunaga Laboratory in Tokyo", "text": "This year, it has reached the point where it will be able to put itself at the top of the list in the way that it is able to put itself at the top."}, {"heading": "6.3 Examing the Performance Gap", "text": "This year, it will be able to fix and fix the mentioned bugs."}, {"heading": "7. Related Work and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Related Work", "text": "The work presented in this book is designed in such a way that it is able to focus on the perception of people, not only in relation to themselves, but also to themselves, but also in relation to themselves."}, {"heading": "7.2 Limitations and Potential Problems", "text": "The approaches described so far have more or less similar limitations and potential problems. Descriptive power conned to nite domains is the most common limitation due to the use of linear programming techniques (Nilsson, 1986), or to the syntactic constraints resulting in nitely many constant, function or predicate symbols (Ng & Subrahmanian, 1992; Lakshmanan & Sadri, 1994).Bayesian networks have the same constraint as well (only a nite number of random variables are representable).Also, there are various semantic / syntactic constraints on logical programs."}, {"heading": "7.3 EM Learning", "text": "Since EM learning is one of the central topics in this paper, we mention work on EM learning for symbolic frameworks. Koller and Pfe er (1997) used EM learning in their approach to KBMC EM learning (knowledge-based model construction) to estimate parameter recognition clauses, expressing probability dependencies between events through de nite clauses commented with probabilities, similar to the approach of Ngo and Haddawy (1997), and building a Bayesian network on site that is relevant to context and evidence as well as evidence."}, {"heading": "68. However, RPMs (recursive probability models) proposed by Pfe er and Koller (2000) as an extension", "text": "They are organized as attributes of classes and a probability measurement via attribute values is introduced. Parameters are queried by applying the specialized EM algorithm for Bayesian networks to the constructed network (Castillo et al., 1997). Dealing with a PCFG through a statically constructed Bayesian network was suggested, Pynadath and Wellman (1998), and it is possible to combine the EM algorithm with their method for estimating parameters in the PCFG. Unfortunately, the constructed network is not individually connected, and the temporal complexity of the probability calculation is potentially exponential in the length of an input set. Closely related to our EM learning is parameter learning of log-linear models. Riezler (1998) proposed the IM algorithm in his approach to probabilistic limitation of programming."}, {"heading": "7.4 Future Directions", "text": "It is time for us to set out in search of new ways to travel the world, to travel the world in which we live."}, {"heading": "8. Conclusion", "text": "We have proposed a logical / mathematical framework for learning parametrized logic programs, i.e. we have a probability distribution over possible worlds (Herbrand interpretations) that is unconditionally applicable to any logic program, including those for HMMs, PCFGs and bailouts. We also have a new algorithm that we find in Section 4, which has statistical parameters for a class of parametric systems in which every decision is exclusive and independent."}, {"heading": "Acknowledgments", "text": "The authors thank three anonymous arbitrators for their comments and suggestions. Special thanks go to Takashi Mori and Shigeru Abe for stimulating discussions and learning experiments, as well as to the Tanaka Tokunaga laboratory, which kindly allowed them to use the MSLR parser and the linguistic data."}, {"heading": "Appendix A. Properties of P", "text": "DBIn this appendix we list some properties of PDBde ned by a parameterized logic programDB = > DB = F [R in a countable language rst-order L.69First of all, PDBassigns consistentabilities70 exist for each closed formula in L byPDB () def = PDB (f! 2DBj! j = g) and guarantee continuity in the sense of limn! 1PDB ((t1) ^ (tn))))) = PDB (8x (x))) limn! 1PDB ((((t1) _ _ (tn)) = PDB (9x (x))), where t1; t2;: is a list of basic terms in L.The next proposition, A.1, refers to the PDBB- to the Herbrand model."}, {"heading": "69. For de nitions of", "text": "We also assume that variables are bound to Q2QnF. Q2QnF = Then Q91G = DB = 9DB. By consistency, we mean to respect probabilities associated with logical formulas, the probability laws such as 0 P (A) 1, P (: A) = 1 P (A _ B) = P (A) + P (B) P (A ^ B). (Proof) Remember that a closed formula has an equivalent prenex disjunctive normal form that belongs to it. We prove the proposition for formulas in the use of Q1Q2QnF at the multi-set ordering level via fr () 2 g. If r () =;, has no quanti cation. So the proposition of Lemma A.1. Suppose is correct otherwise. Write = G [Q1Q2QnF] where Q2QnF is treated on a single occurrence of a factor in G.71We assume that 8x is treated similarly."}, {"heading": "Appendix B. The MAR (missing at random) Condition", "text": "In the original formulation of the EM algorithm by Dempster et al. (1977), it is assumed that there is a multiple-to-one mapping y = (x) from complete data x to incomplete (observed) data y. In the case of parsing, however, x is a parser tree and y is the input set and x determines unambiguously y. In this paper, the unambiguity condition ensures the existence of such multiple-to-one mapping of explanations for observations. However, we are sometimes faced with a situation in which there is no such multiple-to-one mapping of complete data, but nevertheless, we would like to apply the EM algorithm. This dilemma can be solved by introducing a missing-to-one mechanism that makes complete data incomplete. The missing-end mechanism m has a distribution g (m j x), which we parameterise through and y, the observed data."}], "references": [{"title": "Stochastic attribute-value grammars", "author": ["S. Abney"], "venue": "Computational Linguistics, 23 (4), 597{618.", "citeRegEx": "Abney,? 1997", "shortCiteRegEx": "Abney", "year": 1997}, {"title": "Learning acyclic rst-order horn sentences from entailment", "author": ["H. Arimura"], "venue": "Proceedings of the Eighth International Workshop on Algorithmic Learning Theory. Ohmsha/Springer-Verlag.", "citeRegEx": "Arimura,? 1997", "shortCiteRegEx": "Arimura", "year": 1997}, {"title": "From statistical knowledge bases to degrees of belief", "author": ["F. Bacchus", "A. Grove", "J. Halpern", "D. Koller"], "venue": "Arti cial Intelligence,", "citeRegEx": "Bacchus et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 1996}, {"title": "Trainable grammars for speech recognition", "author": ["J.K. Baker"], "venue": "Proceedings of Spring Conference of the Acoustical Society of America, pp. 547{550. 450", "citeRegEx": "Baker,? 1979", "shortCiteRegEx": "Baker", "year": 1979}, {"title": "Valence induction with a head-lexicalized PCFG", "author": ["G. Carroll", "M. Rooth"], "venue": null, "citeRegEx": "Carroll and Rooth,? \\Q1998\\E", "shortCiteRegEx": "Carroll and Rooth", "year": 1998}, {"title": "Probability Theory (3rd ed.)", "author": ["Y. Chow", "H. Teicher"], "venue": "Negation as failure", "citeRegEx": "Chow and Teicher,? \\Q1997\\E", "shortCiteRegEx": "Chow and Teicher", "year": 1997}, {"title": "Introduction to Algorithms", "author": ["T. Press. Cormen", "C. Leiserson", "R. Rivest"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 1990}, {"title": "Maximum likelihood from incomplete", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "International Conference on Logic Programming", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "From Logic to Logic Programming", "author": ["K. Doets"], "venue": "EM algorithm. Royal Statistical Society,", "citeRegEx": "Doets,? \\Q1994\\E", "shortCiteRegEx": "Doets", "year": 1994}, {"title": "A probabilistic parsing", "author": ["T. Fujisaki", "F. Jelinek", "J. Cocke", "E. Black", "T. Nishino"], "venue": "Arti cial Intelligence,", "citeRegEx": "Fujisaki et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Fujisaki et al\\.", "year": 1989}, {"title": "Abductive logic programming", "author": ["A.C. Kakas", "R.A. Kowalski", "F. Toni"], "venue": "Journal of Logic and Computation,", "citeRegEx": "Kakas et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Kakas et al\\.", "year": 1992}, {"title": "Learning and Representation of Symbolic-Statistical Knowledge (in Japanese)", "author": ["Y. Kameya"], "venue": "Ph. D. dissertation, Tokyo Institute of Technology.", "citeRegEx": "Kameya,? 2000", "shortCiteRegEx": "Kameya", "year": 2000}, {"title": "E cient EM learning for parameterized logic programs", "author": ["Y. Kameya", "T. Sato"], "venue": "In Proceedings of the 1st Conference on Computational Logic (CL2000),", "citeRegEx": "Kameya and Sato,? \\Q2000\\E", "shortCiteRegEx": "Kameya and Sato", "year": 2000}, {"title": "Probabilistic Language Models (in Japanese)", "author": ["K. Kita"], "venue": "Tokyo Daigaku Syuppan-kai.", "citeRegEx": "Kita,? 1999", "shortCiteRegEx": "Kita", "year": 1999}, {"title": "E ective Bayesian inference for stochastic programs", "author": ["D. Koller", "D. McAllester", "A. Pfe er"], "venue": "In Proceedings of 15th National Conference on Arti cial Intelligence", "citeRegEx": "Koller et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Koller et al\\.", "year": 1997}, {"title": "Learning probabilities for noisy rst-order rules", "author": ["D. Koller"], "venue": "Pfe er,", "citeRegEx": "Koller,? \\Q1997\\E", "shortCiteRegEx": "Koller", "year": 1997}, {"title": "Uncertainty logics", "author": ["H. Kyburg"], "venue": "Gabbay, D., Hogger, C., & Robinson, J. (Eds.), Handbook of Logics in Arti cial Intelligence and Logic Programming, pp. 397{438. Oxford Science Publications.", "citeRegEx": "Kyburg,? 1994", "shortCiteRegEx": "Kyburg", "year": 1994}, {"title": "A derivation of the Inside-Outside Algorithm from the EM algorithm", "author": ["J. La erty"], "venue": "Technical report, IBM T.J.Watson Research Center.", "citeRegEx": "erty,? 1993", "shortCiteRegEx": "erty", "year": 1993}, {"title": "Probabilistic deductive databases", "author": ["L.V.S. Lakshmanan", "F. Sadri"], "venue": "In Proceedings of the 1994 International Symposium on Logic Programming", "citeRegEx": "Lakshmanan and Sadri,? \\Q1994\\E", "shortCiteRegEx": "Lakshmanan and Sadri", "year": 1994}, {"title": "The estimation of stochastic context-free grammars using the Inside-Outside algorithm", "author": ["K. Lari", "S.J. Young"], "venue": "Computer Speech and Language,", "citeRegEx": "Lari and Young,? \\Q1990\\E", "shortCiteRegEx": "Lari and Young", "year": 1990}, {"title": "E cient inference in Bayes networks as a combinatorial optimization problem", "author": ["Z. Li", "B. D'Ambrosio"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Li and D.Ambrosio,? \\Q1994\\E", "shortCiteRegEx": "Li and D.Ambrosio", "year": 1994}, {"title": "Foundations of Logic Programming", "author": ["J.W. Lloyd"], "venue": "Springer-Verlag.", "citeRegEx": "Lloyd,? 1984", "shortCiteRegEx": "Lloyd", "year": 1984}, {"title": "Probabilistic deduction with conditional constraints over basic events", "author": ["T. Lukasiewicz"], "venue": "Journal of Arti cial Intelligence Research, 10, 199{241.", "citeRegEx": "Lukasiewicz,? 1999", "shortCiteRegEx": "Lukasiewicz", "year": 1999}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["C.D. Manning", "H. Sch\u007futze"], "venue": null, "citeRegEx": "Manning and Sch\u007futze,? \\Q1999\\E", "shortCiteRegEx": "Manning and Sch\u007futze", "year": 1999}, {"title": "The EM Algorithm and Extensions", "author": ["G.J. McLachlan", "T. Krishnan"], "venue": null, "citeRegEx": "McLachlan and Krishnan,? \\Q1997\\E", "shortCiteRegEx": "McLachlan and Krishnan", "year": 1997}, {"title": "Stochastic logic programs", "author": ["S. Muggleton"], "venue": "de Raedt, L. (Ed.), Advances in Inductive Logic Programming, pp. 254{264. IOS Press.", "citeRegEx": "Muggleton,? 1996", "shortCiteRegEx": "Muggleton", "year": 1996}, {"title": "Probabilistic logic programming", "author": ["R. Ng", "V.S. Subrahmanian"], "venue": "Information and Computation,", "citeRegEx": "Ng and Subrahmanian,? \\Q1992\\E", "shortCiteRegEx": "Ng and Subrahmanian", "year": 1992}, {"title": "Answering queries from context-sensitive probabilistic knowledge bases", "author": ["L. Ngo", "P. Haddawy"], "venue": "Theoretical Computer Science,", "citeRegEx": "Ngo and Haddawy,? \\Q1997\\E", "shortCiteRegEx": "Ngo and Haddawy", "year": 1997}, {"title": "Probabilistic logic", "author": ["N.J. Nilsson"], "venue": "Arti cial Intelligence, 28, 71{87. 452", "citeRegEx": "Nilsson,? 1986", "shortCiteRegEx": "Nilsson", "year": 1986}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": "Morgan Kaufmann.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Inside-Outside reestimation from partially bracketed corpora", "author": ["F.C.N. Pereira", "Y. Schabes"], "venue": "In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Pereira and Schabes,? \\Q1992\\E", "shortCiteRegEx": "Pereira and Schabes", "year": 1992}, {"title": "De nite clause grammars for language analysis | a survey of the formalism and a comparison with augmented transition networks", "author": ["F.C.N. Pereira", "D.H.D. Warren"], "venue": "Arti cial Intelligence,", "citeRegEx": "Pereira and Warren,? \\Q1980\\E", "shortCiteRegEx": "Pereira and Warren", "year": 1980}, {"title": "Semantics and inference for recursive probability models", "author": ["A. Pfe er", "D. Koller"], "venue": "In Proceedings of the Seventh National Conference on Arti cial Intelligence", "citeRegEx": "er and Koller,? \\Q2000\\E", "shortCiteRegEx": "er and Koller", "year": 2000}, {"title": "Probabilistic Horn abduction and Bayesian networks", "author": ["D. Poole"], "venue": "Arti cial Intelligence, 64 (1), 81{129.", "citeRegEx": "Poole,? 1993", "shortCiteRegEx": "Poole", "year": 1993}, {"title": "Generalized queries on probabilistic context-free grammars", "author": ["D.V. Pynadath", "M.P. Wellman"], "venue": "IEEE Transaction on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Pynadath and Wellman,? \\Q1998\\E", "shortCiteRegEx": "Pynadath and Wellman", "year": 1998}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE, 77 (2), 257{286.", "citeRegEx": "Rabiner,? 1989", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "Foundations of Speech Recognition", "author": ["L.R. Rabiner", "B. Juang"], "venue": null, "citeRegEx": "Rabiner and Juang,? \\Q1993\\E", "shortCiteRegEx": "Rabiner and Juang", "year": 1993}, {"title": "E cient tabling mechanisms for logic programs", "author": ["I. Ramakrishnan", "P. Rao", "K. Sagonas", "T. Swift", "D. Warren"], "venue": "In Proceedings of the 12th International Conference on Logic Programming", "citeRegEx": "Ramakrishnan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Ramakrishnan et al\\.", "year": 1995}, {"title": "Learning rst-order acyclic horn programs from entailment", "author": ["C. Reddy", "P. Tadepalli"], "venue": "In Proceedings of the 15th International Conference on Machine Learning; (and Proceedings of the 8th International Conference on Inductive Logic Programming)", "citeRegEx": "Reddy and Tadepalli,? \\Q1998\\E", "shortCiteRegEx": "Reddy and Tadepalli", "year": 1998}, {"title": "Probabilistic Constraint Logic Programming", "author": ["S. Riezler"], "venue": "Ph.D. thesis, Universit\u007f", "citeRegEx": "Riezler,? 1998", "shortCiteRegEx": "Riezler", "year": 1998}, {"title": "Inference and missing data", "author": ["D. Rubin"], "venue": "Biometrika, 63 (3), 581{592.", "citeRegEx": "Rubin,? 1976", "shortCiteRegEx": "Rubin", "year": 1976}, {"title": "XSB as an e cient deductive database engine", "author": ["K. Sagonas", "S. T", "D. Warren"], "venue": "In Proceedings of the 1994 ACM SIGMOD International Conference on Management of Data,", "citeRegEx": "Sagonas et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Sagonas et al\\.", "year": 1994}, {"title": "A statistical learning method for logic programs with distribution semantics", "author": ["T. Sato"], "venue": "Proceedings of the 12th International Conference on Logic Programming (ICLP'95), pp. 715{729.", "citeRegEx": "Sato,? 1995", "shortCiteRegEx": "Sato", "year": 1995}, {"title": "Modeling scienti c theories as PRISM programs", "author": ["T. Sato"], "venue": "Proceedings of ECAI'98 Workshop on Machine Discovery, pp. 37{45.", "citeRegEx": "Sato,? 1998", "shortCiteRegEx": "Sato", "year": 1998}, {"title": "Minimum likelihood estimation from negative examples in statistical abduction", "author": ["T. Sato"], "venue": "Proceedings of IJCAI-01 workshop on Abductive Reasoning, pp. 41{47.", "citeRegEx": "Sato,? 2001", "shortCiteRegEx": "Sato", "year": 2001}, {"title": "PRISM: a language for symbolic-statistical modeling", "author": ["T. Sato", "Y. Kameya"], "venue": "In Proceedings of the 15th International Joint Conference on Arti cial Intelligence", "citeRegEx": "Sato and Kameya,? \\Q1997\\E", "shortCiteRegEx": "Sato and Kameya", "year": 1997}, {"title": "A Viterbi-like algorithm and EM learning for statistical abduction", "author": ["T. Sato", "Y. Kameya"], "venue": "In Proceedings of UAI2000 Workshop on Fusion of Domain Knowledge with Data for Decision Support", "citeRegEx": "Sato and Kameya,? \\Q2000\\E", "shortCiteRegEx": "Sato and Kameya", "year": 2000}, {"title": "Fast EM learning of a family of PCFGs", "author": ["T. Sato", "Y. Kameya", "S. Abe", "K. Shirai"], "venue": "Titech technical report (Dept. of CS) TR01-0006,", "citeRegEx": "Sato et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sato et al\\.", "year": 2001}, {"title": "Linear tabulated resolution based on Prolog control strategy", "author": ["Y. Shen", "L. Yuan", "J. You", "N. Zhou"], "venue": "Theory and Practice of Logic Programming,", "citeRegEx": "Shen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2001}, {"title": "The Art of Prolog", "author": ["L. Sterling", "E. Shapiro"], "venue": null, "citeRegEx": "Sterling and Shapiro,? \\Q1986\\E", "shortCiteRegEx": "Sterling and Shapiro", "year": 1986}, {"title": "An e cient probabilistic context-free parsing algorithm that computes pre x probabilities", "author": ["A. Stolcke"], "venue": "Computational Linguistics, 21 (2), 165{201.", "citeRegEx": "Stolcke,? 1995", "shortCiteRegEx": "Stolcke", "year": 1995}, {"title": "Unfold/fold transformation of logic programs", "author": ["H. Tamaki", "T. Sato"], "venue": "In Proceedings of the 2nd International Conference on Logic Programming (ICLP'84), Lecture Notes in Computer Science,", "citeRegEx": "Tamaki and Sato,? \\Q1984\\E", "shortCiteRegEx": "Tamaki and Sato", "year": 1984}, {"title": "OLD resolution with tabulation", "author": ["H. Tamaki", "T. Sato"], "venue": "In Proceedings of the 3rd International Conference on Logic Programming (ICLP'86),", "citeRegEx": "Tamaki and Sato,? \\Q1986\\E", "shortCiteRegEx": "Tamaki and Sato", "year": 1986}, {"title": "Japanese grammar for speech recognition considering the MSLR method", "author": ["H. Tanaka", "T. Takezawa", "J. Etoh"], "venue": "In Proceedings of the meeting of SIG-SLP (Spoken Language Processing),", "citeRegEx": "Tanaka et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tanaka et al\\.", "year": 1997}, {"title": "ATR integrated speech and language database", "author": ["N. Uratani", "T. Takezawa", "H. Matsuo", "C. Morita"], "venue": "Technical report TR-IT-0056,", "citeRegEx": "Uratani et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Uratani et al\\.", "year": 1994}, {"title": "Memoing for logic programs", "author": ["D.S. Warren"], "venue": "Communications of the ACM, 35 (3), 93{111.", "citeRegEx": "Warren,? 1992", "shortCiteRegEx": "Warren", "year": 1992}, {"title": "Probabilistic languages: a review and some open questions", "author": ["C.S. Wetherell"], "venue": "Computing Surveys, 12 (4), 361{379.", "citeRegEx": "Wetherell,? 1980", "shortCiteRegEx": "Wetherell", "year": 1980}, {"title": "An Anatomy of Kinship", "author": ["H.C. White"], "venue": "Prentice-Hall.", "citeRegEx": "White,? 1963", "shortCiteRegEx": "White", "year": 1963}, {"title": "Exploiting causal independence in Bayesian network inference", "author": ["N. Zhang", "D. Poole"], "venue": "Journal of Arti cial Intelligence Research,", "citeRegEx": "Zhang and Poole,? \\Q1996\\E", "shortCiteRegEx": "Zhang and Poole", "year": 1996}], "referenceMentions": [{"referenceID": 29, "context": "4 Statistical abduction is powerful in that it not only subsumes diverse symbolic-statistical frameworks such as HMMs (hidden Markov models, Rabiner, 1989), PCFGs (probabilistic context free grammars, Wetherell, 1980; Manning & Sch\u007f utze, 1999) and (discrete) Bayesian networks (Pearl, 1988; Castillo, Gutierrez, & Hadi, 1997) but gives us freedom of using arbitrarily complex logic programs for modeling.", "startOffset": 278, "endOffset": 326}, {"referenceID": 42, "context": "The semantic basis for statistical abduction is distribution semantics introduced by Sato (1995). It de nes a parameterized distribution, actually a probability measure, over the set of possible truth assignments to ground atoms and enables us to derive a new EM algorithm 6", "startOffset": 85, "endOffset": 97}, {"referenceID": 55, "context": "Redundancy in the rst phase is eliminated by tabulating partial explanations using OLDT search (Tamaki & Sato, 1986; Warren, 1992; Sagonas, T., & Warren, 1994; Ramakrishnan, Rao, Sagonas, Swift, & Warren, 1995; Shen, Yuan, You, & Zhou, 2001).", "startOffset": 95, "endOffset": 241}, {"referenceID": 21, "context": "A program including general clauses is sometimes called a general program (Lloyd, 1984; Doets, 1994).", "startOffset": 74, "endOffset": 100}, {"referenceID": 8, "context": "A program including general clauses is sometimes called a general program (Lloyd, 1984; Doets, 1994).", "startOffset": 74, "endOffset": 100}, {"referenceID": 8, "context": "A program including general clauses is sometimes called a general program (Lloyd, 1984; Doets, 1994). 2. Throughout this paper, for familiarity and readability, we will somewhat loosely use \\distribution\" as a synonym for \\probability measure\". 3. In logic programming, the adjective \\ground\" means no variables contained. 4. Abduction means inference to the best explanation for a set of observations. Logically, it is formalized as a search for an explanation E such that E;KB ` G where G is an atom representing our observation, KB a knowledge base and E a conjunction of atoms chosen from abducibles, i.e. a class of formulas allowed as primitive hypotheses (Kakas, Kowalski, & Toni, 1992; Flach & Kakas, 2000). E must be consistent with KB. 5. Existing symbolic-statistical modeling frameworks have restrictions and limitations of various types compared with arbitrary logic programs (see Section 7 for details). For example, Bayesian networks do not allow recursion. HMMs and PCFGs, stochastic grammars, allow recursion but lack variables and data structures. Recursive logic programs are allowed in Ngo and Haddawy's (1997) framework but they assume domains are nite and function symbols seem prohibited.", "startOffset": 88, "endOffset": 1131}, {"referenceID": 35, "context": "the Baum-Welch algorithm for HMMs (Rabiner, 1989) and the Inside-Outside algorithm for PCFGs (Baker, 1979), despite its generality.", "startOffset": 34, "endOffset": 49}, {"referenceID": 3, "context": "the Baum-Welch algorithm for HMMs (Rabiner, 1989) and the Inside-Outside algorithm for PCFGs (Baker, 1979), despite its generality.", "startOffset": 93, "endOffset": 106}, {"referenceID": 21, "context": "The reader is assumed to be familiar with the basics of logic programming (Lloyd, 1984; Doets, 1994), probability theory (Chow & Teicher, 1997), Bayesian networks (Pearl, 1988; Castillo et al.", "startOffset": 74, "endOffset": 100}, {"referenceID": 8, "context": "The reader is assumed to be familiar with the basics of logic programming (Lloyd, 1984; Doets, 1994), probability theory (Chow & Teicher, 1997), Bayesian networks (Pearl, 1988; Castillo et al.", "startOffset": 74, "endOffset": 100}, {"referenceID": 29, "context": "The reader is assumed to be familiar with the basics of logic programming (Lloyd, 1984; Doets, 1994), probability theory (Chow & Teicher, 1997), Bayesian networks (Pearl, 1988; Castillo et al., 1997) and stochastic grammars (Rabiner, 1989; Manning & Sch\u007f utze, 1999).", "startOffset": 163, "endOffset": 199}, {"referenceID": 35, "context": ", 1997) and stochastic grammars (Rabiner, 1989; Manning & Sch\u007f utze, 1999).", "startOffset": 32, "endOffset": 74}, {"referenceID": 29, "context": "sc-BN is a shorthand for a singly connected Bayesian network (Pearl, 1988).", "startOffset": 61, "endOffset": 74}, {"referenceID": 21, "context": "9 Usually there is more than one refutation for G, and the search space for all refutations is described by an SLD tree which may be in nite depending on the program and the goal (Lloyd, 1984; Doets, 1994).", "startOffset": 179, "endOffset": 205}, {"referenceID": 8, "context": "9 Usually there is more than one refutation for G, and the search space for all refutations is described by an SLD tree which may be in nite depending on the program and the goal (Lloyd, 1984; Doets, 1994).", "startOffset": 179, "endOffset": 205}, {"referenceID": 55, "context": "OLDT is such an instance of memoizing scheme (Tamaki & Sato, 1986; Warren, 1992; Sagonas et al., 1994; Ramakrishnan et al., 1995; Shen et al., 2001).", "startOffset": 45, "endOffset": 148}, {"referenceID": 41, "context": "OLDT is such an instance of memoizing scheme (Tamaki & Sato, 1986; Warren, 1992; Sagonas et al., 1994; Ramakrishnan et al., 1995; Shen et al., 2001).", "startOffset": 45, "endOffset": 148}, {"referenceID": 37, "context": "OLDT is such an instance of memoizing scheme (Tamaki & Sato, 1986; Warren, 1992; Sagonas et al., 1994; Ramakrishnan et al., 1995; Shen et al., 2001).", "startOffset": 45, "endOffset": 148}, {"referenceID": 48, "context": "OLDT is such an instance of memoizing scheme (Tamaki & Sato, 1986; Warren, 1992; Sagonas et al., 1994; Ramakrishnan et al., 1995; Shen et al., 2001).", "startOffset": 45, "endOffset": 148}, {"referenceID": 35, "context": "The development of a concrete EM algorithm such as the Baum-Welch algorithm for HMMs (Rabiner, 1989) and the Inside-Outside algorithm for PCFGs (Baker, 1979) requires individual e ort for each case.", "startOffset": 85, "endOffset": 100}, {"referenceID": 3, "context": "The development of a concrete EM algorithm such as the Baum-Welch algorithm for HMMs (Rabiner, 1989) and the Inside-Outside algorithm for PCFGs (Baker, 1979) requires individual e ort for each case.", "startOffset": 144, "endOffset": 157}, {"referenceID": 21, "context": "Next we extend it to a probability space over the Herbrand interpretations of all ground atoms in L by using the least model semantics (Lloyd, 1984; Doets, 1994).", "startOffset": 135, "endOffset": 161}, {"referenceID": 8, "context": "Next we extend it to a probability space over the Herbrand interpretations of all ground atoms in L by using the least model semantics (Lloyd, 1984; Doets, 1994).", "startOffset": 135, "endOffset": 161}, {"referenceID": 42, "context": "The choice of parameterized nite distributions made by Sato (1995) was simple:", "startOffset": 55, "endOffset": 67}, {"referenceID": 42, "context": "It might look too simple but expressive enough for Bayesian networks, Markov chains and HMMs (Sato, 1995; Sato & Kameya, 1997).", "startOffset": 93, "endOffset": 126}, {"referenceID": 21, "context": "Herbrand model (Lloyd, 1984; Doets, 1994).", "startOffset": 15, "endOffset": 41}, {"referenceID": 8, "context": "Herbrand model (Lloyd, 1984; Doets, 1994).", "startOffset": 15, "endOffset": 41}, {"referenceID": 21, "context": "Then imagine a de nite clause program DB 0 = R [ F and its least Herbrand model M DB 0 (Lloyd, 1984; Doets, 1994).", "startOffset": 87, "endOffset": 113}, {"referenceID": 8, "context": "Then imagine a de nite clause program DB 0 = R [ F and its least Herbrand model M DB 0 (Lloyd, 1984; Doets, 1994).", "startOffset": 87, "endOffset": 113}, {"referenceID": 21, "context": "Likewise comp(R) is a rst-order theory which deductively simulates SLD refutation with the help of E q by replacing a clause head atom with the clause body (Lloyd, 1984; Doets, 1994).", "startOffset": 156, "endOffset": 182}, {"referenceID": 8, "context": "Likewise comp(R) is a rst-order theory which deductively simulates SLD refutation with the help of E q by replacing a clause head atom with the clause body (Lloyd, 1984; Doets, 1994).", "startOffset": 156, "endOffset": 182}, {"referenceID": 40, "context": "This de nition of a support set di ers from the one used by Sato (1995) and Kameya and Sato (2000).", "startOffset": 60, "endOffset": 72}, {"referenceID": 11, "context": "This de nition of a support set di ers from the one used by Sato (1995) and Kameya and Sato (2000). 19.", "startOffset": 76, "endOffset": 99}, {"referenceID": 7, "context": "The uniqueness condition guarantees that there exists a (many-to-one) mapping from explanations to observations so that the EM algorithm is applicable (Dempster et al., 1977).", "startOffset": 151, "endOffset": 174}, {"referenceID": 7, "context": "The uniqueness condition guarantees that there exists a (many-to-one) mapping from explanations to observations so that the EM algorithm is applicable (Dempster et al., 1977). It is possible, however, to relax the uniqueness condition while justifying the application of the EM algorithm. We assume the MAR (missing at random) condition introduced by Rubin (1976) which is a statistical condition on how a complete data (explanation) becomes an incomplete data (observation), and is customarily assumed implicitly or explicitly in statistics (see Appendix B).", "startOffset": 152, "endOffset": 364}, {"referenceID": 3, "context": "In this subsection, we generalize the notion of inside probability and outside probability (Baker, 1979; Lari & Young, 1990) to logic programs.", "startOffset": 91, "endOffset": 124}, {"referenceID": 55, "context": "One way to obtain such t-explanations is to use OLDT search (Tamaki & Sato, 1986; Warren, 1992), a complete refutation method for logic programs.", "startOffset": 60, "endOffset": 95}, {"referenceID": 11, "context": "We describe here an e cient EM learning algorithm termed the graphical EM algorithm (Figure 8) introduced by Kameya and Sato (2000), that runs on support graphs.", "startOffset": 109, "endOffset": 132}, {"referenceID": 11, "context": "A formal proof is given by Kameya (2000). It is proved there that under the common parameters , [i; v] in learn-naive(DB,G) coincides with [i; v] in learn-gEM(DB,G).", "startOffset": 27, "endOffset": 41}, {"referenceID": 35, "context": "The standard EM algorithm for HMMs is the Baum-Welch algorithm (Rabiner, 1989; Rabiner & Juang, 1993).", "startOffset": 63, "endOffset": 101}, {"referenceID": 40, "context": "Sagonas et al. (1994) and Ramakrishnan et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 37, "context": "(1994) and Ramakrishnan et al. (1995) discuss about the implementation of OLDT.", "startOffset": 11, "endOffset": 38}, {"referenceID": 35, "context": "By the way, the Viterbi algorithm (Rabiner, 1989; Rabiner & Juang, 1993) provides for HMMs an e cient way of nding the most likely transition path for a given input/output string.", "startOffset": 34, "endOffset": 72}, {"referenceID": 3, "context": "We now compare the graphical EM algorithm with the Inside-Outside algorithm (Baker, 1979; Lari & Young, 1990).", "startOffset": 76, "endOffset": 109}, {"referenceID": 56, "context": "The Inside-Outside algorithm is a well-known EM algorithm for PCFGs (Wetherell, 1980; Manning & Sch\u007f utze, 1999).", "startOffset": 68, "endOffset": 112}, {"referenceID": 29, "context": "Bayesian networks are a representational/computational framework that ts best this type of probabilistic inference (Pearl, 1988; Castillo et al., 1997).", "startOffset": 115, "endOffset": 151}, {"referenceID": 29, "context": "In such case, the computation is possible in O(jV j) time where V is the set of vertices in the graph (Pearl, 1988).", "startOffset": 102, "endOffset": 115}, {"referenceID": 29, "context": "E cient computation of marginal distributions is not always possible but there is a well-known class of Bayesian networks, singly connected Bayesian networks, for which there exists an e cient algorithm to compute marginal distributions by message passing (Pearl, 1988; Castillo et al., 1997).", "startOffset": 256, "endOffset": 292}, {"referenceID": 29, "context": "O(jV j) is the time complexity required to compute a marignal distribution for a singly connected Bayesian network by a standard algorithm (Pearl, 1988; Castillo et al., 1997), and also is that of the EM algorithm using it.", "startOffset": 139, "endOffset": 175}, {"referenceID": 42, "context": "jp/prism/) as an implementation of distribution semantics (Sato, 1995; Sato & Kameya, 1997; Sato, 1998).", "startOffset": 58, "endOffset": 103}, {"referenceID": 43, "context": "jp/prism/) as an implementation of distribution semantics (Sato, 1995; Sato & Kameya, 1997; Sato, 1998).", "startOffset": 58, "endOffset": 103}, {"referenceID": 11, "context": "Detalis are reported by Sato, Kameya, Abe, and Shirai (2001). Before proceeding, we review the Inside-Outside algorithm for completeness.", "startOffset": 30, "endOffset": 61}, {"referenceID": 3, "context": "The Inside-Outside algorithm was proposed by Baker (1979) as a generalization of the Baum-Welch algorithm to PCFGs.", "startOffset": 45, "endOffset": 58}, {"referenceID": 3, "context": "The Inside-Outside algorithm was proposed by Baker (1979) as a generalization of the Baum-Welch algorithm to PCFGs. The algorithm is designed to estimate parameters for a CFG grammar in Chomsky normal form containing rules expressed by numbers like i! j; k (1 i; j; k N for N nonterminals, where 1 is a starting symbol). Suppose an input sentence w 1 ; : : : ; w L is given. In each iteration, it rst computes in a bottom up manner inside probabilities e(s; t; i) = P (i ) w s ; : : : ; w t ) and then computes outside probabilities f(s; t; i) = P (S ) w 1 ; : : : ; w s 1 i w t+1 ; : : : ; w L ) in a top-down manner for every s, t and i (1 s t L; 1 i N). After computing both probabilities, parameters are updated by using them, and this process iterates until some predetermined criterion such as a convergence of the likelihood of the input sentence is achieved. Although Baker did not give any analysis of the Inside-Outside algorithm, Lari and Young (1990) showed that it takes O(N 3 L 3 ) time in one iteration and La erty (1993) proved that it is the EM algorithm.", "startOffset": 45, "endOffset": 963}, {"referenceID": 3, "context": "The Inside-Outside algorithm was proposed by Baker (1979) as a generalization of the Baum-Welch algorithm to PCFGs. The algorithm is designed to estimate parameters for a CFG grammar in Chomsky normal form containing rules expressed by numbers like i! j; k (1 i; j; k N for N nonterminals, where 1 is a starting symbol). Suppose an input sentence w 1 ; : : : ; w L is given. In each iteration, it rst computes in a bottom up manner inside probabilities e(s; t; i) = P (i ) w s ; : : : ; w t ) and then computes outside probabilities f(s; t; i) = P (S ) w 1 ; : : : ; w s 1 i w t+1 ; : : : ; w L ) in a top-down manner for every s, t and i (1 s t L; 1 i N). After computing both probabilities, parameters are updated by using them, and this process iterates until some predetermined criterion such as a convergence of the likelihood of the input sentence is achieved. Although Baker did not give any analysis of the Inside-Outside algorithm, Lari and Young (1990) showed that it takes O(N 3 L 3 ) time in one iteration and La erty (1993) proved that it is the EM algorithm.", "startOffset": 45, "endOffset": 1037}, {"referenceID": 3, "context": "The Inside-Outside algorithm was proposed by Baker (1979) as a generalization of the Baum-Welch algorithm to PCFGs. The algorithm is designed to estimate parameters for a CFG grammar in Chomsky normal form containing rules expressed by numbers like i! j; k (1 i; j; k N for N nonterminals, where 1 is a starting symbol). Suppose an input sentence w 1 ; : : : ; w L is given. In each iteration, it rst computes in a bottom up manner inside probabilities e(s; t; i) = P (i ) w s ; : : : ; w t ) and then computes outside probabilities f(s; t; i) = P (S ) w 1 ; : : : ; w s 1 i w t+1 ; : : : ; w L ) in a top-down manner for every s, t and i (1 s t L; 1 i N). After computing both probabilities, parameters are updated by using them, and this process iterates until some predetermined criterion such as a convergence of the likelihood of the input sentence is achieved. Although Baker did not give any analysis of the Inside-Outside algorithm, Lari and Young (1990) showed that it takes O(N 3 L 3 ) time in one iteration and La erty (1993) proved that it is the EM algorithm. While it is true that the Inside-Outside algorithm has been recognized as a standard EM algortihm for training PCFGs, it is notoriously slow. Although there is not much literature explicitly stating time required by the Inside-Outside algorithm (Carroll & Rooth, 1998; Beil, Carroll, Prescher, Riezler, & Rooth, 1999), Beil et al. (1999) reported for example that when they trained a PCFG with 5,508 rules for a corpus of 450,526 German subordinate clauses whose average ambiguity is 9,202 trees/clause using four machines (167MHz Sun UltraSPARC 2 and 296MHz Sun UltraSPARC-II 2), it took 2.", "startOffset": 45, "endOffset": 1411}, {"referenceID": 53, "context": "As a skeleton of PCFG, we employed a context free grammar G atr comprising 860 rules (172 nonterminals and 441 terminals) manually developed for ATR corpus (Tanaka et al., 1997) which yields 958 parses/sentence.", "startOffset": 156, "endOffset": 177}, {"referenceID": 13, "context": "Actually Kita gave a re ned InsideOutside algorithm (Kita, 1999).", "startOffset": 52, "endOffset": 64}, {"referenceID": 47, "context": "We therefore conducted learning experiments with the entire ATR corpus using these two implementations and measured updating time per iteration (Sato et al., 2001).", "startOffset": 144, "endOffset": 163}, {"referenceID": 47, "context": "Assuming 100 iterations for learning ATR corpus however, it is estimated that even considering parsing time, the graphical EM algorithm combined with MSLR parser runs orders of magnitude faster than the three implementations (ours, Kita's and Johnson's) of the Inside-Outside algorithm (Sato et al., 2001).", "startOffset": 286, "endOffset": 305}, {"referenceID": 3, "context": "Since we implemented the Inside-Outside algorithm faithfully to Baker (1979), Lari and Young (1990), there is much room for improvement.", "startOffset": 64, "endOffset": 77}, {"referenceID": 3, "context": "Since we implemented the Inside-Outside algorithm faithfully to Baker (1979), Lari and Young (1990), there is much room for improvement.", "startOffset": 64, "endOffset": 100}, {"referenceID": 50, "context": "Finally we remark that the use of parsing as a preprocess for EM learning of PCFGs is not unique to the graphical EM algorithm (Fujisaki, Jelinek, Cocke, Black, & Nishino, 1989; Stolcke, 1995).", "startOffset": 127, "endOffset": 192}, {"referenceID": 50, "context": "Finally we remark that the use of parsing as a preprocess for EM learning of PCFGs is not unique to the graphical EM algorithm (Fujisaki, Jelinek, Cocke, Black, & Nishino, 1989; Stolcke, 1995). These approaches however still seem to contain redundancies compared with the graphical EM algorithm. For instance Stolcke (1995) uses an Earley chart to compute inside and outside probability, but parses are implicitly reconstructed in each iteration dynamically by combining completed items.", "startOffset": 178, "endOffset": 324}, {"referenceID": 28, "context": "A typical constraint approach is seen in the early work of probabilistic logic by Nilsson (1986). His central problem, \\probabilistic entailment problem\", is to compute the upper and lower bound of probability P( ) of a target sentence in such a way that the bounds are compatible with a given knowledge base containing logical sentences (not necessarily logic programs) annotated with a probability.", "startOffset": 82, "endOffset": 97}, {"referenceID": 41, "context": "Linguistic evaluations of the estimated parameters by the graphical EM algorithm are also reported by Sato et al. (2001). 65.", "startOffset": 102, "endOffset": 121}, {"referenceID": 16, "context": "For logic(s) concerning uncertainty, see an overview by Kyburg (1994).", "startOffset": 56, "endOffset": 70}, {"referenceID": 21, "context": "Later Lukasiewicz (1999) investigated the computational complexity of the probabilistic entailment problem in a slightly di erent setting.", "startOffset": 6, "endOffset": 25}, {"referenceID": 21, "context": "Later Lukasiewicz (1999) investigated the computational complexity of the probabilistic entailment problem in a slightly di erent setting. His knowledge base comprises statements of the form (H j G)[u 1 ; u 2 ] representing u 1 P(H j G) u 2 . He showed that inferring \\tight\" u 1 ; u 2 is NP-hard in general, and proposed a tractable class of knowledge base called conditional constraint trees. After the in uential work of Nilsson, Frish and Haddawy (1994) introduced a deductive system for probabilistic logic that remedies \\drawbacks\" of Nilsson's approach, that of computational intractability and the lack of a proof system.", "startOffset": 6, "endOffset": 458}, {"referenceID": 21, "context": "Later Lukasiewicz (1999) investigated the computational complexity of the probabilistic entailment problem in a slightly di erent setting. His knowledge base comprises statements of the form (H j G)[u 1 ; u 2 ] representing u 1 P(H j G) u 2 . He showed that inferring \\tight\" u 1 ; u 2 is NP-hard in general, and proposed a tractable class of knowledge base called conditional constraint trees. After the in uential work of Nilsson, Frish and Haddawy (1994) introduced a deductive system for probabilistic logic that remedies \\drawbacks\" of Nilsson's approach, that of computational intractability and the lack of a proof system. Their system deduces a probability range of a proposition by rules of probabilistic inferences about unconditional and conditional probabilities. For instance, one of the rules infers P ( j ) 2 [0 y] from P ( _ j ) 2 [x y] where , and are propositional variables and [x y] (x y) designates a probability range. Turning to logic programming, probabilistic logic programming formalized by Ng and Subrahmanian (1992) and Dekhtyar and Subrahmanian (1997) was also a constraint approach.", "startOffset": 6, "endOffset": 1044}, {"referenceID": 21, "context": "Later Lukasiewicz (1999) investigated the computational complexity of the probabilistic entailment problem in a slightly di erent setting. His knowledge base comprises statements of the form (H j G)[u 1 ; u 2 ] representing u 1 P(H j G) u 2 . He showed that inferring \\tight\" u 1 ; u 2 is NP-hard in general, and proposed a tractable class of knowledge base called conditional constraint trees. After the in uential work of Nilsson, Frish and Haddawy (1994) introduced a deductive system for probabilistic logic that remedies \\drawbacks\" of Nilsson's approach, that of computational intractability and the lack of a proof system. Their system deduces a probability range of a proposition by rules of probabilistic inferences about unconditional and conditional probabilities. For instance, one of the rules infers P ( j ) 2 [0 y] from P ( _ j ) 2 [x y] where , and are propositional variables and [x y] (x y) designates a probability range. Turning to logic programming, probabilistic logic programming formalized by Ng and Subrahmanian (1992) and Dekhtyar and Subrahmanian (1997) was also a constraint approach.", "startOffset": 6, "endOffset": 1081}, {"referenceID": 18, "context": "A similar framework was proposed by Lakshmanan and Sadri (1994) under the same syntactic restrictions ( nitely many constant and predicate symbols but no function symbols) in a di erent uncertainty setting.", "startOffset": 36, "endOffset": 64}, {"referenceID": 29, "context": "(Pearl, 1988; Castillo et al., 1997).", "startOffset": 0, "endOffset": 36}, {"referenceID": 28, "context": "(Pearl, 1988; Castillo et al., 1997). Researchers in Bayesian networks have been seeking for a way of mixing Bayesian networks with a logical representation to increase their inherently propositional expressive power. Breese (1992) used logic programs to automatically build a Bayesian network from a query.", "startOffset": 1, "endOffset": 232}, {"referenceID": 27, "context": "Ngo and Haddawy (1997) extended Breese's approach by incorporating a mechanism re ecting context.", "startOffset": 0, "endOffset": 23}, {"referenceID": 32, "context": "Instead of de ning a local distribution for each query, Poole (1993) de ned a global distribution in his \\probabilistic Horn abduction\".", "startOffset": 56, "endOffset": 69}, {"referenceID": 2, "context": "Bacchus et al. (1996) used a much more powerful rst-order probabilistic language than clauses annotated with probabilities.", "startOffset": 0, "endOffset": 22}, {"referenceID": 28, "context": "Descriptive power con ned to nite domains is the most common limitation, which is due to the use of the linear programming technique (Nilsson, 1986), or due to the syntactic restrictions not allowing for in nitely many constant, function or predicate symbols (Ng & Subrahmanian, 1992; Lakshmanan & Sadri, 1994).", "startOffset": 133, "endOffset": 148}, {"referenceID": 25, "context": "In SLPs, P(A) and P(A ^ A) do not necessarily coincide because A and A^A may have di erent refutations (Muggleton, 1996; Cussens, 1999, 2001).", "startOffset": 103, "endOffset": 141}, {"referenceID": 26, "context": "Descriptive power con ned to nite domains is the most common limitation, which is due to the use of the linear programming technique (Nilsson, 1986), or due to the syntactic restrictions not allowing for in nitely many constant, function or predicate symbols (Ng & Subrahmanian, 1992; Lakshmanan & Sadri, 1994). Bayesian networks have the same limitation as well (only a nite number of random variables are representable). 68 Also there are various semantic/syntactic restrictions on logic programs. For instance the acyclicity condition imposed by Poole (1993) and Ngo and Haddawy (1997) prevents the unconditional use of clauses with local variables, and the range-restrictedness imposed by Muggleton (1996) and Cussens (1999) excludes programs such as the usual membership Prolog program.", "startOffset": 134, "endOffset": 562}, {"referenceID": 26, "context": "For instance the acyclicity condition imposed by Poole (1993) and Ngo and Haddawy (1997) prevents the unconditional use of clauses with local variables, and the range-restrictedness imposed by Muggleton (1996) and Cussens (1999) excludes programs such as the usual membership Prolog program.", "startOffset": 66, "endOffset": 89}, {"referenceID": 25, "context": "For instance the acyclicity condition imposed by Poole (1993) and Ngo and Haddawy (1997) prevents the unconditional use of clauses with local variables, and the range-restrictedness imposed by Muggleton (1996) and Cussens (1999) excludes programs such as the usual membership Prolog program.", "startOffset": 193, "endOffset": 210}, {"referenceID": 25, "context": "For instance the acyclicity condition imposed by Poole (1993) and Ngo and Haddawy (1997) prevents the unconditional use of clauses with local variables, and the range-restrictedness imposed by Muggleton (1996) and Cussens (1999) excludes programs such as the usual membership Prolog program.", "startOffset": 193, "endOffset": 229}, {"referenceID": 25, "context": "For instance the acyclicity condition imposed by Poole (1993) and Ngo and Haddawy (1997) prevents the unconditional use of clauses with local variables, and the range-restrictedness imposed by Muggleton (1996) and Cussens (1999) excludes programs such as the usual membership Prolog program. There is another type of problem, the possibility of assigning con icting probabilities to logically equivalent formulas. In SLPs, P(A) and P(A ^ A) do not necessarily coincide because A and A^A may have di erent refutations (Muggleton, 1996; Cussens, 1999, 2001). Consequently in SLPs, we would be in trouble if we naively interpret P(A) as the probability of A's being true. Also assigning probabilities to arbitrary quanti ed formulas seems out of scope of both approaches to SLPs. Last but not least, there is a big problem common to any approach using probabilities: where do the numbers come from? Generally speaking, if we use n binary random variables in a model, we have to determine 2 n probabilities to completely specify their joint distribution, and ful lling this requirement with reliable numbers quickly becomes impossible as n grows. The situation is even worse when there are unobservable variables in the model such as possible causes of a disease. Apparently parameter learning from observed data is a natural solution to this problem, but parameter learning of logic programs has not been well studied. Distribution semantics proposed by Sato (1995) was an attempt to solve these problems along the line of the global distribution approach.", "startOffset": 193, "endOffset": 1463}, {"referenceID": 15, "context": "Koller and Pfe er (1997) used in their approach to KBMC (knowledge-based model construction) EM learning to estimate parameters labeling clauses.", "startOffset": 0, "endOffset": 25}, {"referenceID": 15, "context": "Koller and Pfe er (1997) used in their approach to KBMC (knowledge-based model construction) EM learning to estimate parameters labeling clauses. They express probabilistic dependencies among events by de nite clauses annotated with probabilities, similarly to Ngo and Haddawy's (1997) approach, and locally build a Bayesian network relevant to the context and evidence as well as the", "startOffset": 0, "endOffset": 286}, {"referenceID": 15, "context": "However, RPMs (recursive probability models) proposed by Pfe er and Koller (2000) as an extension of Bayesian networks allow for in nitely many random variables.", "startOffset": 68, "endOffset": 82}, {"referenceID": 32, "context": "Dealing with a PCFG by a statically constructed Bayesian network was proposed Pynadath and Wellman (1998), and it is possible to combine the EM algorithm with their method to estimate parameters in the PCFG.", "startOffset": 78, "endOffset": 106}, {"referenceID": 32, "context": "Dealing with a PCFG by a statically constructed Bayesian network was proposed Pynadath and Wellman (1998), and it is possible to combine the EM algorithm with their method to estimate parameters in the PCFG. Unfortunately, the constructed network is not singly connected, and time complexity of probability computation is potentially exponential in the length of an input sentence. Closely related to our EM learning is parameter learning of log-linear models. Riezler (1998) proposed the IM algorithm in his approach to probabilistic constraint programming.", "startOffset": 78, "endOffset": 476}, {"referenceID": 32, "context": "Dealing with a PCFG by a statically constructed Bayesian network was proposed Pynadath and Wellman (1998), and it is possible to combine the EM algorithm with their method to estimate parameters in the PCFG. Unfortunately, the constructed network is not singly connected, and time complexity of probability computation is potentially exponential in the length of an input sentence. Closely related to our EM learning is parameter learning of log-linear models. Riezler (1998) proposed the IM algorithm in his approach to probabilistic constraint programming. The IM algorithm is a general parameter estimation algorithm from incomplete data for log-linear models whose probability function P(x) takes the form P(x) = Z 1 exp ( P n i=1 i i (x)) p 0 (x) where ( 1 ; : : : ; n ) are parameters to be estimated, i (x) the i-th feature of an observed object x and Z the normalizing constant. Since a feature can be any function of x, the log-linear model is highly exible and includes our distribution P msw as a special case of Z = 1. There is a price to pay however; the computational cost of Z. It requires a summation over exponentially many terms. To avoid the cost of exact computation, approximate computation by a Monte Carlo method is possible. Whichever one may choose however, learning time increases compared to the EM algorithm for Z = 1. The FAM (failure-adjusted maximization) algorithm proposed by Cussens (2001) is an EM algorithm applicable to pure normalized SLPs that may fail.", "startOffset": 78, "endOffset": 1424}, {"referenceID": 14, "context": "Being slightly tangential to EM learning, Koller et al. (1997) developed a functional modeling language de ning a probability distribution over symbolic structures in which they showed \\cashing\" of computed results leads to e cient probability computation of singly connected Bayesian networks and PCFGs.", "startOffset": 42, "endOffset": 63}, {"referenceID": 57, "context": "We have tried various types of modeling, besides stochastic grammars and Bayesian networks, such as the modeling of gene inheritance in the Kariera tribe (White, 1963) where the rules of bi-lateral cross-cousin marriage for four clans interact with the rules of genetic inheritance (Sato, 1998).", "startOffset": 154, "endOffset": 167}, {"referenceID": 43, "context": "We have tried various types of modeling, besides stochastic grammars and Bayesian networks, such as the modeling of gene inheritance in the Kariera tribe (White, 1963) where the rules of bi-lateral cross-cousin marriage for four clans interact with the rules of genetic inheritance (Sato, 1998).", "startOffset": 282, "endOffset": 294}, {"referenceID": 1, "context": "Also investigating the role of the acyclicity condition seems theoretically interesting as the acyclicity is often related to the learning of logic programs (Arimura, 1997; Reddy & Tadepalli, 1998).", "startOffset": 157, "endOffset": 197}, {"referenceID": 0, "context": "For instance, uni cation-based grammars such as HPSGs (Abney, 1997) may be a good target beyond PCFGs because they use feature structures logically describable, and the ambiguity of feature values seems to be expressible by a probability distribution.", "startOffset": 54, "endOffset": 67}, {"referenceID": 0, "context": "Also investigating the role of the acyclicity condition seems theoretically interesting as the acyclicity is often related to the learning of logic programs (Arimura, 1997; Reddy & Tadepalli, 1998). In this paper we only scratched the surface of individual research elds such as HMMs, PCFGs and Bayesian networks. Therefore, there remains much to be done about clarifying how experiences in each research eld are re ected in the framework of parameterized logic programs. For example, we need to clarify the relationship between symbolic approaches to Bayesian networks such as SPI (Li, Z. & D'Ambrosio, B., 1994) and our approach. Also it is unclear how a compiled approach using the junction tree algorithm for Bayesian networks can be incorporated into our approach. Aside from exact methods, approximate methods of probability computation specialized for parameterized logic programs must also be developed. There is also a direction of improving learning ability by introducing priors instead of ML estimation to cope with data sparseness. The introduction of basic distributions that make probabilistic switches correlated seems worth trying in the near future. It is also important to take advantage of the logical nature of our approach to handle uncertainty. For example, it is already shown by Sato (2001) that we can learn parameters from negative examples such as \\the grass is not wet\" but the treatment of negative examples in parameterized logic programs is still in its infancy.", "startOffset": 158, "endOffset": 1316}, {"referenceID": 21, "context": "This de nition is di erent from the usual one (Lloyd, 1984; Doets, 1994) as we are here talking at ground level.", "startOffset": 46, "endOffset": 72}, {"referenceID": 8, "context": "This de nition is di erent from the usual one (Lloyd, 1984; Doets, 1994) as we are here talking at ground level.", "startOffset": 46, "endOffset": 72}, {"referenceID": 7, "context": "In the original formulation of the EM algorithm by Dempster et al. (1977), it is assumed that there exists a many-to-one mapping y = (x) from a complete data x to an incomplete (observed) data y.", "startOffset": 51, "endOffset": 74}, {"referenceID": 40, "context": "Rubin (1976) derived two conditions on g (data are missing at random and data are observed at random) collectively called theMAR (missing at random) condition, and showed that if we assume a missing-data mechanism behind our observations that satis es the MAR condition, we may estimate parameters of the distribution over x by simply applying the EM algorithm to y, the observed data.", "startOffset": 0, "endOffset": 13}], "year": 2011, "abstractText": "We propose a logical/mathematical framework for statistical parameter learning of parameterized logic programs, i.e. de nite clause programs containing probabilistic facts with a parameterized distribution. It extends the traditional least Herbrand model semantics in logic programming to distribution semantics , possible world semantics with a probability distribution which is unconditionally applicable to arbitrary logic programs including ones for HMMs, PCFGs and Bayesian networks. We also propose a new EM algorithm, the graphical EM algorithm, that runs for a class of parameterized logic programs representing sequential decision processes where each decision is exclusive and independent. It runs on a new data structure called support graphs describing the logical relationship between observations and their explanations, and learns parameters by computing inside and outside probability generalized for logic programs. The complexity analysis shows that when combined with OLDT search for all explanations for observations, the graphical EM algorithm, despite its generality, has the same time complexity as existing EM algorithms, i.e. the Baum-Welch algorithm for HMMs, the Inside-Outside algorithm for PCFGs, and the one for singly connected Bayesian networks that have been developed independently in each research eld. Learning experiments with PCFGs using two corpora of moderate size indicate that the graphical EM algorithm can signi cantly outperform the Inside-Outside algorithm.", "creator": "dvi2ps"}}}