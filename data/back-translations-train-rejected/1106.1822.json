{"id": "1106.1822", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Jun-2011", "title": "Efficient Solution Algorithms for Factored MDPs", "abstract": "This paper addresses the problem of planning under uncertainty in large Markov Decision Processes (MDPs). Factored MDPs represent a complex state space using state variables and the transition model using a dynamic Bayesian network. This representation often allows an exponential reduction in the representation size of structured MDPs, but the complexity of exact solution algorithms for such MDPs can grow exponentially in the representation size. In this paper, we present two approximate solution algorithms that exploit structure in factored MDPs. Both use an approximate value function represented as a linear combination of basis functions, where each basis function involves only a small subset of the domain variables. A key contribution of this paper is that it shows how the basic operations of both algorithms can be performed efficiently in closed form, by exploiting both additive and context-specific structure in a factored MDP. A central element of our algorithms is a novel linear program decomposition technique, analogous to variable elimination in Bayesian networks, which reduces an exponentially large LP to a provably equivalent, polynomial-sized one. One algorithm uses approximate linear programming, and the second approximate dynamic programming. Our dynamic programming algorithm is novel in that it uses an approximation based on max-norm, a technique that more directly minimizes the terms that appear in error bounds for approximate MDP algorithms. We provide experimental results on problems with over 10^40 states, demonstrating a promising indication of the scalability of our approach, and compare our algorithm to an existing state-of-the-art approach, showing, in some problems, exponential gains in computation time.", "histories": [["v1", "Thu, 9 Jun 2011 13:58:37 GMT  (892kb)", "http://arxiv.org/abs/1106.1822v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["c guestrin", "d koller", "r parr", "s venkataraman"], "accepted": false, "id": "1106.1822"}, "pdf": {"name": "1106.1822.pdf", "metadata": {"source": "CRF", "title": "Efficient Solution Algorithms for Factored MDPs", "authors": ["Carlos Guestrin", "Daphne Koller", "Ronald Parr", "Shobha Venkataraman"], "emails": ["guestrin@cs.stanford.edu", "koller@cs.stanford.edu", "parr@cs.duke.edu", "shobha@cs.cmu.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. In fact, it is so that they are able to survive themselves, and that they are able to survive themselves. In fact, it is so that they are able to survive themselves, and that they are able to survive themselves. In fact, it is so that they are able to survive themselves, and that they are able to survive themselves. In fact, it is so that they are able to survive themselves."}, {"heading": "2. Factored Markov Decision Processes", "text": "A Markov Decision Process (MDP) is a mathematical framework for sequential decision problems in stochastic domains. It thus provides a basic semantics for the task of planning under uncertainty. We begin with a concise overview of the MDP framework and then describe the presentation of factored MDPs."}, {"heading": "2.1 Markov Decision Processes", "text": "We briefly review the MDP framework by referring the reader to the books of Bertsekas and Tsitsiklis, who each form a network for this network. (1996) or Puterman (1994) for an in-depth review. AMarkov Decision Process (MDP) is defined as a 4-tuple (X, A, R, P) where: X is a finite set of | X | = N states; A is a finite set of actions; R is a reward function R: X \u00d7 A 7 \u2192 R, so that R (a) represents the reward achieved by the agent in state x after taking action; and P is a Markovian transition model in which P (x, a) represents the probability of going from state x to state x to state x. We assume that the rewards are limited, that is, that there are Rmax such that Rmax machines exist."}, {"heading": "2.2 Factored MDPs", "text": "The idea of representing a large MDP with a factored model is first proposed by Xi and al. (1995) In a factored MDP, the set of states is described using a series of random variables. (Generally, we use uppercase letters (e.g., X) to name random variables (e.g., x) to name their values. We use Boldface to denote the vectors of variables (e.g., X) or their values (e.g., X). For an instance y Dom (Y) and a subset of these variables Z Y, we use y [Z] to define the value of the variables."}, {"heading": "3. Approximate Solution Algorithms", "text": "The three most commonly used techniques are value titeration, policy iteration, and linear programming. A key component in all three algorithms is the calculation of value functions as defined in Section 2.1. Remember that a value function defines a value for each state x in the state space. Unfortunately, in the case of factored MDPs, the state space is exponential in the number of variables in the domain. In the SysAdmin problem, all can be implemented as a series of simple algebraic steps. In this case, all three can be implemented very efficiently. Unfortunately, in the case of factored MDPs, the state space is exponential in the number of variables in the domain. In the SysAdmin problem, for example, the state x of the system is an assignment that describes which machines are working or failing; that is, a state x is an assignment to each random variable Xi."}, {"heading": "3.1 Linear Value Functions", "text": "A very popular choice for the approximation of value functions is the use of linear regression, as first proposed by Bellman et al. (1963). Here, we define our space of permissible value functions V-H-RN through a series of basic functions: Definition 3.1 A linear value function through a series of basic functions H = {h1,.., hk} is a function V that can be written as V (x) = \u2211 k matrix H (x) for some coefficients w = (w1,., wk) \u2032. We can now define H as the linear subspace of the RN spanned by the basic functions. It is useful to define an N-k matrix H whose columns are the basic functions considered as vectors. In a more compact notation, our approximate value function is then by Hw.The expressive force of this linear representation is equivalent to the programming of a uniform network, which we consider the task."}, {"heading": "3.2 Policy Iteration", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 The Exact Algorithm", "text": "Starting with some initial policies \u03c0 (0), each iteration consists of two phases. Value determination for a policy \u03c0 (t) calculates the value function V\u03c0 (t) by finding the fixed point of the equation T\u03c0 (t) V\u03c0 (t) = V\u03c0 (t), i.e. the unique solution for the set of linear equations: V\u03c0 (t) (x) = R (x, \u03c0 (t) (x)) + \u03b3 \u2211 x \u2032 P (x \u2032 | x, \u03c0 (t) (x)) V\u03c0 (t \u2032), \u0445x.The step of policy improvement defines the next policy ashion (t + 1) = greedy (V\u03c0 (t)). It can be shown that this process converges to the optimal policy (Bertsekas & Tsisiti, 1996)."}, {"heading": "3.2.2 Approximate Policy Iteration", "text": "The steps in the political iteration algorithm require manipulation of both the value functions and the guidelines, both of which often cannot be explicitly represented in large MDPs. To define a version of the political iteration algorithm that uses approximate value functions, we use the following basic idea: We limit the algorithm to using only value functions within the provided H; whenever the algorithm takes a step that leads to a value function V that is outside this space, we project the result back into space by finding the value function within the space closest to V. More precisely: Definition 3.2 A projection operator, which is an image of what leads to an image of it."}, {"heading": "3.2.3 Max-norm Projection", "text": "An approach along the lines described above has been used in various papers, with several current theoretical and algorithmic results (Schweitzer & Seidmann, 1985; Tsitsiklis & Van Roy, 1996b; Van Roy, 1998; Koller & Parr, 1999, 2000).However, these approaches suffer from what we might call \"normative incompatibility\"; when calculating the projection, they use the standard Euclidean projection operator with respect to the L2 standard or a weighted L2 standard. On the other hand, most convergence and error analyses for MDP algorithms require the maximum standard (L), which has made it difficult to provide error guarantees. We can tie the projection operator more closely to the error limits by using a projection operator in the L standard. The problem of minimizing the L standard has been examined in the optimization literature as the problem of determining the Chebyshev solution."}, {"heading": "3.2.4 Error Analysis", "text": "We motivate our use of the max-normative projection within the approximate policy iteration algorithm through its compatibility with standard error analysis techniques for MDP algorithms. We now provide a careful analysis of the effects of the L errors introduced by the projection step. Analysis provides motivation for the use of a projection step that directly minimizes this quantity. However, we acknowledge that the main effect of this analysis is motivating. In practice, we can give no guarantees that an L error will outperform other methods. Our goal is to analyze the approximate political iteration in terms of the amount of error introduced by the projection operation in each step. If the error is zero, then we perform accurate value calculation, and no error should occur. If the error is small, we should get an approximation that is accurate. This result follows from the analysis below."}, {"heading": "3.3 Approximate Linear Programming", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1 The Exact Algorithm", "text": "Linear programming offers an alternative method of solving MDPs. It formulates the problem of finding a value function as a linear program (LP). Here are the LP variables V1,..., VN, where Vi V (xi) represents: the value that begins with the ith state of the system. LP is given by variables: V1,...., VN; Minimize: \u2211 xi \u03b1 (xi) Vi; Subject: Vi \u2265 [R (xi, a) + \u03b3 j P (xj | xi, a) Vj] xi-X, a-A, (7), where the state relevance weighted \u03b1 is positive. Note that in this case the obtained solution for each positive weight vector is the same. It is interesting to note that steps of the simplex algorithm correspond to policy changes in individual states, while steps of policy iteration may involve policy changes in several states. In practice, the policy iteration tends to be faster than the pulse approach (1994)."}, {"heading": "3.3.2 Approximate Linear Program", "text": "The approximate formulation for the LP approach, first proposed by Schweitzer and Seidmann (1985), limits the space of permissible value functions to the linear space spanned by our basic functions. In this approximate formulation, the variables w1,..., wk: the weights for our basic functions. However, the LP is minimized by: Variables: w1,.., wk; x x x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x), x), x (x), x (x), x (x), x (x), x, x, x, x, x (x), x, x, x, x, x), x, x, x, x, x)."}, {"heading": "4. Factored Value Functions", "text": "The linear value function approach and the algorithms described in Section 3 apply to any choice of basic functions. In the context of factored MDPs, Koller and Parr (1999) propose a certain type of basic function that is particularly compatible with the structure of a factored MDP. They suggest that although the value function is typically unstructured, there are many cases in which it could be \"close\" to the structure, that is, it could be well approximated with a linear combination of functions, each of which refers to only a small number of variables. Specifically, we define: Definition 4.1 A factored (linear) value function is a linear function based on the basic theorem h1,... hk, where the scope of each hi is limited to a subset of variables. Ci.Value functions of this type have a long history in the field of multiattributed utility theory (Keeney & Raiffa, 1976)."}, {"heading": "4.1 One-step Lookahead", "text": "A key step in all of our algorithms is the calculation of the step lookahead value of an action (x), which is necessary, for example, when calculating the greedy policy as in Equation (1). Consider the calculation of a Q function, Qa (x), which represents the expected value that the agent receives after taking a measure at the present time and receiving a long-term value. This Q function can be briefly expressed by: Qa (x) = R (x, a) x (P (x), a) V (x). (9) That is, Qa (x) is given by the current reward plus the discounted future value. With this notation, we can express the greedy policy as follows: Greedy (V) = maxaQa (x).Recall what we estimate the long-term value of our policy using a number of functions: V (x)."}, {"heading": "4.2 Representing Exponentially Many Constraints", "text": "As seen in Section 3, both of our approximation algorithms require the solution of linear programs: the LP in (5) for approximate policy iteration and the LP in (8) for approximate policy iteration. These LPs have some common features: They have a small number of free variables (for basic k functions, there are k + 1 free variables in approximate policy iteration and k in approximate linear programming), but the number of constraints is still exponential in the number of state variables. However, these LP constraints have another very useful feature: the functionalities in the constraints have a limited scope. This key observation allows us to present these constraints very compactly. First, it should be noted that the constraints in the linear programs are all of the form: the LP constraints in the form (x) of the (-11) of the Linix (-11)."}, {"heading": "4.2.1 Maximizing Over the State Space", "text": "The key to the calculation in our algorithms is the representation of a non-linear constraint of form in Equation (12) and Space (12), which efficiently depends on a small set of linear constraints. Before we present this construction, let us first consider a simpler problem: in view of some fixed weights wi, we would like to consider maximizing: \"We have an analog construction that we present in this section by changing the characters of ci (x) and b (x).\" The approximate linear programming of (8) can also be formulated in this form, as we do in Section 5.that the difference between the two formats (x) and b (x) is. We cannot explicitly show the explicit linear programming of states and differences in this form."}, {"heading": "4.2.2 Factored LP", "text": "In this section, we present the centerpiece of our planning algorithms: a new, generic approach to the compact representation of exponentially large groups of LP constraints in problems with factored structure - those where the functions in the constraints can be decomposed as the sum of limited functions. Consider our original problem of compactly representing the non-linear constraints in Equation (12) without creating a constraint for each state as in Equation (11). The new, important finding is that these non-linear constraints can be implemented with a construction that follows the structure of variable elimination in cost."}, {"heading": "4.2.3 Factored Max-norm Projection", "text": "We can now use our method to condense the exponential number of constraints in Equation (11) to calculate efficient max-standard projections, as in Equation (4): w * argmin w \u2212 Cw \u2212 b \u00b2. The max-standard projection is calculated by the linear program in (5). There are two sets of constraints in this equation (11), which we have just mentioned in the previous section. So, if each of the k \u2212 basic functions in C is a constraint function, and the target function b is the sum of constraint functions, then we can use our factored LP technique to represent the constraints in Equation (11), which are exponential factors in C, and the target function b is the sum of the constraint functions, then we can represent the factorial constraints in the LP standard configuration."}, {"heading": "5. Approximate Linear Programming", "text": "Let's start with the simplest of our approximate MDP solution algorithms, based on the approximate linear programming formulation in Section 3.3. With the basic operations described in Section 4, we can formulate an algorithm that is both simple and efficient."}, {"heading": "5.1 The Algorithm", "text": "As discussed in Section 3.3, the approximate linear program formulation is based on the linear programming approach to solving the MDPs presented in Section 3.3. However, in this approximate version, we limit the space of the value functions to the linear space defined by our basic functions. More specifically, in this approximate LP formulation, however, the variables are limited to w1,., wk - the weights for our basic functions. The LP is given by: Variables: w1,., wk; Minimize: x \u03b1 (x). The subject: i wi hi hi (x)."}, {"heading": "5.2 An Example", "text": "We now present a complete example of the operations required by the approximate LP algorithm = hello = hello = hello = hello = hello = hello = hello = constant MDP shown in Figure 2 (a). Our representation follows four steps: \"Problem representation, basic function selection, rear projections and LP construction.Problem representation: First, we need to fully specify the factored MDP model for the problem.\" The structure of the DBN is shown in Figure 2 (b). This structure is maintained for all action options. Next, we need to define the transition probabilities for each action. There are 5 actions in this problem: do nothing or restart one of the 4 machines in the network. CPDs for these actions are shown in Figure 2 (c). Finally, we need to define the reward function as the sum of 4 local reward functions, one for each machine, so that there is a reward when the machine works. Concrete: Ri = 0, Xi = the function breaks."}, {"heading": "6. Approximate Policy Iteration with Max-norm Projection", "text": "The factored approximate linear programming approach described in the previous section is both elegant and easy to implement. However, we generally cannot give strong guarantees for the error achieved. An alternative is to use the approximate policy iteration described in Section 3.2, which sets some limits to the error. However, as we will see, this algorithm is much more complicated and requires additional constraints on the factorised MDP. Specifically, an approximate policy iteration requires a policy representation for each iteration. In order to obtain a compact policy representation, we must make an additional assumption: each measure affects only a small number of government variables. First, we formulate this assumption, then we show how to obtain a compact representation of greedy policies in relation to a factored value function under this assumption. Finally, we describe our factored approximate policy iteration algorithm using max-norm projections."}, {"heading": "6.1 Default Action Model", "text": "In Section 2.2, we introduced the Factored MDP model, in which each action is linked to its own Factored Transition model, which is presented as a DBN and its own Factored Reward function. However, different actions often have very similar transition dynamics, which differ only in their effects on a few small groups of variables. In particular, in many cases, a variable has a standard development model that only changes when an action directly affects it (Boutilier et al., 2000). This type of structure is useful for compact representation measures, a property that is important in our approximate iteration algorithms. In this section of the paper, we limit attention to factored MDPs defined using a standard transition model."}, {"heading": "6.2 Computing Greedy Policies", "text": "The policy improvement step calculates the greedy policy in relation to a value function V (t \u2212 1): \u03c0 (t) = a small insight (V (t \u2212 1).Let us remember that our value function can have the linear form Hw. As we have described in Section 4.1, the greedy policy for this kind of value function is due to: Greedy (Hw) (x) = argmax a Qa (x), where every Qa action is due to: Qa (x, a) + \u2211 iwi g a (x).If we try to portray this policy naively, we are again confronted with the problem of exponentially large state spaces. Fortunately, as von Koller and Parr have shown, greedy policy relative to a factored value function has the form of a decision list. More precisely, policy can be written in the form < t1 >, a2 >."}, {"heading": "6.3 Value Determination", "text": "In the approximate value determination step calculate our algorithms: w (t) = argmin w = argmin w, so we can fall back on our system (t). < p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p (t) p) p (t) p (t) p) p (t) p (t) p) p (t) p (t) p) p (t) p (t) p) p (t) p (t) p) p (t) p) p (t) p) p (t) p) p (t) p (t) p) p (t) p) p (t) p) p (t) p) p (t) p) p (t) p) p (t) p) p (t) p) p (t) p) p (t) p) p) p (t) p) p (t) p) p) p (t) p) p) p (t) p) p) p (t) p) p (t) p) p) p (t) p) p (t) p) p (t) p) p (t) p) p (t) p) p (t) p) p (t) p) p (t) p (t) p) p (t) p) p (t) p) p (t) p) p) p (t) p (t) p (t) p) p (t) p (t) p) p (t) p) p) p (t) p (t) p) p (t) p) p (t) p) p) p (t) p (t) p (t) p) p (t) p) p (t) p) p) p) p (t) p (t) p (t) p (t) p) p (t) p) p (t) p (t) p) p (t) p) p (t) p (t) p) p)"}, {"heading": "6.4 Comparisons", "text": "D eeisrcnlrrrrrrrrrrr rf\u00fc ide eeisrrrrrrrrrcehncnlrrrrrrrrrrrrrso ni rde eeisrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "7. Computing Bounds on Policy Quality", "text": "We have presented two algorithms for calculating approximate solutions for the Factored MDPs. (All of these algorithms generate linear value functions from Hw, where they represent the resulting base function. (All of these algorithms generate linear value functions from Hw, where they represent the resulting base function.) In practice, the agent will define his behavior by acting according to the greedy policy. (Another possible procedure is how this policy compares the actual optimal policy.) That is, how the actual value of the policy is compared with the loss of the ability to act according to the greedy policy. (Bellman points out some priority limits to the quality of the policy.) Another possible procedure is the calculation of an ex post facto limit. That is, given our resulting weights w, we calculate the loss of the ability to act according to the greedy policy, rather than using the optimal policy. This can be achieved by defining the error analysis of Williams and Bellman (The Bellman 1993)."}, {"heading": "8. Exploiting Context-specific Structure", "text": "So far, we have presented a number of algorithms that take advantage of the additive structure in the reward and base functions and the sparse connectivity in the DBN that represent the transition model. However, there is another important type of structure that should also be used for efficient decision-making: context-specific independence (CSI). For example, if we consider a player responsible for building and maintaining a house, if the painting work can only be completed after the installation of the plumbing and electrical wiring, then the likelihood that the painting will be done is exponentially high in the number of variables in the scope of the function and ignores the context-specific structure inherent in the problem definition. Boutilier et al. (Boutilier et al. (1999, Boutilier et al., 1997, Departing DP; Boutilier specific in this phase; Boutilier specific in this phase; Boutilier is frequently developed)."}, {"heading": "8.1 Factored MDPs with Context-specific and Additive Structure", "text": "There are several representations for context-specific functions, the most common of which are decision trees (Boutilier et al., 1995), algebraic decision diagrams (ADDs) (Hoey, St-Aubin, Hu, & Boutilier, 1999), and rules (Zhang & Poole, 1999). We opt for rules as our basic representation, for two main reasons. Firstly, rule-based representation allows a relatively simple variable elimination algorithm, which is a key operation within our framework. Secondly, rules are not required to be mutually exclusive and comprehensive, a requirement that can be restrictive when we want to use additive independence, where functions can be presented as a linear combination of a set of non-mutually exclusive functions. We start by describing rule-based representation (along the lines of Zhang and Poole's presentation) for the true transition model, in particular the CPDs of our DBN model."}, {"heading": "8.2 Adding, Multiplying and Maximizing Consistent Rules", "text": "In our tabular algorithms, we have relied on standard sums and product operators applied to tables. To use CSI = > consistent procedures, we need to redefine these standard operations. < c: v1 > and sc2 = < c: v2 > are two rules with context. Let's define the rule product as \u03c12 = < c: v1 \u00b7 v2 >, and the rule sum as \u03c12 = < c: v1 + v2 >. This definition is limited to rules with the same context. We will address this problem in a moment. We will introduce an additional operation that maximizes a variable from a set of rules that otherwise have a common context: Definition 8.8 Let Y be variable with the same context."}, {"heading": "In these rules, the context c1 of \u03c11 is a \u2227 b, and the context c2 of \u03c12 is a \u2227 \u00acc \u2227 d.", "text": "The rules \u03c11 and \u03c12 are consistent, so we have to divide them to perform the addition operation: split (\u03c11 6 c2) = < a-b-c-5 >, < a-b-c-d-5 >, < a-b-c-d-5 >, < a-c-d-5 >. Likewise split (\u04212 6 c1) = {< a-b-c-c-d-3 >, < a-b-c-c-c-d-3 >. The result of the addition of rules \u04211 and \u04212 is < a-b-c-c-c-c-5 >, < a-c-d-d-5 >, < a-b-c-d-d-8 >, < a-b-c-c-c-d-d-3 >."}, {"heading": "8.3 Rule-based One-step Lookahead", "text": "Using this compact rule-based representation (hj) i, we are able to efficiently calculate a one-step suggestion plan for models with significant context-specific or additive independence (x x).As in Section 4.1 for the tabular case, we can represent the rule-based Qa function as the sum of the reward function and the discounted expected value of the next state. Again, due to our linear approach to the value function, the expectation concept is presented as a linear combination of the reprojections of our functions. To exploit CSI, we represent the rewards and base functions as rule-based functions. To represent Qa as a rule-based function, it is sufficient to show how we represent the reprojection gj as a rule-based function. Each hj is a rule-based function that can be written as hj (x)."}, {"heading": "8.4 Rule-based Maximization Over the State Space", "text": "The second step, which is necessary to solve the problem, is that most people who are able to survive are able to survive themselves, and that they are able to survive themselves. \"The second step, which is able to survive themselves, is that most of them are able to survive themselves, and that they are able to survive themselves.\" The second step, which is going in the right direction, is that they are able to survive themselves. \""}, {"heading": "8.5 Rule-based Factored LP", "text": "In section 4.2.2 we have shown that the LPs we apply in our algorithms have exponentially many limitations of form (< p >) < p > p > p > p > p > p (x) p > p (x) -p (x) -p (x) -p (x) -p (x) -p (x) -p (x) -p (x) p (x) p (x) p (x) p > p (x) p (e) p (e) p (e) p (x) p (x) p (x) p (x) p (x) p (x) p (e) p (e) p (c) p (e) p (e) p \"e (e) p\" e e (e) p \"e (e) p\" e (e) p \"e (e) p\" e (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p \"e) p (e) p (e) p (e) p (e) p\" e) p (e) p (p) p (e) p \"e) p (e) p (e) p (e) p\" e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p \"e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e"}, {"heading": "9. Experimental Results", "text": "As Herbert Simon (1981) argues in \"Architecture of Complexity,\" many complex systems have a \"near-decomposable hierarchical structure,\" with subsystems interacting only weakly with each other. To evaluate our algorithm, we selected problems that we believe to have this kind of structure. In this section, we conduct various experiments to explore the performance of our algorithms. First, we compare our factored approximate linear programming (LP) and approximate policy iteration (PI) algorithms, and we also compare the L2 projection algorithm of Koller and Parr (2000). Our second assessment compares a tabular implementation with a rules-based implementation that CSI can exploit (finally, we present Bouisal's approach)."}, {"heading": "9.1 Approximate LP and Approximate PI", "text": "In fact, the fact is that most of them will be able to show themselves, that they are able to assert themselves, that they are able to assert themselves, and that they are able to assert themselves, and that they are able to assert themselves."}, {"heading": "9.2 Comparing Table-based and Rule-based Implementations", "text": "\"Our next evaluation compares a tabular representation that uses only additive independence to the rules-based representation presented in Section 8, which can take advantage of both additive and context-specific independence. For these experiments, we implement our factored approximate linear programming algorithms with potentially faster and rule-based representations in C + +, using a more complex extension of the SysAdmin problem than the LP solver. This problem, called the Process SysAdmin problem, contains three state variables for each machine i: Loadi, Statusi, and Selectori. Each computer executes processes and receives rewards when the processes end, which are represented by the Loadi variable, the values in {Idle, Loaded, Success}, and the computer receives a reward for the mapping."}, {"heading": "9.3 Comparison to Apricodd", "text": "In fact, it is the case that most people who are able to survive themselves are able to survive themselves by putting themselves in a position to survive themselves. (...) In fact, it is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is not the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) It is the case that they are able to survive themselves. (...) (...) (...) (...) (...) (...) (...). (...) (...) (...) ()."}, {"heading": "10. Related Work", "text": "The work most closely related to ours is a line of research that began with the work of Boutilier et al. (1995). We will deal with this comparison separately below, but we will begin this section with some more comprehensive background references."}, {"heading": "10.1 Approximate MDP Solutions", "text": "The field of MDPs, as it is popularly known, was formalized by Bellman (1957) in the 1950s; the importance of value function approximation was recognized early by Bellman himself (1963); in the early 1990s, the MDP approach was recognized by AI researchers as a formal framework that could be used to address the problem of planning under uncertainty (Dean, Kaelbling, Kirman, & Nicholson, 1993); within the AI community, the value function approximation developed at the same time as the concept of value function representation for Markov chains; Sutton's groundbreaking paper on time difference learning (1988), which focused on the use of value functions to predict, but not on planning, was based on a very general representation of value function and established the link to general function approximation structures such as neural networks; however, the stability of this combination was not directly addressed at that time; several important developments gave the AI community deeper insights into the relationship between dynamic approximation and radio networks."}, {"heading": "10.2 Factored Approaches", "text": "This year, it has come to the point where it only takes one year for it to come to a conclusion."}, {"heading": "11. Conclusions", "text": "This year it is so far that it only takes one year to get there, to get there."}, {"heading": "Acknowledgements", "text": "We thank Craig Boutilier, Dirk Ormoneit and Uri Lerner for many useful discussions and the anonymous critics for their detailed and thorough comments. We also thank Jesse Hoey, Robert St-Aubin, Alan Hu and Craig Boutilier for spreading their algorithm and for their very useful help in using Apricodd and selecting its parameters. This work was supported by the DoD MURI program, managed by the Office of Naval Research under Grant N00014-00-1-0637, by the Air Force Contract F30602-00-2-0598 under DARPA's TASK program, and by the Sloan Foundation. The first author was also supported by a Siebel Fellowship."}, {"heading": "Appendix A. Proofs", "text": "There is a constellation of weights - the total zero setting - that gives a limited maximum projection error \u03b2P for each policy (\u03b2P \u2264 Rmax). \u2212 Our maximum projection operator selects the set of weights that minimizes the projection error \u03b2 (t) for each policy \u03c0 (t). \u2212 Thus, the projection error \u03b2 (t) must be at least as low as that of the null weights \u03b2P (which is limited). \u2212 The error remains limited for all iterations. \u2212 2 Proof Theorem 3.5First, we must define our alignment with the Vp (t). \u2212 The constellation of Xp (t) \u2212 Hw (t). \u2212 The constellation of Xp (t). \u2212 Hw (t). \u2212 Hw (t). \u2212 Hw (t)."}], "references": [{"title": "Complexity of finding embeddings in a K-tree", "author": ["S. Arnborg", "D.G. Corneil", "A. Proskurowski"], "venue": "SIAM Journal of Algebraic and Discrete Methods,", "citeRegEx": "Arnborg et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Arnborg et al\\.", "year": 1987}, {"title": "A sufficiently fast algorithm for finding close to optimal clique trees", "author": ["A. Becker", "D. Geiger"], "venue": "Artificial Intelligence,", "citeRegEx": "Becker and Geiger,? \\Q2001\\E", "shortCiteRegEx": "Becker and Geiger", "year": 2001}, {"title": "Polynomial approximation \u2013 a new computational technique in dynamic programming", "author": ["R. Bellman", "R. Kalaba", "B. Kotkin"], "venue": "Math. Comp.,", "citeRegEx": "Bellman et al\\.,? \\Q1963\\E", "shortCiteRegEx": "Bellman et al\\.", "year": 1963}, {"title": "Dynamic Programming", "author": ["R.E. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Nonserial Dynamic Programming", "author": ["U. Bertele", "F. Brioschi"], "venue": null, "citeRegEx": "Bertele and Brioschi,? \\Q1972\\E", "shortCiteRegEx": "Bertele and Brioschi", "year": 1972}, {"title": "Decision theoretic planning: Structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Approximating value trees in structured dynamic programming", "author": ["C. Boutilier", "R. Dearden"], "venue": "In Proc. ICML,", "citeRegEx": "Boutilier and Dearden,? \\Q1996\\E", "shortCiteRegEx": "Boutilier and Dearden", "year": 1996}, {"title": "Exploiting structure in policy construction", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "In Proc. IJCAI,", "citeRegEx": "Boutilier et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1995}, {"title": "Stochastic dynamic programming with factored representations", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "Artificial Intelligence,", "citeRegEx": "Boutilier et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 2000}, {"title": "Approximation Theory (2nd edition)", "author": ["E.W. Cheney"], "venue": "Chelsea Publishing Co.,", "citeRegEx": "Cheney,? \\Q1982\\E", "shortCiteRegEx": "Cheney", "year": 1982}, {"title": "The linear programming approach to approximate dynamic programming", "author": ["D. de Farias", "B. Van Roy"], "venue": "Submitted to Operations Research", "citeRegEx": "Farias and Roy,? \\Q2001\\E", "shortCiteRegEx": "Farias and Roy", "year": 2001}, {"title": "On constraint sampling for the linear programming approach to approximate dynamic programming", "author": ["D. de Farias", "B. Van Roy"], "venue": "To appear in Mathematics of Operations Research", "citeRegEx": "Farias and Roy,? \\Q2001\\E", "shortCiteRegEx": "Farias and Roy", "year": 2001}, {"title": "Planning with deadlines in stochastic domains", "author": ["T. Dean", "L.P. Kaelbling", "J. Kirman", "A. Nicholson"], "venue": "In Proceedings of the Eleventh National Conference on Artificial Intelligence", "citeRegEx": "Dean et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1993}, {"title": "A model for reasoning about persistence and causation", "author": ["T. Dean", "K. Kanazawa"], "venue": "Computational Intelligence,", "citeRegEx": "Dean and Kanazawa,? \\Q1989\\E", "shortCiteRegEx": "Dean and Kanazawa", "year": 1989}, {"title": "Model minimization in Markov decision processes", "author": ["T. Dean", "R. Givan"], "venue": "In Proceedings of the Fourteenth National Conference on Artificial Intelligence", "citeRegEx": "Dean and Givan,? \\Q1997\\E", "shortCiteRegEx": "Dean and Givan", "year": 1997}, {"title": "Abstraction and approximate decision theoretic planning", "author": ["R. Dearden", "C. Boutilier"], "venue": "Artificial Intelligence,", "citeRegEx": "Dearden and Boutilier,? \\Q1997\\E", "shortCiteRegEx": "Dearden and Boutilier", "year": 1997}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artificial Intelligence,", "citeRegEx": "Dechter,? \\Q1999\\E", "shortCiteRegEx": "Dechter", "year": 1999}, {"title": "Stable function approximation in dynamic programming", "author": ["G. Gordon"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning,", "citeRegEx": "Gordon,? \\Q1995\\E", "shortCiteRegEx": "Gordon", "year": 1995}, {"title": "Max-norm projections for factored MDPs", "author": ["C.E. Guestrin", "D. Koller", "R. Parr"], "venue": "In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Guestrin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2001}, {"title": "Multiagent planning with factored MDPs", "author": ["C.E. Guestrin", "D. Koller", "R. Parr"], "venue": "In 14th Neural Information Processing Systems", "citeRegEx": "Guestrin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2001}, {"title": "Solving factored POMDPs with linear value functions", "author": ["C.E. Guestrin", "D. Koller", "R. Parr"], "venue": "In Seventeenth International Joint Conference on Artificial Intelligence (IJCAI-01) workshop on Planning under Uncertainty and Incomplete Information,", "citeRegEx": "Guestrin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2001}, {"title": "Context specific multiagent coordination and planning with factored MDPs", "author": ["C.E. Guestrin", "S. Venkataraman", "D. Koller"], "venue": "In The Eighteenth National Conference on Artificial Intelligence", "citeRegEx": "Guestrin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2002}, {"title": "SPUDD: Stochastic planning using decision diagrams", "author": ["J. Hoey", "R. St-Aubin", "A. Hu", "C. Boutilier"], "venue": "In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hoey et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hoey et al\\.", "year": 1999}, {"title": "Stochastic planning using decision diagrams \u2013 C implementation. http://www.cs.ubc.ca/spider/staubin/Spudd", "author": ["J. Hoey", "R. St-Aubin", "A. Hu", "C. Boutilier"], "venue": null, "citeRegEx": "Hoey et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hoey et al\\.", "year": 2002}, {"title": "Influence diagrams", "author": ["R.A. Howard", "J.E. Matheson"], "venue": "Readings on the Principles and Applications of Decision Analysis,", "citeRegEx": "Howard and Matheson,? \\Q1984\\E", "shortCiteRegEx": "Howard and Matheson", "year": 1984}, {"title": "Decisions with Multiple Objectives: Preferences and Value Tradeoffs", "author": ["R.L. Keeney", "H. Raiffa"], "venue": null, "citeRegEx": "Keeney and Raiffa,? \\Q1976\\E", "shortCiteRegEx": "Keeney and Raiffa", "year": 1976}, {"title": "Solving factored Mdps using non-homogeneous partitioning", "author": ["Kim", "K.-E", "T. Dean"], "venue": "In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Kim et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2001}, {"title": "Triangulation of graphs \u2013 algorithms giving small total state space", "author": ["U. Kjaerulff"], "venue": "Tech. rep. TR R 90-09,", "citeRegEx": "Kjaerulff,? \\Q1990\\E", "shortCiteRegEx": "Kjaerulff", "year": 1990}, {"title": "Computing factored value functions for policies in structured MDPs", "author": ["D. Koller", "R. Parr"], "venue": "In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Koller and Parr,? \\Q1999\\E", "shortCiteRegEx": "Koller and Parr", "year": 1999}, {"title": "Policy iteration for factored MDPs", "author": ["D. Koller", "R. Parr"], "venue": "In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Koller and Parr,? \\Q2000\\E", "shortCiteRegEx": "Koller and Parr", "year": 2000}, {"title": "Solving very large weakly-coupled Markov decision processes", "author": ["N. Meuleau", "M. Hauskrecht", "K. Kim", "L. Peshkin", "L. Kaelbling", "T. Dean", "C. Boutilier"], "venue": "In Proceedings of the 15th National Conference on Artificial Intelligence,", "citeRegEx": "Meuleau et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 1998}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Finding approximate separators and computing tree-width quickly", "author": ["B. Reed"], "venue": "In 24th Annual Symposium on Theory of Computing,", "citeRegEx": "Reed,? \\Q1992\\E", "shortCiteRegEx": "Reed", "year": 1992}, {"title": "Direct value-approximation for factored MDPs", "author": ["D. Schuurmans", "R. Patrascu"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Schuurmans and Patrascu,? \\Q2001\\E", "shortCiteRegEx": "Schuurmans and Patrascu", "year": 2001}, {"title": "Generalized polynomial approximations in Markovian decision processes", "author": ["P. Schweitzer", "A. Seidmann"], "venue": "Journal of Mathematical Analysis and Applications, 110,", "citeRegEx": "Schweitzer and Seidmann,? \\Q1985\\E", "shortCiteRegEx": "Schweitzer and Seidmann", "year": 1985}, {"title": "The Sciences of the Artificial (second edition)", "author": ["H.A. Simon"], "venue": null, "citeRegEx": "Simon,? \\Q1981\\E", "shortCiteRegEx": "Simon", "year": 1981}, {"title": "How to dynamically merge Markov decision processes", "author": ["S. Singh", "D. Cohn"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Singh and Cohn,? \\Q1998\\E", "shortCiteRegEx": "Singh and Cohn", "year": 1998}, {"title": "APRICODD: Approximate policy construction using decision diagrams", "author": ["R. St-Aubin", "J. Hoey", "C. Boutilier"], "venue": "In Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference,", "citeRegEx": "St.Aubin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "St.Aubin et al\\.", "year": 2001}, {"title": "Note on Jordan elimination, linear programming and Tchebycheff approximation", "author": ["E. Stiefel"], "venue": "Numerische Mathematik,", "citeRegEx": "Stiefel,? \\Q1960\\E", "shortCiteRegEx": "Stiefel", "year": 1960}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Scaling up average reward reinforcmeent learning by approximating the domain models and the value function", "author": ["P. Tadepalli", "D. Ok"], "venue": "In Proceedings of the Thirteenth International Conference on Machine Learning,", "citeRegEx": "Tadepalli and Ok,? \\Q1996\\E", "shortCiteRegEx": "Tadepalli and Ok", "year": 1996}, {"title": "Dynamic programming and influence diagrams", "author": ["J.A. Tatman", "R.D. Shachter"], "venue": "IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "Tatman and Shachter,? \\Q1990\\E", "shortCiteRegEx": "Tatman and Shachter", "year": 1990}, {"title": "Feature-based methods for large scale dynamic programming", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "Machine Learning,", "citeRegEx": "Tsitsiklis and Roy,? \\Q1996\\E", "shortCiteRegEx": "Tsitsiklis and Roy", "year": 1996}, {"title": "An analysis of temporal-difference learning with function approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "Technical report LIDS-P-2322,", "citeRegEx": "Tsitsiklis and Roy,? \\Q1996\\E", "shortCiteRegEx": "Tsitsiklis and Roy", "year": 1996}, {"title": "Learning and Value Function Approximation in Complex Decision Processes", "author": ["B. Van Roy"], "venue": "Ph.D. thesis,", "citeRegEx": "Roy,? \\Q1998\\E", "shortCiteRegEx": "Roy", "year": 1998}, {"title": "Tight performance bounds on greedy policies based on imperfect value functions", "author": ["R.J. Williams", "L.C.I. Baird"], "venue": null, "citeRegEx": "Williams and Baird,? \\Q1993\\E", "shortCiteRegEx": "Williams and Baird", "year": 1993}, {"title": "Generalized belief propagation", "author": ["J. Yedidia", "W. Freeman", "Y. Weiss"], "venue": "In Advances in Neural Information Processing Systems 13: Proceedings of the 2000 Conference,", "citeRegEx": "Yedidia et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2001}, {"title": "On the role of context-specific independence in probabilistic reasoning", "author": ["N. Zhang", "D. Poole"], "venue": "In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Zhang and Poole,? \\Q1999\\E", "shortCiteRegEx": "Zhang and Poole", "year": 1999}], "referenceMentions": [{"referenceID": 40, "context": "A common choice is the use of linear value functions as an approximation \u2014 value functions that are a linear combination of potentially non-linear basis functions (Bellman, Kalaba, & Kotkin, 1963; Sutton, 1988; Tsitsiklis & Van Roy, 1996b).", "startOffset": 163, "endOffset": 239}, {"referenceID": 8, "context": "The capability to exploit both types of structure distinguishes these algorithms differ from earlier approaches (Boutilier et al., 2000), which only exploit context-specific structure.", "startOffset": 112, "endOffset": 136}, {"referenceID": 4, "context": "Finally, we present experimental results comparing our approach to the work of Boutilier et al. (2000), illustrating some of the tradeoffs between the two methods.", "startOffset": 79, "endOffset": 103}, {"referenceID": 18, "context": "This paper is a greatly expanded version of work that was published before in Guestrin et al. (2001a), and some of the work presented in Guestrin et al.", "startOffset": 78, "endOffset": 102}, {"referenceID": 32, "context": "We briefly review the MDP framework, referring the reader to the books by Bertsekas and Tsitsiklis (1996) or Puterman (1994) for a more in-depth review.", "startOffset": 109, "endOffset": 125}, {"referenceID": 5, "context": "The idea of representing a large MDP using a factored model was first proposed by Boutilier et al. (1995). In a factored MDP, the set of states is described via a set of random variables X = {X1, .", "startOffset": 82, "endOffset": 106}, {"referenceID": 2, "context": "A very popular choice for approximating value functions is by using linear regression, as first proposed by Bellman et al. (1963). Here, we define our space of allowable value functions V \u2208 H \u2286 R via a set of basis functions:", "startOffset": 108, "endOffset": 130}, {"referenceID": 17, "context": "Averagers (Gordon, 1995) are stable and non-expansive in L\u221e, but require that the mixture weights be determined a priori.", "startOffset": 10, "endOffset": 24}, {"referenceID": 9, "context": "an overdetermined linear system of equations (Cheney, 1982).", "startOffset": 45, "endOffset": 59}, {"referenceID": 39, "context": "We use an algorithm due to Stiefel (1960), that solves this problem by linear programming: Variables: w1, .", "startOffset": 27, "endOffset": 42}, {"referenceID": 32, "context": "In practice, policy iteration tends to be faster than the linear programming approach (Puterman, 1994).", "startOffset": 86, "endOffset": 102}, {"referenceID": 35, "context": "The approximate formulation for the LP approach, first proposed by Schweitzer and Seidmann (1985), restricts the space of allowable value functions to the linear space spanned by our basis functions.", "startOffset": 67, "endOffset": 98}, {"referenceID": 45, "context": "However, the recent work of de Farias and Van Roy (2001a) provides some analysis of the error relative to that of the best possible approximation in the subspace, and some guidance as to selecting \u03b1 so as to improve the quality of the approximation.", "startOffset": 46, "endOffset": 58}, {"referenceID": 28, "context": "In the context of factored MDPs, Koller and Parr (1999) suggest a particular type of basis function, that is particularly compatible with the structure of a factored MDP.", "startOffset": 33, "endOffset": 56}, {"referenceID": 25, "context": "Fortunately, as discussed by Koller and Parr (1999), this expectation operation, or backprojection, can be performed efficiently if the transition model and the value function are both factored appropriately.", "startOffset": 29, "endOffset": 52}, {"referenceID": 5, "context": "The linearity of the value function permits a linear decomposition, where each summand in the expectation can be viewed as an independent value function and updated in a manner similar to the value iteration procedure used by Boutilier et al. (2000). We now recap the construction briefly, by first defining:", "startOffset": 226, "endOffset": 250}, {"referenceID": 16, "context": "As shown by Dechter (1999), this cost is exponential", "startOffset": 12, "endOffset": 27}, {"referenceID": 27, "context": "These issues have been confronted successfully for a large variety of practical problems in the Bayesian network community, which has benefited from a large variety of good heuristics which have been developed for the variable elimination ordering problem (Bertele & Brioschi, 1972; Kjaerulff, 1990; Reed, 1992; Becker & Geiger, 2001).", "startOffset": 256, "endOffset": 334}, {"referenceID": 33, "context": "These issues have been confronted successfully for a large variety of practical problems in the Bayesian network community, which has benefited from a large variety of good heuristics which have been developed for the variable elimination ordering problem (Bertele & Brioschi, 1972; Kjaerulff, 1990; Reed, 1992; Becker & Geiger, 2001).", "startOffset": 256, "endOffset": 334}, {"referenceID": 8, "context": "In particular, in many cases a variable has a default evolution model, which only changes if an action affects it directly (Boutilier et al., 2000).", "startOffset": 123, "endOffset": 147}, {"referenceID": 28, "context": "Fortunately, as shown by Koller and Parr (2000), the greedy policy relative to a factored value function has the form of a decision list.", "startOffset": 25, "endOffset": 48}, {"referenceID": 28, "context": "It is instructive to compare our max-norm policy iteration algorithm to the L2-projection policy iteration algorithm of Koller and Parr (2000) in terms of computational costs per iteration and implementation complexity.", "startOffset": 120, "endOffset": 143}, {"referenceID": 3, "context": "This can be achieved by using the Bellman error analysis of Williams and Baird (1993). The Bellman error is defined as BellmanErr(V) = \u2016T \u2217V \u2212 V\u2016\u221e.", "startOffset": 34, "endOffset": 86}, {"referenceID": 3, "context": "Thus, we can use the Bellman error BellmanErr(H\u0175) to evaluate the quality of our resulting greedy policy. Note that computing the Bellman error involves a maximization over the state space. Thus, the complexity of this computation grows exponentially with the number of state variables. Koller and Parr (2000) suggested that structure in the factored MDP can be exploited to compute the Bellman error efficiently.", "startOffset": 21, "endOffset": 310}, {"referenceID": 7, "context": "(Boutilier et al., 1995; Dearden & Boutilier, 1997; Boutilier, Dean, & Hanks, 1999; Boutilier et al., 2000) have developed a set of algorithms which can exploit CSI in the transition and reward models to perform efficient (approximate) planning.", "startOffset": 0, "endOffset": 107}, {"referenceID": 8, "context": "(Boutilier et al., 1995; Dearden & Boutilier, 1997; Boutilier, Dean, & Hanks, 1999; Boutilier et al., 2000) have developed a set of algorithms which can exploit CSI in the transition and reward models to perform efficient (approximate) planning.", "startOffset": 0, "endOffset": 107}, {"referenceID": 7, "context": "The most common are decision trees (Boutilier et al., 1995), algebraic decision diagrams (ADDs) (Hoey, St-Aubin, Hu, & Boutilier, 1999), and rules (Zhang & Poole, 1999).", "startOffset": 35, "endOffset": 59}, {"referenceID": 5, "context": "The most common are decision trees (Boutilier et al., 1995), algebraic decision diagrams (ADDs) (Hoey, St-Aubin, Hu, & Boutilier, 1999), and rules (Zhang & Poole, 1999). We choose to use rules as our basic representation, for two main reasons. First, the rule-based representation allows a fairly simple algorithm for variable elimination, which is a key operation in our framework. Second, rules are not required to be mutually exclusive and exhaustive, a requirement that can be restrictive if we want to exploit additive independence, where functions can be represented as a linear combination of a set of non-mutually exclusive functions. We begin by describing the rule-based representation (along the lines of Zhang and Poole\u2019s presentation (1999)) for the probabilistic transition model, in particular, the CPDs of our DBN model.", "startOffset": 36, "endOffset": 754}, {"referenceID": 5, "context": "This notion of a rule-based function is related to the tree-structure functions used by Boutilier et al. (2000), but is substantially more general.", "startOffset": 88, "endOffset": 112}, {"referenceID": 16, "context": "1, which, in turn, was exponential only in the induced width of the cost network graph (Dechter, 1999).", "startOffset": 87, "endOffset": 102}, {"referenceID": 31, "context": "As argued by Herbert Simon (1981) in \u201cArchitecture of Complexity,\u201d many complex systems have a \u201cnearly decomposable, hierarchical structure,\u201d with the subsystems interacting only weakly between themselves.", "startOffset": 21, "endOffset": 34}, {"referenceID": 25, "context": "We also compare to the L2-projection algorithm of Koller and Parr (2000). Our second evaluation compares a table-based implementation to a rule-based implementation that can exploit CSI.", "startOffset": 50, "endOffset": 73}, {"referenceID": 5, "context": "Finally, we present comparisons between our approach and the algorithms of Boutilier et al. (2000).", "startOffset": 75, "endOffset": 99}, {"referenceID": 28, "context": "algorithm of Koller and Parr (2000). As we discussed in Section 6.", "startOffset": 13, "endOffset": 36}, {"referenceID": 5, "context": "The most closely related work to ours is a line of research that began with the work of Boutilier et al. (1995). In particular, the approximate Apricodd algorithm of Hoey et al.", "startOffset": 88, "endOffset": 112}, {"referenceID": 5, "context": "The most closely related work to ours is a line of research that began with the work of Boutilier et al. (1995). In particular, the approximate Apricodd algorithm of Hoey et al. (1999), which uses analytic decision diagrams (ADDs) to represent the value function is a strong alternative approach for solving factored MDPs.", "startOffset": 88, "endOffset": 185}, {"referenceID": 5, "context": "The most closely related work to ours is a line of research that began with the work of Boutilier et al. (1995). In particular, the approximate Apricodd algorithm of Hoey et al. (1999), which uses analytic decision diagrams (ADDs) to represent the value function is a strong alternative approach for solving factored MDPs. As discussed in detail in Section 10, the Apricodd algorithm can successfully exploit context-specific structure in the value function, by representing it with the set of mutually-exclusive and exhaustive branches of the ADD. On the other hand, our approach can exploit both additive and context-specific structure in the problem, by using a linear combination of non-mutually-exclusive rules. To better understand this difference, we evaluated both our rule-based approximate linear programming algorithm and Apricodd in two problems, Linear and Expon, designed by Boutilier et al. (2000) to illustrate respectively the best-case and the worst-case behavior of their algorithm.", "startOffset": 88, "endOffset": 913}, {"referenceID": 8, "context": "Using an ADD, the optimal value function for this problem can be represented in linear space, with n+1 leaves (Boutilier et al., 2000).", "startOffset": 110, "endOffset": 134}, {"referenceID": 8, "context": "Using an ADD, the optimal value function for this problem requires an exponential number of leaves (Boutilier et al., 2000), which is illustrated by the exponential running time in Figure 19(b).", "startOffset": 99, "endOffset": 123}, {"referenceID": 5, "context": "The most closely related work to ours is a line of research that began with the work of Boutilier et al. (1995). We address this comparison separately below, but we begin this section with some broader background references.", "startOffset": 88, "endOffset": 112}, {"referenceID": 3, "context": "The field of MDPs, as it is popularly known, was formalized by Bellman (1957) in the 1950\u2019s.", "startOffset": 63, "endOffset": 78}, {"referenceID": 3, "context": "The field of MDPs, as it is popularly known, was formalized by Bellman (1957) in the 1950\u2019s. The importance of value function approximation was recognized at an early stage by Bellman himself (1963). In the early 1990\u2019s the MDP framework was recognized by AI researchers as a formal framework that could be used to address the problem of planning under uncertainty (Dean, Kaelbling, Kirman, & Nicholson, 1993).", "startOffset": 63, "endOffset": 199}, {"referenceID": 3, "context": "The field of MDPs, as it is popularly known, was formalized by Bellman (1957) in the 1950\u2019s. The importance of value function approximation was recognized at an early stage by Bellman himself (1963). In the early 1990\u2019s the MDP framework was recognized by AI researchers as a formal framework that could be used to address the problem of planning under uncertainty (Dean, Kaelbling, Kirman, & Nicholson, 1993). Within the AI community, value function approximation developed concomitantly with the notion of value function representations for Markov chains. Sutton\u2019s seminal paper on temporal difference learning (1988), which addressed the use of value functions for prediction but not planning, assumed a very general representation of the value function and noted the connection to general function approximators such as neural networks.", "startOffset": 63, "endOffset": 620}, {"referenceID": 3, "context": "The field of MDPs, as it is popularly known, was formalized by Bellman (1957) in the 1950\u2019s. The importance of value function approximation was recognized at an early stage by Bellman himself (1963). In the early 1990\u2019s the MDP framework was recognized by AI researchers as a formal framework that could be used to address the problem of planning under uncertainty (Dean, Kaelbling, Kirman, & Nicholson, 1993). Within the AI community, value function approximation developed concomitantly with the notion of value function representations for Markov chains. Sutton\u2019s seminal paper on temporal difference learning (1988), which addressed the use of value functions for prediction but not planning, assumed a very general representation of the value function and noted the connection to general function approximators such as neural networks. However, the stability of this combination was not directly addressed at that time. Several important developments gave the AI community deeper insight into the relationship between function approximation and dynamic programming. Tsitsiklis and Van Roy (1996a) and, independently, Gordon (1995) popularized the analysis of approximate MDP methods via the contraction properties of the dynamic programming operator and function approximator.", "startOffset": 63, "endOffset": 1102}, {"referenceID": 3, "context": "The field of MDPs, as it is popularly known, was formalized by Bellman (1957) in the 1950\u2019s. The importance of value function approximation was recognized at an early stage by Bellman himself (1963). In the early 1990\u2019s the MDP framework was recognized by AI researchers as a formal framework that could be used to address the problem of planning under uncertainty (Dean, Kaelbling, Kirman, & Nicholson, 1993). Within the AI community, value function approximation developed concomitantly with the notion of value function representations for Markov chains. Sutton\u2019s seminal paper on temporal difference learning (1988), which addressed the use of value functions for prediction but not planning, assumed a very general representation of the value function and noted the connection to general function approximators such as neural networks. However, the stability of this combination was not directly addressed at that time. Several important developments gave the AI community deeper insight into the relationship between function approximation and dynamic programming. Tsitsiklis and Van Roy (1996a) and, independently, Gordon (1995) popularized the analysis of approximate MDP methods via the contraction properties of the dynamic programming operator and function approximator.", "startOffset": 63, "endOffset": 1136}, {"referenceID": 3, "context": "The field of MDPs, as it is popularly known, was formalized by Bellman (1957) in the 1950\u2019s. The importance of value function approximation was recognized at an early stage by Bellman himself (1963). In the early 1990\u2019s the MDP framework was recognized by AI researchers as a formal framework that could be used to address the problem of planning under uncertainty (Dean, Kaelbling, Kirman, & Nicholson, 1993). Within the AI community, value function approximation developed concomitantly with the notion of value function representations for Markov chains. Sutton\u2019s seminal paper on temporal difference learning (1988), which addressed the use of value functions for prediction but not planning, assumed a very general representation of the value function and noted the connection to general function approximators such as neural networks. However, the stability of this combination was not directly addressed at that time. Several important developments gave the AI community deeper insight into the relationship between function approximation and dynamic programming. Tsitsiklis and Van Roy (1996a) and, independently, Gordon (1995) popularized the analysis of approximate MDP methods via the contraction properties of the dynamic programming operator and function approximator. Tsitsiklis and Van Roy (1996b) later established a general convergence result for linear value function approximators and TD(\u03bb), and Bertsekas and", "startOffset": 63, "endOffset": 1313}, {"referenceID": 35, "context": "Approximate linear programming for MDPs using linear value function approximation was introduced by Schweitzer and Seidmann (1985), although the approach was somewhat deprecated until fairly recently due the lack of compelling error analyses and the lack of an effective method for handling the large number of constraints.", "startOffset": 100, "endOffset": 131}, {"referenceID": 22, "context": "These methods rely on the use of context-specific structures such as decision trees or analytic decision diagrams (ADDs) (Hoey et al., 1999) to represent both the transition dynamics of the DBN and the value function.", "startOffset": 121, "endOffset": 140}, {"referenceID": 25, "context": "Techniques for exploiting reward functions that decompose additively were studied by Meuleau et al. (1998), and by Singh and Cohn (1998).", "startOffset": 85, "endOffset": 107}, {"referenceID": 25, "context": "Techniques for exploiting reward functions that decompose additively were studied by Meuleau et al. (1998), and by Singh and Cohn (1998). The use of factored representations such as dynamic Bayesian networks was pioneered by Boutilier et al.", "startOffset": 85, "endOffset": 137}, {"referenceID": 5, "context": "The use of factored representations such as dynamic Bayesian networks was pioneered by Boutilier et al. (1995) and has developed steadily in recent years.", "startOffset": 87, "endOffset": 111}, {"referenceID": 5, "context": "The use of factored representations such as dynamic Bayesian networks was pioneered by Boutilier et al. (1995) and has developed steadily in recent years. These methods rely on the use of context-specific structures such as decision trees or analytic decision diagrams (ADDs) (Hoey et al., 1999) to represent both the transition dynamics of the DBN and the value function. The algorithms use dynamic programming to partition the state space, representing the partition using a tree-like structure that branches on state variables and assigns values at the leaves. The tree is grown dynamically as part of the dynamic programming process and the algorithm creates new leaves as needed: A leaf is split by the application of a DP operator when two states associated with that leaf turn out to have different values in the backprojected value function. This process can also be interpreted as a form of model minimization (Dean & Givan, 1997). The number of leaves in a tree used to represent a value function determines the computational complexity of the algorithm. It also limits the number of distinct values that can be assigned to states: since the leaves represent a partitioning of the state space, every state maps to exactly one leaf. However, as was recognized early on, there are trivial MDPs which require exponentially large value functions. This observation led to a line of approximation algorithms aimed at limiting the tree size (Boutilier & Dearden, 1996) and, later, limiting the ADD size (St-Aubin, Hoey, & Boutilier, 2001). Kim and Dean (2001) also explored techniques for discovering tree-structured value functions for factored MDPs.", "startOffset": 87, "endOffset": 1563}, {"referenceID": 5, "context": "The use of factored representations such as dynamic Bayesian networks was pioneered by Boutilier et al. (1995) and has developed steadily in recent years. These methods rely on the use of context-specific structures such as decision trees or analytic decision diagrams (ADDs) (Hoey et al., 1999) to represent both the transition dynamics of the DBN and the value function. The algorithms use dynamic programming to partition the state space, representing the partition using a tree-like structure that branches on state variables and assigns values at the leaves. The tree is grown dynamically as part of the dynamic programming process and the algorithm creates new leaves as needed: A leaf is split by the application of a DP operator when two states associated with that leaf turn out to have different values in the backprojected value function. This process can also be interpreted as a form of model minimization (Dean & Givan, 1997). The number of leaves in a tree used to represent a value function determines the computational complexity of the algorithm. It also limits the number of distinct values that can be assigned to states: since the leaves represent a partitioning of the state space, every state maps to exactly one leaf. However, as was recognized early on, there are trivial MDPs which require exponentially large value functions. This observation led to a line of approximation algorithms aimed at limiting the tree size (Boutilier & Dearden, 1996) and, later, limiting the ADD size (St-Aubin, Hoey, & Boutilier, 2001). Kim and Dean (2001) also explored techniques for discovering tree-structured value functions for factored MDPs. While these methods permit good approximate solutions to some large MDPs, their complexity is still determined by the number of leaves in the representation and the number of distinct values than can be assigned to states is still limited as well. Tadepalli and Ok (1996) were the first to apply linear value function approximation to Factored MDPs.", "startOffset": 87, "endOffset": 1927}, {"referenceID": 34, "context": "Finally, we note that Schuurmans and Patrascu (2001), based on our earlier work on max-norm projection using cost networks and linear programs, independently developed an alternative approach to approximate linear programming using a cost network.", "startOffset": 22, "endOffset": 53}, {"referenceID": 4, "context": "We also compared our approach to the work of Boutilier et al. (2000), which exploits only context-specific structure.", "startOffset": 45, "endOffset": 69}, {"referenceID": 4, "context": "We also compared our approach to the work of Boutilier et al. (2000), which exploits only context-specific structure. For problems with significant context-specific structure in the value function, their approach can be faster due to their efficient handling of the ADD representation. However, there are problems with significant context-specific structure in the problem representation, rather than in the value function, which require exponentially large ADDs. In some such problems, we demonstrated that by using a linear value function our algorithm can obtain a polynomial-time near-optimal approximation of the true value function. The success of our algorithm depends on our ability to capture the most important structure in the value function using a linear, factored approximation. This ability, in turn, depends on the choice of the basis functions and on the properties of the domain. The algorithms currently require the designer to specify the factored basis functions. This is a limitation compared to the algorithms of Boutilier et al. (2000), which are fully automated.", "startOffset": 45, "endOffset": 1060}, {"referenceID": 31, "context": "We plan to leverage on ideas from loopy belief propagation algorithms for approximate inference in Bayesian networks (Pearl, 1988; Yedidia, Freeman, & Weiss, 2001) to address this issue.", "startOffset": 117, "endOffset": 163}], "year": 2011, "abstractText": "This paper addresses the problem of planning under uncertainty in large Markov Decision Processes (MDPs). Factored MDPs represent a complex state space using state variables and the transition model using a dynamic Bayesian network. This representation often allows an exponential reduction in the representation size of structured MDPs, but the complexity of exact solution algorithms for such MDPs can grow exponentially in the representation size. In this paper, we present two approximate solution algorithms that exploit structure in factored MDPs. Both use an approximate value function represented as a linear combination of basis functions, where each basis function involves only a small subset of the domain variables. A key contribution of this paper is that it shows how the basic operations of both algorithms can be performed efficiently in closed form, by exploiting both additive and context-specific structure in a factored MDP. A central element of our algorithms is a novel linear program decomposition technique, analogous to variable elimination in Bayesian networks, which reduces an exponentially large LP to a provably equivalent, polynomial-sized one. One algorithm uses approximate linear programming, and the second approximate dynamic programming. Our dynamic programming algorithm is novel in that it uses an approximation based on max-norm, a technique that more directly minimizes the terms that appear in error bounds for approximate MDP algorithms. We provide experimental results on problems with over 10 states, demonstrating a promising indication of the scalability of our approach, and compare our algorithm to an existing state-of-the-art approach, showing, in some problems, exponential gains in computation time.", "creator": "dvips(k) 5.90a Copyright 2002 Radical Eye Software"}}}