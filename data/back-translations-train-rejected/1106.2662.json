{"id": "1106.2662", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2011", "title": "Learning Equilibria with Partial Information in Decentralized Wireless Networks", "abstract": "In this article, a survey of several important equilibrium concepts for decentralized networks is presented. The term decentralized is used here to refer to scenarios where decisions (e.g., choosing a power allocation policy) are taken autonomously by devices interacting with each other (e.g., through mutual interference). The iterative long-term interaction is characterized by stable points of the wireless network called equilibria. The interest in these equilibria stems from the relevance of network stability and the fact that they can be achieved by letting radio devices to repeatedly interact over time. To achieve these equilibria, several learning techniques, namely, the best response dynamics, fictitious play, smoothed fictitious play, reinforcement learning algorithms, and regret matching, are discussed in terms of information requirements and convergence properties. Most of the notions introduced here, for both equilibria and learning schemes, are illustrated by a simple case study, namely, an interference channel with two transmitter-receiver pairs.", "histories": [["v1", "Tue, 14 Jun 2011 09:58:36 GMT  (142kb,D)", "http://arxiv.org/abs/1106.2662v1", "16 pages, 5 figures, 1 table. To appear in IEEE Communication Magazine, special Issue on Game Theory"]], "COMMENTS": "16 pages, 5 figures, 1 table. To appear in IEEE Communication Magazine, special Issue on Game Theory", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.GT cs.MA", "authors": ["luca rose", "samir m perlaza", "samson lasaulce", "m\\'erouane debbah"], "accepted": false, "id": "1106.2662"}, "pdf": {"name": "1106.2662.pdf", "metadata": {"source": "CRF", "title": "Learning Equilibria with Partial Information in Decentralized Wireless Networks", "authors": ["L. Rose", "S. M. Perlaza", "S. Lasaulce", "M. Debbah"], "emails": ["luca.rose@fr.thalesgroup.com)", "(samir.medinaperlaza@supelec.fr)", "(samson.lasaulce@lss.supelec.fr)", "(merouane.debbah@supelec.fr)"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "II. DEFINITIONS AND ABBREVIATIONS", "text": "In game theory, the normal form is a convenient mathematical representation of a game. Essentially, it consists of a treble: the series of players K = {1, 2,..., K}, the series of actions Ak = {A (1) k,..., A (Nk) k, and the auxiliary functions uk (a), where A = A1 \u00b7 A2 \u00b7... \u00b7 AK is an action profile / vector. In case of slight misuse of the notation, we use a \u2212 k, A \u2212 k to denote the action vector of all players except the k-th player, and we write the vector a as (ak, a \u2212 k) to emphasize the k-th component. For example, the group of players can consist of the set of wireless terminals present in the network, the action set can be any vector of transmission forces, and the auxiliary function can represent the spectral efficiency of the game."}, {"heading": "III. FROM COARSE CORRELATED EQUILIBRIA TO NASH EQUILIBRIA", "text": "The general nature of the equilibrium we use in this work is the so-called roughly correlated equilibrium (CCE). The idea behind CCE is that actions chosen by the players of a game can be correlated. In such a context, a correlation can occur when a common transmission signal is observed by several transmitters that choose their transmission configuration, e.g. a power control policy. We call the signals the action recommendation received by the players. In such a context, a CCE can represent a greater distribution of probabilities than a set of action profiles of the game from which no player is interested in unilaterally deviating action recommendations. The findings of this common distribution are the recommendations that can be written as a result. Definition 1 (Coarse Correlated Equilibrium): A common probability distribution is a CCE distribution."}, {"heading": "IV. LEARNING EQUILIBRIA", "text": "The learning equilibrium process is essentially an iterative process. Each iteration of the learning process (amplified) can be roughly divided into three phases: (i) the observation of the environment during iteration n, which gives players an idea of how well they have played in the previous iteration; (ii) the improvement of strategy \u03c0k (n) based on current observation; and (iii) the selection of players \"ability to act according to strategies \u03c0k (n). Therefore, we say that players learn to play a balance when, after a given number of iterations, the strategy profile \u03c0 (n) = (n) the ability of players to act is played out."}, {"heading": "B. Discussion", "text": "It is time for a new beginning, \"he said in an interview with the German Press Agency.\" It is time for a new beginning, \"he said,\" but it is time for a new beginning. \""}, {"heading": "V. CASE STUDY: THE PARALLEL INTERFERENCE CHANNEL", "text": "This year, it has reached the point where it will be able to take the lead in the EU in order to win the Presidency of the Council of the European Union."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we have presented several concepts of balance and several learning dynamics that enable wireless networks to achieve such balances. In particular, we have described a general concept of balance, namely the grossly correlated balance (CCE). Subsequently, we have presented some specific cases of CCE, such as correlated balance (CE) and Nash balance (NE), which are also analyzed. In terms of learning dynamics, we have presented the best response dynamics (BRD), fictive play (FP), smooth fictive play (SFP), regret matching (RM), affirmation learning (RL), and joint use and strategy-assessment based reinforcement learning (JUSTE-RL). We have identified the relevance of these algorithms for wireless communication (SFP), regret matching (RM), and collective reinforcement learning (RL)."}], "references": [{"title": "The Theory of Learning in Games, ser", "author": ["D. Fudenberg", "D.K. Levine"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Strategic learning and its limits (arne ryde memorial lectures sereis)", "author": ["H.P. Young"], "venue": "Oxford University Press, USA, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Game theory", "author": ["D. Fudenberg", "J. Tirole"], "venue": "MIT Press, 1991.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1991}, {"title": "Methodologies for analyzing equilibria in wireless games", "author": ["S. Lasaulce", "M. Debbah", "E. Altman"], "venue": "IEEE Signal Processing Magazine, Special issue on Game Theory for Signal Processing, vol. 26, no. 5, pp. 41\u201352, Sep. 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "How can ignorant but patient cognitive terminals learn their strategy and utility?", "author": ["S.M. Perlaza", "H. Tembine", "S. Lasaulce"], "venue": "in the 11th IEEE Intl. Workshop on Signal Processing Advances in Wireless Communications (SPAWC", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Game Theory and the Flat-Fading Gaussian Interference Channel: Analyzing Resource Conflicts in Wireless Networks", "author": ["E.G. Larsson", "E.A. Jorswieck", "J. Lindblom", "R. Mochaourab"], "venue": "IEEE signal processing magazine (Print), vol. 26, no. 5, pp. 18\u201327, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Game theory and the frequency selective interference channel - a tutorial", "author": ["A. Leshem", "E. Zehavi"], "venue": "IEEE Signal Processing Magazine, vol. 26, no. 5, pp. 28\u201340, 2009. [Online]. Available: http://arxiv.org/abs/0903.2174", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimal linear precoding strategies for wideband non-cooperative systems based on game theory \u2013 part II: Algorithms", "author": ["G. Scutari", "D. Palomar", "S. Barbarossa"], "venue": "IEEE Trans. on Signal Processing, vol. 56, no. 3, pp. 1250\u20131267, mar. 2008. June 15, 2011  DRAFT  11", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Decentralized learning of Nash equilibria in multi-person stochastic games with incomplete information", "author": ["P. Sastry", "V. Phansalkar", "M. Thathachar"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, vol. 24, no. 5, pp. 769\u2013777, May 1994.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}], "referenceMentions": [{"referenceID": 0, "context": "Interestingly, in some cases, an equilibrium can be reached by using particular iterative procedures similar to learning processes [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "The most general type of equilibria we use in this paper is the so called coarse correlated equilibrium (CCE) [2].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "A special case of CCE is the correlated equilibrium (CE, [2]).", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "It follows, in particular, that every CE is a CCE [2].", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "As shown in [3], this type of equilibria always exists in games with finite number of players and finite action sets.", "startOffset": 12, "endOffset": 15}, {"referenceID": 3, "context": "the reader is referred to [4].", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "As a last notion of equilibrium, we introduce the idea of \u2212equilibrium [2].", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "An instance of \u2212NE [2] is the logit equilibrium [2].", "startOffset": 19, "endOffset": 22}, {"referenceID": 1, "context": "An instance of \u2212NE [2] is the logit equilibrium [2].", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "For instance, if such a function is the entropy function [2], the resulting probability distribution is given by the logit probability distribution.", "startOffset": 57, "endOffset": 60}, {"referenceID": 4, "context": "6) Joint Utility and Strategy Estimation based - Reinforcement Learning (JUSTE-RL): A variant of the former algorithm has been proposed in [5].", "startOffset": 139, "endOffset": 142}, {"referenceID": 5, "context": "In some particular cases, this condition can be relaxed and less information is required [6], [7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "In some particular cases, this condition can be relaxed and less information is required [6], [7].", "startOffset": 94, "endOffset": 97}, {"referenceID": 7, "context": "However, this is highly dependent on the topology of the network and the explicit form of the utility function [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "This is in fact, their main advantage, since such information requires a simple feedback message from the receiver to the corresponding transmitters [9], [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 4, "context": "This is in fact, their main advantage, since such information requires a simple feedback message from the receiver to the corresponding transmitters [9], [5].", "startOffset": 154, "endOffset": 157}, {"referenceID": 0, "context": "2) Knowledge and Calculation Capabilities: Learning algorithms such as BRD, FP, SFP and RM involve an optimization problem at each iteration [1], that is, either the maximization of the (expected or instantaneous) utility or minimization of the regret.", "startOffset": 141, "endOffset": 144}, {"referenceID": 3, "context": "For instance, action sets are discrete in problems where a channel, constellation size or discrete power levels must be selected, whereas continuous sets are more common in power allocation problems [4].", "startOffset": 199, "endOffset": 202}, {"referenceID": 1, "context": "As shown in Table I, the considered algorithms typically converge in certain classes of games [2] such as dominance solvable games (DSG), potential games (PG), super-modular games (SMG), 2\u00d7N non-degenerated games (NDG) or zero-sum games (ZSG).", "startOffset": 94, "endOffset": 97}, {"referenceID": 7, "context": "This problem has been analysed in the context of compact and convex sets of actions in [8] and in discrete and finite sets in [10], which is the case of this section.", "startOffset": 87, "endOffset": 90}], "year": 2011, "abstractText": "In this article, a survey of several important equilibrium concepts for decentralized networks is presented. The term decentralized is used here to refer to scenarios where decisions (e.g., choosing a power allocation policy) are taken autonomously by devices interacting with each other (e.g., through mutual interference). The iterative long-term interaction is characterized by stable points of the wireless network called equilibria. The interest in these equilibria stems from the relevance of network stability and the fact that they can be achieved by letting radio devices to repeatedly interact over time. To achieve these equilibria, several learning techniques, namely, the best response dynamics, fictitious play, smoothed fictitious play, reinforcement learning algorithms, and regret matching, are discussed in terms of information requirements and convergence properties. Most of the notions introduced here, for both equilibria and learning schemes, are illustrated by a simple case study, namely, an interference channel with two transmitter-receiver pairs.", "creator": "LaTeX with hyperref package"}}}