{"id": "1106.3396", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2011", "title": "Large margin filtering for signal sequence labeling", "abstract": "Signal Sequence Labeling consists in predicting a sequence of labels given an observed sequence of samples. A naive way is to filter the signal in order to reduce the noise and to apply a classification algorithm on the filtered samples. We propose in this paper to jointly learn the filter with the classifier leading to a large margin filtering for classification. This method allows to learn the optimal cutoff frequency and phase of the filter that may be different from zero. Two methods are proposed and tested on a toy dataset and on a real life BCI dataset from BCI Competition III.", "histories": [["v1", "Fri, 17 Jun 2011 06:54:35 GMT  (1560kb)", "http://arxiv.org/abs/1106.3396v1", "IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 2010, Dallas : United States (2010)"]], "COMMENTS": "IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 2010, Dallas : United States (2010)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["r\\'emi flamary", "benjamin labb\\'e", "alain rakotomamonjy"], "accepted": false, "id": "1106.3396"}, "pdf": {"name": "1106.3396.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 110 6.33 96v1 [cs.LG] 1 7Ju n20 11Index Terms - Filtering, SVM, BCI, Sequence Marking"}, {"heading": "1. INTRODUCTION", "text": "This year, it has come to the point that it is only a matter of time before there is a result, until there is a result."}, {"heading": "2. LARGE MARGIN FILTER", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Problem definition", "text": "We assume that the training samples are collected in a matrix X-RN-d that contains d channels and N samples. Xi-j is the value of the channel j for the sample. The vector y-1-1-N contains the class of each sample. To filter the noise in the samples. To filter the variability in the samples. To reduce the noise in the samples or the variability in the features, a common approach is to filter X before the learning phase of the classifier. In literature, normally all channels are filtered with the same filter (Savisky-Golay, for example, in [7]), although there is no reason that a single filter is optimal for all channels. We define the filters applied to X through the matrix F-Rf-d. Each column of F is a filter for the corresponding channel in the matrix and f is the size of the matrix FIR-Filter.X-Samprix = m-m-x in the matrix."}, {"heading": "2.2. Windowed-SVM (W-SVM)", "text": "As emphasized by Equation (1), a filter stage essentially consists of taking into account a given time i, instead of the sample Xi, \u00b7, a linear combination of its temporal neighborhood. However, instead of introducing a filter channel F, it is possible to consider a time frame around the current sample for classification, but such an approach would lead to this decision function for the sample of X: fW (i, X) = f \u00b2 m = 1d \u00b2 j = 1Wm, jXi + 1 \u2212 m + n0, j + w0 (2), where W \u00b2 Rf \u00b7 d and w0 \u00b2 R are the classification parameters and f is the size of the time window. Note that W plays the role of the filter and the weights of a linear classifier. Within a large scale, W and w0 \u00b0 C can be learned by minimizing this functional classification: JWSV M (W) = 12 | W | | 2F | C2N \u00b2 i = H, W y (W)."}, {"heading": "2.3. Large margin filtering (Filter-SVM)", "text": "We propose to find the filter F = 12, which maximises the margin of linear classification (X, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II, II"}, {"heading": "3. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Toy Example", "text": "We use a toy example that consists of nbtot channels, of which only nbrel is discriminatory. Discriminative channels have a switching average value {\u2212 1, 1}, which is controlled by the label and corrupted by a Gaussian noise of deviation \u03c3. The length of the regions with constant labeling follows a uniform distribution law between [30, 40] samples and different time delays are applied to the channels. We have selected f = 21 and n0 = 11, which corresponds to a good average filtering centered on the current sample. Figure 1 shows how the samples are transformed into a one-dimensional signal thanks to the F filter. In this case, the mean test error due to noise is 16% for the unfiltered signal, while only 2% for the optimally filtered signal. Window SVM and filter SVM are compared to SVM without filtering, SVM are selected with an average filter of the size (Vitf-MHM) and SVM with an SVM sparing."}, {"heading": "3.2. BCI Dataset", "text": "We test our method using the BCI data set of BCI Competition III [1]. The problem is to obtain a sequence of labels from the signals of brain activity of 3 people. The data consists of 96 channels with PSD characteristics (3 training sessions, 1 test session, N \u2248 3000 per session) and the problem has 3 labels (left arm, right arm or feet).We use filter SVM, which showed a better result for the toy in size dimension.The multi-class assay aspect of the problem is handled with a one-against-all strategy. The regulation parameters are adjusted by means of a validation method for the grid search on the third training set. We compare our method with the best BCI competition results (with only 8 samples) and with the SVM without filtering. Test errors for different filter size and delay n0 can be seen on Table 1. The results show that filtering can be improved by using a longer result (0)."}, {"heading": "4. CONCLUSIONS", "text": "We have proposed two methods for the automatic learning of a spatio-temporal filter used for multi-channel classification, using a toy example and a real data set from BCI Competition III.Empirical results clearly show the advantages of adapting the signal filter to the problem of large channel classification despite the non-convexity of the criterion. In future work we plan to extend our approach to non-linear cases, we believe that a differentiated core can be used instead of internal products at the expense of solving SVM in dual space. Another perspective would be the adaptation of our methods to the multi-task situation, where one matrix F and several classifiers (one per task) are to be learned together."}, {"heading": "5. REFERENCES", "text": "[1] B. Blankertz et al., \"The BCI competition 2003: progress and perspectives in detection and discrimination of EEG single trial,\" IEEE Transactions on Biomedical Engineering, vol. 51, no. 6, pp. 1044-1051, 2004. [2] J. del R Milla \"n,\" On the need for on-line learning in braincomputer interfaces, \"in Proc. Int. Joint Conf. on Neural Networks, 2004. [3] O. Cappe, E. Moulines, and T. Ryde\" n, Inference in Hidden Markov Models, Springer, 2005. [4] J. Lafferty, A.McCallum, and F. Pereira, \"Conditional Random fields: Probabilistic models for segmenting and labeling sequence data,\" in Proc., 18th International Conf. on Machine Learning, 2001, pp."}], "references": [{"title": "The BCI competition 2003: progress and perspectives in detection and discrimination of EEG single trials,", "author": ["B. Blankertz"], "venue": "IEEE Transactions on Biomedical Engineering,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "On the need for on-line learning in braincomputer interfaces,", "author": ["J. del R Mill\u00e1n"], "venue": "in Proc. Int. Joint Conf. on Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Inference in Hidden Markov Models", "author": ["O. Capp\u00e9", "E. Moulines", "T. Ryd\u00e8n"], "venue": "Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data,", "author": ["J. Lafferty", "A.McCallum", "F. Pereira"], "venue": "in Proc. 18th International Conf. on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Large margin methods for structured and interdependent output variables,", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal Of Machine Learning Research", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "and C", "author": ["F. Desobry", "M. Davy"], "venue": "Doncarli, \u201cAn online kernel change detection algorithm,\u201d IEEE Transactions on Signal Processing, vol. 53, pp. 2961\u20132974", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Prediction of arm movement trajectories from ecog-recordings in humans,", "author": ["T. Pistohl", "T. Ball", "A. Schulze-Bonhage", "A. Aertsen", "C. Mehring"], "venue": "Journal of Neuroscience Methods,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "Training a support vector machine in the primal,", "author": ["O. Chapelle"], "venue": "Neural Comput.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Optimization problems with pertubation : A guided tour,", "author": ["J.F. Bonnans", "A. Shapiro"], "venue": "SIAM Review,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Indeed, in real-time BCI applications, each sample of an electro-encephalography signal has to be interpreted as a specific command for a virtual keyboard or a robot hence the need for sample labeling [1, 2].", "startOffset": 201, "endOffset": 207}, {"referenceID": 1, "context": "Indeed, in real-time BCI applications, each sample of an electro-encephalography signal has to be interpreted as a specific command for a virtual keyboard or a robot hence the need for sample labeling [1, 2].", "startOffset": 201, "endOffset": 207}, {"referenceID": 2, "context": "For instance, Hidden Markov Models (HMM) [3] are statistical models that are able to learn a joint probability distribution of samples in a sequence and their labels.", "startOffset": 41, "endOffset": 44}, {"referenceID": 3, "context": "In some cases, Conditional Random Fields (CRF) [4] have been shown to outperform the HMM approach as they do not suppose the observation are independent.", "startOffset": 47, "endOffset": 50}, {"referenceID": 4, "context": "Structural Support Vector Machines (StructSVM), which are SVMs that learn a mapping from structured input to structured output, have also been considered for signal segmentation [5].", "startOffset": 178, "endOffset": 181}, {"referenceID": 5, "context": "For instance, a Kernel Change Detection algorithm [6] can be used for detecting abrupt changes in a signal and afterwards a classifier applied for labeling the segmented regions.", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "[7] had to select by a validation method a delay in their signal processing method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "These two methods are tested on a toy dataset and on a real life BCI signal sequence labeling problem from BCI Competition III [1].", "startOffset": 127, "endOffset": 130}, {"referenceID": 6, "context": "In literature, all channels are usually filtered with the same filter (Savisky-Golay for instance in [7]) although there is no reason for a single filter to be optimal for all channels.", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "Hence, we can take advantage of many linear SVM solvers existing in the literature such as the one proposed by Chapelle [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 8, "context": "[9] is differentiable.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "We test our method on the BCI Dataset from BCI Competition III [1].", "startOffset": 63, "endOffset": 66}], "year": 2014, "abstractText": "Signal Sequence Labeling consists in predicting a sequence of labels given an observed sequence of samples. A naive way is to filter the signal in order to reduce the noise and to apply a classification algorithm on the filtered samples. We propose in this paper to jointly learn the filter with the classifier leading to a large margin filtering for classification. This method allows to learn the optimal cutoff frequency and phase of the filter that may be different from zero. Two methods are proposed and tested on a toy dataset and on a real life BCI dataset from BCI Competition III.", "creator": "LaTeX with hyperref package"}}}