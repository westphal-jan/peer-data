{"id": "1106.3397", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Jun-2011", "title": "Handling uncertainties in SVM classification", "abstract": "This paper addresses the pattern classification problem arising when available target data include some uncertainty information. Target data considered here is either qualitative (a class label) or quantitative (an estimation of the posterior probability). Our main contribution is a SVM inspired formulation of this problem allowing to take into account class label through a hinge loss as well as probability estimates using epsilon-insensitive cost function together with a minimum norm (maximum margin) objective. This formulation shows a dual form leading to a quadratic problem and allows the use of a representer theorem and associated kernel. The solution provided can be used for both decision and posterior probability estimation. Based on empirical evidence our method outperforms regular SVM in terms of probability predictions and classification performances.", "histories": [["v1", "Fri, 17 Jun 2011 06:55:24 GMT  (2066kb)", "http://arxiv.org/abs/1106.3397v1", "IEEE Workshop on Statistical Signal Processing, Nice: France (2011)"]], "COMMENTS": "IEEE Workshop on Statistical Signal Processing, Nice: France (2011)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["emilie niaf", "r\\'emi flamary", "carole lartizien", "st\\'ephane canu"], "accepted": false, "id": "1106.3397"}, "pdf": {"name": "1106.3397.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["St\u00e9phane CANU"], "emails": [], "sections": [{"heading": null, "text": "We have a problem that leads to a set of data that are then used as inputs for training. [1] For example, this is often the case in medical imaging, where radiologists think what they think are malignant tissues via medical images without access to the reference histopatologic information. [2] We propose to deal with these uncertainties by adding probable labels in the learning phase such as: 1. Stick to the real life annotation classes, 3. Balance to the unsafe data in the classification. [3] We use the unlikely labels in the learning phase such as: 1. Stick to the real life annotation problems, 3. Balance to the unsafe data in the classification."}], "references": [{"title": "Learning SVMs from Sloppily Labeled Data,", "author": ["G. Stempfel", "L. Ralaivola"], "venue": "Artificial Neural Networks\u2013ICANN", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning)", "author": ["Bernhard Sch\u00f6lkopf", "Alexander J. Smola"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2001}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods,\u201d in Advances in large margin classifiers", "author": ["John C. Platt"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Probabilistic methods for support vector machines,", "author": ["Peter Sollich"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "A Simple Method For Estimating Conditional Probabilities For SVMs,", "author": ["S. R\u00fcping"], "venue": "LWA", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "and S", "author": ["Y. Grandvalet", "J. Mari\u00e9thoz"], "venue": "Bengio, \u201cA probabilistic interpretation of SVMs with an application to unbalanced classification,\u201d Advances in Neural Information Processing Systems, vol. 18, pp. 467", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "SVM Classifier Estimation from Group Probabilities,", "author": ["S. Rueping"], "venue": "ICML", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "and A", "author": ["S. Canu", "Y. Grandvalet", "V. Guigue"], "venue": "Rakotomamonjy, \u201cSvm and kernel methods matlab toolbox,\u201d Perception Syst\u00e8mes et Information, INSA de Rouen, Rouen, France", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "In the end the training data set may contain inaccurate classes for some examples, which leads to non robust classifiers[1].", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "Our study focuses on the widely used Support Vector Machines (SVM) two-class classification problem [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 2, "context": "Several mappings from SVM scores to class membership probabilities have been proposed in the literature [3, 4].", "startOffset": 104, "endOffset": 110}, {"referenceID": 3, "context": "Several mappings from SVM scores to class membership probabilities have been proposed in the literature [3, 4].", "startOffset": 104, "endOffset": 110}, {"referenceID": 0, "context": "n (in classification), \u2022 real values: li = pi \u2208 [0, 1] for i = n + 1 .", "startOffset": 48, "endOffset": 54}, {"referenceID": 4, "context": "Then, the regression problem consists in finding optimal parameters w and b such that | 1 1 + e \u22a4xi+b) \u2212 pi |< \u03b7 , Thus constraining the probability prediction for point xi to remain around to 1 1+e\u2212a(w xi+b) within distance \u03b7 [5, 6, 7].", "startOffset": 227, "endOffset": 236}, {"referenceID": 5, "context": "Then, the regression problem consists in finding optimal parameters w and b such that | 1 1 + e \u22a4xi+b) \u2212 pi |< \u03b7 , Thus constraining the probability prediction for point xi to remain around to 1 1+e\u2212a(w xi+b) within distance \u03b7 [5, 6, 7].", "startOffset": 227, "endOffset": 236}, {"referenceID": 6, "context": "Then, the regression problem consists in finding optimal parameters w and b such that | 1 1 + e \u22a4xi+b) \u2212 pi |< \u03b7 , Thus constraining the probability prediction for point xi to remain around to 1 1+e\u2212a(w xi+b) within distance \u03b7 [5, 6, 7].", "startOffset": 227, "endOffset": 236}, {"referenceID": 7, "context": "We implemented our method using the SVMKM Toolbox [8].", "startOffset": 50, "endOffset": 53}, {"referenceID": 2, "context": "In the first case, probabilities are estimated by using Platt\u2019s scaling algorithm [3] while in the second case, probabilities are directly estimated via the formula defined in (2): P (y = 1|x) = 1 1+e\u2212a(w\u22a4x+b) .", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "Contrary to P-SVM which, by combining both classification and regression, predicts good probabilities, C-SVM is sensitive to classification noise and is no more converging to the Bayes rule as seen in [1].", "startOffset": 201, "endOffset": 204}], "year": 2011, "abstractText": "This paper addresses the pattern classification problem arising when available target data include some uncertainty information. Target data considered here is either qualitative (a class label) or quantitative (an estimation of the posterior probability). Our main contribution is a SVM inspired formulation of this problem allowing to take into account class label through a hinge loss as well as probability estimates using \u03b5-insensitive cost function together with a minimum norm (maximum margin) objective. This formulation shows a dual form leading to a quadratic problem and allows the use of a representer theorem and associated kernel. The solution provided can be used for both decision and posterior probability estimation. Based on empirical evidence our method outperforms regular SVM in terms of probability predictions and classification performances.", "creator": "LaTeX with hyperref package"}}}