{"id": "1106.3967", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2011", "title": "Intelligent Self-Repairable Web Wrappers", "abstract": "The amount of information available on the Web grows at an incredible high rate. Systems and procedures devised to extract these data from Web sources already exist, and different approaches and techniques have been investigated during the last years. On the one hand, reliable solutions should provide robust algorithms of Web data mining which could automatically face possible malfunctioning or failures. On the other, in literature there is a lack of solutions about the maintenance of these systems. Procedures that extract Web data may be strictly interconnected with the structure of the data source itself; thus, malfunctioning or acquisition of corrupted data could be caused, for example, by structural modifications of data sources brought by their owners. Nowadays, verification of data integrity and maintenance are mostly manually managed, in order to ensure that these systems work correctly and reliably. In this paper we propose a novel approach to create procedures able to extract data from Web sources -- the so called Web wrappers -- which can face possible malfunctioning caused by modifications of the structure of the data source, and can automatically repair themselves.", "histories": [["v1", "Mon, 20 Jun 2011 17:02:40 GMT  (848kb)", "http://arxiv.org/abs/1106.3967v1", "12 pages, 4 figures; Proceedings of the 12th International Conference of the Italian Association for Artificial Intelligence, 2011"]], "COMMENTS": "12 pages, 4 figures; Proceedings of the 12th International Conference of the Italian Association for Artificial Intelligence, 2011", "reviews": [], "SUBJECTS": "cs.AI cs.IR", "authors": ["emilio ferrara", "robert baumgartner"], "accepted": false, "id": "1106.3967"}, "pdf": {"name": "1106.3967.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Robert Baumgartner"], "emails": ["emilio.ferrara@unime.it", "robert.baumgartner@lixto.com"], "sections": [{"heading": null, "text": "ar Xiv: 110 6.39 67v1 [cs.AI] 20 Jun 20Keywords: web data extraction, wrapper, automatic customization"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 Background and Related Work", "text": "In fact, it is a kind of vanity that is able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to fight, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "3 The Tree Matching Algorithm", "text": "This work is based on some assumptions: i) web pages are represented by using DOM trees, as imposed by the HTML standards3; ii) it is possible to identify elements within a DOM tree using the XPath language4; iii) the logic of XPath underpins the functioning of web wrappers (this is explained further in the following sections and in [1,2]). In light of these milestones, the main idea of our approach is to compare two trees, one representing the original web page and another representing the page after some modifications have occurred. This is handy to automate the adaptive process of automatically repairing our wrappers. To achieve this, we use a variant of the ground-breaking Simple Tree Matching (STM) [18], optimized by Ferrara and Baumgartner [6,7]. Let d (n) specify the degree of a node (i.e. the number of the first tree-specific number) of the tree-sibling (s); let the T be the specific number of the tree-sibling (s)."}, {"heading": "4 Web Wrappers", "text": "In the supervised and interactive wrapper generation, the application designer is responsible for deciding how to characterize web objects used to traverse the web and extract information. It is one of the most important aspects of a wrapper to be resistant to change (both changes over time and variations in similarly structured pages), and parts of the robustness of a data extractor depend on how the application designer configures it. However, it is crucial that the wrapper generation system helps the wrapper designer and suggests that how3 http: / / / www.w3.org / TR / \u2032 www.\u2032 \u2032 \u2032 \u2032 s level 2 HTML / html.html 4 http: / / / / www.w3.org / TR / xpath / algorithm 1 WeightedTreeMatching (T \u2032, T \u2032 \u2032 \u2032 \u2032 \u2032 \u2032 \u2032 s) 1: if T \u2032, T \u2032 \u2032 \u2032 \u2032 s www.\u2032 \u2032 \u2032 s \u2032 s level 2 \u2032 \u2032 s, 1: if matt objects have the same label as Webpath-T / xhtml-2 / HTM.T (otherwise html / HTM.T: HTM.T / 1)"}, {"heading": "4.1 Robust XPath Generation and Fall-back Strategies", "text": "Lixto Visual Developer (VD) offers a number of mechanisms to create a robust wrapper. One task is to generate a robust XPath or regular expression that is interactive and supported by the system. In many cases, only a flagged sample object is available, especially when using in-depth web sequences. In such cases, efficient heuristics are required in XPath generation and fallback strategies during repetition. Typical heuristics during recording of such web objects include generating a selected XPath property, textual properties and formatting properties. During repetition, these ingredients will be used as input for an algorithm in which these properties are best applied to fulfill integrity information."}, {"heading": "4.2 Configuring Adaptable Wrappers", "text": "The procedures described in the previous section do not adapt the wrapper, but address situations where the originally selected XPath no longer matches and simply tries different web objects based on that one page. In configuring the wrapper customization, we go one step further: on the one hand, we use tree and string similarity techniques to find the most similar web objects on the new page, and on the other, if the customization is triggered, the application designer can spontaneously change the wrapper using the new configuration created by the adaptation algorithms. As before, integrity restrictions can be imposed on extraction and navigation rules. In addition, the application designer can choose to apply the wrapper customization to a specific rule if the restrictions are violated during runtime. If the customization is selected, alternatively to using XPath-based means to identify web objects, we can store the actual result tree."}, {"heading": "5 Automatic Wrapper Adaptation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Self-repairing rules", "text": "Figure 3 describes the adjustment process. The adjustment process of the wrapper is triggered by the violation of defined constraints. If the wrapper definition already saves the sample tree and the similarity calculation produces results that violate the defined constraints, the threshold is lowered or raised until a perfect match is established. During runtime, the saved tree is compared with the elements on the new page, and the most suitable element (s) are considered as extraction results. During configuration, wrapper designers can choose an algorithm (e.g. Weighted Tree Matching), and a similarity threshold may be constant or within an interval of acceptable thresholds. During execution, various thresholds within the allowed range are taken into account, and make a generalization of the best adjustment to the defined constraints."}, {"heading": "5.2 Wrapper Re-Induction", "text": "In practice, individual rule and action customization steps are embedded in the entire wrapper execution process, and the customized wrapper is stored in the repository after all customization steps are completed. The need to customize a particular rule influences subsequent execution steps. To define a rule that matches such entities, the wrapper designer visually selects an example and generalizes the rule configuration until the desired instances match. To support the automatic customization process at runtime, the wrapper designer specifies what it means that the extraction failed."}, {"heading": "6 Performances Measurement", "text": "For our initial performance evaluation, we tested the robustness of our wrappers compared to real-world use cases. Actual areas of interest for web data extraction problems are social networks, retail markets and web communities. We have defined a total of 7 scenarios and designed 10 adaptive wrappers each. Results from precision, retrieval and F1 score are as shown in Table 1. Column threshold represents the specified threshold; tp, fp and fn summarize true and false positive or false negative. Performance achieved by using simple and weighted tree mapping is good; these algorithms are definitely workable solutions for our original purpose and offer a high degree of reliability (F measurement > 90%)."}, {"heading": "7 Conclusions and Future Work", "text": "In the literature, several implementations of systems for extracting data from web sources have been presented, but there is a lack of solutions for their maintenance. This paper attempts to address this problem by describing adaptive techniques to make web data extraction systems based on wrappers self-maintainable, using algorithms optimized for this purpose. Enhanced Web wrappers will therefore be able to detect structural modifications of web sources and adapt their functionality accordingly. Characteristics of our self-paralleling solution will be discussed in detail and provide initial experimental results to evaluate their robustness. Further experiments will have to follow in the near future. In addition, additional algorithms would be incorporated into future work to improve the capabilities of the adaptation function; in particular, a viable idea could be to generalize a bigram-based tree matching algorithm capable of adapting node perations in a more efficient way to address issues of grammatics."}], "references": [{"title": "Web data extraction system", "author": ["R. Baumgartner", "W. Gatterbauer", "G. Gottlob"], "venue": "Encyclopedia of Database Systems pp. 3465\u20133471", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Scalable web data extraction for online market intelligence", "author": ["R. Baumgartner", "G. Gottlob", "M. Herzog"], "venue": "Proceedings of the VLDB Endowment 2(2), 1512\u20131523", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey on tree edit distance and related problems", "author": ["P. Bille"], "venue": "Theoretical computer science 337(1-3), 217\u2013239", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Automatic repairing of web wrappers by combining redundant views", "author": ["B. Chidlovskii"], "venue": "Proceedings of the 14th International Conference on Tools with Artificial Intelligence. pp. 399\u2013406. IEEE", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "A machine learning approach to web mining", "author": ["F. Esposito", "D. Malerba", "L. Di Pace", "P. Leo"], "venue": "AI* IA 99: Advances in Artificial Intelligence pp. 190\u2013201", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Automatic wrapper adaptation by tree edit distance matching", "author": ["E. Ferrara", "R. Baumgartner"], "venue": "Combinations of Intelligent Methods and Applications pp. 41\u201354", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Design of automatically adaptable web wrappers", "author": ["E. Ferrara", "R. Baumgartner"], "venue": "Proceedings of the 3rd International Conference on Agents and Artificial Intelligence. pp. 211\u2013217", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Web data extraction, application and techniques: A survey", "author": ["E. Ferrara", "G. Fiumara", "R. Baumgartner"], "venue": "Technical Report", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Web information extraction by HTML tree edit distance matching", "author": ["Y. Kim", "J. Park", "T. Kim", "J. Choi"], "venue": "Proceedings of the International Conference on Convergence Information Technology. pp. 2455\u20132460. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Wrapper verification", "author": ["N. Kushmerick"], "venue": "World Wide Web 3(2), 79\u201394", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Finite-state approaches toWeb information extraction", "author": ["N. Kushmerick"], "venue": "Extraction in the Web Era pp. 77\u201391", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Regression testing for wrapper maintenance", "author": ["N Kushmerick"], "venue": "Proceedings of the National Conference on Artificial Intelligence. pp. 74\u2013284", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "A brief survey of web data extraction tools", "author": ["A. Laender", "B. Ribeiro-Neto", "A. da Silva", "J. Teixeira"], "venue": "ACM Sigmod Record 31(2), 84\u201393", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Wrapper maintenance: A machine learning approach", "author": ["K. Lerman", "S. Minton", "C. Knoblock"], "venue": "Journal of Artificial Intelligence Research 18(1), 149\u2013181", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Schema-guided wrapper maintenance for web-data extraction", "author": ["X. Meng", "D. Hu", "C. Li"], "venue": "Proceedings of the 5th ACM international workshop on Web information and data management. pp. 1\u20138. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatically generating labeled examples for web wrapper maintenance", "author": ["J. Raposo", "A. Pan", "M. Alvarez", "J. Hidalgo"], "venue": "Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence. pp. 250\u2013256", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Information extraction", "author": ["S. Sarawagi"], "venue": "Foundations and Trends in Databases 1(3), 261\u2013377", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "The tree-to-tree editing problem", "author": ["S. Selkow"], "venue": "Information processing letters 6(6), 184\u2013186", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1977}, {"title": "Identifying syntactic differences between two programs", "author": ["W. Yang"], "venue": "Software: Practice and Experience 21(7), 739\u2013755", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1991}], "referenceMentions": [{"referenceID": 4, "context": "The first one relies on machine learning platforms [5]; a system analyzes, possibly, huge amount of positive and negative examples during a training period, and, then, it infers some set of rules that makes it able to perform its tasks in the same domain or Web site.", "startOffset": 51, "endOffset": 54}, {"referenceID": 12, "context": "[13] provided the first rigorous taxonomical classification of Web data extraction systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Kushmerick [11] classified several finite-state approaches to generate wrappers, such as the wrapper induction, natural language processing approaches and hidden Markov models.", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "Sarawagi [17] provided the most comprehensive survey on the information extraction panorama.", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "[1] and later Ferrara et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] provided two different surveys on the discipline of Web data extraction.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Kushmerick [12,10] for first introduced the concept of wrapper maintenance as the process of verifying the correct functioning of the data extraction procedures and manually, automatically or in a semi-automatic way, intervene in case of malfunctioning.", "startOffset": 11, "endOffset": 18}, {"referenceID": 9, "context": "Kushmerick [12,10] for first introduced the concept of wrapper maintenance as the process of verifying the correct functioning of the data extraction procedures and manually, automatically or in a semi-automatic way, intervene in case of malfunctioning.", "startOffset": 11, "endOffset": 18}, {"referenceID": 13, "context": "Lerman and Minton [14], instead, faced both the problems of verifying the correctness of data extracted by a wrapper and eventually try to repair it.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "Another approach based on machine learning has been provided by Chidlovskii [4]; he described a system which can automatically classify Web pages in order to extract information from those pages which can be handled adopting both conventional extraction rules and ensemble methods of machine learning, such as the content features analysis.", "startOffset": 76, "endOffset": 79}, {"referenceID": 14, "context": "[15] developed the SG-WRAM (Schema-Guided WRApper Maintenance) slightly modifying the perspective of Web wrappers generation, observing that changes in Web pages, even substantial, always preserve syntactic features (i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16]; they adopted a collected sample of positive labeled examples during the normal execution of the wrappers, to be exploited in case of malfunctioning, in order to re-induct the broken wrapper ensuring a good accuracy of the process.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The tree edit distance problem is a well-known NP-hard problem [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 17, "context": "Several approximate solutions have been advanced during the years; the most appropriate algorithm to face the problem of matching up similar trees, has been suggested by Selkow [18].", "startOffset": 177, "endOffset": 181}, {"referenceID": 5, "context": "Ferrara and Baumgartner [6,7] so as Yang [19] adopt weights, obtaining a variant of this algorithm with the capability of discovering clusters of similar sub-trees.", "startOffset": 24, "endOffset": 29}, {"referenceID": 6, "context": "Ferrara and Baumgartner [6,7] so as Yang [19] adopt weights, obtaining a variant of this algorithm with the capability of discovering clusters of similar sub-trees.", "startOffset": 24, "endOffset": 29}, {"referenceID": 18, "context": "Ferrara and Baumgartner [6,7] so as Yang [19] adopt weights, obtaining a variant of this algorithm with the capability of discovering clusters of similar sub-trees.", "startOffset": 41, "endOffset": 45}, {"referenceID": 8, "context": "[9], has been performed exploiting these two algorithms to extract information from HTML Web pages.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "This work relies on some assumptions: i) Web pages are represented by using DOM trees, as the HTML standard imposes; ii) it is possible to identify elements within a DOM tree by using the XPath language; iii) the logics of XPath underly the functioning of Web wrappers (this is further explained in following sections and in [1,2]).", "startOffset": 325, "endOffset": 330}, {"referenceID": 1, "context": "This work relies on some assumptions: i) Web pages are represented by using DOM trees, as the HTML standard imposes; ii) it is possible to identify elements within a DOM tree by using the XPath language; iii) the logics of XPath underly the functioning of Web wrappers (this is further explained in following sections and in [1,2]).", "startOffset": 325, "endOffset": 330}, {"referenceID": 17, "context": "To do so, we utilize a variant of the seminal Simple Tree Matching (STM) [18], optimized by Ferrara and Baumgartner [6,7].", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "To do so, we utilize a variant of the seminal Simple Tree Matching (STM) [18], optimized by Ferrara and Baumgartner [6,7].", "startOffset": 116, "endOffset": 121}, {"referenceID": 6, "context": "To do so, we utilize a variant of the seminal Simple Tree Matching (STM) [18], optimized by Ferrara and Baumgartner [6,7].", "startOffset": 116, "endOffset": 121}, {"referenceID": 1, "context": "In Lixto Visual Developer (VD) [2], a number of mechanisms are offered to create a resilient wrapper.", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": "Wrapper designers can choose between various similarity measures: this includes in particular the Simple Tree Matching algorithm [18] and the Weighted Tree Matching algorithm described in Section 3.", "startOffset": 129, "endOffset": 133}], "year": 2011, "abstractText": "The amount of information available on the Web grows at an incredible high rate. Systems and procedures devised to extract these data from Web sources already exist, and different approaches and techniques have been investigated during the last years. On the one hand, reliable solutions should provide robust algorithms of Web data mining which could automatically face possible malfunctioning or failures. On the other, in literature there is a lack of solutions about the maintenance of these systems. Procedures that extract Web data may be strictly interconnected with the structure of the data source itself; thus, malfunctioning or acquisition of corrupted data could be caused, for example, by structural modifications of data sources brought by their owners. Nowadays, verification of data integrity and maintenance are mostly manually managed, in order to ensure that these systems work correctly and reliably. In this paper we propose a novel approach to create procedures able to extract data from Web sources \u2013 the so called Web wrappers \u2013 which can face possible malfunctioning caused by modifications of the structure of the data source, and can automatically repair themselves.", "creator": "LaTeX with hyperref package"}}}