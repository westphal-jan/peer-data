{"id": "1106.4557", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2011", "title": "Learning When Training Data are Costly: The Effect of Class Distribution on Tree Induction", "abstract": "For large, real-world inductive learning problems, the number of training examples often must be limited due to the costs associated with procuring, preparing, and storing the training examples and/or the computational costs associated with learning from them. In such circumstances, one question of practical importance is: if only n training examples can be selected, in what proportion should the classes be represented? In this article we help to answer this question by analyzing, for a fixed training-set size, the relationship between the class distribution of the training data and the performance of classification trees induced from these data. We study twenty-six data sets and, for each, determine the best class distribution for learning. The naturally occurring class distribution is shown to generally perform well when classifier performance is evaluated using undifferentiated error rate (0/1 loss). However, when the area under the ROC curve is used to evaluate classifier performance, a balanced distribution is shown to perform well. Since neither of these choices for class distribution always generates the best-performing classifier, we introduce a budget-sensitive progressive sampling algorithm for selecting training examples based on the class associated with each example. An empirical analysis of this algorithm shows that the class distribution of the resulting training set yields classifiers with good (nearly-optimal) classification performance.", "histories": [["v1", "Wed, 22 Jun 2011 20:11:46 GMT  (253kb)", "http://arxiv.org/abs/1106.4557v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["f provost", "g m weiss"], "accepted": false, "id": "1106.4557"}, "pdf": {"name": "1106.4557.pdf", "metadata": {"source": "CRF", "title": "Learning When Training Data are Costly: The Effect of Class Distribution on Tree Induction", "authors": ["Gary M. Weiss"], "emails": ["GMWEISS@ATT.COM", "FPROVOST@STERN.NYU.EDU"], "sections": [{"heading": null, "text": "\u00a9 2003 AI Access Foundation and Morgan Kaufmann Publishers. All Rights Reserved.For large, real inductive learning problems, the number of training examples must be limited due to the costs associated with obtaining, preparing and storing the training examples and / or the computing costs associated with them. In such circumstances, a question of practical importance is: If only n training examples can be selected, in what proportion should the classes be represented? In this article, we help answer this question by analyzing the relationship between the class distribution of the training data and the performance of the classification trees resulting from this data for a specified training set size. We examine twenty-six sets of data and determine for each the best class distribution for learning. The naturally occurring class distribution is generally good when the classification performance is evaluated using an undifferentiated error rate (0 / 1 loss). However, when the area below the ROC curve is used to evaluate classification performance, a balanced class distribution is shown to work well."}, {"heading": "1. Introduction", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is not a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country,"}, {"heading": "2. Background and Notation", "text": "Let x be an instance drawn from any fixed distribution. Each instance x is assigned (perhaps > probabilistically) to a class (classification) (classification) (classification) (classification) by function c, where c represents the true but unknown classification function.1 Let \u03c1 be the marginal probability of belonging to the positive class and 1 - \u03c1 the marginal probability of belonging to the negative class. These marginal probabilities are sometimes referred to as \"class priorities\" or the \"base rate.\" A classifier t is an assignment of x to the classes {p, n} and is an approximation of c. For notational convenience, we leave t (x) [P, N} so that it is always clear whether a class value is an actual (lowercase) or a predicted (uppercase) threshold. The expected accuracy of a classifier t, \u03b1t, is defined as Pr (c) = (x) (x = ()."}, {"heading": "3. Correcting for Changes to the Class Distribution of the Training Set", "text": "Many classification algorithms assume that the training and test data are drawn from the same fixed, underlying, distribution D. In particular, these algorithms assume that rtrain and rtest, the fractions of positive examples in the training and test categories, approximate \u03c1, the actual \"previous\" probability of encountering a positive example, but generally use the estimated class priorities based on rtrain, either implicitly or explicitly, to construct a model and assign classifications. If the estimated value of the class priorities is not accurate, then the rear probabilities of the model will be improperly distorted. Specifically, this means: \"the increase in the previous probability of a class increases the probability of the class shifting the classification limit for that class, so that further cases are classified into the class.\" (SAS, 2001) Thus, if the training data are selected in such a way that rtrain does not represent approximate probabilities for the rear probabilities, then the differences between the classes should be based on the differences."}, {"heading": "4. Experimental Setup", "text": "In this section, we will describe the data sets analyzed in this article, the sampling strategy to change the class distribution of training data, the classification induction program used, and finally, the indicators used to evaluate the performance of the induced classifiers."}, {"heading": "4.1 The Data Sets and the Method for Generating the Training Data", "text": "This collection includes twenty records from the UCI repository (Blake & Merz, 1998), five records identified with a \"+\" from previously published work by researchers at AT & T (Cohen & Singer, 1999), and a new record, the telephone record generated by the authors. The records in Table 2 are listed in the order of decreasing class imbalance, a convention used during this article. To simplify the presentation and analysis of our results, records with more than two classes were mapped to two-class problems, achieved by naming one of the original classes, typically the least common, and then classifying the remaining classes into the majority class. Data sets that originally contained more than two classes are identified by an asterisk (*) listed in Table 2."}, {"heading": "4.2 C4.5 and Pruning", "text": "The experiments in this article use C4.5, a program for generating classification trees from marked examples (Quinlan, 1993). C4.5 uses the uncorrected frequency-based estimate to mark the leaves of the decision tree, as it is assumed that the training data resembles the true underlying distribution. Given that we modify the class distribution of the training set, it is essential that we use the corrected estimates to re-mark the leaves of the induced tree. C4.5 does not take into account the differences between the class distributions of the training and test sets - we generally adjust this as a post-processing step. If C4.5 uses the trimming strategy that attempts to minimize the 2001 error rate based on the assumption of a preparation, it would not affect the year."}, {"heading": "4.3 Evaluating Classifier Performance", "text": "A variety of metrics used to evaluate classification performance are based on the terms listed in the confusion matrix, which are listed hereinafter. (x) Positive Prediction Negative Prediction Actual Positive tp (true positive) fn (false negative) Actual Negative fp (false positive) tn (true negative) Table 3 summarizes eight such metrics. The metrics described in the first two lines measure the ability of a classifier to correctly classify positive and negative examples, while the metrics described in the last two lines measure the effectiveness of predictions made by a classifier. For example, the positive prediction value (PPV) or the precision of a classifier that measures the proportion of positive predictions correctly classified. (x) rows of Table 3 are used in this article to evaluate the second groups of measurements, such as the classifications induced by the training groups."}, {"heading": "5. Learning from Unbalanced Data Sets", "text": "We will now analyze the classifiers that emerge from the twenty-six naturally unbalanced data sets described in Table 2, focusing on the performance differences for the minority and majority classes. We will not change the class distribution of the training data in this section, so the classifiers do not need to be adjusted using the method described in Section 3. However, for these experiments to be consistent with those in Section 6 that use the natural distribution, the size of the training set will be reduced as described in Section 4.1. Before addressing these differences, it is important to discuss a problem that, if left untreated, can lead to confusion. Practitioners have found that learning performance in learning from data sets in which the minority class is significantly underrepresented is often insufficient. In particular, they observe that there is a large error rate for the minority class. As should be clear from Table 3 and the related discussion, there are two distinct perceptions of \"the minority error rate\" (the high pre-healing class may indicate a high PH)."}, {"heading": "5.1 Experimental Results", "text": "The performance of the classifiers resulting from the twenty-six unbalanced data sets is described in Table 4, while this table deserves an explanation; the first column specifies the name of the data set, while the second column, copied from Table 2 for convenience, indicates the percentage of minority class error examples in the natural class distribution; by comparing the values in columns two and three, we see that in all cases a disproportionate percentage of errors originate from minority class examples; and the third column specifies the percentage of test errors that can be assigned to the test examples, which comprise only 3.9% of the character sets but contribute 58.3% of the errors."}, {"heading": "5.2 Discussion", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "6. The Effect of Training-Set Class Distribution on Classifier Performance", "text": "Turning now to the central questions of our study: How do different class distributions affect the performance of the induced classifiers and which class distributions lead to the best classifiers? We begin by describing the methodology for determining which class distribution performs best. Then, in the next two sections, we evaluate and analyze the performance of the classifiers for the twenty-six sets against a variety of class distributions. We use the error rate as a measure of performance in Section 6.2 and the AUC as a measure of performance in Section 6.3."}, {"heading": "6.1 Methodology for Determining the Optimum Training Class Distribution(s)", "text": "We evaluate the following twelve class distributions (expressed as a percentage of class distributions): 2%, 5%, 20%, 30%, 50%, 60%, 80% and 95%. Before trying to determine the \"best\" class distribution for a training set, there are several problems that need to be addressed. First, because we cannot evaluate any class distribution, we can determine the best distribution among the 13 evaluated distributions."}, {"heading": "6.2 The Relationship between Class Distribution and Classification Error Rate", "text": "In fact, most of them are able to trump themselves, and they are able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are not able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...)"}, {"heading": "6.3 The Relationship between Class Distribution and AUC", "text": "In fact, the fact is that most of them are able to assert themselves, that they are able to assert themselves, and that they are able to assert themselves."}, {"heading": "7. Forming a Good Class Distribution with Sensitivity to Procurement Costs", "text": "The results from the previous section show that some marginal class distributions produce classifiers that perform much better than the classifiers produced by other training distributions. Unfortunately, the formation of all thirteen training groups of size n, each with different class distribution, requires almost two examples. If it is costly to obtain training examples in a form suitable for learning, this approach is self-destructive. Ideally, if a budget allows n training examples, a total of n training examples would be selected that would all be used in the final training set - and the associated class distribution would produce classifiers that perform better than those generated from any other class distribution (specified n training examples). In this section, we describe and evaluate a heuristic, budget-sensitive, progressive sampling algorithm that comes closer to this ideal thesis. To evaluate this progressive sampling algorithm, it is necessary to measure how class distribution affects a variety of classifications."}, {"heading": "7.1 The Effect of Class Distribution and Training-Set Size on Classifier Performance", "text": "To ensure that the training sets contain only a sufficient number of training sets to provide meaningful results when the training set size is drastically reduced, only those sets of data that provide relatively large training sets are selected (however, this is determined based on the size of the data set and the percentage of minority examples in the data set).The detailed results associated with these experiments are contained in Appendix B. The results for one of these sets, the adult data sets, are graphically shown in Figure 4 and Figure 5, classified performance with error rate and AUC, respectively. Each of the nine performance curves in these numbers is associated with a specific training set size that contains between 1 / 128 and the available training data."}, {"heading": "7.2 A Budget-Sensitive Progressive sampling Algorithm for Selecting Training Data", "text": "For the sake of simplicity, we assume that there is a budget n that allows us to obtain exactly n training examples. Furthermore, we assume that the number of training examples that can potentially be obtained is sufficiently large to form a training set of size n with any desired marginal class distribution. We would like to apply a sampling strategy that selects x minority class examples and y majority class examples where x + y = n so that the best possible class distribution yields the best possible classification performance for a training set of size. Sampling strategy is based on several assumptions. We assume that the cost of executing the learning algorithm is compared to the cost of obtaining examples so that the learning algorithms can be run multiple times. This will certainly be true if the training data is costly."}, {"heading": "7.3 Results for the Sampling Algorithm", "text": "The budget-sensitive progressive sampling algorithm was applied to the phone, Adults, Covertype, kr-vs-kp, weather, letter-a and blackjack data sets with both error rate and AUC to measure classifier performance. However, the method of setting the bar (described in lines 6-12 in Table 7) is modified so that the results from the experiments described in Section 7.1 (and detailed in Appendix B), which evaluate only the 13 class distributions listed, could be used so that in each iteration the low end (high end) of the bar is set to the class distribution specified in Appendix B, which is just below (above) the best class distribution from the previous iteration. For example, if in one iteration the best performance class distribution contains 30% minority class examples, then in the next iteration we will include the bottom of the bar containing the 20% minority class examples at the top and the preceding iteration."}, {"heading": "8. Related Work", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they"}, {"heading": "9. Limitations and Future Research", "text": "One limitation of the research described in this article is that, because all results are based on the use of a decision tree that is used only for the distribution of data, our conclusions may only apply to this class of learners. However, there are reasons to believe that our conclusions will apply to other learners as well. Namely, since the role that class distribution methods play in learning - and the reasons discussed in Section 5.2 why a classifier performs worse in the minority class - are not specific to decision tree learners, one would expect other learners to behave similarly. However, one class of learners that particularly merits further attention are those learners who do not form disjunctive concepts. These learners will not suffer in the same way from the \"problem of small disjuncts,\" which our results indicate that predictions are partially responsible for minority classes that have a higher error rate than majority class decisions. [8] It would be helpful to extend this study to include other classes of learners in order to extend other classes."}, {"heading": "10. Conclusion", "text": "This article deals with the question to what extent distributional justice between the sexes can be brought to the fore, and to what extent distributional justice between the sexes must be brought to the fore."}, {"heading": "Acknowledgments", "text": "We would like to thank Haym Hirsh for the comments and feedback he provided during this research, and we would also like to thank the anonymous reviewers for helpful comments and IBM for a faculty partnership award."}, {"heading": "Appendix A: Impact of Class Distribution Correction on Classifier Performance", "text": "In this context, it should be noted that this is a very complex and complex matter."}, {"heading": "Appendix B: The Effect of Training-Set Size and Class Distribution on Learning", "text": "For these experiments, the sizes of the training sessions are varied so that they contain the following fractions of the total training data available: 1 / 128, 1 / 64, 1 / 32, 1 / 16, 1 / 2, 2, 3, 4, 5, 5, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7"}, {"heading": "Appendix C: Detailed Results for the Budget-Sensitive Sampling Algorithm", "text": "This appendix describes the execution of the progressive sampling algorithm described in Table 7. Execution of the algorithm is evaluated on the basis of the detailed results from Appendix B. First, Table C1 presents a detailed iteration-by-iteration description of the sampling algorithm as applied to examples of minority examples to measure the performance of minorities. Table C2 then provides a more compact version of this description by showing only the key variables as they change from iteration to iteration. Finally, in Table C3a and Table C3b this compact description is used to describe the execution of the sampling algorithm for the phone, adults, cover type, kr-vs-kp, weather and blackjack datasets, using both error rate and AUC to measure performance. Note that for each of these tables the column labeled \"budget\" will be used to describe the execution of the case - which means that none of the examples is used in the budget - or that none of the results will be used."}], "references": [{"title": "An empirical comparison of voting classification algorithms: bagging, boosting, and variants", "author": ["E. Bauer", "R. Kohavi"], "venue": "Machine Learning,", "citeRegEx": "Bauer and Kohavi,? \\Q1999\\E", "shortCiteRegEx": "Bauer and Kohavi", "year": 1999}, {"title": "Classification and Regression Trees", "author": ["L. Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": null, "citeRegEx": "Breiman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "UCI Repository of Machine Learning Databases, (http://www.ics.uci.edu/~mlearn/MLRepository.html), Department of Computer Science, University of California", "author": ["C. Blake", "C. Merz"], "venue": null, "citeRegEx": "Blake and Merz,? \\Q1998\\E", "shortCiteRegEx": "Blake and Merz", "year": 1998}, {"title": "The use of the area under the ROC curve in the evaluation of machine learning algorithms", "author": ["A. Bradley"], "venue": "Pattern Recognition, 30(7), 1145-1159.", "citeRegEx": "Bradley,? 1997", "shortCiteRegEx": "Bradley", "year": 1997}, {"title": "Pruning decision trees with misclassification costs", "author": ["J.P. Bradford", "C. Kunz", "R. Kohavi", "C. Brunk", "C.E. Brodley"], "venue": "In Proceedings of the European Conference on Machine Learning,", "citeRegEx": "Bradford et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bradford et al\\.", "year": 1998}, {"title": "Megainduction: machine learning on very large databases", "author": ["J. Catlett"], "venue": "Ph.D. thesis, Department of Computer Science, University of Sydney.", "citeRegEx": "Catlett,? 1991", "shortCiteRegEx": "Catlett", "year": 1991}, {"title": "Toward scalable learning with non-uniform class and cost distributions: a case study in credit card fraud detection", "author": ["P. Chan", "S. Stolfo"], "venue": "In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Chan and Stolfo,? \\Q1998\\E", "shortCiteRegEx": "Chan and Stolfo", "year": 1998}, {"title": "SMOTE: synthetic minority over-sampling technique", "author": ["N. Chawla", "K. Bowyer", "L. Hall", "W.P. Kegelmeyer"], "venue": "In International Conference on Knowledge Based Computer Systems", "citeRegEx": "Chawla et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Chawla et al\\.", "year": 2000}, {"title": "A simple, fast, and effective rule learner", "author": ["W. Cohen", "Y. Singer"], "venue": "In Proceedings of the Sixteenth National Conference on Artificial Intelligence,", "citeRegEx": "Cohen and Singer,? \\Q1999\\E", "shortCiteRegEx": "Cohen and Singer", "year": 1999}, {"title": "Improved generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Machine Learning, 15:201-221.", "citeRegEx": "Cohn et al\\.,? 1994", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Exploiting the cost (in)sensitivity of decision tree splitting criteria", "author": ["C. Drummond", "R.C. Holte"], "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning,", "citeRegEx": "Drummond and Holte,? \\Q2000\\E", "shortCiteRegEx": "Drummond and Holte", "year": 2000}, {"title": "The foundations of cost-sensitive learning", "author": ["C. Elkan"], "venue": "Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence, pp. 973-978.", "citeRegEx": "Elkan,? 2001", "shortCiteRegEx": "Elkan", "year": 2001}, {"title": "A Mixture-of-Experts Framework for ConceptLearning from Imbalanced Data Sets", "author": ["A. Estabrooks", "N. Japkowicz"], "venue": "In Proceedings of the 2001 Intelligent Data Analysis Conference", "citeRegEx": "Estabrooks and Japkowicz,? \\Q2001\\E", "shortCiteRegEx": "Estabrooks and Japkowicz", "year": 2001}, {"title": "Adaptive Fraud Detection", "author": ["T. Fawcett", "F. Provost"], "venue": "Data Mining and Knowledge Discovery", "citeRegEx": "Fawcett and Provost,? \\Q1997\\E", "shortCiteRegEx": "Fawcett and Provost", "year": 1997}, {"title": "The Estimation of Probabilities", "author": ["I.J. Good"], "venue": "Cambridge, MA: M.I.T. Press.", "citeRegEx": "Good,? 1965", "shortCiteRegEx": "Good", "year": 1965}, {"title": "Construction and Assessment of Classification Rules", "author": ["D.J. Hand"], "venue": "Chichester, UK: John Wiley and Sons.", "citeRegEx": "Hand,? 1997", "shortCiteRegEx": "Hand", "year": 1997}, {"title": "Concept learning and the problem of small disjuncts", "author": ["R.C. Holte", "L.E. Acker", "B.W. Porter"], "venue": "In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence,", "citeRegEx": "Holte et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Holte et al\\.", "year": 1989}, {"title": "The Class Imbalance Problem: A Systematic Study", "author": ["N. Japkowicz", "S. Stephen"], "venue": "Intelligent Data Analysis Journal,", "citeRegEx": "Japkowicz and Stephen,? \\Q2002\\E", "shortCiteRegEx": "Japkowicz and Stephen", "year": 2002}, {"title": "In Papers from the AAAI Workshop on Learning from Imbalanced Data Sets", "author": ["N. Japkowicz", "R.C. Holte", "C.X. Ling", "Matwin S"], "venue": "Tech, rep. WS-00-05,", "citeRegEx": "Japkowicz et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Japkowicz et al\\.", "year": 2000}, {"title": "Multiple comparisons in induction algorithms", "author": ["D.D. Jensen", "P.R. Cohen"], "venue": "Machine Learning,", "citeRegEx": "Jensen and Cohen,? \\Q2000\\E", "shortCiteRegEx": "Jensen and Cohen", "year": 2000}, {"title": "Static versus dynamic sampling for data mining", "author": ["G.H. John", "P. Langley"], "venue": "In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "John and Langley,? \\Q1996\\E", "shortCiteRegEx": "John and Langley", "year": 1996}, {"title": "Addressing the curse of imbalanced training sets: one-sided selection", "author": ["M. Kubat", "S. Matwin"], "venue": "In Proceedings of the Fourteenth International Conference on Machine Learning,", "citeRegEx": "Kubat and Matwin,? \\Q1997\\E", "shortCiteRegEx": "Kubat and Matwin", "year": 1997}, {"title": "Heterogeneous uncertainty sampling for supervised learning", "author": ["D.D. Lewis", "J. Catlett"], "venue": "In Proceedings of the Eleventh International Conference on Machine Learning,", "citeRegEx": "Lewis and Catlett,? \\Q1994\\E", "shortCiteRegEx": "Lewis and Catlett", "year": 1994}, {"title": "The case against accuracy estimation for comparing classifiers", "author": ["F. Provost", "T. Fawcett", "R. Kohavi"], "venue": "In Proceedings of the Fifteenth International Conference on Machine Learning", "citeRegEx": "Provost et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Provost et al\\.", "year": 1998}, {"title": "Efficient progressive sampling", "author": ["F. Provost", "D. Jensen", "T. Oates"], "venue": "In Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining. ACM Press", "citeRegEx": "Provost et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Provost et al\\.", "year": 1999}, {"title": "Robust classification for imprecise environments", "author": ["F. Provost", "T Fawcett"], "venue": "Machine Learning,", "citeRegEx": "Provost and Fawcett,? \\Q2001\\E", "shortCiteRegEx": "Provost and Fawcett", "year": 2001}, {"title": "Well-trained PETs: improving probability estimation trees", "author": ["F. Provost", "P. Domingos"], "venue": "CeDER Working Paper #IS-00-04, Stern School of Business,", "citeRegEx": "Provost and Domingos,? \\Q2001\\E", "shortCiteRegEx": "Provost and Domingos", "year": 2001}, {"title": "C4.5: Programs for Machine Learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "Quinlan,? \\Q1993\\E", "shortCiteRegEx": "Quinlan", "year": 1993}, {"title": "Active learning for class probability estimation and ranking", "author": ["M. Saar-Tsechansky", "F. Provost"], "venue": "In Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Saar.Tsechansky and Provost,? \\Q2001\\E", "shortCiteRegEx": "Saar.Tsechansky and Provost", "year": 2001}, {"title": "Active Sampling for Class Probability Estimation and Ranking", "author": ["M. Saar-Tsechansky", "F. Provost"], "venue": "To appear in Machine Learning.", "citeRegEx": "Saar.Tsechansky and Provost,? 2003", "shortCiteRegEx": "Saar.Tsechansky and Provost", "year": 2003}, {"title": "Getting Started With SAS Enterprise Miner", "author": ["SAS Institute"], "venue": "Cary, NC: SAS Institute Inc.", "citeRegEx": "Institute,? 2001", "shortCiteRegEx": "Institute", "year": 2001}, {"title": "Adjusting the outputs of a classifier to new a priori probabilities: a simple procedure", "author": ["M. Saerens", "P. Latinne", "C. Decaestecker"], "venue": "Neural Computation,", "citeRegEx": "Saerens et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Saerens et al\\.", "year": 2002}, {"title": "Better decisions through science", "author": ["J. Swets", "R. Dawes", "J. Monahan"], "venue": "Scientific American,", "citeRegEx": "Swets et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Swets et al\\.", "year": 2000}, {"title": "Types of cost in inductive learning", "author": ["P. Turney"], "venue": "Workshop on Cost-Sensitive Learning at the Seventeenth International Conference on Machine Learning, 15-21, Stanford, CA.", "citeRegEx": "Turney,? 2000", "shortCiteRegEx": "Turney", "year": 2000}, {"title": "When small disjuncts abound, try lazy learning: a case study", "author": ["Van den Bosch A", "A. Weijters", "H.J. Van den Herik", "W. Daelemans"], "venue": "In Proceedings of the Seventh BelgianDutch Conference on Machine Learning,", "citeRegEx": "A. et al\\.,? \\Q1997\\E", "shortCiteRegEx": "A. et al\\.", "year": 1997}, {"title": "A quantitative study of small disjuncts", "author": ["G.M. Weiss", "H. Hirsh"], "venue": "In Proceedings of the Seventeenth National Conference on Artificial Intelligence,", "citeRegEx": "Weiss and Hirsh,? \\Q2000\\E", "shortCiteRegEx": "Weiss and Hirsh", "year": 2000}, {"title": "The effect of class distribution on classifier learning: an empirical study", "author": ["G.M. Weiss", "F Provost"], "venue": "Tech rep. ML-TR-44,", "citeRegEx": "Weiss and Provost,? \\Q2001\\E", "shortCiteRegEx": "Weiss and Provost", "year": 2001}, {"title": "Learning and making decisions when costs and probabilities are both unknown", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Tech. rep. CS2001-0664,", "citeRegEx": "Zadrozny and Elkan,? \\Q2001\\E", "shortCiteRegEx": "Zadrozny and Elkan", "year": 2001}], "referenceMentions": [{"referenceID": 33, "context": "These costs include the cost of obtaining the raw data, cleaning the data, storing the data, and transforming the data into a representation suitable for learning, as well as the cost of computer hardware, the cost associated with the time it takes to learn from the data, and the \u201copportunity cost\u201d associated with suboptimal learning from extremely large data sets due to limited computational resources (Turney, 2000).", "startOffset": 406, "endOffset": 420}, {"referenceID": 11, "context": "Elkan (2001) presents an equivalent method for adjusting the posterior probabilities, including a formal derivation.", "startOffset": 0, "endOffset": 13}, {"referenceID": 14, "context": "We consider a version based on the Laplace law of succession (Good, 1965).", "startOffset": 61, "endOffset": 73}, {"referenceID": 31, "context": "In some real-world situations this is not true and different methods are required to compensate for changes to the training set (Provost & Fawcett, 2001; Saerens et al., 2002).", "startOffset": 128, "endOffset": 175}, {"referenceID": 5, "context": "Previous work on modifying the class distribution of the training set (Catlett, 1991; Chan & Stolfo, 1998; Japkowicz, 2002) did not take these differences into account and this undoubtedly affected the results.", "startOffset": 70, "endOffset": 123}, {"referenceID": 10, "context": "Elkan (2001) provides a more complex, but equivalent, formula that uses fractions instead of ratios.", "startOffset": 0, "endOffset": 13}, {"referenceID": 27, "context": "5, a program for inducing classification trees from labeled examples (Quinlan, 1993).", "startOffset": 69, "endOffset": 84}, {"referenceID": 4, "context": "This decision is supported by recent research, which indicates that when target misclassification costs (or class distributions) are unknown then standard pruning does not improve, and may degrade, generalization performance (Provost & Domingos, 2001; Zadrozny & Elkan, 2001; Bradford et al., 1998; Bauer & Kohavi, 1999).", "startOffset": 225, "endOffset": 320}, {"referenceID": 4, "context": "This decision is supported by recent research, which indicates that when target misclassification costs (or class distributions) are unknown then standard pruning does not improve, and may degrade, generalization performance (Provost & Domingos, 2001; Zadrozny & Elkan, 2001; Bradford et al., 1998; Bauer & Kohavi, 1999). Indeed, Bradford et al. (1998) found that even if the pruning strategy is adapted to take misclassification costs and class distribution into account, this does not generally improve the performance of the classifier.", "startOffset": 276, "endOffset": 353}, {"referenceID": 3, "context": "We use two performance measures to gauge the overall performance of a classifier: classification accuracy and the area under the ROC curve (Bradley, 1997).", "startOffset": 139, "endOffset": 154}, {"referenceID": 23, "context": "These assumptions are unrealistic in many domains (Provost et al., 1998).", "startOffset": 50, "endOffset": 72}, {"referenceID": 32, "context": "In such cases, Receiver Operating Characteristic (ROC) analysis is more appropriate (Swets et al., 2000; Bradley, 1997; Provost & Fawcett, 2001).", "startOffset": 84, "endOffset": 144}, {"referenceID": 3, "context": "In such cases, Receiver Operating Characteristic (ROC) analysis is more appropriate (Swets et al., 2000; Bradley, 1997; Provost & Fawcett, 2001).", "startOffset": 84, "endOffset": 144}, {"referenceID": 15, "context": "To assess the overall quality of a classifier we measure the fraction of the total area that falls under the ROC curve (AUC), which is equivalent to several other statistical measures for evaluating classification and ranking models (Hand, 1997).", "startOffset": 233, "endOffset": 245}, {"referenceID": 5, "context": "Both Catlett (1991) and Chan & Stolfo (1998) study the relationship between (marginal) training class distribution and classifier performance when the training-set size is held fixed, but focus most of their attention on other issues.", "startOffset": 5, "endOffset": 20}, {"referenceID": 5, "context": "Both Catlett (1991) and Chan & Stolfo (1998) study the relationship between (marginal) training class distribution and classifier performance when the training-set size is held fixed, but focus most of their attention on other issues.", "startOffset": 5, "endOffset": 45}, {"referenceID": 5, "context": "Both Catlett (1991) and Chan & Stolfo (1998) study the relationship between (marginal) training class distribution and classifier performance when the training-set size is held fixed, but focus most of their attention on other issues. These studies also analyze only a few data sets, which makes it impossible to draw general conclusions about the relationship between class distribution and classifier performance. Nonetheless, based on the results for three data sets, Chan & Stolfo (1998) show that when accuracy is the performance metric, a training set that uses the natural class distribution yields the best results.", "startOffset": 5, "endOffset": 492}, {"referenceID": 5, "context": "Both Catlett (1991) and Chan & Stolfo (1998) study the relationship between (marginal) training class distribution and classifier performance when the training-set size is held fixed, but focus most of their attention on other issues. These studies also analyze only a few data sets, which makes it impossible to draw general conclusions about the relationship between class distribution and classifier performance. Nonetheless, based on the results for three data sets, Chan & Stolfo (1998) show that when accuracy is the performance metric, a training set that uses the natural class distribution yields the best results. These results agree partially with our results\u2014although we show that the natural distribution does not always maximize accuracy, we show that the optimal distribution generally is close to the natural distribution. Chan & Stolfo also show that when actual costs are factored in (i.e., the cost of a false positive is not the same as a false negative), the natural distribution does not perform best; rather a training distribution closer to a balanced distribution performs best. They also observe, as we did, that by increasing the percentage of minority-class examples in the training set, the induced classifier performs better at classifying minority examples. It is important to note, however, that neither Chan & Stolfo nor Catlett adjusted the induced classifiers to compensate for changes made to the class distribution of the training set. This means that their results are biased in favor of the natural distribution (when measuring classification accuracy) and that they could improve the classification performance of minority class examples simply by changing (implicitly) the decision threshold. As the results in Appendix A show, compensating for the changed class distribution can affect the performance of a classifier significantly. Several researchers have looked at the general question of how to reduce the need for labeled training data by selecting the data intelligently, but without explicitly considering the class distribution. For example, Cohn et al. (1994) and Lewis and Catlett (1994) use \u201cactive learning\u201d to add examples to the training set for which the classifier is least certain about the classification.", "startOffset": 5, "endOffset": 2111}, {"referenceID": 5, "context": "Both Catlett (1991) and Chan & Stolfo (1998) study the relationship between (marginal) training class distribution and classifier performance when the training-set size is held fixed, but focus most of their attention on other issues. These studies also analyze only a few data sets, which makes it impossible to draw general conclusions about the relationship between class distribution and classifier performance. Nonetheless, based on the results for three data sets, Chan & Stolfo (1998) show that when accuracy is the performance metric, a training set that uses the natural class distribution yields the best results. These results agree partially with our results\u2014although we show that the natural distribution does not always maximize accuracy, we show that the optimal distribution generally is close to the natural distribution. Chan & Stolfo also show that when actual costs are factored in (i.e., the cost of a false positive is not the same as a false negative), the natural distribution does not perform best; rather a training distribution closer to a balanced distribution performs best. They also observe, as we did, that by increasing the percentage of minority-class examples in the training set, the induced classifier performs better at classifying minority examples. It is important to note, however, that neither Chan & Stolfo nor Catlett adjusted the induced classifiers to compensate for changes made to the class distribution of the training set. This means that their results are biased in favor of the natural distribution (when measuring classification accuracy) and that they could improve the classification performance of minority class examples simply by changing (implicitly) the decision threshold. As the results in Appendix A show, compensating for the changed class distribution can affect the performance of a classifier significantly. Several researchers have looked at the general question of how to reduce the need for labeled training data by selecting the data intelligently, but without explicitly considering the class distribution. For example, Cohn et al. (1994) and Lewis and Catlett (1994) use \u201cactive learning\u201d to add examples to the training set for which the classifier is least certain about the classification.", "startOffset": 5, "endOffset": 2140}, {"referenceID": 5, "context": "Both Catlett (1991) and Chan & Stolfo (1998) study the relationship between (marginal) training class distribution and classifier performance when the training-set size is held fixed, but focus most of their attention on other issues. These studies also analyze only a few data sets, which makes it impossible to draw general conclusions about the relationship between class distribution and classifier performance. Nonetheless, based on the results for three data sets, Chan & Stolfo (1998) show that when accuracy is the performance metric, a training set that uses the natural class distribution yields the best results. These results agree partially with our results\u2014although we show that the natural distribution does not always maximize accuracy, we show that the optimal distribution generally is close to the natural distribution. Chan & Stolfo also show that when actual costs are factored in (i.e., the cost of a false positive is not the same as a false negative), the natural distribution does not perform best; rather a training distribution closer to a balanced distribution performs best. They also observe, as we did, that by increasing the percentage of minority-class examples in the training set, the induced classifier performs better at classifying minority examples. It is important to note, however, that neither Chan & Stolfo nor Catlett adjusted the induced classifiers to compensate for changes made to the class distribution of the training set. This means that their results are biased in favor of the natural distribution (when measuring classification accuracy) and that they could improve the classification performance of minority class examples simply by changing (implicitly) the decision threshold. As the results in Appendix A show, compensating for the changed class distribution can affect the performance of a classifier significantly. Several researchers have looked at the general question of how to reduce the need for labeled training data by selecting the data intelligently, but without explicitly considering the class distribution. For example, Cohn et al. (1994) and Lewis and Catlett (1994) use \u201cactive learning\u201d to add examples to the training set for which the classifier is least certain about the classification. Saar-Tsechansky and Provost (2001, 2003) provide an overview of such methods and also extend them to cover AUC and other non-accuracy based performance metrics. The setting where these methods are applicable is different from the setting we consider. In particular, these methods assume either that arbitrary examples can be labeled or that the descriptions of a pool of unlabeled examples are available and the critical cost is associated with labeling them (so the algorithms select the examples intelligently rather than randomly). In our typical setting, the cost is in procuring the descriptions of the examples\u2014the labels are known beforehand. There also has been some prior work on progressive sampling strategies. John and Langley (1996) show how one can use the extrapolation of learning curves to determine when classifier performance using a subset of available training data comes close to the performance that would be achieved by using the full data set.", "startOffset": 5, "endOffset": 3012}, {"referenceID": 5, "context": "Both Catlett (1991) and Chan & Stolfo (1998) study the relationship between (marginal) training class distribution and classifier performance when the training-set size is held fixed, but focus most of their attention on other issues. These studies also analyze only a few data sets, which makes it impossible to draw general conclusions about the relationship between class distribution and classifier performance. Nonetheless, based on the results for three data sets, Chan & Stolfo (1998) show that when accuracy is the performance metric, a training set that uses the natural class distribution yields the best results. These results agree partially with our results\u2014although we show that the natural distribution does not always maximize accuracy, we show that the optimal distribution generally is close to the natural distribution. Chan & Stolfo also show that when actual costs are factored in (i.e., the cost of a false positive is not the same as a false negative), the natural distribution does not perform best; rather a training distribution closer to a balanced distribution performs best. They also observe, as we did, that by increasing the percentage of minority-class examples in the training set, the induced classifier performs better at classifying minority examples. It is important to note, however, that neither Chan & Stolfo nor Catlett adjusted the induced classifiers to compensate for changes made to the class distribution of the training set. This means that their results are biased in favor of the natural distribution (when measuring classification accuracy) and that they could improve the classification performance of minority class examples simply by changing (implicitly) the decision threshold. As the results in Appendix A show, compensating for the changed class distribution can affect the performance of a classifier significantly. Several researchers have looked at the general question of how to reduce the need for labeled training data by selecting the data intelligently, but without explicitly considering the class distribution. For example, Cohn et al. (1994) and Lewis and Catlett (1994) use \u201cactive learning\u201d to add examples to the training set for which the classifier is least certain about the classification. Saar-Tsechansky and Provost (2001, 2003) provide an overview of such methods and also extend them to cover AUC and other non-accuracy based performance metrics. The setting where these methods are applicable is different from the setting we consider. In particular, these methods assume either that arbitrary examples can be labeled or that the descriptions of a pool of unlabeled examples are available and the critical cost is associated with labeling them (so the algorithms select the examples intelligently rather than randomly). In our typical setting, the cost is in procuring the descriptions of the examples\u2014the labels are known beforehand. There also has been some prior work on progressive sampling strategies. John and Langley (1996) show how one can use the extrapolation of learning curves to determine when classifier performance using a subset of available training data comes close to the performance that would be achieved by using the full data set. Provost et al. (1999) suggest using a geometric sampling", "startOffset": 5, "endOffset": 3257}, {"referenceID": 18, "context": "There is a considerable amount of research on how to build \u201cgood\u201d classifiers when the class distribution of the data is highly unbalanced and it is costly to misclassify minority-class examples (Japkowicz et al., 2000).", "startOffset": 195, "endOffset": 219}, {"referenceID": 15, "context": "The techniques described by John and Langley (1996) and Provost et al.", "startOffset": 28, "endOffset": 52}, {"referenceID": 15, "context": "The techniques described by John and Langley (1996) and Provost et al. (1999) do not change the distribution of examples in the training set, but rather rely on taking random samples from the available training data.", "startOffset": 28, "endOffset": 78}, {"referenceID": 10, "context": "Estabrooks and Japkowicz (2001) address this issue by showing that a mixture-of-experts approach, which combines classifiers built using under-sampling and over-sampling methods with various sampling rates, can produce consistently good results.", "startOffset": 0, "endOffset": 32}, {"referenceID": 10, "context": "Estabrooks and Japkowicz (2001) address this issue by showing that a mixture-of-experts approach, which combines classifiers built using under-sampling and over-sampling methods with various sampling rates, can produce consistently good results. Both under-sampling and over-sampling have known drawbacks. Under-sampling throws out potentially useful data while over-sampling increases the size of the training set and hence the time to build a classifier. Furthermore, since most over-sampling methods make exact copies of minority class examples, overfitting is likely to occur\u2014classification rules may be induced to cover a single replicated example. Recent research has focused on improving these basic methods. Kubat and Matwin (1997) employ an under-sampling strategy that intelligently removes majority examples by removing only those majority examples that are \u201credundant\u201d or that \u201cborder\u201d the minority examples\u2014figuring they may be the result of noise.", "startOffset": 0, "endOffset": 740}, {"referenceID": 6, "context": "Chawla et al. (2000) combine under-sampling and over-sampling methods, and, to avoid the overfitting problem, form new minority class examples by interpolating between minority-class examples that lie close together.", "startOffset": 0, "endOffset": 21}, {"referenceID": 6, "context": "Chan and Stolfo (1998) take a somewhat different, and innovative, approach.", "startOffset": 0, "endOffset": 23}, {"referenceID": 1, "context": "Reducing the class imbalance in the training set effectively causes the learner to impose a greater cost for misclassifying minority-class examples (Breiman et al., 1984).", "startOffset": 148, "endOffset": 170}, {"referenceID": 18, "context": "For example, Kubat and Matwin (1997) motivate the use of under-sampling to handle skewed data sets by saying that \u201cadding examples of the majority class to the training set can have a detrimental effect on the learner\u2019s behavior: noisy or otherwise unreliable examples from the majority class can overwhelm the minority class\u201d (p.", "startOffset": 13, "endOffset": 37}, {"referenceID": 1, "context": "Reducing the class imbalance in the training set effectively causes the learner to impose a greater cost for misclassifying minority-class examples (Breiman et al., 1984). Thus, when the cost of acquiring and learning from the data is not an issue, cost-sensitive or probabilistic learning methods are a more direct and arguably more appropriate way of dealing with class imbalance, because they do not have the problems, noted earlier, that are associated with under-sampling and over-sampling. Such approaches have been shown to outperform under-sampling and over-sampling (Japkowicz & Stephen, 2002). To quote Drummond and Holte (2000) \u201call of the data available can be used to produce the tree, thus throwing away no information, and learning speed is not degraded due to duplicate instances\u201d (p.", "startOffset": 149, "endOffset": 639}, {"referenceID": 10, "context": "In particular, Drummond and Holte (2000) showed that there are splitting criteria that are completely insensitive to the class distribution and that", "startOffset": 15, "endOffset": 41}, {"referenceID": 4, "context": "Moreover, other research (Bradford et al., 1998) indicates that classifier performance does not generally improve when pruning takes class distribution and costs into account.", "startOffset": 25, "endOffset": 48}, {"referenceID": 5, "context": "We consider this to be particularly significant because previous research on the effect of class distribution on learning has not employed this, or any other, adjustment (Catlett, 1991; Chan & Stolfo, 1998; Japkowicz & Stephen, 2002).", "startOffset": 170, "endOffset": 233}], "year": 2011, "abstractText": "For large, real-world inductive learning problems, the number of training examples often must be limited due to the costs associated with procuring, preparing, and storing the training examples and/or the computational costs associated with learning from them. In such circumstances, one question of practical importance is: if only n training examples can be selected, in what proportion should the classes be represented? In this article we help to answer this question by analyzing, for a fixed training-set size, the relationship between the class distribution of the training data and the performance of classification trees induced from these data. We study twenty-six data sets and, for each, determine the best class distribution for learning. The naturally occurring class distribution is shown to generally perform well when classifier performance is evaluated using undifferentiated error rate (0/1 loss). However, when the area under the ROC curve is used to evaluate classifier performance, a balanced distribution is shown to perform well. Since neither of these choices for class distribution always generates the best-performing classifier, we introduce a \u201cbudget-sensitive\u201d progressive sampling algorithm for selecting training examples based on the class associated with each example. An empirical analysis of this algorithm shows that the class distribution of the resulting training set yields classifiers with good (nearly-optimal) classification performance.", "creator": "PScript5.dll Version 5.2"}}}