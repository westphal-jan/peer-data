{"id": "1106.4572", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Jun-2011", "title": "Specific-to-General Learning for Temporal Events with Application to Learning Event Definitions from Video", "abstract": "We develop, analyze, and evaluate a novel, supervised, specific-to-general learner for a simple temporal logic and use the resulting algorithm to learn visual event definitions from video sequences. First, we introduce a simple, propositional, temporal, event-description language called AMA that is sufficiently expressive to represent many events yet sufficiently restrictive to support learning. We then give algorithms, along with lower and upper complexity bounds, for the subsumption and generalization problems for AMA formulas. We present a positive-examples--only specific-to-general learning method based on these algorithms. We also present a polynomial-time--computable ``syntactic'' subsumption test that implies semantic subsumption without being equivalent to it. A generalization algorithm based on syntactic subsumption can be used in place of semantic generalization to improve the asymptotic complexity of the resulting learning algorithm. Finally, we apply this algorithm to the task of learning relational event definitions from video and show that it yields definitions that are competitive with hand-coded ones.", "histories": [["v1", "Wed, 22 Jun 2011 20:58:18 GMT  (3491kb)", "http://arxiv.org/abs/1106.4572v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["a fern", "r givan", "j m siskind"], "accepted": false, "id": "1106.4572"}, "pdf": {"name": "1106.4572.pdf", "metadata": {"source": "CRF", "title": "Specific-to-General Learning for Temporal Events with Application to Learning Event Definitions from Video", "authors": ["Alan Fern", "Robert Givan", "Jeffrey Mark Siskind"], "emails": ["AFERN@PURDUE.EDU", "GIVAN@PURDUE.EDU", "QOBI@PURDUE.EDU"], "sections": [{"heading": null, "text": "First, we introduce a simple, propositional, temporal description language called AMA that is expressive enough to represent many events but sufficiently restrictive to support learning; then we introduce algorithms, along with smaller and higher complexity limits, for the subsumption and generalization problems of AMA formulas; we present an exclusively positive example learning method based on these algorithms; we also present a polynomial time-calculable \"syntactic\" subsumption test that implies semantic subsumption without equivalence; a generalization algorithm based on syntactic submissions can be used instead of semantic generalizations to improve the asymptotic complexity of the resulting learning algorithm; and finally, we apply this algorithm to the task of learning relational event definitions from videos, showing that it delivers definitions that are hand-coded."}, {"heading": "1. Introduction", "text": "This year, we have reached the point where we see ourselves as being able to live in a country where most people are able to move, and where they are able to unfold, to unfold."}, {"heading": "2. System Overview", "text": "This section provides an overview of our video-based event recognition system, with the goal of giving us an intuitive picture of our system before providing technical details. A formal presentation of our event description language, algorithms, and both theoretical and empirical results appears in Sections 3-6. First, we present the scope of the visual event recognition and LEONARD system, the event recognition system that our learner builds on. Second, we describe how our exclusively positive learner blends into the overall system. Third, we introduce informally into the AMA event description language used by our learner. Finally, we give an informal presentation of the learning algorithm."}, {"heading": "2.1 Recognizing Visual Events", "text": "LEONARD (Siskind, 2001) is a system for detecting visual events from the input of the video camera - an example of a simple visual event is \"a hand picking up a block.\" This research was originally motivated by the problem of adding a learning component to LEONARD - which allows LEONARD to detect an event by looking at sample events of the same type. Below, we give a high-level description of the LEONARD system.LEONARD is a three-step pipeline represented in Figure 1. Raw input consists of a video image sequence representing events. First, a segmentation and tracking component transforms this input into a polygon film: a sequence of frames, each frame being a series of convex polygons placed around the objects being tracked in the video. Figure 2a shows a partial video sequence of a pick-up event representing an event."}, {"heading": "2.2 Adding a Learning Component", "text": "s Event Recognition Library. Here, we add a learning component to LEONARD so that it can learn to recognize events. Figure 1 shows how the event learner fits into the overall system. Input to the event learner consists of force dynamic models from the model reconstruction phase, along with event labels whose output consists of event definitions used by the event detector. We take a supervised learning approach in which the force dynamic model reconstruction process is applied to training videos of a target event. The resulting force dynamic models together with labels specifying the target event type that causes the candidate to define the event type. For example, input to our learners consists of two models matching two videos, with one hand taking a red block from a green block with a green hand label; ICP a green block with a green hand label; and ICP a red hand with a green hand label."}, {"heading": "2.3 The AMA Hypothesis Space", "text": "The complete event logic supported by LEONARD is quite expressive in terms of specifying a variety of temporary patterns (formulas).To support successful learning processes, we use a moreLEARNING TEMPORAL EVENTSFERN, GIVAN, & SISKINDERS logic, called AMA, as hypothesis space. This subset excludes many practically useless formulas that can confuse the learner while maintaining substantial expressivity, thereby representing and learning many useful event types. Our limitation to AMA formulas is a form of syntactic learning, which means that most formulas are constant."}, {"heading": "2.4 Specific-to-General Learning from Positive Data", "text": "Remember that the examples we want to classify and learn from are compulsory dynamic models that can later informally be thought of (and derived from) movies depicting temporal events. Also, remember that our learner issues definitions from the formula formula formula formula formula formula AMA. Given a formula formula formula AMA, we say that it covers an example model if it applies in this model. For a particular event type (such as PICKUP), the ultimate goal is for the learner to issue a formula AMA that covers an example model if and only if the model represents an instance of the target event type. To understand our learner, it is useful to define a general relationship between AMA formulas. We say that the formula AMA is more general (less specific) than the formula LGA 2 when and only when 2 covers each example 1 (and possibly more).11 In our formal analysis we will use two different terms of generality (tactical) and (tactical)."}, {"heading": "2.5 Computing the AMA LGCF", "text": "In order to enhance the legibility of our representation, we shall refrain from presenting examples in which the primitive properties are meaningfully referred to as force dynamic relations. Rather, our examples use abstract sentences such as a and b. In our current application, these sentences correspond exclusively to force dynamic properties, but may not be used for other applications. We shall now show how our system calculates the LGCF of an example model. Consider the following example model: fa [1; 4; b [3; 6; 6; d [1; 3; d [5; 6 g. Here, we take each number (1,.., 6) to represent a time interval of arbitrary (possibly with the number of varying) duration during which nothing changes, and then each fact p [i; j indicates that the proposition p is true throughout the time intervals of positions."}, {"heading": "2.6 Computing the AMA LGG", "text": "We describe our algorithm for calculating the LGG of two AMA formulas at the same time (the LGG of m formulas is not represented at the same time); IG thus provides two figures; IG is always true when this sequence of m 1 pairs of LGG applications can be considered; IG is always true when both timelines can be true at the same time: 1 = (a ^ b ^); (b ^ d); e and 2 = (a ^ e); a; (e ^ d). It is useful to consider the different ways in which both timelines can be true at the same time. To do this, we consider the different ways in which the two timelines can be aligned along a time interval. Figure 4a shows one of the many possible adjustments of these timelines. We call such adjustments interdigitalizations interdigitalizations - in general, there are many interdigitizations, each of which is ordered differently."}, {"heading": "3. Representing Events with AMA", "text": "Here we present a formal representation of the AMA hypotheses space and an analytical development of the algorithms required for specific-general learning processes for AMA. Readers primarily interested in a high-level view of the algorithms and their empirical evaluation may skip sections 3 and 4 and instead proceed directly to sections 5 and 6, where we will discuss several practical extensions of the basic learner and then present our empirical evaluation. We will examine a subset of an interval-based logic called event logic (Siskind, 2001) used by LEONARD for event detection in video sequences, which is interval-based on the explicit repetition of TEMPORAL EVENTS representation of each of the possible interval relationships originally used by Allen (1983) in his calculation of interval relationships, believing that the interval logic of our intervals (e.g. \"overlaps,\" interconnections \"during\" large \"intervals\" may represent)."}, {"heading": "3.1 AMA Syntax and Semantics", "text": "It is natural to describe temporal events by specifying a sequence of properties that must be held over successive time intervals. For example, \"a hand that holds a block\" might \"the block is not supported by the hand and then the block is supported by the hand.\" We represent such sequences with MA timeframes that coincide temporally with the sequence of consecutive conjunctions that are sequences of conjunctive states. An AMA formula is then the conjunction of a number of MA timeframes that represent events that can be considered satisfactory at the same time, and is taken to represent the series of events that coincide temporally with the sequence of consecutive conjunctions. An AMA formula is then the conjunction of a number of MA timeframes that represent events that can each be regarded as a contiguous formula."}, {"heading": "3.2 Motivation for AMA", "text": "MA timelines are a very natural way to capture stretchy sequences of state constraints, but why do we look at linking such sequences, i.e. AMA? We have several reasons for this language enrichment. First, we show below that the AMA is the least general generalization (LGG) unique - this does not apply to MA. Second, and more informally, we argue that parallel conjunctive constraints can be important for learning efficiency. In particular, the space of MA formulas of length k grows exponentially with k, making it difficult to induce long MA formulas. However, finding several shorter MA timelines, each characterizing part of a long sequence of changes, is exponentially simpler. (At least the space in which we seek is exponentially smaller than k.) The AMA conjunction of these timelines places these shorter MA formulas simultaneously and often captures a large part of the concept structure. For this reason, we are analyzing AMA empirically and empirically using AMA in 2001 as well as we are analyzing AMA year 2001."}, {"heading": "3.3 Conversion to First-Order Clauses", "text": "In fact, we are able to go in search of a solution that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution, that is able to find a solution that is capable of finding a solution."}, {"heading": "3.4 Basic Concepts and Properties of AMA", "text": "In naming our results, we use the following convention: \"propositions\" and \"theorems\" are the key results of our work, where theorems are the results with the greatest technical difficulty, and \"lemmas\" are technical results that are needed for the later proofs of propositions or theorems. We count all results in an order, regardless of type. Evidence of theorems and propositions is presented in the main text - omitted proofs of lemmas are provided in the appendix. We provide a pseudo-code for our methods in a non-deterministic style. In a non-deterministic language, functions can return more than one value non-deterministically, either because they contain non-deterministic pickpoints or because they call other non-deterministic functions. Since a non-deterministic function can return more than one possible value, depending on the choices made at the pickpoints encountered, specifying such a function is a natural method of determining a rich structure (if the function has no given arguments or if the arguments actually exist) and if a relationship between the given values (and the actual arguments) exists."}, {"heading": "3.4.1 SUBSUMPTION AND GENERALIZATION FOR STATES", "text": "The most basic formulas we deal with are states (conjunctions of propositions).In our sentence structure, the calculation of subsumption and generalization at the state level is straightforward. A state S1 subsumes S2 (S2 S1) iff S1 is a subset of S2 and considers states as sets of propositions. From this, we deduce that the intersection of states is the least general subsumer of these states and that the community of states is also the most general subsume.LEARNING TEMPORAL EVENTS"}, {"heading": "3.4.2 INTERDIGITATIONS", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "3.4.3 LEAST-GENERAL COVERING FORMULA", "text": "It turns out that AMA formulas can distinguish two models precisely when much richer internal positive event logic (IPEL) formulas can do so. Internal formulas are those that define event events only in terms of properties within the defining interval. That is, satisfaction by hM; Ii only depends on the proposition truth values given by M within interval I. Positive formulas are those that do not contain negation. Appendix A gives the complete syntax and semantics of IPEL (which are only used to indicate and prove Lemma 3. The fact that AMA can discriminate against models as well as IPEL indicates that our restriction to AMA formulas does not include negation. Appendix A gives the complete syntax and semantics of IPEL (which are only used to explain and prove the Lemma 3 model)."}, {"heading": "3.4.4 COMBINING INTERDIGITATION WITH GENERALIZATION OR SPECIALIZATION", "text": "If we connect a series of timelines, each model of the conjunction will trigger an interdigitization of the timelines, which will require simultaneously the sum of the actual propositions in M (x).FERN, GIVAN, & SISKINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINATION and take the size of a model M = hM; Ii to the sum of the actual propositions in M (x).FINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINATION, and INDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINDINATION, and DINDINDINDINDINDINDINDINDINDINDINDINDINDIN"}, {"heading": "4. Subsumption and Generalization", "text": "In this section, we will examine the subsumption and generalization of AMA formulas. First, we will specify a polynomic time algorithm to decide the subsumption between MA formulas, and then show that the critical subsumption for AMA formulas is coNP-complete. Second, we will specify algorithms and complexity limits for constructing formulas of the least general generalization (LGG) based on our FERN, GIVAN, and & SISKINDanalysis of the subsumption, including existence, uniqueness, lower / upper limits, and an algorithm for the LGG on AMA formulas. Third, we will present a polynomial-time-computable syntactic conception of subsumption and an algorithm that calculates the corresponding syntactic LGG that is exponentially faster than our semantic LGG algorithm. Fourth, in Section 4.4, we will show a detailed example of how our LGG algorithms calculate the two semantic and two formulas LGG."}, {"heading": "4.1 Subsumption", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4.2 Least-General Generalization.", "text": "The existence of an AMA LGG is therefore not trivial, since there can be infinite chains of increasingly specific formulas, all of which generalize. Example 2 has shown that such chains can be extended for an MA subsumee and for AMA subsumees. Despite such complications, any member of the chain P; Q; Q; P; Q; P; P; P; Q;:::: covers 1 = (P ^ Q); Q and 2 = P; (P ^ Q). Despite such complications, the AMA LGG exists. Theorem 12. There is an LGG for each finite set of AMA formulas subsummed by all other generalizations. Proof: Let be the set S 02 IS (0)."}, {"heading": "4.3 Syntactic Subsumption and Syntactic Least-General Generalization.", "text": "However, the use of syntactic generality for efficiency is familiar in ILP (Muggleton & De Raedt, 1994), where, for example, subsumption is often used instead of decoupling generality. Unlike AMA, syntactic subsumption is 1 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn 2 syn. There is an MA gap, each of which exists in polynomial time (via theorem 9). AMA syntactic subsumption can be decided in polynomic time."}, {"heading": "4.4 Examples: Least-General Generalization Calculations", "text": "In the following we work through the details of a semantic and a syntactic LGG calculation: We consider the AMA formulas = (A; B) ^ (B; A) and = A; B; A, for which the semantic LGG is A; B; A and the syntactic LGG (A; B; true) ^ (true; B; A).FERN, GIVAN, & SISKIND"}, {"heading": "4.4.1 SEMANTIC LGG EXAMPLE", "text": "The first step in calculating the semantic LGG according to the algorithm given in Figure 8 is to calculate the interdigitization specializations of the input formulas (i.e. IS () and IS ()). Trivially, we have this IS () = = A; B; A. To calculate IS (), we have to consider the possible interdigitizations of, for which there are three, f hA; Bi; hB; Ai gf hA; Bi; hB; Ai gf hA; Bi; hA; hA; Ai; hB; Ai g. Each interdigitization leads to the corresponding member of the IS () by uniting (connecting) the states in each tuple so that IS () is f (A ^ B) (A ^ B); (A ^ B); (A ^ B) g: Lines 5-9 of the semantic LGG () so that the algorithm (A = S) is removed."}, {"heading": "4.4.2 SYNTACTIC LGG EXAMPLE", "text": "The syntactic LGG algorithm shown in Figure 9 calculates a set of semantic LGGs for the MA timeline, returning the conjunction of the results (after truncation). Line 5 of the algorithm cyclizes by timeline tuples from the cross product of the input formulas AMA. In our case, the tuples are in T1 = hA; B; A; Bi and T2 = hA; B; A; B; Ai - for each tupel, the algorithm calculates the semantic LGG formulas. (In our case, the tuples are in tupel timelines.) The semantic LGG calculation for each tupel uses the algorithm shown in Figure 8, but the argument is always a set of MA timelines instead of AMA formulas. For this reason, lines 4- 9 are superfluous, as for a MA timeline 0, IS (0) = 0."}, {"heading": "5. Practical Extensions", "text": "The first is to control exponential complexity by limiting the length of the timelines we are looking at. Second, we describe an often more efficient LGG algorithm, based on a modified algorithm for calculating paired LGGs. Third, we apply our statement algorithm to relational data needed for the scope of visual event detection. Fourth, we insert negation into the AMA language and show how to calculate the corresponding LGCFs and LGGs with our algorithms for AMA (without negation)."}, {"heading": "5.1 k-AMA Least-General Generalization", "text": "We have already pointed out that our syntactic AMA LGG algorithm requires exponential time relative to the lengths of timelines in the AMA input formulas, which motivates limiting the AMA language to k-AMA in practice, where formulas contain timelines with no more than k states. k is increased, the algorithm is able to output increasingly specific formulas at the expense of exponential increase in computing time. However, in the visual event detection experiments that were shown later when we increased k, the resulting formulas became hyperspecific before a computational bottleneck was reached - i.e. for this application, the best values of k were practically predictable and the ability to limit k provided a useful language bias. We use an operator to cover k to limit our syntactical LGG algorithm to k-AMA."}, {"heading": "5.2 Incremental Pairwise LGG Computation", "text": "Our implemented learner calculates the syntactic k-AMA LGG of the AMA formula sets - however, it does not directly use the algorithm described above. Instead of calculating the LGG of the formula sets using a single call to the formula above, it is typically more efficient to break the calculation into a sequence of paired LGG calculations. In the following, we describe this approach and the potential efficiency gains. It is easy to show that LGG (1;:; m) = LGG (1; LGG (2;: m) = LGG (2;)) where the i AMA formulas are. Thus, by applying this transformation recursively, we can apply the LGG of the m AMA formulas a sequence of m 1 paired LGG calculations."}, {"heading": "5.3 Relational Data", "text": "LEONARD produces relational models that include objects and (force dynamic) relations between these objects. Thus, event definitions contain variables that allow generalization about objects. For example, a definition for PICKUP (x; y; z) recognizes both PICKUP (hand; block; table) and PICKUP (man; box; floor). Although our k-AMA learning algorithm is propositional, we can still use it to learn relational definitions. We take a simple object correspondence approach to relational learning. We consider the models issued by LEONARD as relationships that are applied to constants. As we only support superordinate learning (currently), we have a number of different training examples for each event type. There is an implicit correspondence between objects that fulfills the same role across the different training models for a particular type."}, {"heading": "5.3.1 COMPLETE OBJECT CORRESPONDENCE", "text": "This first approach assumes that a complete object correspondence is given as input along with the training examples. On the basis of this information, we can proposition the training models by replacing corresponding objects with unique constants. Then, the propositioned models are given to our propositional k-AMA learning algorithm, which returns a propositional k-AMA formula. We then cancel this propositional formula by replacing each constant with a unique variable. Lavrac et al. (1991) have taken a similar approach."}, {"heading": "5.3.2 PARTIAL OBJECT CORRESPONDENCE", "text": "The above approaches are based on complete object correspondence. Although it is sometimes possible to provide all correspondence (e.g. by color coding objects that fill identical roles in recording movies), such information is not always available. If only partial object correspondence (or none at all) is available, we can automatically complete the correspondence and apply the above techniques. At the moment, we assume that we have an evaluation function that includes two relationship models and a candidate object correspondence as input, and provides an evaluation of the correspondence. In the face of a number of training examples with missing object correspondence, we perform a greedy search for the best set of object correspondence. Our method works by storing a set of propositioned training examples (initially blank) and a set of non-propositioned training examples (initially the whole set of training examples with missing object correspondence) by playing the first step, when we evaluate all pairs of pairs are empty."}, {"heading": "5.4 Negative Information", "text": "In this section, we will look at the language AMA, which is a superset of AMA, with the addition of negated suggestions. We will first give the syntax and semantics of AMA and extend AMA syntactical subsumption to AMA. Next, we will consider our approach toFERN, GIVAN, & SISKINDlearning AMA formulas, which use the above algorithms for AMA. We will show that our approach calculates AMA LGCF and the syntactic AMA LGG correctly. Finally, we will discuss an alternative, related approach to reduce the overmatch resulting from the full consideration of negated propositions.AMA has the same syntax as AMA, only with a new grammar for building states with negated propositions.Literally: = true j prop j j: 3p state."}, {"heading": "5.5 Overall Complexity and Scalability", "text": "We review the overall complexity of our visual event learning component and discuss some scalability issues. Faced with a set of temporal models (i.e., a set of movies), our system can do the following: 1) Proposition the training models by translating the negation techniques described in Section 5.4. 2) Calculate the LGCF of each propositional model. 3) Calculate the k-AMA LGCFs as such by looking at each type of input and output in a linear way. Steps one and three are the system's computational bottlenecks - they encompass the inherent exponential complexity arising from the relational and temporal problem structure. Step One. Recall from Section 5.3.2 that our system allows the user to associate training examples with object correspondence."}, {"heading": "6. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Data Set", "text": "In fact, it is a purely mental game, in which it is a question of enabling people to put themselves in the position they are in. (...) It is as if they are able to hide. (...) It is not as if they are able to hide. (...) It is as if they are able to hide. (...) It is as if they are able to hide. (...) It is as if they are able to hide. (...)"}, {"heading": "6.2 Experimental Procedure", "text": "For each type of event, we evaluate the k-AMA learning algorithm with a leave-one-movie-out crossvalidation technique with training-set sampling. The parameters for our learning algorithm are k and the degree D of the negative information used. The value of D is either P, for positive propositions only, BN, for border negation, or N, for full negation. The parameters for our evaluation method include the target event type E and training size N. In light of this information, the evaluation proceeds as follows: For each film M (the film held) from the 210 films, apply the kAMA learning algorithm to a randomly drawn training sample of N films."}, {"heading": "6.3 Results", "text": "To evaluate our k-AMA learning approach, we conducted leave-one-movie-out experiments that varied k, D, and N. The 210 sample films were recorded with color-coded objects to provide complete object correspondence information as described above. We compared our learned event definitions with the performance of two sets of hand-coded definitions, the first set of hand-coded definitions appearing in Siskind (2001). In response to a later deeper understanding of the behavior of LEONARD model reconstruction methods, we manually revised these definitions to obtain another set of hand-coded definitions that provides significantly better FN performance at a certain cost in FP performance. Appendix C contains the event definitions in HD1 and HD2 along with a set of machine-generated definitions created by the k-AMA learning algorithm, with all training data given for 30 k = N and BD."}, {"heading": "6.3.1 OBJECT CORRESPONDENCE", "text": "In order to evaluate our algorithm for finding object correspondence, we ignored the correspondence information provided by color coding > and applied the algorithm to all training models for each type of event. The algorithm selected the correct match for all 210 training models. Therefore, if no correspondence information is given, the learning results will be identical to where the correspondence is provided manually, except that in the first case the rules do not specify specific object roles as discussed in Section 5.3.2. Since our evaluation method uses role information, the remaining experiments will use the manual correspondence information provided by color coding instead of calculating it. While our correspondence technique was perfect in these experiments, it may not be suitable for some event types. Furthermore, it is likely to produce more errors as the level of noise increases. As correspondence errors are a form of noise, and are not likely to be common for us to deal with noise."}, {"heading": "6.3.3 VARYING D", "text": "Series four to six of Table 1 show FP and FN for all 7 event types for D 2 fP; BN; Ng, N = 29 and k = 3. Similar trends have been observed for other values of k. The general trend is that as the degree of negative information increases, the learned event definitions become more specific. In other words, FP decreases and FN increases. This makes sense because the more negative information is added to the training models, the more specific structures are found in the data and are exploited by the k-AMA formulas. We can see that the definitions for pick-up and lay-down with D = P are too general, as they generate high FP. Alternatively, with D = N the learned definitions are too specific, resulting in FP = 0 at the expense of high FN. In these experiments, as in others, we have found that D = BN yields the best results in both worlds: FP = 0 for all event types and lower FP = N for the NN, which we do not have significant NING events, and NING = N for the NN which indicates an increase."}, {"heading": "6.3.4 COMPARISON TO HAND-CODED DEFINITIONS", "text": "The bottom two rows of Table 1 show the results for HD1 and HD2. We have not yet tried to automatically select the learning parameters (i.e. k and D), but we are focusing on comparing the hand-coded definitions with the set of parameters that we have classified as best for all types of events. However, we believe that these parameters could be reliably selected using cross-validation techniques that are applied to a larger dataset. In this case, the parameters would be selected on the basis of the Perevent type and would probably result in an even more favorable comparison with the hand-coded definitions. Results show that the learned definitions significantly exceed HD1 over the current dataset. HD1 definitions yielded a large number of false negatives on the current datasets. Note that HD2 produces significantly fewer false negatives for all types of events, but more false positives for recording and setting. This is due to the fact that the macros are used for pick-up and other events."}, {"heading": "6.3.5 VARYING N", "text": "For this application, it is important that our method works well with relatively small data sets, as it can be tedious to collect event data. Table 2 shows the FN of our learning algorithm for each event type, as N is reduced from 29 to 5. For these experiments, we used k = 3 and D = BN. Note that FP = 0 is shown for all event types and all N and therefore not shown. We expect FN to increase when N is decreased, as more data results in more general definitions for specific to general learning. Generally, FN is flat for N > 20, rises slowly for 10 < N < 20 and rises abruptly by 5 < N < 10. We also see that FN decreases slowly for several event types, as N decreases from 20 to 29. This suggests that a larger data set could provide better results for these event types."}, {"heading": "6.3.6 PERSPICUITY OF LEARNED DEFINITIONS", "text": "This year, it has come to the point where it is only a matter of time before a decision is reached in which a decision is reached."}, {"heading": "7. Related Work", "text": "Here we discuss two related works: First, we present earlier work on visual event detection and its relation to our experiments here; second, we discuss earlier approaches to learn temporal patterns from positive data."}, {"heading": "7.1 Visual Event Recognition", "text": "Our system is unique in that it combines only positive learning with a temporal, relational and force dynamic representation, and the inclusion of one of these parts in a system is a significant undertaking. In this respect, there are no competing approaches to directly compare our system with ours. In view of this, the following is a representative list of systems that share characteristics with ours. It is not meant to be comprehensive and focuses on showing the primary differences between each of these systems and ours, since these primary differences actually only make these systems very loosely related to us. Borchardt (1985) presents a representation for temporal, relational, force dynamic event definitions, but these definitions are neither learned nor applied to video. Regier (1992) presents techniques for learning temporal event definitions, but the definitions learned are neither relational, force dynamic nor applicable to video."}, {"heading": "7.2 Learning Temporal Patterns", "text": "In fact, most people who are in a position to put themselves in the world, to put themselves in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which, in which they, live, in which they, in which they, in fact, live, in which they, in which they, in the world, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in the world, in which they, in which they, in the world, in which they, in which they, in which they,"}, {"heading": "8. Conclusion", "text": "We presented a simple logic for the representation of temporal events called AMA and showed theoretical and empirical results for learning AMA formulas. Empirically, we gave the first system for learning temporal, relational, force dynamic event definitions using positive-only input, and we applied this system to learn such definitions from real video inputs. The resulting performance is similar to that of event definitions, which are hand-coded by experts in the human domain with considerable effort. On the theoretical side, Table 3 summarizes the upper and lower limits that we have shown for the submission and generalization problems associated with this logic. In any case, we provided a demonstrably correct algorithm that matches the upper limit shown. The table also shows the worst case sizes that the smallest LGG could possibly take in relation to the input size, both for AMA and MA inputs. The most important results in this subsummation table are the subcognitive AMA and the subcognitive XA."}, {"heading": "Acknowledgments", "text": "The authors would like to thank our anonymous reviewers for their help in improving this work. This work was supported in part by NSF grants 9977981-IIS and 0093100-IIS, an NSF graduate fellowship for remote learning, and the Center for Education and Research in Information Assurance and Security at Purdue University. Part of this work was done during Siskind's time at the NEC Research Institute, Inc."}, {"heading": "Appendix A. Internal Positive Event Logic", "text": "This logic is essentially only used to define our selection of models if it contains any model that is able to define all kinds of models that can define IPEL. (i.e.) A complete event logic allows the definition of non-internal events, for example, the formula of 3 < P is satisfied if there is any model that identifies with the truth. (i.e.) A complete event logic allows the definition of non-internal events, for example, the formula of 3 < P is satisfied if there is a complete procedure of I that is fulfilled by hM; I 0i is satisfied, so is not able to consider the applications that require non-internal events."}, {"heading": "Appendix B. Omitted Proofs", "text": "The proof: M = hM; Ii fulfills the MA timeline = s1;:; sn, and let 0 = MAP (M). It is easy to argue by including the starting point of I, V 0 (sn) that it includes an end point of I, and for all i 2 (si) V 0 (s), M 0 (s), V 0 (s1) includes the starting point of I, V 0 (sn) includes the end point of I, and for all i 2 (si) V 0 (si + 1) applies (see Table 4).LEARNING TEMPORAL EVENTSLet V be the relation between states s 2 and members i 2 I that is true when i 2 V 0 (s)."}, {"heading": "Appendix C. Hand-coded and Learned Definitions Used in Our Experiments", "text": "In the following, we specify the two hand-coded definitions HD1 and HD2 used in our experimental evaluation. In addition, we specify a number of learned AMA event definitions for the same seven event types. The learned definitions correspond to the output of our k-AMA learning algorithm taking into account all available training examples (30 examples per event type) with k = 3 and D = BN. All event definitions are written in event logic, with: 3p denoting the negation of sentence p.LEARNING TEMPORAL EVENTS."}], "references": [{"title": "Mining sequential patterns", "author": ["R. Agrawal", "R. Srikant"], "venue": "In Proceedings of the Eleventh International Conference on Data Engineering,", "citeRegEx": "Agrawal and Srikant,? \\Q1995\\E", "shortCiteRegEx": "Agrawal and Srikant", "year": 1995}, {"title": "Maintaining knowledge about temporal intervals", "author": ["J.F. Allen"], "venue": "Communications of the ACM, 26(11), 832\u2013843.", "citeRegEx": "Allen,? 1983", "shortCiteRegEx": "Allen", "year": 1983}, {"title": "Learning regular sets from queries and counterexamples", "author": ["D. Angluin"], "venue": "Information and Computation, 75, 87\u2013106.", "citeRegEx": "Angluin,? 1987", "shortCiteRegEx": "Angluin", "year": 1987}, {"title": "Using temporal logics to express search control knowledge for planning", "author": ["F. Bacchus", "F. Kabanza"], "venue": "Artificial Intelligence,", "citeRegEx": "Bacchus and Kabanza,? \\Q2000\\E", "shortCiteRegEx": "Bacchus and Kabanza", "year": 2000}, {"title": "Action recognition using probabilistic parsing", "author": ["A.F. Bobick", "Y.A. Ivanov"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Bobick and Ivanov,? \\Q1998\\E", "shortCiteRegEx": "Bobick and Ivanov", "year": 1998}, {"title": "Event calculus", "author": ["G.C. Borchardt"], "venue": "Proceedings of the Ninth International Joint Conference on Artificial Intelligence, pp. 524\u2013527, Los Angeles, CA.", "citeRegEx": "Borchardt,? 1985", "shortCiteRegEx": "Borchardt", "year": 1985}, {"title": "The inverse Hollywood problem: From video to scripts and storyboards via causal analysis", "author": ["M. Brand"], "venue": "Proceedings of the Fourteenth National Conference on Artificial Intelligence, pp. 132\u2013137, Providence, RI.", "citeRegEx": "Brand,? 1997a", "shortCiteRegEx": "Brand", "year": 1997}, {"title": "Physics-based visual understanding", "author": ["M. Brand"], "venue": "Computer Vision and Image Understanding, 65(2), 192\u2013205.", "citeRegEx": "Brand,? 1997b", "shortCiteRegEx": "Brand", "year": 1997}, {"title": "Causal analysis for visual gesture understanding", "author": ["M. Brand", "I. Essa"], "venue": "In Proceedings of the AAAI Fall Symposium on Computational Models for Integrating Language and Vision", "citeRegEx": "Brand and Essa,? \\Q1995\\E", "shortCiteRegEx": "Brand and Essa", "year": 1995}, {"title": "Coupled hidden Markov models for complex action recognition", "author": ["M. Brand", "N. Oliver", "A. Pentland"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "citeRegEx": "Brand et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Brand et al\\.", "year": 1997}, {"title": "Fluent learning: Elucidating the structure of episodes", "author": ["P. Cohen"], "venue": "Proceedings of the Fourth Symposium on Intelligent Data Analysis.", "citeRegEx": "Cohen,? 2001", "shortCiteRegEx": "Cohen", "year": 2001}, {"title": "Grammatically biased learning: Learning logic programs using an explicit antecedent description lanugage", "author": ["W. Cohen"], "venue": "Artificial Intelligence, 68, 303\u2013366.", "citeRegEx": "Cohen,? 1994", "shortCiteRegEx": "Cohen", "year": 1994}, {"title": "Learning the CLASSIC description logic: Theoretical and experimental results", "author": ["W. Cohen", "H. Hirsh"], "venue": "In Proceedings of the Fourth International Conference on Principles of Knowledge Representation and Reasoning,", "citeRegEx": "Cohen and Hirsh,? \\Q1994\\E", "shortCiteRegEx": "Cohen and Hirsh", "year": 1994}, {"title": "DLAB: A declarative language bias formalism", "author": ["L. Dehaspe", "L. De Raedt"], "venue": "In Proceedings of the Ninth International Syposium on Methodologies for Intelligent Systems,", "citeRegEx": "Dehaspe and Raedt,? \\Q1996\\E", "shortCiteRegEx": "Dehaspe and Raedt", "year": 1996}, {"title": "STRIPS: A new approach to the application of theorem proving to problem solving", "author": ["R. Fikes", "N. Nilsson"], "venue": "Artificial Intelligence,", "citeRegEx": "Fikes and Nilsson,? \\Q1971\\E", "shortCiteRegEx": "Fikes and Nilsson", "year": 1971}, {"title": "Discovery of temporal patterns\u2014Learning rules about the qualitative behaviour of time series", "author": ["F. Hoppner"], "venue": "Proceedings of the Fifth European Conference on Principles and Practice of Knowledge Discovery in Databases.", "citeRegEx": "Hoppner,? 2001", "shortCiteRegEx": "Hoppner", "year": 2001}, {"title": "Discovering temporal patterns for interval-based events", "author": ["P. Kam", "A. Fu"], "venue": "In Proceedings of the Second International Conference on Data Warehousing and Knowledge Discovery", "citeRegEx": "Kam and Fu,? \\Q2000\\E", "shortCiteRegEx": "Kam and Fu", "year": 2000}, {"title": "Learning concepts from sensor data of a mobile robot", "author": ["V. Klingspor", "K. Morik", "A.D. Rieger"], "venue": "Artificial Intelligence,", "citeRegEx": "Klingspor et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Klingspor et al\\.", "year": 1996}, {"title": "Results of the Abbadingo one DFA learning competition and a new evidence-driven state merging algorithm", "author": ["K. Lang", "B. Pearlmutter", "R. Price"], "venue": "In Proceedings of the Fourth International Colloquium on Grammatical Inference", "citeRegEx": "Lang et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lang et al\\.", "year": 1998}, {"title": "Learning nonrecursive definitions of relations with LINUS", "author": ["N. Lavrac", "S. Dzeroski", "M. Grobelnik"], "venue": "In Proceedings of the Fifth European Working Session on Learning,", "citeRegEx": "Lavrac et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Lavrac et al\\.", "year": 1991}, {"title": "Toward the computational perception of action", "author": ["R. Mann", "A.D. Jepson"], "venue": "In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Mann and Jepson,? \\Q1998\\E", "shortCiteRegEx": "Mann and Jepson", "year": 1998}, {"title": "Discovery of frequent episodes in sequences", "author": ["H. Mannila", "H. Toivonen", "A.I. Verkamo"], "venue": "In Proceedings of the First International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "Mannila et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Mannila et al\\.", "year": 1995}, {"title": "Generalization as search", "author": ["T. Mitchell"], "venue": "Artificial Intelligence, 18(2), 517\u201342.", "citeRegEx": "Mitchell,? 1982", "shortCiteRegEx": "Mitchell", "year": 1982}, {"title": "Pal: A pattern-based first-order inductive system", "author": ["E. Morales"], "venue": "Machine Learning, 26, 227\u2013 252.", "citeRegEx": "Morales,? 1997", "shortCiteRegEx": "Morales", "year": 1997}, {"title": "Inverting entailment and Progol", "author": ["S. Muggleton"], "venue": "Machine Intelligence, 14, 133\u2013188.", "citeRegEx": "Muggleton,? 1995", "shortCiteRegEx": "Muggleton", "year": 1995}, {"title": "Efficient induction of logic programs", "author": ["S. Muggleton", "C. Feng"], "venue": "Inductive Logic Programming,", "citeRegEx": "Muggleton and Feng,? \\Q1992\\E", "shortCiteRegEx": "Muggleton and Feng", "year": 1992}, {"title": "Inductive logic programming: Theory and methods", "author": ["S. Muggleton", "L. De Raedt"], "venue": "Journal of Logic Programming,", "citeRegEx": "Muggleton and Raedt,? \\Q1994\\E", "shortCiteRegEx": "Muggleton and Raedt", "year": 1994}, {"title": "Scripts in machine understanding of image sequences", "author": ["C. Pinhanez", "A. Bobick"], "venue": "In Proceedings of the AAAI Fall Symposium Series on Computational Models for Integrating Language and Vision", "citeRegEx": "Pinhanez and Bobick,? \\Q1995\\E", "shortCiteRegEx": "Pinhanez and Bobick", "year": 1995}, {"title": "Automatic Methods of Inductive Inference", "author": ["G.D. Plotkin"], "venue": "Ph.D. thesis, Edinburgh University.", "citeRegEx": "Plotkin,? 1971", "shortCiteRegEx": "Plotkin", "year": 1971}, {"title": "The Acquisition of Lexical Semantics for Spatial Terms: A Connectionist Model of Perceptual Categorization", "author": ["T.P. Regier"], "venue": "Ph.D. thesis, University of California at Berkeley.", "citeRegEx": "Regier,? 1992", "shortCiteRegEx": "Regier", "year": 1992}, {"title": "Relational learning via propositional algorithms: An information extraction case study", "author": ["D. Roth", "W. Yih"], "venue": "In Proeedings of the Seventeenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Roth and Yih,? \\Q2001\\E", "shortCiteRegEx": "Roth and Yih", "year": 2001}, {"title": "Temporal logics in AI: Semantical and ontological considerations", "author": ["Y. Shoham"], "venue": "Artificial Intelligence, 33(1), 89\u2013104.", "citeRegEx": "Shoham,? 1987", "shortCiteRegEx": "Shoham", "year": 1987}, {"title": "Visual event classification via force dynamics", "author": ["J.M. Siskind"], "venue": "Proceedings of the Seventeenth National Conference on Artificial Intelligence, pp. 149\u2013155, Austin, TX.", "citeRegEx": "Siskind,? 2000", "shortCiteRegEx": "Siskind", "year": 2000}, {"title": "Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic", "author": ["J.M. Siskind"], "venue": "Journal of Artificial Intelligence Research, 15, 31\u201390.", "citeRegEx": "Siskind,? 2001", "shortCiteRegEx": "Siskind", "year": 2001}, {"title": "A maximum-likelihood approach to visual event classification", "author": ["J.M. Siskind", "Q. Morris"], "venue": "In Proceedings of the Fourth European Conference on Computer Vision,", "citeRegEx": "Siskind and Morris,? \\Q1996\\E", "shortCiteRegEx": "Siskind and Morris", "year": 1996}, {"title": "Force dynamics in language and cognition", "author": ["L. Talmy"], "venue": "Cognitive Science, 12, 49\u2013100.", "citeRegEx": "Talmy,? 1988", "shortCiteRegEx": "Talmy", "year": 1988}, {"title": "Recognizing human action in time-sequential images using hidden Markov model", "author": ["J. Yamoto", "J. Ohya", "K. Ishii"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Yamoto et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Yamoto et al\\.", "year": 1992}], "referenceMentions": [{"referenceID": 35, "context": "This suggests that the primitive relations used to build event representations are force dynamic (Talmy, 1988).", "startOffset": 97, "endOffset": 110}, {"referenceID": 33, "context": "LEONARD (Siskind, 2001) classifies events from video using temporal, relational, force-dynamic representations but does not learn these representations.", "startOffset": 8, "endOffset": 23}, {"referenceID": 33, "context": "LEONARD (Siskind, 2001) is a system for recognizing visual events from video camera input\u2014 an example of a simple visual event is \u201ca hand picking up a block.", "startOffset": 8, "endOffset": 23}, {"referenceID": 32, "context": "LEONARD (Siskind, 2001) is a system for recognizing visual events from video camera input\u2014 an example of a simple visual event is \u201ca hand picking up a block.\u201d This research was originally motivated by the problem of adding a learning component to LEONARD\u2014allowing LEONARD to learn to recognize an event by viewing example events of the same type. Below, we give a high-level description of the LEONARD system. LEONARD is a three-stage pipeline depicted in Figure 1. The raw input consists of a video-frame image sequence depicting events. First, a segmentation-and-tracking component transforms this input into a polygon movie: a sequence of frames, each frame being a set of convex polygons placed around the tracked objects in the video. Figure 2a shows a partial video sequence of a pick up event that is overlaid with the corresponding polygon movie. Next, a model-reconstruction component transforms the polygon movie into a force-dynamic model. This model describes the changing support, contact, and attachment relations between the tracked objects over time. Constructing this model is a somewhat involved process as described in Siskind (2000). Figure 2b shows a visual depiction of the force-dynamic model corresponding to the pick up event.", "startOffset": 9, "endOffset": 1153}, {"referenceID": 31, "context": "An important assumption leveraged by our learner is that the primitive propositions used to construct states describe liquid properties (Shoham, 1987).", "startOffset": 136, "endOffset": 150}, {"referenceID": 28, "context": "2 This learning approach has been pursued for a variety of different languages within the machine-learning literature, including clausal first-order logic (Plotkin, 1971), definite clauses (Muggleton & Feng, 1992), and description logic (Cohen & Hirsh, 1994).", "startOffset": 155, "endOffset": 170}, {"referenceID": 22, "context": "This avoids the need for negative examples and corresponds to finding the specific boundary of the version space (Mitchell, 1982).", "startOffset": 113, "endOffset": 129}, {"referenceID": 33, "context": "We study a subset of an interval-based logic called event logic (Siskind, 2001) utilized by LEONARD for event recognition in video sequences.", "startOffset": 64, "endOffset": 79}, {"referenceID": 1, "context": "resenting each of the possible interval relationships given originally by Allen (1983) in his calculus of interval relations (e.", "startOffset": 74, "endOffset": 87}, {"referenceID": 32, "context": "We note that Siskind (2001) gives a continuous-time semantics for event logic where the models", "startOffset": 13, "endOffset": 28}, {"referenceID": 32, "context": "Siskind (2001) provides a method to determine whether a given model satisfies a given AMA formula.", "startOffset": 0, "endOffset": 15}, {"referenceID": 28, "context": "However, it is well known that least-general generalizations relative to such background theories need not exist (Plotkin, 1971), so prior work on clausal generalization does not simply subsume our results for the AMA language.", "startOffset": 113, "endOffset": 128}, {"referenceID": 19, "context": "Lavrac et al. (1991) has taken a similar approach.", "startOffset": 0, "endOffset": 21}, {"referenceID": 28, "context": "Since it is well known that computing first-order LGGs can be intractable (Plotkin, 1971), practical generalization algorithms retain tractability by constraining the LGGs in various ways (e.", "startOffset": 74, "endOffset": 89}, {"referenceID": 23, "context": "Since it is well known that computing first-order LGGs can be intractable (Plotkin, 1971), practical generalization algorithms retain tractability by constraining the LGGs in various ways (e.g., Muggleton & Feng, 1992; Morales, 1997).", "startOffset": 188, "endOffset": 233}, {"referenceID": 32, "context": "For a detailed description and sample video sequences of these event types, see Siskind (2001). Key frames from sample video sequences of these event types are shown in Figure 11.", "startOffset": 80, "endOffset": 95}, {"referenceID": 32, "context": "The first set HD1 of hand-coded definitions appeared in Siskind (2001). In response to subsequent deeper understanding of the behavior of LEONARD\u2019s model-reconstruction methods, we manually revised these definitions to yield another set HD2 of hand-coded definitions that gives a significantly better FN performance at some cost in FP performance.", "startOffset": 56, "endOffset": 71}, {"referenceID": 5, "context": "Borchardt (1985) presents a representation for temporal, relational, force-dynamic event definitions but these definitions are neither learned nor applied to video.", "startOffset": 0, "endOffset": 17}, {"referenceID": 5, "context": "Borchardt (1985) presents a representation for temporal, relational, force-dynamic event definitions but these definitions are neither learned nor applied to video. Regier (1992) presents techniques for learning temporal event definitions but the learned definitions are neither relational, force dynamic, nor applied to video.", "startOffset": 0, "endOffset": 179}, {"referenceID": 5, "context": "Yamoto, Ohya, and Ishii (1992), Brand and Essa (1995), Siskind and Morris (1996), Brand, Oliver, and Pentland (1997), and Bobick and Ivanov (1998) present techniques for learning temporal event definitions from video but the learned definitions are neither relational nor force dynamic.", "startOffset": 32, "endOffset": 54}, {"referenceID": 5, "context": "Yamoto, Ohya, and Ishii (1992), Brand and Essa (1995), Siskind and Morris (1996), Brand, Oliver, and Pentland (1997), and Bobick and Ivanov (1998) present techniques for learning temporal event definitions from video but the learned definitions are neither relational nor force dynamic.", "startOffset": 32, "endOffset": 81}, {"referenceID": 5, "context": "Yamoto, Ohya, and Ishii (1992), Brand and Essa (1995), Siskind and Morris (1996), Brand, Oliver, and Pentland (1997), and Bobick and Ivanov (1998) present techniques for learning temporal event definitions from video but the learned definitions are neither relational nor force dynamic.", "startOffset": 32, "endOffset": 117}, {"referenceID": 4, "context": "Yamoto, Ohya, and Ishii (1992), Brand and Essa (1995), Siskind and Morris (1996), Brand, Oliver, and Pentland (1997), and Bobick and Ivanov (1998) present techniques for learning temporal event definitions from video but the learned definitions are neither relational nor force dynamic.", "startOffset": 122, "endOffset": 147}, {"referenceID": 4, "context": "Yamoto, Ohya, and Ishii (1992), Brand and Essa (1995), Siskind and Morris (1996), Brand, Oliver, and Pentland (1997), and Bobick and Ivanov (1998) present techniques for learning temporal event definitions from video but the learned definitions are neither relational nor force dynamic. Pinhanez and Bobick (1995) and Brand (1997a) present temporal, relational event definitions that recognize events in video but these definitions are neither learned nor force dynamic.", "startOffset": 122, "endOffset": 314}, {"referenceID": 4, "context": "Yamoto, Ohya, and Ishii (1992), Brand and Essa (1995), Siskind and Morris (1996), Brand, Oliver, and Pentland (1997), and Bobick and Ivanov (1998) present techniques for learning temporal event definitions from video but the learned definitions are neither relational nor force dynamic. Pinhanez and Bobick (1995) and Brand (1997a) present temporal, relational event definitions that recognize events in video but these definitions are neither learned nor force dynamic.", "startOffset": 122, "endOffset": 332}, {"referenceID": 4, "context": "Yamoto, Ohya, and Ishii (1992), Brand and Essa (1995), Siskind and Morris (1996), Brand, Oliver, and Pentland (1997), and Bobick and Ivanov (1998) present techniques for learning temporal event definitions from video but the learned definitions are neither relational nor force dynamic. Pinhanez and Bobick (1995) and Brand (1997a) present temporal, relational event definitions that recognize events in video but these definitions are neither learned nor force dynamic. Brand (1997b) and Mann and Jepson (1998) present techniques for analyzing force dynamics in video but neither formulate event definitions nor apply these techniques to recognizing events or learning event definitions.", "startOffset": 122, "endOffset": 485}, {"referenceID": 4, "context": "Yamoto, Ohya, and Ishii (1992), Brand and Essa (1995), Siskind and Morris (1996), Brand, Oliver, and Pentland (1997), and Bobick and Ivanov (1998) present techniques for learning temporal event definitions from video but the learned definitions are neither relational nor force dynamic. Pinhanez and Bobick (1995) and Brand (1997a) present temporal, relational event definitions that recognize events in video but these definitions are neither learned nor force dynamic. Brand (1997b) and Mann and Jepson (1998) present techniques for analyzing force dynamics in video but neither formulate event definitions nor apply these techniques to recognizing events or learning event definitions.", "startOffset": 122, "endOffset": 512}, {"referenceID": 10, "context": "The sequence-mining literature contains many general-to-specific (\u201clevelwise\u201d) algorithms for finding frequent sequences (Agrawal & Srikant, 1995; Mannila, Toivonen, & Verkamo, 1995; Kam & Fu, 2000; Cohen, 2001; Hoppner, 2001).", "startOffset": 121, "endOffset": 226}, {"referenceID": 15, "context": "The sequence-mining literature contains many general-to-specific (\u201clevelwise\u201d) algorithms for finding frequent sequences (Agrawal & Srikant, 1995; Mannila, Toivonen, & Verkamo, 1995; Kam & Fu, 2000; Cohen, 2001; Hoppner, 2001).", "startOffset": 121, "endOffset": 226}, {"referenceID": 21, "context": ", sequential patterns (Agrawal & Srikant, 1995) and episodes (Mannila et al., 1995).", "startOffset": 61, "endOffset": 83}, {"referenceID": 10, "context": "More recently there has been work on mining temporal patterns using interval-based pattern languages (Kam & Fu, 2000; Cohen, 2001; Hoppner, 2001).", "startOffset": 101, "endOffset": 145}, {"referenceID": 15, "context": "More recently there has been work on mining temporal patterns using interval-based pattern languages (Kam & Fu, 2000; Cohen, 2001; Hoppner, 2001).", "startOffset": 101, "endOffset": 145}, {"referenceID": 24, "context": "Exceptions include GOLEM (Muggleton & Feng, 1992), PROGOL (Muggleton, 1995), and CLAUDIEN (De Raedt & Dehaspe, 1997), among others.", "startOffset": 58, "endOffset": 75}, {"referenceID": 10, "context": "We believe a reasonable alternative to our approach may be to incorporate syntactic biases into ILP systems as done, for example, in Cohen (1994), Dehaspe and De Raedt (1996), Klingspor, Morik, and Rieger (1996).", "startOffset": 133, "endOffset": 146}, {"referenceID": 10, "context": "We believe a reasonable alternative to our approach may be to incorporate syntactic biases into ILP systems as done, for example, in Cohen (1994), Dehaspe and De Raedt (1996), Klingspor, Morik, and Rieger (1996).", "startOffset": 133, "endOffset": 175}, {"referenceID": 10, "context": "We believe a reasonable alternative to our approach may be to incorporate syntactic biases into ILP systems as done, for example, in Cohen (1994), Dehaspe and De Raedt (1996), Klingspor, Morik, and Rieger (1996). In this work, however, we chose to work directly in a temporal logic representation.", "startOffset": 133, "endOffset": 212}, {"referenceID": 2, "context": "Finite-State Machines Finally, we note there has been much theoretical and empirical research into learning finite-state machines (FSMs) (Angluin, 1987; Lang, Pearlmutter, & Price, 1998).", "startOffset": 137, "endOffset": 186}, {"referenceID": 32, "context": "We conjecture, but have not yet proven, that all positive internal events representable in the full event logic of Siskind (2001) can be represented by some IPEL formula.", "startOffset": 115, "endOffset": 130}, {"referenceID": 1, "context": "where the Ei are IPEL formulas, prop is a primitive proposition (sometimes called a primitive event type), R is a subset of the thirteen Allen interval relations fs,f,d,b,m,o,=,si,fi,di,bi,ai,oig (Allen, 1983), and R0 is a subset of the restricted set of Allen relations fs,f,d,=g, the semantics for each Allen relation is given in Table 4.", "startOffset": 196, "endOffset": 209}], "year": 2011, "abstractText": "We develop, analyze, and evaluate a novel, supervised, specific-to-general learner for a simple temporal logic and use the resulting algorithm to learn visual event definitions from video sequences. First, we introduce a simple, propositional, temporal, event-description language called AMA that is sufficiently expressive to represent many events yet sufficiently restrictive to support learning. We then give algorithms, along with lower and upper complexity bounds, for the subsumption and generalization problems for AMA formulas. We present a positive-examples\u2013only specific-to-general learning method based on these algorithms. We also present a polynomialtime\u2013computable \u201csyntactic\u201d subsumption test that implies semantic subsumption without being equivalent to it. A generalization algorithm based on syntactic subsumption can be used in place of semantic generalization to improve the asymptotic complexity of the resulting learning algorithm. Finally, we apply this algorithm to the task of learning relational event definitions from video and show that it yields definitions that are competitive with hand-coded ones.", "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}