{"id": "1106.4632", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2011", "title": "Inferring 3D Articulated Models for Box Packaging Robot", "abstract": "Given a point cloud, we consider inferring kinematic models of 3D articulated objects such as boxes for the purpose of manipulating them. While previous work has shown how to extract a planar kinematic model (often represented as a linear chain), such planar models do not apply to 3D objects that are composed of segments often linked to the other segments in cyclic configurations. We present an approach for building a model that captures the relation between the input point cloud features and the object segment as well as the relation between the neighboring object segments. We use a conditional random field that allows us to model the dependencies between different segments of the object. We test our approach on inferring the kinematic structure from partial and noisy point cloud data for a wide variety of boxes including cake boxes, pizza boxes, and cardboard cartons of several sizes. The inferred structure enables our robot to successfully close these boxes by manipulating the flaps.", "histories": [["v1", "Thu, 23 Jun 2011 05:32:39 GMT  (7004kb,D)", "http://arxiv.org/abs/1106.4632v1", "For: RSS 2011 Workshop on Mobile Manipulation: Learning to Manipulate"]], "COMMENTS": "For: RSS 2011 Workshop on Mobile Manipulation: Learning to Manipulate", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV", "authors": ["heran yang", "tiffany low", "matthew cong", "ashutosh saxena"], "accepted": false, "id": "1106.4632"}, "pdf": {"name": "1106.4632.pdf", "metadata": {"source": "CRF", "title": "Inferring 3D Articulated Models for Box Packaging Robot", "authors": ["Heran Yang", "Tiffany Low", "Matthew Cong"], "emails": ["hy279@cornell.edu,", "twl46@cornell.edu,", "mdc238@cornell.edu,", "asaxena@cs.cornell.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTIONFaced with a point cloud of a scene, we present a method for extracting an articulated 3D model that represents the kinematic structure of an object like a box. We apply this to enable a robot to autonomously close boxes of multiple shapes and sizes, a skill that is of interest to a personal assistant robot as well as to commercial robots in applications such as packaging and shipping. Previous work on articulated structures has been able to present planar kinematic models as linear structures, such as a chain of rigid bodies connected by joints. Linear structures greatly simplify the problem of inference because they break down the articular problem into independent sub-problems. Katz et al. [1, 2] considered linear articulated structures in 2D, relying on active visual techniques to learn kinematic properties of objects. However, complex objects cannot simply be represented by linear chains. For example, a box is highly oriented to each other, an array of segments is highly oriented to each other."}, {"heading": "II. RELATED WORK", "text": "Katz et al. [1, 2] developed a relational representation of the kinematic structure. In their model, a chain structure is used to model the kinematic properties of objects. Previous work by Sturm et al. [3] models movements that cannot be described by a simple prismatic or rotatable connection. They have successfully predicted the movement of drawers and cabinet doors. However, these connections are modeled within a linear structure. Extensive preparatory work has been done to identify objects in 3D environments using the RANSAC algorithm [4, 5] and the Hough transformation [6]. These methods scan object primitives from the entered image data to identify complex objects and are successfully applied in Xiv: 110 6.46 32v1 [cs. RO] 2 3Ju n20 11 to perform a variety of manipulation tasks. For example, Rusu et al [7] identify levels in a domestic kitchen environment, such as tables and borders have been adapted."}, {"heading": "III. OUR APPROACH", "text": "The robot needs a good estimate of the box configuration before it can plan and execute a series of movements in order to close the box. The robot needs to recognize boxes based on the input point cloud data (see Fig. 3). The task is difficult because we often have several boxes (along with other clutter) in the scene and because we consider a variety of boxes. In addition, the boxes are located in random orientations and positions within the user environment. An object consists of several segments. For example, a box can be described as a series of structured levels. First, we segment the point cloud into clusters, each of which is divided into segments. Our goal is to learn the kinematic model G by correctly identifying the segments. In our application, the model is a box consisting of four sides, a base and four flaps. Our goal is to find the optimal model G \u0445 = arg maxG J (G) in terms of a scoring function J (G)."}, {"heading": "A. Conditional Random Field (CRF)", "text": "The relationships we introduce in Section III-D provide further examples. Therefore, we use a conditional random field (CRF) for the undirected, probable graphical model to determine the 3D articulated structure. Formally, we define G = (V, E) as an undirected graph, so that each of the random variables representing a segment in the 3D structure corresponds to a node v-V. Each edge (vi, vj) represents that the nodes vi and vj are not independent, while a binary potential function, referred to as \u0432 (vi, vj), describes the relationship or dependence between vi and vj. Furthermore, segments of 3D articulated structures in some cases exhibit internal properties that are not dependent on any part of the object itself, such as the natural orientations or nature functions associated with any nodes vi and vj."}, {"heading": "B. Score Function for 3D Structure Modeling", "text": "The evaluation function J (G) is a measure of the suitability of G over a series of characteristics \u03c6 and relations \u0445. The corresponding weights are w\u03c6 and w\u0445 for \u03c6 and \u0439 respectively. Detailed explanations of \u03c6 and \u0439 can be found in section III-D and the learning algorithm for the weights is discussed in section III-E.J (G) = \u2211 i-V wT\u03c6\u03c6 (vi) + \u0445 (i, j) \u0394E wT\u0445 (vi, vj) (1)."}, {"heading": "C. Complexity Analysis", "text": "An exhaustive search for all mappings of n segments to a model that consists of m segments is combinatorial in the order O (n! (n \u2212 m)!) = O (nm). Allowing empty levels to be mapped to the model segments does not increase the order of complexity O (nm). We show that the state space can be significantly reduced if the segments are conditionally independent of each other. Each node vi has a set of adjacent nodes. vi is independent of all nodes outside \u2206 i. If we search across the various mappings of the model and assign possible segments to a node v, all other independent nodes can be ignored due to conditional independence. In other words, if there are ti segments that have not yet been mapped, and there are ki nodes that are not yet determined, we can limit the search space to O (ti! ki!) for the node."}, {"heading": "D. Feature and Relation Sets", "text": "For each collection of objects in a manipulation task, a set of characteristics and relationships can be selected to distinguish them from other objects in the environment. A characteristic describes the segment relative to a ground reference or the properties of the segment. In comparison, a relationship encodes the relative values of a set of properties between at least two segments within the model. However, both characteristics and relationships require the tolerance parameter to define the boundary of correctness. Such a segment is defined as an angle / distance when used for bound orientation / position. In our model, we have the following characteristics and relationships: 1: Absolute orientation: A majority of object models have a natural orientation, e.g. windows are vertical."}, {"heading": "E. Learning", "text": "For each pair of \u03c6 and \u03b3, we obtain J (G) and verify the correctness of G by comparing it with the described data, i.e. the ratio of the correctly designated planes to the total number of planes in the described model. Note that the equation 1 in w\u03c6 and w\u03b3 is linear. Therefore, we can use the normal equations with regulation to estimate the parameters [15].F. As a result, we remove the assumption of conditional independence encoded by the graphic model in Fig. 2 (see section III-C), we only have to consider a tractable number of fields to derive the optimal configuration from it. We evaluate the values for all cases and select the optimal ones. In practice, we evaluate those with a higher probability by forming a priority queue that assigns the fields in order to derive the optimal configuration."}, {"heading": "G. Segment Identification", "text": "We use the RANSAC algorithm [4] to extract segments from the point cloud. In our case, RANSAC is applied iteratively, and the levels best suited to the clusters are also determined sequentially. However, the levels returned from RANSAC are not limited by rectangles and may contain outliers. We therefore extract the extracted levels by outliers by means of Euclidean clustering. To obtain the corners from the point cloud connected to each plane, we construct the convex pods of the points to eliminate redundant points and find the minimal boundary rectangles of the tarpaulins. By going through the above procedures, levels can be achieved with almost 100% accuracy. However, any noise in the environment connected to the main cluster can potentially also be identified as layers that can confuse the search algorithm."}, {"heading": "IV. PLANNING AND CONTROL", "text": "To close a box in the vicinity, the robot must identify and locate the box to bring it into the desired state. Each flap can be closed from a series of paths. Considering the box model with the position and orientation of each flap, we use OpenRAVE's [16] inverse kinematic solver. The program applies a brute force search approach by defining rotating intermediate planes. The planner selects points scanned from each intermediate plane and searches for ways through them. To determine the order of closing the flaps, our program sorts the flaps in the order of the ascending range specified by the box models. Subsequently, the manipulator greedily closes the flaps in this order. For the three flaps closer to the robot arm, the planner is able to find valid paths in 90% of the experiments."}, {"heading": "V. HARDWARE", "text": "Our experiments were carried out with an Adept Viper s850 6-DOF arm with a parallel plate gripper, which allowed a range of about 100 centimetres. To obtain point cloud data, a Microsoft Kinect was mounted on the robot in a position (Figure 4), which made it possible to change the orientation of the camera in order to obtain a variety of vantage points."}, {"heading": "VI. EXPERIMENTS", "text": "To demonstrate the robustness of our algorithm, we collected point cloud data for several classes of boxes, including different sizes of standard packaging boxes and unusually shaped boxes (see Figure 3). We then conducted a series of experiments with an extensive dataset to determine the accuracy of the inference algorithm, and then verified that the robot was able to close the box by simulation."}, {"heading": "A. Data", "text": "Our test environment consists of a total of 50 point clouds and comprises a total of 15 different types of boxes. We used 12 of these point clouds for training purposes and the rest for testing purposes. We looked at a series of standard boxes sorted into three size categories and tested the algorithm on unusual boxes, including cake boxes, pizza boxes and a box. For each box type, we collected images of the box from different orientations. The point clouds in our data set often included strange common household items placed in the scene, such as toys, dishes and cups."}, {"heading": "B. Accuracy Evaluation", "text": "We measure the accuracy of the flap conclusions and the complete accuracy of the model conclusions on our box model (see Table I). We define the accuracy of the flap conclusions of a model as (the number of correctly identified flaps - the number of incorrectly identified flaps) / (the number of flaps in the point cloud data). Likewise, we define the complete model sequence accuracy = (the number of correctly identified layers - the number of incorrectly identified layers) / (the number of layers in the point cloud data).Note that all four rotations of the box model are acceptable."}, {"heading": "C. Results on the Dataset", "text": "Fig. 3 shows the original image, the point cloud data, the observed segments, the matching returned by the algorithm, and finally the conditional random field model. We see that the robot can identify a variety of fields. Performance remains stable for fields of different sizes and the algorithm is able to detect non-standard fields, as shown in Examples 3 and 7, where the structure of a flap on a flap is correctly identified despite its deviation from the described model. Our algorithm is able to filter out noise in the data. In the second example, the segments belonging to elements in the flap are discarded in the adapted model. Likewise, the algorithm is able to remove the sources of noise in Examples 4 and 7. If there is more than one box in the scene, the algorithm is able to recognize the segments as individual fields."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Yun Jiang, Akram Helou, Marcus Lim and Stephen Moseson for useful discussions."}], "references": [{"title": "Learning to Manipulate Articulated Objects in Unstructured Environments using a Grounded Relational Representation", "author": ["D. Katz", "Y. Pyuro", "O. Brock"], "venue": "RSS, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Extracting Planar Kinematic Models using Interactive Perception", "author": ["D. Katz", "O. Brock"], "venue": "Unifying Perspectives in Computational and Robot Vision. Springer, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "3D Pose Estimation, Tracking and Model Learning of Articulated Objects from Dense Depth Video using Projected Texture Stereo", "author": ["J. Sturm", "K. Konolige", "C. Stachniss", "W. Burgard"], "venue": "RSS, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography", "author": ["M. Fischler", "R. Bolles"], "venue": "Communications of the ACM, 1981.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1981}, {"title": "Efficient RANSAC for Point-Cloud Shape Detection", "author": ["R. Schnabel", "R. Wahl", "R. Klein"], "venue": "Computer Graphics Forum, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Generalizing the Hough Transform to Detect Arbitrary Shapes", "author": ["D. Ballard"], "venue": "Pattern Recognition, 1981.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1981}, {"title": "Towards 3D Point Cloud Based Object Maps for Household Environments", "author": ["R. Rusu", "Z. Marton", "N. Blodow", "M. Dolha", "M. Beetz"], "venue": "RSS, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Beyond Trees: Common Factor Models for 2D Human Pose Recovery", "author": ["X. Lan", "D.P. Huttenlocher"], "venue": "ICCV, 2005.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Spatial Priors for Part-based Recognition using Statistical Models", "author": ["D. Crandall", "P. Felzenszwalb", "D. Huttenlocher"], "venue": "CVPR, 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning Compact 3D Models of Indoor and Outdoor Environments with a Mobile Robot", "author": ["D. H\u00e4hnel", "W. Burgard", "S. Thrun"], "venue": "RSS, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient Grasping from RGBD Images: Learning using a New Rectangle Representation", "author": ["Y. Jiang", "S. Moseson", "A. Saxena"], "venue": "ICRA, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning Grasp Strategies with Partial Shape Information", "author": ["A. Saxena", "L. Wong", "A.Y. Ng"], "venue": "AAAI, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning to Place New Objects", "author": ["Y. Jiang", "C. Zheng", "M. Lim", "A. Saxena"], "venue": "RSS Workshop on Mobile Manipulation, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Human activity detection from rgbd images", "author": ["J.Y. Sung", "C. Ponce", "B. Selman", "A. Saxena"], "venue": "AAAI workshop on Pattern, Activity and Intent Recognition (PAIR), 2011.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Openrave: A Planning Architecture for Autonomous Robotics", "author": ["R. Diankov", "J. Kuffner"], "venue": "Robotics Institute, Pittsburgh, PA, Tech. Rep. CMU-RI-TR-08-34, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "[1, 2] considered linear articulated structures in 2D by relying on active vision techniques to learn kinematic properties of objects.", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[1, 2] considered linear articulated structures in 2D by relying on active vision techniques to learn kinematic properties of objects.", "startOffset": 0, "endOffset": 6}, {"referenceID": 0, "context": "[1, 2] developed a relational representation of kinematic structure.", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[1, 2] developed a relational representation of kinematic structure.", "startOffset": 0, "endOffset": 6}, {"referenceID": 2, "context": "[3] models motion that cannot be described by a simple prismatic or revolute joint.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "There has been extensive prior work on object recognition in 3D environments using the RANSAC algorithm [4, 5] and the Hough transform [6].", "startOffset": 104, "endOffset": 110}, {"referenceID": 4, "context": "There has been extensive prior work on object recognition in 3D environments using the RANSAC algorithm [4, 5] and the Hough transform [6].", "startOffset": 104, "endOffset": 110}, {"referenceID": 5, "context": "There has been extensive prior work on object recognition in 3D environments using the RANSAC algorithm [4, 5] and the Hough transform [6].", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "[7] identified planes in a household", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] used a kinematic tree model to model human motion.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "This reduces the complexity of the search space [9] but maintains the representational power of the kinematic structure obtained.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "[10] produced accurate models of indoor and outdoor environments that compare favorably to other methods which decompose and approximate environments using flat surfaces.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Other related works also use RGB-D data for different purposes in robot manipulation such as grasping [11, 12], placing objects [13] and human activity detection [14], where the point cloud data is used together with learning algorithms for the respective tasks.", "startOffset": 102, "endOffset": 110}, {"referenceID": 11, "context": "Other related works also use RGB-D data for different purposes in robot manipulation such as grasping [11, 12], placing objects [13] and human activity detection [14], where the point cloud data is used together with learning algorithms for the respective tasks.", "startOffset": 102, "endOffset": 110}, {"referenceID": 12, "context": "Other related works also use RGB-D data for different purposes in robot manipulation such as grasping [11, 12], placing objects [13] and human activity detection [14], where the point cloud data is used together with learning algorithms for the respective tasks.", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "Other related works also use RGB-D data for different purposes in robot manipulation such as grasping [11, 12], placing objects [13] and human activity detection [14], where the point cloud data is used together with learning algorithms for the respective tasks.", "startOffset": 162, "endOffset": 166}, {"referenceID": 14, "context": "Therefore, we can use the normal equations with regularization to estimate the parameters [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "We apply the RANSAC algorithm [4] to extract segments from the point cloud.", "startOffset": 30, "endOffset": 33}, {"referenceID": 15, "context": "Given the box model with the location and orientation of each flap, we use OpenRAVE\u2019s [16] under-constrained inverse kinematics solver.", "startOffset": 86, "endOffset": 90}], "year": 2011, "abstractText": "Given a point cloud, we consider inferring kinematic models of 3D articulated objects such as boxes for the purpose of manipulating them. While previous work has shown how to extract a planar kinematic model (often represented as a linear chain), such planar models do not apply to 3D objects that are composed of segments often linked to the other segments in cyclic configurations. We present an approach for building a model that captures the relation between the input point cloud features and the object segment as well as the relation between the neighboring object segments. We use a conditional random field that allows us to model the dependencies between different segments of the object. We test our approach on inferring the kinematic structure from partial and noisy point cloud data for a wide variety of boxes including cake boxes, pizza boxes, and cardboard cartons of several sizes. The inferred structure enables our robot to successfully close these boxes by manipulating the flaps.", "creator": "LaTeX with hyperref package"}}}