{"id": "1106.6258", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "A Note on Improved Loss Bounds for Multiple Kernel Learning", "abstract": "The paper \\cite{hs-11} presented a bound on the generalisation error of classifiers learned through multiple kernel learning. The bound has (an improved) \\emph{additive} dependence on the number of kernels (with the same logarithmic dependence on this number). However, parts of the proof were incorrectly presented in that paper. This note remedies this weakness by restating the problem and giving a detailed proof of the Rademacher complexity bound from \\cite{hs-11}.", "histories": [["v1", "Thu, 30 Jun 2011 15:03:58 GMT  (9kb)", "https://arxiv.org/abs/1106.6258v1", "Extended proof"], ["v2", "Mon, 12 May 2014 19:40:40 GMT  (13kb)", "http://arxiv.org/abs/1106.6258v2", "Extended proof"]], "COMMENTS": "Extended proof", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["zakria hussain", "john shawe-taylor", "mario marchand"], "accepted": false, "id": "1106.6258"}, "pdf": {"name": "1106.6258.pdf", "metadata": {"source": "CRF", "title": "A Note on Improved Loss Bounds for Multiple Kernel Learning", "authors": ["Zakria Hussain"], "emails": ["z.hussain@cs.ucl.ac.uk", "jst@cs.ucl.ac.uk", "mario.marchand@ift.ulaval.ca"], "sections": [{"heading": null, "text": "ar Xiv: 110 6.62 58"}, {"heading": "1 Introduction", "text": "For motivation and definition of multi-core learning, we refer to [4], which shows a number of results, among them a new Rademacher complexity, which is bound to the generalization error of classifiers, which learn from a multi-core class with a logarithmic dependence on the number of used cores and with this logarithm additively enter into the limit - thus independently of the complexity of the individual cores or the margin of the classifier on the training set. In this essay we follow the approach set out in [4], but correct some of the existing errors. Unfortunately, it turns out that the Rademacher complexity risk limit has a multiplicative dependence on the number of cores and the margin of the classifier."}, {"heading": "2 Detailed proof", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Preliminaries", "text": "A kernel is a function that contains the input vectors for all x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, x, etc. Definition 1 ([1]: A kernel is a function that contains the input vectors for all x, x, x, x, x, x, x, etc. Definition is an association of X with an (inner product) Hilbert space H\u03c6: X 7 \u2192 H.Kernel learning algorithms [7, 8] use the m, x, m, x, x, etc. < matrix, i, i, i, i, i, etc. Definition is an association of X with an (inner product) Hilbert space. When using the kernel representation, it is not always possible to use the weight vector directly as a function and we can explicitly specify the pref."}, {"heading": "FK = {x 7\u2192 \u3008w,\u03c6\u03ba(x)\u3009 | \u2016w\u20162 \u2264 1, for some \u03ba \u2208 K} .", "text": "For a distribution D, we use the notation ED [f (x)] to indicate the expected value of f (x) if x (D). For a training set X, we use the notation E [f] to indicate the empirical average over the sample x.For the generalization error limit, we assume that the data iid is derived from a fixed but unknown probability distribution D over the common space X \u00b7 Y. For an actual error of a function f: err (f) = E (x, y). D (yf (x) \u2264 0) = ED [yf (x)], the empirical margin error of f with the marginal value \u03b3 > 0: e, rrg (f) = 1mm."}, {"heading": "2.2 Rademacher complexity bound for MKL", "text": "We start with the following definition of Rademacher complexity (Rademacher complexity): Definition 2 (Rademacher complexity): Definition 2 (Rademacher complexity): Definition 2 (Rademacher complexity): Definition 3 (Rademacher complexity): Definition 3 (Rademacher complexity): Definition 3 (Rademacher complexity): Definition 3 (Rademacher complexity): Definition 3 (Rademacher complexity): Definition 3 (Rademacher complexity): Definition 3 (Rademacher complexity): Definition 3 (Rademacher complexity): Definition 3 (Rademacher complexity): (Rademacher complexity): (Rademacher-Rademacher complexity): (Rademacher complexity): (Rademacher-Rademacher: 3): (Rademacher complexity): (Rademacher-Rademacher): (Rademacher): (Rademacher-Complexity): (Rademacher): (Rademacher-Complexity): (Rademacher): (Rademacher-Complexity): (Rademacher): (Rademacher-Complexity): (Rademacher): (Rademacher-Complexity): (Rademacher): (Rademacher): (Rademacher-Complexity): (Rademacher): (Rademacher): (Rademacher-Complexity): (Rademacher): (Rademacher-Complexity: (Rademacher): (Rademacher): (Rademacher): (Rademacher-Complexity: (Rademacher): (Rademacher): (Rademacher-Complexity): (Rademacher): (Rademacher-Complexity: (Rademacher): (Rademacher): (Rademacher-Complexity): (Rademacher): (Rademacher-Complexity): (Rademacher): (Rademacher-Complexity): (Rademacher-Complexity): (Rade"}, {"heading": "3 Discussion", "text": "Using the above notation, we obtain the unnormalized version of the boundary of theorem 8 of [4] iserr (f) \u2264 e-rr\u03b3 (f) + 2 \u03b3m max 1 \u2264 j \u2264 p \u221a \u221a \u221a m \u2211 i = 1\u03baj (xi, xi) + 5 \u221a ln (((p + 3) / \u03b4) 2m. Comparing this with theorem 5 (the corrected version), we can see that the main difference is that in theorem 5 1 / \u03b3 is multiplied by ln p, while it is not contained in theorem 8 of [4]. However, the latter was achieved by incorrectly assuming that R-m (Con (F)) is limited by R-m (con (F)). While theorem 2 of [4] has an additive dependence on the logarithm of the number of nuclei, it has an additional term that includes the number of nuclei involved in the final solution."}], "references": [{"title": "Theoretical foundations of the potential function method in pattern recognition learning", "author": ["M. Aizerman", "E. Braverman", "L. Rozonoer"], "venue": "Automation and Remote Control, 25:821 \u2013 837", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1964}, {"title": "Complexity of pattern classes and lipschitz property", "author": ["A. Ambroladze", "J. Shawe-Taylor"], "venue": "Algorithmic Learning Theory, volume 3244 of Lecture Notes in Computer Science, pages 181\u2013193. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Rademacher and gaussian complexities: risk bounds and structural results", "author": ["P.L. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research, 3:463\u2013482", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Improved loss bounds for multiple kernel learning", "author": ["Z. Hussain", "J. Shawe-Taylor"], "venue": "International Conference on Artificial Intelligence and Statistics", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Rademacher composition", "author": ["S. Kakade", "A. Tewari"], "venue": "CMSC 35900 Learning Theory, pages 1\u201322", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "On the method of bounded differences", "author": ["C. McDiarmid"], "venue": ". L. M. S. L. N. Series, editor, Surveys in Combinatorics 1989, pages 148\u2013188. Cambridge University Press, Cambridge", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning with Kernels", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge University Press, Cambridge, U.K.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning bounds for support vector machines with learned kernels", "author": ["N. Srebro", "S. Ben-David"], "venue": "Computational Learning Theory, volume 4005 of Lecture Notes in Computer Science, pages 169\u2013183. Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 3, "context": "Abstract In this paper, we correct an upper bound, presented in [4], on the generalisation error of classifiers learned through multiple kernel learning.", "startOffset": 64, "endOffset": 67}, {"referenceID": 3, "context": "The bound in [4] uses Rademacher complexity and has anadditive dependence on the logarithm of the number of kernels and the margin achieved by the classifier.", "startOffset": 13, "endOffset": 16}, {"referenceID": 3, "context": "1 Introduction We refer to [4] for the motivation and definitions of multiple kernel learning.", "startOffset": 27, "endOffset": 30}, {"referenceID": 3, "context": "In this paper, we follow the approach presented in [4] but correct some of the errors that are present.", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "Definition 1 ([1]).", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "Kernel learning algorithms [7, 8] make use of the m \u00d7m kernel matrix K = [\u03ba(xi, xi\u2032)] m i,i=1 defined using the training inputs x.", "startOffset": 27, "endOffset": 33}, {"referenceID": 7, "context": "Kernel learning algorithms [7, 8] make use of the m \u00d7m kernel matrix K = [\u03ba(xi, xi\u2032)] m i,i=1 defined using the training inputs x.", "startOffset": 27, "endOffset": 33}, {"referenceID": 8, "context": "Hence, learning with a kernel \u03ba can be described as finding a function from the class of functions [9] F\u03ba = {x 7\u2192 \u3008w,\u03c6\u03ba(x)\u3009 | \u2016w\u20162 \u2264 1, } 2", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "where we call \u03b3 \u2208 [0, 1] the margin.", "startOffset": 18, "endOffset": 24}, {"referenceID": 3, "context": "2 Rademacher complexity bound for MKL In this section we correct the MKL risk bound of [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 2, "context": "Theorem 1 ([3]).", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "Fix \u03b4 \u2208 (0, 1), and let F be a class of functions mapping from Z = X \u00d7Y to [0, 1].", "startOffset": 75, "endOffset": 81}, {"referenceID": 2, "context": "We have attributed this bound to [3], though, strictly speaking, they used the slightly weaker version of Rademacher complexity including an absolute value of the sum.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "Theorem 2 ([3]).", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "(4) Furthermore, following [5] and [2], we have the following result.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "(4) Furthermore, following [5] and [2], we have the following result.", "startOffset": 35, "endOffset": 38}, {"referenceID": 4, "context": "Theorem 3 ([5]).", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": ", xm} be an m-sample of points from X , then the empirical Rademacher complexity R\u0302m of the class F = \u222apj=1Fj, where the range of all the functions in F is [0, 1], satisfies: R\u0302m(F) \u2264 max 1\u2264j\u2264p R\u0302m(Fj) + \u221a 8 ln(p) m .", "startOffset": 156, "endOffset": 162}, {"referenceID": 5, "context": "A basic result of McDiarmid [6] states that for any \u03bb \u2265 0, we have Ee \u2264 e\u03bb 2 8 \u2211m i=1 c 2 i \u00b7 e , where, for all i, we have sup \u03c31,.", "startOffset": 28, "endOffset": 31}, {"referenceID": 3, "context": "3 Discussion Using the notation from above, the un-normalized version of the bound of Theorem 8 of [4] is err(f) \u2264 \u00earr(f) + 2 \u03b3m max 1\u2264j\u2264p \u221a \u221a", "startOffset": 99, "endOffset": 102}], "year": 2014, "abstractText": "In this paper, we correct an upper bound, presented in [4], on the generalisation error of classifiers learned through multiple kernel learning. The bound in [4] uses Rademacher complexity and has anadditive dependence on the logarithm of the number of kernels and the margin achieved by the classifier. However, there are some errors in parts of the proof which are corrected in this paper. Unfortunately, the final result turns out to be a risk bound which has a multiplicative dependence on the logarithm of the number of kernels and the margin achieved by the classifier.", "creator": "LaTeX with hyperref package"}}}