{"id": "1106.6341", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "Vision-Based Navigation III: Pose and Motion from Omnidirectional Optical Flow and a Digital Terrain Map", "abstract": "An algorithm for pose and motion estimation using corresponding features in omnidirectional images and a digital terrain map is proposed. In previous paper, such algorithm for regular camera was considered. Using a Digital Terrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the absolute position and orientation of the camera. In order to do this, the DTM is used to formulate a constraint between corresponding features in two consecutive frames. In this paper, these constraints are extended to handle non-central projection, as is the case with many omnidirectional systems. The utilization of omnidirectional data is shown to improve the robustness and accuracy of the navigation algorithm. The feasibility of this algorithm is established through lab experimentation with two kinds of omnidirectional acquisition systems. The first one is polydioptric cameras while the second is catadioptric camera.", "histories": [["v1", "Thu, 30 Jun 2011 18:59:53 GMT  (1727kb)", "http://arxiv.org/abs/1106.6341v1", "6 pages, 9 figures"], ["v2", "Tue, 16 Aug 2011 09:22:33 GMT  (1727kb)", "http://arxiv.org/abs/1106.6341v2", "6 pages, 9 figures"]], "COMMENTS": "6 pages, 9 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["ronen lerner", "oleg kupervasser", "ehud rivlin"], "accepted": false, "id": "1106.6341"}, "pdf": {"name": "1106.6341.pdf", "metadata": {"source": "META", "title": "Pose and Motion from Omnidirectional Optical Flow and a Digital Terrain Map\u20ac", "authors": ["Ronen Lerner", "Oleg Kupervasser", "Ehud Rivlin"], "emails": ["ronenl@cs.technion.ac.il", "kup@yahoo.com", "ehudr@cs.technion.ac.il"], "sections": [{"heading": null, "text": "This year, the time has come for us to be able to live in a country in which we are able to move and in which we are able to leave a country in which we are able to move."}, {"heading": "II. PROBLEM DEFINITION AND NOTATIONS", "text": "The problem can be briefly described as follows: At a given point in time, the coordinate system C (t) is on an omni-directional camera. At this point, the camera is in a geographical location p (t) - a 3D vector - and has a given orientation R (t) - an orthonormal rotation matrix, in relation to a global coordinate system W. p (t) and R (t) define the transformation from the camera image C (t) to the world view W, where if Cv and Wv are vectors in C (t) or W, then Wv = R (t) Cv + p (t) is the transformation from the camera image t1 and t2: the transformation from C (t1) to C (t2) by the translation vector \u0394p (t1, t2) and the rotation matrix \u0394R (t2) t2."}, {"heading": "III. THE NAVIGATION ALGORITHM", "text": "The following section describes a navigation algorithm that estimates the above-mentioned parameters; the camera's pose and ego motion are calculated using a DTM and the optical flow field of two consecutive frames; unlike the landmark approach, no specific features should be detected and matched; only the correspondence between the two consecutive images should be found to directly derive the optical algorithm field; as mentioned in the previous section, a rough estimate of the required parameters is provided as input; however, since the algorithm only uses this input as the first guess and recalculation of the position and ego motion, no integration of previous errors will take place and the accuracy will be based on the following observation. As the DTM provides information about the structure of the observed terrain, the depth of the observed features is dictated by the camera's pose."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "The laboratory experiments were carried out with a real 3D model of a terrain and images of an omni-directional acquisition system. The dimensions of the model were 115 x 95 cm with height differences of up to 32 cm (see Fig.3 (a)). A laser-based 3D scanner was used to capture the terrain and to create a DTM with a spatial grid of 1 mm (see Fig.3 (b)). Two types of omni-directional acquisition systems were tested: a configuration of three normal cameras travelling in different directions and a catadioptric system with a parabolic mirror."}, {"heading": "A. Three Cameras Configuration", "text": "In fact, it is the case that you will be able to put yourself at the top without being able to put yourself at the top."}, {"heading": "B. Catadioptric System", "text": "In the second experiment, the three conventional cameras were replaced by a single catadioptric system consisting of a parabolic mirror mounted in front of an orthographic camera (see Fig. 8 (a)).With this camera, images of 1024 x 768 pixels were taken and 300 matches between two consecutive images were calculated for the algorithm using the Lucas Kanade method (see Fig. 8 (b)).It should be noted that this tracking method is not optimal for catadioptric images due to the nature of the distortion of this type of images. However, since the catadioptric system was first calibrated, these distortions can be calculated and then canceled. For each feature, a distorted image can be reproduced from the original images in such a way that the local area of the feature appears to be in a normal perspective camera. Next, the LucasKanade tracking method can be activated on these distorted images without any particular difficulty."}, {"heading": "V. CONCLUSIONS", "text": "An algorithm for position and motion estimation with corresponding features in omni-directional images and a DTM were presented. DTM served as a global reference and its data was used to restore the absolute position and orientation of the camera. The resulting limitation eliminates the requirement for the commonly used assumption of a single effective viewing angle. As a result, the presented algorithm is applicable to all omni-directional detection systems. In addition, the performance of the presented algorithm was demonstrated with both polydioptric cameras and catadioptric cameras. Both position and orientation estimates proved to be sufficiently precise to bind the accumulated errors and prevent trajectory drifts. Furthermore, the use of omni-directional data improved the robustness and accuracy of the navigation algorithm compared to its normal camera counterpart, attributing the improvement to the wide segment of the visible terrain."}], "references": [{"title": "Statistical image analysis for pose estimation without point correspondences", "author": ["Y. Liu", "M.A. Rodrigues"], "venue": "Patt. Recognition Letters, vol. 22, pp. 1191\u20131206, 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Softposit: Simultaneous pose and correspondence determination", "author": ["P. David", "D. DeMenthon", "R. Duraiswami", "H. Samet"], "venue": "Proc. of the European Conf. of Comp. Vision, 2002, pp. 698\u2013714.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Recursive estimation of time-varying motion and structure parameters", "author": ["J.L. Barron", "R. Eagleson"], "venue": "Patt. Recognition, vol. 29, no. 5, pp. 797\u2013818, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "Comparison of approaches to egomotion computation", "author": ["T. Tian", "C. Tomasi", "D. Heeger"], "venue": "Proc. IEEE Conf. Comp. Vision Patt. Recog., pp. 315\u2013320, 1996.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "MFm: 3-D motion from 2-D motion causally integrated over time", "author": ["A. Chiuso", "P. Favaro", "H. Jin", "S. Soatto"], "venue": "Proc. of the European Conf. of Comp. Vision, 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Robust recovery of ego-motion", "author": ["M. Irani", "B. Rousso", "S. Peleg"], "venue": "Proc. Of Comp. Analysis of Images and Patt., 1993, pp. 371\u2013378.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1993}, {"title": "Integrated position estimation using aerial image sequences", "author": ["D.G. Sim", "R.H. Park", "R.C. Kim", "S.U. Lee", "I.C. Kim"], "venue": "IEEE Trans. on Patt. Analysis and Machine Intelligence, vol. 24, no. 1, pp. 1\u201318, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "A critique of structure-from-motion algorithms", "author": ["J. Oliensis"], "venue": "Comp. Vision and Image Understanding, vol. 80, pp. 172\u2013214, 2000.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Error analysis for a navigation algorithm based on optical-flow and a digital terrain map", "author": ["R. Lerner", "P. Rotstein", "E. Rivlin"], "venue": "Proc. IEEE Conf. Comp. Vision Patt. Recog., vol. 1, 2004, pp. 604\u2013610.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Epipolar geometry for panoramic cameras", "author": ["T. Svoboda", "T. Pajdla", "V. Hlavac"], "venue": "ECCV, vol. 1406, pp. 218\u2013232, 1998.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Epipolar geometry for central catadioptric cameras", "author": ["T. Svoboda", "T. Pajdla"], "venue": "international Journal of Computer Vision, vol. 49, no. 1, pp. 23\u201337, 2002.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Estimation of omnidirectional camera model from epipolar geometry", "author": ["B. Micusik", "T. Pajdla"], "venue": "CVPR, vol. 1, pp. 485\u2013490, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "A unifying theory for central panoramic systems", "author": ["C. Geyer", "K. Daniilidis"], "venue": "ECCV, vol. 1843, pp. 445\u2013461, 2000.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1843}, {"title": "Para-catadioptric camera auto-calibration from epipolar geometry. Research Report CTU-CMP-2003-18, CMP K13133", "author": ["B. Micusik", "T. Pajdla"], "venue": "FEE Czech Technical University in Prague,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Epipolar geometry for panoramic cameras", "author": ["T. Svoboda", "T. Pajdla", "V. Hlavac"], "venue": "ECCV, vol. 1406, pp. 218\u2013232, 1998.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Epipolar geometry for panoramic cameras", "author": ["C. Geyer", "K. Daniilidis"], "venue": "ECCV, vol. 1406, pp. 218\u2013232, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Catadioptric self-calibration", "author": ["S.B. Kang"], "venue": "CVPR, vol. 1, pp. 201\u2013207, 2000.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2000}, {"title": "Omnidirectional camera model and epipolar geometry estimation by ransac with bucketing", "author": ["B. Micusk", "T. Pajdla"], "venue": "SCIA, vol. 1, pp. 83\u201390, 2003.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Robust wide baseline stereo from maximally stable extremal regions", "author": ["J. Matas", "O. Chum", "M. Urban", "T. Pajdla"], "venue": "BMVC, vol. 1, pp. 384\u2013 393, 2002.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "A theory of catadioptric image formation", "author": ["S. Baker", "S. Nayar"], "venue": "Proc. of IEEE Int. Conf. on Comp. Vision, 1998, pp. 35\u201342.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Pose and motion recovery from feature correspondences and a digital terrain map", "author": ["R. Lerner", "E. Rivlin", "H. Rotstein"], "venue": "2006, submitted for publication in TPAMI.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "An iterative image registration technique with an application to stereo vision", "author": ["B.D. Lucas", "T. Kanade"], "venue": "Proc. of the 7th Int. Joint Conf. on Artificial Intelligence, Vancouver, Canada, 1981, pp. 674\u2013679.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1981}, {"title": "Pyramidal implementation of the lucas kanade feature tracker, description of the algorithm", "author": ["J.Y. Bouguet"], "venue": "Intel Research Lab, Tech. Rep., 1999.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "Few examples for such algorithms are [1], [2].", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "Few examples for such algorithms are [1], [2].", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "Several ego-motion estimation algorithms can be found in [3], [4], [5], [6].", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "Several ego-motion estimation algorithms can be found in [3], [4], [5], [6].", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "Several ego-motion estimation algorithms can be found in [3], [4], [5], [6].", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "Several ego-motion estimation algorithms can be found in [3], [4], [5], [6].", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "In [7] such navigation-system is being suggested.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7] a segment of the ground was reconstructed using \u2018structurefrom-motion\u2019 (SFM) algorithm and was matched to the DTM in order to derive the camera\u2019s pose.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "Using SFM algorithm, which does not make any use of the information obtained from the DTM but bases its estimate on the flow-field alone, positions their technique under the same critique that applies for SFM algorithms [8].", "startOffset": 220, "endOffset": 223}, {"referenceID": 8, "context": "The algorithm presented in the previous work [9] does not require an intermediate explicit reconstruction of the 3D world.", "startOffset": 45, "endOffset": 48}, {"referenceID": 9, "context": "sequence of images that covers a field of view of 360 degrees [10], [11], [12], [13], [14], [15], [16], [17], [18], [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "sequence of images that covers a field of view of 360 degrees [10], [11], [12], [13], [14], [15], [16], [17], [18], [19].", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "sequence of images that covers a field of view of 360 degrees [10], [11], [12], [13], [14], [15], [16], [17], [18], [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "sequence of images that covers a field of view of 360 degrees [10], [11], [12], [13], [14], [15], [16], [17], [18], [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "sequence of images that covers a field of view of 360 degrees [10], [11], [12], [13], [14], [15], [16], [17], [18], [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "sequence of images that covers a field of view of 360 degrees [10], [11], [12], [13], [14], [15], [16], [17], [18], [19].", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "sequence of images that covers a field of view of 360 degrees [10], [11], [12], [13], [14], [15], [16], [17], [18], [19].", "startOffset": 98, "endOffset": 102}, {"referenceID": 16, "context": "sequence of images that covers a field of view of 360 degrees [10], [11], [12], [13], [14], [15], [16], [17], [18], [19].", "startOffset": 104, "endOffset": 108}, {"referenceID": 17, "context": "sequence of images that covers a field of view of 360 degrees [10], [11], [12], [13], [14], [15], [16], [17], [18], [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "sequence of images that covers a field of view of 360 degrees [10], [11], [12], [13], [14], [15], [16], [17], [18], [19].", "startOffset": 116, "endOffset": 120}, {"referenceID": 19, "context": "In [20] a theorem is presented stating that a catadioptric camera has a single effective viewpoint if and only if the mirrors cross-section is a conic section.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "In this paper the navigation algorithm that was presented in [9] is extended to handle omnidirectional data.", "startOffset": 61, "endOffset": 64}, {"referenceID": 8, "context": "As was shown in [9], one of the most important factors that influence the robustness and the accuracy of the navigation algorithm is the complexity of the observed terrain.", "startOffset": 16, "endOffset": 19}, {"referenceID": 20, "context": "As was suggested in [21], M-estimator can be integrated into this scheme to increase its robustness in the presence of outliers.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "Correspondence between about 100 features per camera (300 features all together) was derived using the Lucas-Kanade tracking method [22], [23].", "startOffset": 132, "endOffset": 136}, {"referenceID": 22, "context": "Correspondence between about 100 features per camera (300 features all together) was derived using the Lucas-Kanade tracking method [22], [23].", "startOffset": 138, "endOffset": 142}, {"referenceID": 8, "context": "In [9], the sensitivities of the proposed algorithm were studied.", "startOffset": 3, "endOffset": 6}], "year": 2006, "abstractText": "An algorithm for pose and motion estimation using corresponding features in omnidirectional images and a digital terrain map is proposed. In previous paper, such algorithm for regular camera was considered. Using a Digital Terrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the absolute position and orientation of the camera. In order to do this, the DTM is used to formulate a constraint between corresponding features in two consecutive frames. In this paper, these constraints are extended to handle non-central projection, as is the case with many omnidirectional systems. The utilization of omnidirectional data is shown to improve the robustness and accuracy of the navigation algorithm. The feasibility of this algorithm is established through lab experimentation with two kinds of omnidirectional acquisition systems. The first one is polydioptric cameras while the second is catadioptric camera.", "creator": null}}}