{"id": "1107.0024", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "Complexity Results and Approximation Strategies for MAP Explanations", "abstract": "MAP is the problem of finding a most probable instantiation of a set of variables given evidence. MAP has always been perceived to be significantly harder than the related problems of computing the probability of a variable instantiation Pr, or the problem of computing the most probable explanation (MPE). This paper investigates the complexity of MAP in Bayesian networks. Specifically, we show that MAP is complete for NP^PP and provide further negative complexity results for algorithms based on variable elimination. We also show that MAP remains hard even when MPE and Pr become easy. For example, we show that MAP is NP-complete when the networks are restricted to polytrees, and even then can not be effectively approximated. Given the difficulty of computing MAP exactly, and the difficulty of approximating MAP while providing useful guarantees on the resulting approximation, we investigate best effort approximations. We introduce a generic MAP approximation framework. We provide two instantiations of the framework; one for networks which are amenable to exact inference Pr, and one for networks for which even exact inference is too hard. This allows MAP approximation on networks that are too complex to even exactly solve the easier problems, Pr and MPE. Experimental results indicate that using these approximation algorithms provides much better solutions than standard techniques, and provide accurate MAP estimates in many cases.", "histories": [["v1", "Thu, 30 Jun 2011 20:33:03 GMT  (257kb)", "http://arxiv.org/abs/1107.0024v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["a darwiche", "j d park"], "accepted": false, "id": "1107.0024"}, "pdf": {"name": "1107.0024.pdf", "metadata": {"source": "CRF", "title": "Complexity Results and Approximation Strategies for MAP Explanations", "authors": ["James D. Park", "Adnan Darwiche"], "emails": ["jd@cs.ucla.edu", "darwiche@cs.ucla.edu"], "sections": [{"heading": null, "text": "Given the difficulty of calculating MAP accurately, and the difficulty of approximating MAP while providing useful guarantees for the resulting approximation, we examine approximations to the best of our knowledge and belief. We are introducing a generic MAP approximation system. We provide two instances of the framework: one for networks for which exact conclusions are possible (Pr), and one for networks for which even exact conclusions are too difficult. This enables approximation to MAP in networks that are too complex to solve even the simpler problems accurately, Pr and MPE. Experimental results suggest that the use of these approximation algorithms offers much better solutions than standard techniques and in many cases provides accurate MAP estimates."}, {"heading": "1. Introduction", "text": "The task of calculating the maximum a-posteriori hypothesis (MAP) is to find the most likely configuration of a set of variables that provide partial evidence for the completion of this theory.The focus of this paper is on the complexity of calculating MAP in Bayesian networks and on a class of methods for approximating MAP. The primary reason for this attention is that MPE seems to be a much simpler problem than its MAP generalization. MPE is not always suitable for the task of finding a set of variables that provide complete evidence for the completion of this theory.The primary reason for this attention is that MPE is a much simpler problem than its MAP generalization."}, {"heading": "2. MAP Complexity", "text": "We begin this section by examining some complexity-theoretical classes and terminologies related to the complexity of MAP. Then we examine the complexity of MAP in general, followed by the complexity when the number of MAP variables is limited, and then we look at the complexity of MAP algorithms based on the elimination of variables. Finally, we conclude the complexity section by examining the complexity of MAP on polytrees."}, {"heading": "2.1 Complexity Review", "text": "We assume that the reader is familiar with the basic concepts of complexity theory, such as the hardness and completeness of PP. For PP, the class for which there is a non-deterministic Turing machine, in which the majority of non-deterministic calculations accept whether and only if the string is in the language. PP can be considered the decision version of function class # P., as such PP is a powerful language. In fact, NP is PP, and the inequality is severe unless the polynomial hierarchy collapses to the second level. 1Another idea we need is the concept of an oracle. Sometimes it is useful to ask questions about what could be done if an operation were free. In complexity theory, this is modeled as a turing machine with an oracle."}, {"heading": "2.2 Decision Problems", "text": "We will look at the decision versions of Bayesian network problems in this paper, which we will formally define in this section > In addition, a Bayesian network is a pair (G), where G is a directed acyclic graph (DAG) over variables X, and vice versa a a conditional probability table (CPT) for each variable X and its parents U in the DAG G. That is, for each value x of variable X and each instance u of parent U, the CPT-X-U assigns a number in [0, 1] designated by phenomena x-u to determine the probability x-given u.2 The probability distribution Pr induced by a Bayesian network, and each instantiation u of parent U is given as follows. For a complete instantiation x of network variables X, 1] the probability of x variables we have given is of Pr-stability (algorithm x-xu is xu)."}, {"heading": "2.3 MAP Complexity for the General Case", "text": "Calculating the probability of a complete instance is trivial, so the only real difficulty is to determine which instance to choose. D-PR is PP-complete (Litmman, Majercik, & Pitassi, 2001) - Note that this is a completely different type of problem, characterized by counting instead of optimizing how we must add the probability of network instances. D-PR is PP-complete (Litmman, Majercik, 2001) - Note that this is the decision version that is not the functional version that is # P-complete (Roth, 1996). MAP combines both the counting and the optimization of paradigms."}, {"heading": "2.4 Complexity Parameterized by the Number of Maximization Variables", "text": "Let n be the number of non-evidence variables, and k be the number of maximization variables. In the extreme case of k = 0, this is simply D-PR, so it is PP-complete. Let D-MAPm be the subset of D-MAP problems where k = O (m), and let D-MAPm have the subset of D-MAP problems where n \u2212 k = O (m). We can then look at the complexity of these parameterized problem groups. Primary results are the following: Theorem 4 D-MAPlogn is in P PP, and D-MAPlogn is in NP."}, {"heading": "2.5 Results for Elimination Algorithms", "text": "The solution to the general MAP problem seems out of reach, but what about \"simpler\" networks for eliminating variables? State-of-the-art exact sequencing algorithms (variable elimination, Dechter, 1996), join trees (Lauritzen & Spiegelhalter, 1988; Shenoy & Shafer, 1986; Jensen, Lauritzen, & Olesen, 1990), recursive conditioning (Darwiche, 2001), you can solve Pr and MPE in space and time complexity that is exponentially only in the width of a given elimination order, which allows many networks to be solved with reasonable resources, although the general problems are very difficult. Similarly, state-of-the-art algorithms can solve MAP with time and space that is exponentially only in the width of the applied elimination order, but not all orders can be used. In this section we will examine the complexity of the elimination variables for MAP."}, {"heading": "2.6 MAP on Polytrees", "text": "In fact, it is such that most of them are able to hide themselves in a position to hide when they are able to move to another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live that they live, in which they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they"}, {"heading": "3. Approximating MAP when Inference is Easy", "text": "It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (It is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it. (it.) It. (it.) It is. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it.) It. (it.) It. (it. (it.) is. (it. (it. (it.) It. (it. (it.) It. (it.) It. (it. (it.) It. (it. (it.) It. (it. (it. It.) It. (it. (it.) It. It. (it.) It. (it. It. It. (it. (it.) is. It. (it. (it. It.) It. (it. It. (it.) It. (it.) It. It. (it. (it. It. (it.) It. (it. It. It. It. It. (it.) It. (it. It. It. (it.) It. (it."}, {"heading": "3.1 Computing Neighbor Scores Efficiently", "text": "The key to calculating the neighbor scores is to express the inference problem as a function through the indicators for detection. For each state x of the variable X, the indicator \u03bbx is one if it is compatible with the evidence, and zero otherwise. This is a common technique that is typically used to allow the modeling of a wider range of evidence requirements, for example, enabling evidence claims such as X 6 = x by setting all indicators forX to one. However, we will use it for a different purpose. If the follow-up problem is cast as a function f of the evidence indicators (f) = Pr (e) = x, where all indicators are set to be e compatible, then the inference problem is presented as a function f of the evidence indicators (f)."}, {"heading": "3.2 Search Methods", "text": "We tested two common local search methods, stochastic mountaineering and taboo search. Stochastic mountaineering occurs by repeatedly changing the state of the variable that generates the maximum probability change, or by randomly changing a variable. Figure 5 explicitly specifies the algorithm. Taboo search is similar to mountaineering except that the next state is selected as the best one that has not been visited recently. Since the number of iterations is relatively small, we store all previous states so that a unique point is selected for each iteration. Taboo search pseudo code appears in Figure 6."}, {"heading": "3.3 Initialization", "text": "The quality of the solution returned by a local search routine depends to a large extent on which part of the search space is to be explored. We have implemented several algorithms to compare the solution quality with different initialization schemes. Suppose n is the number of network variables, w is the width of a given elimination order, and m is the number of MAP variables. 1. Random initialization (border). For each MAP variable, we choose a value uniformly from its state set. This method takes O (n exp (w)) time. 2. MPE-based initialization (MPE). We calculate the MPE solution based on the evidence. Then, for each MAP variable, we set its value to the value that the variable in the MPE solution assumes. This method takes O (n exp (w) time. 3. Maximum probability of initialization (ML)."}, {"heading": "3.4 Experimental Results", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4. Approximating MAP when Inference is Hard", "text": "The techniques developed to date depend on the ability to make accurate inferences, and in many networks, even inference is insoluble. In these cases, the approximate inference can be replaced to generate MAP approximations. We examine the use of propagation as approximate inference and the local search for the optimization scheme; iterative propagation is a useful approximate inference algorithm to approach MAP for a number of reasons and has proven to be a very effective and efficient approach method for a variety of domains; it has the ability to approximate MPE, rear margins, and probability of evidence, enabling the same initialization schemes that we used for exact conclusions. Moreover, as we will show in Section 4.2, after a single call to inferences, the scores of neighbors in the search space can be calculated, allowing us to achieve the same linear velocity that we achieved with a similar approach to exact inferences."}, {"heading": "4.1 Belief Propagation Review", "text": "It is a message transmission algorithm in which each node in the network sends a message to its neighbors. These messages, along with the CPTs and evidence, can be used to calculate subordinate margins for all variables. It has been shown that messages in looped networks are no longer guaranteed to be accurate, and successive iterations generally produce different results, so that faith transmission is typically carried out until the message values converge, providing very good approximations for a variety of networks (McEliece, Rodemich, & Cheng, 1995; Murphy, Weiss, & Jordan, 1999) and has recently received a theoretical explanation (Yedidia, Freeman, & Wei\u00df, 2000). Belief propagation works as follows. Each node X, has a proof indicator (X) in which evidence can be entered."}, {"heading": "4.2 Approximating Neighbors\u2019 Scores", "text": "The key we will show next is that we are able to efficiently calculate the quantity Pr (x | e \u2212 X) for each variable X, because we can use this quantity to classify the neighbors according to the desired scale. Especially in polytrees, the incoming messages are independent of the value of the local CPT or any evidence entered. So, if the evidence is left out of the product, Pr (X | e \u2212 X) = \u03b1, UPr (X | U) and ZMZX are obtained. Therefore, we can calculate the above quantity for each variable according to a single belief spread. In networks that are not polytrees, the incoming messages are not necessarily independent of the evidence or the local CPT, but as is the case with other BP methods, we ignore them and hope that it is approximately independent."}, {"heading": "4.3 Experimental Results", "text": "In fact, most of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "5. Conclusion", "text": "MAP is a mathematically very difficult problem that cannot be solved accurately, even for very limited classes (e.g. polytrees) in general. Even an approximation is difficult. However, we can make approaches that are much better than those currently used by practitioners (MPE, ML) by applying approximate optimization and conclusion methods. We showed a method based on faith propagation and stochastic mountaineering that has yielded significant improvements over these methods, broadening the scope for which MAP can be approached to networks that work well with faith propagation."}, {"heading": "Acknowledgement", "text": "This work was partially supported by the MURI grant N00014-00-1-0617."}, {"heading": "Appendix A. Proofs of Theorems", "text": "Indeed it is so that the pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-"}, {"heading": "Appendix B. Generating Random Networks", "text": "We created several types of networks to perform our experiments. We used two methods for generating the structure and a single parametric method for generating the quantifiation.B.1 Generating the network structureThe first method is parameterized by the number of variables N and the connectivity c. This method tends to generate structures with a width similar to that of c. darwiche (2001) provides an algorithmic description. The second method is parameterized by the number of variables N and the probability p of an existing edge. We create an ordered list of N variables and add an edge between the variables X and Y with the probability p.The added edges are directed to the variable later in the order.B.2 Quantification of dependencies Quantification of the quantification method is parameterized by a bias parameter b. The values of the CPTs for the roots are uniformly selected values. The rest of the values are based on a node default."}], "references": [{"title": "An optimal approximation algorithm for Bayesian inference", "author": ["P. Dagum", "M. Luby"], "venue": "Artificial Intelligence,", "citeRegEx": "Dagum and Luby,? \\Q1997\\E", "shortCiteRegEx": "Dagum and Luby", "year": 1997}, {"title": "Recursive conditioning", "author": ["A. Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Darwiche,? \\Q2001\\E", "shortCiteRegEx": "Darwiche", "year": 2001}, {"title": "A differential approach to inference in Bayesian networks", "author": ["A. Darwiche"], "venue": "Journal of the ACM,", "citeRegEx": "Darwiche,? \\Q2003\\E", "shortCiteRegEx": "Darwiche", "year": 2003}, {"title": "Partial abductive inference in Bayesian belief networks using a genetic algorithm", "author": ["L. de Campos", "J. Gamez", "S. Moral"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Campos et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Campos et al\\.", "year": 1999}, {"title": "Mini-buckets: A general scheme for approximate inference", "author": ["R. Dechter", "I. Rish"], "venue": "Tech. rep. R62a,", "citeRegEx": "Dechter and Rish,? \\Q1998\\E", "shortCiteRegEx": "Dechter and Rish", "year": 1998}, {"title": "Bucket elimination: A unifying framework for probabilistic inference", "author": ["R. Dechter"], "venue": "In Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Dechter,? \\Q1996\\E", "shortCiteRegEx": "Dechter", "year": 1996}, {"title": "Inference in belief networks: A procedural guide", "author": ["C. Huang", "A. Darwiche"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Huang and Darwiche,? \\Q1996\\E", "shortCiteRegEx": "Huang and Darwiche", "year": 1996}, {"title": "Bayesian updating in recursive graphical models by local computation", "author": ["F.V. Jensen", "S. Lauritzen", "K. Olesen"], "venue": "Computational Statistics Quarterly,", "citeRegEx": "Jensen et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Jensen et al\\.", "year": 1990}, {"title": "Stochastic local search for Bayesian networks", "author": ["K. Kask", "R. Dechter"], "venue": "In Seventh International Workshop on Artificial Intelligence,", "citeRegEx": "Kask and Dechter,? \\Q1999\\E", "shortCiteRegEx": "Kask and Dechter", "year": 1999}, {"title": "Triangulation of graphs\u2014algorithms giving small total state space", "author": ["U. Kjaerulff"], "venue": "Tech. rep. R-90-09,", "citeRegEx": "Kjaerulff,? \\Q1990\\E", "shortCiteRegEx": "Kjaerulff", "year": 1990}, {"title": "Local computations with probabilities on graphical structures and their application to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of Royal Statistics Society, Series B,", "citeRegEx": "Lauritzen and Spiegelhalter,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "Stochastic boolean satisfiability", "author": ["M. Litmman", "S.M. Majercik", "T. Pitassi"], "venue": "Journal of Automated Reasoning,", "citeRegEx": "Litmman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Litmman et al\\.", "year": 2001}, {"title": "Initial experiments in stochastic satisfiability", "author": ["M. Littman"], "venue": "In Sixteenth National Conference on Artificial Intelligence,", "citeRegEx": "Littman,? \\Q1999\\E", "shortCiteRegEx": "Littman", "year": 1999}, {"title": "The computational complexity of probabilistic planning", "author": ["M. Littman", "J. Goldsmith", "M. Mundhenk"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Littman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1998}, {"title": "The turbo decision algorithm", "author": ["R.J. McEliece", "E. Rodemich", "J.F. Cheng"], "venue": "In 33rd Allerton Conference on Communications, Control and Computing,", "citeRegEx": "McEliece et al\\.,? \\Q1995\\E", "shortCiteRegEx": "McEliece et al\\.", "year": 1995}, {"title": "Stochastic greedy search: Efficiently computing a most probable explanation in Bayesian networks", "author": ["O.J. Mengshoel", "D. Roth", "D.C. Wilkins"], "venue": "Tech. rep. UIUCDSR-2000-2150, U of Illinois Urbana-Champaign", "citeRegEx": "Mengshoel et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Mengshoel et al\\.", "year": 2000}, {"title": "Loopy belief propagation for approximate inference: an emperical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": "In Proceedings of Uncertainty in AI", "citeRegEx": "Murphy et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 1999}, {"title": "The complexity of Markov decision processes", "author": ["C. Papadimitriou", "J. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "A differential semantics for jointree algorithms", "author": ["J. Park", "A. Darwiche"], "venue": "In Neural Information Processing Systems (NIPS)", "citeRegEx": "Park and Darwiche,? \\Q2003\\E", "shortCiteRegEx": "Park and Darwiche", "year": 2003}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "On the hardness of approximate reasoning", "author": ["D. Roth"], "venue": "Artificial Intelligence,", "citeRegEx": "Roth,? \\Q1996\\E", "shortCiteRegEx": "Roth", "year": 1996}, {"title": "Propagating belief functions with local computations", "author": ["P.P. Shenoy", "G. Shafer"], "venue": "IEEE Expert,", "citeRegEx": "Shenoy and Shafer,? \\Q1986\\E", "shortCiteRegEx": "Shenoy and Shafer", "year": 1986}, {"title": "Finding MAPs for belief networks is NP\u2013hard", "author": ["S.E. Shimony"], "venue": "Artificial Intelligence,", "citeRegEx": "Shimony,? \\Q1994\\E", "shortCiteRegEx": "Shimony", "year": 1994}, {"title": "PP is as hard as the polynomial-time hierarchy", "author": ["S. Toda"], "venue": "SIAM Journal of Computing,", "citeRegEx": "Toda,? \\Q1991\\E", "shortCiteRegEx": "Toda", "year": 1991}, {"title": "Generalized belief propagation", "author": ["J. Yedidia", "W. Freeman", "Y. Weiss"], "venue": "In NIPS,", "citeRegEx": "Yedidia et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2000}], "referenceMentions": [{"referenceID": 23, "context": "This is a direct result of Toda\u2019s theorem (Toda, 1991).", "startOffset": 42, "endOffset": 54}, {"referenceID": 22, "context": "D-MPE is NP-complete (Shimony, 1994).", "startOffset": 21, "endOffset": 36}, {"referenceID": 20, "context": "D-PR is PP-complete (Litmman, Majercik, & Pitassi, 2001)\u2014notice that this is the complexity of the decision version, not the functional version which is #P-complete (Roth, 1996).", "startOffset": 165, "endOffset": 177}, {"referenceID": 12, "context": "This result was stated without proof by Littman (1999).", "startOffset": 40, "endOffset": 55}, {"referenceID": 13, "context": "NP contains other important AI problems, such as probabilistic planning problems (Littman et al., 1998).", "startOffset": 81, "endOffset": 103}, {"referenceID": 23, "context": "In fact, NP contains the entire polynomial hierarchy (Toda, 1991).", "startOffset": 53, "endOffset": 65}, {"referenceID": 5, "context": "5 Results for Elimination Algorithms Solution to the general MAP problem seems out of reach, but what about for \u201ceasier\u201d networks? State\u2013of\u2013the\u2013art exact inference algorithms (variable elimination (Dechter, 1996), join trees (Lauritzen & Spiegelhalter, 1988; Shenoy & Shafer, 1986; Jensen, Lauritzen, & Olesen, 1990), recursive conditioning (Darwiche, 2001)) can compute Pr and MPE in space and time complexity that is exponential only in the width of a given elimination order.", "startOffset": 197, "endOffset": 212}, {"referenceID": 1, "context": "5 Results for Elimination Algorithms Solution to the general MAP problem seems out of reach, but what about for \u201ceasier\u201d networks? State\u2013of\u2013the\u2013art exact inference algorithms (variable elimination (Dechter, 1996), join trees (Lauritzen & Spiegelhalter, 1988; Shenoy & Shafer, 1986; Jensen, Lauritzen, & Olesen, 1990), recursive conditioning (Darwiche, 2001)) can compute Pr and MPE in space and time complexity that is exponential only in the width of a given elimination order.", "startOffset": 341, "endOffset": 357}, {"referenceID": 9, "context": "For each network, we computed the width using the min\u2013fill heuristic (Kjaerulff, 1990; Huang & Darwiche, 1996).", "startOffset": 69, "endOffset": 110}, {"referenceID": 15, "context": "6 Similar reductions were used by Papadimitriou and Tsitsiklis (1987) and Littman et al.", "startOffset": 34, "endOffset": 70}, {"referenceID": 12, "context": "6 Similar reductions were used by Papadimitriou and Tsitsiklis (1987) and Littman et al. (1998) relating to partially observable Markov decision problems, and probabilistic planning respectively.", "startOffset": 74, "endOffset": 96}, {"referenceID": 2, "context": "We can use the jointree algorithm (Park & Darwiche, 2003), or the differential inference approach (Darwiche, 2003) to compute all of the partial derivatives efficiently.", "startOffset": 98, "endOffset": 114}, {"referenceID": 19, "context": "Belief propagation was introduced as an exact inference method on polytrees (Pearl, 1988).", "startOffset": 76, "endOffset": 89}, {"referenceID": 16, "context": "We implemented the second scheme since empirically it seems to converge faster than the first scheme (Murphy et al., 1999).", "startOffset": 101, "endOffset": 122}, {"referenceID": 1, "context": "Darwiche (2001) provides an algorithmic description.", "startOffset": 0, "endOffset": 16}], "year": 2011, "abstractText": "MAP is the problem of finding a most probable instantiation of a set of variables given evidence. MAP has always been perceived to be significantly harder than the related problems of computing the probability of a variable instantiation (Pr), or the problem of computing the most probable explanation (MPE). This paper investigates the complexity of MAP in Bayesian networks. Specifically, we show that MAP is complete for NP and provide further negative complexity results for algorithms based on variable elimination. We also show that MAP remains hard even when MPE and Pr become easy. For example, we show that MAP is NP-complete when the networks are restricted to polytrees, and even then can not be effectively approximated. Given the difficulty of computing MAP exactly, and the difficulty of approximating MAP while providing useful guarantees on the resulting approximation, we investigate best effort approximations. We introduce a generic MAP approximation framework. We provide two instantiations of the framework; one for networks which are amenable to exact inference (Pr), and one for networks for which even exact inference is too hard. This allows MAP approximation on networks that are too complex to even exactly solve the easier problems, Pr and MPE. Experimental results indicate that using these approximation algorithms provides much better solutions than standard techniques, and provide accurate MAP estimates in many cases.", "creator": "dvips(k) 5.92a Copyright 2002 Radical Eye Software"}}}