{"id": "1107.0027", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "Effective Dimensions of Hierarchical Latent Class Models", "abstract": "Hierarchical latent class (HLC) models are tree-structured Bayesian networks where leaf nodes are observed while internal nodes are latent. There are no theoretically well justified model selection criteria for HLC models in particular and Bayesian networks with latent nodes in general. Nonetheless, empirical studies suggest that the BIC score is a reasonable criterion to use in practice for learning HLC models. Empirical studies also suggest that sometimes model selection can be improved if standard model dimension is replaced with effective model dimension in the penalty term of the BIC score. Effective dimensions are difficult to compute. In this paper, we prove a theorem that relates the effective dimension of an HLC model to the effective dimensions of a number of latent class models. The theorem makes it computationally feasible to compute the effective dimensions of large HLC models. The theorem can also be used to compute the effective dimensions of general tree models.", "histories": [["v1", "Thu, 30 Jun 2011 20:34:11 GMT  (177kb)", "http://arxiv.org/abs/1107.0027v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["t kocka", "n l zhang"], "accepted": false, "id": "1107.0027"}, "pdf": {"name": "1107.0027.pdf", "metadata": {"source": "CRF", "title": "Effective Dimensions of Hierarchical Latent Class Models", "authors": ["Nevin L. Zhang", "Tom\u00e1\u0161 Ko\u010dka"], "emails": ["lzhang@cs.ust.hk", "kocka@lisp.vse.cz"], "sections": [{"heading": null, "text": "In this thesis, we demonstrate a theorem that relates the effective dimension of an HLC model to the effective dimensions of a number of latent class models, which makes it mathematically possible to calculate the effective dimensions of large HLC models. It can also be used to calculate the effective dimensions of general tree models."}, {"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. Effective Dimensions of Bayesian Networks", "text": "In this paper, we use uppercase letters such as X and Y to denote variables and lowercase letters such as x and y to denote states of variables; the domain and cardinality of a variable X is also denoted by uppercase letters such as Y and is sometimes referred to as states of Y. We only look at variables that have a limited number of states; we look at a Bayesian network model that may contain latent variables; the default dimension ds (M) of the linear independent parameters in the default parameters; the parameters denote, for each variable and each parent configuration of the variables."}, {"heading": "3. Selecting among HLC Models", "text": "A hierarchical latent class model (HLC) is a Bayesian network in which (1) the network structure is a rooted tree and (2) the variables at the leaf nodes are observed and all other variables are not. The observed variables are sometimes referred to as manifest variables and all other variables as latent variables. Figure 1 shows the structures of two HLC models. A latent class model (LC) is an HLC model in which there is only one latent variable.The topic of this paper is the calculation of effective dimensions of HLC models. As mentioned in the introduction, this is interesting because effective dimensions, when used in the BIC score, give us a better approximation of the boundary probability. In this section, we will give an example to illustrate that the use of effective dimensions sometimes leads to better model selection."}, {"heading": "3.1 An Example of Model Selection", "text": "Consider the two HLC models shown in Figure 1. In one experiment, we randomly instantiated the parameters of M1 and sampled a set of D1 of 10,000 datasets on the observed variables. Then, we evaluated SHC and HSHC on dataset D1 under the guidance of the BIC score. Both algorithms produced Model M2. Below, we explain why, based on D1, M2 would be preferred to M1 if BIC is used for model selection and why M1 would be preferred if BICe is used instead. We argue that M1 based on D1 and hence BICe is a better scoring metric for this case. The BIC and BICe values of a Model M in the face of a dataset D are defined as follows: BIC (M | D) = logP (D | M) = MICE values (M), which are MIC2 logical."}, {"heading": "3.2 Regularity", "text": "Let us now consider another model M \u2032 1, which is the same as M1, except that the cardinality of X1 is increased from 2 to 3. It is easy to show that M2 includes M \u2032 1 and vice versa. So the two models are equivalent in terms of their ability to represent probability distributions of the observed variables, and they are therefore considered to be slightly equivalent. However, M \u2032 1 has more standard parameters than M2, and therefore we would always prefer M2 over M \u2032 1. To formalize this consideration, we introduce a concept of regularity. For a latent variable Z in an HLC model, its neighbors (parents and children) count as X1, X2, Xk. An HLC model is regular if for each latent variable Z, | Z | \u2264 k i = 1 Xi | maxki = 1 | Xi de, (1) and holds strict inequality when Z has two neighbors and at least one of them is a latent node."}, {"heading": "3.3 The CS and CSe Scores", "text": "We have argued for empirical reasons that the BIC score is a reasonable scoring function that can be used to learn HLC models, and that the BICe score can sometimes improve model selection, but the two scores are not free of problems. One problem is that their derivation as Laplace approximations of the limit probability at the boundary of the parameter space is not valid; the CS score facilitates this problem in a way. It is the BIC score based on completed data and the BIC score based on the original data. In other words, it is two Laplace approximations of the limit probability. It allows errors in the two approximate values to be offset against each other. Chickering and Heckerman (1997) have empirically found that the CS score is a fairly accurate approximation of the limit probability and robust at the boundary probability of parameter1. The definition of regularity differs from the 2002 paragraph, which is stated in this paragraph, although the two are lower in this paragraph)."}, {"heading": "4. Effective Dimensions of HLC Models", "text": "As we have seen, the effective model dimension is interesting for P (Z | X) for a number of reasons. Our main result in this thesis is a theorem on the effective dimension de (M) of a regular HLC model M, which contains more than one latent variable. Let X be the root of M, which is obviously a latent node. Since there are at least two latent nodes, there must be another latent node Z, which is a child of X. In the following, we will use the terms X-branch and Z-branch to refer respectively to the sets of nodes separated by X or X. Let Y be the set of observed variables in the Z-branch and let O be the set of all other observed variables. Note that the X-branch is the node X. The relationship between X, Z, Y and O is shown in the linkiest image of Figure 2.The default parameters of M contain parameters for P (parameters) and for Z (P)."}, {"heading": "In words, the effective dimension of M equals the sum of the effective dimensions of M1 and M2 minus the number of common parameters that M1 and M2 share.", "text": "To appreciate the importance of this theorem, we consider the task of calculating the effective dimension of a regular HLC model that contains two or more latent nodes. By repeatedly applying the theorem, we can reduce the task to partial tasks of calculating effective dimensions of LC models. As an example, we consider the HLC model represented by the image on the left of Figure 3. Theorem 1 allows us to divide the HLC model into five LC models, the right of Figure 3.How could we calculate the effective dimension of an LC model? One possibility is to use the algorithm proposed by Geiger et al. (1996). \u2212 The algorithm first symbolically calculates the jacobic matrix that is possible based on the assumption 1. Then it randomly assigns values to the parameters that result in a numerical dimension X3. The rank of the numerical matrix is calculated by means of diagramming."}, {"heading": "5. Proof of Main Result", "text": "This section is devoted to the proof of theorem 1. We begin with some properties of Jacobite matrices of Bayesian network models."}, {"heading": "5.1 Properties of Jacobian Matrices", "text": "Consider the Jacobic matrix JM of a Bayesian network model M. It is a matrix defined by the parameters ~ \u03b8 of M. Let us leave v1, v2,. < vm are column vectors of JM. Lemma 1 A number of column vectors v1, v2,.., vm of the Jacobic matrix JM are either linear everywhere or linearly independent almost everywhere. They are linearly everywhere dependent if and only if there is at least one column vj that can be expressed as a linear combination of other column vectors. Proof: Consider the diagramming of the following transposed matrix: [v1, v2,.] T. According to assumption 1, elements of the matrix are polynomials, which are polynomials, which are polynomials (of ~ 2001)."}, {"heading": "5.2 Proof of Theorem 1", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "5.3 Proof of Claim 1", "text": "Lemma 3 Let Z be a latent nodes in an HLC model M and Y be the set of the observed nodes in the subtree (1).If M's regular, then we can set conditional distributions of nodes in the subtree in such as as as as an injective mapping \u03c1 from A to Z in the sense that there is only one latent node, namely Z. In this case, Z is the parentage of all nodes in Y. Enumerate all these nodes as Y1, Y2,., Yk. Because M is regular, we have the case if there is only one latent node, namely Z. In this case, Z is the parentage of all nodes in Y. Enumerate all these nodes as Y1, Y."}, {"heading": "5.4 Proof of Claim 2", "text": "Each column vector of JM1 depends linearly on vectors listed in (4) everywhere. Note that \u2202 P (O, Y) zipzipzipzipp (0) zipp (O, Z) zipzipp (0) i, i = 1,..., k0 zipp (O, Y) zipzipp (1) i = \u2211 ZP (Y | Z) zipp (O, Z) zipp (1) i, i = 1,.., k1.Therefore, any column vector of JM that matches vectors in JM1 linearly depends on the first k0 + r vectors listed in (6) everywhere. By symmetry, each column vector of JM that matches vectors in JM2 linearly depends on the first k0 and the last vectors in (6) everywhere."}, {"heading": "5.5 Proof of Claim 3", "text": "We prove this assertion by contradiction i = 1c (2) i (O) i (O) i (O) i (O) i (O) i i (O) i (O) i (O) i (O) i (O) i (O) i (O) i (O) i (O) i (O) i (O) i i (O) i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i. \"i.\" i \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i\" i. \"i.\" i. \"i.\" i \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \"i.\" i. \""}, {"heading": "6. Effective Dimensions of Trees", "text": "When we root a tree model to one of its nodes, we obtain a tree-structured Bayesian network model. In a tree model, we define leaf nodes as those that have only one neighbor. An HLC model is a tree model in which all leaf nodes are observed while all others are latent. It turns out that Theorem 1 allows us to calculate the effective dimension of each tree model. Let's consider an arbitrary tree model. If some of its leaf nodes are latent, we can remove such nodes without compromising their effective dimension.After removing latent leaf nodes, all leaf nodes are observed. If some non-leaf nodes are observed, we can decompose the model into submodels on each observed non-leaf node. The following theory tells us how the model and submodels stand in relation to the terms of the effective dimensions.Theorem 2 we are observed in a submodel."}, {"heading": "7. Concluding Remarks", "text": "In this paper, we examine the effective dimensions of HLC models. The work is motivated by empirical evidence that the BIC performs quite well when using several climbing algorithms to learn HLC models, and that the BICe score sometimes leads to a better model selection than the BIC score. We have proven a theorem that relates the effective dimension of an HLC model to the effective dimensions of two other HLC models that contain less latent variables. Repeated application of the theorem allows the task of calculating the effective dimension of an HLC model to be reduced to partial tasks of calculating effective dimensions of LC models. This and our main theorem allow the calculation of the effective dimensions of any HLC model. In addition, we have proven a theorem on effective dimensions of general tree models."}, {"heading": "Acknowledgements", "text": "We would like to thank Poul S. Eriksen, Finn V. Jensen, Jiri Vomlel, Marta Vomlelova, Thomas D. Nielsen, Olav Bangso, Jose Pena, Kristian G. Olesen. We are also grateful to the renowned reviewers whose comments have helped us greatly to improve this work. Research on this work has been supported in part by the GA CR Scholarship 201 / 02 / 1269 and the Hong Kong Research Grant Council under the grant HKUST6088 / 01E."}], "references": [{"title": "A new look at the statistical model identification", "author": ["H. Akaike"], "venue": "IEEE Trans. Autom. Contr.,", "citeRegEx": "Akaike,? \\Q1974\\E", "shortCiteRegEx": "Akaike", "year": 1974}, {"title": "Latent variable models and factor analysis, 2nd edition", "author": ["D.J. Bartholomew", "M. Knott"], "venue": "Kendall\u2019s Library of Statistics", "citeRegEx": "Bartholomew and Knott,? \\Q1999\\E", "shortCiteRegEx": "Bartholomew and Knott", "year": 1999}, {"title": "Bayesian classification (AutoClass): Theory and results", "author": ["P. Cheeseman", "J. Stutz"], "venue": "Advancesin Knowledge Discovery and Data Mining,", "citeRegEx": "Cheeseman and Stutz,? \\Q1995\\E", "shortCiteRegEx": "Cheeseman and Stutz", "year": 1995}, {"title": "Efficient Approximations for the Marginal Likelihood of Bayesian Networks with Hidden variables", "author": ["M. Chickering D", "D. Heckerman"], "venue": "Machine Learning,", "citeRegEx": "D. and Heckerman,? \\Q1997\\E", "shortCiteRegEx": "D. and Heckerman", "year": 1997}, {"title": "Probabilistic networks and expert", "author": ["R.G. Cowell", "A.P. Dawid", "S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": null, "citeRegEx": "Cowell et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Cowell et al\\.", "year": 1999}, {"title": "Dimension correction for hierarchical latent class models", "author": ["T. Kocka", "N.L. Zhang"], "venue": "in Proc. of the 18th Conference on Uncertainty in Artificial Intelligence (UAI-02)", "citeRegEx": "Kocka and Zhang,? \\Q2002\\E", "shortCiteRegEx": "Kocka and Zhang", "year": 2002}, {"title": "Asymptotic model selection for directed networks with hidden variables", "author": ["D. Geiger", "D. Heckerman", "C. Meek"], "venue": "In Proc. of the 12th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Geiger et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Geiger et al\\.", "year": 1996}, {"title": "Exploratory latent structure analysis using both identifiable and unidentifiable", "author": ["L.A. Goodman"], "venue": "models. Biometrika,", "citeRegEx": "Goodman,? \\Q1974\\E", "shortCiteRegEx": "Goodman", "year": 1974}, {"title": "Latent structure analysis", "author": ["P.F. Lazarsfeld", "N.W. Henry"], "venue": null, "citeRegEx": "Lazarsfeld and Henry,? \\Q1968\\E", "shortCiteRegEx": "Lazarsfeld and Henry", "year": 1968}, {"title": "Asymptotic model selection for Naive Bayesian networks. UAI-02", "author": ["D. Rusakov", "D. Geiger"], "venue": null, "citeRegEx": "Rusakov and Geiger,? \\Q2002\\E", "shortCiteRegEx": "Rusakov and Geiger", "year": 2002}, {"title": "Automated analytic asymptotic evaluation of marginal likelihood for latent models. UAI-03", "author": ["D. Rusakov", "D. Geiger"], "venue": null, "citeRegEx": "Rusakov and Geiger,? \\Q2003\\E", "shortCiteRegEx": "Rusakov and Geiger", "year": 2003}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "In Annals of Statistics,", "citeRegEx": "Schwarz,? \\Q1978\\E", "shortCiteRegEx": "Schwarz", "year": 1978}, {"title": "On the geometry of Bayesian graphical models with hidden variables", "author": ["R. Settimi", "J.Q. Smith"], "venue": "In Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Settimi and Smith,? \\Q1998\\E", "shortCiteRegEx": "Settimi and Smith", "year": 1998}, {"title": "Geometry, moments and Bayesian networks with hidden variables", "author": ["R. Settimi", "J.Q. Smith"], "venue": "In Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "Settimi and Smith,? \\Q1999\\E", "shortCiteRegEx": "Settimi and Smith", "year": 1999}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["L. Zhang N"], "venue": null, "citeRegEx": "N.,? \\Q2002\\E", "shortCiteRegEx": "N.", "year": 2002}, {"title": "Learning hierarchical latent class models", "author": ["N.L. Zhang", "T. Kocka", "G. Karciauskas", "F.V. Jensen"], "venue": "Technical Report HKUST-CS03-01,", "citeRegEx": "Zhang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2003}, {"title": "Structural EM for Hierarchical Latent Class Models", "author": ["N.L. Zhang"], "venue": "Technical Report HKUST-CS03-06,", "citeRegEx": "Zhang,? \\Q2003\\E", "shortCiteRegEx": "Zhang", "year": 2003}, {"title": "Hierarchical latent class models for cluster analysis", "author": ["N.L. Zhang"], "venue": "Journal of Machine Learning Research, to appear", "citeRegEx": "Zhang,? \\Q2003\\E", "shortCiteRegEx": "Zhang", "year": 2003}], "referenceMentions": [{"referenceID": 8, "context": "They generalize latent class models (Lazarsfeld and Henry, 1968) and were first identified as a potentially useful class of Bayesian networks by Pearl (1988).", "startOffset": 36, "endOffset": 64}, {"referenceID": 11, "context": "The BIC score (Schwarz, 1978) is a popular metric that researchers use to select among Bayesian network models.", "startOffset": 14, "endOffset": 29}, {"referenceID": 11, "context": "When all variables are observed, the BIC score is an asymptotic approximation of (the logarithm) of the marginal likelihood (Schwarz, 1978).", "startOffset": 124, "endOffset": 139}, {"referenceID": 6, "context": "When latent variables are present, the BIC score is no longer an asymptotic approximation of the marginal likelihood (Geiger et al., 1996).", "startOffset": 117, "endOffset": 138}, {"referenceID": 9, "context": "In fact if we replace standard model dimension with effective model dimension in the BIC score, the resulting scoring function, called the BICe score, is an asymptotic approximation of the marginal likelihood almost everywhere except for some singular points (Rusakov and Geiger, 2002).", "startOffset": 259, "endOffset": 285}, {"referenceID": 7, "context": "They generalize latent class models (Lazarsfeld and Henry, 1968) and were first identified as a potentially useful class of Bayesian networks by Pearl (1988). We are concerned with learning HLC models from data.", "startOffset": 37, "endOffset": 158}, {"referenceID": 15, "context": "There are three related searchbased algorithms for learning HLC models, namely double hill-climbing (DHC) (Zhang, 2002), single hill-climbing (SHC) (Zhang et al., 2003), and heuristic SHC (HSHC) (Zhang, 2003).", "startOffset": 148, "endOffset": 168}, {"referenceID": 16, "context": ", 2003), and heuristic SHC (HSHC) (Zhang, 2003).", "startOffset": 34, "endOffset": 47}, {"referenceID": 0, "context": "In the absence of a theoretically well justified model selection criterion, Zhang (2002) tested DHC with four existing scoring functions, namely the AIC score (Akaike, 1974), the BIC score, the Cheeseman-Stutz (CS) score (Cheeseman and Stutz, 1995), and the holdout logarithmic score (HLS)(Cowell et al.", "startOffset": 159, "endOffset": 173}, {"referenceID": 2, "context": "In the absence of a theoretically well justified model selection criterion, Zhang (2002) tested DHC with four existing scoring functions, namely the AIC score (Akaike, 1974), the BIC score, the Cheeseman-Stutz (CS) score (Cheeseman and Stutz, 1995), and the holdout logarithmic score (HLS)(Cowell et al.", "startOffset": 221, "endOffset": 248}, {"referenceID": 4, "context": "In the absence of a theoretically well justified model selection criterion, Zhang (2002) tested DHC with four existing scoring functions, namely the AIC score (Akaike, 1974), the BIC score, the Cheeseman-Stutz (CS) score (Cheeseman and Stutz, 1995), and the holdout logarithmic score (HLS)(Cowell et al., 1999).", "startOffset": 289, "endOffset": 310}, {"referenceID": 0, "context": "In the absence of a theoretically well justified model selection criterion, Zhang (2002) tested DHC with four existing scoring functions, namely the AIC score (Akaike, 1974), the BIC score, the Cheeseman-Stutz (CS) score (Cheeseman and Stutz, 1995), and the holdout logarithmic score (HLS)(Cowell et al., 1999). Both real-world and synthetic data were used. On the real-world data, BIC and CS have enabled DHC to find models that are regarded as the best by domain experts. On the synthetic data, BIC and CS have enabled DHC to find models that either are identical to or resemble closely the true generative models. When coupled with AIC and HLS, on the other hand, DHC performed significantly worse. SHC and HSHC were tested on synthetic data sampled from fairly large HLC models (as much as 28 nodes). Only BIC was used in those tests. In all cases, BIC has enabled SHC and HSHC to find models that either are identical to or resemble closely the true generative models. Those empirical results not only indicate that the algorithms perform well, but also suggest that the BIC is a reasonable scoring function to use for learning HLC models. The experiments also reveal that model selection can sometimes be improved if the BICe score is used instead of the BIC score. We will explain this in detail in Section 3 In order to use the BICe score in practice, we need a way to compute effective dimensions. This is not a trivial task. The effective dimension of an HLC model is the rank of the Jacobian matrix of the mapping from the parameters of the model to the parameters of the joint distribution of the observed variables. The number of rows in the Jacobian matrix increases exponentially with the number of observed variables. The construction of the Jacobian matrix and the calculation of its rank are both computationally demanding. Moreover they have to be done algebraically or with very high numerical precision to avoid degenerate cases. The necessary precision grows with the size of the matrix. Settimi and Smith (1998, 1999) studied effective dimensions for two classes of models: trees with binary variables and latent class (LC) models with two observed variables. They have obtained a complete characterization of these two classes. Geiger et al. (1996) computed the effective dimensions of a number of models.", "startOffset": 160, "endOffset": 2273}, {"referenceID": 0, "context": "In the absence of a theoretically well justified model selection criterion, Zhang (2002) tested DHC with four existing scoring functions, namely the AIC score (Akaike, 1974), the BIC score, the Cheeseman-Stutz (CS) score (Cheeseman and Stutz, 1995), and the holdout logarithmic score (HLS)(Cowell et al., 1999). Both real-world and synthetic data were used. On the real-world data, BIC and CS have enabled DHC to find models that are regarded as the best by domain experts. On the synthetic data, BIC and CS have enabled DHC to find models that either are identical to or resemble closely the true generative models. When coupled with AIC and HLS, on the other hand, DHC performed significantly worse. SHC and HSHC were tested on synthetic data sampled from fairly large HLC models (as much as 28 nodes). Only BIC was used in those tests. In all cases, BIC has enabled SHC and HSHC to find models that either are identical to or resemble closely the true generative models. Those empirical results not only indicate that the algorithms perform well, but also suggest that the BIC is a reasonable scoring function to use for learning HLC models. The experiments also reveal that model selection can sometimes be improved if the BICe score is used instead of the BIC score. We will explain this in detail in Section 3 In order to use the BICe score in practice, we need a way to compute effective dimensions. This is not a trivial task. The effective dimension of an HLC model is the rank of the Jacobian matrix of the mapping from the parameters of the model to the parameters of the joint distribution of the observed variables. The number of rows in the Jacobian matrix increases exponentially with the number of observed variables. The construction of the Jacobian matrix and the calculation of its rank are both computationally demanding. Moreover they have to be done algebraically or with very high numerical precision to avoid degenerate cases. The necessary precision grows with the size of the matrix. Settimi and Smith (1998, 1999) studied effective dimensions for two classes of models: trees with binary variables and latent class (LC) models with two observed variables. They have obtained a complete characterization of these two classes. Geiger et al. (1996) computed the effective dimensions of a number of models. They conjectured that it is rare for the effective and standard dimensions of an LC model to differ. As a matter of fact, they found only one such model. Kocka and Zhang (2002) found quite a number of LC models whose effective and standard dimensions differ.", "startOffset": 160, "endOffset": 2507}, {"referenceID": 6, "context": "The term reflects the fact that, for almost every value of ~ \u03b8, a small enough open ball around T (~ \u03b8) resembles Euclidean space of dimension d (Geiger et al., 1996).", "startOffset": 145, "endOffset": 166}, {"referenceID": 15, "context": "The definition of regularity given in this paper is slightly different from the one given in Zhang (2002). Nonetheless, the two conclusions mentioned in this paragraph remain true.", "startOffset": 93, "endOffset": 106}, {"referenceID": 6, "context": "Just as BICe is better than BIC as approximations of the marginal likelihood (Geiger et al., 1996), CSe is better than CS.", "startOffset": 77, "endOffset": 98}, {"referenceID": 5, "context": "How might one compute the effective dimension of an LC model? One way is to use the algorithm suggested by Geiger et al. (1996). The algorithm first symbolically computes the Jacobian matrix, which is possible due to Assumption 1.", "startOffset": 107, "endOffset": 128}, {"referenceID": 5, "context": "How might one compute the effective dimension of an LC model? One way is to use the algorithm suggested by Geiger et al. (1996). The algorithm first symbolically computes the Jacobian matrix, which is possible due to Assumption 1. Then it randomly assigns values to the parameters, resulting a numerical matrix. The rank of the numerical matrix is computed by diagonalization. Because the rank of Jacobian matrix equals the effective dimension of the LC model almost everywhere, we get the regular rank with probability one. This algorithm has recently been implemented by Rusakov and Geiger (2003). Kocka and Zhang (2002) suggest an alternative algorithm that computes an upper bound.", "startOffset": 107, "endOffset": 599}, {"referenceID": 5, "context": "Kocka and Zhang (2002) suggest an alternative algorithm that computes an upper bound.", "startOffset": 0, "endOffset": 23}], "year": 2011, "abstractText": "Hierarchical latent class (HLC) models are tree-structured Bayesian networks where leaf nodes are observed while internal nodes are latent. There are no theoretically well justified model selection criteria for HLC models in particular and Bayesian networks with latent nodes in general. Nonetheless, empirical studies suggest that the BIC score is a reasonable criterion to use in practice for learning HLC models. Empirical studies also suggest that sometimes model selection can be improved if standard model dimension is replaced with effective model dimension in the penalty term of the BIC score. Effective dimensions are difficult to compute. In this paper, we prove a theorem that relates the effective dimension of an HLC model to the effective dimensions of a number of latent class models. The theorem makes it computationally feasible to compute the effective dimensions of large HLC models. The theorem can also be used to compute the effective dimensions of general tree models.", "creator": "dvips(k) 5.90a Copyright 2002 Radical Eye Software"}}}