{"id": "1107.0031", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "Grounded Semantic Composition for Visual Scenes", "abstract": "We present a visually-grounded language understanding model based on a study of how people verbally describe objects in scenes. The emphasis of the model is on the combination of individual word meanings to produce meanings for complex referring expressions. The model has been implemented, and it is able to understand a broad range of spatial referring expressions. We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework. The implemented system selects the correct referents in response to natural language expressions for a large percentage of test cases. In an analysis of the system's successes and failures we reveal how visual context influences the semantics of utterances and propose future extensions to the model that take such context into account.", "histories": [["v1", "Thu, 30 Jun 2011 20:35:04 GMT  (3216kb)", "http://arxiv.org/abs/1107.0031v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["p gorniak", "d roy"], "accepted": false, "id": "1107.0031"}, "pdf": {"name": "1107.0031.pdf", "metadata": {"source": "CRF", "title": "Grounded Semantic Composition for Visual Scenes", "authors": ["Peter Gorniak", "Deb Roy"], "emails": ["pgorniak@media.mit.edu", "dkroy@media.mit.edu"], "sections": [{"heading": "1. Introduction", "text": "We are introducing a visually based understanding of speech based on a study of how people describe objects in visual scenes as shown in Figure 1. We designed the study to elicit descriptions that would naturally occur in a common frame of reference and that are easy to produce and understand by a human listener. A typical reference term for Figure 1 could be \"the distant purple cone that stands behind a series of green ones.\" Speakers construct expressions to draw listeners \"attention to intended objects. Such reference expressions are successful in communication because speakers and listeners find similar characteristics of the visual scene to stand out, and share an understanding of how language is grounded in relation to those characteristics. This work is a step toward our longer-term goals of developing a talking robot, the Talking Robot, Mavridis, & Roy, 2003) that can seamlessly connect language with perception."}, {"heading": "1.1 Grounded Semantic Composition", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "1.2 Related Work", "text": "In fact, most people who are in a position to understand and understand themselves and their environment put themselves in a situation in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, live, in which they, in which, in which they, in which they, in which they, live, in which they, in which they, in which they, in which, in which they, in which they, in which they, in which, in which they, live, live, in which they, live, in which they, in which they, in which"}, {"heading": "2. A Spatial Description Task", "text": "We designed a task that requires people to describe objects in computer-generated scenes that contain up to 30 objects with random positions on a virtual interface, all of which had identical shapes and sizes and were colored either green or purple. Each object had a 50% chance of being green, otherwise it was purple. We call this task a episcopal task and the resulting speech comprehension model and implemented system simply as a bishop."}, {"heading": "2.1 Motivation for Task Design", "text": "This year, it will be able to establish itself in the region in order to pave the way to the region."}, {"heading": "2.2 Data Collection", "text": "In fact, the fact is that you are able to be in a position without being able to play by the rules."}, {"heading": "2.3 Descriptive Strategies for Achieving Joint Reference", "text": "As we noted earlier, we refer to a combination of a visual feature measured on the current scene (or, in the case of Anaphora, on the previous scene), together with its linguistic implementation, as a descriptive strategy. In this section, we catalog the most common strategies used by descriptors to communicate to listeners. This analysis is strictly based on the developmental data set. We discuss how our implemented system handles these categories in Section 4.We distinguish three subsets of our developmental data: \u2022 A group containing those statement / selection pairs that contain errors. An error may be due to a repair or error by the human speaker, a segmentation error by our language segmentator, or an error by our statement / selection algorithm. \u2022 A group containing those statement / selection pairs that use descriptive strategies other than those that we cover in our computational understanding system (we are covering only select pairs in the 2.1 to 2.1 sections of each)."}, {"heading": "2.3.1 Colour", "text": "In designing the task, we deliberately trivialized the problem of color reference. Objects exist only in two different colors, green and purple. Unsurprisingly, all participants used the terms \"green\" and \"purple\" to refer to these colors. In previous work, we dealt with the problems of learning visually sound models of color words (Roy & Pentland, 2002; Roy, 2002). Here, our focus is on the semantic composition of the terms, which is why we decided to simplify the problem of color naming. Figure 2 shows one of the few cases in which only color is used to select a speaker. \"Most examples in the following sections will consist of colors that are put together with other descriptive strategies. Syntactically, colors are expressed by adjectives (as mentioned:\" green \"and\" violet \"), which always precede the nouns that change them directly. That is, no one has ever said that\" the green side \"would be a modified element in most cases, but rather in the green ones."}, {"heading": "2.3.2 Spatial Regions and Extrema", "text": "In fact, it is such that most of them will be able to put themselves in a situation in which they are able to move, in which they are able to move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they are able to put themselves in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they are able to move, in which they live, in which they live, in which they are able to move, in which they live, in which they are able to live, in which they live, in which they live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they"}, {"heading": "2.3.3 Grouping", "text": "Figure 6 shows two examples of such grouping constructs, the first of which uses an unnumbered group of cones (\"the green cones\"), the second a counting to specify the group (\"three\"). The function of the group differs in the two examples: in the left scene, the participant specifies the group as an orientation point to serve in a spatial relationship (see Section 2.3.4), while in the right scene, the participants first specify a group containing the target object, and then output a different description for selection within that group. Note that grouping alone never produces an individual reference, so in all cases, the participants compose grouping constructs with additional reference tactics (mainly extreme and spatial relationships); the participants used grouping to identify objects in 12% of the data and 10% of the clean data; they selected objects within the described groups in 7.5% of the data (predominantly extreme and spatial relationships) (two% of the \"clean data\") and 8% of the \"clean data (\" 8% of the \")."}, {"heading": "2.3.4 Spatial Relations", "text": "As mentioned in Section 2.3.3, participants sometimes used spatial relationships between objects or groups of objects. Examples of such relationships are expressed by prepositions such as \"bottom\" or \"back\" and phrases such as \"left of\" or \"before.\" We have already seen an example of a spatial relationship between a group of objects in Figure 6, and Figure 7 shows two other examples that include spatial relations between individual objects: the first example is one of the few examples of \"purely\" spatial relations between two individual objects that are referred to only by color; the second example is a more typical one that combines the spatial relationship with another strategy, in this case an extreme (as well as two language errors by the recorder). Participants used spatial relations in 6% of the data (7% of the clean data)."}, {"heading": "2.3.5 Anaphora", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "In a number of cases participants used anaphoric references to the previous object removed during the description task. Figure 8 shows a sequence of two scenes and corresponding utterances in which the second utterance refers back to the object selected in the first.", "text": "Participants used spatial relationships in 4% of the data (3% of the clean data)."}, {"heading": "2.3.6 Other", "text": "In addition to the phenomena listed in the preceding sections, the participants used a small number of other descriptive strategies. Some that occurred more than once, but which we have not yet taken into account in our mathematical model, are selection by distance (lexicalized as \"near\" or \"next to\"), selection by neighborhood (\"the green surrounded by violet\"), selection by symmetry (\"one versus the other\"), and selection by something like local connectivity (\"the second from the left in the row of the five violet\"). There are also other types of groupings, for example grouping by linearity (\"the row of the green,\" \"the three violet on a diagonal\"), and selection of objects within a group by number (\"the second from the left in the row of the five violet\"), which we do not cover here. Each of these strategies appears less frequently in the data than the anaphora (it occurs in 4% of utterances, see the previous section)."}, {"heading": "2.4 Summary", "text": "The preceding sections include strategies for describing objects. A computer system that understands utterances based on these strategies must meet the following requirements: \u2022 The system must have access to the visual scene and be able to calculate visual features used by human speakers: natural groupings, distances between objects, orders, and spatial relations \u2022 It must have a robust language analysis mechanism that detects grammatical patterns related to descriptive strategies \u2022 The feed into the analysis mechanism must be a visually sound lexicon; each entry in this lexicon must contain information about which descriptive strategies it participates in and how these descriptive strategies combine with others \u2022 The semantic interpretation and composition machines must be embedded in the analysis process \u2022 The system must be able to interpret the results of the omission of an utterance and best guess which describes the entire utterance."}, {"heading": "3. The Understanding Framework", "text": "In this section we will describe in detail the components of our episcopal understanding system, focusing on how they fit together to function as a visually grounded understanding system. We will treat the episcopal visual system, its parser and its lexicon, and give a brief overview of how our implementation of descriptive strategies fits into the framework."}, {"heading": "3.1 Synthetic Vision", "text": "Instead of relying on the information we use to represent the scenes in Bishop, which include 3D object locations and viewing angle, we implemented a simple synthetic visual algorithm in 2001. This algorithm generates a map that maps each pixel of the rendered image to one of the objects or the background. In addition, we use full color information for each pixel drawn in the rendered scene. Our goal is to loosely simulate the view of a camera focused on a scene of real-world objects, the situation our robots find themselves in. We have successfully migrated models from synthetic vision (Roy, 2002) to computer vision (Roy et al., 2002) in the past, and set the plan on a similar route to deploy Bishop - many of the tough problems of object detection as well as lighting and noise robustness do not need to be solved in synthetic cases, but we hope that transferring back to the camera will be facilitated by working from a 2D image."}, {"heading": "3.2 Knowledge Representation", "text": "Objects are represented in our system as integer IDs. For each ID or group of IDs, the vision system can calculate the visual characteristics described in Section 3.1, based on the corresponding set of pixels in the image. The differentiation ID along with the visual characteristics represents the entire knowledge of the system about the objects present in the scene. The system can also instantiate new objects in the vision system from the convex shell of groups of other objects. The system also remembers the ID of the last removed object and can ask the vision system to perform a feature calculation on the visual scene as it was before the object was removed. Groups of objects have their own integer IDs so that they can be treated as objects themselves (all visual characteristics are available to them). Their IDs are stored together with a list of their constituent objects, so that groups can be broken apart if necessary. Finally, as in the lexicon file in Annex B, each word is visible with a series of typical parameters."}, {"heading": "3.3 Lexical Entries and Concepts", "text": "In this context, it should be noted that the solution to the problem is not only a problem, but also a problem that cannot be solved. (...) In this context, it should be noted that the solution to the problem is a problem that cannot be solved. (...) In this context, it should be noted that the solution to the problem is a problem that cannot be solved. (...) In this context, it should be noted that the solution to the problem is a problem that cannot be solved. (...) In this context, it should be noted that it is a problem that cannot be solved. (...)"}, {"heading": "3.4 Parsing", "text": "In fact, most of them are able to move in a direction in which they are in a position in which they are in."}, {"heading": "3.5 Post-Parse Filtering", "text": "First, this algorithm extracts the longest components from the diagram that are marked as object-related, assuming that analyzing a larger portion of the utterance implies a better understanding, and the filtering process then checks these candidates for consistency based on the following criteria: \u2022 All candidates must refer to either a group or to a single object \u2022 If candidates are marked as referring to a uniquely specified single object, they must clearly select a referent \u2022 The referent in the specified object case must be the same for all candidates \u2022 If candidates are marked as selecting a group, everyone must select the same group. If one of these consistency checks fails, the filter algorithm can provide accurate information about what kind of inconsistency has occurred (within the ambiguity group, conflicting components that meet the description), which components were involved in the inconsistency object. If we ignore this object, we can select the individual objects best (according to consistency)."}, {"heading": "4. Semantic Composition", "text": "Most of the composers presented here follow the same compositional scheme: they take one or more concepts as arguments and produce a different concept that refers to a possibly different group of objects. Concepts refer to objects with real numbered values that indicate the strength of the reference. Composers can introduce new objects, even those that do not exist in the scene as such, and they can introduce new types of objects (e.g. groups of objects that are referred to as if they were an object). To perform compositions, each concept provides functionality to produce a single speaker or a group of referents. The single object produced simply has the maximum reference strength, while a group is created by using a reference strength threshold below which objects are not considered as possible references of that concept. The threshold is relative to the minimum and maximum reference strength of the concept."}, {"heading": "4.1 Colour - Probabilistic Attribute Composers", "text": "As mentioned in Section 3.1, we have decided not to use the information used to render the scene and therefore need to restore color information from the final rendered image. This is not a serious problem as Bishop only presents virtual objects in two colors. The renderer produces color variations on objects due to different angles and distances from light sources and the camera. The color cross section of the 2D projection of each object also differs due to occlusion by other objects. To make our frame more easily transferable to a more noisy visual system, we worked within a probabilistic frame. We separately collected a number of labeled instances of \"green\" and \"purple\" cones and estimated a three-dimensional Gaussian distribution from the average red, green and blue values of each pixel belonging, for example. When we were asked to compose with a specific concept, this type of probabilistic attributes assigns each object referred to by the source concept, the probability function of the measured color of the object based on the probability function."}, {"heading": "4.2 Spatial Extrema and Spatial Regions - Ordering Composers", "text": "To determine spatial regions and extremes, the composer arranges objects along a particular dimension of characteristics (e.g. x-coordinate relative to a group) and selects referents at an extreme end of the order. To do this, he assigns an exponential weight function to objects. The maximum case is weighted similarly, but using the reverse order, the fraction of the object is subtracted in the order in which v is its value along the specified dimension of characteristics, normalized to lie between 0 and 1 for the objects under consideration; the maximum case is similarly weighted, with the reverse order subtracting the fraction in the exponent of 2 (imax \u2212 i) (2 \u2212 v), with imax the number of objects under consideration ranging from 0 to 1. This formula causes reference weights to drop exponentially, both with their position in the order and with their distance from the extreme object."}, {"heading": "4.3 Grouping Composers", "text": "For non-numbered groupings (for example, when the descriptor says \"group\" or \"cone\"), the grouping composer searches the scene for object groups, all within a maximum distance threshold from another group member. Currently, this threshold is set manually, based on a small number of random scenes in which designers identify isolated groups and set the threshold to find them correctly, but not others. It only takes into account objects that are referred to by the concept as an argument. For numbered groups (\"two,\" \"three\"), the composer applies the additional restriction that the groups must contain the correct number of objects, but not others. Reference strengths for the concept are determined by the average distance from objects within the group. We acknowledge that this approach to grouping is simplistic, and we are currently investigating more powerful visual grouping algorithms that take topological features into account."}, {"heading": "4.4 Spatial Relations - Spatial Composers", "text": "The spatial semantic composer uses a version of the Attentional Vector Sum (AVS) proposed by Regier and Carlson (2001). He calculates an interpolation between the angle between the mass centers of the objects and the angle between the next two points of the objects, in addition to a value relative to the height relative to the top of the boundary object. Although our participants talked about 2D projections of 3D scenes, we found that the AVS distinguishes quite well the spatial relationships used in our data when it is simply applied to 2D projections. Participants often used spatial descriptors such as \"below,\" indicating that they sometimes conceived the scenes as 2D projections of 3D scenes. In a 3D environment, we expect a consistent use of semantic patterns such as \"before\" instead of \"below.\""}, {"heading": "4.5 Anaphoric Composers", "text": "Triggered by words such as \"the\" (as in \"to the left of this\") or \"previous,\" an anaphorical composer produces a concept that refers to a single object, namely the last object removed from the scene during the session. This object specifically identifies the concept as referring not to the current but to the previous visual scene, and any further calculations with this concept are performed in this visual context, for example, when the parser asks the anaphorical composer to add \"that\" to the lexical entry to provide an interpretation of \"that,\" this composer marks the produced concept as a reference to the previous visual scene and uses the previously selected object as the only possible reference. Now, if we look at another composer, let's say the spatial composer who is located \"to the left\" of that entry. \""}, {"heading": "5. Example: Understanding a Description", "text": "To illustrate the workings of the overall system, we will go through some examples of how Bishop works in detail in this section. Consider the scene in the upper left corner of Figure 9 and the output of the diagram parser for the utterance, \"the violet left\" in Figure 10. The parser finds \"the\" in the lexicon as GRT (article) with a selector composer taking an argument. It finds two lexical entries for \"violet,\" one as CADJ (color lens) and one as N (noun). Each of them has the same composer, a probabilistic attribute marked P (), but the adjective does not expect any argument, while the noun does not expect any. Given that the noun does not expect any arguments and the grammar contains a rule of the form NP, an NP (noun) is applied as a noun to the standard set of objects consisting only of violet objects (the composition of the diagram consisting of T)."}, {"heading": "NP spanning the first two words and again contains only the purple objects, but is marked as unambiguously referring to an object. S(NP) marks the application of this selecting composer called S.", "text": "The parser continues to produce a similar NP covering the first three words by combining the \"violet\" CADJ with \"one\" and the result with \"the.\" The \"on\" P (preposition) is left hanging for the moment because it needs an ingredient that follows it. It contains a modifying semantic composer who simply bypasses the P and applies the first argument to the second. After another \"the,\" \"left\" has several lexical entries: in its ADJ and one of its N forms it contains an ordering semantic composer who takes a single argument, while its second N form contains a spatial semantic composer who needs two arguments to determine a target and a landmark object. At this point the parser can combine \"the\" and \"left\" into two possible NPs, one of which contains the order and the other the spatial composer."}, {"heading": "6. Results and Discussion", "text": "In this section, we first discuss the overall performance of our system based on the data collected, followed by a detailed discussion of the performance of our implemented descriptive strategies."}, {"heading": "6.1 Overall Performance", "text": "This year, the time has come for us to be able to go to a place where we can go to a place where we can go to a place where we can go to a place where we can move."}, {"heading": "6.2 Performance of Composers", "text": "We will now review every descriptive strategy and discuss the successes and failures of our composers who were designed to deal with them."}, {"heading": "6.2.1 Colour", "text": "Due to the simple nature of the color naming in the Bishop task, the likely composers responsible for selecting objects based on color made no mistakes."}, {"heading": "6.2.2 Spatial Regions and Extrema", "text": "In fact, it is the case that most of them are able to put themselves in the centre, without having to put themselves in the centre. (...) It is the case that they are able to put themselves in the centre. (...) It is not the case that they are able to put themselves in the centre. (...) It is the case that they are able to put themselves in the centre. (...) It is the case that they are able to put themselves in the centre. (...) \"(...)\" (...) \"(...)\" (...) \"(...\" (...) \"(...\") (... \"(...)\" (... \") () ()\" (... \") (...\") (... \"() () () (...) () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () ()) () () () () () () () () () () ()) () () () () () () ()) () () ()) () () () ()) () () () () ()) () ()) () () () () () ()) () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () () (() (() () () () () () () ()"}, {"heading": "6.2.3 Grouping", "text": "Our composers who implement the grouping strategies employed by the participants are the simplest of all the composers we have implemented, compared to the depth of the actual phenomenon of visual grouping. The left scene in Figure 16 is an example that our grouping composer can easily handle. In this example, the group of two cones is isolated from all other cones and therefore easy to find by our distance threshold algorithm. In contrast, the right scene is an example that would require much greater sophistication to find the right group. In this scene, the target group of the three cones is not visually isolated and requires additional criteria such as colinarity to make it a candidate at all. Furthermore, there is a second column group of three cones that would easily be the best example of a \"series of three violet cones\" without the absence of the target group. It is only the orientation of the target group to the vertical axis that makes it \"stand out\" and become most likely to be interpreted."}, {"heading": "6.2.4 Spatial Relations", "text": "Any errors that occur in statements that involve spatial relations are due to the fact that the possible boundaries or targets are not correctly identified (grouping or composers of the region may not provide the right references).Our spatial relations composer selects the right speaker in all cases where boundaries and targets are the right ones, e.g. in Figure 17. See also the next section for another correct example of spatial relations. Obviously, there are types of spatial relationships such as relationships that are based solely on distance and combined relationships (\"left and back\") that we did not want to cover in this implementation, but that occur in the data and should be covered in future efforts."}, {"heading": "6.2.5 Anaphora", "text": "Our solution for using anaphora in the Bishop task works perfectly by replicating the reference to a single object in the clean data. This reference is usually combined with a spatial relationship in our data, as in Figure 18. Due to the equally good performance of our spatial relationship composer, we cover all cases of anaphora in the developmental data. However, there are more complex variants of anaphora that we do not currently cover, for example, we refer back to object groups as in the sequence in Figure 19, which follows the correct example in Figure 16."}, {"heading": "7. Future Directions", "text": "It is one of the largest hubs in the history of the European Union, home to the EU Accession Academy, for which the EU Accession Academy has applied."}, {"heading": "8. Summary", "text": "A robust parsing algorithm finds fragments of syntactically related words from an input utterance. To determine the semantics of phrases, the parser activates semantic composers who combine words to determine their common reference. The robust parser is able to process grammatically poorly formed transcripts of natural spoken utterances. In evaluations, the system selected correct objects in response to utterances for 76.5% of the development data and for 58.7% of the test data. On clean records with various language and processing errors, performance was even higher. We proposed several ways to improve the performance of the system, including better methods for spatial grouping, semantically controlled tracing during sentence processing, the use of machine learning to replace the manual construction of models, and the interactive use of dialogues to improve the system's interactivity, including better methods for grouping sentences in 2003, retroactive model sequencing during the year for improving the interactive processing of sentences."}, {"heading": "Acknowledgments", "text": "Thanks to Ripley, Newt and Jones."}, {"heading": "Appendix A. Utterances in the Test Data Set", "text": "That means that they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes, they make mistakes."}, {"heading": "Appendix B", "text": "The following is the complete encyclopedia used in Bishop in XML format; the first comment explains the attributes of lexical entries. See the online attachment file 'lexicon.xml'."}], "references": [{"title": "Part-of-speech tagging and partial parsing", "author": ["S. Abney"], "venue": "Corpus-Based Methods in Language and Speech, chap. 4, pp. 118\u2013136. Kluwer Academic Press, Dordrecht.", "citeRegEx": "Abney,? 1997", "shortCiteRegEx": "Abney", "year": 1997}, {"title": "Natural Language Understanding, chap", "author": ["J. Allen"], "venue": "3. The Benjamin/Cummings Publishing Company, Inc, Redwood City, CA, USA.", "citeRegEx": "Allen,? 1995", "shortCiteRegEx": "Allen", "year": 1995}, {"title": "When push comes to shove: A computational model of the role of motor control in the acquisition of action verbs", "author": ["D. Bailey"], "venue": "Ph.D. thesis, Computer science division, EECS Department, University of California at Berkeley.", "citeRegEx": "Bailey,? 1997", "shortCiteRegEx": "Bailey", "year": 1997}, {"title": "The dynamics of vagueness", "author": ["C. Barker"], "venue": "Linguistics and Philosophy, 25, 1\u201336.", "citeRegEx": "Barker,? 2002", "shortCiteRegEx": "Barker", "year": 2002}, {"title": "Blender 3D graphics creation suite", "author": ["Blender Foundation"], "venue": "http://www.blender3d.org.", "citeRegEx": "Foundation,? 2003", "shortCiteRegEx": "Foundation", "year": 2003}, {"title": "SAM: A perceptive spoken languageunderstanding robot", "author": ["M. Brown", "B. Buntschuh", "J. Wilpon"], "venue": "IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Reference resolution in the wild", "author": ["S. Brown-Schmidt", "E. Campana", "M.K. Tanenhaus"], "venue": "In Proceedings of the Cognitive Science Society", "citeRegEx": "Brown.Schmidt et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Brown.Schmidt et al\\.", "year": 2002}, {"title": "Risk-taking and recovery in task-oriented dialogue", "author": ["J. Carletta", "C. Mellish"], "venue": "Journal of Pragmatics,", "citeRegEx": "Carletta and Mellish,? \\Q1996\\E", "shortCiteRegEx": "Carletta and Mellish", "year": 1996}, {"title": "A grouping principle and four applications", "author": ["A. Desolneux", "L. Moisan", "J. Morel"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Desolneux et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Desolneux et al\\.", "year": 2003}, {"title": "A computational model to connect gestalt perception and natural language", "author": ["S. Dhande"], "venue": "Master\u2019s thesis, Massachusetts Institure of Technology.", "citeRegEx": "Dhande,? 2003", "shortCiteRegEx": "Dhande", "year": 2003}, {"title": "Design considerations for generic grouping in vision", "author": ["E. Engbers", "A. Smeulders"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Engbers and Smeulders,? \\Q2003\\E", "shortCiteRegEx": "Engbers and Smeulders", "year": 2003}, {"title": "The agreement process: An empirical investigation of human-human computer-mediated collaborative dialogues", "author": ["B.D. Eugenio", "P.W. Jordan", "R.H. Thomason", "J.D. Moore"], "venue": "International Journal of Human-Computer Studies,", "citeRegEx": "Eugenio et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Eugenio et al\\.", "year": 2000}, {"title": "Augmenting user interfaces with adaptive speech commands", "author": ["P. Gorniak", "D. Roy"], "venue": "In Proceedings of the International Conference for Multimodal Interfaces", "citeRegEx": "Gorniak and Roy,? \\Q2003\\E", "shortCiteRegEx": "Gorniak and Roy", "year": 2003}, {"title": "What the eyes say about speaking", "author": ["Z. Griffin", "K. Bock"], "venue": "Psychological Science,", "citeRegEx": "Griffin and Bock,? \\Q2000\\E", "shortCiteRegEx": "Griffin and Bock", "year": 2000}, {"title": "Computational models of incremental semantic interpretation", "author": ["N. Haddock"], "venue": "Language and Cognitive Processes, 4, 337\u2013368.", "citeRegEx": "Haddock,? 1989", "shortCiteRegEx": "Haddock", "year": 1989}, {"title": "Coupling perception and simulation: Steps towards conversational robotics", "author": ["K. Hsiao", "N. Mavridis", "D. Roy"], "venue": "In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "citeRegEx": "Hsiao et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Hsiao et al\\.", "year": 2003}, {"title": "What\u2019s in the lexicon", "author": ["R. Jackendoff"], "venue": "Noteboom, S., Weerman, F., & Wijnen (Eds.), Storage and Computation in the Language Faculty, chap. 2. Kluwer Academic Press.", "citeRegEx": "Jackendoff,? 2002", "shortCiteRegEx": "Jackendoff", "year": 2002}, {"title": "Fitting words: vague words in context", "author": ["A. Kyburg", "M. Morreau"], "venue": "Linguistics and Philosophy,", "citeRegEx": "Kyburg and Morreau,? \\Q2000\\E", "shortCiteRegEx": "Kyburg and Morreau", "year": 2000}, {"title": "A computational model of color perception and color naming", "author": ["J.M. Lammens"], "venue": "Ph.D. thesis, State University of New York.", "citeRegEx": "Lammens,? 1994", "shortCiteRegEx": "Lammens", "year": 1994}, {"title": "what\u201d and \u201cwhere\u201d in spatial language and spatial cognition", "author": ["B. Landau", "R. Jackendoff"], "venue": "Behavioural and Brain Sciences,", "citeRegEx": "Landau and Jackendoff,? \\Q1993\\E", "shortCiteRegEx": "Landau and Jackendoff", "year": 1993}, {"title": "Language and Perception", "author": ["G. Miller", "P. Johnson-Laird"], "venue": null, "citeRegEx": "Miller and Johnson.Laird,? \\Q1976\\E", "shortCiteRegEx": "Miller and Johnson.Laird", "year": 1976}, {"title": "Ubiquitous talker: Spoken language interaction with real world objects", "author": ["K. Nagao", "J. Rekimoto"], "venue": "In Proceeding of the International Joint Conference on Artificial Intelligence", "citeRegEx": "Nagao and Rekimoto,? \\Q1995\\E", "shortCiteRegEx": "Nagao and Rekimoto", "year": 1995}, {"title": "KARMA: Knowledge-based Action Representations for Metaphor and Aspect", "author": ["S. Narayanan"], "venue": "Ph.D. thesis, Univesity of California, Berkeley.", "citeRegEx": "Narayanan,? 1997", "shortCiteRegEx": "Narayanan", "year": 1997}, {"title": "Lexical semantics and compositionality", "author": ["B.H. Partee"], "venue": "Gleitman, L. R., & Liberman, M. (Eds.), An Invitation to Cognitive Science: Language, Vol. 1, chap. 11, pp. 311\u2013360. MIT Press, Cambridge, MA.", "citeRegEx": "Partee,? 1995", "shortCiteRegEx": "Partee", "year": 1995}, {"title": "Incremental speech production and referential overspecification", "author": ["T. Pechmann"], "venue": "Linguistics, 27, 89\u2013110.", "citeRegEx": "Pechmann,? 1989", "shortCiteRegEx": "Pechmann", "year": 1989}, {"title": "The Generative Lexicon", "author": ["J. Pustejovsky"], "venue": "MIT Press, Cambridge, MA, USA.", "citeRegEx": "Pustejovsky,? 1995", "shortCiteRegEx": "Pustejovsky", "year": 1995}, {"title": "The Human Semantic Potential", "author": ["T. Regier"], "venue": "MIT Press.", "citeRegEx": "Regier,? 1996", "shortCiteRegEx": "Regier", "year": 1996}, {"title": "Grounding spatial language in perception: An empirical and computational investigation", "author": ["T. Regier"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "Regier,? \\Q2001\\E", "shortCiteRegEx": "Regier", "year": 2001}, {"title": "Learning visually-grounded words and syntax for a scene description task", "author": ["D. Roy"], "venue": "Computer Speech and Language, 16 (3).", "citeRegEx": "Roy,? 2002", "shortCiteRegEx": "Roy", "year": 2002}, {"title": "A trainable spoken language understanding system", "author": ["D. Roy", "P.J. Gorniak", "N. Mukherjee", "J. Juster"], "venue": "In Proceedings of the International Conference of Spoken Language Processing", "citeRegEx": "Roy et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2002}, {"title": "Learning words from sights and sounds: A computational model", "author": ["D. Roy", "A. Pentland"], "venue": "Cognitive Science,", "citeRegEx": "Roy and Pentland,? \\Q2002\\E", "shortCiteRegEx": "Roy and Pentland", "year": 2002}, {"title": "Using model-theoretic semantic interpretation to guide statistical parsing and word recognition in a spoken language interface", "author": ["W. Schuler"], "venue": "Proceedings of the Association for Computational Linguistics.", "citeRegEx": "Schuler,? 2003", "shortCiteRegEx": "Schuler", "year": 2003}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Shi and Malik,? \\Q2000\\E", "shortCiteRegEx": "Shi and Malik", "year": 2000}, {"title": "Grounding the lexical semantics of verbs in visual perception using force dynamics and event logic", "author": ["J.M. Siskind"], "venue": "Journal of Artificial Intelligence Research, 15, 31\u201390.", "citeRegEx": "Siskind,? 2001", "shortCiteRegEx": "Siskind", "year": 2001}, {"title": "Laws of organization in perceptual forms", "author": ["M. Wertheimer"], "venue": "A source book of Gestalt psychology, pp. 71\u201388. Routledge, New York.", "citeRegEx": "Wertheimer,? 1999", "shortCiteRegEx": "Wertheimer", "year": 1999}, {"title": "Procedures as a representation for data in a computer program for understanding natural language", "author": ["T. Winograd"], "venue": "Ph.D. thesis, Massachusetts Institute of Technology.", "citeRegEx": "Winograd,? 1970", "shortCiteRegEx": "Winograd", "year": 1970}, {"title": "Utterance segmenation for spontaneous speech recognition", "author": ["N. Yoshida"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology. 470", "citeRegEx": "Yoshida,? 2002", "shortCiteRegEx": "Yoshida", "year": 2002}], "referenceMentions": [{"referenceID": 28, "context": "While our previous work on visually-grounded language has centred on machine learning approaches (Roy, Gorniak, Mukherjee, & Juster, 2002; Roy & Pentland, 2002; Roy, 2002), we chose not to apply machine learning to the problem of compositional grounded semantics in this investigation.", "startOffset": 97, "endOffset": 171}, {"referenceID": 24, "context": "These results are similar to those reported in prior studies (Brown-Schmidt, Campana, & Tanenhaus, 2002; Griffin & Bock, 2000; Pechmann, 1989).", "startOffset": 61, "endOffset": 142}, {"referenceID": 35, "context": "Winograd\u2019s SHRDLU is a well known system that could understand and generate natural language referring to objects and actions in a simple blocks world (Winograd, 1970).", "startOffset": 151, "endOffset": 167}, {"referenceID": 23, "context": "Partee provides an overview of this approach and to the problems of context based meanings and meaning compositionality from this perspective (Partee, 1995).", "startOffset": 142, "endOffset": 156}, {"referenceID": 25, "context": "Pustejovsky\u2019s theory of the Generative Lexicon (GL) in particular takes seriously noun phrase semantics and semantic compositionality (Pustejovsky, 1995).", "startOffset": 134, "endOffset": 153}, {"referenceID": 6, "context": ", for example, engage participants in a free-form dialogue (as opposed to the one-sided descriptions in our task) producing referential descriptions to solve a spatial arrangement problem (Brown-Schmidt et al., 2002).", "startOffset": 188, "endOffset": 216}, {"referenceID": 24, "context": "Similar tasks have been used in other studies of dialogue and referring expressions (Pechmann, 1989; Griffin & Bock, 2000).", "startOffset": 84, "endOffset": 122}, {"referenceID": 3, "context": "Formal theories of vagueness support our findings in the expressions produced by our participants (Kyburg & Morreau, 2000; Barker, 2002).", "startOffset": 98, "endOffset": 136}, {"referenceID": 18, "context": "Models have been suggested for visual representations underlying colour (Lammens, 1994) and spatial relations (Regier, 1996; Regier & Carlson, 2001).", "startOffset": 72, "endOffset": 87}, {"referenceID": 26, "context": "Models have been suggested for visual representations underlying colour (Lammens, 1994) and spatial relations (Regier, 1996; Regier & Carlson, 2001).", "startOffset": 110, "endOffset": 148}, {"referenceID": 33, "context": "Models for verbs include grounding their semantics in the perception of actions (Siskind, 2001), and grounding in terms of motor control programs (Bailey, 1997; Narayanan, 1997).", "startOffset": 80, "endOffset": 95}, {"referenceID": 2, "context": "Models for verbs include grounding their semantics in the perception of actions (Siskind, 2001), and grounding in terms of motor control programs (Bailey, 1997; Narayanan, 1997).", "startOffset": 146, "endOffset": 177}, {"referenceID": 22, "context": "Models for verbs include grounding their semantics in the perception of actions (Siskind, 2001), and grounding in terms of motor control programs (Bailey, 1997; Narayanan, 1997).", "startOffset": 146, "endOffset": 177}, {"referenceID": 34, "context": "Their work draws from findings of Gestalt psychology that provide many insights into visual grouping behaviour (Wertheimer, 1999; Desolneux, Moisan, & Morel, 2003).", "startOffset": 111, "endOffset": 163}, {"referenceID": 9, "context": "In parallel with the work presented in this paper, we also been studying visual grouping and will fold the results into the systen described here (Dhande, 2003).", "startOffset": 146, "endOffset": 160}, {"referenceID": 14, "context": "Our model of incremental semantic interpretation during parsing follows a tradition of employing constraint satisfaction algorithms to incorporate semantic information starting with SHRDLU and continued in other systems (Haddock, 1989).", "startOffset": 220, "endOffset": 235}, {"referenceID": 15, "context": "Even though the system described here senses a synthetic scene, it makes continuous measurements during the parsing process and we are now integrating it into an active vision system (Hsiao et al., 2003).", "startOffset": 183, "endOffset": 203}, {"referenceID": 1, "context": "Object shape is clearly important when connecting language to the world, but remains a challenging problem in computational models of language grounding. In previous work, we have used histograms of local geometric features which we found sufficient for grounding names of basic objects (dogs, shoes, cars, etc.) (Roy & Pentland, 2002). This representation captures characteristics of the overall outline form of an object that is invariant to in-plane rotations and changes of scale. Landau and Jackendoff provide a detailed analysis of additional visual shape features that play a role in language (Landau & Jackendoff, 1993). For example, they suggest the importance of extracting the geometric axes of objects in order to ground words such as \u201cend\u201d, as in \u201cend of the stick\u201d. Shi and Malik propose an approach to performing visual grouping on images (Shi & Malik, 2000). Their work draws from findings of Gestalt psychology that provide many insights into visual grouping behaviour (Wertheimer, 1999; Desolneux, Moisan, & Morel, 2003). Engbers e.a. give an overview and formalization of the grouping problem in general and various approaches to its solution (Engbers & Smeulders, 2003). In parallel with the work presented in this paper, we also been studying visual grouping and will fold the results into the systen described here (Dhande, 2003). Our model of incremental semantic interpretation during parsing follows a tradition of employing constraint satisfaction algorithms to incorporate semantic information starting with SHRDLU and continued in other systems (Haddock, 1989). Most prior systems use a declaratively stated set of semantic facts that is disconnected from perception. Closely related to our work in this area is Schuler\u2019s (2003), who integrates determination of referents to the parsing process by augmenting a grammar with logical expressions, much like we augment a grammar with grounded composition rules (see Section 3.", "startOffset": 89, "endOffset": 1757}, {"referenceID": 29, "context": "We have previously proposed methods for visually-grounded language learning (Roy & Pentland, 2002), understanding (Roy et al., 2002), and generation (Roy, 2002).", "startOffset": 114, "endOffset": 132}, {"referenceID": 28, "context": ", 2002), and generation (Roy, 2002).", "startOffset": 24, "endOffset": 35}, {"referenceID": 28, "context": "For example, the Describer system (Roy, 2002) encodes spatial locations in absolute terms within the frame of reference of a visual scene.", "startOffset": 34, "endOffset": 45}, {"referenceID": 28, "context": "In our previous work, we have investigated how speakers describe objects with distinctive attributes like colour, shape and size in a constraint speaking task and in scenes with a constant number of objects (Roy, 2002).", "startOffset": 207, "endOffset": 218}, {"referenceID": 36, "context": "The raw audio was segmented using our speech segmentation algorithm based on pause structure (Yoshida, 2002).", "startOffset": 93, "endOffset": 108}, {"referenceID": 28, "context": "In previous work we have addressed the problems of learning visually-grounded models of colour words (Roy & Pentland, 2002; Roy, 2002).", "startOffset": 101, "endOffset": 134}, {"referenceID": 28, "context": "We have in the past successfully migrated models from synthetic vision (Roy, 2002) to computer vision (Roy et al.", "startOffset": 71, "endOffset": 82}, {"referenceID": 29, "context": "We have in the past successfully migrated models from synthetic vision (Roy, 2002) to computer vision (Roy et al., 2002) and plan on a similar route to deploy Bishop.", "startOffset": 102, "endOffset": 120}, {"referenceID": 28, "context": "While in previous work we have used Markov models to parse and generate utterances (Roy, 2002), we here employ to context free grammars.", "startOffset": 83, "endOffset": 94}, {"referenceID": 1, "context": "bottom-up chart parser to guide the interpretation of phrases (Allen, 1995).", "startOffset": 62, "endOffset": 75}, {"referenceID": 0, "context": "For an overview of related partial parsing techniques, see the work of Abney (1997). The grammar used for the partial chart parser is shown in Figure 1.", "startOffset": 71, "endOffset": 84}, {"referenceID": 26, "context": "The spatial semantic composer employs a version of the Attentional Vector Sum (AVS) suggested by Regier and Carlson (2001). The AVS is a measure of spatial relation meant to approximate human judgements corresponding to words like \u201cabove\u201d and \u201cto the left of\u201d in 2D scenes of objects.", "startOffset": 97, "endOffset": 123}, {"referenceID": 24, "context": "Speakers may be listing visually salient characteristics such as colour before determining whether colour is a distinguishing feature of the intended referent (Pechmann, 1989).", "startOffset": 159, "endOffset": 175}, {"referenceID": 34, "context": "A rich source of models for possible human grouping strategies like co-linearity comes from research in Gestalt Grouping (Wertheimer, 1999).", "startOffset": 121, "endOffset": 139}, {"referenceID": 15, "context": "In the near future, we plan to transplant Bishop into an interactive conversational robot (Hsiao et al., 2003), vastly improving the robot\u2019s ability to comprehend spatial language in situated spoken dialogue.", "startOffset": 90, "endOffset": 110}], "year": 2011, "abstractText": "We present a visually-grounded language understanding model based on a study of how people verbally describe objects in scenes. The emphasis of the model is on the combination of individual word meanings to produce meanings for complex referring expressions. The model has been implemented, and it is able to understand a broad range of spatial referring expressions. We describe our implementation of word level visually-grounded semantics and their embedding in a compositional parsing framework. The implemented system selects the correct referents in response to natural language expressions for a large percentage of test cases. In an analysis of the system\u2019s successes and failures we reveal how visual context influences the semantics of utterances and propose future extensions to the model that take such context into account.", "creator": "(Preview: cgpdftops CUPS filter)"}}}