{"id": "1107.0042", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "Restricted Value Iteration: Theory and Algorithms", "abstract": "Value iteration is a popular algorithm for finding near optimal policies for POMDPs. It is inefficient due to the need to account for the entire belief space, which necessitates the solution of large numbers of linear programs. In this paper, we study value iteration restricted to belief subsets. We show that, together with properly chosen belief subsets, restricted value iteration yields near-optimal policies and we give a condition for determining whether a given belief subset would bring about savings in space and time. We also apply restricted value iteration to two interesting classes of POMDPs, namely informative POMDPs and near-discernible POMDPs.", "histories": [["v1", "Thu, 30 Jun 2011 20:38:52 GMT  (275kb)", "http://arxiv.org/abs/1107.0042v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["n l zhang", "w zhang"], "accepted": false, "id": "1107.0042"}, "pdf": {"name": "1107.0042.pdf", "metadata": {"source": "CRF", "title": "Restricted Value Iteration: Theory and Algorithms", "authors": ["Weihong Zhang", "Nevin L. Zhang"], "emails": ["wzhang@cs.wustl.edu", "lzhang@cs.ust.hk"], "sections": [{"heading": "1. Introduction", "text": "There are a number of reasons why it has come so far, that it has come so far. But there are also other reasons why it has come so far. There are a number of reasons why it has come so far. There are many reasons why it has come so far. There are many reasons why it has come so far. There are many reasons why it has come so far. There are many reasons why it has come so far. There are many reasons why it has come so far. There are many reasons why it has come so far. There are many reasons why it has come so far. There are many reasons why it has come so far. There are many reasons why it has come so far. There are many reasons why it has come so far."}, {"heading": "2. POMDPs and Value Iteration", "text": "This section gives a brief overview of the POMDP model and value repetition."}, {"heading": "2.1 POMDPs", "text": "A POMDP is a sequential decision model for an agent acting in a stochastic environment with only partial knowledge of the state of his environment. The agent does not observe the state directly, but receives an observation about it. We refer to the set of all possible observations as Z. After obtaining the observation, the agent selects an action from a series of possible actions and performs this action. Afterwards, the agent receives an immediate reward and the environment develops stochastically to a next status. Mathematically, a POMDP is specified by: three sentences S, Z and A; a reward function r (s, a) for s in S and a in A; a transition probability function P (s) and an observation probability function P (s) for z in Z and s \u00b2 in S. The reward function characterizes the current function of the reward function and the immediate dependence of the current state on the current state."}, {"heading": "2.2 Policies and Value Functions", "text": "Since the current observation does not necessarily fully reveal the identity of the current state, the actor must take into account all previous observations and actions when selecting an action. The distribution of probability is sometimes referred to as the state of faith and referred to by b. for each possible state, b (s) is the probability that the current state is s. the totality of all possible states of faith is referred to as the space of faith. We refer to it by B.A policy, which prescribes an action for each possible state of faith. In other words, it is an assignment from B to A. Associated with a policy is its value function V \u03c0. For each state of faith b, V \u03c0 (b) is the expected total discounted reward that the actor receives by following the policy, starting with b, i.e., V \u03c0 (b) = Equity, b = 0 (trt), where the reward is paid out in due course and b (b) an optimal total reward is that the actor receives such a reward by starting from politics, i.e. an optimal policy, i.e. V = an equilibrium."}, {"heading": "2.3 Value Iteration", "text": "To explain the titeration of values, we must take into account how the state of faith develops over time. Let b be the current state of faith. \u2022 The state of faith at the next point of time is determined by the current state of faith, the current act a, the next observation z. We implicitly refer to it as \"p\" (b, a, z). For each state is \"p,\" \"p\" (b, a, z), \"p\" (b, z), \"p\" (z, s), \"p\" (b, a), \"p\" (b, a), (1), where P (z, s), \"p,\" (z), \"s,\" (z), \"p,\" \"s,\" p, \"a,\" p. \"(z,\" b. \""}, {"heading": "2.4 Technical and Notational Considerations", "text": "For the sake of simplicity, we consider state-space functions to be vectors of magnitude | S |. We use lowercase Greek letters \u03b1 and \u03b2 to denote vectors and letters V and U to refer to vector propositions. In contrast, capital letters V and U always refer to value functions, i.e. functions based on the faith space B.A sentence V of vectors induces a gradual linear convex value function (say f) as follows: f (b) = max\u03b1 \u0445 V \u03b1 \u00b7 b for each b in B, where \u03b1 \u00b7 b is the inner product of \u03b1 and b. For the sake of simplicity, we abuse notation and use V to denote both a set of vectors and the value function induced by the set. According to this convention, the quantity f (b) can be written as V (b). A vector in a set is useless if its distance does not affect the function that induces the set."}, {"heading": "2.5 Finite Representation of Value Functions and Value Iteration", "text": "A value function V is represented by a series of vectors when it corresponds to the value function induced by the set. If a value function is represented by a finite set of vectors, there is a unique minimum set that represents the function (Littman, Cassandra, & Kaelbling, 1995). Sondik (1971) has shown that if a value function can be represented by a finite set of vectors, this also applies to subsequent value functions derived from DP updates. The process of determining the minimum representation for Vn + 1 from the minimum representation of Vn is usually performed as dynamic programming (DP) updating.In practice, the value attribution for POMDPs is not performed directly in relation to the value functions themselves, but in relation to groups of vectors that represent the value functions."}, {"heading": "3. Belief Subset Selection", "text": "In this section we will show how to select a subset of faith for the iteration of values. We will describe a condition that determines whether the selected subset of faith space is correct. In addition, we will discuss the minimum representation of value functions for the selected subset. In the next section, we will develop the partial value titeration algorithm and show why it is able to achieve approximately optimal values."}, {"heading": "3.1 Subset Selection", "text": "Let the current faith of the Agent b be. His next faith is \u03c4 (b, a, z) if he performs action a and observes. If we vary the faith state b in the faith space B, we receive a sentence {\u03c4 (b, a, z) | b, B}. If we abuse our spelling, we designate this sentence \u03c4 (B, a, z). In words, no matter with which belief the actor begins, if he receives z after performing one, his next state of faith must be in order (B, a, z). The union of the beliefs (B, a, z) takes into account the sentences of the beliefs for all possible combinations of actions and observations. It contains all the beliefs that the actor may encounter. In other words, the faith state of the actor at any point in time must be in order (B, a, z). The state of faith received by the act, regardless of its original act, We must belong to that act and only by that act B."}, {"heading": "3.2 Subset Representation", "text": "The subset represents the subsets \u03c4 (B, a, z) and \u03c4 (B). To this end, we present the concept of faith simplex.Definition 1 Lass B = {b1, b2,..., bk} represents a series of faith states. A belief simplex generated by B is the set of faith states (or simply faith) is the set of convex combinations of faith states in the base. If we follow the standard expressions in linear algebra, we can also talk about the minimum basis of a simplex. For convenience, we use the notation B to name a basis of a given simplex."}, {"heading": "3.3 Belief Subset and Belief Space", "text": "We discuss the relationship between the set \u03c4 (B) \u00b7 P (A) \u00b7 \u00b7 b (B) \u00b7 \u00b7 b (B) is a union of simplifications, it helps to show how each simplex \u03c4 (B, a, z) is related to the space of belief B. For an action a and observation z, it turns out that a matrix derived from transition and observation models plays a central role in determining the simplex. Such a matrix, designated by Paz, is of the dimension | S | S | S | S | | and its input on (s, s) is the common probability P (s \u2032, z | s, one), a central role in determining the simplex. Paz = P (s \u2032 1, s1, a) P (s \u2032 2, a \u00b7 P (s \u2032 2, a \u00b7 P (s \u2032 1, z \u2032 1, z \u00b2)."}, {"heading": "3.4 Subset Value Functions", "text": "For the sake of simplicity, we refer to them as partial value functions. The problem we are investigating is that a matrix is invertible if its determinant is not zero. It degenerates otherwise. To calculate a minimum amount of vectors, one must first determine the usefulness of a vector in a series of vectors w.r.t. The matrix is invertible if its determinant is not zero. (B, a, z) The vector \u03b2 is useful w.r.t. (B, z) if and only if there is a belief state."}, {"heading": "4. Subset Value Iteration", "text": "In this section, we first describe the value iteration algorithm in the belief subset \u03c4 (B) and then show that the algorithm is capable of achieving nearly optimal results. Finally, we analyze its complexity and report on empirical studies."}, {"heading": "4.1 Belief Subset MDP", "text": "Since the subset \u03c4 (B) is closed, we are able to define a so-called faith subset MDP (or simply subset MDP) whose state space is the selected subset \u03c4 (B) and other components are the same as those in the MDP that were transformed from the original POMDP (Section 2.3).The only difference between the two MDPs lies in their state spaces: The state space of the faith subset MDP is a subset of the state space of the faith space MDP."}, {"heading": "4.2 Subset DP Updates", "text": "With the MDP theory, the subset MDP allows the following DP actualization equation where V \u03c4 (B) n (B) n represents its nth-step value (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (n) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n) n (B) n (B) n (B) n (B) n) n (B) n (B) n (B) n (B) n (B) n (B) n (B) n) n (B) n (B) n (B) n) n (B (B) n) n (B) n (B) n (B) n (B) n) n (B (B) n) n (B (B) n) n (B (B) n) n (B) n (B (B) n) n) n (B (B (B) n) n) n (B (B (B) n) n) n (B (B) n) n (B (B) n) n (B (B (B) n) n) n (B (B (B) n (B) n) n (B, B (B (B) n) n (B (B) n (B) n) n (B (B (B) n (B) n) n (B, B (B (B) n (B) n (B) n (B (B) n (B) n) n (B) n (B (B) n (B) n (B) n (B (B) n) n (B"}, {"heading": "4.3 Analysis", "text": "We analyze several theoretical properties of the subset value iteration algorithm. Our most important results include: value functions generated by subset value iteration are in a sense equivalent to those generated by standard value iteration; in order to achieve approximately optimal values, the value iteration must take at least the faith subset \u03c4 (B) into account; the value function generated by subset value iteration can be used for nearly optimal decisions throughout the faith space if the algorithm is terminated accordingly."}, {"heading": "4.3.1 Belief Subset, Value Functions and Value Iterations", "text": "Subset value iteration generates a series of {V \u03c4 (B) n} value functions. If its initial value function V > Faith (B) 0 is the same as the initial value V0 of default value iteration in \u03c4 (B), subset value iteration generates the same series of value functions as default value iteration in \u03c4 (B).Theorem 5 If V \u03c4 (B) 0 (b) = V0 (b) for any B value (B), then subset value iteration generates the same series of value functions as default value iteration (B).Proof: We first consider a DP update of the calculation function Vn + 1 from the current Vn by DP equation (2).In its right side, since it (b, a, z) must belong to the subset (B), the notation Vn (.)."}, {"heading": "4.3.2 Stopping criterion and decision making", "text": "In the further course, the maximum difference between two values functions in relation to the sub-scale Celsius (B) must become smaller. In the following, we show that the \"Idealvorstellung\" can be extended to the entire belief space by appropriately ending the \"Idealvorstellung\" action algorithm. \"Idealvorstellung\" action algorithm is ended. \"Idealvorstellung-Handlungsalgorithmus\" -scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-and-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-and-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-and-scale-scale-scale-scale-scale-scale-scale-scale-scale-and-scale-scale-scale-scale-scale-scale-scale-scale-and-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-scale-"}, {"heading": "4.4 Complexity", "text": "The first step is the polynomial part of the complexity of the subset-value iteration, but the second step is much more difficult than the first step. It is well known that finding the optimal policy for a simplified horizon is POMDP PSPACE-complete (Papadimitriou & Tsitsiklis, 1987; Burago, de Rougemont, & Slissekno, 1996; Goldsmith, Mundhenk, 1998)."}, {"heading": "4.5 Empirical Studies", "text": "In this section we present our empirical results on two variants of a designed labyrinth problem and the problems in a standard test rig. Some common settings for all experiments in the work are the following: The experiments are performed on an UltraSparc II computer with two CPUs and 256 MB RAM. Our codes are written in C and run under a UNIX operating system Sola 2.6. When solving linear programs, we use a commercial package CPLEX V6.0. The discount factor is set to 0.95 and the rounding precision to 10 \u2212 6. Unless otherwise specified, the quality requirement is set to 0.01. For each iteration, we use incremental circumcision to represent value functions over the faith space or faith subsets. We compare the performance of subsets and default value iterations."}, {"heading": "4.5.1 The Maze Problem", "text": "There are 10 locations and the goal is location VI. VI. \"A robot agent can perform four\" move \"actions to change his position, either a\" look \"action to observe his surroundings, and an\" explanation \"action to announce his success. The\" move \"actions can achieve their intended effects with a probability of 0.8, but they could not have effects with a probability of 0.1 (the agent's position remains unchanged) or cause a 0.1 probability to leave the agent at his original location. Other actions do not change the agent's position. At each point, the robot receives a\" zero \"observation that does not give any useful information at all, or reads four sensors as the reason for his current position. Each sensor informs the robot if there is a wall or nothing along one direction. In the figure, thick lines for walls and thin lines stand for nothing (open)."}, {"heading": "4.5.2 More Experiments on the Test-Bed", "text": "In the literature, the eight problems are commonly referred to as 4x3CO, Cheese, 4x4, Part Painting, Tiger, Shuttle, Network, and Aircraft. In the table, Row 2-4, specify the sizes of the problem parameters, namely the number of states, observations, and actions. Row 5 and 6 show the CPU seconds for the standard and the subset of algorithms to calculate the 0.01-optimal policy for each problem. Row 7 shows the number of vectors representing the 0.01-optimal value function in the default iteration."}, {"heading": "5. Informative POMDPs", "text": "In this section, we will examine a specific POMDP class, informative POMDPs. For this POMDP class, there are natural beliefs to work with. We will show how to formally define these subsets. As the value iteration on these beliefs has been described (Zhang & Liu, 1997), our focus is on comparing the algorithm with the general partial value iteration developed in the previous section."}, {"heading": "5.1 Motivation", "text": "In reality, an agent often has a good, if imperfect, idea of his locations (Roy & Gordon, 2002). Suppose that an agent at any time receives a chain of four letters with certainty. In total, there are 6 observations, owww, owoo, owoo, wwow, where and woww independent of actions performed. If we list all possible observations and the series of places where the agent obtains such observations, we end with the following table. Observations an agent makes, owow, wow, wow, and woww, independently of actions performed. If we list all possible observations and the series of places where the agent obtains such observations, we end with the following table. Observations an agent makes, statesowww, oww, oww, owow, wow, 5} owow, wow, wow, wow, wow, wow, wow, wow, {7,8} ww, [9,10} the world, ww, ww, ww, woww, etc, the world, etc, etc."}, {"heading": "5.2 Belief Subset Selection", "text": "For informative POMDPs, we can select a subset of the faith space. Thus, the iteration of the value versus \u03c4 (B) saves space and time. In this section, we choose an alternative subset of faith for the iteration of the value. Compared to the subset of faith (B), the subset we choose has several advantages. First, it is conceptually simple and geometrically intuitive. Second, it facilitates the use of the low-dimensional representation of vectors. Third, it can lead to additional time savings if the observation models of a POMDP are independent of actions. The latter two advantages are shown later. To define the subset of faith (say: \"either\"? \"(B), we first define a subset of value x.\" (B, a, z) for an action and observation pair."}, {"heading": "5.3 Value Iteration over \u03c6(B)", "text": "From a theoretical perspective, the feasibility of the value titeration in \u03c6 (B) is justified by Lemma 1. In combination with Lemma 5, the subset \u03c6 (B) is a closed theorem. Therefore, the MDP theory is applicable to the definition of the DP actualization equation. However, our discussion of the relationship and value titeration in Section 4.3 removes the value titeration that works with \u03c6 (B). The basic idea is to reduce the dimensions of vectors in the representation of value functions above \u03c6 (B). We briefly sketch the partial value iteration algorithm and refer readers to a detailed description (Zhang & Liu, 1997). The basic idea is to reduce the dimensions of vectors in the representation of value functions. Note that for each pair [a, z], since the beliefs in states outside Saz are zero, a vector in the representation of a value function over the variable is justified."}, {"heading": "5.4 Empirical Studies", "text": "The experiments with Maze1 (defined in Section 4.5) can be found elsewhere (Zhang & Zhang, 2001; Zhang, 2001).The results, together with existing results (Zhang & Liu, 1997) have shown that a value titeration above \u03c6 (B) can be much more efficient than a standard value titeration.For reference, it is possible to integrate a point-based technique and a value titeration above \u03c6 (B) to use both the reduction of the iteration number and the acceleration of the iteration steps (Zhang & Zhang, 2001b).To demonstrate this, we refer to the results of a 96-state POMDP in Appendix B."}, {"heading": "5.5 Restricted Value Iteration and Dimension Reduction", "text": "We compare the value iteration algorithms in this and the previous section. By comparing, we want to emphasize clearly that working with faith subsets does not mean working with low-dimensional vectors.Although both algorithms work with faith subsets, the mechanisms used to achieve the computational gains are different. However, the general value iteration works with the faith subset (B), but the dimension of the representation of vectors is the same as the number of states, while the value iteration above \u03c6 (B) works with a superset of \u03c4 (B), but the dimension of the vectors is smaller than the number of states. To show how a reduced belief and low-dimensional representation each contribute to the computational gains, we experimented with a carefully designed labyrinth problem accessible to both algorithms. However, it is worth pointing out that working with a reduced faith-iteration value does not mean that the vectors can be represented in low dimensions."}, {"heading": "6. Near-Discernible POMDPs", "text": "In this section, we examine nearly recognizable POMDPs. For this POMDP class, we develop an iteration algorithm that is available at all times and works with growing beliefs."}, {"heading": "6.1 Motivation", "text": "We assume that once in a while the uncertainty about world conditions disappears when a particular action is performed, and the observations pertaining to the action fully reveal the identities of the world (Hansen, 1998). Our research on nearly recognizable POMDPs has been motivated by two aspects, one of which arises from the origin of the use of POMDP as a framework for planning under uncertainty. To achieve a goal, an agent must change his positions not only by performing purposeful actions, but also reason for his environment by conducting information gathering actions. However, the agent cannot shift his positions and observe his surroundings at the same time. For example, when an information gathering action is carried out, the agent cannot shift his positions by motivating the concept of near-discreditability of existing research in the community."}, {"heading": "6.2 Histories, Belief Subsets and Value Functions", "text": "A story is a sequence of ordered pairs of actions and observations. Normally, we denote a story by h. The number of pairs of actions and observations is denoted as the length of a story. A story of length l is denoted by [a1, z1, \u00b7 \u00b7 \u00b7, al, zl]. If the original state of faith of an agent is b and a story h of length l is realized, its state of faith can be updated at any time step. The notation \u03c4 (b, h) denotes faith at the time. If h is of length 1 (say h = [a, z]), the defined state of faith (B, h) consisting of all possible beliefs is defined, stating that the actor can be in step l when it begins with any faith and history h is realized. If h is of length 1 (say h = [a, z]), the procedure (B, h) is degenerated by the procedure H (B, h)."}, {"heading": "6.3 Space Progressive Value Iteration", "text": "We describe the Space Progressive Value Iteration (SPVI) algorithm. As an anytime algorithm, SPVI starts with a belief subset and gradually expands it. When a certain stop criterion is met, SPVI exits and returns a set of vectors for the agent's decision-making."}, {"heading": "6.3.1 Algorithmic structure", "text": "SPVI interleaves value iteration (computing a value function for a faith subset) and subset expansion (expanding the current faith subset to a large one). Faith subsets in SPVI are introduced by sets of histories. Subset expansion is achieved by including more histories. For simplicity's sake, the set of histories determining the i-th faith subset is called Hi. The pseudo-code in Table 3 implements SPVI. A set of histories H0 (and thus the faith environment subset \u03c4 (B, H0))), a value function V\u03c4 (B, H0) and the quality precision \u03b7 are initialized by line 1. This step can be considered the 0th-step expansion of faith subset. Note that we set the initial value function as the minimum reward for all trading pairs and states."}, {"heading": "6.3.2 Value iteration in a belief subset", "text": "In view of a set V of vectors, a set H of histories, and a precision threshold \u03b7, all VDP functions are replaced by removing the VDA function (VDA function) with an improved value function via the faith subgroup \u03c4 (B, H). This is achieved by performing a sequence of DP updates. In the following, we discuss implicit DP updates, the convergence problem, and the stop criterion in the value subgroup. Thus, a DP update computes the value function Uj + 1 of Uj. The procedure of calculating Uj + 1 of Uj is updated in parallel to the collective DP update value in Section 4. Leave Uj (U0 = V) the J value function. Thus, a DP update computes the value function Uj + 1 of Uj. The procedure of calculating Uj + 1 of Uj is updated in parallel to the collective DP value in Section 4. In particular, when a vector size UJ is defined, a function Uj is given (an action is given) and a mapping (an action is given in mapping)."}, {"heading": "6.3.3 Subset expansion", "text": "Considering a set of V of vectors and a set of H of stories (B, H), the subset expansion step must be extended the belief subset \u03c4 (B, H) to a larger one. This is achieved by generating a superset H of H. Below, we propose two approaches to generate the story with our intuition for almost discretionary POMDPs. Both approaches generate new stories by exploiting the vectors in V. We begin with an analysis of the vectors in set V and show how they are used to generate a story. Remember that \u03b2 is a vector defined in set V and \u03b2 is defined by a pair of actions and a mapping procedure. For convenience, such an action is referred to as the associated action of \u03b2."}, {"heading": "6.3.4 Stopping criterion and Decision-Making", "text": "Another stop criterion of interest can be defined as follows: Given a sufficiently long time span, SPVI would consider as many stories as possible. If a (near) optimal POMDP policy requires that information-rich actions be performed following a sequence of information-poor actions, SPVI should be able to calculate a value function for the faith subset consisting of all possible beliefs stating that the actor encounters such a policy. After sufficiently many expansions of historical propositions and thus faith subgroups, each vector associated with a maximum history will write an information-rich action. If all vectors in the performing group prescribe information-rich actions, SPVI will end. If a (near) optimal policy has the desired structure in its action sequence, the initial value function in the final belief document should be nearly optimal. When SPVI ends, the value function VIII (B, Hi \u2212 1) can be used to improve the decision-making process (Hi)."}, {"heading": "6.3.5 Efficiency of SPVI", "text": "The efficiency of SPVI depends on the beliefs selected. If these beliefs are close to the faith space in size, SPVI must be inefficient. Fortunately, our approach to the expansion of beliefs ensures that the initial beliefs are small and the subsequent subsets grow slowly. Firstly, because H0 is the set of informative actions and observations, the initial beliefs (B, Hi) are relatively small. Secondly, the subsequent beliefs (B, Hi) do not grow too fast. The reason is that when a story expands, the low-information pairs are added to its end. Therefore, the first action and observation pair of history must be informative in a series Hi. Therefore, for a story h in the set Hi the size (B, h) is small. Meanwhile, due to the heuristics for generating parts of history, the sizes | Hi | would not rise too fast."}, {"heading": "6.4 Empirical Results", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "7. Related Work", "text": "In this article, we propose algorithms for iterating limited value values to speed up the iteration of values for POMDPs. Two basic ideas behind limited value iterations are (1) to reduce the complexity of DP updates and (2) to reduce the complexity of value functions. In this section, we discuss related work in these two categories. In addition, we give an overview of specific POMDPs in the literature and the algorithms that exploit their problem characteristics."}, {"heading": "7.1 Reducing the Complexity of DP Updates", "text": "In a broad sense, approaches to reducing the complexity of DP updates can be roughly divided into two classes: approaches to performing value updates via a (stationary) belief subset and approaches to performing value updates over a growing subset, although the boundary between these two classes is ambiguous in some cases. First, class includes a family of net-based beliefs, algorithms based on accessibility analysis, algorithms that use state space decomposition and others. Grid-based algorithms update values for a finite grid and extrapolitical values for non-network-based beliefs (Lovejoy, 1991; Hauskrecht, 1997; Zhou & Hansen, 2001). However, to ensure optimality, the grid is often exponential in the dimension of state space."}, {"heading": "7.2 Reducing the Complexity of Value Functions", "text": "Another idea behind the limited repetition of value concerns the objective complexity of value functions: Intuitively, the set of value functions over a faith subset contains fewer vectors than the same value function over the faith space, a fact that has been observed (Boutilier & Poole, 1996; Hauskrecht & Fraser, 1998), where POMDPs are represented in a compact way. If states are represented by a set of variables, they are classified into observable variables and hidden variables. It is also noted that some states of faith cannot be achieved for certain combinations of observable variables and hidden variables, a fact that has been exploited to adapt the solution to a medical treatment example (Hauskrecht & Fraser, 1998). Recent work along this thread includes a compression technique between state and space that exploits the representational advantage (Poupart & Boutilier, 2002), and a technique of principle component analysis (PCA) that aims to reduce value before we can compress space (Gordon, before we can compose space and function)."}, {"heading": "7.3 Solving Special POMDPs", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "8. Conclusions", "text": "The subset is (1) closed insofar as no actions can lead the agent to states of faith outside of them; (2) sufficient in the value function defined by it, which can be extended into the space of faith; and (3) minimal in this value definition, at least the subset must be taken into account if it intends to achieve the quality of the value functions; the fact that the subset is closed makes it possible to formulate a subset of MDP. We dealt with the problems of representing the subset and the section of a number of vectors w.r.t. of the subset; then we described the algorithm for the partial value attribution. In a given POMDP, it can be determined from the outset whether the subset is correct. If this is the case, the partial value attribution carries the advantages of representation in space and efficiency. We also examined informative POMDPs and virtually discountable POMDPs. For informative POMPs, there is a subvalue appreciation between faith and the efficiency."}, {"heading": "Acknowledgments", "text": "The authors thank Tony Cassandra and Eric Hansen for providing us with their programs, the first author thanks Eric Hansen for in-depth discussions on faith selection and low dimensional representation, and Judy Goldsmith for valuable comments on an earlier writing of the ideas in this essay. We are also grateful to the three anonymous reviewers who provided insightful comments and suggestions on this essay."}, {"heading": "Appendix A. Proofs", "text": "Theorem 2 For any pair [a, z], the subset \u03c4 (B, a, z) is a simple lexicon. 7. To complete the definition of the approximate model of observation, model parameters must be normalized in such a way that for an action and a state the probabilities for all observations are 1.0. Proof: Suppose bi is a state of faith in such a way that bi (s1), b (s2), b (sn) for s = si and 0 can be represented differently. It can be seen that {b1, b2, bn} is a basis of faith space B. Any state of faith b (= s1), b (s2), b (s2), b (sn))), as a sequence n i = 1 b (si) bi.Let k be the cardinality of the proposition {b. (bi, a), z (z), z | bi, bi, bi, bi, b, a)."}, {"heading": "Appendix B. Informative POMDPs: An Elevator Problem", "text": "The annex also describes on which floor it is located. In the order of meeting the requirements on the second floor, the requirements of passengers on the second floor are filled in. The problem is adapted from existing research results (Choi et al., 1999). Our purpose is to show that a limited arrival rate on the first floor and a low arrival rate on the second floor can solve larger problems than standard iteration.Problem formulationA lift works in a two-storey apartment building. There are three patterns on the first floor and a low arrival rate on the second floor; same arrival rates on arrival. As time variations from morning to night, these patterns change according to a probability distribution. To keep the arrival rate on the first floor and second floor low, the lift puts four buttons in its control panel: two buttons accommodate the pick-up and drop-off requests for the first floor, two more buttons, two more buttons hold the second floor information."}], "references": [{"title": "Optimal control of Markov decision processes with incomplete state estimation", "author": ["K.J. Astr\u00f6m"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "Astr\u00f6m,? \\Q1965\\E", "shortCiteRegEx": "Astr\u00f6m", "year": 1965}, {"title": "Learning to act using real-time dynamic programming", "author": ["A.G. Barto", "S.J. Bradtke", "S.P. Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "Barto et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1995}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Planning with incomplete information as heuristic search in belief space", "author": ["B. Bonet", "H. Geffner"], "venue": "In Proceedings of the 6th International Conference on Artificial Intelligence in Planning Systems (AIPS),", "citeRegEx": "Bonet and Geffner,? \\Q2000\\E", "shortCiteRegEx": "Bonet and Geffner", "year": 2000}, {"title": "Structured reachability analysis for Markov decision processes", "author": ["C. Boutilier", "R.I. Brafman", "C. Geib"], "venue": "In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Boutilier et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1998}, {"title": "Computing optimal policies for partially observable decision processes using compact representations", "author": ["C. Boutilier", "D. Poole"], "venue": "In Thirteenth National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Boutilier and Poole,? \\Q1996\\E", "shortCiteRegEx": "Boutilier and Poole", "year": 1996}, {"title": "On the complexity of partially observed Markov decision processes", "author": ["D. Burago", "M. de Rougemont", "A. Slissekno"], "venue": "Theoretical Computer Science,", "citeRegEx": "Burago et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Burago et al\\.", "year": 1996}, {"title": "Exact and approximate algorithms for partially observable Markov decision processes", "author": ["A.R. Cassandra"], "venue": "Ph.D. thesis,", "citeRegEx": "Cassandra,? \\Q1998\\E", "shortCiteRegEx": "Cassandra", "year": 1998}, {"title": "A survey of POMDP applications", "author": ["A.R. Cassandra"], "venue": "In Working Notes of AAAI 1998 Fall Symposium on Planning with Partially Observable Markov Decision Processes,", "citeRegEx": "Cassandra,? \\Q1998\\E", "shortCiteRegEx": "Cassandra", "year": 1998}, {"title": "Incremental pruning: a simple, fast, exact method for partially observable Markov decision processes", "author": ["A.R. Cassandra", "M.L. Littman", "N.L. Zhang"], "venue": "In Proceedings of the 13th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Cassandra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1997}, {"title": "An environment model for nonstationary reinforcement learning", "author": ["S.P.M. Choi", "D.Y. Yeung", "N.L. Zhang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Choi et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Choi et al\\.", "year": 1999}, {"title": "Solving planning problems with large state and action spaces", "author": ["T. Dean", "R. Givan", "K. Kim"], "venue": "In Proceedings of the 4th International Conference on Artificial Intelligence in Planning Systems (AIPS),", "citeRegEx": "Dean et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1998}, {"title": "Planning with deadlines in stochastic domains", "author": ["T.L. Dean", "L.P. Kaelbling", "J. Kirman", "A. Nicholson"], "venue": "In Proceedings of the 9th National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Dean et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1993}, {"title": "Decomposition techniques for planning in stochastic domains", "author": ["T.L. Dean", "S.H. Lin"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Dean and Lin,? \\Q1995\\E", "shortCiteRegEx": "Dean and Lin", "year": 1995}, {"title": "Anytime synthetic projection: maximizing the probability of goal satisfaction", "author": ["M. Drummond", "J. Bresina"], "venue": "In Proceedings of National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Drummond and Bresina,? \\Q1990\\E", "shortCiteRegEx": "Drummond and Bresina", "year": 1990}, {"title": "Solving large POMDPs using real time dynamic programming", "author": ["H. Geffner", "B. Bonet"], "venue": "In Working Notes Fall AAAI Symposium on POMDPs,", "citeRegEx": "Geffner and Bonet,? \\Q1998\\E", "shortCiteRegEx": "Geffner and Bonet", "year": 1998}, {"title": "Finite memory control of partially observable systems", "author": ["E.A. Hansen"], "venue": "Ph.D. thesis, Dept of Computer Science,", "citeRegEx": "Hansen,? \\Q1998\\E", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Heuristic search in cyclic AND/OR graphs", "author": ["E.A. Hansen", "S. Ziberstein"], "venue": "In Proceedings of National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Hansen and Ziberstein,? \\Q1998\\E", "shortCiteRegEx": "Hansen and Ziberstein", "year": 1998}, {"title": "Incremental methods for computing bounds in partially observable Markov decision processes", "author": ["M. Hauskrecht"], "venue": "In Proceedings of National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Hauskrecht,? \\Q1997\\E", "shortCiteRegEx": "Hauskrecht", "year": 1997}, {"title": "Value-function approximations for partially observable Markov decision processes", "author": ["M. Hauskrecht"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Hauskrecht,? \\Q2000\\E", "shortCiteRegEx": "Hauskrecht", "year": 2000}, {"title": "Modeling treatment of ischemic heart disease with partially observable Markov decision processes", "author": ["M. Hauskrecht", "H. Fraser"], "venue": "In American Medical Informatics Association annual symposium on Computer Applications in Health Care,", "citeRegEx": "Hauskrecht and Fraser,? \\Q1998\\E", "shortCiteRegEx": "Hauskrecht and Fraser", "year": 1998}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Policy iteration for factored MDPs", "author": ["D. Koller", "R. Parr"], "venue": "In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Koller and Parr,? \\Q2000\\E", "shortCiteRegEx": "Koller and Parr", "year": 2000}, {"title": "Efficient dynamic programming updates in partially observable Markov decision processes", "author": ["M.L. Littman", "A.R. Cassandra", "L.P. Kaelbling"], "venue": "Tech. rep. CS-95-19,", "citeRegEx": "Littman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1995}, {"title": "The computational complexity of probabilistic planning", "author": ["M.L. Littman", "J. Goldsmith", "M. Mundhenk"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Littman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Littman et al\\.", "year": 1998}, {"title": "Computationally feasible bounds for partially observed Markov decision processes", "author": ["W.S. Lovejoy"], "venue": "Operations Research,", "citeRegEx": "Lovejoy,? \\Q1991\\E", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "On the undecidability of probabilistic planning and infinite horizon partially observable Markov decision problems", "author": ["O. Madani", "S. Hanks", "A. Condon"], "venue": null, "citeRegEx": "Madani. et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Madani. et al\\.", "year": 1999}, {"title": "A survey of partially observable Markov decision processes: theory, models, and algorithms", "author": ["G.E. Monahan"], "venue": "Management Science,", "citeRegEx": "Monahan,? \\Q1982\\E", "shortCiteRegEx": "Monahan", "year": 1982}, {"title": "The complexity of Markov decision processes", "author": ["C.H. Papadimitriou", "J.N. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "Flexible decomposition algorithms for weakly coupled Markov decision problems", "author": ["R. Parr"], "venue": "In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Parr,? \\Q1998\\E", "shortCiteRegEx": "Parr", "year": 1998}, {"title": "Approximating optimal policies for partially observable stochastic domains", "author": ["R. Parr", "S. Russell"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Parr and Russell,? \\Q1995\\E", "shortCiteRegEx": "Parr and Russell", "year": 1995}, {"title": "Point-based value iteration: an anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "Value-directed compresseion of POMDPs", "author": ["P. Poupart", "C. Boutilier"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Poupart and Boutilier,? \\Q2002\\E", "shortCiteRegEx": "Poupart and Boutilier", "year": 2002}, {"title": "Markov decision processes: discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Exponential family PCA for belief compression in POMDPs", "author": ["N. Roy", "G. Gordon"], "venue": "In Proceedings of Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Roy and Gordon,? \\Q2002\\E", "shortCiteRegEx": "Roy and Gordon", "year": 2002}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "The optimal control of partially observable decision processes", "author": ["E.J. Sondik"], "venue": "Ph.D. thesis,", "citeRegEx": "Sondik,? \\Q1971\\E", "shortCiteRegEx": "Sondik", "year": 1971}, {"title": "BI-POMDP: Bounded, incremental partially-observable Markovmodel planning", "author": ["R. Washington"], "venue": "In Proceedings of the 4th European Conference on Planning (ECP),", "citeRegEx": "Washington,? \\Q1997\\E", "shortCiteRegEx": "Washington", "year": 1997}, {"title": "A model approximation scheme for planning in partially observable stochastic domains", "author": ["N.L. Zhang", "W. Liu"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Zhang and Liu,? \\Q1997\\E", "shortCiteRegEx": "Zhang and Liu", "year": 1997}, {"title": "Space-progressive value iteration: an anytime algorithm for a class of POMDPs", "author": ["N.L. Zhang", "W. Zhang"], "venue": "In Sixth European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU),", "citeRegEx": "Zhang and Zhang,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Zhang", "year": 2001}, {"title": "Speeding up the convergence of value iteration in partially observable Markov decision processes", "author": ["N.L. Zhang", "W. Zhang"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Zhang and Zhang,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Zhang", "year": 2001}, {"title": "Algorithms for partially observable Markov decision processes", "author": ["W. Zhang"], "venue": "Ph.D. thesis, Department of Computer Science, the Hong Kong University of Science and Technology", "citeRegEx": "Zhang,? \\Q2001\\E", "shortCiteRegEx": "Zhang", "year": 2001}, {"title": "Solving informative partially observable Markov decision processes", "author": ["W. Zhang", "N.L. Zhang"], "venue": "In Proceedings of the 6th European Conference on Planning (ECP)", "citeRegEx": "Zhang and Zhang,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Zhang", "year": 2001}, {"title": "An improved grid-based approximation algorithm for POMDPs", "author": ["R. Zhou", "E. Hansen"], "venue": "In Proceedings of the 17th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Zhou and Hansen,? \\Q2001\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2001}, {"title": "A POMDP approximation algorithm that anticipates the need to observe", "author": ["V.B. Zubek", "T.G. Dietterich"], "venue": "In Proceedings of PRICAI-2000,", "citeRegEx": "Zubek and Dietterich,? \\Q2000\\E", "shortCiteRegEx": "Zubek and Dietterich", "year": 2000}], "referenceMentions": [{"referenceID": 27, "context": "Due to the model generality, POMDPs have found a variety of potential applications in reality (Monahan, 1982; Cassandra, 1998b).", "startOffset": 94, "endOffset": 127}, {"referenceID": 16, "context": "Extensive efforts have been devoted to developing efficient algorithms for finding solutions to POMDPs (Parr & Russell, 1995; Cassandra, Littman, & Zhang, 1997; Cassandra, 1998a; Hansen, 1998; Zhang, 2001).", "startOffset": 103, "endOffset": 205}, {"referenceID": 41, "context": "Extensive efforts have been devoted to developing efficient algorithms for finding solutions to POMDPs (Parr & Russell, 1995; Cassandra, Littman, & Zhang, 1997; Cassandra, 1998a; Hansen, 1998; Zhang, 2001).", "startOffset": 103, "endOffset": 205}, {"referenceID": 37, "context": "1991; Hauskrecht, 1997; Zhou & Hansen, 2001), and several (maybe anytime) algorithms where DP updates calculate values for a growing belief subset (Dean, Kaelbling, Kirman, & Nicholson, 1993; Washington, 1997; Hansen & Ziberstein, 1998; Hansen, 1998; Bonet & Geffner, 2000).", "startOffset": 147, "endOffset": 273}, {"referenceID": 16, "context": "1991; Hauskrecht, 1997; Zhou & Hansen, 2001), and several (maybe anytime) algorithms where DP updates calculate values for a growing belief subset (Dean, Kaelbling, Kirman, & Nicholson, 1993; Washington, 1997; Hansen & Ziberstein, 1998; Hansen, 1998; Bonet & Geffner, 2000).", "startOffset": 147, "endOffset": 273}, {"referenceID": 0, "context": "Information about the current state contained in the current observation, previous observations, and previous actions can be summarized by a probability distribution over the state space (Astr\u00f6m, 1965).", "startOffset": 187, "endOffset": 201}, {"referenceID": 33, "context": "It is known that there exists a policy \u03c0\u2217 such that V \u03c0 \u2217 (b)\u2265V (b) for any other policy \u03c0 and any belief state b (Puterman, 1994).", "startOffset": 114, "endOffset": 130}, {"referenceID": 2, "context": "It has been proven that the reformulated MDP has a stationary optimal policy, which can be found by stochastic dynamic programming (Bellman, 1957; Puterman, 1994).", "startOffset": 131, "endOffset": 162}, {"referenceID": 33, "context": "It has been proven that the reformulated MDP has a stationary optimal policy, which can be found by stochastic dynamic programming (Bellman, 1957; Puterman, 1994).", "startOffset": 131, "endOffset": 162}, {"referenceID": 33, "context": "The following theorem tells one when to terminate value iteration given a precision requirement \u01eb (Puterman, 1994).", "startOffset": 98, "endOffset": 114}, {"referenceID": 33, "context": "The quantity is often called Bellman residual between Vn and Vn\u22121 (Puterman, 1994).", "startOffset": 66, "endOffset": 82}, {"referenceID": 36, "context": "Fortunately, value functions that one encounters in the process of value iteration admit implicit finite representations (Sondik, 1971).", "startOffset": 121, "endOffset": 135}, {"referenceID": 6, "context": "When a value function is representable by a finite set of vectors, there is a unique minimal set that represents the function (Littman, Cassandra, & Kaelbling, 1995). Sondik (1971) has shown that if a value function is representable by a finite set of vectors, then so are the subsequent value functions derived by DP updates.", "startOffset": 136, "endOffset": 181}, {"referenceID": 12, "context": "Its definition is an application of reachability analysis (Boutilier, Brafman, & Geib, 1998; Dean et al., 1993).", "startOffset": 58, "endOffset": 111}, {"referenceID": 27, "context": "Here, we present a two-pass algorithm due to its conceptual simplicity (Monahan, 1982).", "startOffset": 71, "endOffset": 86}, {"referenceID": 9, "context": "Let us take incremental pruning, one of the most efficient algorithms, as an example (Cassandra et al., 1997; Zhang & Liu, 1997).", "startOffset": 85, "endOffset": 128}, {"referenceID": 41, "context": "5) can be found elsewhere (Zhang & Zhang, 2001; Zhang, 2001).", "startOffset": 26, "endOffset": 60}, {"referenceID": 16, "context": "A discernible POMDP assumes that once in a while the uncertainty about world states vanishes if a particular action is executed and the observations pertain to the action fully reveal the identities of the world (Hansen, 1998).", "startOffset": 212, "endOffset": 226}, {"referenceID": 23, "context": "Otherwise, we simply compare value functions from SPVI against those from an approximate algorithm QMDP (Littman et al., 1995; Hauskrecht, 2000).", "startOffset": 104, "endOffset": 144}, {"referenceID": 19, "context": "Otherwise, we simply compare value functions from SPVI against those from an approximate algorithm QMDP (Littman et al., 1995; Hauskrecht, 2000).", "startOffset": 104, "endOffset": 144}, {"referenceID": 41, "context": "In one variant, SPVI terminated after a finite number of iterations and the output value function is near optimal; in the other variant, SPVI can quickly find a high-quality value function as time goes by (Zhang & Zhang, 2001a; Zhang, 2001).", "startOffset": 205, "endOffset": 240}, {"referenceID": 41, "context": "A note is about the number of iterations in the third column: when conducting value iteration over subsets, we also use the point-based improvements (Zhang, 2001).", "startOffset": 149, "endOffset": 162}, {"referenceID": 25, "context": "Grid-based algorithms update values for a finite grid and extrapolate values for non-grid belief states (Lovejoy, 1991; Hauskrecht, 1997; Zhou & Hansen, 2001).", "startOffset": 104, "endOffset": 158}, {"referenceID": 18, "context": "Grid-based algorithms update values for a finite grid and extrapolate values for non-grid belief states (Lovejoy, 1991; Hauskrecht, 1997; Zhou & Hansen, 2001).", "startOffset": 104, "endOffset": 158}, {"referenceID": 37, "context": "These belief states can be structured in a decision tree or AND/OR tree (Washington, 1997; Hansen & Ziberstein, 1998; Hansen, 1998; Bonet & Geffner, 2000).", "startOffset": 72, "endOffset": 154}, {"referenceID": 16, "context": "These belief states can be structured in a decision tree or AND/OR tree (Washington, 1997; Hansen & Ziberstein, 1998; Hansen, 1998; Bonet & Geffner, 2000).", "startOffset": 72, "endOffset": 154}, {"referenceID": 16, "context": "Although sometimes near optimality can be achieved at the initial belief state (Hansen, 1998), the algorithms in the cited articles cannot be applied to the case with unknown initial belief.", "startOffset": 79, "endOffset": 93}, {"referenceID": 29, "context": "This approach has been successfully applied to MDPs (Dean & Lin, 1995; Dean, Givan, & Kim, 1998; Parr, 1998; Koller & Parr, 2000).", "startOffset": 52, "endOffset": 129}, {"referenceID": 12, "context": "Approaches conducting value updates for a growing belief subset include real-time dynamic programming (RTDP) in the POMDP context (Barto, Bradtke, & Singh, 1995; Geffner & Bonet, 1998), a synthetic projection algorithm (Drummond & Bresina, 1990) and the envelope algorithm for Plexus planner in the MDP context (Dean et al., 1993).", "startOffset": 311, "endOffset": 330}, {"referenceID": 16, "context": "Special POMDPs examined in the literature include regional-observable POMDPs (Zhang & Liu, 1997), memory-resetting and discernible POMDPs (Hansen, 1998), even-odd POMDPs (Zubek & Dietterich, 2000) and generalized near-discernible POMDPs (Zhang, 2001).", "startOffset": 138, "endOffset": 152}, {"referenceID": 41, "context": "Special POMDPs examined in the literature include regional-observable POMDPs (Zhang & Liu, 1997), memory-resetting and discernible POMDPs (Hansen, 1998), even-odd POMDPs (Zubek & Dietterich, 2000) and generalized near-discernible POMDPs (Zhang, 2001).", "startOffset": 237, "endOffset": 250}, {"referenceID": 16, "context": "Another application domain is machine maintenance problems (Smallwood & Sondik, 1973; Hansen, 1998), where an agent usually can execute the following set of actions: manufacture, examine, inspect and replace.", "startOffset": 59, "endOffset": 99}, {"referenceID": 41, "context": "We also experimented with one extension of using SPVI to approximate the solutions of more general POMDPs (Zhang, 2001).", "startOffset": 106, "endOffset": 119}, {"referenceID": 41, "context": "We have designed another maze problem that has no informative action/observation pair and therefore is expected to be not amenable to SPVI (Zhang, 2001).", "startOffset": 139, "endOffset": 152}, {"referenceID": 10, "context": "The problem is adapted from existing research (Choi et al., 1999).", "startOffset": 46, "endOffset": 65}, {"referenceID": 41, "context": "We collect the time costs and the actual number of vectors generated at each iteration of algorithms VI, ssVI, infoVI and infoVIPB referring to infoVI integrated with a pointbased procedure (Zhang, 2001).", "startOffset": 190, "endOffset": 203}], "year": 2011, "abstractText": "Value iteration is a popular algorithm for finding near optimal policies for POMDPs. It is inefficient due to the need to account for the entire belief space, which necessitates the solution of large numbers of linear programs. In this paper, we study value iteration restricted to belief subsets. We show that, together with properly chosen belief subsets, restricted value iteration yields near-optimal policies and we give a condition for determining whether a given belief subset would bring about savings in space and time. We also apply restricted value iteration to two interesting classes of POMDPs, namely informative POMDPs and near-discernible POMDPs.", "creator": "dvips(k) 5.90a Copyright 2002 Radical Eye Software"}}}