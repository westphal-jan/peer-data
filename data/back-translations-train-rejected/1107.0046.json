{"id": "1107.0046", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "Explicit Learning Curves for Transduction and Application to Clustering and Compression Algorithms", "abstract": "Inductive learning is based on inferring a general rule from a finite data set and using it to label new data. In transduction one attempts to solve the problem of using a labeled training set to label a set of unlabeled points, which are given to the learner prior to learning. Although transduction seems at the outset to be an easier task than induction, there have not been many provably useful algorithms for transduction. Moreover, the precise relation between induction and transduction has not yet been determined. The main theoretical developments related to transduction were presented by Vapnik more than twenty years ago. One of Vapnik's basic results is a rather tight error bound for transductive classification based on an exact computation of the hypergeometric tail. While tight, this bound is given implicitly via a computational routine. Our first contribution is a somewhat looser but explicit characterization of a slightly extended PAC-Bayesian version of Vapnik's transductive bound. This characterization is obtained using concentration inequalities for the tail of sums of random variables obtained by sampling without replacement. We then derive error bounds for compression schemes such as (transductive) support vector machines and for transduction algorithms based on clustering. The main observation used for deriving these new error bounds and algorithms is that the unlabeled test points, which in the transductive setting are known in advance, can be used in order to construct useful data dependent prior distributions over the hypothesis space.", "histories": [["v1", "Thu, 30 Jun 2011 20:39:52 GMT  (245kb)", "http://arxiv.org/abs/1107.0046v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["p derbeko", "r el-yaniv", "r meir"], "accepted": false, "id": "1107.0046"}, "pdf": {"name": "1107.0046.pdf", "metadata": {"source": "CRF", "title": "Explicit Learning Curves for Transduction and Application to Clustering and Compression Algorithms", "authors": ["Philip Derbeko", "Ran El-Yaniv", "Ron Meir"], "emails": ["philip@cs.technion.ac.il", "rani@cs.technion.ac.il", "rmeir@ee.technion.ac.il"], "sections": [{"heading": "1. Introduction", "text": "Most of the work in Statistical Learning Theory has so far not dealt with the inductive approach to learning, which is actually simpler than the transmission situations. Here, one is given a finite set of designated training examples from which a rule is derived, and this rule is then used to mark new examples. Indeed, as dealt with by Vapnik (1998) in many realistic situations, one is faced with a simpler problem in which one finds a set of designated training examples, along with an undesignated set of points that need to be labeled. In this transductive setting, one is not interested in submitting a general rule, but only in the labeling of that blank set as accurately as possible. One solution, of course, is to present a rule as in the inductive setting, and then use it to label the required points. However, as argued by Vapnik (1982, 1998), there is little point to solving what appears to be an easier solution. 2004 AI Access Foundation. All rights are reduced by reserving it to a more difficult situation."}, {"heading": "2. Problem Setup and Vapnik\u2019s Basic Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Problem Setup", "text": "The problem of transduction can be described informally as follows: A learner is given a series of examples (1) (1). (xm, ym), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2, (2), (2), (2, (2), (2), (2), (2, (2), (2), (2), (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2), (2, (2), (2), (2), (2), (2, (2), (2), (2), (2), (2, (2), (2), (2), (2, (2), (2), (2), (2), (2), (2, (2), (2), (2, (2), (2), (2), (2, (2), (2), (2, (2), (2), (2, (2), (2), (2, (2), (2), (), (2), (2, (, (2), (2), (2), (, (2), (2), (2), (2), (, (2), (, (2), (2), (2), (, (2), (2), ("}, {"heading": "2.2 Vapnik\u2019s Implicit Bounds", "text": "& # he is # 222; he is # he is # 222; he is # he is # 2220; he is # he is # he is # 222; he is # he is # he is # 222; he is # he is # he is # 2220; he is # he is # he is # 222; he is # he is # he is # 2220; he is # he is # he is # he is # he is # 222; he is # he is # he is # he is # 2220; he is # he is # he is # he is # he is # 222; he is # he is # he is # he is # 2220; he is # he is # he is # he is # he is # he is # 222; he is # he is # he is # he is # 2220; he is # he is # he is # he is # 222; he is # he is # he is # 2220; he is # he is # he is # 222; he is # he is # he is # 222; he is # he # he is # 2220; he is # he # he # he is # 222; he is # he # he is # he # 222; he is # he is # he # 2220; he is # he # he # he is # 222; he is # he # he is # 222; he is # he is # 2220; he is # he # he # he is # 222; he is # he # he # he is # 222; he is # he is # 222; he # he is # he # 222; he is # he is # 2220; he is 8220; he is # he is # he is # 222; he is # he is # he # he # 222; he is # he is # he # 222; he is # he # he is # he # 222."}, {"heading": "3. Concentration Inequalities for Sampling without Replacement", "text": "In this section, we present several concentration imbalances used in Section 4 to develop PAC-Bayesian limits for transduction. As discussed in Section 2 (see Note 1), non-replacement sampling leads to dependent data, precluding direct application of the large standard deviation limits designed for independent samples."}, {"heading": "3.1 Inequalities Based on Reduction to Independence", "text": "Although the sample without substitution leads to dependent samples, Hoeffding (1963) pointed out a simple procedure to turn the problem into one with independent data. While this procedure results in non-trivial limits, it involves a certain loss of leakage (see Section 3.2). Lemma 12 (Hoeffding 1963) Let C = {c1,.., cN} be a finite set with N elements, let {X1,. \u2212, Xm} be selected uniformly at random with replacement of C, and let {Z1,., Zm} be uniformly and without substitution of C.8, then select for each continuous and convex real function f (x), Ef (p).m i = 1 Zi = 1Xi).7 Even the boundary of Blum and Langford (2003) may be shorter than the boundary of interest. A careful numerical comparison should be performed to determine the advantage of these two variables."}, {"heading": "3.2 Sampling Without Replacement - Direct Inequalities", "text": "In this section, we look at approaches that directly set exponential limits for sampling without substitution. Contrary to the results of Vapnik (1982, 1998), which stipulate strict but implicit limits, we strive for limits that depend exponentially on all parameters of interest. Note that the limit of Theorem 13 does not depend on all parameters (in particular, the population size N does not appear and the small population size should affect the convergence rate), it is reasonable to expect that the limits that are developed directly for sampling without substitution should be tighter than those based on the reduction on independence, because we have sampled k from N points without substitution, and the next point is to be stampled from a series of N points, which would be the case in sampling with substitution (where the samples are independent)."}, {"heading": "4. PAC-Bayesian Transduction Bounds", "text": "s PAC-Bayesian inductive boundaries (1999, 2003a, 2003b). In Section 4.1, we focus on simple randomized learning algorithms, typically referred to as \"Gibbs algorithms.\" In Section 4.2, we then consider a default deterministic setting. In the case of binary classification, the boundaries for deterministic learning are comparable to Vapnik's boundaries, which are presented in Section 2.2. Unlike the implicit but narrow PACBayesian boundary of theorem 8 (and episode 9), the new boundaries are somewhat looser but explicit."}, {"heading": "4.1 Bounds for Transductive Gibbs Learning", "text": "The first guarantee is provided for a McAllester algorithm that simply needs to be classified. (6) The second guarantee is based on the \"direct approach\" and is therefore of greater interest in many cases. (6) The selection of the previous distribution p in the PAC-Bayesian system must be made before viewing the data. (4) As we will show later, it is possible to obtain much more compact (and effective) priorities by using the complete input sample Xm + u to construct a previous p = p (Xm + u). (5) As McAllester has shown, it is possible to offer performance guarantees under certain conditions, even if we select a \"posterior\" distribution across the hypothetical space after observing the training points Xm."}, {"heading": "4.2 Bounds for Deterministic Learning Algorithms", "text": "In this section, we provide three transductive PAC-Bayesian error limits for deterministic learning algorithms (u > u).u Note that the two limits we show for the (stochastic) Gibbs algorithms in the previous subdivision are not (u).u This is done by selecting a \"posterior\" q that reduces the probability 1 to a desired hypothesis h (m + uu).u The term D (q) p (1 / p (h)).u Example: the limit of theorem 17 reduces toRh (Xu) \u2264 R (Sm) \u2264 h (m + uu).u The limit of 2R (Sm).u The limit of theorem 17 reduces toRh (Xu) \u2264 R (Xu) \u2264 h (Sm).u The limit of theorem 17 reduces toRh (Xu)."}, {"heading": "5. Bounds for Specific Algorithms", "text": "PAC-Bayesian error bounds (both inductive and transductive) are interesting because they provide a very simple but general formulation of learning. However, in order to provide more concrete statements (e.g. about specific learning algorithms), one must apply such limits with some concrete priors (and backers, in the case of Gibbs learning, see Theorems 17 and 18). In the context of inductive learning, a major obstacle to deriving effective boundaries 11 with the PAC-Bayesian framework is the construction of \"compact priors.\" For example, 11. Informal, we say that a boundary is \"effective\" if its concept of complexity disappears with m (the size of the learning sample) and it is sufficiently small for \"reasonable\" values. McAllester's generalization bound (McAllester, 1999) contains a concept of complexity that includes a component of the form ln (1 / p)."}, {"heading": "5.1 Bounds for Compression Algorithms", "text": "We propose a technique for selecting a previous p (h) above H, based on the complete (undescribed) pattern Xm +. Given that learners consider a uniform mix of all these \"sub-priorities,\" this technique will generate transductive error margins for \"compression\" algorithms. Let's be a learning algorithm. Intuitively, A is a \"compression scheme\" if there is the same hypothesis using a subset of the (labeled) training data.Definition A learning algorithm considered as a function of examples from a hypothesis class is a compression scheme related to a sample Z if there is a subsample Z."}, {"heading": "5.2 Transductive Learning via Clustering", "text": "Some learning problems are then selected on the basis of a PAC-Bayesian error for appropriate production, and the number of supporting vectors can sometimes be very large, see e.g. Baram et al., 2004, Table 1). A much stronger type of compression can often be achieved by clustering algorithms. While there is a lack of formal linkages between completely uncontrolled clusters and classifications, we can suggest a principled approach to using clustering algorithms for classification: the learner applies a clustering algorithm (or a number of clustering algorithms) to generate multiple (unsupervised) models, and the learner then uses the marked data to guess entire labels for the same label (so that all dots are in the same label), generating a number of hypotheses, one of which is selected on the basis of a PAC-Bayesian error."}, {"heading": "6. Concluding Remarks", "text": "This year, it has come to the point that there will only be one time that there will be such a process, in which there will be such a process."}, {"heading": "Acknowledgments", "text": "The work of Ran El-Yaniv and Ron Meir was supported partly by the Technion V.P.R. Fund for the Promotion of Funded Research and partly by the Excellence Network PASCAL. Also the support of the Ollendorff Center of the Faculty of Electrical Engineering at Technion is appreciated. We also thank anonymous arbitrators and Dmitry Pechyony for their useful comments."}, {"heading": "Appendix A. Proof of Theorem 2", "text": "Proof The proof we present is identical to the original proof of Vapnik and is provided for the sake of self-completeness. Let A be a learning algorithm that chooses a hypothesis hA-H based on Sm-Xu. DefineCA (x1, y1;..; xm + u, ym + u) = 1 m \u00b2, i = 1 (yi, hA (xi)) \u2212 1um + u \u00b2, j = m + 1 (yi, hA (xi))."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "Inductive learning is based on inferring a general rule from a finite data set and using it to label new data. In transduction one attempts to solve the problem of using a labeled training set to label a set of unlabeled points, which are given to the learner prior to learning. Although transduction seems at the outset to be an easier task than induction, there have not been many provably useful algorithms for transduction. Moreover, the precise relation between induction and transduction has not yet been determined. The main theoretical developments related to transduction were presented by Vapnik more than twenty years ago. One of Vapnik\u2019s basic results is a rather tight error bound for transductive classification based on an exact computation of the hypergeometric tail. While tight, this bound is given implicitly via a computational routine. Our first contribution is a somewhat looser but explicit characterization of a slightly extended PAC-Bayesian version of Vapnik\u2019s transductive bound. This characterization is obtained using concentration inequalities for the tail of sums of random variables obtained by sampling without replacement. We then derive error bounds for compression schemes such as (transductive) support vector machines and for transduction algorithms based on clustering. The main observation used for deriving these new error bounds and algorithms is that the unlabeled test points, which in the transductive setting are known in advance, can be used in order to construct useful data dependent prior distributions over the hypothesis space.", "creator": "dvips(k) 5.90a Copyright 2002 Radical Eye Software"}}}