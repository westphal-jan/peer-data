{"id": "1107.0047", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "Decentralized Control of Cooperative Systems: Categorization and Complexity Analysis", "abstract": "Decentralized control of cooperative systems captures the operation of a group of decision makers that share a single global objective. The difficulty in solving optimally such problems arises when the agents lack full observability of the global state of the system when they operate. The general problem has been shown to be NEXP-complete. In this paper, we identify classes of decentralized control problems whose complexity ranges between NEXP and P. In particular, we study problems characterized by independent transitions, independent observations, and goal-oriented objective functions. Two algorithms are shown to solve optimally useful classes of goal-oriented decentralized processes in polynomial time. This paper also studies information sharing among the decision-makers, which can improve their performance. We distinguish between three ways in which agents can exchange information: indirect communication, direct communication and sharing state features that are not controlled by the agents. Our analysis shows that for every class of problems we consider, introducing direct or indirect communication does not change the worst-case complexity. The results provide a better understanding of the complexity of decentralized control problems that arise in practice and facilitate the development of planning algorithms for these problems.", "histories": [["v1", "Thu, 30 Jun 2011 20:40:04 GMT  (234kb)", "http://arxiv.org/abs/1107.0047v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["c v goldman", "s zilberstein"], "accepted": false, "id": "1107.0047"}, "pdf": {"name": "1107.0047.pdf", "metadata": {"source": "CRF", "title": "Decentralized Control of Cooperative Systems: Categorization and Complexity Analysis", "authors": ["Claudia V. Goldman", "Shlomo Zilberstein"], "emails": ["clag@cs.umass.edu", "shlomo@cs.umass.edu"], "sections": [{"heading": "1. Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2. The Dec-POMDP Model", "text": "In fact, it is so that it is a matter of a way in which one blames oneself and others. (...) It is not so that one sees oneself in a position to take responsibility. (...) It is as if one takes oneself into responsibility. (...) It is as if one takes oneself into responsibility. (...) It is as if one takes oneself into responsibility. (...) It is as if one takes oneself into responsibility. (...) It is as if one takes oneself into responsibility. (...) It is as if one takes oneself into responsibility. (...) It is as if one takes oneself into responsibility. (...) It is as if one takes oneself into responsibility. (...) (...) (...) () (...) () (() () () (() () () (()) (()) (()) ((())) ((())) ((())) ((())) ((())))) (((()))) ((()))) ((())) () () ()) () () ()) () () ()) () () ()) () ()) () ()) () ()) () ()) () ()) () ()) () ()) () ()) () ()) () () ()) ()) () () ()) () ()) () ()) () () ()) () () () ()) () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ("}, {"heading": "2.1 Classes of Dec-POMDPs", "text": "It is well known that the optimal solution of general decentralized problems is very difficult. We are interested in identifying interesting subclasses of the general problem and their characteristics. As we show in Section 3, this classification reveals interesting complexity results and facilitates the design of algorithms. The first two categories we define include the independence of the transitions or the observations of the agents. Figure 1 represents a snapshot of the system at times t and t + 1, which illustrates these categories. We assume that the global states are factored and we name the characteristics of the world that can be fully observed by agents i.3 after si. Specifically, agent i observes oi observations, which may in some cases be identical to the local state of the agent. As a result of the action of agent i, if the system is at the local level si, the system transitions to state i, where the agent i perceives observations, the transition oi. The resulting state is not affected by the resulting state of the other agent."}, {"heading": "In other words, the transition probability P of the Dec-POMDP can be represented as", "text": "P = P1 \u00b7 P2, where P1 = Pr (s '1, a1) and P2 = Pr (s '2, a2) depend on each other, where P1 = Pr (s '1, a1) and P2 = Pr (s '2, a2) depend on each other. As a result of this action, the position of the robot is updated based on the direction of the train. For example, let us assume that a rover is in (1,1) and finds an obstacle in position (1,2) that blocks its movement towards a certain target. The rover decides to perform a pull-obstacle action at that obstacle in the east. This action can have a stochastic result, for example, with a certain probability that the rover will succeed and with a probability that the rover will find an obstacle in position (1,2) that blocks its movement towards a certain target."}, {"heading": "In other words, the observation probability O of the Dec-POMDP can be decomposed into", "text": "In fact, it is not the case that one sees oneself in a position to comply with the rules imposed on oneself \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2.2 Information Sharing", "text": "This year it is more than ever before."}, {"heading": "2.3 Goal-oriented Behavior", "text": "We characterize decentralized processes in which the actors aim to achieve certain global target states. The problem of achieving under uncertainty fulfills this requirement, since the objective of the agents is to meet in a particular place. Other practical scenarios may include assembling a machine, transferring objects from one place to a final destination and answering a query. Definition 7 (Finite Horizon Target-Oriented Dec-MDPs (GO-Dec-MDP)) An end goal Dec-MDP is goal-oriented if the following conditions apply: 1. There is a special subset of G global target states. At least one of the global target states g-G is achievable through a common policy. 2. The process ends in time T (the finite horizon of the problem).3. All actions in A incur costs, C (ai) < 0."}, {"heading": "3. A Taxonomy of Decentralized POMDPs: Complexity Results", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr lrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "4. Algorithms for Decentralized Control with No Information Sharing", "text": "So far, the only known algorithms for solving optimally decentralized control problems are the generalized version of dynamic programming for Dec-POMDPs (Hansen et al., 2004) and the coverage-set algorithm (Becker et al., 2003) for Dec-MDPs with independent transitions and observations; the first algorithm optimally solves a general Dec-POMDP; its usefulness is limited by the complexity of these problems (NEXP-complete); the coverage-set algorithm assumes that the actions of the agents could lead to superadditive or subadditive common rewards as follows; in the first case, the reward the system receives from agents performing certain actions is greater than the sum of the local reward of each agent for these actions; in the second case, subadditive common rewards are achieved when the agents are penalized for redundant actions."}, {"heading": "4.1 Single-goal, Goal-oriented Dec-MDPs", "text": "Based on Lemma 5, the optimal solution for a GO-Dec-MDP with a single global target state is calculated by solving individual agent MDPs targeting the corresponding components of the given global target state. Opt1Goal algorithm is shown in Figure 5. The MDP of each agent i finds the most cost-effective way to gi. Since the cost of measures (unlike NOP) is uniform, any value set on GR (gi) is suitable to solve the problems."}, {"heading": "4.2 Many-goals, Goal-oriented Dec-MDPs", "text": "The algorithm that optimally and decentrally solves a goal-oriented Dec-MDP problem with many global target states is OptNGoals, presented in Figure 5. Assuming the conditions of Lemma 6, this algorithm is optimal because the Lemma problem guarantees that a change of local targets is not advantageous.9. Finally, the optimal common policy is the one with the highest value. This algorithm is described for a situation in which the global target states are distinguishable, i.e. each actor iteratively solves its induced MDP over each of the possible components of each of the global target states. Finally, the optimal common policy is the one with the highest value. This algorithm is described for a situation in which the global target states are distinguishable, i.e. each actor solves its induced MDP for each of the possible components of each of the global target states."}, {"heading": "5. Decentralized Control with Communication", "text": "This means that the value of the optimal common policy that allows communication can be greater than the value of the optimal common policy without communication. If we assume that direct communication leads to the full observability of the system state, that direct communication is free, and that the observations then obviously benefit the agents the most by constantly communicating, these results in a fully observable decentralized process corresponding to an MMDP (Boutilier, 1999), this problem is known as P-complete (Papadimitriou & Tsitsiklis, 1987).In the real world, it is reasonable to assume that direct communication does indeed have additional costs associated with it."}, {"heading": "5.1 Languages of Communication", "text": "Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-"}, {"heading": "5.2 The Effect of Communicating on the Complexity Analysis", "text": "The complexity of the results obtained in Section 3 also apply to the same classes of problems when direct communication is possible. Although agents achieve full observability each time they exchange information, the problem of finding the policy of communication offline (if there are costs associated with each communication act) remains as severe as the general problem without communication. In the worst case, the transmission of messages can be prohibitively expensive. Therefore, adding direct communication does not simplify the problem. In all cases that are considered to be in NEXP, adding direct communication can not make them more difficult. The complexity of choosing a Dec-MDP when observations are independent and direct communication is allowed remains the same as if direct communication is not adopted, as in Lemma 9. The effects of direct communication on the classes of Dec-POMDPs with independent transitions and observations and with possible goal-oriented observations DP. It is interesting to note that the decentralized control with direct communication can be reduced if the observations are dependent on the observations."}, {"heading": "6. Discussion", "text": "In fact, it is a purely mental game, in which it is a question of finding a solution as to how it can be found."}, {"heading": "Acknowledgments", "text": "The authors thank Dan Bernstein for the interesting discussions on the complexity of Dec-MDPs. This work has been partially supported by the National Science Foundation under grant IIS-0219606, the Air Force Office of Scientific Research under grant F49620-03-10090, and NASA under grant NCC 2-1311. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not reflect the views of NSF, AFOSR, or NASA."}], "references": [{"title": "Communication in reactive multiagent robotic systems", "author": ["T. Balch", "R.C. Arkin"], "venue": "Autonomous Robots,", "citeRegEx": "Balch and Arkin,? \\Q1994\\E", "shortCiteRegEx": "Balch and Arkin", "year": 1994}, {"title": "Transition-independent decentralized Markov decision processes", "author": ["R. Becker", "S. Zilberstein", "V. Lesser", "C.V. Goldman"], "venue": "In Proceedings of the Second International Joint Conference on Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Becker et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Becker et al\\.", "year": 2003}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["D. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bernstein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2002}, {"title": "Communication-proof equilibria in cheap-talk games", "author": ["A. Blume", "J. Sobel"], "venue": "Journal of Economic Theory,", "citeRegEx": "Blume and Sobel,? \\Q1995\\E", "shortCiteRegEx": "Blume and Sobel", "year": 1995}, {"title": "Sequential optimality and coordination in multiagent systems", "author": ["C. Boutilier"], "venue": "In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Boutilier,? \\Q1999\\E", "shortCiteRegEx": "Boutilier", "year": 1999}, {"title": "Exploiting structure in policy construction", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Boutilier et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1995}, {"title": "The dynamics of reinforcement learning in cooperative multiagent systems", "author": ["C. Claus", "C. Boutilier"], "venue": "In Proceedings of the Fifteenth National Conference on Artificial Intelligence,", "citeRegEx": "Claus and Boutilier,? \\Q1998\\E", "shortCiteRegEx": "Claus and Boutilier", "year": 1998}, {"title": "Planning under time constraints in stochastic domains", "author": ["T. Dean", "L.P. Kaelbling", "J. Kirman", "A. Nicholson"], "venue": "Artificial Intelligence,", "citeRegEx": "Dean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1995}, {"title": "Generalizing the partial global planning algorithm", "author": ["K.S. Decker", "V.R. Lesser"], "venue": "International Journal of Intelligent Cooperative Information Systems,", "citeRegEx": "Decker and Lesser,? \\Q1992\\E", "shortCiteRegEx": "Decker and Lesser", "year": 1992}, {"title": "Coordination of Distributed Problem Solvers", "author": ["E.H. Durfee"], "venue": null, "citeRegEx": "Durfee,? \\Q1988\\E", "shortCiteRegEx": "Durfee", "year": 1988}, {"title": "Symbolic heuristic search for factored Markov decision processes", "author": ["Z. Feng", "E.A. Hansen"], "venue": "In Proceedings of the Eighteenth National Conference on Artificial Intelligence,", "citeRegEx": "Feng and Hansen,? \\Q2002\\E", "shortCiteRegEx": "Feng and Hansen", "year": 2002}, {"title": "KQML as an agent communication language", "author": ["T. Finin", "Y. Labrou", "J. Mayfield"], "venue": null, "citeRegEx": "Finin et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Finin et al\\.", "year": 1997}, {"title": "Decentralized language learning through acting", "author": ["C.V. Goldman", "M. Allen", "S. Zilberstein"], "venue": "In Proceedings of the Third International Joint Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "Goldman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Goldman et al\\.", "year": 2004}, {"title": "Optimizing information exchange in cooperative multi-agent systems", "author": ["C.V. Goldman", "S. Zilberstein"], "venue": "In Proceedings of the Second International Joint Conference on Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Goldman and Zilberstein,? \\Q2003\\E", "shortCiteRegEx": "Goldman and Zilberstein", "year": 2003}, {"title": "Goal-oriented Dec-MDPs with direct communication", "author": ["C.V. Goldman", "S. Zilberstein"], "venue": "Technical report 04\u201344,", "citeRegEx": "Goldman and Zilberstein,? \\Q2004\\E", "shortCiteRegEx": "Goldman and Zilberstein", "year": 2004}, {"title": "Collaborative plans for complex group action", "author": ["B.J. Grosz", "S. Kraus"], "venue": "Artificial Intelligence,", "citeRegEx": "Grosz and Kraus,? \\Q1996\\E", "shortCiteRegEx": "Grosz and Kraus", "year": 1996}, {"title": "Distributed planning in hierarchical factored MDPs", "author": ["C. Guestrin", "G. Gordon"], "venue": "In Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Guestrin and Gordon,? \\Q2002\\E", "shortCiteRegEx": "Guestrin and Gordon", "year": 2002}, {"title": "Multiagent planning with factored MDPs", "author": ["C. Guestrin", "D. Koller", "R. Parr"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guestrin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2001}, {"title": "Efficient solution algorithms for factored MDPs", "author": ["C. Guestrin", "D. Koller", "R. Parr", "S. Venkataraman"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Guestrin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2003}, {"title": "Dynamic programming for partially observable stochastic games", "author": ["E. Hansen", "D. Bernstein", "S. Zilberstein"], "venue": "In Proceedings of the Nineteenth National Conference on Artificial Intelligence,", "citeRegEx": "Hansen et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hansen et al\\.", "year": 2004}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Games and Decisions", "author": ["R.D. Luce", "H. Raiffa"], "venue": null, "citeRegEx": "Luce and Raiffa,? \\Q1957\\E", "shortCiteRegEx": "Luce and Raiffa", "year": 1957}, {"title": "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings", "author": ["R. Nair", "M. Tambe", "M. Yokoo", "D. Pynadath", "S. Marsella"], "venue": "In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence,", "citeRegEx": "Nair et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2003}, {"title": "On the complexity of designing distributed protocols", "author": ["C.H. Papadimitriou", "J. Tsitsiklis"], "venue": "Information and Control,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1982\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1982}, {"title": "Intractable problems in control theory", "author": ["C.H. Papadimitriou", "J. Tsitsiklis"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1986\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1986}, {"title": "The complexity of Markov decision processes", "author": ["C.H. Papadimitriou", "J. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Learning to cooperate via policy search", "author": ["L. Peshkin", "Kim", "K.-E", "N. Meuleau", "L.P. Kaelbling"], "venue": "In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Peshkin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Peshkin et al\\.", "year": 2000}, {"title": "The communicative multiagent team decision problem: Analyzing teamwork theories and models", "author": ["D.V. Pynadath", "M. Tambe"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pynadath and Tambe,? \\Q2002\\E", "shortCiteRegEx": "Pynadath and Tambe", "year": 2002}, {"title": "The complexity of multiagent systems: The price of silence", "author": ["Z. Rabinovich", "C.V. Goldman", "J.S. Rosenschein"], "venue": "In Proceedings of the Second International Joint Conference on Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "Rabinovich et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Rabinovich et al\\.", "year": 2003}, {"title": "A real-time world model for multi-robot teams with high-latency communication", "author": ["M. Roth", "D. Vail", "M. Veloso"], "venue": "In Proceedings of IROS,", "citeRegEx": "Roth et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2003}, {"title": "Principles of metareasoning", "author": ["S. Russell", "E. Wefald"], "venue": "Artificial Intelligence,", "citeRegEx": "Russell and Wefald,? \\Q1991\\E", "shortCiteRegEx": "Russell and Wefald", "year": 1991}, {"title": "Distributed value functions", "author": ["J. Schneider", "Wong", "W.-K", "A. Moore", "M. Riedmiller"], "venue": "In Proceedings of the Sixteenth International Conference on Machine Learning,", "citeRegEx": "Schneider et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Schneider et al\\.", "year": 1999}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research,", "citeRegEx": "Smallwood and Sondik,? \\Q1973\\E", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "The contract net protocol: High level communication and control in a distributed problem solver", "author": ["R.G. Smith"], "venue": "Readings in Distributed Artificial Intelligence,", "citeRegEx": "Smith,? \\Q1988\\E", "shortCiteRegEx": "Smith", "year": 1988}, {"title": "Cheap talk, coordination, and evolutionary stability", "author": ["K. W\u00e4rneryd"], "venue": "Games and Economic Behavior,", "citeRegEx": "W\u00e4rneryd,? \\Q1993\\E", "shortCiteRegEx": "W\u00e4rneryd", "year": 1993}, {"title": "General principles of learning-based multi-agent systems", "author": ["D.H. Wolpert", "K.R. Wheeler", "K. Tumer"], "venue": "In Proceedings of the Third International Conference on Autonomous Agents,", "citeRegEx": "Wolpert et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Wolpert et al\\.", "year": 1999}, {"title": "Communication decisions in multi-agent cooperation: Model and experiments", "author": ["P. Xuan", "V. Lesser", "S. Zilberstein"], "venue": "In Proceedings of the Fifth International Conference on Autonomous Agents,", "citeRegEx": "Xuan et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Xuan et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 4, "context": "In particular, single-agent planning problems in stochastic domains were modeled as partially observable Markov decision processes (POMDPs) or fully-observable MDPs (Dean, Kaelbling, Kirman, & Nicholson, 1995; Kaelbling, Littman, & Cassandra, 1998; Boutilier, Dearden, & Goldszmidt, 1995). Borrowing from Operations Research techniques, optimal plans can be computed for these planning problems by solving the corresponding Markov decision problem. There has been a vast amount of progress in solving individual MDPs by exploiting domain structure (e.g., Boutilier et al., 1995; Feng & Hansen, 2002). Approximations of MDPs have also been studied, for example, by Guestrin et al. (2003), assuming that the reward function can be decomposed into local reward functions each depending on only a small set of variables.", "startOffset": 249, "endOffset": 687}, {"referenceID": 2, "context": "Bernstein et al. have shown that solving optimally a Dec-MDP is NEXP-complete by reducing the control problem to the tiling problem. Rabinovich et al. (2003) have shown that even approximating the off-line optimal solution of a Dec-MDP remains NEXP.", "startOffset": 0, "endOffset": 158}, {"referenceID": 2, "context": "Bernstein et al. have shown that solving optimally a Dec-MDP is NEXP-complete by reducing the control problem to the tiling problem. Rabinovich et al. (2003) have shown that even approximating the off-line optimal solution of a Dec-MDP remains NEXP. Nair et al. (2003) have presented the Joint Equilibrium-based Search for Policies (JESP) algorithm that finds a locally-optimal joint solution.", "startOffset": 0, "endOffset": 269}, {"referenceID": 2, "context": "Bernstein et al. have shown that solving optimally a Dec-MDP is NEXP-complete by reducing the control problem to the tiling problem. Rabinovich et al. (2003) have shown that even approximating the off-line optimal solution of a Dec-MDP remains NEXP. Nair et al. (2003) have presented the Joint Equilibrium-based Search for Policies (JESP) algorithm that finds a locally-optimal joint solution. Researchers have attempted to approximate the coordination problem by proposing on-line learning procedures. Peshkin et al. (2000) have studied how to approximate the decentralized solution based on a gradient descent approach for on-line learning (when the agents do not know the model).", "startOffset": 0, "endOffset": 525}, {"referenceID": 2, "context": "Bernstein et al. have shown that solving optimally a Dec-MDP is NEXP-complete by reducing the control problem to the tiling problem. Rabinovich et al. (2003) have shown that even approximating the off-line optimal solution of a Dec-MDP remains NEXP. Nair et al. (2003) have presented the Joint Equilibrium-based Search for Policies (JESP) algorithm that finds a locally-optimal joint solution. Researchers have attempted to approximate the coordination problem by proposing on-line learning procedures. Peshkin et al. (2000) have studied how to approximate the decentralized solution based on a gradient descent approach for on-line learning (when the agents do not know the model). Schneider et al. (1999) assume that each decision-maker is assigned a local optimization problem.", "startOffset": 0, "endOffset": 707}, {"referenceID": 2, "context": "Bernstein et al. have shown that solving optimally a Dec-MDP is NEXP-complete by reducing the control problem to the tiling problem. Rabinovich et al. (2003) have shown that even approximating the off-line optimal solution of a Dec-MDP remains NEXP. Nair et al. (2003) have presented the Joint Equilibrium-based Search for Policies (JESP) algorithm that finds a locally-optimal joint solution. Researchers have attempted to approximate the coordination problem by proposing on-line learning procedures. Peshkin et al. (2000) have studied how to approximate the decentralized solution based on a gradient descent approach for on-line learning (when the agents do not know the model). Schneider et al. (1999) assume that each decision-maker is assigned a local optimization problem. Their analysis shows how to approximate the global optimal value function when agents may exchange information about their local values at no cost. Neither convergence nor bounds have been established for this approach. Wolpert et al. (1999) assume that each agent runs a predetermined reinforcement learning algorithm and transforms the coordination problem into updating the local reward function so as to maximize the global reward function.", "startOffset": 0, "endOffset": 1023}, {"referenceID": 9, "context": "Coordination and cooperation have been studied extensively by the distributed artificial intelligence community (Durfee, 1988; Grosz & Kraus, 1996; Smith, 1988) assuming a known and fixed language of communication.", "startOffset": 112, "endOffset": 160}, {"referenceID": 34, "context": "Coordination and cooperation have been studied extensively by the distributed artificial intelligence community (Durfee, 1988; Grosz & Kraus, 1996; Smith, 1988) assuming a known and fixed language of communication.", "startOffset": 112, "endOffset": 160}, {"referenceID": 0, "context": "Becker et al. (2003) presented the first algorithm for optimal off-line decentralized control when a certain structure of the joint reward was assumed.", "startOffset": 0, "endOffset": 21}, {"referenceID": 0, "context": "Becker et al. (2003) presented the first algorithm for optimal off-line decentralized control when a certain structure of the joint reward was assumed. Recently, Hansen et al. (2004) showed how to generalize dynamic programming to solve optimally general decentralized problems.", "startOffset": 0, "endOffset": 183}, {"referenceID": 0, "context": "Becker et al. (2003) presented the first algorithm for optimal off-line decentralized control when a certain structure of the joint reward was assumed. Recently, Hansen et al. (2004) showed how to generalize dynamic programming to solve optimally general decentralized problems. Nevertheless, no existing technique solves efficiently special classes of Dec-POMDPs that we identify in this paper. Pynadath and Tambe (2002) studied a similar model to ours, although they did not propose an algorithm for solving the decentralized control problem.", "startOffset": 0, "endOffset": 422}, {"referenceID": 0, "context": "Becker et al. (2003) presented the first algorithm for optimal off-line decentralized control when a certain structure of the joint reward was assumed. Recently, Hansen et al. (2004) showed how to generalize dynamic programming to solve optimally general decentralized problems. Nevertheless, no existing technique solves efficiently special classes of Dec-POMDPs that we identify in this paper. Pynadath and Tambe (2002) studied a similar model to ours, although they did not propose an algorithm for solving the decentralized control problem. Claus and Boutilier (1998) studied a simple case of decentralized control where agents share information about each other\u2019s actions during the off-line planning stage.", "startOffset": 0, "endOffset": 572}, {"referenceID": 0, "context": "Balch and Arkin\u2019s (1994) approach to communication between robots is inspired by biological models and refers to specific tasks such as foraging, consumption and grazing.", "startOffset": 0, "endOffset": 25}, {"referenceID": 0, "context": "Balch and Arkin\u2019s (1994) approach to communication between robots is inspired by biological models and refers to specific tasks such as foraging, consumption and grazing. Their empirical study was performed in the context of reactive systems and communication was free. Our aim is to find optimal policies of communication and action off-line, taking into account information that agents can acquire on-line. Game theory researchers (Luce & Raiffa, 1957; Aumann & Hart, 1994) have also looked at communication, although the approaches and questions are somewhat different from ours. For example, W\u00e4rneryd (1993), and Blume and Sobel (1995) study how the receiver of a message may alter its actions in", "startOffset": 0, "endOffset": 612}, {"referenceID": 0, "context": "Balch and Arkin\u2019s (1994) approach to communication between robots is inspired by biological models and refers to specific tasks such as foraging, consumption and grazing. Their empirical study was performed in the context of reactive systems and communication was free. Our aim is to find optimal policies of communication and action off-line, taking into account information that agents can acquire on-line. Game theory researchers (Luce & Raiffa, 1957; Aumann & Hart, 1994) have also looked at communication, although the approaches and questions are somewhat different from ours. For example, W\u00e4rneryd (1993), and Blume and Sobel (1995) study how the receiver of a message may alter its actions in", "startOffset": 0, "endOffset": 640}, {"referenceID": 26, "context": "We use properties of conditional probabilities established by Pearl (1988). The notation I(X,Y |Z) means that the set of variables X is independent of the set of variables Y given the set of variables Z.", "startOffset": 62, "endOffset": 75}, {"referenceID": 2, "context": "Process Class Observations Needed by Agent i Reference Dec-POMDP The local sequence of observations: oi (Bernstein et al., 2002) IT, IO Dec-POMDP The local sequence of observations: oi Conjecture 1 IT Dec-MDP (no IO) The local sequence of observations: oi Conjecture 2 IT, IO Dec-MDP The last local observation: oi = si Lemma 2", "startOffset": 104, "endOffset": 128}, {"referenceID": 2, "context": "It is already known that deciding a finite-horizon decentralized MDP is NEXP-complete (Bernstein et al., 2002).", "startOffset": 86, "endOffset": 110}, {"referenceID": 2, "context": "This case can be proved through the same reduction applied by Bernstein et al. (2002). We can reduce the general goal-oriented Dec-MDP problem to the tiling problem by adding a goal state to the last state of the Dec-MDP defined in the reduction.", "startOffset": 62, "endOffset": 86}, {"referenceID": 19, "context": "So far, the only known algorithms for solving optimally decentralized control problems are the generalized version of dynamic programming for Dec-POMDPs (Hansen et al., 2004) and the Coverage-set algorithm (Becker et al.", "startOffset": 153, "endOffset": 174}, {"referenceID": 1, "context": ", 2004) and the Coverage-set algorithm (Becker et al., 2003) for Dec-MDPs with independent transitions and observations.", "startOffset": 39, "endOffset": 60}, {"referenceID": 2, "context": "IO and IT NP-C [Bernstein et al. 2002] [Rabinovich et al.", "startOffset": 15, "endOffset": 38}, {"referenceID": 29, "context": "2002] [Rabinovich et al. 2003]", "startOffset": 6, "endOffset": 30}, {"referenceID": 2, "context": "IO and IT NEXP [Bernstein et al. 2002] [Rabinovich et al.", "startOffset": 15, "endOffset": 38}, {"referenceID": 29, "context": "2002] [Rabinovich et al. 2003]", "startOffset": 6, "endOffset": 30}, {"referenceID": 19, "context": "dynamic programming (Hansen et al., 2004) IT, IO Dec-MDP, Coverage-set (Becker et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 1, "context": ", 2004) IT, IO Dec-MDP, Coverage-set (Becker et al., 2003) no information sharing IT, IO Dec-MDP, Not Known Yet9 Section 5 with direct communication IT, IO, GO-Dec-MDP (|G| = 1) Opt1Goal Section 4.", "startOffset": 37, "endOffset": 58}, {"referenceID": 4, "context": "This results in a fully observable decentralized process, which is equivalent to an MMDP (Boutilier, 1999).", "startOffset": 89, "endOffset": 106}, {"referenceID": 13, "context": "The model was presented by Goldman and Zilberstein (2003). 11.", "startOffset": 27, "endOffset": 58}, {"referenceID": 31, "context": "This approach is based on meta-level control of communication, motivated by a similar decision-theoretic approach to meta-level reasoning that was developed by Russell and Wefald (1991). We assume that the designer of the system also designs a mechanism for communication.", "startOffset": 160, "endOffset": 186}, {"referenceID": 19, "context": "From a theoretical perspective, decentralized partiallyobservable Markov decision processes serve as a formal framework to study the foundations of multi-agent systems (e.g., Becker et al., 2003; Hansen et al., 2004; Guestrin & Gordon, 2002; Peshkin et al., 2000; Pynadath & Tambe, 2002; Claus & Boutilier, 1998).", "startOffset": 168, "endOffset": 312}, {"referenceID": 27, "context": "From a theoretical perspective, decentralized partiallyobservable Markov decision processes serve as a formal framework to study the foundations of multi-agent systems (e.g., Becker et al., 2003; Hansen et al., 2004; Guestrin & Gordon, 2002; Peshkin et al., 2000; Pynadath & Tambe, 2002; Claus & Boutilier, 1998).", "startOffset": 168, "endOffset": 312}, {"referenceID": 9, "context": "The typical distinction previously made in the literature is between systems with no communication and systems with a predefined language of communication, which typically does not incur any costs, overlooking the fact that dependent observations offer yet another form of communication (Pynadath & Tambe, 2002; Decker & Lesser, 1992; Grosz & Kraus, 1996; Durfee, 1988; Roth, Vail, & Veloso, 2003).", "startOffset": 287, "endOffset": 397}, {"referenceID": 9, "context": "The typical distinction previously made in the literature is between systems with no communication and systems with a predefined language of communication, which typically does not incur any costs, overlooking the fact that dependent observations offer yet another form of communication (Pynadath & Tambe, 2002; Decker & Lesser, 1992; Grosz & Kraus, 1996; Durfee, 1988; Roth, Vail, & Veloso, 2003). The problem of combining communication acts into the decision problem of a group of cooperative agents was addressed by Xuan et al. (2001). Their framework is similar to ours but their approach is heuristic.", "startOffset": 356, "endOffset": 538}, {"referenceID": 1, "context": "The Coverageset algorithm that solves optimally decentralized MDPs with a certain reward structure appeared in (Becker et al., 2003).", "startOffset": 111, "endOffset": 132}], "year": 2011, "abstractText": "Decentralized control of cooperative systems captures the operation of a group of decision-makers that share a single global objective. The difficulty in solving optimally such problems arises when the agents lack full observability of the global state of the system when they operate. The general problem has been shown to be NEXP-complete. In this paper, we identify classes of decentralized control problems whose complexity ranges between NEXP and P. In particular, we study problems characterized by independent transitions, independent observations, and goal-oriented objective functions. Two algorithms are shown to solve optimally useful classes of goal-oriented decentralized processes in polynomial time. This paper also studies information sharing among the decision-makers, which can improve their performance. We distinguish between three ways in which agents can exchange information: indirect communication, direct communication and sharing state features that are not controlled by the agents. Our analysis shows that for every class of problems we consider, introducing direct or indirect communication does not change the worst-case complexity. The results provide a better understanding of the complexity of decentralized control problems that arise in practice and facilitate the development of planning algorithms for these problems.", "creator": "dvips(k) 5.86 Copyright 1999 Radical Eye Software"}}}