{"id": "1107.0048", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "Reinforcement Learning for Agents with Many Sensors and Actuators Acting in Categorizable Environments", "abstract": "In this paper, we confront the problem of applying reinforcement learning to agents that perceive the environment through many sensors and that can perform parallel actions using many actuators as is the case in complex autonomous robots. We argue that reinforcement learning can only be successfully applied to this case if strong assumptions are made on the characteristics of the environment in which the learning is performed, so that the relevant sensor readings and motor commands can be readily identified. The introduction of such assumptions leads to strongly-biased learning systems that can eventually lose the generality of traditional reinforcement-learning algorithms. In this line, we observe that, in realistic situations, the reward received by the robot depends only on a reduced subset of all the executed actions and that only a reduced subset of the sensor inputs (possibly different in each situation and for each action) are relevant to predict the reward. We formalize this property in the so called 'categorizability assumption' and we present an algorithm that takes advantage of the categorizability of the environment, allowing a decrease in the learning time with respect to existing reinforcement-learning algorithms. Results of the application of the algorithm to a couple of simulated realistic-robotic problems (landmark-based navigation and the six-legged robot gait generation) are reported to validate our approach and to compare it to existing flat and generalization-based reinforcement-learning approaches.", "histories": [["v1", "Thu, 30 Jun 2011 20:40:15 GMT  (350kb)", "http://arxiv.org/abs/1107.0048v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["e celaya", "j m porta"], "accepted": false, "id": "1107.0048"}, "pdf": {"name": "1107.0048.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning for Agents with Many Sensors and Actuators Acting in Categorizable Environments", "authors": ["Josep M Porta", "Enric Celaya"], "emails": ["porta@science.uva.nl", "celaya@iri.upc.edu"], "sections": [{"heading": null, "text": "In this line, we note that in realistic situations, the reward the robot receives depends only on a reduced subset of all actions performed, and that only a reduced subset of sensor inputs (possibly different for each situation and for each action) is relevant for predicting the reward. We formalize this property in the so-called categorization assumption and present an algorithm that takes advantage of the categorizability of the environment and allows a shortening of the learning time in relation to existing algorithms to enhance learning. The results of applying the algorithm to a few simulated realistic problems (ground-breaking navigation and the creation of a six-legged robot walk) should confirm our approach and compare it with existing flat and generalization-based approaches to enhancement."}, {"heading": "1. Introduction", "text": "The separation between knowledge-based and behavior-based artificial intelligence is the basis for successful applications within the field of autonomous robots (Arkin, 1998), but so far this division has had little impact on the affirmation of learning. Within artificial intelligence, the affirmation problem has been formalized in a very general way, without worrying about how these sets of states and actions are defined (for an introduction to affirmation learning one can Kaelbling, Littmann, Moore, 1998, among many others). Algorithms developed within this general framework can be used in various areas without any alteration. eachc \u00a9 2005 AI Access Foundation reserves all rights."}, {"heading": "2. Problem Formalization", "text": "In fact, it is such that it is a matter of a way in which people are able to recognize themselves and understand what they are doing. (...) The way in which people learn to live in the world, in which they are able to live in the world, in which they live, in which they live, in the world and in the world in which they live, in which they live, in the world in which they live, in the world in which they live, in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they are able to understand, in which they are able to understand, in which they live."}, {"heading": "3. The Categorizability Assumption", "text": "From our experience in developing controllers for autonomous robots, we find that in many realistic situations, the reward the robot receives depends only on a reduced subset of all actions performed by the robot, and that most of the sensor inputs are irrelevant to predicting this reward. For example, the value resulting from the \"capture the object in front of the robot\" action depends on what the object is: the object the robot is supposed to bring to the user, an electrical cable, or an unimportant object. However, the result is likely to be the same whether the robot moves its cameras while capturing the object or not, whether it is day or night, whether the robot simultaneously checks the distance to the nearest wall, or whether it can see a red light nearby (aspects, all of which may become important in other circumstances). If an agent observes the environment and acts in an environment where a reduced fraction of the available inputs and actuators need to be generated, we say that the agent is not fully categorized in a highly categorizable or non-classifiable situation."}, {"heading": "4. Reinforcement Learning in Categorizable Environments: the Partial", "text": "We need a representation system that is able to transmit information between similar situations and also between similar actions. However, clustering techniques or successive subdivisions of state space (such as that presented by McCallum, 1995) focus on the perception side of the problem and aim at determining the reward that can be expected in a given state, with only some of the feature detectors in that state not being completely avoidable. This subset of relevant feature detectors is used to calculate the expected reward in that state for any possible action (the Q (s, a) function, however, with this type of problem definition the curse of the dimensionality of the problem is not completely avoided, as some of the features may be relevant to one action but not to another and this produces an unnecessary (from the point of view of each action) differentiation between equivalent situations, the reduction of the learning speed. This problem can be avoided by finding the action specific characteristics for each action."}, {"heading": "4.1 Value Prediction using Partial Rules", "text": "In a given situation, many partial views are active simultaneously by triggering a subset of the subrules of the controller C. We call this subset the active subrules and refer to them as C. \"To evaluate a given action, we can only consider the rules in C\" with a partial command in accordance with a. \"We refer to this subset as C\" (a). Note that in our approach, when we refer to an action, we can consider the corresponding set of elementary actions (one per motor) and not a single element, since it is the general case in amplification. Each rule w = (v, c) in C \"(a) provides a value prediction for a subrule associated with the subrule. This is an average value that does not provide information about the accuracy of that prediction. As also shown by Wilson (1995), we should favor the use of the subrules with a high accuracy in the value prediction or, as we say, rules with high relevance in the prediction."}, {"heading": "4.2 Partial Rules Value Adjustment", "text": "We adjust the value forecasts for all rules in C \"(a), where a is the last action executed. (Q.\") For each rule to be adjusted, we must update its qw, ew and cw statistics. The effect of each action a in accordance with the partial command c, which is coupled to a partial rule. (v, c) can be defined (using a Bellman-like equation) asq \"w.\" (c. \"), where rw is the average reward achieved immediately after executing c, is the discount factor used to balance the importance of the immediate in terms of delayed reward. (C\") represents the goodness (or value) of the situation in which rules C \"are active, and p.\" (w, C \") is the probability of reaching this situation after executing c if v.\" The value of a situation is executed with the help of the best action."}, {"heading": "4.3 Controller Initialization and Partial Rule Creation/Elimination", "text": "This year, it's gotten to the point that it will be able to leave the country to save it, \"he said.\" We've never had as much time to do it, \"he said.\" We've never had as much time to do it as we do, \"he said."}, {"heading": "5. The Partial Rule Approach in Context", "text": "The classification in the list of countries in which most of them are able to orient themselves in a certain direction, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they"}, {"heading": "6. Experiments", "text": "We show the results of applying our learning algorithm to two robotic-like simulated problems: robotic navigation and hard walking. The first problem is simpler (although it involves more delayed reward) and we use it to clearly describe how the algorithm works. The second problem approaches a realistic robotic application, our long-term goal. We use the two examples to compare the performance of our learning system with the performance of generalizing and non-generalizing reinforcement learning algorithms. The problems addressed are different enough to show the universality of the proposed learning system."}, {"heading": "6.1 Simulated Landmark-Based Navigation", "text": "We face a simple simulated landmark-based navigation task in the forest-like environment shown in Figure 1. The goal for the learner is to go from the starting position (marked with a cross at the bottom of the figure) to the destination position where there is the destination (marked with a cross at the top right corner of the area).The active ingredient can neither go into the lake nor escape from the terrain depicted. The active ingredient can allow the use of some binary landmarks (i.e., function) detectors to identify its position in the environment and decide what action to perform next. In the example, the landmark detectors of the active ingredient are: 1. Rock detector: Active when the rock is seen.Boat detector: Active when the boat is seen.3. Flower detector: Active when the bouquet is seen.4. Tree detector: Active when the tree is seen.5. Bush detector: Active when a bush is seen.6. water."}, {"heading": "XCSPR Algorithm", "text": "In fact, it is the case that most people who are able to live and work in the USA are not able to live in the USA, but in the USA, in Europe, in Europe, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "6.2 Gait Generation for a Six-Legged Robot", "text": "In fact, most of them are able to decide for themselves what they want to do, and they don't."}, {"heading": "Q\u2212LearningPR Algorithm", "text": "The advantage of our algorithm over non-generalizations is increased for problems where some of the sensors provide information that is not related to the task. To test this point, we conducted an experiment in which randomly added 6 feature detectors to the initial 12. With these new features, the number of possible combinations of feature activations increases, and thus the number of states that are taken into account by Q-Learning. Figure 6 shows the comparison between our algorithm and Q-Learning for this problem. Q-Learning is unable to learn a reasonable gait strategy in the 5000 time steps shown in the figure, while the performance of the partial rule algorithm is almost the same as before. This means that the partial rule algorithm is able to detect which sets of features are relevant and use them effectively to determine the behavior of the robot. It is noteworthy that in this case, the ratio of memory problems using this sub-rule is not necessarily comparable to that of the sub-rule 2."}, {"heading": "Q\u2212LearningPR Algorithm", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Exploration 0.1 Exploration 0.01 References", "text": "For example, a rule such as asv (In the Air (1)) \u2192 c (Step (2)) predicts a highly relevant negative reward and this prevents leg 2 from being lifted when leg 1 is in the air. Rules with an order higher than 2 (i.e., not provided to the robot in the initial control) are necessary, for example, to avoid that two adjacent legs are lifted simultaneously. A rule similar (\u00ac In the Air (1))) \u2192 c (Step (1), Step (2))) becomes active when the robot evaluates any action that involves leg 1 and leg 2 being lifted simultaneously. As the value prediction of this rule is very negative and its relevance is high, the action would be discarded in the context of the evaluation, which prevents the robot from falling off. Similar rules must be generated for each pair of adjacent legs that we proceed to generate the robot, we must generate rules with even higher order.Figure 8, we can start the process by seeing the algorithm when we start the process."}, {"heading": "PR Algorithm Hand Coded", "text": "generates the tripod movement from the beginning, which leads to a greater progress of the robot in the initial stages of movement."}, {"heading": "7. Conclusions", "text": "In fact, most people who are able to survive themselves are not able to survive themselves, \"he said.\" But it's not that they feel able to survive themselves. \"He added,\" It's not that they are able to survive themselves. \"He added,\" It's not that they are able to survive themselves. \"He added,\" It's not that they are able to survive themselves, but that they are able to survive themselves. \"Indeed,\" It's as if they are able to survive themselves, as if they are able to survive themselves. \""}, {"heading": "Acknowledgments", "text": "The authors would like to thank the anonymous reviewers of the paper. Their contributions to improving the quality of this paper are relevant enough to be considered in a sense as co-authors of the paper. The shortcomings that are still contained in the paper can only be attributed to the nominal authors. The second author was partially supported by the funds of the Spanish Ministry of Ciencia y Tecnolog \u0301 \u0131a and FEDER as part of the project DPI2003-05193-C02-01 of the Plan Nacional de I + D + I."}, {"heading": "Appendix A: The Partial-Rule Learning Algorithm", "text": "In this appendix we describe in detail the approach described in the main part of the paper. The learning algorithm of the sub-rule (the top level of which is shown in Figure 11) stores the following information for each sub-rule \u2022 the value (i.e. the discounted cumulative reward) estimate qw, \u2022 the error estimate ew and \u2022 the trust index iw.To estimate the trust on qw and ew, we use a trust index iw, which roughly tracks the number of times the sub-rule is used. Confidence is derived from iw by using a trust function in the following way: cw = trust function (iw), the trust function being a non-decreasing function in the span [0, \u03b2]. \u03b2 should be less than 1, as in this way the system always maintains a certain degree of exploration and is therefore able to adapt to changes in the environment. Different trust systems can be implemented by changing the trust function."}, {"heading": "Action Evaluation", "text": "The simplest way to determine the estimated value for actions is a brute force approach, which consists of the independent evaluation of each and every one of these actions. In simple cases, this approach would suffice, but if the number of valid combinations of elementary actions (i.e. actions) is large, the separate evaluation of each action would take a long time, which would extend the time of each robotic decision and reduce the reactivity of the control. To avoid this, Appendix B represents a more efficient method to determine the value of each act.Figure 13 summarizes the action evaluation process using partial rules. The value for each action is guessed using the most relevant rule for that action (i.e. the winning rule).This winning rule is randomly (C) = arg max."}, {"heading": "Statistics Update", "text": "In the Statistics Update Procedure (Figure 14), qw and ew are adjusted for all rules that were active in the previous time step, and a subcommand is proposed according to a (the last action performed).Both qw and ew are updated with a learning rate (mw) calculated using the MAM function, which is initially 1, and consequently the initial values of qw and ew have no effect on the future values of these variables. These initial values become relevant when using a constant learning rate, as many existing reinforcement learning algorithms do. If the observed effects of the last action performed match the current estimated interval for the value (Iw), the trust index increases by one unit. Otherwise, the trust index decreases, allowing for a faster adjustment of the statistics to the most recently obtained surprising reward values."}, {"heading": "Partial-Rule Management", "text": "This approach (Figure 15) involves the generation of new sub-rules and the elimination of sub-rules that have already been created that have proved useless. (Figure 15) This approach (Figure 15) involves the generation of new sub-rules and the elimination of sub-rules that have already been created. (Figure 1) In this way, we focus our efforts on improving these situations with major errors in the value prediction. (Figure 2) Each time a false prediction is made, new sub-rules are generated by the combination of rules contained in the defined C'ant (a), this approach becomes consistent with the action performed a. (Figure 2) These rules relate to the situation action whose value prediction we must improve. The combination of two sub-rules w1 consists of a new sub-rule with a sub-rule that includes all the features contained in the sub-rule."}, {"heading": "Appendix B: Efficient Action Evaluation", "text": "This year, it will be ready to leave the country in which it is located."}, {"heading": "Brute Force Evaluation Tree\u2212Based Evaluation", "text": "The cost of our algorithm largely depends on the specific sub-rules accepted by the motors. This is because, in the worst case, the cost of our algorithm is as follows: O (nr l nm), with nr the number of rules, nm the number of motors and l the maximum range of values accepted by the motors. This is because, in order to insert a given rule, we must visit all the nodes of a maximally extended tree (i.e., a tree in which each node l has sub-trees and where all the final nodes of the branches are still open).The number of nodes of such a tree is nm, which is near the toy + 1 \u2212 1l \u2212 1 = O (lnm).We can calculate the cost printing taking into account the total number of the total number of costs of the presented O (cost).This optimal action is given by the leaf circled with a dashed line, which is the leaf with a greater guess value."}, {"heading": "Appendix C: Notation", "text": "Upper case letters are used for sets, and Greek letters represent parameters of algorithms. S set of partial determinations of states. s, s \u2032 individual determinations. Complete view. ns number of states. FD = {fdi \u2032 i = 1.. nf} set of characteristic identification. v (fdi1,.., fdik) Partial view of order k. A set of actions of the robot. A = {eai | i = 1.. ne} set of elementary actions. nm number of motors of the robot. eai = (mi \u2190 w) Elementary action that value k the motor mi. c (eai1,.., eaik) Partial arrangement k. a = (ea1,., eanm) action. Combination of elementary actions."}], "references": [{"title": "Behavior-Based Robotics. Intelligent Robotics and Autonomous Agents", "author": ["R.C. Arkin"], "venue": null, "citeRegEx": "Arkin,? \\Q1998\\E", "shortCiteRegEx": "Arkin", "year": 1998}, {"title": "Dynamic Programming", "author": ["R.E. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Decision-theoretic planning: Structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Intelligence without representation", "author": ["R.A. Brooks"], "venue": "Artificial Intelligence,", "citeRegEx": "Brooks,? \\Q1991\\E", "shortCiteRegEx": "Brooks", "year": 1991}, {"title": "C-XCS: An implementation of the XCS in C. (http://www.cs.bath.ac.uk/ amb/LCSWEB/computer.htm)", "author": ["M. Butz"], "venue": null, "citeRegEx": "Butz,? \\Q1999\\E", "shortCiteRegEx": "Butz", "year": 1999}, {"title": "Control of a six-legged robot walking on abrupt terrain", "author": ["E. Celaya", "J.M. Porta"], "venue": "In Proceedings of the IEEE International Conference on Robotics and Automation,", "citeRegEx": "Celaya and Porta,? \\Q1996\\E", "shortCiteRegEx": "Celaya and Porta", "year": 1996}, {"title": "A control structure for the locomotion of a legged robot on difficult terrain", "author": ["E. Celaya", "J.M. Porta"], "venue": "IEEE Robotics and Automation Magazine, Special Issue on Walking Robots,", "citeRegEx": "Celaya and Porta,? \\Q1998\\E", "shortCiteRegEx": "Celaya and Porta", "year": 1998}, {"title": "Input generalization in delayed reinforcement learning: An algorithm and performance comparisons", "author": ["D. Chapman", "L.P. Kaelbling"], "venue": "In Proceedings of the International Joint Conference on Artificial Intelligence,", "citeRegEx": "Chapman and Kaelbling,? \\Q1991\\E", "shortCiteRegEx": "Chapman and Kaelbling", "year": 1991}, {"title": "The dynamics of reinforcement learning in cooperative multiagent systems", "author": ["C. Claus", "C. Boutilier"], "venue": "In Proceedings of the Fifteenth National Conference on Artificial Intelligence,", "citeRegEx": "Claus and Boutilier,? \\Q1998\\E", "shortCiteRegEx": "Claus and Boutilier", "year": 1998}, {"title": "Accelerating reinforcement learning by composing solutions of automatically identified subtasks", "author": ["C. Drummond"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Drummond,? \\Q2002\\E", "shortCiteRegEx": "Drummond", "year": 2002}, {"title": "Neuronal Darwinism", "author": ["G.M. Edelman"], "venue": null, "citeRegEx": "Edelman,? \\Q1989\\E", "shortCiteRegEx": "Edelman", "year": 1989}, {"title": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, chap. Distributed Representations", "author": ["G. Hinton", "J. McClelland", "D. Rumelhart"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 1986}, {"title": "Hybrid learning architecture based on neural networks for adaptive control of a walking machine", "author": ["W. Ilg", "T. M\u00fchlfriedel", "K. Berns"], "venue": "In Proceedings of the 1997 IEEE International Conference on Robotics and Automation,", "citeRegEx": "Ilg et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Ilg et al\\.", "year": 1997}, {"title": "Learning in Embedded Systems. A Bradford Book", "author": ["L.P. Kaelbling"], "venue": null, "citeRegEx": "Kaelbling,? \\Q1993\\E", "shortCiteRegEx": "Kaelbling", "year": 1993}, {"title": "Reinforcement learning: A survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Kaelbling et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "Sparse Distributed Memory", "author": ["P. Kanerva"], "venue": null, "citeRegEx": "Kanerva,? \\Q1988\\E", "shortCiteRegEx": "Kanerva", "year": 1988}, {"title": "Q-learning of complex behaviors on a six-legged walking machine", "author": ["F. Kirchner"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Kirchner,? \\Q1998\\E", "shortCiteRegEx": "Kirchner", "year": 1998}, {"title": "Evolution and development of modular control architectures for 1-d locomotion in six-legged animats", "author": ["J. Kodjabachia", "J.A. Meyer"], "venue": "Connection Science,", "citeRegEx": "Kodjabachia and Meyer,? \\Q1998\\E", "shortCiteRegEx": "Kodjabachia and Meyer", "year": 1998}, {"title": "Learning to coordinate behaviors", "author": ["P. Maes", "R.A. Brooks"], "venue": "In Proceedings of the AAAI-90,", "citeRegEx": "Maes and Brooks,? \\Q1990\\E", "shortCiteRegEx": "Maes and Brooks", "year": 1990}, {"title": "Automatic programming of behavior-based robots using reinforcement learning", "author": ["S. Mahadevan", "J.H. Connell"], "venue": "Artificial Intelligence,", "citeRegEx": "Mahadevan and Connell,? \\Q1992\\E", "shortCiteRegEx": "Mahadevan and Connell", "year": 1992}, {"title": "Reinforcement Learning with Selective Perception and Hidden State", "author": ["A.K. McCallum"], "venue": "Ph.D. thesis,", "citeRegEx": "McCallum,? \\Q1995\\E", "shortCiteRegEx": "McCallum", "year": 1995}, {"title": "Co-evolving model parameters for anytime learning in evolutionary robotics", "author": ["G.B. Parker"], "venue": "Robotics and Autonomous Systems,", "citeRegEx": "Parker,? \\Q2000\\E", "shortCiteRegEx": "Parker", "year": 2000}, {"title": "C-trace: A new algorithm for reinforcement learning of robotic control", "author": ["M.D. Pendrith", "M.R.K. Ryan"], "venue": "In Proceedings of the 1996 International Workshop on Learning for Autonomous Robots (Robotlearn\u201396)", "citeRegEx": "Pendrith and Ryan,? \\Q1996\\E", "shortCiteRegEx": "Pendrith and Ryan", "year": 1996}, {"title": "Regularization algorithms for learning that are equivalent to multilayer networks", "author": ["T. Poggio", "F. Girosi"], "venue": null, "citeRegEx": "Poggio and Girosi,? \\Q1990\\E", "shortCiteRegEx": "Poggio and Girosi", "year": 1990}, {"title": "The speed prior: A new simplicity measure yielding near-optimal computable predictions", "author": ["J. Schmidhuber"], "venue": "In Proceedings of the 15th Annual Conference on Computational Learning Theory (COLT 2OO2). Lecture Notes In Artificial Intelligence", "citeRegEx": "Schmidhuber,? \\Q2002\\E", "shortCiteRegEx": "Schmidhuber", "year": 2002}, {"title": "Learning to coordinate without sharing information", "author": ["S. Sen"], "venue": "In Proceedings of the Twelfth National Conference on Artificial Intelligence,", "citeRegEx": "Sen,? \\Q1994\\E", "shortCiteRegEx": "Sen", "year": 1994}, {"title": "Reinforcement learning architectures for animats", "author": ["R.S. Sutton"], "venue": "Proceedings of the First International Conference on Simulation of Adaptive Behavior. From Animals to Animats,", "citeRegEx": "Sutton,? \\Q1991\\E", "shortCiteRegEx": "Sutton", "year": 1991}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Online learning with random representations", "author": ["R.S. Sutton", "S.D. Whitehead"], "venue": "In Proceedings of the Eleventh International Conference on Machine Learning,", "citeRegEx": "Sutton and Whitehead,? \\Q1993\\E", "shortCiteRegEx": "Sutton and Whitehead", "year": 1993}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["R. Sutton"], "venue": "In Proceedings of the 1995 Conference on Advances in Neural Information Processing,", "citeRegEx": "Sutton,? \\Q1996\\E", "shortCiteRegEx": "Sutton", "year": 1996}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Multi-agent reinforcement learning: Independent vs. cooperative agents", "author": ["M. Tan"], "venue": "In Reading in Agents,", "citeRegEx": "Tan,? \\Q1997\\E", "shortCiteRegEx": "Tan", "year": 1997}, {"title": "A distributed genetic programming architecture for the evolution of robust insect locomotion controllers", "author": ["E.E. Vallejo", "F. Ramos"], "venue": "Supplement Proceedings of the Sixth International Conference on Simulation of Adaptive Behavior: From Animals to Animats,", "citeRegEx": "Vallejo and Ramos,? \\Q2000\\E", "shortCiteRegEx": "Vallejo and Ramos", "year": 2000}, {"title": "Apprentissage Adaptatif et Apprentissage Supervis\u00e9 par Algorithme G\u00e9n\u00e9tique", "author": ["G. Venturini"], "venue": "Ph.D. thesis", "citeRegEx": "Venturini,? \\Q1994\\E", "shortCiteRegEx": "Venturini", "year": 1994}, {"title": "Adaptive switching circuits", "author": ["B. Widrow", "M. Hoff"], "venue": "In Western Electronic Show and Convention,", "citeRegEx": "Widrow and Hoff,? \\Q1960\\E", "shortCiteRegEx": "Widrow and Hoff", "year": 1960}, {"title": "Classifier fitness based on accuracy", "author": ["S.W. Wilson"], "venue": "Evolutionary Computation,", "citeRegEx": "Wilson,? \\Q1995\\E", "shortCiteRegEx": "Wilson", "year": 1995}, {"title": "Explore/exploit strategies in autonomy. In From Animals to Animats", "author": ["S.W. Wilson"], "venue": "Proceedings of the 4th International Conference on Simulation of Adaptive Behavior,", "citeRegEx": "Wilson,? \\Q1996\\E", "shortCiteRegEx": "Wilson", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "The division between knowledge-based and behavior-based artificial intelligence has been fundamental to achieving successful applications within the field of autonomous robots (Arkin, 1998).", "startOffset": 176, "endOffset": 189}, {"referenceID": 3, "context": "However, as clearly pointed by Brooks (1991), in autonomous robots the major hurdles are those related with perception and action representations.", "startOffset": 31, "endOffset": 45}, {"referenceID": 3, "context": "As Brooks (1991) remarked, dealing with a real environment is not necessarily a problem since real environments have properties that can be exploited to reduce the complexity of the robot\u2019s controller.", "startOffset": 3, "endOffset": 17}, {"referenceID": 29, "context": "Generalizing reinforcement-learning algorithms such as those using gradient-descent techniques (Widrow & Hoff, 1960), coarse codings (Hinton, McClelland, & Rumelhart, 1986), radial-basis functions (Poggio & Girosi, 1990), tile coding (Sutton, 1996) or decision trees (Chapman & Kaelbling, 1991; McCallum, 1995) can partially palliate this problem since they can deal with large state spaces.", "startOffset": 234, "endOffset": 248}, {"referenceID": 20, "context": "Generalizing reinforcement-learning algorithms such as those using gradient-descent techniques (Widrow & Hoff, 1960), coarse codings (Hinton, McClelland, & Rumelhart, 1986), radial-basis functions (Poggio & Girosi, 1990), tile coding (Sutton, 1996) or decision trees (Chapman & Kaelbling, 1991; McCallum, 1995) can partially palliate this problem since they can deal with large state spaces.", "startOffset": 267, "endOffset": 310}, {"referenceID": 1, "context": "This is the well known curse of dimensionality introduced by Bellman (1957), whose research presaged some of the work in reinforcement learning.", "startOffset": 61, "endOffset": 76}, {"referenceID": 1, "context": "This is the well known curse of dimensionality introduced by Bellman (1957), whose research presaged some of the work in reinforcement learning. Although the size of the action set (na) is as important as the size of the state set (ns) in the curse of dimensionality, less attention is paid to actions in the reinforcement-learning literature. However, a robot with many degrees of freedom can execute many elementary actions simultaneously and this makes the cost of the learning algorithms also increase exponentially with the number of motors of the robot (nm). Suppose we address the same task but with two different sets of feature detectors FD1 and FD2 such that FD1 \u2282 FD2. Using a plain reinforcement-learning algorithm, the cost of finding a proper policy would be larger using the larger set of features (FD2). And this is so even if one of the features in FD2 \u2212 FD1 has a stronger correlation with the reward than any of the features in FD1. Non-generalizing reinforcement-learning algorithms are not able to take advantage of this situation, and, even having better input information, their performance decreases. A similar argument can be made for actions in addition to feature detectors. Generalizing reinforcement-learning algorithms such as those using gradient-descent techniques (Widrow & Hoff, 1960), coarse codings (Hinton, McClelland, & Rumelhart, 1986), radial-basis functions (Poggio & Girosi, 1990), tile coding (Sutton, 1996) or decision trees (Chapman & Kaelbling, 1991; McCallum, 1995) can partially palliate this problem since they can deal with large state spaces. However, as we approach complex realistic problems, the number of dimensions of the state-space grows to the point of making the use of some of these generalization techniques impractical and other function approximation techniques must be used (Sutton & Barto, 1998, page 209). Adding relevant inputs or actions to a task should make this task easier or at least not more difficult. Only methods whose complexity depends on the relevance of the available inputs and actions and not on their number would scale well to real domain problems. Examples of systems fulfilling this property are, for instance, the Kanerva coding system presented by Kanerva (1988) and the random representation method by Sutton and Whitehead (1993).", "startOffset": 61, "endOffset": 2253}, {"referenceID": 1, "context": "This is the well known curse of dimensionality introduced by Bellman (1957), whose research presaged some of the work in reinforcement learning. Although the size of the action set (na) is as important as the size of the state set (ns) in the curse of dimensionality, less attention is paid to actions in the reinforcement-learning literature. However, a robot with many degrees of freedom can execute many elementary actions simultaneously and this makes the cost of the learning algorithms also increase exponentially with the number of motors of the robot (nm). Suppose we address the same task but with two different sets of feature detectors FD1 and FD2 such that FD1 \u2282 FD2. Using a plain reinforcement-learning algorithm, the cost of finding a proper policy would be larger using the larger set of features (FD2). And this is so even if one of the features in FD2 \u2212 FD1 has a stronger correlation with the reward than any of the features in FD1. Non-generalizing reinforcement-learning algorithms are not able to take advantage of this situation, and, even having better input information, their performance decreases. A similar argument can be made for actions in addition to feature detectors. Generalizing reinforcement-learning algorithms such as those using gradient-descent techniques (Widrow & Hoff, 1960), coarse codings (Hinton, McClelland, & Rumelhart, 1986), radial-basis functions (Poggio & Girosi, 1990), tile coding (Sutton, 1996) or decision trees (Chapman & Kaelbling, 1991; McCallum, 1995) can partially palliate this problem since they can deal with large state spaces. However, as we approach complex realistic problems, the number of dimensions of the state-space grows to the point of making the use of some of these generalization techniques impractical and other function approximation techniques must be used (Sutton & Barto, 1998, page 209). Adding relevant inputs or actions to a task should make this task easier or at least not more difficult. Only methods whose complexity depends on the relevance of the available inputs and actions and not on their number would scale well to real domain problems. Examples of systems fulfilling this property are, for instance, the Kanerva coding system presented by Kanerva (1988) and the random representation method by Sutton and Whitehead (1993). While those systems rely on large collections of fixed prototypes (i.", "startOffset": 61, "endOffset": 2321}, {"referenceID": 19, "context": "This technique is used, for instance, by Mahadevan and Connell (1992). Unfortunately, in the problem we are confronting, this is not enough since, in our case, actions are composed by combinations of elementary actions and we also want to transfer reward information between similar combinations of actions.", "startOffset": 41, "endOffset": 70}, {"referenceID": 35, "context": "As also pointed by Wilson (1995), we should favor the use of the partial rules with a high accuracy in value prediction or, as we say it, rules with a high relevance.", "startOffset": 19, "endOffset": 33}, {"referenceID": 10, "context": "The existence of (almost) redundant rules is not necessarily negative, since they provide robustness to the controller, the so called degeneracy effect introduced by Edelman (1989). What must be avoided is to generate the same rule twice, since this is not useful at all.", "startOffset": 166, "endOffset": 181}, {"referenceID": 24, "context": "The categorizability assumption is closely related with complexity theory principles such as the Minimum Description Length (MDL) that has been used by authors such as Schmidhuber (2002) to bias learning algorithms.", "startOffset": 168, "endOffset": 187}, {"referenceID": 25, "context": "In multiagent learning (Claus & Boutilier, 1998; Sen, 1994; Tan, 1997) the objective is to learn an optimal behavior for a group of agents trying to cooperatively solve a given task.", "startOffset": 23, "endOffset": 70}, {"referenceID": 31, "context": "In multiagent learning (Claus & Boutilier, 1998; Sen, 1994; Tan, 1997) the objective is to learn an optimal behavior for a group of agents trying to cooperatively solve a given task.", "startOffset": 23, "endOffset": 70}, {"referenceID": 9, "context": "Finally, the way in which we define complex actions from elementary actions has some points in common with the works in reinforcement learning where macro-actions are defined as the learner confronts different tasks (Sutton, Precup, & Singh, 1999; Drummond, 2002).", "startOffset": 216, "endOffset": 263}, {"referenceID": 27, "context": "The partial-rule formalism can be seen as a generalization of that of the XCS classifier systems described by Wilson (1995). This XCS learning system aims at determining a set of classifiers (that are combinations of features with an associated action) with their associated value and relevance predictions.", "startOffset": 110, "endOffset": 124}, {"referenceID": 3, "context": "For instance, the work of Maes and Brooks (1990) includes the possible execution of elementary actions in parallel.", "startOffset": 35, "endOffset": 49}, {"referenceID": 3, "context": "For instance, the work of Maes and Brooks (1990) includes the possible execution of elementary actions in parallel. However this system does not include any mechanism detecting interactions between actions and, thus, the coordination of actions relies on sensory conditions. For instance, this system has difficulties detecting that the execution of two actions results always (i.e., independently of the active/inactive feature detectors) in positive/negative reward. The CASCADE algorithm by Kaelbling (1993) learns each bit of a complex action separately.", "startOffset": 35, "endOffset": 511}, {"referenceID": 4, "context": "To perform this test, we used the implementation of Wilson\u2019s XCS developed by Butz (1999). To make XCS work in the same search space as the partial-rule algorithm, we modified the XCS implementation to be able to deal with non-binary actions.", "startOffset": 78, "endOffset": 90}, {"referenceID": 4, "context": "To perform this test, we used the implementation of Wilson\u2019s XCS developed by Butz (1999). To make XCS work in the same search space as the partial-rule algorithm, we modified the XCS implementation to be able to deal with non-binary actions. No other modification, but parameter adjustment, were introduced in the original code. The results presented here corresponds to the average of 10 runs using the set of parameters that gave a better result. Nominally, these parameters were: learning rate \u03b1 = 0.1, decay rate \u03b3 = 0.9, maximum number of classifiers \u03bc = 200 (however, the initial set is empty), the genetic algorithm is applied in average every 5 time steps, the deletion experience is 5, the subsume experience is 15, the fall off rate is 0.1, the minimum error 0.01, a prediction threshold of 0.5, the crossover probability is 0.8, the mutation probability 0.04 and the initial don\u2019t care probability 1/3. The prediction and the fitness of new classifiers are initialized to 10 and the error to 0. A detailed explanation of the meaning of those parameters is provided by Wilson (1995) and also by the comments in the code of Butz (1999).", "startOffset": 78, "endOffset": 1094}, {"referenceID": 4, "context": "To perform this test, we used the implementation of Wilson\u2019s XCS developed by Butz (1999). To make XCS work in the same search space as the partial-rule algorithm, we modified the XCS implementation to be able to deal with non-binary actions. No other modification, but parameter adjustment, were introduced in the original code. The results presented here corresponds to the average of 10 runs using the set of parameters that gave a better result. Nominally, these parameters were: learning rate \u03b1 = 0.1, decay rate \u03b3 = 0.9, maximum number of classifiers \u03bc = 200 (however, the initial set is empty), the genetic algorithm is applied in average every 5 time steps, the deletion experience is 5, the subsume experience is 15, the fall off rate is 0.1, the minimum error 0.01, a prediction threshold of 0.5, the crossover probability is 0.8, the mutation probability 0.04 and the initial don\u2019t care probability 1/3. The prediction and the fitness of new classifiers are initialized to 10 and the error to 0. A detailed explanation of the meaning of those parameters is provided by Wilson (1995) and also by the comments in the code of Butz (1999).", "startOffset": 78, "endOffset": 1146}, {"referenceID": 3, "context": "For instance, Maes and Brooks (1990) implemented a specific method based on immediate reward to derive the preconditions for each leg to perform the step.", "startOffset": 23, "endOffset": 37}, {"referenceID": 3, "context": "For instance, Maes and Brooks (1990) implemented a specific method based on immediate reward to derive the preconditions for each leg to perform the step. Pendrith and Ryan (1996) used a simplified version of the", "startOffset": 23, "endOffset": 180}, {"referenceID": 16, "context": "six-legged walking problem to test an algorithm able to deal with Non-Markovian spaces of states and Kirchner (1998) presented a hierarchical version of Q-learning to learn the low-level movements of each leg, as well as a coordination scheme between the low-level learned behaviors.", "startOffset": 101, "endOffset": 117}, {"referenceID": 16, "context": "six-legged walking problem to test an algorithm able to deal with Non-Markovian spaces of states and Kirchner (1998) presented a hierarchical version of Q-learning to learn the low-level movements of each leg, as well as a coordination scheme between the low-level learned behaviors. Ilg, M\u00fchlfriedel, and Berns (1997) introduced a learning architecture based on self-organizing neural networks, and Kodjabachia and Meyer (1998) proposed an evolutionary strategy to develop a neural network to control the gait of the robot.", "startOffset": 101, "endOffset": 319}, {"referenceID": 16, "context": "six-legged walking problem to test an algorithm able to deal with Non-Markovian spaces of states and Kirchner (1998) presented a hierarchical version of Q-learning to learn the low-level movements of each leg, as well as a coordination scheme between the low-level learned behaviors. Ilg, M\u00fchlfriedel, and Berns (1997) introduced a learning architecture based on self-organizing neural networks, and Kodjabachia and Meyer (1998) proposed an evolutionary strategy to develop a neural network to control the gait of the robot.", "startOffset": 101, "endOffset": 429}, {"referenceID": 16, "context": "six-legged walking problem to test an algorithm able to deal with Non-Markovian spaces of states and Kirchner (1998) presented a hierarchical version of Q-learning to learn the low-level movements of each leg, as well as a coordination scheme between the low-level learned behaviors. Ilg, M\u00fchlfriedel, and Berns (1997) introduced a learning architecture based on self-organizing neural networks, and Kodjabachia and Meyer (1998) proposed an evolutionary strategy to develop a neural network to control the gait of the robot. Vallejo and Ramos (2000) used a parallel genetic algorithm architecture and Parker (2000) described an evolutionary computation where the robot executes the best controller found up to a given moment while a new optimal controller is computed in an off-line simulation.", "startOffset": 101, "endOffset": 550}, {"referenceID": 16, "context": "six-legged walking problem to test an algorithm able to deal with Non-Markovian spaces of states and Kirchner (1998) presented a hierarchical version of Q-learning to learn the low-level movements of each leg, as well as a coordination scheme between the low-level learned behaviors. Ilg, M\u00fchlfriedel, and Berns (1997) introduced a learning architecture based on self-organizing neural networks, and Kodjabachia and Meyer (1998) proposed an evolutionary strategy to develop a neural network to control the gait of the robot. Vallejo and Ramos (2000) used a parallel genetic algorithm architecture and Parker (2000) described an evolutionary computation where the robot executes the best controller found up to a given moment while a new optimal controller is computed in an off-line simulation.", "startOffset": 101, "endOffset": 615}, {"referenceID": 5, "context": "Using this simulator, we implemented the behaviors described by Celaya and Porta (1996) except those in charge of the gait generation.", "startOffset": 64, "endOffset": 88}], "year": 2011, "abstractText": "In this paper, we confront the problem of applying reinforcement learning to agents that perceive the environment through many sensors and that can perform parallel actions using many actuators as is the case in complex autonomous robots. We argue that reinforcement learning can only be successfully applied to this case if strong assumptions are made on the characteristics of the environment in which the learning is performed, so that the relevant sensor readings and motor commands can be readily identified. The introduction of such assumptions leads to strongly-biased learning systems that can eventually lose the generality of traditional reinforcement-learning algorithms. In this line, we observe that, in realistic situations, the reward received by the robot depends only on a reduced subset of all the executed actions and that only a reduced subset of the sensor inputs (possibly different in each situation and for each action) are relevant to predict the reward. We formalize this property in the so called categorizability assumption and we present an algorithm that takes advantage of the categorizability of the environment, allowing a decrease in the learning time with respect to existing reinforcement-learning algorithms. Results of the application of the algorithm to a couple of simulated realisticrobotic problems (landmark-based navigation and the six-legged robot gait generation) are reported to validate our approach and to compare it to existing flat and generalizationbased reinforcement-learning approaches.", "creator": "dvips(k) 5.92b Copyright 2002 Radical Eye Software"}}}