{"id": "1107.0051", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "On Prediction Using Variable Order Markov Models", "abstract": "This paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models. The class of such algorithms is large and in principle includes any lossless compression algorithm. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real life sequences from three domains: proteins, English text and music pieces. The comparison is made with respect to prediction quality as measured by the average log-loss. We also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks. Our results indicate that a \"decomposed\" CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in sequence prediction tasks. Somewhat surprisingly, a different algorithm, which is a modification of the Lempel-Ziv compression algorithm, significantly outperforms all algorithms on the protein classification problems.", "histories": [["v1", "Thu, 30 Jun 2011 20:43:01 GMT  (411kb)", "http://arxiv.org/abs/1107.0051v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["r begleiter", "r el-yaniv", "g yona"], "accepted": false, "id": "1107.0051"}, "pdf": {"name": "1107.0051.pdf", "metadata": {"source": "CRF", "title": "On Prediction Using Variable Order Markov Models", "authors": ["Ron Begleiter", "Ran El-Yaniv"], "emails": ["ronbeg@cs.technion.ac.il", "rani@cs.technion.ac.il", "golan@cs.cornell.edu"], "sections": [{"heading": "1. Introduction", "text": "Examples of such applications include biological sequence analysis (Bejerano & Yona, 2001), language and language modeling (Protectors & Singer, 1994), text analysis and extraction (McCallum et al., 2000), music production and classification (Pachet, 2002; Dubnov et al., 2003), handwritten recognition (Singer & Tishby, 1994), and behavioral identification (Clarkson Pentum et al.)."}, {"heading": "2. Preliminaries", "text": "In fact, it is a way in which most people are able to decide whether they are able to make a decision, or whether they are able to make a decision, and whether they are able to make a decision, whether they are able to make a decision, whether they are able to make a decision, whether they are able to make a decision, whether they are able to make a decision, and whether they are able to make a decision."}, {"heading": "3. VMM Prediction Algorithms", "text": "In order to assess the significance of our results, it is important to understand the differences between the different algorithms discussed in this study and their variations. In this section, each of these six algorithms is described in detail."}, {"heading": "3.1 Lempel-Ziv 78 (LZ78)", "text": "The lz78 algorithm begins with a phrase that contains the empty phrase that we do not overlap. \"The lz78 algorithms are among the most popular lossless compression algorithms.\" We are a phrase that does not overlap. \"It is used as the basis for the Unix compression tool and other popular archiving tools for PCs.\" The predictive component of this algorithm was first discussed by Langdon (1983) and Rissanen (1983). Presentation of this algorithm is simplified after the well-known lz78 algorithm compression, which works as follows. Given a sequence qn1 that we incorporate incrementally into non-overlapping phrases \"phrases\" that are conceived into a phrase. \""}, {"heading": "3.2 Prediction by Partial Match (PPM)", "text": "These \"phases\" can be combined and operated together. In particular, the ppm-ii variant of the ppm-ii problem is regarded as flight and exclusion using two mechanisms."}, {"heading": "3.3 The Context Tree Weighting Method (CTW)", "text": "It is possible to generate this suffix tree in time to generate the sum of all suffix trees that allow a fast suffix tree development. (...) It is possible to generate this suffix tree in time. (...) It is possible that we generate a corresponding suffix tree that allows a fast suffix tree development. (...) It is possible to generate this suffix tree in time. (...) It is possible to generate a corresponding suffix tree that allows a fast suffix tree development. (...) It is possible to generate this suffix tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree tree suffix tree tree tree tree tree tree tree tree suffix tree tree tree tree tree suffix tree tree tree tree tree suffix tree tree tree tree tree tree tree suffix tree tree tree"}, {"heading": "3.4 CTW for Multi-Alphabets", "text": "The binary representation shown above is, of course, a power of the second condition, that we can deal with sequences of larger alphabets in a natural way. (i) one must expand the KT estimator for larger alphabets, and (ii) extend the recourse relationship (8). While these extensions are easy to achieve, the resulting algorithms are reported to be poorly developed (see Volf, 2002, chap. 4). As Volf notes, the reason for this is that the extended KT estimator does not provide an efficient smoothing function for large alphabets. Therefore, the problem of expanding ctw algorithms for large alphabets is a challenge. Several ctw extensions for larger alphabets have been proposed, the first being a naive application of the standard binary ctw algorithms over a binary representation of the sequence. Binary representation is, of course, when the size of the alphabets is a power of the second condition."}, {"heading": "3.5 Probabilistic Suffix Trees (PST)", "text": "The Probabilistic Suffix Tree (pst) prediction algorithm (Ron et al., 1996) attempts to construct the individual \"best\" D-delimited VMMs according to the training sequence. It is assumed that an upper boundary D in the Markov order of the \"true source\" is known to the learner. These edge labels define a unique sequence s for each path from a node v to the root. The sequence s denotes the node v. Each such Pst tree induces a \"suffix set\" S consisting of the elements of all nodes. The goal of the Pst learning algorithm is to identify a good suffix set S for a Pst tree and assign a probability distribution."}, {"heading": "3.6 LZ-MS: An Improved Lempel-Ziv Algorithm", "text": "In fact, most people who are in the city are also able to move, move and move."}, {"heading": "4. Experimental Setup", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "5. Prediction Results", "text": "Performance is measured by the log loss, as discussed above. Remember that the average log loss of a predictor (over a sequence) is a slightly optimistic estimate of the compression rate (bits / symbol) that could be achieved by the algorithm over that sequence. However, there are no statistically significant differences between the text domains (Calgary Corpus). We see that de-ctw is the overall winner with an average loss of 3.02. Runner-up is ppm-c with an average loss of 3.03. However, there is no statistically significant difference between the text domains. The Calgary Corpus is available at ftp: / ftp.cpsc.ucalgary.ca / pub / projects / text.compression.corpus contains this corpus, in addition to text, also a few source code of computer programs, a number of digital programs."}, {"heading": "6. Protein Classification", "text": "In fact, most of them will be able to move to another world where they are able to integrate, \"he said.\" It's not as if they are able to move to another world, \"he said.\" But it's not as if they are going to slide into another world. \""}, {"heading": "7. Related work", "text": "In this section, we will briefly discuss some of the results related to the current work. We will confine ourselves to comparative studies containing algorithms that we are looking at here and to discussions of some recent extensions of these algorithms. We will also discuss some of the results in terms of protein compression and prediction. The ppm idea and its overwhelming success in benchmark testing has attracted considerable attention and numerous ppm variants have been proposed and studied. ppm-d (Howard, 1993) is very similar to ppm-c, with a slight modification of the escape mechanism; see Section 3.2. The ppm variant eliminates the need to specify a maximum order (Cleary & Teahan, 1997). In order to predict the probability of the next symbol, ppm uses \"contexts that give probability. If no such context exists, ppm\" uses the longest possible context (i.e.). One of the most successful ppm variants over Calgary Corgary."}, {"heading": "8. Concluding Remarks and Open Questions", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "9. Acknowledgments", "text": "We are very grateful to Gil Bejerano, Charles Bloom and Paul Volf for providing valuable advice and support. We would also like to thank Bob Carpenter for his PPMC implementation in compression Java, Moti Nisenson for his LZms Java implementation and Adiel Ben Shalom for his MIDI support. Finally, we would like to thank the anonymous referees for their good comments. Ran El-Yaniv's work was supported in part by the Technion V.P.R. Fund for Sponsored Research and in part by the PASCAL Network of Excellence."}, {"heading": "Appendix A. Algorithm Hyper-Parameters Selection", "text": "In our experimental protocol, each algorithm optimizes its hyperparameters during the training phase. \"478\" observed parameters. \"The best set of parameters is then used for training (see Section 4 for details).This appendix sets out the possible hyperparameter values that were originally considered for each algorithm, both the predictive and the classification experiments. These values were selected (based on common sense) before starting each experiment. Again, for each algorithm, we use the averages that were actually selected. We present these hyperparameter values in two tables: Table 8 for the predictive setup and Table 9 for the protein classification setup. We used two different hyperparameter sets for these two setups. In particular, in the classification setup, as described in Section 6, the length of the training sequences is significantly greater than the length of the training sequences in the pre-experiment section 4."}, {"heading": "Appendix B. Music Representation", "text": "Our music data consists of polyphonic pieces of music, which are specified as MIDI files. A polyphonic piece of music is usually represented in a MIDI file as a collection of channels. Each channel is typically associated with a single musical instrument and contains instructions (so-called \"events\") about which notes should be played and how long and loud they should be played. We treat each channel as an individual sequence. If a piece of five instruments is played, encoded in five MIDI channels, we generate five individual sequences, one for each channel. Note that each entry in Table 4 is an average of all channels of the corresponding piece. We now describe how we represent each MIDI channel. Each note in a channel is represented as a triple: pitch: volume: duration. Both pitch and volume have 128 possible values according to the MIDI protocol. The duration is measured in milliseconds. In addition, we include a special note that we call a \"silent note\" and the duration of that sequence as a negative note."}, {"heading": "Appendix C. On the LZ-MS Hyper-Parameters", "text": "This appendix provides an indication of the effectiveness of the M and S hyperparameters of the Lz-ms algorithm. The experimental setup is identical to that in Section 4. Table 10 shows the log loss results of the Calgary Corpus of Applications of 29. The first ten notes of Chopin's Etude 12 op. 10 are: \"83: 120: 240: 128: 0: -240: 79: 120: 240: 128: 0: 240: -240: -240: 77: 120: 240: 128: 0: -240: 74: 120: 240: 128: 0: 240: 71: 120: 240: 128: 0: 180:.\" The Lz-ms algorithm: In the second column only the M parameter is activated (and S = 0); and the third column returns the results when only the S parameter is activated (and M = 0). The losses in the last column correspond to applications where M and S are the best choice (where S alone is the best choice) as the best choice between the two."}], "references": [{"title": "On the computational complexity of approximating probability distributions by probabilistic automata", "author": ["N. Abe", "M. Warmuth"], "venue": "Machine Learning,", "citeRegEx": "Abe and Warmuth,? \\Q1992\\E", "shortCiteRegEx": "Abe and Warmuth", "year": 1992}, {"title": "Text compression by context tree weighting", "author": ["J. Aberg", "Y. Shtarkov"], "venue": "In Proceedings Data Compression Conference (DCC),", "citeRegEx": "Aberg and Shtarkov,? \\Q1997\\E", "shortCiteRegEx": "Aberg and Shtarkov", "year": 1997}, {"title": "A new method for analyzing protein sequence relationships based on sammon maps", "author": ["D. Agrafiotis"], "venue": "Protein Science, 6, 287\u2013293.", "citeRegEx": "Agrafiotis,? 1997", "shortCiteRegEx": "Agrafiotis", "year": 1997}, {"title": "A corpus for the evaluation of lossless compression algorithms", "author": ["R. Arnold", "T. Bell"], "venue": "In Designs, Codes and Cryptography,", "citeRegEx": "Arnold and Bell,? \\Q1997\\E", "shortCiteRegEx": "Arnold and Bell", "year": 1997}, {"title": "Texture mixing and texture movie synthesis using statistical learning", "author": ["Z. Bar-Joseph", "R. El-Yaniv", "D. Lischinski", "M. Werman"], "venue": "IEEE Transactions on Visualization and Computer Graphics,", "citeRegEx": "Bar.Joseph et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Bar.Joseph et al\\.", "year": 2001}, {"title": "Variations on probabilistic suffix trees: Statistical modeling and the prediction of protein", "author": ["G. Bejerano", "G. Yona"], "venue": "families.. Bioinformatics,", "citeRegEx": "Bejerano and Yona,? \\Q2001\\E", "shortCiteRegEx": "Bejerano and Yona", "year": 2001}, {"title": "Solving problems of context modeling", "author": ["C. Bloom"], "venue": "http://www.cbloom.com/papers/index.html.", "citeRegEx": "Bloom,? 1998", "shortCiteRegEx": "Bloom", "year": 1998}, {"title": "Semantically motivated improvements for PPM variants", "author": ["S. Bunton"], "venue": "The Computer Journal, 40 (2/3), 76\u201392.", "citeRegEx": "Bunton,? 1997", "shortCiteRegEx": "Bunton", "year": 1997}, {"title": "A block-sorting lossless data compression algorithm", "author": ["M. Burrows", "D.J. Wheeler"], "venue": "Tech. rep. 124,", "citeRegEx": "Burrows and Wheeler,? \\Q1994\\E", "shortCiteRegEx": "Burrows and Wheeler", "year": 1994}, {"title": "Protein structure prediction center", "author": ["CASP"], "venue": "http://predictioncenter.llnl.gov/.", "citeRegEx": "CASP,? 2002", "shortCiteRegEx": "CASP", "year": 2002}, {"title": "Compressing XML with multiplexed hierarchical PPM models", "author": ["J. Cheney"], "venue": "Data Compression Conference, pp. 163\u2013172.", "citeRegEx": "Cheney,? 2001", "shortCiteRegEx": "Cheney", "year": 2001}, {"title": "Predicting daily behavior via wearable sensors", "author": ["B. Clarkson", "A. Pentland"], "venue": "Tech. rep. Vismod TR#540,", "citeRegEx": "Clarkson and Pentland,? \\Q2001\\E", "shortCiteRegEx": "Clarkson and Pentland", "year": 2001}, {"title": "Unbounded length contexts for PPM", "author": ["J. Cleary", "W. Teahan"], "venue": "Computer Journal,", "citeRegEx": "Cleary and Teahan,? \\Q1997\\E", "shortCiteRegEx": "Cleary and Teahan", "year": 1997}, {"title": "Data compression using adaptive coding and partial string matching", "author": ["J. Cleary", "I. Witten"], "venue": "IEEE Transactions on Communications,", "citeRegEx": "Cleary and Witten,? \\Q1984\\E", "shortCiteRegEx": "Cleary and Witten", "year": 1984}, {"title": "A convergent gambling estimate of the entropy of English", "author": ["T. Cover", "R. King"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cover and King,? \\Q1978\\E", "shortCiteRegEx": "Cover and King", "year": 1978}, {"title": "Elements of Information Theory", "author": ["T. Cover", "J. Thomas"], "venue": null, "citeRegEx": "Cover and Thomas,? \\Q1991\\E", "shortCiteRegEx": "Cover and Thomas", "year": 1991}, {"title": "The origin and evolution of protein superfamilies", "author": ["M. Dayhoff"], "venue": "Federation Proceedings, pp. 2132\u20132138.", "citeRegEx": "Dayhoff,? 1976", "shortCiteRegEx": "Dayhoff", "year": 1976}, {"title": "Universal lossless data compression algorithms", "author": ["S. Deorowicz"], "venue": "Ph.D. thesis, Silesian University of Technology.", "citeRegEx": "Deorowicz,? 2003", "shortCiteRegEx": "Deorowicz", "year": 2003}, {"title": "Multi-class protein fold recognition using support vector machines and neural", "author": ["C. Ding", "I. Dubchak"], "venue": "networks. Bioinformatics,", "citeRegEx": "Ding and Dubchak,? \\Q2001\\E", "shortCiteRegEx": "Ding and Dubchak", "year": 2001}, {"title": "PPMexe: PPM for compressing software", "author": ["M. Drinic", "D. Kirovski"], "venue": "In Data Compression Conference,", "citeRegEx": "Drinic and Kirovski,? \\Q2002\\E", "shortCiteRegEx": "Drinic and Kirovski", "year": 2002}, {"title": "Using machine-learning methods for musical style modeling", "author": ["S. Dubnov", "G. Assayag", "O. Lartillot", "G. Bejerano"], "venue": "IEEE Computer,", "citeRegEx": "Dubnov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Dubnov et al\\.", "year": 2003}, {"title": "Relations between entropy and error probability", "author": ["M. Feder", "N. Merhav"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Feder and Merhav,? \\Q1994\\E", "shortCiteRegEx": "Feder and Merhav", "year": 1994}, {"title": "Parser for protein folding", "author": ["L. Holm", "C. Sander"], "venue": "units. Proteins,", "citeRegEx": "Holm and Sander,? \\Q1994\\E", "shortCiteRegEx": "Holm and Sander", "year": 1994}, {"title": "The design and analysis of efficient lossless data compression systems", "author": ["P. Howard"], "venue": "Ph.D. thesis, Brown University.", "citeRegEx": "Howard,? 1993", "shortCiteRegEx": "Howard", "year": 1993}, {"title": "Using the Fisher kernel method to detect remote protein homologies", "author": ["T. Jaakkola", "M. Diekhans", "D. Haussler"], "venue": "In Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology,", "citeRegEx": "Jaakkola et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jaakkola et al\\.", "year": 1999}, {"title": "Prediction of protein structural classes by a new measure of information discrepancy", "author": ["L. Jin", "W. Fang", "H. Tang"], "venue": "Computational Biology and Chemistry,", "citeRegEx": "Jin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2003}, {"title": "On-line algorithms for combining language models", "author": ["A. Kalai", "S. Chen", "A. Blum", "R. Rosenfeld"], "venue": "In Proceedings of the International Conference on Accoustics,", "citeRegEx": "Kalai et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 1999}, {"title": "Improved smoothing for probabilistic suffix trees seen as variable order Markov chains", "author": ["C. Kermorvant", "P. Dupont"], "venue": "In European Conference on Machine Learning (ECML),", "citeRegEx": "Kermorvant and Dupont,? \\Q2002\\E", "shortCiteRegEx": "Kermorvant and Dupont", "year": 2002}, {"title": "The performance of universal encoding", "author": ["R. Krichevsky", "V. Trofimov"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Krichevsky and Trofimov,? \\Q1981\\E", "shortCiteRegEx": "Krichevsky and Trofimov", "year": 1981}, {"title": "Structural class prediction: An application of residue distribution along the sequence", "author": ["T. Kumarevel", "M. Gromiha", "M. Ponnuswamy"], "venue": "Biophys Chem.,", "citeRegEx": "Kumarevel et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kumarevel et al\\.", "year": 2000}, {"title": "A note on the Ziv-Lempel model for compressing individual sequences", "author": ["G. Langdon"], "venue": "IEEE Transactions on Information Theory, 29, 284\u2013 287.", "citeRegEx": "Langdon,? 1983", "shortCiteRegEx": "Langdon", "year": 1983}, {"title": "Folding units in globular proteins", "author": ["A. Lesk", "G. Rose"], "venue": "In Proceedings of the National Academy of Sciences,", "citeRegEx": "Lesk and Rose,? \\Q1981\\E", "shortCiteRegEx": "Lesk and Rose", "year": 1981}, {"title": "Mismatch string kernels for discriminative protein", "author": ["C. Leslie", "E. Eskin", "A. Cohen", "J. Weston", "W. Noble"], "venue": "classification. Bioinformatics,", "citeRegEx": "Leslie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Leslie et al\\.", "year": 2004}, {"title": "Structural patterns in globular proteins", "author": ["M. Levitt", "C. Chothia"], "venue": null, "citeRegEx": "Levitt and Chothia,? \\Q1976\\E", "shortCiteRegEx": "Levitt and Chothia", "year": 1976}, {"title": "An analysis of the burrows-wheeler transform", "author": ["G. Manzini"], "venue": "Journal of the ACM, 48 (3), 407\u2013430.", "citeRegEx": "Manzini,? 2001", "shortCiteRegEx": "Manzini", "year": 2001}, {"title": "Concentration inequalities for the missing mass and for histogram rule error", "author": ["D. McAllester", "L. Ortiz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "McAllester and Ortiz,? \\Q2003\\E", "shortCiteRegEx": "McAllester and Ortiz", "year": 2003}, {"title": "Maximum entropy Markov models for information extraction and segmentation", "author": ["A. McCallum", "D. Freitag", "F. Pereira"], "venue": "In Proc. 17th International Conf. on Machine Learning,", "citeRegEx": "McCallum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "What are the baselines for protein fold recognition", "author": ["L. McGuffin", "K. Bryson", "D. Jones"], "venue": null, "citeRegEx": "McGuffin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "McGuffin et al\\.", "year": 2001}, {"title": "A strong version of the redundancy-capacity theorem of universal coding", "author": ["N. Merhav", "M. Feder"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Merhav and Feder,? \\Q1995\\E", "shortCiteRegEx": "Merhav and Feder", "year": 1995}, {"title": "On loss functions which minimize to conditional expected values and posterior probabilities", "author": ["J. Miller", "R. Goodman", "P. Smyth"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Miller et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1993}, {"title": "Implementing the PPM data compression scheme", "author": ["A. Moffat"], "venue": "IEEE Transactions on Communications, 38 (11), 1917\u20131921.", "citeRegEx": "Moffat,? 1990", "shortCiteRegEx": "Moffat", "year": 1990}, {"title": "SCOP: A structural classification of proteins database for the investigation of sequences and structures", "author": ["A. Murzin", "S. Brenner", "T. Hubbard", "C. Chothia"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Murzin et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Murzin et al\\.", "year": 1995}, {"title": "The Data Compression Book (2nd ed.). MIS:Press", "author": ["M. Nelson", "J. Gailly"], "venue": null, "citeRegEx": "Nelson and Gailly,? \\Q1996\\E", "shortCiteRegEx": "Nelson and Gailly", "year": 1996}, {"title": "Protein is incompressible", "author": ["C. Nevill-Manning", "I. Witten"], "venue": "In Data Compression Conference,", "citeRegEx": "Nevill.Manning and Witten,? \\Q1999\\E", "shortCiteRegEx": "Nevill.Manning and Witten", "year": 1999}, {"title": "Towards behaviometric security systems: Learning to identify a typist", "author": ["M. Nisenson", "I. Yariv", "R. El-Yaniv", "R. Meir"], "venue": "In The 7th European Conference on Principles and Practice of Knowledge Discovery in Databases", "citeRegEx": "Nisenson et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nisenson et al\\.", "year": 2003}, {"title": "Always Good Turing: Asymptotically optimal probability estimation", "author": ["A. Orlitsky", "N. Santhanam", "J. Zhang"], "venue": "Science,", "citeRegEx": "Orlitsky et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Orlitsky et al\\.", "year": 2003}, {"title": "Playing with virtual musicians: The continuator in practice", "author": ["F. Pachet"], "venue": "IEEE MultiMedia, 9 (3), 77\u201382.", "citeRegEx": "Pachet,? 2002", "shortCiteRegEx": "Pachet", "year": 2002}, {"title": "Sequence comparisons using multiple sequences detect three times as many remote homologues as pairwise methods", "author": ["J. Park", "K. Karplus", "C. Barrett", "R. Hughey", "D. Haussler", "T. Hubbard", "C. Chothia"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Park et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Park et al\\.", "year": 1998}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L. Rabiner"], "venue": "Proceedings of the IEEE, 77 (3), 257\u2013286.", "citeRegEx": "Rabiner,? 1989", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "Fundamentals of Speech Recognition", "author": ["L. Rabiner", "B. Juang"], "venue": null, "citeRegEx": "Rabiner and Juang,? \\Q1993\\E", "shortCiteRegEx": "Rabiner and Juang", "year": 1993}, {"title": "A universal data compression system", "author": ["J. Rissanen"], "venue": "IEEE Transactions on Information Theory, 29 (5), 656\u2013664.", "citeRegEx": "Rissanen,? 1983", "shortCiteRegEx": "Rissanen", "year": 1983}, {"title": "Universal coding, information, prediction, and estimation", "author": ["J. Rissanen"], "venue": "IEEE Transactions on Information Theory, 30 (4), 629\u2013636.", "citeRegEx": "Rissanen,? 1984", "shortCiteRegEx": "Rissanen", "year": 1984}, {"title": "The power of amnesia: Learning probabilistic automata with variable memory length", "author": ["D. Ron", "Y. Singer", "N. Tishby"], "venue": "Machine Learning,", "citeRegEx": "Ron et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Ron et al\\.", "year": 1996}, {"title": "Hierarchic organization of domains in globular proteins", "author": ["G. Rose"], "venue": "Journal of Molecular Biology, 134, 447\u2013470.", "citeRegEx": "Rose,? 1979", "shortCiteRegEx": "Rose", "year": 1979}, {"title": "Implementing the context tree weighting method for text compression", "author": ["K. Sadakane", "T. Okazaki", "H. Imai"], "venue": "In Data Compression Conference,", "citeRegEx": "Sadakane et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sadakane et al\\.", "year": 2000}, {"title": "Redundancy of the Lempel-Ziv incremental parsing rule", "author": ["S. Savari"], "venue": "IEEE Transactions on Information Theory, 43, 9\u201321.", "citeRegEx": "Savari,? 1997", "shortCiteRegEx": "Savari", "year": 1997}, {"title": "Part-of-speech tagging using a variable memory Markov model", "author": ["H. Sch\u00fctze", "Y. Singer"], "venue": "In Proceedings of the 32nd Conference on Association for Computational Linguistics,", "citeRegEx": "Sch\u00fctze and Singer,? \\Q1994\\E", "shortCiteRegEx": "Sch\u00fctze and Singer", "year": 1994}, {"title": "PPM: One step to practicality", "author": ["D. Shkarin"], "venue": "Data Compression Conference, pp. 202\u2013212.", "citeRegEx": "Shkarin,? 2002", "shortCiteRegEx": "Shkarin", "year": 2002}, {"title": "Adaptive mixtures of probabilistic transducers", "author": ["Y. Singer"], "venue": "Neural Computation, 9 (8), 1711\u2013 1733.", "citeRegEx": "Singer,? 1997", "shortCiteRegEx": "Singer", "year": 1997}, {"title": "Dynamical encoding of cursive handwriting", "author": ["Y. Singer", "N. Tishby"], "venue": "Biological Cybernetics,", "citeRegEx": "Singer and Tishby,? \\Q1994\\E", "shortCiteRegEx": "Singer and Tishby", "year": 1994}, {"title": "Pfam: A comprehensive database of protein domain families based on seed", "author": ["E. Sonnhammer", "S. Eddy", "R. Durbin"], "venue": "alignments. Proteins,", "citeRegEx": "Sonnhammer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Sonnhammer et al\\.", "year": 1997}, {"title": "A genomic perspective on protein families", "author": ["R. Tatusov", "V. Eugene", "J. David"], "venue": null, "citeRegEx": "Tatusov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tatusov et al\\.", "year": 1997}, {"title": "Using compression based language models for text categorization", "author": ["W. Teahan", "D. Harper"], "venue": "In Workshop on Language Modeling and Information Retrieval,", "citeRegEx": "Teahan and Harper,? \\Q2001\\E", "shortCiteRegEx": "Teahan and Harper", "year": 2001}, {"title": "Using compression for source based classification of text", "author": ["N. Thaper"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology.", "citeRegEx": "Thaper,? 2001", "shortCiteRegEx": "Thaper", "year": 2001}, {"title": "A context-tree weighting method for text generating sources", "author": ["T. Tjalkens", "P. Volf", "F. Willems"], "venue": "In Data Compression Conference,", "citeRegEx": "Tjalkens et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Tjalkens et al\\.", "year": 1997}, {"title": "Implementing the context-tree weighting method: Arithmetic coding", "author": ["T. Tjalkens", "F. Willems"], "venue": "In International Conference on Combinatorics, Information Theory and Statistics,", "citeRegEx": "Tjalkens and Willems,? \\Q1997\\E", "shortCiteRegEx": "Tjalkens and Willems", "year": 1997}, {"title": "Weighting Techniques in Data Compression Theory and Algorithms", "author": ["P. Volf"], "venue": "Ph.D. thesis, Technische Universiteit Eindhoven.", "citeRegEx": "Volf,? 2002", "shortCiteRegEx": "Volf", "year": 2002}, {"title": "The context-tree weighting method: Extensions", "author": ["F. Willems"], "venue": "IEEE Transactions on Information Theory, 44 (2), 792\u2013798.", "citeRegEx": "Willems,? 1998", "shortCiteRegEx": "Willems", "year": 1998}, {"title": "The context-tree weighting method: Basic properties", "author": ["F. Willems", "Y. Shtarkov", "T. Tjalkens"], "venue": "IEEE Transactions on Information", "citeRegEx": "Willems et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Willems et al\\.", "year": 1995}, {"title": "The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression", "author": ["I. Witten", "T. Bell"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Witten and Bell,? \\Q1991\\E", "shortCiteRegEx": "Witten and Bell", "year": 1991}, {"title": "Text mining: A new frontier for lossless compression", "author": ["I. Witten", "Z. Bray", "M. Mahoui", "B. Teahan"], "venue": "In Proceedings of the Conference on Data Compression,", "citeRegEx": "Witten et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Witten et al\\.", "year": 1999}, {"title": "Within the twilight zone: A sensitive profile-profile comparison tool based on information theory", "author": ["G. Yona", "M. Levitt"], "venue": "Journal of Molecular Biology,", "citeRegEx": "Yona and Levitt,? \\Q2002\\E", "shortCiteRegEx": "Yona and Levitt", "year": 2002}, {"title": "Protomap: Automatic classification of protein sequences, a hierarchy of protein families, and local maps of the protein", "author": ["G. Yona", "N. Linial", "M. Linial"], "venue": "space. Proteins,", "citeRegEx": "Yona et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Yona et al\\.", "year": 1999}, {"title": "Compression of individual sequences via variable-rate coding", "author": ["J. Ziv", "A. Lempel"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Ziv and Lempel,? \\Q1978\\E", "shortCiteRegEx": "Ziv and Lempel", "year": 1978}], "referenceMentions": [{"referenceID": 36, "context": "Examples of such applications are biological sequence analysis (Bejerano & Yona, 2001), speech and language modeling (Sch\u00fctze & Singer, 1994; Rabiner & Juang, 1993), text analysis and extraction (McCallum et al., 2000) music generation and classification (Pachet, 2002; Dubnov et al.", "startOffset": 195, "endOffset": 218}, {"referenceID": 46, "context": ", 2000) music generation and classification (Pachet, 2002; Dubnov et al., 2003), hand-writing recognition (Singer & Tishby, 1994) and behaviormetric identification (Clarkson & Pentland, 2001; Nisenson et al.", "startOffset": 44, "endOffset": 79}, {"referenceID": 20, "context": ", 2000) music generation and classification (Pachet, 2002; Dubnov et al., 2003), hand-writing recognition (Singer & Tishby, 1994) and behaviormetric identification (Clarkson & Pentland, 2001; Nisenson et al.", "startOffset": 44, "endOffset": 79}, {"referenceID": 44, "context": ", 2003), hand-writing recognition (Singer & Tishby, 1994) and behaviormetric identification (Clarkson & Pentland, 2001; Nisenson et al., 2003).", "startOffset": 92, "endOffset": 142}, {"referenceID": 4, "context": ", (Bar-Joseph et al., 2001).", "startOffset": 2, "endOffset": 27}, {"referenceID": 48, "context": "Perhaps the most commonly used techniques are based on Hidden Markov Models (HMMs) (Rabiner, 1989).", "startOffset": 83, "endOffset": 98}, {"referenceID": 30, "context": "The results of our protein classification experiments are rather surprising as both of these two excellent predictors are inferior to an algorithm, which is obtained by simple modifications of the prediction component of the well-known Lempel-Ziv-78 compression algorithm (Ziv & Lempel, 1978; Langdon, 1983).", "startOffset": 272, "endOffset": 307}, {"referenceID": 30, "context": "The results of our protein classification experiments are rather surprising as both of these two excellent predictors are inferior to an algorithm, which is obtained by simple modifications of the prediction component of the well-known Lempel-Ziv-78 compression algorithm (Ziv & Lempel, 1978; Langdon, 1983). This rather new algorithm, recently proposed by Nisenson et al. (2003), is a consistent winner in the all the protein classification experiments and achieves surprisingly good results that may be of independent interest in protein analysis.", "startOffset": 293, "endOffset": 380}, {"referenceID": 51, "context": "A lower bound on the redundancy of any universal prediction (and compression) algorithm is \u03a9(K( log T 2T )), where K is (roughly) the number of parameters of the model encoding the distribution P\u0302 (Rissanen, 1984).", "startOffset": 197, "endOffset": 213}, {"referenceID": 51, "context": ", (Rissanen, 1984).", "startOffset": 2, "endOffset": 18}, {"referenceID": 38, "context": "A related lower bound in terms of channel capacity is given by Merhav and Feder (1995).", "startOffset": 63, "endOffset": 87}, {"referenceID": 44, "context": "We also included a recent prediction algorithm from Nisenson et al. (2003) that is a modification of the lz78 prediction algorithm.", "startOffset": 52, "endOffset": 75}, {"referenceID": 30, "context": "The prediction component of this algorithm was first discussed by Langdon (1983) and Rissanen (1983).", "startOffset": 66, "endOffset": 81}, {"referenceID": 30, "context": "The prediction component of this algorithm was first discussed by Langdon (1983) and Rissanen (1983). The presentation of this algorithm is simplified after the well-known lz78 compression algorithm, which works as follows, is understood.", "startOffset": 66, "endOffset": 101}, {"referenceID": 34, "context": "com/bzip2), which is based on the successful Burrows-Wheeler Transform (Burrows & Wheeler, 1994; Manzini, 2001).", "startOffset": 71, "endOffset": 111}, {"referenceID": 55, "context": "Within a probabilistic setting (see Section 2), when the unknown source is stationary and ergodic Markov of finite order, the redundancy is bounded above by (1/ ln n) where n is the length of the training sequence (Savari, 1997).", "startOffset": 214, "endOffset": 228}, {"referenceID": 30, "context": "An lz78-based prediction algorithm was proposed by Langdon (1983) and Rissanen (1983).", "startOffset": 51, "endOffset": 66}, {"referenceID": 30, "context": "An lz78-based prediction algorithm was proposed by Langdon (1983) and Rissanen (1983). We describe separately the learning and prediction phases.", "startOffset": 51, "endOffset": 86}, {"referenceID": 57, "context": "Specifically, the ppm-ii variant of ppm currently achieves the best compression rates over the standard Calgary Corpus benchmark (Shkarin, 2002).", "startOffset": 129, "endOffset": 144}, {"referenceID": 7, "context": "This implementation of the escape mechanism by Moffat (1990) is considered to be among the better ppm variants, based on empirical examination (Bunton, 1997).", "startOffset": 143, "endOffset": 157}, {"referenceID": 39, "context": "This implementation of the escape mechanism by Moffat (1990) is considered to be among the better ppm variants, based on empirical examination (Bunton, 1997).", "startOffset": 47, "endOffset": 61}, {"referenceID": 7, "context": "This implementation of the escape mechanism by Moffat (1990) is considered to be among the better ppm variants, based on empirical examination (Bunton, 1997). As noted by Witten and Bell (1991) there is no principled justification for any of the various ppm escape mechanisms.", "startOffset": 144, "endOffset": 194}, {"referenceID": 68, "context": "The Context Tree Weighting Method (ctw) algorithm (Willems et al., 1995) is a strong lossless compression algorithm that is based on a clever idea for combining exponentially many VMMs of bounded order.", "startOffset": 50, "endOffset": 72}, {"referenceID": 68, "context": "The paper presenting this idea (Willems et al., 1995) received the 1996 Paper Award of the IEEE Information Theory Society.", "startOffset": 31, "endOffset": 53}, {"referenceID": 67, "context": "A more sophisticated solution is proposed by Willems (1998); see also Section 7.", "startOffset": 45, "endOffset": 60}, {"referenceID": 63, "context": "As shown by Tjalkens et al. (1997), the above simple form of the KT estimator is equivalent to the original form, given in terms of a Dirichlet distribution by Krichevsky and Trofimov (1981).", "startOffset": 12, "endOffset": 35}, {"referenceID": 28, "context": "(1997), the above simple form of the KT estimator is equivalent to the original form, given in terms of a Dirichlet distribution by Krichevsky and Trofimov (1981). The (original) KT-", "startOffset": 132, "endOffset": 163}, {"referenceID": 66, "context": "However, as already mentioned by Willems et al. (1995), it is possible to obtain linear time complexities for both training and prediction.", "startOffset": 33, "endOffset": 55}, {"referenceID": 54, "context": "More details on this efficient implementation are nicely explained by Sadakane et al. (2000) (see also Tjalkens & Willems, 1997).", "startOffset": 70, "endOffset": 93}, {"referenceID": 66, "context": "The second method we consider is Volf\u2019s \u2018decomposed ctw\u2019, denoted here by de-ctw (Volf, 2002).", "startOffset": 81, "endOffset": 93}, {"referenceID": 64, "context": "A more sophisticated binary decomposition of ctw was considered by Tjalkens et al. (1997). There eight binary machines were simultaneously constructed, one for each of the eight binary digits of the ascii representation of text.", "startOffset": 67, "endOffset": 90}, {"referenceID": 52, "context": "The Probabilistic Suffix Tree (pst) prediction algorithm (Ron et al., 1996) attempts to construct the single \u201cbest\u201d D-bounded VMM according to the training sequence.", "startOffset": 57, "endOffset": 75}, {"referenceID": 44, "context": "algorithm due to Nisenson et al. (2003). The algorithm has two parameters M and S and, therefore, its acronym here is lz-ms.", "startOffset": 17, "endOffset": 40}, {"referenceID": 14, "context": "Cover and King (1978) considered a very similar prediction game in their well-known work on the estimation of the entropy of English text.", "startOffset": 0, "endOffset": 22}, {"referenceID": 41, "context": "\u2022 The protein set includes proteins (amino-acid sequences) from the well-known Structural Classification of Proteins (SCOP) database (Murzin et al., 1995).", "startOffset": 133, "endOffset": 154}, {"referenceID": 12, "context": "75, while the compression results of Cleary and Teahan (1997) are 3.", "startOffset": 37, "endOffset": 62}, {"referenceID": 5, "context": "A similar finding was observed by Bejerano and Yona (2001) where models that were trained over specific protein families coded unrelated non-member proteins using an average code length that was higher than the entropy of the background distribution.", "startOffset": 34, "endOffset": 59}, {"referenceID": 41, "context": "Specifically, the SCOP database (Murzin et al., 1995)", "startOffset": 32, "endOffset": 53}, {"referenceID": 16, "context": "Biologists classify proteins into relational sets such as \u2018families\u2019, \u2018superfamilies\u2019, \u2018fold families\u2019 and \u2018classes\u2019; these terms were coined by Dayhoff (1976) and Levitt and Chothia (1976).", "startOffset": 145, "endOffset": 160}, {"referenceID": 16, "context": "Biologists classify proteins into relational sets such as \u2018families\u2019, \u2018superfamilies\u2019, \u2018fold families\u2019 and \u2018classes\u2019; these terms were coined by Dayhoff (1976) and Levitt and Chothia (1976). Being able to classify a new protein in its proper group can help to elucidate relationships between novel genes and existing proteins and characterize unknown genes.", "startOffset": 145, "endOffset": 190}, {"referenceID": 29, "context": "The seven major SCOP classes are quite distinct and can be easily distinguished (Kumarevel et al., 2000; Jin et al., 2003).", "startOffset": 80, "endOffset": 122}, {"referenceID": 25, "context": "The seven major SCOP classes are quite distinct and can be easily distinguished (Kumarevel et al., 2000; Jin et al., 2003).", "startOffset": 80, "endOffset": 122}, {"referenceID": 23, "context": "ppm-d (Howard, 1993) is very similar to to ppm-c, with a slight modification of the escape mechanism; see Section 3.", "startOffset": 6, "endOffset": 20}, {"referenceID": 17, "context": "Two examples are the Canterbury Corpus and the \u2018Large Canterbury Corpus\u2019 (Arnold & Bell, 1997) and the Silesia Compression Corpus (Deorowicz, 2003), which contains significantly larger files than both the Calgary and Canterbury corpora.", "startOffset": 130, "endOffset": 147}, {"referenceID": 53, "context": "To date, the most successful ppm variant is the \u2018complicated PPM with information-inheritance\u2019 (ppm-ii) of Shkarin (2002), which achieves a compression rate of 2.", "startOffset": 107, "endOffset": 122}, {"referenceID": 6, "context": "26 Bunton (1997) provides a (Calgary Corpus) comparison between ppm-c, ppm-d and ppm\u2217 (with its \u2018C\u2019 and \u2018D\u2019 versions).", "startOffset": 3, "endOffset": 17}, {"referenceID": 6, "context": "According to Bloom27, his latest version of ppm-z achieves an average rate of 2.086. Bunton also proposes various improvements for the ppm schemes. Her best improved ppm achieves a 2.177 rate.28 The general-purpose ppm scheme turns out to be rather flexible and allows adaptations to particular domains. For example, Nevill-Manning and Witten (1999) achieve a slightly better rate than the trivial (log2(20)) one for proteins, using a specialized variant of ppm-d", "startOffset": 13, "endOffset": 350}, {"referenceID": 6, "context": "cbloom.com/src/ppmz.html 28. At the time (1996) this variant was probably the most successful ppm version.", "startOffset": 1, "endOffset": 48}, {"referenceID": 64, "context": "A possible reason is that the original binary ctw (and straightforward extensions to larger alphabets) did not achieve the best lossless compression results (Tjalkens et al., 1997).", "startOffset": 157, "endOffset": 180}, {"referenceID": 60, "context": "Previous studies on protein classification used a variety of methods, including generative models such as HMMs (Sonnhammer et al., 1997), PSTs (Bejerano & Yona, 2001), and discriminative models such as Support Vector Machines (Ding & Dubchak, 2001; Leslie et al.", "startOffset": 111, "endOffset": 136}, {"referenceID": 32, "context": ", 1997), PSTs (Bejerano & Yona, 2001), and discriminative models such as Support Vector Machines (Ding & Dubchak, 2001; Leslie et al., 2004).", "startOffset": 97, "endOffset": 140}, {"referenceID": 16, "context": "By combining domain knowledge into ppm using preprocessing and ppm that predicts over dual-alphabets, Drinic and Kirovski (2002) develop a specialized algorithm for compressing computer executable files (.", "startOffset": 102, "endOffset": 129}, {"referenceID": 8, "context": "A ppm variant for compressing XML files was proposed by Cheney (2001). A text-mining utilization of the ppm algorithm was tested by Witten et al.", "startOffset": 56, "endOffset": 70}, {"referenceID": 8, "context": "A ppm variant for compressing XML files was proposed by Cheney (2001). A text-mining utilization of the ppm algorithm was tested by Witten et al. (1999) and a few text classification setups were studied by Thaper (2001), Teahan and Harper (2001).", "startOffset": 56, "endOffset": 153}, {"referenceID": 8, "context": "A ppm variant for compressing XML files was proposed by Cheney (2001). A text-mining utilization of the ppm algorithm was tested by Witten et al. (1999) and a few text classification setups were studied by Thaper (2001), Teahan and Harper (2001).", "startOffset": 56, "endOffset": 220}, {"referenceID": 8, "context": "A ppm variant for compressing XML files was proposed by Cheney (2001). A text-mining utilization of the ppm algorithm was tested by Witten et al. (1999) and a few text classification setups were studied by Thaper (2001), Teahan and Harper (2001). Context Tree Weighting (ctw), with its proven redundancy bound, has also attracted much attention.", "startOffset": 56, "endOffset": 246}, {"referenceID": 1, "context": "For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model.", "startOffset": 10, "endOffset": 36}, {"referenceID": 1, "context": "For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model. They show that this combination of ppm-d and ctw outperforms ppm-d. This idea was examined by Sadakane et al. (2000) for both text and biological sequences.", "startOffset": 10, "endOffset": 231}, {"referenceID": 1, "context": "For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model. They show that this combination of ppm-d and ctw outperforms ppm-d. This idea was examined by Sadakane et al. (2000) for both text and biological sequences. Their results also support the observation that the combination of a ppm predictor with the ctw model outperforms ppm-d compression both for textual data as well as for compressing DNA. We are familiar with a number of extensions of the pst scheme. Singer (1997) extends the prediction scheme for sequence transduction and also utilizes the ctw model averaging idea.", "startOffset": 10, "endOffset": 534}, {"referenceID": 1, "context": "For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model. They show that this combination of ppm-d and ctw outperforms ppm-d. This idea was examined by Sadakane et al. (2000) for both text and biological sequences. Their results also support the observation that the combination of a ppm predictor with the ctw model outperforms ppm-d compression both for textual data as well as for compressing DNA. We are familiar with a number of extensions of the pst scheme. Singer (1997) extends the prediction scheme for sequence transduction and also utilizes the ctw model averaging idea. Bejerano and Yona (2001) incorporated biological prior knowledge into pst prediction and showed that the resulting pst-based similarity measure can outperform standard methods such as Gapped-BLAST, and is almost as sensitive as a hidden Markov model that is trained from a multiple alignment of the input sequences, while being much faster.", "startOffset": 10, "endOffset": 663}, {"referenceID": 1, "context": "For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model. They show that this combination of ppm-d and ctw outperforms ppm-d. This idea was examined by Sadakane et al. (2000) for both text and biological sequences. Their results also support the observation that the combination of a ppm predictor with the ctw model outperforms ppm-d compression both for textual data as well as for compressing DNA. We are familiar with a number of extensions of the pst scheme. Singer (1997) extends the prediction scheme for sequence transduction and also utilizes the ctw model averaging idea. Bejerano and Yona (2001) incorporated biological prior knowledge into pst prediction and showed that the resulting pst-based similarity measure can outperform standard methods such as Gapped-BLAST, and is almost as sensitive as a hidden Markov model that is trained from a multiple alignment of the input sequences, while being much faster. Finally, Kermorvant and Dupont (2002) propose to improve the smoothing mechanism used in the original pst algorithm.", "startOffset": 10, "endOffset": 1017}, {"referenceID": 1, "context": "For text, Aberg and Shtarkov (1997) proposed to replace the ppm estimator with the KT-estimator in the ctw model. They show that this combination of ppm-d and ctw outperforms ppm-d. This idea was examined by Sadakane et al. (2000) for both text and biological sequences. Their results also support the observation that the combination of a ppm predictor with the ctw model outperforms ppm-d compression both for textual data as well as for compressing DNA. We are familiar with a number of extensions of the pst scheme. Singer (1997) extends the prediction scheme for sequence transduction and also utilizes the ctw model averaging idea. Bejerano and Yona (2001) incorporated biological prior knowledge into pst prediction and showed that the resulting pst-based similarity measure can outperform standard methods such as Gapped-BLAST, and is almost as sensitive as a hidden Markov model that is trained from a multiple alignment of the input sequences, while being much faster. Finally, Kermorvant and Dupont (2002) propose to improve the smoothing mechanism used in the original pst algorithm. Experiments with a protein problem indicate that the proposed smoothing method can help. As mentioned earlier, there are many extensions of the lz78 algorithm. See (Nelson & Gailly, 1996) for more details. It is interesting to note that Volf considered an \u201censemble\u201d consisting of both de-ctw and ppm. He shows that the resulting algorithm performs nearly as well as the ppm-z on the Calgary Corpus while beating the ppm-z on the Canterbury Corpus. Previous studies on protein classification used a variety of methods, including generative models such as HMMs (Sonnhammer et al., 1997), PSTs (Bejerano & Yona, 2001), and discriminative models such as Support Vector Machines (Ding & Dubchak, 2001; Leslie et al., 2004). Jaakkola et al. (1999) describe a model that combines the SVM discriminative model with a generative model (the HMM-based Fischer kernel).", "startOffset": 10, "endOffset": 1839}, {"referenceID": 72, "context": "Other studies used unsupervised learning techniques, such as clustering (Yona et al., 1999; Tatusov et al., 1997) and self-organizing maps (Agrafiotis, 1997).", "startOffset": 72, "endOffset": 113}, {"referenceID": 61, "context": "Other studies used unsupervised learning techniques, such as clustering (Yona et al., 1999; Tatusov et al., 1997) and self-organizing maps (Agrafiotis, 1997).", "startOffset": 72, "endOffset": 113}, {"referenceID": 2, "context": ", 1997) and self-organizing maps (Agrafiotis, 1997).", "startOffset": 33, "endOffset": 51}, {"referenceID": 47, "context": "Of all models, the HMM model is perhaps the most popular and successful model for protein classification to-date (Park et al., 1998).", "startOffset": 113, "endOffset": 132}, {"referenceID": 9, "context": "This is a more difficult task and it has been the subject of strong interest in fold prediction tasks (CASP, 2002).", "startOffset": 102, "endOffset": 114}, {"referenceID": 37, "context": "Other studies that addressed a somewhat similar problem to ours used sequence comparison algorithms (McGuffin et al., 2001) or SVMs and Neural Networks (Ding & Dubchak, 2001) reporting a maximal accuracy of 56%.", "startOffset": 100, "endOffset": 123}, {"referenceID": 66, "context": "One of the most successful general-purpose predictors we examined is the de-ctw algorithm, with its alphabet decomposition mechanism, as suggested by Volf (2002). As it turns out, this mechanism is crucial for the success of the the ctw scheme.", "startOffset": 150, "endOffset": 162}, {"referenceID": 43, "context": "It would be interesting to see if some of the recent results on the missing mass problem, such as those presented by Orlitsky et al. (2003) and McAllester and Ortiz (2003), can help in devising a formal optimality measure for the ppm escape implementations.", "startOffset": 117, "endOffset": 140}, {"referenceID": 34, "context": "(2003) and McAllester and Ortiz (2003), can help in devising a formal optimality measure for the ppm escape implementations.", "startOffset": 11, "endOffset": 39}, {"referenceID": 34, "context": "(2003) and McAllester and Ortiz (2003), can help in devising a formal optimality measure for the ppm escape implementations. We can view the ctw algorithm as an ensemble of numerous VMMs. However, the weights of the various VMM models are fixed. Various ensemble methods in machine learning such as boosting and online expert advice algorithms attempt to optimize the weights of the various ensemble members. Could we achieve better performance guarantees and better empirical performance using adaptive weight learning approaches? Volf (2002) has some results (on combining lz78 and de-ctw) indicating that this is a promising direction.", "startOffset": 11, "endOffset": 544}, {"referenceID": 26, "context": "Other possibilities would be to employ portfolio selection algorithms to dynamically change the weights of several models, along the lines suggested by Kalai et al. (1999).", "startOffset": 152, "endOffset": 172}], "year": 2011, "abstractText": "This paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models. The class of such algorithms is large and in principle includes any lossless compression algorithm. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real life sequences from three domains: proteins, English text and music pieces. The comparison is made with respect to prediction quality as measured by the average log-loss. We also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks. Our results indicate that a \u201cdecomposed\u201d CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in sequence prediction tasks. Somewhat surprisingly, a different algorithm, which is a modification of the Lempel-Ziv compression algorithm, significantly outperforms all algorithms on the protein classification problems.", "creator": "dvips(k) 5.90a Copyright 2002 Radical Eye Software"}}}