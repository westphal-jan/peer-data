{"id": "1107.0053", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "Finding Approximate POMDP solutions Through Belief Compression", "abstract": "Standard value function approaches to finding policies for Partially Observable Markov Decision Processes (POMDPs) are generally considered to be intractable for large models. The intractability of these algorithms is to a large extent a consequence of computing an exact, optimal policy over the entire belief space. However, in real-world POMDP problems, computing the optimal policy for the full belief space is often unnecessary for good control even for problems with complicated policy classes. The beliefs experienced by the controller often lie near a structured, low-dimensional subspace embedded in the high-dimensional belief space. Finding a good approximation to the optimal value function for only this subspace can be much easier than computing the full value function. We introduce a new method for solving large-scale POMDPs by reducing the dimensionality of the belief space. We use Exponential family Principal Components Analysis (Collins, Dasgupta and Schapire, 2002) to represent sparse, high-dimensional belief spaces using small sets of learned features of the belief state. We then plan only in terms of the low-dimensional belief features. By planning in this low-dimensional space, we can find policies for POMDP models that are orders of magnitude larger than models that can be handled by conventional techniques. We demonstrate the use of this algorithm on a synthetic problem and on mobile robot navigation tasks.", "histories": [["v1", "Thu, 30 Jun 2011 20:44:33 GMT  (952kb)", "https://arxiv.org/abs/1107.0053v1", null], ["v2", "Tue, 4 Oct 2011 15:16:13 GMT  (952kb)", "http://arxiv.org/abs/1107.0053v2", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["n roy", "g gordon", "s thrun"], "accepted": false, "id": "1107.0053"}, "pdf": {"name": "1107.0053.pdf", "metadata": {"source": "CRF", "title": "Finding Approximate POMDP Solutions Through Belief Compression", "authors": ["Nicholas Roy", "Geoffrey Gordon", "Sebastian Thrun"], "emails": ["nickroy@mit.edu", "ggordon@cs.cmu.edu", "thrun@stanford.edu"], "sections": [{"heading": null, "text": "We introduce a new method for solving large-scale POMDPs by reducing the dimensionality of the space of faith. We use the Exponential Family Principal Components Analysis (Collins, Dasgupta, & Schapire, 2002) to represent sparse, high-dimensional spaces of faith using small groups of learned features of the state of faith. We then plan only in terms of low-dimensional faith characteristics. By planning in this low-dimensional space we can find guidelines for POMDP models that are orders of magnitude larger than models that can be handled using conventional techniques. We demonstrate the application of this algorithm to a synthetic problem and to mobile robot navigation tasks."}, {"heading": "1. Introduction", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "2. Partially Observable Markov Decision Processes", "text": "A partially observable Markov decision-making process (POMDP) is a model for deciding how to act in \"an accessible, stochastic environment with a known transition model\" (Russell and Norvig (1995), pg. 500). A POMDP is described as: \u2022 a set of states S = {s1, s2,. \u2022 a set of actions A = {a1, a2,. \u2022 a set of beliefs Z = {z1, z2, s2 \u2022 a set of transition chances T (si, sj) = p (sj) = p (sj, a) a set of observation probabilities O (zi, sj) = p (zi | sj, a) \u2022 a set of rewards R: S \u00d7 A 7 \u2192 R \u2022 a discount factor P0 (0, 1] \u2022 an initial belief p0 (s)."}, {"heading": "3. Dimensionality Reduction", "text": "In order to find a low-dimensional representation of our beliefs, we will use statistical dimensionality reduction algorithms (Cox & Cox, 1994), which look for a projection from our original high-dimensional representation of our beliefs to a low-dimensional compact representation, i.e., they look for a surface embedded in the high-dimensional belief space that runs close to all beliefs of the sample. If we look at the evolution of beliefs from a POMDP as a path within the belief space of the three-dimensional POMDP, then our assumption is that the paths for most large, real POMDPs are close to a low-dimensional surface embedded in the belief space. Figure 6 shows an example of a low-dimensional surface embedded in the belief space of the three-dimensional POMDP. Ideally, the dimensional reduction is not associated with any loss of information - all aspects of the data can be recovered equally well from the low-dimensional representation as from the high-dimensional representation."}, {"heading": "Principal Components Analysis", "text": "One of the most common forms of dimensionality reduction is the Principal Components Analysis (Joliffe, 1986). In the face of a dataset, PCA finds the linear low-dimensional representation of the data in such a way that the variance of the reconstructed data is preserved. Intuitively, PCA finds a low-dimensional hyperplane that when we project our data on the hyperplane, the variance of our data is changed as little as possible. A transformation that preserves the variance appears attractive because it maximizes our ability to distinguish between beliefs that are far apart in the Euclidean norm. As we will see below, the Euclidean norm is not the most appropriate way to measure the distance between beliefs when our goal is to maintain the ability to choose good activities.We first assume that we have a dataset of n beliefs that are far apart in the Euclidean norm."}, {"heading": "PCA Performance", "text": "The abstract model has a two-dimensional state space: a dimension of positioning along one of two circular corridors and a binary variable that essentially determines which corridor we are in. Therefore, the agent must discover his corridor in order to identify his corridors. Many descriptions of the PCA are based on factoring the distribution of UPS T, with U and V column orthonormally. We could force a similar constraint by identifying B = V S; in this case, the columns of U would have to be orthonormally, while those of B column would have to be orthonormally."}, {"heading": "4. Exponential Family PCA", "text": "The conventional view of PCA is a geometric model as \"exponential family PCA\" or \"PCA-E.\" The conventional view of PCA is a geometric model that has a low dimensional projection that minimizes the quadratic error loss. (B) The conventional view of PCA is a probabilistic model: When the data is generated by a probability distribution, PCA is an algorithm to find the parameters of the generative distribution that maximizes the probability of data loss. (B) The quadrilateral error functions of PCA can be generalized by showing the different exponential families of the probability distribution such as Gaussian, binomial, or Poisson. (2002) The exponential family distribution corresponds to another loss function of PCA, and Collins et al. (2002) refer to the generalization of PCA to arbitrary family models."}, {"heading": "Finding the E-PCA Parameters", "text": "As a rule, we do not have this property: the loss function (8) can have several different local minimums. \u2212 So the problem of searching for the best B and U is that the possible local minima in the common space of U and B are highly constrained, and the search for U and B does not require global solutions. \u2212 The problem of searching for the best B and B is that the possible local minima in the common space of U and B are highly constrained, and the search for U and B does not require general non-convex optimizations. \u2212 The problem of searching for the best B and B type is that the possible local minima in the common space of U and B are highly constrained. \u2212 The problem of searching for U and B type is that it requires a general non-convex-based optimization. \u2212 The problem of Gordon (2003) describes a fast Newton's method for U and B type that we summarize here."}, {"heading": "The E-PCA Algorithm", "text": "We now have an algorithm to automatically find a good low-dimensional representation, such as a finite number of iterations, or when a minimum error quotient is reached. Steps 7 and 9 pose a problem. Although the solution for each U-row or B-column separately is a convex optimization problem, the solution for the two matrices is not possible at the same time. Therefore, we are subject to potential local minimums; in our experiments, we did not find this a problem, but we expect to find ways to address the local minimum problem in order to scale to even more complicated range.Once bases U are found, finding the low-dimensional representation of a high-dimensional representation of a high-dimensional PCB definition is not a problem; we can calculate the best answer by iterating the equation (26)."}, {"heading": "5. E-PCA Performance", "text": "Using the loss function from Equation (8) with the iterative optimization method described in Equation (26) and Equation (27) to determine the low-dimensional factorization, we can see how well this dimensionality reduction method performs in some POMDP examples."}, {"heading": "Toy Problem", "text": "Let us remember from Figure 9 that we could not find good representations of the data with less than 10 or 15 bases, even though our domain knowledge indicated that the data had 3 degrees of freedom (horizontal position of the mode along the corridor, concentration on the mode and probability of being in the upper or lower corridor). In examining one of the beliefs in Figure 10, we saw that the representation was worst in the low probability regions. We can now take the same dataset from the toy example, use E-PCA to find a low-dimensional representation and compare the performance of PCA and E-PCA. Figure 11 (a) shows that E-PCA is much more efficient in displaying the data, since we see that the KL divergence falls very close to 0 after 4 bases. Additionally, the square L2 error at 4 bases is 4.64 x 10 \u2212 4. (We need 4 bases for a perfect reconstruction, we must include a constant function."}, {"heading": "Robot Beliefs", "text": "Although the performance of E-PCA in finding good representations of the abstract problem is convincing, ideally we would like to be able to apply this algorithm to real problems, such as the robot navigation problem in Figure 2. Figures 12 and 13 show results from two such robot navigation problems performed using a physico-realistic simulation (although with artificially limited sensors and dead calculations).We collected an example set of 500 beliefs by moving the robot through the environment with a heuristic controller, and calculated the low-dimensional belief space B according to the algorithm in Table 1. The complete state space is 47.7 m x 17 m, discredited to a resolution of 1 m x 1 m per pixel, for a total of 799 states. Figure 12 (a) shows an example belief, and Figure 12 (b) the reconstruction with 5 bases that PCs are not sufficient."}, {"heading": "6. Computing POMDP policies", "text": "The exponential Family Principal Components Analysis model gives us a way to find a low-dimensional representation of the beliefs that occur in each particular problem. For the two real navigation problems we have tried, the algorithm has proven effective in finding very low-dimensional representations that have reductions from 2600 states and 26000 states down to 5 or 6 foundations. A 5 or 6-dimensional belief space will allow a much more tractable calculation of the value function, and so we will be able to solve much larger POMDPs than we could have solved previously. Unfortunately, we can no longer use conventional POMDP value iteration to find the optimal policy given the low-dimensional properties of the belief space. POMDP value iteration depends on the fact that the value function across the faith space is convex. Whenwe calculate a non-linear transformation of our beliefs to restore their value surface to the low-dimensional coordinate of the expression of the value."}, {"heading": "Computing the Reward Function", "text": "The original reward function R (s, a) represents the immediate reward of action a at state s. Therefore, according to current belief, we cannot know what the immediate reward will be, but we can calculate the expected reward. Therefore, we present the reward as the expected value of the immediate reward of the complete model. (32) Equation (32) requires us to recover the high-dimensional belief b from the low-dimensional representation b (s, a) (31) = | S | \u2211 i = 1R (si, a) b (si), as shown in Equation (28). In many problems, the reward function R (2) will have the effect of providing a low immediate reward for beliefs with high entropy. That is, for many problems, the planner is driven to beliefs that focus on highly rewarded states and exhibit low uncertainty. This characteristic is intuitively desirable: in such beliefs, the robot does not have to worry about the immediate result."}, {"heading": "Computing the Transition Function", "text": "The calculation of the low-dimensional transition function T (b) is not as simple as the calculation of the low-dimensional reward function R (B): We must consider pairs of low-dimensional beliefs, b) and b). In the original high-dimensional belief space, the transition from a previous belief to a rear-dimensional belief is B (33) Here is an action we have selected and z) is the observation we saw; T is the original POMDP transition process distribution, and O is the original POMDP probability distribution. Equation (33) describes a deterministic transition, an action, and an observation."}, {"heading": "Computing the Value Function", "text": "With the reward and transition functions calculated in the previous sections, we can use the value variation to calculate the value function for our faith space MDP. Full algorithm is given in Table 2."}, {"heading": "7. Solving Large POMDPs", "text": "In this section we present the use of our algorithm to find guidelines for large POMDPs."}, {"heading": "Toy problem", "text": "To ensure that we only needed a small number of beliefs, we enlarged the target region. We also used a coarser discretization of the underlying state space (40 states instead of 200 states) to calculate the low-dimensional model more quickly. Figure 15 shows a comparison of the strategies from the different algorithms. The advanced MDP algorithm reported by Roy and Thrun (1999) works about twice as well as the heuristic maximum likelihood method; this heuristic method estimates its corridor and is correct only about half the time. This \"AMDP heuristics\" algorithm is the advanced MDP algorithm reported by Roy and Thrun (1999). This controller tries to find the policies that lead to the lowest entropy belief to achieve the goal."}, {"heading": "Robot Navigation", "text": "We feel that we are able to put the world in order, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we will be able to put the world in order. \"He added:\" I don't think we will be able to put the world in order. \"He added:\" I don't think we will be able to put the world in order. \"He added:\" I don't think we are able to put the world in order. \""}, {"heading": "Finding People", "text": "In fact, most of them are able to survive on their own, without being able to survive on their own."}, {"heading": "8. Discussion", "text": "These experiments show that the E-PCA algorithm can scale to find low-dimensional surfaces embedded in very high-dimensional spaces."}, {"heading": "Time Complexity", "text": "The algorithm is iterative and therefore no simple printout is available for the entire runtime. For a dataset of | B | examples of dimensionality n that calculates a surface of size l, each iteration of the algorithm is O (| B | nl2 + | B | l3 + nl3). Each step of the Newton algorithm is multiplied by a set of matrix and the last step of inverting an l \u00b7 l matrix that is O (| B | l). The U step consists of | B | iterations in which each iteration is multiplied by O (nl) and the O (l3) inversion. The V step consists of n iterations in which each iteration has O (| B | l) problems and the O (l3) inversion, which leads to the entire complexity, is given above. Figure 23 shows the time to calculate the PCA basics for sample 500, and the O (l3) inversion, and the O (l3) inversion, which leads to the complete complexity, the above."}, {"heading": "Sample Belief Collection", "text": "For all the example problems we have addressed, we have used a standard sample size of 500 beliefs. In addition, we have used hand-coded heuristic controllers to sample beliefs from the model. In practice, we found 500 beliefs collected with a semi-random controller sufficient for our sample problems. However, we may be able to improve the overall performance of our algorithm in future problems by iterating between phases of building the belief space representation (i.e., gathering beliefs and generating the low-dimensional representation) and calculating a good controller. Once an initial set of beliefs have been collected and used to build a first set of foundations and a corresponding policy, we can continue to evaluate the error of representation (e.g. the K-L divergence between the current belief and its low-dimensional representation). If the original representation with too few beliefs has been learned, then we may find the job matches this representation by finding the bad representation over the representation; that we may be satisfied with the job."}, {"heading": "Model Selection", "text": "One of the open questions we have not addressed so far is the selection of the appropriate number of bases for our representation. Unless we have problem-specific information, such as the true number of degrees of freedom in the faith space (as in the toy example of Section 3), it is difficult to identify the appropriate dimensionality of the underlying surface for inspection. A common approach is to examine the eigenvalues of decomposition, which can be recovered using the orthonormalization step of the algorithm in Table 1. (This assumes that our special linking function is able to express the surface on which our data is based.) The eigenvalues from conventional PCA are often used to determine the appropriate dimensionality of the underlying surface; certainly the reconstruction will be loss-free if we use as many bases as there are not zero eigenvalues. Unfortunately, we remember the description of E-PCA in Section 4 that we do not have any singular set of eigenvalues or eigenvalues."}, {"heading": "9. Related Work", "text": "Many attempts to apply accessibility analyses to limit the number of convictions for planning (Washington, 1997; Hauskrecht, 2000; Zhou & Hansen, 2001; Pineau, Gordon, & Thrun, 2003a). If the achievable set of convictions is relatively small, then the search for this set is a perfectly reasonable approach. Of course, the policy calculated beyond these convictions is optimal, although it is relatively rare in real world problems to be able to list the achievable convictions. Reachability analysis has also been used with some success as a guide to search methods, especially for focusing on functional approximators (Washington, 1997; Hansen, 1998). In this approach, the problem remains of how to compile low-dimensional representation in the face of the limited set of representative convictions. Discretization of the space of faith itself has been explored a number of times, both regular network-based discretification (Lovejoy, 2001, Zhou, resolution) and non-representative (Hansen)."}, {"heading": "10. Conclusion", "text": "In fact, most of us are able to move to another world, in which we are able, in which we are able, in which we are able, in which we are able to move, and in which we are able, in which we are able to assert ourselves."}, {"heading": "Acknowledgements", "text": "Thanks to Tom Mitchell, Leslie Kaelbling, Reid Simmons, Drew Bagnell, Aaron Courville, Mike Montemerlo, and Joelle Pineau for useful comments and insights into this work. Nicholas Roy was funded by the National Science Foundation under ITR Grant # IIS-0121426. Geoffrey Gordon was funded by AFRL Contract F30602-01-C-0219, DARPA's MICA Program, and AFRL Contract F30602-98-2-0137, DARPA's CoABS Program."}], "references": [{"title": "Autonomous helicopter control using reinforcement learning policy search methods", "author": ["J.A. Bagnell", "J. Schneider"], "venue": "In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Bagnell and Schneider,? \\Q2001\\E", "shortCiteRegEx": "Bagnell and Schneider", "year": 2001}, {"title": "GTM: the generative topographic mapping", "author": ["C. Bishop", "M. Svens\u00e9n", "C. Williams"], "venue": "Neural Computation,", "citeRegEx": "Bishop et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bishop et al\\.", "year": 1998}, {"title": "Computing optimal policies for partially observable Markov decision processes using compact representations", "author": ["C. Boutilier", "D. Poole"], "venue": "In Proceedings of the 13th National Conference on Artificial Intelligence", "citeRegEx": "Boutilier and Poole,? \\Q1996\\E", "shortCiteRegEx": "Boutilier and Poole", "year": 1996}, {"title": "Tractable inference for complex stochastic processes", "author": ["X. Boyen", "D. Koller"], "venue": "In Proceedings of the 14th Annual Conference on Uncertainty in AI (UAI),", "citeRegEx": "Boyen and Koller,? \\Q1998\\E", "shortCiteRegEx": "Boyen and Koller", "year": 1998}, {"title": "A heuristic variable grid solution method for POMDPs", "author": ["R.I. Brafman"], "venue": "Kuipers, B. K., & Webber, B. (Eds.), Proceedings of the 14th National Conference on Artificial Intelligence (AAAI), pp. 727\u2013733, Providence, RI.", "citeRegEx": "Brafman,? 1997", "shortCiteRegEx": "Brafman", "year": 1997}, {"title": "Acting under uncertainty: Discrete Bayesian models for mobile-robot navigation", "author": ["A.R. Cassandra", "L. Kaelbling", "J.A. Kurien"], "venue": "In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems", "citeRegEx": "Cassandra et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1996}, {"title": "Robust and H-\u221e Control", "author": ["B.M. Chen"], "venue": "Springer-Verlag.", "citeRegEx": "Chen,? 2000", "shortCiteRegEx": "Chen", "year": 2000}, {"title": "Algorithms for Partially Observable Markov Decision Processes", "author": ["Cheng", "H.-T."], "venue": "Ph.D. thesis, University of British Columbia, Vancouver, Canada.", "citeRegEx": "Cheng and H..T.,? 1988", "shortCiteRegEx": "Cheng and H..T.", "year": 1988}, {"title": "A generalization of principal components analysis to the exponential family", "author": ["M. Collins", "S. Dasgupta", "R. Schapire"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Collins et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Collins et al\\.", "year": 2002}, {"title": "Multidimensional Scaling", "author": ["T. Cox", "M. Cox"], "venue": null, "citeRegEx": "Cox and Cox,? \\Q1994\\E", "shortCiteRegEx": "Cox and Cox", "year": 1994}, {"title": "Markov localization for mobile robots in dynamic environments", "author": ["D. Fox", "W. Burgard", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Fox et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Fox et al\\.", "year": 1999}, {"title": "Singular value decomposition and least squares solutions", "author": ["G. Golub", "C. Reinsch"], "venue": "Numerische Mathematik,", "citeRegEx": "Golub and Reinsch,? \\Q1970\\E", "shortCiteRegEx": "Golub and Reinsch", "year": 1970}, {"title": "Stable function approximation in dynamic programming", "author": ["G. Gordon"], "venue": "Prieditis, A., & Russell, S. (Eds.), Proceedings of the 12 International Conference on Machine Learning (ICML), pp. 261\u2013268, San Francisco, CA. Morgan Kaufmann. Gordon, G. (2003). Generalized2 linear2 models. In Becker, S., Thrun, S., & Obermayer, K. (Eds.), Advances in Neural Information Processing Systems 15 (NIPS). MIT Press.", "citeRegEx": "Gordon,? 1995", "shortCiteRegEx": "Gordon", "year": 1995}, {"title": "An experimental comparison of localization methods", "author": ["Gutmann", "J.-S", "W. Burgard", "D. Fox", "K. Konolige"], "venue": "In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Gutmann et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 1998}, {"title": "An experimental comparison of localization methods continued", "author": ["Gutmann", "J.-S", "D. Fox"], "venue": "In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "Gutmann et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gutmann et al\\.", "year": 2002}, {"title": "Dynamic programming for POMDPs using a factored state representation", "author": ["E. Hansen", "Z. Feng"], "venue": "In Proceedings of the Fifth International Conference on Artificial Intelligence Planning and Scheduling (AIPS-00),", "citeRegEx": "Hansen and Feng,? \\Q2000\\E", "shortCiteRegEx": "Hansen and Feng", "year": 2000}, {"title": "Solving POMDPs by searching in policy space", "author": ["E. Hansen"], "venue": "Proceedings of the 14th Conference on Uncertainty in Artifical Intelligence (UAI), pp. 211\u2013219, Madison, WI.", "citeRegEx": "Hansen,? 1998", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Value-function approximations for partially observable Markov decision processes", "author": ["M. Hauskrecht"], "venue": "Journal of Artificial Intelligence Research, 13, 33\u201394.", "citeRegEx": "Hauskrecht,? 2000", "shortCiteRegEx": "Hauskrecht", "year": 2000}, {"title": "Stochastic neighbor embedding", "author": ["G. Hinton", "S. Roweis"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Hinton and Roweis,? \\Q2003\\E", "shortCiteRegEx": "Hinton and Roweis", "year": 2003}, {"title": "Dynamic Programming and Markov Processes", "author": ["R.A. Howard"], "venue": "MIT.", "citeRegEx": "Howard,? 1960", "shortCiteRegEx": "Howard", "year": 1960}, {"title": "CONDENSATION \u2013 conditional density propagation for visual tracking", "author": ["M. Isard", "A. Blake"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Isard and Blake,? \\Q1998\\E", "shortCiteRegEx": "Isard and Blake", "year": 1998}, {"title": "Principal Component Analysis", "author": ["I.T. Joliffe"], "venue": "Springer-Verlag.", "citeRegEx": "Joliffe,? 1986", "shortCiteRegEx": "Joliffe", "year": 1986}, {"title": "Stochastic simulation algorithms for dynamic probabilistic networks", "author": ["K. Kanazawa", "D. Koller", "S. Russell"], "venue": "In Proceedings of the 11th Annual Conference on Uncertainty in AI (UAI),", "citeRegEx": "Kanazawa et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kanazawa et al\\.", "year": 1995}, {"title": "Self-organized formation of topologically correct feature maps", "author": ["T. Kohonen"], "venue": "Biological Cybernetics, 48, 59\u201369.", "citeRegEx": "Kohonen,? 1982", "shortCiteRegEx": "Kohonen", "year": 1982}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": null, "citeRegEx": "Lee and Seung,? \\Q1999\\E", "shortCiteRegEx": "Lee and Seung", "year": 1999}, {"title": "Mobile robot localization by tracking geometric beacons", "author": ["J. Leonard", "H. Durrant-Whyte"], "venue": "IEEE Transactions on Robotics and Automation,", "citeRegEx": "Leonard and Durrant.Whyte,? \\Q1991\\E", "shortCiteRegEx": "Leonard and Durrant.Whyte", "year": 1991}, {"title": "Computationally feasible bounds for partially observable Markov decison processes", "author": ["W.S. Lovejoy"], "venue": "Operations Research, 39, 192\u2013175.", "citeRegEx": "Lovejoy,? 1991", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "Directional Statistics (2nd edition)", "author": ["K.V. Mardia", "P.E. Jupp"], "venue": null, "citeRegEx": "Mardia and Jupp,? \\Q2000\\E", "shortCiteRegEx": "Mardia and Jupp", "year": 2000}, {"title": "Generalized Linear Models (2nd edition)", "author": ["P. McCullagh", "J.A. Nelder"], "venue": null, "citeRegEx": "McCullagh and Nelder,? \\Q1983\\E", "shortCiteRegEx": "McCullagh and Nelder", "year": 1983}, {"title": "Learning finite-state controllers for partially observable environments", "author": ["N. Meuleau", "L. Peshkin", "Kim", "K.-E", "L.P. Kaelbling"], "venue": "Proceedings of the Fifteenth International Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Meuleau et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 1999}, {"title": "Variable resolution discretization for high-accuracy solutions of optimal control problems", "author": ["R. Munos", "A. Moore"], "venue": "Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Munos and Moore,? \\Q1999\\E", "shortCiteRegEx": "Munos and Moore", "year": 1999}, {"title": "Variable resolution discretization in optimal control", "author": ["R. Munos", "A. Moore"], "venue": "Machine Learning,", "citeRegEx": "Munos and Moore,? \\Q2002\\E", "shortCiteRegEx": "Munos and Moore", "year": 2002}, {"title": "DERVISH an office-navigating robot", "author": ["I. Nourbakhsh", "R. Powers", "S. Birchfield"], "venue": "AI Magazine,", "citeRegEx": "Nourbakhsh et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Nourbakhsh et al\\.", "year": 1995}, {"title": "Probabilistic self-localization for mobile robots", "author": ["C.F. Olson"], "venue": "IEEE Transactions on Robotics and Automation, 16 (1), 55\u201366.", "citeRegEx": "Olson,? 2000", "shortCiteRegEx": "Olson", "year": 2000}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "Policy-contingent abstraction for robust robot control", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "Proceedings of the 19th Annual Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "Value-directed compression of POMDPs", "author": ["P. Poupart", "C. Boutilier"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Poupart and Boutilier,? \\Q2002\\E", "shortCiteRegEx": "Poupart and Boutilier", "year": 2002}, {"title": "Convex Analysis", "author": ["R.T. Rockafellar"], "venue": "Princeton University Press, New Jersey.", "citeRegEx": "Rockafellar,? 1970", "shortCiteRegEx": "Rockafellar", "year": 1970}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S. Roweis", "L. Saul"], "venue": "Science,", "citeRegEx": "Roweis and Saul,? \\Q2000\\E", "shortCiteRegEx": "Roweis and Saul", "year": 2000}, {"title": "Global coordination of local linear models", "author": ["S.T. Roweis", "L.K. Saul", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Roweis et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Roweis et al\\.", "year": 2002}, {"title": "Coastal navigation with mobile robots", "author": ["N. Roy", "S. Thrun"], "venue": "Advances in Neural Processing Systems", "citeRegEx": "Roy and Thrun,? \\Q1999\\E", "shortCiteRegEx": "Roy and Thrun", "year": 1999}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig,? \\Q1995\\E", "shortCiteRegEx": "Russell and Norvig", "year": 1995}, {"title": "Learning geometrically-constrained hidden markov models for robot navigation: Bridging the geometrical-topological gap", "author": ["H. Shatkay", "L.P. Kaelbling"], "venue": "Journal of AI Research", "citeRegEx": "Shatkay and Kaelbling,? \\Q2002\\E", "shortCiteRegEx": "Shatkay and Kaelbling", "year": 2002}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford"], "venue": "Science,", "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Robust Monte Carlo localization for mobile robots", "author": ["S. Thrun", "D. Fox", "W. Burgard", "F. Dellaert"], "venue": "Artificial Intelligence,", "citeRegEx": "Thrun et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Thrun et al\\.", "year": 2000}, {"title": "BI-POMDP: Bounded, incremental partially-observable Markovmodel planning", "author": ["R. Washington"], "venue": "Proceedings of the 4th European Conference on Planning (ECP).", "citeRegEx": "Washington,? 1997", "shortCiteRegEx": "Washington", "year": 1997}, {"title": "Speeding up the convergence of value iteration in partially observable Markov decision processes", "author": ["N.L. Zhang", "W. Zhang"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Zhang and Zhang,? \\Q2001\\E", "shortCiteRegEx": "Zhang and Zhang", "year": 2001}, {"title": "An improved grid-based approximation algorithm for POMDPs", "author": ["R. Zhou", "E. Hansen"], "venue": "Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Zhou and Hansen,? \\Q2001\\E", "shortCiteRegEx": "Zhou and Hansen", "year": 2001}], "referenceMentions": [{"referenceID": 6, "context": "Some planners compensate for the inevitable estimation errors through robust control (Chen, 2000; Bagnell & Schneider, 2001), but few deployed systems incorporate a full probabilistic state estimate into planning.", "startOffset": 85, "endOffset": 124}, {"referenceID": 17, "context": "plans for POMDPs typically cannot handle problems with more than a few hundred states (Hauskrecht, 2000; Zhang & Zhang, 2001).", "startOffset": 86, "endOffset": 125}, {"referenceID": 19, "context": "An optimal policy is known to always exist in the discounted (\u03b3 < 1) case with bounded immediate reward (Howard, 1960).", "startOffset": 104, "endOffset": 118}, {"referenceID": 40, "context": "Partially Observable Markov Decision Processes A partially observable Markov decision process (POMDP) is a model for deciding how to act in \u201can accessible, stochastic environment with a known transition model\u201d (Russell and Norvig (1995), pg.", "startOffset": 211, "endOffset": 237}, {"referenceID": 8, "context": "We can measure the quality of our representation by penalizing reconstruction errors with a loss function (Collins et al., 2002).", "startOffset": 106, "endOffset": 128}, {"referenceID": 21, "context": "Principal Components Analysis One of the most common forms of dimensionality reduction is Principal Components Analysis (Joliffe, 1986).", "startOffset": 120, "endOffset": 135}, {"referenceID": 8, "context": "We can find the parameters of an E-PCA model by maximizing the log-likelihood of the data under the model, which has been shown (Collins et al., 2002) to be equivalent to minimizing a generalized Bregman divergence BF \u2217(b \u2016Ub\u0303) = F (Ub\u0303)\u2212 b \u00b7 Ub\u0303+ F (b) (5) between the low-dimensional and high-dimensional representations, which we can solve using convex optimization techniques.", "startOffset": 128, "endOffset": 150}, {"referenceID": 8, "context": "Collins et al. (2002) demonstrated that PCA can be generalized to a range of loss functions by modeling the data with different exponential families of probability distributions such as Gaussian, binomial, or Poisson.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "Collins et al. (2002) demonstrated that PCA can be generalized to a range of loss functions by modeling the data with different exponential families of probability distributions such as Gaussian, binomial, or Poisson. Each such exponential family distribution corresponds to a different loss function for a variant of PCA, and Collins et al. (2002) refer to the generalization of PCA to arbitrary exponential family data-likelihood models as \u201cExponential family PCA\u201d or E-PCA.", "startOffset": 0, "endOffset": 349}, {"referenceID": 23, "context": "E-PCA is related to Lee and Seung\u2019s (1999) non-negative matrix factorization.", "startOffset": 20, "endOffset": 43}, {"referenceID": 23, "context": "E-PCA is related to Lee and Seung\u2019s (1999) non-negative matrix factorization. One of the NMF loss functions presented by Lee and Seung (1999) penalizes the KL-divergence between a matrix and its reconstruction, as we do in equation 8; but, the NMF loss does not incorporate a link function and so is not an E-PCA loss.", "startOffset": 20, "endOffset": 142}, {"referenceID": 23, "context": "E-PCA is related to Lee and Seung\u2019s (1999) non-negative matrix factorization. One of the NMF loss functions presented by Lee and Seung (1999) penalizes the KL-divergence between a matrix and its reconstruction, as we do in equation 8; but, the NMF loss does not incorporate a link function and so is not an E-PCA loss. Another NMF loss function presented by Lee and Seung (1999) penalizes squared error but constrains the factors to be nonnegative; the resulting model is an example of a (GL)M, a generalization of E-PCA described by Gordon (2003).", "startOffset": 20, "endOffset": 379}, {"referenceID": 12, "context": "Another NMF loss function presented by Lee and Seung (1999) penalizes squared error but constrains the factors to be nonnegative; the resulting model is an example of a (GL)M, a generalization of E-PCA described by Gordon (2003).", "startOffset": 215, "endOffset": 229}, {"referenceID": 37, "context": "However, the problem of finding the best B\u0303 given B and U is convex; convex optimization problems are well studied and have unique global solutions (Rockafellar, 1970).", "startOffset": 148, "endOffset": 167}, {"referenceID": 12, "context": "Gordon (2003) describes a fast, Newton\u2019s Method approach for computing U and B\u0303 which we summarize here.", "startOffset": 0, "endOffset": 14}, {"referenceID": 12, "context": "Table 1: The E-PCA Algorithm for finding a low-dimensional representation of a POMDP, including Gordon\u2019s Newton\u2019s method (2003).", "startOffset": 96, "endOffset": 128}, {"referenceID": 12, "context": "Gordon (1995) proved that the fitted value iteration algorithm is guaranteed to find a bounded-error approximation to a (possibly discounted) MDP\u2019s value function, so long as we use it in combination with a function approximator that is an averager.", "startOffset": 0, "endOffset": 14}, {"referenceID": 12, "context": "Gordon (1995) proved that the fitted value iteration algorithm is guaranteed to find a bounded-error approximation to a (possibly discounted) MDP\u2019s value function, so long as we use it in combination with a function approximator that is an averager. Averagers are function approximators which are non-expansions in max-norm; that is, they do not exaggerate errors in their training data. In our experiments below, we use regular grids as well as irregular, variable-resolution grids based on 1-nearest-neighbour discretization, represented by a set of low-dimensional beliefs B\u0303\u2217, B\u0303 = {b\u03031, b\u0303 \u2217 2, . . . , b\u0303 \u2217 |B\u0303| }. (29) Both of these approximations are averagers; other averagers include linear interpolation, knearest-neighbours, and local weighted averaging. We will not focus in detail on the exact mechanism for discretizing the low-dimensional space, as this is outside the scope of this paper. The resolution of the regular grid in all cases was chosen empirically; in section 7 we describe a specific variable resolution discretization scheme that worked well empirically. The reader can consult Munos and Moore (2002) or Zhou and Hansen (2001) for more sophisticated representations.", "startOffset": 0, "endOffset": 1132}, {"referenceID": 12, "context": "Gordon (1995) proved that the fitted value iteration algorithm is guaranteed to find a bounded-error approximation to a (possibly discounted) MDP\u2019s value function, so long as we use it in combination with a function approximator that is an averager. Averagers are function approximators which are non-expansions in max-norm; that is, they do not exaggerate errors in their training data. In our experiments below, we use regular grids as well as irregular, variable-resolution grids based on 1-nearest-neighbour discretization, represented by a set of low-dimensional beliefs B\u0303\u2217, B\u0303 = {b\u03031, b\u0303 \u2217 2, . . . , b\u0303 \u2217 |B\u0303| }. (29) Both of these approximations are averagers; other averagers include linear interpolation, knearest-neighbours, and local weighted averaging. We will not focus in detail on the exact mechanism for discretizing the low-dimensional space, as this is outside the scope of this paper. The resolution of the regular grid in all cases was chosen empirically; in section 7 we describe a specific variable resolution discretization scheme that worked well empirically. The reader can consult Munos and Moore (2002) or Zhou and Hansen (2001) for more sophisticated representations.", "startOffset": 0, "endOffset": 1158}, {"referenceID": 40, "context": "The \u201cAMDP Heuristic\u201d algorithm is the Augmented MDP algorithm reported by Roy and Thrun (1999). This controller attempts", "startOffset": 74, "endOffset": 95}, {"referenceID": 17, "context": ", using k-nearest-neighbour interpolations such as described by Hauskrecht (2000). These experiments are beyond the scope of this paper, as our focus is on the utility of the E-PCA decomposition.", "startOffset": 64, "endOffset": 82}, {"referenceID": 45, "context": "Many attempts have been made to use reachability analysis to constrain the set of beliefs for planning (Washington, 1997; Hauskrecht, 2000; Zhou & Hansen, 2001; Pineau, Gordon, & Thrun, 2003a).", "startOffset": 103, "endOffset": 192}, {"referenceID": 17, "context": "Many attempts have been made to use reachability analysis to constrain the set of beliefs for planning (Washington, 1997; Hauskrecht, 2000; Zhou & Hansen, 2001; Pineau, Gordon, & Thrun, 2003a).", "startOffset": 103, "endOffset": 192}, {"referenceID": 45, "context": "Reachability analysis has also been used with some success as a heuristic in guiding search methods, especially for focusing computation on finding function approximators (Washington, 1997; Hansen, 1998).", "startOffset": 171, "endOffset": 203}, {"referenceID": 16, "context": "Reachability analysis has also been used with some success as a heuristic in guiding search methods, especially for focusing computation on finding function approximators (Washington, 1997; Hansen, 1998).", "startOffset": 171, "endOffset": 203}, {"referenceID": 26, "context": "Discretization of the belief space itself has been explored a number of times, both regular grid-based discretization (Lovejoy, 1991), regular variable resolution approaches (Zhou & Hansen, 2001) and non-regular variable resolution representations (Brafman, 1997; Hauskrecht, 2000).", "startOffset": 118, "endOffset": 133}, {"referenceID": 4, "context": "Discretization of the belief space itself has been explored a number of times, both regular grid-based discretization (Lovejoy, 1991), regular variable resolution approaches (Zhou & Hansen, 2001) and non-regular variable resolution representations (Brafman, 1997; Hauskrecht, 2000).", "startOffset": 248, "endOffset": 281}, {"referenceID": 17, "context": "Discretization of the belief space itself has been explored a number of times, both regular grid-based discretization (Lovejoy, 1991), regular variable resolution approaches (Zhou & Hansen, 2001) and non-regular variable resolution representations (Brafman, 1997; Hauskrecht, 2000).", "startOffset": 248, "endOffset": 281}, {"referenceID": 4, "context": "Discretization of the belief space itself has been explored a number of times, both regular grid-based discretization (Lovejoy, 1991), regular variable resolution approaches (Zhou & Hansen, 2001) and non-regular variable resolution representations (Brafman, 1997; Hauskrecht, 2000). In the same vein, state abstraction (Boutilier & Poole, 1996) has been explored to take advantage of factored state spaces, and of particular interest is the algorithm of Hansen and Feng (2000) which can perform state abstraction in the absence of a prior factorization.", "startOffset": 249, "endOffset": 477}, {"referenceID": 4, "context": "Discretization of the belief space itself has been explored a number of times, both regular grid-based discretization (Lovejoy, 1991), regular variable resolution approaches (Zhou & Hansen, 2001) and non-regular variable resolution representations (Brafman, 1997; Hauskrecht, 2000). In the same vein, state abstraction (Boutilier & Poole, 1996) has been explored to take advantage of factored state spaces, and of particular interest is the algorithm of Hansen and Feng (2000) which can perform state abstraction in the absence of a prior factorization. So far, however, all of these approaches have fallen victim to the \u201ccurse of dimensionality\u201d and have failed to scale to more than a few dozen states at most. The value-directed POMDP compression algorithm of Poupart and Boutilier (2002) is a dimensionality-reduction technique that is closer in spirit to ours, if not in technique.", "startOffset": 249, "endOffset": 792}, {"referenceID": 36, "context": "our algorithm with PCA instead of E-PCA, we can realize much of the same compression as the Poupart and Boutilier (2002) method: we can take advantage of regularities in the same transition matrices T a,z but not in the reward function R.", "startOffset": 92, "endOffset": 121}, {"referenceID": 6, "context": "Cheng (1988) described a method for backing up the value function at specific belief points in a procedure called \u201cpoint-based dynamic programming\u201d (PB-DP).", "startOffset": 0, "endOffset": 13}, {"referenceID": 6, "context": "Cheng (1988) described a method for backing up the value function at specific belief points in a procedure called \u201cpoint-based dynamic programming\u201d (PB-DP). These PB-DP steps are interleaved with standard backups as in full value iteration. Zhang and Zhang (2001) improved this method by choosing the Witness points as the backup belief points, iteratively increasing the number of such points.", "startOffset": 0, "endOffset": 264}, {"referenceID": 6, "context": "Cheng (1988) described a method for backing up the value function at specific belief points in a procedure called \u201cpoint-based dynamic programming\u201d (PB-DP). These PB-DP steps are interleaved with standard backups as in full value iteration. Zhang and Zhang (2001) improved this method by choosing the Witness points as the backup belief points, iteratively increasing the number of such points. The essential idea is that point-based backups are significantly cheaper than full backup steps. Indeed, the algorithm described by Zhang and Zhang (2001) out-performs Hansen\u2019s exact policy-search method by an order of magnitude for small problems.", "startOffset": 0, "endOffset": 550}, {"referenceID": 34, "context": "More recently, Pineau et al. (2003a) have abandoned full value function backups in favour of only point-based backups in the \u201cpoint-based value iteration\u201d (PBVI) algorithm.", "startOffset": 15, "endOffset": 37}, {"referenceID": 23, "context": "E-PCA is not the only possible technique for non-linear dimensionality reduction; there exists a large body of work containing different techniques such as Self-Organizing Maps (Kohonen, 1982), Generative Topographic Mapping (Bishop, Svens\u00e9n, & Williams, 1998), Stochastic Neighbour Embedding (Hinton & Roweis, 2003).", "startOffset": 177, "endOffset": 192}, {"referenceID": 43, "context": "Two of the most successful algorithms to emerge recently have are Isomap (Tenenbaum et al., 2000) and Locally Linear Embedding (Roweis & Saul, 2000).", "startOffset": 73, "endOffset": 97}, {"referenceID": 12, "context": "Peshkin, Kim, & Kaelbling, 1999) and hierarchical methods (Pineau, Gordon, & Thrun, 2003b) have also been able to solve large POMDPs. It is interesting to note that controllers based on the E-PCA representations are often essentially independent of policy complexity but strongly dependent on belief complexity, whereas the policy search and hierarchical methods are strongly dependent on policy complexity but largely independent of belief space complexity. It seems likely that progress in solving large POMDPs in general will lie in a combination of both approaches. The E-PCA algorithm finds a low-dimensional representation B\u0303 of the full belief space B from sampled data. We demonstrated that the reliance on sampled data is not an obstacle for some real world problems. Furthermore, using only sampled beliefs could be an asset for large problems where generating and tracking beliefs can be considerably easier than planning. It may however be preferable to try to compute a low-dimensional representation directly from the model parameters. Poupart and Boutilier (2002) use the notion of a Krylov subspace to do this.", "startOffset": 67, "endOffset": 1079}], "year": 2011, "abstractText": "Standard value function approaches to finding policies for Partially Observable Markov Decision Processes (POMDPs) are generally considered to be intractable for large models. The intractability of these algorithms is to a large extent a consequence of computing an exact, optimal policy over the entire belief space. However, in real-world POMDP problems, computing the optimal policy for the full belief space is often unnecessary for good control even for problems with complicated policy classes. The beliefs experienced by the controller often lie near a structured, low-dimensional subspace embedded in the high-dimensional belief space. Finding a good approximation to the optimal value function for only this subspace can be much easier than computing the full value function. We introduce a new method for solving large-scale POMDPs by reducing the dimensionality of the belief space. We use Exponential family Principal Components Analysis (Collins, Dasgupta, & Schapire, 2002) to represent sparse, high-dimensional belief spaces using small sets of learned features of the belief state. We then plan only in terms of the low-dimensional belief features. By planning in this low-dimensional space, we can find policies for POMDP models that are orders of magnitude larger than models that can be handled by conventional techniques. We demonstrate the use of this algorithm on a synthetic problem and on mobile robot navigation tasks.", "creator": "dvips(k) 5.92b Copyright 2002 Radical Eye Software"}}}