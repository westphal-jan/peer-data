{"id": "1107.0054", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2011", "title": "A Comprehensive Trainable Error Model for Sung Music Queries", "abstract": "We propose a model for errors in sung queries, a variant of the hidden Markov model (HMM). This is a solution to the problem of identifying the degree of similarity between a (typically error-laden) sung query and a potential target in a database of musical works, an important problem in the field of music information retrieval. Similarity metrics are a critical component of query-by-humming (QBH) applications which search audio and multimedia databases for strong matches to oral queries. Our model comprehensively expresses the types of error or variation between target and query: cumulative and non-cumulative local errors, transposition, tempo and tempo changes, insertions, deletions and modulation. The model is not only expressive, but automatically trainable, or able to learn and generalize from query examples. We present results of simulations, designed to assess the discriminatory potential of the model, and tests with real sung queries, to demonstrate relevance to real-world applications.", "histories": [["v1", "Thu, 30 Jun 2011 20:44:46 GMT  (5572kb)", "http://arxiv.org/abs/1107.0054v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["w p birmingham", "c j meek"], "accepted": false, "id": "1107.0054"}, "pdf": {"name": "1107.0054.pdf", "metadata": {"source": "CRF", "title": "A Comprehensive Trainable Error Model for Sung Music Queries", "authors": ["Colin J. Meek", "William P. Birmingham"], "emails": ["CMeek@microsoft.com", "WPBirmingham@gcc.edu"], "sections": [{"heading": "1. Introduction", "text": "Many approaches have been proposed to identify viable targets for a query in a music database. Query-by-hum systems attempt to meet the needs of the unskilled user for whom a natural query format - for the purpose of finding a melody, hook, or melody of unknown providence - is to sing it. Our goal is to demonstrate a unified model that is expressive enough to take into account the full range of modifications observed during the execution and transcription of sung music queries. In a complete model of singer error, we can accurately determine the likelihood that the singer would generate some queries for a specific target (or song in a database), and these probabilities provide a useful level of similarity that makes it possible to identify a query-by-hum (QBH) system in order to find strong matches in order to return to the user."}, {"heading": "2. Problem Formulation and Notation", "text": "One assumption of our work is that pitch and IOI adequately represent both the target and the query, limiting our approach to monophonic lines or sequences of non-overlapping note events. An event consists of a < pitch, IOI > duple. IOI is the time difference between the beginnings of consecutive notes and pitches. We take as input an abstraction at the note level of music. Other systems act at a lower level than representations of the query. For example, a frame-based frequency representation is commonly used (Durey, 2001; Mazzoni, 2001). Various methods for transferring frequencies and amplitudes of data into abstraction exist (Pollastrin, 2001; Shifrin Pardo, Meek & Birmingham, 2002). Our group is currently using a transcriber based on pitch (Boersma, 1993), designed to analyze pitch."}, {"heading": "3. Query Transcription", "text": "Translating from an audio query into a sequence of note events is a non-trivial problem. We will now outline the two most important steps of this translation: frequency analysis and segmentation."}, {"heading": "3.1 Frequency Analysis", "text": "For this phase, we use the Praat Pitch Tracker (Boersma, 1993), an advanced autocorrelation algorithm developed for voice analysis, which identifies multiple autocorrelation peaks for each analytical frame and chooses a path through these peaks that avoids pitch jumps and favors high correlation peaks. No peak needs to be selected for a particular image, which results in gaps in frequency analysis. In addition, the algorithm provides the autocorrelation value at the selected peak (which we use as a measure of pitch tracker confidence) and the RMS amplitude by frame (see Figure 1 for example)."}, {"heading": "3.2 Segmentation", "text": "A binary classifier is used to determine whether or not each analytical frame contains the beginning of a new note, and the characteristics the classifier takes into account are derived from the output of the Pitchtracker, a component currently under development at the University of Michigan. In its current implementation, a five-input single-layer neural network performs the classification. We assign a single pitch to each note segment based on the weighted average pitch based on the trust of the frames included in the segment. An alternative implementation is currently being investigated that treats query analysis as a signal (ideal query) with noise and attempts to detect the underlying signal using Kalman filtering techniques."}, {"heading": "4. Error Classes", "text": "A query model should be able to express the following musical - or non-musical - transformations relative to a target: 1. Insertions and deletions: Adding or removing notes from the target. These changes are often introduced by transcription tools. 2. Transposition: The query can be sung in a key or register other than the target. Essentially, the query may sound \"higher\" or \"lower\" than the target. 3. Tempo: The query may be slower or faster than the target. 4. Modulation: In the course of a query, the transposition may change. 5. Change in tempo: The singer may speed up or slow down during a query. 6. Non-cumulative local error: The singer may sing a note off-pitch or with poor rhythm."}, {"heading": "4.1 Edit Errors", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die rf\u00fc die r"}, {"heading": "4.2 Transposition and Tempo", "text": "We explain the phenomenon of people playing the same \"melody\" at different speeds and in different registers or keys. Few people have the ability to memorize and reproduce exact pitches (Terhardt & Ward, 1982), a skill known as \"absolute\" or \"perfect\" pitches. As such, transpositional invariance is a desirable feature of any query / retrieval model. In this context, the effect of transposition is simply to add a certain value to all pitches. Consider, for example, the transposition shown in Figure 6, Section a, of Trans = + 4.0 1 2 3 4 5 6 7 8Time (sec.). Figure 7: Tempo exceeTempo is simply the translation of rhythm, which describes duration relationships, into actual time duration. Again, it is difficult to remember and reproduce an exact tempo. Also, it is very unlikely that two people would choose the same metronome marker, much less unlimited time for each piece of music."}, {"heading": "4.3 Modulation and Tempo Change", "text": "During a query, the degree of transposition or tempo scaling may change, which is referred to as modulation or tempo change. Consider a query that begins with the identity transposition Trans = 0 and the identity tempo scaling = 1, as in Figure 6, section b. When a modulation or tempo change is introduced, this is always in relation to the previous transposition and tempo change. For example, the third note of the example has a modulation of Modu = + 2. For the rest of the query, the transposition is equal to 0 + 2 = + 2, starting from the initial tranposition of 0. Likewise, the tempo change of Change = 1.5 on the second note means that all subsequent events occur with a rhythmic scaling of 1 \u00b7 1.5 = 1.5. Consider Figure 7, in which the apparent tempo scaling is represented in a rendition of \"Row, Row Your Boat\" based on note to note. While our model considers several local interpretations to be at least one variation, the only one variation is considered."}, {"heading": "4.4 Local Pitch and IOI Errors", "text": "In addition to the \"gross\" errors we have discussed so far, there are often local errors in pitch and rhythm. These errors are related to the modifications described above. A local error of \u2206 (P) simply adds some value to the \"ideal\" pitch, with the ideal being determined by the relevant target note and the current transposition. A local IOI error of \u0445 (R) has a scalar effect (or additive effect in the quantified range) on the ideal IOI, which is derived from the relevant target note and the current tempo. Figure 6, Section c, shows examples of each error. Note that these errors do not spread to subsequent events and are referred to as such as non-cumulative or local errors. Transposition and tempo change are examples of cumulative errors. In some cases, there are multiple interpretations of the error source in a query. For example, consider Figure 8, which shows a specific interpretation of three inconsistencies between a target and a second note, which may be one of two possible queries in a final pair."}, {"heading": "5. Existing Error Models", "text": "Many QBH applications adopt this rhythm approach (Mazzoni, 2001; Meek & Birmingham, 2002; Pauws, 2002; McNab, Smith, Bainbridge, & Witten, 1997; McNab, Smith, Witten, Henderson, & Cunningham, 1996).In this study, we deal primarily with local and cumulative errors. Far less is known about this area, which is largely a matter of convenience: a particular musical presentation will tend to favor one approach over the other. For example, pitch and tempo invariant representation (pitch and interval ratio) (Shifrin et al., 2002; Pauws, 2002) is a new transposition and tempo context for each note."}, {"heading": "5.1 Alternative Approaches", "text": "It is primarily a sequence-based approach to music retrieval, and this assumption can be somewhat loosened by translating targets into Markov models in which the state is merely a characteristic relationship between consecutive notes and loops in the model (Shifrin et al., 2002). Similar to the text search world, we can also model music as a collection of note-n-grams and apply standard text retrieval algorithms (Downie, 1999; Tseng, 1999). In query-by-sum systems, the user looks for a song that \"sounds like...,\" rather than a song that is \"about\" any short snippet of music, if it makes sense at all to discuss music in these terms. 2 For this reason, we believe that sequence-based methods can render music more accurately in this context.2 Beethoven's Fifth Symphony is a notable exception."}, {"heading": "5.2 Indexing and Optimization Techniques", "text": "It should be noted that, as a general model, JCS cannot take advantage of optimizations specific to specific specializations, such as the cumulative-only version for relative note rendering, transposition and tempo-invariant rendering, eliminating the need to compare a query with multiple permutations of the target. Existing index techniques for string editing distance metrics - for example, by using suffix trees (Cha \u0301 vez & Navarro, 2002; Gusfield, 1997) and so-called \"wavelet\" approximations (Kahveci & Singh, 2001) - are suitable for k distance searches and may prove useful as a pre-filtering mechanism."}, {"heading": "6. Hidden Markov Models", "text": "Hidden Markov models (HMM) are the basis of our approach. We start by describing a simple HMM and then describe the enhancements to the model that are necessary for the current task. As the name suggests, HMMs contain hidden or unnoticed states. Let's take a simple example of a dishonest player who is known to occasionally exchange a fair die for a loaded die (using the Biological Sequence Analysis (Durbin, Eddy, Krogh, & G.Mitchison, 1998). Unfortunately, it is impossible (directly) to observe which die is being used, as they are visually indistinguishable. Therefore, we define two hidden states, a and b, which represent the assumptions that the player is using fair and charged cubes, each q. In addition, we represent our expectation that the player will switch cubes or remain with a cube, using a transition diagram where transition associations are shown (see figure 1)."}, {"heading": "6.1 Honest or Dishonest? An example", "text": "The strength of the HMM approach is that it is easy to determine the probability of any observation sequence when the transition probabilities and emission probabilities are known. Conceptually, the idea is to look at all possible paths through the model in accordance with the observation sequence (e.g. the observed roll of the dice) and to take the sum of the probabilities of each path into account. For example, the roll sequence {1, 5, 4} could be generated by one of four paths in the model of the dishonest player, assuming that the most dishonest player always begins with the fair roll: {a, a}, {a, a, b}, {a, b, a}, {a, b, b}}}. For example, the probability of the second path is equal to the probability of the most dishonest player that the probability of transitions (Pr (a \u2192 a) \u00b7 Pr (a) \u00b7 b) = 50.1 = 0.09)."}, {"heading": "7. Extended HMM", "text": "In the context of our query error model, we take into account editing errors (insertions and deletions) in the \"hidden\" part of the model. Using the term state \"clusters,\" we take into account transposition, modulation, tempo and tempo changes. Fine tone and rhythm errors are taken into account in the observation distribution function."}, {"heading": "7.1 State Definition", "text": "The state definition consists of three elements: edit type (Edit), transposition (Key), and tempo (Speed). Stations are recorded as follows: sx = < E [x], K [x], S [x] >, 1 \u2264 x \u2264 n (5) If E is the set of all editing types, K is the set of all transpositions, and S is the set of all tempi, then the set of all states S is equal: E \u00b7 K \u00b7 S (6) We now define each of these sets:"}, {"heading": "7.1.1 Edit Type", "text": "For reasons of notationality, Join1i plays all three roles. We do not enumerate the editing types in E, but define them in terms of symbols that indirectly refer to events in the target sequence, with information about the encoding position. There are three types of symbols: \u2022 Samei: refers to the correspondence between the i th note in the target and an eventin the query. \u2022 Joinli: refers to a \"connection\" of the pit note, starting from the i th note in the target. In other words: a single note in the query replaces l notes in the target. \u2022 Elabmi, j: refers to the J th note elaborating the ith th note of the target. In other words, asingle note in the target is replaced by m notes in the query. Note that Samei = Join 1 i = Elab 1 i, 1, any reference to a one-to-to-to-to-to-correspondence between target and notes."}, {"heading": "7.1.2 Transposition and Tempo", "text": "In order to consider the different ways in which target and query might relate to each other, we need to refine our state definition further and include transposition and tempo clusters. Intuition is that the edit type determines the orientation of events between the query and the target (see Figure 12 for example), and the cluster determines the exact relationship between these events. By using the pitch class, there are only twelve possible different transpositions due to the Module 12 relationship to pitch. While each offset is enough, we set K = {\u2212 5, \u2212 4,..., + 6}. We set limits on how far a query can be from the target in terms of tempo, so that the query can be between half and double the speed, corresponding to values in the range S = {\u2212 4, \u2212 3,..., + 4} in quantified tempo units (based on the logarithmic quantization scale described in Section B)."}, {"heading": "7.2 Transition Matrix", "text": "We now describe the transition matrix A, which is mapped by S \u00b7 S \u2192 \u0442. Where qt is the state at the time t (as defined by the position in the query or observation sequence), axie is equal to the probability Pr (qt = sx | qt + 1 = sy, \u03bb), or in other words, the probability of a transition from state sx to state sy.The transition probability consists of three values, a processing type, modulation and tempo change probability: axie = an E xy \u00b7 a K xy \u00b7 a S xy (8). We describe each of these values individually."}, {"heading": "7.2.1 Edit Type Transition", "text": "Most of the edit-type transitions have zero probability, as the state descriptions suggest. (For example, Samei states can only precede states that point to index i + 1 in the target. Elaboration states are even more restrictive because they form deterministic chains of form: Elabmi, 1 \u2192 Elab m i, 2, \u2192. (Elab m i, m. This latter state can then go to the i + 1 states like Samei. Similarly, Joinli states can only proceed to i + l states. An example topology model is shown in Figure 13, for M = L = 2. Note that this is a left-right model in which transitions impose a partial arrangement on states. Based on the characteristics of the target, we can create these transition chances. We define PJoin (i, l) as the probability that the ith grade will be modified in the target."}, {"heading": "7.2.2 Modulation and Tempo Change", "text": "Modulation and tempo changes are modeled as transitions between clusters. We refer to the probability of modulation by \u2206 (K) semitones on the ith target event as PModu (i, \u2206 (K)) (redefined by the range \u2212 5 \u2264 (K) \u2264 + 6). The probability of a tempo change of \u2206 (S) quantization units is referred to as PChange (i, \u0445 (S), which allows a halving to doubling of the tempo in each step (\u2212 4 \u2264 (S) \u2264 + 4). Again, we propose the following alternatives for modulation by specifying contexts for transposition (as CKi with associated probability function PKi) and tempo change (as C S i with associated probability function PSi). Without limiting the definition of these contexts, we propose the following alternatives: \u2022 In our current implementation, we simply apply a normal distribution over the modulation around itself (K)."}, {"heading": "7.2.3 Anatomy of a Transition", "text": "In a transition sx \u2192 sy (where sx = < E [x], K [x], S [x] >), sx belongs to three contexts: C E i, CKj and C S k. The second state is an example of an input classification \u0445 (E), i.e. aExy = PEi (\u0445 (E)). The transition corresponds to a modulation of \u0445 (K) = K [y] \u2212 K [x], i.e. aKxy = PKj (\u0445 (K)). Finally, the transition contains a tempo change \u0394( S) = S [y] \u2212 S [x], i.e. aSxy = PSk (\u0445 (S))."}, {"heading": "7.3 Initial State Distribution", "text": "We associate the initial state distribution in the hidden model with a single target event, so a separate model must be created for each possible starting point. Note, however, that we can actually point to a single larger model and generate different initial state distributions for each separate starting point model to address any concerns about the memory and time cost of building the models. Essentially, this function consists of parts for the processing types (\u03c0Ex), transposition (\u03c0 K x), and tempo (\u03c0 S x). Our initial processing distribution (\u03c0Ex) for an alignment that begins with the i th event in thetarget consists of only those processing types that are associated with i: Samei, {Join l i} l = 2, and {Elab m i, 1} m = 2. We tie initial processing probabilities to the processing of transition types that are directly associated with x."}, {"heading": "7.4 Emission Function", "text": "Conventionally, a hidden state is said to emit an observation, of any discrete or continuous domain. A matrix B maps from S \u00b7 O \u2192 B, where S is the set of all states and O is the set of all observations. bx (ot) is the probability of sending an observation error (Pr (ot | qt = sx, \u03bb). In our model, it is easier to consider a hidden state as sending observation errors, relative to our expectation of what the tone class and IOI is on the processing type, transposition and tempo. Equation 7 defines our expectation of the relationship between target and query events based on the processing type. For the hidden state sx = < E [x], K [x], S [x] > we represent this relationship by defining the abbreviation < P [i] > < P [t] > roof classes."}, {"heading": "7.5 Alternative View", "text": "In Figure 14.A, the first interpretation is shown. In the hidden states (S), each state is defined by si = < E [i], K [i], S [i] >, and according to the Markov first-order assumption, the current state depends only on the previous state. Observations (O) are assumed only by the hidden state and are defined by ot = < P [t], R [t] >. The second view provides more details (Figure 14.B). Dependencies between the individual components are shown. The hidden chains E, K, and S denote only the respective components of a hidden state. The edit type (E) only depends on the previous edit type (for a detailed image of this component, see Figure 13)."}, {"heading": "8. Probability of a Query", "text": "In the context of music retrieval, one critical task is to calculate the probability that a particular target would generate a query, given the model. On the basis of these probability values, we can classify a number of potential database targets in terms of their relevance to the query. Conceptually, the idea is to look at any possible path through the hidden model. Each path is represented by a sequence of hidden states Q = {q1, o2,.., qT}, which path has a probability that is equal to the product of the transition probabilities of each successive state pair. Furthermore, there is a certain probability that each path represents the observation sequence O = {o1, o2,., oT} (or the query.) Thus, the probability of a query is a significant probability given the model (referred to as a deviation): Pr (O | \u03bb) = x x."}, {"heading": "8.1 Complexity Analysis", "text": "Based on the topology of the hidden model, we can calculate the complexity of the forward variable algorithm for this implementation. Since each processing type has transition probabilities for at most L + M \u2212 1 other processing types, this defines a branching factor (b) for the forward algorithm. Furthermore, each model can have a maximum of b | D | states, where | D | is the length of the target. Updating the transposition and tempo probabilities between two processing types (including all cluster permutations) requires k = (9 \u00b7 12) 2 multiplications given the current tempo quantization and the limits for tempo changes. Note that increasing the permissible range for tempo fluctuation or resolving the quantification results in a superlinear increase in time requirements! Thus, for each introductory step (for t = 1, 2,...) we need at most k | D | b2 multiplications as extensive quantification."}, {"heading": "8.2 Optimizations", "text": "While asymptotic improvements in complexity are not possible, certain optimizations have proved quite effective, as they have a tenfold improvement in runtime. An alternative approach to calculating the probability of a query in the model is to find the probability of the most likely (single) path through the model using the Viterbi algorithm. This is a classic dynamic programming approach based on the observation that the optimal path must consist of optimal partial paths. Instead of taking the sum probability of all paths leading to a state, we simply take the maximum probability for each state at a time t + 1 based on the most likely path to each state at a time. Thus, the algorithm is a simple modification of the forward variable algorithm. Instead of taking the sum probability of all paths leading to a state, we simply take the maximum probability: \u03b1t + 1 (y) = max [t = 1 by [x] (x) (1) by axot."}, {"heading": "8.2.1 Branch and Bound", "text": "With Viterbi, it is possible to use branches and pre-emptively prune paths if it can be shown that no possible completion can lead to a sufficiently high probability. First, we should explain what we mean by \"high enough\": If only a fixed number (k) of results is required, we reject paths that are unable to generate a probability greater or equal to the kth highest probability observed so far in the database. How can we set an upper limit on the probability of a path? We find that each event (or observation) in an optimal Viterbi path introduces a factor that is the product of the probability of observation, the probability of transition, and the probability of transition between clusters. Knowing the maximum possible value of this factor (f) allows us to predict the minimum \"cost\" of completing the algorithm along a given path."}, {"heading": "9. Training", "text": "We need to learn the following parameters for our HMM: \u2022 the probabilities of observing all pitch and rhythm errors (the functions PPc and P R c for all contexts c); \u2022 the probabilities of modulation and tempo change by all relevant amounts (PKc and PSc); and, \u2022 the probabilities of transitions to each of the processing types (PEc). We set some parameters in our model. For example, the initial processing type distributions are not explicitly specified as they are linked to the processing type transition function as described above. In addition, we assume a uniform distribution over the initial transposition and a normal distribution over the initial tempo. This is because we see no way to generalize the initial distribution data on songs for which we have no training examples. Remember that, for example, the tendency of the user to sing \"Hey Jude\" sharply and quickly, \"the choice of the transposition of the River or the Moon should not affect the conditions described in the\" Extension procedure. \""}, {"heading": "9.1 Training a Simple HMM", "text": "With a fully observable Markov model, it is fairly easy to learn transition probabilities: we simply count the number of transitions between each pair of states. While we cannot directly count transitions between each pair of states, we can use the forward variable and a backward variable (defined below) to calculate our expectation that each hidden transition has occurred, and thus we \"count\" the number of transitions between each pair of states indirectly. Until we have parameters for the HMM, we cannot calculate the forwarding and backward variables. Therefore, we select start parameters either randomly or based on previous expectations and iteratively reestimated model parameters. This method is known as the Tree-Welch, or as the Expectation Maximization Algorithm (Baum & Eagon, 1970).Consider a simple HMM state (denoted) with a transition matrix A, where the galaxy is the probability of transition from state to state, an observation matrix."}, {"heading": "9.2 Training the Query Error Model", "text": "Our query model has some key differences from the model outlined above: heavy parameters for the transition phase and multiple components for both transitions and observations. Instead of asking, \"How likely is a transition from sx \u2192 sy (or what is one?)?,\" we ask, for example, \"How likely is a modulation of \u2206 (K) in the modulation context c (or what is P-Kc (K))?\" To answer this question, we define an intermediate variable, Kt (K), c) = prognos sx (K), y), if K [y] \u2212 K [x] \u2212 K (K) 0 otherwise, (32) the probability of a modulation of systems (K) in the modulation context c between the time steps t and t + 1. We can now answer the question as follows: P-Kc (K) \u2212 St \u2212 t = Prognos-K (K)."}, {"heading": "9.3 Starting Parameters", "text": "The components of our model have clear musical meanings that guide the selection of starting parameters in the training process. We use normal distributions of error and cluster modification parameters that focus on \"no error\" or \"no change.\" This is based solely on the assumption (without which the whole MIR exercise would be a lost cause) that singers generally introduce small errors rather than big errors. The initial probabilities for processing can be determined by the handwriting of a few automatically transcribed queries. It is important to make a good guess at the initial parameters, as the reassessment approach only approaches a local maximum."}, {"heading": "10. Analysis", "text": "In order to maintain generality in our discussion and to draw conclusions that are not specific to our experimental data or approach to note rendering, it is useful to analyze the model entropy with respect to cumulative and local errors. What influences does the retrieval performance have? From an information perspective, entropy provides a clue. Intuitively, entropy measures our uncertainty about what will happen next in the query. Formally, the entropy value of a process is the average amount of information required to predict its result. If the entropy is higher, we will cast a wider net retrievally because our ability to predict how the singer err will be reduced. What happens if we assume cumulative errors in pitch are the average amount of information required to predict its result? Consider the following simplified analysis: Assuming that two marks are distributed after a normal note, where X represents the distributed error, X represents the second one representing the normal note where X represents the distributed error."}, {"heading": "11. Results", "text": "Subject A is a professional instrumental musician, and Subject C has a certain musical pre-school education, but the other subjects have no formal musical background. Each subject was asked to sing eight passages from well-known songs. We recorded four versions of each passage for each theme, twice only with reference to the text of the passage. After these first two attempts, the subjects were allowed to listen to a MIDI rendition of this passage - transposed to their vocal range - as many times as it was necessary to familiarise themselves with the melody, and sang the questions two more times."}, {"heading": "11.1 Training", "text": "JCS can be configured to support only certain types of errors by controlling the initial parameters for the training. For example, if we set the probability of a transposition to zero, the reappraisal method will maintain this value throughout. The results of this training for three versions of the model over the full set of 160 queries are shown in Figure 16, which shows the total parameters learned for each model. For all versions, a similar processing distribution results: the probability of no error being processed is about 0.85, the probability of consolidation is 0.05, and the probability of fragmentation is 0.1. These values are primarily related to the behavior of the underlying note segmentation mechanism. In one of the models, both local and cumulative errors are considered, in the figure as \"full.\" Restricted versions with the expected assumptions are referred to as \"local\" or \"cumulative.\" It should be obvious that the full model allows for narrower distribution over local errors."}, {"heading": "11.2 Retrieval Performance", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "11.3 Training Generalization", "text": "Because of the redundancy in the query collection - multiple versions of each passage and multiple examples of each subject's vocals - it is instructive to examine the query performance when the models are trained using examples that have nothing to do with the test set. We randomly selected four of the eight passages in our study and two of the five subjects and used these queries for training, and the remaining passages performed by the remaining subjects were used for training. Thus, the two sets do not share singers or passages. As a starting point, we provide query results when the model is trained on the test set (see Table 2, Figures 19 and 20). In these experiments, we see evidence of overmatch of the more parameterized \"complete\" model, which is partially due to the relatively small training set available for this experiment. There is a significant performance difference between the baseline and generalization results."}, {"heading": "11.4 Run-time Performance", "text": "While various parameterization-specific optimizations are possible, the general implementation of JCS has a lowest common denominator in terms of runtime performance. On a 1.6 GHz Pentium 4 Linux machine, a Java version of JCS 10 \u2212 410 \u2212 310 \u2212 210 \u2212 110 000.10.30.40.50.60.70.80.91False positive rate (log scale) T rue posi tive rate Figure 19: Retrieval performance for independent test and training sets10 \u2212 410 \u2212 310 \u2212 210 \u2212 110 000.10.20.30.40.50.60.70.80.91False positive rate (log scale) T rue posi tive rateFull Restricted Local Cumulative Simple HMM RandomFigure 20: Baseline performance averages 15 queries to achieve comparisons per second."}, {"heading": "12. Future Work", "text": "Even with the generalizations described in this model, a large number of parameters remain in place. We are currently collecting query data to train the model, since more in-depth assessments of performance in non-synthetic queries will be critical. Several important questions remain to be answered, such as: \u2022 What effect does the query presentation have, for example, using conventional note rendering instead of pitch rendering? \u2022 How can we best link training parameters? \u2022 Instead of modelling the query as a sequence of discrete note events, will (or should) it be presented as a sequence of fixed timeframe analyses? \u2022 HMMs are available for \"frame-based\" representations that would allow us to bypass the problematic note segmentation stage of query transcription. Instead of modelling it as a sequence of discrete events, it is presented as a sequence of fixed timeframe analyses. Each state in the target model then has a probability to remain in the duration for the duration - some pertaining to the distribution of the duration."}, {"heading": "13. Conclusion", "text": "Some of these errors are tempo, pitch (both octave and absolute), melodic contour, and the particularly difficult class of connections (\"skipped\" notes in the query) and elaborations (note inserted in the query). It has been shown that a natural \"musical\" interpretation of these changes helps to alleviate the problem of target similarity in processing, as noted in a previous QBH feasibility study (Sorsa, 2001). Our experimental results show that the model is effective in dealing with these errors, meaning that we can detect when these errors occur and take them into account in the query when necessary. Experimental results suggest good retrieval performance when these errors occur in queries. Furthermore, our model is able to model cumulative errors and local errors separately, or a combination of both. Generally, music information retrieval researchers have been split on the meaning of these errors."}, {"heading": "Acknowledgements", "text": "We are grateful for the support of the National Science Foundation in funding IIS0085945 and the seed grant from the University of Michigan College of Engineering for the MusEn project. The opinions expressed in this essay are solely those of the authors and do not necessarily reflect the opinions of the funding agencies."}, {"heading": "Appendix A. Deriving Re-estimation Formulae", "text": "The estimation method converges to a critical point in the parameter space in terms of probability. Tree defines an auxiliary function q (44), where \u03bb (44) represents the \"current\" model parameter values, and we try to reestimate this number iteratively because: q (44) = QP (O, Q | 4) log P (O, Q | 4) log P (40) By maximizing this function we maximize P (O | 4) because: q (4) q (4) = QP (O, 4) \u00b2 (O | 4) log P (O | 5) print P (41) We will now derive this implication. Notice that P (O | 5) = x P (O, Q | 5) or the sum of probabilities of all possible paths through the model. There is a finite number N of paths. Where Qi is the path, pi = P (O, Qi | 5) is the sum of all possible paths through the model."}, {"heading": "Appendix B. Notation", "text": "We will now sketch the notation used to describe the error model: Notation description < pitch [x], IOI [x] > xth note event < P [x], R [x] > xth note event, quantized ot and di t th observation (query note) and dith target event (database note) = \u2212 1 a pitch spacing error, a semitone flattening p (R) = + 1 a rhythm error, a quantization unit too long x = < E [x], K [x], S [x] > xth HMM hidden state = < Same1, + 2, \u2212 3 > E [x] = Same1: Processing type, replacement of the first target noteK [x] = + 2: Transposition (clef), 2 semitones sharp S [x] = \u2212 3: tempo scaling, 3 units fasteraxy = a K \u00b7 a probability of a transitional state of a missed K [x]."}], "references": [{"title": "Gapped BLAST and PSI-BLAST: A New Generation of Protein Database Search Programs", "author": ["S. Altschul", "T. Madden", "A. Schaffer", "J. Zhang", "Z. Zhang", "W. Miller", "D. Lipman"], "venue": "Nucleic Acids Research,", "citeRegEx": "Altschul et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Altschul et al\\.", "year": 1997}, {"title": "To catch a chorus: Using chroma-based representations for audio thumbnailing", "author": ["M. Bartsch", "G. Wakefield"], "venue": "In Proceedings of Workshop on Applications of Signal Processing to Audio and Acoustics", "citeRegEx": "Bartsch and Wakefield,? \\Q2001\\E", "shortCiteRegEx": "Bartsch and Wakefield", "year": 2001}, {"title": "A maximization technique occurring in the statistical analysis of probabilistic functions of markov chains", "author": ["L.E. Baum", "J.A. Eagon"], "venue": "Annals of Mathematical Statistics,", "citeRegEx": "Baum and Eagon,? \\Q1970\\E", "shortCiteRegEx": "Baum and Eagon", "year": 1970}, {"title": "Musart: Music retrieval via aural queries", "author": ["W. Birmingham", "R. Dannenberg", "G. Wakefield", "M. Bartsch", "D. Bykowski", "D. Mazzoni", "C. Meek", "M. Mellody", "W. Rand"], "venue": "In Proceedings of International Symposium on Music Information Retrieval", "citeRegEx": "Birmingham et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Birmingham et al\\.", "year": 2001}, {"title": "The musart music-retrieval system: An overview", "author": ["W. Birmingham", "B. Pardo", "C. Meek", "J. Shifrin"], "venue": "D-Lib Magazine,", "citeRegEx": "Birmingham et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Birmingham et al\\.", "year": 2002}, {"title": "Accurate short-term analysis of the fundamental frequency and the harmonics-to-noise ratio of a sampled sound", "author": ["P. Boersma"], "venue": "Proceedings of the Institute of Phonetic Sciences, Vol. 17.", "citeRegEx": "Boersma,? 1993", "shortCiteRegEx": "Boersma", "year": 1993}, {"title": "Melody retrieval on the web", "author": ["W. Chai"], "venue": "Master\u2019s thesis, Massachussetts Institute of Technology.", "citeRegEx": "Chai,? 2001", "shortCiteRegEx": "Chai", "year": 2001}, {"title": "A Metric Index for Approximate String Matching", "author": ["E. Ch\u00e1vez", "G. Navarro"], "venue": "In LATIN,", "citeRegEx": "Ch\u00e1vez and Navarro,? \\Q2002\\E", "shortCiteRegEx": "Ch\u00e1vez and Navarro", "year": 2002}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["M. Dempster", "N. Laird", "D. Jain"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Evaluating a simple approach to music information retrieval: conceiving melodic n-grams as text", "author": ["S. Downie"], "venue": "Ph.D. thesis, University of Western Ontario.", "citeRegEx": "Downie,? 1999", "shortCiteRegEx": "Downie", "year": 1999}, {"title": "Biological Sequence Analysis", "author": ["R. Durbin", "S. Eddy", "A. Krogh"], "venue": "G.Mitchison", "citeRegEx": "Durbin et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Durbin et al\\.", "year": 1998}, {"title": "Melody spotting using hidden markov models", "author": ["A. Durey"], "venue": "Proceedings of International Symposium on Music Information Retrieval.", "citeRegEx": "Durey,? 2001", "shortCiteRegEx": "Durey", "year": 2001}, {"title": "Algorithms on strings, trees, and sequences", "author": ["D. Gusfield"], "venue": "Cambridge University Press.", "citeRegEx": "Gusfield,? 1997", "shortCiteRegEx": "Gusfield", "year": 1997}, {"title": "Spoken language processing, chap. Large-vocabulary search algorithms, pp", "author": ["X. Huang", "A. Acero", "H. Hon"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2001}, {"title": "An Efficient Index Structure for String Databases", "author": ["T. Kahveci", "A.K. Singh"], "venue": "In Proceedings of 27th International Conference on Very Large Data Bases,", "citeRegEx": "Kahveci and Singh,? \\Q2001\\E", "shortCiteRegEx": "Kahveci and Singh", "year": 2001}, {"title": "String matching techniques for music retrieval", "author": ["K. Lemstrom"], "venue": "Tech. rep., University of Helsinki.", "citeRegEx": "Lemstrom,? 2000", "shortCiteRegEx": "Lemstrom", "year": 2000}, {"title": "Melody matching directly from audio", "author": ["D. Mazzoni"], "venue": "Proceedings of International Symposium on Music Information Retrieval.", "citeRegEx": "Mazzoni,? 2001", "shortCiteRegEx": "Mazzoni", "year": 2001}, {"title": "Towards the digital music library: Tune retrieval from acoustic input", "author": ["R.J. McNab", "L.A. Smith", "I.H. Witten", "C.L. Henderson", "S.J. Cunningham"], "venue": "In Digital Libraries,", "citeRegEx": "McNab et al\\.,? \\Q1996\\E", "shortCiteRegEx": "McNab et al\\.", "year": 1996}, {"title": "The new zealand digital library MELody inDEX", "author": ["R. McNab", "L. Smith", "D. Bainbridge", "I. Witten"], "venue": null, "citeRegEx": "McNab et al\\.,? \\Q1997\\E", "shortCiteRegEx": "McNab et al\\.", "year": 1997}, {"title": "Johnny can\u2019t sing: A comprehensive error model for sung music queries", "author": ["C. Meek", "W. Birmingham"], "venue": "In Proceedings of International Symposium on Music Information Retrieval, pp", "citeRegEx": "Meek and Birmingham,? \\Q2002\\E", "shortCiteRegEx": "Meek and Birmingham", "year": 2002}, {"title": "Oasis: An online and accurate technique for localalignment searches on biological sequences", "author": ["C. Meek", "J. Patel", "S. Kasetty"], "venue": "In Proceedings of the 29th International Conference on Very Large Data Bases,", "citeRegEx": "Meek et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Meek et al\\.", "year": 2003}, {"title": "Comparison of musical sequences", "author": ["M. Mongeau", "D. Sankoff"], "venue": "Computers and the Humanities,", "citeRegEx": "Mongeau and Sankoff,? \\Q1990\\E", "shortCiteRegEx": "Mongeau and Sankoff", "year": 1990}, {"title": "Timing information for musical query matching", "author": ["B. Pardo", "W. Birmingham"], "venue": "In Proceedings of International Symposium on Music Information Retrieval", "citeRegEx": "Pardo and Birmingham,? \\Q2002\\E", "shortCiteRegEx": "Pardo and Birmingham", "year": 2002}, {"title": "Algorithms for chordal analysis", "author": ["B. Pardo", "W. Birmingham"], "venue": "Computer Music Journal, 26, 27 \u2013 49.", "citeRegEx": "Pardo and Birmingham,? 2002", "shortCiteRegEx": "Pardo and Birmingham", "year": 2002}, {"title": "Cubyhum: a fully functional, \u201cquery by humming\u201d system", "author": ["S. Pauws"], "venue": "Proceedings of International Symposium on Music Information Retrieval.", "citeRegEx": "Pauws,? 2002", "shortCiteRegEx": "Pauws", "year": 2002}, {"title": "Flexible sequence similarity searching with the fasta3 program package", "author": ["W. Pearson"], "venue": "http://www.people.virginia.edu/ wrp/papers/mmol98f.pdf.", "citeRegEx": "Pearson,? 1998", "shortCiteRegEx": "Pearson", "year": 1998}, {"title": "An audio front end for query-by-humming systems", "author": ["E. Pollastri"], "venue": "Proceedings of International Symposium on Music Information Retrieval.", "citeRegEx": "Pollastri,? 2001", "shortCiteRegEx": "Pollastri", "year": 2001}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of IEEE, Vol. 77 (2), pp. 257 \u2013 286.", "citeRegEx": "Rabiner,? 1989", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "Hmm-based musical query retrieval", "author": ["J. Shifrin", "B. Pardo", "C. Meek", "W. Birmingham"], "venue": "In Proceedings of Joint Conference on Digital Libraries", "citeRegEx": "Shifrin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Shifrin et al\\.", "year": 2002}, {"title": "Sensitivity and Selectivity in Protein Similarity Searches: A Comparison of Smith-Waterman in Hardware to BLAST and FASTA", "author": ["E. Shpaer", "M. Robinson", "D. Yee", "J. Candlin", "R. Mines", "T. Hunkapiller"], "venue": null, "citeRegEx": "Shpaer et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Shpaer et al\\.", "year": 1996}, {"title": "Melodic resolution in music retrieval", "author": ["T. Sorsa"], "venue": "Proceedings of International Symposium on Music Information Retrieval.", "citeRegEx": "Sorsa,? 2001", "shortCiteRegEx": "Sorsa", "year": 2001}, {"title": "Recognition of musical key: Exploratory study", "author": ["E. Terhardt", "W. Ward"], "venue": "Journal of the Acoustical Society of America, 72,", "citeRegEx": "Terhardt and Ward,? \\Q1982\\E", "shortCiteRegEx": "Terhardt and Ward", "year": 1982}, {"title": "Content-based retrieval for music collections", "author": ["Tseng", "Y.-H."], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pp. 176 \u2013 182. ACM Press.", "citeRegEx": "Tseng and Y..H.,? 1999", "shortCiteRegEx": "Tseng and Y..H.", "year": 1999}, {"title": "Overview of the fifth text retrieval conference", "author": ["E.M. Voorhees", "D.K. Harman"], "venue": "In The Fifth Text REtrieval Conference", "citeRegEx": "Voorhees and Harman,? \\Q1997\\E", "shortCiteRegEx": "Voorhees and Harman", "year": 1997}, {"title": "Sia(m)ese: An algorithm for transposition invariant, polyphonic content-based music retrieval", "author": ["G. Wiggins", "K. Lemstrom", "D. Meredith"], "venue": "In Proceedings of International Symposium on Music Information Retrieval", "citeRegEx": "Wiggins et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Wiggins et al\\.", "year": 2002}, {"title": "Warping indexes with envelope transforms for query by humming", "author": ["Y. Zhu", "D. Shasha"], "venue": "In Proceedings of SIGMOD", "citeRegEx": "Zhu and Shasha,? \\Q2003\\E", "shortCiteRegEx": "Zhu and Shasha", "year": 2003}], "referenceMentions": [{"referenceID": 11, "context": "For instance, a frame-based frequency representation is often used (Durey, 2001; Mazzoni, 2001).", "startOffset": 67, "endOffset": 95}, {"referenceID": 16, "context": "For instance, a frame-based frequency representation is often used (Durey, 2001; Mazzoni, 2001).", "startOffset": 67, "endOffset": 95}, {"referenceID": 26, "context": "Various methods for the translation of frequency and amplitude data into note abstraction exist (Pollastri, 2001; Shifrin, Pardo, Meek, & Birmingham, 2002).", "startOffset": 96, "endOffset": 155}, {"referenceID": 5, "context": "Our group currently uses a transcriber based on the Praat pitch-tracker (Boersma, 1993), designed to analyze voice pitch contour.", "startOffset": 72, "endOffset": 87}, {"referenceID": 26, "context": "Adopting an approach proposed for a QBH \u201caudio front end\u201d (Pollastri, 2001), we consider several offsets (O = {0.", "startOffset": 58, "endOffset": 75}, {"referenceID": 5, "context": "1 Frequency Analysis We use the Praat pitch-tracker (Boersma, 1993), an enhanced auto-correlation algorithm developed for speech analysis, for this stage.", "startOffset": 52, "endOffset": 67}, {"referenceID": 15, "context": "However, previous work assumes this kind of influence: noting that intervallic contour tends to be the strongest component in our memory of pitch; one researcher has proposed that insertions and deletions could in some cases have a \u201cmodulating\u201d effect (Lemstrom, 2000), where the edit introduces a pitch offset, so that pitch intervals rather than the pitches themselves are maintained.", "startOffset": 252, "endOffset": 268}, {"referenceID": 16, "context": "Many QBH applications adopt this approach to rhythm (Mazzoni, 2001; Meek & Birmingham, 2002; Pauws, 2002; McNab, Smith, Bainbridge, & Witten, 1997; McNab, Smith, Witten, Henderson, & Cunningham, 1996).", "startOffset": 52, "endOffset": 200}, {"referenceID": 24, "context": "Many QBH applications adopt this approach to rhythm (Mazzoni, 2001; Meek & Birmingham, 2002; Pauws, 2002; McNab, Smith, Bainbridge, & Witten, 1997; McNab, Smith, Witten, Henderson, & Cunningham, 1996).", "startOffset": 52, "endOffset": 200}, {"referenceID": 28, "context": "For instance, a pitch- and tempo-invariant representation (pitch interval and inter-onset interval ratio) (Shifrin et al., 2002; Pauws, 2002) establishes a new transposition and tempo context for each note, thus introducing the implicit assumption that all errors are cumulative (Pardo & Birmingham,", "startOffset": 106, "endOffset": 141}, {"referenceID": 24, "context": "For instance, a pitch- and tempo-invariant representation (pitch interval and inter-onset interval ratio) (Shifrin et al., 2002; Pauws, 2002) establishes a new transposition and tempo context for each note, thus introducing the implicit assumption that all errors are cumulative (Pardo & Birmingham,", "startOffset": 106, "endOffset": 141}, {"referenceID": 26, "context": "A study of sung queries (Pollastri, 2001) determined that cumulative error is in fact far less common than local error, a conclusion supported by our studies.", "startOffset": 24, "endOffset": 41}, {"referenceID": 16, "context": "Dynamic time-warping approaches (Mazzoni, 2001) and non-distributed HMM techniques (Durey, 2001) are wellsuited to this technique.", "startOffset": 32, "endOffset": 47}, {"referenceID": 11, "context": "Dynamic time-warping approaches (Mazzoni, 2001) and non-distributed HMM techniques (Durey, 2001) are wellsuited to this technique.", "startOffset": 83, "endOffset": 96}, {"referenceID": 6, "context": "An alternative is to normalize the tempo of the query by either automated beat-tracking, a difficult problem for short queries, or, more effectively, by giving the querier an audible beat to sing along with \u2013 a simple enough requirement for users with some musical background (Chai, 2001).", "startOffset": 276, "endOffset": 288}, {"referenceID": 28, "context": "It is possible to relax this assumption somewhat, by translating targets into Markov models where the state is simply a characteristic relationship between consecutive notes, allowing for loops in the model (Shifrin et al., 2002).", "startOffset": 207, "endOffset": 229}, {"referenceID": 9, "context": "Borrowing from the text search world, we can also model music as a collection of note n-grams, and apply standard text retrieval algorithms (Downie, 1999; Tseng, 1999).", "startOffset": 140, "endOffset": 167}, {"referenceID": 12, "context": "Existing indexing techniques for string-edit distance metrics \u2013 for instance using suffix trees (Ch\u00e1vez & Navarro, 2002; Gusfield, 1997) and so-called \u2018wavelet\u2019 approximations (Kahveci & Singh, 2001) \u2013 are appropriate for k-distance searches, and thus might prove useful as a pre-filtering mechanism.", "startOffset": 96, "endOffset": 136}, {"referenceID": 25, "context": "Linear search using sub-optimal heuristics has been applied to sequence matching in bio-informatics (Pearson, 1998; Altschul, Madden, Schaffer, Zhang, Zhang, Miller, & Lipman, 1997).", "startOffset": 100, "endOffset": 181}, {"referenceID": 27, "context": "The \u201cstandard\u201d forward-variable algorithm (Rabiner, 1989) provides a significant reduction in complexity.", "startOffset": 42, "endOffset": 57}, {"referenceID": 26, "context": "93 there is no modulation), further evidence (Pollastri, 2001) that local error is indeed the critical component.", "startOffset": 45, "endOffset": 62}, {"referenceID": 28, "context": "For comparison, we provide results from another HMM-based QBH system, listed as \u201cSimple HMM\u201d (Shifrin et al., 2002), using default parameters.", "startOffset": 93, "endOffset": 115}, {"referenceID": 30, "context": "It has been shown that a natural \u201cmusical\u201d interpretation of these edits helps alleviate the problem of target similarity when edits are observed, noted in an earlier QBH feasibility study (Sorsa, 2001).", "startOffset": 189, "endOffset": 202}], "year": 2011, "abstractText": "We propose a model for errors in sung queries, a variant of the hidden Markov model (HMM). This is a solution to the problem of identifying the degree of similarity between a (typically error-laden) sung query and a potential target in a database of musical works, an important problem in the field of music information retrieval. Similarity metrics are a critical component of \u201cquery-by-humming\u201d (QBH) applications which search audio and multimedia databases for strong matches to oral queries. Our model comprehensively expresses the types of error or variation between target and query: cumulative and noncumulative local errors, transposition, tempo and tempo changes, insertions, deletions and modulation. The model is not only expressive, but automatically trainable, or able to learn and generalize from query examples. We present results of simulations, designed to assess the discriminatory potential of the model, and tests with real sung queries, to demonstrate relevance to real-world applications.", "creator": "dvips(k) 5.94a Copyright 2003 Radical Eye Software"}}}