{"id": "1112.4105", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2011", "title": "epsilon-Samples of Kernels", "abstract": "We study the worst case error of kernel density estimates via subset approximation. A kernel density estimate of a distribution is the convolution of that distribution with a fixed kernel (e.g. Gaussian kernel). Given a subset (i.e. a point set) of the input distribution, we can compare the kernel density estimates of the input distribution with the subset, and bound the worst case error. If the maximum error is eps, then this subset can be thought of as an eps-sample (aka an eps-approximation) of the range space defined with the input distribution as the ground set and the fixed kernel representing the family of ranges. Note in this case the ranges are not binary, but have a continuous range (for simplicity we mainly discuss kernels with range of [0,1]); these allow for smoother notions of range spaces, the same way that a kernel density estimates have improved upon histograms. It turns out, the use of this smoother family of range spaces has an added benefit of greatly decreasing the size required for eps-samples (in the plane) from O((1/eps^{4/3}) log^{2/3}(1/eps)) for disks to O((1/eps) sqrt{log (1/eps)}) for Gaussian kernels and for kernels with bounded slope that only affect a bounded domain.", "histories": [["v1", "Sun, 18 Dec 2011 01:19:25 GMT  (212kb,D)", "https://arxiv.org/abs/1112.4105v1", "14 pages, 3 figures"], ["v2", "Fri, 27 Jan 2012 05:35:25 GMT  (48kb,D)", "http://arxiv.org/abs/1112.4105v2", "13 pages, 1 figures. Corrected a mistake in previous version (no change to main results) and simplified some proofs"], ["v3", "Tue, 3 Apr 2012 22:46:53 GMT  (93kb,D)", "http://arxiv.org/abs/1112.4105v3", "13 pages, 2 figures. Cleaned up writing"]], "COMMENTS": "14 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CG cs.DS cs.LG", "authors": ["jeff m phillips"], "accepted": false, "id": "1112.4105"}, "pdf": {"name": "1112.4105.pdf", "metadata": {"source": "CRF", "title": "\u03b5-Samples for Kernels", "authors": ["Jeff M. Phillips"], "emails": ["jeffp@cs.utah.edu"], "sections": [{"heading": null, "text": "It turns out that the use of this smoother family of range spaces has an additional advantage, namely to greatly reduce the size required for \u03b5 samples. In the plane, for example, the size O (((1 / \u03b54 / 3) log2 / 3 (1 / \u03b5)) is for plates (based on arguments of the VC dimension), but only O ((((1 / \u03b5) \u221a log (1 / \u03b5)))) is for Gaussian nuclei and for nuclei with limited inclination, which affect only a limited domain. These limits are reached by investigating the discrepancy of these \"kernel\" range spaces, and here the improvement of the limits is even more pronounced. In the plane, we show that the discrepancy for these nuclei is O (\u221a log n), while for balls there is a lower limit (n1 / 4).ar Xiv: 111 2.41 05v3 [cs.CG] 3. Apr 201 2"}, {"heading": "1 Introduction", "text": "We examine the L-value of all functions K (x, \u00b7) represented by a fixed kernel K and an arbitrary center. We examine the L-value of all functions K (x, \u00b7 r). We examine the L-value of all functions K (x, \u00b7 r). We examine the L-value of all functions K (x, \u00b7 r). We examine the L-value of all functions K (x, \u00b7 r)."}, {"heading": "1.1 Our Results", "text": "Our most important structural result is an algorithm for constructing a small discrepancy (P, T) or P (P, P, P). (1) The algorithm is relatively simple; we construct a minimal cost fit of the points (minimizes the sum of distances), and for each pair of points in agreement we depend on one point + 1 and the other. (1) The ForP (n1 / 2) -Rd of size n, the above coloring has a discrepancy of at least 1 \u2212 O (n1 / 2 \u2212 2) and d2 (P, G) -1 \u2212 2 (n1 / 2 \u2212 2). The ForP-Rd-Rd of size n, which corresponds to the probability). This implies an efficient algorithm for constructing small samples of the core area. Theorem 1,2. For P-Rd, with a probability of at least 1 \u2212 2, we can construct a time in O (n / 2)."}, {"heading": "1.2 Motivation", "text": "Most reasonable ranges in R1 admit that most reasonable ranges in R1 have an \u03b5 sample of size 1 / \u03b5, simply by sorting the points and ignoring each \u03b5 point in sorted order. (However, near-linear results in higher dimensions are known only for ranges defined by axis-aligned rectangles (and other variants defined by fixed but not necessarily orthogonal axes).All results are based on VC dimensions and admit that superlinear polynomial spaces are added in 1 / \u03b5, with powers approaching 2 as d increments. And, of course, random samples only provide samples of size O (1 / 2).This polynomial distinction is quite important, since they are then 1 / 2 for small units (i.e. with a spacing of 0.001, which is important for summing up large datasets)."}, {"heading": "2 Preliminaries", "text": "For the sake of simplicity, we focus only on the rotation and displacement of invariant nuclei, so that K (pi, pj) = k (pi \u2212 pj) can be written as a function of the distance between their two arguments. Rotation invariant constraint can be easily eliminated, but would complicate the technical presentation. We also assume that the nuclei are scaled so that K (p, p) = k (0) = 1. Section 5 deals with the elimination of this assumption. We generalize the family of nuclei T (see above) to S\u03c3, which we call \"bounded\"; they have a slope limited by a constant x, q, p, p \u00b2 Rd."}, {"heading": "3 Min-Cost Matchings within Balls", "text": "We say that M (P) (or M, if the choice of P is clear) is a perfect match of P, if there is a set of n disordered pairs (q, p) of P, so each p-P is in exactly one pair. We define the cost of a perfect match in terms of the sum of distances: c (M) = 1, p-2, p-2, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-4, p-4, p-4, p-4, p-4, p-4, p-4, p, p-4, p, p-4, p, p-4, p, p-4, p, p-4, p-4, p, p-4, p-4, p, p-4, p-4, p-4, p-4, p, p-4, p-4, p-4, p-4, p-4, p-4, p-4, p-4, p-4, p-4, p-4, p, p-4, p-4, p-4, p, p-4, p-4, p-4, p, p-4, p-4, p, p-4, p-4, p-4, p-4, p, p-4, p-4, p-4, p-4, p, p-4, p-4, p-4, p-4, p, p-4, p-4, p-4, p-4, p, p-4, p-4, p-4, p, p-4, p-4, p-4, p, p-4, p-4, p-4, p, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p-3, p"}, {"heading": "4 Small Discrepancy for Kernel Range Spaces", "text": "The key finding is that we have the results of Section 3. Then, since each Xj is an independent random variable and we have the results of Section 3. We have the key finding that we have the results of Section 3. Then, since each Xj is a separate variable."}, {"heading": "4.1 From a Single Kernel to a Range Space, and to \u03b5-Samples", "text": "The above theorems imply a slight discrepancy (O (1) in R2) for any selection of K (E) or G (G), and specify a randomized algorithm to construct such a coloring that does not use information about the kernel. However, this does not yet imply a small discrepancy for all decisions made by K (S) or G (G) at the same time. To do this, we only need to consider a polynomial in n number of kernels, and then show that the discrepancy is limited for all of them. Note for binary ranges, this result is usually achieved by shifting to VC dimensions, where there is at most a subset of points that can be contained in a range. Unlike binary ranges, this approach does not work for ranges, since even if the same set of Px (P) for P (X) is not zero K (P) for P (P)."}, {"heading": "G, the Pr[d\u03c7(P,K) > \u03a8(n, d, \u03b4)] \u2264 \u03b4.", "text": "In order to convert this discrepancy algorithm into one for \u03b5 samples, we must successfully repeat it O (log n) times. Thus, in order to achieve a probability of success, we can set \u03b4 = \u03c6 / log n above and obtain a discrepancy of maximum \u221a 2 log (n\u03c6 log n) with probability 1 \u2212 \u03c6. This and [30] now implies theorem 1.2. We can specify the logical conclusion using Varadarajan's O (n1.5 log5 n) time algorithm [37] to calculate the minimum cost agreement in R2; the log factor in runtime can be expected time by better SSPD constructions [1] to O (n1.5 log2 n) or O (n1.5 log3 n) deterministic time.Sequence 4.1. For each specified point P-R2, for each class K of circular floor cores or Gaussian cores we can generate expected time in O (n1,5 log2 n) or Log2 (1 / 3 n) or Logis (1 / 3) time."}, {"heading": "5 Extensions", "text": "A common topic in kernel density estimates is setting the integral perspective under the kernel (usually at 1) and scaling the \"bandwidth.\" For a shift and rotation invariant kernel, where the default kernel k has bandwidth 1, a kernel with bandwidth w is written and defined (z) = (1 / wd) k (z / w).Our results do not apply to arbitrarily small bandwidths, as then a kernel (0) = 1 / wd becomes arbitrarily large as w shrinks; see Appendix A. However, W is fixed as a small constant, and consider all kernels KW extended from K to allow a bandwidth w. We can construct a sample of size O (1 / wd) 1 \u2212 1 / 2 / d."}, {"heading": "5.1 Future Directions", "text": "We believe that we should be able to make this algorithm deterministic by using iterative reweighting of the poly (n) cores in N\u03c4. We suspect in R2 that this would result in a discrepancy between O (log n) and an \u03b5 sample of the sizeO ((((1 / \u03b5) log (1 / \u03b5)). However, we are not sure that the boundaries in Rd for d > 2 will require superlinear results, but these are more difficult to analyze. Furthermore, we have not attempted to optimize or solve a constant. Perhaps a matching that directly minimizes the sum of lengths to depth or (d / 2) th power will yield better results, but these are more difficult to analyze. A version of Bern and Eppstein's result [8] with explicit constants is of interest."}, {"heading": "Acknowledgements", "text": "I am deeply grateful to Joel Spencer for discussing this issue and for leading me to other results that have helped to solve some early technical problems. He also provided proof (I think he discussed it with Prasad Tetali) through personal communication that if the match directly minimizes the sum of square lengths, the same results apply. Finally, I thank David Gamarnik for communicating the Berner-Eppstein result [8]."}, {"heading": "A Lower Bounds", "text": "We can show that in Rd for a constant d, a random sample of sizeO ((1 / \u03b52) sample (1 / \u03b52) sample (1 / \u03b52) sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample sample"}], "references": [{"title": "New constructions of sspds and their applications", "author": ["Mohammad A. Abam", "Sariel Har-Peled"], "venue": "Computational Geometry: Theory and Applications,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Deterministic algorithms for sampling count data", "author": ["H\u00fcseyin Akcan", "Alex Astashyn", "Herv\u00e9 Br\u00f6nnimann"], "venue": "Data & Knowledge Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "A non-linear lower bound for planar epsilon-nets", "author": ["Noga Alon"], "venue": "In Proceedings 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Scale-sensitive dimensions, uniform convergence, and learnability", "author": ["Noga Alon", "Shai Ben-David", "Nocol\u00f2 Cesa-Bianchi", "David Haussler"], "venue": "Journal of ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Small-size epsilon-nets for axis-parallel rectangles and boxes", "author": ["Boris Aronov", "Esther Ezra", "Micha Sharir"], "venue": "SIAM Journal of Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Constructive algorithms for discrepancy minimization", "author": ["Nikhil Bansal"], "venue": "In Proceedings 51st Annual IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Fat-shattering and the learnability of real-valued functions", "author": ["Peter L. Bartlett", "Philip M. Long", "Robert C. Williamson"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Worst-case bounds for subadditive geometric graphs", "author": ["Marshall Bern", "David Eppstein"], "venue": "Proceedings 9th ACM Symposium on Computational Geometry,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1993}, {"title": "Learnability and the Vapnik-Chervonenkis dimension", "author": ["Anselm Blumer", "A. Ehrenfeucht", "David Haussler", "Manfred K. Warmuth"], "venue": "Journal of ACM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1989}, {"title": "The Discrepancy Method", "author": ["Bernard Chazelle"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "The convergence of bird flocking", "author": ["Bernard Chazelle"], "venue": "In Proceedings 26th Annual Symposium on Computational Geometry,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "On linear-time deterministic algorithms for optimization problems in fixed dimensions", "author": ["Bernard Chazelle", "Jiri Matousek"], "venue": "J. Algorithms,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Super-samples from kernel hearding", "author": ["Yutian Chen", "Max Welling", "Alex Smola"], "venue": "In Conference on Uncertainty in Artificial Intellegence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Structure-aware sampling: Flexible and accurate summarization", "author": ["Edith Cohen", "Graham Cormode", "Nick Duffield"], "venue": "In Proceedings 37th International Conference on Very Large Data Bases,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Nonparametric Density Estimation: The L1 View", "author": ["Luc Devroye", "L\u00e1szl\u00f3 Gy\u00f6rfi"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1984}, {"title": "Combinatorial Methods in Density Estimation", "author": ["Luc Devroye", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Paths, trees, and flowers", "author": ["Jack Edmonds"], "venue": "Canadian Journal of Mathematics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1965}, {"title": "A note about weak epsilon-nets for axis-parallel boxes in d-space", "author": ["Esther Ezra"], "venue": "Information Processing Letters,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "epsilon-nets and simplex range queries", "author": ["David Haussler", "Emo Welzl"], "venue": "Disc. & Comp. Geom.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1987}, {"title": "Comparing distributions and shapes using the kernel distance", "author": ["Sarang Joshi", "Raj Varma Kommaraju", "Jeff M. Phillips", "Suresh Venkatasubramanian"], "venue": "In 27th Annual Symposium on Computational Geometry,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Efficient distribution-free learning of probabilistic concepts", "author": ["Michael Kerns", "Robert E. Shapire"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "Almost tight bounds for epsilon nets", "author": ["J. Koml\u00f3s", "J\u00e1nos Pach", "Gerhard Woeginger"], "venue": "Discrete and Computational Geometry,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1992}, {"title": "Improved bounds on the samples complexity of learning", "author": ["Yi Li", "Philip M. Long", "Aravind Srinivasan"], "venue": "J. Comp. and Sys. Sci.,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Constructive discrepancy minimization by walking on the edges", "author": ["Shachar Lovett", "Raghu Meka"], "venue": "Technical report,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Geometric Discrepancy; An Illustrated", "author": ["Jiri Matousek"], "venue": "Guide. Springer,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "How to net a lot with a little: Small \u03b5-nets for disks and halfspaces", "author": ["Jiri Matou\u0161ek", "Raimund Seidel", "Emo Welzl"], "venue": "In Proceedings 6th Annual ACM Symposium on Computational Geometry,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1990}, {"title": "The probabilistic method (lecture notes)", "author": ["Jir\u0159i Matou\u0161ek", "Jan Vondrak"], "venue": "http://kam.mff. cuni.cz/ \u0303matousek/prob-ln.ps.gz,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Tight lower bounds for the size of epsilon-nets", "author": ["J\u00e1nos Pach", "G\u00e1bor Tardos"], "venue": "In Proceedings 27th Annual Symposium on Computational Geometry,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Algorithms for \u03b5-approximations of terrains", "author": ["Jeff M. Phillips"], "venue": "In ICALP,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "The VCdimension of queries and selectivity estimation through sampling", "author": ["Matteo Riondato", "Mert Akdere", "Ugur Cetintemel", "Stanley B. Zdonik", "Eli Upfal"], "venue": "Technical report,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Multivariate Density Estimation: Theory, Practice, and Visualization", "author": ["David W. Scott"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1992}, {"title": "Density Estimation for Statistics and Data Analysis", "author": ["Bernard W. Silverman"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1986}, {"title": "Range counting over multidimensional data streams", "author": ["S. Suri", "C. Toth", "Y. Zhou"], "venue": "Discrete and Computational Geometry,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Inductive principles of the search for emperical dependencies", "author": ["Vladimir Vapnik"], "venue": "In Proceedings of the Second Annual Workshop on Computational Learning Theory,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1989}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["Vladimir Vapnik", "Alexey Chervonenkis"], "venue": "The. of Prob. App.,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1971}, {"title": "A divide-and-conquer algorithm for min-cost perfect matching in the plane", "author": ["Kasturi R. Varadarajan"], "venue": "In Proceedings 39th IEEE Symposium on Foundations of Computer Science,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Interestingly, in this case the ranges are not binary, but have a continuous range (for simplicity we focus on kernels with range of [0, 1]); these allow for smoother notions of range spaces.", "startOffset": 133, "endOffset": 139}, {"referenceID": 0, "context": "Thus we can re-imagine a kernel range space (P,K) as the family of fractional subsets of P , that is, each p \u2208 P does not need to be completely in (1) or not in (0) a range, but can be fractionally in a range described by a value in [0, 1].", "startOffset": 233, "endOffset": 239}, {"referenceID": 24, "context": "See Matou\u015bek\u2019s [26] and Chazelle\u2019s [10] books for a masterful treatments of this field when restricted to combinatorial discrepancy.", "startOffset": 15, "endOffset": 19}, {"referenceID": 9, "context": "See Matou\u015bek\u2019s [26] and Chazelle\u2019s [10] books for a masterful treatments of this field when restricted to combinatorial discrepancy.", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": "[21] showed that for any kernel range space (P,K) where all super-level sets of kernels are described by elements of a binary range space (P,A), then an \u03b5-sample of (P,A) is also an \u03b5-sample of (P,K).", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2, 30, 14, 31]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 28, "context": "[2, 30, 14, 31]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 13, "context": "[2, 30, 14, 31]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 29, "context": "[2, 30, 14, 31]).", "startOffset": 0, "endOffset": 15}, {"referenceID": 34, "context": "For binary range spaces with constant VCdimension [36] a random sample S of size O((1/\u03b52) log(1/\u03b4)) provides an \u03b5-sample with probability at least 1 \u2212 \u03b4 [24].", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "For binary range spaces with constant VCdimension [36] a random sample S of size O((1/\u03b52) log(1/\u03b4)) provides an \u03b5-sample with probability at least 1 \u2212 \u03b4 [24].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "Better bounds can be achieved through deterministic approaches as outlined by Chazelle and Matousek [12], or see either of their books for more details [10, 26].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "Better bounds can be achieved through deterministic approaches as outlined by Chazelle and Matousek [12], or see either of their books for more details [10, 26].", "startOffset": 152, "endOffset": 160}, {"referenceID": 24, "context": "Better bounds can be achieved through deterministic approaches as outlined by Chazelle and Matousek [12], or see either of their books for more details [10, 26].", "startOffset": 152, "endOffset": 160}, {"referenceID": 28, "context": "As spelled out explicitly by Phillips [30] (see also [26, 10] for more classic references), for a range space (P,A) with discrepancy O(log |P |) (resp.", "startOffset": 38, "endOffset": 42}, {"referenceID": 24, "context": "As spelled out explicitly by Phillips [30] (see also [26, 10] for more classic references), for a range space (P,A) with discrepancy O(log |P |) (resp.", "startOffset": 53, "endOffset": 61}, {"referenceID": 9, "context": "As spelled out explicitly by Phillips [30] (see also [26, 10] for more classic references), for a range space (P,A) with discrepancy O(log |P |) (resp.", "startOffset": 53, "endOffset": 61}, {"referenceID": 5, "context": "But, recently Bansal [6] provided a randomized constructive algorithm; also see a similar, simpler and more explicit, approach recently on the arXiv [25].", "startOffset": 21, "endOffset": 24}, {"referenceID": 23, "context": "But, recently Bansal [6] provided a randomized constructive algorithm; also see a similar, simpler and more explicit, approach recently on the arXiv [25].", "startOffset": 149, "endOffset": 153}, {"referenceID": 28, "context": "1, the above stated results in [30], and Edmond\u2019s O(n3) time algorithm for min-cost matching M [18].", "startOffset": 31, "endOffset": 35}, {"referenceID": 16, "context": "1, the above stated results in [30], and Edmond\u2019s O(n3) time algorithm for min-cost matching M [18].", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": "This follows (after some technical details) from a result of Bern and Eppstein [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 32, "context": "[34, 30]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 28, "context": "[34, 30]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 5, "context": "Furthermore, most techniques other than random sampling (size 1/\u03b52) are quite complicated and rarely implemented (many require Bansal\u2019s recent result [6] or its simplification [25]).", "startOffset": 150, "endOffset": 153}, {"referenceID": 23, "context": "Furthermore, most techniques other than random sampling (size 1/\u03b52) are quite complicated and rarely implemented (many require Bansal\u2019s recent result [6] or its simplification [25]).", "startOffset": 176, "endOffset": 180}, {"referenceID": 18, "context": "In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases.", "startOffset": 34, "endOffset": 56}, {"referenceID": 8, "context": "In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases.", "startOffset": 34, "endOffset": 56}, {"referenceID": 25, "context": "In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases.", "startOffset": 34, "endOffset": 56}, {"referenceID": 21, "context": "In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases.", "startOffset": 34, "endOffset": 56}, {"referenceID": 4, "context": "In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases.", "startOffset": 34, "endOffset": 56}, {"referenceID": 17, "context": "In that context, a series of work [20, 9, 27, 23, 5, 19] has shown that size bound of O((1/\u03b5) log(1/\u03b5)) based on VC-dimension can be improved toO((1/\u03b5) log log(1/\u03b5)) or better in some cases.", "startOffset": 34, "endOffset": 56}, {"referenceID": 2, "context": "Super-linear lower bounds are known as well [3, 29].", "startOffset": 44, "endOffset": 51}, {"referenceID": 27, "context": "Super-linear lower bounds are known as well [3, 29].", "startOffset": 44, "endOffset": 51}, {"referenceID": 31, "context": "Highlights are the books of Silverman [33] on L2 error in R1, Scott [32] one L2 error in Rd, and books by Devroye, Gy\u00f6rfi, and/or Lugosi [15, 17, 16] on L1 error.", "startOffset": 38, "endOffset": 42}, {"referenceID": 30, "context": "Highlights are the books of Silverman [33] on L2 error in R1, Scott [32] one L2 error in Rd, and books by Devroye, Gy\u00f6rfi, and/or Lugosi [15, 17, 16] on L1 error.", "startOffset": 68, "endOffset": 72}, {"referenceID": 14, "context": "Highlights are the books of Silverman [33] on L2 error in R1, Scott [32] one L2 error in Rd, and books by Devroye, Gy\u00f6rfi, and/or Lugosi [15, 17, 16] on L1 error.", "startOffset": 137, "endOffset": 149}, {"referenceID": 15, "context": "Highlights are the books of Silverman [33] on L2 error in R1, Scott [32] one L2 error in Rd, and books by Devroye, Gy\u00f6rfi, and/or Lugosi [15, 17, 16] on L1 error.", "startOffset": 137, "endOffset": 149}, {"referenceID": 12, "context": "Recently Chen, Welling, and Smola [13] showed that for any positive definite kernel (including G, but not", "startOffset": 34, "endOffset": 38}, {"referenceID": 19, "context": "Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling).", "startOffset": 54, "endOffset": 58}, {"referenceID": 20, "context": "Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling).", "startOffset": 119, "endOffset": 123}, {"referenceID": 6, "context": "Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling).", "startOffset": 165, "endOffset": 179}, {"referenceID": 3, "context": "Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling).", "startOffset": 165, "endOffset": 179}, {"referenceID": 33, "context": "Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling).", "startOffset": 165, "endOffset": 179}, {"referenceID": 19, "context": "Kernel range spaces were defined in a paper last year [21] (although similar concepts such as fat-shattering dimension [22] appear in the learning theory literature [7, 16, 4, 35], their results were not as strong as [21] requiring larger subsets S, and they focus on random sampling).", "startOffset": 217, "endOffset": 221}, {"referenceID": 10, "context": "This raises several questions: are these binary range spaces which require size super-linear in 1/\u03b5 really necessary for downstream analysis? Can we simplify many analyses by using kernels in place of binary ranges? One possible target are the quite fascinating, but enormous bounds for bird-flocking [11].", "startOffset": 301, "endOffset": 305}, {"referenceID": 7, "context": "In fact, Bern and Eppstein [8] perform just such an analysis considering the minimum cost matching of points within [0, 1]d.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "In fact, Bern and Eppstein [8] perform just such an analysis considering the minimum cost matching of points within [0, 1]d.", "startOffset": 116, "endOffset": 122}, {"referenceID": 0, "context": "To apply this result of Bern and Eppstein, we can consider a ball of radius 1/2 that fits inside of [0, 1]d by scaling down all lengths uniformly by 1/2rad(B).", "startOffset": 100, "endOffset": 106}, {"referenceID": 0, "context": "Now the sum of dth power of edge lengths where both endpoints are within B is at most O(1) since these points are also within [0, 1]d.", "startOffset": 126, "endOffset": 132}, {"referenceID": 28, "context": "Now this and [30] implies Theorem 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "5 log n) time algorithm [37] for computing the min-cost matching in R2; the log factor in the runtime can be improved using better SSPD constructions [1] toO(n1.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "5 log n) time algorithm [37] for computing the min-cost matching in R2; the log factor in the runtime can be improved using better SSPD constructions [1] toO(n1.", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "A version of Bern and Eppstein\u2019s result [8] with explicit constants would be of some interest.", "startOffset": 40, "endOffset": 43}, {"referenceID": 14, "context": "This can include kernels which may be negative (such as the sinc or trapezoidal kernels [15, 17]) and have nice L2 KDE approximation properties.", "startOffset": 88, "endOffset": 96}, {"referenceID": 15, "context": "This can include kernels which may be negative (such as the sinc or trapezoidal kernels [15, 17]) and have nice L2 KDE approximation properties.", "startOffset": 88, "endOffset": 96}, {"referenceID": 19, "context": "These sometimes-negative kernels cannot use the \u03b5-sample result from [21] because their superlevel sets have unbounded VC-dimension since k(z) = 0 for infinitely many disjoint values of z.", "startOffset": 69, "endOffset": 73}, {"referenceID": 16, "context": "Edmond\u2019s min-cost matching algorithm [18] runs inO(n3) time in Rd and Varadarajan\u2019s improvement [37] runs in O(n1.", "startOffset": 37, "endOffset": 41}, {"referenceID": 35, "context": "Edmond\u2019s min-cost matching algorithm [18] runs inO(n3) time in Rd and Varadarajan\u2019s improvement [37] runs in O(n1.", "startOffset": 96, "endOffset": 100}, {"referenceID": 0, "context": "5 log n) [1]).", "startOffset": 9, "endOffset": 12}, {"referenceID": 7, "context": "Finally, I thank David Gamarnik for communicating the Bern-Eppstein result [8].", "startOffset": 75, "endOffset": 78}], "year": 2012, "abstractText": "We study the worst case error of kernel density estimates via subset approximation. A kernel density estimate of a distribution is the convolution of that distribution with a fixed kernel (e.g. Gaussian kernel). Given a subset (i.e. a point set) of the input distribution, we can compare the kernel density estimates of the input distribution with that of the subset and bound the worst case error. If the maximum error is \u03b5, then this subset can be thought of as an \u03b5-sample (aka an \u03b5-approximation) of the range space defined with the input distribution as the ground set and the fixed kernel representing the family of ranges. Interestingly, in this case the ranges are not binary, but have a continuous range (for simplicity we focus on kernels with range of [0, 1]); these allow for smoother notions of range spaces. It turns out, the use of this smoother family of range spaces has an added benefit of greatly decreasing the size required for \u03b5-samples. For instance, in the plane the size is O((1/\u03b5) log(1/\u03b5)) for disks (based on VC-dimension arguments) but is only O((1/\u03b5) \u221a log(1/\u03b5)) for Gaussian kernels and for kernels with bounded slope that only affect a bounded domain. These bounds are accomplished by studying the discrepancy of these \u201ckernel\u201d range spaces, and here the improvement in bounds are even more pronounced. In the plane, we show the discrepancy is O( \u221a log n) for these kernels, whereas for balls there is a lower bound of \u03a9(n). ar X iv :1 11 2. 41 05 v3 [ cs .C G ] 3 A pr 2 01 2", "creator": "LaTeX with hyperref package"}}}