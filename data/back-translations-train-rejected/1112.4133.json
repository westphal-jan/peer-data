{"id": "1112.4133", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Dec-2011", "title": "Evaluation of Performance Measures for Classifiers Comparison", "abstract": "The selection of the best classification algorithm for a given dataset is a very widespread problem, occuring each time one has to choose a classifier to solve a real-world problem. It is also a complex task with many important methodological decisions to make. Among those, one of the most crucial is the choice of an appropriate measure in order to properly assess the classification performance and rank the algorithms. In this article, we focus on this specific task. We present the most popular measures and compare their behavior through discrimination plots. We then discuss their properties from a more theoretical perspective. It turns out several of them are equivalent for classifiers comparison purposes. Futhermore. they can also lead to interpretation problems. Among the numerous measures proposed over the years, it appears that the classical overall success rate and marginal rates are the more suitable for classifier comparison task.", "histories": [["v1", "Sun, 18 Dec 2011 08:02:49 GMT  (445kb)", "http://arxiv.org/abs/1112.4133v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vincent labatut", "hocine cherifi"], "accepted": false, "id": "1112.4133"}, "pdf": {"name": "1112.4133.pdf", "metadata": {"source": "CRF", "title": "EVALUATION OF PERFORMANCE MEASURES FOR CLASSIFIERS COMPARISON", "authors": ["Vincent Labatut", "Hocine Cherifi"], "emails": ["vlabatut@gsu.edu.tr", "hocine.cherifi@u-bourgogne.fr"], "sections": [{"heading": null, "text": "Selecting the best classification algorithm for a particular data set is a common problem that occurs every time you have to select a classifier to solve a real problem. It is also a complex task with many important methodological decisions to be made. One of the most important is the selection of a suitable measure to properly evaluate the classification performance and classify the algorithms. In this article, we focus on this specific task. We present the most popular measures and compare their behavior using discrimination charts. Afterwards, we discuss their properties from a more theoretical perspective. It turns out that several of them are equivalent for classifier comparison purposes, and they can also lead to interpretation problems. Among the numerous measures that have been proposed over the years, it seems that the classical overall success rate and marginal rates are best suited for classifier comparison purposes."}, {"heading": "1 INTRODUCTION", "text": "The question that arises is whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is, whether it is a question, whether it is a question, whether it is, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is a question, whether it is, whether it is a question, whether it is a question, whether it is a question, whether it is, whether it is a question, whether it is a question, whether it is a question, whether it is, whether it is a question, whether it is a question, whether it is a question, whether it is a question,"}, {"heading": "2 RELATED WORKS", "text": "It is indeed the case that we are able to go in search of a solution that enables us, enables us to put ourselves in the position we are in."}, {"heading": "3 NOTATIONS AND TERMINOLOGY", "text": "Consider the problem of estimating the k-classes for a test set with n instances. Trueclas are quoted iC, while the estimated classes as defined by the considered classifier are notediC (ki1). The proportion of instances belonging to the class iC in the data set is noted i. Most measurements are not processed directly by the raw classifier outputs, but by the confusion matrix constructed from these results. This matrix represents how the instances are distributed over estimated (rows) and true (columns) classes, corresponding to the proportion of instances estimated by the classifier in class i (i.e. iC) if they actually belong to class j (i.e.jC). Consequently, diagonal terms (ji) correspond to correctly classified instances, whereas positive (ji) positives (diagonal terms) i.i.i.i.i.the matrix leads the authors to a true class and transponix."}, {"heading": "4 SELECTED MEASURES", "text": "In this section, we formally describe the most widely used metrics for comparing classifiers in the literature. These include association metrics, various metrics based on marginal rates of the confusion matrix, and randomly corrected coefficients of agreement. Since these can be described using many characteristics, there are as many typologies as authors. In this article, we mainly address class-specific metrics, i.e. metrics designed to assess the accuracy of a single class, and metrics capable of assessing general classification accuracy."}, {"heading": "4.1 Nominal Association Measures", "text": "An association variable is a numerical index, a single number that describes the strength or magnitude of a relationship. Many association variables have been used to evaluate classification accuracy, such as: chi-square-based metrics (coefficient, Pearson's C, Cramer's V, etc. [2]), Yule coefficients, Matthew's correlation coefficient, proportional reduction of error variables (Goodman & Kruskals and Part'Suncertainty coefficient, etc.), mutual information-based metrics [19], etc. Association variables quantify how predictable one variable is when known to the other. They have been applied to classification accuracy assessment by applying these variables taking into account the distribution of instances over true and estimated classes. In our context, we consider the distribution of instances over estimated classes and want to measure how much similar their distribution over true classes is, since the association problem is reversed by both."}, {"heading": "4.2 Overall Success Rate", "text": "Surely the most popular measure of classification accuracy [20], the overall success rate is defined as the trace of the confusion matrix: ki iipOSR 1 (1) This measurement is multiclassical, symmetrical and ranges from 0 (perfect misclassification) to 1 (perfect classification).Its popularity is certainly based on its simplicity, not only in terms of processing, but also on interpretation, since it corresponds to the observed proportion of correctly classified cases."}, {"heading": "4.3 Marginal Rates", "text": "We collect a number of widely used asymmetric class specifications under the term Limit Rates. The TP rate and the TN rate are both reference-oriented, i.e. they look at the columns of the confusion matrix (true classes). The former is also called sensitivity [20], the accuracy of the manufacturer [11], and the asymmetric index of Dice [21]. The latter is alternatively called specificity [20].FNTPTPi pppTPR (2) FPTNTNi pppTNR (3). The estimation-oriented measures focusing on the rows of the confusion matrix (estimated classes) are the Positive Predisposition Value (PPV) and the Negative Predisposition Value (NPV) [20]. The former are also called precision [20], the accuracy of the user [11], and the association index of Dice [21].FPTPPi (PPR) are actually classified as PP4 and P.PPR (P.PV)."}, {"heading": "4.4 F-measure and Jaccard Coefficient", "text": "s coefficient of similarity [24], cube coefficient [21], and Hellden's mean accuracy index [25]: FPFNTPTPiiii ippTPRPPTPRPPPPPV F22 2 (6). It can be interpreted as a measure of overlap between the true and estimated classes (other instances, i.e. TN, are ignored) ranging from 0 (no overlap at all) to 1 (complete overlap). Originally, the measure known as the Jaccard coefficient of the community was also defined to compare sentences [4]. It is a class-specific symmetrical measure defined as: FNFPTPTPi ppJCC (7)."}, {"heading": "4.5 Classification Success Index", "text": "The Individual Classification Success Index (ICSI) is a class-specific symmetric measure defined for the purpose of classification evaluation [1]: 1111iiiiiTPRPPVTPRPPVICSI (8) The terms iPPV1 and iTPR1 correspond to the proportion of type I and II errors for the class under consideration, respectively. ICSI is thus one minus the sum of these errors. It ranges from -1 (both errors are maximum, i.e. 1) to 1 (both errors are minimal, i.e. 0), but the value 0 has no clear meaning. The measure is symmetrically and linearly linked to the arithmetic mean of TPR and PPV, which is itself referred to as Kulczynski's measure [28]. The Classification Success Index (CSI) is a comprehensive measure defined simply by the averaging of ICSI across all classes [1]."}, {"heading": "4.6 Agreement Coefficients", "text": "It is based on the following general formula: eeo PPPA 1 (9) Where oP and eP are the observed or expected matches. The idea is to consider the observed match as the result of an intended match and a random match. To obtain the intended match, one must estimate the random match and remove it from the observed one. [3] Most authors use OSRPo, but disagree on how the random match should be formally defined, leading to different estimates of the eP. Forhis popular kappa coefficient (CKC). Cohen used the product of the confusion matrix marginal proportions [3]: iiie ppP. Scott's picoefficient (SPC) is instead based on the class proportions measured on the entire dataset (or itsestimulation)."}, {"heading": "4.7 Ground Truth Index", "text": "T\u00fcrk's Ground Truth Index (GTI) is another randomly corrected measure, but it has been specifically defined to evaluate classification accuracy [18]. T\u00fcrk assumes that the classifier has two components: one is always correct, and the others classify randomly. For a given class jC, the proportionality of instances is determined by the infallible classifier, and thus in jC. The remaining instances (i.e. the confusion matrix jb 1) are distributed by the classifier across all estimated classes (including jC) with a probability tsia for iC. Other words according to this model can be written as a product of the form jiij bap (ji), which corresponds to the hypothesis of quasi independence of Goodman's non-diagonals, whose iterative proportional adjustment method is stimulating."}, {"heading": "5 CASE STUDIES", "text": "In this section, we will discuss the results of some confusion matrices to analyze the properties and behavior of the measures reviewed in the previous section. First, we will look at extreme cases, i.e. perfect classification and misclassification, followed by a more realistic confusion matrix with some classification errors."}, {"heading": "5.1 Extreme Cases", "text": "All measurements agree to consider diagonal confusion matrices such as those shown in Table 4 as the result of a perfect classification. In this case, the classifier assigns each instance to its true class and the measurement variable reaches its maximum value. A perfect misclassification corresponds to a matrix whose track is zero, as in Tables 2 and 5. In this case, the measurements vary. Their behavior depends on how they view the distribution of errors across the non-diagonal cells. OSR is not sensitive to this distribution, since only the trace of the confusion matrix is taken into account. TPR, PPV, JCC and F measurements are also not affected, since no TP automatically causes these measurements to have a zero value. Consequently, CSI always achieves its minimum value, as it depends directly on TPR and PPV, and so is ICSI. Random corrected measurements are not affected by the random coincidence model on which they are based."}, {"heading": "5.2 Intermediate Cases", "text": "iSe rf\u00fc ide rf\u00fc ide rf\u00fc the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green. \"iSe rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc the green for the green for the green for the green"}, {"heading": "6 SENSITIVITY TO MATRIX CHANGES", "text": "Since metrics may differ from classifiers, we will now focus on the nature of this discrepancy. We will examine three points that are likely to affect the accuracy metrics: the error distribution in the classifier, the class proportions of the datasets, and the number of classes."}, {"heading": "6.1 Methods", "text": "This year, the time has come for us to be able to find a solution that is able to find a solution, that is able to find a solution, and that is able to find a solution that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution, that is able to find a solution. \""}, {"heading": "6.2 Error Distribution Sensitivity", "text": "We generated matrices for 3 balanced classes (0p) with the above described methodology. Figure 1 and 2 show the discrimination lines for class-specific and multicultural actions, each with the exception of TPR, all discrimination lines are xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"}, {"heading": "6.3 Class Proportions Distribution Sensitivity", "text": "Roughly speaking, we observe two different types of behavior: Measures are either sensitive or insensitive to deviations in the distribution of class proportions. In the first case, the discrimination lines are identical for the different values of p. In the second case, increasing the imbalance leads to lines on the left side of the 0p line. The stronger the imbalance and the more the line is on the left, the more similar the two rows of matrices are. Fig.3 is a typical example of this behavior. It represents the results of the F measure applied to classes 1 and 3. Note that JCC and the F measurements have similar discrimination lines. CKC (below), with 3k. Unlike the F measurements, measurements that combine two limit rates (ICSI, JCC), are sensitive to classes 1 and F measurements."}, {"heading": "6.4 Class Number Sensitivity", "text": "Finally, we will focus on the effect of the number of classes on the multi-class dimensions. Figure 5 shows the results for OSR applied to matrices with a size change of 3 to 10, for balanced (0p) and dimbalanced (1p) cases. All measurements follow the same behavior. An increase in the number of classes strengthens the preference over the y-series matrices. In other words, more classes give greater importance to the additional errors contained in the x-series matrices. The effect is stronger for the unbalanced matrices. In this case, most instances are in the first class, which is the only one that is similar between the two models, so that their dilution has a greater influence on the accuracy measured."}, {"heading": "7 DISCUSSION", "text": "As shown in the preceding sections, the metrics differ in the way they differentiate between different classifiers, but apart from this important aspect, they must also be compared on the basis of several theoretical features."}, {"heading": "7.1 Class Focus", "text": "As illustrated in the previous sections, a measurement variable can evaluate accuracy for a particular class or across all classes. The former is adapted to situations in which one is interested in a particular class or perform a class-specific analysis of the classification outcomes. It is possible to define a general measurement by combining class-specific measurements for all classes, for example by averaging them, as in the CSI. However, even if the class-specific measurement under consideration has a clear meaning [2], it is difficult to give a simple interpretation of the resulting total measurement, except for the combination of class-specific values. Conversely, it is possible to use a general measurement to evaluate a particular class accuracy by merging all classes except the one considered [2]. In this case, however, the interpretation is simple and depends directly on the overall measurement. In general terms, a class-specific measurement is used to distinguish classes in terms of their importance. This is not possible because they only allow all classes to determine weights equally as a complex measurement (but only one)."}, {"heading": "7.2 Functional Relationships", "text": "It is interesting to note that different combinations of two quantities can be sorted by increasing order, regardless of the quantities considered: minimum, harmonic mean, geometric mean, arithmetic mean, square mean, maximum [32]. If the quantities belong to 1; 0, we can put your product at the top of the previous list as the smallest combination. If we look at the quantities presented, this means that combinations of the same limits have a predefined order for a certain classifier. For example, the sensitive precision product will always be smaller than the F (harmonic mean), which in turn will always be smaller than Kulczynski's measure (arithmetic mean). In addition to these combinations of TPR and PPV, this also applies to various quantities corresponding to combinations of TPR and TNR, which are not presented here because they are not very popular."}, {"heading": "7.3 Range", "text": "In the classification context, two extreme situations can be considered: the perfect classification (i.e. diagonal confusion matrix) and the complete misclassification (i.e. all diagonal elements are zeros): the former should be associated with the upper limit of the measure of accuracy, and the latter with its lower limit. Measuring limits can either be set or depend on the data processed. The former is generally considered a favorable property because it allows the comparison of values measured on different sets of data without having to normalize them for scale issues. Furthermore, the presence of fixed limits facilitates an absolute interpretation of the measured characteristics. In our case, we want to compare classifiers evaluated on the same data. Furthermore, we are interested in their relative accuracy, i.e. we focus only on their relative differences. Consequently, this feature is not necessary, but it turns out that most authors have normalized their measurements to give them fixed limits (in rule 1; or 1)."}, {"heading": "7.4 Interpretation", "text": "Our goal is to compare classifiers on the basis of a given set of data for which we only need the measured accuracy. In other words, numerical values are sufficient to determine which classifier is the best on the data under consideration. However, determining the best classifier is useless if we do not know the criteria underlying this discrimination, i.e. if we are unable to interpret the measurement variable. For example, the best classification in terms of PPV or TPR has a completely different meaning, since these measurements focus on Type I and Type II errors. Among the measurements used in the literature to evaluate the accuracy of classifiers, some are analytically designed to have a clear interpretation (e.g. Jaccard's coefficient [4]). Sometimes, this interpretation is called into question, or there are various alternatives that lead to multiple related measurements (e.g. compatibility coefficients). In some other cases, the measurement is an AHO construct, or the direct observation can be described as a CSR."}, {"heading": "7.5 Correction for Chance", "text": "The correction of random variables is still an open debate. Firstly, the authors do not agree on the necessity of this correction, depending on the context of application [7, 27]. In our case, we want to apply the accuracy measured in a sample to the population as a whole. In other words, we want to distinguish the proportion of success that the algorithm can reproduce on the basis of different data from the assumptions made on the sample, so that this correction appears necessary. Secondly, the authors disagree on the nature of the concept of correction, as illustrated in our description of the coefficients of agreement. We can distinguish between two types of corrections: those that depend only on the true class distribution (e.g. Scott and Maxwell) and those that also depend on the estimated class distribution (e.g. Cohen and T\u00fcrk). The former are of little practical interest to us, since such a measure is linearly related to the OSR (the correction value for each tested algorithm is therefore identical to the order of algorithms)."}, {"heading": "8 CONCLUSION", "text": "In this paper, we reviewed the most important measures for evaluating accuracy, from a specific classification perspective. We look at the case of comparing different classification algorithms by testing them against a given sample of data to determine which will be best for the sampled population. We first reviewed and described the most common measures and introduced the concept of discrimination to compare their behavior in the context of our specific situation. We looked at three factors: changes in the error rate, in class proportions, and in the number of classes. As expected, most measures have an adequate way to handle the error factor, although there are some similarities between them. The effect of the other factors is more homogeneous: the reduction in the number of classes and / or the increase in their imbalance tend to reduce the importance of the error level for all measures. We then compared the measure from a theoretical point of view against which it is measured."}, {"heading": "9 REFERENCES", "text": "[1] S. Koukoulas and G. A. Blackburn: Introducingnew indices for accuracy, Evaluation of classified images represent semi-natural woodland environments, Photogramm Eng Rem S, vol. 67, pp. 499-510 (2001). [2] L. Goodman and W. H. Kruskal: Measuresof Association for Cross Classification, J Am Stat Assoc, pp. 37-46 (1960). [4] P. Jaccard: The distribution of the flora in thealpine zone, New Phytol. 11, pp. 37-50 (1912). G. M. Foody: Status of land cover classificationaccuracy assessment, Remote Sens Environ, vol. 80, pp. 185-201 (2002)."}], "references": [{"title": "Introducing new indices for accuracy evaluation of classified images representing semi-natural woodland environments", "author": ["S. Koukoulas", "G.A. Blackburn"], "venue": "Photogramm Eng Rem S,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Measures of Association for Cross Classification", "author": ["L.A. Goodman", "W.H. Kruskal"], "venue": "J Am Stat Assoc,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1954}, {"title": "A Coefficient of Agreement for Nominal Scales", "author": ["J. Cohen"], "venue": "Educ Psychol Meas,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1960}, {"title": "The distribution of the flora in the alpine zone", "author": ["P. Jaccard"], "venue": "New Phytol,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1912}, {"title": "Status of land cover classification accuracy assessment", "author": ["G.M. Foody"], "venue": "Remote Sens Environ,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "A systematic analysis of performance measures for classification tasks, Information", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Processing  Management,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Comparing thematic maps based on map value", "author": ["S.V. Stehman"], "venue": "Int J Remote Sens,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Selecting and interpreting measures of thematic classification accuracy", "author": ["S.V. Stehman"], "venue": "Remote Sens Environ,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "How Reliable Are Chance-Corrected Measures of Agreement", "author": ["I. Guggenmoos-Holzmann"], "venue": "Stat Med,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Accuracy Measures for the Comparison of Classifiers, in International Conference on Information Technology", "author": ["V. Labatut", "H. Cherifi"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Congalton: A Review of Assessing the Accuracy of Classifications of Remotely Sensed Data", "author": ["G. R"], "venue": "Remote Sens Environ,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1991}, {"title": "AUC: a statistically consistent and more discriminating measure than accuracy", "author": ["C.X. Ling", "J. Huang", "H. Zhang"], "venue": "in 18th International Conference on Artificial Intelligence", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "The geometry of ROC space: understanding machine learning metrics through ROC isometrics", "author": ["P.A. Flach"], "venue": "Twentieth International Conference on Machine Learning (ICML) Washington DC", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "On Similarity Indices and Correction for Chance Agreement", "author": ["A.N. Albatineh", "M. Niewiadomska-Bugaj", "D. Mihalko"], "venue": "J Classif,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Data Mining in Metric Space: An Empirical Analysis of Supervised Learning Performance Criteria, in International Conference on  Knowledge Discovery and Data Mining Seattle, US-WA", "author": ["R. Caruana", "A. Niculescu-Mizil"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Comparative assessment of the measures of thematic classification accuracy", "author": ["C.R. Liu", "P. Frazier", "L. Kumar"], "venue": "Remote Sens Environ,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Modroiu: An experimental comparison of performance measures for classification", "author": ["C. Ferri", "J. Hern\u00e1ndez-Orallo"], "venue": "Pattern Recognition Letters,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "index: A measure of the success of predictions", "author": ["GT G. T\u00fcrk"], "venue": "Remote Sens Environ,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1979}, {"title": "Use of the average mutual information index in evaluating classification error and consistency", "author": ["J.T. Finn"], "venue": "Int J Geogr Inf Syst,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "Data Mining: Practical Machine Learning", "author": ["I.H. Witten", "E. Frank"], "venue": "Tools and Techniques,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Dice: Measures of the amount of ecologic association between species, Ecology", "author": ["R. L"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1945}, {"title": "Visual scene classification for image and video home content, in International Workshop on Content- Based Multimedia Indexing (CBMI), pp", "author": ["P. Villegas", "E. Bru", "B. Mayayo", "L. Carpio", "E. Alonso", "V.J. Ruiz"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Modelling the False Alarm and Missed Detection Rate for Electronic Watermarks", "author": ["J.-P. Linnartz", "T. Kalker", "G. Depovere"], "venue": "Lecture Notes in Computer Science,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons", "author": ["T. S\u00f8rensen"], "venue": "Biologiske Skrifter / Kongelige Danske Videnskabernes Selskab,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1948}, {"title": "A test of landsat-2 imagery and digital data for thematic mapping illustrated by  an environmental study in northern Kenya", "author": ["U. Hellden"], "venue": "Lund Univ. Nat. Geog. Inst, Lund, Sweden", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1980}, {"title": "Short: The landsat tutorical workbook\u2014 Basics of satellite remote sensing, Goddard Space Flight Center, Greenbelt, MD", "author": ["M. N"], "venue": "NASA ref. pub", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1982}, {"title": "Map Evaluation and \u2015Chance Correction", "author": ["G. T\u00fcrk"], "venue": "Photogramm Eng Rem S,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Reliability of Content Analysis: The Case of Nominal Scale Coding", "author": ["W.A. Scott"], "venue": "Public Opin Quart,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1955}, {"title": "Maxwell: Coefficients of agreement between observers and their interpretation", "author": ["E. A"], "venue": "British Journal of Psychiatry,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1977}, {"title": "The analysis of crossclassified data: independence, quasiindependence, and interaction in contingency tables with and without missing entries", "author": ["L.A. Goodman"], "venue": "J Am Stat Assoc,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1968}, {"title": "Handbook of Means and Their Inequalities, 2nd ed", "author": ["P.S. Bullen"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2003}, {"title": "High agreement but low kappa: II. Resolving the paradoxes", "author": ["D.V. Cicchetti", "A.R. Feinstein"], "venue": "J Clin Epidemiol,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1990}, {"title": "A Family of Association Coefficients for Metric", "author": ["F.E. Zegers", "J.M.F. ten Berge"], "venue": "Scales, Psychometrika,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1985}, {"title": "The determination of optimal threshold levels for change detection using various accuracy indices", "author": ["T. Fung", "E. LeDrew"], "venue": "Photogramm Eng Rem S,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1988}], "referenceMentions": [{"referenceID": 1, "context": "Some were specifically designed to compare classifiers , but most were initially defined for other purposes, such as measuring the association between two random variables [2], the agreement between two raters [3] or the similarity between two sets [4].", "startOffset": 172, "endOffset": 175}, {"referenceID": 2, "context": "Some were specifically designed to compare classifiers , but most were initially defined for other purposes, such as measuring the association between two random variables [2], the agreement between two raters [3] or the similarity between two sets [4].", "startOffset": 210, "endOffset": 213}, {"referenceID": 3, "context": "Some were specifically designed to compare classifiers , but most were initially defined for other purposes, such as measuring the association between two random variables [2], the agreement between two raters [3] or the similarity between two sets [4].", "startOffset": 249, "endOffset": 252}, {"referenceID": 4, "context": "Most measures are designed to focus on a specific aspect of the overall classification results [5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "Finally, the measures may also differ in the nature of the situations they can handle [6].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "Finally, some measures are sensitive to the sampling design used to retrieve the test data [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "Many different measures exist, but yet, there is no such thing as a perfect measure, which would be the best in every situation [8]: an appropriate measure must be chosen according to the classification context and objectives.", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "the question of chance-correction [9]).", "startOffset": 34, "endOffset": 37}, {"referenceID": 9, "context": "We discuss the case where one wants to select the best classification algorithm to process a given data set [10].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "In [11], Congalton described the various aspects of accuracy assessment and compared a few measures in terms of functional traits.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "defined the notions of consistency and discriminancy to compare measures [12].", "startOffset": 73, "endOffset": 77}, {"referenceID": 12, "context": "In [13], Flach compared 7 measures through the use of ROC plots.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "\u2019s consistency [12].", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "Sokolova & Lapalme considered 24 measures, on both binary and multiclass problems (and others) [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 12, "context": "Using the same general idea than Flach [13] (isometrics), they developed the notion of invariance, by identifying the changes in the confusion matrix which did not affect the measure value.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "In [14], Albatineh et.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "Caruana & Niculescu-Mizil adopted this method to compare 9 accuracy measures [15], but their focus was on binary classification problems, and classifiers able to output real-valued scores (by opposition to the discrete scores we treat here).", "startOffset": 77, "endOffset": 81}, {"referenceID": 15, "context": "[16] and and Ferri et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] considered 34 and 18 measures, respectively, for both binary and multiclass problems (amongst others).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "The main limitation with these studies is they either use data coming from a single applicative domain (such as remote sensing in [16]), or rely on a small number of datasets (7 in [15] and 30 in [17]).", "startOffset": 130, "endOffset": 134}, {"referenceID": 14, "context": "The main limitation with these studies is they either use data coming from a single applicative domain (such as remote sensing in [16]), or rely on a small number of datasets (7 in [15] and 30 in [17]).", "startOffset": 181, "endOffset": 185}, {"referenceID": 16, "context": "The main limitation with these studies is they either use data coming from a single applicative domain (such as remote sensing in [16]), or rely on a small number of datasets (7 in [15] and 30 in [17]).", "startOffset": 196, "endOffset": 200}, {"referenceID": 12, "context": "Note some authors invert estimated and true classes, resulting in a transposed matrix [13, 18].", "startOffset": 86, "endOffset": 94}, {"referenceID": 17, "context": "Note some authors invert estimated and true classes, resulting in a transposed matrix [13, 18].", "startOffset": 86, "endOffset": 94}, {"referenceID": 1, "context": "[2]), Yule\u2019s coefficients, Matthew\u2019s correlation coefficient, Proportional reduction in error measures (Goodman & Kruskal\u2019s and , Theil\u2019s uncertainty coefficient, etc.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "), mutual informationbased measures [19] and others.", "startOffset": 36, "endOffset": 40}, {"referenceID": 1, "context": "The relationship assessed by an association measure is more general [2], since a high level of association only means it is possible to predict estimated classes when knowing the true ones (and vice-versa).", "startOffset": 68, "endOffset": 71}, {"referenceID": 19, "context": "2 Overall Success Rate Certainly the most popular measure for classification accuracy [20], the overall success rate is defined as the trace of the confusion matrix:", "startOffset": 86, "endOffset": 90}, {"referenceID": 19, "context": "The former is also called sensitivity [20], producer\u2019s accuracy [11] and Dice\u2019s asymmetric index [21].", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "The former is also called sensitivity [20], producer\u2019s accuracy [11] and Dice\u2019s asymmetric index [21].", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "The former is also called sensitivity [20], producer\u2019s accuracy [11] and Dice\u2019s asymmetric index [21].", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "The latter is alternatively called specificity [20].", "startOffset": 47, "endOffset": 51}, {"referenceID": 19, "context": "The estimation-oriented measures, which focus on the confusion matrix rows (estimated classes), are the Positive Predictive Value (PPV) and Negative Predictive Value (NPV) [20].", "startOffset": 172, "endOffset": 176}, {"referenceID": 19, "context": "The former is also called precision [20], user\u2019s accuracy [11] and Dice\u2019s association index [21].", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "The former is also called precision [20], user\u2019s accuracy [11] and Dice\u2019s association index [21].", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "The former is also called precision [20], user\u2019s accuracy [11] and Dice\u2019s association index [21].", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": "fallout [22] or false alarm rate [23], and is notably used to build ROC curves [20].", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "fallout [22] or false alarm rate [23], and is notably used to build ROC curves [20].", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "fallout [22] or false alarm rate [23], and is notably used to build ROC curves [20].", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "4 F-measure and Jaccard Coefficient The F-measure corresponds to the harmonic mean of PPV and TPR [20], therefore it is classspecific and symmetric.", "startOffset": 98, "endOffset": 102}, {"referenceID": 14, "context": "It is also known as F-score [15], S\u00f8rensen\u2019s similarity coefficient [24], Dice\u2019s coincidence index [21] and Hellden\u2019s mean accuracy index [25]:", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "It is also known as F-score [15], S\u00f8rensen\u2019s similarity coefficient [24], Dice\u2019s coincidence index [21] and Hellden\u2019s mean accuracy index [25]:", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "It is also known as F-score [15], S\u00f8rensen\u2019s similarity coefficient [24], Dice\u2019s coincidence index [21] and Hellden\u2019s mean accuracy index [25]:", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "It is also known as F-score [15], S\u00f8rensen\u2019s similarity coefficient [24], Dice\u2019s coincidence index [21] and Hellden\u2019s mean accuracy index [25]:", "startOffset": 138, "endOffset": 142}, {"referenceID": 3, "context": "The measure known as Jaccard\u2019s coefficient of community was initially defined to compare sets [4], too.", "startOffset": 94, "endOffset": 97}, {"referenceID": 25, "context": "It is alternatively called Short\u2019s measure [26].", "startOffset": 43, "endOffset": 47}, {"referenceID": 26, "context": "It is related to the F-measure [27]: i i i F F JCC 2 ,", "startOffset": 31, "endOffset": 35}, {"referenceID": 0, "context": "5 Classification Success Index The Individual Classification Success Index (ICSI), is a class-specific symmetric measure defined for classification assessment purpose [1]:", "startOffset": 167, "endOffset": 170}, {"referenceID": 0, "context": "The Classification Success Index (CSI) is an overall measure defined simply by averaging ICSI over all classes [1].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "proportions [3]:", "startOffset": 12, "endOffset": 15}, {"referenceID": 27, "context": "coefficient (SPC) relies instead on the class proportions measured on the whole data set (or its estimation), noted i p [29]: 2", "startOffset": 120, "endOffset": 124}, {"referenceID": 28, "context": "authors, including Maxwell for his Random Error (MRE) [30], made the assumption classes are", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "7 Ground Truth Index T\u00fcrk\u2019s Ground Truth Index (GTI) is another chance-corrected measure, but this one was defined specially for classification accuracy assessment [18].", "startOffset": 164, "endOffset": 168}, {"referenceID": 26, "context": "The way this measure handles chance correction is more adapted to classification than the agreement coefficients [27].", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "less than 10% of the real-world cases considered in [16]).", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "Our approach is related to the isometrics concept described in [13].", "startOffset": 63, "endOffset": 67}, {"referenceID": 1, "context": "Inversely, it is possible to use an overall measure to assess a given class accuracy, by merging all classes except the considered one [2].", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "Certain more sophisticated measures allow associating a weight to each class, though [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 26, "context": "It consists in associating a weight to each cell in the confusion matrix, and then using a regular (unweighted) overall measure [27].", "startOffset": 128, "endOffset": 132}, {"referenceID": 30, "context": "[32].", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Besides these combinations of TPR and PPV, this also holds for various measures corresponding to combinations of TPR and TNR, not presented here because they are not very popular [20, 33].", "startOffset": 179, "endOffset": 187}, {"referenceID": 31, "context": "Besides these combinations of TPR and PPV, this also holds for various measures corresponding to combinations of TPR and TNR, not presented here because they are not very popular [20, 33].", "startOffset": 179, "endOffset": 187}, {"referenceID": 32, "context": "Thus, several supposedly different measures are actually the same, but transposed to different scales [34].", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "Jaccard\u2019s coefficient [4]).", "startOffset": 22, "endOffset": 25}, {"referenceID": 33, "context": "the combination of OSR and marginal rates described in [35]).", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "First, authors disagree on the necessity of this correction, depending on the application context [7, 27].", "startOffset": 98, "endOffset": 105}, {"referenceID": 26, "context": "First, authors disagree on the necessity of this correction, depending on the application context [7, 27].", "startOffset": 98, "endOffset": 105}], "year": 2011, "abstractText": "The selection of the best classification algorithm for a given dataset is a very widespread problem, occuring each time one has to choose a classifier to solve a real-world problem. It is also a complex task with many important methodological decisions to make. Among those, one of the most crucial is the choice of an appropriate measure in order to properly assess the classification performance and rank the algorithms. In this article, we focus on this specific task. We present the most popular measures and compare their behavior through discrimination plots. We then discuss their properties from a more theoretical perspective. It turns out several of them are equivalent for classifiers comparison purposes. Futhermore. they can also lead to interpretation problems. Among the numerous measures proposed over the years, it appears that the classical overall success rate and marginal rates are the more suitable for classifier comparison task.", "creator": "Microsoft\u00ae Word 2010"}}}