{"id": "1112.4243", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Dec-2011", "title": "Online Learning for Classification of Low-rank Representation Features and Its Applications in Audio Segment Classification", "abstract": "In this paper, a novel framework based on trace norm minimization for audio segment is proposed. In this framework, both the feature extraction and classification are obtained by solving corresponding convex optimization problem with trace norm regularization. For feature extraction, robust principle component analysis (robust PCA) via minimization a combination of the nuclear norm and the $\\ell_1$-norm is used to extract low-rank features which are robust to white noise and gross corruption for audio segments. These low-rank features are fed to a linear classifier where the weight and bias are learned by solving similar trace norm constrained problems. For this classifier, most methods find the weight and bias in batch-mode learning, which makes them inefficient for large-scale problems. In this paper, we propose an online framework using accelerated proximal gradient method. This framework has a main advantage in memory cost. In addition, as a result of the regularization formulation of matrix classification, the Lipschitz constant was given explicitly, and hence the step size estimation of general proximal gradient method was omitted in our approach. Experiments on real data sets for laugh/non-laugh and applause/non-applause classification indicate that this novel framework is effective and noise robust.", "histories": [["v1", "Mon, 19 Dec 2011 05:29:18 GMT  (1025kb)", "http://arxiv.org/abs/1112.4243v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.MM", "authors": ["ziqiang shi", "jiqing han", "tieran zheng", "shiwen deng"], "accepted": false, "id": "1112.4243"}, "pdf": {"name": "1112.4243.pdf", "metadata": {"source": "CRF", "title": "Online Learning for Classification of Low-rank Representation Features and Its Applications in Audio Segment Classification", "authors": ["Ziqiang Shi", "Jiqing Han", "Tieran Zheng", "Shiwen Deng"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 111 2.42 43v1 [cs.LG] 1 9"}, {"heading": "1 Introduction", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "2 Low-Rank Matrix Representation Features", "text": "In recent decades, much has been developed on audio and speech functions for audio and speech processing (A + E). (2, 3, 5) Due to the convenience and the short-term stationary assumption that these features are mainly based on frames in vector form, although it is assumed that features that are based on a longer duration will help a lot in decision making. (3) To build long-term features, the successive frame signals are used together as rows, then the audio segments become matrices. (4) It is generally assumed and assumed that the successive frame signals are influenced by a few factors, so that these matrices are combinations of low-rank and noise components. (4) It is natural to approach these matrices by low ranks. (4) In this work, transformations of these safe frame signals are used as thresholds.) Given an observed data matrix D (Rm), it is assumed that the frame is composed by which the number of examples is represented and n is assumed."}, {"heading": "3 Low Rank Matrix Classification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Notation and Problem Statement", "text": "After robust matrix representation characteristics have been extracted, the linear matrix classification approach is used to classify them based on the framework proposed in [12]. The motivation for the framework for regulating the trace standard is twofold: a) Trace standard takes into account the interactive information between the frames in the matrix, while the simple approach, which treats the matrix as a long vector, would lose the information.b) Trace standard is a suitable quantity measuring the complexity of the linear classifier. Generally, the problem in regulating trace standards based on the matrix classification is formulated as asmin W, bFs (W, b) = fs (W, b) + \u03bb W value (5), where W-Rm is the unknown weight matrix, b-R is the bias standard based on the matrix classification."}, {"heading": "3.2 APG Method for Matrix Classification", "text": "The condition for using the APG algorithm is that the loss function should be smooth, convex, and smooth, and the gradient should fulfill Lipschitz's state. Since fs (W, b) in this work is a composition of the smooth convex function with an affine image, it is convex and smooth [18]. For Lipschitz, it is shown that the gradient of fs (W, b), which is referred to as \"W fs\" (W, b) = 2 s \"s\" (yi \u2212 Tr)."}, {"heading": "3.3 Determination of Lipschitz Constant", "text": "As a special case of general convex optimization problems we derive the closed form of the Lipschitz constant, therefore the step size estimation [13, 14] of the general APG method was omitted in all our approaches. Determination of the Lipschitz constant is given in the following theory.Theorem 1st sentence W fs (\u00b7, b) is Lipschitz continuous with constant L = 2mn s \u00b2 i = 1 x Xi \u00b2 2F, i.e. the application of the equation (6) with U, V \u00b2 R \u00b2 n, the requirements on the education technology (U, b) \u2212 W fs (V, b) \u0445L \u00b2 U \u2212 V \u00b2 F \u00b2 optimization, (13) where the Frobenius F standard denotes."}, {"heading": "4 Online Learning for Matrix Classification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Online Learning with APG", "text": "In this section, we present the basic components of our online learning algorithm for matrix classification, as well as a few smaller variants that accelerate our implementation in practice. \u2212 Our procedure is summarized in algorithm 4.1. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 The operator in step 6 of the algorithm denotes the Kronecker product. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 n2, defined by blocks of sizes m2 \u00b7 n2 equal to A [i, j] B. GridTr (Zk, t, Bt) in step 13 denotes an operator with input Zk, t Rm \u00b7 n and Bt Rmm \u00b7 nn, defined by blocks of sizes m2 \u00b7 n2 equal to A \u2212 n2, B. \u2212 nTr (Zk, t, Bt) in step 13 \u2212 n. \u2212 nWt: an operator with input Zk, Rx \u00b7 n, Rx \u00b7 m2 equal to blocks \u2212 nn."}, {"heading": "4.2 Online Learning with inexact APG", "text": "Algorithm 4.1 calls APG to update the weight matrix for each coming sample by solving the partial problem with fixed distortion (bWt = min Wt \u2211 i = 1 (yi \u2212 Tr (WTXi) \u2212 bt \u2212 1), which causes exactly the computational load for large training sets. Fortunately, due to the proximity of the consecutive weight matrix, we do not have to solve the partial problem exactly. Instead, it is sufficient to update Wt \u2212 1 once if we solve this partial problem in practice. This leads to an online MC learning method based on inaccurate APG, described in Algorithm 4.2.Online MC Learning with Inexact APG. Initiate W0 Rm \u00b7 n, b0 R, L0 = 0,... R. 1: A0 Rm \u00b7 n: LT \u00b7 n: 0, B0 Rmm \u00b7 nn \u00b2."}, {"heading": "4.3 Online Learning with Mini-batch", "text": "Under certain conditions, we can improve the convergence speed of our algorithm by taking \u00b5 > 1 training samples instead of a single iteration for each iteration. Let's mark the samples drawn during iteration (Xt, 1, yt, 1),..., (Xt, \u00b5, yt, \u00b5) We can now mark lines 5 and 9 of algorithm 4.1 and 4.2 byAt \u2190 At \u2212 1 + \u00b5, i = 1yt, iXt, i, Bt, Bt \u2190 Bt \u2212 1 + \u00b5, i = 1Xt, i Xt, i, ct."}, {"heading": "5 Experimental Validation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Dataset", "text": "We have downloaded about 20 hours of videos from Youku [21], with different programs and different languages, manually labeling the starting and ending positions of all applause and laughter tracks, and the database includes 800 segments of each sound effect, each segment is about 3-8 seconds long, and a total of about 1 hour of data for each sound effect. All audio recordings have been converted into a monaural waveform with a sampling frequency of 8 kHz and quantified 16 bits, and the audio signals have been normalized so that they have a mean amplitude with unit variance of zero to eliminate all factors related to recording conditions."}, {"heading": "5.2 Online Learning", "text": "In this section, we will conduct detailed experiments to demonstrate the properties and benefits of online learning for matrix classification problems. Five algorithms will be compared: the traditional batch algorithm with the exact APG algorithm (APG); the online learning algorithm with the exact APG (OL APG); the online learning algorithm with the exact APG and update Equation (15) (OL APFCG Batch); the online learning algorithm with the inexact APG and update Equation (15) (OL IAPG Batch). All algorithms will be stored in Matlab on a personal computer with an Intel 3.40GHz Dual-Core Central Processing Unit (CPU) and 2GB memory. For this experiment, audio streams were extracted into a sequence of short-term frames (20 ms long) with non-overlapping 13-dimensional MFCCCs including energy and C50 (Chix)."}, {"heading": "5.3 Robustness", "text": "In this section, the effectiveness of the robust PCA extracted low rank matrix characteristics under different conditions will be seen as the function of the PCM in general. Original features (MFCCs matrix), corrupted with 0dB and -5dB of white Gaussian noise (WGN SNR = 5dB, 0dB, -5dB) and 10%, 30%, 50% random large errors (LE 10%, 30%, 50%) and parallelism of robust PCA extracted characteristics (rPCA) will be compared. In the comparisons, the parameters set in the stop criteria (11) \u03b51 = 10 \u2212 6 and \u03b52 = 10 \u2212 6, which are determined by the same method as in Section 5.2. The regularization constant \u03bb is a classical normalization factor according to [22]. The classification accuracy of a second audio segment is used to evaluate the performance of the methods Fig. If the 4 features are used under different conditions of the matrix in general, the performance of the functions will be different."}, {"heading": "6 Conclusions", "text": "In this paper, we present a novel framework based on minimizing track standards for classification of audio segments. The novel method unifies the extraction of features and classification of patterns in the same framework. In this framework, a robust PCA-extracted low component of the original signal is more robust against corrupt noise and errors, especially random large errors. In addition, we have introduced online learning algorithms for classification of matrices. We obtain the closed form of the weight matrix and bias. We derive the explicit form of the Lipschitz constant, which saves the computing load in the search for step size. Experiments show that even the percentage of the original feature elements corrupted with random large errors is up to 50%, the performance of the robust PCA-extracted features shows almost no reduction in the matrix. In future work, we plan to extend this to include more robust language processing or other features in the matrix."}], "references": [{"title": "Content analysis for audio classification and segmentation", "author": ["L. Lu"], "venue": "IEEE Transactions on Speech and Audio processing, vol. 10, no. 7, pp. 504-516", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Highlight sound effects detection in audio stream", "author": ["R. Cui", "L. Lu", "H.J. Zhung", "L.H. Cai"], "venue": "Proceedings of IEEE International Conference on Multimedia and Expo, pp. 37-40", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Audio based event detection for multimedia surveillance", "author": ["K.A. Pradeep", "C.M. Namunu", "S.K. Mohan"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Audio signal feature extraction and classification using local discriminant bases", "author": ["K. Umapathy", "S. Krishnan", "R.K. Rao"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 1, pp. 1236-1246", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Feature analysis and selection for acoustic event detection", "author": ["X. Zhuang", "X. Zhou", "T.S. Huang", "M. Hasegawa-Johnson"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 17-20", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Content-based audio classification and retrieval by support vector machines", "author": ["G. Guo", "S.Z. Li"], "venue": "IEEE Transactions on Neural Networks vol. 14, no. 1, pp. 209-215", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "A rank minimization heuristic with application to minimum order system approximation", "author": ["M. Fazel", "H. Hindi", "S.P. Boyd"], "venue": "Proceedings of the American Control Conference, pp. 4734-4739", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J.D.M. Rennie", "T.S. Jaakkola"], "venue": "Proceedings of Advances in Neural Information Processing Systems, pp. 1329-1336", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, vol. 73, no. 3, pp. 243-272", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization", "author": ["J. Wright", "A. Ganesh", "S. Rao", "Y. Peng", "Y. Ma"], "venue": "Proceedings of Advances in Neural Information Processing Systems", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices", "author": ["Z. Lin", "M. Chen", "L. Wu", "Y. Ma"], "venue": "arXiv:1009.5055", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Classifying matrices with a spectral regularization", "author": ["R. Tomioka", "K. Aihara"], "venue": "24th International Conference on Machine Learning, pp. 895-902", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems", "author": ["K. Toh", "S. Yun"], "venue": "Pacific J. Optim., vol. 6, pp. 615-640", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "An accelerated gradient method for trace norm minimization", "author": ["S. Ji", "J. Ye"], "venue": "26th International Conference on Machine Learning, pp. 457-464", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "An implementable proximal point algorithmic framework for nuclear norm minimization", "author": ["Y.J. Liu", "D. Sun", "K.C. Toh"], "venue": "Mathematical Programming, pp. 1-38", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Principal Component Analysis", "author": ["I.T. Jolliffe"], "venue": "Springer Series in Statistics, Berlin: Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1986}, {"title": "Exact matrix completion via convex optimization", "author": ["E.J. Candes", "B. Recht"], "venue": "Technical Report, UCLA Computational and Applied Math", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "Convex optimization", "author": ["S.P. Boyd", "L. Vandenberghe"], "venue": "Cambridge Univ Pr", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "A method of solving a convex programming problem with convergence rate O( 1 k2  )", "author": ["Y. Nesterov"], "venue": "Soviet Mathematics Doklady, vol. 27, no. 2, pp. 372-376", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1983}, {"title": "Smooth minimization of non-smooth functions", "author": ["Y. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction Audio feature extraction and classification methods have been studied by many researchers over the years [1, 2, 3, 4].", "startOffset": 120, "endOffset": 132}, {"referenceID": 1, "context": "1 Introduction Audio feature extraction and classification methods have been studied by many researchers over the years [1, 2, 3, 4].", "startOffset": 120, "endOffset": 132}, {"referenceID": 2, "context": "1 Introduction Audio feature extraction and classification methods have been studied by many researchers over the years [1, 2, 3, 4].", "startOffset": 120, "endOffset": 132}, {"referenceID": 3, "context": "1 Introduction Audio feature extraction and classification methods have been studied by many researchers over the years [1, 2, 3, 4].", "startOffset": 120, "endOffset": 132}, {"referenceID": 3, "context": "Feature commonly exploited for audio classification can be roughly classified into time domain features, transformation domain features, time-transformation domain features or their combinations [4, 5].", "startOffset": 195, "endOffset": 201}, {"referenceID": 4, "context": "Feature commonly exploited for audio classification can be roughly classified into time domain features, transformation domain features, time-transformation domain features or their combinations [4, 5].", "startOffset": 195, "endOffset": 201}, {"referenceID": 6, "context": "is a principled approach to learn low-rank matrices through convex optimization problems [7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 7, "context": "These similar problems arise in many machine learning tasks such as matrix completion [8], multi-task learning [9], robust principle component antilysis (robust PCA) [10, 11], and matrix classification [12].", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "These similar problems arise in many machine learning tasks such as matrix completion [8], multi-task learning [9], robust principle component antilysis (robust PCA) [10, 11], and matrix classification [12].", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "These similar problems arise in many machine learning tasks such as matrix completion [8], multi-task learning [9], robust principle component antilysis (robust PCA) [10, 11], and matrix classification [12].", "startOffset": 166, "endOffset": 174}, {"referenceID": 10, "context": "These similar problems arise in many machine learning tasks such as matrix completion [8], multi-task learning [9], robust principle component antilysis (robust PCA) [10, 11], and matrix classification [12].", "startOffset": 166, "endOffset": 174}, {"referenceID": 11, "context": "These similar problems arise in many machine learning tasks such as matrix completion [8], multi-task learning [9], robust principle component antilysis (robust PCA) [10, 11], and matrix classification [12].", "startOffset": 202, "endOffset": 206}, {"referenceID": 3, "context": "[4, 5, 6].", "startOffset": 0, "endOffset": 9}, {"referenceID": 4, "context": "[4, 5, 6].", "startOffset": 0, "endOffset": 9}, {"referenceID": 5, "context": "[4, 5, 6].", "startOffset": 0, "endOffset": 9}, {"referenceID": 11, "context": "After extraction of the robust low-rank matrix feature, the regularization framework based matrix classification approach proposed by Tomioka and Aihara in [12] is used to predict the label.", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "The problem of matrix classification (MC) with spectral regularization was first proposed by Tomioka and Aihara in [12].", "startOffset": 115, "endOffset": 119}, {"referenceID": 6, "context": "For the matrix rank minimization is NP-hard in general due to the combinatorial nature of the rank function, a commonly-used convex relaxation of the rank function is the trace norm (nuclear norm) [7], defined as the sum of the singular values of the matrix.", "startOffset": 197, "endOffset": 200}, {"referenceID": 12, "context": "Recent related researches are not focused on matrix classification directly, but rather on general trace norm minimization problem [13, 14, 15].", "startOffset": 131, "endOffset": 143}, {"referenceID": 13, "context": "Recent related researches are not focused on matrix classification directly, but rather on general trace norm minimization problem [13, 14, 15].", "startOffset": 131, "endOffset": 143}, {"referenceID": 14, "context": "Recent related researches are not focused on matrix classification directly, but rather on general trace norm minimization problem [13, 14, 15].", "startOffset": 131, "endOffset": 143}, {"referenceID": 12, "context": "In these methods, most are iterative batch procedures [13, 14, 15], accessing the whole training set at each iteration in order to minimize a weighted sum of a cost function and the trace norm.", "startOffset": 54, "endOffset": 66}, {"referenceID": 13, "context": "In these methods, most are iterative batch procedures [13, 14, 15], accessing the whole training set at each iteration in order to minimize a weighted sum of a cost function and the trace norm.", "startOffset": 54, "endOffset": 66}, {"referenceID": 14, "context": "In these methods, most are iterative batch procedures [13, 14, 15], accessing the whole training set at each iteration in order to minimize a weighted sum of a cost function and the trace norm.", "startOffset": 54, "endOffset": 66}, {"referenceID": 12, "context": "We transform the general batch-mode accelerated proximal gradient (APG) [13, 14] method for trace norm minimization to the online learning framework.", "startOffset": 72, "endOffset": 80}, {"referenceID": 13, "context": "We transform the general batch-mode accelerated proximal gradient (APG) [13, 14] method for trace norm minimization to the online learning framework.", "startOffset": 72, "endOffset": 80}, {"referenceID": 12, "context": "In addition, as a special case of general convex optimization problem, we derived the closed-form of the Lipschitz constant, hence the step size estimation [13, 14] of the general APG method was omitted in our approach.", "startOffset": 156, "endOffset": 164}, {"referenceID": 13, "context": "In addition, as a special case of general convex optimization problem, we derived the closed-form of the Lipschitz constant, hence the step size estimation [13, 14] of the general APG method was omitted in our approach.", "startOffset": 156, "endOffset": 164}, {"referenceID": 1, "context": "2 Low-Rank Matrix Representation Features Over the past decades, a lot work has been done on audio and speech features for audio and speech processing [2, 3, 5].", "startOffset": 151, "endOffset": 160}, {"referenceID": 2, "context": "2 Low-Rank Matrix Representation Features Over the past decades, a lot work has been done on audio and speech features for audio and speech processing [2, 3, 5].", "startOffset": 151, "endOffset": 160}, {"referenceID": 4, "context": "2 Low-Rank Matrix Representation Features Over the past decades, a lot work has been done on audio and speech features for audio and speech processing [2, 3, 5].", "startOffset": 151, "endOffset": 160}, {"referenceID": 15, "context": "For this problem, PCA is a suitable approach that it can find the low-dimensional approximating subspace by forming a lowrank approximation to the data matrix [16].", "startOffset": 159, "endOffset": 163}, {"referenceID": 10, "context": "However, it breaks down under large corruption, even if that corruption affects only a very few of the observation which is often encountered in practice [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 9, "context": "This optimization is refereed to as robust PCA in [10] for its ability to exactly recover underlying low-rank structure in data even in the presence of large errors or outliers.", "startOffset": 50, "endOffset": 54}, {"referenceID": 10, "context": "In order to solve Equation (2), several algorithms have been proposed, among which the augmented Lagrange multiplier method is the most efficient and accurate at present [11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 10, "context": "[11] identify the problem as", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Two ALM algorithms to solve the above formulation are proposed in [11].", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "The S\u03b5[\u00b7] is the soft-thresholding operator introduced in [11].", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "1 Notation and Problem Statement Having extracted robust matrix representation features, the linear matrix classification approach based on trace norm regularization framework proposed in [12] is used to classify them.", "startOffset": 188, "endOffset": 192}, {"referenceID": 12, "context": "2 APG Method for Matrix Classification Recently Toh and Yun [13], Ji and Ye [14], and Liu et al.", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "2 APG Method for Matrix Classification Recently Toh and Yun [13], Ji and Ye [14], and Liu et al.", "startOffset": 76, "endOffset": 80}, {"referenceID": 14, "context": "[15] independently proposed similar algorithms that converge as O( 1 k2 ) for problem (5) by using APG, where k is the iteration counter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Since fs(W, b) in this work is a composition of smooth convex function with an affine mapping, hence it is convex and smooth [18].", "startOffset": 125, "endOffset": 129}, {"referenceID": 18, "context": "Based on the the work of Nesterov [19, 20], Toh and Yun [13], Ji and Ye [14], and Liu et al.", "startOffset": 34, "endOffset": 42}, {"referenceID": 19, "context": "Based on the the work of Nesterov [19, 20], Toh and Yun [13], Ji and Ye [14], and Liu et al.", "startOffset": 34, "endOffset": 42}, {"referenceID": 12, "context": "Based on the the work of Nesterov [19, 20], Toh and Yun [13], Ji and Ye [14], and Liu et al.", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "Based on the the work of Nesterov [19, 20], Toh and Yun [13], Ji and Ye [14], and Liu et al.", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "[15] showed that setting Zk = Wk + tk\u22121\u22121 tk (Wk \u2212Wk\u22121) for a sequence tk satisfying tk+1\u2212 tk+1 \u2264 t2k results in a convergence rate of O( 1 k2 ).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Due to Lemma 1, the estimation of step size tk in general APG [13, 14, 15] is omitted, for we have explicit Lipschitz constant.", "startOffset": 62, "endOffset": 74}, {"referenceID": 13, "context": "Due to Lemma 1, the estimation of step size tk in general APG [13, 14, 15] is omitted, for we have explicit Lipschitz constant.", "startOffset": 62, "endOffset": 74}, {"referenceID": 14, "context": "Due to Lemma 1, the estimation of step size tk in general APG [13, 14, 15] is omitted, for we have explicit Lipschitz constant.", "startOffset": 62, "endOffset": 74}, {"referenceID": 10, "context": "2 is the soft-thresholding operator introduced in [11]: S\u03b5[x] .", "startOffset": 50, "endOffset": 54}, {"referenceID": 12, "context": "The general APG [13, 14, 15] algorithms only provide the methods for learning weight matrices, do not give out the bias updating rules.", "startOffset": 16, "endOffset": 28}, {"referenceID": 13, "context": "The general APG [13, 14, 15] algorithms only provide the methods for learning weight matrices, do not give out the bias updating rules.", "startOffset": 16, "endOffset": 28}, {"referenceID": 14, "context": "The general APG [13, 14, 15] algorithms only provide the methods for learning weight matrices, do not give out the bias updating rules.", "startOffset": 16, "endOffset": 28}, {"referenceID": 12, "context": "3 Determination of Lipschitz Constant As a special case of general convex optimization problem, we derived the closed-form of the Lipschitz constant, hence the step size estimation [13, 14] of the general APG method was omitted in all our approach.", "startOffset": 181, "endOffset": 189}, {"referenceID": 13, "context": "3 Determination of Lipschitz Constant As a special case of general convex optimization problem, we derived the closed-form of the Lipschitz constant, hence the step size estimation [13, 14] of the general APG method was omitted in all our approach.", "startOffset": 181, "endOffset": 189}], "year": 2011, "abstractText": "In this paper, a novel framework based on trace norm minimization for audio segment is proposed. In this framework, both the feature extraction and classification are obtained by solving corresponding convex optimization problem with trace norm regularization. For feature extraction, robust principle component analysis (robust PCA) via minimization a combination of the nuclear norm and the l1-norm is used to extract low-rank features which are robust to white noise and gross corruption for audio segments. These low-rank features are fed to a linear classifier where the weight and bias are learned by solving similar trace norm constrained problems. For this classifier, most methods find the weight and bias in batch-mode learning, which makes them inefficient for large-scale problems. In this paper, we propose an online framework using accelerated proximal gradient method. This framework has a main advantage in memory cost. In addition, as a result of the regularization formulation of matrix classification, the Lipschitz constant was given explicitly, and hence the step size estimation of general proximal gradient method was omitted in our approach. Experiments on real data sets for laugh/non-laugh and applause/non-applause classification indicate that this novel framework is effective and noise robust.", "creator": "LaTeX with hyperref package"}}}