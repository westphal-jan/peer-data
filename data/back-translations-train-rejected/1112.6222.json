{"id": "1112.6222", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2011", "title": "A comparison of two suffix tree-based document clustering algorithms", "abstract": "Document clustering as an unsupervised approach extensively used to navigate, filter, summarize and manage large collection of document repositories like the World Wide Web (WWW). Recently, focuses in this domain shifted from traditional vector based document similarity for clustering to suffix tree based document similarity, as it offers more semantic representation of the text present in the document. In this paper, we compare and contrast two recently introduced approaches to document clustering based on suffix tree data model. The first is an Efficient Phrase based document clustering, which extracts phrases from documents to form compact document representation and uses a similarity measure based on common suffix tree to cluster the documents. The second approach is a frequent word/word meaning sequence based document clustering, it similarly extracts the common word sequence from the document and uses the common sequence/ common word meaning sequence to perform the compact representation, and finally, it uses document clustering approach to cluster the compact documents. These algorithms are using agglomerative hierarchical document clustering to perform the actual clustering step, the difference in these approaches are mainly based on extraction of phrases, model representation as a compact document, and the similarity measures used for clustering. This paper investigates the computational aspect of the two algorithms, and the quality of results they produced.", "histories": [["v1", "Thu, 29 Dec 2011 04:25:10 GMT  (235kb)", "http://arxiv.org/abs/1112.6222v1", null], ["v2", "Tue, 10 Jan 2012 15:40:29 GMT  (235kb)", "http://arxiv.org/abs/1112.6222v2", "Information and Emerging Technologies (ICIET), 2010 International Conference"]], "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["muhammad rafi", "m maujood", "m m fazal", "s m ali"], "accepted": false, "id": "1112.6222"}, "pdf": {"name": "1112.6222.pdf", "metadata": {"source": "CRF", "title": "A comparison of two suffix tree-based document clustering algorithms", "authors": ["Muhammad Rafi", "Mehdi Maujood", "Murtaza", "Munawar Fazal", "Syed Muhammad Ali"], "emails": ["@nu.edu.pk"], "sections": [{"heading": null, "text": "In fact, most of them will be able to move to another world, where they will be able to move to another world, where they will be able to move to another world, where they will be able to move to where they are."}, {"heading": "A. Datasets", "text": "The OHSUMED collection consists of over 348,566 references from the MEDLINE database, which is a database of medical literature maintained by the National Library of Medicine (NLM). Most references have summaries, and all have assigned MeSH (Medical Subject Headings), which index terms, with some of the MeSH terms marked as primary. We generate two data sets from the OHSUMED collection. For the first data set, we select the categories that have identified MSH1262, MSH1473, MSH1486, MSH1713 and MSH2025 as primary subjects, and MSH2030. We generate two data sets from the OHSUMED collection. For the first data set, we select the data collection, MSH1473, MSH1486, MSH1725, and MSH2030 as documents."}, {"heading": "B. Algorithms", "text": "Neither the source code nor the binary data of both algorithms were available; we implemented them according to the description in [11] and [12]. In the CFWS, we use the a priori algorithm to find common 2 items, and calculate the set of common words (WS). Then, all words that are not in WS are removed from the documents, and the resulting compact documents are inserted into a generalized suffix tree. Next, we get the cluster candidates from the tree based on the mismatch concept. We use a simple dynamic programming algorithm to calculate all k-non-matching sequences. Hierarchical clustering is then performed on the cluster candidate to obtain the final cluster result. [11] For first, we add all documents to a generalized suffix tree, and get the feature vector with tf-idf weighting for each document, as this will determine the predominantly two of the documents that are contained in at least two of the documents [11]."}, {"heading": "C. Measure", "text": "To measure the effectiveness of cluster results, we compare the results of the two algorithms based on the f-measurement. Note that we did not use entropy or purity, since CFWS allows a document to appear in more than one cluster, and therefore the measurement would not strictly be taken for the quality of the result. f-measurement uses a combination of precision and retrieval values of clusters. We leave ni the number of documents in class i and cj the number of documents in cluster j. In addition, we leave cij the number of elements of class i in cluster j. Then we can define prec (i, j), the precision of cluster j in relation to class i and rec (i, j), the recall of a cluster j in relation to class i as and, and, and, and, the f-measurement, F (i, j), a class i in relation to cluster i and c (rej, the recall of a cluster ij)."}, {"heading": "Purity", "text": "Purity can be defined as the maximum precision value for each class j. We calculate the purity for a cluster j as. We then define the purity of the overall cluster result as follows: Where \u2211, i.e. the sum of the cardinalities of each cluster, Note that we use this quantity and not the size of the document collection to calculate the purity."}, {"heading": "Entropy", "text": "Entropy measures how homogeneous each cluster j is. It can be calculated using the following formula:, log, The total entropy for a cluster set is calculated as the sum of entropy for each cluster, weighted by the size of each cluster: We need to maximize the purity and minimize the entropy of the clusters to achieve high-quality cluster results.IV. RESULTS The result of this experiment shows that the F score obtained from the test sets clearly demonstrates the superiority of the algorithm [11] over the algorithm [12] due to different situations.The results reported by the respective authors are different.The calculation requirements of the CFWS [12] are much higher than the NSTC [11].The following table records the Fscore of the two algorithms against the test sets."}], "references": [{"title": "Data Clutering: a review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computing Survey , pp. 264-323, 1999.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "The Role of Clustering in Search Computing", "author": ["A. Campi", "S. Ronchi"], "venue": "20th International Workshop on Databases and Expert Systems Application , Linz, Austria, 2009, pp. 432-436.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Scatter/Gather: A Cluster-based Approach to Browsing Large Document Collections", "author": ["D.R. Cutting", "D.R. Karger", "J.O. Pedersen", "J.W. Tukey"], "venue": "Fifteenth Annual International ACM SIGIR Conference, June 1992, pp. 318- 329.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "Reexamining the cluster hypothesis: scatter/gather on retrieval results", "author": ["M.A. Hearst", "J.O. Pedersen"], "venue": "19th annual international ACM SIGIR conference on Research and development in information retrieval, Zurich, Switzerland , 1996, pp. 74-84.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "Similarity Search-The Metric Space Approach.", "author": ["G. Amato", "V. Dohnal", "M.B. Pavel Zezula"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Finding Groups in Data: An Introduction to Cluster Analysis.", "author": ["I. Kaufman", "P.J. Rousseeuw"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1990}, {"title": "Fast and effective text mining using linear time document clustering", "author": ["B. Larsen", "C. Aone"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 1999, pp. 16-22.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "A comparison of document clustering techniques", "author": ["M. Steianbach", "G. Karypis", "V. Kumar"], "venue": "KDD-Workshop on Text Mining, 2000.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient Phrase- Based Document Indexing for Web Document Clustering", "author": ["K.M. Hammouda", "M.S. Kamel"], "venue": "IEEE Transaction on Knowledge and Data Engineering, vol. 16, no. 10, pp. 1279-1296, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Efficient Phrase-Based Document Similarity for Clustering", "author": ["H. Chim", "X. Deng"], "venue": "IEEE Transaction on Knowledge and Data Engineering, vol. 20, no. September, pp. 1217-1229, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Text document clustering based on frequent word meaning sequences", "author": ["S.M. Chung", "J.D. Holt", "Y. Li"], "venue": "Data & Knowledge Engineering, vol. 64, pp. 381-404, 2008", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 9, "context": "We compare two approaches that utilize the Suffix Tree Data Model; the New Suffix Tree Clustering (NSTC) algorithm proposed by Chim and Deng [11], and Clustering Based on Frequent Word/Word Meaning Sequences (CFWS/WMS) proposed by Yanjun Li et al.", "startOffset": 141, "endOffset": 145}, {"referenceID": 10, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "A detail on different data clustering approaches can be found in [1].", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "Clustering is an effective method for search computing [2].", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "It offers the possibilities like: grouping similar results [3], comprehend the links between the results [4] and creating the succinct representation and display of search results.", "startOffset": 59, "endOffset": 62}, {"referenceID": 3, "context": "It offers the possibilities like: grouping similar results [3], comprehend the links between the results [4] and creating the succinct representation and display of search results.", "startOffset": 105, "endOffset": 108}, {"referenceID": 0, "context": "Agglomerative hierarchical clustering [1] (AHC) initially treats each document as a cluster, and compute a similarity measure for every pair of document, a variety of similarity measures [5] have been utilized for this algorithm.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "Agglomerative hierarchical clustering [1] (AHC) initially treats each document as a cluster, and compute a similarity measure for every pair of document, a variety of similarity measures [5] have been utilized for this algorithm.", "startOffset": 187, "endOffset": 190}, {"referenceID": 2, "context": "The second category of document clustering algorithms is partitioned based algorithms, [3] [6] [7] which create a one level partitioning of the document collection.", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "The second category of document clustering algorithms is partitioned based algorithms, [3] [6] [7] which create a one level partitioning of the document collection.", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "The second category of document clustering algorithms is partitioned based algorithms, [3] [6] [7] which create a one level partitioning of the document collection.", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "A contrast and evaluation of these two major categories of document clustering algorithms can be found in [8], which also suggests that Bisecting k-mean is a modified version of k-mean that outperforms AHC in terms of accuracy and is computationally efficient from the quadratic time requirements of AHC.", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "This model is utilized to cluster web documents in [10].", "startOffset": 51, "endOffset": 55}, {"referenceID": 9, "context": "The more recent work on the phrase based approach is in [11].", "startOffset": 56, "endOffset": 60}, {"referenceID": 10, "context": "A similar work that also utilizes suffix tree model is from [12].", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "This paper is in fact a comparative study of the algorithms in [11] and [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "This paper is in fact a comparative study of the algorithms in [11] and [12].", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "In this section, we evaluate the performance of the two algorithms [11] and [12].", "startOffset": 67, "endOffset": 71}, {"referenceID": 10, "context": "In this section, we evaluate the performance of the two algorithms [11] and [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "Neither the source code nor binaries of either algorithm were available; we implemented them ourselves following the description in [11] and [12].", "startOffset": 132, "endOffset": 136}, {"referenceID": 10, "context": "Neither the source code nor binaries of either algorithm were available; we implemented them ourselves following the description in [11] and [12].", "startOffset": 141, "endOffset": 145}, {"referenceID": 9, "context": "For [11], we first add all documents to a generalized suffix tree, and obtain the feature vector with tf-idf weighting for each document.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "We only consider the phrases present in at least two documents for constructing the feature vectors since[11] claims that these are the nodes that predominantly determine the result of clustering, and including other nodes only has a slight effect on the result.", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "The result of this experiment shows that the F-score obtained from the test data sets clearly exhibits the superiority of algorithm [11] over algorithm [12], on variety of situations.", "startOffset": 132, "endOffset": 136}, {"referenceID": 10, "context": "The result of this experiment shows that the F-score obtained from the test data sets clearly exhibits the superiority of algorithm [11] over algorithm [12], on variety of situations.", "startOffset": 152, "endOffset": 156}, {"referenceID": 10, "context": "The computational demands of CFWS [12] are much higher than NSTC [11].", "startOffset": 34, "endOffset": 38}, {"referenceID": 9, "context": "The computational demands of CFWS [12] are much higher than NSTC [11].", "startOffset": 65, "endOffset": 69}], "year": 2010, "abstractText": "Document clustering as an unsupervised approach extensively used to navigate, filter, summarize and manage large collection of document repositories like the World Wide Web (WWW). Recently, focuses in this domain shifted from traditional vector based document similarity for clustering to suffix tree based document similarity, as it offers more semantic representation of the text present in the document. In this paper, we compare and contrast two recently introduced approaches to document clustering based on suffix tree data model. The first is an Efficient Phrase based document clustering, which extracts phrases from documents to form compact document representation and uses a similarity measure based on common suffix tree to cluster the documents. The second approach is a frequent word/word meaning sequence based document clustering, it similarly extracts the common word sequence from the document and uses the common sequence/ common word meaning sequence to perform the compact representation, and finally, it uses document clustering approach to cluster the compact documents. These algorithms are using agglomerative hierarchical document clustering to perform the actual clustering step, the difference in these approaches are mainly based on extraction of phrases, model representation as a compact document, and the similarity measures used for clustering. This paper investigates the computational aspect of the two algorithms, and the quality of results they produced. KeywordsDocument Clustering, Feature Extraction, Document Representation Model, Suffix Tree, Similarity Measure.", "creator": "'Certified by IEEE PDFeXpress at 06/07/2010 8:03:22 AM'"}}}