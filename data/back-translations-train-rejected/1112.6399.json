{"id": "1112.6399", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Dec-2011", "title": "Two-Manifold Problems", "abstract": "Recently, there has been much interest in spectral approaches to learning manifolds---so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together and allowing information to flow between them, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias in the same way that an instrumental variable allows us to remove bias in a {linear} dimensionality reduction problem. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space. Finally, we discuss situations where two-manifold problems are useful, and demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.", "histories": [["v1", "Thu, 29 Dec 2011 19:52:14 GMT  (7479kb,D)", "http://arxiv.org/abs/1112.6399v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["byron boots", "geoffrey j gordon"], "accepted": false, "id": "1112.6399"}, "pdf": {"name": "1112.6399.pdf", "metadata": {"source": "CRF", "title": "Two-Manifold Problems", "authors": ["Byron Boots", "Geoffrey J. Gordon"], "emails": ["beb@cs.cmu.edu", "ggordon@cs.cmu.edu"], "sections": [{"heading": null, "text": "Recently, there has been a great deal of interest in spectral approaches to learning manifolds - the so-called kernel eigenmap methods. These methods have had some success, but their applicability is limited because they are not robust to noise. To address this constraint, we look at two manifold problems where we simultaneously reconstruct two interconnected manifolds, each representing a different view of the same data. By solving these interrelated learning problems together and allowing the flow of information between them, two manifold algorithms can succeed where one non-integrated approach would fail: Each view allows us to suppress noise in the other by reducing distortion in the same way that an instrumental variable allows us to eliminate distortion in a linear dimensionality problem."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Preliminaries", "text": "We start with two well-known classes of nonlinear dimensionality reduction: kernel component analysis (kernel PCA) and multifaceted learning."}, {"heading": "2.1 Kernel PCA", "text": "Kernel PCA [6] is a generalization of the main component analysis [12]: We first map our d-dimensional inputs x1,.., xn \u00b2 Rd to a higher-dimensional attribute space F using a feature mapping \u03c6: Rd \u2192 F, and then find the main components in this new space. If the features are sufficiently expressive, Kernel PCA can find a structure that misses the regular PCA. However, if F is high or infinitely dimensional, the simple approach to PCA can come via a self-decomposition of a covariance matrix. Kernel PCA overcomes this problem by assuming that F is a reproducing Hilbert kernel space (RKHS) and that the feature mapping solution is implicitly defined by an efficiently computable eigencomponent function (x \u00b2)."}, {"heading": "2.2 Manifold Learning", "text": "In fact, it is as if most people who are able are able to recognize themselves and understand how they have behaved have to behave. (...) It is as if they were able to behave, as if they were able to behave. (...) It is as if they were able to behave. (...) It is as if they were able to behave. (...) It is as if they were able to overtake each other. (...) It is as if they were able to overtake each other. (...) It is as if they were able to overtake each other. (...) It is as if they were able to overtake each other. (...) It is as if they are able to overtake each other. (...) It is as if they are able to overtake each other. (...) It is as if they are able to overtake each other. (...) It is as if they are able to overtake each other. (...) It is as if they are able to overtake each other. (...)"}, {"heading": "3 Bias and Instrumental Variables", "text": "Kernel eigenmap methods are very good at reducing dimensionality when the original data depicts a high-dimensional manifold relatively densely, and when the noise in each sample is small compared to the local curvature of the manifold (or when we don't care if the curvature on a scale is smaller than the noise). In practice, however, observations are often loud. Depending on the type of noise, multidimensional learning algorithms applied to these datasets produce distorted embeddings. See Figures 1-2, the \"loud Swiss rollers,\" for an example of how noise can distort manifold learning algorithms. To see why we study PCA, a special case of manifold learning methods, and consider why it produces one-sided embeddings in the presence of noise. We will first show how to overcome this problem in a linear case, and the original algorithm actually looks for the smallest eigenvalues from C."}, {"heading": "3.1 Bias in Finite-Dimensional Linear Models", "text": "Suppose xi is a noisy view of any underlying low-dimensional latent variable zi: xi = Mzi + i for a linear transformation M and i.i.d. zero mean of noise expression i. Without loss of generality, we assume xi and zi are centered and that Cov [zi] and M both have the full column rank: each component of zi in zero space of M has no effect on xi. In this case, PCA on X will generally not restore Z: the expectation of \u03a3-XX = 1 nXX T is M Cov [zi] M T + Cov [i], while we need M Cov [zi] M T to restore a transformation of M or Z."}, {"heading": "3.1.1 Instrumental Variables", "text": "We can solve this problem for linear embedding: instead of the simple PCA, we can use what could be called two-dimensional noise. This method finds a statistically consistent solution by using an instrumental variable [11, 12], an observation yi that is correlated with the true latent variables, but not with the noise in xi.It is important that the selection of an instrumental variable is not just a statistical help, but rather a value judgment about the nature of the latent variables and noise in the observations. In particular, we define noise as that part of the variable that is not correlated with the instrumental variable, and the signal as that part that is correlated. In our example above, a good instrumental variable yi is another (loud) view of the same underlying low latent variable: yi = Nzi + \u0441i for any complete linear transformation N and i.i.0 mean Y is the expectation."}, {"heading": "3.1.2 Whitening: Reduced-Rank Regression and Cannonical Correlation Analysis", "text": "Beyond two subranges of PCA, there are a number of interesting spectral compositions of cross-covariance matrices where the xi and yi variables need to be transformed before a single-value decomposition is applied [12]. In the reduced-rank regression [20, 12], for example, we want to estimate E [xi | yi]. Let's define the degree of regression Y = 1 nYY T and vice versa [XY = 1 nXY T]. Then the ordinary regression of Y is transferred to the series of rank-k matrices: < U, D > = SVD (V > 0), with the regularization term \"I\" ensuring that the matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix is well defined."}, {"heading": "3.2 Bias in Learning Nonlinear Models", "text": "We now extend the analysis from Section 3.1 to nonlinear models. We assume that noisy observations are xi = f (zi) + i, where zi is the desired low-dimensional latent variable, i is an i.i.d. noise term, and f is a smooth function with smooth inversion (so that f (zi) lies on a multiple). Our goal is to restore f and zi to identifiability. Kernel PCA (Sec. 2.1) is a common approach to this problem. In the realizable case, Kernel PCA gets the correct answer: that is, we assume zi has dimension k and that we have at least k independent samples, and suppose \u03c6 (f (z)) has a linear function of, for example, then the gram matrix or the covariance \"matrix\" will rank k, and we can reconstruct a basis for the range of p (f) eigenvectors of the matrix."}, {"heading": "3.2.1 Instrumental Variables", "text": "In analogy to the two-dimensional PCA, a natural generalization of the kernel PCA is a two-dimensional subordinate function of the PCA, which we can achieve via a kernel SVD of an operator with cross covariance in the Hilbert space. In view of a common distribution P [X, Y] over two variables X on X and Y on Y, with characteristic maps \u03c6 and \u0432 (corresponding to the nuclei Kx and Ky), the operator with cross covariance XY can be regarded as E [X) (y). The operator with cross covariance is reduced to an ordinary cross-covariance matrix in the end-dimensional case; in the case with cross covariance XY, it can be regarded as an average map descriptor [22] for the common distribution P [X, Y]. The concept of a cross-covariance operator is helpful because it allows us to extend the methods of the instrumental variables to the end-dimensional RKHSs."}, {"heading": "3.2.2 SVD via Gram matrices", "text": "We start by looking at a gram matrix formulation of finite dimensional SVDs. In the standard SVD, the singular values of \u03a3 XY = 1 n (XH) (YH) T are the square roots of the eigenvalues of \u03a3 XY \u03a3 Y X (where \u03a3 Y = \u03a3 Y) T (XH) T and BY = 1 n (YH) T (YH) T (YH) T (YH). We can find identical eigenvectors and eigenvalues by centered gram matrices BX = 1 n (XH) T (XH) and BY = 1 n (YH) T (YH). Let us leave vi = the right eigenvector of BY BX \u2212 \u2212 so that BY BXvi = \u03bbivi. Pre-multiplying with (XH) results in 1n2 (XH) T (YH) (XH) T (XH) T (XH) \u2212 and the regrouping of terms results in a corresponding XY-Y value and vice versa."}, {"heading": "3.2.3 Two-subspace PCA in RKHSs", "text": "The machinery developed in Section 3.2.2 allows us to solve the problem of the two-space nucleus PCA by calculating the singular values of the weighted empirical covariance operator \u03a3-PXY. We define GX and GY as the gram matrices whose elements are Kx (xi, xj) and Ky (yi, yj), respectively, and then calculate the eigencomposition of the CY vectors CX = (PY HGY HPY) (PXHGXPX). This method avoids any calculations in infinite dimensional spaces and provides us with compact representations of the left and right singular vectors. For example, if vi is a right eigenvector of CY CX, then the corresponding singular vector is wi = planned j vi, j \u00b2 vector (j \u00b2) another (xj \u2212 x \u00b2) pxj, where pxj is assigned the weight of the data point."}, {"heading": "3.2.4 Whitening and Kernel SVD", "text": "Just as you can use the kernel SVD to solve the PCA problem of the two-dimensional space for high or infinite dimensional functionality, you can also compute the kernel versions of CCA and RRR. (Kernel CCA is a well-known algorithm, although our formulation is different here than in [23], and the kernel RRR is new to our knowledge.) Again, these problems can be solved by binding the functionality before applying the kernel SVD. To calculate an RRR from centered covariants HUK in the Hilbert space, to center answers HUK in the Hilbert space, we find the kernel SVD of the HUK white covariance matrix: first we define BX = HGXH and BY = HGY H; next we compute BXBY-Y positions (B 2 Y + 1BY shape) \u2212 1BY; and finally we run the kernel singular BY by including the value of the BY-DeulD (BY-B5B)."}, {"heading": "4 Two-Manifold Problems", "text": "This year it is more than ever before."}, {"heading": "5 Two-Manifold Detailed Example: Learning Dynamical Systems", "text": "A long-standing goal in machine learning and robotics has been to learn precise, economic models of dynamic systems directly from observations. This task requires two interrelated subtasks: 1) to learn a low-dimensional state space, which is often known to be based on diversity; and 2) to learn system dynamics. We propose to address this problem by combining spectral learning algorithms for nonlinear dynamic systems [14, 15, 16, 18, 25, 17] with two methods. Each of the spectral learning algorithms cited above can be combined with two methods. Here, we will focus on a specific example: We will show how to combine HSE-HMMs [17], a powerful non-parametric approach to modeling dynamic systems, with manifold learning. Specifically, we will consider an important step in the HSE-HMM learning algorithm: The original approach uses the kernel to detect the SVD to yield a low-dimensional MSE and the SE-SE method."}, {"heading": "5.1 Hilbert Space Embeddings of HMMs", "text": "The key idea behind the spectral learning of dynamic systems is that a good latent state is one that allows us to predict the future. HSE HMMs implement this idea by finding a low-dimensional embedding of the conditional probability distribution of sequences of future observations. Song et al. [17] suggest using this low-dimensional state space as a subspace of an infinite dimensional RKHS. Intuitively, we might think that we can find the best state space by performing PCA or kernel PCA of the sequences of future observations. That is, we are trying n sequences of future observations x1,."}, {"heading": "5.2 Manifold HSE-HMMs", "text": "In contrast to HSE HMMs, we are interested in modeling a dynamic system whose state space is on a low-dimensional multiplicity, even if that multiplicity is curved to take up a higher-dimensional subspace (an example is given in Section 5.3 below). We want to use this additional knowledge to narrow the learning algorithm and build a more accurate model for a given amount of training data by replacing the kernel SVD with a two-dimensional method. That is, we learn weighted centered gram matrices CX and CY for future and past observations, using a varied method such as LE or LLE (see Section 2.2). Then, we apply an SVD to CXCY to restore the latent state space. 7Other approaches such as CCA and RRR have also been successfully used for end-dimensional spectral learning algorithms [18], suggesting that kernel could be replaced by SVA (Kernel 3.CCR) or RD RR."}, {"heading": "5.3 Slotcar: A Real-World Dynamical System", "text": "Here we look at the problem of tracking and predicting the position of a slot car with attached inertial measuring unit (IMU) racing around a racetrack. The setup consisted of a racetrack and a miniature car (1: 32 scale model) that was guided through a slot into the racetrack. Figure 3 (A) shows that setup. At a rate of 10 Hz we extracted the estimated 3-D acceleration and angular speed of the car. An overhead camera also provided estimates of the 2-dimensional position of the car. We collected 3000 consecutive observations while the slot car was controlled by a constant policy (at different speeds). The goal was to learn a dynamic model of the noisy IMU data and, after filtering, to predict current and future 2-dimensional locations. We used the first 2000 data points as training data and held the last 500 data points to test the learned models."}, {"heading": "6 Related Work", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "7 Conclusion", "text": "In this paper, we propose a class of problems known as two-dimensional problems, where two sets of corresponding data points generated from a single latent multiplicity and corrupted by noise are located on or near two different higher-dimensional multiplicities. We design algorithms by associating dual problems with cross-covariance operators in RKHSs, and show that these algorithms result in a significant improvement over traditional multidimensional learning approaches in the presence of noise. This is an appealing result: Multiple learning algorithms typically assume that observations are (nearly) noiseless, an assumption rarely fulfilled in practice. In addition, we demonstrate the utility of two-dimensional problems by extending a newer dynamic system identification algorithm to learn a system with a multidimensional state. The resulting algorithm meets the current state of the art of predicting algorithm more accurately."}, {"heading": "Acknowledgements", "text": "Byron Boots and Geoffrey J. Gordon were supported by ONR MURI grant number N00014-09-1-1052. Byron Boots was supported by NSF grant number EEEC-0540865."}], "references": [{"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["Joshua B. Tenenbaum", "Vin De Silva", "John Langford"], "venue": "Science, 290:2319\u20132323,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Sam T. Roweis", "Lawrence K. Saul"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["Mikhail Belkin", "Partha Niyogi"], "venue": "Neural Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Learning a kernel matrix for nonlinear dimensionality reduction", "author": ["Kilian Q. Weinberger", "Fei Sha", "Lawrence K. Saul"], "venue": "Proceedings of the 21st International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Spectral dimensionality reduction via maximum entropy", "author": ["Neil D. Lawrence"], "venue": "In Proc. AISTATS,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Nonlinear component analysis as a kernel eigenvalue problem", "author": ["Bernhard Sch\u00f6lkopf", "Alex J. Smola", "Klaus- Robert M\u00fcller"], "venue": "Neural Computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "A kernel view of the dimensionality reduction of manifolds", "author": ["Jihun Ham", "Daniel D. Lee", "Sebastian Mika", "Bernhard Schlkopf"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Neighborhood smoothing embedding for noisy manifold learning", "author": ["Guisheng Chen", "Junsong Yin", "Deyi Li"], "venue": "In GrC,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Robust local tangent space alignment via iterative weighted PCA", "author": ["Yubin Zhan", "Jianping Yin"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1985}, {"title": "Causality: models, reasoning, and inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Principal component analysis", "author": ["I.T. Jolliffe"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["Steven J. Bradtke", "Andrew G. Barto"], "venue": "In Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "A spectral algorithm for learning hidden Markov models", "author": ["Daniel Hsu", "Sham Kakade", "Tong Zhang"], "venue": "In COLT,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Reduced-rank hidden Markov models", "author": ["Sajid Siddiqi", "Byron Boots", "Geoffrey J. Gordon"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS-2010),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Closing the learning-planning loop with predictive state representations", "author": ["Byron Boots", "Sajid M. Siddiqi", "Geoffrey J. Gordon"], "venue": "In Proceedings of Robotics: Science and Systems VI,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Hilbert space embeddings of hidden Markov models", "author": ["L. Song", "B. Boots", "S.M. Siddiqi", "G.J. Gordon", "A.J. Smola"], "venue": "In Proc. 27th Intl. Conf. on Machine Learning (ICML),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Predictive state temporal difference learning", "author": ["Byron Boots", "Geoff Gordon"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Diffusion maps, spectral clustering and eigenfunctions of fokkerplanck operators", "author": ["Boaz Nadler", "St\u00e9phane Lafon", "Ronald R. Coifman", "Ioannis G. Kevrekidis"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Multivariate Reduced-rank Regression: Theory and Applications", "author": ["Gregory C. Reinsel", "Rajabather Palani Velu"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "The most predictable criterion", "author": ["Harold Hotelling"], "venue": "Journal of Educational Psychology,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1935}, {"title": "A Hilbert space embedding for distributions", "author": ["A.J. Smola", "A. Gretton", "L. Song", "B. Sch\u00f6lkopf"], "venue": "Algorithmic Learning Theory, Lecture Notes on Computer Science. Springer,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Consistency of kernel canonical correlation analysis", "author": ["K. Fukumizu", "F. Bach", "A. Gretton"], "venue": "Technical Report 942,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Hilbert space embeddings of conditional distributions", "author": ["L. Song", "J. Huang", "A. Smola", "K. Fukumizu"], "venue": "In Proceedings of the International Conference on Machine Learning (ICML),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "An online spectral learning algorithm for partially observable nonlinear dynamical systems", "author": ["Byron Boots", "Sajid Siddiqi", "Geoffrey Gordon"], "venue": "In Proceedings of the 25th National Conference on Artificial Intelligence (AAAI-2011),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Subspace Identification for Linear Systems: Theory, Implementation, Applications", "author": ["P. Van Overschee", "B. De Moor"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1996}, {"title": "Maximum covariance unfolding: Manifold learning for bimodal data", "author": ["Vijay Mahadevan", "Chi Wah Wong", "Jose Costa Pereira", "Tom Liu", "Nuno Vasconcelos", "Lawrence Saul"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Semisupervised alignment of manifolds", "author": ["Jihun Ham", "Daniel Lee", "Lawrence Saul"], "venue": "10th International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "A general framework for manifold alignment", "author": ["Chang Wang", "Sridhar Mahadevan"], "venue": "In Proc. AAAI,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Estimating linear restrictions on regression coefficients for multivariate normal distributions", "author": ["T.W. Anderson"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1951}, {"title": "Reduced-rank regression for the multivariate linear model", "author": ["Alan Julian Izenman"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1975}, {"title": "Sliced inverse regression for dimension reduction", "author": ["Ker-Chau Li"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1991}, {"title": "Theory and methods: Special invited paper: Dimension reduction and visualization in discriminant analysis (with discussion)", "author": ["R. Dennis Cook", "Xiangrong Yin"], "venue": "Australian and New Zealand Journal of Statistics,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "Dimensionality reduction for supervised learning with reproducing kernel hilbert spaces", "author": ["Kenji Fukumizu", "Francis R. Bach", "Michael I. Jordan", "Chris Williams"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2004}, {"title": "Regression on manifolds using kernel dimension reduction", "author": ["Jens Nilsson", "Fei Sha", "Michael I. Jordan"], "venue": "In ICML,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Nonparametric regression between general riemannian manifolds", "author": ["Florian Steinke", "Matthias Hein", "Bernhard Sch\u00f6lkopf"], "venue": "SIAM J. Imaging Sciences,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "A recursive filter for linear systems on riemannian manifolds", "author": ["Ambrish Tyagi", "James W. Davis"], "venue": "Computer Vision and Pattern Recognition, IEEE Computer Society Conference on,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2008}, {"title": "Temporal extension of laplacian eigenmaps for unsupervised dimensionality reduction of time series", "author": ["Michal Lewandowski", "Jes\u00fas Mart\u0301\u0131nez del Rinc\u00f3n", "Dimitrios Makris", "Jean-Christophe Nebel"], "venue": "In ICPR,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Learning nonlinear manifolds from time series", "author": ["Ruei sung Lin", "Che bin Liu", "Ming hsuan Yang", "Narendra Ahuja", "Stephen Levinson"], "venue": "In In Proc. ECCV,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2006}, {"title": "Simultaneous learning of nonlinear manifold and dynamical models for high-dimensional time series", "author": ["Rui Li", "Stan Sclaroff Phd", "Margrit Betke Phd", "David J. Fleet. S"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "These kernel eigenmap methods include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], Maximum Variance Unfolding (MVU) [4], and Maximum Entropy Unfolding (MEU) [5].", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "These kernel eigenmap methods include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], Maximum Variance Unfolding (MVU) [4], and Maximum Entropy Unfolding (MEU) [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 2, "context": "These kernel eigenmap methods include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], Maximum Variance Unfolding (MVU) [4], and Maximum Entropy Unfolding (MEU) [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "These kernel eigenmap methods include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], Maximum Variance Unfolding (MVU) [4], and Maximum Entropy Unfolding (MEU) [5].", "startOffset": 149, "endOffset": 152}, {"referenceID": 4, "context": "These kernel eigenmap methods include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], Maximum Variance Unfolding (MVU) [4], and Maximum Entropy Unfolding (MEU) [5].", "startOffset": 190, "endOffset": 193}, {"referenceID": 5, "context": "These approaches can be viewed as kernel principal component analysis [6] with specific choices of manifold kernels [7]: they seek a small set of latent variables that, through a nonlinear mapping, explains the observed high-dimensional data.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "These approaches can be viewed as kernel principal component analysis [6] with specific choices of manifold kernels [7]: they seek a small set of latent variables that, through a nonlinear mapping, explains the observed high-dimensional data.", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "Several authors have attacked this problem using methods including neighborhood smoothing [8] and robust principal components analysis [9, 10], with some success under limited noise.", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "Several authors have attacked this problem using methods including neighborhood smoothing [8] and robust principal components analysis [9, 10], with some success under limited noise.", "startOffset": 135, "endOffset": 142}, {"referenceID": 9, "context": "Instrumental variables have been used to allow consistent estimation of model parameters in many statistical learning problems, including linear regression [11], principal component analysis [12], and temporal difference learning [13].", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "Instrumental variables have been used to allow consistent estimation of model parameters in many statistical learning problems, including linear regression [11], principal component analysis [12], and temporal difference learning [13].", "startOffset": 191, "endOffset": 195}, {"referenceID": 11, "context": "Instrumental variables have been used to allow consistent estimation of model parameters in many statistical learning problems, including linear regression [11], principal component analysis [12], and temporal difference learning [13].", "startOffset": 230, "endOffset": 234}, {"referenceID": 12, "context": "As an example of this last point, subspace identification approaches to learning non-linear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinitedimensional) covariance operators [14, 15, 16, 17, 18].", "startOffset": 243, "endOffset": 263}, {"referenceID": 13, "context": "As an example of this last point, subspace identification approaches to learning non-linear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinitedimensional) covariance operators [14, 15, 16, 17, 18].", "startOffset": 243, "endOffset": 263}, {"referenceID": 14, "context": "As an example of this last point, subspace identification approaches to learning non-linear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinitedimensional) covariance operators [14, 15, 16, 17, 18].", "startOffset": 243, "endOffset": 263}, {"referenceID": 15, "context": "As an example of this last point, subspace identification approaches to learning non-linear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinitedimensional) covariance operators [14, 15, 16, 17, 18].", "startOffset": 243, "endOffset": 263}, {"referenceID": 16, "context": "As an example of this last point, subspace identification approaches to learning non-linear dynamical systems depend critically on instrumental variables and the spectral decomposition of (potentially infinitedimensional) covariance operators [14, 15, 16, 17, 18].", "startOffset": 243, "endOffset": 263}, {"referenceID": 5, "context": "Kernel PCA [6] is a generalization of principal component analysis [12]: we first map our d-dimensional inputs x1, .", "startOffset": 11, "endOffset": 14}, {"referenceID": 10, "context": "Kernel PCA [6] is a generalization of principal component analysis [12]: we first map our d-dimensional inputs x1, .", "startOffset": 67, "endOffset": 71}, {"referenceID": 5, "context": "length eigenvectors of \u03a3\u0302XX are given by \u03a6Hvi\u03bb \u22121/2 i , where \u03bbi and vi are the eigenvalues and eigenvectors of HGH [6].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "Spectral algorithms for manifold learning, sometimes called kernel eigenmap methods, include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], and Maximum Variance Unfolding (MVU) [4].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "Spectral algorithms for manifold learning, sometimes called kernel eigenmap methods, include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], and Maximum Variance Unfolding (MVU) [4].", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "Spectral algorithms for manifold learning, sometimes called kernel eigenmap methods, include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], and Maximum Variance Unfolding (MVU) [4].", "startOffset": 166, "endOffset": 169}, {"referenceID": 3, "context": "Spectral algorithms for manifold learning, sometimes called kernel eigenmap methods, include Isomap [1], Locally Linear Embedding (LLE) [2], Laplacian Eigenmaps (LE) [3], and Maximum Variance Unfolding (MVU) [4].", "startOffset": 208, "endOffset": 211}, {"referenceID": 6, "context": "Interestingly, these algorithms can be viewed as special cases of kernel PCA where the Gram matrix G is constructed over the finite domain of the training data in a particular way [7].", "startOffset": 180, "endOffset": 183}, {"referenceID": 4, "context": ", see [5].", "startOffset": 6, "endOffset": 9}, {"referenceID": 17, "context": "A minor difference is that LE does not scale its embedding using the eigenvalues of G; the related diffusion maps algorithm [19] does.", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "This method finds a statistically consistent solution through the use of an instrumental variable [11, 12], an observation yi that is correlated with the true latent variables, but uncorrelated with the noise in xi.", "startOffset": 98, "endOffset": 106}, {"referenceID": 10, "context": "This method finds a statistically consistent solution through the use of an instrumental variable [11, 12], an observation yi that is correlated with the true latent variables, but uncorrelated with the noise in xi.", "startOffset": 98, "endOffset": 106}, {"referenceID": 10, "context": "Going beyond two-subspace PCA, there are a number of interesting spectral decompositions of crosscovariance matrices that involve transforming the variables xi and yi before applying a singular value decomposition [12].", "startOffset": 214, "endOffset": 218}, {"referenceID": 18, "context": "For example, in reduced-rank regression [20, 12], we want to estimate E [xi | yi].", "startOffset": 40, "endOffset": 48}, {"referenceID": 10, "context": "For example, in reduced-rank regression [20, 12], we want to estimate E [xi | yi].", "startOffset": 40, "endOffset": 48}, {"referenceID": 19, "context": "An SVD of the resulting doubly-whitened cross-covariance matrix (\u03a3\u0302XX + \u03b7I)\u03a3\u0302XY (\u03a3\u0302Y Y + \u03b7I) \u22121/2 is called canonical correlation analysis [21], and the resulting singular values are called canonical correlations.", "startOffset": 139, "endOffset": 143}, {"referenceID": 20, "context": "The cross-covariance operator reduces to an ordinary cross-covariance matrix in the finite-dimensional case; in the infinite-dimensional case, it can be viewed as a kernel mean map descriptor [22] for the joint distribution P[X,Y ].", "startOffset": 192, "endOffset": 196}, {"referenceID": 15, "context": "The kernel SVD algorithm previously appeared as an intermediate step in [17, 23]; here we generalize the algorithm to weighted cross covariance operators in Hilbert space and give a more complete description, both because the method is interesting in its own right, and because this generalized SVD will serve as a step in our two-manifold algorithms.", "startOffset": 72, "endOffset": 80}, {"referenceID": 21, "context": "The kernel SVD algorithm previously appeared as an intermediate step in [17, 23]; here we generalize the algorithm to weighted cross covariance operators in Hilbert space and give a more complete description, both because the method is interesting in its own right, and because this generalized SVD will serve as a step in our two-manifold algorithms.", "startOffset": 72, "endOffset": 80}, {"referenceID": 15, "context": "The remainder of the proof follows from the proof of Theorem 1 in [17] (the convergence of the empirical estimator of the kernel covariance operator).", "startOffset": 66, "endOffset": 70}, {"referenceID": 21, "context": "(Kernel CCA is a well-known algorithm, though our formulation here is different than in [23], and kernel RRR is to our knowledge novel.", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "The consistency of both approaches follows directly from the consistency of kernel SVD, so long as we let \u03b7 \u2192 0 as n\u2192\u221e [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.", "startOffset": 108, "endOffset": 132}, {"referenceID": 13, "context": "We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.", "startOffset": 108, "endOffset": 132}, {"referenceID": 14, "context": "We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.", "startOffset": 108, "endOffset": 132}, {"referenceID": 16, "context": "We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.", "startOffset": 108, "endOffset": 132}, {"referenceID": 23, "context": "We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.", "startOffset": 108, "endOffset": 132}, {"referenceID": 15, "context": "We propose tackling this problem by combining spectral learning algorithms for non-linear dynamical systems [14, 15, 16, 18, 25, 17] with two-manifold methods.", "startOffset": 108, "endOffset": 132}, {"referenceID": 15, "context": "Here, we focus on one specific example: we show how to combine HSE-HMMs [17], a powerful nonparametric approach to modeling dynamical systems, with manifold learning.", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "[17] suggest finding this low-dimensional state space as a subspace of an infinite dimensional RKHS.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Other approaches such as CCA and RRR have also been successfully used for finite-dimensional spectral learning algorithms [18], suggesting that kernel SVD could be replaced by kernel CCA or kernel RRR (Section 3.", "startOffset": 122, "endOffset": 126}, {"referenceID": 15, "context": "[17], using sequences of 150 consecutive observations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": ") Third, for comparison\u2019s sake, we trained a 20-dimensional Kalman filter using the N4SID algorithm [26] with Hankel matrices of 150 time steps; and finally, we learned a 20-state discrete HMM (with 400 levels of discretization for observations) using the EM algorithm.", "startOffset": 100, "endOffset": 104}, {"referenceID": 25, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "A similar problem to the two-manifold problem is manifold alignment [28, 29], which builds connections between two or more data sets by aligning their underlying manifolds.", "startOffset": 68, "endOffset": 76}, {"referenceID": 27, "context": "A similar problem to the two-manifold problem is manifold alignment [28, 29], which builds connections between two or more data sets by aligning their underlying manifolds.", "startOffset": 68, "endOffset": 76}, {"referenceID": 27, "context": "Generally, manifold alignment algorithms either first learn the manifolds separately and then attempt to align them based on their lowdimensional geometric properties, or they take the union of several manifolds and attempt to learn a latent space that preserves the geometry of all of them [29].", "startOffset": 291, "endOffset": 295}, {"referenceID": 28, "context": "This type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34].", "startOffset": 127, "endOffset": 139}, {"referenceID": 29, "context": "This type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34].", "startOffset": 127, "endOffset": 139}, {"referenceID": 18, "context": "This type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34].", "startOffset": 127, "endOffset": 139}, {"referenceID": 30, "context": "This type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34].", "startOffset": 181, "endOffset": 193}, {"referenceID": 31, "context": "This type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34].", "startOffset": 181, "endOffset": 193}, {"referenceID": 32, "context": "This type of interconnected learning problem has been explored before in a different context via reduced-rank regression (RRR) [30, 31, 20] and sufficient dimension reduction (SDR) [32, 33, 34].", "startOffset": 181, "endOffset": 193}, {"referenceID": 33, "context": "Manifold kernel dimension reduction [35], finds an embedding of covariates xi using a kernel eigenmap method, and then attempts to find a linear transformation of some of the dimensions of the embedded points to predict response variables yi.", "startOffset": 36, "endOffset": 40}, {"referenceID": 34, "context": "There has also been some work on finding a mapping between manifolds [36] and learning a dynamical system on a manifold [37]; however, in both of these cases it was assumed that the manifold was known.", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "There has also been some work on finding a mapping between manifolds [36] and learning a dynamical system on a manifold [37]; however, in both of these cases it was assumed that the manifold was known.", "startOffset": 120, "endOffset": 124}, {"referenceID": 36, "context": "[38] introduces Laplacian eigenmaps that accommodate time series data by picking neighbors based on temporal ordering.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[39] and Li et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[40] propose learning a piecewise linear model that approximates a non-linear manifold and then attempt to learn the dynamics in the low-dimensional space.", "startOffset": 0, "endOffset": 4}], "year": 2011, "abstractText": "Recently, there has been much interest in spectral approaches to learning manifolds\u2014 so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together and allowing information to flow between them, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias in the same way that an instrumental variable allows us to remove bias in a linear dimensionality reduction problem. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space. Finally, we discuss situations where two-manifold problems are useful, and demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.", "creator": "LaTeX with hyperref package"}}}