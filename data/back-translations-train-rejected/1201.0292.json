{"id": "1201.0292", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2011", "title": "T-Learning", "abstract": "Traditional Reinforcement Learning (RL) has focused on problems involving many states and few actions, such as simple grid worlds. Most real world problems, however, are of the opposite type, Involving Few relevant states and many actions. For example, to return home from a conference, humans identify only few subgoal states such as lobby, taxi, airport etc. Each valid behavior connecting two such states can be viewed as an action, and there are trillions of them. Assuming the subgoal identification problem is already solved, the quality of any RL method---in real-world settings---depends less on how well it scales with the number of states than on how well it scales with the number of actions. This is where our new method T-Learning excels, by evaluating the relatively few possible transits from one state to another in a policy-independent way, rather than a huge number of state-action pairs, or states in traditional policy-dependent ways. Illustrative experiments demonstrate that performance improvements of T-Learning over Q-learning can be arbitrarily large.", "histories": [["v1", "Sat, 31 Dec 2011 17:29:08 GMT  (154kb,D)", "http://arxiv.org/abs/1201.0292v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["vincent graziano", "faustino gomez", "mark ring", "juergen schmidhuber"], "accepted": false, "id": "1201.0292"}, "pdf": {"name": "1201.0292.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Vincent Graziano", "Faustino Gomez", "Mark Ring", "J\u00fcrgen Schmidhuber"], "emails": [], "sections": [{"heading": null, "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Motivation and overview", "text": "It is indeed the case that we are able to go in search of a solution that is capable of finding a solution, that is capable of finding a solution that is capable of finding a solution, that is capable of finding a solution and that is capable of finding a solution that is capable of bringing about a solution."}, {"heading": "2 Environments requiring precision", "text": "Consider the transition diagram of an MDP, in which the vertices of the diagram are the states of the environment and the edges represent transitions between states. Define a function \u03c4: S \u2192 S, which maps s to the adjacent vertex s \u00b2, the value of which under the optimal policy, V \u0445 (s \u00b2) is the highest of all neighbors of s, where V \u0445 (s) is calculated on the basis of a given value for the state, as if the agent had available actions in each state that could move him deterministically along the curve of the environment. The class of MDPs for which T-Learning is particularly suitable can be formally described as follows: If s \u03c47 \u2192 s \u00b2 s \u00b2 s \u00b2 s \u00b2, the state becomes agency, then1. E [Pr (s \u00b2 s, an A) >], and2. Pr (s \u00b2 s, a) > 1 \u2212, for some countries for which T-Learning is particularly suitable."}, {"heading": "3 State transition functions", "text": "The general reward for an MDP is a function of three variables, R: S \u00b7 A \u00b7 S \u2192 R. For the reasons discussed below, we consider rewards to be functions of state transitions, regardless of the action taken; i.e., R: S \u00b7 S \u2192 R. We designate the limited function of Rss \u00b2 and the more general reward function of Rass \u2032. We call back TD (0), which learns a function V: S \u2192 R by using the following updating rule V (s), V (s) \u2190 V (s) + \u03b1 [r + \u03b3V (s) \u2212 V (s).For a fixed policy, this function merges with V \u03c0, which is given recursively by V \u03c0 (s)."}, {"heading": "3.1 An on-policy learning rule", "text": "In the rest of the paper, the term transition functions refers to functions T: S \u00b7 S \u2192 R. Their values are called transition values or T values. Consider the following rule of update: T (s, s \") T (s, s\"), s \") + \u03b1 [r + \u03b3T (s,\" s \") \u2212 T (s, s\"). The value T\u03c0 (s, s \") represents the reward for the transition from s\" to s \"plus the cumulative future expected discounted reward within the given policy. Convergence therefore implies the theorem that defines the convergence of the state value function used by TD (0). In a fixed political formula, this function converges to T\u03c0 (s,\" s, \"s\"), s \"s,\" s, \"s,\" s, \"s,\" s \",\" s \",\" s"}, {"heading": "3.2 T-Learning", "text": "Let us now consider a function T: S \u00d7 S \u2192 R, which is learned as follows: T (s, s \u2032) \u2190 T (s, s \u2032) + \u03b1 [r + \u03b3maxs \u2032 T (s \u2032, s \u2032) \u2212 T (s, s \u2032).We call this learning rule T-Learning. This rule captures the values associated with the best available transition. If the behavior of the agent is determined by these values, it becomes possible to search the action space - in the valuable states - to discover the reliable actions. Furthermore, this can be done in a simple and natural way. To take the maximum of the possible state transitions reminds of Q-Learning; instead of capturing the ideal action associated with each state, T-Learning stores the topology of the ideal transitions between states. The ideal transitions between states can be determined without having to use a model. In the state s, the ideal transition is simply argmaxs, that is s \u2192 essay, the value is weighted from step 1 to step 3, without the first step of es. In the example, 1, the transition is weighted from the agent to step 1."}, {"heading": "3.2.1 Appropriate environments for T -learning", "text": "However, these limitations can be relaxed. We refer to the function to which T-learning by T] converges and the Q values of the optimal policy \u03c0 by Q *. We call this criterion the precision characteristic. We call it the precision characteristic because it guarantees that the valuable transitions can be performed with as much reliability as necessary. These reliable measures can be rare among all possible measures and can be considered qualified actions or behaviors. In other words, they are MDPs where measures are available that make the paths to the state transition curve with the highest value (as described in Section 2)."}, {"heading": "4 Experiments", "text": "We compare T -learning toQ-learning in a model of a balance beam environment (TD (0) and other methods are discussed in Section 5). The MDP has 16 states, the transition graph is shown in Figure 1b. Transitions are similar to the smaller version of this environment. We vary the number of actions, 2n + 1, during the experiments. The first n-actions move the agent deterministically from state 1 to 2 and the second n-actions move the agent to state 3. Action a-2n + 1 Transitions of the agent to state 2 Algorithm 1: Action selection using T values. The previous data points {s-1, a-0, r0}. Increment counters Csas and Csa. Current state s = s0 if RandomReal [0, 1] < then A-Update P-s as follows."}, {"heading": "5 Results and Discussion", "text": "In fact, it is not so that one sees oneself in a position to adhere to the rules that one has imposed on oneself. (...) It is not so that one adheres to the rules. (...) It is not so that one adheres to the rules. (...) It is not so that one adheres to the rules. (...) \"It is not so that one adheres to the rules. (...)\" \"It is as if one adheres to the rules. (...)\" It is as if one adheres to the rules. (...) \"It is as if one adheres to the rules.\" (...) \"It is not so, as if one adheres to the rules.\" (...). \"(.)\" (.) \"(.). (.)\" (.) \"(.)\" (.) \"(. (.)\" (.) \"(. (.)\" (.) \"(.\" (.) \"(.)\" (. \"(.)\" (.) \"(.\" (.) \"(.)\" (. \"(.)\" (.) \"(.\" (.) \"(.)\" (.). \"(.)\" (.) \"(.).\" (. \"(.)\" (.). \"(.). (.). (.).\" (. \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (. \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(It is.\" (. \"(.). (.). (.).\" (.).). \"(It is. (. (.). (.). (.).). (. (.). (.). (.).).). (It is. (It is.)."}, {"heading": "6 Conclusion", "text": "The T-Learning algorithm learns values that are fundamentally different from Q-Learning, and enables an agent to quickly identify the valuable transitions in an environment, regardless of the size of the scope for action. The behavior that T-Learning exhibits allows an agent to experiment with the environment in a way that amounts to focused learning of a skill. As a result, in realistic scenarios, T-Learning can behave arbitrarily better than Q-Learning."}], "references": [{"title": "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization", "author": ["B. Bakker", "J. Schmidhuber"], "venue": "F. G. et al., editor, Proc. 8th Conference on Intelligent Autonomous Systems IAS-8, pages 438\u2013445, Amsterdam, NL", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Building portable options: skill transfer in reinforcement learning", "author": ["G. Konidaris", "A. Barto"], "venue": "Proceedings of the 20th international joint conference on Artifical intelligence, pages 895\u2013900, San Francisco, CA, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Prioritized sweeping: Reinforcement learning with less data and less time", "author": ["A.W. Moore", "C.G. Atkeson"], "venue": "Machine Learning, pages 103\u2013130", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1993}, {"title": "Learning options in reinforcement learning", "author": ["M. Stolle", "D. Precup"], "venue": "Lecture Notes in Computer Science, pages 212\u2013223", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Dyna, an integrated architecture for learning, planning, and reacting", "author": ["R.S. Sutton"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "Learning from Delayed Rewards", "author": ["C. Watkins"], "venue": "PhD thesis, King\u2019s College,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Q-learning", "author": ["C.J.C.H. Watkins", "P. Dayan"], "venue": "Machine Learning, 8:279\u2013292", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 4, "context": "T -learning is a temporal-difference (TD) method [6], and as such it has much in common with other TD-methods, especially action-value methods, such as Sarsa and Q-learning [8, 9].", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "T -learning is a temporal-difference (TD) method [6], and as such it has much in common with other TD-methods, especially action-value methods, such as Sarsa and Q-learning [8, 9].", "startOffset": 173, "endOffset": 179}, {"referenceID": 7, "context": "T -learning is a temporal-difference (TD) method [6], and as such it has much in common with other TD-methods, especially action-value methods, such as Sarsa and Q-learning [8, 9].", "startOffset": 173, "endOffset": 179}, {"referenceID": 0, "context": "Current state s = s0 if RandomReal[0, 1] < then A\u25e6 \u2190 A else Update P\u0302s as follows: if Csa = 0 then Get number n of states s\u2032 transitioned to from s.", "startOffset": 34, "endOffset": 40}, {"referenceID": 3, "context": "This is a remarkable feature of the T-Learning algorithm and might play an important role in the so-called options [4] framework, discussed below.", "startOffset": 115, "endOffset": 118}, {"referenceID": 5, "context": "For example, Dyna-Q [7] which learn a model and take advantage of planning to speed learning, still needs to first find the actions that represent the skilled movement(s) before the learning is sped-up.", "startOffset": 20, "endOffset": 23}, {"referenceID": 2, "context": "Using TD(0) under the hood of Dyna or Prioritized Sweeping [3] does not address the fundamental problem either.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "On a more abstract level, rather than focusing on single actions, we can consider subgoals [1] or behaviors [2] that transition the agent between relevant states.", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "On a more abstract level, rather than focusing on single actions, we can consider subgoals [1] or behaviors [2] that transition the agent between relevant states.", "startOffset": 108, "endOffset": 111}], "year": 2013, "abstractText": "Traditional Reinforcement Learning (RL) has focused on problems involving many states and few actions, such as simple grid worlds. Most real world problems, however, are of the opposite type, Involving Few relevant states and many actions. For example, to return home from a conference, humans identify only few subgoal states such as lobby, taxi, airport etc. Each valid behavior connecting two such states can be viewed as an action, and there are trillions of them. Assuming the subgoal identification problem is already solved, the quality of any RL method\u2014in real-world settings\u2014depends less on how well it scales with the number of states than on how well it scales with the number of actions. This is where our new method T-Learning excels, by evaluating the relatively few possible transits from one state to another in a policy-independent way, rather than a huge number of state-action pairs, or states in traditional policy-dependent ways. Illustrative experiments demonstrate that performance improvements of T-Learning over Q-learning can be arbitrarily large. 1 Motivation and overview Traditional Reinforcement Learning (RL) has focused on problems involving many states and few actions, such as simple grid worlds. Most real world problems, however, are of the opposite type, involving few relevant states and many actions. For example, to return home from a conference, humans identify only few subgoal states such as lobby, taxi, airport etc. Each valid behavior connecting two such states can be viewed as an action, and there are trillions of them. Assuming the subgoal identification problem is already solved by a method outside the scope of this paper, the quality of any RL method\u2014in real-world settings\u2014depends less on how well it scales with the number of states than on how well it scales with the number of actions. Likewise, when we humans reach an unfamiliar state, we generally resist testing every possible action before determining the good states to transition to. We can, for example, observe the state transitions that other humans progress through while accomplishing the same task, or reach some rewarding state by happenstance. Then we can focus on reproducing that sequence of states. That is, we are able to first identify a task before acquiring the skills to reliably perform it. Take for example the task of walking along a balance beam. In order to traverse the length of the beam without falling, a precise action must be chosen at every step from a very large set of possibilities. The probability of failure is high because almost all actions at every step lead to imbalance and falling, and therefore a good deal of training is required to learn the precise movements that reliably take one across. However, throughout the procedure the desired trajectory of states is well understood; the more difficult part is achieving them reliably. Reinforcement-learning methods that learn action values, such as Q-learning, Sarsa, and TD(0) are guaranteed to converge to the optimal value function provided all state-action pairs in the underlying MDP are visited infinitely often. These methods therefore can converge extremely slowly in environments with large action spaces. Technical Report No. IDSIA-XX-11-2011 2 This paper introduces an elegant new algorithm that automatically focuses search in action space by learning state-transition values independent of action. We call the method T -learning, and it represents a novel off-policy approach to reinforcement learning. T -learning is a temporal-difference (TD) method [6], and as such it has much in common with other TD-methods, especially action-value methods, such as Sarsa and Q-learning [8, 9]. But it is quite different. Instead of learning the values of state-action pairs as action-value methods do, it learns the values of state-state pairs (here referred to as transitions). The value of the transitions between states is recorded explicitly, rather than the value of the states themselves or the value of state-action pairs. The learning task is decomposed into two separate and independent components: (1) the learning of transition values, (2) the learning of the optimal actions. The transition-value function allows high payoff transitions to be easily identified, allowing for a focused search in action space to discover those actions that make the valuable transitions reliably. Agents that learn the values of state-transitions can exhibit markedly different behavior from those that learn state-action pairs. Action-value methods are particularly suited to tasks with small action spaces where learning about all state-action pairs is not much more cumbersome than learning about the states alone. However, as the size of the action space increases, such methods become less feasible. Furthermore, action-value methods have no explicit mechanism for identifying valuable state transitions and focusing learning there. They lack an important\u2014real-world\u2014bias: that valuable state transitions can often be achieved with high reliability. As a result, in these common situations, action-value methods require extensive and undue search before converging to an optimal policy. T -learning, on the other hand, has an initial bias: it presumes the existence of reliable actions that will achieve any valuable transition yet observed. This bias enables the valuable transitions to be easily identified and search to be focused there. As a result, the difficulties induced by large action spaces are significantly reduced. 2 Environments requiring precision Consider the transition graph of an MDP, where the vertices of the graph are the states of the environment and the edges represent transitions between states. Define a function \u03c4 : S \u2192 S that maps s to the neighboring vertex s\u2032 whose value under the optimal policy, V \u2217(s\u2032) is the highest of all the neighbors of s, where V \u2217(s) is calculated using a given value for \u03b3 as though the agent had actions available in every state that can move it deterministically along the graph of the environment. The class of MDPs for which T-Learning is particularly suited can be described formally as follows: If s \u03c4 7\u2192 s\u2032, then 1. E[Pr(s\u2032|s, a \u2208 A) > ], and 2. Pr(s\u2032|s, a\u2217) > 1\u2212 , for some a\u2217 \u2208 A, where is a small positive value. These environments are those where specific skills can accomplish tasks reliably. Walking across a balance beam, for example, requires specific skills. The first constraint ensures that the rewarding transitions are likely to be observed. The second constraint ensures that the transitions associated with large reward signals can be achieved by finding a specific skill, i.e., a reliable action. Without this guarantee, one might never attempt to acquire certain skills because the average outcome during learning may be undesirable. Consider the example of Figure 1a. This MDP has two parts, one requiring high skill (which yields large reward) and one requiring low skill (which yields small reward). Episodes begin in state 1 and end in states 4, 5, and 6. There are 2n + 1 actions and the transition table is defined as follows: from state 1, n actions, {a1, . . . , an}, take the agent to state 2 deterministically; n actions {an+1, . . . , a2n} take the agent to state 3 deterministically, and one action, a\u2217 \u2261 a2n+1, takes the agent to either state 2 or 3 with equal probability. All actions from state 2 take the agent to state 4, ending the episode. From state 3, 2n actions move the agent to either state 5 or 6 with equal probability, while action a\u2217 moves the agent to state Technical Report No. IDSIA-XX-11-2011 3 R=1.1 2n P(1,{a }, 3)=1 j j P(1,{a }, 3)=1 1 n 1 P(3,{a }, 6)=0.5 j 2n", "creator": "LaTeX with hyperref package"}}}