{"id": "1201.0838", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Jan-2012", "title": "A Topic Modeling Toolbox Using Belief Propagation", "abstract": "Latent Dirichlet allocation (LDA) is an important class of hierarchical Bayesian models for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. This toolbox is implemented by MEX C++/MATLAB platform for either Windows or Linux. The current version includes various learning algorithms for latent Dirichlet allocation (LDA), author-topic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more and more BP-based learning algorithms for various LDA-based topic models will be added in the near future. Interested readers may also extend this toolbox for solving more complicated topic modeling problems. The source code is freely available under the GNU General Public Licence, Version 1.0 at", "histories": [["v1", "Wed, 4 Jan 2012 07:07:06 GMT  (15kb)", "http://arxiv.org/abs/1201.0838v1", null], ["v2", "Thu, 5 Apr 2012 06:48:35 GMT  (9kb)", "http://arxiv.org/abs/1201.0838v2", "4 pages"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jia zeng"], "accepted": false, "id": "1201.0838"}, "pdf": {"name": "1201.0838.pdf", "metadata": {"source": "CRF", "title": "TMBP: A Topic Modeling Toolbox Using Belief Propagation", "authors": ["Jia Zeng"], "emails": ["j.zeng@ieee.org"], "sections": [{"heading": null, "text": "ar Xiv: 120 1.08 38v1 [cs.LG] 4 JLatent Dirichlet allocation (LDA) is an important class of hierarchical Bayesian models for probabilistic topic modeling that attracts worldwide interest and touches on many important applications in text mining, computer vision and computer biology. In this paper, a toolbox for topic modeling (TMBP) based on belief propagation algorithms is presented, which is implemented by MEX C + + / MATLAB for Windows or Linux. The current version includes various learning algorithms for latent dirichlet allocation (LDA), author topic models (ATM), Relational theme models (RTM) and labeled LDA (LaLDA). This toolbox is an ongoing project and more and more and more BP-based learning algorithms for various LDA-based topic models will be added in the near future. Interesting readers can add this toolbox to the solution of more complicated problems."}, {"heading": "1 Introduction", "text": "Over the past decade, latent dirichlet allocation (LDA) [2] has rapidly evolved to solve various topic modeling and computer education problems due to its elegant three-layered graphical representation and two more efficient approximation methods such as Variational Bayes (VB) [2] and Collapsed Gibbs Sampling (GS) [6]. Both VB and GS have been widely used to learn variants of LDA-based topic models, until a recent paper [1] shows that there is yet another learning algorithm for LDA based on the loopy of arbitrary propagation (BP). Extensive experiments show that BP is faster and more precise than both VB and GS and has the potential to become a generic learning scheme for variants of LDA-based topic models. Similar ideas were also proposed within the approximate midfield framework [7] when zero order approximation of broken CVB (B8) algorithms [7]."}, {"heading": "1.1 License", "text": "This program is provided free of charge for research and educational purposes, but for commercial use you will need a license from the authors. Scientific results obtained with the provided software confirm the use of TMBP. When using this toolbox, please quote [1]. The software may be modified and distributed with the prior authorization of the author."}, {"heading": "2 Belief Propagation for Topic Modeling", "text": "In view of the vocabulary 1 \u2264 w \u2264 W and the documents 1 \u2264 d \u2264 D in the corpus \u03b2 = VB, subject modeling means mapping the subject names z = {zkw, d}, z k w, d \u00b2 {0, 1}, \u2211 K = 1 z k w, d = 1, 1 \u2264 k \u2264 K to partition the observed document-word-coexistence matrix x x x = {xw, d} (xw, d is the number of word counts in the index {w, d}) in K topics according to three subject modeling rules: 1. Coexistence: Different word indexes w in the same document d tend to have the same subject title.2. Smoothness: the same word index w in the different documents d tend to have the same subject title.3. Clustering: all word indexes w do not tend to be associated with the same subject title.3."}, {"heading": "3 Using TMBP", "text": "In fact, it is that we are able to hide, and that we are able to hide, \"he said.\" We have to be able to hide, \"he said.\" We have to be able to hide. We have to be able to hide, \"he said.\" We have to be able to hide, \"he said.\" We have to be able to show that we are able to hide, \"he said."}, {"heading": "4 Data Structure", "text": "In this toolbox we add four publicly available document sets [1]: 1) BLOG, 2) CORA, 3) MEDLINE and 4) NIPS. Without losing generality we will use CORA as an example to introduce the data structure. CORA data set has five files in the standard MATLAB *.mat format. We load these variables into the MATLAB environment: > > load cora _ wd > > load cora _ voc > > load cora _ ad > > load cora _ author > > load cora _ dd"}, {"heading": "4.1 Word Count Matrix", "text": "The variable cora _ wd is a sparse W \u00b7 D matrix, where W is the size of the vocabulary, D is the total number of documents in the corpus and each element is the number of words xw, d 6 = 0. We can visualize this sparse matrix by spy command: > > spy (cora _ wd); We show parts of the contents in the first document: > > cora _ wd (1: 10.1) ans = (1.1) 1 (2.1) 4 (3.1) 2 (4.1) 2 (4.1) 2 (6.1) 2 (7.1) 1 (8.1) 2 (9.1) 1 (10.1) 1We can transform this sparse matrix into full matrix and vice versa, > cora _ wd = full (cora _ wd); > cora _ wd = sparse (cora _ wd); Note that all tools can only process sparse matrix."}, {"heading": "4.2 Vocabulary", "text": "The variable cora _ voc is a W-long cell array for the vocabulary. We show the first ten words in the vocabulary > > cora _ voc (1: 10) ans = \"Computer\" \"Algorithms\" \"Discovery\" \"Pattern\" \"Groups\" \"Protein\" \"Sequences\" \"Based\" \"Adaptation\" \"Parameters\""}, {"heading": "4.3 Co-author Matrix", "text": "The variable cora _ ad is a sparse A \u00b7 D matrix, where A is the total number of unique authors, D is the total number of documents in the corpus, the element is 1 if and only if the author is associated with the document. We show the co-authors of the first document: > > cora _ ad (:, 1) ans = (1481,1) 1 (2225,1) 1There are two authors with indexes 1481 and 2225 associated with document 1."}, {"heading": "4.4 Author Names", "text": "The cora _ author variable is an A-length cell array for author names. We show two author names with indexes 1481 and 2225 linked to document 1: > > cora _ author ([1481,2225]) ans = 'M Gribskov' \"T Bailey '"}, {"heading": "4.5 Citation Links", "text": "The variable cora _ dd is a sparse D \u00b7 D matrix, where the element 1 is if and only if two documents have a citation link. We show that document 1 has two citations with documents 389 and 484. > > cora _ dd (:, 1) ans = (389.1) 1 (484.1) 1Note that we consider citations as undirected links, where citations and cited documents are not distinguished in topic modeling."}, {"heading": "5 Compilation", "text": "In the MATLAB environment, we need to set up the compiler for MEX files: > > mex -setup Please select your compiler that creates external interface files (MEX): Do you want mex to locate installed compilers [y] / n? ySelect a compiler: [1] Microsoft Visual C + + 2010 Express [0] NoneCompiler: 1Please check your selection: Compiler: Microsoft Visual C + + 2010 Express Location: C:\\ Program Files (x86)\\ Microsoft Visual Studio 10.Are they correct [y] / n? yThen we compile the MEX C + + files using the following commands > mex -largeArrayDims sBPtrain.cppwhere sBPtrain.cpp is the source file."}, {"heading": "6 Parameters", "text": "In the test.m script, we provide the following parameters: ALPHA = 1e-2; BETA = 1e-2; OMEGA = 0.05; N = 20 \u00b7 M = 1; SEED = 1; OUTPUT = 1; J = 10; We explain these parameters as follows: 1. ALPHA and BETA are dirichlet hyperparameters. In real-world applications, the asymmetric previous ALPHA theme may present significant advantages over the symmetric theme, while the asymmetric previous BETA may not. In general, the hyperparameters determine the thrift of the multinomic parameters THETA and PHI that affect the topic of modeling performance."}, {"heading": "7 Latent Dirichlet Allocation (LDA)", "text": "This toolbox mainly contains various source codes for learning LDA, including Variational Bayes (VB) [2], Collapsed Gibbs Sampling (GS) [6] and Belief Propagation (BP) [1]."}, {"heading": "7.1 Variational Bayes (VB)", "text": "We use Bly's implementation of digamma functions.1 [phi, theta, mu] = VBtrain (cora _ wd, J, N, M, ALPHA, ALPHA, SEED, OUTPUT); [phi, theta, mu] = VBtrain (cora _ wd, J, N, M, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = VBpredict (cora _ wd, phi, N, M, ALPHA, BETA, SEED, OUTPUT); [theta, mu] = VBpredict (cora _ wd, phi, N, M, ALPHA, BETA, SEED, OUTPUT, muin); the difference between VBtrain and VBforecast (VBtrain, SEED, OUTPUT, OUTPUT) is that VBpredict theta is fixed for invisible test sets under the parameter i."}, {"heading": "7.2 Collapsed Gibbs Sampling (GS)", "text": "For the GS algorithm, we rewrite the MATLAB codes in the Topic Modeling Toolbox2 for our data structure. [phi, theta, z] = GStrain (cora _ wd, J, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, z] = GStrain (cora _ wd, J, N, ALPHA, BETA, SEED, OUTPUT, zin); [theta, z] = GSpredict (cora _ wd, phi, N, ALPHA, BETA, SEED, zin); [theta, z] = GSpredict (cora _ wd, SEED, OUTPUT, zin); the difference between Gunct and Spredict is that Spredict is visible."}, {"heading": "7.3 Belief Propagation (BP)", "text": "There are three major BP implementations, referred to as CVB0 [8], BP and Simplified BP [1] (siBP). CVB0 passes messages via Word tokens, while BP and siBP pass messages via Word indexes. There is little difference between BP and siBP in the message update equation. There are two schemes for each implementation, called synchronous schedule and asynchronous schedule."}, {"heading": "7.3.1 CVB0", "text": "Here is the synchronous CVB0 algorithm: [phi, theta, mu] = sCVB0train (cora _ wd, J, J, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, mu] = sCVB0train (cora _ wd, J, N, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = sCVB0predict (cora _ wd, phi, N, ALPHA, BETA, SEED, OUTPUT); [theta, mu] = sCVB0predict (cora _ wd, phi, ALPHA, muin); below is the asynchronous CVB0 algorithm: [phi, theta, mu] = aCVB0train (cora _ wd, BETA, N _ BEED, SEED, SEED, SEED, SEED, SEED, SEED, SEED, SEED, SEED, SEED, SEED, SEED-VB0ED, SEED, SEED, SEED, SEED, SEED-VB0train, SEED, SEED."}, {"heading": "7.3.2 BP", "text": "Here is the BP synchronous algorithm: [phi, theta, mu] = sBPtrain (cora _ wd, J, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, mu] = sBPtrain (cora _ wd, J, N, ALPHA, BETA, SEED, OUTPUT); [theta, mu] = sBPforecast (cora _ wd, phi, N, ALPHA, BETA, SEED, OUTPUT); [theta, mu] = sBPtrain (cora _ wd, phi, N, ALPHA, BEED); below is the asynchronous BP algorithm: [phi, theta, mu] = aBPtrain (cora _ wd, J, N, BETA, OUTPUT, SET, BET) [SET, SET, BET] = SET, BET (BET, BET = SET)."}, {"heading": "7.3.3 siBP", "text": "Here is the synchronous siBP algorithm: [phi, theta, mu] = ssiBPtrain (cora _ wd, J, N, ALPHA, BETA, OUTPUT, muin); [phi, theta, mu] = ssiBPtrain (cora _ wd, J, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = ssiBPforecast (cora _ wd, N, ALPHA, muin); Below is the asynchronous siBP algorithm: [phi, theta, mu] = ssiBPPforecast (cora _ wd, phi, N, ALPHA, muin these sections."}, {"heading": "8 Author-Topic Models (ATM)", "text": "Unlike LDA, ATM [3] treats each author as a mixture of topics rather than a document. Thus, the multinomial parameter theta is a J-A matrix, where A is the total number of unique authors in the corpus. We include GS and synchronous BP algorithms for learning ATM in this toolbox."}, {"heading": "8.1 GS", "text": "The GS algorithm is rewritten based on Topic Modeling Toolbox3 for our data structure. [phi, theta, z, x] = ATMGStrain (cora _ wd, cora _ ad, J, N, ALPHA, BETA, SEED, zin, xin); [phi, theta, z, x] = ATMGStrain (cora _ wd, cora _ ad, phi, N, N, ALPHA, SEED, OUTPUT); [theta, z, x] = ATMGSpredict (cora _ wd, cora _ ad, phi, N, ALPHA, BETA, SEED, OUTPUT); [theta, z, x] = ATMGSpredict (cora _ wd, cora _ ad, phi _ ad, BETA, OUTPUT, toztoztoment, in > 22x25 = 22x1 = 22x25 The first character in 22x25 = 22x25 is assigned."}, {"heading": "8.2 BP", "text": "The BP algorithm can be considered a soft version of the GS algorithm. [phi, theta, mu, x] = ATMBPtrain (cora _ wd, cora _ ad, J, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, mu, mu, x] = ATMBPtrain (cora _ wd, cora _ ad, J, N, ALPHA, SEED, OUTPUT); [theta, mu, x] = ATMBPtrain (cora _ wd, cora _ ad, phi, cora _ ad, SEED, OUTPUT); [theta, mu, x] = ATMBPtrain (cora _ wd, mu, x] = ATMBPtrain (cora _ wd, muUT, muUT, muUT, OUTPUT); PHeta, cormu, corx, prognmu, BATmu = Mmu; (cora, muUT = MATmu, muUT); (muUT, muUT in muUT, muUT, muUT, OUTPUT); PHeta, cormu, corx, cormu, MATmu = MATmu, MATmu, MATmu = MATmu."}, {"heading": "9 Relational Topic Models (RTM)", "text": "RTM [4] introduces LDA-based citation modeling, and the GS algorithm for RTM can be found in the R package. [phi, theta, gamma, mu] = RTMBPtrain (cora _ wd, cora _ dd, J, N, OMEGA, ALPHA, BETA, SEED, OUTPUT); [phi, theta, gamma, mu] = RTMBPtrain (cora _ wd, cora _ dd, J, N, OMEGA, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = RTMBPforecast (cora _ wd, cora _ dd, phi, gamma _ dd, J, N, OMEGA, ALPHA, BETA, SEED, OUTPUT, muin); [theta, mu] = RTMBPforecast (PHa _ dd, phgammi, SEgamma, phamma, phamma, phamma, MEGA, mudd, muin)."}, {"heading": "10 Labeled LDA (LaLDA)", "text": "The GS learning algorithm for LaLDA can be found in the Stanford Topic Modeling Toolbox. 5 We only provide the BP algorithm for learning LaLDA. [phi, theta, z] = LaLDABPtrain (cora _ wd, cora _ ad, N, ALPHA, BETA, SEED, OUTPUT); [phi, theta, z] = LaLDABPtrain (cora _ wd, cora _ ad, N, ALPHA, BETA, SEED, OUTPUT, zin); here we use the author names cora _ ad as class labels for each document. Generally, each document will have several class labels. For invisible test labels, we do not know class labels for each document, so all labels should be considered in the training set. In this case, we can simply.. t.t.t.t.mt > BSignatures > P881 > The output of RT14d.1 / 14d.z is fixed."}, {"heading": "11 Other Tools", "text": "We also provide two scripts topicshow.m and perplexity.m. The former shows the uppermost N-words in each topic, and the latter calculates the confusion on the dataset. > > topicshow (phi, cora _ voc, 5) Network model neural networks Data learning algorithm Problem model algorithms Paper learning control Planning of government learning paper systems Neural networks Bayesian algorithm Show decision paper learning algorithm Search results Models Data models Bayesian algorithms Learning algorithms Results Decision algorithms Neural network Design problem system geneticsThe first input is the multinomical parameter phi, the second input is the vocabulary cora _ voc, and the third input is the uppermost N = 5 words in each topic. > > Perplexity (cora _ wd, phi, theta, ALPHA, BETA) PHans 1.0020 + cora input is the multipheta and 3d input."}], "references": [{"title": "Learning topic models by belief propagation", "author": ["J. Zeng", "W.K. Cheung", "J. Liu"], "venue": "arXiv:1109.3437v3 [cs.LG], 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Latent Dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "UAI, 2004, pp. 487\u2013494.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Hierarchical relational models for document networks", "author": ["J. Chang", "D.M. Blei"], "venue": "Annals of Applied Statistics, vol. 4, no. 1, pp. 124\u2013150, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora", "author": ["D. Ramage", "D. Hall", "R. Nallapati", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing, 2009, pp. 248\u2013256.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proc. Natl. Acad. Sci., vol. 101, pp. 5228\u20135235, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Approximate Mean Field for Dirichlet-Based Models", "author": ["A.U. Asuncion"], "venue": "ICML Workshop on Topic Models, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "On smoothing and inference for topic models", "author": ["A. Asuncion", "M. Welling", "P. Smyth", "Y.W. Teh"], "venue": "UAI, 2009, pp. 27\u201334. 13", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction The past decade has seen rapid development of latent Dirichlet allocation (LDA) [2] for solving various topic modeling and computer vision problems because of its elegant three-layer graphical representation as well as two efficient approximate inference methods like Variational Bayes (VB) [2] and collapsed Gibbs Sampling (GS) [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "1 Introduction The past decade has seen rapid development of latent Dirichlet allocation (LDA) [2] for solving various topic modeling and computer vision problems because of its elegant three-layer graphical representation as well as two efficient approximate inference methods like Variational Bayes (VB) [2] and collapsed Gibbs Sampling (GS) [6].", "startOffset": 306, "endOffset": 309}, {"referenceID": 5, "context": "1 Introduction The past decade has seen rapid development of latent Dirichlet allocation (LDA) [2] for solving various topic modeling and computer vision problems because of its elegant three-layer graphical representation as well as two efficient approximate inference methods like Variational Bayes (VB) [2] and collapsed Gibbs Sampling (GS) [6].", "startOffset": 344, "endOffset": 347}, {"referenceID": 0, "context": "Both VB and GS have been widely used to learn variants of LDA-based topic models until a recent work [1] reveals that there is yet another learning algorithm for LDA based on loopy belief propagation (BP).", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "Similar ideas have also been proposed within the approximate mean-field framework [7] as the zero-order approximation of collapsed VB (CVB0) algorithm [8].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "Similar ideas have also been proposed within the approximate mean-field framework [7] as the zero-order approximation of collapsed VB (CVB0) algorithm [8].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "When using this toolbox, please cite [1].", "startOffset": 37, "endOffset": 40}, {"referenceID": 0, "context": "Table 1: Comparison of message update equations [1].", "startOffset": 48, "endOffset": 51}, {"referenceID": 7, "context": "VB updates messages by involving relatively complicated digamma functions, which introduces bias but can be corrected by proper settings of hyperparameters [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "In addition, it is the digamma function that slows down VB in updating messages [1].", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "Similar ideas have also been proposed within the approximate mean-field framework as the zero-order approximation of collapsed VB (CVB0) algorithm [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 2, "context": "It also includes GS and BP for author-topic models (ATM) [3], and BP for relational topic models (RTM) [4] and labeled LDA [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "It also includes GS and BP for author-topic models (ATM) [3], and BP for relational topic models (RTM) [4] and labeled LDA [5].", "startOffset": 103, "endOffset": 106}, {"referenceID": 4, "context": "It also includes GS and BP for author-topic models (ATM) [3], and BP for relational topic models (RTM) [4] and labeled LDA [5].", "startOffset": 123, "endOffset": 126}, {"referenceID": 0, "context": "4 Data Structure In this toolbox, we include four publicly available document data sets [1]: 1) BLOG, 2) CORA, 3) MEDLINE and 4) NIPS.", "startOffset": 88, "endOffset": 91}, {"referenceID": 0, "context": ">> mex -setup Please choose your compiler building external interface (MEX) files: Would you like mex to locate installed compilers [y]/n? y Select a compiler: [1] Microsoft Visual C++ 2010 Express [0] None Compiler: 1", "startOffset": 160, "endOffset": 163}, {"referenceID": 0, "context": "OMEGA is a balancing weight OMEGA \u2208 [0, 1] for relational topic models (RTM) [1], which balances messages from document content and document links.", "startOffset": 36, "endOffset": 42}, {"referenceID": 0, "context": "OMEGA is a balancing weight OMEGA \u2208 [0, 1] for relational topic models (RTM) [1], which balances messages from document content and document links.", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "7 Latent Dirichlet Allocation (LDA) This toolbox contains mainly various source codes for learning LDA including Variational Bayes (VB) [2], collapsed Gibbs Sampling (GS) [6], and Belief Propagation (BP) [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 5, "context": "7 Latent Dirichlet Allocation (LDA) This toolbox contains mainly various source codes for learning LDA including Variational Bayes (VB) [2], collapsed Gibbs Sampling (GS) [6], and Belief Propagation (BP) [1].", "startOffset": 171, "endOffset": 174}, {"referenceID": 0, "context": "7 Latent Dirichlet Allocation (LDA) This toolbox contains mainly various source codes for learning LDA including Variational Bayes (VB) [2], collapsed Gibbs Sampling (GS) [6], and Belief Propagation (BP) [1].", "startOffset": 204, "endOffset": 207}, {"referenceID": 7, "context": "3 Belief Propagation (BP) There are three major BP implementations called CVB0 [8], BP and simplified BP [1] (siBP).", "startOffset": 79, "endOffset": 82}, {"referenceID": 0, "context": "3 Belief Propagation (BP) There are three major BP implementations called CVB0 [8], BP and simplified BP [1] (siBP).", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "8 Author-Topic Models (ATM) Unlike LDA, ATM [3] treats each author rather than document as a mixture of topics.", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "9 Relational Topic Models (RTM) RTM [4] introduces citation link modeling based on LDA.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "10 Labeled LDA (LaLDA) LaLDA [5] is a supervised topic model for multi-label classification based on LDA.", "startOffset": 29, "endOffset": 32}], "year": 2017, "abstractText": "Latent Dirichlet allocation (LDA) is an important class of hierarchical Bayesian models for probabilistic topic modeling, which attracts worldwide interests and touches on many important applications in text mining, computer vision and computational biology. This paper introduces a topic modeling toolbox (TMBP) based on the belief propagation (BP) algorithms. This toolbox is implemented by MEX C++/MATLAB platform for either Windows or Linux. The current version includes various learning algorithms for latent Dirichlet allocation (LDA), author-topic models (ATM), relational topic models (RTM), and labeled LDA (LaLDA). This toolbox is an ongoing project and more and more BP-based learning algorithms for various LDA-based topic models will be added in the near future. Interested readers may also extend this toolbox for solving more complicated topic modeling problems. The source code is freely available under the GNU General Public Licence, Version 1.0 at http://code.google.com/p/tmbp-topicmodel-beliefpropagation/", "creator": "LaTeX with hyperref package"}}}