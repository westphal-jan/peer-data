{"id": "1201.2241", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jan-2012", "title": "Distance-Based Bias in Model-Directed Optimization of Additively Decomposable Problems", "abstract": "For many optimization problems it is possible to define a distance metric between problem variables that correlates with the likelihood and strength of interactions between the variables. For example, one may define a metric so that the dependencies between variables that are closer to each other with respect to the metric are expected to be stronger than the dependencies between variables that are further apart. The purpose of this paper is to describe a method that combines such a problem-specific distance metric with information mined from probabilistic models obtained in previous runs of estimation of distribution algorithms with the goal of solving future problem instances of similar type with increased speed, accuracy and reliability. While the focus of the paper is on additively decomposable problems and the hierarchical Bayesian optimization algorithm, it should be straightforward to generalize the approach to other model-directed optimization techniques and other problem classes. Compared to other techniques for learning from experience put forward in the past, the proposed technique is both more practical and more broadly applicable.", "histories": [["v1", "Wed, 11 Jan 2012 04:59:57 GMT  (110kb)", "http://arxiv.org/abs/1201.2241v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["martin pelikan", "mark w hauschild"], "accepted": false, "id": "1201.2241"}, "pdf": {"name": "1201.2241.pdf", "metadata": {"source": "CRF", "title": "Distance-Based Bias in Model-Directed Optimization of Additively Decomposable Problems", "authors": ["Martin Pelikan", "Mark W. Hauschild"], "emails": ["medal@cs.umsl.edu", "pelikan@cs.umsl.edu", "mwh308@umsl.edu"], "sections": [{"heading": null, "text": "For example, one can define a metric so that the dependencies between the variables that are closer to each other in terms of metrics are likely to be stronger than the dependencies between the variables that are further apart. The purpose of this paper is to describe a method that combines such problem-specific distance metrics with information from probability models obtained in previous runs of estimating distribution algorithms with the aim of solving future problem cases with higher speed, accuracy and reliability. While the paper focuses on additive decompatibility problems and Bayesian hierarchical optimization algorithms, it should be easy to generalize the approach to other model-based optimization techniques and other problem classes."}, {"heading": "1 Introduction", "text": "In fact, it is the case that we will be able to go in search of a solution that is capable, that we are able, that we are able to find a solution that is capable of us, that we are able, that we are able to find a solution, that we are able to find a solution, that we are able, that we are able to find a solution, that we are able, that we are able, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution."}, {"heading": "2 Hierarchical BOA", "text": "In fact, it is not the case that one can embark on a search for a solution. (...) It is not the case that one can engage in a solution. (...) It is not the case that one can engage in a solution. (...) It is not the case that one can engage in a solution. (...) It is the case that one can engage in a solution. (...) It is the case that one can engage in a solution. (...) It is the case that one can engage in a solution. (...) It is the case that one can engage in a solution. (...) It is the case that one can engage in a solution. (...) It is the case that one can engage in a solution. (...) It is the case that one can engage in a solution. (...) It is the case that one can engage in a solution. (...). (...) It is. (...). (...) It is. (...). (It is. (...) It is. (...) It is. (...) It is. (it is. (...) It is. (...) It is. (it is. (...) It is. (it is. (...) It is. (it is. (...) It is. (it is. (...) It is. (it is. (...). (it is. (it is). (it is. (it is. (it is.). (it is. (it is.) It is. (it is. (it. (it is.). (it is. (it is.)"}, {"heading": "3 Bias Based on Previous EDA Runs", "text": "Building a precise probability model in hBOA and other EDAs is often the most difficult task in building models based on complex probability problems, which are often time-consuming and require a relatively large number of approaches, which is why much effort has been put into improving the efficiency of modelling in EDAs and improving the quality of EDA models even with smaller populations of candidate solutions (Baluja, 2006; Hauschild & Pelikan, 2008; Hauschild & Pelikan, 2009; Hauschild, Pelikan, 2011; Pelikan, 2005). Learning from models discovered by the EDAs in previous runs, regularities and the models discovered are used in future runs."}, {"heading": "4 Distance-Based Bias", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Additively Decomposable Functions", "text": "For many optimization problems, the objective function (fitness function) can be expressed in the form of an additive decomposable function (ADF) of m partial problems: f (X1,..., Xn) = m \u2211 i = 1fi (Si), (4), where (X1,.., Xn) are decision variables of the problem, fi is the ith subfunction, and Si-X1, X2,.., Xn} is the subset of variables that contribute to fi. Although there are often several ways to decompose the problem by means of additive decomposition, one would typically prefer decomposition processes that minimize the size of subsets (Si). It should be noted that the difficulty of ADFs is not entirely determined by the order of the partial problems, but also by the definition of the partial problems and their interaction. In fact, there are a number of NP complete problems that can be formulated as subproblems of order 2 or 3, such as can be easily solved with a SXB3F."}, {"heading": "4.2 Measuring Variable Distances for ADFs", "text": "The definition of a distance between two variables of an ADF used in this paper follows Hauschild and Pelikan (2008) and Hauschild et al. (2011). Faced with an additive decomposable problem, we define the distance between two variables with a graph G of n nodes, one node per variable. For all two variables Xi and Xj in the same subset Sk, that is, we define the distance between two variables asD, Xj, we create anedge in G between the nodes Xi and Xj. Denoting by li, the number of edges along the shortest path between Xi and Xj in G (in terms of the number of edges), we define the distance between two variables asD, Xj, j, if a path between Xi and Xj exists n otherwiseThe aforementioned distance makes variables near the substance each other, while the other variables in the remaining variables vary, we correlate the distance between the two variables defining asD."}, {"heading": "4.3 Using Distance-Based Bias in hBOA", "text": "The basic idea of including distance metrics on the basis of previous runs in hBOA is mainly based on the work of Hauschild et al. (Hauschild & Pelikan, 2009). (Hauschild & Pelikan, 2009) is only applicable to problems where the strength of interactions between two variables is not expected to change much from instance to instance, which is why this approach can only be applied in a limited set of problem domains and it is difficult to use this approach if the problem size is not fixed in all runs. In this paper we propose to capture the type of dependencies between the variables in relation to their distance using the metric definition of ADFs or other distance metrics."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Test Problems", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they are able to move, in which they are able to move, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which"}, {"heading": "5.2 10-Fold Crossvalidation", "text": "In order to ensure that the same problem cases were not used in defining the bias as in testing the bias, a tenfold cross-validation was performed; for each problem size and problem, 1,000 random problem cases were used in the experiments; the 1,000 instances in each set were randomly divided into 10 equally large subsets of 100 instances each; in each round of cross-validation, 1 substance per 100 instances was omitted; and hBOA was run on the remaining 9 substances for a total of 900 instances. The runs in the 9 subsets yielded a number of models that were analyzed to determine the probabilities Pk (d, j) for all d, j, and k. The bias based on the obtained values of Pk (d, j) was then used in hBOA runs on the remaining subset that were not factored.The same procedure was repeated for each subgroup; in total, 10 rounds of cross-validation were performed for each group of 1,000 instances, each test was performed with exactly the same BOA as the other subset, and each of the BOA problem substance was proposed once in each case."}, {"heading": "5.3 Experimental Setup", "text": "The maximum number of iterations for each problem instance was set at the total number of bits in the problem; however, according to preliminary experiments, this upper limit essentially exceeded the actual number of iterations required to solve each problem. Each run was terminated either when the global optimum was found, when the population consisted of copies of a single candidate solution, or when the maximum number of iterations was reached. For each problem, we used Bisection (Sastry, 2001; Pelikan, 2005) to ensure that the population size was within 5% of the minimum population size to find the optimum in 10 out of 10 independent runs. Bit Flip Hill Clipping (HC) is integrated into hBOA to improve its performance. HC represents a candidate solution through an n-bit binary string on input, then makes one-bit changes to the solution that result in maximum improvement in solution quality."}, {"heading": "5.4 Results", "text": "The results show that the acceleration in terms of execution time for both NK and 2D spin glasses is significant; the maximum acceleration for NK scenery was above 2.26, while it was above 1.66 for spin glasses. However, for NK scenery, the acceleration in terms of execution time increases with both problem size n and 2D spin glasses, and is expected to continue to increase with even greater values of n or B glasses. For spin glasses, the acceleration appears to be almost independent of the problem size and the best acceleration for the execution time. Multiplicative accelerations in terms of number of evaluations, number of HC steps, and population size indicate that the acceleration in terms of spin size is almost independent."}, {"heading": "6 Summary and Conclusions", "text": "In fact, most of the people mentioned are people who are able to decide whether they will be able to vote for another party, or whether they will be able to vote for another party."}, {"heading": "Acknowledgments", "text": "This project was sponsored by the National Science Foundation under the auspices of ECS-0547013 and IIS1115352, and the University of Missouri in St. Louis through the High Performance Computing Collaboratory sponsored by Information Technology Services. Most of the experiments were conducted at the Beowulf Cluster maintained by ITS at the University of Missouri in St. Louis and the HPC resources of the Bioinformatics Consortium at the University of Missouri. HBOA was developed at the Illinois Genetics Algorithm Laboratory at the University of Illinois at Urbana-Champaign. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation."}], "references": [{"title": "Population-based incremental learning: A method for integrating genetic search based function optimization and competitive learning (Tech", "author": ["S. Baluja"], "venue": "Rep. No. CMU-CS-94-163). Pittsburgh, PA: Carnegie Mellon University.", "citeRegEx": "Baluja,? 1994", "shortCiteRegEx": "Baluja", "year": 1994}, {"title": "Incorporating a priori knowledge in probabilistic-model based optimization", "author": ["S. Baluja"], "venue": "Cant\u00fa-Paz, E., Pelikan, M., & Sastry, K. (Eds.), Scalable optimization via probabilistic modeling: From algorithms to applications (pp. 205\u2013219). Springer.", "citeRegEx": "Baluja,? 2006", "shortCiteRegEx": "Baluja", "year": 2006}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning , 28 , 41\u201375.", "citeRegEx": "Caruana,? 1997", "shortCiteRegEx": "Caruana", "year": 1997}, {"title": "A Bayesian approach to learning Bayesian networks with local structure (Technical Report MSR-TR-97-07)", "author": ["D.M. Chickering", "D. Heckerman", "C. Meek"], "venue": null, "citeRegEx": "Chickering et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Chickering et al\\.", "year": 1997}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E.H. Herskovits"], "venue": "Machine Learning ,", "citeRegEx": "Cooper and Herskovits,? \\Q1992\\E", "shortCiteRegEx": "Cooper and Herskovits", "year": 1992}, {"title": "Using hybrid metaheuristics for the one-way and two-way network design problem", "author": ["Z. Drezner", "S. Salhi"], "venue": "Naval Research Logistics,", "citeRegEx": "Drezner and Salhi,? \\Q2002\\E", "shortCiteRegEx": "Drezner and Salhi", "year": 2002}, {"title": "Learning Bayesian networks with local structure", "author": ["N. Friedman", "M. Goldszmidt"], "venue": null, "citeRegEx": "Friedman and Goldszmidt,? \\Q1999\\E", "shortCiteRegEx": "Friedman and Goldszmidt", "year": 1999}, {"title": "Linkage learning via probabilistic modeling in the ECGA (IlliGAL Report No", "author": ["G. Harik"], "venue": "99010). Urbana, IL: University of Illinois at Urbana-Champaign, Illinois Genetic Algorithms Laboratory.", "citeRegEx": "Harik,? 1999", "shortCiteRegEx": "Harik", "year": 1999}, {"title": "Finding multimodal solutions using restricted tournament selection", "author": ["G.R. Harik"], "venue": "Proc. of the Int. Conf. on Genetic Algorithms (ICGA-95), 24\u201331.", "citeRegEx": "Harik,? 1995", "shortCiteRegEx": "Harik", "year": 1995}, {"title": "Enhancing efficiency of hierarchical BOA via distancebased model restrictions. Parallel Problem Solving from Nature, 417\u2013427", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": null, "citeRegEx": "Hauschild and Pelikan,? \\Q2008\\E", "shortCiteRegEx": "Hauschild and Pelikan", "year": 2008}, {"title": "Intelligent bias of network structures in the hierarchical BOA", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": "Genetic and Evol. Comp. Conf", "citeRegEx": "Hauschild and Pelikan,? \\Q2009\\E", "shortCiteRegEx": "Hauschild and Pelikan", "year": 2009}, {"title": "Network crossover performance on NK landscapes and deceptive problems", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": "Genetic and Evol. Comp. Conf", "citeRegEx": "Hauschild and Pelikan,? \\Q2010\\E", "shortCiteRegEx": "Hauschild and Pelikan", "year": 2010}, {"title": "An introduction and survey of estimation of distribution algorithms", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": "Swarm and Evolutionary Computation,", "citeRegEx": "Hauschild and Pelikan,? \\Q2011\\E", "shortCiteRegEx": "Hauschild and Pelikan", "year": 2011}, {"title": "Using previous models to bias structural learning in the hierarchical BOA", "author": ["M.W. Hauschild", "M. Pelikan", "K. Sastry", "D.E. Goldberg"], "venue": null, "citeRegEx": "Hauschild et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hauschild et al\\.", "year": 2011}, {"title": "Analyzing probabilistic models in hierarchical BOA", "author": ["M.W. Hauschild", "M. Pelikan", "K. Sastry", "C.F. Lima"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "Hauschild et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hauschild et al\\.", "year": 2009}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data (Technical Report MSR-TR-94-09)", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": null, "citeRegEx": "Heckerman et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1994}, {"title": "Adaptation on rugged fitness landscapes", "author": ["S. Kauffman"], "venue": "Stein, D. L. (Ed.), Lecture Notes in the Sciences of Complexity (pp. 527\u2013618). Addison Wesley.", "citeRegEx": "Kauffman,? 1989", "shortCiteRegEx": "Kauffman", "year": 1989}, {"title": "Estimation of distribution algorithms: A new tool for evolutionary computation", "author": ["P. Larra\u00f1aga", "J.A. Lozano"], "venue": null, "citeRegEx": "Larra\u00f1aga and Lozano,? \\Q2002\\E", "shortCiteRegEx": "Larra\u00f1aga and Lozano", "year": 2002}, {"title": "Towards a new evolutionary computation: Advances on estimation of distribution", "author": ["J.A. Lozano", "P. Larra\u00f1aga", "I. Inza", "E. Bengoetxea"], "venue": null, "citeRegEx": "Lozano et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Lozano et al\\.", "year": 2006}, {"title": "Evolutionary optimization and the estimation of search distributions with applications to graph bipartitioning", "author": ["H. M\u00fchlenbein", "T. Mahnig"], "venue": "International Journal of Approximate Reasoning ,", "citeRegEx": "M\u00fchlenbein and Mahnig,? \\Q2002\\E", "shortCiteRegEx": "M\u00fchlenbein and Mahnig", "year": 2002}, {"title": "Bayesian optimization algorithm: From single level to hierarchy", "author": ["M. Pelikan"], "venue": "Doctoral dissertation, University of Illinois at Urbana-Champaign, Urbana, IL.", "citeRegEx": "Pelikan,? 2002", "shortCiteRegEx": "Pelikan", "year": 2002}, {"title": "Hierarchical Bayesian optimization algorithm: Toward a new generation of evolutionary algorithms", "author": ["M. Pelikan"], "venue": "Springer.", "citeRegEx": "Pelikan,? 2005", "shortCiteRegEx": "Pelikan", "year": 2005}, {"title": "NK landscapes, problem difficulty, and hybrid evolutionary algorithms", "author": ["M. Pelikan"], "venue": "Genetic and Evol. Comp. Conf. (GECCO-2010), 665\u2013672.", "citeRegEx": "Pelikan,? 2010", "shortCiteRegEx": "Pelikan", "year": 2010}, {"title": "Escaping hierarchical traps with competent genetic algorithms", "author": ["M. Pelikan", "D.E. Goldberg"], "venue": "Genetic and Evol. Comp. Conf", "citeRegEx": "Pelikan and Goldberg,? \\Q2001\\E", "shortCiteRegEx": "Pelikan and Goldberg", "year": 2001}, {"title": "A hierarchy machine: Learning to optimize from nature and humans", "author": ["M. Pelikan", "D.E. Goldberg"], "venue": "Complexity ,", "citeRegEx": "Pelikan and Goldberg,? \\Q2003\\E", "shortCiteRegEx": "Pelikan and Goldberg", "year": 2003}, {"title": "A survey of optimization by building and using probabilistic models", "author": ["M. Pelikan", "D.E. Goldberg", "F. Lobo"], "venue": "Computational Optimization and Applications,", "citeRegEx": "Pelikan et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pelikan et al\\.", "year": 2002}, {"title": "Searching for ground states of Ising spin glasses with hierarchical BOA and cluster exact approximation", "author": ["M. Pelikan", "A.K. Hartmann"], "venue": null, "citeRegEx": "Pelikan and Hartmann,? \\Q2006\\E", "shortCiteRegEx": "Pelikan and Hartmann", "year": 2006}, {"title": "Scalable optimization via probabilistic modeling: From algorithms to applications", "author": ["M. Pelikan", "K. Sastry", "E. Cant\u00fa-Paz"], "venue": null, "citeRegEx": "Pelikan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pelikan et al\\.", "year": 2006}, {"title": "Direct transfer of learned information among neural networks", "author": ["L.Y. Pratt", "J. Mostow", "C.A. Kamm", "A.A. Kamm"], "venue": "In Proceedings of the Ninth National Conference on Artificial Intelligence (pp. 584589)", "citeRegEx": "Pratt et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Pratt et al\\.", "year": 1991}, {"title": "Representations for genetic and evolutionary algorithms", "author": ["F. Rothlauf"], "venue": "Springer.", "citeRegEx": "Rothlauf,? 2006", "shortCiteRegEx": "Rothlauf", "year": 2006}, {"title": "Evaluation-relaxation schemes for genetic and evolutionary algorithms", "author": ["K. Sastry"], "venue": "Master\u2019s thesis, University of Illinois at Urbana-Champaign, Department of General Engineering, Urbana, IL.", "citeRegEx": "Sastry,? 2001", "shortCiteRegEx": "Sastry", "year": 2001}, {"title": "A problem-knowledge based evolutionary algorithm KBOA for hypergraph partitioning", "author": ["J. Schwarz", "J. Ocenasek"], "venue": "In Proc. of the Fourth Joint Conf. on Knowledge-Based Software Engineering (pp. 51\u201358)", "citeRegEx": "Schwarz and Ocenasek,? \\Q2000\\E", "shortCiteRegEx": "Schwarz and Ocenasek", "year": 2000}, {"title": "Crossnet: a framework for crossover with", "author": ["F. Stonedahl", "W. Rand", "U. Wilensky"], "venue": null, "citeRegEx": "Stonedahl et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Stonedahl et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "Even for optimization problems that are extremely difficult to solve, it may be straightforward to extract information about important dependencies between variables and other problem regularities directly from the problem definition (Baluja, 2006; Drezner & Salhi, 2002; Hauschild & Pelikan, 2010; Stonedahl, Rand, & Wilensky, 2008; Schwarz & Ocenasek, 2000).", "startOffset": 234, "endOffset": 359}, {"referenceID": 20, "context": "The use of information from previous runs to introduce bias into future runs of an evolutionary algorithm is often referred to as learning from experience (Hauschild & Pelikan, 2008; Hauschild, Pelikan, Sastry, & Goldberg, 2011; Pelikan, 2002).", "startOffset": 155, "endOffset": 243}, {"referenceID": 2, "context": "The use of bias based on the results of other learning tasks in the same problem domain is also commonplace in machine learning where it is referred to as inductive transfer or transfer learning (Pratt, Mostow, Kamm, & Kamm, 1991; Caruana, 1997).", "startOffset": 195, "endOffset": 245}, {"referenceID": 1, "context": "Numerous studies have shown that using prior knowledge and learning from experience promise improved efficiency and problem solving capabilities (Baluja, 2006; Drezner & Salhi, 2002; Hauschild & Pelikan, 2010; Hauschild, Pelikan, Sastry, & Goldberg, 2011; Rothlauf, 2006; Stonedahl, Rand, & Wilensky, 2008; Schwarz & Ocenasek, 2000).", "startOffset": 145, "endOffset": 332}, {"referenceID": 29, "context": "Numerous studies have shown that using prior knowledge and learning from experience promise improved efficiency and problem solving capabilities (Baluja, 2006; Drezner & Salhi, 2002; Hauschild & Pelikan, 2010; Hauschild, Pelikan, Sastry, & Goldberg, 2011; Rothlauf, 2006; Stonedahl, Rand, & Wilensky, 2008; Schwarz & Ocenasek, 2000).", "startOffset": 145, "endOffset": 332}, {"referenceID": 21, "context": "The hierarchical Bayesian optimization algorithm (hBOA) (Pelikan & Goldberg, 2001; Pelikan & Goldberg, 2003; Pelikan, 2005) is an estimation of distribution algorithm (EDA) (Baluja, 1994; Larra\u00f1aga & Lozano, 2002; Pelikan et al.", "startOffset": 56, "endOffset": 123}, {"referenceID": 0, "context": "The hierarchical Bayesian optimization algorithm (hBOA) (Pelikan & Goldberg, 2001; Pelikan & Goldberg, 2003; Pelikan, 2005) is an estimation of distribution algorithm (EDA) (Baluja, 1994; Larra\u00f1aga & Lozano, 2002; Pelikan et al., 2002; Lozano et al., 2006; Pelikan et al., 2006; Hauschild & Pelikan, 2011).", "startOffset": 173, "endOffset": 305}, {"referenceID": 25, "context": "The hierarchical Bayesian optimization algorithm (hBOA) (Pelikan & Goldberg, 2001; Pelikan & Goldberg, 2003; Pelikan, 2005) is an estimation of distribution algorithm (EDA) (Baluja, 1994; Larra\u00f1aga & Lozano, 2002; Pelikan et al., 2002; Lozano et al., 2006; Pelikan et al., 2006; Hauschild & Pelikan, 2011).", "startOffset": 173, "endOffset": 305}, {"referenceID": 18, "context": "The hierarchical Bayesian optimization algorithm (hBOA) (Pelikan & Goldberg, 2001; Pelikan & Goldberg, 2003; Pelikan, 2005) is an estimation of distribution algorithm (EDA) (Baluja, 1994; Larra\u00f1aga & Lozano, 2002; Pelikan et al., 2002; Lozano et al., 2006; Pelikan et al., 2006; Hauschild & Pelikan, 2011).", "startOffset": 173, "endOffset": 305}, {"referenceID": 27, "context": "The hierarchical Bayesian optimization algorithm (hBOA) (Pelikan & Goldberg, 2001; Pelikan & Goldberg, 2003; Pelikan, 2005) is an estimation of distribution algorithm (EDA) (Baluja, 1994; Larra\u00f1aga & Lozano, 2002; Pelikan et al., 2002; Lozano et al., 2006; Pelikan et al., 2006; Hauschild & Pelikan, 2011).", "startOffset": 173, "endOffset": 305}, {"referenceID": 8, "context": "maintain useful diversity in the population, the new candidate solutions are incorporated into the original population using restricted tournament selection (RTS) (Harik, 1995).", "startOffset": 163, "endOffset": 176}, {"referenceID": 21, "context": "this network\u2019s parameters (Friedman & Goldszmidt, 1999; Pelikan, 2005):", "startOffset": 26, "endOffset": 70}, {"referenceID": 1, "context": "That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations of candidate solutions (Baluja, 2006; Hauschild & Pelikan, 2008; Hauschild & Pelikan, 2009; Hauschild, Pelikan, Sastry, & Goldberg, 2011; M\u00fchlenbein & Mahnig, 2002).", "startOffset": 178, "endOffset": 319}, {"referenceID": 21, "context": "Learning from experience (Hauschild & Pelikan, 2008; Hauschild & Pelikan, 2009; Hauschild, Pelikan, Sastry, & Goldberg, 2011; Pelikan, 2005) represents one approach to dealing with this issue.", "startOffset": 25, "endOffset": 140}, {"referenceID": 16, "context": "For example, consider optimization of NK landscapes (Kauffman, 1989), in which the fitness function is defined as the sum of n subfunctions {fi} n i=1, and the subfunction fi is applied to the ith bit and its k neighbors.", "startOffset": 52, "endOffset": 68}, {"referenceID": 1, "context": "A hard bias encodes hard restrictions on model structure or variable assignments, restricting the class of allowable models (M\u00fchlenbein & Mahnig, 2002; Baluja, 2006; Hauschild & Pelikan, 2008).", "startOffset": 124, "endOffset": 192}, {"referenceID": 9, "context": "The definition of a distance between two variables of an ADF used in this paper follows Hauschild and Pelikan (2008) and Hauschild et al.", "startOffset": 88, "endOffset": 117}, {"referenceID": 9, "context": "The definition of a distance between two variables of an ADF used in this paper follows Hauschild and Pelikan (2008) and Hauschild et al. (2011). Given an additively decomposable problem, we define the distance between two variables using a graph G of n nodes, one node per variable.", "startOffset": 88, "endOffset": 145}, {"referenceID": 16, "context": "Both these problem classes were shown to be challenging for conventional genetic algorithms and many other optimization techniques due to the rugged landscape, strong epistasis, and complex structure of interactions between problem variables (Kauffman, 1989; Young, 1998; Pelikan, 2010; Pelikan & Hartmann, 2006).", "startOffset": 242, "endOffset": 312}, {"referenceID": 22, "context": "Both these problem classes were shown to be challenging for conventional genetic algorithms and many other optimization techniques due to the rugged landscape, strong epistasis, and complex structure of interactions between problem variables (Kauffman, 1989; Young, 1998; Pelikan, 2010; Pelikan & Hartmann, 2006).", "startOffset": 242, "endOffset": 312}, {"referenceID": 16, "context": "An NK fitness landscape (Kauffman, 1989) is fully defined by the following components: (1) The number of bits, n, (2) the number of neighbors per bit, k, (3) a set of k neighbors \u03a0(Xi) of the ith bit for every i \u2208 {1, .", "startOffset": 24, "endOffset": 40}, {"referenceID": 22, "context": "The reason for restricting neighborhoods to nearest neighbors was to ensure that the problem instances can be solved in polynomial time even for k > 1 using dynamic programming (Pelikan, 2010).", "startOffset": 177, "endOffset": 192}, {"referenceID": 16, "context": "Both these problem classes were shown to be challenging for conventional genetic algorithms and many other optimization techniques due to the rugged landscape, strong epistasis, and complex structure of interactions between problem variables (Kauffman, 1989; Young, 1998; Pelikan, 2010; Pelikan & Hartmann, 2006). However, for both problem classes, it is straightforward to generate a large number of problem instances with known optima. For each problem class and problem size, we use 1,000 unique problem instances; the reason for using such a large number of instances is that for these problem classes, algorithm performance often varies substantially from one instance to another and the results would thus be unreliable if only a few instances were used. An NK fitness landscape (Kauffman, 1989) is fully defined by the following components: (1) The number of bits, n, (2) the number of neighbors per bit, k, (3) a set of k neighbors \u03a0(Xi) of the ith bit for every i \u2208 {1, . . . , n}, and (4) a subfunction fi defining a real value for each combination of values of Xi and \u03a0(Xi) for every i \u2208 {1, . . . , n}. Typically, each subfunction is defined as a lookup table. The objective function fnk to maximize is defined as fnk(X1,X2, . . . ,Xn) = \u2211n i=1 fi(Xi,\u03a0(Xi)). The difficulty of optimizing NK landscapes depends on all components defining an NK problem instance. In this paper, we consider nearest-neighbor NK landscapes, in which neighbors of each bit are restricted to the k bits that immediately follow this bit. The neighborhoods wrap around; thus, for bits which do not have k bits to the right, the neighborhood is completed with the first few bits of solution strings. The reason for restricting neighborhoods to nearest neighbors was to ensure that the problem instances can be solved in polynomial time even for k > 1 using dynamic programming (Pelikan, 2010). The subfunctions are represented by look-up tables (a unique value is used for each instance of a bit and its neighbors), and each entry in the look-up table is generated with the uniform distribution from [0, 1). The used class of NK landscapes with nearest neighbors is thus the same as that in Pelikan (2010). In all experiments, we use k = 5 and n \u2208 {100, 150, 200}.", "startOffset": 243, "endOffset": 2192}, {"referenceID": 30, "context": "For each problem instance, we used bisection (Sastry, 2001; Pelikan, 2005) to ensure that the population size was within 5% of the minimum population size to find the optimum in 10 out of 10 independent runs.", "startOffset": 45, "endOffset": 74}, {"referenceID": 21, "context": "For each problem instance, we used bisection (Sastry, 2001; Pelikan, 2005) to ensure that the population size was within 5% of the minimum population size to find the optimum in 10 out of 10 independent runs.", "startOffset": 45, "endOffset": 74}, {"referenceID": 7, "context": "For example, it should be straightforward to adapt this framework to the extended compact genetic algorithm (Harik, 1999) or classes of facility location problems.", "startOffset": 108, "endOffset": 121}], "year": 2012, "abstractText": "For many optimization problems it is possible to define a distance metric between problem variables that correlates with the likelihood and strength of interactions between the variables. For example, one may define a metric so that the dependencies between variables that are closer to each other with respect to the metric are expected to be stronger than the dependencies between variables that are further apart. The purpose of this paper is to describe a method that combines such a problem-specific distance metric with information mined from probabilistic models obtained in previous runs of estimation of distribution algorithms with the goal of solving future problem instances of similar type with increased speed, accuracy and reliability. While the focus of the paper is on additively decomposable problems and the hierarchical Bayesian optimization algorithm, it should be straightforward to generalize the approach to other model-directed optimization techniques and other problem classes. Compared to other techniques for learning from experience put forward in the past, the proposed technique is both more practical and more broadly applicable.", "creator": "gnuplot 4.4 patchlevel 4"}}}