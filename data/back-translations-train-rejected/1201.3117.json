{"id": "1201.3117", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2012", "title": "Design of Emergent and Adaptive Virtual Players in a War RTS Game", "abstract": "Basically, in (one-player) war Real Time Strategy (wRTS) games a human player controls, in real time, an army consisting of a number of soldiers and her aim is to destroy the opponent's assets where the opponent is a virtual (i.e., non-human player controlled) player that usually consists of a pre-programmed decision-making script. These scripts have usually associated some well-known problems (e.g., predictability, non-rationality, repetitive behaviors, and sensation of artificial stupidity among others). This paper describes a method for the automatic generation of virtual players that adapt to the player skills; this is done by building initially a model of the player behavior in real time during the game, and further evolving the virtual player via this model in-between two games. The paper also shows preliminary results obtained on a one player wRTS game constructed specifically for experimentation.", "histories": [["v1", "Sun, 15 Jan 2012 20:06:07 GMT  (642kb,D)", "http://arxiv.org/abs/1201.3117v1", "IWINAC International Work-conference on the Interplay between Natural and Artificial Computation 2011"]], "COMMENTS": "IWINAC International Work-conference on the Interplay between Natural and Artificial Computation 2011", "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["jos\\'e a garc\\'ia guti\\'errez", "carlos cotta", "antonio j fern\\'andez-leiva"], "accepted": false, "id": "1201.3117"}, "pdf": {"name": "1201.3117.pdf", "metadata": {"source": "CRF", "title": "Design of Emergent and Adaptive Virtual Players in a War RTS Game", "authors": ["Jos\u00e9 A. Gar\u0107\u0131a Guti\u00e9rrez", "Carlos Cotta", "Antonio J. Fern\u00e1ndez Leiva"], "emails": ["ccottap@lcc.uma.es", "afdez@lcc.uma.es"], "sections": [{"heading": "1 Introduction and related work", "text": "In an era when computing power has increased the graphical quality of video games, players should have turned their attention to other aspects of the game, in particular challenging opponents who exhibit intelligent behavior. However, intelligence in general is not synonymous with gameplay, but rather with interesting behaviors. This problem is specifically relevant in real-time strategies (RTS) games, which often use two types of artificial intelligence (AI): one represented by a virtual player (VP, or non-player character - NPC) who makes decisions about a number of units (i.e. warriors, machines, etc.), and another that corresponds to the small units (usually with little or no intelligence). The design of these AIs is a complex task, and the reality in the industry is that in most RTS games, the NPC is basically controlled by a fixed script that has previously been based on the experience of the designer / programmer."}, {"heading": "2 The Game", "text": "The world is heterogeneous because the terrain is not uniform, hostile because there is a virtual army whose mission is to destroy the human player-driven army, and dynamic because the playing conditions vary according to the actions of the units of both armies. Considering the heterogeneity of the terrain, each grid in the region can have one of the following three values: passable (each uni can traverse this grid), impassable (no unit can traverse it), and semi-impassable (there is a penalty of 1 point of energy - see below).Army. There are two armies (also called indistinctly teams, represented by spiders and ladybugs) with a number of units."}, {"heading": "2.1 Notes on specific issues", "text": "Note that \"fighting\" is not a specific action that needs to be performed by the Army Units. This should not be surprising, since \"fighting\" is not usually controlled by the players in standard RTS games, as this action should often be performed at unit level and not at army level; in our game, this action is performed automatically when a unit and a rival soldier meet in any grid of the scenario. Also, note that five of the six possible actions that units can perform require movement in the game world. Basically, this means that some kind of pathfinding should be processed to make the movement realistic. Classic algorithms such as A * were not good candidates for this task, because, as already indicated, most of the scenario for the Army Units is not known. A practical solution was to let units perform sufficient movement depending on their scenario knowledge; several cases were covered: Cave Running: The Unit moves in that particular direction, causing its angles to change."}, {"heading": "3 Virtual player design", "text": "Here we describe our proposal to design adaptive virtual players in our wRTS."}, {"heading": "3.1 General issues", "text": "Our goal is to generate controllers that automatically (i.e. without human intervention) control the behavior of an entire team (i.e. a number of autonomous agents / soldiers). An army controller can be considered a set of rules that controls the reactions of its agents according to the current game score. However, depending on the situation, the agent performs a specific action that could be modified.1 A prototype version of this game can be downloaded from http: / / www.lcc.uma.es / www.afdez / aracnia.Algorithm 1: PMEA (VP) 1 VP \u2190 RBP; / / Rules-based Expert System 2 for i-N-do 3 PlayerModel \u2190 PlayGameOn (VP); / Player Modeling (PM) 4 VP-EA (PlayerModel, VP); / Evolutionary Optimization (EA) 5 End for 6 Return VP; the state of the agent itself. The global team strategy results from the sum of the specific behavior of all actors."}, {"heading": "3.2 A two phases process", "text": "The procedure described below is aimed at creating a virtual player whose behavior develops according to the abilities that the human player exhibits during the game. Algorithm 1 indicates the general scheme of the process: first, (in the first game), the virtual player (VP) is constructed as an expert system (i.e., a rules-based prototype (RBP) containing dozens of rules that attempt to cover all possible situations in which an agent might be involved. \u2212 This system was the result of a series of experiments designed from the experiences of the authors who play wRTS games. Then, let Nh be {1,.., h} and assume that the human player will be a series of computer games, the procedure consists of two phases that will be executed sequentially. Firstly, a modeling phase is performed; this step is described in Section 3.3 and essentially consists of building a model of behavior that the human player shows during the game (i.e., on-line)."}, {"heading": "3.3 On-line phase: User modeling", "text": "The behavior of the player is modeled as a rules-based strategy encoded as described above. This process requires collecting during the execution of the game (i.e. in real time) of the actions that the human player performs, additionally recording the specific conditions under which these are performed. At the end of this process, we generate an extended response matrix (v +) that is a response matrix (i.e., algorithm 2: EA (PModel, VP)) / / PModel = player model, VP = virtual player 1 for i-Npopsize \u2212 1 make 2 popi \u2190 random solution (); 3 end for 4 poppopsize \u2190 VP; 5 i \u2190 0; 6 during i < MaxGenerations 7 rank population (pop), VP = virtual player 1 for i-Npopsize \u2212 1 do 2 popi \u2190 random solution (); 3 end for 4 poppopsize \u2190 VP; 5 i \u2190 0; 6 during i < MaxGenerations 7 rank population (pop) (Select) \u2212 1 for i-Npopsize \u2212 1 do 2 popi / pop (); 3 end for 4 poppopsize \u2190 pop (); 5 \u2190 0; 6 during Maxkinkinkinkini 7 (7) Select Population (7 (7) &lti (7) &lti (7) &lti / pop (Sori) &lti / pop (1)."}, {"heading": "3.4 Off-line phase: evolutionary optimization", "text": "Algorithm 2 shows the basic scheme of our EA. The initial population is randomly generated, except for one person in charge of the virtual player (lines 1-4). Then a classic evolutionary process is carried out (lines 6-21) and the best solution in the population is eventually returned (ties are randomly broken). The assessment of the fitness of a person x requires the simulation (offline) of the game between the player model and the virtual player strategy encoded in x. The fitness function depends on the statistical data collected at the end of the simulation. The higher the number of statistical data to be collected, the higher the computational costs. A good policy is to consider a limited number of data from the outset. In our experiments, four data were used during the offline evaluation: A: Number of deaths in the army of the human player; B: Number of deaths in the army of the virtual player; C: Number of deaths in the movements; and the fitness function was better than the fitness function (2: Fitness function, i.e. 1 and fitness function was better than the fitness function)."}, {"heading": "4 Experimental analysis", "text": "The experiments were performed using two algorithms: our initial expert system (RBP) and the algorithm PMEA (i.e., player modeling + EA), which is represented in algorithm 1. With regard to the PMEA, the EA uses popsize = 50, pX =.7, pM =.01 and MaxGenerations = 125; the mutation is performed at the gene level as usual by converting an action into any other randomly selected action. Three different scenarios were created for the experiments: (1) a map with the size of 50 x 50 grids, 48 agents in the VP army and 32 soldiers in the human player team (HP); (2) a map 54 x 46, with 43 VP soldiers and 43 HP units; and (3) a map 50 x 28, with 48 VP soldiers and 53 HP units. Algorithm 1 was executed for a value of 50 x 50, 48 agents in the VP army and 32 soldiers in the human player team (HP); (i.e. 20 games were also performed sequentially and the BP was performed 20 times)."}, {"heading": "5 Conclusions", "text": "We have described an algorithm to automatically design strategies that exhibit emerging behaviors that adapt to the skills of the user in a one-player real-time war strategy game (wRTS), constructing a model to mimic how the human player behaves in the game, initially during the game, and also developing a strategy for a virtual player (between games) using an evolutionary algorithm. Our proposal has been compared with an expert system specifically designed for the game. While the experiments did not highlight significant differences, we note that our approach has obvious advantages over classic scripts (i.e. expert systems) used in the video game industry: for example, it avoids the predictability of actions performed by the virtual player, thus guaranteeing the interest of the player. This is especially interesting when the game involves more than one player, as our approach would allow virtual players to adapt to each one in a particular way (and)."}], "references": [{"title": "Artificial stupidity: The art of intentional mistakes", "author": ["L. Lid\u00e9n"], "venue": "AI Game Programming Wisdom 2, Charles River Media, INC.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Game Artificial Intelligence", "author": ["J.B. Ahlquist", "J. Novak"], "venue": "Game Development essentials. Thomson Delmar Learning, Canada", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Call for AI research in RTS games", "author": ["M. Buro"], "venue": "In Fu, D., Orkin, J., eds.: AAAI workshop on Challenges in Game AI, San Jose", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Steps toward building of a good ai for complex wargame-type simulation games", "author": ["V. Corruble", "C.A.G. Madeira", "G. Ramalho"], "venue": "In Mehdi, Q.H., Gough, N.E., eds.: 3rd International Conference on Intelligent Games and Simulation (GAME-ON 2002), London, UK", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "How qualitative spatial reasoning can improve strategy game ais", "author": ["K.D. Forbus", "J.V. Mahoney", "K. Dill"], "venue": "IEEE Intelligent Systems 17(4)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Playing to learn: case-injected genetic algorithms for learning to play computer games", "author": ["S.J. Louis", "C. Miles"], "venue": "IEEE Trans. Evol. Comput. 9(6)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Real-time neuroevolution in the nero video game", "author": ["K.O. Stanley", "B.D. Bryant", "R. Miikkulainen"], "venue": "IEEE Trans. Evol. Comput. 9(6)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Coevolution in hierarchical ai for strategy games", "author": ["D. Livingstone"], "venue": "IEEE Symposium on Computational Intelligence and Games (CIG05), Essex, UK, IEEE", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Co-evolving real-time strategy game playing influence map trees with genetic algorithms", "author": ["C. Miles", "S.J. Louis"], "venue": "In Press, I., ed.: International Congress on Evolutionary Computation, Portland, Oregon", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Evolving teams of cooperating agents for real-time strategy game", "author": ["P. Lichocki", "K. Krawiec", "W. Jaskowski"], "venue": "In et al., M.G., ed.: 1st European Workshop on Bio-inspired Algorithms in Games. Volume 5484 of LNCS., T\u00fcbingen, Germany, Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Intelligent anti-grouping in real-time strategy games", "author": ["N Beume"], "venue": "In Press, I., ed.: International Symposium on Computational Intelligence in Games, Perth, Australia", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Evolving robust strategies for an abstract realtime strategy game", "author": ["D. Keaveney", "C. O\u2019Riordan"], "venue": "In Press, I., ed.: International Symposium on Computational Intelligence in Games, Milano. Italy", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "A multi-agent potential field-based bot for a full RTS game scenario", "author": ["J. Hagelb\u00e4ck", "S.J. Johansson"], "venue": "In Darken, C., Youngblood, G.M., eds.: Proc.Fifth Artificial Intelligence and Interactive Digital Entertainment Conference (AIIDE 2009), Stanford, California, USA, The AAAI Press", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Emergence in Games", "author": ["P. Sweetser"], "venue": "Game development. Charles River Media, Boston, Massachussetts", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "However, intelligence does not generally equate to playing proficiency but rather to interesting behaviors [1].", "startOffset": 107, "endOffset": 110}, {"referenceID": 1, "context": "This issue is specifically relevant in real-time strategy (RTS) games that often employ two kinds of artificial intelligence (AI) [2]: one represented by a virtual player (VP, or non-player character \u2013 NPC) making decisions on a set of units (i.", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "As consequence, many RTS games contain \u2019holes\u2019 in the sense that the game stagnates or behaves incorrectly under very specific conditions (these problems rely on the category of \u2019artificial stupidity\u2019 [1]).", "startOffset": 201, "endOffset": 204}, {"referenceID": 2, "context": "In addition, there are very interesting research problems in developing AI for Real-Time Strategy (RTS) games including planning in an uncertain world with incomplete information, learning, opponent modeling, and spatial and temporal reasoning [3].", "startOffset": 244, "endOffset": 247}, {"referenceID": 3, "context": "Particular problems are caused by the large search spaces (environments consisting of many thousands of possible positions for each of hundreds, possibly thousands, of units) and the parallel nature of the problem unlike traditional games, any number of moves may be made simultaneously [4].", "startOffset": 287, "endOffset": 290}, {"referenceID": 4, "context": ", using abstract representations of the space [5]).", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 6, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 7, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 8, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 9, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 10, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 11, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 12, "context": "Regarding evolutionary techniques, a number of biologically-inspired algorithms and multiagent based methods have already been applied to handle many of the mentioned problems in the implementation of RTS games [6\u201313].", "startOffset": 211, "endOffset": 217}, {"referenceID": 13, "context": "This behavior emerges according to the player skill, and this emergent feature can make a RTS game more entertaining and less predictable in the sense that emergent behavior is not explicitly programmed but simply happens [14].", "startOffset": 222, "endOffset": 226}, {"referenceID": 0, "context": "Full body combat can be executed at the soldier level by assigning a random value (in the interval [0,1]) to each unit involved in a fight with a rival soldier; the energy level of the unit with lowest value decreases 1 unit.", "startOffset": 99, "endOffset": 104}, {"referenceID": 0, "context": "9 parent2 \u2190 Select (pop); 10 if Rand[0, 1] < pX then // recombination is done", "startOffset": 36, "endOffset": 42}, {"referenceID": 0, "context": "an individual encoding v) where each cell v[i] (for 0 \u2264 i < k) represents now a vector of 6 positions (one per action) and v[i][a] (for some action a \u2208 [1, 6]) contains the probability that human player executes action a under the environment conditions (i.", "startOffset": 152, "endOffset": 158}, {"referenceID": 5, "context": "an individual encoding v) where each cell v[i] (for 0 \u2264 i < k) represents now a vector of 6 positions (one per action) and v[i][a] (for some action a \u2208 [1, 6]) contains the probability that human player executes action a under the environment conditions (i.", "startOffset": 152, "endOffset": 158}, {"referenceID": 0, "context": "This extended answer matrix is finally used to design the virtual player as follows: VP[i] = argmaxa\u2208[1,6]{v[i][a]}, for all possible situations i.", "startOffset": 101, "endOffset": 106}, {"referenceID": 5, "context": "This extended answer matrix is finally used to design the virtual player as follows: VP[i] = argmaxa\u2208[1,6]{v[i][a]}, for all possible situations i.", "startOffset": 101, "endOffset": 106}], "year": 2012, "abstractText": "Basically, in (one-player) war Real Time Strategy (wRTS) games a human player controls, in real time, an army consisting of a number of soldiers and her aim is to destroy the opponent\u2019s assets where the opponent is a virtual (i.e., non-human player controlled) player that usually consists of a pre-programmed decision-making script. These scripts have usually associated some well-known problems (e.g., predictability, non-rationality, repetitive behaviors, and sensation of artificial stupidity among others). This paper describes a method for the automatic generation of virtual players that adapt to the player skills; this is done by building initially a model of the player behavior in real time during the game, and further evolving the virtual player via this model in-between two games. The paper also shows preliminary results obtained on a oneplayer wRTS game constructed specifically for experimentation.", "creator": "TeX"}}}