{"id": "1201.3204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2012", "title": "Evaluation of a Simple, Scalable, Parallel Best-First Search Strategy", "abstract": "Large-scale, parallel clusters composed of commodity processors are increasingly available, enabling the use of vast processing capabilities and distributed RAM to solve hard search problems. We investigate Hash-Distributed A* (HDA*), a simple approach to parallel best-first search that asynchronously distributes and schedules work among processors based on a hash function of the search state. We use this approach to parallelize the A* algorithm in an optimal sequential version of the Fast Downward planner, as well as a 24-puzzle solver. The scaling behavior of HDA* is evaluated experimentally on a shared memory, multicore machine with 8 cores, a cluster of commodity machines us- ing up to 64 cores, and a large-scale high-performance cluster using up to 1024 processors. We show that this approach scales well, allowing the effective utilization of large amount of distributed memory to optimally solve problems which require more than a terabyte of RAM. We also compare HDA* to Transposition-table Driven Scheduling (TDS), a hash-based parallelization of IDA*, and show that, in planning, HDA* significantly outperforms TDS. A simple hybrid which combines HDA* and TDS to exploit both of their strengths is proposed.", "histories": [["v1", "Mon, 16 Jan 2012 10:31:47 GMT  (262kb,S)", "https://arxiv.org/abs/1201.3204v1", null], ["v2", "Thu, 25 Oct 2012 03:39:16 GMT  (119kb)", "http://arxiv.org/abs/1201.3204v2", "in press, to appear in Artificial Intelligence"]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["akihiro kishimoto", "alex fukunaga", "adi botea"], "accepted": false, "id": "1201.3204"}, "pdf": {"name": "1201.3204.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Akihiro Kishimoto", "Alex Fukunaga", "Adi Botea"], "emails": ["kishimoto@is.titech.ac.jp", "fukunaga@idea.c.u-tokyo.ac.jp", "adibotea@ie.ibm.com"], "sections": [{"heading": null, "text": "ar Xiv: 120 1.32 04v2 [cs.AI] 25 OLarge-scale, parallel clusters consisting of raw material processors are increasingly available, allowing the use of huge processing capacities and distributed RAM to solve hard search problems. We are investigating hash-distributed A * (HDA *), a simple parallel best-first search approach in Australia that distributes and terminates work asynchronously between processors based on a hash function of the search state. We are using this approach to parallelise the A * algorithm in an optimal sequential version of the Fast Downward Planner, as well as a 24 puzzle solver. The scaling behaviour of HDA * is evaluated experimentally on a shared memory, multicore machine with 8 cores, a cluster of raw material machines with up to 64 cores and large-area high-performance clusters with up to 2400 processors."}, {"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2. Background", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "3. Hash Distributed A*", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4. Scalability of HDA*", "text": "Our hardware environments, including a single multicore machine (multicore), a commodity cluster (commodity), and two high-performance clusters (HPC clusters) with up to 2400 processors (HPC1, HPC2), are presented in Table 1. In all of our experiments, HDA * is implemented in C + +, compiled with g + +, and paralleled using the MPI message that the library passes on. We first describe experimental results for domain-independent planning. We have compared the sequential optimal version of the Fast Downward Planner, enriched with LFPA heuristics based on explicit (merge-and-shrink) requirements, with the sequentially optimal version of the abstraction."}, {"heading": "4.1. Asynchronous vs Synchronous Communications: Experiments on a Single, Multicore Machine", "text": "First, we evaluate HDA * on a single, multicore machine (presented in Table 1) to investigate the effects of asynchronous vs. synchronous communication in parallel A *. We compare HDA * with sequential A * and a shared memory implementation of A * (PRA *) [5] on a single, multicore machine. As described in Section 2, PRA * uses the same hash-based work allocation strategy as HDA *, but uses synchronous communication. \"Our PRA * implementation does not include the node retraction scheme, because the main goal of our experiments is to eliminate synchronization of synchronizations.\" Our HDA * implementation is the same MPI-based implementation on our larger scale that includes the reactionary schemes."}, {"heading": "4.2. Planning Experiments on a HPC Cluster", "text": "Next, we examine the scaling behavior of the HDA nodes on the HPC2 cluster, that is, we assume that it is a real problem that includes the time needed to execute the search algorithm, and exclude the time needed to calculate the abstraction table for the LFPA heuristically, since this phase was not paralleled by Fast Downward + LFPA, and therefore takes the same time to execute regardless of the number of cores. For example, the IPC-6 Pegsol-30 instance, which requires 168.18 seconds with 60 cores, requires 1200 cores plus 1.30 seconds for the abstraction table generation. \"A runtime value in Table 3 indicates failure, i.e."}, {"heading": "4.2.1. Load Balance", "text": "A common measurement of how evenly the work is distributed among the cores is the load balance, defined as the ratio of the maximum number of states that are extended by a core, and the average number of states that are extended by each core. As shown in Figure 2, HDA * achieves a good load balance when p / pmin \u2264 8. While the load balance tends to decrease when the number of processors increases, it is important to note that the load imbalance does not appear to be caused simply by the use of a large, absolute number of processors. This is caused, for example, by transpositions in the search space where the load balance for 2400 cores is less than 1.10. One possible reason for the load imbalance may be \"hotspots\" - frequently generated duplicate nodes that are assigned to a small number of cores by the hash function. This is caused by states where the transpositions in the search space, which are states, can be reached by the list of paths, are already achieved by the DA, although the closed state is * in the closed state."}, {"heading": "4.2.2. Search Overhead", "text": "The search effort, which indicates the additional states explored by parallel search, is defined as: SO = > DA = > Case Costs * > Number of states that can be expanded by parallel search < R = > Number of states that can be expanded by parallel search < R = > Number of states that can be expanded by parallel search < R = > Number of states that can be expanded by parallel search cannot be expanded. * This has the advantage that the basic configuration by definition will always be successful, so that a direct measurement of the search effort is possible. As we show in this section, it is also possible to analyze the search over the head."}, {"heading": "4.2.3. Node Expansion Rate", "text": "The expansion rate of the node helps in evaluating other parallel expansion rates, in addition to the search overhead. As the number of processors increases, the communication overhead can reduce the expansion rate of the node. Part of the communication overhead is tempered by the fact that the search and travel overlap to a large extent. In simple terms, some states are expanded while other states travel to their owner processor. As each core sends successors to their home processors in HDA *, the total number of messages exchanged between processors increases with a larger number of cores. This is exacerbated by the fact that more cores can end up generating more states (search overhead). Therefore, another cause for lower expansion rates of the node in message processing can be overhead - even with asynchronous communication, HDA * must deal with a larger number of messages as the number of cores increases. Figure 6 records the expansion rate of the expansion rate of the cluster as each one follows the expansion rate of the planning of the node 2."}, {"heading": "4.2.4. Termination Detection", "text": "If an optimal (but not yet proven optimal) solution is found, the cost c * will be emitted, so nodes with f \u2265 c * will not be expanded from now on. Therefore, the only nodes that will be expanded after emitting (if any) are nodes with f < c *, which will have to be expanded anyway to prove their optimality (Series A * is also expanding). So, if c * has been emitted and all nodes with f < c * have been expanded, state expansion and generation will cease, messages with states will no longer be sent around, and the abortion test will be successful."}, {"heading": "4.2.5. Scaling Behavior and the Number of Nodes (Machines) on a HPC Cluster", "text": "In fact, most of us are able to play by the rules that we have set ourselves to play by, \"he told the Deutsche Presse-Agentur.\" We have to play by the rules, \"he said,\" but we have to play by the rules. \"He added,\" We have to play by the rules that we have imposed on ourselves, and we have to play by the rules. \"He added,\" We have to play by the rules. \"He added,\" We have to play by the rules that we play by the rules. \""}, {"heading": "4.3. Results on the 24-Puzzle on a HPC Cluster", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4.4. Scaling Behavior on Planning on a Commodity Cluster", "text": "While the previous large-scale experiments were conducted on a high-performance computer cluster on campus, we also evaluated the scalability of HDA * on the commodity cluster (see Table 1 for machine specifications).Table 6 shows the relative efficiency of 16, 32 and 64 cores compared to a baseline of 8 cores (1 processing node).The results are organized by pmin, the minimum number of tested cores that solved the instance. For each Pmin, the average relative efficiency and relative acceleration.Several trends can be seen: First, when pmin = 8 and the number of cores used is increased to 16, the relative efficiency (0.55) and acceleration (1.10) are very low. After this initial threshold (the jump from 1 processing node to 2 processing nodes), the relative efficiency and relative acceleration in nodes become almost linear, with low search results."}, {"heading": "5. Tuning HDA* Performance", "text": "The previous section examined the scaling behavior of HDA * in different parallel environments, as the amount of available resources varied. In this section, we look at how the behavior of HDA * can be customized by adjusting two parameters: the number of cores to be used per processor node, and the number of states to be packed in each message between processes."}, {"heading": "5.1. Adjusting the Number of Cores to Utilize Per Processing Node", "text": "In fact, most of us will be able to do so."}, {"heading": "5.2. The Effect of the Number of States Packed into Each Message", "text": "In HDA *, each generation of states requires that the state be sent from the processor in which a state is created to the processor that \"owns\" the state. Sending a message from the processor P to Q each time a state owned by Q is generated at P can result in excessive communication effort, as well as overhead for creating / manipulating MPI message structures. To amortize these overheads, Romein et al. [12], in their work on TDS, could lead to the proposed packing of several states for the same purpose. On the other hand, packing too many states into a message from the processor P to Q could lead to worse performance for two reasons: First, the target Q could be starving for work and idle. Second, too much packing can lead to a search effort, as follows. Consider a state S on an optimal path that is \"delayed\" because the processor that generates S waits to pack more states per time in the message S."}, {"heading": "6. Hash-Based Work Distribution vs. Random Work Distribution on a HPC Cluster", "text": "Kumar et al. [23] and Karp and Zhang [24] proposed a simple, random work distribution strategy for the best initial search, in which generated nodes are sent to a random processor. Although this is similar to HDA * in that a random mechanism is used to distribute the work, the difference is that duplicate states are not necessarily sent to the same processor because a state does not have an \"owner.\" Although duplicates are truncated locally on each processor, there is no global duplicate detection, so in the worst case, a state may be on the local open / closed lists of each individual processor. We evaluated this \"random\" work distribution strategy based on our FastDownward domain-independent planners on the HPC1 cluster. First, we tried to compare HDA * and random work using 16 cores, 2GB per core."}, {"heading": "7. Comparison of HDA* vs. TDS on a Commodity Cluster", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "7.1. A Simple, Hybrid Strategy Combining HDA* and TDS", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "8. Related Work", "text": "In fact, it is the case that most of them are able to survive themselves if they do not put themselves in a position to survive themselves. In fact, it is the case that they are able to survive themselves, and that they are able to survive themselves. In fact, it is the case that they are not able to survive themselves. In fact, it is the case that they are able to survive themselves."}, {"heading": "9. Discussion and Conclusion", "text": "This year it is more than ever before in the history of the city."}, {"heading": "Acknowledgments", "text": "This research is supported by the JSPS Compview GCOE, the JST PRESTO program and JSPS grants-in-aid for research. Thanks to Malte Helmert for providing the Fast Downward code, and to Rich Korf for providing his IDA * 24 puzzle solvers, sample database code, and puzzle instances. We thank the anonymous reviewers for their feedback. [1] P. Haslum, H. Geffner, Admissible heuristics for optimal planning, in: Proceedings of the Fifth International Conference on AI Planning and Scheduling, 2000, pp. 140-149. [2] S. Edelkamp, Planning with pattern databases, in: Proceedings of the European Conference on Planning ECP-01, pp. 13-34. [3] M. Helmert, P. Haslum, J. Hoffmann, Flexible abstraction heuristics for optimal sequential planning, in: Proceedings of the Seventeth Planning Systems International Conference and Scheduling."}], "references": [{"title": "Admissible heuristics for optimal planning", "author": ["P. Haslum", "H. Geffner"], "venue": "in: Proceedings of the Fifth International Conference on AI Planning and Scheduling", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Planning with pattern databases", "author": ["S. Edelkamp"], "venue": "in: Proceedings of the European Conference on Planning ECP-01", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Flexible abstraction heuristics for optimal sequential planning", "author": ["M. Helmert", "P. Haslum", "J. Hoffmann"], "venue": "in: Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling ICAPS-07", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "A formal basis for the heuristic determination of minimum cost paths", "author": ["P. Hart", "N. Nilsson", "B. Raphael"], "venue": "IEEE Transactions on Systems Science and Cybernetics 4 (2) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1968}, {"title": "PRA\u2217: Massively parallel heuristic search", "author": ["M. Evett", "J. Hendler", "A. Mahanti", "D. Nau"], "venue": "Journal of Parallel and Distributed Computing 25 (2) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Scalable global and local hashing strategies for duplicate pruning in parallel A* graph search", "author": ["N. Mahapatra", "S. Dutt"], "venue": "IEEE Transactions on Parallel and Distributed Systems 8 (7) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1997}, {"title": "Large-scale parallel breadth-first search", "author": ["R. Korf", "P. Schultze"], "venue": "in: Proceedings of the National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Parallel structured duplicate detection", "author": ["R. Zhou", "E. Hansen"], "venue": "in: Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Best-first heuristic search for multi-core machines", "author": ["E. Burns", "S. Lemons", "R. Zhou", "W. Ruml"], "venue": "in: Proceedings of the Twenty-First International Joint Conference on Artificial Intelligence IJCAI-09", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "MPI: the complete reference", "author": ["M. Snir", "W. Gropp"], "venue": "MIT Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Disjoint pattern database heuristics", "author": ["R.E. Korf", "A. Felner"], "venue": "Artificial Intelligence 134 (1-2) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Transposition table driven work scheduling in distributed search", "author": ["J.W. Romein", "A. Plaat", "H.E. Bal", "J. Schaeffer"], "venue": "in: Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "A performance analysis of transposition-table-driven work scheduling in distributed search", "author": ["J.W. Romein", "H.E. Bal", "J. Schaeffer", "A. Plaat"], "venue": "IEEE Transactions on Parallel and Distributed Systems 13 (5) ", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Parallel depth-first search on multiprocessors part I: Implementation", "author": ["V.N. Rao", "V. Kumar"], "venue": "International Journal of Parallel Programming 16 (6) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1987}, {"title": "Spielbaumsuche auf massiv parallelen systemen", "author": ["R. Feldmann"], "venue": "Ph.D. thesis, University of Paderborn ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1993}, {"title": "The implementation of the Cilk-5 multithreaded language", "author": ["M. Frigo", "C.E. Leiserson", "K.H. Randall"], "venue": "in: Proceedings of the ACM SIG- PLAN Conference on Programming Language Design and Implementation (PLDI)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Depth-first heuristic search on a SIMD machine", "author": ["C. Powley", "C. Ferguson", "R. Korf"], "venue": "Artificial Intelligence 60 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1993}, {"title": "A SIMD approach to parallel heuristic search", "author": ["A. Mahanti", "C. Daniels"], "venue": "Artificial Intelligence 60 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Best-first frontier search with delayed duplicate detection", "author": ["R. Korf"], "venue": "in: Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Structured duplicate detection in external-memory graph search", "author": ["R. Zhou", "E. Hansen"], "venue": "in: Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Cost-optimal external planning", "author": ["S. Edelkamp", "S. Jabbar"], "venue": "in: Proceedings of the Twenty-First National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Parallel best-first search of statespace graphs: A summary of results", "author": ["V. Kumar", "K. Ramesh", "V.N. Rao"], "venue": "in: Proceedings of the 7th National Conference on Artificial Intelligence AAAI-88", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1988}, {"title": "A randomized parallel branch-and-bound procedure", "author": ["R. Karp", "Y. Zhang"], "venue": "in: Proceedings of the 20th ACM Symposium on Theory of Computing (STOC)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1988}, {"title": "Randomized parallel algorithms for backtrack search and branch-and-bound computation", "author": ["R. Karp", "Y. Zhang"], "venue": "Journal of the Association for Computing Machinery 40 (3) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1993}, {"title": "Scalable load balancing strategies for parallel A* algorithms", "author": ["S. Dutt", "N. Mahapatra"], "venue": "Journal of Parallel and Distributed Computing 22 ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "Divide-and-conquer frontier search applied to optimal sequence alignment", "author": ["R.E. Korf", "W. Zhang"], "venue": "in: Proceedings of the 17th National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2000}, {"title": "Domain-independent structured duplicate detection", "author": ["R. Zhou", "E. Hansen"], "venue": "in: Proceedings of the 21st National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "S", "author": ["P. Chakrabarti", "S. Ghose", "A. Acharya"], "venue": "de Sarkar, Heuristic search in restricted memory, Artificial Intelligence 41 (2) ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1989}, {"title": "Efficient memory-bounded search methods", "author": ["S. Russell"], "venue": "in: Proceedings of the 10th European Conference on Artificial Intelligence (ECAI-92)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1992}, {"title": "Evaluations of Hash Distributed A* in optimal sequence alignment", "author": ["Y. Kobayashi", "A. Kishimoto", "O. Watanabe"], "venue": "in: Proceedings of the 22nd International Joint Conference on Artificial Intelligence", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Distributed game-tree search using transposition table driven work scheduling", "author": ["A. Kishimoto", "J. Schaeffer"], "venue": "in: Proceedings of the 31st International Conference on Parallel Processing", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Solving Awari with Parallel Retrograde Analysis", "author": ["J.W. Romein", "H.E. Bal"], "venue": "IEEE Computer 36 (10) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2003}, {"title": "Scalable distributed Monte-Carlo Tree Search", "author": ["K. Yoshizoe", "A. Kishimoto", "T. Kaneko", "H. Yoshimoto", "Y. Ishikawa"], "venue": "in: Proceedings of the 4th Symposium on Combinatorial Search SOCS-11", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalable", "author": ["A. Kishimoto", "A. Fukunaga", "A. Botea"], "venue": "parallel best-first search for optimal sequential planning, in: Proceedings of the International Conference on Automated Planning and Scheduling", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Algorithms for distributed termination detection", "author": ["F. Mattern"], "venue": "Distributed Computing 2 (3) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1987}, {"title": "A new hashing method with applications for game playing", "author": ["A.L. Zobrist"], "venue": "Tech. rep., Department of Computer Science, University of Wisconsin, Madison, reprinted in International Computer Chess Association Journal, 13(2):169-173, 1990 ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1970}, {"title": "Complexity results for SAS planning", "author": ["C. B\u00e4ckstr\u00f6m", "B. Nebel"], "venue": "Computational Intelligence 11 (4) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1995}, {"title": "Macro-FF: Improving AI planning with automatically learned Macro-operators", "author": ["A. Botea", "M. Enzenberger", "M. M\u00fcller", "J. Schaeffer"], "venue": "Journal of Artificial Intelligence Research 24 ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Sequential and parallel algorithms for frontier A* with delayed duplicate detection", "author": ["R. Niewiadomski", "J.N. Amaral", "R.C. Holte"], "venue": "in: Proceedings of the 21st National Conference on Artificial Intelligence (AAAI)", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Iterative resource allocation for memory intensive parallel search algorithms on clouds", "author": ["A. Fukunaga", "A. Kishimoto", "A. Botea"], "venue": "grids, and shared clusters, in: Proceedings of the 26th AAAI Conference on Artificial Intelligence", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "On transposition tables for singleagent search and planning: Summary of results", "author": ["Y. Akagi", "A. Kishimoto", "A. Fukunaga"], "venue": "in: Proceedings of the 3rd Symposium on Combinatorial Search (SOCS)", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Parallelizing the Murphi verifier", "author": ["U. Stern", "D.L. Dill"], "venue": "in: Proceedings of the 9th International Conference on Computed Aided Verification", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1997}, {"title": "Parallelizing the Murphi verifier", "author": ["U. Stern", "D.L. Dill"], "venue": "Formal Methods in System Design 18 (2) ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2001}, {"title": "Parallel and distributed model checking in Eddy", "author": ["I. Melatti", "R. Palmer", "G. Sawaya", "Y. Yang", "R.M. Kirby", "G. Gopalakrishnan"], "venue": "International Journal on Software Tools for Technology Transfer 11 (1) ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Distributed-memory model checking with SPIN", "author": ["F. Lerda", "R. Sisto"], "venue": "in: Theoretical and Practical Aspects of SPIN Model Checking, 5th and 6th International SPIN Workshops, Vol. 1680 of Lecture Notes in Computer Science", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1999}, {"title": "The design of a multicore extension of the SPIN model checker", "author": ["G.J. Holzmann", "D. Bo\u015dna\u0109ki"], "venue": "IEEE Transactions on Software Engineering 33 (10) ", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "Parallel state space construction for model-checking", "author": ["H. Garavel", "R. Mateescu", "I.M. Smarandache"], "venue": "in: Proceedings of the 8th International SPIN Workshop", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2001}, {"title": "Achieving scalability in parallel reachability analysis of very large circuits", "author": ["T. Heyman", "D. Geist", "O. Grumberg", "A. Schuster"], "venue": "in: Proceedings 12th International Conference on Computer Aided Verification", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient large-scale model checking", "author": ["K. Verstoep", "H. Bal", "J. Barnat", "L. Brim"], "venue": "in: 23rd IEEE International Parallel and Distributed Processing Symposium ", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep Blue", "author": ["M. Campbell", "J. Hoane", "F. Hsu"], "venue": "Artificial Intelligence 134 (1- 2) ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2002}, {"title": "On the parallelization of UCT", "author": ["T. Cazenave", "N. Jouandeau"], "venue": "in: H. van den Herik et al. (Ed.), Proceedings of Computers and Games CG-08, Vol. 5131 of Lecture Notes in Computer Science, Springer", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2008}, {"title": "Parallel planning via the distribution of operators", "author": ["D. Vrakas", "I. Refanidis", "I. Vlahavas"], "venue": "Journal of Experimental and Theoretical Artificial Intelligence 13 (3) ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2001}, {"title": "An economics approach to hard computational problems", "author": ["B. Huberman", "R. Lukose", "T. Hogg"], "venue": "Science 275 (5296) ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1997}, {"title": "Heavy-tailed phenomena in satisfiability and constraint satisfaction problems", "author": ["C. Gomes", "B. Selman", "N. Crato", "H. Kautz"], "venue": "Journal of Automated Reasoning 24 (1-2) ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2000}, {"title": "ManySAT: a parallel SAT solver", "author": ["Y. Hamadi", "S. Jabbour", "L. Sais"], "venue": "Journal on Satisfiability, Boolean Modeling and Computation 6 ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2009}, {"title": "Single-agent parallel window search", "author": ["C. Powley", "R. Korf"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 13 (5) ", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1991}, {"title": "Adaptive k-parallel best-first search: A simple but efficient algorithm for multi-core domain-independent planning", "author": ["V. Vidal", "L. Bordeaux", "Y. Hamadi"], "venue": "in: Proceedings of the 3rd Symposium on Combinatorial Search (SOCS\u201910)", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "Kbfs: K-best-first search", "author": ["A. Felner", "S. Kraus", "R.E. Korf"], "venue": "Annals of Mathematics and Artificial Intelligence 39 ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2003}, {"title": "Adaptive parallel iterative deepening search", "author": ["D. Cook", "R. Varnell"], "venue": "Journal of Artificial Intelligence Research 9 ", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1998}, {"title": "Best-first heuristic search for multicore machines", "author": ["E. Burns", "S. Lemons", "W. Ruml", "R. Zhou"], "venue": "Journal of Artificial Intelligence Research (JAIR) 39 ", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Despite significant progress in recent years in developing domain-independent admissible heuristics [1, 2, 3], scaling up optimal planning remains a challenge.", "startOffset": 100, "endOffset": 109}, {"referenceID": 1, "context": "Despite significant progress in recent years in developing domain-independent admissible heuristics [1, 2, 3], scaling up optimal planning remains a challenge.", "startOffset": 100, "endOffset": 109}, {"referenceID": 2, "context": "Despite significant progress in recent years in developing domain-independent admissible heuristics [1, 2, 3], scaling up optimal planning remains a challenge.", "startOffset": 100, "endOffset": 109}, {"referenceID": 3, "context": "We introduce and evaluate Hash Distributed A* (HDA*), a parallelization of A* [4].", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "[5], and later extended by Mahapatra and Dutt [6], the scalability and limitations of hash-based work distribution and duplicate pruning have not been previously evaluated in depth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5], and later extended by Mahapatra and Dutt [6], the scalability and limitations of hash-based work distribution and duplicate pruning have not been previously evaluated in depth.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "While there has been some recent work in parallel search [7, 8, 9], these approaches are multi-threaded, and limited to a single, shared-memory machine.", "startOffset": 57, "endOffset": 66}, {"referenceID": 7, "context": "While there has been some recent work in parallel search [7, 8, 9], these approaches are multi-threaded, and limited to a single, shared-memory machine.", "startOffset": 57, "endOffset": 66}, {"referenceID": 8, "context": "While there has been some recent work in parallel search [7, 8, 9], these approaches are multi-threaded, and limited to a single, shared-memory machine.", "startOffset": 57, "endOffset": 66}, {"referenceID": 9, "context": "When implemented using the standard MPI message passing library [10], the exact same code can be executed on a wide array of parallel environments, ranging from a standard desktop multicore to a massive cluster with thousands of cores, effectively using all of the aggregate CPU and memory resources available on the system.", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "We use the cost-optimal version with the explicit (merge-and-shrink) abstraction heuristic reported by Helmert, Haslum, and Hoffman [3].", "startOffset": 132, "endOffset": 135}, {"referenceID": 10, "context": "The second solver is an application-specific 24-puzzle solver, which uses the pattern database heuristic code provided by Korf and Felner [11].", "startOffset": 138, "endOffset": 142}, {"referenceID": 11, "context": "The experiments include a comparison with TDS [12, 13], a successful parallelization of IDA* with a distributed transposition table.", "startOffset": 46, "endOffset": 54}, {"referenceID": 12, "context": "The experiments include a comparison with TDS [12, 13], a successful parallelization of IDA* with a distributed transposition table.", "startOffset": 46, "endOffset": 54}, {"referenceID": 13, "context": "[14, 15, 16]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 14, "context": "[14, 15, 16]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 15, "context": "[14, 15, 16]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "While most work on work-stealing has been on MIMD systems, parallelization of IDA* on SIMD machines using an alternating, two-phase mechanism, with a search phase and a load balancing phase, has also been investigated [17, 18].", "startOffset": 218, "endOffset": 226}, {"referenceID": 17, "context": "While most work on work-stealing has been on MIMD systems, parallelization of IDA* on SIMD machines using an alternating, two-phase mechanism, with a search phase and a load balancing phase, has also been investigated [17, 18].", "startOffset": 218, "endOffset": 226}, {"referenceID": 18, "context": "Another approach to search space partitioning (particularly in sharedmemory search) is derived from a line of work on addressing memory capacity limitations by using a large amount of slower, external memory (such as disks), to store states in search [19, 20, 21] (external memory was also used specifically for planning [22]).", "startOffset": 251, "endOffset": 263}, {"referenceID": 19, "context": "Another approach to search space partitioning (particularly in sharedmemory search) is derived from a line of work on addressing memory capacity limitations by using a large amount of slower, external memory (such as disks), to store states in search [19, 20, 21] (external memory was also used specifically for planning [22]).", "startOffset": 251, "endOffset": 263}, {"referenceID": 20, "context": "Another approach to search space partitioning (particularly in sharedmemory search) is derived from a line of work on addressing memory capacity limitations by using a large amount of slower, external memory (such as disks), to store states in search [19, 20, 21] (external memory was also used specifically for planning [22]).", "startOffset": 321, "endOffset": 325}, {"referenceID": 6, "context": "Korf has implemented a multithreaded, breadth-first search using a shared work queue which uses external memory [7, 20].", "startOffset": 112, "endOffset": 119}, {"referenceID": 7, "context": "Zhou and Hansen [8] introduce a parallel, breadth-first search algorithm.", "startOffset": 16, "endOffset": 19}, {"referenceID": 8, "context": "[9] have investigated best-first search algorithms that include enhancements such as structured duplicate detection and speculative search.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "In an early study, Kumar, Ramesh, and Rao [23] identified two broad approaches to parallelizing best-first search, based on how the usage and maintenance of the open list was parallelized.", "startOffset": 42, "endOffset": 46}, {"referenceID": 21, "context": "Kumar, Ramesh and Rao [23], as well as Karp and Zhang [24, 25] proposed a random work allocation strategy, where newly generated states were sent to random processors.", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": "Kumar, Ramesh and Rao [23], as well as Karp and Zhang [24, 25] proposed a random work allocation strategy, where newly generated states were sent to random processors.", "startOffset": 54, "endOffset": 62}, {"referenceID": 23, "context": "Kumar, Ramesh and Rao [23], as well as Karp and Zhang [24, 25] proposed a random work allocation strategy, where newly generated states were sent to random processors.", "startOffset": 54, "endOffset": 62}, {"referenceID": 24, "context": ", [26]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 25, "context": "[27, 28]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "[27, 28]).", "startOffset": 0, "endOffset": 8}, {"referenceID": 4, "context": "Parallel Retracting A* (PRA*) [5] simultaneously addresses the problem", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "While PRA* incorporated the idea of hash-based work distribution, PRA* differs significantly from a parallel A* in that it is a parallel version of RA* [5], a limited memory search algorithm closely related to MA* [29] and SMA* [30].", "startOffset": 152, "endOffset": 155}, {"referenceID": 27, "context": "While PRA* incorporated the idea of hash-based work distribution, PRA* differs significantly from a parallel A* in that it is a parallel version of RA* [5], a limited memory search algorithm closely related to MA* [29] and SMA* [30].", "startOffset": 214, "endOffset": 218}, {"referenceID": 28, "context": "While PRA* incorporated the idea of hash-based work distribution, PRA* differs significantly from a parallel A* in that it is a parallel version of RA* [5], a limited memory search algorithm closely related to MA* [29] and SMA* [30].", "startOffset": 228, "endOffset": 232}, {"referenceID": 4, "context": "On the other hand, the implementation of this retraction mechanism in [5] incurs a significant synchronization overhead: when a processor P generates a new state s and sends it to the destination processor Q, P blocks and waits for Q to confirm that s has successfully been received and stored (or whether the send operation failed due to memory exhaustion at the target process).", "startOffset": 70, "endOffset": 73}, {"referenceID": 5, "context": "The idea of hash-based work distribution was investigated further by Mahapatra and Dutt [6], who studied parallel A* on a Hypercube architecture, where CPUs are connected by a hypercube network, while in current standard architectures machines are connected by either a mesh or torus network.", "startOffset": 88, "endOffset": 91}, {"referenceID": 24, "context": "process using Dutt and Mahapatra\u2019s Quality Equalizing (QE) strategy [26].", "startOffset": 68, "endOffset": 72}, {"referenceID": 5, "context": "Therefore, Mahapatra and Dutt also proposed Local Hashing of Nodes and QE (LOHA&QE), which incorporates a state space partitioning strategy and allocates disjoint partitions to disjoint processor groups in order to minimize communication costs [6].", "startOffset": 244, "endOffset": 247}, {"referenceID": 29, "context": "The notion of the levelized graph can sometimes be extended to exploit local hashing if the search space has some regularities on depths such as multiple sequence alignment [31].", "startOffset": 173, "endOffset": 177}, {"referenceID": 11, "context": "Transposition-table driven work scheduling (TDS) [12, 13] is a distributed memory, parallel IDA* algorithm.", "startOffset": 49, "endOffset": 57}, {"referenceID": 12, "context": "Transposition-table driven work scheduling (TDS) [12, 13] is a distributed memory, parallel IDA* algorithm.", "startOffset": 49, "endOffset": 57}, {"referenceID": 30, "context": "The ideas behind TDS have also been successfully integrated in adversarial two-player search [32, 33, 34].", "startOffset": 93, "endOffset": 105}, {"referenceID": 31, "context": "The ideas behind TDS have also been successfully integrated in adversarial two-player search [32, 33, 34].", "startOffset": 93, "endOffset": 105}, {"referenceID": 32, "context": "The ideas behind TDS have also been successfully integrated in adversarial two-player search [32, 33, 34].", "startOffset": 93, "endOffset": 105}, {"referenceID": 4, "context": "[5] and Mahapatra and Dutt [6] was done, and the parallel systems which are prevalent today have very different architectures.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5] and Mahapatra and Dutt [6] was done, and the parallel systems which are prevalent today have very different architectures.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "either as a component of a more complex algorithm [5] or as a straw man against which local hash-based distribution was considered [6], this is the first paper which analyzes the scalability and limitations of global hashing in depth.", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "either as a component of a more complex algorithm [5] or as a straw man against which local hash-based distribution was considered [6], this is the first paper which analyzes the scalability and limitations of global hashing in depth.", "startOffset": 131, "endOffset": 134}, {"referenceID": 33, "context": "An early version of this work has been previously presented in a conference paper [35].", "startOffset": 82, "endOffset": 86}, {"referenceID": 4, "context": "We now describe Hash Distributed A* (HDA*), a simple parallelization of A* which uses the hash-based work distribution strategy originally proposed in PRA* [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 2, "context": "Even if the heuristic function [3] is consistent, parallel A* search may sometimes have to re-open a state saved in the closed list.", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "This results in significant synchronization overhead \u2013 for example, it was observed in [9] that a straightforward implementation of PRA* exhibited extremely poor performance on the Grid search problem, and multicore performance for up to 8 cores was consistently slower than sequential A*.", "startOffset": 87, "endOffset": 90}, {"referenceID": 11, "context": "[12] showed that this communication overhead could be overcome by packing multiple states with the same destination into a single message.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "In a decentralized parallel A* (including HDA*), when a solution is discovered, there is no guarantee at that time that the solution is optimal [23].", "startOffset": 144, "endOffset": 148}, {"referenceID": 34, "context": "In our implementation of HDA*, we used the time algorithm of Mattern [36], which was also used in TDS.", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "Our implementation of HDA* uses the Zobrist function [37] to map a SAS+ state representation [38] to a hash key.", "startOffset": 53, "endOffset": 57}, {"referenceID": 36, "context": "Our implementation of HDA* uses the Zobrist function [37] to map a SAS+ state representation [38] to a hash key.", "startOffset": 93, "endOffset": 97}, {"referenceID": 37, "context": "The Zobrist function was previously used in domainindependent planning in MacroFF [39].", "startOffset": 82, "endOffset": 86}, {"referenceID": 33, "context": "In MacroFF, as well as an earlier version of HDA* [35], duplicate checking in the open/list was performed by checking if the hash key of a state was present in the open/closed list, so there was a", "startOffset": 50, "endOffset": 54}, {"referenceID": 2, "context": "We parallelized the sequential optimal version of the Fast Downward planner, enhanced with the so-called LFPA heuristic, which is based on explicit (merge-and-shrink) state abstraction [3].", "startOffset": 185, "endOffset": 188}, {"referenceID": 4, "context": "We compare HDA* with sequential A* and a shared-memory implementation of Parallel Retracting A* (PRA*) [5] on a single, multicore machine.", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "\u2019s experiments [9], our PRA* implementation does not include the node retraction scheme because the main goal of our experiments is to show the impact of eliminating synchronization overhead from PRA*.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "Mahapatra and Dutt also noted that sequential runtimes were not available for their scalability experiments [6].", "startOffset": 108, "endOffset": 111}, {"referenceID": 38, "context": "[40] (who called it \u201cspeedup efficiency\u201d).", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] have observed that the re-expansion rate increases in domains with non-unit transition costs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": ", number of CPUs) [41].", "startOffset": 18, "endOffset": 22}, {"referenceID": 10, "context": "We evaluated HDA* on the 24-puzzle, using an application-specific solver based on IDA* code provided by Rich Korf, which uses a disjoint pattern database heuristic [11].", "startOffset": 164, "endOffset": 168}, {"referenceID": 10, "context": "As benchmark instances, we used the 50-instance set of 24-puzzles reported in [11], Table 2.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "IDA* solves all 50 instances [11] whereas with our HDA* using 12 cores only 10 instances can be solved with 54GB of memory.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "[12], in their work on TDS, proposed packing multiple states with the same destination.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] and Karp and Zhang [24] proposed a simple, random work distribution strategy for best-first search where generated nodes are sent to a random processor.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] and Karp and Zhang [24] proposed a simple, random work distribution strategy for best-first search where generated nodes are sent to a random processor.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "Transposition-Driven Scheduling (TDS) is a parallelization of IDA* with a distributed transposition table, where hash-based work distribution is used to map states to processors for both scheduling and transposition table checks [13].", "startOffset": 229, "endOffset": 233}, {"referenceID": 12, "context": "TDS has been applied successfully to sliding tiles puzzles and Rubik\u2019s Cube [13], and has also been adapted for adversarial search [32, 33].", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": "TDS has been applied successfully to sliding tiles puzzles and Rubik\u2019s Cube [13], and has also been adapted for adversarial search [32, 33].", "startOffset": 131, "endOffset": 139}, {"referenceID": 31, "context": "TDS has been applied successfully to sliding tiles puzzles and Rubik\u2019s Cube [13], and has also been adapted for adversarial search [32, 33].", "startOffset": 131, "endOffset": 139}, {"referenceID": 40, "context": "While TDS has not been applied to planning, recent work has shown that IDA* with a transposition table (IDA*+TT) is a successful search strategy for optimal, domain-independent planning [42] \u2013 on problems which can be solved by A*, the runtime of IDA*+TT is usually within a factor of 4 of A*, and IDA*+TT can eventually solve problems where A* exhausts memory.", "startOffset": 186, "endOffset": 190}, {"referenceID": 12, "context": "Therefore, we compared HDA* and TDS [13] for planning.", "startOffset": 36, "endOffset": 40}, {"referenceID": 40, "context": "Our implementation of TDS uses a transposition table implementation based on [42], where the table entry replacement policy is a batch replacement policy which sorts entries according to access frequency and periodically frees 30% of the entries (preferring to keep most frequently accessed entries).", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "We incorporated techniques to overcome higher latency in a lower-bandwidth network described in [13], such as their modification to the termination detection algorithm.", "startOffset": 96, "endOffset": 100}, {"referenceID": 40, "context": "Although replacement based on subtree size performed best in sequential search [42], we did not implement this policy because subtree size computation would require extensive message passing in parallel search.", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "\u2019s stack does not exceed 1MB in their applications [13], our stack often used hundreds of megabytes of memory.", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "A possible future improvement is to combine Dutt and Mahapatra\u2019s technique in SEQ A* [6] with TDS and restrict initiating parallelism.", "startOffset": 85, "endOffset": 88}, {"referenceID": 11, "context": "While it is possible to achieve drastically large (sometimes super linear) speedup over IDA* [12, 13], the speedup diminishes as IDA* has access to larger and larger transposition tables.", "startOffset": 93, "endOffset": 101}, {"referenceID": 12, "context": "While it is possible to achieve drastically large (sometimes super linear) speedup over IDA* [12, 13], the speedup diminishes as IDA* has access to larger and larger transposition tables.", "startOffset": 93, "endOffset": 101}, {"referenceID": 4, "context": "ample, PRA* [5], has a state retraction mechanism which frees memory by retracting some states at the search frontier (however, as explained in Section 2), the PRA* retraction policy results in synchronization overhead).", "startOffset": 12, "endOffset": 15}, {"referenceID": 41, "context": "Parallel Mur\u03c6 [43, 44] addresses verification tasks that involve exhaustively enumerating all reachable states in a state space.", "startOffset": 14, "endOffset": 22}, {"referenceID": 42, "context": "Parallel Mur\u03c6 [43, 44] addresses verification tasks that involve exhaustively enumerating all reachable states in a state space.", "startOffset": 14, "endOffset": 22}, {"referenceID": 4, "context": "Similarly to HDA* and other work described in Section 2 [5, 6], Parallel Mur\u03c6 implements a hash-based work distribution schema where each state is assigned to a unique owner processor.", "startOffset": 56, "endOffset": 62}, {"referenceID": 5, "context": "Similarly to HDA* and other work described in Section 2 [5, 6], Parallel Mur\u03c6 implements a hash-based work distribution schema where each state is assigned to a unique owner processor.", "startOffset": 56, "endOffset": 62}, {"referenceID": 43, "context": "The Eddy Murphi model checker [46] specializes processors\u2019 tasks, defining two threads for each processing node.", "startOffset": 30, "endOffset": 34}, {"referenceID": 44, "context": "Lerda and Sisto parallelized the SPIN model checker to increase the availability of memory resources [47].", "startOffset": 101, "endOffset": 105}, {"referenceID": 45, "context": "Holzmann and Bo\u015dna\u0109ki [48] introduce an extension to SPIN to multicore, shared memory machines.", "startOffset": 22, "endOffset": 26}, {"referenceID": 46, "context": "[49] use hashbased work distribution to convert an implicitly defined model-checking state space into an explicit file representation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "has been addressed in [50].", "startOffset": 22, "endOffset": 26}, {"referenceID": 41, "context": ", [43, 44, 47, 49]), which involves visiting all reachable states, does not necessarily require optimality.", "startOffset": 2, "endOffset": 18}, {"referenceID": 42, "context": ", [43, 44, 47, 49]), which involves visiting all reachable states, does not necessarily require optimality.", "startOffset": 2, "endOffset": 18}, {"referenceID": 44, "context": ", [43, 44, 47, 49]), which involves visiting all reachable states, does not necessarily require optimality.", "startOffset": 2, "endOffset": 18}, {"referenceID": 46, "context": ", [43, 44, 47, 49]), which involves visiting all reachable states, does not necessarily require optimality.", "startOffset": 2, "endOffset": 18}, {"referenceID": 48, "context": "Finally, while previous work in model checking has used up to 256 processors [51], our work presents the largest scale experiments with hash-based work distribution to date, showing that hash-based work distribution can scale efficiently relative to pmin even for up to 2400 processors.", "startOffset": 77, "endOffset": 81}, {"referenceID": 29, "context": "[31] have applied HDA* to multiple sequence alignment (MSA).", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": ", [52, 53]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 50, "context": ", [52, 53]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 51, "context": "The Operator Distribution Method for parallel Planning (ODMP) [54] parallelizes the computation at each node.", "startOffset": 62, "endOffset": 66}, {"referenceID": 52, "context": "This approach, which is a parallel version of an algorithm portfolio [55], seeks to exploit the long-tailed runtime distribution behavior encountered in search algorithms [56] by using different versions of search algorithms to search different (potentially overlapping) portions of the search space.", "startOffset": 69, "endOffset": 73}, {"referenceID": 53, "context": "This approach, which is a parallel version of an algorithm portfolio [55], seeks to exploit the long-tailed runtime distribution behavior encountered in search algorithms [56] by using different versions of search algorithms to search different (potentially overlapping) portions of the search space.", "startOffset": 171, "endOffset": 175}, {"referenceID": 54, "context": "An example of this is the ManySAT solver [57], which executes a different version of a DPLL-based backtracking SAT solver on each processor and periodically shares lemmas among the processes.", "startOffset": 41, "endOffset": 45}, {"referenceID": 55, "context": "A third approach is parallel-window search for IDA* [58], where each processor searches from the same root node, but is assigned a different bound \u2013 that is, each processor is assigned a different, independent iteration of IDA*.", "startOffset": 52, "endOffset": 56}, {"referenceID": 56, "context": "[59] propose a multicore version of the KBFS algorithm [60].", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[59] propose a multicore version of the KBFS algorithm [60].", "startOffset": 55, "endOffset": 59}, {"referenceID": 58, "context": "The EUREKA system [61] used machine learning to automatically configure parallel IDA* for various problems (including nonlinear planning) and machine architectures.", "startOffset": 18, "endOffset": 22}, {"referenceID": 38, "context": "[40] propose PFA*-DDD, a parallel version of Frontier A* with Delayed Duplicate Detection.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The key idea, first used in Parallel Retracting A* [5], is to distribute work according to the hash value for generated states.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "Also, unlike previous work such as PRA* and GOHA [6], which implemented hash-based work distribution on variants of A*, HDA* is a straightforward implementation of hash-based work distribution for standard A*.", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "We evaluated HDA* as a replacement for the sequential A* search engine for a state-of-the-art, optimal sequential planner, Fast Downward [3].", "startOffset": 137, "endOffset": 140}, {"referenceID": 10, "context": "We also evaluated HDA* on the 24-puzzle domain by implementing a parallel solver with a disjoint pattern database heuristic [11].", "startOffset": 124, "endOffset": 128}, {"referenceID": 59, "context": "have recently proposed PBNF, a shared memory, parallel best-first search algorithm, and showed that PBNF outperforms HDA* on planning in shared memory environments [62].", "startOffset": 164, "endOffset": 168}, {"referenceID": 21, "context": "Comparison with a randomized work distribution strategy which performs load balancing but no duplicate detection [23, 24] showed the simple hash-based duplicate detection mechanism is essential to the performance of HDA*.", "startOffset": 113, "endOffset": 121}, {"referenceID": 22, "context": "Comparison with a randomized work distribution strategy which performs load balancing but no duplicate detection [23, 24] showed the simple hash-based duplicate detection mechanism is essential to the performance of HDA*.", "startOffset": 113, "endOffset": 121}, {"referenceID": 2, "context": ", computation of the abstraction heuristic table [3] and pattern database generation [11]) is another area for future work.", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": ", computation of the abstraction heuristic table [3] and pattern database generation [11]) is another area for future work.", "startOffset": 85, "endOffset": 89}, {"referenceID": 8, "context": "machines using hash-based distribution, but within a single machine incorporate techniques such as speculative expansion that have been shown to scale well on a shared memory environment [9].", "startOffset": 187, "endOffset": 190}, {"referenceID": 39, "context": "We have recently analyzed an iterative resource allocation policy to address this [41].", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "[1] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] P.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] N.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] R.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] E.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[29] P.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[30] S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[31] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[33] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[35] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "[36] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "[37] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "[38] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[39] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[40] R.", "startOffset": 0, "endOffset": 4}, {"referenceID": 39, "context": "[41] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 40, "context": "[42] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[43] U.", "startOffset": 0, "endOffset": 4}, {"referenceID": 42, "context": "[44] U.", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[46] I.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[47] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[48] G.", "startOffset": 0, "endOffset": 4}, {"referenceID": 46, "context": "[49] H.", "startOffset": 0, "endOffset": 4}, {"referenceID": 47, "context": "[50] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "[51] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 49, "context": "[52] M.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "[53] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[54] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "[55] B.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "[56] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[57] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "[58] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "[59] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "[60] A.", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "[61] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 59, "context": "[62] E.", "startOffset": 0, "endOffset": 4}], "year": 2012, "abstractText": "Large-scale, parallel clusters composed of commodity processors are increasingly available, enabling the use of vast processing capabilities and distributed RAM to solve hard search problems. We investigate Hash-Distributed A* (HDA*), a simple approach to parallel best-first search that asynchronously distributes and schedules work among processors based on a hash function of the search state. We use this approach to parallelize the A* algorithm in an optimal sequential version of the Fast Downward planner, as well as a 24-puzzle solver. The scaling behavior of HDA* is evaluated experimentally on a shared memory, multicore machine with 8 cores, a cluster of commodity machines using up to 64 cores, and large-scale high-performance clusters, using up to 2400 processors. We show that this approach scales well, allowing the effective utilization of large amounts of distributed memory to optimally solve problems which require terabytes of RAM. We also compare HDA* to Transposition-table Driven Scheduling (TDS), a hash-based parallelization of IDA*, and show that, in planning, HDA* significantly outperforms TDS. A simple hybrid which combines HDA* and TDS to exploit strengths of both algorithms is proposed and evaluated.", "creator": "gnuplot 4.4 patchlevel 3"}}}