{"id": "1201.4080", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Jan-2012", "title": "Progress in animation of an EMA-controlled tongue model for acoustic-visual speech synthesis", "abstract": "We present a technique for the animation of a 3D kinematic tongue model, one component of the talking head of an acoustic-visual (AV) speech synthesizer. The skeletal animation approach is adapted to make use of a deformable rig controlled by tongue motion capture data obtained with electromagnetic articulography (EMA), while the tongue surface is extracted from volumetric magnetic resonance imaging (MRI) data. Initial results are shown and future work outlined.", "histories": [["v1", "Thu, 19 Jan 2012 15:29:56 GMT  (883kb,D)", "http://arxiv.org/abs/1201.4080v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ingmar steiner", "slim ouni"], "accepted": false, "id": "1201.4080"}, "pdf": {"name": "1201.4080.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["Firstname.Lastname@loria.fr"], "sections": [{"heading": "1 Introduction", "text": "As part of ongoing research into the development of a fully data-driven acoustic visual (AV) text tospeech (TTS) synthesizer [16], we are integrating a tongue model to enhance visual intelligibility and naturalness. To expand the kinematic paradigm used for facial animation in synthesizers, we are adapting state-of-the-art animation techniques with motion capture data for use with electromagnetic articulation (EMA). Our AV synthesizer1 is based on a non-uniform TTS system for French. [4], which links bimodal units of acoustics and visual data, and expands the selection algorithms with cost."}, {"heading": "2 EMA-based tongue model animation", "text": "In order to maintain the data-driven paradigm of the AV synthesizer, the tongue model2 consists of a geometric mesh that is represented in the GUI together with (or rather \"behind\") the face. Since the primary purpose of the tongue model is to improve the visual aspects of the synthesizer and it has no impact on the acoustics, there is no need for a complex tongue model to calculate the transmission function of the vocal tract, etc. Therefore, unlike previous work [e.g. 5, 6, 8, 10, 14, 17], most of which attempts to predict the tongue shape and / or movement by simulating the dynamics of one form or another, we simply have to create realistic tongue kinematics without having to model the anatomical structure of the human tongue or satisfy physical or biomechanical limitations. This scenario allows us to use the tongue shape and / or movement using tongue contour to temporarily capture the tongue contour language during the mid-2000s (where we normally use electromagnetic)."}, {"heading": "2.1 Tongue motion capture", "text": "In fact, it is as if most people are able to understand themselves and what they do. (...) It is not as if people are able to understand themselves. (...) It is not as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...) It is as if they do not understand the world. (...). (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. () It is. (...) It is. (...) It is. (...) It is. (...) It is. (it is. (...) It is. (it is. (...). It is. (...) It is. (it is. (...) It is. (it is. It is. (...). It is. (it is. It is. It is. (...). (it is. It is.). (it is. (it is.). (it is. It is. (it is.). (it is. (it is.). (it is.). (it is. (it is. (it is.). (it is."}, {"heading": "2.2 Tongue model animation", "text": "In fact, most of them are able to decide for themselves what they want."}, {"heading": "3 Discussion and Outlook", "text": "This year is the highest in the history of the country."}, {"heading": "Acknowledgments", "text": "We would like to thank Se'bastien Demange for assisting with the inclusion of the EMA test corpus and Korin Richmond for providing the resources to record the MRI data used here. 7Bullet Physics library, http: / / www.bulletphysics.com / 8For example, COLLADA (http: / / www.collada.org /) or OGRE (http: / / www.ogre3d.org /)"}], "references": [{"title": "LAPRIE: Registration of multimodal data for estimating the parameters of an articulatory model", "author": ["M. ARON", "A. TOUTIOS", "M.-O. BERGER", "E. KERRIEN", "Y.B. WROBEL-DAUTCOURT"], "venue": "In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "POPOVI\u0106: Automatic rigging and animation of 3D characters", "author": ["I. BARAN"], "venue": "In Proc. 34th International Conference and Exhibition on Computer Graphics and Interactive Techniques (SIGGRAPH),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "GOLDSTEIN: Articulatory Phonology: An overview", "author": ["C. BROWMAN"], "venue": "Phonetica,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "BEAUFORT: Linguistic features weighting for a text-to-speech system without prosody model", "author": ["V. COLOTTE"], "venue": "In Proc. Interspeech,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Combining MRI, EMA & EPG measurements in a three-dimensional tongue model", "author": ["O. ENGWALL"], "venue": "Speech Communication,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "PAYAN: 3D biomechanical tongue modeling to study speech production", "author": ["GERARD", "J.-M", "Y.P. PERRIER"], "venue": "In HARRINGTON, J. and M. TABAIN (eds.): Speech Production: Models, Phonetic Processes, and Techniques,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Three-dimensional electromagnetic articulography: A measurement principle", "author": ["T. KABURAGI", "K. WAKAMIYA", "M. HONDA"], "venue": "Journal of the Acoustical Society of America,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "PARENT: Creating speech-synchronized animation", "author": ["S.A. KING", "R. E"], "venue": "IEEE Transactions on Visualization and Computer Graphics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "SENDA: Difference in vocal tract shape between upright and supine postures: Observations by an open-type MRI scanner", "author": ["T. KITAMURA", "H. TAKEMOTO", "K. HONDA", "Y. SHIMADA", "I. FUJIMOTO", "Y. SYAKUDO", "S. MASAKI", "K. KURODA", "M.N. OKU-UCHI"], "venue": "Acoustical Science and Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "HUNTER: From experiments to articulatory motion \u2013 a three dimensional talking head model", "author": ["X.B. LU", "W. THORPE", "P.K. FOSTER"], "venue": "In Proc. Interspeech,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "THALMANN: Human motion capture driven by orientation measurements", "author": ["T. MOLET", "D.R. BOULIC"], "venue": "Presence: Teleoperators and Virtual Environments,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "OUNI: Introducing visual target cost within an acoustic-visual unit-selection speech synthesizer", "author": ["U. MUSTI", "V. COLOTTE", "S.A. TOUTIOS"], "venue": "In Proc. 10th International Conference on Auditory-Visual Speech Processing (AVSP), Volterra,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "SEAH: Modeling and animating the human tongue during speech production", "author": ["C. PELACHAUD", "C.C. VAN OVERVELD"], "venue": "In Proc. Computer Animation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Measurement of temporal changes in vocal tract area function from 3D cine-MRI data", "author": ["H. TAKEMOTO", "K. HONDA", "S. MASAKI", "Y. SHIMADA", "I. FUJIMOTO"], "venue": "Journal of the Acoustical Society of America,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "BERGER: Towards a true acoustic-visual speech synthesis", "author": ["A. TOUTIOS", "U. MUSTI", "S. OUNI", "V. COLOTTE", "B. WROBEL-DAUTCOURT", "M.-O"], "venue": "In Proc. 9th International Conference on Auditory-Visual Speech Processing (AVSP), pp. POS1\u20138,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "FELS: An efficient biomechanical tongue model for speech research", "author": ["F. VOGT", "J.E. LLOYD", "S. BUCHAILLARD", "P. PERRIER", "M. CHABANAS", "S.S.Y. PAYAN"], "venue": "In Proc. 7th International Seminar on Speech Production (ISSP),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}], "referenceMentions": [{"referenceID": 14, "context": "As part of ongoing research in developing a fully data-driven acoustic-visual (AV) text-tospeech (TTS) synthesizer [16], we integrate a tongue model to increase visual intelligibility and naturalness.", "startOffset": 115, "endOffset": 119}, {"referenceID": 3, "context": "Our AV synthesizer1 is based on a non-uniform unit-selection TTS system for French [4], concatenating bimodal units of acoustic and visual data, and extending the selection algorithm with visual target and join costs [13].", "startOffset": 83, "endOffset": 86}, {"referenceID": 11, "context": "Our AV synthesizer1 is based on a non-uniform unit-selection TTS system for French [4], concatenating bimodal units of acoustic and visual data, and extending the selection algorithm with visual target and join costs [13].", "startOffset": 217, "endOffset": 221}, {"referenceID": 6, "context": "Specifically, we apply electromagnetic articulography (EMA) using a Carstens AG5003 to obtain high-speed (200Hz), 3D motion capture data of the tongue during speech [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 13, "context": "de/ 43D cine-MRI of the vocal tract [15], while possible, is far from realistic for the compilation of a full speech corpus sufficient for TTS.", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "Technically, this corresponds to motion capture approaches such as [11], although the geometry is of course quite different for the tongue than for a humanoid skeleton.", "startOffset": 67, "endOffset": 71}, {"referenceID": 2, "context": "the 3D software\u2019s NLA editor (which, for these purposes, is conceptually similar to a gestural score in articulatory phonology [3]).", "startOffset": 127, "endOffset": 130}], "year": 2012, "abstractText": "We present a technique for the animation of a 3D kinematic tongue model, one component of the talking head of an acoustic-visual (AV) speech synthesizer. The skeletal animation approach is adapted to make use of a deformable rig controlled by tongue motion capture data obtained with electromagnetic articulography (EMA), while the tongue surface is extracted from volumetric magnetic resonance imaging (MRI) data. Initial results are shown and future work outlined.", "creator": "LaTeX with hyperref package"}}}