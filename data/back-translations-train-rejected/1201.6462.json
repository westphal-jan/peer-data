{"id": "1201.6462", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2012", "title": "Active Learning of Custering with Side Information Using $\\eps$-Smooth Relative Regret Approximations", "abstract": "Clustering is considered a non-supervised learning setting, in which the goal is to partition a collection of data points into disjoint clusters. Often a bound $k$ on the number of clusters is given or assumed by the practitioner. Many versions of this problem have been defined, most notably $k$-means and $k$-median.", "histories": [["v1", "Tue, 31 Jan 2012 07:46:08 GMT  (16kb)", "http://arxiv.org/abs/1201.6462v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nir ailon", "ron begleiter"], "accepted": false, "id": "1201.6462"}, "pdf": {"name": "1201.6462.pdf", "metadata": {"source": "CRF", "title": "Active Learning of Custering with Side Information Using \u03b5-Smooth Relative Regret Approximations", "authors": ["Nir Ailon"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 120 1.64 62v1 [cs.LG] 3 1Ja n20 12An underlying problem with the unattended nature of clustering is the determination of a similarity function. One approach to alleviating this difficulty is clustering with ancillary information, alternatively semi-monitored clustering. In this case, the user includes ancillary information in the form of \"must cluster\" or \"must be separated\" labels for data point pairs. Any such information comes at a \"query\" expense (often involving human response queries), and the collection of labels is then incorporated into the usual cluster algorithm either as strict or as soft constraints, possibly adding a picturesque streamlining function to the selected cluster objective. Our work mainly relates to clustering with ancillary information. We ask how to select the pairs of data points. Our analysis leads to a method that is demonstrably better than simply selecting them randomly."}, {"heading": "1 Introduction", "text": "Clustering data is probably the most important problem in the theory of unattended learning. By default, the goal is to paritize a collection of data points into related groups. Virtually any large-scale application that uses machine learning uses clustering either as a pre-processing step or as an end in itself. In the most tranditional sense, clustering is an unattended learning problem because the solution is calculated from the data itself, without human labeling involved. There are many versions, especially k-means and k-median. The number k typically serves as an assumed upper limit on the number of output clusters. A fundamental difficulty in the unattended nature of the cluster is the fact that a similarity (or distance) function between data points must be selected by the practitioner as a preliminary step. This may often not be an easy task, even if our data sets are easily embedded in a natural vector (attribute space), we still have the guarantee to choose a metric that replaces the good metric freedom."}, {"heading": "1.1 Previous Related Work", "text": "Clustering with ancillary information is a relatively new variant of clustering that is first described independently by Demiriz et al. [1999] and Ben-Dor et al. [1999]. In the machine learning community, it is also widely known as semi-supervised clustering. There are a few alternatives for the form of feedback that provides ancillary information, the most natural ones being the labels of individual items [e.g. Demiriz et al., 1999] and the paired limitations [e.g. Ben-Dor et al., 1999].In our study, ancillary information is cumbersome, has its price and is treated sparingly. In a related but different environment, similarity information is available to all (square many) pairs but is noisy. The combinatorial optimization problem theoretical problem of noise purification is known as correlation clustering [Bansal et al., 2002] or cluster editing [Shamir al, 2004]."}, {"heading": "1.2 Our Contribution", "text": "Our main motivation is to reduce the number of pairs of similarity labels (query costs) required for clustering k data using an active learning approach. Specifically, we are asking how to select which pairs of data point to a query. Our analysis leads to a method that is demonstrably better than simply randomly selecting them. Specifically, we are showing that the distribution from which we should draw pairs must be biased in order to place more emphasis on pairs that affect elements in smaller clusters in an optimal solution. Of course, we do not know the optimal solution, let alone the bias. Using the recently introduced method of \u03b5-smooth relative remorse approximations (\u03b5-SRRA) by Ailon et al. [2011], we can demonstrate an iterative process that improves both clustering and bias in tandem. The process is demonstrably moving closer to the optimal solution (in terms of algorithm selection costs) than an algorithm."}, {"heading": "2 Notation and Definitions", "text": "There are two sources of information that guide us in this process, one of which is unattended and may result from attributes attached to each element v, V together with a chosen distance function. An edge (u, v) corresponds to the constraint u, v should be summarized, and a non-edge (u, v) 6, E corresponds to the reverse conclusion. Each edge or non-edge comes at a query cost. This means that G only exists implicitly. We reveal the truth value of the predicates \"(u, v), E\" for all selected pairs u, v for a price. We also assume that G is riddled with human errors, so there is not necessarily a perfect aggregation of the data."}, {"heading": "3 The \u03b5-Smooth Relative Regret Approximation (\u03b5-SRRA)Method", "text": "The goal is to minimize the number of non-negative costs that are considered to be the distance d (f, h) between f, X, and h. It is also assumed that the distance function d between X and h is an extension of a metric on1. The initial problem definition does not limit the number of output clusters. 2The polynomial degree depends on the number of output clusters. 2The polynomial degree depends on the number of output clusters. [2011] has shown the following general scheme for determining the best f, X, and h. To explain this scheme, we need to define an idea of a smooth relative approximation."}, {"heading": "5 Conclusions and Future Work", "text": "Our study looked at the information theory problem of selecting which questions to ask in a game in which combinatorial pairs of hostile noisy information are entered into a cluster algorithm. We designed and analyzed a distribution from which pairs of characters are demonstrably better than the uniform distribution. Our analysis did not take into account geometric information (e.g. a feature vector attached to each data point) and treated similarity markers as secondary information, as suggested in a recent literature series. It would be interesting to study our solution in conjunction with geometric information. It would also be interesting to study our approach in the context of metric learning, where the goal is to skillfully select for which pairs (noisy) distance markers to obtain."}], "references": [{"title": "A new active learning scheme with applications", "author": ["Nir Ailon", "Ron Begleiter", "Esther Ezra"], "venue": "J. ACM,", "citeRegEx": "Ailon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ailon et al\\.", "year": 2008}, {"title": "Semi-supervised clustering using genetic algorithms", "author": ["Ayhan Demiriz", "Kristin Bennett", "Mark J. Embrechts"], "venue": "Artificial Neural Networks in Engineering", "citeRegEx": "Demiriz et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Demiriz et al\\.", "year": 1999}, {"title": "Correlation clustering with a fixed number of clusters", "author": ["Ioannis Giotis", "Venkatesan Guruswami"], "venue": "Theory of Computing,", "citeRegEx": "Giotis and Guruswami.,? \\Q2006\\E", "shortCiteRegEx": "Giotis and Guruswami.", "year": 2006}, {"title": "From instance-level constraints to space-level constraints: Making the most of prior knowledge in data clustering", "author": ["Dan Klein", "Sepandar D. Kamvar", "Christopher D. Manning"], "venue": "In ICML,", "citeRegEx": "Klein et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Klein et al\\.", "year": 2002}, {"title": "Approximation algorithm for correlation clustering", "author": ["P. Mitra", "M. Samal"], "venue": "In Networked Digital Technologies,", "citeRegEx": "Mitra and Samal.,? \\Q2009\\E", "shortCiteRegEx": "Mitra and Samal.", "year": 2009}, {"title": "Cluster graph modification problems", "author": ["Ron Shamir", "Roded Sharan", "Dekel Tsur"], "venue": "Discrete Applied Math,", "citeRegEx": "Shamir et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Shamir et al\\.", "year": 2004}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["Eric P. Xing", "Andrew Y. Ng", "Michael I. Jordan", "Stuart Russell"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Xing et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 5, "context": ", 2002] or cluster editing [Shamir et al., 2004].", "startOffset": 27, "endOffset": 48}, {"referenceID": 2, "context": "A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006].", "startOffset": 84, "endOffset": 112}, {"referenceID": 0, "context": "1 Previous Related Work Clustering with side information is a fairly new variant of clustering first described, independently, by Demiriz et al. [1999], and Ben-Dor et al.", "startOffset": 130, "endOffset": 152}, {"referenceID": 0, "context": "1 Previous Related Work Clustering with side information is a fairly new variant of clustering first described, independently, by Demiriz et al. [1999], and Ben-Dor et al. [1999]. In the machine learning community it is also widely known as semi-supervised clustering.", "startOffset": 130, "endOffset": 179}, {"referenceID": 0, "context": "Constant factor approximations are known for various versions of this problems [Charikar and Wirth, 2004, Ailon et al., 2008]. A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006]. Roughly speaking, there are two main approches for utilizing pairwise side information. In the first approach, this information is used to fine tune or learn a distance function, which is then passed on to any standard clustering algorithm. Examples include Cohn et al. [2000], Klein et al.", "startOffset": 106, "endOffset": 518}, {"referenceID": 0, "context": "Constant factor approximations are known for various versions of this problems [Charikar and Wirth, 2004, Ailon et al., 2008]. A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006]. Roughly speaking, there are two main approches for utilizing pairwise side information. In the first approach, this information is used to fine tune or learn a distance function, which is then passed on to any standard clustering algorithm. Examples include Cohn et al. [2000], Klein et al. [2002], and Xing et al.", "startOffset": 106, "endOffset": 539}, {"referenceID": 0, "context": "Constant factor approximations are known for various versions of this problems [Charikar and Wirth, 2004, Ailon et al., 2008]. A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006]. Roughly speaking, there are two main approches for utilizing pairwise side information. In the first approach, this information is used to fine tune or learn a distance function, which is then passed on to any standard clustering algorithm. Examples include Cohn et al. [2000], Klein et al. [2002], and Xing et al. [2002]. The second approach, which is the starting point to our work, modifies the clustering algorithms\u2019s objective so as to incorporate the pairwise constraints.", "startOffset": 106, "endOffset": 563}, {"referenceID": 0, "context": "Constant factor approximations are known for various versions of this problems [Charikar and Wirth, 2004, Ailon et al., 2008]. A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006]. Roughly speaking, there are two main approches for utilizing pairwise side information. In the first approach, this information is used to fine tune or learn a distance function, which is then passed on to any standard clustering algorithm. Examples include Cohn et al. [2000], Klein et al. [2002], and Xing et al. [2002]. The second approach, which is the starting point to our work, modifies the clustering algorithms\u2019s objective so as to incorporate the pairwise constraints. Basu [2005] in his thesis, which also serves as a comprehensive survey, has championed this approach in conjunction with k-means, and hidden Markov random field clustering algorithms.", "startOffset": 106, "endOffset": 732}, {"referenceID": 0, "context": "Constant factor approximations are known for various versions of this problems [Charikar and Wirth, 2004, Ailon et al., 2008]. A PTAS is known for a minimization version in which the number of clusters is fixed [Giotis and Guruswami, 2006]. Roughly speaking, there are two main approches for utilizing pairwise side information. In the first approach, this information is used to fine tune or learn a distance function, which is then passed on to any standard clustering algorithm. Examples include Cohn et al. [2000], Klein et al. [2002], and Xing et al. [2002]. The second approach, which is the starting point to our work, modifies the clustering algorithms\u2019s objective so as to incorporate the pairwise constraints. Basu [2005] in his thesis, which also serves as a comprehensive survey, has championed this approach in conjunction with k-means, and hidden Markov random field clustering algorithms. 1.2 Our Contribution Our main motivation is reducing the number of pairwise similarity labels (query cost) required for k-clustering data using an active learning approach. More precisely, we ask how to choose which pairs of data points to query. Our analysis gives rise to a method provably better than simply choosing them uniformly at random. More precisely, we show that the distribution from which we should draw pairs from must be biased so as more weight is placed on pairs incident to elements in smaller clusters in some optimal solution. Of course we do not know the optimal solution, let alone the bias. Using the recently introduced method of \u03b5-smooth relative regret approximations (\u03b5-SRRA) of Ailon et al. [2011] we can show an iterative process that improves both the clustering 1", "startOffset": 106, "endOffset": 1631}, {"referenceID": 0, "context": "We then present the \u03b5-SRRA method of Ailon et al. [2011] for the purpose of self containment in Section 3.", "startOffset": 37, "endOffset": 57}, {"referenceID": 0, "context": "Ailon et al. [2008], Charikar et al.", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Ailon et al. [2008], Charikar et al. [2005], Mitra and Samal [2009]).", "startOffset": 0, "endOffset": 44}, {"referenceID": 0, "context": "Ailon et al. [2008], Charikar et al. [2005], Mitra and Samal [2009]).", "startOffset": 0, "endOffset": 68}, {"referenceID": 0, "context": "Ailon et al. [2008], Charikar et al. [2005], Mitra and Samal [2009]). 1 Mitra and Samal [2009] achieved a PTAS for this problem, namely, a polynomial time algorithm returning a k-clustering with cost at most (1 + \u03b5) that of the optimal.", "startOffset": 0, "endOffset": 95}, {"referenceID": 0, "context": "Ailon et al. [2011] have recently shown the following general scheme for finding the best f \u2208 X .", "startOffset": 0, "endOffset": 20}, {"referenceID": 0, "context": "Ailon et al. [2011] have recently shown the following general scheme for finding the best f \u2208 X . To explain this scheme, we need to define a notion of \u03b5-smooth relative regret approximation. Given a solution f \u2208 X (call it the pivotal solution) and another solution g \u2208 X , we define \u2206f (g) to be d(g, h) \u2212 d(f, h), namely, the difference between the cost of the solution g and the cost of the solution f . We call this the relative regret function with respect to f . Assume we have oracle access to a function \u2206\u0302f : X \u2192 R such that for all g \u2208 X , |\u2206\u0302f (g) \u2212\u2206f (g)| \u2264 \u03b5d(f, g) . If such an estimator function \u2206\u0302f exists, we say that it is an \u03b5-smooth regret approximation (\u03b5SRRA) for with respect to f . Ailon et al. [2011] show that if we have an \u03b5-smooth regret approximation function, then it is possible to obtain a (1 + \u03b5)-approximation to the optimal solution by repeating the iterative process presented in Figure 3.", "startOffset": 0, "endOffset": 727}], "year": 2012, "abstractText": "Clustering is considered a non-supervised learning setting, in which the goal is to partition a collection of data points into disjoint clusters. Often a bound k on the number of clusters is given or assumed by the practitioner. Many versions of this problem have been defined, most notably k-means and k-median. An underlying problem with the unsupervised nature of clustering it that of determining a similarity function. One approach for alleviating this difficulty is known as clustering with side information, alternatively, semi-supervised clustering. Here, the practitioner incorporates side information in the form of \u201cmust be clustered\u201d or \u201cmust be separated\u201d labels for data point pairs. Each such piece of information comes at a \u201cquery cost\u201d (often involving human response solicitation). The collection of labels is then incorporated in the usual clustering algorithm as either strict or as soft constraints, possibly adding a pairwise constraint penalty function to the chosen clustering objective. Our work is mostly related to clustering with side information. We ask how to choose the pairs of data points. Our analysis gives rise to a method provably better than simply choosing them uniformly at random. Roughly speaking, we show that the distribution must be biased so as more weight is placed on pairs incident to elements in smaller clusters in some optimal solution. Of course we do not know the optimal solution, hence we don\u2019t know the bias. Using the recently introduced method of \u03b5-smooth relative regret approximations of Ailon, Begleiter and Ezra, we can show an iterative process that improves both the clustering and the bias in tandem. The process provably converges to the optimal solution faster (in terms of query cost) than an algorithm selecting pairs uniformly.", "creator": "LaTeX with hyperref package"}}}