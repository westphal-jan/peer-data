{"id": "1201.6583", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2012", "title": "Empowerment for Continuous Agent-Environment Systems", "abstract": "This paper develops generalizations of empowerment to continuous states. Empowerment is a recently introduced information-theoretic quantity motivated by hypotheses about the efficiency of the sensorimotor loop in biological organisms, but also from considerations stemming from curiosity-driven learning. Empowemerment measures, for agent-environment systems with stochastic transitions, how much influence an agent has on its environment, but only that influence that can be sensed by the agent sensors. It is an information-theoretic generalization of joint controllability (influence on environment) and observability (measurement by sensors) of the environment by the agent, both controllability and observability being usually defined in control theory as the dimensionality of the control/observation spaces. Earlier work has shown that empowerment has various interesting and relevant properties, e.g., it allows us to identify salient states using only the dynamics, and it can act as intrinsic reward without requiring an external reward. However, in this previous work empowerment was limited to the case of small-scale and discrete domains and furthermore state transition probabilities were assumed to be known. The goal of this paper is to extend empowerment to the significantly more important and relevant case of continuous vector-valued state spaces and initially unknown state transition probabilities. The continuous state space is addressed by Monte-Carlo approximation; the unknown transitions are addressed by model learning and prediction for which we apply Gaussian processes regression with iterated forecasting. In a number of well-known continuous control tasks we examine the dynamics induced by empowerment and include an application to exploration and online model learning.", "histories": [["v1", "Tue, 31 Jan 2012 15:46:27 GMT  (233kb)", "http://arxiv.org/abs/1201.6583v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["tobias jung", "daniel polani", "peter stone"], "accepted": false, "id": "1201.6583"}, "pdf": {"name": "1201.6583.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Tobias Jung", "Daniel Polani"], "emails": ["tjung@cs.utexas.edu", "d.polani@herts.ac.uk", "pstone@cs.utexas.edu"], "sections": [{"heading": null, "text": "Keywords: information theory, learning, dynamic systems, self-motivated behaviourShort title: Empowerment for Continuous Agent-Environment-Systems2"}, {"heading": "1 Introduction", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "2 Illustrative example", "text": "In fact, the fact is that most of them will be able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there"}, {"heading": "3 Computation of empowerment", "text": "This section formally defines empowerment and specifies an algorithm for its calculation."}, {"heading": "3.1 General definition of empowerment", "text": "It is defined for stochastic dynamic systems in which transitions arise as a result of decision-making, e.g. as an agent that interacts with an environment. Here, we will start from a vectorally evaluated state space that indicates the probability of transition from the state xt to xt + 1 when we make decisions. While we assume that the system is fully defined in terms of these 1-step interactions, we will also be interested in more general n-step interactions. For n-step interactions, we will consider the sequence ~ ant = (at,.) n-step actions and the induced probability density p (xt + n) of the corresponding n-step transition."}, {"heading": "3.2 A concrete numerical example", "text": "Before proceeding, we should clarify the previous definition by looking at a numerical example = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "3.3 Empowerment or mutual information?", "text": "Let us summarize: Empowerment measures the extent to which an agent can influence the environment through its actions. It works specifically for stochastic systems (where state transitions are given in terms of probabilities), but it can also be applied to deterministic systems (which are only a specific case of stochastic systems). Empowerment is zero if, regardless of what the agent does, the result will be the same (i.e., the distribution of results for a particular successor state x \u2032 is independent of the action), and it is maximum if each action will have a unique result (i.e. the probability that a single result is generated by two different actions is zero). Let us now briefly discuss why the associated information-theoretical amount of reciprocal information, which has largely the same properties and would be easier to calculate, is not as powerful as the channel capacity to identify interesting states of the environment, which we do not consider as one."}, {"heading": "3.4 Computing empowerment when a model is available", "text": "Next, we describe the Blahut-Arimoto algorithm for calculating the channel capacity specified in Equation (4). For the time being, we assume that the (n-stage) transition probabilities p (x \u2032 | x, ~ a\u03bd) are known for all actions ~ a\u03bd, \u03bd = 1,..., Nn."}, {"heading": "3.4.1 Blahut-Arimoto algorithm", "text": "The Blahut-Arimoto algorithm (Blahut, 1972) is an EM-like algorithm that iterates over distributions pk (~ a), where k denotes the k-th iteration step to generate the distribution p \u0445 (~ a), which reaches the maximum in Equation (4). Since we are looking at a discrete action domain, pk (~ a) is defined by a vector pk (~ a) \u2261 (p1k,..., p Nn k). (5) To avoid confusing notation, we start with k: = \"Xp\" (x \u2032 | x, ~ a\u03bd) log [p (x \u2032 | x, ~ a\u03bd) \u2211 Nni = 1 p (x) pik] dx. \"We start with an initial distribution p0 (~ a), which is chosen using the even distribution p. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 n Problem in equation px.\""}, {"heading": "3.4.2 Monte-Carlo integration", "text": "Taking a closer look at Equation (5), we find that d\u03bd, k can also be written as an expectation with respect to the density p (x, x, ~ a\u03bd). Assuming that each density p (x, x, ~ a\u03bd) is of a simple form (e.g. parametric, as with a Gaussian or a mixture of Gaussians) from which we can easily take NMC samples {x, \u03bd, i}, we have: d\u03bd, k \u2248 1NMCNMC \u0445 j = 1log [p (x, x, j | x, ~ a\u03bd) \u0445 Nni = 1 p (x, x, ~ ai) p i k] (9) 10"}, {"heading": "3.4.3 Example: Gaussian model", "text": "As an example, we consider the case in which p (x \u2032 | x, ~ a\u03bd) is a multivariate Gaussian (or at least relatively approximated by it) with known mean-value-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-here-vector-vector-vector-here-vector-vector-vector-vector-here-vector-vector-vector-vector-vector-vector-that-here-vector-vector-vector-vector-here-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-that-here-x."}, {"heading": "4 Model learning", "text": "In this section, we further reduce our assumptions and consider an environment in which neither n-step nor 1-step transition probabilities are readily available. On the basis of the regression from these samples, we first conclude a 1-step transition model. On the basis of this 1-step transition model, we can then obtain a more general n-step transition model by iteratively predicting n steps ahead in time. In general, there would be many ways to achieve the task of regression. Here, we use Gaussian process regression (GP) (Rasmussen & Williams, 2006). GPs are simple and mathematically elegant but very powerful tools that provide some significant benefits. One is that GP directly generate a predictive distribution over the target values, which is exactly what is required in equation. (4) In addition, the predictable problem is a GPs selection that GPs represent directly in the GAP sample class."}, {"heading": "4.1 Learning 1-step system dynamics", "text": "To learn the state probabilities p (x), a) = (n), i.e. to predict the succession state x (x) when we perform 1-step action a (x), we combine several univariate GPs. Each individual GP\u03bdj, where j = 1. Prediction: the desired results that we take into account are the change of the state variables (i.e. we predict the difference xt + 1 \u2212 xt) Since both state variables and actions are treated separately, we need a set of D \u00b7 NA independent GPs. A detailed description of how uniform regression can be found with GPs work4 (Rasmussen & Williams, 2006). Training GP\u03bdj gives us a distribution p (x)."}, {"heading": "5 Experiments", "text": "We have previously indicated that empowerment has shown intuitively responsive identification of significant states in discrete scenarios, and we are now ready to study a number of complex continuous scenarios, which are used as a benchmark for typical learning algorithms (e.g. enhancement of learning or optimal control), but it should be noted that learning algorithms are encouraged to use the optimization criteria in the learning process, where we will always use empowerment as a criterion and show that the resulting behaviors actually closely correspond to the optimization of an external quality criterion. Observing that these optimization criteria correspond is a subtle point and will be discussed in the discussion."}, {"heading": "5.1 The domains", "text": "As test beds for our experiments, we consider simulations of the three physical systems described below. We repeat that, in the literature, systems like these are usually used in the context of control and learning behavior, where a goal (desired target states) is defined externally and, by optimizing such a specific performance criterion, the system is specifically geared toward that goal. In contrast, empowerment will intrinsically drive the system (near) to states that are typically selected externally as target states. However, with empowerment, we do not force that goal through any external reward, but through a generic intrinsic quantity generated for each domain, all tasks are generated in exactly the same way."}, {"heading": "5.2 First scenario: model-based", "text": "In our first series of experiments, we typically require an expanded view of the future as a single empowerment step. In all areas, we assume that the transition opportunities are known; the control loop is performed as follows: each time the agent successfully observes the current state XT, we determine the 1-step succession states under each of the possible 1-step actions; for each of these states, we calculate the empowerment value as described in Section 3.4.3, with NMC = 200, TOL = 10 \u2212 5, and MAX ITER = 150, and add Gaussian white noise with (state-independent) covariance to \"smear out\" the otherwise deterministic state transformations; the agent then executes the measures corresponding to the successor state with the highest empowerment value (empowerment-greedy choice of action), advancing the time and producing the next state XT + 1.Note that in practice, we typically require an increased horizon for empowerment values in the future."}, {"heading": "5.3 Second scenario: model learning and exploration", "text": "In the second experiment, we will discuss a scenario for empowerment that expands its potential applicability; here we are interested in model learning and the use of empowerment to \"intelligently\" extrapolate which part of the state space we should explore next. In particular, we will look at the case of online modeling, i.e. learning the state transition probabilities from the samples an agent experiences while interacting with the environment (which is more difficult because we generally cannot create transitions at any point in the state space and have to cope with the states that occur during a particular - and realistically attainable - run). The key idea here will be to show that with empowerment we can avoid testing the state space and instead learn the target behavior from very few system-agent interactions."}, {"heading": "5.3.1 Overview of the learning architecture", "text": "An overview of the learning architecture is shown in Figure 9. The agent consists of two components. However, one is the model learner Mt, which stores a history of all transitions Dt = {xi, ai, x \u2032 i} t i = 1 up to the current time t and which implements several GPs to provide 1-step predictions p (xt + 1 | xt, at, Mt) (Section 4.1) and n-step predictions p (xt + n | xt, Mt) (Section 4.2). The second component is the action selector. In view of the current state of the environment, we first determine the succession states using the possible 1-step actions using the meaning 8 of the predictions from Mt. For each successor state, we then determine its empowerment value (Section 3.4.3) using n-step predictions from Mt. Since the predicted succession states depend on the accuracy of the Mt, we adjust the results of the uncertainty associated with the step 1 prediction."}, {"heading": "5.3.2 Results", "text": "rf\u00fc rf\u00fc rf\u00fc rrf\u00fc eeglrrrrrrll\u00fceeer\u00fc rrf\u00fc ide rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu rfu"}, {"heading": "6 Discussion", "text": "In fact, most of them are able to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world, to move to another world."}, {"heading": "7 Summary", "text": "This paper discusses empowerment, an information theory parameter that measures the extent to which the agent can influence the environment through its actions for each agent environment system with stochastic transitions. While previous work with empowerment has already shown its various uses in a number of different domains, the empowerment calculation so far has been limited to the case of small and discrete domains where the agent assumed that the probabilities for the transition of states are known. The main contribution of this paper is to loosen both assumptions. First, this paper extends the calculation of empowerment to the case of continuous vector-evaluated state spaces. Second, we discuss an application of empowerment to exploration and online model learning where we no longer assume that the exact state transition probabilities are a priori known to the agent. Instead, the agent must learn them through interaction with the environment."}, {"heading": "Acknowledgments", "text": "This work was partly funded by the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory of the University of Texas at Austin, supported by grants from the National Science Foundation (IIS-0917122), ONR (N00014-09-1-0658), DARPA (FA8650-08-C-7812) and the Federal Highway Administration (DTFH61-07-H-00030), and partly supported by the European Commission under the FEELIX GROWING project (http: / / www.feelix-growing.org) under FP6 IST-045169."}, {"heading": "A Dynamic model of the inverted pendulum", "text": "Referring to the schematic representation of the reverse pendulum in Figure 4. The state variables are the angle measured from the vertical axis, \u03c6 (t) [rad], and the angular velocity \u03c6 (t) [rad / s]. The control variable is the applied torque u (t) [Nm], which is limited to the interval [\u2212 5, 5]. The movement of the pendulum is described by the differential equation:?? (t) = 1ml2 (\u2212 \u00b5\u03c6 (t) + mgl sin\u03c6 (t) + u (t))). (13) The angular velocity is limited by saturation to the interval? (\u2212 10, 10]. The values and meaning of the physical parameters are given in Table 1.The solution for the continuous time dynamic equation in Equation (13) is obscured by a Runge-Kutta solver."}, {"heading": "B Dynamic model of the acrobot", "text": "The state variables are the angle of the first connection (measured by the horizontal axis), the constant of the constant (measured by the horizontal axis), the constant of the constant (measured by the horizontal axis), and their constant of the constant of the constant (measured by the horizontal axis), and their constant of the constant of the constant (measured by the constant).The constant of the constant of the constant is the constant of the constant of the constant of the constant (measured by the constant of the horizontal axis).The constant of the constant of the constant of the constant is the constant of the constant of the (measured by the constant of the model) of the constant (the model of the constant) of the (the model of the dynamic)."}, {"heading": "C Dynamic model of the bicycle", "text": "Let us refer to the schematic representation of the bicycle area in Figure 4. The state variables are the rolling angle of the bicycle (measured from the vertical axis, \u03c9 (t) [rad], the rolling speed \u03c9 (t) [rad / s], the angle of the handlebar \u03b1 (t) [rad] [rad] [rad] [rad] [rad] (measured from the longitudinal axis of the bicycle), and its angular speed \u03b1 (t) [rad / s]. The control variables are the displacement (t) [m] of the cyclist's common center of mass perpendicular to the plane of the bicycle, and the torque speed \u03b1 (t) is applied to the handlebar. The dynamic model of the bicycle system is (Ernst et al., 2005): \u03c9 (t) = 1Ibc {sin (\u03b2 (t)))) (Mc) gh \u2212 cos (\u03b2)."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "This paper develops generalizations of empowerment to continuous states. Empowerment is a recently introduced information-theoretic quantity motivated by hypotheses about the efficiency of the sensorimotor loop in biological organisms, but also from considerations stemming from curiositydriven learning. Empowemerment measures, for agent-environment systems with stochastic transitions, how much influence an agent has on its environment, but only that influence that can be sensed by the agent sensors. It is an information-theoretic generalization of joint controllability (influence on environment) and observability (measurement by sensors) of the environment by the agent, both controllability and observability being usually defined in control theory as the dimensionality of the control/observation spaces. Earlier work has shown that empowerment has various interesting and relevant properties, e.g., it allows us to identify salient states using only the dynamics, and it can act as intrinsic reward without requiring an external reward. However, in this previous work empowerment was limited to the case of small-scale and discrete domains and furthermore state transition probabilities were assumed to be known. The goal of this paper is to extend empowerment to the significantly more important and relevant case of continuous vector-valued state spaces and initially unknown state transition probabilities. The continuous state space is addressed by Monte-Carlo approximation; the unknown transitions are addressed by model learning and prediction for which we apply Gaussian processes regression with iterated forecasting. In a number of well-known continuous control tasks we examine the dynamics induced by empowerment and include an application to exploration and online model learning.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}