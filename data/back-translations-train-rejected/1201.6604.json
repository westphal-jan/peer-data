{"id": "1201.6604", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2012", "title": "Gaussian Processes for Sample Efficient Reinforcement Learning with RMAX-like Exploration", "abstract": "We present an implementation of model-based online reinforcement learning (RL) for continuous domains with deterministic transitions that is specifically designed to achieve low sample complexity. To achieve low sample complexity, since the environment is unknown, an agent must intelligently balance exploration and exploitation, and must be able to rapidly generalize from observations. While in the past a number of related sample efficient RL algorithms have been proposed, to allow theoretical analysis, mainly model-learners with weak generalization capabilities were considered. Here, we separate function approximation in the model learner (which does require samples) from the interpolation in the planner (which does not require samples). For model-learning we apply Gaussian processes regression (GP) which is able to automatically adjust itself to the complexity of the problem (via Bayesian hyperparameter selection) and, in practice, often able to learn a highly accurate model from very little data. In addition, a GP provides a natural way to determine the uncertainty of its predictions, which allows us to implement the \"optimism in the face of uncertainty\" principle used to efficiently control exploration. Our method is evaluated on four common benchmark domains.", "histories": [["v1", "Tue, 31 Jan 2012 16:36:51 GMT  (307kb)", "http://arxiv.org/abs/1201.6604v1", "European Conference on Machine Learning (ECML'2010)"]], "COMMENTS": "European Conference on Machine Learning (ECML'2010)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["tobias jung", "peter stone"], "accepted": false, "id": "1201.6604"}, "pdf": {"name": "1201.6604.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["tjung@cs.utexas.edu", "pstone@cs.utexas.edu"], "sections": [{"heading": null, "text": "ar Xiv: 120 1.66 04v1 [cs.AI] 31 Yes"}, {"heading": "1 Introduction", "text": "In Reinforcement Learning (RL), an agent interacts with an environment and tries to select his actions in such a way that an externally defined measure of performance, the accumulated reward per step, is maximized over time. A characteristic feature of RL is that the environment is unknown and the agent must learn how to act directly from experience. In practical applications, such as robotics, this experience means that a physical system interacts in real time with the physical environment. Therefore, RL methods that are able to learn quickly and minimize the time it takes for the robot to interact with the environment until good or optimal behavior is learned are highly desirable. In this paper, we are interested in online RL for tasks with continuous states and fluid transition dynamics typical of robotic control areas. Our primary goal is to have an algorithm that keeps the complexity of samples as low as possible."}, {"heading": "1.1 Overview of the contribution", "text": "To maximize efficiency, we consider online RL, which is based in the spirit of RMAX [3], but based on continuous state spaces, as in RMAX and related methods, our algorithm consists of two parts: a model learner and a planner. The model learner estimates the dynamics of the environment from the sample transitions he experiences while interacting with the environment. The planner is used to take the best possible measures, while the predictions of the model learner become increasingly accurate."}, {"heading": "1.2 Assumptions and limitations", "text": "Our approach is based on the following assumptions (most of which are also made in related work, although they are not always explicitly mentioned): - Low dimensionality of state space. With a uniform grid, the number of grid points used to solve the Bellman equation can be scaled exponentially with dimensionality. Alternatively, non-linear functional approximation can be used; however, despite some encouraging results, it is unclear whether this approach would really work better in general applications. Today, breaking the curse of dimensionality is still an open research problem. While continuous action can be discredited, it is not feasible in practice for higher dimensional action spaces."}, {"heading": "2 Background: Planning when the model is exact", "text": "Consider the amplification problem for MDPs with continuous state space, finite action space, discounted reward criterion, and deterministic dynamics [19]. In this section, we assume that dynamics and rewards are available to the learning agent. Let's leave space X as a hyperrectangle in Rd (this assumption is justified, for example, if the system is a motor control task), A is the finite action space (assuming continuous controls are discredited), xt + 1 = f (xt, at) is the transition function (assuming that continuous time problems are discredited in time), and r (x, a) is the reward function. For the following theoretical reasoning function, we need both transitions and reward functions to be continued in actions; i.e., there are constants Lf, Lr that such systems exist (x, a)."}, {"heading": "3 Our algorithm: GP-RMAX", "text": "In the last section, we have seen how for a continuous state space, the optimal behavior of an agent can be achieved in a numerically robust manner, since the transition function xt + 1 = f (xt, at) is known. 2For model-based RL, we are now interested in solving the same problem in case the transition function is not known. Instead, the agent must interact with the environment and use only the samples he observes to calculate optimal behavior. Our goal in this essay is to develop a learning framework.2 Remember our working hypothesis: Reward as a performance criterion is given externally and does not need to be appreciated by the agent. Also, note that discretization (even with more advanced methods such as adaptive or sparse grids) is likely to be feasible only in states with low to medium dimensionality. Breaking the curse of dimensionality is an open research problem, where this number is kept as small as possible by using this estimation (x to pass a sample in advance)."}, {"heading": "3.1 Overview", "text": "A sketch of our architecture is shown in Figure 2. GP-RMAX consists of the two parts of the learning and planning model, which are interwoven for online learning. As the model learner appreciates the dynamics of the environment based on the transitions that the agent experiences while interacting with the environment, the planner is used to find the best possible action given the current model. As the predictions of the model learner become increasingly accurate, the actions derived from the planner are getting closer to the optimum. In the following, a top-level overview of the algorithm: - Input: \u2022 Reward function r (x, a) \u2022 Discount factor: \u2022 Power parameters: Planning and model update frequency K model: Accuracy M1; M 2 (Interruption criterion model learning): Discretion of the planner N - Initialization: \u2022 Model M1, Q function Q1, observed transitions D1 - loop: Accuracy M2 (Interruption criterion model learning): not determined by Qterization on Qterization (Qterization on Q1 = 1)."}, {"heading": "3.2 Model learning with GPs", "text": "While in theory all non-linear regression algorithms could serve this purpose, we believe that GPs are particularly well suited: (1) because they provide non-parametric means that provide great flexibility in modelling; (2), the setting of hyperparameters can be done automatically (and in principle) by optimizing marginal probability and enabling the automatic determination of relevant inputs; and (3) GPs provide a natural way to determine the uncertainty of their predictions that are used to guide exploration. In addition, uncertainty in GPs is monitored by being dependent on the target function that is estimated (2); other methods only consider the density of the data (not monitored) and will tend to exaggerate whether the target function is simple. Assume we have observed a number of transitions that are considered triplets of the actions undertaken and resulting successor states, e.g. D = xt, at} that the target function is simple."}, {"heading": "3.3 Planning with a model", "text": "For each state x and plot a, the model Mt = 4 can be evaluated to solve the transition f (x, a) together with the normalized scalar uncertainty c (x, a), where 0 means maximum safe and maximum unsafe (see section 3.2). Let's solve the discretization of state space X with nodes i, i = 1,... We now solve the planning phase by inserting f into the procedure described in section 2. First, let's calculate z-ai = f, a), c (i, a) of (6) and the associated interpolation coefficient waij of (3) for each node i and plot. Let Ca denotet the N-1 vector according to the uncertainties, [Ca] i = c."}, {"heading": "4 Experiments", "text": "We are now examining the online learning performance of GP-RMAX in various well-known RL benchmark areas."}, {"heading": "4.1 Description of domains", "text": "Specifically, we choose the following exceptions (with a large number of comparative results available in the literature): Mountain car: In Mountain car, the goal is to drive an underpowered car from the bottom of the valley to the top of a hill; the car is not strong enough to climb the hill directly, instead it must build the necessary dynamics by reversing the gas and going up the hill on the opposite side first. The problem is two-dimensional, the state variable x1 [\u2212 1,2, 0.5] describes the position of the car, x2 [\u2212 0.07, 0.07] its necessary dynamics. Possible actions are a two-dimensional, 0, + 1}. Learning is episodic: each step gives a reward from \u2212 1 to the top of the hill at x1, 0.5 is achieved. Our experimental setup (dynamics and domain-specific constants) is the same as in [19], with the following exceptions: Contraction factor = 0.99 and each episode begins with the agent of the city."}, {"heading": "4.2 Results", "text": "10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-10-"}, {"heading": "5 Summary", "text": "Our algorithm separates the approximation of problem functions in the model learner from the approximation / interpolation of problem functions in the planner. If the transition function is easier to learn, i.e. requires only a few examples relative to the representation of the optimal value function, large savings in sample complexity can be achieved. Related model-free methods, such as adaptive Q-iteration, cannot take advantage of this situation. The fundamental limitation of our approach is that it relies on solving the Bellman equation globally across the state space. Even with more advanced discretization methods, such as adaptive grids or sparse grids, the curse of dimensionality limits its applicability to problems with low or moderate dimensionality. Other, more minor limitations affect the simplistic assumptions we have made: deterrent methods or sparse grids, but these reward functions are not limited to simply present."}, {"heading": "Acknowledgments", "text": "This work took place in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory of the University of Texas at Austin. LARG research is supported in part by grants from the National Science Foundation (IIS-0917122), ONR (N00014-09-1-0658), DARPA (FA8650-08-C-7812) and the Federal Highway Administration (DTFH61-07-H-00030)."}], "references": [{"title": "Adaptive-resolution reinforcement learning with efficient exploration", "author": ["A. Bernstein", "N. Shimkin"], "venue": "Machine Learning (published online: 5 May 2010). DOI:10.1007/s10994-010-5186-7,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Minimum-time control of the acrobot", "author": ["G. Boone"], "venue": "Proc. of IEEE International Conference on Robotics and Automation, 4:3281\u20133287,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "R-MAX, a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R. Brafman", "M. Tennenholtz"], "venue": "JMLR, 3:213\u2013231,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Online least-squares policy iteration for reinforcement learning control", "author": ["L. Busoniu", "D. Ernst", "B. De Schutter", "R. Babuska"], "venue": "In American Control Conference (ACC-10),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Multidimensional triangulation and interpolation for reinforcement learning", "author": ["S. Davies"], "venue": "In NIPS 9. Morgan,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Gaussian process dynamic programming", "author": ["M.P. Deisenroth", "C.E. Rasmussen", "J. Peters"], "venue": "Neurocomputing, 72(7-9):1508\u20131524,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Bayes meets Bellman: The Gaussian process approach to temporal difference learning", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "In Proc. of ICML 20, pages 154\u2013161,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Tree-based batch mode reinforcement learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "JMLR, 6:503\u2013556,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "An adaptive grid scheme for the discrete Hamilton-Jacobi-Bellman equation", "author": ["L. Gr\u00fcne"], "venue": "Numerische Mathematik, 75:319\u2013337,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Model-based exploration in continuous state spaces", "author": ["N.K. Jong", "P. Stone"], "venue": "In The 7th Symposium on Abstraction, Reformulation and Approximation,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning robocup-keepaway with kernels", "author": ["T. Jung", "D. Polani"], "venue": "JMLR: Workshop and Conference Proceedings (Gaussian Processes in Practice), 1:33\u201357,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "JMLR, 4:1107\u20131149,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Online exploration in least-squares policy iteration", "author": ["L. Li", "M.L. Littman", "C.R. Mansley"], "venue": "In Proc. of 8th AAMAS,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Variable resolution discretization in optimal control", "author": ["R. Munos", "A. Moore"], "venue": "Machine Learning, 49:291\u2013323,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-resolution exploration in continuous spaces", "author": ["A. Nouri", "M.L. Littman"], "venue": "In NIPS 21,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximation methods for gaussian process regression", "author": ["J. Qui\u00f1onero-Candela", "C.E. Rasmussen", "C.K.I. Williams"], "venue": "In Leon Bottou, Olivier Chapelle, Dennis DeCoste, and Jason Weston, editors, Large Scale Learning Machines, pages 203\u2013 223. MIT Press,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "MIT Press,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural fitted q-iteration", "author": ["M. Riedmiller"], "venue": "In Proc. of 16th ECML,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 2, "context": "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].", "startOffset": 151, "endOffset": 159}, {"referenceID": 9, "context": "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].", "startOffset": 151, "endOffset": 159}, {"referenceID": 4, "context": "To maximize sample efficiency, we consider online RL that is model-based in the spirit of RMAX [3], but extended to continuous state spaces similar to [1,10,5].", "startOffset": 151, "endOffset": 159}, {"referenceID": 17, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 57, "endOffset": 66}, {"referenceID": 7, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 57, "endOffset": 66}, {"referenceID": 14, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 57, "endOffset": 66}, {"referenceID": 11, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 108, "endOffset": 120}, {"referenceID": 3, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 108, "endOffset": 120}, {"referenceID": 12, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 108, "endOffset": 120}, {"referenceID": 10, "context": "Competing model-free methods, such as fitted Q-iteration [18,8,15] or policy iteration based LSPI/LSTD/LSPE [12,4,13,11], do not have this advantage, as they need the actual sample transitions to estimate and represent the value function.", "startOffset": 108, "endOffset": 120}, {"referenceID": 9, "context": "Conceptually, our approach is closely related to Fitted R-MAX, which was proposed in [10] and uses an instance-based approach in the model-learner, and related work in [5,1], which uses grid-based interpolation in the model-learner.", "startOffset": 85, "endOffset": 89}, {"referenceID": 4, "context": "Conceptually, our approach is closely related to Fitted R-MAX, which was proposed in [10] and uses an instance-based approach in the model-learner, and related work in [5,1], which uses grid-based interpolation in the model-learner.", "startOffset": 168, "endOffset": 173}, {"referenceID": 0, "context": "Conceptually, our approach is closely related to Fitted R-MAX, which was proposed in [10] and uses an instance-based approach in the model-learner, and related work in [5,1], which uses grid-based interpolation in the model-learner.", "startOffset": 168, "endOffset": 173}, {"referenceID": 0, "context": "For example, unlike the recent ARL [1], for which PAC-style performance bounds could be derived (because of its grid-based implementation of model-learning), a GP is much better able to handle generalization and as a consequence can achieve much lower sample complexity.", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": ", [9,14], for our purpose here, a uniform grid is sufficient as proof of concept.", "startOffset": 2, "endOffset": 8}, {"referenceID": 13, "context": ", [9,14], for our purpose here, a uniform grid is sufficient as proof of concept.", "startOffset": 2, "endOffset": 8}, {"referenceID": 18, "context": "Consider the reinforcement learning problem for MDPs with continuous state space, finite action space, discounted reward criterion and deterministic dynamics [19].", "startOffset": 158, "endOffset": 162}, {"referenceID": 8, "context": ", see [9,14] for details).", "startOffset": 6, "endOffset": 12}, {"referenceID": 13, "context": ", see [9,14] for details).", "startOffset": 6, "endOffset": 12}, {"referenceID": 9, "context": "Instead of trying to estimate f directly (which corresponds to absolute transitions), we try to estimate the relative change xt+1\u2212xt as in [10].", "startOffset": 139, "endOffset": 143}, {"referenceID": 16, "context": "The details of working with GPs can be found in [17]; using GPs to learn a model for RL was previously also studied in [6] (for offline RL and without uncertainty-guided exploration).", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "The details of working with GPs can be found in [17]; using GPs to learn a model for RL was previously also studied in [6] (for offline RL and without uncertainty-guided exploration).", "startOffset": 119, "endOffset": 122}, {"referenceID": 15, "context": "For the lack of space we can only sketch our particular implementation, see [16] for more detailed information.", "startOffset": 76, "endOffset": 80}, {"referenceID": 16, "context": "Once trained, for any testpoint x, GP ij provides a distribution over target values, N (\u03bcij(x), \u03c3 ij(x)), with mean \u03bcij(x) and variance \u03c3 2 ij(x) (exact formulas for \u03bc and \u03c3 can be found in [17]).", "startOffset": 190, "endOffset": 194}, {"referenceID": 0, "context": "For any state x and action a, model Mt can be evaluated to \u201cproduce\u201d the transition f\u0303(x, a) along with normalized scalar uncertainty c(x, a) \u2208 [0, 1], where 0 means maximally certain and 1 maximally uncertain (see Section 3.", "startOffset": 144, "endOffset": 150}, {"referenceID": 8, "context": "To reduce the number of iterations necessary, we adapt Gr\u00fcne\u2019s increasing coordinate algorithm [9] to the case of Q-functions: instead of Eq.", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "In [9] it was proved that Eq.", "startOffset": 3, "endOffset": 6}, {"referenceID": 14, "context": "To implement the \u201coptimism in the face of uncertainty\u201d principle, that is, to make the agent explore regions of the state space where the model predictions are uncertain, we employ the heuristic modification of the Bellman operator which was suggested in [15] and shown to perform well.", "startOffset": 255, "endOffset": 259}, {"referenceID": 18, "context": "Our experimental setup (dynamics and domain specific constants) is the same as in [19], with the following exceptions: maximal episode length is 500 steps, discount factor \u03b3 = 0.", "startOffset": 82, "endOffset": 86}, {"referenceID": 5, "context": "The remaining experimental setup (equations of motion and domain specific constants) is the same as in [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 7, "context": "Bicycle: Next we consider the problem of balancing a bicycle that rides at a constant speed [8],[12].", "startOffset": 92, "endOffset": 95}, {"referenceID": 11, "context": "Bicycle: Next we consider the problem of balancing a bicycle that rides at a constant speed [8],[12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 7, "context": "Our experimental setup so far is similar to [8].", "startOffset": 44, "endOffset": 47}, {"referenceID": 18, "context": "Acrobot: Our final problem is the acrobot swing-up task [19].", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "Our experimental setup and implementation of state transition dynamics is similar to [19].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "The discount factor was set to \u03b3 = 1, as in [19].", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": "4 Note that 64 steps is not the optimal solution, [2] demonstrated swing-up with 61 steps.", "startOffset": 50, "endOffset": 53}, {"referenceID": 18, "context": "we repeat the experiments using the standard online model-free RL algorithm Sarsa(\u03bb) with tile coding [19], where we consider two different setup of the tilings (one finer and one coarser).", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 98, "endOffset": 107}, {"referenceID": 7, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 98, "endOffset": 107}, {"referenceID": 14, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 98, "endOffset": 107}, {"referenceID": 11, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 147, "endOffset": 159}, {"referenceID": 3, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 147, "endOffset": 159}, {"referenceID": 12, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 147, "endOffset": 159}, {"referenceID": 10, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 147, "endOffset": 159}, {"referenceID": 6, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 190, "endOffset": 195}, {"referenceID": 5, "context": "While direct comparison with other high performance RL algorithms, such as fitted value iteration [18,8,15], policy iteration based LSPI/LSTD/LSPE [12,4,13,11], or other kernelbased methods [7,6] is difficult, because they are either batch methods or handle exploration in a more ad-hoc way, from the respective results given in the literature it is clear that for the domains we examined GP-RMAX performs relatively well.", "startOffset": 190, "endOffset": 195}, {"referenceID": 18, "context": "This in turn allows the optimal value function to be learned from very few sample transitions: panel (b) shows that after only 120 transitions (still in the middle of the very first episode) the approximated value function already resembles the true one [19].", "startOffset": 254, "endOffset": 258}], "year": 2012, "abstractText": "We present an implementation of model-based online reinforcement learning (RL) for continuous domains with deterministic transitions that is specifically designed to achieve low sample complexity. To achieve low sample complexity, since the environment is unknown, an agent must intelligently balance exploration and exploitation, and must be able to rapidly generalize from observations. While in the past a number of related sample efficient RL algorithms have been proposed, to allow theoretical analysis, mainly model-learners with weak generalization capabilities were considered. Here, we separate function approximation in the model learner (which does require samples) from the interpolation in the planner (which does not require samples). For model-learning we apply Gaussian processes regression (GP) which is able to automatically adjust itself to the complexity of the problem (via Bayesian hyperparameter selection) and, in practice, often able to learn a highly accurate model from very little data. In addition, a GP provides a natural way to determine the uncertainty of its predictions, which allows us to implement the \u201coptimism in the face of uncertainty\u201d principle used to efficiently control exploration. Our method is evaluated on four common benchmark domains.", "creator": "LaTeX with hyperref package"}}}