{"id": "1201.6615", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2012", "title": "Feature Selection for Value Function Approximation Using Bayesian Model Selection", "abstract": "Feature selection in reinforcement learning (RL), i.e. choosing basis functions such that useful approximations of the unkown value function can be obtained, is one of the main challenges in scaling RL to real-world applications. Here we consider the Gaussian process based framework GPTD for approximate policy evaluation, and propose feature selection through marginal likelihood optimization of the associated hyperparameters. Our approach has two appealing benefits: (1) given just sample transitions, we can solve the policy evaluation problem fully automatically (without looking at the learning task, and, in theory, independent of the dimensionality of the state space), and (2) model selection allows us to consider more sophisticated kernels, which in turn enable us to identify relevant subspaces and eliminate irrelevant state variables such that we can achieve substantial computational savings and improved prediction performance.", "histories": [["v1", "Tue, 31 Jan 2012 16:57:55 GMT  (397kb)", "http://arxiv.org/abs/1201.6615v1", "European Conference on Machine Learning (ECML'09)"]], "COMMENTS": "European Conference on Machine Learning (ECML'09)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["tobias jung", "peter stone"], "accepted": false, "id": "1201.6615"}, "pdf": {"name": "1201.6615.pdf", "metadata": {"source": "CRF", "title": "Feature Selection for Value Function Approximation Using Bayesian Model Selection", "authors": ["Tobias Jung"], "emails": ["tjung@cs.utexas.edu", "pstone@cs.utexas.edu"], "sections": [{"heading": null, "text": "ar Xiv: 120 1.66 15v1 [cs.AI] 31 Jan 20"}, {"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules that they need for their work, and they will be able to play by the rules that they need for their work."}, {"heading": "2 Related work", "text": "The overarching goal of learning representations and selecting characteristics for linearly parameterized V functions is not new in the context of RL. Broadly speaking, past methods can be categorized along two dimensions: how the basic functions are represented (e.g., either by parameterized and predefined basic functions such as RBF or by non-parameterized basic functions derived directly from the data) and which quantity / target function is considered as a guideline for their construction process (e.g., either by supervised methods that take into account the Bellman error and depend on the respective reward / objective, or by unsupervised graph-based methods that take into account the connectivity properties of the state space). Conceptually closely related to our work is the approach described in [12], which adapts the hyperparameters of RBF basic functions (both their location and longitudinal scales) by choosing either a gradient-based error or the ascending method for the ascending of the problem or the descent of the number of the gradient."}, {"heading": "3 Background: GPs for policy evaluation", "text": "In this section, we briefly summarize how GPs [17] can be used for approximate policy evaluation (1). Here, we follow the GPTD formulation of [6]. Suppose we have observed the order of states x1, x2,.., xn and rewards r1,., rn \u2212 1, where xi \u0445 p (\u00b7 xi \u2212 1, \u03c3n) and ri = R (xi, xi), xi + 1). In practice, MDPs considered in RL will often be of an episodic character with absorbing finite states. Therefore, we must transform the problem in such a way that the resulting Markov chain is still ergotic by making a zero-reward transition from the final state of an episode to the initial state of the next episode. In addition to the order of states and rewards of our training data, we include a sequence."}, {"heading": "4 Computational considerations", "text": "In terms of its implementation, the GPTD has the same weakness for policy assessment as GPs have for traditional machine learning tasks: solving Equation (8) requires inversion3 of a dense matrix (n \u2212 1) \u00b7 (n \u2212 1), which, if performed accurately, would require O (n3) operations and is therefore unfeasible for anything but minor problems (say, anything with n < 5000)."}, {"heading": "4.1 Subset of regressors", "text": "In the subset of regressors (SR) approach first proposed for regulatory networks [15,10], one selects a subset (x) mi = 1 of the data, with m \u00b2 n, and approaches the covariance for any x, x \u00b2 by savings (x, x \u00b2) = km (x) TC \u2212 1mmkm (x \u00b2). (11) Here km (\u00b7) means km \u2212 \u2212 K for any x, \u00b7),..., k (x \u00b2 m, \u00b7) T, andKmm is the submatrix [Kmm] ij = k (x \u00b2) of K. The approximation in Eq (11) can be motivated, for example, by the Nystro \u00b2 m approximation [22]. Leave Knm the submatrix [Knm] ij = k (xi, x \u00b2 j), which corresponds to the m columns of the data in the subset."}, {"heading": "4.2 Selecting the subset (unsupervised)", "text": "Selecting the best subset is a combinatorial problem that cannot be effectively solved. Instead, we try to find a compact subset that summarizes the relevant information by incremental forward selection. In each step of the process, we add this element from the set of remaining, unselected elements to the active set that performs best in terms of a particular criterion. In general, we distinguish between supervised and unsupervised approaches, i.e. those that look at the target variable that we fall back on and those that do not. Here, we focus on incomplete cholesky decomposition (ICD) as an uncontrolled approach [7,1,2]. ICD aims to reduce in each step the error that results from approaching the covariance matrix: In each step, we add this element to the active K-K distance, the distance from the currently selected elements being the largest (if the remaining number is the smallest)."}, {"heading": "5 Model selection for GPTD", "text": "The great advantage of a GP-based functional approach (as opposed to neural networks or tree-based approaches, for example) is that both the \"learning\" of the weight vector and the specification of the architecture / hyperparameter / basic functions can be handled in principle and essentially automatically."}, {"heading": "5.1 Optimizing the marginal likelihood", "text": "To determine hyperparameters for GPTD, we consider the marginal probability of the process, i.e. the probability of generating the rewards we observed in view of the sequence of states and a certain setting of the hyperparameter \u03b8. We then maximize this function (its logarithm) in relation to \u03b8. From Equation. (6) we see that for GPTD we have p (r | D, \u03b8) = N (0, Q). Thus, plugging into the definition for a multivariate gauss and taking the logarithm, we obtainL (\u03b8) = \u2212 12 log detQ \u2212 1 2 rTQ \u2212 n 2 log 2\u03c0. (16) Optimizing this function in relation to a non-convex problem and we have recourse to iterative gradient-based solvers (such as the scaled conjugate gradient values, e.g. see [13]."}, {"heading": "5.2 Choosing the covariance", "text": "A common choice for k (\u00b7, \u00b7) is to consider a (positive definite) function parameterized by a small number of scalar parameters, such as the stationary isotropic Gaussian (or square exponential factors) parameterized by the length scale (bandwidth h). In the following, we will consider three variants of the form: k (x, x \u00b2) = v0 exp (x, x \u00b2) Ti (x, \u00b2)} where the hyperparameters v0 > 0 denote the vertical length scale, b > 0 denote bias, and the symmetric positive matrix."}, {"heading": "5.3 Example: gradient for ARD", "text": "As an example, we will now show how the course of Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q ="}, {"heading": "6 Experiments", "text": "This shows that our proposed model selection can be used to solve the approximate political evaluation of the problem in a fully automated manner = 51. So it is only a matter of time before we can use the hyperparameter functions (see Section 5.2), which depend on the alarming number of hyperparameters, so that the regularities of a particular dataset fit better and thus do not waste unnecessary resources on irrelevant aspects of the state vector, the latter aspect being particularly interesting for computational purposes (see Section 4) and becoming important in large areas of application. 6.1 Pendulum Swing-up taskFirst, we consider the Pendulum Swing-up task, a common benchmark in RL, with the goal of creating an underpowered pendulum and a balance that understands it as an episodic task."}, {"heading": "6.2 A 2D gridworld with 1 latent dimension", "text": "To better illustrate how our approach deals with irrelevant state variables, we use a specially designed 2D grid world with 11 \u00d7 11 states. Each step brings a reward of \u2212 1, except when we are in a state of x = 6 that starts a new episode (teleports to a new random state with zero reward). We look at the politics that move to the left when x > 6 and to the right when x < 6. In addition, each time we move to the left or right, we will also randomly move up or down (each with 50%). The corresponding value function is shown in Figure 5 (left). We have 500 transitions and apply GPTD with covariance (I) and (II) with automatic model selection that results in 9hyperparameters that the data L fit (smaller is better) (I) h = 2.89 -2378.2 54.78 -2323.4 (II) 1 = 3.a53 = 2 = 3.a8 = 278.a53 \u2212 II."}, {"heading": "7 Future work", "text": "It should be noted that the proposed framework for automatic feature generation and model selection should be considered primarily as a practical tool: although it provides a principled solution to an important problem in RL, it ultimately does not provide theoretical guarantees (due to some modeling assumptions of GPTD and the way the hyperparameters are obtained).For most practical applications, this may be less of a problem, but in general it needs to be taken into account.The framework can be easily expanded to perform a policy assessment on the Common State Action Space in order to learn the model-free Q function (instead of the V function): We just need to choose another covariance function, for example the product k ([x, a], [x \u2032, a \u2032, a \u2032) = k (a, a \u2032) with k (a, a \u2032) with k (a, a \u2032) with k (a \u2032) = \u03b4a, a \u2032 for problems with a small number of discrete measures [8], this opens the way for modelling-free policy improvements (a \u2032) with a (a)."}, {"heading": "Acknowledgments", "text": "This work took place in the Learning Agents Research Group (LARG) at the Artificial Intelligence Laboratory of the University of Texas at Austin. LARG research is supported in part by grants from NSF (CNS-0615104), DARPA (FA8750-05-2-0283 and FA8650-08-C-7812), the Federal Highway Administration (DTFH61-07-H-00030) and General Motors."}], "references": [{"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "JMLR, 3:1\u201348,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Predictive low-rank decomposition for kernel methods", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "In Proc. of ICML 22,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Dynamic programming and Optimal Control, Vol", "author": ["D. Bertsekas"], "venue": "II. Athena Scientific,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse online Gaussian processes", "author": ["L. Csat\u00f3", "M. Opper"], "venue": "Neural Computation, 14(3):641\u2013668,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Gaussian process dynamic programming", "author": ["M.P. Deisenroth", "C.E. Rasmussen", "J. Peters"], "venue": "Neurocomputing, 72(7-9):1508\u20131524,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Reinforcement learning with Gaussian processes", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "In Proc. of ICML 22,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient SVM training using low-rank kernel representation", "author": ["S. Fine", "K. Scheinberg"], "venue": "JMLR, 2:243\u2013264,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning robocup-keepaway with kernels", "author": ["T. Jung", "D. Polani"], "venue": "JMLR: Workshop and Conference Proceedings (Gaussian Processes in Practice), 1:33\u201357,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic basis function construction for approximate dynamic programming and reinforcement learning", "author": ["P. Keller", "S. Mannor", "D. Precup"], "venue": "In Proc. of ICML 23,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Hybrid adaptive splines", "author": ["Z. Luo", "G. Wahba"], "venue": "J. Amer. Statist. Assoc., 92:107\u2013 116,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Proto-value functions: A Laplacian framework for learning representation and control in Markov decision processes", "author": ["S. Mahadevan", "M. Maggioni"], "venue": "JMLR, 8:2169\u2013 2231,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Basis function adaptation in temporal difference reinforcement learning", "author": ["N. Menache", "N. Shimkin", "S. Mannor"], "venue": "Annals of Operations Research, 134:215\u2013238,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2005}, {"title": "Netlab : Algorithms for Pattern Recognition", "author": ["I.T. Nabney"], "venue": "Springer,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Analyzing feature generation for value-function approximation", "author": ["R. Parr", "C. Painter-Wakefield", "L. Li", "M. Littman"], "venue": "In Proc. of ICML 24,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Networks for approximation and learning", "author": ["T. Poggio", "F. Girosi"], "venue": "Proceedings of the IEEE, 78(9):1481\u20131497,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1990}, {"title": "Gaussian processes in reinforcement learning", "author": ["C.E. Rasmussen", "M. Kuss"], "venue": "In Advances in Neural Information Processing Systems 16, pages 751\u2013759. MIT Press,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Gaussian Processes for Machine Learning", "author": ["C.E. Rasmussen", "C.K.I. Williams"], "venue": "MIT Press,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Online kernel selection for Bayesian reinforcement learning", "author": ["J. Reisinger", "P. Stone", "R. Miikkulainen"], "venue": "In Proc. of ICML 25,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast forward selection to speed up sparse Gaussian process regression", "author": ["M. Seeger", "C.K.I. Williams", "N. Lawrence"], "venue": "In Proc. of 9th Int\u2019l Workshhop on AI and Statistics. Soc. for AI and Statistics,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Sparse Gaussian processes using pseudo-inputs", "author": ["E. Snelson", "Z. Ghahramani"], "venue": "In NIPS 18,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "In NIPS 13, pages 682\u2013688,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 20, "context": "see [21,3].", "startOffset": 4, "endOffset": 10}, {"referenceID": 2, "context": "see [21,3].", "startOffset": 4, "endOffset": 10}, {"referenceID": 5, "context": "Recent work in applying nonparametric function approximation to RL, such as Gaussian processes (GP) [6,16,18,5], or equivalently, regularization networks [8], is a very promising step in this direction.", "startOffset": 100, "endOffset": 111}, {"referenceID": 15, "context": "Recent work in applying nonparametric function approximation to RL, such as Gaussian processes (GP) [6,16,18,5], or equivalently, regularization networks [8], is a very promising step in this direction.", "startOffset": 100, "endOffset": 111}, {"referenceID": 17, "context": "Recent work in applying nonparametric function approximation to RL, such as Gaussian processes (GP) [6,16,18,5], or equivalently, regularization networks [8], is a very promising step in this direction.", "startOffset": 100, "endOffset": 111}, {"referenceID": 4, "context": "Recent work in applying nonparametric function approximation to RL, such as Gaussian processes (GP) [6,16,18,5], or equivalently, regularization networks [8], is a very promising step in this direction.", "startOffset": 100, "endOffset": 111}, {"referenceID": 7, "context": "Recent work in applying nonparametric function approximation to RL, such as Gaussian processes (GP) [6,16,18,5], or equivalently, regularization networks [8], is a very promising step in this direction.", "startOffset": 154, "endOffset": 157}, {"referenceID": 5, "context": "Here, we will focus on the Bayesian setting, and adapt marginal likelihood optimization for the GP-based approximate policy evaluation method GPTD, introduced without model selection in [6].", "startOffset": 186, "endOffset": 189}, {"referenceID": 17, "context": "Despite its many promises, previous work with GPs in RL rarely explores the benefits of model selection: in [18], a variant of stochastic search was used to determine hyperparameters of the covariance for GPTD using as score function the online performance of an agent.", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "In [16], standard GPs with marginal likelihood based model selection were employed; however, since their approach was based on fitted value iteration, the task of value function approximation was reduced to ordinary regression.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "Conceptually closely related to our work is the approach described in [12], which adapts the hyperparameters of RBFbasis functions (both their location and lengthscales) using either gradient descent or the cross-entropy method on the Bellman error.", "startOffset": 70, "endOffset": 74}, {"referenceID": 8, "context": "Other alternative approaches do not rely on predefined basis functions: The method in [9] is an incremental approach that uses dimensionality reduction and state aggregation to create new basis functions such that for every step the remaining Bellman error for a trajectory of states is successively reduced.", "startOffset": 86, "endOffset": 89}, {"referenceID": 13, "context": "A related approach is given in [14] which incrementally constructs an orthogonal basis for the Bellman error.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "A graph-based unsupervised approach is presented in [11], which derives basis functions from the eigenvectors of the graph Laplacian induced from the underlying MDP.", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "In this section we briefly summarize how GPs [17] can be used for approximate policy evaluation; here we will follow the GPTD formulation of [6].", "startOffset": 45, "endOffset": 49}, {"referenceID": 5, "context": "In this section we briefly summarize how GPs [17] can be used for approximate policy evaluation; here we will follow the GPTD formulation of [6].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "If the environment is stochastic, the noise model \u03a3 = \u03c3 0HH T is more appropriate, see [6] for more detailed explanations.", "startOffset": 87, "endOffset": 90}, {"referenceID": 5, "context": "See [6] for details.", "startOffset": 4, "endOffset": 7}, {"referenceID": 14, "context": "1 Subset of regressors In the subset of regressors (SR) approach initially proposed for regularization networks [15,10], one chooses a subset {x\u0303}i=1 of the data, with m \u226a n, and approximates the covariance for arbitrary x,x by taking k\u0303(x,x) = km(x) K mmkm(x ).", "startOffset": 112, "endOffset": 119}, {"referenceID": 9, "context": "1 Subset of regressors In the subset of regressors (SR) approach initially proposed for regularization networks [15,10], one chooses a subset {x\u0303}i=1 of the data, with m \u226a n, and approximates the covariance for arbitrary x,x by taking k\u0303(x,x) = km(x) K mmkm(x ).", "startOffset": 112, "endOffset": 119}, {"referenceID": 21, "context": "(11) can be motivated for example from the Nystr\u00f6m approximation [22].", "startOffset": 65, "endOffset": 69}, {"referenceID": 3, "context": "The situation can be remedied by considering the projected process approximation [4,19], which results in the same expression for the mean in Eq.", "startOffset": 81, "endOffset": 87}, {"referenceID": 18, "context": "The situation can be remedied by considering the projected process approximation [4,19], which results in the same expression for the mean in Eq.", "startOffset": 81, "endOffset": 87}, {"referenceID": 6, "context": "Here we focus on the incomplete Cholesky decomposition (ICD) as an unsupervised approach [7,1,2].", "startOffset": 89, "endOffset": 96}, {"referenceID": 0, "context": "Here we focus on the incomplete Cholesky decomposition (ICD) as an unsupervised approach [7,1,2].", "startOffset": 89, "endOffset": 96}, {"referenceID": 1, "context": "Here we focus on the incomplete Cholesky decomposition (ICD) as an unsupervised approach [7,1,2].", "startOffset": 89, "endOffset": 96}, {"referenceID": 3, "context": "In [4,8,6] online variants thereof are considered (where instead of repeatedly inspecting all remaining elements only one pass over the dataset is made and every element is examined only once).", "startOffset": 3, "endOffset": 10}, {"referenceID": 7, "context": "In [4,8,6] online variants thereof are considered (where instead of repeatedly inspecting all remaining elements only one pass over the dataset is made and every element is examined only once).", "startOffset": 3, "endOffset": 10}, {"referenceID": 5, "context": "In [4,8,6] online variants thereof are considered (where instead of repeatedly inspecting all remaining elements only one pass over the dataset is made and every element is examined only once).", "startOffset": 3, "endOffset": 10}, {"referenceID": 12, "context": "see [13]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 12, "context": "In the following we will consider three variants of the form [13,17]:", "startOffset": 61, "endOffset": 68}, {"referenceID": 16, "context": "In the following we will consider three variants of the form [13,17]:", "startOffset": 61, "endOffset": 68}, {"referenceID": 4, "context": "[5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "see [20].", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "The framework can be easily extended to perform policy evaluation over the joint state-action space to learn the model-free Q-function (instead of the V-function): we just have to choose a different covariance function, taking for example the product k([x, a], [x, a]) = k(x,x)k(a, a) with k(a, a) = \u03b4a,a\u2032 for problems with a small number of discrete actions [8].", "startOffset": 359, "endOffset": 362}, {"referenceID": 5, "context": "Our next step then is to apply this approach to real-world highdimensional control tasks, both in batch settings and hybrid batch/online settings; in the latter case exploiting the gain in computational efficiency obtained through model selection to improve [6].", "startOffset": 258, "endOffset": 261}], "year": 2012, "abstractText": "Feature selection in reinforcement learning (RL), i.e. choosing basis functions such that useful approximations of the unkown value function can be obtained, is one of the main challenges in scaling RL to real-world applications. Here we consider the Gaussian process based framework GPTD for approximate policy evaluation, and propose feature selection through marginal likelihood optimization of the associated hyperparameters. Our approach has two appealing benefits: (1) given just sample transitions, we can solve the policy evaluation problem fully automatically (without looking at the learning task, and, in theory, independent of the dimensionality of the state space), and (2) model selection allows us to consider more sophisticated kernels, which in turn enable us to identify relevant subspaces and eliminate irrelevant state variables such that we can achieve substantial computational savings and improved prediction performance.", "creator": "LaTeX with hyperref package"}}}