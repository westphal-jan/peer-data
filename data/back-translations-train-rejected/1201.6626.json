{"id": "1201.6626", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Jan-2012", "title": "Learning RoboCup-Keepaway with Kernels", "abstract": "We apply kernel-based methods to solve the difficult reinforcement learning problem of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in keepaway are the high-dimensionality of the state space (rendering conventional discretization-based function approximation like tilecoding infeasible), the stochasticity due to noise and multiple learning agents needing to cooperate (meaning that the exact dynamics of the environment are unknown) and real-time learning (meaning that an efficient online implementation is required). We employ the general framework of approximate policy iteration with least-squares-based policy evaluation. As underlying function approximator we consider the family of regularization networks with subset of regressors approximation. The core of our proposed solution is an efficient recursive implementation with automatic supervised selection of relevant basis functions. Simulation results indicate that the behavior learned through our approach clearly outperforms the best results obtained earlier with tilecoding by Stone et al. (2005).", "histories": [["v1", "Tue, 31 Jan 2012 17:26:17 GMT  (71kb)", "http://arxiv.org/abs/1201.6626v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.MA", "authors": ["tobias jung", "daniel polani"], "accepted": false, "id": "1201.6626"}, "pdf": {"name": "1201.6626.pdf", "metadata": {"source": "CRF", "title": "Learning RoboCup-Keepaway with Kernels", "authors": ["Tobias Jung", "Daniel Polani"], "emails": ["tjung@informatik.uni-mainz.de", "d.polani@herts.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 120 1.66 26v1 [cs.AI] 3Keywords: Enhanced Learning, Least-Squares Policy Iteration, Regularization Networks, RoboCup"}, {"heading": "1. Introduction", "text": "The central challenge that it is to overcome is on the one hand the high dimensionality of the state space (every observed state is a carrier of 13 measurements), which means that conventional approaches to the alignment of the functionality in the RL are impracticable, on the other hand the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that we know the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that the uncertainty, that we know the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the us, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the us, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the us, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the us, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the us, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the us, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the and the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the us, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the and the uncertainty, the uncertainty, the uncertainty, the uncertainty, the us, the uncertainty, the uncertainty, the and the uncertainty, the uncertainty, the uncertainty, the uncertainty, the uncertainty, the we, the and the uncertainty, the"}, {"heading": "2. Background", "text": "In this section, we will briefly discuss the topics of the Directive and regulatory networks."}, {"heading": "2.1 Reinforcement Learning", "text": "Reinforcement Learning (RL) is a simulation-based form of approximate dynamic programming, see e.g. (Bertsekas and Tsitsiklis, 1996). Consider a temporally separated dynamic system with states S = {1,.., N} (to simplify the representation, we assume the finite case). At each step t, when the system is in state st, a decision maker selects a control action (again selected from a finite set of permissible actions A) that probably changes the state of the system to st + 1, with distribution P (st + 1 | st, at). Any such transition leads to an immediate reward rt + 1 = R (st + 1 | st, at). The ultimate goal of the decision maker is to choose a procedure that maximizes long-term performance, a measure of the accumulated sum of rewards."}, {"heading": "2.1.1 Model-free Q-value function and optimal control", "text": "Let us have \u03c0 a decision rule (called policy) that maps the states to measures. Q = Q = Q = Then we want to evaluate the state action value function (Q function), which is considered for each state s as the expected approximate sum of the rewards obtained in each state. Q = Q = improved Q = Q function (Q function), which is considered for each state s as the expected approximate sum of the rewards obtained in each state. Q = improved Q = Q function (Q function), which leads to the selection of actions according to \u03c0: Q\u03c0 (s, a), which is used for each state s as the approximate sum of the rewards received."}, {"heading": "2.1.2 Approximate policy evaluation with least-squares methods", "text": "The three methods assume that an (infinitely) long path of states and rewards is generated with a simulation of the system (e.g. an agent interacting with its environment).The path starts from an initial state s0 and consists of tuples (s0, a0), (s1, a1),. and rewards r1, r2,. where the action ai is selected according to the underlying transition probabilities. From now on, we will understand these state actions as denoting xt."}, {"heading": "2.2 Standard regression with regularization networks", "text": "From the previous discussion, we have seen that (approximate) policy evaluation can amount to a traditional problem of functional convergence. To this end, we consider here the family of regulatory networks (Girosi et al., 1995) that correspond functionally to granular regression and Bayesian regression with Gaussian processes (Rasmussen and Williams, 2006), but introduce them here from the non-Bayesian regulatory perspective as in (Smola and Schoolhead, 2000)."}, {"heading": "2.2.1 Solving the full problem", "text": "In view of t training examples {xi, yi} t i = 1 with inputs xi and observed outputs yi to reconstruct the underlying function, candidates are considered from a function space Hk in which Hk is a reproducing kernel Hilbert space with reproducing kernel k (e.g. Wahba, 1990), and from all possible candidates are searched for the function f-Hk, which reaches the minimum in the risk function space (yi-f (xi)) 2 + \u03c32-f-Hk. Scalar \u03c3 2 is a regularization parameter. since solutions to this variation problem can be represented solely by the data (Wahba, 1990) as f (\u00b7) = \u2211 k (xi, \u00b7) wi, the unknown weight vector w results from the solution of the quadratic problem w-Rt (Kw \u2212 y) T (Kw \u2212 y) T (Kw \u2212 y) + \u03c32wTKw (6), with y \u2212 yk (j) (x)."}, {"heading": "2.2.2 Subset of regressor approximation", "text": "Often one is not ready to solve the complete t-by-t problem in (6) if the number of training examples t is large and instead considers means of approximation. In the subset of the regressors (SR) approach (Poggio and Girosi, 1990; Luo and Wahba, 1997; Smola and Schoolhead, 2000) one selects a subset {x-i} m i = 1 of the data, with m \u00b2 t, and approaches the core for any x, x \u00b2 by takingk (x, x \u00b2) = km (x) TK \u2212 1mmkm (x \u00b2). (7) Here km (x) stands for the m \u2212 1 feature vector km (x) = (k (x-1, x),.. k (x-m, x \u00b2))) Tand the m matrix Kmm is the submatrix [Kmm] ij = k-i, x-j) of the complete rinel matrix (K)."}, {"heading": "2.2.3 Online selection of the subset", "text": "In order to select the subset of relevant base functions, we must assume that the data is available only sequentially. (This is only possible if the subset of relevant base functions (referred to as dictionary or set of base vectors BV = costly) contains many different approaches; typically, they can be distinguished as unattended or supervised. Unsupervised approaches such as random selection (Williams and Seeger, 2001) or incomplete cholesky decomposition (Fine and Scheinberg, 2001) will not use information about the task we want to solve, i.e. the response variable we want to scale back to use all the information at all, while incomplete cholesky aims to reduce the error resulting from the approximation of the kernel matrix. Supervised selection of the subset takes into account the variable response and generally goes back through greedy procedures. (Smola and Bartlett, 2001, of which we know only that none of these approaches is directly applicable to the beginning)."}, {"heading": "3. Policy evaluation with regularization networks", "text": "We now present an efficient online implementation for the least square based policy assessment (applicable to the methods LSPE, LSTD, BRM) to be used in the framework of an approximate policy iteration (see Figure 1).Our implementation combines the above mentioned automatic selection of basic functions (from Section 2.2.3) with a recursive calculation of the weight vector corresponding to the regulatory network (from Section 2.2.2) to represent the underlying Q function, with the aim of deriving an approximation of the Q function (\u00b7; w) of Q\u03c0, the unknown Q function of some given policy areas."}, {"heading": "3.1 Stating LSPE, LSTD and BRM with regularization networks", "text": "Let us assume that the BV dictionary contains basic functions. Furthermore, let us assume that at the time t (after observing the t-transitions) a new transition from xt to xt + 1 is observed under reward rt + 1. From now on, we use a double index (also for vectors) to indicate dependence in the number of examples t and the number of basic functions. We define matrices: Kt + 1, m = km (x0) T... km (x0) T \u2212 \u03b3km (x1) T \u2212 \u03b3km (xt) T \u2212 \u03b3km (xt) T + 1... rrt + 1, \u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441chisik-chisik-chicicicicicicicicicicicic"}, {"heading": "3.1.1 The LSPE(\u03bb) method", "text": "Then for LSPE (\u03bb), the smallest square problem (2) is given as (wtm is the weight vector of the previous step): w = t + 1, m = argmin w {Kt + 1, mw \u2212 Kt + 1, mwtm \u2212 t + 1 (rt + 1 \u2212 Ht + 1, mwtm), w = 2 + \u03c32 (w \u2212 wtm) TKmm (w \u2212 wtm)} Calculation of the derived wrt w and setting to zero is obtained for w + 1, m: w = t + 1, m = wtm + (KTt + 1, mKt + 1, m + 1, m + SPt (+ 1), mHt (m + 1, mHt + 1, mtm + 1, mtm), with Zt + 1 \u2212 m: T + 1Kt + 1, m + 1, mt\u00b2 (1), + 1 (1) (1), wm (1), (1) (1), mtm (1) (m)."}, {"heading": "3.1.2 The LSTD(\u03bb) method", "text": "Similarly, for LSTD (\u03bb) the fixed point equation (4) is given as follows: w = argmin w {1 Kt + 1, mw \u2212 Kt + 1, mw \u2012 t + 1 (rt + 1 \u2212 Ht + 1, mw \u00b2) Calculating the derivative with respect to w and setting it to zero, one gets (ZTt + 1, mHt + 1, m + \u03c3 2Kmm) w = ZTt + 1, mrt + 1. Thus, the solution results in wt + 1, m to the fixed point equation in LSTD (\u03bb): wt + 1, m = (ZTt + 1, mHt + 1, m + \u03c3 2Kmm) \u2212 1 ZTt + 1, mrt + 1 (13)."}, {"heading": "3.1.3 The BRM method", "text": "Finally, in the case of BRM, the smallest problem (5) is given as follows: wt + 1, m = argmin w {0rt + 1 \u2212 Ht + 1, mw 02 + \u03c32wTKmmw} Thus the weight vector wt + 1, m bywt + 1, m = (HTt + 1, mHt + 1, m + \u03c3 2Kmm) \u2212 1 HTt + 1, mrt + 1 (14)"}, {"heading": "3.2 Outline of the recursive implementation", "text": "Note that all three methods on solving a very similar set of linear equations in eqs = 1 TB + 1 TB = 1 TB + 1 TB = 1 TB (1), (14). The overloading of the notation can be specified here as: \u2022 LSPE: solvewt + 1, m = wtm + 1, m (bt + 1, m \u2212 At + 1, mwtm) (12 '), where \u2212 P \u2212 1t + 1, mHt + 1, m: (K T + 1, mKit) \u2212 b: 1, mKT + 1, m: (Z T + 1, mrt + 1- Base) (12') where \u2212 P \u2212 1t + 1, mHt + 1, m: (K T + 1, mKit) \u2212 b: 1, mKT + 1, m + 1, m: b + 1, m: b + 1, m: b + 1), m: b + 1, m (b + 1), m: b + 1, m: b + 1, m: b + 1, m: b + 1, m: b + 1, m: b + 1, m: b + 1, m: b + 1, m: b + 1, m: b + 1, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b b, b, b, b, b, b, b, b, b, b, b, b, b, b, b b, b, b, b, b b, b, b, b, b, b b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b"}, {"heading": "3.3 Derivation of recursive updates for the case BRM", "text": "-1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) -1 (1) (1) (2) (1) (2) (1) (1) (2) (1) (1) (2) (1) (1) (2) (1) (1) (1) (2) (1) (1) (1) (2) (1) (1) (2) (1) (1) (2) (1) (1) (1) (2) (1 (1) (1) (2) (1 (1) (1) (1) (2) (1 (1) (2) (1 (1) (1) (1) (1 (1) (1 (1) (1) (1 (1) (1 (1) (1) (1 (1) (1 (1) (1) (1 (1) (1) (1) (1 (1 (1) (1) (1) (1) (1 (2) (1 (1) (1 (2) (1 (1) (1) (1) (1 (1) (1) (1) (1 (1) (1 (1 (1) (1) (1) (1 (1 (1) (1) -1 (1 (1) -1 (1) -1) -1 (1) -1 (1 (1) -1 (1) -1 (1) -1 (1 (1) -1 (1 (1 (1"}, {"heading": "4. RoboCup-keepaway as RL benchmark", "text": "The experimental work we have done for this article uses the publicly available framework 3 Keepaway (Stone et al., 2005), which builds on the standard football simulator RoboCup, which is also used for official competitions (Noda et al., 1998).Agents in RoboCup are autonomous entities; they sense and act independently and asynchronously, run as individual processes and cannot communicate directly. Agents receive visual perceptions every 150 msec and can act once every 100 msec. The state description consists of relative distances and angles to visible objects in the world, such as the ball, other agents or fixed beacons for localization. In addition, random noise affects both the agent sensors and their actuators. In Keepaway, a team of \"Keepers\" must learn how to maximize the time they can control within a limited region of the field against an opposing team of \"takers.\""}, {"heading": "5. Experiments", "text": "In this section, we are finally ready to apply our proposed approach to the problem of \"keepaway.\" We implemented and compared two different variations of the basic algorithm within an iteration-based policy framework: (a) Optimistic policy iteration using LSPE (\u03bb) and (b) Actorcritical policy iteration using LSTD (\u03bb). As a basic method, we used Sarsa (\u03bb) with tilecoding, which we used in our experiments due to the stochastic state transitions in the US (leading to highly variable outcomes) and BRM's inability to adequately deal with this situation."}, {"heading": "6. Discussion and related work", "text": "We have presented a kernel-based approach to least-squares-based policy assessment in RL. Specifically, the proposed method was designed with high-dimensional stochastic control tasks for RL in mind; we demonstrate their effectiveness with the RoboCup Keepaway benchmark. Overall, the results suggest that kernel-based online learning in RL is very possible and recommended. Even the rather few simulation runs we run clearly show that our approach is superior to conventional functional approximation in RL by using grid-based tilecoding. What could be more important, the kernel-based approach in RL only requires setting some fairly general parameters that do not depend on the specific control problem you want to solve. On the other hand, by using tilecoding or a fixed functional network in high dimensions."}, {"heading": "Acknowledgments", "text": "The authors thank the anonymous reviewers for their useful comments and suggestions."}, {"heading": "Appendix A. A summary of the updates", "text": "Let xt + 1 = (st + 1, with + 1) the next step of the state action Tupel and rt + tmt = b = 1b = 1mT = 1mT = 1mT = 1mT = 1mT + 1mT + 1mT + 1mT + 1mT + 1mT + 1mT = 1mT + 1mT = 1mT + 1mT = 1mT + 1mT + 1mT + 1mT + 1mT + 1mT + 1mT + 1mT + 1mT + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + 1T + T + 1T + 1T + T + 1T + 1T + T + 1T + T + 1T + T + 1T + T + 1T + T + 1T + T + 1T + T + 1T + T + T + 1T + T + 1T + T + T + 1T + T + 1T + T + T + 1T + T + T + 1T + T + T + 1T + T + 1T + T + T + 1T + T + T + 1T + T + T + 1T + T + T + T + 1T + T + T + 1T + T + T + T + 1T + T + T + 1T + T + T + T + T + T + T + T + T + T + 1T + T + T + T + T + T + T + T + T + T + T + T = 1T + T + T = 1T + T + T + T + T + T + T + T + T + T + T + T + T + T = 1T + T + T + T + T + T + T = 1T"}], "references": [{"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "Dynamic Programming and Reinforcement Learning,", "citeRegEx": "Lagoudakis and Parr.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr.", "year": 2003}, {"title": "Fundamentals of Adaptive Filtering", "author": ["A. Sayed"], "venue": "Wiley Interscience,", "citeRegEx": "Sayed.,? \\Q2003\\E", "shortCiteRegEx": "Sayed.", "year": 2003}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "keepaway. Adaptive Behavior,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}], "referenceMentions": [{"referenceID": 2, "context": "(2005) successfully applied RL to keepaway, using the textbook approach with online Sarsa(\u03bb) and tilecoding as underlying function approximator (Sutton and Barto, 1998).", "startOffset": 144, "endOffset": 168}, {"referenceID": 2, "context": "(2005) successfully applied RL to keepaway, using the textbook approach with online Sarsa(\u03bb) and tilecoding as underlying function approximator (Sutton and Barto, 1998). However, tilecoding is a local method and places parameters (i.e. basis functions) in a regular fashion throughout the entire state space, such that the number of parameters grows exponentially with the dimensionality of the space. In (Stone et al., 2005) this very serious shortcoming was adressed by exploiting problem-specific knowledge of how the various state variables interact. In particular, each state variable was considered independently from the rest. Here, we will demonstrate that one can also learn using the full (untampered) state information, without resorting to simplifying assumptions. In this paper we propose a (non-parametric) kernel-based approach to approximate the value function. The rationale for doing this is that by representing the solution through the data and not by some basis functions chosen before the data becomes available, we can better adapt to the complexity of the unknown function we are trying to estimate. In particular, parameters are not \u2018wasted\u2019 on parts of the input space that are never visited. The hope is that thereby the exponential growth of parameters is bypassed. To solve the RL problem of optimal control we consider the framework of approximate policy iteration with the related least-squares based policy evaluation methods LSPE(\u03bb) proposed by Nedi\u0107 and Bertsekas (2003) and LSTD(\u03bb) proposed by Boyan (1999).", "startOffset": 145, "endOffset": 1505}, {"referenceID": 2, "context": "(2005) successfully applied RL to keepaway, using the textbook approach with online Sarsa(\u03bb) and tilecoding as underlying function approximator (Sutton and Barto, 1998). However, tilecoding is a local method and places parameters (i.e. basis functions) in a regular fashion throughout the entire state space, such that the number of parameters grows exponentially with the dimensionality of the space. In (Stone et al., 2005) this very serious shortcoming was adressed by exploiting problem-specific knowledge of how the various state variables interact. In particular, each state variable was considered independently from the rest. Here, we will demonstrate that one can also learn using the full (untampered) state information, without resorting to simplifying assumptions. In this paper we propose a (non-parametric) kernel-based approach to approximate the value function. The rationale for doing this is that by representing the solution through the data and not by some basis functions chosen before the data becomes available, we can better adapt to the complexity of the unknown function we are trying to estimate. In particular, parameters are not \u2018wasted\u2019 on parts of the input space that are never visited. The hope is that thereby the exponential growth of parameters is bypassed. To solve the RL problem of optimal control we consider the framework of approximate policy iteration with the related least-squares based policy evaluation methods LSPE(\u03bb) proposed by Nedi\u0107 and Bertsekas (2003) and LSTD(\u03bb) proposed by Boyan (1999). Least-squares based policy evaluation is ideally suited for the use with linear models and is a very sample-efficient variant of RL.", "startOffset": 145, "endOffset": 1542}, {"referenceID": 0, "context": "Computational experiments in (Bertsekas and Ioffe, 1996) or (Lagoudakis and Parr, 2003) indicate that both approaches can perform much better than TD(\u03bb).", "startOffset": 60, "endOffset": 87}, {"referenceID": 0, "context": "A third approach, related to LSTD(0) is the direct minimization of the Bellman residuals (BRM), as proposed in (Baird, 1995; Lagoudakis and Parr, 2003).", "startOffset": 111, "endOffset": 151}, {"referenceID": 1, "context": "(Sayed, 2003).", "startOffset": 0, "endOffset": 13}, {"referenceID": 2, "context": "Optimistic policy iteration (OPI) is an online method that immediately processes the observed transitions as they become available from the agent interacting with the environment (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998).", "startOffset": 179, "endOffset": 235}, {"referenceID": 0, "context": "One advantage of the actor-critic approach is that we can reuse the same set of observed transitions to evaluate different policies, as proposed in (Lagoudakis and Parr, 2003).", "startOffset": 148, "endOffset": 175}], "year": 2012, "abstractText": "We apply kernel-based methods to solve the difficult reinforcement learning problem of 3vs2 keepaway in RoboCup simulated soccer. Key challenges in keepaway are the highdimensionality of the state space (rendering conventional discretization-based function approximation like tilecoding infeasible), the stochasticity due to noise and multiple learning agents needing to cooperate (meaning that the exact dynamics of the environment are unknown) and real-time learning (meaning that an efficient online implementation is required). We employ the general framework of approximate policy iteration with least-squares-based policy evaluation. As underlying function approximator we consider the family of regularization networks with subset of regressors approximation. The core of our proposed solution is an efficient recursive implementation with automatic supervised selection of relevant basis functions. Simulation results indicate that the behavior learned through our approach clearly outperforms the best results obtained with tilecoding by Stone et al. (2005).", "creator": "LaTeX with hyperref package"}}}