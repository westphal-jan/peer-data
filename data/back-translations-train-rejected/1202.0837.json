{"id": "1202.0837", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Feb-2012", "title": "On the influence of intelligence in (social) intelligence testing environments", "abstract": "This paper analyses the influence of including agents of different degrees of intelligence in a multiagent system. The goal is to better understand how we can develop intelligence tests that can evaluate social intelligence. We analyse several reinforcement algorithms in several contexts of cooperation and competition. Our experimental setting is inspired by the recently developed Darwin-Wallace distribution.", "histories": [["v1", "Fri, 3 Feb 2012 22:38:04 GMT  (223kb,DS)", "http://arxiv.org/abs/1202.0837v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["javier insa-cabrera", "jose-luis benacloch-ayuso", "jose hernandez-orallo"], "accepted": false, "id": "1202.0837"}, "pdf": {"name": "1202.0837.pdf", "metadata": {"source": "CRF", "title": "On the influence of intelligence in (social) intelligence testing environments", "authors": ["Javier Insa-Cabrera", "Jos\u00e9-Luis Benacloch-Ayuso"], "emails": ["jinsa@dsic.upv.es", "jobeay@fiv.upv.es", "jorallo@dsic.upv.es"], "sections": [{"heading": "1 Introduction", "text": "From the late nineties, but with a stronger dynamics lately, we can find several works, including the preliminary results of intelligence 11, 14, 8, 2] that deal with the problem of measuring intelligence in a principled and general manner. With the common cause of using terms from (algorithmic) information theory, MML and two-part compression, Kolmogorov complexity and Solomonoff Priors (see [15] for the correct definition of all these terms), some of these works current definitions and tests for evaluating the intelligence of agents in some of these tests is that the complexity of a problem, task or environment can be derived from its Kolmogorov complexity. This allows the application of the setting to many different fields of artificial intelligence, including inductive or deductive tasks [4, 6]: Given each task or problem, we can take its intrinsic complexity and use as a yardstick."}, {"heading": "2 Universal intelligence tests and social intelligence", "text": "In fact, it is the case that most of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there"}, {"heading": "3 Extending an intelligence test to consider several agents", "text": "This year it is so far that it will only take one year to move on to the next round."}, {"heading": "4 Evaluating agents isolatedly", "text": "We begin our experiments with the scenario in which agents are simply taken in isolation and evaluated. This is the same attitude as in [11], with the only (slight) difference that good and evil are easily reactive because they try to avoid cell division with other agents. Furthermore, we limit the evaluation to environments with nine cells only. The result of fig. 3 is clear (and agrees with the results in [11]. The oracle achieves very good results (but not optimal, as there is stochastic behavior that it cannot predict, and also because the oracle does not plan further than the immediate best action) and the trivial pendant stabilizes slightly above 0.5. The random agent has an average reward of 0, as predicted in theory. The three RL agents are very slow learners and come closer to the trivial pendant only after 10,000 iterations. Their behavior is similar and the differences are small."}, {"heading": "5 Evaluating agents in a competitive scenario", "text": "More interesting things can be observed if we switch to the competition scenario. Remember that all agents are in the area at the same time, and they compete for rewards. Note that the sum of rewards is limited, so if one agent performs well (e.g. better than 1 / 6 = 0.167), then the others will get worse, as most positive rewards are eaten up by the other agents and they can still get the negative rewards. This is exactly what Fig. 2 (left) shows. If we look at the random agent, we see that he gets a value that is even lower than 0, as most positive rewards are eaten up by the other agents, and he can only collect negative rewards. It is not much lower than 0, but this can be explained because there are nine cells and rewards for each iteration divided by two, so that a random gait is often found cells with almost no (negative) rewards, but we also see good results, because the Okel has to compete with others."}, {"heading": "6 Evaluating agents in a cooperative scenario", "text": "The next scenario we want to investigate is when the 6 agents are asked to cooperate, and this is done by putting all the rewards in the same bag, so that the agents consider the reward merely as the average reward of all agents, and the results for all agents are shown in fig. 4 (left). The oracle, the trivial follower, and the random agent cannot change their behavior, so it is clear that their results should be similar to those in fig. 2 (left) if the other agents do not make dramatic improvements, but that is not the case. RL agents have already been lost in the competitive case of the state, and they are also lost in the cooperative case, because the length of the observations is the same. Figure 4 (middle) changes from fig. 2 (middle). How is it possible that the movement is a competition to a cooperative case that we get worse average results? Shouldn't it be the other way around? Here is the explanation a bit more complicated. The problem of the cooperation is the way we allocate rewards."}, {"heading": "7 Scenario measuring both competition and cooperation", "text": "We define two teams, one with two Q-Learning agents and the other with two SARSA agents. Within each team, the rewards go into the same pocket, but different teams compete for the rewards. This is shown in Fig. 5. In general, the results are poorer than with three agents in the cooperative case (Fig. 4, right). This can be explained because we have four instead of three agents here, but also because it is a more complex scenario to have two teams than to have only one team. Results show that there are no significant differences between the two components of each team. However, there are important differences between the two components of each team. This can be observed in Fig. 5, where we assign the best results in the team to the first entry and the worst results to the second entry. This means that the graph merely shows the difference in (average) performance between the best component in the team and the worst component in the team. We see that this difference is very significant, while this agent is close to one in the other 13."}, {"heading": "8 Discussion", "text": "In the previous sections, we have analyzed several scenarios, in which a test that was originally developed to measure an agent's intelligence against an environment without other agents is adapted to other scenarios in which other agents are introduced in the environment. As expected, working with many agents makes things much more complex. We see that performance can be seriously degraded by involving other agents with zero intelligence, rather than random agents. This is surprising when we look at this from the point of view of game theory (especially from the point of view of two-player games), but it is much more natural when we realize that it is harder to achieve a goal when there is another agent bugging around (even randomly). This is extreme in the case of RL agents, because random agents can be considered noise, and these multiplications the state space.All this means that the difficulty of a task is no longer related to the complexity of the environment."}, {"heading": "9 Conclusions", "text": "In fact, most of them are able to follow the rules that they have imposed on themselves, and they are able to follow the rules that they have imposed on themselves. (...) It is not that they follow the rules. (...) It is not that they obey the rules. (...) It is that they obey the rules. (...) It is that they obey the rules. (...) It is that they obey the rules. (...) It is that they obey the rules. (...) It is that they obey the rules. (...) It is that they obey the rules. (...) It is that they obey the rules. (...)"}, {"heading": "10 Acknowledgements", "text": "We would like to thank Hado van Hasselt for his RL algorithm library, which was useful when we implemented our algorithms, supported by the MEC projects EXPLORAINGENIO TIN 2009-06078-E, CONSOLIDER-INGENIO 26706 and TIN 2010-21062-C02-02 and the GVA project PROMETEO / 2008 / 051. Javier Insa-Cabrera was sponsored by the Spanish MEC FPU scholarship AP2010-4389."}], "references": [{"title": "A computational extension to the Turing Test", "author": ["D.L. Dowe", "A.R. Hajek"], "venue": "Proceedings of the 4th Conference of the Australasian Cognitive Science Society,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "IQ tests are not for machines", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "yet. Intelligence, (0):\u2013", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond the Turing Test", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "J. Logic, Language & Information, 9(4):447\u2013 466", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Computational measures of information gain and reinforcement in inference processes", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "AI Communications, 13(1):49\u201350", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Constructive reinforcement learning", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "International Journal of Intelligent Systems, 15(3):241\u2013264", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "On the computational measurement of intelligence factors", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "A. Meystel, editor, Performance metrics for intelligent systems workshop, pages 1\u20138. National Institute of Standards and Technology, Gaithersburg, MD, U.S.A.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "A (hopefully) non-biased universal environment class for measuring intelligence of biological and artificial systems", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "M. H. et al., editor, Artificial General Intelligence, 3rd Intl Conf, pages 182\u2013183. Atlantis Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Measuring universal intelligence: Towards an anytime intelligence test", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "Artificial Intelligence, 174(18):1508 \u2013 1539", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "On more realistic environment distributions for defining", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Insa- Cabrera"], "venue": "evaluating and developing intelligence. In J. Schmidhuber, K. Th\u00f3risson, and M. L. (eds), editors, Artificial General Intelligence 2011, volume 6830, pages 82\u201391. LNAI series, Springer", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "A formal definition of intelligence based on an intensional variant of Kolmogorov complexity", "author": ["J. Hern\u00e1ndez-Orallo", "N. Minaya-Collado"], "venue": "Proc. Intl Symposium of Engineering of Intelligent Systems (EIS\u201998), February 1998, La Laguna, Spain, pages 146\u2013163. ICSC Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "Comparing humans and AI agents", "author": ["J. Insa-Cabrera", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Hern\u00e1ndez- Orallo"], "venue": "J. Schmidhuber, K. Th\u00f3risson, and M. L. (eds), editors, Artificial General Intelligence 2011, volume 6830, pages 122\u2013132. LNAI series, Springer", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Evaluating a reinforcement learning algorithm with a general intelligence test", "author": ["J. Insa-Cabrera", "D.L. Dowe", "J. Hernandez-Orallo"], "venue": "J. M. J.A. Lozano, J.A. Gamez, editor, Current Topics in Artificial Intelligence. 14th Conference of the Spanish Association for Artificial Intelligence, CAEPIA 2011. Lecture Notes in Artificial Intelligence, Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Frequency adjusted multi-agent q-learning", "author": ["M. Kaisers", "K. Tuyls"], "venue": "Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 1-Volume 1, pages 309\u2013316. International Foundation for Autonomous Agents and Multiagent Systems", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["S. Legg", "M. Hutter"], "venue": "Minds and Machines, 17(4):391\u2013444", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "An introduction to Kolmogorov complexity and its applications (3rd ed.)", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Evolution and the Theory of Games", "author": ["J. Maynard-Smith"], "venue": "Cambridge Univ Pr", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1982}, {"title": "Algorithmic game theory", "author": ["N. Nisan"], "venue": "Cambridge Univ Pr", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "On-line Q-learning using connectionist systems", "author": ["G. Rummery", "M. Niranjan"], "venue": "Cambridge University Engineering Department, TR 166", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1994}, {"title": "If multi-agent learning is the answer", "author": ["Y. Shoham", "R. Powers", "T. Grenager"], "venue": "what is the question? Artificial Intelligence, 171(7):365\u2013377", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "The MIT Press", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}, {"title": "Q-learning", "author": ["C. Watkins", "P. Dayan"], "venue": "Machine learning, 8(3):279\u2013292", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1992}, {"title": "Evolutionary game theory", "author": ["J. Weibull"], "venue": "The MIT press", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Qv (\u03bb)-learning: A new on-policy reinforcement learning algorithm", "author": ["M. Wiering"], "venue": "7th European Ws. on Reinforcement Learning, pages 17\u201318", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Dating back from the late nineties, but with a stronger momentum recently, we can find several works [1, 10, 3, 14, 8, 2] addressing the problem of measuring agent intelligence in a principled and general way.", "startOffset": 101, "endOffset": 121}, {"referenceID": 9, "context": "Dating back from the late nineties, but with a stronger momentum recently, we can find several works [1, 10, 3, 14, 8, 2] addressing the problem of measuring agent intelligence in a principled and general way.", "startOffset": 101, "endOffset": 121}, {"referenceID": 2, "context": "Dating back from the late nineties, but with a stronger momentum recently, we can find several works [1, 10, 3, 14, 8, 2] addressing the problem of measuring agent intelligence in a principled and general way.", "startOffset": 101, "endOffset": 121}, {"referenceID": 13, "context": "Dating back from the late nineties, but with a stronger momentum recently, we can find several works [1, 10, 3, 14, 8, 2] addressing the problem of measuring agent intelligence in a principled and general way.", "startOffset": 101, "endOffset": 121}, {"referenceID": 7, "context": "Dating back from the late nineties, but with a stronger momentum recently, we can find several works [1, 10, 3, 14, 8, 2] addressing the problem of measuring agent intelligence in a principled and general way.", "startOffset": 101, "endOffset": 121}, {"referenceID": 1, "context": "Dating back from the late nineties, but with a stronger momentum recently, we can find several works [1, 10, 3, 14, 8, 2] addressing the problem of measuring agent intelligence in a principled and general way.", "startOffset": 101, "endOffset": 121}, {"referenceID": 14, "context": "With the common thing of using notions taken from (algorithmic) information theory, MML and two-part compression, Kolmogorov complexity and Solomonoff priors (see [15] for proper definitions of all these notions), some of these works present definitions and tests to evaluate agent intelligence.", "startOffset": 163, "endOffset": 167}, {"referenceID": 3, "context": "This allows for the application of the setting to many different fields in artificial intelligence, including inductive or deductive tasks [4, 6]: given any task or problem, we can derive its intrinsic complexity and use it as a measure of difficulty.", "startOffset": 139, "endOffset": 145}, {"referenceID": 5, "context": "This allows for the application of the setting to many different fields in artificial intelligence, including inductive or deductive tasks [4, 6]: given any task or problem, we can derive its intrinsic complexity and use it as a measure of difficulty.", "startOffset": 139, "endOffset": 145}, {"referenceID": 7, "context": "A prototype of the universal test introduced in [8] has been used to evaluate several agents, including humans and reinforcement learning agents.", "startOffset": 48, "endOffset": 51}, {"referenceID": 10, "context": "Some preliminary results of this evaluation [11, 12] show that the setting is able to compare and evaluate different kinds of agents, but it fails at placing them on the same scale, since humans usually get similar scores to other relatively simple agents.", "startOffset": 44, "endOffset": 52}, {"referenceID": 11, "context": "Some preliminary results of this evaluation [11, 12] show that the setting is able to compare and evaluate different kinds of agents, but it fails at placing them on the same scale, since humans usually get similar scores to other relatively simple agents.", "startOffset": 44, "endOffset": 52}, {"referenceID": 7, "context": "This is related to the question of evaluating intelligence with games (also suggested in [8]), where the difficulty is not only given by the complexity of the game, but from the opponent\u2019s intelligence.", "startOffset": 89, "endOffset": 92}, {"referenceID": 8, "context": "The Darwin-Wallace distribution [9] establishes a distribution of agents based on an evolutionary process.", "startOffset": 32, "endOffset": 35}, {"referenceID": 15, "context": "This is, of course, related to evolutionary game theory [16, 17, 22], where the \u2018game\u2019 is now an intelligence test.", "startOffset": 56, "endOffset": 68}, {"referenceID": 16, "context": "This is, of course, related to evolutionary game theory [16, 17, 22], where the \u2018game\u2019 is now an intelligence test.", "startOffset": 56, "endOffset": 68}, {"referenceID": 21, "context": "This is, of course, related to evolutionary game theory [16, 17, 22], where the \u2018game\u2019 is now an intelligence test.", "startOffset": 56, "endOffset": 68}, {"referenceID": 8, "context": "In fact, it is suggested in [9] that intelligence tests could be used to make this choice of agents, using off-the-shelf algorithms in AI.", "startOffset": 28, "endOffset": 31}, {"referenceID": 17, "context": "We will use very simple reinforcement learning (RL) algorithms: SARSA [18], Q-learning [21] and QV-learning [23].", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": "We will use very simple reinforcement learning (RL) algorithms: SARSA [18], Q-learning [21] and QV-learning [23].", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "We will use very simple reinforcement learning (RL) algorithms: SARSA [18], Q-learning [21] and QV-learning [23].", "startOffset": 108, "endOffset": 112}, {"referenceID": 19, "context": "This leads to interactive scenarios, which can be well represented by (discrete) environments, very much like the observaton-action-reward environments which are typical in reinforcement learning [20, 5] and other areas in agent theory.", "startOffset": 196, "endOffset": 203}, {"referenceID": 4, "context": "This leads to interactive scenarios, which can be well represented by (discrete) environments, very much like the observaton-action-reward environments which are typical in reinforcement learning [20, 5] and other areas in agent theory.", "startOffset": 196, "endOffset": 203}, {"referenceID": 0, "context": "These issues have been addressed in [1, 10, 3, 14, 8].", "startOffset": 36, "endOffset": 53}, {"referenceID": 9, "context": "These issues have been addressed in [1, 10, 3, 14, 8].", "startOffset": 36, "endOffset": 53}, {"referenceID": 2, "context": "These issues have been addressed in [1, 10, 3, 14, 8].", "startOffset": 36, "endOffset": 53}, {"referenceID": 13, "context": "These issues have been addressed in [1, 10, 3, 14, 8].", "startOffset": 36, "endOffset": 53}, {"referenceID": 7, "context": "These issues have been addressed in [1, 10, 3, 14, 8].", "startOffset": 36, "endOffset": 53}, {"referenceID": 7, "context": "In this context, [8] introduces the idea of universal test, a test which is conceived to be feasibly applicable to any kind of agent: humans, non-human animals, artificial agents, including hybrids and communities, of any degree of intelligence and speed.", "startOffset": 17, "endOffset": 20}, {"referenceID": 13, "context": "The test is based on a set of environments as in [14].", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "The focus of [8] is set on feasibility, highlighting that a sample of environments must be chosen carefully.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "First, according to [8] we need environments to be discriminative, so the behaviour of the agent has impact on its rewards, i.", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "In [7], a hopefully unbiased environment class (called \u039b) is introduced, as a class of environments composed of spaces and agents with universal descriptive (Turing-complete) power.", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "The first evaluations using these tests [11] show that they work well at evaluating very different agents (humans, and RL algorithms), but they do not properly reflect their supposed difference in intelligence.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "Many possible explanations are suggested in [11] for this phenomenon, with incremental knowledge acquisition and social intelligence being two of the abilities which this test is not giving enough importance.", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "In [9], instead of incorporating other agents in an ad-hoc way, the authors stick to the fundamental notions of the previous test proposals, i.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "As [9] states: \u201cThe basic idea is straightforward: intelligence is the result of evolution through millions of generations interacting with other live beings.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "The first intelligence tests based on the theory developed in [8] have been based on the environment class \u039b, introduced in [7].", "startOffset": 62, "endOffset": 65}, {"referenceID": 6, "context": "The first intelligence tests based on the theory developed in [8] have been based on the environment class \u039b, introduced in [7].", "startOffset": 124, "endOffset": 127}, {"referenceID": 10, "context": "We summarise this class first and then we see the way in which tests can be constructed using this class, as done in [11].", "startOffset": 117, "endOffset": 121}, {"referenceID": 7, "context": "This makes Good and Evil symmetric, which ensures that the environment is balanced (random agents score 0 on average) [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "For more details of the environment class \u039b, see [7].", "startOffset": 49, "endOffset": 52}, {"referenceID": 10, "context": "Spaces are generated by first determining the number of cells nc (in this paper we set nc = 9 but other configurations have been explored in [11]).", "startOffset": 141, "endOffset": 145}, {"referenceID": 10, "context": "This is a simplification by the prototype implemented by [11] of the originally Turing-complete behaviour of Good and Evil, and it also makes them non-reactive.", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "This configuration has been used to evaluate agents separately in [11].", "startOffset": 66, "endOffset": 70}, {"referenceID": 10, "context": "This re-introduces some degree of reactivity (with respect to the prototype in [11]), even in the single agent case (remember that we do not count Good and Evil as proper agents).", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "\u2022 Q-learning: the most common algorithm in reinforcement learning, as explained in [21] and [20].", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "\u2022 Q-learning: the most common algorithm in reinforcement learning, as explained in [21] and [20].", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "\u2022 SARSA [18].", "startOffset": 8, "endOffset": 12}, {"referenceID": 22, "context": "\u2022 QV-learning [23].", "startOffset": 14, "endOffset": 18}, {"referenceID": 10, "context": "This is the same setting as in [11], with the only (minor) difference that Good and Evil are slightly reactive because they try to avoid sharing a cell with other agents.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "3 is clear (and consistent with the results in [11]).", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "In fact, it would be extremely informative to repeat the experiment performed with humans and RL agents in [11] by using one of these simple multiagent environments.", "startOffset": 107, "endOffset": 111}, {"referenceID": 7, "context": "The closest area is an old companion of artificial intelligence, games, especially multiplayer games, but it has only been recently proposed as a testbed for measuring intelligence [8].", "startOffset": 181, "endOffset": 184}, {"referenceID": 12, "context": "Similarly, some other RL algorithms which are specialised for multiagent settings, such as Frequently Adjusted Q-learning [13] might give different results.", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "In the end, we could say that it is important to see which questions multiagent learning is the answer to [19], but it is also important to see how much the technology of autonomous agents and multiagent systems is progressing in order to meet these expectations.", "startOffset": 106, "endOffset": 110}], "year": 2012, "abstractText": "This paper analyses the influence of including agents of different degrees of intelligence in a multiagent system. The goal is to better understand how we can develop intelligence tests that can evaluate social intelligence. We analyse several reinforcement algorithms in several contexts of cooperation and competition. Our experimental setting is inspired by the recently developed Darwin-Wallace distribution.", "creator": "LaTeX with hyperref package"}}}