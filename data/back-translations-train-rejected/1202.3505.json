{"id": "1202.3505", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2012", "title": "Near-optimal Coresets For Least-Squares Regression", "abstract": "A rich coreset is a subset of the data which contains nearly all the essential information. We give deterministic, low order polynomial-time algorithms to construct rich coresets for simple and multiple response linear regression, together with lower bounds indicating that there is not much room for improvement upon our results.", "histories": [["v1", "Thu, 16 Feb 2012 03:07:35 GMT  (41kb)", "http://arxiv.org/abs/1202.3505v1", "15 pages; working paper"], ["v2", "Fri, 21 Jun 2013 20:58:43 GMT  (46kb)", "http://arxiv.org/abs/1202.3505v2", "To appear in IEEE Transactions on Information Theory"]], "COMMENTS": "15 pages; working paper", "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["christos boutsidis", "petros drineas", "malik magdon-ismail"], "accepted": false, "id": "1202.3505"}, "pdf": {"name": "1202.3505.pdf", "metadata": {"source": "CRF", "title": "Rich Coresets For Constrained Linear Regression", "authors": ["Christos Boutsidis"], "emails": ["cboutsi@us.ibm.com", "drinep@cs.rpi.edu", "magdon@cs.rpi.edu"], "sections": [{"heading": null, "text": "ar Xiv: 120 2.35 05v1 [cs.DS] 1"}, {"heading": "1 Introduction", "text": "Linear regression is an important technique in data analysis (Seber and Lee, 1977). Research in this field ranges from numerical techniques (A. Bjo \ufffd rck, 1996) to the robustness of predictive errors to noise (e.g. by using feature selection (Guyon and Elisseeff, 2003). Is it possible to efficiently identify a small subset of data that contains all the essential information of a learning problem? Such a subset is referred to as a \"rich\" coreset. We show that the answer is yes, for linear regression. Such a rich coreset is analogous to the support vectors in support vector machines (Christianini and Shawe-Taylor, 2000). Such rich coresets contain the significant or important points in the data and can be used to find good safe solutions to the full problem by solving a (much) smaller problem."}, {"heading": "1.1 Problem Setup", "text": "Suppose the usual setting with n data points (z1, y1) is a problem (zn, yn); zi Rd are feature vectors (which could be achieved by applying a non-linear attribute to raw data) and yi R are targets (answers); the linear regression problem asks for a vector xopt (x), for all x D Rd, the minimizesE (x) = n rank (z T i \u00b7 x \u2212 yi) 2over x D, where wi are positive weights. So, E (xopt) \u2264 E (x), for all x D. Domain D represents the constraints on the solution, e.g. in non-negative least squares (NNLS) (Lawson and Hanson, 1974; Bellavia et al., 2006), D = Rd +, the non-negative orthant."}, {"heading": "1.2 Our contributions", "text": "We have the methods we use are comparable, and provide deterministic guarantees. (drineas et al.) We have the methods we use. (drineas et al.) We have the same methods we use. (drineas et al.) We have comparable, low, polynomial runtimes and deterministic guarantees. (drineas et al.) We have the methods we use. (drineas et al.) We have a logarithmic factor that is greater. (We have comparable, low, polynomial runtimes and deterministic guarantees. (drineas et al.) We have the methods we use. (despite the koreset that we are a logarithmic factor greater) We have comparable, low, polynomial runtimes and deterministic guarantees. (drineas et al al al.)"}, {"heading": "2 Constrained Linear Regression", "text": "We define a limited linear regression as follows: \"A\" Rn \".\" A \".\" B \".\" D. \".\" D. \"\" D. \"\". \"\" D. \"\". \"\" D. \"\". \"\" D. \".\". \".\" D. \".\". \".\" \"D.\" \".\" \".\" \"\". \"\" \".\" \"\". \"\" \".\" \"\". \"\" \"..\" \"..\" \"\".. \"\" \"..\" \"\".. \"\" \"\".. \"\" \"\" \"..\" \"\" \"\".. \"\" \"\".. \"\".. \"\" \"\".. \"\" \"\" \"..\" \"\" \"\".. \"\" \"\" \"..\" \"\" \"\" \"..\" \"\" \"..\" \"\" \"..\" \"\" \"..\" \"\".. \"\" \"\".. \"\" \"\".. \"\" \"..\" \"\" \".\" \"\" \"..\" \"\" \"..\" \"\" \"\".. \"\" \"\" \"\".. \"\" \"\" \"\".. \"\" \"\" \".\" \"\" \"\" \"\" \"..\" \"\" \"\" \"\".. \"\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \"\" \"\""}, {"heading": "3 Constrained Multiple-Response Regression", "text": "Restricted multiple-response regression in the Frobenius standard can be reduced to simple regression, so we can apply the results of the previous section to this setting."}, {"heading": "3.1 Multi-Objective Regression", "text": "The task is to minimize the Frobenius standard error,,,,,,, A \"[x,.., x] \u2212,, B,\" 2F. \"Consider that, A\" Rn, \"B\" Rn, \"B\" Rn, \"B\" Rn, \"and, X\" \u2212 \"B,\" B., \"Lemma 3. For X\" X, \"x\" Rd, \"B\" Rn, \"B\" B, \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\" B., \"B.,\""}, {"heading": "3.2 Arbitrarily-Constrained Multiple-Response Regression", "text": "In general, the problem is still reduced to simple regression, but the coresets are larger now. We want to minimize the number of AX-B techniques via X-D-Rd-\u03c9; according to domain D, the domain is D-Rd-\u03c9. Since Rd-N-Rd-N-Rd-N-Rd-N is isomorphic, we can consider X-Rd-N-Rd-N as a \"stretched\" vector X-Rd-N; according to domain D, domain D-Rd-N-Rd-O-Rd-D. Likewise, we can consider B-Rd-N-Rn-N-Rd-D. To complete the transformation to a simple linear regression, we build a transformed block diagonal data matrix A-R series by repeating the copies of A along the diagonals."}, {"heading": "4 Unconstrained Multiple-Response Regression", "text": "I want to make sure that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job, that I do not lose my job."}, {"heading": "4.1 Proofs of Theorems 6 and 7", "text": "Let E = AXopt \u2212 B be the regression residual. Then, rank (E) \u2264 min {\u03c9, n \u2212 k}.Proof. Using our notation, AXopt \u2212 B = (In \u2212 UAUTA) B = U \u00b2 A (U \u00b2 A) T B. To note that rank (XY) \u2264 min {rank (X), rank (Y)}.We now give our main tool for recoreset regression. The proof is shifted to the appendix.Lemma 9. Assume that the rank of the matrix DSUA \u00b2 Rr \u00b7 k is equal to k (i.e., the matrix has full rank). Then, for a = 2, F, AX opt \u2212 2 the coreset regression."}, {"heading": "4.2 B-Agnostic Coreset Construction", "text": "All the coreset construction algorithms we have presented so far construct the coreset carefully using knowledge of the response vector. If the algorithm does not require knowledge of B to construct the coreset, and yet can provide an approximation guarantee for each B, then the algorithm is B-agnostic. A bagnostic coreset construction algorithm is attractive because the coreset, as specified by the sampling and recalculation of the S and D matrices, is calculated off-line and applied to each B. We will briefly digress to show how our methods can be expanded to develop B-agnostic coreset constructions. Theorem 10 (B-agnostic coresets) \u2212 n \u2212 Given a matrix A-Rn \u00b7 d with rank k and a matrix B-R \u00b2 n \u00b2 n \u00b2 n \u00b2 s \u00b2 s \u00b2 s \u00b2 s, there is an algorithm to develop such a coreset construction."}, {"heading": "5 Lower Bounds on Coreset Size", "text": "We have just seen a B-agnostic coreset construction algorithm with a rather weak worst-case guarantee for O (n / r) approximation errors. We will now show that no deterministic B-agnostic coreset construction algorithm can guarantee a better error (Theorem 11). (Drineas et al., 2008) delivers another B-agnostic coreset construction algorithm with r = O (k log (k) / 2). For a fixed B, the method comes in (Drineas et al., 2008) with a probability limit for the approximation error. However, there are target matrices B where the limit is arbitrarily large. Therefore, the probability algorithms come into a low probability event (Drineas et al., 2008) with respect to random decisions made in the algorithm by brushing all of these (possibly large) errors, the probability algorithms are not B-agnostic, as they do not result in a probability for a constant coreset value, but a probability for a constant value (B is a probability for a constant value)."}, {"heading": "5.1 An Impossibility Result for B-Agnostic Coreset Construction", "text": "There is a matrix A, which for each coreset C, there is a \"bad\" b.Theorem 11 (Deterministic b-agnostic coresets).There is a matrix A, which for each coreset C, there is a \"bad\" b.Theorem 11 (Deterministic b-agnostic coresets).There is a matrix A, which for each coreset C, there is a \"real\" r, there is a \"bad\" b-agnostic coresets).There is a matrix A, which for each coreset C, there is."}, {"heading": "5.2 Lower Bounds for Non-Agnostic Multiple Regression", "text": "The results are presented in theorems 13 and 14 theorems B + DSR + DSR \u00b7 r, the solution for coreset regression X + DSR, the solution for coreset regression X + DSB, which applies to each r > d and each type of sampling and rescaling matrices S-Rr \u00b7 n and D-DSR \u00b7 r, the solution for coreset regression X + DSB, which applies to each r > d and each type of sampling matrix S-Rr \u00b7 n and D-DSRr \u00b7 r, the solution for coreset regression X-R, DSB-DSB and DSR-DSR, which apply to each r > d and each type of sampling matrix S (DSR + 1)."}, {"heading": "6 Open problems", "text": "Can we determine the minimum size of a coreset that provides a (1 +) relative error guarantee for simple linear regression? We suspect that this coreset represents a lower limit, which makes our results almost narrow. Of course, rich coresets of size exactly k cannot be guaranteed: Consider two data points (1, 1), (\u2212 1, 1). The optimal regression is 0, but each coreset of size one leads to non-zero regression. Is it possible to get strong guarantees for small corsets for other learning problems?"}, {"heading": "A Linear Algebra Background", "text": "The individual values (SVD) of a matrix A-Rn-d of rank A (A-Rn-D) is a decomposition A-Rn-K (A-Rn-K), the individual values (A-Rn-K) are contained in the diagonal matrix A-Rk-K (A-Rn-K), and the individual values (A-Rd-K) contain the correct singular vectors. The SVD of A-Rk can be calculated in the deterministic matrix O-Rk-K (n-Rk-K)."}, {"heading": "B Algorithms", "text": "Input: A-Rn \u00b7 d from rank k, b-Rn and r > k + 1. Output: Sampling matrix S and recalculation matrix D1: Calculation of the SVD of Y = [A, b]. Output: Sampling matrix S and recalculation matrix V, with \u2264 k + 1 (rank Y). Output: Sampling matrix S and recalculation matrix D1: Deterministic corset construction for restricted linear regression. Output: A-Rn \u00b7 d from rank k, B-Rn \u00b7 and r > k. Output: Sampling matrix S and recalculation matrix D1: Deterministic corset construction for restricted linear regression. Output: A-Rn \u00b7 d from rank k, B-Rn \u00b7 and r > k. Output: Sampling matrix S and recalculation matrix D1: Deterministic correction of A: UA2, Ux A \u00b7 A: A \u00b7 E-K (rank K-K)."}, {"heading": "C Technical Proofs", "text": "Proof. (Theorem 4) We first construct D and S via Theorem 1 followed to A and bavg. (note: A + TB) The duration is O (note: A) (note: B) plus the duration of Theorem 1. The result is directly derived from the following derivative: B (note: A) + B (note: B) + B (note: B) + B (note: B) + B (note: A) + B (note: A) + B (note: B) + B (note: A) 2 (note: A) + B (note: A) + B (note: A) + B (note: A) + B (note: B) + (note: A) + B (note: A) + B (note: A) + B (note: A) (note: A) + B (note: A) + B (note: A). (note: A) + B (note: A). (note: A) + B (note: A). (note: A) + B (note: A) + B (note: A) + B (note: A). (note: A).) (note: A) + B (note: A) (note: A). (note: A) + B (note: A)."}], "references": [{"title": "Numerical Methods for Least Squares Problems", "author": ["A. Bj\u00f6rck"], "venue": "Twice-ramanujan sparsifiers. In Proc. 41st Annual ACM", "citeRegEx": "Bj\u00f6rck.,? \\Q1996\\E", "shortCiteRegEx": "Bj\u00f6rck.", "year": 1996}], "referenceMentions": [], "year": 2017, "abstractText": "A rich coreset is a subset of the data which contains nearly all the essential information. We give deterministic, low order polynomial-time algorithms to construct rich coresets for simple and multiple response linear regression, together with lower bounds indicating that there is not much room for improvement upon our results.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}