{"id": "1202.3639", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Feb-2012", "title": "Finding a most biased coin with fewest flips", "abstract": "We study the problem of learning the most biased coin among a set of coins by tossing the coins adaptively. The goal is to minimize the number of tosses to identify a coin i* such that prob{coin i* is most biased} is at least 1-\\delta\\ for any given \\delta&gt;0. Under a particular probabilistic model, we give an optimal algorithm, i.e., an algorithm that minimizes the expected number of tosses, to learn a most biased coin. The problem is equivalent to finding the best arm in the multi-armed bandit problem using adaptive strategies. Dar et al. (2002) and Mannor and Tsitsiklis (2004) show upper and lower bounds matching up to constant factors on the number of coin tosses for several underlying settings of the bias probabilities. For a class of such settings we bridge the constant factor gap by giving an optimal adaptive strategy -- a strategy that performs the best possible action under any given history of outcomes. For any given history, tossing the coin chosen by our strategy minimizes the expected number of tosses needed to learn a most biased coin. To our knowledge, this is the first algorithm that employs an optimal adaptive strategy under a Bayesian setting for this problem.", "histories": [["v1", "Thu, 16 Feb 2012 16:40:56 GMT  (67kb,D)", "https://arxiv.org/abs/1202.3639v1", null], ["v2", "Mon, 13 Aug 2012 14:53:35 GMT  (69kb,D)", "http://arxiv.org/abs/1202.3639v2", null], ["v3", "Sat, 7 Sep 2013 17:09:32 GMT  (71kb,D)", "http://arxiv.org/abs/1202.3639v3", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["karthekeyan chandrasekaran", "richard karp"], "accepted": false, "id": "1202.3639"}, "pdf": {"name": "1202.3639.pdf", "metadata": {"source": "CRF", "title": "Finding a most biased coin with fewest flips", "authors": ["Karthekeyan Chandrasekaran", "Richard Karp"], "emails": ["karthe@seas.harvard.edu,", "karp@icsi.berkeley.edu,"], "sections": [{"heading": null, "text": "For a given probability model, we specify an optimal algorithm, i.e. an algorithm that minimizes the expected number of future rolls. The problem is closely related to finding the best arm in the multi-armed bandit problem using adaptive strategies. Our algorithm uses an optimal adaptive strategy - a strategy that performs the best possible action at each step after observing the results of all previous coin flips. Consequently, our algorithm is also optimal for each initial history of the results. To our knowledge, this is the first algorithm to apply an optimal adaptive strategy to this problem in a Bayesian environment. Our proof of optimism uses tools from the Markov game field."}, {"heading": "1 Introduction", "text": "The multi-armed bandit problem is a classic decision problem with applications in bioinformatics, medical studies, stochastic algorithms, etc. [18]. Entering the problem is a series of weapons, each associated with an unknown reward problem. At each step, an agent selects an arm and receives a reward. The goal is to find a strategy for selecting the weapons to achieve the best expected reward asymptotically. This problem has produced a rich literature on the trade between exploration and exploitation, while the choice of weapons [6, 21, 2, 3]. The motivation to identify the best bandit arm stems from problems in which one wants to minimize one's regret within a fixed budget. In the models being considered, the goal is to choose an arm after a finite number of steps to minimize the regret. Here, the difference between the expected reward of the chosen strategy and the expected reward is defined."}, {"heading": "1.1 Results", "text": "Our main result is an optimal adaptive algorithm for the adjustment shown above. Theorem 1. Given \u03b4 > 0, there is an algorithm A that applies an optimal adaptive strategy in coin toss to identify a coin whose posterior probability of being heavy is at least 1 \u2212 \u03b4. However, at each step, the time it takes A to identify the coin is O (1).We also quantify the number of coin toss performed by our optimal adaptive algorithm. We assume an infinite stock of coins under the same probabilistic setting. Let q: = 1 \u2212 p, \u2206 H: = Protocol ((((p +) / (p \u2212))), \u2206 T: = Protocol ((q +) / (q \u2212))), B: = Protocol ((1 \u2212 g) (1 \u2212 g) (1 \u2212) (a) (1 \u2212 g) (a) (a) (1 \u2212 g) (1 \u2212 g) (3) / 3 (c) (a) () ()) (3) (a)."}, {"heading": "1.2 Algorithm", "text": "Let the history of the results of a coin i give at each stage of the algorithm of Di: = (hi, ti), where hi and ti indicate the number of results that were heads and tails respectively. In view of the history Di we define the probability ratio of the coin to beLi: = Pr (coin i is heavy | Di) Pr (coin i is non-heavy | Di) = (p + p \u2212) hi (q \u2212 q +) ti. Algorithm Likelihood-Toss1. Initiate Li = 1 for the i th coin. 2. While (Li < (1 \u2212 \u03b1) (1 \u2212 \u03b1) (1 \u2212 \u03b4) / p \u2212 i [n]) (a) Toss coin i so that i \u0445 = arg max {Li: i [n]}. (Break arbitrary ties). Letbi \u0445 = {1 if the result is heads, 0 if the result is tails. (b) Update Li automatically."}, {"heading": "2 Preliminaries", "text": "We now formally define the multitoken Markov game and specify the optimal strategy that has been studied for this game. We use the notation and results from [12].A Markov system S = (V, P, C, s, t) consists of a state space V, a transition probability function P: V \u00b7 V \u2192 [0, 1], a positive real cost factor associated with each state v, a starting state and a destination state t. Let v (0), v (k) denote a number of states that are hit by following the Markov system for k steps. The cost of such a journey on S is the sum of the cost of exotic systems. Let S1,.,., Sn be n Markov systems, each of which has a token on its initial state."}, {"heading": "3 Correctness", "text": "s Theorem.Pr (coin i is heavy | Di) = Pr (Di | Coin i is heavy) Pr (coin i is heavy) Pr (coin i is heavy) Pr (coin i is heavy) Pr (coin i is heavy) Pr (Di) = \u03b1 (p +) hi (q \u2212) ti\u03b1 (p +) hi (q \u2212 \u03b1) hi (q +) ti = \u03b1Li\u03b1Li + (1 \u2212 \u03b1).It follows that Pr (coin i is heavy | Di) \u2265 1 \u2212 \u03b4 if and only if Li \u0430 (1 \u2212 \u043c) ti + (1 \u2212 \u03b1) (p \u2212).The algorithm calculates the probability ratio Li for each coin \u2212 i based on the history of the coin results."}, {"heading": "4 Optimality of the Algorithm", "text": "Consider the probability of a coin that I have defined as Xi: = logLi. Given the history of a coin, the probability of a coin is clearly defined in the protocol. At the beginning, the history is empty and therefore all probabilities in the protocol are zero. The influence of a coin toss on the probability in the protocol is a random step for Xi - if the result of the coin toss is a head, then Xi \u2190 Xi + \u2206 H and if the result is a tail, then Xi \u2190 Xi \u2212 \u2206 T. Thus, the toss of the coin leads to a one-dimensional random exit of the probability function related to the coin. Since we stop toss with the probability of a coin, the probability is greater than B = protocol (1 \u2212 \u03b1). The toss of the coin leads to a one-dimensional random exit of the probability function related to the coin. We observe that the random exits executed by the coins are independent of each other, since each coin is heavy, independent of the rest of the cost system."}, {"heading": "4.1 Proof of Optimality", "text": "We now show that the class is a non-monotonically increasing function of the log-Likely system. (F) We show that the class is a non-monotonically increasing function of the log-Likely system. (F) We consider the Markov system S = (V, P, C, s, t) associated with the log-Likelihood function. Let X, Y-V so that X-Level Y in the first step is Sg (Y) and E-Level [Sg (Y)] = G-Level. We will define a mixed strategy for Sg (X) so that there is a purely optimal strategy that decides to play in the system S (X) in the first step Y. It follows from the definition that the pure strategy can be expressed in a (possibly infinite) decision tree."}, {"heading": "5 Number of Coin Tosses", "text": "In this section, we specify an upper limit on the number of coin flips performed by the Likelihood-Toss algorithm. < The algorithm repeatedly flips a coin while the coin's log probability is at least zero and begins with a fresh coin when the coin's log probability is less than zero. < The algorithm terminates when the coin's log probability is at least B. Consider the random walk of the log likelihood function. The random walk has absorption barriers at B and in any state less than 0, Lemma 9. Let C and D each consider the expected number of throws for a light and heavy coin as B. Let the probability that a heavy coin is absorbed at B. Then, under the assumptions of theorem 2,1,1p."}, {"heading": "6 Discussion", "text": "We have developed an adaptive strategy where the coins are tossed in order to achieve a certain stop condition, namely the existence of a coin whose rear probability of being heavy is at least a certain threshold. Our strategy has minimal cost, where the cost is measured by the expected number of future throws, by following the strategy to achieve the stop condition. We have achieved this by performing the best possible action after observing the result of each coin flip. We find that our algorithm can also be modified to proceed from any set result history by modifying the initialization step accordingly. The optimality of the action is demonstrated by means of tools from the field of Markov games. A major limitation of our algorithm is that it is optimal only when the coins are independently heavy and not heavy. It would be very interesting to design an adaptive strategy where the coins are not necessarily independent - let us say coins with a heavy coin have a fixed probability to achieve the stop, and that the stop is conditional on the coin being empty."}], "references": [{"title": "Best Arm Identification in Multi-Armed Bandits", "author": ["J.-Y. Audibert", "S. Bubeck", "R. Munos"], "venue": "In Proceedings of the Twenty-third Conference on Learning Theory, COLT", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Finite-time Analysis of the Multiarmed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "The Nonstochastic Multiarmed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM Journal of Computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "A Single-Sample Multiple Decision Procedure for Ranking Means of Normal Populations with known Variances", "author": ["R.E. Bechhofer"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1954}, {"title": "Design and Analysis of Experiments for Statistical Selection, Screening, and Multiple Comparisons", "author": ["R.E. Bechhofer", "T.J. Santner", "D.M. Goldsman"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Bandit Problems: Sequential Allocation of Experiments (Monographs on Statistics and Applied Probability)", "author": ["D.A. Berry", "B. Fristedt"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1985}, {"title": "Using Ranking and Selection to \u201cClean Up\u201d after Simulation Optimization", "author": ["J. Boesel", "B.L. Nelson", "S.-H. Kim"], "venue": "Operations Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Pure exploration in multi-armed bandits problems", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "In Proceedings of the 20th international conference on Algorithmic learning theory,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Economic Analysis of Simulation Selection Problems", "author": ["S.E. Chick", "N. Gans"], "venue": "Management Science,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "New Two-Stage and Sequential Procedures for Selecting the Best Simulated System", "author": ["S.E. Chick", "K. Inoue"], "venue": "Operations Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "The Max k-Armed Bandit: A New Model for Exploration Applied to Search Heuristic Selection", "author": ["V. Cicirello", "S. Smith"], "venue": "In 20th National Conference on Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "On Playing Golf with Two Balls", "author": ["I. Dumitriu", "P. Tetali", "P. Winkler"], "venue": "SIAM Journal of Discrete Mathematics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Bounds on Gambler\u2019s Ruin Probabilities in Terms of Moments", "author": ["S.N. Ethier", "D. Khoshnevisan"], "venue": "Methodology and Computing in Applied Probability,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "PAC Bounds for Multi-armed Bandit and Markov Decision Processes", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "In Proceedings of the 15th Annual Conference on Computational Learning Theory, COLT", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "A Knowledge-Gradient Policy for Sequential Information Collection", "author": ["P.I. Frazier", "W.B. Powell", "S. Dayanik"], "venue": "SIAM Journal on Control and Optimization,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Multi-Bandit Best Arm Identification", "author": ["V. Gabillon", "M. Ghavamzadeh", "A. Lazaric", "S. Bubeck"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Multi-armed Bandit Allocation Indices", "author": ["J. Gittins", "K. Glazebrook", "R. Weber"], "venue": "Wiley, 2nd edition,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Bayesian look ahead one-stage sampling allocations for selection of the best population", "author": ["S.S. Gupta", "K.J. Miescke"], "venue": "Journal of Statistical Planning and Inference,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1996}, {"title": "Selecting the best system", "author": ["S.H. Kim", "B.L. Nelson"], "venue": "Handbooks in Operations Research and Management Science: Simulation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Asymptotically Efficient Adaptive Allocation Rules", "author": ["T. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1985}, {"title": "The Sample Complexity of Exploration in the Multi-Armed Bandit Problem", "author": ["S. Mannor", "J.N. Tsitsiklis"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation", "author": ["O. Maron", "A. Moore"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1994}, {"title": "A Sequential Procedure for Selecting the Population with the Largest Mean from k Normal Populations", "author": ["E. Paulson"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1964}, {"title": "Selection-of-the-best procedures for optimization via simulation", "author": ["J. Pichitlamken", "B.L. Nelson"], "venue": "In Proceeding of the 2001 Winter Simulation Conference,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2001}], "referenceMentions": [{"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].", "startOffset": 121, "endOffset": 134}, {"referenceID": 20, "context": "This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].", "startOffset": 121, "endOffset": 134}, {"referenceID": 1, "context": "This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].", "startOffset": 121, "endOffset": 134}, {"referenceID": 2, "context": "This problem has spawned a rich literature on the trade off between exploration and exploitation while choosing the arms [6, 21, 2, 3].", "startOffset": 121, "endOffset": 134}, {"referenceID": 7, "context": "In the models considered in [8, 1, 17], the goal is to choose an arm after a finite number of steps to minimize regret.", "startOffset": 28, "endOffset": 38}, {"referenceID": 0, "context": "In the models considered in [8, 1, 17], the goal is to choose an arm after a finite number of steps to minimize regret.", "startOffset": 28, "endOffset": 38}, {"referenceID": 16, "context": "In the models considered in [8, 1, 17], the goal is to choose an arm after a finite number of steps to minimize regret.", "startOffset": 28, "endOffset": 38}, {"referenceID": 7, "context": "The work of [8] suggested that the exploration-exploitation trade offs for this setting are much different from the setting where the number of steps is asymptotic.", "startOffset": 12, "endOffset": 15}, {"referenceID": 0, "context": "[1] proposed exploration strategies to perform essentially as well as the best strategy that knows all distributions up to permutations of the arms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] addressed the problem of identifying the best arm for each bandit among a collection of bandits within a fixed budget.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "This is identical to racing and action elimination algorithms [23, 15, 14] which address the sample complexity of the pure exploration problem \u2013 given any \u03b4 > 0, identify the arm with maximum expected reward with error probability at most \u03b4 while minimizing the total number of steps needed.", "startOffset": 62, "endOffset": 74}, {"referenceID": 14, "context": "This is identical to racing and action elimination algorithms [23, 15, 14] which address the sample complexity of the pure exploration problem \u2013 given any \u03b4 > 0, identify the arm with maximum expected reward with error probability at most \u03b4 while minimizing the total number of steps needed.", "startOffset": 62, "endOffset": 74}, {"referenceID": 13, "context": "This is identical to racing and action elimination algorithms [23, 15, 14] which address the sample complexity of the pure exploration problem \u2013 given any \u03b4 > 0, identify the arm with maximum expected reward with error probability at most \u03b4 while minimizing the total number of steps needed.", "startOffset": 62, "endOffset": 74}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] showed that a total of O((n/ 2) log(1/\u03b4)) steps is sufficient to identify an arm whose expected reward is at most away from the optimal arm with correctness at least 1 \u2212 \u03b4.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "Mannor and Tsitsiklis [22] showed lower bounds matching up to constant factors under various settings of the rewards.", "startOffset": 22, "endOffset": 26}, {"referenceID": 13, "context": "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d.", "startOffset": 137, "endOffset": 159}, {"referenceID": 21, "context": "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d.", "startOffset": 137, "endOffset": 159}, {"referenceID": 10, "context": "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d.", "startOffset": 137, "endOffset": 159}, {"referenceID": 7, "context": "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d.", "startOffset": 137, "endOffset": 159}, {"referenceID": 0, "context": "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d.", "startOffset": 137, "endOffset": 159}, {"referenceID": 16, "context": "Although the PAC-style learning problem appears to have garnered the interest of the learning theory community only over the past decade [14, 22, 11, 8, 1, 17], it has been actively studied in the field of operations research for several decades as the \u201cranking and selection problem\u201d.", "startOffset": 137, "endOffset": 159}, {"referenceID": 3, "context": "It was introduced for normally distributed rewards by Bechhofer [4].", "startOffset": 64, "endOffset": 67}, {"referenceID": 23, "context": "Adaptive strategies for this problem, known as \u201csequential selection\u201d, can be traced back to Paulson [24].", "startOffset": 101, "endOffset": 105}, {"referenceID": 23, "context": "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].", "startOffset": 131, "endOffset": 149}, {"referenceID": 4, "context": "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].", "startOffset": 131, "endOffset": 149}, {"referenceID": 19, "context": "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].", "startOffset": 131, "endOffset": 149}, {"referenceID": 6, "context": "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].", "startOffset": 131, "endOffset": 149}, {"referenceID": 24, "context": "Variants of the problem find applications in minimizing the number of experimental simulations to achieve a given confidence level [24, 5, 20, 7, 25].", "startOffset": 131, "endOffset": 149}, {"referenceID": 3, "context": "This special case is known as the \u201cindifference-zone\u201d assumption [4].", "startOffset": 65, "endOffset": 68}, {"referenceID": 19, "context": "Strategies and their measure of optimality are known for various relaxations of independence, normality, equal and known variances and indifference-zone assumptions [20].", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9].", "startOffset": 115, "endOffset": 130}, {"referenceID": 9, "context": "In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9].", "startOffset": 115, "endOffset": 130}, {"referenceID": 15, "context": "In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9].", "startOffset": 115, "endOffset": 130}, {"referenceID": 8, "context": "In the Bayesian setting, the mean rewards of the normal distributions are chosen from some underlying distribution [19, 10, 16, 9].", "startOffset": 115, "endOffset": 130}, {"referenceID": 21, "context": "The implications of our upper bound when the number of coins is bounded but much larger than 1/\u03b1 needs to be contrasted with the lower bounds by [22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 21, "context": "In this case, setting n = c/\u03b1 in the above expression suggests that our algorithm beats the lower bound shown in Theorem 9 of [22].", "startOffset": 126, "endOffset": 130}, {"referenceID": 21, "context": "We observe that Theorem 9 of [22] shows a lower bound in the most general Bayesian setting \u2013 there exists a prior distribution of the probabilities of the n coins so that any algorithm requires at least O((n/ 2) log (1/\u03b4)) tosses in expectation.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "We use the notation and results from [12].", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "A Markov system S = (V, P,C, s, t) consists of a state space V , a transition probability function P : V \u00d7V \u2192 [0, 1], a positive real cost Cv associated with each state v, a start state s and a target state t.", "startOffset": 110, "endOffset": 116}, {"referenceID": 11, "context": "[12] Every Markov game has a pure optimal strategy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] Given the states u1, .", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": ", the number of possible transitions from any fixed state is finite), by working through the proofs in [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 12, "context": "We use the following results from [13] to bound the number of tosses.", "startOffset": 34, "endOffset": 38}, {"referenceID": 12, "context": "[13] Let X \u2208 [\u2212\u03bd, \u03bc] be the random variable that determines the step-sizes of a one dimensional random walk with absorbing barriers at \u2212L and W such that Pr (X > 0) > 0, Pr (X < 0) > 0, E (X) 6= 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "If the step in the system is a forward step (outcome of coin toss is a head), then \u03c0 generates a random number r \u2208 [0, 1] and moves the pointer to the node uHH if r < Pr (Heads|lY (u)) /Pr (Heads|lX(u)) and to the node uHT if r \u2265 Pr (Heads|lY (u)) /Pr (Heads|lX(u)).", "startOffset": 115, "endOffset": 121}], "year": 2013, "abstractText": "We study the problem of learning a most biased coin among a set of coins by tossing the coins adaptively. The goal is to minimize the number of tosses until we identify a coin i\u2217 whose posterior probability of being most biased is at least 1 \u2212 \u03b4 for a given \u03b4. Under a particular probabilistic model, we give an optimal algorithm, i.e., an algorithm that minimizes the expected number of future tosses. The problem is closely related to finding the best arm in the multi-armed bandit problem using adaptive strategies. Our algorithm employs an optimal adaptive strategy \u2013 a strategy that performs the best possible action at each step after observing the outcomes of all previous coin tosses. Consequently, our algorithm is also optimal for any starting history of outcomes. To our knowledge, this is the first algorithm that employs an optimal adaptive strategy under a Bayesian setting for this problem. Our proof of optimality employs tools from the field of Markov games.", "creator": "LaTeX with hyperref package"}}}