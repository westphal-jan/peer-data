{"id": "1202.3699", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Learning is planning: near Bayes-optimal reinforcement learning via Monte-Carlo tree search", "abstract": "Bayes-optimal behavior, while well-defined, is often difficult to achieve. Recent advances in the use of Monte-Carlo tree search (MCTS) have shown that it is possible to act near-optimally in Markov Decision Processes (MDPs) with very large or infinite state spaces. Bayes-optimal behavior in an unknown MDP is equivalent to optimal behavior in the known belief-space MDP, although the size of this belief-space MDP grows exponentially with the amount of history retained, and is potentially infinite. We show how an agent can use one particular MCTS algorithm, Forward Search Sparse Sampling (FSSS), in an efficient way to act nearly Bayes-optimally for all but a polynomial number of steps, assuming that FSSS can be used to act efficiently in any possible underlying MDP.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (307kb)", "http://arxiv.org/abs/1202.3699v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["john asmuth", "michael l littman"], "accepted": false, "id": "1202.3699"}, "pdf": {"name": "1202.3699.pdf", "metadata": {"source": "CRF", "title": "Learning is planning: near Bayes-optimal reinforcement learning via Monte-Carlo tree search", "authors": ["John Asmuth", "Michael Littman"], "emails": [], "sections": [{"heading": null, "text": "Bayes-optimal behavior is well defined, but often difficult to achieve. Recent advances in the use of the Monte Carlo Tree Search (MCTS) have shown that it is possible to act almost optimally in Markov decision-making processes (MDPs) with very large or infinite state spaces. Bayes-optimal behavior in an unknown MDP corresponds to optimal behavior in the known faith space MDP, although the size of that faith space MDP exponentially grows with the amount of history preserved and is potentially infinite. We show how an agent can use a particular MCTS algorithm, Forward Search Sparse Sampling (FSSS), to act almost Bayes-optimally for all but a polynomial number of steps, provided FSSS can be efficiently used in any possible underlying MDP."}, {"heading": "1 Introduction", "text": "In Reinforcement Learning (RL), a central theme is the exploration / exploitation trade-off (Sutton & Barto, 1998).Simply put, this dilemma relates to the balance between acting optimally according to the information you have (exploitation) and acting potentially suboptimally to improve the quality of your information (exploration).The classic approach to this topic takes into account the \"general\" model and makes its warranties (if any) about the behavior of an algorithm on any model that meets some limitations.A maximum probability is used with a promise that when the right exploration steps are taken, the resulting model is close to the turkey. This frequency-oriented approach to optimal behavior is effective in many scenarios, and has the added benefit that the MLE is often easy to calculate."}, {"heading": "2 Background", "text": "In RL, an agent is provided with an observation to which he responds with an action. In response to each action taken, the environment returns a new observation as well as a numerical reward signal. An MDP is the Tupel DP = < S, T, R, L, L, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S,"}, {"heading": "3 Related Work", "text": "The approach we propose in this paper is rooted in a number of existing algorithms that we have used as a reward, a way we have seen in recent years. There are few examples that use the faith and MDP formulation of Bayesian RL to achieve near-optimal behavior, using the FDM approach and the known properties of the value function to calculate approximation across all states. Bayesian Exploration Bonus (Kolter & Ng, 2009), or BEB, is a near-optimal algorithm for the FDM before and after it. It acts collectively on the maximum probability of MDP, but uses the alternative reward function RBEB (s, a) = R (s, a) + n (s, a), where it is a domain-dependent constant and n (s, a)."}, {"heading": "4 Bayesian FSSS", "text": "The contribution of Bayesian FSSS (BFS3) is the application of FSSS to a faith MDP and is outlined in Al-Input: state s, maximum depth d, current depth l, MDPM if terminal (s) thenUd (s) = > reward (s) = 0 return nif d = l then return if \"Visitedd\" (s) then \"Visitedd\" s \"s\" true foreach a \"A doRd (s, a), Countd (s, a, s\" reward \"), Childrend (s, a) DP = 0, {} for C times do s, r\" L \"(s, a), RM\" s \"(s, a) Countd\" s \"Countd\" (s \") + 1 Childrend (s, a), a\" Childrend \"(s, a) Childrend (s, a\" Childrend \"(s, a\" Childrend \"(s, a), a\" s, \"s,\" Rd \"(a)."}, {"heading": "5 Near Bayes-optimal behavior", "text": "In fact, the fact is that most of them will be able to be in a position to be in the position in which they are able to be in, to be in a position to be in, to be in a position to be in, and to be able to be in a position to be in a position to be in, to be in a position to be in, to be in a position to be in the position in which they are able to be in."}, {"heading": "5.1 Proof of Near Bayes-optimality", "text": "Next, we present an argument for why BFS3 near Bayesian has optimal behavior for discrete state and action room MDPs with known reward functions and known terminal states. In this context, sufficient statistics for the belief state are the pairing of the real state and next-state histograms for each state pair."}, {"heading": "5.1.1 Theorem statement", "text": "If: 1. FSSS, taking into account the true MDP m0 and some factors tied to computational resources, can provide accurate estimates with a high probability, and 2. The posterior next-state distribution for any state-action pair, in N examples, will be an accurate estimate of the next-state distribution of m0 with a high probability, then: with a high probability, BFS3 Bayes will behave optimally for all but a polynomial number of steps."}, {"heading": "5.1.2 Proof (sketch)", "text": "First, we will show that there is a faith MDP constructed from the previous period whose optimal policy is the Bayesian optimal policy for m0 \u0445 \u03c6. Then, we will show that BFS3 operating in the faith MDP will meet the three criteria required for PAC-MDP behavior (Cockade, 2003; Li, 2009) in this faith MDP. These criteria are: 1. Accuracy, 2. Limited discoveries, and 3. Optimism. First, because condition 2 in our theoretical statement, we know that once we have examples of transitions from a state action pair (s, a), our estimate of the next-state distribution for that pair will be accurate. (This condition does not have to be considered degenerate priors, but it seems to be quite broad.) Second, since we have forgotten all the additional transitions from the states for which we have seen N examples, the number of possible state histories that an agent can observe will increase."}, {"heading": "6 Experimentation", "text": "This flexibility in the use of different priorities is a compelling reason to use MCTS algorithms in general, and BFS3 in particular, when the use of FDM mainly affects model-based reinforcement. However, these algorithms are limited in that they require a known reward function; they can only deal with uncertainty in transitional functionality. BFS3, with the right approach, can deal with unknown rewards. In many domains, there are only a few possible reward values. For example, many pioneering domains give a reward of \u2212 1 for all actions. Or there is a reward for a particular outcome that can be achieved from multiple states: these states share the value of this common representation."}, {"heading": "7 Concluding Remarks", "text": "This year it has come to the point that it has never come as far as this year."}], "references": [{"title": "A Bayesian sampling approach to exploration in reinforcement learning", "author": ["J. Asmuth", "L. Li", "M. Littman", "A. Nouri", "D. Wingate"], "venue": "Proceedings of the 25th Conference on Uncertainty in Artifical Intelligence (UAI-09)", "citeRegEx": "Asmuth et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Asmuth et al\\.", "year": 2009}, {"title": "Appendix (Technical Report DCS-tr-687)", "author": ["J. Asmuth", "M. Littman"], "venue": "Rutgers University department of Computer Science", "citeRegEx": "Asmuth and Littman,? \\Q2011\\E", "shortCiteRegEx": "Asmuth and Littman", "year": 2011}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "R-MAX\u2014A general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman and Tennenholtz,? \\Q2002\\E", "shortCiteRegEx": "Brafman and Tennenholtz", "year": 2002}, {"title": "Design for an optimal probe", "author": ["M. Duff"], "venue": "Proceedings of the 20th International Conference on Machine Learning", "citeRegEx": "Duff,? \\Q2003\\E", "shortCiteRegEx": "Duff", "year": 2003}, {"title": "Local bandit approximation for optimal learning problems", "author": ["M.O. Duff", "A.G. Barto"], "venue": "Advances in Neural Information Processing Systems (pp. 1019\u20131025)", "citeRegEx": "Duff and Barto,? \\Q1997\\E", "shortCiteRegEx": "Duff and Barto", "year": 1997}, {"title": "Achieving master level play in 9x9 computer go", "author": ["S. Gelly", "D. Silver"], "venue": "Proceedings of the 23rd national conference on Artificial intelligence - Volume", "citeRegEx": "Gelly and Silver,? \\Q2008\\E", "shortCiteRegEx": "Gelly and Silver", "year": 2008}, {"title": "On the sample complexity of reinforcement learning", "author": ["S.M. Kakade"], "venue": "Doctoral dissertation,", "citeRegEx": "Kakade,? \\Q2003\\E", "shortCiteRegEx": "Kakade", "year": 2003}, {"title": "A sparse sampling algorithm for near-optimal planning in large Markov decision processes", "author": ["M. Kearns", "Y. Mansour", "A.Y. Ng"], "venue": "Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence", "citeRegEx": "Kearns et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 1999}, {"title": "Bandit based Monte-Carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "Proceedings of the 17th European Conference on Machine Learning", "citeRegEx": "Kocsis and Szepesv\u00e1ri,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesv\u00e1ri", "year": 2006}, {"title": "Near-Bayesian exploration in polynomial time", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning (pp. 513\u2013520)", "citeRegEx": "Kolter and Ng,? \\Q2009\\E", "shortCiteRegEx": "Kolter and Ng", "year": 2009}, {"title": "Efficient reinforcement learning with relocatable action models", "author": ["B.R. Leffler", "M.L. Littman", "T. Edmunds"], "venue": "Proceedings of the Twenty-Second Conference on Artificial Intelligence (AAAI-07)", "citeRegEx": "Leffler et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Leffler et al\\.", "year": 2007}, {"title": "A unifying framework for computational reinforcement learning theory (pp 78-79)", "author": ["L. Li"], "venue": null, "citeRegEx": "Li,? \\Q2009\\E", "shortCiteRegEx": "Li", "year": 2009}, {"title": "Estimating mixture of Dirichlet process models", "author": ["S.N. MacEachern", "P. Muller"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "MacEachern and Muller,? \\Q1998\\E", "shortCiteRegEx": "MacEachern and Muller", "year": 1998}, {"title": "Markov chain sampling methods for dirichlet process mixture models", "author": ["R.M. Neal"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Neal,? \\Q2000\\E", "shortCiteRegEx": "Neal", "year": 2000}, {"title": "An analytic solution to discrete Bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "Proceedings of the 23rd International Conference on Machine Learning (pp. 697\u2013704)", "citeRegEx": "Poupart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Markov decision processes\u2014discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Artificial intelligence: A modern approach", "author": ["S.J. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig,? \\Q1994\\E", "shortCiteRegEx": "Russell and Norvig", "year": 1994}, {"title": "Variance-based rewards for approximate Bayesian reinforcement learning", "author": ["J. Sorg", "S. Singh", "R.L. Lewis"], "venue": "Proceedings of the 26th Conference on Uncertainty in Artifical Intelligence (UAI-10)", "citeRegEx": "Sorg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sorg et al\\.", "year": 2010}, {"title": "Efficient structure learning in factored-state MDPs", "author": ["A.L. Strehl", "C. Diuk", "M.L. Littman"], "venue": "Proceedings of the Twenty-Second National Conference on Artificial Intelligence (AAAI-07)", "citeRegEx": "Strehl et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2007}, {"title": "An analysis of modelbased interval estimation for Markov decision processes", "author": ["A.L. Strehl", "M.L. Littman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Strehl and Littman,? \\Q2008\\E", "shortCiteRegEx": "Strehl and Littman", "year": 2008}, {"title": "A Bayesian framework for reinforcement learning", "author": ["M.J.A. Strens"], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning (ICML", "citeRegEx": "Strens,? \\Q2000\\E", "shortCiteRegEx": "Strens", "year": 2000}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Integrating samplebased planning and model-based reinforcement learning", "author": ["T. Walsh", "S. Goschin", "M. Littman"], "venue": "Proceedings of the Association for the Advancement of Artificial Intelligence", "citeRegEx": "Walsh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Walsh et al\\.", "year": 2010}, {"title": "Exploring compact reinforcement-learning representations with linear regression", "author": ["T.J. Walsh", "I. Szita", "C. Diuk", "M.L. Littman"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (pp. 591\u2013598)", "citeRegEx": "Walsh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Walsh et al\\.", "year": 2009}, {"title": "Bayesian sparse sampling for on-line reward optimization", "author": ["T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "ICML \u201905: Proceedings of the 22nd International Conference on Machine Learning (pp. 956\u2013963)", "citeRegEx": "Wang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2005}, {"title": "Multi-task reinforcement learning: A hierarchical Bayesian approach", "author": ["A. Wilson", "A. Fern", "S. Ray", "P. Tadepalli"], "venue": "Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML", "citeRegEx": "Wilson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 19, "context": "For example, if we know that the model dynamics can be treated as a separate problem for any one of a number of state features, we can more efficiently do factor learning (Strehl et al., 2007).", "startOffset": 171, "endOffset": 192}, {"referenceID": 11, "context": "Or, if we know that certain groups of states have identical dynamics, we can learn the group dynamics by using relocatable action models (Leffler et al., 2007).", "startOffset": 137, "endOffset": 159}, {"referenceID": 26, "context": "The Bayesian approach to model estimation in RL (Wilson et al., 2007) introduces the use of model priors, and much of the innovation required moves from algorithm design to prior engineering and inference.", "startOffset": 48, "endOffset": 69}, {"referenceID": 18, "context": "Near Bayes-optimal behavior, where an agent is approximately Bayes-optimal for all but a small number of steps, has been achieved for Dirichlet priors (Kolter & Ng, 2009) and some more general priors (Sorg et al., 2010).", "startOffset": 200, "endOffset": 219}, {"referenceID": 16, "context": "Formally, an environment is described as a Markov Decision Process (Puterman, 1994), or MDP.", "startOffset": 67, "endOffset": 83}, {"referenceID": 4, "context": "This is also called a Bayes-adaptive MDP (Duff, 2003).", "startOffset": 41, "endOffset": 53}, {"referenceID": 4, "context": "M P (s\u2032, r|s, a,M)\u03c6(M |h)dM , the optimal policy in m\u03c6|h corresponds to the Bayes-optimal policy, given the MDP prior \u03c6 (Duff, 2003).", "startOffset": 120, "endOffset": 132}, {"referenceID": 15, "context": "For example, the Flat-Dirichlet-Multinomial (Poupart et al., 2006), or FDM,", "startOffset": 44, "endOffset": 66}, {"referenceID": 16, "context": "The use of many standard planning techniques, such as value iteration or policy iteration (Sutton & Barto, 1998; Puterman, 1994), becomes impossible with an infinite horizon domain.", "startOffset": 90, "endOffset": 128}, {"referenceID": 15, "context": "Bayesian Exploration/Exploitation Trade-off in LEarning (Poupart et al., 2006), or BEETLE, is an algorithm that uses the belief-MDP formulation of Bayesian RL in order to achieve approximately Bayes-optimal behavior.", "startOffset": 56, "endOffset": 78}, {"referenceID": 7, "context": "This reward-supplement strategy is also used in PAC-MDP (Kakade, 2003) approaches (Strehl & Littman, 2008), and biases an agent toward states in which it has less experience.", "startOffset": 56, "endOffset": 70}, {"referenceID": 18, "context": "There are ways to use posterior variance to create reward bonuses for more general Bayesian priors (Sorg et al., 2010).", "startOffset": 99, "endOffset": 118}, {"referenceID": 21, "context": "Bayesian Dynamic Programming (Strens, 2000), or Bayesian-DP, and Best Of Sampled Set (Asmuth et al.", "startOffset": 29, "endOffset": 43}, {"referenceID": 0, "context": "Bayesian Dynamic Programming (Strens, 2000), or Bayesian-DP, and Best Of Sampled Set (Asmuth et al., 2009), or BOSS, are two examples of Bayesian RL algorithms that can work with a much more flexible prior.", "startOffset": 85, "endOffset": 106}, {"referenceID": 8, "context": "Sparse Sampling (Kearns et al., 1999) works by recursively expanding a full search tree up to a certain depth d.", "startOffset": 16, "endOffset": 37}, {"referenceID": 23, "context": "Sparse Sampling is the basis for a number of Monte-Carlo Tree Search (MCTS) algorithms, which are considerably faster in practice (Kocsis & Szepesv\u00e1ri, 2006; Walsh et al., 2010; Wang et al., 2005).", "startOffset": 130, "endOffset": 196}, {"referenceID": 25, "context": "Sparse Sampling is the basis for a number of Monte-Carlo Tree Search (MCTS) algorithms, which are considerably faster in practice (Kocsis & Szepesv\u00e1ri, 2006; Walsh et al., 2010; Wang et al., 2005).", "startOffset": 130, "endOffset": 196}, {"referenceID": 25, "context": "Bayesian Sparse Sampling (Wang et al., 2005) is a modification of Sparse Sampling that applies only in the Bayesian setting.", "startOffset": 25, "endOffset": 44}, {"referenceID": 2, "context": "In addition to the value estimates for each node, which are computed by running backups backwards along trajectories, UCT maintains upper confidence bounds using the UCB algorithm (Auer et al., 2002).", "startOffset": 180, "endOffset": 199}, {"referenceID": 23, "context": "Forward Search Sparse Sampling (Walsh et al., 2010), or FSSS, is the approach we adopt in this paper.", "startOffset": 31, "endOffset": 51}, {"referenceID": 25, "context": "An agent achieves Bayes-optimal behavior (Wang et al., 2005; Duff & Barto, 1997) if it operates according to a policy that maximizes expected discounted cumulative reward with respect to some MDP prior \u03c6.", "startOffset": 41, "endOffset": 80}, {"referenceID": 12, "context": "In the worst case (combination lock (Li, 2009)), t must be equal to the number of possible combinations, or A.", "startOffset": 36, "endOffset": 46}, {"referenceID": 7, "context": "Then, we will show that BFS3, acting in the belief-MDP, will satisfy the three criteria required for PAC-MDP behavior (Kakade, 2003; Li, 2009) in that belief-MDP3.", "startOffset": 118, "endOffset": 142}, {"referenceID": 12, "context": "Then, we will show that BFS3, acting in the belief-MDP, will satisfy the three criteria required for PAC-MDP behavior (Kakade, 2003; Li, 2009) in that belief-MDP3.", "startOffset": 118, "endOffset": 142}, {"referenceID": 24, "context": "The Paint/Polish world (Walsh et al., 2009) provides a situation where the simple and convenient FDM prior is insufficient.", "startOffset": 23, "endOffset": 43}, {"referenceID": 18, "context": "We replicate the dynamics presented in detail by Sorg et al. (2010).", "startOffset": 49, "endOffset": 68}, {"referenceID": 18, "context": "We compare this to a variance-based reward bonus strategy (Sorg et al., 2010) which, when tuned, averaged 0.", "startOffset": 58, "endOffset": 77}, {"referenceID": 14, "context": "The aforementioned Cluster prior is difficult to use with MCTS strategies because sampling from its posterior involves Markov chain MonteCarlo techniques (Neal, 2000), and requires a great deal of computation.", "startOffset": 154, "endOffset": 166}, {"referenceID": 0, "context": "BOSS (Asmuth et al., 2009) can make use of flexible priors, and uses sample redundancy and posterior variance to ensure constant optimism, and is PAC-MDP.", "startOffset": 5, "endOffset": 26}, {"referenceID": 18, "context": "Like BOSS, the variance-based reward bonus approach (Sorg et al., 2010) draws upon posterior variance of any prior to encourage exploration to unknown states, but does so in a different way that gives it near Bayes-optimality.", "startOffset": 52, "endOffset": 71}], "year": 2011, "abstractText": "Bayes-optimal behavior, while well-defined, is often difficult to achieve. Recent advances in the use of Monte-Carlo tree search (MCTS) have shown that it is possible to act near-optimally in Markov Decision Processes (MDPs) with very large or infinite state spaces. Bayes-optimal behavior in an unknown MDP is equivalent to optimal behavior in the known belief-space MDP, although the size of this belief-space MDP grows exponentially with the amount of history retained, and is potentially infinite. We show how an agent can use one particular MCTS algorithm, Forward Search Sparse Sampling (FSSS), in an efficient way to act nearly Bayes-optimally for all but a polynomial number of steps, assuming that FSSS can be used to act efficiently in any possible underlying MDP.", "creator": "TeX"}}}