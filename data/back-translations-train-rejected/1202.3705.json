{"id": "1202.3705", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Filtered Fictitious Play for Perturbed Observation Potential Games and Decentralised POMDPs", "abstract": "Potential games and decentralised partially observable MDPs (Dec-POMDPs) are two commonly used models of multi-agent interaction, for static optimisation and sequential decisionmaking settings, respectively. In this paper we introduce filtered fictitious play for solving repeated potential games in which each player's observations of others' actions are perturbed by random noise, and use this algorithm to construct an online learning method for solving Dec-POMDPs. Specifically, we prove that noise in observations prevents standard fictitious play from converging to Nash equilibrium in potential games, which also makes fictitious play impractical for solving Dec-POMDPs. To combat this, we derive filtered fictitious play, and provide conditions under which it converges to a Nash equilibrium in potential games with noisy observations. We then use filtered fictitious play to construct a solver for Dec-POMDPs, and demonstrate our new algorithm's performance in a box pushing problem. Our results show that we consistently outperform the state-of-the-art Dec-POMDP solver by an average of 100% across the range of noise in the observation function.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (121kb)", "http://arxiv.org/abs/1202.3705v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["archie c chapman", "simon a williamson", "nicholas r jennings"], "accepted": false, "id": "1202.3705"}, "pdf": {"name": "1202.3705.pdf", "metadata": {"source": "CRF", "title": "Filtered Fictitious Play for Perturbed Observation Potential Games and Decentralised POMDPs", "authors": ["Archie C. Chapman", "Simon A. Williamson", "Nicholas R. Jennings"], "emails": [], "sections": [{"heading": null, "text": "Potential games and decentralized, partially observable MDPs (Dec-POMDPs) are two commonly used models of multi-agent interaction for static optimization and sequential decision-making. In this article, we present a filtered fictional game for solving repeated potential games in which the observations of other players are disturbed by random noise, and use this algorithm to construct an online learning method for solving Dec-POMDPs. Specifically, we demonstrate that noise in observations prevents the standard fictive game from approaching the Nash equilibrium in potential games with loud observations, making fictitious games impractical for solving Dec-POMDPs as well. To combat this, we derive filtered fictitious games and provide conditions under which it converges to a Nash equilibrium in potential games with loud observations."}, {"heading": "1 Introduction", "text": "This year, it has come to the point that there will only be one such process, in which the question is to what extent it is a country, in which it is a country, in which it is a country."}, {"heading": "2 Preliminaries", "text": "This section first describes uncooperative games, potential games, and the concept of p-dominant NE. We then present GWFP processes and show how they are analyzed using stochastic approximations and differential inclusions."}, {"heading": "2.1 Noncooperative Games and Potential Games", "text": "We consider repeated play of a finite non-cooperative game (a) (a) (a) i) i) i) i) i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\""}, {"heading": "2.2 p\u2013Dominant Equilibria", "text": "The p-dominance criterion is a stability concept. Specifically, a strict pure strategy is NE - dominant if the action of each actor is the best reaction to a common mixed strategy that sets at least the probability p on the other actors that play their pure strategies in balance. Formally, the action of each actor must be the best reaction to a common mixed profile that sets at least p on the pure balance profile. Definitively: b) p - i, a - i - i: (a - i) p: (a - i) p: (a - i) p) as a set of common profiles of i opponents that set at least the probability p on a certain balance profile. Definitively: b - p, a pure NE element is called p-dominant if the following dominance applies to all other p-i, a - i - i: a common element p on a certain balance profile a certain balance profile a certain balance profile a."}, {"heading": "2.3 Generalised Weakened Fictitious Play", "text": "Similarly, we use these tools to analyze GWFP processes and FFP in the presence of interference noises. To begin, we describe the classic fictional game process. Let's leave the historical frequency of Agent i's reactions to be the same as an indicator function if an \"i\" is the action played by i in due course and is otherwise defined as zero. We write \u03c3ti = \"i\" (\"i\") i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \".\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i.\" i. \"\" i. \"\" i. \""}, {"heading": "3 Games with Perturbed Observations", "text": "In this section we present a model of disturbed observations in repeated games. Now, we assume that some actors in the learning process also assume that when players observe the actions of their opponents, they do so without disturbing them. A more realistic scenario is that action observations involve a certain degree of disturbance: This could happen, for example, because noisy sensors are used to detect the actions of other agents, or agents report their actions with a loud communication medium. After defining the environment, we examine how the assumption of perfect observations affects fictional game processes. In detail, although we could use any model of observation noise (e.g. Gaussian), we choose the following model noisy action observations observations: Definition 1 Game with perturbed action observations Observations is a game in which the joint action a is played, in which each agent i displays the observation of each individual agent in a j \u2212 i with a probability 1 \u2212 and another randomly generated action with a probability."}, {"heading": "4 Filtered Fictitious Play", "text": "In this section, we present a filtered fictitious game (FFP) divided into three parts. In the first part, we derive a general method for integrating a noise filter into the verification process of the GWFP. This method allows us to analyze the trajectories of each FFP process using the same techniques as the GWFP. As a concrete example of an FFP process, we derive a specific version of FFP that applies a Bayes filter to noise in action, which is suitable for environments in which the noise production process is known to the actors. In the second part of the section, we assume that the actors have access to a noise filter process of limited precision, and present our main result: FFP converges with each p-dominant NE for p up to (1 \u2212 \u03b7) N \u2212 1. A logical consequence is that if the noise production process is known to the actors, a Bayes filter gives accurate estimates of the frequency of the GFP to a specific class \u2212 1 when the GFP is configured to a specific frequency of the GFP."}, {"heading": "4.1 Filtered Belief Revision", "text": "FFP uses a filter to remove the noise in the observations while we are working within the general analytical framework provided by GWFP. As such, we can prove its convergence in many games with disturbed action observations. To understand why this is necessary, we see that from (6), in the standard GWFP processes, a disturbed observation changes the belief in the action of an actor in the following ways: (a), (a), (a), (a), (a), (a), (b), (b), (a), (a), (a), (a), (c), (a), (a), (b), (a), (a), (a), (a), (a), (a), (a), (b), (b), (b), (b), (b), (b), (b), (b), (c), (c, (c), (c), (c, c, c, c, (c), (a), (a), (a), (a), (a), (a), (a), (c, c (b), c, c (c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c), c, c, c, c, c, c, c, c, c, c, c (c), c, c, c, c, c), c, c, c, c, c, c, c, c (c, c, c, c, c), c, c (c, c, c, c), c, c, c, c, c (c, c, c, c), (c, c, c, c), c (c, c, c, c, c), (c, c, c (c, c), c (c, c, c), (c), c), (c, (c, c), (c), c), (c), (c), (c), (c, c,"}, {"heading": "4.2 Convergence to Nash Equilibrium of FFP", "text": "We now prove that in many games where FFP observations prevent the convergence of standard GWFP processes, the FFP process defined in (8) converges to strict NE. The result depends on proving that the filtered faith actualization process is closer to the \"true\" faith, which corresponds to a p-dominated NE than a standard GWFP process, and uses the definition of precision in (9).5C.f. Several authors have used dirichlet processes to update distributions across mixed strategies to detect mixed NE convergence in games with perfectly observable actions, but to no avail (see e.g. [6, 15]."}, {"heading": "4.3 Numerical Demonstration", "text": "We will now show the non-convergence of the GWFP processes in the simple drone surveillance game with disruptive action observations as described in Example 1, and also show how FFP processes converge in the same area. We do this to demonstrate both the fragility of the GWFP in the presence of noise and the robustness of FFP in the same sound. This evaluation provides a concrete demonstration of the previous two theoretical outcomes, shown in Figure 1, with error bars showing two standard errors. GWFP initially converges almost all the time with the NE (% N > 96), but its performance quickly drops to 0. This is because it cannot learn an accurate mixed strategy for the other actor, seeing very loud observations and assuming that this is the mixed strategy being played (% N > 96). In contrast, FFP is much more resistant to noise and remains unaffected until it reaches a value of 0.4."}, {"heading": "5 Sequential Decision\u2013Making", "text": "Now that we have proven the convergence of FFP in one-state games with disrupted action observations, we can use FFP as part of an algorithm to solve dec POMDPs (i.e. team POSGs), focusing on the specific subclass in which agents know the usefulness and transition functions in advance, the problem being to learn the best action in each state and also follow exactly what stage play is being played. In the first part of this section, we derive an algorithm that uses FFP for this problem, and in the second part, we demonstrate its usefulness in a standard dec POMDP, namely cooperative box-pushing."}, {"heading": "5.1 FFP\u2013based Lookahead Search Algorithm: LFFP", "text": "Our algorithm, LFFP, is designed for POSGs, where agents jointly optimize a potential function, and where the actions of agents are the only cause of transitions between stage games. (In the case of a team game, where agents directly optimize a potential function given by the global target function, this class of POSGs must correspond to a subclass of Dec POMDPs with action-dependent status transitions. (In the case of these games, tracking the current stage game is very important, as the rewards for various actions are affected, and in addition, agents must be able to predict state transitions to achieve the optimal reward.) A POSG formally consists of a finite series of stage games, S = {1,.) in which each stage game is a non-coop-erative game, as described in Section 2.1, with actions Asi and Utilities."}, {"heading": "5.2 Numerical Evaluation of LFFP", "text": "In this section, we evaluate the performance of LFFP for POSGs in the canonical cooperative box that pushes the problem [17]. This problem consists of a network world that contains two agents, two small boxes and a large box. Agents can independently push the small boxes into the target to get a small reward, but the highest reward is when both agents together push the big box to the target, which neither of them can do on their own. Agents are punished for taking too long or hitting each other or the walls of the world. Agents can take a step forward, turn left or right or remain silent. Finally, they can only see the state of the other agent when he is nearby. The game consists of 100 states, 4 actions and 5 observations, which makes it a challenging problem. Specifically, agents are not provided with the stage play they are playing, and further on to this, the only positive reward in the game is that they push the boxes into the target area."}, {"heading": "6 Conclusions", "text": "In this paper, we demonstrated that iterative learning, which is computationally cheap, can be applied to static optimization and sequential decision-making with disturbances in action observation. Specifically, we have introduced filtered fictional game that can learn NE in potential games when action observation is disturbed by noise. As such, it is suitable for distributed optimization problems with loud observations of actions or communication channels. We have shown that standard fictional game processes generally do not converge with disturbing action observation, and have demonstrated the advantages of the FFP over the GWFP in a loud coordination game. Subsequently, we have used FFP to derive a solution approach, LFFP, for Dec-POMDPs, where state transitions are a function of action and uncertainty arises as a result of noise in action observation."}], "references": [{"title": "Optimizing fixed-size stochastic controllers for POMDPs and decentralized POMDPs", "author": ["C. Amato", "D.S. Bernstein", "S. Zilberstein"], "venue": "J. Autonomous Agents and Multi\u2013Agent Systems, 21(3):293\u2013320", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-agent reinforcement learning in common interest and fixed sum stochastic games: An experimental study", "author": ["A. Bab", "R.I. Brafman"], "venue": "JMLR, 9:2635\u2013 2675", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Stochastic approximation and differential inclusions", "author": ["M. Bena\u0131\u0308m", "J. Hofbauer", "S. Sorin"], "venue": "SIAM J. Control and Optimisation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["D.S. Bernstein", "S. Zilberstein", "N. Immerman"], "venue": "Proc. of UAI \u201900, pages 32\u201337, San Francisco, CA, USA", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Iterative solution of games by fictitious play", "author": ["G.W. Brown"], "venue": "T. C. Koopmans, editor, Activity Analysis of Production and Allocation, pages 374\u2013376. New York", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1951}, {"title": "The Theory of Learning in Games", "author": ["D. Fudenberg", "D.K. Levine"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Extensive games and the problem of information", "author": ["H.W. Kuhn"], "venue": "Annals of Mathematics Studies, 28", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1953}, {"title": "Individual Q-learning in normal form games", "author": ["D.S. Leslie", "E.J. Collins"], "venue": "SIAM J. Control and Optimization, 44:495\u2013514", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Generalised weakened fictitious play", "author": ["D.S. Leslie", "E.J. Collins"], "venue": "Games and Economic Behavior, 56:285\u2013298", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Value-function reinforcement learning in Markov games", "author": ["M.L. Littman"], "venue": "Cognitive Systems Research, 2(1):55\u201366", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Distributed welfare games with applications to sensor coverage", "author": ["J.R. Marden", "A. Wierman"], "venue": "Proc. of CDC \u201908), pages 1708\u20131713", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Potential games", "author": ["D. Monderer", "L.S. Shapley"], "venue": "Games and Economic Behavior, 14:124\u2013143", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1996}, {"title": "p\u2013dominance and belief potential", "author": ["S. Morris", "R. Rob", "H.S. Shin"], "venue": "Econometrica, 63:145\u2013157", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1995}, {"title": "The complexity of Markov decision processes", "author": ["C. Papadimitriou", "J.N. Tsitsiklis"], "venue": "Math. Oper. Res., 12:441\u2013450", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1987}, {"title": "On similarities between inference in game theory and machine learning", "author": ["I. Rezek", "S. Reece", "S.J. Roberts", "A. Rogers", "R.K. Dash", "N.R. Jennings", "D.S. Leslie"], "venue": "JAIR, 33:259\u2013283", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Reasoning about joint beliefs for execution\u2013time communication decisions", "author": ["M. Roth", "R. Simmons", "M. Veloso"], "venue": "Proc. of AAMAS \u201905, pages 786\u2013 793, Utrect, Netherlands", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Memory-bounded dynamic programming for DEC-POMDP", "author": ["S. Seuken", "S. Zilberstein"], "venue": "Proc. of UAI \u201907, pages 2009\u20132015, Vancouver, Canada", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Reinforcement learning to play an optimal Nash equilibrium in team Markov games", "author": ["X. Wang", "T. Sandholm"], "venue": "Advances in Neural Information Processing Systems 15, pages 1571\u20131578. MIT Press, Cambridge, MA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Reward shaping for valuing communications during multi-agent coordination", "author": ["S.A. Williamson", "E.H. Gerding", "N.R. Jennings"], "venue": "Proc. of AAMAS \u201909, pages 641\u2013648, Budapest, Hungary", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Collective intelligence", "author": ["D.H. Wolpert", "K. Tumer"], "venue": "data routing and Braess\u2019 paradox. JAIR, 16:359\u2013387", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Point\u2013based policy generation for decentralized POMDPs", "author": ["F. Wu", "S. Zilberstein", "X. Chen"], "venue": "Proc. of AAMAS \u201910, pages 1307\u20131314, Toronto, Canada", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "[11]), and one general method of optimisation is for each agent to iteratively choose a better reply to the actions of the others, given its beliefs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "This paper focuses on one particular iterative algorithm, fictitious play [5], and its application to controlling teams of agents in single state and sequential decision\u2013making problems.", "startOffset": 74, "endOffset": 77}, {"referenceID": 11, "context": "Against this background, potential games [12] are an important class of games that can be used as a design template for agent\u2013based distributed optimisation problems.", "startOffset": 41, "endOffset": 45}, {"referenceID": 17, "context": "This class contains as special cases several important models of multi\u2013agent interaction, including: team or common\u2013 interest games [18], where players have the same utility function, and; marginal contribution games [11], where the players are each rewarded with their marginal contribution to a global target function.", "startOffset": 132, "endOffset": 136}, {"referenceID": 10, "context": "This class contains as special cases several important models of multi\u2013agent interaction, including: team or common\u2013 interest games [18], where players have the same utility function, and; marginal contribution games [11], where the players are each rewarded with their marginal contribution to a global target function.", "startOffset": 217, "endOffset": 221}, {"referenceID": 19, "context": "That is, payoffs are derived so that a player\u2019s payoff for changing strategy increases if and only if the global reward also increases (as in [20]).", "startOffset": 142, "endOffset": 146}, {"referenceID": 6, "context": "Building on these results, we then move on to consider partially observable stochastic games [7].", "startOffset": 93, "endOffset": 96}, {"referenceID": 9, "context": "2 One prominent example of this is called a team Markov game [10], which has as a potential function the team objective function that is to be optimised by the agents.", "startOffset": 61, "endOffset": 65}, {"referenceID": 17, "context": "As such, fictitious play could be used to solve these games, and indeed, a version of a related algorithm called adaptive play was used by [18] to solve team Markov games.", "startOffset": 139, "endOffset": 143}, {"referenceID": 1, "context": "Related work: Several researchers have investigated reinforcement learning techniques for computing NE in stochastic games, and general\u2013purpose algorithms such as R\u2013max, OAL and WoLF have been derived (for a review and empirical comparison, see [2]).", "startOffset": 245, "endOffset": 248}, {"referenceID": 0, "context": "[1], or the current state of the art, Point Based Policy Generation, PBPG [21]), but these approaches are approximate and not proven to converge to a NE.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[1], or the current state of the art, Point Based Policy Generation, PBPG [21]), but these approaches are approximate and not proven to converge to a NE.", "startOffset": 74, "endOffset": 78}, {"referenceID": 3, "context": "This is because, in general, solving POSGs offline for a decentralised policy is a NEXP\u2013complete problem [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 13, "context": "Now, there are several possible ways to combat this problem, such as finding a centralised policy offline (PSPACE) and executing it online using a communication protocol [14].", "startOffset": 170, "endOffset": 174}, {"referenceID": 15, "context": "In response, approaches such as [16] use heuristic communication protocols during execution, however they have no performance guarantees.", "startOffset": 32, "endOffset": 36}, {"referenceID": 18, "context": "[19]), which are scalable as the entire problem is not solved, but these approaches also are not guaranteed to find a NE.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Finally, we could have chosen to ignore the other agents entirely by treating them as a noisy part of the environment, and then apply standard reinforcement learning techniques, as in [8].", "startOffset": 184, "endOffset": 187}, {"referenceID": 11, "context": "The class of potential games is characterised as those games that admit a function specifying the participants\u2019 joint preference over outcomes [12], known as a potential function.", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "We make use of the concept of p\u2013dominance [13] to explain the effect of perturbed action observations on GWFP", "startOffset": 42, "endOffset": 46}, {"referenceID": 2, "context": "In [3], the convergence properties of this recursion are analysed using the theory of stochastic approximation of differential inclusions.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "Building on this, the class of generalised weakened fictitious play (GWFP) processes are characterised in [9].", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "Furthermore, under the assumption that \u03b4 \u2192 0, the stationary points of this differential inclusion are limit points of the difference process, which correspond to a subset of NE to which any GWFP process converges (as shown in [9]).", "startOffset": 227, "endOffset": 230}, {"referenceID": 5, "context": "[6, 15]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 14, "context": "[6, 15]).", "startOffset": 0, "endOffset": 7}, {"referenceID": 18, "context": "LFFP is an online lookahead search algorithm (similar to [19]) that uses the likelihood\u2019s of other agents\u2019 actions to track the current game being played, and searches into the future for a limited horizon.", "startOffset": 57, "endOffset": 61}, {"referenceID": 9, "context": "6The NashQ algorithm [10] uses a similar bias towards coordinated equilibria, which are those NE that maximise all agents\u2019 rewards.", "startOffset": 21, "endOffset": 25}, {"referenceID": 16, "context": "In this section, we evaluate the performance of LFFP for POSGs in the canonical cooperative box pushing problem [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "In contrast, LFFP does scale, since it effectively regards the other agents as a single entity and learns a best response to their joint action, and furthermore, avoids estimating the joint history such as PBPG [21].", "startOffset": 211, "endOffset": 215}], "year": 2011, "abstractText": "Potential games and decentralised partially observable MDPs (Dec\u2013POMDPs) are two commonly used models of multi\u2013agent interaction, for static optimisation and sequential decision\u2013 making settings, respectively. In this paper we introduce filtered fictitious play for solving repeated potential games in which each player\u2019s observations of others\u2019 actions are perturbed by random noise, and use this algorithm to construct an online learning method for solving Dec\u2013POMDPs. Specifically, we prove that noise in observations prevents standard fictitious play from converging to Nash equilibrium in potential games, which also makes fictitious play impractical for solving Dec\u2013POMDPs. To combat this, we derive filtered fictitious play, and provide conditions under which it converges to a Nash equilibrium in potential games with noisy observations. We then use filtered fictitious play to construct a solver for Dec\u2013POMDPs, and demonstrate our new algorithm\u2019s performance in a box pushing problem. Our results show that we consistently outperform the state\u2013of\u2013the\u2013art Dec\u2013 POMDP solver by an average of 100% across the range of noise in the observation function.", "creator": "gnuplot 4.4 patchlevel 0"}}}