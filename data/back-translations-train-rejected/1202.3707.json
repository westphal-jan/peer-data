{"id": "1202.3707", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "A temporally abstracted Viterbi algorithm", "abstract": "Hierarchical problem abstraction, when applicable, may offer exponential reductions in computational complexity. Previous work on coarse-to-fine dynamic programming (CFDP) has demonstrated this possibility using state abstraction to speed up the Viterbi algorithm. In this paper, we show how to apply temporal abstraction to the Viterbi problem. Our algorithm uses bounds derived from analysis of coarse timescales to prune large parts of the state trellis at finer timescales. We demonstrate improvements of several orders of magnitude over the standard Viterbi algorithm, as well as significant speedups over CFDP, for problems whose state variables evolve at widely differing rates.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (730kb)", "http://arxiv.org/abs/1202.3707v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["shaunak chatterjee", "stuart russell"], "accepted": false, "id": "1202.3707"}, "pdf": {"name": "1202.3707.pdf", "metadata": {"source": "CRF", "title": "A temporally abstracted Viterbi algorithm", "authors": ["Shaunak Chatterjee"], "emails": ["shaunakc@cs.berkeley.edu", "russell@cs.berkeley.edu"], "sections": [{"heading": "1 Introduction", "text": "The Viterbi algorithm is a minimal cost path through a state time measurement (Forney, 1973) finds the most likely sequence of hidden states, known as the \"Viterbi path,\" which is based on a sequence of observations in a hidden Markov model (HMM). If the HMM actually has N states and the sequence of lengths is T, there are NT possible state sequences, but because it uses dynamic programming (DP), the time complexity of the Viterbi algorithm is only O (N2T). It is one of the most important and basic algorithms in the entire field of information technology; its original application was in signal decoding, but has since been used in numerous other applications, including speech recognition (Rabiner, 1989), language parsing (Klein and Manning, 2003), and bioinformatics (Lytynoja and Milinkovitch, 2003).Finding a most probable state sequence to find in a HM is a minimal path."}, {"heading": "2 Problem Formulation", "text": "Consider a hidden Markov model (HMM), whose latent Markovian state X = natural, in one of N discrete states {1, 2,., N}. Let the actual state at the time t be denoted by Xt. The transition matrix A = {aij: i, j = 1, 2,., N} defines the state transition probabilities in which aij = p (Xt + 1 = j | Xt = i). The Markov chain is assumed to be stationary, so that aij is independent of t. Let the discrete observation space be a set {1, 2,., M}. Let Yt be the observation symbol at the time t. The observation matrix B = {bik: i = 1, 2., N; k = 1, 2., M} defines the emission probabilities in which bik = p (Yt = k)."}, {"heading": "3 Main algorithm", "text": "The distinguishing feature of TAV is its ability to argue with temporally abstract links. However, a link in the Viterbi algorithm and CFDP-like approaches describes the probability of transition between the states in a single step of time. However, a temporally abstract link that begins in state s1 to time t1 and ends in state s2 represents all links to these endpoints and is denoted by a quadruple - (s1, t1), (s2, t2). Links (s, t)) is the set of incoming links to state s to time t2 > t1 Children (sl) = {s \u2032 Sl \u2212 1, \u03c6 Sl \u2212 sl} = sl} is the set of children of the state sl in the abstraction hierarchy. We define three different types of temporally abstract links: 1. Direct links: d (s, t1, t2, t2) represents the linkages of the tractorial links."}, {"heading": "3.1 Refinement constructions", "text": "A refinement of an abstract temporal link replaces the original link with a series of refined links that represent a division - a mutually exclusive and exhaustive decomposition - of the link represented by the original link. This refinement enables us to think separately about subsets of the original link, potentially limiting ourselves to a single optimal orbit. There are two different types of refinement constructions."}, {"heading": "3.1.1 Spatial refinement", "text": "If a direct link, d (sl, t1, t2), is on the optimal path, the natural thing is to refine (partition) the paths it represents; the original direct connection is replaced by all possible lateral, direct and reentry connections between children (sl) at t1 and t2; this is shown in Figure 3. A link is also spatially refined if its time span (t2 \u2212 t1) is 1 time step, since temporal refinement is not possible; the pseudo code for spatial refinement (see algorithm 1) returns all the necessary details; it is trivial to show that the new connections represent a partition of the paths represented by the original link (pt2); algorithm 1 spatial refinement (((p1, t1, t1, t1, p2, t2) links (p2, t2) children (p2), t1 > 1 links (pt1, 1, 1, pt2, 2, 2, tlinks), 2, 2, ptlinks (pt2), 2, 2, 2, ptlinks (pt2), 2, 2, 2, 2, pt2, 2, pt2, 2, 2, pt2, 2, 2, 2, pt2, 2, 2, pt2, 2, 2, pt2, 2, 2, pt2, 2, 2, 2, pt2, 2, pt2, 2, 2, 2, pt2, 2, 2, pt2, 2, pt2, 2, 2, 2, pt2, 2, 2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2), pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2, pt2"}, {"heading": "3.1.2 Temporal refinement", "text": "If a combination (s1, t1), (s2, t2), (s2), (s1, t2), (s1, t2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), s2, (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), (s2), s2), (s2), (s2), (s2), (s2), s2), (s2), s2), (s2), (s2), s2, s2, (s2), s2), s2, (s2), s2), s2, s2, s2, s2, s2, s2, s2, s2, s2, (), s2, s2), (s2, s2), s2, s2, (, s2), s2, (s2), s2, s2), s2, s2, s2, s2, s2, s2, s2, s2, s2 (), s2), s2, s2, s2"}, {"heading": "3.2 Modified Viterbi algorithm", "text": "It is possible to have links to a state where there is a slightly modified scoring and traceability phase. usedT imes is a sorted list of time steps that have links to or from it. usedStates (t) is the composition of nodes at a time when the information arrives or is sorted out at a time. usedStates (t) is the composition of nodes at a time when there is incoming or out algorithm 3 BestPath, usedStates 3 BestPath, usedStates, usedT imes 1, while curT ime < T do curT imes curime (curT ime, usedT imes) for all UsedStates."}, {"heading": "3.3 Complete algorithm", "text": "The algorithmic structure of TAV and CFDP is quite similar; the complete specification of TAV is represented in Algorithm 4. Both initializations are shown in Figure 5 (a).The initial configuration of CFDP has no temporally abstract associations. The algorithm iterates between two stages: Calculation of the optimal path in the current graph and refinement of associations along the current optimal path. A few steps of executing the two algorithms are shown in an example in Figure 5. Algorithm 4 TAV (A, B, E, E, E, Y1: T) and refinement of associations along the current optimal path."}, {"heading": "4 Heuristics for temporal abstraction", "text": "In hierarchical state abstraction programs, the calculation of heuristics requires the maximum of a set of single-step transition probabilities. As mentioned in Section 2, this can be achieved by the hierarchical construction of Al, Bl and \u0394l. However, for temporal abstractions, there are more design decisions to be made when calculating the heuristic values of linkages. Thus, there is a more significant compromise between the cost of the calculation and the quality of heuristics. The heuristic score of a direct relationship is very easy to calculate. We do not have to choose between possible state transitions. Therefore, the heuristic calculation of a relationship over the interval (t1, t2) can be performed in O (t2 \u2212 t1), as we still have to take into account all observations in that interval. If the score for any state transition is cracked, the score is calculable for any subinterval (1)."}, {"heading": "5 Experiments", "text": "The simulations we carried out were aimed at demonstrating the benefits of TAV over Viterbi and CFDP. The benefits are magnified in systems where variables evolve in very different time scales; the timeframe of a random variable is the expected number of time steps in which it changes its state; a person's continental location would have a very large timeframe, while their postcode location would have a much smaller timeframe; a natural way to generate transition matrices with time separation is to use a dynamic Bayesian network (DBN); we consider a DBN with n variables, each of which has a cardinality of k; hence the size of the state space N kn. We used fully networked DBNs in our simulations; our observation matrix was multimodal (hence somewhat informative); a DBN with one parameter means that the timeframes of successive variables have a ratio of 1 to 1, which is the slowest in terms of time frames."}, {"heading": "5.1 Varying T, N and", "text": "To investigate the effect of increasing T on the computation time, we have two sequences of length 100000 with = 1 (case 1) and.05 (case 2). N was 256 and the abstraction hierarchy was a binary tree. For each sequence we found the Viterbi path with TAV, CFDP and the Viterbi algorithm."}, {"heading": "5.3 Impact of heuristics", "text": "As discussed in section 4, there is a trade-off between accuracy and computational time in computer heuristics. Figure 6 (b) compares the effect of using viterbi heuristics instead of the cheap heuristics described above. As T increased, there was a small improvement in computational time, although the acceleration was never greater than two times. As time went on, the two computational times were practically the same. Viterbi heuristics produced more than five times the acceleration (making it comparable to the CFDP) for large states. The main reason for the lack of improvement (in computational time) is the randomness of the data generation process. Viterbi heuristics can only significantly exceed cheap heuristics if the most likely state sequences according to the transition model receive very little support from the observations. In this case, cheap heuristics will offer very imprecise boundaries and mislead the search."}, {"heading": "6 Hierarchy induction", "text": "Up to this point, we have assumed that the abstraction hierarchy will be an input to the algorithm, although in many cases we have to construct our own hierarchy. Spectral clustering (Ng et al., 2001) is a technique we have used in our experiments to successfully induce hierarchies. If the underlying structure is a DBN of binary variables with a temporal separation between individual variables (as discussed in Section 5), there will be a gap in the eigenvalue spectrum according to the two largest values. The first two eigenvectors will be analog with indicator functions for branching the slowest variables. We can then apply the method recursively within each cluster. The main drawback is the O (N3) calculation costs. It can be argued that these are an offline and one-time cost - yet it is quite expensive."}, {"heading": "7 Conclusion", "text": "We have presented a time-abstracted Viterbi algorithm that can think about trajectories and use the A * search to demonstrate the right solution. Direct linkages provide a way to think about trajectories within a series of states - something that previous DP algorithms did not do. In systems with highly varying time scales, TAV can significantly outperform the CFDP. Our experiments confirm intuition - the greater the temporal separation, the greater the computational usefulness. Another intelligent feature of our algorithm is that it can exploit multiple time scales present in a system through adaptive spatial and temporal refinements. The limits of TAV occur when the system exhibits frequent state transitions, and in such cases it is better to resort to the conventional Viterbi algorithm (CFDP is often slower in such cases as well)."}, {"heading": "Acknowledgements", "text": "We thank NSF for their support (grant no. IIS-0904672) and Jason Wolfe, Aastha Jain and the anonymous reviewers for their comments and suggestions."}], "references": [{"title": "Why are DBNs sparse", "author": ["S. Chatterjee", "S. Russell"], "venue": "Journal of Machine Learning Research Proceedings Track,", "citeRegEx": "Chatterjee and Russell,? \\Q2010\\E", "shortCiteRegEx": "Chatterjee and Russell", "year": 2010}, {"title": "The generalized A* architecture", "author": ["P.F. Felzenszwalb", "D. McAllester"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Felzenszwalb and McAllester,? \\Q2007\\E", "shortCiteRegEx": "Felzenszwalb and McAllester", "year": 2007}, {"title": "The Viterbi algorithm", "author": ["G.D. Forney"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Forney and J.,? \\Q1973\\E", "shortCiteRegEx": "Forney and J.", "year": 1973}, {"title": "Hierarchical A*: Searching Abstraction Hierarchies Efficiently", "author": ["R.C. Holte", "M.B. Perez", "R.M. Zimmer", "A.J. MacDonald"], "venue": "In AAAI/IAAI,", "citeRegEx": "Holte et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Holte et al\\.", "year": 1996}, {"title": "A* Parsing: Fast Exact Viterbi Parse Selection", "author": ["D. Klein", "C.D. Manning"], "venue": "In HLT-NAACL", "citeRegEx": "Klein and Manning,? \\Q2003\\E", "shortCiteRegEx": "Klein and Manning", "year": 2003}, {"title": "A hidden markov model for progressive multiple", "author": ["A. Lytynoja", "M.C. Milinkovitch"], "venue": "alignment. Bioinformatics,", "citeRegEx": "Lytynoja and Milinkovitch,? \\Q2003\\E", "shortCiteRegEx": "Lytynoja and Milinkovitch", "year": 2003}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "In NIPS,", "citeRegEx": "Ng et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ng et al\\.", "year": 2001}, {"title": "Multiscale methods: Averaging and homogenization", "author": ["G.A. Pavliotis", "A.M. Stuart"], "venue": null, "citeRegEx": "Pavliotis and Stuart,? \\Q2007\\E", "shortCiteRegEx": "Pavliotis and Stuart", "year": 2007}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L. Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rabiner,? \\Q1989\\E", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "Coarse-to-Fine Dynamic Programming", "author": ["C. Raphael"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Raphael,? \\Q2001\\E", "shortCiteRegEx": "Raphael", "year": 2001}, {"title": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning", "author": ["R.S. Sutton", "D. Precup", "S.P. Singh"], "venue": "Artif. Intell.,", "citeRegEx": "Sutton et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Error bounds for convolutional codes and an asymptotically optimum decoding algorithm", "author": ["A. Viterbi"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Viterbi,? \\Q1967\\E", "shortCiteRegEx": "Viterbi", "year": 1967}], "referenceMentions": [{"referenceID": 11, "context": "The Viterbi algorithm (Viterbi, 1967; Forney, 1973) finds the most likely sequence of hidden states, called the \u201cViterbi path,\u201d conditioned on a sequence of observations in a hidden Markov model (HMM).", "startOffset": 22, "endOffset": 51}, {"referenceID": 8, "context": "It is one of the most important and basic algorithms in the entire field of information technology; its original application was in signal decoding but has since been used in numerous other applications including speech recognition (Rabiner, 1989), language parsing (Klein and Manning, 2003), and bioinformatics (Lytynoja and Milinkovitch, 2003).", "startOffset": 232, "endOffset": 247}, {"referenceID": 4, "context": "It is one of the most important and basic algorithms in the entire field of information technology; its original application was in signal decoding but has since been used in numerous other applications including speech recognition (Rabiner, 1989), language parsing (Klein and Manning, 2003), and bioinformatics (Lytynoja and Milinkovitch, 2003).", "startOffset": 266, "endOffset": 291}, {"referenceID": 5, "context": "It is one of the most important and basic algorithms in the entire field of information technology; its original application was in signal decoding but has since been used in numerous other applications including speech recognition (Rabiner, 1989), language parsing (Klein and Manning, 2003), and bioinformatics (Lytynoja and Milinkovitch, 2003).", "startOffset": 312, "endOffset": 345}, {"referenceID": 9, "context": "Coarse-to-fine dynamic programming or CFDP (Raphael, 2001) begins with SL and iteratively finds the shortest path in the current (abstracted) version of the graph and refines along it until the current shortest path is completely refined.", "startOffset": 43, "endOffset": 58}, {"referenceID": 3, "context": ", hierarchical A* (Holte et al., 1996) and HA*LD (Felzenszwalb and McAllester, 2007)\u2014are able to refine only the necessary part of the hierarchy tree and compute heuristics only when needed.", "startOffset": 18, "endOffset": 38}, {"referenceID": 1, "context": ", 1996) and HA*LD (Felzenszwalb and McAllester, 2007)\u2014are able to refine only the necessary part of the hierarchy tree and compute heuristics only when needed.", "startOffset": 18, "endOffset": 53}, {"referenceID": 10, "context": "Temporal abstractions have been well-studied in the context of planning (Sutton et al., 1999) and inference in dynamic Bayesian networks (Chatterjee and Russell, 2010).", "startOffset": 72, "endOffset": 93}, {"referenceID": 0, "context": ", 1999) and inference in dynamic Bayesian networks (Chatterjee and Russell, 2010).", "startOffset": 51, "endOffset": 81}, {"referenceID": 7, "context": "An excellent survey of temporal abstraction for dynamical systems can be found in (Pavliotis and Stuart, 2007).", "startOffset": 82, "endOffset": 110}, {"referenceID": 8, "context": "Following Rabiner (1989), we define", "startOffset": 10, "endOffset": 25}, {"referenceID": 8, "context": "The complete procedure (Rabiner, 1989) is as follows:", "startOffset": 23, "endOffset": 38}, {"referenceID": 6, "context": "Spectral clustering (Ng et al., 2001) is one technique which we have used in our experiments to successfully induce hierarchies.", "startOffset": 20, "endOffset": 37}], "year": 2011, "abstractText": "Hierarchical problem abstraction, when applicable, may offer exponential reductions in computational complexity. Previous work on coarse-to-fine dynamic programming (CFDP) has demonstrated this possibility using state abstraction to speed up the Viterbi algorithm. In this paper, we show how to apply temporal abstraction to the Viterbi problem. Our algorithm uses bounds derived from analysis of coarse timescales to prune large parts of the state trellis at finer timescales. We demonstrate improvements of several orders of magnitude over the standard Viterbi algorithm, as well as significant speedups over CFDP, for problems whose state variables evolve at widely differing rates.", "creator": "TeX"}}}