{"id": "1202.3709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "EDML: A Method for Learning Parameters in Bayesian Networks", "abstract": "We propose a method called EDML for learning MAP parameters in binary Bayesian networks under incomplete data. The method assumes Beta priors and can be used to learn maximum likelihood parameters when the priors are uninformative. EDML exhibits interesting behaviors, especially when compared to EM. We introduce EDML, explain its origin, and study some of its properties both analytically and empirically.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (1779kb)", "http://arxiv.org/abs/1202.3709v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["arthur choi", "khaled s refaat", "adnan darwiche"], "accepted": false, "id": "1202.3709"}, "pdf": {"name": "1202.3709.pdf", "metadata": {"source": "CRF", "title": "EDML: A Method for Learning Parameters in Bayesian Networks", "authors": ["Arthur Choi", "Khaled S. Refaat", "Adnan Darwiche"], "emails": ["darwiche}@cs.ucla.edu"], "sections": [{"heading": null, "text": "We propose a method called EDML to learn MAP parameters in binary Bayesian networks under incomplete data. It is based on beta-priors and can be used to learn maximum probability parameters when the priors are uninformative. EDML shows interesting behaviors, especially compared to EM. We introduce EDML, explain its origin, and examine some of its properties both analytically and empirically."}, {"heading": "1 INTRODUCTION", "text": "In this paper, we consider the problem of learning Baiesian network parameters in the face of incomplete data, while assuming that all network variables are binary. We propose a specific method, EDML, 1, which has a similar structure and complexity of the EM algorithm. When using uninformed primates, the maximum probability (ML) is reduced to parameters derived from the application of an approximate inference algorithm (Choi & Darwiche, 2006) to parameters to be calculated."}, {"heading": "2 TECHNICAL PRELIMINARIES", "text": "We use uppercase letters (X) to name variables and lowercase letters (x) to name their values. Variable sets are denoted by uppercase letters (X) and their instances (X). Therefore, since our focus is on binary variables, we will use the generic form of x (positive) and x (negative) to denote the two values of the binary variable X. Generally, we will use X to denote a variable on a Bayesian network and U to denote its parents. Thus, a network parameter will have the generic form that represents the probability of Pr (X = x). Note: Variable X can be thought of as a number of conditional random variables naming Xu, where the values of the conditional distribution Xu are drawn."}, {"heading": "3 AN OVERVIEW OF EDML", "text": "The first step, line 4, consists of two steps. However, the first step, line 3, calculates marginal information about the families of a Bayesian network parameterized by the current estimates. Step 2, line 4, uses the calculated probabilities of the algorithm 1 EMinput: G: A Bayesian network structure D: An incomplete dataset d1,., dN: An initial parameterization of the structure G \u03b1Xu, \u03b2Xu: Beta before each random variable Xu 1: while unconverted 2: Pr projected network structure d1,., dN: An initial parameterization of the structure G \u03b1Xu, \u03b2Xu: Beta before each variable Xu 1: while unconverted 2: Pr projected network structure D: An incomplete dataset d1,."}, {"heading": "3.1 ESTIMATION FROM SOFT OBSERVATIONS", "text": "Let us look at a random variable X with the values x and x, and let us assume that we have N > 0 independent observations of X, where Nx is the number of positive observations. It is well known that the ML parameter estimates for the random variable X in this case are unique and are characterized by \u03b8x = Nx / N. If we assume a more general problem where the observations are soft in that they only provide soft evidence for the values of the random variable X, it is also known that the estimates of the MAP parameters are unique and are characterized by a Bayes factor exacix = O (x | \u03b7i) / O (x), which quantifies the evidence that the observations regarding the observation of the value x of the random variable X. We will later show that the ML estimates remain unique in this more general case if at least one particular AP factor exacix = x / O (x) provides the proof that we are unique."}, {"heading": "3.2 EXAMPLES AS SOFT OBSERVATIONS", "text": "The second key concept underlying EDML is to interpret each example di in a dataset to provide a soft observation for each random variable Xu. As already mentioned, soft observations are specified by Bayes factors, and therefore one must specify the Bayes factor \u03baix | u that this example di induces on the random variable Xu. EDML uses Equation 1 for this purpose, which is in Section 5. Next, we will consider some specific cases of this equation to emphasize their behavior. First, consider the case in which the example di implies the parental instantiation u (i.e. the parents U of the variable X are instantiated to u in example di). In this case, Equation 1 reduces the equation to Squix | u = O (x | u) O (x | u), which is the relative change in probability of the x given variable u due to the conditioning on example di."}, {"heading": "4 ESTIMATION FROM SOFT OBSERVATIONS", "text": "Consider a binary variable X. Figure 1 represents a network in which \u03b8x is a parameter, the Pr (X = x) and X1 (.., XN are independent observations of X. Let us further assume that we have a beta before this problem, where we have only soft evidence for each observation whose strength is quantified by a Bayes factor. (x | i) / O (x). Here \u03baix represents the change in the probability that the i-th observation is positive on the basis of evidence. We refer to agoni as a soft observation of the variable X, and our goal in this section is to calculate (and optimize) the posterior constant of the constant."}, {"heading": "5 THE ORIGIN OF EDML", "text": "This section shows the technical origin of EDML, which shows how equation 1 is derived from algorithm 2 and provides the basis for the overall structure of EDML as set out in algorithm 2.EDML. Figure 2 is an example of a meta-network in which the case of \u03baix = \u221e must be treated carefully in equation 2. First of all, note that \u03baix = \u221e iff Pr (\u03b7i | x) = 0 is in the derivation of this equation. Since the value of equation 2 does not depend on the constant c, we assume that c = 1. Hence, when we consider the variation of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arithmetic parameters of arise of arithmetic parameters of aric of arithmetic of arithmetic parameters of numbers of arithmetic of arithmetic parameters of arithmetic of arithmetic of arithmetic parameters of"}, {"heading": "5.1 INTRODUCING GENERATORS", "text": "Let Xi specify the instance of variable X in the base network that corresponds to the example di. EDML's first choice is that for each edge \u03b8x | u \u2212 \u2192 Xi in the meta network, we introduce a generator variable Xiu, which leads to the edge pair \u03b8x | u \u2212 \u2192 Xiu \u2212 \u2192 Xi. Figure 3 (a) represents a fragment of the meta network in Figure 2, in which we have introduced two generator variables for edges \u03b8e | h \u2212 \u2192 E3 and \u03b8e | h examples \u0432x \u2212 \u2192 E3, which lead to successe | h \u2212 \u2192 E3 and \u03b8e | h variables. \u2212 \u2192 Xiu variable is intended to generate values of the variable X i according to the distribution specified by the parameters \u0445x | u. Therefore, the conditional distribution of a generator Xiu is such that Pr (x i u | \u0445x | u) is interpreted as a meta variable."}, {"heading": "5.2 DELETING COPY EDGES", "text": "The second choice EDML has made is that we delete only the edges of the form Xiu \u2212 \u2192 Xi from the extended meta network, which we will call copy edges. Figure 3 (b) shows an example in which we deleted the two copy edges from Figure 3 (a). Note at this point the addition of another helper variable Xiu:, which is called a clone for each generator X i u. Adding clones is required by the edge deletion framework. Moreover, if the CPT of clone Xiu: is carefully selected, it can compensate for the parent-child information lost when deleting the Xiu \u2212 \u2192 Xi edge. We will see later how EDML sets these CPTs. The other aspect of compensating a deleted edge is to specify soft evidence on each Xiu generator. This is also required by the edge deletion framework and is intended to compensate Xiu for the deletion of the meta-parent information."}, {"heading": "5.3 PARAMETER & EXAMPLE ISLANDS", "text": "Consider the network in Figure 4, which is extracted from the meta-network in Figure 2 according to the previously specified process of edge deletion. The edge deletion network contains a series of separate structures, so-called islands. Each island belongs to one of two classes: a parameter island for each network parameter \u03b8x | u and an example island for each example di in the dataset. Figure 4 provides the full details for an example island and a parameter island. Note that each parameter island corresponds to a structure of Naive Bay, with the parameters successx | u as root and the generators Xiu as children. If soft evidence is invoked for these generators, we obtain the estimation problem discussed in Section 4.EDML by (1) describing the soft evidence for each Xiu generator in a parameter island and (2) the CPT of each Xiu clone in full."}, {"heading": "5.4 CHILD-TO-PARENT COMPENSATION", "text": "The edge deletion approach suggests the following soft evidence for generators Xiu, which are given as Bayes factors: (3) where Pri is the distribution induced by the island of the example di. We will now show that this equation to Equation 1 of the algorithm 2.Let us assume that we marginalize all clones Xiu: from the island of the example di, resulting in a network that induces a distribution Pr. The new network has the following properties: First, it has the same structure as the base network. Second, Pr (x | u) = Pri (xiu:), which means that the CPTs of clones on sample islands correspond to parameters in the base network. Finally, if we use u to denote the disjunction of all parental instances without u, we get: Pu | u = Pri (Pi | r) (Pi | r) (Pi | r (Pi) (Pxir) (Pxir) (Pxir) Pxir () Pxir (xir) (xir | xii) Pxii (xii) Pxii (xii (xii) Pxii (xii)."}, {"heading": "5.5 PARENT-TO-CHILD COMPENSATION", "text": "We now complete the derivation of EDML by showing how it specifies the CPTs of sample island clones that are needed to calculate soft evidence as in the previous section. In short, EDML starts from an initial value of these CPTs, which is typically randomly selected. In view of these CPTs, sample islands are fully specified and EDML calculates soft evidence as in Equation 3. The computed soft evidence is then injected into the generators of parameter islands, leading to a complete specification of these islands. EDML then estimates parameters by solving an exact optimization problem on each parameter island, as in Section 4. The estimated parameters are then injected as new values of CPTs for clones on sample islands. This process repeats until convergence. We have shown in the previous section that the CPTs of clones correspond one to one with the base network parameters."}, {"heading": "6 SOME PROPERTIES OF EDML", "text": "As an approximate inference method, one can sometimes identify good EDML behaviors by identifying situations under which the underlying conclusions are drawn. (There is only one estimate under which the underlying estimates elicit a high quality approximation.) If the parents U of a variable X node are observed to u in an example di, all edges of the mesh are flooded and can be flooded, except for one edge that satisfies the satisfaction of the other edge. (In addition, edges originating from observed nodes can also be pruned from a meta network.) The parents of each variable are observed in a dataset."}, {"heading": "7 MORE ON CONVERGENCE", "text": "This year it is so far that it will be able to use the aforementioned lcihsrteeSe rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "8 FUTURE AND RELATED WORK", "text": "In the literature, however, learning (and Bayesian learning in particular) continues to be a challenge in a variety of situations, especially when there are hidden (latent) variables; a variety of techniques, some of which involve more traditional approaches to optimization, have been suggested in the literature; see e.g. (Thiesson, Meek, & Heckerman, 2001); the slow convergence of EM has also been recognized, especially in the presence of hidden variables; a variety of techniques, some of which involve more traditional approaches to optimization, are proposed in the literature; see e.g. (Thiesson, Meek, & Heckerman, 2001); variational approaches are an increasingly popular formalism for learning tasks, and for topic models in particular where variable alternatives to EM are used to maximize a lower probability (Lead, Ng, & Jordan, 2003)."}], "references": [{"title": "On the revision of probabilistic beliefs using uncertain evidence", "author": ["H. Chan", "A. Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Chan and Darwiche,? \\Q2005\\E", "shortCiteRegEx": "Chan and Darwiche", "year": 2005}, {"title": "An edge deletion semantics for belief propagation and its practical impact on approximation quality", "author": ["A. Choi", "A. Darwiche"], "venue": "In AAAI,", "citeRegEx": "Choi and Darwiche,? \\Q2006\\E", "shortCiteRegEx": "Choi and Darwiche", "year": 2006}, {"title": "Modeling and Reasoning with Bayesian Networks", "author": ["A. Darwiche"], "venue": null, "citeRegEx": "Darwiche,? \\Q2009\\E", "shortCiteRegEx": "Darwiche", "year": 2009}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Learning hidden variable networks: The information bottleneck approach", "author": ["G. Elidan", "N. Friedman"], "venue": null, "citeRegEx": "Elidan and Friedman,? \\Q2005\\E", "shortCiteRegEx": "Elidan and Friedman", "year": 2005}, {"title": "Learning bounded treewidth", "author": ["G. Elidan", "S. Gould"], "venue": "Bayesian networks. JMLR,", "citeRegEx": "Elidan and Gould,? \\Q2008\\E", "shortCiteRegEx": "Elidan and Gould", "year": 2008}, {"title": "Data perturbation for escaping local maxima in learning", "author": ["G. Elidan", "M. Ninio", "N. Friedman", "D. Shuurmans"], "venue": "In AAAI/IAAI,", "citeRegEx": "Elidan et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Elidan et al\\.", "year": 2002}, {"title": "Probability and the Weighing of Evidence", "author": ["I.J. Good"], "venue": null, "citeRegEx": "Good,? \\Q1950\\E", "shortCiteRegEx": "Good", "year": 1950}, {"title": "A tutorial on learning with Bayesian networks", "author": ["D. Heckerman"], "venue": "Learning in Graphical Models,", "citeRegEx": "Heckerman,? \\Q1998\\E", "shortCiteRegEx": "Heckerman", "year": 1998}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "The EM algorithm for graphical association models with missing data", "author": ["S. Lauritzen"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "Lauritzen,? \\Q1995\\E", "shortCiteRegEx": "Lauritzen", "year": 1995}, {"title": "Expectation propagation for approximate Bayesian inference", "author": ["T.P. Minka"], "venue": "In UAI,", "citeRegEx": "Minka,? \\Q2001\\E", "shortCiteRegEx": "Minka", "year": 2001}, {"title": "Expectationpropogation for the generative aspect model", "author": ["T.P. Minka", "J.D. Lafferty"], "venue": "In UAI,", "citeRegEx": "Minka and Lafferty,? \\Q2002\\E", "shortCiteRegEx": "Minka and Lafferty", "year": 2002}, {"title": "Optimization with EM and expectation-conjugategradient", "author": ["R. Salakhutdinov", "S.T. Roweis", "Z. Ghahramani"], "venue": "In ICML,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2003}, {"title": "Accelerating EM for large databases", "author": ["B. Thiesson", "C. Meek", "D. Heckerman"], "venue": "Machine Learning,", "citeRegEx": "Thiesson et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Thiesson et al\\.", "year": 2001}], "referenceMentions": [{"referenceID": 10, "context": "We propose a specific method, EDML, which has a similar structure and complexity to the EM algorithm (Dempster, Laird, & Rubin, 1977; Lauritzen, 1995).", "startOffset": 101, "endOffset": 150}, {"referenceID": 2, "context": "See (Darwiche, 2009; Koller & Friedman, 2009) for recent treatments on parameter learning in Bayesian networks via EM and related methods.", "startOffset": 4, "endOffset": 45}, {"referenceID": 2, "context": ", (Darwiche, 2009)):", "startOffset": 2, "endOffset": 18}, {"referenceID": 7, "context": "Our method for learning MAP and ML parameters makes heavy use of two notions: (1) the odds of an event, which is the probability of the event over the probability of its negation, and (2) the Bayes factor (Good, 1950), which is the relative change in the odds of one event, say, X=x, due to observing some other event, say, \u03b7.", "startOffset": 205, "endOffset": 217}, {"referenceID": 2, "context": "parameters are represented explicitly as nodes (Darwiche, 2009).", "startOffset": 47, "endOffset": 63}, {"referenceID": 3, "context": "EM has played a critical role in learning probabilistic graphical models and Bayesian networks (Dempster et al., 1977; Lauritzen, 1995; Heckerman, 1998).", "startOffset": 95, "endOffset": 152}, {"referenceID": 10, "context": "EM has played a critical role in learning probabilistic graphical models and Bayesian networks (Dempster et al., 1977; Lauritzen, 1995; Heckerman, 1998).", "startOffset": 95, "endOffset": 152}, {"referenceID": 8, "context": "EM has played a critical role in learning probabilistic graphical models and Bayesian networks (Dempster et al., 1977; Lauritzen, 1995; Heckerman, 1998).", "startOffset": 95, "endOffset": 152}, {"referenceID": 11, "context": "Expectation Propagation also provides variations of EM (Minka & Lafferty, 2002) and is closely related to (loopy) belief propagation (Minka, 2001).", "startOffset": 133, "endOffset": 146}], "year": 2011, "abstractText": "We propose a method called EDML for learning MAP parameters in binary Bayesian networks under incomplete data. The method assumes Beta priors and can be used to learn maximum likelihood parameters when the priors are uninformative. EDML exhibits interesting behaviors, especially when compared to EM. We introduce EDML, explain its origin, and study some of its properties both analytically and empirically.", "creator": "TeX"}}}