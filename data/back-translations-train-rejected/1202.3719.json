{"id": "1202.3719", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Inference in Probabilistic Logic Programs using Weighted CNF's", "abstract": "Probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities. Several classical probabilistic inference tasks (such as MAP and computing marginals) have not yet received a lot of attention for this formalism. The contribution of this paper is that we develop efficient inference algorithms for these tasks. This is based on a conversion of the probabilistic logic program and the query and evidence to a weighted CNF formula. This allows us to reduce the inference tasks to well-studied tasks such as weighted model counting. To solve such tasks, we employ state-of-the-art methods. We consider multiple methods for the conversion of the programs as well as for inference on the weighted CNF. The resulting approach is evaluated experimentally and shown to improve upon the state-of-the-art in probabilistic logic programming.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (350kb)", "http://arxiv.org/abs/1202.3719v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daan fierens", "guy van den broeck", "ingo thon", "bernd gutmann", "luc de raedt"], "accepted": false, "id": "1202.3719"}, "pdf": {"name": "1202.3719.pdf", "metadata": {"source": "CRF", "title": "Inference in Probabilistic Logic Programs using Weighted CNF\u2019s", "authors": ["Daan Fierens", "Guy Van den Broeck", "Ingo Thon", "Bernd Gutmann", "Luc De Raedt"], "emails": [], "sections": [{"heading": null, "text": "Probabilistic logic programs are logic programs in which some facts are commented on with probabilities. Several classical probabilistic inference tasks (such as MAP and computational margins) have not yet received much attention for this formalism. The contribution of this work is that we develop efficient inference algorithms for these tasks, which are based on converting the probabilistic logic program and the query and evidence into a weighted CNF formula. This allows us to reduce the inference tasks to well-studied tasks such as the weighted model counting. To solve such tasks, we apply state-of-the-art methods. We consider several methods for converting the programs as well as for inferring to the weighted CNF. The resulting approach is evaluated experimentally and shows that it is improved compared to the state of the art in probabilistic logic programming."}, {"heading": "1 Introduction", "text": "There is a lot of interest in the combination of probability and logic for dealing with complex relationship domains. This interest has resulted in the areas of statistical learning (SRL) and probabilistic logic programming (PLP)."}, {"heading": "2 Background", "text": "We now come to the basics of logic programming [15] and logic of the first order."}, {"heading": "2.1 First Order Logic (FOL)", "text": "An atom is of the form p (t1,..., tn), where p is a predicate of species n and the ti terms are. A formula consists of atoms using universal and existential quantifiers and the usual logical connectives \u00ac. An FOL theory is a set of formulas that implicitly form a conjunction. An expression is called earth if it does not contain variables. A disjunction consisting of a single letter is called a unity clause. Any soil theory can be in the conjunction of normal form (CNF) if it is a conjunction of disjunctions of literals. A letter is an atom or its negation. Every disjunction of literals is called a clause. A disjunction consisting of a single letter is called a unity clause."}, {"heading": "2.2 Logic Programming (LP)", "text": "Syntactically speaking, a normal logic program, or short logic program (LP), is a set of rules.1 A rule is a universal rule1Rules are also called normal clauses, but we avoid this terminology because we use \"clause\" in the context of the CNF.quantified expression of the form h: - b1,..., bn, where h is an atom and the bi are literal. The atom h is called the head of the rule and b1,.., bn the body that embodies the conjunction b1.... A fact is a rule that is true as a body and more compact than h. Note that \": -\" can also be written as \"\u2190.\" Therefore, any rule can be considered syntactically as an FOL formula. There is a crucial difference in semantics, but LP is a crucial difference in semantics, however.We use the well-based semantics for LPs."}, {"heading": "2.3 Differences between FOL and LP", "text": "In semantics, there is a crucial difference between LP and FOL: LP makes the CWA, while FOL does not. Thus, for example, in FOL theory {a \u2190 b} there are three models {\u00ac a, \u00ac b}, {a, \u00ac b} and {a, b}. Syntactically equivalent LP {a: - b} has only one model, namely the least Herbrand model {\u00ac a, \u00ac b} (intuitively, a and b are wrong, because there is no rule that makes b true, and therefore there is no applicable rule that makes true). Since LP is syntactically a subset of FOL, it is tempting to believe that FOL is more \"expressive\" than LP. This is wrong because of the difference in semantics. In the literature on knowledge presentation, it has been shown that certain concepts that can be expressed in (non-ground) LP cannot be expressed in (non-ground) FOL, e.g., inductive definitions [5]."}, {"heading": "3 Probabilistic Logic Programming", "text": "Most probable programming languages, including PRISM [3], ICL [3], ProbLog [4], ProbLog [4], and LPAD [5], are, in fact, not only the simplest choices, but also the least likely choices of these languages. [5] However, our approach to the other languages is as impenetrable as the ability to define a whole set of probable facts and a logical program. [6] It is a fact that comes with a whole set of probable distributions. [7] It is a fact that is united with a whole set of probable distributions, while a set of rules that are written with a set of rules: f, is a fact f annotated with a probability p. An atom that unites with a probable fact is referred to as a probable fact that is united with the head of a rule is referred to as derived atoms. The set of probable atoms must be separated from the set of atoms that we derive from the series of atoms that we use."}, {"heading": "4 Inference Tasks", "text": "We assume that we obtain a set of E-At of the observed atoms (atoms of proof) and a vector e with their observed truth values (i.e. the proof is E = e). We also obtain a set of Q-At of the atoms of interest (query atoms). The two inference tasks we consider are MARG and MAP. MARG is the task of calculating the boundary distribution of each quantum atom in the face of the evidence, i.e. the calculation of P (Q | E = e) for each Q-Q. MAP (maximum a posteriori) is the task of finding the most likely common state of all quantum atoms in the face of the evidence, i.e. the search for argmaxqP (Q = q | E = e).In the literature on probability models and statistical relative programs, they are regarded [ProgmaxqP] and [MAP] as true quantum atoms."}, {"heading": "5 Conversion to Weighted CNF", "text": "The following algorithm outlines how to convert a ProbLog program L together with a query Q = e to a weighted CNF: 1. The bottom L returns a program Lg that captures the distribution P (Q) E = e. We refer to the result-oriented program Lg that converts the ground rules in relation to Q and E = e.2 to an equivalent CNF, that is, the part that captures the distribution P (Q) E."}, {"heading": "5.1 The Relevant Ground Program", "text": "In fact, most of them are able to determine for themselves what they want and what they want to do."}, {"heading": "5.2 The CNF for the Ground Program", "text": "(It.). (It.). (It.). (It.). (It.). (It.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it.). (it. (it.). (it.). (it.). (it. (it.). (it.). (it. (it.). (it.). (it.). (it. (it.). (it.). (it. (it.). (it.). (it. (it.). (it.). (it. (it.). (it.). (it. (it.). (it.). (it. (it.). (it. (it.). (it.). (it.). (.). (it. (it.). (it. (it.). (it.). (it. (it.). (it.). (it. (it.). (it.). (it.). (it. (it.). (it.). (it.). (it. (it.).). (.). (it. (.).). (. (.). (.).). (. (.).). (.). (.).).). (It. (. (.). (.).).). (It. (.). (.).). (.).). (It.). (.).). (.). (It. (.).).). (It. (.).). (.).). (It.).).). (. (.).).).).). ("}, {"heading": "5.3 The Weighted CNF", "text": "The last step constructs the weighted CNF from the CNF r. First, we define the CNF-Q value as the conjunction of the CNF weight (weighted) and a CNF weight (weighted). Second, we define the weight function for all letters in the resulting CNF. If the ProbLog program contains a probable fact, p: f, then we assign the weight p to f and the weight 1 \u2212 p. (weighted the weight function for all letters in the resulting CNF)."}, {"heading": "6 Inference on Weighted CNFs", "text": "In order to solve the MARG and MAP inference tasks for the original probabilistic logic program L, the Q query and the proof E = e, we have converted the program into a weighted CNF. A decisive advantage is that the original MARG and MAP inference tasks can now be reformulated in relation to known tasks such as the weighted model that counts on the weighted CNF. This \"hard\" clause implies an infinity of weight (any world that violates the clause has a zero probability). Logarithms, e.g. ln (0.3), are needed because an MLN is a log-linear model. Logarithms are negative, but any MLN with negative weights can be rewritten into an equivalent MLN with only positive weights [3]."}, {"heading": "6.1 MARG Inference", "text": "First, let's discuss how to tackle MARG, the task of calculating the marginal P (Q | E = e) for each query atom Q-Q, and its special case \"MARG-1,\" where Q consists of a single query atom (Q = {Q})."}, {"heading": "1) Exact/approximate MARG-1 by means of", "text": "Weighted model counting. By definition, P (Q = q | E = e) = P (Q = q, E = e) / P (E = e). The denominator corresponds to the weighted model counting of the weighted CNF, namely the weighted model counting of Q (namely Q if q = true and \u00ac Q if q = false).7 Similarly, the counter is the weighted model counting of the weighted CNF q, which is obtained by combining the originally weighted CNF number with the corresponding unit clause for Q (namely Q if q = true and \u00ac Q if q = false).Therefore, each boundary counting can be calculated by solving two weighted model counts (WMC). WMC is a well-studied task in the SAT community. Solving these WMC cases can be done with any of the existing algorithms (exactly [1] or approximately [7]).It is well known that MARG inference can be solved with Bayesian networks using WMC [14]."}, {"heading": "2) Exact MARG by means of compilation. To", "text": "A popular solution to avoid this is to first compile the weighted CNF into a more efficient representation [2]. Specifically, we can compile the CNF into d-DNNF (deterministic decomposable negation normal form [1]) and then calculate all the required marginals from the (weighted) d-DNNF. The latter can be done efficiently for all marginals in parallel, namely by traversing the d-DNNF twice [2]. In probabilistic logic programming (PLP), commu-7This is because P (E = e) = the weighted MODE = e (L) PL (N) = the weighted d-shape (D) of the BAT (n) w (n), where the second equation from Theorem 2 follows."}, {"heading": "3) Approximate MARG by means of MCMC.", "text": "Since the CNF itself is deterministic, standard MCMC approaches such as Gibbs sampling are not suitable. We use the MC-SAT algorithm, which is specifically designed for determinism (MC-SAT uses a SAT solver at each step of the Markov chain to construct a new sample) [13]. MC-SAT was developed for MLNs. Theorem 3 ensures that MCMC assumes the correct distribution P (Q | E = e) on the corresponding MLN samples."}, {"heading": "6.2 MAP Inference", "text": "We consider the following algorithms.1) Exact MAP by compilation. We can compile the weighted CNF to a weighted d-DNNF and then use this d-DNNF to find the MAP solution, see Darwiche [2]. The compilation phase is actually independent of the specific task (MARG or MAP), only the traversal is different. Compilation to BDD is also possible."}, {"heading": "2) Approximate MAP/MPE by means of", "text": "MPE is the special case of MAP, where one wants to find the state of all non-evidence-based atoms. MPE conclusions on a weighted CNF are reduced to the weighted MAX-SAT problem [12], a standard problem in the SAT literature. A popular approximate approach is stochastic local search [12]. An example algorithm is MaxWalkSAT, which is also the standard MPE algorithm for MLNs [3]."}, {"heading": "7 Experiments", "text": "Our implementation currently supports (1) the exact MARG by compiling to d-DNNF or BDD, (2) the approximate MARG with MC-SAT, (3) the approximate MAP / MPE with MaxWalkSAT. Other algorithms for inferring to the weighted CNF could also be applied, so the above list is not exhaustive. The aim of our experiments is to determine the feasibility of our approach and to analyze the influence of the various parameters. We focus on MARG inference (for MAP / MPE, our current implementation is a proof-of-concept)."}, {"heading": "7.1 Experimental Setup", "text": "The main rule in the ProbLog program is Smoking (X): - Friend (X, Y), Smokes (Y), inf (Y, X). This means that every smoking friend X is likely to smoke regardless of the probability p: inf (X, Y). Other rules state that people also smoke for other reasons, that smoking causes cancer, and so on. All probabilities in the program have been set manually. We also use the WebKB dataset, a collective classification domain (http: / www.cs.cmu.edu / other rules)."}, {"heading": "7.2 Influence of the Grounding Algorithm", "text": "We compare the calculation of the relevant soil program (RGP) with naively performing full grounding. Grounding. The idea behind the RGP is to reduce grounding by circumcising clauses that are irrelevant or inactive. Queries and evidence. Our setup is such that all clauses are relevant. Therefore, the only reduction comes from circumcising inactive clauses (which literally leave false evidence in the body).The effect of this circumcising is small: on average, the size of the soil program is reduced by 17% (results not shown).Effects on conversion to CNF. Evidence-based conversion becomes insoluble for large domain sizes, but the size in which this occurs is significantly greater when working on the RGP, rather than on complete grounding (see Figure 1a / 2a).Effects on conversion to CNF. Also, the size of the CNF is significantly reduced by reducing the RGP from 90% to just one reason per cent of the RGP (why can be reduced from 90% to 2b per reason)."}, {"heading": "7.3 Influence of the Conversion Algorithm", "text": "We compare the rules-based and sample-based algorithm for converting basic rules to CNF (Section 5.2).Conversion. The sample-based algorithm naturally makes more efforts to convert the program into a compact CNF. This has an impact on the scalability of the algorithm: on small domains the algorithm is fast, but on larger domains it becomes insoluble (Fig. 1a / 2a). In contrast, the rules-based algorithm can handle all the domain sizes considered and is always fast (runtime at most 0.5s).A similar trend applies in terms of CNF size. For small domains the sample-based algorithm generates the smallest CNFs, but for larger domain sizes the opposite applies (Fig. 1b / 2b).Implications on inference. We discuss the influence of the conversion algorithm on exact inference."}, {"heading": "7.4 Influence of the Inference Algorithm", "text": "We will focus on comparing the two exact inference algorithms, the composition of d-DNNFs or BDDs. We distinguish between conclusions based on rules and proof-based CNFs (in the PLP literature, BDDs are used almost exclusively for proof-based CNFs [4, 8]).9Proof-based CNFs. BDDs perform relatively well on the Smokers domain, but are still significantly outperformed by d-DNNFs (Fig. 1c).On WebKB, the difference is even greater: BDDs are tractable only to domains of size 3 or 4, while d-DNNFs range up to size 18 (Fig. 2c).If BDDs become insoluble, this is largely due to storage problems. 10rule-based CNFs solve these CNFs less compactly than proof-based CNFs (at least for domains sizes where exact inferences are possible)."}, {"heading": "8 Conclusion", "text": "This paper contributes to a two-step approach for MAP and MARG in the general probability calculation logic9Compiling our proof-based CNFs to BDDs that produce exactly the same BDDs as Gutmann et al. [8]. However, in the specific case of a single query and without evidence, one should keep in mind that we use BDDs for exact conclusions. BDDs are also used for approximate inferences, you just command an approximate CNF into a BDD that is no better. However, it should be kept in mind that we use BDDs for exact conclusions. BDDs are also used for approximate inferences, you command them into a BDD [4]."}, {"heading": "Acknowledgements", "text": "DF, GVdB and BG are supported by the Research Foundation Flanders (FWO-Vlaanderen) and supported by the European Commission under contract number FP7-248258-First-MM. We thank Maurice Bruynooghe, Theofrastos Mantadelis and Kristian Kersting for their useful discussions."}], "references": [{"title": "New advances in compiling CNF into decomposable negation normal form", "author": ["A. Darwiche"], "venue": "In Proc. 16th European Conf. on Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Modeling and Reasoning with Bayesian Networks", "author": ["A. Darwiche"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Probabilistic Inductive Logic Programming - Theory and Applications, volume 4911", "author": ["L. De Raedt", "P. Frasconi", "K. Kersting", "S. Muggleton", "editors"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "ProbLog: A probabilistic Prolog and its application in link discovery", "author": ["L. De Raedt", "A. Kimmig", "H. Toivonen"], "venue": "In Proc. 20th International Joint Conf. on Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Logic programming revisited: Logic programs as inductive definitions", "author": ["M. Denecker", "M. Bruynooghe", "V.W. Marek"], "venue": "ACM Transactions on Computational Logic,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Inference in probabilistic logic programs using weighted CNF\u2019s", "author": ["D. Fierens", "G. Van den Broeck", "I. Thon", "B. Gutmann", "L. De Raedt"], "venue": "Technical Report CW 607,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "From sampling to model counting", "author": ["C.P. Gomes", "J. Hoffmann", "A. Sabharwal", "B. Selman"], "venue": "In Proc. 20th International Joint Conf. on Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Learning the parameters of probabilistic logic programs from interpretations", "author": ["B. Gutmann", "I. Thon", "L. De Raedt"], "venue": "In European Conf. on Machine Learning and Principles and Practice of Knowledge Discovery in Databases,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Representing normal programs with clauses", "author": ["T. Janhunen"], "venue": "In Proc. of 16th European Conf. on Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Dedicated tabling for a probabilistic setting", "author": ["T. Mantadelis", "G. Janssens"], "venue": "In Tech. Comm. of 26th International Conf. on Logic Programming,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "CP-logic theory inference with contextual variable elimination and comparison to BDD based inference methods", "author": ["W. Meert", "J. Struyf", "H. Blockeel"], "venue": "In Proc. 19th International Conf. of Inductive Logic Programming,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Using weighted MAX-SAT engines to solve MPE", "author": ["J.D. Park"], "venue": "In Proc. 18th National Conf. on Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Sound and efficient inference with probabilistic and deterministic dependencies", "author": ["H. Poon", "P. Domingos"], "venue": "In Proc. 21st National Conf. on Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Solving Bayesian networks by Weighted Model Counting", "author": ["T. Sang", "P. Beame", "H. Kautz"], "venue": "In Proc. 20th National Conf. on Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "The well-founded semantics for general logic programs", "author": ["A. Van Gelder", "K.A. Ross", "J.S. Schlipf"], "venue": "Journal of the ACM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1991}], "referenceMentions": [{"referenceID": 2, "context": "This interest has resulted in the fields of Statistical Relational Learning (SRL) and Probabilistic Logic Programming (PLP) [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "Our approach is similar to the work of Darwiche [2] and others [14, 12], who perform Bayesian network inference by conversion to weighted propositional formulae, in particular weighted CNFs.", "startOffset": 48, "endOffset": 51}, {"referenceID": 13, "context": "Our approach is similar to the work of Darwiche [2] and others [14, 12], who perform Bayesian network inference by conversion to weighted propositional formulae, in particular weighted CNFs.", "startOffset": 63, "endOffset": 71}, {"referenceID": 11, "context": "Our approach is similar to the work of Darwiche [2] and others [14, 12], who perform Bayesian network inference by conversion to weighted propositional formulae, in particular weighted CNFs.", "startOffset": 63, "endOffset": 71}, {"referenceID": 12, "context": "We also identify a novel connection between PLP and Markov Logic [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "We now review the basics of logic programming [15] and first order logic.", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "Markov Logic Networks (MLNs) [13] are a probabilistic extension of FOL.", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "For LPs with negation, we use the well-founded model, see [15].", "startOffset": 58, "endOffset": 62}, {"referenceID": 4, "context": "In the knowledge representation literature, it has been shown that certain concepts that can be expressed in (non-ground) LP cannot be expressed in (non-ground) FOL, for instance inductive definitions [5].", "startOffset": 201, "endOffset": 204}, {"referenceID": 2, "context": "Most probabilistic programming languages, including PRISM [3], ICL [3], ProbLog [4] and LPAD [11], are", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "Most probabilistic programming languages, including PRISM [3], ICL [3], ProbLog [4] and LPAD [11], are", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "Most probabilistic programming languages, including PRISM [3], ICL [3], ProbLog [4] and LPAD [11], are", "startOffset": 80, "endOffset": 83}, {"referenceID": 10, "context": "Most probabilistic programming languages, including PRISM [3], ICL [3], ProbLog [4] and LPAD [11], are", "startOffset": 93, "endOffset": 97}, {"referenceID": 2, "context": "based on Sato\u2019s distribution semantics [3].", "startOffset": 39, "endOffset": 42}, {"referenceID": 2, "context": "PRISM and ICL require the rules to be acyclic (or contingently acyclic) [3].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "This task is often referred to as computing the success probability of Q [4].", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "The only works related to the more general MARG or MAP task in the PLP literature [3, 11, 8] make a number of restrictive assumptions about the given program such as acyclicity [8] and the mutual exclusiveness assumption (for PRISM [3]).", "startOffset": 82, "endOffset": 92}, {"referenceID": 10, "context": "The only works related to the more general MARG or MAP task in the PLP literature [3, 11, 8] make a number of restrictive assumptions about the given program such as acyclicity [8] and the mutual exclusiveness assumption (for PRISM [3]).", "startOffset": 82, "endOffset": 92}, {"referenceID": 7, "context": "The only works related to the more general MARG or MAP task in the PLP literature [3, 11, 8] make a number of restrictive assumptions about the given program such as acyclicity [8] and the mutual exclusiveness assumption (for PRISM [3]).", "startOffset": 82, "endOffset": 92}, {"referenceID": 7, "context": "The only works related to the more general MARG or MAP task in the PLP literature [3, 11, 8] make a number of restrictive assumptions about the given program such as acyclicity [8] and the mutual exclusiveness assumption (for PRISM [3]).", "startOffset": 177, "endOffset": 180}, {"referenceID": 2, "context": "The only works related to the more general MARG or MAP task in the PLP literature [3, 11, 8] make a number of restrictive assumptions about the given program such as acyclicity [8] and the mutual exclusiveness assumption (for PRISM [3]).", "startOffset": 232, "endOffset": 235}, {"referenceID": 10, "context": "There also exist approaches that transform ground probabilistic programs to Bayesian networks and then use standard Bayesian network inference procedures [11].", "startOffset": 154, "endOffset": 158}, {"referenceID": 14, "context": "Some LPs have a three-valued WFM (atoms are true, false or unknown), but we consider only ProbLog programs for which all LPs are two-valued (no unknowns) [15].", "startOffset": 154, "endOffset": 158}, {"referenceID": 0, "context": "Step 3 also defines the weight function, which assigns a weight (\u2208 [0, 1]) to each literal in \u03c6; see Section 5.", "startOffset": 67, "endOffset": 73}, {"referenceID": 7, "context": "To do so, we make use of the concept of a dependency set with respect to a ProbLog program [8].", "startOffset": 91, "endOffset": 94}, {"referenceID": 7, "context": "It is safe to restrict the grounding to the relevant rules only [8].", "startOffset": 64, "endOffset": 67}, {"referenceID": 8, "context": "In contrast, each ground LP can be converted to an equivalent ground FOL formula or CNF [9].", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "The proofs of all theorems can be found in a technical report [6].", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "[9]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "For acyclic rules, the conversion is straightforward, we simply take Clark\u2019s completion of the rules [9, 8].", "startOffset": 101, "endOffset": 107}, {"referenceID": 7, "context": "For acyclic rules, the conversion is straightforward, we simply take Clark\u2019s completion of the rules [9, 8].", "startOffset": 101, "endOffset": 107}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "the resulting CNF is not equivalent to the rules [9].", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "The first algorithm [9] belongs to the field of Answer Set Programming.", "startOffset": 20, "endOffset": 23}, {"referenceID": 9, "context": "The second algorithm [10] is proof-based.", "startOffset": 21, "endOffset": 25}, {"referenceID": 9, "context": "The proofs are collected in a recursive structure (a set of \u2018nested tries\u2019 [10]), which will have loops if the given rules had loops.", "startOffset": 75, "endOffset": 79}, {"referenceID": 2, "context": "The logarithms are negative, but any MLN with negative weights can be rewritten into an equivalent MLN with only positive weights [3].", "startOffset": 130, "endOffset": 133}, {"referenceID": 0, "context": "Solving these WMC instances can be done using any of the existing algorithms (exact [1] or approximate [7]).", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "Solving these WMC instances can be done using any of the existing algorithms (exact [1] or approximate [7]).", "startOffset": 103, "endOffset": 106}, {"referenceID": 13, "context": "It is well-known that MARG inference with Bayesian networks can be solved using WMC [14].", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "A popular solution to avoid this is to first compile the weighted CNF into a more efficient representation [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "Concretely, we can compile the CNF to d-DNNF (deterministic Decomposable Negation Normal Form [1]) and then compute all required marginals from the (weighted) d-DNNF.", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "The latter can be done efficiently for all marginals in parallel, namely by traversing the d-DNNF twice [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 3, "context": "nity, the state-of-the-art is to compile the program into another form, namely a BDD (reduced ordered Binary Decision Diagram) [4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 7, "context": "The BDD approach has recently also been used for MARG inference (to compute all marginals, the BDD is then traversed in a way that is very similar to that for d-DNNFs [8]).", "startOffset": 167, "endOffset": 170}, {"referenceID": 0, "context": "BDDs form a subclass of d-DNNFs [1].", "startOffset": 32, "endOffset": 35}, {"referenceID": 0, "context": "So far, general d-DNNFs have not been considered in the PLP community, despite the theoretical and empirical evidence that compilation to d-DNNF outperforms compilation to BDD in the context of model counting [1].", "startOffset": 209, "endOffset": 212}, {"referenceID": 12, "context": "We use the MC-SAT algorithm that was developed specifically to deal with determinism (in each step of the Markov chain, MC-SAT makes use of a SAT solver to construct a new sample) [13].", "startOffset": 180, "endOffset": 184}, {"referenceID": 1, "context": "We can compile the weighted CNF to a weighted d-DNNF and then use this d-DNNF to find the MAP solution, see Darwiche [2].", "startOffset": 117, "endOffset": 120}, {"referenceID": 11, "context": "MPE inference on a weighted CNF reduces to the weighted MAX SAT problem [12], a standard problem in the SAT literature.", "startOffset": 72, "endOffset": 76}, {"referenceID": 11, "context": "A popular approximate approach is stochastic local search [12].", "startOffset": 58, "endOffset": 62}, {"referenceID": 2, "context": "An example algorithm is MaxWalkSAT, which is also the standard MPE algorithm for MLNs [3].", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "As a social network domain we use the standard \u2018Smokers\u2019 domain [3].", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "All probabilities were learned from data [8].", "startOffset": 41, "endOffset": 44}, {"referenceID": 12, "context": "We use the same solution as the original MC-SAT paper: we let MC-SAT run for a fixed time (10 minutes) and measure the quality of the estimated marginals as the likelihood of the \u2018query ground truth\u2019 according to these estimates (see [13] for the motivation).", "startOffset": 234, "endOffset": 238}, {"referenceID": 3, "context": "We make the distinction between inference on rule-based and proof-based CNFs (in the PLP literature, BDDs have almost exclusively been used for proof-based CNFs [4, 8]).", "startOffset": 161, "endOffset": 167}, {"referenceID": 7, "context": "We make the distinction between inference on rule-based and proof-based CNFs (in the PLP literature, BDDs have almost exclusively been used for proof-based CNFs [4, 8]).", "startOffset": 161, "endOffset": 167}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "BDDs are also used for approximate inference, one simply compiles an approximate CNF into a BDD [4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "Our two-step approach is akin to that employed in the Bayesian network community where many inference problems are also cast in terms of weighted CNFs [2, 12, 14].", "startOffset": 151, "endOffset": 162}, {"referenceID": 11, "context": "Our two-step approach is akin to that employed in the Bayesian network community where many inference problems are also cast in terms of weighted CNFs [2, 12, 14].", "startOffset": 151, "endOffset": 162}, {"referenceID": 13, "context": "Our two-step approach is akin to that employed in the Bayesian network community where many inference problems are also cast in terms of weighted CNFs [2, 12, 14].", "startOffset": 151, "endOffset": 162}], "year": 2011, "abstractText": "Probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities. Several classical probabilistic inference tasks (such as MAP and computing marginals) have not yet received a lot of attention for this formalism. The contribution of this paper is that we develop efficient inference algorithms for these tasks. This is based on a conversion of the probabilistic logic program and the query and evidence to a weighted CNF formula. This allows us to reduce the inference tasks to wellstudied tasks such as weighted model counting. To solve such tasks, we employ state-ofthe-art methods. We consider multiple methods for the conversion of the programs as well as for inference on the weighted CNF. The resulting approach is evaluated experimentally and shown to improve upon the state-of-theart in probabilistic logic programming.", "creator": "TeX"}}}