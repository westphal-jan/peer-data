{"id": "1202.3720", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Efficient Inference in Markov Control Problems", "abstract": "Markov control algorithms that perform smooth, non-greedy updates of the policy have been shown to be very general and versatile, with policy gradient and Expectation Maximisation algorithms being particularly popular. For these algorithms, marginal inference of the reward weighted trajectory distribution is required to perform policy updates. We discuss a new exact inference algorithm for these marginals in the finite horizon case that is more efficient than the standard approach based on classical forward-backward recursions. We also provide a principled extension to infinite horizon Markov Decision Problems that explicitly accounts for an infinite horizon. This extension provides a novel algorithm for both policy gradients and Expectation Maximisation in infinite horizon problems.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (223kb)", "http://arxiv.org/abs/1202.3720v1", null]], "reviews": [], "SUBJECTS": "cs.SY cs.AI", "authors": ["thomas furmston", "david barber"], "accepted": false, "id": "1202.3720"}, "pdf": {"name": "1202.3720.pdf", "metadata": {"source": "CRF", "title": "Efficient Inference in Markov Control Problems", "authors": ["Thomas Furmston", "David Barber"], "emails": [], "sections": [{"heading": null, "text": "Markov control algorithms that perform smooth, non-greedy policy updates have proven to be very general and versatile, with policy gradients and expectation maximization algorithms being particularly popular; these algorithms require a marginal inference of reward-weighted trajectory distribution to perform policy updates; we are discussing a new, accurate inference algorithm for these marginals in the case of the finite horizon, which is more efficient than the standard approach based on classical backward recursions; we are also offering a principled extension of Markov decision-making problems to the endless horizon, which explicitly takes into account a finite horizon; and this extension provides a novel algorithm for both policy gradients and expectation maximization for finite horizon problems."}, {"heading": "1 MARKOV DECISION PROBLEMS", "text": "A Markov decision problem (MDP) is described by an initial state distribution p1 (s1), transition distributions p (st + 1 | st, at) and reward function Rt (st, at), in which the state and action are discreetly designated t by st in due time and in due time1 (Sutton and Barto, 1998). The state and action space can be either discrete or continuous. For a discount factor, the reward is defined as Rt (st, at) = \u03b3t \u2212 1R (st, at) for a stationary reward R (st, at), in which a stationary reward R (st, at) is difficult (0, 1). We assume that a stationary policy, \u03c0, is defined as a series of conditional distributions across the action space, s = p (at = a = s, \u03c0). The total expected reward of the MDP (the political benefit) 1To avoid a state of emergency, we {emergency,}."}, {"heading": "1.1 EXPECTATION MAXIMISATION", "text": "By expressing the objective function (1) as the probability function of a suitably constructed mixture model, we can solve the MDP using techniques from the probabilistic conclusion, such as EM (Dayan and Hinton, 1997) or MCMC (Hoffman et al., 2006). Without loss of generality, we assume that the reward is not negative and define the weighted path distribution (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al., 2008). (3) This distribution is properly normalized, as can be seen from (1) and (2). The graphic structure of this distribution results from a series of chains, each corresponding to a different time."}, {"heading": "1.2 POLICY GRADIENTS", "text": "The algorithm of the political gradients iteratively updates the political parameters in the direction of the gradient of \u03c0U (\u03c0) to increase U (\u03c0) and thus improve policy.These gradients can be calculated on the basis of identity, see e.g. (Salakhutdinov et al., 2003), \u2202 \u03c0 logU (\u03c0) = H \u2211 t = 1 t \u2211 \u03c4 = 1 < \u0445 \u03c0 log \u03c0 (a\u03c4 | s\u03c4) > p \u0445 (s\u03c4, a\u03c4, t | \u03c0). (8) In order to carry out a political update of both the EM and the political gradient algorithms, sufficient statistics on the reward-weighted path distribution in Equation (3) are therefore required; either the margins of government action on discrete problems or the moments in exponential family long-term problems. We point out that in terms of conclusions the only difference between these two algorithms is that in the algorithm the reward-weighted path depends on the current policy, during the previous algorithm-Q)."}, {"heading": "1.3 FORWARD-BACKWARD INFERENCE", "text": "In order to perform an update of the policy for finite horizon problems, we must calculate the marginal q (z\u03c4, t), as well as the marginal q (z\u03c4, t) and \u03c4 {1,.., t). For each component, the distribution q (z1: t | t) is chain-structured, and therefore all marginal q (z\u03c4 | t) can be calculated in linear time using message transmission, see e.g. (Wainwright and Jordan, 2008). Specifically, these marginal can be calculated using forward-backward recursions, also known as \u03b1-\u03b2 recursions. The initial messages, \u03b11, \u03b21, are given by 1 (z) = p0 (s) p (a | s), \u03b21 (z) = R (s, a), and the forward-backward recursions are given by forward-backward recursions."}, {"heading": "2 Q-INFERENCE", "text": "We proceed to show that the previously described O (H2) forward-backward algorithm = q = q functions (Q = q functions) does not fully exhaust the conditional independence structure of the distribution (3) and that a more efficient O (H) procedure exists. We focus first on the case of the finite horizon for which an exact algorithm exists, before extending the approach to the finite horizon in \u00a7 2.1. We first prove the following problem, which shows that due to the following state pair z\u03c4 + 1 and the time component t, the reward weighting of the orbit distributions via zaires is independent and equal to the system inversion dynamic.Lemma 1. We give the following problem: \""}, {"heading": "2.1 INFINITE PLANNING HORIZON", "text": "While we have considered only finite horizon problems, the recursive relation of Q functions (Q = fixed Q functions) can be used in endless horizon problems. To do this, we rely on the fact that, since the system is controllable, it will achieve its stationary distribution of states in finite time. (16) This relation can now be used to obtain a formula for calculating the finite number of state actions, then it is easy to show that for each future future distribution we can obtain the infinite summation in terms before and after the stationary state action distribution. (16) This relation can be used to obtain a formula for calculating the infinite number of state actions that have marginal reward weighted distribution of states. (17) First, we divide the infinite summation in terms before and after the stationary state action distribution."}, {"heading": "3 DYNAMIC PROGRAMMING", "text": "Equations (12) and (19) show a strong similarity to the policy evaluation of classical, infinite Q functions, see e.g. (Sutton and Barto, 1998), Q\u03c0 (s, a) = R (s, a) + \u03b3 \"s,\" a \"\u03c0 (a\" | s \") p (s\" | s, \"a) Q\u03c0 (s,\" a \"). However, although there is a strong similarity, there are also some significant differences. First, the Q functions (12) and (19) are inversely weighted by the total expected reward, because EM and political gradients function in the probability space. Furthermore, the standard Q functions of policy evaluation represent the total expected future reward given the current state-action pair, and therefore we do not depend on the fact that the Qgorithms have recently become aware of a new formulation of the EM algorithm called incremental EM (Toussaint et al)."}, {"heading": "4 CONTINUOUS MDPs", "text": "The proofs of lemma 1 and lemma 2 easily follow the continuous case, and the continuous version of the equation (12) takes the formQ\u03c4 (z\u03c4) = q (z\u03c4) = q (1). (21) Based on the summation in (21) we can see that the Qfunctions take the form of a two-component mixture model, with one component corresponding to the immediate reward, while the second corresponds to the future rewards. Although this mixture could be modelled explicitly, it is generally only necessary to calculate the momentary \u00b5i form (z). (N), N) Nto perform a policy update, and the recursive equation (21) can then be used to recursively calculate these moments in linear time as we illustrate it now."}, {"heading": "5 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 INFINITE HORIZON MDPs", "text": "The first experiment we conducted was the double reward chain problem, which was designed to highlight the vulnerability of the infinite horizon Q = 400p = reward function to get caught in the local optima when the \"time marginality criterion\" (Toussaint et al., 2006) is used as a convergence criterion and the \"time marginal\" is multimodal. The problem of the double reward chain N \u2212 length has N states in which we label the states from left to right in the chain, see Figure 3. In each state there are three possible actions; moving to the left in the chain, moving to the right in the chain, or staying in the current state. If the actor is in the left end point of the chain and moving to the left, he stays in the same state, with a similar situation in the right end point. All the dynamics of the transition to the system are deterministic. The actor gets a preference for staying in one of the two end points of the chain."}, {"heading": "5.2 ROBOT JOINT MANIPULATOR", "text": "The N -link rigid arm manipulator is a standard continuous model, consisting of an end reward q results q q results q connected with N connected rigid bodies. A graphic representation of a 3-link rigid manipulator is shown in Figure 5. A typical continuous control problem for such systems is to apply appropriate torque forces to the joints of the manipulator to move the end effector into a desired position. The state of the system is determined by q, q, q and q denote the angles, velocities and accelerations of the joints respectively the control variables are the torques applied to the joints. The nonlinear equations of state of the system are indicated by, e.g. (Spong et al., 2005), M (q) q \u00b2 + C (q) q) q + g (24) where M (q) is the inertia matrix, C (q)."}, {"heading": "6 CONCLUSION", "text": "Our new inference algorithm scales linearly with the planning horizon, while the standard forward-backward recursions scale quadratically with the horizon. While we have limited our attention to Markov decision processes, the methods in this paper are readily applicable to other Markovian control problems, such as partially observable Markov decision processes (Kaelbling et al., 1998). Furthermore, we have presented a novel algorithm for calculating the sufficient statistics of these distributions in endless horizon problems, where it is necessary to calculate an infinite number of margins over a distribution with an infinite number of variables, resulting in an alternative method for implementing the EM algorithm into finite horizon problems."}], "references": [{"title": "Variational Methods for Reinforcement Learning", "author": ["T. Furmston", "D. Barber"], "venue": "AISTATS, 9(13):241\u2013248,", "citeRegEx": "Furmston and Barber.,? \\Q2010\\E", "shortCiteRegEx": "Furmston and Barber.", "year": 2010}, {"title": "Trans-dimensional MCMC for Bayesian Policy Learning", "author": ["M. Hoffman", "A. Doucet", "N. de Freitas", "A. Jasra"], "venue": null, "citeRegEx": "Hoffman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2008}, {"title": "An Expectation Maximization Algorithm for Continuous Markov Decision Processes with Arbitrary Rewards", "author": ["M. Hoffman", "N. de Freitas", "A. Doucet", "J. Peters"], "venue": null, "citeRegEx": "Hoffman et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2009}, {"title": "Planning and Acting in Partially Observable Stochastic Domains", "author": ["L. Kaelbling", "M. Littman", "A. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Nonlinear Systems", "author": ["H. Khalil"], "venue": null, "citeRegEx": "Khalil.,? \\Q2001\\E", "shortCiteRegEx": "Khalil.", "year": 2001}, {"title": "Policy search for motor primitives in robotics", "author": ["J. Kober", "J. Peters"], "venue": "NIPS, 21:849\u2013856,", "citeRegEx": "Kober and Peters.,? \\Q2009\\E", "shortCiteRegEx": "Kober and Peters.", "year": 2009}, {"title": "Policy Gradient Methods for Robotics", "author": ["J. Peters", "S. Schaal"], "venue": "IROS, 21:2219\u20132225,", "citeRegEx": "Peters and Schaal.,? \\Q2006\\E", "shortCiteRegEx": "Peters and Schaal.", "year": 2006}, {"title": "Optimization with EM and Expectation-ConjugateGradient", "author": ["R. Salakhutdinov", "S. Roweis", "Z. Ghahramani"], "venue": "ICML, (20):672\u2013679,", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2003}, {"title": "Robot Modelling and Control", "author": ["M. Spong", "S. Hutchinson", "M. Vidyasagar"], "venue": null, "citeRegEx": "Spong et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Spong et al\\.", "year": 2005}, {"title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation", "author": ["R. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": null, "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Pros and Cons of truncated Gaussian EP in the context of Approximate Inference Control", "author": ["M. Toussaint"], "venue": "NIPS - Workshop on Probabilistic Approaches for Robotics and Control.,", "citeRegEx": "Toussaint.,? \\Q2009\\E", "shortCiteRegEx": "Toussaint.", "year": 2009}, {"title": "Probabilistic inference for solving (PO)MDPs", "author": ["M. Toussaint", "S. Harmeling", "A. Storkey"], "venue": "Research Report EDI-INF-RR-0934,", "citeRegEx": "Toussaint et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Toussaint et al\\.", "year": 2006}, {"title": "Bayesian Time Series Models, chapter Expectation-Maximization methods for solving (PO)MDPs and optimal control problems", "author": ["M. Toussaint", "A. Storkey", "S. Harmeling"], "venue": null, "citeRegEx": "Toussaint et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Toussaint et al\\.", "year": 2011}, {"title": "Graphical Models, Exponential Families, and Variational Inference", "author": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan.,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan.", "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "A Markov Decision Problem (MDP) is described by an initial state distribution p1(s1), transition distributions p(st+1|st, at) and reward function Rt(st, at), where the state and action at time t are denoted by st and at respectively (Sutton and Barto, 1998).", "startOffset": 233, "endOffset": 257}, {"referenceID": 10, "context": "Classical planning algorithms, such as Policy Iteration or Value Iteration (Sutton and Barto, 1998), generally focus on greedy updates of the policy.", "startOffset": 75, "endOffset": 99}, {"referenceID": 9, "context": "(Sutton et al., 2000), and the Expectation Maximisation algorithm, e.", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "(Toussaint et al., 2006), have been particularly popular and have been successfully applied to a large range of complex domains including optimal control (Toussaint et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 12, "context": ", 2006), have been particularly popular and have been successfully applied to a large range of complex domains including optimal control (Toussaint et al., 2006), robotics (Kober and Peters, 2009; Peters and Schaal, 2006) and Bayesian reinforcement learning (Furmston and Barber, 2010).", "startOffset": 137, "endOffset": 161}, {"referenceID": 5, "context": ", 2006), robotics (Kober and Peters, 2009; Peters and Schaal, 2006) and Bayesian reinforcement learning (Furmston and Barber, 2010).", "startOffset": 18, "endOffset": 67}, {"referenceID": 6, "context": ", 2006), robotics (Kober and Peters, 2009; Peters and Schaal, 2006) and Bayesian reinforcement learning (Furmston and Barber, 2010).", "startOffset": 18, "endOffset": 67}, {"referenceID": 0, "context": ", 2006), robotics (Kober and Peters, 2009; Peters and Schaal, 2006) and Bayesian reinforcement learning (Furmston and Barber, 2010).", "startOffset": 104, "endOffset": 131}, {"referenceID": 12, "context": "By expressing the objective function (1) as the likelihood function of an appropriately constructed mixture model the MDP can be solved using techniques from probabilistic inference, such as EM (Dayan and Hinton, 1997; Toussaint et al., 2006; Kober and Peters, 2009; Toussaint et al., 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al.", "startOffset": 194, "endOffset": 290}, {"referenceID": 5, "context": "By expressing the objective function (1) as the likelihood function of an appropriately constructed mixture model the MDP can be solved using techniques from probabilistic inference, such as EM (Dayan and Hinton, 1997; Toussaint et al., 2006; Kober and Peters, 2009; Toussaint et al., 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al.", "startOffset": 194, "endOffset": 290}, {"referenceID": 13, "context": "By expressing the objective function (1) as the likelihood function of an appropriately constructed mixture model the MDP can be solved using techniques from probabilistic inference, such as EM (Dayan and Hinton, 1997; Toussaint et al., 2006; Kober and Peters, 2009; Toussaint et al., 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al.", "startOffset": 194, "endOffset": 290}, {"referenceID": 11, "context": ", 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al.", "startOffset": 12, "endOffset": 56}, {"referenceID": 0, "context": ", 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al.", "startOffset": 12, "endOffset": 56}, {"referenceID": 1, "context": ", 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al., 2008).", "startOffset": 65, "endOffset": 87}, {"referenceID": 7, "context": "(Salakhutdinov et al., 2003),", "startOffset": 0, "endOffset": 28}, {"referenceID": 14, "context": "(Wainwright and Jordan, 2008).", "startOffset": 0, "endOffset": 29}, {"referenceID": 14, "context": "Using the previous remarks it is clear that each of these terms can be calculated in linear time (Wainwright and Jordan, 2008).", "startOffset": 97, "endOffset": 126}, {"referenceID": 12, "context": "Before proceeding we make a brief note about the \u2018time-marginal\u2019 criterion used in (Toussaint et al., 2006).", "startOffset": 83, "endOffset": 107}, {"referenceID": 12, "context": "It is not possible to implement the infinite horizon policy update function of (Toussaint et al., 2006) exactly and a finite horizon is therefore selected which will give a good approximation.", "startOffset": 79, "endOffset": 103}, {"referenceID": 12, "context": "To select an appropriate finite horizon (Toussaint et al., 2006) propose to use the \u2018time-marginal\u2019 q(t), which can be calculated up to proportionality using the equation", "startOffset": 40, "endOffset": 64}, {"referenceID": 10, "context": "(Sutton and Barto, 1998),", "startOffset": 0, "endOffset": 24}, {"referenceID": 13, "context": "We have recently become of aware of a new formulation of the EM algorithm, called incremental EM (Toussaint et al., 2011), that also converges in the limit.", "startOffset": 97, "endOffset": 121}, {"referenceID": 2, "context": "For illustrative purposes we consider the specific example of linear continuous MDPs with arbitrary rewards (Hoffman et al., 2009).", "startOffset": 108, "endOffset": 130}, {"referenceID": 2, "context": "(Hoffman et al., 2009),", "startOffset": 0, "endOffset": 22}, {"referenceID": 2, "context": "Given these moments the policy is updated by first solving a set of linear equations in K and m, and then solving for \u03c0\u03c3, see (Hoffman et al., 2009).", "startOffset": 126, "endOffset": 148}, {"referenceID": 2, "context": "To summarize, instead of calculating the forward and backward messages concurrently and then calculating the marginals q(z\u03c4 , t) separately, as in (Hoffman et al., 2009), we have first calculated the forward messages and then used (22) and (23) to calculate the moments of the Q-functions recursively.", "startOffset": 147, "endOffset": 169}, {"referenceID": 2, "context": "These recursive equations allow the moments necessary for a policy update to be calculated in linear time, which compares favorably with the forward-backward recursions of (Hoffman et al., 2009) that have quadratic runtime.", "startOffset": 172, "endOffset": 194}, {"referenceID": 12, "context": "The first experiment we performed was on the double reward chain problem, which was designed to highlight the susceptibility of the infinite horizon EM algorithm to get caught in local optima when the \u2018time-marginal\u2019 criterion (Toussaint et al., 2006) is used as a convergence criterion and the \u2018time-marginal\u2019 is multi-modal.", "startOffset": 227, "endOffset": 251}, {"referenceID": 8, "context": "(Spong et al., 2005),", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "(Khalil, 2001), and in the case of an N -link rigid manipulator recasts the torque action space into the acceleration action space.", "startOffset": 0, "endOffset": 14}, {"referenceID": 2, "context": "While this reward function can be modelled as a mixture of Gaussians, see (Hoffman et al., 2009), for simplicity we consider the simpler problem where the reward is a function of q, q\u0307 and q\u0308 directly.", "startOffset": 74, "endOffset": 96}, {"referenceID": 2, "context": "The plot shows the results for the Q-inference algorithm \u00a74 (blue) and the forward-backward inference algorithm of (Hoffman et al., 2009) (red).", "startOffset": 115, "endOffset": 137}, {"referenceID": 2, "context": "The plot shows the results for our continuous Q-inference algorithm \u00a74 (blue) and the forwardbackward inference algorithm of (Hoffman et al., 2009) (red).", "startOffset": 125, "endOffset": 147}, {"referenceID": 3, "context": "While we have restricted our attention to Markov decision processes the methods in this paper are readily applicable to other Markovian control problems, such as partially observable Markov decision processes (Kaelbling et al., 1998).", "startOffset": 209, "endOffset": 233}], "year": 2011, "abstractText": "Markov control algorithms that perform smooth, non-greedy updates of the policy have been shown to be very general and versatile, with policy gradient and Expectation Maximisation algorithms being particularly popular. For these algorithms, marginal inference of the reward weighted trajectory distribution is required to perform policy updates. We discuss a new exact inference algorithm for these marginals in the finite horizon case that is more efficient than the standard approach based on classical forwardbackward recursions. We also provide a principled extension to infinite horizon Markov Decision Problems that explicitly accounts for an infinite horizon. This extension provides a novel algorithm for both policy gradients and Expectation Maximisation in infinite horizon problems. 1 MARKOV DECISION PROBLEMS A Markov Decision Problem (MDP) is described by an initial state distribution p1(s1), transition distributions p(st+1|st, at) and reward function Rt(st, at), where the state and action at time t are denoted by st and at respectively (Sutton and Barto, 1998). The state and action spaces can be either discrete or continuous. For a discount factor \u03b3 the reward is defined as Rt(st, at) = \u03b3R(st, at) for a stationary reward R(st, at), where \u03b3 \u2208 [0, 1). We assume a stationary policy, \u03c0, defined as a set of conditional distributions over the action space, \u03c0a,s = p(at = a|st = s, \u03c0). The total expected reward of the MDP (the policy utility) To avoid cumbersome notation we also use the notation zt = {st, at} to denote a state-action pair. We use the bold typeface, zt, to denote a vector. is given by", "creator": "TeX"}}}