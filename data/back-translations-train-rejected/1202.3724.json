{"id": "1202.3724", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Probabilistic Theorem Proving", "abstract": "Many representation schemes combining first-order logic and probability have been proposed in recent years. Progress in unifying logical and probabilistic inference has been slower. Existing methods are mainly variants of lifted variable elimination and belief propagation, neither of which take logical structure into account. We propose the first method that has the full power of both graphical model inference and first-order theorem proving (in finite domains with Herbrand interpretations). We first define probabilistic theorem proving, their generalization, as the problem of computing the probability of a logical formula given the probabilities or weights of a set of formulas. We then show how this can be reduced to the problem of lifted weighted model counting, and develop an efficient algorithm for the latter. We prove the correctness of this algorithm, investigate its properties, and show how it generalizes previous approaches. Experiments show that it greatly outperforms lifted variable elimination when logical structure is present. Finally, we propose an algorithm for approximate probabilistic theorem proving, and show that it can greatly outperform lifted belief propagation.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (352kb)", "http://arxiv.org/abs/1202.3724v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["vibhav gogate", "pedro domingos"], "accepted": false, "id": "1202.3724"}, "pdf": {"name": "1202.3724.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Theorem Proving", "authors": ["Vibhav Gogate"], "emails": ["vgogate@cs.washington.edu", "pedrod@cs.washington.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most people who are able to move are able to move, to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move, to move and to move, to move, to move, to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move, to move and to move."}, {"heading": "2 LOGIC AND THEOREM PROVING", "text": "This year is the highest in the history of the country."}, {"heading": "3 PROBLEM DEFINITION", "text": "Following Nilsson [27], we define probabilistic theorems, which are defined as the problem of determining the probability of any query formula (Q = > Q), which includes a set of logical formulas Fi and their probabilities P (Fi). In order for the problem to be well defined, probabilities must be consistent, and Nilsson [27] provides a method for verifying consistency. Probabilities estimated by maximum probability from an observed world are guaranteed to be consistent [13]. Generally, a set of formula probabilities does not establish a complete common distribution across the atoms that appear in them, but a method can be obtained by making the maximum entropy assumption: the distribution does not contain information specified by the formula probabilities [27]. Determining the maximum entropy distribution in the face of a set of formula probabilities is equivalent to learning a maximum probability model whose characteristics are the formulas; many algorithms are available for this purpose (gradualization)."}, {"heading": "4 PROPOSITIONAL CASE", "text": "The probability of a formula is the sum of the probabilities of the worlds it satisfies. Thus, the probability of a formula Q is so high that it is sufficient to add the partition function of K with and without Q as a hard formula: P (Q | K) = 1 Q (x). the probability of a formula Q (K) = Z (K)} Z (2), where 1Q) is the indicator function (1), if Q (x) is the indicator function (1)."}, {"heading": "5 FIRST-ORDER CASE", "text": "Let us first consider the case of PCBs without existential quantifiers. (Algorithms 2 and 3 remain essentially unchanged except that formulas, letters and CNF conversion are now of primary importance.) Especially for theorem 1, to remain true, each new atom Ai in algorithm 2 must now consist of a new predicate symbol, followed by a parenthesized list of variables and constants in the corresponding formula Fi. The proof of the first-ranking version of the theorem then follows through propositionalization. Algorithm 4 is the focus of the rest of this section. Algorithm 5 LWMC (CNF C, nouns). S, weights W) / Lifted base case if all clauses in C are met. (A) (WA + W \u00ac A) nA (S) if C has an empty unfulfilled clause."}, {"heading": "5.1 LIFTING THE BASE CASE", "text": "The basic case changes only by increasing the sum of the weights of each atom of first order to nA (S), the number of bases of A that are compatible with the constraints in S. This is necessary and sufficient, since each atom A has nA (S) bases and all base atoms are independent, because the CNF is satisfied regardless of their truth values. Note that nA (S) is the number of bases of A that are consistent with S and that can be formed using all the constants in the original CNF."}, {"heading": "5.2 LIFTING THE DECOMPOSITION STEP", "text": "It is clear that C is decomposed into two or more CNFs, so that no two CNFs have any unifiable letters in common, an increased decomposition of C is possible (i.e., a decomposition of C into first-order CNFs), but the symmetry of first-order representation can be further exploited (i.e., a decomposition of C into first-order k CNFs cannot be unifiable letters in k CNFs C1,., Ck-sharing, and those identical to Cj for all i, j, Ci up to a renaming of variables and constants, 1 then LWMC (C) = [LWMC).We formalize these conditions below. Definition 1 The set of first-order CNFs {C1,1, C1, m1, m1, m1,"}, {"heading": "5.3 LIFTING THE SPLITTING STEP", "text": "Splitting up on an ungrounded atom means splitting up on all bases that are in accordance with the current substitution provisions. Naively, if the atom c has bases that are in accordance with S, this will result in a sum of 2c recursive calls to the LWMC, one for each possible truth assignment to the c-grounded atoms. However, in general, these calls will have a repeated structure and can be replaced by a much smaller number. The cancelled splitting step takes advantage of this. We introduce some notations and definitions. Let \u03c3A, S designate a truth assignment to the bases of the atom A in accordance with the substitution provisions S and let A, S denote the annulment of all possible assignments. Let C | \u03c3A, S denote the CNF formed by removing A from all clauses in which it appears, and placing on all bases that are fulfilled."}, {"heading": "C under S, then", "text": "LWMC (C, S, W) = l \u2211 i = 1niW ti AW fi \u00ac ALWMC (C | \u03c3j; Sj, W), where ni = | \u03a3 (i) A, S |, \u03c3j improved (i) A, S, ti and fi are the number of true or false atoms in \u03c3j and Sj is extended by the substitution constraints required to form C | \u03c3j. Here, too, we can derive rules for identifying a superseded fission by using the counting arguments in de Salvo Braz [10] and the general binomical rule in Jha et al. [20]. We ignore the details for lack of space. In the worst case, the superseded splitting standard is used for splitting to a soil atom. In most subsequent problems, the PKB will contain many hard unity clauses (the proofs) containing the negative unity clauses (the proofs)."}, {"heading": "5.4 EXTENSIONS", "text": "This year is the highest in the history of the country."}, {"heading": "5.5 THEORETICAL PROPERTIES", "text": "This year it has come to the point that it will be able to eren.n"}, {"heading": "5.6 DISCUSSION", "text": "PTP leads to new algorithms for several of the inference problems in Figure 1. For example, ignoring weights and replacing products with conjunctions and sums by disjunctions in Algorithm 5 leads to an upscale version of DPLL for first-order theorem testing (cf. [2]). Of the standard methods for inference in graphical models, propositional PTP most closely resembles recursive conditioning [5] and AND / OR search [12] with context-sensitive decomposition and caching, but applies to any PKBs, not just Bayesian networks. In addition, PTP effectively performs formula-based inference [17]; if it splits into one of the auxiliary atoms introduced by Algorithm 2.PTP, it realizes some of the benefits of rotten inference for relational models [31] by holding in elevated form what rotten maintenance would leave by default."}, {"heading": "6 APPROXIMATE INFERENCE", "text": "LWMC willingly borrows a Monte Carlo approximation by replacing the sum in the splitting step with a random selection of one of its terms (A > W > W), calling the algorithm many times and calculating the results. This results in the first cancelled sampling algorithm distribution. We first apply this meaning to the current CNF-C approach and the actual weights. Then we return WAQ (A | C). The two algorithms differ only in the last line. Let us specify the meaning or suggestion of the distribution via A. Then we return the WAQ (A | C) and the actual weights. Then we return the WAQ (C) -WMC-WMC (C; W) with the probability. It is easy to show that theorem 6 If Q (A | C, W) MC-WMC-WMC (C)."}, {"heading": "7 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 EXACT INFERENCE", "text": "In this subsection, we compare the performance of PTP and FOVE on randomly generated and linked predictions of PCBs. We implemented PTP in C + + and conducted all our experiments on a Linux machine with a 2.33 GHz Intel Xeon processor and 2GB of RAM. We used a constraint solver based on maximum verification of substitution control to implement the substitution controls. We used the following heuristics for splitting. At each point, we prefer an atom that yields the smallest number of recursive calls to LWMC (i.e., an atom that yields maximum yields). We break bonds by selecting an atom that appears in the largest number of bottom clauses; this number can be calculated using the constraint solver. If it is the same for two or more atoms, we break the same number of atoms, we break clauses with varying."}, {"heading": "7.2 APPROXIMATE INFERENCE", "text": "In this subparagraph, we compare the performance of MCLWMC, MC-WMC, Faith Propagation Survey [38] and MC-SAT [30] across two domains. We used the Entity Resolution (Cora) and Collective Classification Data Sets and Markov Logic Networks used in Singla and Domingos [37] and Poon and Domingos [30], respectively. Cora data set contains 1295 citations from 132 different research papers, and the follow-up task is to identify duplicate citations, authors, titles and venues. The Collective Classification Data Set consists of approximately 3000 query atoms. As the calculation of the exact posterior margins in these domains is not feasible, we used the following evaluation method: We divided the data into two sets of equal size: evidence set and test set. We then calculated the probability of each soil atom in the test set using the four inference algorithms."}, {"heading": "8 CONCLUSION", "text": "Probabilistic Theorem Proproving (PTP) combines theorem evidence and probabilistic conclusions. This work proposed an algorithm for PTP based on the reduction to weighted model counting, and demonstrated, both theoretically and empirically, that it has significant advantages over earlier weighted probabilistic inference algorithms. PTP implementation will be available in the alchemy system [25]. Further research guidelines include: extension of PTP to infinite, first-order non-herbbrand logic; new cancelled inference rules; theoretical liability analysis; porting to PTP faster techniques from logical and probable conclusions; elimination of splitting heuristics; better handling of existential PTP algorithms; better meaning allocations; approximate cancellation; answering multiple requests at the same time; applications; etc. Acknowledgement research was partially funded by FA0000006-FA00000FA01-FA0001-FA0001-FA001-FA00000000001"}], "references": [{"title": "Representing and Reasoning with Probabilistic Knowledge", "author": ["F. Bacchus"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "The model evolution calculus as a first-order DPLL method", "author": ["P. Baumgartner", "C. Tinelli"], "venue": "Artif. Intell., 172(4-5):591\u2013 632", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Jr", "author": ["R.J. Bayardo"], "venue": "and J. D. Pehoushek. Counting models using connected components. In AAAI, pages 157\u2013162", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "On probabilistic inference by weighted model counting", "author": ["M. Chavira", "A. Darwiche"], "venue": "Artif. Intell., 172(6-7):772\u2013799", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Recursive conditioning", "author": ["A. Darwiche"], "venue": "Artif. Intell., 126:5\u2013 41", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "A logical approach to factoring belief networks", "author": ["Adnan Darwiche"], "venue": "In KR, pages 409\u2013420,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "A machine program for theorem proving", "author": ["M. Davis", "G. Logemann", "D. Loveland"], "venue": "CACM, 5:394\u2013397", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1962}, {"title": "A computing procedure for quantification theory", "author": ["M. Davis", "H. Putnam"], "venue": "JACM, 7(3)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1960}, {"title": "Problog: A probabilistic prolog and its application in link discovery", "author": ["L. De Raedt", "A. Kimmig", "H. Toivonen"], "venue": "IJCAI, pages 2462\u20132467", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Lifted First-Order Probabilistic Inference", "author": ["R. de Salvo Braz"], "venue": "PhD thesis, Univ. of Illinois,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artif. Intell., 113:41\u201385", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "AND/OR search spaces for graphical models", "author": ["R. Dechter", "R. Mateescu"], "venue": "Artif. Intell., 171(2-3):73\u2013106", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Inducing features of random fields", "author": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 19:380\u2013392", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Markov Logic: An Interface Layer for Artificial Intelligence", "author": ["P. Domingos", "D. Lowd"], "venue": "Morgan & Claypool", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Logical Foundations of Artificial Intelligence", "author": ["M.R. Genesereth", "N.J. Nilsson"], "venue": "Morgan Kaufmann", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1987}, {"title": "editors", "author": ["L. Getoor", "B. Taskar"], "venue": "Introduction to Statistical Relational Learning. MIT Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Formula-based probabilistic inference", "author": ["V. Gogate", "P. Domingos"], "venue": "UAI, pages 210\u2013219", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "SampleSearch: Importance Sampling in presence of Determinism", "author": ["Vibhav Gogate", "Rina Dechter"], "venue": "Artif. Intell.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "An analysis of first-order logics of probability", "author": ["J. Halpern"], "venue": "Artif. Intell., 46:311\u2013350", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1990}, {"title": "Lifted inference from the other side: The tractable features", "author": ["A. Jha", "V. Gogate", "A. Meliou", "D. Suciu"], "venue": "NIPS, pages 973\u2013981", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Counting Belief Propagation", "author": ["K. Kersting", "B. Ahmadi", "S. Natarajan"], "venue": "UAI, pages 277\u2013284", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Constraint processing in lifted probabilistic inference", "author": ["J. Kisynski", "D. Poole"], "venue": "UAI, pages 293\u2013302", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "S.J. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "IJCAI, pages 1352\u20131359", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Lifted probabilistic inference with counting formulas", "author": ["B. Milch", "L.S. Zettlemoyer", "K. Kersting", "M. Haimes", "L.P. Kaelbling"], "venue": "AAAI, pages 1062\u20131068", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "The Alchemy system for statistical relational AI", "author": ["S. Kok", "M. Sumner", "M. Richardson", "P. Singla", "H. Poon", "P. Domingos"], "venue": "Tech. Rept., Dept. CSE, Univ. Washington", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic modelling", "author": ["K.S. Ng", "J.W. Lloyd", "W.T. Uther"], "venue": "inference and learning using logical theories. AMAI Journal, 54(1-3):159\u2013205", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic logic", "author": ["N. Nilsson"], "venue": "Artif. Intell., 28:71\u201387", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1986}, {"title": "Using weighted MAX-SAT engines to solve MPE", "author": ["James D. Park"], "venue": "In AAAI,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "First-order probabilistic inference", "author": ["D. Poole"], "venue": "IJCAI, pages 985\u2013991", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2003}, {"title": "Sound and efficient inference with probabilistic and deterministic dependencies", "author": ["H. Poon", "P. Domingos"], "venue": "AAAI, pages 458\u2013463", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "A general method for reducing the complexity of relational inference and its application to MCMC", "author": ["H. Poon", "P. Domingos", "M. Sumner"], "venue": "AAAI, pages 1075\u20131080", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "A machine-oriented logic based on the resolution principle", "author": ["J.A. Robinson"], "venue": "JACM, 12:23\u201341", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1965}, {"title": "Simulation and the Monte Carlo Method", "author": ["Reuven Y. Rubinstein"], "venue": null, "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1981}, {"title": "Solving Bayesian networks by weighted model counting", "author": ["T. Sang", "P. Beame", "H. Kautz"], "venue": "AAAI, pages 475\u2013482", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2005}, {"title": "Heuristics for fast exact model counting", "author": ["T. Sang", "P. Beame", "H.A. Kautz"], "venue": "SAT, pages 226\u2013240", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Bisimulation-based approximate lifted inference", "author": ["P. Sen", "A. Deshpande", "L. Getoor"], "venue": "UAI, pages 496\u2013505", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative training of Markov logic networks", "author": ["P. Singla", "P. Domingos"], "venue": "AAAI, pages 868\u2013873", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2005}, {"title": "Lifted first-order belief propagation", "author": ["P. Singla", "P. Domingos"], "venue": "AAAI, pages 1094\u20131099", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "OLD resolution with tabulation", "author": ["H. Tamaki", "T. Sato"], "venue": "ICLP, pages 84\u201398", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1986}, {"title": "From knowledge bases to decision models", "author": ["M. Wellman", "J.S. Breese", "R.P. Goldman"], "venue": "Knowl. Eng. Rev., 7", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1992}], "referenceMentions": [{"referenceID": 26, "context": "Proposals go back to at least Nilsson [27], with substantial progress within the UAI community starting in the 1990s (e.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": ", [1, 19, 40]), and added impetus from the new field of statistical relational learning starting in the 2000s [16].", "startOffset": 2, "endOffset": 13}, {"referenceID": 18, "context": ", [1, 19, 40]), and added impetus from the new field of statistical relational learning starting in the 2000s [16].", "startOffset": 2, "endOffset": 13}, {"referenceID": 39, "context": ", [1, 19, 40]), and added impetus from the new field of statistical relational learning starting in the 2000s [16].", "startOffset": 2, "endOffset": 13}, {"referenceID": 15, "context": ", [1, 19, 40]), and added impetus from the new field of statistical relational learning starting in the 2000s [16].", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": ", [9, 14, 23]), but the state of inference is less advanced.", "startOffset": 2, "endOffset": 13}, {"referenceID": 13, "context": ", [9, 14, 23]), but the state of inference is less advanced.", "startOffset": 2, "endOffset": 13}, {"referenceID": 22, "context": ", [9, 14, 23]), but the state of inference is less advanced.", "startOffset": 2, "endOffset": 13}, {"referenceID": 31, "context": ", reason over large domains in time independent of the number of objects they contain, using techniques like resolution theorem proving [32].", "startOffset": 136, "endOffset": 140}, {"referenceID": 28, "context": "An algorithm for lifted variable elimination was proposed by Poole [29] and extended by de Salvo Braz [10] and others.", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "An algorithm for lifted variable elimination was proposed by Poole [29] and extended by de Salvo Braz [10] and others.", "startOffset": 102, "endOffset": 106}, {"referenceID": 37, "context": "Lifted belief propagation was introduced by Singla and Domingos [38] and extended by others (e.", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": ", [21, 36]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 35, "context": ", [21, 36]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 5, "context": "We first do the corresponding reduction for the propositional case, extending previous work by Darwiche [6] and Sang et al.", "startOffset": 104, "endOffset": 107}, {"referenceID": 33, "context": "[34] (see also [4]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[34] (see also [4]).", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": "We begin with a brief review of propositional logic, firstorder logic and theorem proving [15].", "startOffset": 90, "endOffset": 94}, {"referenceID": 7, "context": "The earliest theorem prover is the Davis-Putnam algorithm (henceforth called DP) [8].", "startOffset": 81, "endOffset": 84}, {"referenceID": 10, "context": "As Dechter [11] points out, DP is in fact just the variable elimination algorithm for the special case of 0-1 potentials.", "startOffset": 11, "endOffset": 15}, {"referenceID": 6, "context": "Modern propositional theorem provers use the DPLL algorithm [7], a variant of DP that replaces the elimination step with a splitting step: instead of eliminating all clauses containing the chosen atom A, resolve all clauses in the KB with A, simplify and recurse, and do the same with \u00acA.", "startOffset": 60, "endOffset": 63}, {"referenceID": 31, "context": "A more sophisticated alternative is first-order resolution [32], which proceeds by resolving pairs of clauses and adding the result to the KB until the empty clause is derived.", "startOffset": 59, "endOffset": 63}, {"referenceID": 26, "context": "Following Nilsson [27], we define probabilistic theorem proving as the problem of determining the probability of an arbitrary query formulaQ given a set of logical formulas Fi and their probabilities P (Fi).", "startOffset": 18, "endOffset": 22}, {"referenceID": 26, "context": "For the problem to be well defined, the probabilities must be consistent, and Nilsson [27] provides a method for verifying consistency.", "startOffset": 86, "endOffset": 90}, {"referenceID": 12, "context": "Probabilities estimated by maximum likelihood from an observed world are guaranteed to be consistent [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 26, "context": "In general, a set of formula probabilities does not specify a complete joint distribution over the atoms appearing in them, but one can be obtained by making the maximum entropy assumption: the distribution contains no information beyond that specified by the formula probabilities [27].", "startOffset": 282, "endOffset": 286}, {"referenceID": 12, "context": ") [13].", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "In order to compactly subsume standard probabilistic models, we interpret a universally quantified formula as a set of features, one for each grounding of the formula, as in Markov logic [14].", "startOffset": 187, "endOffset": 191}, {"referenceID": 13, "context": "Determining whether a KB K logically entails a queryQ is equivalent to determining whether P (Q|K) = 1 [14].", "startOffset": 103, "endOffset": 107}, {"referenceID": 3, "context": "Graphical models are easily converted into equivalent PKBs [4].", "startOffset": 59, "endOffset": 62}, {"referenceID": 3, "context": "Weighted model counting can be defined as follows [4].", "startOffset": 50, "endOffset": 53}, {"referenceID": 27, "context": "Park [28], and de Salvo Braz [10] on the lifted case.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "Park [28], and de Salvo Braz [10] on the lifted case.", "startOffset": 29, "endOffset": 33}, {"referenceID": 4, "context": "This section generalizes the Bayesian network inference techniques in Darwiche [5] and Sang et al.", "startOffset": 79, "endOffset": 82}, {"referenceID": 33, "context": "[34] to arbitrary propositional PKBs, evidence, and query formulas.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": ") WMC(C,W ) can be any weighted model counting algorithm [4].", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "Most model counters are variations of Relsat, itself an extension of DPLL [3].", "startOffset": 74, "endOffset": 77}, {"referenceID": 34, "context": "The atom to split on in the splitting step can be chosen using various heuristics [35].", "startOffset": 82, "endOffset": 86}, {"referenceID": 9, "context": "Rules for identifying lifted decompositions can be derived in a straightforward manner from the inversion argument in de Salvo Braz [10] and the power rule in Jha et al.", "startOffset": 132, "endOffset": 136}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Note that this rule is more general than de Salvo Braz\u2019s inversion elimination [10].", "startOffset": 79, "endOffset": 83}, {"referenceID": 21, "context": "In general, a CNF can be partitioned into subsets of identical but disjoint CNFs using constraint satisfaction techniques, as in Kisynski and Poole [22].", "startOffset": 148, "endOffset": 152}, {"referenceID": 9, "context": "Again, we can derive rules for identifying a lifted split by using the counting arguments in de Salvo Braz [10] and the generalized binomial rule in Jha et al.", "startOffset": 107, "endOffset": 111}, {"referenceID": 19, "context": "[20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "In general, the atom to split on in Algorithm 5 should be chosen with the goal of yielding lifted decompositions in the recursive calls (for example, using lifted versions of the propositional heuristics [35]).", "startOffset": 204, "endOffset": 208}, {"referenceID": 19, "context": "[20] and Milch et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] contain examples of other lifting rules.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Other techniques used in propositional inference that can be ported to LWMC include pure literals, clause learning, clause indexing, and random restarts [3, 35, 4].", "startOffset": 153, "endOffset": 163}, {"referenceID": 34, "context": "Other techniques used in propositional inference that can be ported to LWMC include pure literals, clause learning, clause indexing, and random restarts [3, 35, 4].", "startOffset": 153, "endOffset": 163}, {"referenceID": 3, "context": "Other techniques used in propositional inference that can be ported to LWMC include pure literals, clause learning, clause indexing, and random restarts [3, 35, 4].", "startOffset": 153, "endOffset": 163}, {"referenceID": 34, "context": "Caching in LWMC corresponds to both caching in model counting [35] and recursive conditioning [5] and to memoization of common subproofs in theorem proving [39].", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "Caching in LWMC corresponds to both caching in model counting [35] and recursive conditioning [5] and to memoization of common subproofs in theorem proving [39].", "startOffset": 94, "endOffset": 97}, {"referenceID": 38, "context": "Caching in LWMC corresponds to both caching in model counting [35] and recursive conditioning [5] and to memoization of common subproofs in theorem proving [39].", "startOffset": 156, "endOffset": 160}, {"referenceID": 39, "context": "evant to the query, and then propositionalizes the result and performs standard probabilistic inference on it [40].", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "We now theoretically compare the efficiency of PTP and first-order variable elimination (FOVE) [29, 10].", "startOffset": 95, "endOffset": 103}, {"referenceID": 9, "context": "We now theoretically compare the efficiency of PTP and first-order variable elimination (FOVE) [29, 10].", "startOffset": 95, "endOffset": 103}, {"referenceID": 9, "context": "Neither counting elimination [10] nor inversion elimination [29] is applicable here, and therefore the complexity of FOVE will be the same as that of (propositional) variable elimination, i.", "startOffset": 29, "endOffset": 33}, {"referenceID": 28, "context": "Neither counting elimination [10] nor inversion elimination [29] is applicable here, and therefore the complexity of FOVE will be the same as that of (propositional) variable elimination, i.", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "The main insight for this result comes from previous work on recursive conditioning [5] and AND/OR search [12].", "startOffset": 84, "endOffset": 87}, {"referenceID": 11, "context": "The main insight for this result comes from previous work on recursive conditioning [5] and AND/OR search [12].", "startOffset": 106, "endOffset": 110}, {"referenceID": 9, "context": "De Salvo Braz\u2019s FOVE [10] and lifted BP [38] completely shatter the PKB in advance.", "startOffset": 21, "endOffset": 25}, {"referenceID": 37, "context": "De Salvo Braz\u2019s FOVE [10] and lifted BP [38] completely shatter the PKB in advance.", "startOffset": 40, "endOffset": 44}, {"referenceID": 28, "context": "Like Poole [29] and Ng et al.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "[26], LWMC splits only as needed.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "Of the standard methods for inference in graphical models, propositional PTP is most similar to recursive conditioning [5] and AND/OR search [12] with context-sensitive decomposition and caching, but applies to arbitrary PKBs, not just Bayesian networks.", "startOffset": 119, "endOffset": 122}, {"referenceID": 11, "context": "Of the standard methods for inference in graphical models, propositional PTP is most similar to recursive conditioning [5] and AND/OR search [12] with context-sensitive decomposition and caching, but applies to arbitrary PKBs, not just Bayesian networks.", "startOffset": 141, "endOffset": 145}, {"referenceID": 16, "context": "Also, PTP effectively performs formula-based inference [17] when it splits on one of the auxiliary atoms introduced by Algorithm 2.", "startOffset": 55, "endOffset": 59}, {"referenceID": 30, "context": "PTP realizes some of the benefits of lazy inference for relational models [31] by keeping in lifted form what lazy inference would leave as default.", "startOffset": 74, "endOffset": 78}, {"referenceID": 32, "context": "We first apply this importance sampling approach [33] to WMC, yielding the MC-WMC algorithm.", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "By importance sampling theory [33] and by the law of total expectation, it is easy to show that:", "startOffset": 30, "endOffset": 34}, {"referenceID": 32, "context": "It is well known that the accuracy of the estimate is inversely proportional to its variance [33].", "startOffset": 93, "endOffset": 97}, {"referenceID": 17, "context": "MC-WMC suffers from the rejection problem [18]: it may return a zero.", "startOffset": 42, "endOffset": 46}, {"referenceID": 17, "context": "We can solve this problem by either backtracking when a sample is rejected or by generating samples from the backtrack-free distribution [18].", "startOffset": 137, "endOffset": 141}, {"referenceID": 37, "context": "In this subsection, we compare the performance of MCLWMC, MC-WMC, lifted belief propagation [38], and MC-SAT [30] on two domains.", "startOffset": 92, "endOffset": 96}, {"referenceID": 29, "context": "In this subsection, we compare the performance of MCLWMC, MC-WMC, lifted belief propagation [38], and MC-SAT [30] on two domains.", "startOffset": 109, "endOffset": 113}, {"referenceID": 36, "context": "We used the entity resolution (Cora) and collective classification datasets and Markov logic networks used in Singla and Domingos [37] and Poon and Domingos [30] respectively.", "startOffset": 130, "endOffset": 134}, {"referenceID": 29, "context": "We used the entity resolution (Cora) and collective classification datasets and Markov logic networks used in Singla and Domingos [37] and Poon and Domingos [30] respectively.", "startOffset": 157, "endOffset": 161}, {"referenceID": 24, "context": "An implementation of PTP will be available in the Alchemy system [25].", "startOffset": 65, "endOffset": 69}], "year": 2011, "abstractText": "Many representation schemes combining firstorder logic and probability have been proposed in recent years. Progress in unifying logical and probabilistic inference has been slower. Existing methods are mainly variants of lifted variable elimination and belief propagation, neither of which take logical structure into account. We propose the first method that has the full power of both graphical model inference and first-order theorem proving (in finite domains with Herbrand interpretations). We first define probabilistic theorem proving, their generalization, as the problem of computing the probability of a logical formula given the probabilities or weights of a set of formulas. We then show how this can be reduced to the problem of lifted weighted model counting, and develop an efficient algorithm for the latter. We prove the correctness of this algorithm, investigate its properties, and show how it generalizes previous approaches. Experiments show that it greatly outperforms lifted variable elimination when logical structure is present. Finally, we propose an algorithm for approximate probabilistic theorem proving, and show that it can greatly outperform lifted belief propagation.", "creator": "TeX"}}}