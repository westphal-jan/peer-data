{"id": "1202.3729", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Suboptimality Bounds for Stochastic Shortest Path Problems", "abstract": "We consider how to use the Bellman residual of the dynamic programming operator to compute suboptimality bounds for solutions to stochastic shortest path problems. Such bounds have been previously established only in the special case that \"all policies are proper,\" in which case the dynamic programming operator is known to be a contraction, and have been shown to be easily computable only in the more limited special case of discounting. Under the condition that transition costs are positive, we show that suboptimality bounds can be easily computed even when not all policies are proper. In the general case when there are no restrictions on transition costs, the analysis is more complex. But we present preliminary results that show such bounds are possible.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (138kb)", "http://arxiv.org/abs/1202.3729v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eric a hansen"], "accepted": false, "id": "1202.3729"}, "pdf": {"name": "1202.3729.pdf", "metadata": {"source": "CRF", "title": "Suboptimality Bounds for Stochastic Shortest Path Problems", "authors": ["Eric A. Hansen"], "emails": ["hansen@cse.msstate.edu"], "sections": [{"heading": null, "text": "We are considering how to use the Bellman residual set of the dynamic programming operator to calculate sub-optimal limits for solving stochastic problems with the shortest paths. Such limits have so far only been set in the specific case that \"all policies are correct.\" In this case, the operator for dynamic programming is known as contraction, and it has been shown that it is easy to calculate only in the more limited special case of discounting. On condition that the transition costs are positive, we show that sub-optimal limits can be easily calculated even if not all policies are correct. In general, if there are no constraints on transition costs, the analysis is more complex. However, we present preliminary results that show that such limits are possible."}, {"heading": "1 Introduction", "text": "A stochastic short-term problem is a Markov Decision Process (MDP) in which the goal is to find a minimum cost policy that achieves a goal or end state with probability. It is an elegant model for many planning problems under uncertainty, especially for targeted planning problems in which the policy reaches a target state. Standard solution methods are based on dynamic programming or linear programming. The model is also used in the development and analysis of amplification problems and heuristic search algorithms."}, {"heading": "2 Background", "text": "We begin with a discussion of the stochastic short-path problem as formulated by Bertsekas and Tsitsiklis [3, 4, 5] and extended to the partially observable case of Patek [11]. We also review previous work on the calculation of sub-optimal limits for solutions found by dynamic programming."}, {"heading": "2.1 Stochastic shortest path problem", "text": "Like any discrete Markov temporal decision-making process (MDP), a stochastic short-term problem involves a number of states, S, and a set of control measures, U, which we assume are both finite; a set of transition probabilities, where the probability that the system will move toward a transition to the state is higher than the probability that action will be taken; the expected cost of action in state i, u) = a set of real costs incurred when action is taken in state i; the likely cost of action in state i, which is in state i, u) = a set of real costs (u, j).S pij (i, j) g (i, j).Furthermore, a stochastic short-term problem is characterized by a set of assumptions from which it derives its specific characteristics."}, {"heading": "2.2 Dynamic programming", "text": "The stochastic short-term problem can be solved by means of dynamic programming, with the dynamic programmer defined as follows: \"...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "2.3 Suboptimality bounds", "text": "Next, we will examine how the Bellman residual product of the dynamic programmer can be bound to the expected J qualities required by J-J. \"J-J\" (\"J-J\") (\"J-J\") (\"J-J\") (\"J-J\") (\"J-J\") (\"J-J\") (\"J-J\") (\"J-J\") (\"J-J\") (\"J-J\") (\"J-J\") (\"J-J\") (\"J-J\") (\"J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J)) (J) (J) (J) (J)) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J)) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) (J) ("}, {"heading": "2.4 Partial observability", "text": "Patek [11] extends the scope of stochastic short path problems to the partially observable case. A partially observable MDP (POMDP) includes the same states, actions, transition probabilities, and costs that were previously defined, plus a finite set of observation symbols, Z, and a set of observation probabilities, where pz (j, u) denotes the probability that the symbol z-Z will be observed after the action. Assumption 4: The set of observation symbols contains a special symbol, zt-Z, which is unique for transitions to the end state. That is, a partially observable stochastic short path problem includes an assumption that ensures that the end of the process is fully recognized. Assumption 4: The set of observation symbols contains a special symbol, zt-Z, which is unique for transitions to the end state."}, {"heading": "3 Uniform improvability and proper policies", "text": "Those of Bertsekas and Tsitsiklis [3, 4] and Patek [11] for the general case where not all measures are appropriate because they do not depend on the contractional property. However, without a contractional property, they do not ensure a geometric rate of convergence or offer a way to use the Bellman residual risk to calculate sub-optimality limits. In the rest of the paper, we show a way under which a policy found through dynamic programming is guaranteed to be correct. Theorem 2: For any stochastic short-term test method, the problem and any value function for which TJ \u2264 J is a greedy policy related to J, defined as\u00b5 (i) = argmin and"}, {"heading": "4 Positive transition costs", "text": "In this section we will consider a specific case of the stochastic shortest path problem, where all transition costs are associated with positive costs, except possibly for transitions to the end state. Assuming 1, a transition from the end state to itself is associated with costs of zero. Since a transition from a non-state to the end state occurs only once, we do not need to impose any constraints on its cost (except, of course, that it is limited). On this condition, we will show how to calculate upper limits known as N (i), whereby the minimum number of steps until the completion of all policy measures, for which J\u00b5 \u2264 J, where J is uniformly improvable, indicates the notation N (i) instead of N, because these limits are applicable to the expected number of steps until the completion of these measures."}, {"heading": "5 General case", "text": "Next, we look at the general case where there are no constraints on transition costs and not all measures are appropriate. In this case, it is no longer possible to use the minimum cost of a transition to bind the average number of steps to the end of a policy \u00b5, for which J\u00b5 \u2264 J, as the minimum cost could be zero or negative. Analysis must be more complex. Although we cannot yet describe a good approach to calculating sub-optimality limits in the general case, we present some preliminary results in this direction. First, we show that the dynamic programmer behaves like an m-step contraction operator for stochastic problems with the shortest paths, even in the general case. We also show that it is possible to calculate sub-optimality limits in the general case, although the limits we describe are far too loose to be workable. Finally, we discuss some implications of these results."}, {"heading": "5.1 Contraction property", "text": "We start by showing that the dynamic programmer behaves as an anm-stage contraction when applied to a value function that cannot be uniformly improved. This result refers to Theorem 2, which shows that a greedy policy with respect to a uniformly improvable value function is an appropriate policy. In the special case of positive transition costs, Theorem 5 shows how to calculate such a m; in this case, m = maxi S N (i) we now show how to calculate such a state according to mostm for the general case where there are no constraints on transition costs.Consider a finite-horizon MDP that has set the finite horizon, transition chances and transition costs than the original, but where the goal is to calculate the minimum costs."}, {"heading": "5.2 Implications", "text": "Regardless of whether Theorem 6 supports a practical approach to calculating sub-optimality boundaries in the general case, it has some important theoretical implications. Among them, it points to a stronger convergence evidence for political iteration than that of Bertsekas and Tsitsiklis [3, 4] in the fully observable case, and Patek [11] in the partially observable case. Since political iteration must begin with an initial appropriate policy, the dynamic programmer employed in the policy improvement stage behaves like an m-stage contraction theory that can be invoked to establish a uniform convergence. The m-stage contraction behavior also notes that political iteration converges at a geometric rate, the value-iteration converges at a geometric rate when an initial value function is given that is uniformly improvable."}, {"heading": "6 Conclusion", "text": "For stochastic problems of the shortest path, we have shown that, on condition that the initial value function can be uniformly improved, a greedy policy regarding any value function found by value titeration is correct. We have also shown how to tie the expected number of steps before reaching the final state, if one follows a proper policy found either by value titeration or by policy iteration, which in turn allows us to use the Bellman residual stock of the dynamic programmer to calculate sub-optimality limits. Bertsekas [2] provided the key formula used to calculate sub-optimality limits is correct, but it was not clear how it could be applied to the case where not all policies are correct. Our contribution is to show that it can be used to calculate sub-optimality limits, even if not all policies are correct as long as the initial value function is uniform."}], "references": [{"title": "Learning to act using real-time dynamic programming", "author": ["A. Barto", "S. Bradtke", "S. Singh"], "venue": "Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "Dynamic Programming and Optimal Control, Volume I", "author": ["D. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Parallel and distributed computation: Numerical methods", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1989}, {"title": "Analysis of stochastic shortest path problems", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1991}, {"title": "Neuro-Dynamic Programming", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Heuristic search for planning under uncertainty", "author": ["B. Bonet", "E. Hansen"], "venue": "College Publications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "On the undecidability of probabilistic planning and related stochastic optimization problems", "author": ["O. Madani", "S. Hanks", "A. Condon"], "venue": "Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Decentralized algorithms for netcentric force protection against antiship missiles", "author": ["M. Maskery", "V. Krishnamurthy", "C. O\u2019Regan"], "venue": "IEEE Transactions on Aerospace and Electronic Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Approximating optimal policies for partially observable stochastic domains", "author": ["R. Parr", "S. Russell"], "venue": "In Proc. of the 14th Int. Joint Conf. on Artificial Intelligence", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "On terminating Markov decision processes with a risk averse objective function", "author": ["S. Patek"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Partially observed stochastic shortest path problems with approximate solution by neurodynamic programming", "author": ["S. Patek"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics \u2013 Part A: Systems and Humans,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Stochastic shortest path games", "author": ["S. Patek", "D. Bertsekas"], "venue": "SIAM Journal of Control and Optimization,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S. Russell", "P. Norvig"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Heuristic search value iteration for POMDPs", "author": ["T. Smith", "R. Simmons"], "venue": "In Proceedings of the 20th Conference in Uncertainty in Artificial Intelligence (UAI-", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Asynchronous stochastic approximation and Q-learning", "author": ["J. Tsitsiklis"], "venue": "Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1994}, {"title": "Speeding up the convergence of value iteration in partially observable Markov decision processes", "author": ["N. Zhang", "W. Zhang"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}], "referenceMentions": [{"referenceID": 3, "context": "A stochastic shortest path problem is a Markov decision process (MDP) where the objective is to find a minimumcost policy that reaches a goal or terminal state with probability 1 [4, 2].", "startOffset": 179, "endOffset": 185}, {"referenceID": 1, "context": "A stochastic shortest path problem is a Markov decision process (MDP) where the objective is to find a minimumcost policy that reaches a goal or terminal state with probability 1 [4, 2].", "startOffset": 179, "endOffset": 185}, {"referenceID": 14, "context": "The model is also used in the development and analysis of reinforcement learning and heuristic search algorithms for MDPs [15, 1, 6].", "startOffset": 122, "endOffset": 132}, {"referenceID": 0, "context": "The model is also used in the development and analysis of reinforcement learning and heuristic search algorithms for MDPs [15, 1, 6].", "startOffset": 122, "endOffset": 132}, {"referenceID": 5, "context": "The model is also used in the development and analysis of reinforcement learning and heuristic search algorithms for MDPs [15, 1, 6].", "startOffset": 122, "endOffset": 132}, {"referenceID": 10, "context": "There are extensions of the stochastic shortest path problem for planning under partial observability [11], multi-agent planning [12, 8], and risksensitive planning [10].", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "There are extensions of the stochastic shortest path problem for planning under partial observability [11], multi-agent planning [12, 8], and risksensitive planning [10].", "startOffset": 129, "endOffset": 136}, {"referenceID": 7, "context": "There are extensions of the stochastic shortest path problem for planning under partial observability [11], multi-agent planning [12, 8], and risksensitive planning [10].", "startOffset": 129, "endOffset": 136}, {"referenceID": 9, "context": "There are extensions of the stochastic shortest path problem for planning under partial observability [11], multi-agent planning [12, 8], and risksensitive planning [10].", "startOffset": 165, "endOffset": 169}, {"referenceID": 2, "context": "We begin with a review of the stochastic shortest path problem as formulated by Bertsekas and Tsitsiklis [3, 4, 5] and extended to the partially observable case by Patek [11].", "startOffset": 105, "endOffset": 114}, {"referenceID": 3, "context": "We begin with a review of the stochastic shortest path problem as formulated by Bertsekas and Tsitsiklis [3, 4, 5] and extended to the partially observable case by Patek [11].", "startOffset": 105, "endOffset": 114}, {"referenceID": 4, "context": "We begin with a review of the stochastic shortest path problem as formulated by Bertsekas and Tsitsiklis [3, 4, 5] and extended to the partially observable case by Patek [11].", "startOffset": 105, "endOffset": 114}, {"referenceID": 10, "context": "We begin with a review of the stochastic shortest path problem as formulated by Bertsekas and Tsitsiklis [3, 4, 5] and extended to the partially observable case by Patek [11].", "startOffset": 170, "endOffset": 174}, {"referenceID": 1, "context": "Following a policy that is proper according to this definition, a process reaches the terminal state with probability 1, regardless of the initial state [2].", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "Two special cases of the stochastic shortest path problem play an important role in the analysis of Bertsekas and Tsitsiklis [3, 4, 5].", "startOffset": 125, "endOffset": 134}, {"referenceID": 3, "context": "Two special cases of the stochastic shortest path problem play an important role in the analysis of Bertsekas and Tsitsiklis [3, 4, 5].", "startOffset": 125, "endOffset": 134}, {"referenceID": 4, "context": "Two special cases of the stochastic shortest path problem play an important role in the analysis of Bertsekas and Tsitsiklis [3, 4, 5].", "startOffset": 125, "endOffset": 134}, {"referenceID": 2, "context": "Bertsekas and Tsitsiklis [3, 4, 5] show that the dynamic programming operator for stochastic shortest path problems is an m-stage contraction operator in the special case that all policies are proper.", "startOffset": 25, "endOffset": 34}, {"referenceID": 3, "context": "Bertsekas and Tsitsiklis [3, 4, 5] show that the dynamic programming operator for stochastic shortest path problems is an m-stage contraction operator in the special case that all policies are proper.", "startOffset": 25, "endOffset": 34}, {"referenceID": 4, "context": "Bertsekas and Tsitsiklis [3, 4, 5] show that the dynamic programming operator for stochastic shortest path problems is an m-stage contraction operator in the special case that all policies are proper.", "startOffset": 25, "endOffset": 34}, {"referenceID": 3, "context": "In the words of Bertsekas and Tsitsiklis [4], \u201cour assumptions do not imply that the corresponding dynamic programming mapping is a contraction (unlike the situation in discounted problems), unless all policies are proper.", "startOffset": 41, "endOffset": 44}, {"referenceID": 10, "context": "Patek [11] extends the framework of stochastic shortest path problems to the partially observable case.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "Given the assumptions of the partially observable stochastic shortest path problem, Patek [11] shows that value iteration and policy iteration have the same convergence properties established by Bertsekas and Tsitsiklis [3, 4] in the completely observable case.", "startOffset": 90, "endOffset": 94}, {"referenceID": 2, "context": "Given the assumptions of the partially observable stochastic shortest path problem, Patek [11] shows that value iteration and policy iteration have the same convergence properties established by Bertsekas and Tsitsiklis [3, 4] in the completely observable case.", "startOffset": 220, "endOffset": 226}, {"referenceID": 3, "context": "Given the assumptions of the partially observable stochastic shortest path problem, Patek [11] shows that value iteration and policy iteration have the same convergence properties established by Bertsekas and Tsitsiklis [3, 4] in the completely observable case.", "startOffset": 220, "endOffset": 226}, {"referenceID": 2, "context": "The convergence proofs for value iteration and policy iteration given by Bertsekas and Tsitsiklis [3, 4] and Patek [11] for the general case when not all policies are proper are significant because they do not depend on the contraction property.", "startOffset": 98, "endOffset": 104}, {"referenceID": 3, "context": "The convergence proofs for value iteration and policy iteration given by Bertsekas and Tsitsiklis [3, 4] and Patek [11] for the general case when not all policies are proper are significant because they do not depend on the contraction property.", "startOffset": 98, "endOffset": 104}, {"referenceID": 10, "context": "The convergence proofs for value iteration and policy iteration given by Bertsekas and Tsitsiklis [3, 4] and Patek [11] for the general case when not all policies are proper are significant because they do not depend on the contraction property.", "startOffset": 115, "endOffset": 119}, {"referenceID": 15, "context": "We call a value function J for which TJ \u2264 J a uniformly improvable value function, a term used by others [16, 14].", "startOffset": 105, "endOffset": 113}, {"referenceID": 13, "context": "We call a value function J for which TJ \u2264 J a uniformly improvable value function, a term used by others [16, 14].", "startOffset": 105, "endOffset": 113}, {"referenceID": 12, "context": "Figure 1: Gridworld navigation example [13, 9].", "startOffset": 39, "endOffset": 46}, {"referenceID": 8, "context": "Figure 1: Gridworld navigation example [13, 9].", "startOffset": 39, "endOffset": 46}, {"referenceID": 12, "context": "Russell and Norvig [13] describe a completely observable version of this gridworld and Parr and Russell [9] describe a partially observable version.", "startOffset": 19, "endOffset": 23}, {"referenceID": 8, "context": "Russell and Norvig [13] describe a completely observable version of this gridworld and Parr and Russell [9] describe a partially observable version.", "startOffset": 104, "endOffset": 107}, {"referenceID": 12, "context": "(For convenience, we keep the reward-maximization framework used by Russell and Norvig [13] and Parr and Russell [9].", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "(For convenience, we keep the reward-maximization framework used by Russell and Norvig [13] and Parr and Russell [9].", "startOffset": 113, "endOffset": 116}, {"referenceID": 12, "context": ") We use the same transition and observation probabilities given by Russell and Norvig [13] and Parr and Russell [9].", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": ") We use the same transition and observation probabilities given by Russell and Norvig [13] and Parr and Russell [9].", "startOffset": 113, "endOffset": 116}, {"referenceID": 12, "context": "One reason for adopting this simple example is that it is the same example used by Russell and Norvig [13] to illustrate how to compute suboptimality bounds for solutions found by dynamic programming for discounted infinite-horizon MDPs.", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "However, this example is most naturally formalized in an undiscounted reward-maximization framework that is equivalent to a stochastic shortest path problem, as noted by both Russell and Norvig [13] and Parr and Russell [9].", "startOffset": 194, "endOffset": 198}, {"referenceID": 8, "context": "However, this example is most naturally formalized in an undiscounted reward-maximization framework that is equivalent to a stochastic shortest path problem, as noted by both Russell and Norvig [13] and Parr and Russell [9].", "startOffset": 220, "endOffset": 223}, {"referenceID": 2, "context": "We say that the dynamic programming operator behaves like an m-stage contraction operator, not that it is one, because Bertsekas and Tsitsiklis [3, 4] give an example that shows that the dynamic programming operator for stochastic shortest path problems is not a contraction with respect to any norm, unless all policies are proper.", "startOffset": 144, "endOffset": 150}, {"referenceID": 3, "context": "We say that the dynamic programming operator behaves like an m-stage contraction operator, not that it is one, because Bertsekas and Tsitsiklis [3, 4] give an example that shows that the dynamic programming operator for stochastic shortest path problems is not a contraction with respect to any norm, unless all policies are proper.", "startOffset": 144, "endOffset": 150}, {"referenceID": 1, "context": "The distinction between being an m-stage contraction and behaving like one is necessary if we adopt the definition that an operator is an m-stage contraction if and only if it satisfies the condition expressed by Equation 7 for all bounded value functions [2].", "startOffset": 256, "endOffset": 259}, {"referenceID": 2, "context": "Among them, it points to a stronger convergence proof for policy iteration than the proofs given by Bertsekas and Tsitsiklis [3, 4] in the completely observable case, and by Patek [11] in the partially observable case.", "startOffset": 125, "endOffset": 131}, {"referenceID": 3, "context": "Among them, it points to a stronger convergence proof for policy iteration than the proofs given by Bertsekas and Tsitsiklis [3, 4] in the completely observable case, and by Patek [11] in the partially observable case.", "startOffset": 125, "endOffset": 131}, {"referenceID": 10, "context": "Among them, it points to a stronger convergence proof for policy iteration than the proofs given by Bertsekas and Tsitsiklis [3, 4] in the completely observable case, and by Patek [11] in the partially observable case.", "startOffset": 180, "endOffset": 184}, {"referenceID": 3, "context": "By contrast, the convergence proofs of Bertakas and Tsitsiklis [4] and Patek [11] do not establish that policy iteration and value iteration converge at a geometric rate, unless all policies are proper.", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "By contrast, the convergence proofs of Bertakas and Tsitsiklis [4] and Patek [11] do not establish that policy iteration and value iteration converge at a geometric rate, unless all policies are proper.", "startOffset": 77, "endOffset": 81}, {"referenceID": 6, "context": "But for undiscounted infinite-horizon POMDPs, the problem of finding an \u03b5-optimal policy has been shown to be undecidable, in general [7].", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": "To help make this result seem more plausible, note that the undecidability of \u03b5-approximation for undiscounted infinite-horizon POMDPs is proved by reduction from the problem of maximizing the probability of reaching a goal state, where there is a reward of 1 for reaching the goal state, a reward of 0 for not reaching the goal state, and the goal state cannot be reached with probability 1 [7].", "startOffset": 392, "endOffset": 395}, {"referenceID": 6, "context": "On the other hand, the optimization problem for discounted infinite-horizon POMDPs, which is also undecidable [7], can be reduced to a partially observable stochastic shortest path problem.", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "The key formula used to compute suboptimality bounds is due to Bertsekas [2].", "startOffset": 73, "endOffset": 76}], "year": 2011, "abstractText": "We consider how to use the Bellman residual of the dynamic programming operator to compute suboptimality bounds for solutions to stochastic shortest path problems. Such bounds have been previously established only in the special case that \u201call policies are proper,\u201d in which case the dynamic programming operator is known to be a contraction, and have been shown to be easily computable only in the more limited special case of discounting. Under the condition that transition costs are positive, we show that suboptimality bounds can be easily computed even when not all policies are proper. In the general case when there are no restrictions on transition costs, the analysis is more complex. But we present preliminary results that show such bounds are possible.", "creator": " TeX output 2011.07.13:2347"}}}