{"id": "1202.3741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Noisy Search with Comparative Feedback", "abstract": "We present theoretical results in terms of lower and upper bounds on the query complexity of noisy search with comparative feedback. In this search model, the noise in the feedback depends on the distance between query points and the search target. Consequently, the error probability in the feedback is not fixed but varies for the queries posed by the search algorithm. Our results show that a target out of n items can be found in O(log n) queries. We also show the surprising result that for k possible answers per query, the speedup is not log k (as for k-ary search) but only log log k in some cases.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (224kb)", "http://arxiv.org/abs/1202.3741v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["shiau hong lim", "peter auer"], "accepted": false, "id": "1202.3741"}, "pdf": {"name": "1202.3741.pdf", "metadata": {"source": "CRF", "title": "Noisy Search with Comparative Feedback", "authors": ["Shiau Hong Lim", "Peter Auer"], "emails": [], "sections": [{"heading": null, "text": "In this search model, the noise in the feedback depends on the distance between the query points and the search target. Consequently, the probability of error in the feedback is not fixed, but varies for the questions raised by the search algorithm. Our results show that a target of n items can be found in O (log n) queries. We also show the surprising result that for k possible answers per query, the acceleration is not log k (as in k-based searches), but only log k in some cases."}, {"heading": "1 Introduction", "text": "We examine a form of noisy search that occurs in information retrieval systems, such as a content-based image query [Cox et al., 2000]. Consider a system in which a user can search for a target (unknown to the system) by answering a sequence of \"queries\" generated by the system. Consider, for example, a content-based retrieval system. The system first presents k images to the user. The user then selects one that is most similar to the target image. Based on the image selected by the user, the system presents a new set of k images and the user responds by selecting one of these k images. This continues until the target is found by the system. We assume that the user's response in each query is likely and depends on the similarities between the images presented and the target image, according to a comparative feedback model proposed in [Cox et al., 2000] and [Auer and Leung, 2009]."}, {"heading": "1.1 The comparative feedback model", "text": "We consider a finite set of n data points X = {x1,.., xn} and a search target T-X. In each query, the user is presented with a set of k (k \u2265 2) different data points Q = {q1,.., qk} - X. The search ends when T-Q is used, otherwise the user responds by selecting one of the query points qj, j-1,.., k).Let R-1,., k) be the random response of the user (note that we use the indexes instead of the query points themselves).The comparative feedback model specifies the probability of selecting a particular response R = r | Q, T) = S (T, qr) - k = 1 S (T, qj), where S (\u00b7 \u00b7 \u00b7) measures the similarity between the data points. \u2212 This means that query points close to the target are more likely to be selected than query points, than deviations from the targets that are closer than the T (j)."}, {"heading": "1.2 Overview of results", "text": "The results of our analysis provide both lower limits and upper limits for the complexity of the query. The lower limits apply to each algorithm, and the upper limits provide performance guarantees for actual algorithms presented in this paper. Many formulations of the loud search have been suggested in the literature, each with different assumptions regarding the type of noise / uncertainty in the user's feedback. In the simplest, noise-free case, the search is reduced to the standard k search, and the query complexity can be as low as logbook k, where n is the total number of item openings and k is the number of possible responses to the queries. In the case of binary search, where the user makes an error with a fixed probability p, a lower limit of logn + o (logn) is 1 \u2212 H (p) [Renyi, 1961] and an upper limit of logn + O (logn) deni."}, {"heading": "2 Results", "text": "We assume that the user model and its parameters are known to the search algorithm. Except for the results in Section 2.2, we also assume that X-R and the distance between data points is measured as d (xi, xj) = | xi \u2212 xj |. Because the search algorithm knows the user model, it can maintain the rear target distribution in terms of queries and user responses so far. We designate the rear probability that the data point xi is the target of ai, and we use a to denote the vector (a1,.., an). After we receive a user response R = r to a Q query, this rear probability is updated as p."}, {"heading": "2.1 Efficient search for the polynomial similarity measure", "text": "In this section, for X-R, we show how the query complexity O (log n) can be measured in the comparative feedback model with polynomial similarity. \"To guarantee that most O (log n) query iterations can be achieved, we must select query points that guarantee a constant gain in information, which can be achieved in the following way:\" The quantity is = I (s / 4) for s = 0,. \"4 (i0 = 1, i4 = n), and consider the resulting 4 intervals (see Fig. 1 for an example in which d1, d4 mark the intervals.\" \u2022 Of these four intervals, we find the one with the smallest length. \"Query the endpoints of this interval adjacent to the small interval, which does not contain.\""}, {"heading": "2.3 For the polynomial similarity measure large k may not help much", "text": "Using the k query points, one might expect that the number of query variations can be reduced by a factor of k, similar to the k-like search. Surprisingly, we can show that for some search problems this is not the case in the comparative feedback model with the polynomial similarity. Theorem 3: For the comparative feedback model with the polynomial similarity, there are data points x1 < xn \u00b2 R such that the expected number of queries for each search algorithm in leastlog n \u2212 log (2k) 2 logbook k + 4 = 1 logbook (log n logbook k) if the target is uniformly randomly selected. We will increasingly select remote data points, xi + 1 \u2212 xi \u2212 xi \u2212 1, so that the similarity from xi \u2212 1 to data points with a smaller index is not much smaller than the similarity to data with a larger index."}, {"heading": "3 Proofs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Preliminaries", "text": "The expected information gain in each query results from: E [H (a) \u2212 H (a)] = k = 1 Pr (R = j) H (b1, j,.., b) = 1 aipi, j (log ai + log pi, j Aj) = H (k) = 1 aiD (pi | A) i = 1 aipi, j Aj = \u2212 k) j = 1 aipi, j (log ai + log pi, j Aj) = H (a) \u2212 n \u00b2 i = 1 aiD (pi | A)."}, {"heading": "3.2 Proof of Theorem 1", "text": "Let us know that these are two different types of indices. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know. (...) Let us know that we. (...) Let us know. (...) Let us know that we. (...) Let us know. (...) Let us know."}, {"heading": "3.3 Proof of Theorem 3", "text": "The following is a suitable choice of data points: for all i > 1, xi + 1 = Q = \u00b7 Q = \u00b7 Q = \u00b7 Q (2 \u2212 1 \u2212 1 / empirically so that (xi + 1 \u2212 xi + 1 \u2212 x1) \u03b8 = 1 2, where any data point x 6 = q = q = q = | 2 \u2212 1 (x, y) = x \u2212 y |. Lemma 6. With the above choice of data points is S (x, xi) \u2264 2S (x, x1) \u2264 2S (x, x1) for any data points x = xi.Proof. By constructing we have S (xi + 1, xi) = 2S (xi + 1, x1). SinceS (x, xi) S (\u2264 x.1) decreasing in x, the statement applies to each x."}, {"heading": "3.4 Proof of Theorem 4", "text": "The selection algorithm's query construction only fails if there are k data points with the total probability that the mass is greater than 1 / 2. We will deal with this case later. At the moment we assume that the construction is successful ist.Lemma 9. For each xi-X-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}, {"heading": "4 Empirical testing of the feedback model", "text": "One question with respect to the comparative feedback model is whether it adequately models the uncertainty of user reaction in real-world applications. We performed statistical tests of the fit accuracy of actual user feedback in image search tasks where the distances measured by the Euclidean distance in the image function space are measured [Auer et al., 2011]. In most of the tasks, the polynomic similarity measurement model matches the data very well, while the exponential similarity measurement model can almost certainly be rejected. We believe that distance measurement plays a crucial role here and more conclusive results are needed. Results in this paper assume that the model parameter \u03b8 is known to the algorithm, which is unlikely in practice. However, empirical evidence indicates that search performance is deteriorating fairly reasonably under the discrepancy of the parameters. To illustrate this, Figure 2 shows the deviating performance in relation to different areas assumed by the algorithm during the application."}, {"heading": "5 Open problems", "text": "There are some open questions in terms of the number of queries we want to address in future work: \u2022 Can the exponential dependence on the dimension of D-dimensional data be avoided? \u2022 Is there an algorithm that guarantees O (loglog k) queries for any data point for one-dimensional data? We can already show that only O (loglog k) queries are actually required for data points selected in Section 2.3. \u2022 Can our analysis - which assumes a uniform (or known) prior distribution of the feedback model's possible targets and known parameters - be extended to an unknown prior distribution and unknown parameters of the feedback model?"}, {"heading": "Acknowledgements", "text": "The research leading to these results was funded by the Seventh Framework Programme of the European Community (FP7 / 2007-2013) under Funding Agreement No 231495 (CompLACS), No 216886 (PASCAL2), No 216529, Personal Information Navigator Adapting Through Viewing, PinView and the Austrian Federal Ministry of Science and Research."}], "references": [{"title": "Exploration-exploitation trade-offs with delayed feedback", "author": ["Auer et al", "P. 2011] Auer", "D. Glowacka", "A. Leung", "S.H. Lim", "A. Medlar", "J. Shawe-Taylor"], "venue": "PinView Deliverable D4.3,", "citeRegEx": "al. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al. et al\\.", "year": 2011}, {"title": "Relevance feedback models for contentbased image retrieval. Multimedia Analysis, Processing and Communications", "author": ["Auer", "Leung", "P. 2009] Auer", "A. Leung"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2009}, {"title": "The bayesian learner is optimal for noisy binary search (and pretty good for quantum as well)", "author": ["Ben-Or", "Hassidim", "M. 2008] Ben-Or", "A. Hassidim"], "venue": "In Proceedings of the 2008 49th Annual IEEE Symposium on Foundations of Computer Sci-", "citeRegEx": "Ben.Or et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ben.Or et al\\.", "year": 2008}, {"title": "The Bayesian image retrieval system, PicHunter: theory, implementation, and psychophysical experiments", "author": ["Cox et al", "I. 2000] Cox", "M. Miller", "T. Minka", "T. Papathomas", "P. Yianilos"], "venue": "IEEE Trans. Image Processing,", "citeRegEx": "al. et al\\.,? \\Q2000\\E", "shortCiteRegEx": "al. et al\\.", "year": 2000}], "referenceMentions": [], "year": 2011, "abstractText": "We present theoretical results in terms of lower and upper bounds on the query complexity of noisy search with comparative feedback. In this search model, the noise in the feedback depends on the distance between query points and the search target. Consequently, the error probability in the feedback is not fixed but varies for the queries posed by the search algorithm. Our results show that a target out of n items can be found in O(log n) queries. We also show the surprising result that for k possible answers per query, the speedup is not log k (as for k-ary search) but only log log k in some cases.", "creator": "TeX"}}}