{"id": "1202.3749", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Compact Mathematical Programs For DEC-MDPs With Structured Agent Interactions", "abstract": "To deal with the prohibitive complexity of calculating policies in Decentralized MDPs, researchers have proposed models that exploit structured agent interactions. Settings where most agent actions are independent except for few actions that affect the transitions and/or rewards of other agents can be modeled using Event-Driven Interactions with Complex Rewards (EDI-CR). Finding the optimal joint policy can be formulated as an optimization problem. However, existing formulations are too verbose and/or lack optimality guarantees. We propose a compact Mixed Integer Linear Program formulation of EDI-CR instances. The key insight is that most action sequences of a group of agents have the same effect on a given agent. This allows us to treat these sequences similarly and use fewer variables. Experiments show that our formulation is more compact and leads to faster solution times and better solutions than existing formulations.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (191kb)", "http://arxiv.org/abs/1202.3749v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hala mostafa", "victor lesser"], "accepted": false, "id": "1202.3749"}, "pdf": {"name": "1202.3749.pdf", "metadata": {"source": "CRF", "title": "Compact Mathematical Programs For DEC-MDPs With Structured Agent Interactions", "authors": ["Hala Mostafa", "Victor Lesser"], "emails": ["hmostafa@bbn.com", "lesser@cs.umass.edu"], "sections": [{"heading": null, "text": "To address the prohibitive complexity of calculating guidelines in decentralized development goals, researchers have proposed models that exploit structured drug interactions. Settings in which most drug actions are independent, with the exception of a few actions that influence the transitions and / or rewards of other agents, can be modeled using event-driven interactions with complex rewards (EDI-CR). Finding the optimal common policy can be formulated as an optimization problem. However, existing formulations are too wordy and / or lack optimum guarantees. We propose a compact Mixed Integrated Linear Program for the formulation of EDI-CR instances. The most important finding is that most of the action sequences of a group of agents have the same effect on a particular compound. This allows us to treat these sequences similarly and use fewer variables. Experiments show that our formulation is more compact and leads to faster response times and better formulations than existing formulations."}, {"heading": "1 Introduction", "text": "Consider a robotic team that sets fire to a building. One agent is responsible for extinguishing the fire, another locates and evacuates survivors, and a third provides first aid to the injured. However, most of an agent's actions affect only himself (e.g. the first agent's exact approach to firefighting and the type of extinguisher he uses mainly affect his own progress). However, the decision-making problems of these agents are not entirely independent; for example, the decision of the firefighter when to secure a particular area influences how easily survivors can be found in the area. This work was done while the first agent was a doctoral student at the University of Massachusetts."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 EDI-CR", "text": "Event-related complex reward interactions (EDI-CR) is a model designed for problems with structured transition and reward interactions. [9] It builds on the intuition that it is more natural for agents to independently describe each agent's decision problem and list the interactions linking these processes. Formally, an EDI-CR instance with n agents is a tuple < A, S, A, P1.. n, R1.. n, \u03c1, T > where: \u2022 A is the set of n agents \u2022 Si is the agent's local action space i \u2022 Ai is the agent's local action space i \u2022 Pi: Si \u00d7 Ai \u00d7 Si \u2192 R is the agent's local transition function \u2022 Ri: Si \u00d7 Ai \u00d7 Si \u2192 R is the agent's local reward function \u2022 \u043c = {< (sk1, ak1, skakk, a problem) > rm."}, {"heading": "2.2 Sequence form policy representation", "text": "In game trees, the idea is that a policy can be characterized by the probability distribution it induces through the leaves of the tree. If two strategies produce the same distribution, then they lead to the same reward. For models with local observation, a sequence (or story) of Agent i, s1.a1.. st.at consists of i's actions and local states. A story that contains T (time horizon) actions is a finite story. For Agent i, the totality of all stories is designated by Agent i, finite stories by Zi, and endless stories by Ni. A common story h'H is a tuple that contains one story per agent. The realization weight of a story s1.a1.. st.at under a policy is the probability that the policy measures a1.. t dictates that states s1.. t will be encountered."}, {"heading": "2.3 Existing Mathematical Formulations", "text": "The formulation of DEC-MDP with local observability as a nonlinear program (NLP) is set forth in Table 1. In the objective function, R (h) = \u03b2 (h) r (h) is the expected reward of the finite common history h, where \u03b2 (h) is the probability of hitting the common states in h, since the actions in h and r (h) are the sum of the rewards of states and actions along history. The restraints in the NLP are called political restraints and guarantee that a solution to the NLP is a legal policy in which the sum of the probabilities of action of an agent in each state is 1. The first group of restraints ensures the sum of the probabilities of action in the initial state 1, while the second group ensures that the realization weights of the expansions of a story add up to the weighting of that story, with the sum of the probabilities of action of an agent in each state 1. The problem with the NLP's formulation not guaranteeing an optimal function is that they do not lead to an objective solution for a non-linear program (NLP)."}, {"heading": "3 Formulation of 2-agent EDI-CR", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Binning histories", "text": "In the 2-agent case, the NLP in Table 1 is only affected by a small number of other actions, whose objective function depends on the form xTQx, in which the individual story lines and story lines depend on each other. (Hi, hj) This approach makes sense for DEC MDPs because the decision-making processes of the agents are closely linked and the transitions of one agent depend heavily on the actions of another. (Hi, hj) This approach can contain many different values depending on the hj, and a given row or column in Q, thus necessitating a variable entry into Q.The situation can be very different in the presence of structured interactions."}, {"heading": "3.2 Enforcing the identity", "text": "This is a challenge for all of us, because we have to get involved in other things that we have experienced in the past. (...) We have to stick to the rules that we have set out to do. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules. (...) We have to stick to the rules."}, {"heading": "4 MILP for 3 or more agents", "text": "The idea of internal history naturally extends beyond 2 storylines. (We) The idea of internal history naturally extends to 2 storylines. (We) The idea of internal history extends to 2 storylines. (We) The idea of internal history naturally extends to 2 storylines. (We) The idea of internal history extends to 2 storylines. (We) The idea of internal history extends to 2 storylines. (We) The greater challenge is to design linear constraints that impose the ceilings that resemble the z variables. (6) With 2 storylines we have simply achieved linear constraints by dropping the leading x in identity. But with 3 or more storylines we would do this in a non-linear constraintion.In the following, we will use characteristics of legal policy and structured interactions to derive 2 linear constraints."}, {"heading": "5 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Results of 2-agent formulations", "text": "In fact, most of them are able to play by the rules that they have adopted in recent years."}, {"heading": "5.2 Results of 3-agent formulations", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "6 Related Work", "text": "TransitionIndependent DEC-MDPs [5] were formulated as MILP [14], but this formulation may not be useful for EDI-CR, as the transition dependency adds significant complexity (TI-DEC-MDP is NP-complete, while EDI-CR is NEXP-complete) and changes the way we address the problem. Aras et. al [4] developed a QP for Network Distributed POMDPs (ND-POMDP) [10], a model in which the actors either do not interact or interact very closely (coarse-grained independence). EDICR specifies interactions at the action level, and we take advantage of these fine-grained specifications."}, {"heading": "7 Conclusion", "text": "This paper presents compact MILP formulations of a class of DEC MDPs where there are structured transition and reward interactions between agents. Previously, this problem was formulated as NLP, which is expensive to solve and can lead to suboptimal solutions, or with a generic MILP formulation for DEC MDPs, which is typically prohibitive in size. Our formulation successfully exploits structured interactions and takes advantage of the insight that most of a group of agents \"actions have the same effect on a particular agent, allowing us to treat these stories in a similar way and use fewer variables in the formulation. Experiments show that our MILP is more compact and leads to faster resolution times and generally better solutions than formulations that ignore the structure of interactions. Our formulation allows us to solve larger problems that would otherwise be impossible to solve."}], "references": [], "referenceMentions": [], "year": 2011, "abstractText": "To deal with the prohibitive complexity of<lb>calculating policies in Decentralized MDPs,<lb>researchers have proposed models that ex-<lb>ploit structured agent interactions. Settings<lb>where most agent actions are independent<lb>except for few actions that affect the tran-<lb>sitions and/or rewards of other agents can<lb>be modeled using Event-Driven Interactions<lb>with Complex Rewards (EDI-CR). Finding<lb>the optimal joint policy can be formulated<lb>as an optimization problem. However, exist-<lb>ing formulations are too verbose and/or lack<lb>optimality guarantees. We propose a com-<lb>pact Mixed Integer Linear Program formula-<lb>tion of EDI-CR instances. The key insight<lb>is that most action sequences of a group of<lb>agents have the same effect on a given agent.<lb>This allows us to treat these sequences sim-<lb>ilarly and use fewer variables. Experiments<lb>show that our formulation is more compact<lb>and leads to faster solution times and better<lb>solutions than existing formulations.", "creator": " TeX output 2011.06.17:1618"}}}