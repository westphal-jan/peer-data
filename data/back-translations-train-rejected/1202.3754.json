{"id": "1202.3754", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "A Geometric Traversal Algorithm for Reward-Uncertain MDPs", "abstract": "Markov decision processes (MDPs) are widely used in modeling decision making problems in stochastic environments. However, precise specification of the reward functions in MDPs is often very difficult. Recent approaches have focused on computing an optimal policy based on the minimax regret criterion for obtaining a robust policy under uncertainty in the reward function. One of the core tasks in computing the minimax regret policy is to obtain the set of all policies that can be optimal for some candidate reward function. In this paper, we propose an efficient algorithm that exploits the geometric properties of the reward function associated with the policies. We also present an approximate version of the method for further speed up. We experimentally demonstrate that our algorithm improves the performance by orders of magnitude.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (142kb)", "http://arxiv.org/abs/1202.3754v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eunsoo oh", "kee-eung kim"], "accepted": false, "id": "1202.3754"}, "pdf": {"name": "1202.3754.pdf", "metadata": {"source": "CRF", "title": "A Geometric Traversal Algorithm for Reward-Uncertain MDPs", "authors": ["Eunsoo Oh"], "emails": ["esoh@ai.kaist.ac.kr", "kekim@cs.kaist.ac.kr"], "sections": [{"heading": null, "text": "Markov decision-making processes (MDPs) are widely used in modeling decision problems in stochastic environments. However, the exact specification of reward functions in MDPs is often very difficult. Recent approaches have focused on calculating an optimal policy based on the minimax regret criterion in order to obtain a robust policy under uncertainty in the reward function. One of the core tasks in calculating the minimax regret policy is to obtain all guidelines that can be optimal for a reward function. In this paper, we propose an efficient algorithm that exploits the geometric properties of the reward function associated with the policy. We also present an approximate version of the method for further acceleration. We experimentally demonstrate that our algorithm improves performance by orders of magnitude."}, {"heading": "1 Introduction", "text": "Markov Decision Processes (MDPs) are a popular model for stochastic sequential decision-making problems (Puterman 1994). Once we get the MDP model of the problem, we can efficiently solve it by using various methods such as value repetition, policy iteration, or linear programming (LP) as a solution. However, specifying model parameters, transition probabilities, and rewards can be a difficult task. However, since the transition predictions from the data are estimated or specified by the domain expert, there is bound to be uncertainty as to the accuracy of the estimate affecting the real performance of the optimal policy derived from the model. However, specifying the rewards represents a greater challenge in the sense that current practice involves manual specification by the domain expert, and estimation from the data still remains an emerging emerging field of research, e.g. Boutius Consolidation of Learning (Ng Russell 2000) has engaged in a significant group of researchers seeking a solution for the last years."}, {"heading": "2 Background", "text": "In this section, we cover the basics of MDPs, unrewarded MDPs, and the definition of the minimax remorse criterion for unrewarded MDPs. We also provide an overview of previous work on calculating the minimax remorse policy for unrewarded MDPs."}, {"heading": "2.1 Markov Decision Processes (MDPs)", "text": "An MDP with an infinite horizon, finite state and action spaces is defined as < S, A, T, R, \u03b3 >, with the set S of n states, the set A of m actions, the state transitional function T, which is defined as T (s, a, s) = Pr (s, a), the reward function r (s, a), the initial state distribution \u03b1, and the discount factor g (0, 1).A (deterministic) politics in MDPs is a mapping solution: S \u2192 A with the associated value function V (s, a), the associated function V (s, s) is defined, the associated function V (s, s), which is not defined when the policy V (s), which is expected to discount the sum of rewards for each state when the execution of the policy is executed, is called a vector function, a nector function, a vector, and a vnr."}, {"heading": "2.2 Reward-Uncertain MDPs and Minimax Regret Policies", "text": "Exact specification of the reward function can be difficult in practice. Regan and Boutilier 2010 \"s Reward Uncertain MDP (RUMDP) extends the standard MDP by allowing a number of workable reward functions instead of requiring a single exact reward function. RUMDP is defined by < S, A, T, R, \u03b1, \u03b3 >, where R is the space of workable reward functions and replaces the reward function r in the standard MDP. Therefore, uncertainty in the reward function is limited to space R. In addition, after the initial work on RUMDPs, we assume that R is a limited and convex polytope defined by the series of linear limitations."}, {"heading": "2.3 Computing Minimax Regret Policies", "text": "Although minimax regret is a natural criterion for the most robust form, the Minimax policy is less relevant to a RUMDP policy than the realizable reward spaces R, and only if the realizable reward function should be optimal; the set-up is useful for calculating the Minimax policy in Equation (6) because it should be optimal for each f & # 252; for each f & # 252; for each f & # 252; for each f & # 252; for each f & # 252; for each F & # 252; for F & # 252; for each f & # 252; for each F & # 252; for each F & # 252; for each F & # 252; for each other reward function. & # 8222; The set-up is useful for calculating the Minimax policy in the equation. & # 8222; In some cases, the reward function can be displayed comproactively."}, {"heading": "3 Geometric Traversal for Nondominated Policies", "text": "In this section we present our algorithm for calculating the non-dominated guidelines in RUMDPs, which uses the optimal state of the reward function in the MDPs."}, {"heading": "3.1 Optimality Condition for Rewards", "text": "Let us consider an MDP M = < S \u2212 \u0432, A, T, r \u0432, \u03b1, \u03b3 > with an optimal sensitivity to politics. If the change with respect to the reward function r \u0445 is very small, the reward function r = r \u043a + \u0445 r as a variable vector can continue to remain as an optimal pole-icy. If we treat the reward function r = r \u043a + \u0445 r as a variable vector, we can obtain a necessary and sufficient condition for r that guarantees the optimality of the results in (Ng and Russell 2000). (7) Let us also note that from equation (2), V\u03c0 = (I \u2212 \u03b3T\u03c0) \u2212 1r\u03c0 = (\u2212 \u03b3Ectuation) \u2212 1r\u03c0.Hence equation (7) becomes the exact equation with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the policy with respect to the MDP M = < S \u2212 \u0432, A, T, r \u0432, \u03b1, \u03b1, \u03b3 > with an optimal sensitivity to the policy with respect to the politics."}, {"heading": "3.2 Geometric Traversal Algorithm", "text": "The Reward-Optimality Condition in Equation (8) is essentially a series of inequalities, each of which describes a hyperplanar boundary. (8) The Reward-Optimality Condition is defined by the Reward-Optimality Condition. (8) The Reward-Optimality Condition is determined by the Reward-Optimality Condition. (8) The Reward-Optimality Condition is determined by the Reward-Optimality Condition. (9) The Reward-Optimality-Condition-Condition-Condition-Condition is determined by the Reward-Condition-Condition-Condition-Condition-Condition-Condition-Condition-Condition-Condition-Condition is determined by the Reward-Optimality-Condition-Condition-Condition-Condition-Condition-Condition. (8) (9) The Reward-Optimality-Condition-Condition-Condition-Condition-Condition-Condition-Condition-Condition-Condition-Condition is determined by the Reward-Optimality-Condition-Condition-Condition-Condition-Condition-Condition-Condition-Condition-Condition"}, {"heading": "3.3 Approximate Method For Computing Nondominated Policies", "text": "Although the geometric traverse algorithm significantly improves runtime, it can still take a lot of time, as the algorithm collects each non-dominated policy, potentially as many as | A | | S |. Regan and Boutilier (2010) propose a method to calculate a subset of non-dominated policies, using the \u03c0Witness algorithm at any time. Using a subset of non-dominated policies, they use ICG-ND to calculate a subset of neighboring reward regions that are encountered as they move along a straight line. As our algorithm also constructs a positive policy incrementally, it can also be used at any time to calculate a subset of non-dominated policies. The idea is to traverse a subset of neighboring reward regions in each iteration that are encountered as we move along a straight line. Specifically, our approximate algorithm begins with an arbitrary reward function running in an R and is represented by a random line of random points on a random line."}, {"heading": "4 Experiments", "text": "We tested the performance of our algorithm on randomly generated instances of RUMDPs with different state sizes and reward function dimensions. For each setting of the state size and reward function dimension, we randomly generated 100 instances of RUMDPs, according to the same experimental evaluation setup in (Regan and Boutilier 2010). We performed the geometric traversal algorithm (GT), \u03c0Witness and ICGND. Note that GT and \u03c0Witness are used to presuppose the number of non-dominated strategies, while ICG-ND calculates the minimax repentance policy using it. Figure 2 compares the runtimes of the algorithms with respect to different sizes of ICGND."}, {"heading": "5 Conclusion", "text": "Specifically, we found that the bottleneck of the state-of-the-art RUMDP algorithm, \u03c0Witness, is in the calculation of non-dominated policies, and proposed an efficient algorithm that exploits the geometric properties of reward functions associated with non-dominated policies. The end result is a linear time algorithm in terms of the number of non-dominated policies in the model that achieves performance improvements on an order of magnitude. Firstly, we have presented an approximate version of the method that does not depend on solving any LP. Experimental results show that the approximate method in terms of execution time always exceeds the version of the EphWitness algorithm. There are a number of future research guidelines that are worth pursuing. Firstly, it would be useful to extend the algorithm to factored areas. Currently, our approach requires the construction of | S | A | hyperlevels that define reward optimization conditions, but we can achieve more significant improvements than we can achieve in the second level today."}, {"heading": "Acknowledgements", "text": "This work was supported by the National Research Foundation of Korea (Grant # 2009-0069702) and the Defense Acquisition Program Administration and the Agency for Defense Development of Korea (Contract # UD080042AD)."}], "references": [{"title": "Introduction to Linear Optimization", "author": ["D. Bertsimas", "J. Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "Bertsimas and Tsitsiklis,? \\Q1997\\E", "shortCiteRegEx": "Bertsimas and Tsitsiklis", "year": 1997}, {"title": "Percentile Optimization for Markov Decision Processes with Parameter Uncertainty", "author": ["E. Delage", "S. Mannor"], "venue": "Operations Research", "citeRegEx": "Delage and Mannor,? \\Q2009\\E", "shortCiteRegEx": "Delage and Mannor", "year": 2009}, {"title": "Bounded Parameter Markov Decision Processes", "author": ["R. Givan", "S. Leach", "T. Dean"], "venue": "In Proceedings of the 4 European Conference on Planning,", "citeRegEx": "Givan et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Givan et al\\.", "year": 1997}, {"title": "Algorithms for Inverse Reinforcement Learning", "author": ["A. Ng", "S. Russell"], "venue": "In Proceedings of the International Conference on Machine Learning", "citeRegEx": "Ng and Russell,? \\Q2000\\E", "shortCiteRegEx": "Ng and Russell", "year": 2000}, {"title": "Robust Control of Markov Decision Processes with Uncertain Transition Matrices. Operations Research 53(5):780-798", "author": ["A. Nilim", "L.E. Ghaoui"], "venue": null, "citeRegEx": "Nilim and Ghaoui,? \\Q2005\\E", "shortCiteRegEx": "Nilim and Ghaoui", "year": 2005}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming, Wiley, Newyork", "author": ["M. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Regret-based Reward Elicitation for Markov Decision Processes", "author": ["K. Regan", "C. Boutilier"], "venue": "In Proceedings of the 25 Conference on Uncertainty in Artificial Intelligence (UAI-09)", "citeRegEx": "Regan and Boutilier,? \\Q2009\\E", "shortCiteRegEx": "Regan and Boutilier", "year": 2009}, {"title": "Robust Policy Computation in Reward-uncertain MDPs using Nondominated Policies", "author": ["K. Regan", "C. Boutilier"], "venue": "In Proceedings of the 25 National Conference on Artificial Intelligence (AAAI-10),", "citeRegEx": "Regan and Boutilier,? \\Q2010\\E", "shortCiteRegEx": "Regan and Boutilier", "year": 2010}, {"title": "Parameter Imprecision in Finite State, Finite Action Dynamic Programs", "author": ["C.C. White", "H.K. Eldeib"], "venue": "Operations Research", "citeRegEx": "White and Eldeib,? \\Q1986\\E", "shortCiteRegEx": "White and Eldeib", "year": 1986}, {"title": "Markov Decision Processes with Imprecise Transition Probabilities", "author": ["H.K.C.C. White"], "venue": "Eldeib", "citeRegEx": "White,? \\Q1994\\E", "shortCiteRegEx": "White", "year": 1994}, {"title": "Parametric Regret in Uncertain Markov Decision Processes", "author": ["H. Xu", "S. Mannor"], "venue": "In Proceedings of the 48 IEEE Conference on Decision and Control", "citeRegEx": "Xu and Mannor,? \\Q2009\\E", "shortCiteRegEx": "Xu and Mannor", "year": 2009}], "referenceMentions": [{"referenceID": 5, "context": "Markov Decision Processes (MDPs) have been a popular model for stochastic sequential decision making problems (Puterman 1994).", "startOffset": 110, "endOffset": 125}, {"referenceID": 3, "context": ", inverse reinforcement learning (Ng and Russell 2000).", "startOffset": 33, "endOffset": 54}, {"referenceID": 4, "context": "While most of the work in the past uses the maximin criterion to address the uncertainty in the transition probabilities only (White and Eldeib 1994; Nilim and Ghaoui 2005) or both (White and Eldeib 1986; Givan et al.", "startOffset": 126, "endOffset": 172}, {"referenceID": 8, "context": "While most of the work in the past uses the maximin criterion to address the uncertainty in the transition probabilities only (White and Eldeib 1994; Nilim and Ghaoui 2005) or both (White and Eldeib 1986; Givan et al. 1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 181, "endOffset": 223}, {"referenceID": 2, "context": "While most of the work in the past uses the maximin criterion to address the uncertainty in the transition probabilities only (White and Eldeib 1994; Nilim and Ghaoui 2005) or both (White and Eldeib 1986; Givan et al. 1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 181, "endOffset": 223}, {"referenceID": 1, "context": "1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 85, "endOffset": 181}, {"referenceID": 10, "context": "1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 85, "endOffset": 181}, {"referenceID": 6, "context": "1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 85, "endOffset": 181}, {"referenceID": 7, "context": "1997), some recent approaches solely focus on the imprecise specification of rewards (Delage and Mannor 2009; Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 85, "endOffset": 181}, {"referenceID": 6, "context": "This is not only because the rewards are more difficult to specify than the transition probabilities, but also the uncertainty in the rewards is an important subject for preference elicitation algorithms (Regan and Boutilier 2009).", "startOffset": 204, "endOffset": 230}, {"referenceID": 10, "context": "The minimax regret criterion was also proposed for computing the robust policy for MDPs with imprecise rewards (Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 111, "endOffset": 183}, {"referenceID": 6, "context": "The minimax regret criterion was also proposed for computing the robust policy for MDPs with imprecise rewards (Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 111, "endOffset": 183}, {"referenceID": 7, "context": "The minimax regret criterion was also proposed for computing the robust policy for MDPs with imprecise rewards (Xu and Mannor 2009; Regan and Boutilier 2009; Regan and Boutilier 2010).", "startOffset": 111, "endOffset": 183}, {"referenceID": 6, "context": "Specifically, we build on the \u03c0Witness algorithm by Regan and Boutilier (2010) for computing the set of nondominated policies that are optimal for some reward function.", "startOffset": 52, "endOffset": 79}, {"referenceID": 7, "context": "The reward-uncertain MDP (RUMDP) (Regan and Boutilier 2010) extends the standard MDP by allowing a set of feasible reward", "startOffset": 33, "endOffset": 59}, {"referenceID": 10, "context": "Although the minimax regret is a natural criterion for robustness, computing the minimax policy for a RUMDP is known to be NP-hard (Xu and Mannor 2009).", "startOffset": 131, "endOffset": 151}, {"referenceID": 6, "context": "Since |\u0393| can be very large, Regan and Boutilier (2010) propose ICG-ND, a technique based on constraint generation for LPs.", "startOffset": 29, "endOffset": 56}, {"referenceID": 6, "context": "Hence, Regan and Boutilier (2010) propose the \u03c0Witness algorithm for the exact computation of \u0393.", "startOffset": 7, "endOffset": 34}, {"referenceID": 3, "context": "Treating the reward function r = r + \u2206r as a variable vector, we can obtain a necessary and sufficient condition for r that guarantees the optimality of \u03c0 using the result in (Ng and Russell 2000): V \u2265 Q\u03c0a \u2200a.", "startOffset": 175, "endOffset": 196}, {"referenceID": 0, "context": "The reward optimality condition is essentially equivalent to performing sensitivity analysis on LP (Bertsimas and Tsitsiklis 1997) for solving the MDP in equation (5).", "startOffset": 99, "endOffset": 130}, {"referenceID": 6, "context": "Regan and Boutilier (2010) propose a method for computing a subset of nondominated policies, using the \u03c0Witness algorithm in an anytime manner.", "startOffset": 0, "endOffset": 27}, {"referenceID": 7, "context": "For each setting of the state size and the reward function dimension, we randomly generated 100 instances of RUMDPs, following the same experimental evaluation setup in (Regan and Boutilier 2010).", "startOffset": 169, "endOffset": 195}, {"referenceID": 6, "context": "It would be interesting to investigate whether we can adapt the idea behind the Regan and Boutilier (2009) heuristic into our algorithm.", "startOffset": 80, "endOffset": 107}], "year": 2011, "abstractText": "Markov decision processes (MDPs) are widely used in modeling decision making problems in stochastic environments. However, precise specification of the reward functions in MDPs is often very difficult. Recent approaches have focused on computing an optimal policy based on the minimax regret criterion for obtaining a robust policy under uncertainty in the reward function. One of the core tasks in computing the minimax regret policy is to obtain the set of all policies that can be optimal for some candidate reward function. In this paper, we propose an efficient algorithm that exploits the geometric properties of the reward function associated with the policies. We also present an approximate version of the method for further speed up. We experimentally demonstrate that our algorithm improves the performance by orders of magnitude.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}