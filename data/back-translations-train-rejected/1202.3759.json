{"id": "1202.3759", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Compressed Inference for Probabilistic Sequential Models", "abstract": "Hidden Markov models (HMMs) and conditional random fields (CRFs) are two popular techniques for modeling sequential data. Inference algorithms designed over CRFs and HMMs allow estimation of the state sequence given the observations. In several applications, estimation of the state sequence is not the end goal; instead the goal is to compute some function of it. In such scenarios, estimating the state sequence by conventional inference techniques, followed by computing the functional mapping from the estimate is not necessarily optimal. A more formal approach is to directly infer the final outcome from the observations. In particular, we consider the specific instantiation of the problem where the goal is to find the state trajectories without exact transition points and derive a novel polynomial time inference algorithm that outperforms vanilla inference techniques. We show that this particular problem arises commonly in many disparate applications and present experiments on three of them: (1) Toy robot tracking; (2) Single stroke character recognition; (3) Handwritten word recognition.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (966kb)", "http://arxiv.org/abs/1202.3759v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gungor polatkan", "oncel tuzel"], "accepted": false, "id": "1202.3759"}, "pdf": {"name": "1202.3759.pdf", "metadata": {"source": "CRF", "title": "Compressed Inference for Probabilistic Sequential Models", "authors": ["Gungor Polatkan", "Oncel Tuzel"], "emails": [], "sections": [{"heading": null, "text": "Hidden Markov Models (HMMs) and Conditional Random Fields (CRFs) are two popular techniques for modelling sequential data. Inference algorithms designed via CRFs and HMMs allow an assessment of the state sequence in light of observations. In several applications, estimating the state sequence is not necessarily the end goal, but the goal is to calculate a function from it. In such scenarios, we look in particular at the specific instantiation of the problem, where the goal is to find the state sequences without exact transition points and derive a novel polynomial time inference algorithm that exceeds vanilla inference techniques. We show that this particular problem often occurs in many different applications and present experiments on three of them: (1) Toy Robot Tracking; (2) Character Detection; (3 Character Detection)."}, {"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they live, in which they live, in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Background: Conditional Random Fields and Inference Techniques", "text": "The sequence marking problem can be formulated to find the best function f that y = f (x) can predict, since N training sequences are {(xi, yi)} Ni = 1, where xi = xi, 1, xi, 2,....., xi, Ti is the observation sequence and yi = yi, 1, yi, 2,..... yi, Ti is the label sequence. Linear chain CRFs and HMMs are two probable models aimed at this problem. Linear chain CRFs can be considered conditional HMMs, or HMMs can be considered a special case of CRFs with a specific set of feature functions. While we present algorithms and results for CRFs, they are equally applicable to HMMs without loss of generality."}, {"heading": "2.1 Conditional Random Fields(CRFs)", "text": "In a linear chain-induced random field by Lafferty et al. (2001), the conditional distribution is modeled as follows: asp (y | x) = 1 Z (x) Tt = 1\u044b (yt, yt \u2212 1, xt), (1) \u044b (yt, yt \u2212 1, xt) = exp j\u03bbjgj (yt \u2212 1, yt, xt) (2) + k\u00b5kuk (yt, xt), whereby \u044b (yt, yt \u2212 1, xt) is designated as a potential function; gj (yt \u2212 1, yt, xt) as a transition function from state yt \u2212 1 to yt; uk (yt, xt) as a state attribute function at state yt; \u03bbj and \u00b5k as parameters estimated during the learning process, and Z (x) as a normalization factor as a function of the observation sequence."}, {"heading": "2.2 Vanilla Inference Techniques", "text": "One way to label a test sequence is the most likely label with the common density y \u043e = arg maxy p (y | x), which propagates the most likely path based on the maximum product rule (Sutton & McCallum (2006). However, in many applications it is very difficult to accurately predict the entire label sequence, so individual predictions are used. This is achieved by predicting yt from the boundary distribution p (yt | x) using a similar dynamic programming method, forward-backward. Prediction is achieved by predicting (j) the entire label sequence (j, xt) consequence \u2212 x (i), with forward and backward recursion being variable by predicting (j)."}, {"heading": "3 Compressed Inference", "text": "In this section, we will introduce the core of our follower algorithm designed to solve the compressed identification problem. We will first define a new sequence s = compress (y), for example, if y = {s, s, j, j, j, w, w, r, r}, then s = compress (y) = {s, j, w, r}. From now on, we will use the C symbol to represent the function compression. The aim of the compressed inference is to predict s in the face of observing x. During this paper, we will use a traditionally trained Markov model (e.g. a linear chain CRF). Next, we will construct the mathematical framework for compressing p (s | x). The total joint density p (s | x) can be compressed from p (y | x) using the basic rules of probability theory. Proposition 3.1: y is a random variable that takes values in E and is an F. Then we are values in F."}, {"heading": "3.1 Compressed Inference Algorithm", "text": "In this section, we first present solutions to three subproblems of the compressed sequence and then perform the compressed labeling using these techniques. Finally, we analyze the complexity of the proposed algorithms to calculate the probability distribution of the compressed sequence; the third subproblem is the calculation of the marginal probabilities of the compressed states; and finally, we perform the labeling in the compressed domain by first finding the length of the compressed sequence from the distribution found in the second subproblem and then finding the marginal probabilities from the third subproblem; Subproblem 1: Calculating the probability of a compressed sequence s0, p (s = s0 | x). Here, we present a dynamic programming technique for compressing these probabilities."}, {"heading": "4 Experiments", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "5 Related Work", "text": "Compressed sequence labeling is used as a video interpretation application in Fern & Givan (2004), where we specified the function name \"Compress.\" Their main focus was to model a sequence problem that had a huge number of states that were not previously known. Due to the unknown number of states, conventional probabilistic models cannot be used, so the approach is applicable to a limited range. In contrast, our inference scheme uses a standard CRF or HMM, and we do not make assumptions other than ordinary sequential modeling problems. Therefore, the presented algorithm is applicable to any problem where different states are important and state transitions are ambiguous. A simple transition cost model is proposed in Fern (2005) for video interpretation, where self-transition is assumed to have no cost, while all other possible transitions are assumed to be exact costs."}, {"heading": "6 Discussion", "text": "The maximum probability or marginal estimation of a full-state sequence is the standard approach for conclusions from Markov models. In this paper, we have shown that if the problem consists only in finding the state sequences without exact transition points, conclusions can be made in a more precise manner. In order to directly infer the unique states, we have proposed marginalization of possible transitions and derived a polynomic time algorithm from it. In three different fields of application, we have shown that the proposed compressed labeling algorithm exceeds vanilla techniques, especially in the case of ambiguity of the state transition. The proposed construction also offers considerable potential for future research. In this context, we have proposed a polynomic time conclusion in which the prediction for the length of the signal uses marginal probabilities."}, {"heading": "A Appendix", "text": "(whose elements are called the events) is a la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la"}], "references": [{"title": "Investigating Loss Functions and Optimization Methods for Discriminative Learning of Label Sequences, in \u2018Proc. EMNLP", "author": ["Y. Altun", "M. Johnson", "T. Hofmann"], "venue": null, "citeRegEx": "Altun et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Altun et al\\.", "year": 2003}, {"title": "Hybrid mobile robot localization using switching state-space models, in \u2018ICRA", "author": ["H. Baltzakis", "P. Trahanias"], "venue": null, "citeRegEx": "Baltzakis and Trahanias,? \\Q2002\\E", "shortCiteRegEx": "Baltzakis and Trahanias", "year": 2002}, {"title": "Shape matching and object recognition using shape contexts", "author": ["S. Belongie", "J. Malik", "J. Puzicha"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "citeRegEx": "Belongie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Belongie et al\\.", "year": 2001}, {"title": "Confidence estimation for information extraction, in \u2018Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLTNAACL)", "author": ["A. Culotta", "A. McCallum"], "venue": null, "citeRegEx": "Culotta and McCallum,? \\Q2004\\E", "shortCiteRegEx": "Culotta and McCallum", "year": 2004}, {"title": "A simple-transition model for relational sequences, in \u2018IJCAI", "author": ["A. Fern"], "venue": null, "citeRegEx": "Fern,? \\Q2005\\E", "shortCiteRegEx": "Fern", "year": 2005}, {"title": "Relational sequential inference with reliable observations, in \u2018Proc. of the International Conference on Machine Learning", "author": ["A. Fern", "R. Givan"], "venue": null, "citeRegEx": "Fern and Givan,? \\Q2004\\E", "shortCiteRegEx": "Fern and Givan", "year": 2004}, {"title": "An hdp-hmm for systems with state persistence, in \u2018Advances in Neural Information Processing Systems", "author": ["E.B. Fox", "E.B. Sudderth", "M.I. Jordan", "A.S. Willsky"], "venue": null, "citeRegEx": "Fox et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fox et al\\.", "year": 2008}, {"title": "Interactive information extraction with constrained conditional random fields, in \u2018AAAI", "author": ["T. Kristjansson", "A. Culotta", "P. Viola"], "venue": null, "citeRegEx": "Kristjansson et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kristjansson et al\\.", "year": 2004}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data, in \u2018International Conference on Machine Learning", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": null, "citeRegEx": "Lafferty et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "HCRF library: Discriminative models for sequence labeling", "author": ["Morency", "L.-P"], "venue": null, "citeRegEx": "Morency and L..P.,? \\Q2007\\E", "shortCiteRegEx": "Morency and L..P.", "year": 2007}, {"title": "Vision based single stroke character recognition for wearable computing", "author": ["O. Ozun", "O.F. Ozer", "C.O. Tuzel", "V. Atalay", "A.E. Cetin"], "venue": "IEEE Intelligent Systems and Applications", "citeRegEx": "Ozun et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Ozun et al\\.", "year": 2001}, {"title": "Conditional random fields for object recognition, in \u2018Advances in Neural Information Processing Systems", "author": ["A. Quattoni", "M. Collins", "T. Darrell"], "venue": null, "citeRegEx": "Quattoni et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quattoni et al\\.", "year": 2004}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition, in \u2018Proceedings of the IEEE", "author": ["L.R. Rabiner"], "venue": null, "citeRegEx": "Rabiner,? \\Q1989\\E", "shortCiteRegEx": "Rabiner", "year": 1989}, {"title": "semi-CRF :a java implementation of conditional random fields for sequential labeling", "author": ["S. Sarawagi"], "venue": null, "citeRegEx": "Sarawagi,? \\Q2009\\E", "shortCiteRegEx": "Sarawagi", "year": 2009}, {"title": "Semi-markov conditional random fields for information extraction, in \u2018Advances in Neural Information", "author": ["S. Sarawagi", "W.W. Cohen"], "venue": "Processing Systems\u2019,", "citeRegEx": "Sarawagi and Cohen,? \\Q2004\\E", "shortCiteRegEx": "Sarawagi and Cohen", "year": 2004}, {"title": "Introduction to Conditional Random Fields for Relational Learning", "author": ["C. Sutton", "A. McCallum"], "venue": null, "citeRegEx": "Sutton and McCallum,? \\Q2006\\E", "shortCiteRegEx": "Sutton and McCallum", "year": 2006}, {"title": "Max-margin markov networks, in \u2018Neural Information Processing Systems Conference", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": null, "citeRegEx": "Taskar et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Taskar et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 9, "context": "Assigning labels to sequential data is a common problem extensively studied in several application domains such as computer vision and computational linguistics (Rabiner (1989), Lafferty et al.", "startOffset": 162, "endOffset": 177}, {"referenceID": 7, "context": "Assigning labels to sequential data is a common problem extensively studied in several application domains such as computer vision and computational linguistics (Rabiner (1989), Lafferty et al. (2001), Quattoni et al.", "startOffset": 178, "endOffset": 201}, {"referenceID": 7, "context": "Assigning labels to sequential data is a common problem extensively studied in several application domains such as computer vision and computational linguistics (Rabiner (1989), Lafferty et al. (2001), Quattoni et al. (2004)).", "startOffset": 178, "endOffset": 225}, {"referenceID": 7, "context": "Assigning labels to sequential data is a common problem extensively studied in several application domains such as computer vision and computational linguistics (Rabiner (1989), Lafferty et al. (2001), Quattoni et al. (2004)). For instance, in part-of-speech tagging, the problem is to tag parts of speech by considering the grammatical structure of the language, e.g., (verb verb noun noun verb adjective) is a very unlikely sequence in English. Likewise, one can assign letters to a sequence of images of hand-written characters by again exploiting the structure enforced by the grammar of that language. In these examples, sequential patterns are important and they can be used to extract information from massive data sets. Two common models for solving such problems are hidden Markov models (HMMs), and conditional random fields (CRFs, often as a linear-chain). These models have been extended in various forms to adapt to different types of problems. For example, semi-Markovian CRFs are introduced as a solution to segmentation problem allowing non-Markovian transitions in segments and assigning direct labels not to individual samples but to overall segments (Sarawagi & Cohen (2004)).", "startOffset": 178, "endOffset": 1194}, {"referenceID": 6, "context": "Fox et al. (2008) proposes a non-parametric prior for systems with state persistence to prevent unrealistically many transitions.", "startOffset": 0, "endOffset": 18}, {"referenceID": 8, "context": "In a linear chain conditional random field of Lafferty et al. (2001), the conditional distribution is modeled as", "startOffset": 46, "endOffset": 69}, {"referenceID": 0, "context": "In this paper, we assume that the trained model is given, and refer readers to Sutton & McCallum (2006) and Altun et al. (2003) for detailed discussions on learning model parameters.", "startOffset": 108, "endOffset": 128}, {"referenceID": 7, "context": "In Culotta & McCallum (2004) and Kristjansson et al. (2004), a constrained forward algorithm is used to compute the confidence of a particular state sequence.", "startOffset": 33, "endOffset": 60}, {"referenceID": 13, "context": "In addition, we present comparison with the semi-Markov CRF model in the first two problems where the training and inference are performed using the semi-Markov CRF package (Sarawagi (2009)).", "startOffset": 174, "endOffset": 190}, {"referenceID": 10, "context": "In this experiment, we apply compressed inference to single stroke character recognition application (Ozun et al. (2001)).", "startOffset": 102, "endOffset": 121}, {"referenceID": 10, "context": "Existing systems use stochastic finite state machines (FSM) or HMMs for this purpose (Ozun et al. (2001)).", "startOffset": 86, "endOffset": 105}, {"referenceID": 16, "context": "In this application, we use the data set in Taskar et al. (2003), which includes 16 \u00d7 8 size characters from the English alphabet.", "startOffset": 44, "endOffset": 65}, {"referenceID": 16, "context": "In this application, we use the data set in Taskar et al. (2003), which includes 16 \u00d7 8 size characters from the English alphabet. In the literature, handwritten word recognition is generally performed by first segmenting the characters and then recognizing them by multi class classification such as SVMs. In many studies, the structure of language is used as well (Taskar et al. (2003)).", "startOffset": 44, "endOffset": 388}, {"referenceID": 2, "context": "states refer to the corresponding character of the time step, and observations correspond to the shape context features of Belongie et al. (2001). For feature extraction, we first take overlapping 16 \u00d7 7 patches from the word image by sliding window technique.", "startOffset": 123, "endOffset": 146}, {"referenceID": 2, "context": "states refer to the corresponding character of the time step, and observations correspond to the shape context features of Belongie et al. (2001). For feature extraction, we first take overlapping 16 \u00d7 7 patches from the word image by sliding window technique. Next, we apply shape context descriptor of Belongie et al. (2001) for each data point on the corresponding patch as shown in Figure 4(d).", "startOffset": 123, "endOffset": 327}, {"referenceID": 4, "context": "Compressed labeling of sequences is mentioned as a videointerpretation application in Fern & Givan (2004), where we get the function name \u201ccompress\u201d from.", "startOffset": 86, "endOffset": 106}, {"referenceID": 4, "context": "Compressed labeling of sequences is mentioned as a videointerpretation application in Fern & Givan (2004), where we get the function name \u201ccompress\u201d from. Their main focus was to model a sequence problem that had an enormous number of states which was not known before hand. Due to the unknown number of states the conventional probabilistic models can not be used, hence the approach is applicable to a limited domain. In contrast our inference scheme uses a standard CRF or HMM and we do not make any assumption other than usual sequential modeling assumptions. Therefore the presented algorithm is applicable to any problem where distinct states are important and state transitions are ambiguous. A simple transition-cost model is proposed in Fern (2005) for video interpretation where self transition is assumed to have no cost whereas all other possible transitions are assumed to have the same cost K.", "startOffset": 86, "endOffset": 758}, {"referenceID": 4, "context": "Compressed labeling of sequences is mentioned as a videointerpretation application in Fern & Givan (2004), where we get the function name \u201ccompress\u201d from. Their main focus was to model a sequence problem that had an enormous number of states which was not known before hand. Due to the unknown number of states the conventional probabilistic models can not be used, hence the approach is applicable to a limited domain. In contrast our inference scheme uses a standard CRF or HMM and we do not make any assumption other than usual sequential modeling assumptions. Therefore the presented algorithm is applicable to any problem where distinct states are important and state transitions are ambiguous. A simple transition-cost model is proposed in Fern (2005) for video interpretation where self transition is assumed to have no cost whereas all other possible transitions are assumed to have the same cost K. This is similar to training a probabilistic sequential model which has zero weights for all self transition parameters and same K as the weight for all other transitions. However, this ad-hoc assumption is unrealistic for many applications. Segmentation is the process of identifying the boundaries between segments (e.g., words in natural language processing, set of pixels in image processing). The output of segmentation is a set of segments with exact boundary locations (e.g. super-pixels with contours in images or state trajectories with exact state transition points in sequential models). The Semi-Markovian approach of Sarawagi & Cohen (2004) proposes a solution to the segmentation problem.", "startOffset": 86, "endOffset": 1561}, {"referenceID": 4, "context": "Compressed labeling of sequences is mentioned as a videointerpretation application in Fern & Givan (2004), where we get the function name \u201ccompress\u201d from. Their main focus was to model a sequence problem that had an enormous number of states which was not known before hand. Due to the unknown number of states the conventional probabilistic models can not be used, hence the approach is applicable to a limited domain. In contrast our inference scheme uses a standard CRF or HMM and we do not make any assumption other than usual sequential modeling assumptions. Therefore the presented algorithm is applicable to any problem where distinct states are important and state transitions are ambiguous. A simple transition-cost model is proposed in Fern (2005) for video interpretation where self transition is assumed to have no cost whereas all other possible transitions are assumed to have the same cost K. This is similar to training a probabilistic sequential model which has zero weights for all self transition parameters and same K as the weight for all other transitions. However, this ad-hoc assumption is unrealistic for many applications. Segmentation is the process of identifying the boundaries between segments (e.g., words in natural language processing, set of pixels in image processing). The output of segmentation is a set of segments with exact boundary locations (e.g. super-pixels with contours in images or state trajectories with exact state transition points in sequential models). The Semi-Markovian approach of Sarawagi & Cohen (2004) proposes a solution to the segmentation problem. Semi-Markov models explicitly model duration in a state with different distributions (which violates the Markovian assumption) and are widely used when one is interested in exact transition points and segmentation boundaries. In contrast to Sarawagi & Cohen (2004), our inference algorithm is designed for Markov models and benefits from ambiguities in segmentation boundaries via marginalizing over all boundary locations.", "startOffset": 86, "endOffset": 1875}, {"referenceID": 13, "context": "The problems where semi-Markov CRF models were shown to be successful, such as name entity recognition (Sarawagi & Cohen (2004)), have relatively short maximum segmentation lengths (around 3-4).", "startOffset": 104, "endOffset": 128}, {"referenceID": 2, "context": "Figure 4: 3 instances of a) jake, b) conor; c) Results; d) Shape context feature extraction of Belongie et al. (2001); e) Results in terms of EDS and Exact scores.", "startOffset": 95, "endOffset": 118}], "year": 2011, "abstractText": "Hidden Markov models (HMMs) and conditional random fields (CRFs) are two popular techniques for modeling sequential data. Inference algorithms designed over CRFs and HMMs allow estimation of the state sequence given the observations. In several applications, estimation of the state sequence is not the end goal; instead the goal is to compute some function of it. In such scenarios, estimating the state sequence by conventional inference techniques, followed by computing the functional mapping from the estimate is not necessarily optimal. A more formal approach is to directly infer the final outcome from the observations. In particular, we consider the specific instantiation of the problem where the goal is to find the state trajectories without exact transition points and derive a novel polynomial time inference algorithm that outperforms vanilla inference techniques. We show that this particular problem arises commonly in many disparate applications and present experiments on three of them: (1) Toy robot tracking; (2) Single stroke character recognition; (3) Handwritten word recognition.", "creator": "TeX"}}}