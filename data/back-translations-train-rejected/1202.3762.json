{"id": "1202.3762", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Symbolic Dynamic Programming for Discrete and Continuous State MDPs", "abstract": "Many real-world decision-theoretic planning problems can be naturally modeled with discrete and continuous state Markov decision processes (DC-MDPs). While previous work has addressed automated decision-theoretic planning for DCMDPs, optimal solutions have only been defined so far for limited settings, e.g., DC-MDPs having hyper-rectangular piecewise linear value functions. In this work, we extend symbolic dynamic programming (SDP) techniques to provide optimal solutions for a vastly expanded class of DCMDPs. To address the inherent combinatorial aspects of SDP, we introduce the XADD - a continuous variable extension of the algebraic decision diagram (ADD) - that maintains compact representations of the exact value function. Empirically, we demonstrate an implementation of SDP with XADDs on various DC-MDPs, showing the first optimal automated solutions to DCMDPs with linear and nonlinear piecewise partitioned value functions and showing the advantages of constraint-based pruning for XADDs.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (837kb)", "http://arxiv.org/abs/1202.3762v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["scott sanner", "karina valdivia delgado", "leliane nunes de barros"], "accepted": false, "id": "1202.3762"}, "pdf": {"name": "1202.3762.pdf", "metadata": {"source": "CRF", "title": "Symbolic Dynamic Programming for Discrete and Continuous State MDPs", "authors": ["Scott Sanner", "Karina Valdivia Delgado", "Leliane Nunes de Barros"], "emails": ["ssanner@nicta.com.au", "kvd@ime.usp.br", "leliane@ime.usp.br"], "sections": [{"heading": null, "text": "Many real-world decision-theory planning problems can, of course, be modelled with discrete and continuous Markov decision-making processes (DC-MDPs). While previous work dealt with automated decision-theory planning for DCMDPs, optimal solutions have so far only been defined for limited settings, such as DC-MDPs with hyper-rectangular linear value functions. In this work, we are expanding symbolic dynamic programming techniques (SDP) to provide optimal solutions for a vastly expanded class of DCMDPs. To address the inherent combinatorial aspects of the SDP, we are introducing the XADD - a continuous variable extension of the algebraic decision diagram (ADD) - that maintains compact representations of the exact value function. Empirically, we are demonstrating the implementation of SDP with XADDs on different DC-MDPs and showing the first optimal value-pieced solutions for DDDDRDs and DDDRPs with shared functionality."}, {"heading": "1 Introduction", "text": "We have two different approaches: We look at stochastic planning problems involving resources, time, or spatial configurations, with, of course, continuous variables in their state representation. For example, in the MARS ROVER issue [6], a rover must manage limited continuous resources of battery performance and daylight time while planning scientific discovery tasks for a number of landmarks on a given day. While problems like the MARS ROVER are, of course, modeled by discrete and continuous state Markov decision-making processes (DC-MDPs), little progress seems to have been made in recent years in developing precise solutions for DC-MDPs with multiple continuous state variables - beyond the subset of DC-MDPs that have an optimal hyperrectangular piecewise linear value function [8, 11]. However, even simple DC-MDPs may require optimal value functions that are piecewise functions with non-rectangular boundaries; we consider NAK 1.1 as an illustration KACK."}, {"heading": "2 Discrete and Continuous State MDPs", "text": "We first introduce discrete and continuous Markov decision-making processes (DC-MDPs) and then verify their final horizontal solution by means of dynamic programming [11]."}, {"heading": "2.1 Factored Representation", "text": "In a DC-MDP, states are represented by vectors of variables (~ b, ~ x) = (b1,.., bn, x1,.., xm). We assume that each state variable is bi (1 \u2264 i \u2264 n) Boolean s.t. bi = {0, 1} and each xj (1 \u2264 j \u2264 m) continuous s.t. xj [Lj, Uj] for Lj, Uj \u00b2 R; Lj \u2264 Uj. We also assume a finite series of actions A = {a1,.,., ap}.A DC-MDP is defined by the following: (1) a transitional state model P (~ b \u2032, ~ x \u2032 R; Lj \u2264 Uj. We also assume a finite series of actions from A = {a1,., ap}."}, {"heading": "2.2 Solution Methods", "text": "If we initialize V 0 (~ b, ~ x) (e.g. on V 0 (~ b, ~ x) (e.g. on V 0 (~ b, ~ x) = 0), we define the quality of action in a state (~ b, ~ x) and act to obtain V h (~ b, ~ x) after that as follows: Qh + 1a (~ b, ~ x) = Ra (~ b, ~ x) + \u03b3 \u00b7 (7) X ~ b \u2032 Z ~ x nY i = 1 P (b \u2032 b, ~ x | ~ b, ~ x) then as follows: Qh + 1a (x \u2032 h | h h h) = 1 (h) = 1 h = h = 1 h = h = h = h = h h = h h = h h = h h h = h h h h = h h h h = h h h h h = h h h h h = h h h h h = h h h h h = h h h h = h h h h = h h h = h h h = h h h = h h h = h h = h h h = h h h = h h = h h h = h h = h h = h h h = h h = h h h = h h = h h h h = h h = h h h h = h h = h h h h = h h h = h h h = h h h = h h h h = h h h h h = h h h = h h h h = h h h h h = h h h = h h h = h h h h h = h h h = h h h h h = h h h h = h h h = h h h h h = h h = h = h h h h = h h h h h = h h h = h h h = h h h = h h = h h h h h = h h h = h h h h h (h h h) = h = h = h = h h = h h h = h h h = h h = h = h h h = h h h h = h h = h h h h = h (h = h) = h = h h h = h h = h = h h = h = h h = h h = h = h = h h = h = h"}, {"heading": "3 Symbolic Dynamic Programming", "text": "As the name suggests, symbolic dynamic programming (SDP) [4] is merely the process of performing dynamic programming (in this case value titeration) by means of symbolic manipulation. Whereas SDP in the sense of [4] was previously used only with incrementally constant functions, we are now generalizing the representation to work with general level functions required for DC MDPs in this paper. However, before defining our solution, we must formally define our case representation and symbolic case perators."}, {"heading": "3.1 Case Representation and Operators", "text": "In the course of this work, we assume that all symbolic functions may be presented in case form as follows: f = > 1 > 1 > 1 > 1 > 1 > 1 > 1 > 1 > 1 > 2 > 1 > 1 > 2 > 1 > 1 > 1 > 1 > 1 > 1 > 1 > 2 > 1 > 2. \"[1] [2]\" [1] \"[2]\" [2] \"[3]\" [4] \"[4]\" [5] \"[5]\" [6] \"[7]\" [7] \"[7]\" [7] \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "3.2 Symbolic Dynamic Programming (SDP)", "text": "In the SDP solution for DC-MDPs, our goal will be to take a DC-MDP statement as defined in Section 2, apply the value iteration as defined in Section 2.2, and replace the final value of the optimal function V h in the form of a case case.For the base case of h = 0, we point out that the setting V 0 (~ b, ~ x) = 0 (or the reward case statement if not action dependency) is trivial in the form of a case case.Furthermore, h > 0 requires the application of the SDP. Fortunately, specifying our previously defined operations is simple and can be divided into four steps: 1. Prime the Value Function: Since V h can become the \"next state\" in value iteration, we insert a substitution solution = b1 = b \u2032 1,."}, {"heading": "4 Extended ADDs (XADDs)", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "5 Empirical Results", "text": "We implemented two versions of our proposed SDP algorithm using XADDs - one that does not truncate XADD nodes, and another that uses a linear programmer to truncate unreachable nodes (for problems with linear XADDs) - and tested these algorithms for KNAPSACK and two versions of the Mars Rover domain (adapted from [6]) that we call MARS ROVER LINEAR and MARS ROVER NONLINEAR.4"}, {"heading": "5.1 Domains", "text": "In a general MARS ROVER domain, a rover should approach one or more target points and take pictures of these points. < hi > Reward can consume time and energy. < p / xadd-inference.MARS ROVER LINEAR This version has two continuous variables, time t and energy e. For each target point i = 1. k), there is a Boolean variable pi, which indicates whether the rover is at a point. < p / xadd-inference.MARS ROVER LINEAR This version has two continuous variables, time t and energy e. For each target point i = 1. k), there is a Boolean variable pi, which indicates whether the rover is at a point."}, {"heading": "5.2 Results", "text": "In fact, it is that we are able to assert ourselves, that we are able to assert ourselves in the region, and that we are able to assert ourselves in the region, that we are able to stay in the region, \"he said."}, {"heading": "6 Related Work", "text": "The most relevant vein of related work is that of [8] and [11], which can perform exact dynamic programming of DC-MDPs with rectangular linear reward and transition functions that are delta functions. While the SDP can solve these same problems, it removes both rectangular and piecemeal limitations to reward and value functions while maintaining accuracy. Heuristic search approaches with formal guarantees such as HAO * [13] are an attractive future extension of the SDP; in fact, HAO * currently uses the method of [8], which could be directly replaced by the SDP. While [14] general piecemeal functions with linear boundaries are considered (and indeed we borrow our linear pruning approach from this paper), this work is applied only to fully deterministic settings, not to DC-MDPSs. Other work analyzes limited DC-DPS with only continuous variability."}, {"heading": "7 Conclusions", "text": "This representation facilitated the use of symbolic dynamic programming techniques to generate exact solutions for DC-MDP with arbitrary reward functions and expressive nonlinear transition functions that far exceed the exact solutions that are possible with existing DC-MDP solvers. In an effort to make SDP practicable, we also introduced the new XADD data structure to represent arbitrary, piecemeal symbolic value functions, and we addressed the complications that SDP causes for XADDs, such as the need to rearrange the decision nodes after some operations. All of these are significant contributions that have contributed to a new level of expressiveness for DC-MDPS that would allow precisely piepiepiepieced functions.There are a number of possibilities for future research. First, it is important to investigate which generalizations of the transition function used in this work would allow ADD to focus even more closely on exact functionality that would allow for scalability to use DP."}, {"heading": "Acknowledgements", "text": "The first author is supported by NICTA; NICTA is funded by the Australian government, represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program. This work was also supported by the Brazilian agencies FAPESP (grant 2008 / 03995-5) and CAPES."}], "references": [{"title": "Algebraic Decision Diagrams and their applications", "author": ["R. Iris Bahar", "Erica Frohm", "Charles Gaona", "Gary Hachtel", "Enrico Macii", "Abelardo Pardo", "Fabio Somenzi"], "venue": "In IEEE /ACM ICCAD,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "Dynamic Programming", "author": ["Richard E. Bellman"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1957}, {"title": "Decision-theoretic planning: Structural assumptions and computational leverage", "author": ["Craig Boutilier", "Thomas Dean", "Steve Hanks"], "venue": "JAIR, 11:1\u201394,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Symbolic dynamic programming for first-order MDPs", "author": ["Craig Boutilier", "Ray Reiter", "Bob Price"], "venue": "In IJCAI-01,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Exact solutions to time-dependent MDPs", "author": ["Justin Boyan", "Michael Littman"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Planning under continuous time and resource uncertainty: A challenge for ai", "author": ["John L. Bresina", "Richard Dearden", "Nicolas Meuleau", "Sailesh Ramkrishnan", "David E. Smith", "Richard Washington"], "venue": "InUncertainty in Artificial Intelligence", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "A model for reasoning about persistence and causation", "author": ["Thomas Dean", "Keiji Kanazawa"], "venue": "Computational Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Dynamic programming for structured continuous markov decision problems", "author": ["Zhengzhu Feng", "Richard Dearden", "Nicolas Meuleau", "Richard Washington"], "venue": "In Uncertainty in Artificial Intelligence", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "SPUDD: Stochastic planning using decision diagrams", "author": ["Jesse Hoey", "Robert St-Aubin", "Alan Hu", "Craig Boutilier"], "venue": "In UAI-99,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Solving factored mdps with hybrid state and action variables", "author": ["Branislav Kveton", "Milos Hauskrecht", "Carlos Guestrin"], "venue": "Journal Artificial Intelligence Research (JAIR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Lazy approximation for solving continuous finite-horizon mdps", "author": ["Lihong Li", "Michael L. Littman"], "venue": "In National Conference on Artificial Intelligence AAAI-", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "A fast analytical algorithm for solving markov decision processes with real-valued resources", "author": ["Janusz Marecki", "Sven Koenig", "Milind Tambe"], "venue": "In International Conference on Uncertainty in Artificial Intelligence IJCAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "A heuristic search approach to planning with continuous resources in stochastic domains", "author": ["Nicolas Meuleau", "Emmanuel Benazera", "Ronen I. Brafman", "Eric A. Hansen", "Mausam"], "venue": "Journal Artificial Intelligence Research (JAIR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Temporal planning with continuous change", "author": ["J. Scott Penberthy", "Daniel S. Weld"], "venue": "In National Conference on Artificial Intelligence AAAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Variable resolution discretization in optimal control", "author": ["Andrew Moore Remi Munos"], "venue": "Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}, {"title": "APRICODD: Approximate policy construction using decision diagrams", "author": ["Robert St-Aubin", "Jesse Hoey", "Craig Boutilier"], "venue": "In NIPS-2000,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Exploiting causal independence in bayesian network inference", "author": ["Nevin Lianwen Zhang", "David Poole"], "venue": "J. Artif. Intell. Res. (JAIR),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}], "referenceMentions": [{"referenceID": 5, "context": "For example, in the MARS ROVER problem [6], a rover must manage bounded continuous resources of battery power and daylight time as it plans scientific discovery tasks for a set of landmarks on a given day.", "startOffset": 39, "endOffset": 42}, {"referenceID": 7, "context": "While problems such as the MARS ROVER are naturally modeled by discrete and continuous state Markov decision processes (DC-MDPs), little progress seems to have been made in recent years in developing exact solutions for DC-MDPs with multiple continuous state variables beyond the subset of DC-MDPs which have an optimal hyperrectangular piecewise linear value function [8, 11].", "startOffset": 369, "endOffset": 376}, {"referenceID": 10, "context": "While problems such as the MARS ROVER are naturally modeled by discrete and continuous state Markov decision processes (DC-MDPs), little progress seems to have been made in recent years in developing exact solutions for DC-MDPs with multiple continuous state variables beyond the subset of DC-MDPs which have an optimal hyperrectangular piecewise linear value function [8, 11].", "startOffset": 369, "endOffset": 376}, {"referenceID": 7, "context": "These questions have been affirmatively addressed for the subset of DC-MDPs with transition functions that are mixtures of delta functions and reward functions that are hyper-rectangular piecewise linear, which provably lead to value functions of the same structure [8, 11].", "startOffset": 266, "endOffset": 273}, {"referenceID": 10, "context": "These questions have been affirmatively addressed for the subset of DC-MDPs with transition functions that are mixtures of delta functions and reward functions that are hyper-rectangular piecewise linear, which provably lead to value functions of the same structure [8, 11].", "startOffset": 266, "endOffset": 273}, {"referenceID": 3, "context": "This is precisely the motivation behind symbolic dynamic programming (SDP) [4] used to solve MDPs with transitions and reward functions defined in first-order logic, except that in prior SDP work, only piecewise constant functions have been used; in this work we introduce techniques for working with arbitrary piecewise symbolic functions.", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": "For discrete factored MDPs, algebraic decision diagrams (ADDs) [1] have been successfully used in exact algorithms like SPUDD [9] to maintain compact value representations.", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "For discrete factored MDPs, algebraic decision diagrams (ADDs) [1] have been successfully used in exact algorithms like SPUDD [9] to maintain compact value representations.", "startOffset": 126, "endOffset": 129}, {"referenceID": 13, "context": "We also borrow techniques from [14] for constraint-based pruning of XADDs that can be applied when XADDs meet certain expressiveness restrictions.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "We first introduce discrete and continuous state Markov decision processes (DC-MDPs) and then review their finitehorizon solution via dynamic programming following [11].", "startOffset": 164, "endOffset": 168}, {"referenceID": 2, "context": "DC-MDPs as defined above are naturally factored [3] in terms of state variables (~b, ~x); as such transition structure can be exploited in the form of a dynamic Bayes net (DBN) [7] where the individual conditional probabilities", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "DC-MDPs as defined above are naturally factored [3] in terms of state variables (~b, ~x); as such transition structure can be exploited in the form of a dynamic Bayes net (DBN) [7] where the individual conditional probabilities", "startOffset": 177, "endOffset": 180}, {"referenceID": 7, "context": ", Gaussian noise), but we note that this representation effectively allows modeling of continuous variable transitions as a mixture of \u03b4 functions, which has been used heavily in previous exact DC-MDP solutions [8, 11, 13].", "startOffset": 211, "endOffset": 222}, {"referenceID": 10, "context": ", Gaussian noise), but we note that this representation effectively allows modeling of continuous variable transitions as a mixture of \u03b4 functions, which has been used heavily in previous exact DC-MDP solutions [8, 11, 13].", "startOffset": 211, "endOffset": 222}, {"referenceID": 12, "context": ", Gaussian noise), but we note that this representation effectively allows modeling of continuous variable transitions as a mixture of \u03b4 functions, which has been used heavily in previous exact DC-MDP solutions [8, 11, 13].", "startOffset": 211, "endOffset": 222}, {"referenceID": 7, "context": "Furthermore, we note that our representation is more general than [8, 11, 13] in that we do not restrict the equation to be linear, but rather allow it to specify arbitrary functions (e.", "startOffset": 66, "endOffset": 77}, {"referenceID": 10, "context": "Furthermore, we note that our representation is more general than [8, 11, 13] in that we do not restrict the equation to be linear, but rather allow it to specify arbitrary functions (e.", "startOffset": 66, "endOffset": 77}, {"referenceID": 12, "context": "Furthermore, we note that our representation is more general than [8, 11, 13] in that we do not restrict the equation to be linear, but rather allow it to specify arbitrary functions (e.", "startOffset": 66, "endOffset": 77}, {"referenceID": 1, "context": "Now we provide a continuous state generalization of value iteration [2], which is a dynamic programming algorithm for constructing optimal policies.", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "As it\u2019s name suggests, symbolic dynamic programming (SDP) [4] is simply the process of performing dynamic programming (in this case value iteration) via symbolic manipulation.", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "While SDP as defined in [4] was previously only used with piecewise constant functions, we now generalize the representation to work with general piecewise functions needed for DC-MDPs in this paper.", "startOffset": 24, "endOffset": 27}, {"referenceID": 16, "context": "Using variable elimination [17], when marginalizing over xj we can factor out any functions independent of xj \u2014 that is, for \u222b xj in (7), one can see that initially, the only functions that can include xj are V \u2032h and P (xj |~b,~b\u2032, ~x, a) = \u03b4[xj \u2212 g(~x)]; hence, the first marginal over xj need only be computed over \u03b4[xj \u2212 g(~x)]V \u2032h.", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "Motivated by the SPUDD [9] algorithm which maintains compact value function representations for finite discrete factored MDPs using algebraic decision diagrams (ADDs) [1], we extend this formalism to handle continuous variables in a data structure we refer to as the XADD.", "startOffset": 23, "endOffset": 26}, {"referenceID": 0, "context": "Motivated by the SPUDD [9] algorithm which maintains compact value function representations for finite discrete factored MDPs using algebraic decision diagrams (ADDs) [1], we extend this formalism to handle continuous variables in a data structure we refer to as the XADD.", "startOffset": 167, "endOffset": 170}, {"referenceID": 13, "context": ", as also done in [14]) to prune unreachable nodes in the XADD; later we show results demonstrating impressive reductions in XADD size using this style of pruning.", "startOffset": 18, "endOffset": 22}, {"referenceID": 5, "context": "We implemented two versions of our proposed SDP algorithm using XADDs \u2014 one that does not prune nodes of the XADD and another that uses a linear programming solver to prune unreachable nodes (for problems with linear XADDs) \u2014 and we tested these algorithms on KNAPSACK and two versions of the Mars Rover domain (adapted from [6]) that we call MARS ROVER LINEAR and MARS ROVER NONLINEAR.", "startOffset": 325, "endOffset": 328}, {"referenceID": 7, "context": "The most relevant vein of Related work is that of [8] and [11] which can perform exact dynamic programming on DC-MDPs with rectangular piecewise linear reward and transition functions that are delta functions.", "startOffset": 50, "endOffset": 53}, {"referenceID": 10, "context": "The most relevant vein of Related work is that of [8] and [11] which can perform exact dynamic programming on DC-MDPs with rectangular piecewise linear reward and transition functions that are delta functions.", "startOffset": 58, "endOffset": 62}, {"referenceID": 12, "context": "Heuristic search approaches with formal guarantees like HAO* [13] are an attractive future extension of SDP; in fact HAO* currently uses the method of [8], which could be directly replaced with SDP.", "startOffset": 61, "endOffset": 65}, {"referenceID": 7, "context": "Heuristic search approaches with formal guarantees like HAO* [13] are an attractive future extension of SDP; in fact HAO* currently uses the method of [8], which could be directly replaced with SDP.", "startOffset": 151, "endOffset": 154}, {"referenceID": 13, "context": "While [14] has considered general piecewise functions with linear boundaries (and in fact, we borrow our linear pruning approach from this paper), this work only applied to fully deterministic settings, not DC-MDPs.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "One continuous variable can be useful for optimal solutions to time-dependent MDPs (TMDPs) [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 11, "context": "Or phase transitions can be used to arbitrarily approximate one-dimensional continuous distributions leading to a bounded approximation approach for arbitrary single continuous variable DC-MDPs [12].", "startOffset": 194, "endOffset": 198}, {"referenceID": 9, "context": "Finally, there are a number of general DC-MDP approximation approaches that use approximate linear programming [10] or sampling in a reinforcement learning style approach [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 14, "context": "Finally, there are a number of general DC-MDP approximation approaches that use approximate linear programming [10] or sampling in a reinforcement learning style approach [15].", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": "In terms of better scalability, one avenue would explore the use of initial state focused heuristic search-based value iteration like HAO* [13] that can be readily adapted to use SDP.", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "Another avenue of research would be to adapt the lazy approximation approach of [11] to approximate DC-MDP value functions as piecewise linear XADDs with linear boundaries that may allow for better approximations than current representations that rely on rectangular piecewise functions.", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "Along the same lines, ideas from APRICODD [16] for bounded approximation of discrete ADD value functions by merging leaves could be generalized to XADDs.", "startOffset": 42, "endOffset": 46}], "year": 2011, "abstractText": "Many real-world decision-theoretic planning problems can be naturally modeled with discrete and continuous state Markov decision processes (DC-MDPs). While previous work has addressed automated decision-theoretic planning for DCMDPs, optimal solutions have only been defined so far for limited settings, e.g., DC-MDPs having hyper-rectangular piecewise linear value functions. In this work, we extend symbolic dynamic programming (SDP) techniques to provide optimal solutions for a vastly expanded class of DCMDPs. To address the inherent combinatorial aspects of SDP, we introduce the XADD \u2014 a continuous variable extension of the algebraic decision diagram (ADD) \u2014 that maintains compact representations of the exact value function. Empirically, we demonstrate an implementation of SDP with XADDs on various DC-MDPs, showing the first optimal automated solutions to DCMDPs with linear and nonlinear piecewise partitioned value functions and showing the advantages of constraint-based pruning for XADDs.", "creator": "TeX"}}}