{"id": "1202.3767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Distributed Anytime MAP Inference", "abstract": "We present a distributed anytime algorithm for performing MAP inference in graphical models. The problem is formulated as a linear programming relaxation over the edges of a graph. The resulting program has a constraint structure that allows application of the Dantzig-Wolfe decomposition principle. Subprograms are defined over individual edges and can be computed in a distributed manner. This accommodates solutions to graphs whose state space does not fit in memory. The decomposition master program is guaranteed to compute the optimal solution in a finite number of iterations, while the solution converges monotonically with each iteration. Formulating the MAP inference problem as a linear program allows additional (global) constraints to be defined; something not possible with message passing algorithms. Experimental results show that our algorithm's solution quality outperforms most current algorithms and it scales well to large problems.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (418kb)", "http://arxiv.org/abs/1202.3767v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["joop van de ven", "fabio ramos"], "accepted": false, "id": "1202.3767"}, "pdf": {"name": "1202.3767.pdf", "metadata": {"source": "CRF", "title": "Distributed Anytime MAP Inference", "authors": ["Joop van de Ven", "Fabio Ramos"], "emails": ["j.vandeven@acfr.usyd.edu.au", "f.ramos@acfr.usyd.edu.au"], "sections": [{"heading": null, "text": "The resulting program has a constraint structure that enables the application of the Dantzig-Wolfe decomposition principle. Sub-programs are defined by individual edges and can be calculated in a distributed way, taking into account solutions for graphs whose state space does not fit into memory. The decomposition master program guarantees that the optimal solution is computed in a limited number of iterations, while the solution monotonously converges with each iteration. Formulating the MAP inference problem as a linear program allows for the definition of additional (global) constraints; something that is not possible with message forwarding algorithms. Experimental results show that the solution quality of our algorithm outperforms most current algorithms and it can be easily scaled to major problems."}, {"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Related Work", "text": "This year it is more than ever before."}, {"heading": "3 LP Formulation", "text": "An undirected model G = (V, E) represents a probability distribution pG = (x1,.., xN) overN = (V,.) overN = (V,.) overN = (V,.) overN = (1,.) overN = (1,.) overN = (1,.) overN = (1,.) overN = (1,.) overN = (1,.) overN = (1,.) overN = (1,.) overN = (1,.) overN = (1,.) overN = (1,.) overN = (1,.) overN = (1,.) overN = (1,.) overN = (1,.) overN (1,.) overN = (1,.)."}, {"heading": "3.1 Solution Consistency", "text": "Equation 5 is defined by the edges, since such constraints are necessary to ensure that the solution in the node variables remains consistent. Definition Let ms | t = Astyst be a marginal variable of the edge variable for the node variable xs. Suggestion 3.1 The solution for the edge variables is consistent in the node variable xs, if the marginal variables {ms | t | 0, N (s)} are all the same. The proof can be provided by simply replacing ykst = x i sx j t and i x i s = 1. Consistency constraints are specified by edge pairs, i.e. as the difference between the interface pairs ms | t and ms | u. For a given node s, an edge is used as the reference edge; edge (s, t) is specified in the equation xs. All consistency constraints are specified relative to the reference edge resulting from a minimum edge resulting from an equation."}, {"heading": "3.2 Additional Requirements", "text": "Certain optimization problems have additional requirements (or constraints) that are imposed on them. To ensure that adjacent nodes xs and xt have different (or equal) solutions for the states i or j, add one of the following constraints to Equation 5 for each such state: Dst, ijyst \u2264 1, ijyst \u2264 1, i, i, i, i, j, j: xis = x, j, ijyst = 0, x (s, t, i, j): xis = x j t, (6), where Dst, ij = A i, \u2022 st + A j, \u2022 ensures a unique solution, while Est, ij = A i, \u2022 st \u2212 A j, \u2022 ts ensures an equal solution between the states i and j of the nodes xs and xt. Note that the constraints of Equation 6 are locally defined. This approach can easily be extended to global constraints."}, {"heading": "4 Decomposition", "text": "The Dantzig-Wolfe decomposition principle [10] allows an LP with a special block-matrix structure that can be divided into a number of independent subroutines, which are iteratively adjusted to take into account the global state (simplex multipliers) due to a master program. Readers are referred to [10, Chapter 10] for a detailed discussion and evidence. In this constellation, we offer an interpretation of the principle in the context of MAP multipliers for graphical models.The block-angular system provides detailed requirements cT1 y1 +. + c T KyK Subject to B1y1 +. + BKyK = b F1y1 = f1..... FKyK = yi 0 = 1,. K, (7) allows decomposition to be applied to the MAP problem. Matrices {Bi | i = 1,."}, {"heading": "4.1 Initialisation", "text": "The goal of initialization is to find an initial basic practicable solution. A common approach to initializing the Dantzig-Wolfe decomposition is to use a simplex phase 1 approach [10, Section 10.2.4]. This involves determining the maxima of each subprogram using the actual cost. The resulting solutions are used to start the master program. In our case, the subprograms are trivial. This generally means that the consistency constraints are violated, preventing the decomposition from even starting. Instead, if no additional requirements are specified, the algorithm 1 procedure can be used to find a first basic practicable solution in one step.Algorithm 1 Pseudo-code of the algorithm initialization.1: Input: Graph G = (V, E), potentials that set initial requirements can be used to find an initial basic practicable solution in one step."}, {"heading": "4.2 Subprogram", "text": "For the conclusion in a graph, the subroutines maximize a linear program over the edges as follows: Maximize cTstyst \u2212 BTst\u03c0 Subject to k y k st = 10 \u2264 ykst \u2264 1 k = 1,..., | Xs | | Xt |. (8) The objective function of Equation 8 is the actual cost of the edge, cst, adjusted to the current state of the interactions, BTst\u03c0. Here are the concatenated consistency constraints (and optional additional requirements), while \u03c0 are the corresponding simple multipliers (see Section 4.3). These adjusted costs find the maximum of the edge based on the current global state of the algorithm. However, there is no need to call an LP solver for each subroutron. There are two limitations for each edge, which express the uniqueness of the solution (ykst, 0, 1} and k k st = 1)."}, {"heading": "4.3 Master Program", "text": "The purpose of the master program is twofold. Firstly, it generates a global state in the form of simplex multipliers \u03c0 and \u03b3. Secondly, any workable solution of the master program can be transformed into a solution of the original LP, Equation 5. Each time the algorithm columns are iterated, additional master columns are added to the master program according to the optimality of the subroutine solutions. Additional columns allow the master program to update simple column multipliers based on the subroutine solutions. For the MAP inference problems, the master program is defined according to the optimality of the subroutine solutions. (s, t) The additional columns allow the master program to update the simple column multipliers based on the subroutine solutions. (s, t) The master programs are added according to the equation 9.Maximise (s, t).The additional columns allow the master program to update the complex solution of the subroutine solutions. (s) The additional columns become the subroutine solutions according to the subroutine solutions."}, {"heading": "4.4 Optimal Solution", "text": "The resolution of Dantzig-Wolfe is guaranteed to converge in a finite number of iterations [10, Theorem 10.4]. Once it has converged, the solution for Equation 5 can be found from the convexity variables \u03b1ist and the corresponding convexity variables of the subroutines. (10) The globally optimal solution y-st is the sum of the optimal solutions of the subroutines scaled by their corresponding convexity variables (or probabilities).The y-st can be traced back to a corresponding optimal node solution x-s (see Section 3.1).The rounding may have to be applied to x-s in order to find a holistic solution. Instead of using rounding schemes such as [24, 25], we construct an integer program (IP) using the non-zeros solution x-s (see Section 3.1)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Side-Chain Prediction", "text": "That's why we're able to assert ourselves, that we're able to achieve our goals, \"he said.\" We've got to be able to achieve our goals, \"he said.\" We've got to be able to achieve our goals, \"he said.\" We've got to be able to achieve our goals, \"he said.\" We've got to be able to achieve the goals that we've set ourselves to achieve the goals. \""}, {"heading": "5.2 Shape Matching", "text": "Shape matching as a data aspect in which each point of a curve must be individually associated with another point of a different curve. In [26] Ramos et al. this problem is not so difficult to optimize (the distribution is quite high). However, the scan matching problem is characterized by a very large space in which the LP relaxation takes place, up to 362361 possible combinations. The reader is referred to further details about the CRF and its functions."}, {"heading": "6 Discussion", "text": "Many real-world problems are characterized not only by the difficulty of solving them, but also by the size of the problem and the limitations of its solution. Such problems require a different approach to algorithm design. In this paper, a novel distributed MAP inference algorithm based on LP decomposition was presented. In contrast to other LP (or QP) formulations, ours are defined via edge variables and not via node variables. The advantage of such formulation is that the LP has fewer limitations and allows splitting into a number of subroutines (one for each edge) together with a small master program. The subroutines can be distributed over a network to efficiently solve large-scale problems. In addition, the master program converges monotonously to its optimal solution, resulting in an algorithm for performing MAP inferences that is readily available. Experimental results show that the algorithm finds solutions that are comparable to the current state of the art and large-scale problems."}], "references": [{"title": "Learning conditional random fields for stereo", "author": ["D. Scharstein", "C. Pal"], "venue": "In Proc. of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Tightening lp relaxations for map using message-passing", "author": ["D. Sontag", "T. Meltzer", "A. Globerson", "Y. Weiss", "T. Jaakkola"], "venue": "In 24th Conference in Uncertainty in Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In Proc. of the International Conference on Machine Learning (ICML),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Exact maximum a posteriori estimation for binary images", "author": ["D.M. Greig", "B.T. Porteous", "A.H. Seheult"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1989}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Finding maps for belief networks is np-hard", "author": ["S.E. Shimony"], "venue": "Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K. Murphy", "Y. Weiss", "M. Jordan"], "venue": "In Proc. of the Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Quadratic programming relaxations for metric labeling and markov random field map estimation", "author": ["P. Ravikumar", "J. Lafferty"], "venue": "Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "A linear programming approach to maxsum problem: A review", "author": ["T. Werner"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Linear Programming 2: Theory and Extensions. Springer Series in Operations Research and Financial Engineering", "author": ["G.B. Dantzig", "M.N. Thapa"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Exploring Artificial Intelligence in the New Millennium, chapter Understanding Belief Propagation and Its Generalizations", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Map estimation via agreement on trees: Message passing and linear programming", "author": ["M. Wainwright", "T. Jaakola", "A. Willsky"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Convergent tree-reweighted message passing for energy minimization", "author": ["V. Kolmogorov"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Linear programming relaxations and belief propagation \u2013 an empirical study", "author": ["C. Yanover", "T. Meltzer", "Y. Weiss"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Fixing max-product: Convergent message passing algorithms for map lprelaxations", "author": ["A. Globerson", "T. Jaakkola"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Clusters and coarse partitions in lp relaxations", "author": ["D. Sontag", "A. Globerson", "T. Jaakkola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Mrf optimization via dual decomposition: Message-passing revisited", "author": ["N. Komodakis", "N. Paragios", "G Tziritas"], "venue": "In In ICCV,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "An analysis of convex relaxations for map estimation of discrete mrfs", "author": ["M. Pawan Kumar", "V. Kolmogorov", "P.H.S. Torr"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Map estimation for graphical models by likelihood maximization", "author": ["A. Kumar", "S. Zilberstein"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1977}, {"title": "Constraint Processing", "author": ["R Dechter"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "New finite pivoting rules for the simplex method", "author": ["R.G. Bland"], "venue": "Mathematics of Operations Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1977}, {"title": "Finite Mathematics: An Applied Approach (Eighth Edition)", "author": ["A. Mizrahi", "M. Sullivan"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}, {"title": "Randomized rounding: a technique for provably good algorithms and algorithmic proofs", "author": ["P. Raghavan", "Clark D. Tompson"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1987}, {"title": "Convex relaxation methods for graphical models: Lagrangian and maximum entropy approaches", "author": ["J.K. Johnson"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "CRFmatching: Conditional random fields for feature-based scan matching", "author": ["F. Ramos", "D. Fox", "H. Durrant-Whyte"], "venue": "In Proc. of Robotics: Science and Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "They have been successfully applied to a diverse set of problems such as: image processing [1], protein design [2], and text labelling [3].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "They have been successfully applied to a diverse set of problems such as: image processing [1], protein design [2], and text labelling [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": "They have been successfully applied to a diverse set of problems such as: image processing [1], protein design [2], and text labelling [3].", "startOffset": 135, "endOffset": 138}, {"referenceID": 3, "context": "For tree-structured graphs algorithms exist that are guaranteed to compute the globally optimal MAP solution in polynomial time (see for example: [4, 5]).", "startOffset": 146, "endOffset": 152}, {"referenceID": 4, "context": "For tree-structured graphs algorithms exist that are guaranteed to compute the globally optimal MAP solution in polynomial time (see for example: [4, 5]).", "startOffset": 146, "endOffset": 152}, {"referenceID": 5, "context": "Finding the MAP solution for arbitrary graphs has been proven to be NP-hard [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "One particularly popular algorithm is based on Max-Product Belief Propagation due to Pearl [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "For arbitrary graphs the algorithm has been adapted such that it runs for a number of iterations and is known as Loopy Belief Propagation (LBP, see [7]).", "startOffset": 148, "endOffset": 151}, {"referenceID": 7, "context": "Starting from a quadratic formulation over the nodes, s \u2208 V (analogous to [8]), the problem is transformed into an integer formulation over the edges, (s, t) \u2208 E.", "startOffset": 74, "endOffset": 77}, {"referenceID": 8, "context": "The resulting LP formulation is equivalent to the standard MAP LP formulation (see for example [9]).", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "Our algorithm is particularly suited for these cases as we explore the structure of the constraints to allow the application of the Dantzig-Wolfe decomposition principle [10].", "startOffset": 170, "endOffset": 174}, {"referenceID": 10, "context": "Generalised Belief Propagation [11] extends the message passing from pairs of connected nodes to higher order cliques resulting in better approximations.", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": "Tree-Reweighted Max-Product methods (TRW, [12, 13]) on the other hand decompose the original graph into a convex combination of treestructured graphs.", "startOffset": 42, "endOffset": 50}, {"referenceID": 12, "context": "Tree-Reweighted Max-Product methods (TRW, [12, 13]) on the other hand decompose the original graph into a convex combination of treestructured graphs.", "startOffset": 42, "endOffset": 50}, {"referenceID": 13, "context": "[14] showed that TRW fails to solve the problems used in the experiments of section 5.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "TRW has strong connections to the Max-Product Linear Programming (MPLP) algorithm proposed by Globerson and Jaakkola [15].", "startOffset": 117, "endOffset": 121}, {"referenceID": 1, "context": "[2, 16] is considered the state of the art in MAP inference.", "startOffset": 0, "endOffset": 7}, {"referenceID": 15, "context": "[2, 16] is considered the state of the art in MAP inference.", "startOffset": 0, "endOffset": 7}, {"referenceID": 16, "context": "[17] solve the MAP problem by decomposition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "Ravikumar and Lafferty [8] formulate the MAP problem as a Quadratic Program (QP) relaxation.", "startOffset": 23, "endOffset": 26}, {"referenceID": 17, "context": "In addition, the QP relaxation has been shown to generate poorer results compared to a LP relaxation [18].", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "More recently, Kumar and Zilberstein [19] approached the MAP estimation problem with an interesting mean field approximation method.", "startOffset": 37, "endOffset": 41}, {"referenceID": 19, "context": "ExpectationMaximisation (EM, [20]) is subsequently used to derive a message passing algorithm.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "Finally, we show that our algorithm is able to solve graphs that TRW methods and MPLP ([12, 13] and [15] respectively) are unable to solve, since our method solves the primal directly rather than optimise a bound.", "startOffset": 87, "endOffset": 95}, {"referenceID": 12, "context": "Finally, we show that our algorithm is able to solve graphs that TRW methods and MPLP ([12, 13] and [15] respectively) are unable to solve, since our method solves the primal directly rather than optimise a bound.", "startOffset": 87, "endOffset": 95}, {"referenceID": 14, "context": "Finally, we show that our algorithm is able to solve graphs that TRW methods and MPLP ([12, 13] and [15] respectively) are unable to solve, since our method solves the primal directly rather than optimise a bound.", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "[11] showed that, without loss of generality, it is possible to assume that the graph is a pair-wise Markov Random Field, i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Equation 5 is equivalent to standard MAP LP formulation (see for example [9]).", "startOffset": 73, "endOffset": 76}, {"referenceID": 16, "context": "As a result the proposed method will have a smaller communications overhead compared to, for example, the Dual Decomposition method [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 9, "context": "The Dantzig-Wolfe decomposition principle [10] allows a LP with a special block-matrix structure to be broken up into a number of independent subprograms.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "The decomposition principle exploits the Resolution Theorem [10].", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "In the more general case, algorithms that solve Constraint Satisfaction Problems (see for example [21]) can be used to find an initial solution for x\u0303s.", "startOffset": 98, "endOffset": 102}, {"referenceID": 21, "context": "In the experiments we select either the solution with the lowest index k (analogous to Bland\u2019s rule [22]), or the index k for", "startOffset": 100, "endOffset": 104}, {"referenceID": 22, "context": "which the actual cost is maximal (analogous to the Largest-Coefficient rule [23]).", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "Instead of applying rounding schemes such as [24, 25], we construct an Integer Program (IP) over the non-zeros solution states of x\u030cs.", "startOffset": 45, "endOffset": 53}, {"referenceID": 24, "context": "Instead of applying rounding schemes such as [24, 25], we construct an Integer Program (IP) over the non-zeros solution states of x\u030cs.", "startOffset": 45, "endOffset": 53}, {"referenceID": 13, "context": "The performance of the proposed algorithm (DW-LP) is measured on the Rosetta Side-Chain Prediction data set [14, 2].", "startOffset": 108, "endOffset": 115}, {"referenceID": 1, "context": "The performance of the proposed algorithm (DW-LP) is measured on the Rosetta Side-Chain Prediction data set [14, 2].", "startOffset": 108, "endOffset": 115}, {"referenceID": 13, "context": "This involves finding the three-dimensional configuration of rotamers given the backbone structure of a protein [14].", "startOffset": 112, "endOffset": 116}, {"referenceID": 1, "context": "[2] we apply our algorithm to the 30 graphs that TRW [12] is unable to solve.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "[2] we apply our algorithm to the 30 graphs that TRW [12] is unable to solve.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "We compare our algorithm against TRW-S (which improves on TRW, see [13]) and MPLP [15] as both, like our algorithm, consider only the local marginal polytope.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "We compare our algorithm against TRW-S (which improves on TRW, see [13]) and MPLP [15] as both, like our algorithm, consider only the local marginal polytope.", "startOffset": 82, "endOffset": 86}, {"referenceID": 1, "context": "In addition we also show the result for MPLP with tightening [2].", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "MPLPT is operated as described in [2].", "startOffset": 34, "endOffset": 37}, {"referenceID": 25, "context": "In [26] Ramos et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "The reader is referred to [26] for further details on the CRF and its feature functions.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "In [26] LBP is used which does not allow for such constraints to be considered in the inference process.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "In [26] 20 labelled data sets are used for training.", "startOffset": 3, "endOffset": 7}], "year": 2011, "abstractText": "We present a distributed anytime algorithm for performing MAP inference in graphical models. The problem is formulated as a linear programming relaxation over the edges of a graph. The resulting program has a constraint structure that allows application of the Dantzig-Wolfe decomposition principle. Subprograms are defined over individual edges and can be computed in a distributed manner. This accommodates solutions to graphs whose state space does not fit in memory. The decomposition master program is guaranteed to compute the optimal solution in a finite number of iterations, while the solution converges monotonically with each iteration. Formulating the MAP inference problem as a linear program allows additional (global) constraints to be defined; something not possible with message passing algorithms. Experimental results show that our algorithm\u2019s solution quality outperforms most current algorithms and it scales well to large problems.", "creator": "TeX"}}}