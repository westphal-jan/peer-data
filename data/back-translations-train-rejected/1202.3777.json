{"id": "1202.3777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2012", "title": "Belief Propagation by Message Passing in Junction Trees: Computing Each Message Faster Using GPU Parallelization", "abstract": "Compiling Bayesian networks (BNs) to junction trees and performing belief propagation over them is among the most prominent approaches to computing posteriors in BNs. However, belief propagation over junction tree is known to be computationally intensive in the general case. Its complexity may increase dramatically with the connectivity and state space cardinality of Bayesian network nodes. In this paper, we address this computational challenge using GPU parallelization. We develop data structures and algorithms that extend existing junction tree techniques, and specifically develop a novel approach to computing each belief propagation message in parallel. We implement our approach on an NVIDIA GPU and test it using BNs from several applications. Experimentally, we study how junction tree parameters affect parallelization opportunities and hence the performance of our algorithm. We achieve speedups ranging from 0.68 to 9.18 for the BNs studied.", "histories": [["v1", "Tue, 14 Feb 2012 16:41:17 GMT  (156kb)", "http://arxiv.org/abs/1202.3777v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["lu zheng", "ole mengshoel", "jike chong"], "accepted": false, "id": "1202.3777"}, "pdf": {"name": "1202.3777.pdf", "metadata": {"source": "CRF", "title": "Belief Propagation by Message Passing in Junction Trees: Computing Each Message Faster Using GPU Parallelization", "authors": ["Lu Zheng", "Ole Mengshoel", "Jike Chong"], "emails": ["ole.mengsheol@sv.cmu.edu", "jike@berkeley.edu"], "sections": [{"heading": null, "text": "The compilation of Bayesian tree-linking networks (BNs) and the propagation of beliefs about them are among the most prominent approaches to calculating background information in BNs. However, it is known that the proliferation of beliefs about tree-linkages is generally computationally intensive, and their complexity can increase dramatically with the connectivity and cardinality of Bayesian government space network nodes. In this paper, we address this arithmetic challenge with the help of GPU parallelization. We develop data structures and algorithms that extend existing techniques of tree-linkage, and specifically develop a novel approach for the parallel computation of each belief propagation. We implement our approach on an NVIDIA GPU and test it using BNs from multiple applications. We are experimentally investigating how parameters of tree-linkage chances affect parallelization, and thus the performance of our algorithm."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Belief Propagation in Junction Trees", "text": "A BN is a compact representation of a common distribution across a series of random variables X. A BN is structured as directed acyclic graphics (DAG), the cause of which is the random variables and the directed edges represent the dependence relationship between the random variables. Evidence in a Bayesian network consists of variables that have been instantiated.The node tree algorithm propagates beliefs (or stragglers) via a derived graph called a node tree. A node tree is generated from a BN by moralization and triangulation [6]. Each node Ci of the node tree contains a subset of random variables that forms a clique in the moralized and triangulated BN designated by Xi X. Linked to each vertex of the node tree there is a potential table."}, {"heading": "2.2 CPU/GPU Platform", "text": "GPUs are designed for computationally-intensive, highly parallel calculations. Compared to CPUs, GPUs have more transistors for data processing than for data caching and flow control. GPUs are well suited for problems that can be expressed as data-parallel calculations, where data elements are mapped to parallel processing threads. GPUs are typically used to accelerate computationally-intensive parts of an application and are therefore connected to a host CPU that performs control-dominant calculations. Today, however, a CPU and its GPU communicate via a PCI Express bus. CUDA is a universal parallel computing architecture developed by NVIDIA. CUDA consists of three central parallel computing abstractions: a hierarchy of thread groups, shared memories, and barrier synchronization. These abstractions are exposed to the programmer's programming language, making CUDA a growing number of CPUs."}, {"heading": "3 Computing Each Message Faster", "text": "We are paralleling the atomic functioning of the transmission of beliefs, as shown in Figure 1. The advantage of this is that parallelism at the atomic level can be embedded in different belief algorithms without these algorithms changing. In connection with each vertex of the connecting tree Ci and the set of variables Xi contained therein, there is a potential table \u03c6Xi, which contains non-negative real numbers proportional to the common distribution of Xi. If each variable can assume sj states, the size of the potential table Xi | = Xi | j = 1 sj, where Xi | includes the cardinality of Xi.Message passed by Ci to a neighboring vertex Ck with the delimiter Sik, two steps: 1. Marginalization. The potential table \u041aSik of the divider is updated by marginalizing the potential table \u2012 Sik."}, {"heading": "3.1 Index Mapping for Parallelism", "text": "Although written in a compact form, each of the equations (2) and (3) is in fact a set of many equations that update all the cells in the potential tables \u03c6Sik and \u03c6Xk. Our central contribution is to efficiently parallelise the calculations in (2) and (3) by converting these equations into independent subsets, which can be done by taking a closer look at the data flow in the message transmission processes. We focus on the j-th element of the potential table of the separator, i.e. \u03c6Sik (j) is the potential value associated with a specific instance of the Sik variables in Sik. In the marginalization step to update the value of Sik (j), we must retrieve values from the elements in \u03c6Xi that have the same instantiation for these variables."}, {"heading": "3.2 Index Mapping Table", "text": "To convert the index j to the sequence of variable states, one needs O (| Xi | \u2211 j sj) time. This could be a considerable amount of computing time if the potential size of the table is large. It is extremely inefficient to scan the entire potential table Xi and S for each thread, since only a small fraction of them are used by each thread. To counter this potential inefficiency, we introduce an index mapping table inspired by the cluster sepset mapping technique (Xi XM) [1], in which a \u00b5X, S mapping table is created to store the index mappings from processor X to processor to processor to processor. To adapt CSM to the parallel computation, instead of creating a mapping table, we create mapping tables."}, {"heading": "3.3 Belief Propagation Algorithm", "text": "Algorithm 2 Collect evidence (J, Ci) for each child of Ci do Message Passing (Ci, Collect Evidence (J, Child)) end for return (Ci) Algorithm 3 Distribute evidence (J, Ci) for each child of Ci do Message Passing (Ci, Child) Distribute evidence (J, Child) end for Algorithm 4 Belief Propagation (J, Croot) Input: J, Croot Initialization (J) Collect Evidence (J, Croot) Distribute evidence (J, Croot) end for Algorithm 4 Belief Propagation (J, Croot) Input: J, Croot Initialization (J) Input: In our work we consider the hugin algorithm, which takes over the depth-first dissemination of belief. In view of an established linkage tree J with root vertex Croot, the pseudo code is shown in Algorithm 4."}, {"heading": "3.4 Analysis of Speedup", "text": "From the above description, the height of parallelism is determined by the number of elements in the potential table of delimiters. Suppose the connecting tree has n-corners, then the total number of message transitions for the complete propagation of faith is 2 (n-1). However, if one considers a message transmitted by Ci toCk, the total number of additions (| \u03c6Xi | \u2212 Sik |) and the total number of multiplications (Xi | + | \u03c6Sik |) is the total number of complementary data. Therefore, the theoretical time complexity of a message transmitted between vertex i and k is the total number of complementary data. (The total number of multiplications is (Xi-Xk | + | \u03c6Xi-Sik). The theoretical time complexity of a message transmitted between vertex i and K is (high)."}, {"heading": "4 Experimental Results", "text": "In our work, we use the NVIDIA GeForce GTX460 as the platform for our implementation. This device consists of seven multi-processors, and each multi-processor consists of 48 cores and 48K on-chip shared memory per thread block. The maximum thread level reaches 907GFlop / s parallelism. In addition to the fast shared memory, a much larger but slower off-chip global memory (785 MB) is provided, which is shared by all multi-processors. The bandwidth between the global and the shared memory is about 90 Gbit / s. In the calculation, we use a single precision."}, {"heading": "4.1 Methods and Data", "text": "Our implementation has been tested on a number of Bayesian networks (see http: / / bndg.cs.aau.dk / html / bayesian _ networks.html), which come from different problem areas with different structures and states. These differences lead to very different node trees, as shown in Table 1. In our work, we not only want to compare the performance of our parallel code with the sequential code, but also to examine how the structure of the node tree - for example, the size of the potential table of the separators - affects performance in parallel to the sequential case. We compile the Bayesian networks offline into the node trees and then execute faith propagation across the node trees, see Algorithm 4. As mentioned in Section 3, performance is related to the distribution of the size of the potential table of the separators, i.e. we also present histograms of potential table sizes for all node trees in Figure 4."}, {"heading": "4.2 Optimization on GPU", "text": "Our novel Message Computation Algorithm (Algorithm 1) is wrapped in a kernel to allow the parallelism of the GPU, and a kernel is organized as a series of thread blocks when executed. Variation in the thread block size can affect performance, and we want to optimize thread block and grid size for each of the experimental junction trees. We experimented with different block sizes and chose the best one for a given BN. Figure 5 shows how the execution time changes with the block size for Bayes networks Barley and Munin3. For Barley, GTX460 achieves optimal performance when each block contains 48 threads, while for Munin3 the optimal performance is found with 16 threads per block. The performance differences can be explained by the degree of agreement between the configuration of the GPU architecture and the junction tree structure. On the GPU, each thread block is executed on a multiprocessor."}, {"heading": "4.3 Performance Comparison with Sequential Code", "text": "The execution time of the program is comparable to that of GeNie / SMILE [12], a widely used C + + software package for Bayesian network inferences. We do not directly use GeNie / SMILE as a starting point here, as we do not know the implementation details of GeNie / SMILE. Detailed information for the CPU and GPU platforms can be found in Table 2.Table 1, which provides the execution time comparison for the GTX460 and the Intel CPU. Acceleration obtained ranges from 0.68 to 9.18. Performance is an overall effect of many factors such as parallelism, memory latency, kernel call-up, etc. These factors in turn correlate closely with the underlying structures of the node trees. Networks of pigs, Munin2, Munin3, and Munin4 consist mainly of low speeds and separators (see Figure 4)."}, {"heading": "4.4 Kernel Overhead", "text": "When implementing our parallel inference algorithm on the GPU, we should also take into account the kernel overhead when starting a kernel. Figure 7 shows the kernel overhead as a fraction of the total execution time. The kernel overhead percentage is determined by the number of kernel calls and the computational effort per kernel call, which in turn is determined by the structure of the junction trees.The kernel overhead can greatly affect performance. For Munin3, for example, the overhead is 36% of the total execution time, giving an upper limit of 2.71 for accelerating sequential code. However, for Barley, the overhead is only 1.7% of the total execution time. The deviation in the overhead percentages is caused by differences in the structures of the Munin3 and Barley junction trees, see Table 1 and Figure 4."}, {"heading": "4.5 Memory Layout", "text": "In Figure 8, we compare two storage layouts for the mapping tables. On the left side of Figure 8, this is a na\u00efve approach; mapping tables are simply placed sequentially in global memory, which can lead to bank conflicts when the data is loaded into the shared memory. Therefore, we are introducing an advanced approach: For mapping tables from a delimiter to a clique potential table, we insert the elements with the same index in mapping tables in adjacent memory cells as on the right side of Figure 8. In our experience, this advanced storage layout resulted in a 20% - 30% improvement in overall acceleration compared to the na\u00efve layout."}, {"heading": "5 Conclusion and Future Work", "text": "In this paper, we have developed a novel approach to the parallel propagation of node tree beliefs based on the cluster sepset mapping method. Our approach focuses on the parallelization of message computation in the delivery of messages to node trees. In our approach, the parallel chance theoretically corresponds to the size of the separation potential table. Although practical problems such as kernel overhead and memory latency make it difficult to achieve this theoretical performance, our experimental results still indicate that performance is well in line with the size of the separation potential table. In experiments with a CUDA implementation of our parallel message computing algorithm running on an NVIDIA GeForce GTX460 GPU, we have investigated how performance differs with different node structures. As expected from our analysis, our approach was good for node trees with large separation potential tables, but with the small number of nodes affected."}, {"heading": "Acknowledgments", "text": "This material is based on work supported by the NSF awards CCF0937044 and ECCS0931978."}], "references": [{"title": "Inference in belief networks: A procedural guide", "author": ["C. Huang", "A. Darwiche"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Parallel exact inference on a CPU-GPGPU heterogenous system", "author": ["H. Jeon", "Y. Xia", "V.K. Prasanna"], "venue": "In Proc. of the 39th International Conference on Parallel Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "BEEM: bucket elimination with external memory", "author": ["K. Kask", "R. Dechter", "A. Gelfand"], "venue": "In Proc. of the 26th Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "A parallel Lauritzen-Spiegelhalter algorithm for probabilistic inference", "author": ["A.V. Kozlov", "J.P. Singh"], "venue": "In Proc. of the 1994 ACM/IEEE conference on Supercomputing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "The EM algorithm for graphical association models with missing data", "author": ["S.L. Lauritzen"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Local computations with probabilities on graphical structures and their application to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1988}, {"title": "Highthroughput Bayesian network learning using heterogeneous multicore computers", "author": ["M.D. Linderman", "R. Bruggner", "V. Athalye", "T.H. Meng", "N.B. Asadi", "G.P. Nolan"], "venue": "In Proc. of the 24th ACM International Conference on Supercomputing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "GraphLab: A new framework for parallel machine learning", "author": ["Y. Low", "J. Gonzalez", "A. Kyrola", "D. Bickson", "C. Guestrin", "J. Hellerstein"], "venue": "In Proc. of the 26th Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Understanding the scalability of Bayesian network inference using clique tree growth curves", "author": ["O.J. Mengshoel"], "venue": "Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Scalable parallel implementation of exact inference in Bayesian networks", "author": ["V.K. Namasivayam", "V.K. Prasanna"], "venue": "In Proc. of the 12th International Conference on Parallel and Distributed System,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Efficient inference in large discrete domains", "author": ["R. Sharma", "D. Poole"], "venue": "In Proc. of the 19th Annual Conference on Uncertainity in Artificial Intelligence", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Efficient computation of sumproducts on GPUs through software-managed cache", "author": ["M. Silberstein", "A. Schuster", "D. Geiger", "A. Patney", "J.D. Owens"], "venue": "In Proc. of the 22nd ACM International Conference on Supercomputing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Node level primitives for parallel exact inference", "author": ["Y. Xia", "V.K. Prasanna"], "venue": "In Proc. of the 19th International Symposium on Computer Architechture and High Performance Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "Exact belief updating (or marginalization) is then performed by message passing over the junction tree [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 8, "context": "Computational difficulty increases dramatically with the density of the BN, the treewidth of the network, and the number of states of each network node [9].", "startOffset": 152, "endOffset": 155}, {"referenceID": 0, "context": "In addition, some practical issues associated with the specific implementation platform also affect the computation performance [1].", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "Two fundamental issues, which may cause large cliques in junction trees, are: (i) the topology and connectedness of a BN [9] and (ii) the high cardinality of a significant set of discrete BN nodes [13].", "startOffset": 121, "endOffset": 124}, {"referenceID": 10, "context": "Two fundamental issues, which may cause large cliques in junction trees, are: (i) the topology and connectedness of a BN [9] and (ii) the high cardinality of a significant set of discrete BN nodes [13].", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "Discrete BN nodes can have high cardinalities for several reasons: First, they may represent discrete parameters, for example categorical parameters, that inherently take a large number of values [13].", "startOffset": 196, "endOffset": 200}, {"referenceID": 4, "context": "In addition, there can be major computational challenges when BN inference is in the inner loop of iterative algorithms like the EM algorithm [5].", "startOffset": 142, "endOffset": 145}, {"referenceID": 0, "context": "In this paper, we discuss data structures and algorithms that extend existing junction tree techniques [1,6], and specifically develop a novel approach to parallel message computation using belief propagation in junction trees.", "startOffset": 103, "endOffset": 108}, {"referenceID": 5, "context": "In this paper, we discuss data structures and algorithms that extend existing junction tree techniques [1,6], and specifically develop a novel approach to parallel message computation using belief propagation in junction trees.", "startOffset": 103, "endOffset": 108}, {"referenceID": 1, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 2, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 3, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 6, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 7, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 9, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 11, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 12, "context": "Parallelization of Bayesian network computation has been investigated in previous research [2\u20134, 7, 8, 10, 14, 15].", "startOffset": 91, "endOffset": 114}, {"referenceID": 3, "context": "A data parallel implementation for junction tree inference has been developed for a cachecoherent shared-address-space machine with physically distributed main memory [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 11, "context": "Parallelism in the basic sum-product computation has been investigated for GPUs [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "The efficiency in using disk memory for exact inference, using parallelism and other techniques, has been improved [3]; parallel techniques for BN structure learning have also been developed [7].", "startOffset": 115, "endOffset": 118}, {"referenceID": 6, "context": "The efficiency in using disk memory for exact inference, using parallelism and other techniques, has been improved [3]; parallel techniques for BN structure learning have also been developed [7].", "startOffset": 191, "endOffset": 194}, {"referenceID": 9, "context": "An algorithm for parallel BN inference using pointer jumping has been introduced [10].", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "Both parallelization based on graph structure [8] as well as node level primitives for parallel computing based on a table extension idea have been developed [15]; a GPU implementation based on this idea was later developed [2].", "startOffset": 46, "endOffset": 49}, {"referenceID": 12, "context": "Both parallelization based on graph structure [8] as well as node level primitives for parallel computing based on a table extension idea have been developed [15]; a GPU implementation based on this idea was later developed [2].", "startOffset": 158, "endOffset": 162}, {"referenceID": 1, "context": "Both parallelization based on graph structure [8] as well as node level primitives for parallel computing based on a table extension idea have been developed [15]; a GPU implementation based on this idea was later developed [2].", "startOffset": 224, "endOffset": 227}, {"referenceID": 12, "context": "In such settings, nodelevel operations are often the dominating part of the problem [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "However, we take a different approach from previous research [2, 15], and in particular our approach is motivated by the cluster-sepset mapping method of Huang and Darwiche [1].", "startOffset": 61, "endOffset": 68}, {"referenceID": 12, "context": "However, we take a different approach from previous research [2, 15], and in particular our approach is motivated by the cluster-sepset mapping method of Huang and Darwiche [1].", "startOffset": 61, "endOffset": 68}, {"referenceID": 0, "context": "However, we take a different approach from previous research [2, 15], and in particular our approach is motivated by the cluster-sepset mapping method of Huang and Darwiche [1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 5, "context": "A junction tree is generated from a BN by means of moralization and triangulation [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "To tackle this potential inefficiency, we introduce an index mapping table technique inspired by the cluster-sepset mapping (CSM) technique [1], where a mapping table \u03bcX ,S is created to store the index mappings from \u03c6X to \u03c6S .", "startOffset": 140, "endOffset": 143}, {"referenceID": 0, "context": "To adapt CSM to parallel computing, instead of creating one mapping table [1], we create |\u03c6Sik | mapping tables.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "Then, a two phase belief propagation is adopted [6]: collect evidence and then distribute evidence.", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "Belief propagation is just a sequence of messages passed in a certain order [6].", "startOffset": 76, "endOffset": 79}], "year": 2011, "abstractText": "Compiling Bayesian networks (BNs) to junction trees and performing belief propagation over them is among the most prominent approaches to computing posteriors in BNs. However, belief propagation over junction tree is known to be computationally intensive in the general case. Its complexity may increase dramatically with the connectivity and state space cardinality of Bayesian network nodes. In this paper, we address this computational challenge using GPU parallelization. We develop data structures and algorithms that extend existing junction tree techniques, and specifically develop a novel approach to computing each belief propagation message in parallel. We implement our approach on an NVIDIA GPU and test it using BNs from several applications. Experimentally, we study how junction tree parameters affect parallelization opportunities and hence the performance of our algorithm. We achieve speedups ranging from 0.68 to 9.18 for the BNs studied.", "creator": "dvips(k) 5.98 Copyright 2009 Radical Eye Software"}}}