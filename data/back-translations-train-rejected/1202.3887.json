{"id": "1202.3887", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2012", "title": "Extended Mixture of MLP Experts by Hybrid of Conjugate Gradient Method and Modified Cuckoo Search", "abstract": "This paper investigates a new method for improving the learning algorithm of Mixture of Experts (ME) model using a hybrid of Modified Cuckoo Search (MCS) and Conjugate Gradient (CG) as a second order optimization technique. The CG technique is combined with Back-Propagation (BP) algorithm to yield a much more efficient learning algorithm for ME structure. In addition, the experts and gating networks in enhanced model are replaced by CG based Multi-Layer Perceptrons (MLPs) to provide faster and more accurate learning. The CG is considerably depends on initial weights of connections of Artificial Neural Network (ANN), so, a metaheuristic algorithm, the so-called Modified Cuckoo Search is applied in order to select the optimal weights. The performance of proposed method is compared with Gradient Decent Based ME (GDME) and Conjugate Gradient Based ME (CGME) in classification and regression problems. The experimental results show that hybrid MSC and CG based ME (MCS-CGME) has faster convergence and better performance in utilized benchmark data sets.", "histories": [["v1", "Fri, 17 Feb 2012 11:49:56 GMT  (205kb)", "http://arxiv.org/abs/1202.3887v1", "13 pages, 2 figures"]], "COMMENTS": "13 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hamid salimi", "davar giveki", "mohammad ali soltanshahi", "javad hatami"], "accepted": false, "id": "1202.3887"}, "pdf": {"name": "1202.3887.pdf", "metadata": {"source": "CRF", "title": "Extended Mixture of MLP Experts by Hybrid of Conjugate Gradient Method and Modified Cuckoo Search", "authors": ["Hamid Salimi", "Davar Giveki", "Mohammad Ali Soltanshahi", "Javad Hatami"], "emails": ["salimi.hamid86@gmail.com,", "ali.soltanshahi@gmail.com,", "jvdhtm@gmail.com", "s9dagive@stud.uni-saarland.de"], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijaia.2012.3101 1This paper examines a new method for improving the learning algorithm of the Mixture of Experts (ME) model using a hybrid of Modified Cuckoo Search (MCS) and Conjugate Gradient (CG) as second-order optimization technology. CG technology will be combined with the Back Propagation (BP) algorithm to provide a much more efficient learning algorithm for the ME structure. Furthermore, the experts and gating networks in the extended model will be replaced by CG-based Multi-Layer Perceptrons (MLPs) for faster and more accurate learning."}, {"heading": "ME (GDME) and Conjugate Gradient Based ME (CGME) in classification and regression problems. The experimental results show that hybrid MSC and CG based ME (MCS-CGME) has faster convergence and better performance in utilized benchmark data sets.", "text": "KEYWORDSBack Propagation (BP) Algorithm, Gradient Decent (GD), Conjugate Gradient (CG), Modified Cuckoo Search (MCS), Mix of Experts (MEs)"}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them are able to play by the rules that they have shown in recent years, and they are able to play by the rules."}, {"heading": "2. CONJUGATE GRADIENT MULTI-LAYER PERCEPTRONS (CGMLP)", "text": "The BP learning algorithm is a supervised learning method for multi-layered, forward-facing neural networks. It is a gradient descend method that includes incremental error correction of network weights. Despite the overall success of the BP method in the learning process, several major deficiencies (gradient descend rate (w) still need to be solved (BP's convergence rate is slow and therefore unsuitable for major problems). In addition, the convergence behavior of the BP algorithm is dependent on the choice of initial values of the connecting weights and other parameters used in the algorithm, such as the learning rate and the impulse term.One way to accelerate the learning phase is to use the optimization of higher order. In the BP case, the function is only approximated by the linear terms that contain the first-order derivatives in a Taylor series expansion. In this case, the non-linear terminology of the second order, which also includes the second order of the second order, is also included in the second order of the accuracy of the derivatives."}, {"heading": "3. MIXTURE OF EXPERTS (ME)", "text": "MLPs have been successfully used to solve various regression and classification problems, but when there are large problems, the parameter space of MLPs becomes huge and thus becomes mathematically insoluble during the training phase. To address this problem, the principle of \"parts and victories\" can be used. According to the \"parts and victories\" approach, a complex task can be solved by dividing it into simple tasks and combining the solutions accordingly. ME is a well-known method based on this principle, which falls into the category of dynamic classifiers that combine where the input signal is directly integrated into the mechanism that integrates the experts \"results into an overall result [8]. ME was first proposed in [3]. Their proposed model contains a population of simple linear classifiers (the experts), and the results of the experts are mixed using a gating network. Technically, the experts perform supervised learning because for modelling the desired response, the experts combine the results."}, {"heading": "3.1. Mixture of Experts based on Gradient Decent Based (GDME)", "text": "In order to improve the performance of the expert networks, this version of Mixture of Experts uses MLPs instead of linear networks or expert networks, modifying the learning algorithm using an estimate of the rear probability of the desired output by each expert. Therefore, the gate and expert networks match and this improves the proposed model for selecting the best expert (s). The weights of the expert networks of MLPs are updated based on these estimates and the process repeats itself. As mentioned before, the convergence rate of the GD learning algorithms is slow and therefore unsuitable for major problems. To solve this problem, we use the Conjugate Gradient method in the ME learning process."}, {"heading": "3.2. Mixture of Expert based on Conjugate Gradient (CGME)", "text": "In this section we use the Conjugate Gradient (CG) methods to accelerate the learning phase of neural networks (MEPs = 11). Each expert network is an MLP network with a hidden layer that calculates an output iO as a function of the input vector, x and weights of the hidden and output layers and a sigmoid activation function. We assume that each expert is specialized in a specific area of the input space. The gating network assigns a weighting ig to each expert, iO. The gating network determines ig as a function of the input vector x and a number of parameters such as the weights of its hidden and output layers. The activation function of the hidden layer is sigmoid, and in the output layer we use linear activation function. The softmax function was applied to the outputs of the gating network to create more diversity."}, {"heading": "4. METAHEURISTIC ALGORITHM", "text": "To address this problem, we applied a metaheuristic algorithm, the so-called Modified Cuckoo Search Algorithm. In the 1950s and 1960s, computer scientists explored the possibility of using the concepts of evolution as an optimization tool for engineers, and a subclass of gradient-free methods, called genetic algorithms (GA) [20], emerged. Since then, many other algorithms inspired by nature have been developed, such as Particle Swarm Optimization (PSO) [21], Differential Evolution (DE) [22], and more recently Cuckoo Search (CS) [23]. These are heuristic techniques that utilize a large population of possible solutions in each iteration."}, {"heading": "4.1. Cuckoo Search (CS)", "text": "CS is a metaheuristic search algorithm recently proposed by Yang and Deb. [26] The algorithm is inspired by the cuckoo's reproductive strategy. At the most basic level, cuckoos lay their eggs in the nests of other host birds, which can be of different species. [25] The host bird can find out that the eggs are not its own and either destroy the egg or abandon the nest all together, which has led to the development of cuckoo eggs that mimic the eggs of local host birds [25]. To use this as an optimization tool, Yang and Deb [26] use three idealized rules: \u2022 Each cuckoo clock lays an egg that represents a set of solution coordinates and deposits it in a random nest; \u2022 A fraction of the nests that contain the best eggs or solutions are transferred to the nextgeneration; the number of nests that is an alias and can detect a nest."}, {"heading": "4.2. Modified Cuckoo Search (MCS)", "text": "In view of the fact that the CS will always find the optimum [24], but since the search is based exclusively on random walks, a fast convergence cannot be guaranteed. [25] Two modifications of the method were made with the aim of increasing the convergence rate, which results in the method being more practical for a wider range of applications, but without losing the attractive properties of the original method. [22] The first modification was adapted to the size of the L\u00e9vy flight stage size. In CS, the constant and the value 1\u03b1 = is applied. In the MCS, the value of \u03b1 decreases as the number of generations increases, for the same reasons that the inertia constant in the PSO is reduced. [22] in order to promote the localized search for the individuals or the eggs, the solution approaches. An initial value of the L\u00e9vy flight stage size 1A = is selected in each generation and a new flight level is calculated."}, {"heading": "5. EXPERIMENTAL RESULTS", "text": "In order to evaluate the performance of the proposed method, we conducted three experiments: In the first experiment, we used the hybrid MCS and CGME method for functional approximation; in the second experiment, we tested this method on an artificial data set; and in the third experiment, we evaluated this method on seven UCI data sets [30]."}, {"heading": "5.1. Static function approximation", "text": "For comparison, the underlying function to be approximated is a non-linear function with three inputs, which is widely used to test the efficiency of proposed algorithms [31]: 0.5 1 1.5 2 (1) f x y z \u2212 \u2212 = + + + + (19) The training samples consist of 500 uniformly sampled data with three inputs from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 \u00d7 and the corresponding target data. A further 250 test samples are uniformly evaluated from [2, 5] [2, 5] \u00d7 \u00d7, Table 1 and the error rate of MCS-CGME, GDME and CGME is presented in the function Approximation Task."}, {"heading": "5.2. Artificial Dataset", "text": "We are looking at a complex synthetic dataset; this dataset is generated using identical class of parametric distributions. In this dataset, each class consists of three two-dimensional Gaussian: 223 (,) (,) 1ij ijNi N jC \u00b5 \u03b4\u00b5 \u03b4 \u2032 = = U, where 1, 2.3 = i (20) The classes are parameterized as follows: (6.1) (14.1) (18.1) 1 (2.1) (3.1) (2.1) (2.1) {,} = N N N N N N N NC (21) (5.1) (10.5.1) (20.1) 2 (1.1) (3.5.1) (0.1) {,} \u2212 = N N N N N NC (22) (3.1) (12.1) (18.1) (2.1) (2.1) 3 (2.1) (6.1) (2.1) (2.1). N N N N N N N N N N N C \u2212 = (23) Figure 2 shows our synthetic dataset and the other two we are summarizing in each case."}, {"heading": "5.3. UCI data sets", "text": "Each of these ensemble models consists of five identical MLP networks initialized with (different) random weights, four of which are used as experts and the fifth as gating networks. MLPs and gating networks consist of 5 and 15 sigmoid neurons in the hidden layer, respectively. Classification performance is measured by 10-fold cross-validation. Three ensemble models are trained over 100 epochs and the learning rates of experts and gating networks are set to 0.1 and 0.15, respectively. Results are summarized in Table 4. To investigate the superiority of our proposed combination method over the standalone MLP, a single MLP network is used as a baseline (see Table 4). This MLP consists of a hidden layer of 25 neurons, so that the complexity of this MLP is similar to the CLP and the GDE dataset in this experiment."}, {"heading": "6. CONCLUSION", "text": "In this paper, an enhanced version of ME, MCS-CGME, is presented. In our proposed method, the CG optimization technique is used in the BP learning algorithm by experts and in the gating network in the ME model to address the problems associated with the GD method. Furthermore, MCS is used to initially achieve optimal weights for NN, and as a result, the convergence rate and performance of the model are increased. Experimental results from regression and classification benchmark datasets show the superiority of our proposed method compared to GDME and CGME."}], "references": [{"title": "and R", "author": ["L.I. Kuncheva", "M. Skurichina"], "venue": "P. W. Duin, \u201cAn experimental study on diversity for bagging and boosting with linear classifiers,\u201d information Fusion, val. 3, pp. 245-258", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "Combination of multiple classifiers using local accuracy estimates", "author": ["K. Woods", "W.P. Kegelmeyer", "K. Bowyer"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Adaptive mixture of experts", "author": ["R.A. Jacobs", "M.I. Jordan", "S.E. Nowlan", "G.E. Hinton"], "venue": "Neural Comput.Vol. 3 pp.79\u201387", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1991}, {"title": "A modified mixture of experts network structure for ECG beats classification with diverse features", "author": ["I. Guler", "E.D. Ubeyli"], "venue": "Eng. Appl. Artif. Intell. 18 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "A mixture of experts network structure construction algorithm for modelling and control", "author": ["X. Hong", "C.J. Harris"], "venue": "Appl. Intell. 16 (1) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Improved learning algorithms for mixture of experts in multiclass classification", "author": ["K. Chen", "L. Xu", "H. Chi"], "venue": "Neural Networks 12 (9) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1252}, {"title": "Face detection using mixture of MLP", "author": ["R. Ebrahimpour", "E. Kabir", "M.R. Yousefi"], "venue": "experts, Neural Process. Lett. 26 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Neural Networks- A Comprehensive Foundation", "author": ["S. Haykin"], "venue": "Prentice-Hall, 2nd Edition", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning internal representations by error backpropagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "in: D.E. Rumelhart, J.L. McClelland (Eds.), Parallel Distributed Processing, vol. 1, The MIT Press", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1986}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": "Clarendon Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Understanding Neural Networks and Fuzzy Logic", "author": ["S.V. Kartalopoulos"], "venue": "IEEE Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Modified cuckoo search: A new gradient free optimisation algorithm. Chaos, Solitons& Fractals.Volume", "author": ["S. Walton", "O. Hassan", "K. Morgan", "R. Brown M"], "venue": "Issue", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Cuckoo search via L vy flights", "author": ["Yang X-S", "Deb S"], "venue": "Proceedings of World Congress on Nature & Biologically Inspired Computing (NaBIC", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Hierarchies of adaptive experts\u201d, In Proceedings of the advances in neural information processing systems", "author": ["R. Jordan R", "M. Jacobs"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Hierarchical mixtures of experts and the EM algorithm", "author": ["M. Jordan", "R. Jacobs"], "venue": "Neural Computing, vol. 2, pp. 181\u2013214", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "Convergence results for the EM approach to mixtures of experts architectures", "author": ["M.I. Jordan", "L. Xu"], "venue": "Neural Networks, vol. 8, pp. 1409\u20131431", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Statistical mechanics of the mixture of experts", "author": ["K. Kang", "J-H. Oh"], "venue": "Proceedings of Advances in Neural Information Processing Systems, vol. 9, pp. 183\u2013189", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1997}, {"title": "Bayesian methods for mixture of experts", "author": ["S. Waterhouse", "D. MacKay", "T. Robinson"], "venue": "\u201d, In Proceedings of Advances in Neural Information Processing Systems, vol. 8., pp.351\u2013357", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Mixtures of experts estimate a posteriori probabilities.", "author": ["P. Moerland"], "venue": "Proceedings ofthe international conference on artificial neural networks,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "Organization of face and object recognition in modular neural networks", "author": ["M.N. Dailey", "G.W. Cottrell"], "venue": "Neural Networks, vol.12, pp. 1053\u20131073", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "An introduction to genetic algorithms.sixth ed", "author": ["M. Mitchell"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Defining a standard for particle swarm optimization", "author": ["D Bratton", "J. Kennedy"], "venue": "Proceedings of the 2007 IEEE Swarm Intelligence Symposium,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Differential evolution \u2013 a simple and efficient heuristic for global optimization over continuous spaces", "author": ["R Storn", "K. Price"], "venue": "Journal of Global Optimisation", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "Engineering optimisation by cuckoo search. International Journal of Mathematical Modelling and Numerical Optimisation 2010;1:330\u201343", "author": ["Yang X-S", "Deb S"], "venue": "Journal of Artificial Intelligence & Applications (IJAIA),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Cuckoo search via L \u0301evy flights", "author": ["Yang X-S", "Deb S"], "venue": "Proceedings of World Congress on Nature & Biologically Inspired Computing (NaBIC", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "\u0301evy flights and superdiffusion in the context of biological encounters and random searches", "author": ["L Viswanathan GM"], "venue": "Physics of Life Reviews", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "A particle swarm optimization approach for hexahedral mesh smoothing. International Journal for Numerical Methods in Fluids 2009;60:5578", "author": ["A EgemenYilmaz", "M. Kuzuoglu"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Newman,\u201dUCI Machine Learning Repository\u201d,[http://www.ics.uci.edu/~mlearn/MLRepository.html", "author": ["D.J.A. Asuncion"], "venue": "University of California, School of Information and Computer Science,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Handbook of pattern recognition and computer vision.3rd ed", "author": ["Chen CH", "Wang PSP"], "venue": "World Scientific;", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "Previous experimental and theoretic results show that the combining classifiers with each other lead to higher performance when the base classifiers have small error rates, and their errors are different [1]; in other words, the base classifiers make uncorrelated decision in this case.", "startOffset": 204, "endOffset": 207}, {"referenceID": 1, "context": "Generally, classifier selection and classifier fusion are two types of combining classifiers [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "In the basic form of ME [3], the expert and gating networks are linear classifiers, however, for more", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "[7] proposes a face detection model, in which they used MultiLayer Perceptrons (MLPs) [8, 9, 10] to form the gating and expert networks in order to improve the face detection accuracy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7] proposes a face detection model, in which they used MultiLayer Perceptrons (MLPs) [8, 9, 10] to form the gating and expert networks in order to improve the face detection accuracy.", "startOffset": 86, "endOffset": 96}, {"referenceID": 8, "context": "[7] proposes a face detection model, in which they used MultiLayer Perceptrons (MLPs) [8, 9, 10] to form the gating and expert networks in order to improve the face detection accuracy.", "startOffset": 86, "endOffset": 96}, {"referenceID": 9, "context": "[7] proposes a face detection model, in which they used MultiLayer Perceptrons (MLPs) [8, 9, 10] to form the gating and expert networks in order to improve the face detection accuracy.", "startOffset": 86, "endOffset": 96}, {"referenceID": 10, "context": "For instance, the convergence behavior of the BP algorithm highly depends on the choice of initial values of connection weights and other parameters used in the algorithm such as the learning rate and the momentum term [11, 8].", "startOffset": 219, "endOffset": 226}, {"referenceID": 7, "context": "For instance, the convergence behavior of the BP algorithm highly depends on the choice of initial values of connection weights and other parameters used in the algorithm such as the learning rate and the momentum term [11, 8].", "startOffset": 219, "endOffset": 226}, {"referenceID": 11, "context": "The MCS algorithm proposed by Walton et al [12] is an improved version of another metaheuristic algorithm the so called Cuckoo Search (CS) [13] and it can be used for initialing optimal values of connection weights.", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": "The MCS algorithm proposed by Walton et al [12] is an improved version of another metaheuristic algorithm the so called Cuckoo Search (CS) [13] and it can be used for initialing optimal values of connection weights.", "startOffset": 139, "endOffset": 143}, {"referenceID": 7, "context": "This method is in the category of dynamic classifiers combining where the input signal is directly involved in the mechanism that integrates the output of the experts into an overall result [8].", "startOffset": 190, "endOffset": 193}, {"referenceID": 2, "context": "For the first time ME was proposed in [3].", "startOffset": 38, "endOffset": 41}, {"referenceID": 12, "context": "In [13, 14] this method was extended to the so-called \u201cHierarchical Mixture of Experts\u201d (HME).", "startOffset": 3, "endOffset": 11}, {"referenceID": 13, "context": "The ME has been studied for a wide range of research [15-18].", "startOffset": 53, "endOffset": 60}, {"referenceID": 14, "context": "The ME has been studied for a wide range of research [15-18].", "startOffset": 53, "endOffset": 60}, {"referenceID": 15, "context": "The ME has been studied for a wide range of research [15-18].", "startOffset": 53, "endOffset": 60}, {"referenceID": 16, "context": "The ME has been studied for a wide range of research [15-18].", "startOffset": 53, "endOffset": 60}, {"referenceID": 17, "context": "As pointed out in [19], in the network\u2019s learning process, the experts \u201ccompete\u201d for explaining the input while the gate network rewards the winner of each competition using larger feedbacks.", "startOffset": 18, "endOffset": 22}, {"referenceID": 18, "context": "During the 1950s and 1960s, computer scientists investigated the possibility of applying the concepts of evolution as an optimization tool for engineers and this gave birth to a subclass of gradient free methods called genetic algorithms (GA) [20].", "startOffset": 243, "endOffset": 247}, {"referenceID": 19, "context": "Since then many other algorithms have been developed that have been inspired by nature, for example particle swarm optimization (PSO) [21], differential evolution (DE) [22] and, more recently, the cuckoo search (CS) [23].", "startOffset": 134, "endOffset": 138}, {"referenceID": 20, "context": "Since then many other algorithms have been developed that have been inspired by nature, for example particle swarm optimization (PSO) [21], differential evolution (DE) [22] and, more recently, the cuckoo search (CS) [23].", "startOffset": 168, "endOffset": 172}, {"referenceID": 21, "context": "Since then many other algorithms have been developed that have been inspired by nature, for example particle swarm optimization (PSO) [21], differential evolution (DE) [22] and, more recently, the cuckoo search (CS) [23].", "startOffset": 216, "endOffset": 220}, {"referenceID": 22, "context": "Here, we used a new version of CS, the so called Modified Cuckoo Search (MCS) which was introduced by Walton et al [24].", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": "They showed that MCS is more reliable and faster than CS, GA, DE and PSO [24].", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "CS is a metaheuristic search algorithm which has been recently proposed by Yang and Deb [26].", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "This has resulted in the evolution of cuckoo eggs which mimic the eggs of local host birds [25].", "startOffset": 91, "endOffset": 95}, {"referenceID": 24, "context": "To apply this as an optimization tool, Yang and Deb [26] used three idealized rules:", "startOffset": 52, "endOffset": 56}, {"referenceID": 0, "context": "\u2022 Each cuckoo lays one egg, which represents a set of solution co-ordinates, at a time and dumps it in a random nest; \u2022 A fraction of the nests containing the best eggs, or solutions, will carry over to the next generation; \u2022 The number of nests is fixed and there is a probability that a host can discover an alien egg, say [0,1] a p \u2208 .", "startOffset": 325, "endOffset": 330}, {"referenceID": 21, "context": "The steps involved in the CS are then derived from these rules and are shown in Algorithm 1 [23].", "startOffset": 92, "endOffset": 96}, {"referenceID": 25, "context": "1 3 L\u00e9vy u t \u03bb \u03bb \u223c = \u2264 \u2264 (17) This process represents the optimum random search pattern and is frequently found in nature [28].", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "1 could be beneficial in problems of small domains, in the examples presented here \u03b1 = 1 is used in line with the work by Yang and Deb [26].", "startOffset": 135, "endOffset": 139}, {"referenceID": 24, "context": "Yang and Deb [26] do not discuss boundary handling in their formulation, but an approach similar to PSO boundary handling [29] is adopted here.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Yang and Deb [26] do not discuss boundary handling in their formulation, but an approach similar to PSO boundary handling [29] is adopted here.", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "Yang and Deb [26] found that the convergence rate was not strongly affected by the value and they suggested setting a p = 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "The use of L\u00e9vy flights as the search method means that the CS can simultaneously find all optima in a design space and the method has been shown to perform well in comparison with PSO and GA [24].", "startOffset": 192, "endOffset": 196}, {"referenceID": 22, "context": "Modified Cuckoo Search (MCS) Given enough computation, the CS will always find the optimum [24] but, as the search relies entirely on random walks, a fast convergence cannot be guaranteed.", "startOffset": 91, "endOffset": 95}, {"referenceID": 23, "context": "In [25] two modifications to the method were made with the aim of increasing the convergence rate which leads to making the method more practical for a wider range of applications but without losing the attractive features of the original method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In CS, \u03b1 is constant and the value 1 \u03b1 = is employed [26].", "startOffset": 53, "endOffset": 57}, {"referenceID": 20, "context": "This is done for the same reasons that the inertia constant is reduced in the PSO [22], i.", "startOffset": 82, "endOffset": 86}, {"referenceID": 27, "context": "In second experiment, we tested this method on an artificial data set, and in third experiment we evaluated this method on seven UCI data sets [30].", "startOffset": 143, "endOffset": 147}, {"referenceID": 28, "context": "For the sake of comparison, the underlying function to be approximated is a three-input nonlinear function which is widely used to verify the efficiency of proposed algorithms [31]: 0.", "startOffset": 176, "endOffset": 180}, {"referenceID": 0, "context": "5 2 (1 ) f x y z \u2212 \u2212 = + + + (19) The training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 and the corresponding target data.", "startOffset": 127, "endOffset": 133}, {"referenceID": 5, "context": "5 2 (1 ) f x y z \u2212 \u2212 = + + + (19) The training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 and the corresponding target data.", "startOffset": 127, "endOffset": 133}, {"referenceID": 0, "context": "5 2 (1 ) f x y z \u2212 \u2212 = + + + (19) The training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 and the corresponding target data.", "startOffset": 134, "endOffset": 140}, {"referenceID": 5, "context": "5 2 (1 ) f x y z \u2212 \u2212 = + + + (19) The training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 and the corresponding target data.", "startOffset": 134, "endOffset": 140}, {"referenceID": 0, "context": "5 2 (1 ) f x y z \u2212 \u2212 = + + + (19) The training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 and the corresponding target data.", "startOffset": 141, "endOffset": 147}, {"referenceID": 5, "context": "5 2 (1 ) f x y z \u2212 \u2212 = + + + (19) The training samples consist of 500 uniformly sampled three-input data from the input ranges [1, 6] [1, 6] [1, 6] \u00d7 \u00d7 and the corresponding target data.", "startOffset": 141, "endOffset": 147}, {"referenceID": 1, "context": "Other 250 testing samples are uniformly sampled from[2, 5] [2, 5] [2, 5] \u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and CGME in the function approximation task.", "startOffset": 52, "endOffset": 58}, {"referenceID": 4, "context": "Other 250 testing samples are uniformly sampled from[2, 5] [2, 5] [2, 5] \u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and CGME in the function approximation task.", "startOffset": 52, "endOffset": 58}, {"referenceID": 1, "context": "Other 250 testing samples are uniformly sampled from[2, 5] [2, 5] [2, 5] \u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and CGME in the function approximation task.", "startOffset": 59, "endOffset": 65}, {"referenceID": 4, "context": "Other 250 testing samples are uniformly sampled from[2, 5] [2, 5] [2, 5] \u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and CGME in the function approximation task.", "startOffset": 59, "endOffset": 65}, {"referenceID": 1, "context": "Other 250 testing samples are uniformly sampled from[2, 5] [2, 5] [2, 5] \u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and CGME in the function approximation task.", "startOffset": 66, "endOffset": 72}, {"referenceID": 4, "context": "Other 250 testing samples are uniformly sampled from[2, 5] [2, 5] [2, 5] \u00d7 \u00d7 , Table 1 are shown the error rate of MCS-CGME, GDME and CGME in the function approximation task.", "startOffset": 66, "endOffset": 72}, {"referenceID": 27, "context": "Seven UCI data sets [30] are used in the experiments.", "startOffset": 20, "endOffset": 24}], "year": 2012, "abstractText": "This paper investigates a new method for improving the learning algorithm of Mixture of Experts (ME) model using a hybrid of Modified Cuckoo Search (MCS) and Conjugate Gradient (CG) as a second order optimization technique. The CG technique is combined with Back-Propagation (BP) algorithm to yield a much more efficient learning algorithm for ME structure. In addition, the experts and gating networks in enhanced model are replaced by CG based Multi-Layer Perceptrons (MLPs) to provide faster and more accurate learning. The CG is considerably depends on initial weights of connections of Artificial Neural Network (ANN), so, a metaheuristic algorithm, the so-called Modified Cuckoo Search is applied in order to select the optimal weights. The performance of proposed method is compared with Gradient Decent Based ME (GDME) and Conjugate Gradient Based ME (CGME) in classification and regression problems. The experimental results show that hybrid MSC and CG based ME (MCS-CGME) has faster convergence and better performance in utilized benchmark data sets.", "creator": "PScript5.dll Version 5.2.2"}}}