{"id": "1202.3890", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2012", "title": "PAC Bounds for Discounted MDPs", "abstract": "We study upper and lower bounds on the sample-complexity of learning near-optimal behaviour in finite-state discounted Markov Decision Processes (MDPs). For the upper bound we make the assumption that each action leads to at most two possible next-states and prove a new bound for a UCRL-style algorithm on the number of time-steps when it is not Probably Approximately Correct (PAC). The new lower bound strengthens previous work by being both more general (it applies to all policies) and tighter. The upper and lower bounds match up to logarithmic factors.", "histories": [["v1", "Fri, 17 Feb 2012 11:59:55 GMT  (23kb)", "http://arxiv.org/abs/1202.3890v1", "25 LaTeX pages"]], "COMMENTS": "25 LaTeX pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tor lattimore", "marcus hutter"], "accepted": false, "id": "1202.3890"}, "pdf": {"name": "1202.3890.pdf", "metadata": {"source": "CRF", "title": "PAC Bounds for Discounted MDPs", "authors": ["Tor Lattimore", "Marcus Hutter"], "emails": ["tor.lattimore@anu.edu.au", "marcus.hutter@anu.edu.au"], "sections": [{"heading": null, "text": "ar Xiv: 120 2.38 90v1 [cs.LG] 1 7Contents 1 Introduction 2 2 Notation 2 3 Estimation 3 4 Upper Confidence Reinforcement Learning Algorithm 3 5 Upper PAC Bounds 5 6 Eliminating the Assumption 11 7 Lower PAC Bound 11 8 Conclusion 13 References 13 A Proof Lower PAC Bound 14 B Technical Results 18 C Proof of Lemma 8 20 D Constants 23 E Table of Notation 24Keywords Reinforcement learning; sample complexity; exploration exploitation; PAC-MDP; Markov decision processes."}, {"heading": "1 Introduction", "text": "The goal of reinforcement learning is to construct algorithms that learn to act optimally or almost so in unknown environments. In this paper, we limit our attention to finite, discounted MDPs with unknown transitions. The performance of reinforcement learning algorithms in this environment can be measured in various ways, for example by the use of remorse or PAC boundaries (Kakade, 2003). We focus on the latter, which is a measure of the number of time steps in which an algorithm is highly likely not to be nearly optimal. Many previous algorithms have proven to be PACs with different boundaries (Kakade, 2003; Strehl and Littman, 2005; Strehl and al, 2006, 2009; Szita and Szepesva, 2010; Auer, 2011). We best modify the Upper Confidence Reinforcement Learning (UCRL) (Streer and Littman, 2005)."}, {"heading": "2 Notation", "text": "While we have tried to define everything before using it, readers are encouraged to consult the tables of notation and constants in the appendix. General: N = {0, 1, 2, \u00b7 \u00b7 \u00b7} is the natural number. If A is a set, then | A | is its size and A * is the set of all finite ordered subsets. Unless otherwise mentioned, Log represents the natural logarithm. For random variable X we write EX and VarX for their expectation and / or variance. We often make use of the progression zi = 2 i \u2212 2 for i \u2265 1. Define a set Z (a): = zi: 1 \u2264 i \u2264 i \u2264 argmini a}.Markov Definition Process A multiple is a function."}, {"heading": "3 Estimation", "text": "In the next section we present the new algorithm, but first we give an intuitive introduction to the kind of parameter estimation required to prove the limits of sample complexity for MDPs. \u2212 \u03b3 \u2212 \u2212 \u2212 \u2212 \u2212 The general idea is to show the empirical estimation of a transition probability that approaches the true probability exponentially quickly in the number of samples collected. (There is a wide variety of concentration imbalances, each of which serves a slightly different purpose. \u2212 \u03b3 \u2212 \u2212 \u2212 n We are improving the work to date by using Bernstein's inequality that takes variance into account (as opposed to Hoeffding). The following example shows the need for Bernstein's inequality in estimating the value functions of MDPs. \u2212 p There is also insight into how the evidence works in the next two sections. \u2212 s0 r = 1s1 r = 01 \u2212 pp1 \u2212 qq Let's look at the Markov reward process on the right-hand side with two transitional states, and in the transitional states being shown."}, {"heading": "4 Upper Confidence Reinforcement Learning Algorithm", "text": "The idea is to choose the smallest model class, which guarantees to contain the true model with a high probability and act according to the most optimistic model within that class. By choosing the model class well, this guarantees a policy that distorts its research towards unknown states that can yield good rewards while avoiding states that are known to be bad. The approach has been successful in obtaining uniform complexity (or regret) in various areas where exploration / exploitation is a problem (Lai and Robbins, 1985; Agrawal, 1995; Strehl and Littman, 2005; Auer and Ortner, 2007; Auer et al, 2011).Unfortunately, in order to prove our new problem that we need to maintain the transition."}, {"heading": "5 Upper PAC Bounds", "text": "We introduce two new PAC stages with probability. (Umax and Emax are defined in Appx D., although we are really based on the assumptions.) We introduce two new PAC stages with probability. (The first improves on all previous analyses, but relies on assumption 1. The second is completely general, but gains an additional dependence on the second phase, which leads to a PAC tied to the second and third phases. (This is worse than the previous best phase in relation to the previous phase, but better in relation to the second phase, in which the second phase is completed.) Let M be the true MDP satisfaction phase 1. Let the actual (non-stationary) politics of the UCRL be (algorithm 1), then V stages (st) - V stages (v) - V stages (st) - V stages (st) - and the third phase of the third phase, in which the third phase is not completed. (The third phase is the actual (non-stationary phase) politics of the UCL, third stage)."}, {"heading": "6 Eliminating the Assumption", "text": "The upper limit in the previous section could only be proved by assuming 1. In this section, we describe a possible approach to generalizing the evidence and why this cannot be trivial. In the work above, we used the assumption that \u2212 p \u00b7 s is bound (ps, \u03c0 \u2212 p, \u03c0) \u00b7 V \u00b7 Inequality (s). \u00b7 A natural approach to generalizing this approach results from Amber's inequality (Theorem 30). If V \u00b2 R | S | is a value function independent of p, then Amber's inequality can be used to show that (ps, p \u00b2 s, \u03c0) \u00b7 p \u00b2 s, p \u00b2 s is non-formal value (s)."}, {"heading": "7 Lower PAC Bound", "text": "The approach is similar to that of Strehl et al. (2009), but we are making two refinements to improve the binding, which depends on 1 / (1 \u2212) 3 and removes the political constraints; the first is to add a delaying state in which no information can be obtained, but where an algorithm is still unable to be a PAC; the second is more subtle and becomes in the proof definition 14. A non-stationary policy is a function in which no (possibly non-stationary) policy depending on S, A, R, E, E and E, then there is a Markov decision-making process Mhard, so that V (st) > V for at least N time steps with the probability that N: = c1 | S \u00b7 A \u00b7 2 (1) 3 log c2 and c2 > 0."}, {"heading": "8 Conclusion", "text": "Abstract. While the lower limit is entirely general, the upper limit depends on the assumption that there are at most two next states for each state / action pair. Apart from this assumption, the new upper limit improves on the best-known Auer limit so far (2011). If the assumption is dropped, the new evidence can be used to construct an algorithm that is both larger and more general. The class of MDPs used for the counterexample meets assumption 1, but worse in | S |. The lower limit, which comes without assumptions, improves the work of Strehl et al. (2009) by being both larger and more general. Assumption 1 meets the class of MDPs used for the counterexample, and thus the upper and lower limits now match in this limited case Running Time."}, {"heading": "A Proof of Lower PAC Bound", "text": "The proof of this is the use of a simple form of bandit and theorem 16, which maps the sample complexity of bandit algorithms. (1) We need a new notation, which is required for non-stationary politics and bandits. (1) We write s1: t = s1, s2, \u00b7, st for the historical sequence of length. History can be concatenated, so s1: t = s2, \u00b7, \u00b7, st where an armed bandit is a vector p: A \u2192 [0, 1]. Politics interacts with a bandit sequentially. In the time step t an arm is played, eupon gets the politics reward 1 with probability p (a) and reward 0 otherwise. This is repeated over all time periods. A bandit policy is a function. (0, 1)"}, {"heading": "B Technical Results", "text": "Theorem 29 (Hoeffding Inequality) \u2212 Should X1, \u00b7 \u00b7, Xn be independent (0, 1] -weighted random variables with probability 1. ThenP (Bernstein, 1924) \u2212 Should X1, \u00b7 \u00b7, Xn be independent Real random variables with zero mean and variance VarXi = 2 i. If | Xk | < c with probability one thenP (Bernstein, 1924) \u2212 Should X1, \u00b7 \u00b7, Xn be independent Real random variables with zero mean and variance VarXi = 2 i. If | Xk | < c with probability one thenP (Bernstein, 1924) \u2212 Should we use 1 n = 1Xi = 2 + 2 + 2 random variables with zero mean and variance VarXi = 2 i."}, {"heading": "C Proof of Lemma 8", "text": "We must define some higher \"moments\" of the value function, as this makes the proof difficult, but possibly unavoidable ist.Definition 32. We define the space of the bounded value / reward functions. \u2212 Definition: \u2212 Definition: \u2212 Definition: \u2212 Definition: \u2212 Definition: \u2212 Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition: Definition:: Definition: Definition:: Definition:: Definition:: Definition: Definition:: Definition:: Definition:: Definition:: Definition:: Definition:: Definition:: Definition:: Definition::: Definition:: Definition:: Definition:: Definition::: Definition::: Definition::: Definition::: Definition::: Definition:::: Definition:"}, {"heading": "D Constants", "text": "For convenience, we list them below, including approximate upper / lower limits, as appropriate. Constant O / A value: = 1 Log 2 Log 8 | S value (1 \u2212 value) 2 Log (1 \u2212 value) 2 Log (1 \u2212 value) 2 \u03b2: = 1 Log 2 Log 1 \u2212 value (1 \u2212 value) | D | = | K |: = | Z (1 \u2212 value) | S | I value (1 \u2212 value) H: = 11 \u2212 value (1 \u2212 value) | K | K value (1 \u2212 value) H: = 11 \u2212 value (1 \u2212 value) H: = 11 \u2212 value (1 \u2212 value) H: = 11 \u2212 value (1 \u2212 value) H: = 11 \u2212 value (1 \u2212 value) H: = 11 \u2212 value (1 \u2212 value) H: = 11 \u2212 value (1 \u2212 value) S | 2 Log S: 1 \u2212 value (1 \u2212 value) S | 2 \u2212 value (1 \u2212 value) S (1 \u2212 value) S (1 \u2212 value) S (1 \u2212 value) S (1 \u2212 value) S (1 \u2212 value) S (1 \u2212 S) (S) (S \u2212 value) S (S \u2212 value): 1 \u2212 value | S (S \u2212 value) S (S \u2212 value) (S \u2212 S \u2212 value) (S \u2212 value) S (S \u2212 value) (S \u2212 value) S (S \u2212 S (S \u2212 value) (S \u2212 value) S (S \u2212 value) S (S (S \u2212 value): 1 \u2212 value) S (S (S \u2212 value) S (S (S \u2212 value) (S (S \u2212 value): 1 \u2212 value) S (S (S (S \u2212 value) 1 \u2212 value) S (S (S (S (S \u2212 value) | 1 \u2212 value) | S (S (S \u2212 value) (S (S \u2212 value) 1 \u2212 value): 1 \u2212 value) S (S (S (1 \u2212 value) (S (S (1 \u2212 value) | 1 \u2212 value) 1 \u2212 value), S (S (1 \u2212 value), S (S (S (1 \u2212 value) | 1 \u2212 value): 1 \u2212 value), S (1 \u2212 value), S (1 \u2212 value), S (S (1 \u2212 value): 1 \u2212 value),"}, {"heading": "E Table of Notation", "text": "S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S, S S S, S S, S S S S, S S, S S S, S S S, S S S, S S S S, S S, S S, S S, S S, S S S, S S, S S S, S S, S S S, S S, S S, S S S, S S S, S S, S S S, S S, S S S, S S, S S S, S S S, S S, S S, S, S S S, S S S, S S, S S, S S S S S, S, S S S, S S, S S, S S S, S S, S S S, S S S, S S, S S S S, S, S S S S, S S, S S, S S S, S S S, S S, S, S S S, S S S, S, S S S, S, S S S, S S, S S, S S, S S, S S, S S S, S, S S, S S, S, S S S, S S, S, S S S, S S, S S, S S, S S, S, S S, S S S S, S, S, S S S, S S, S S, S S, S S, S, S, S S S, S S S, S, S S S, S, S S, S"}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Neural Information Processing Systems", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Near-optimal regret bounds for reinforcement", "author": ["Mach. Learn"], "venue": null, "citeRegEx": "Learn.,? \\Q2002\\E", "shortCiteRegEx": "Learn.", "year": 2002}, {"title": "On a modification of Chebyshev\u2019s inequality and of the error formula of Laplace", "author": ["S. Bernstein"], "venue": "Mathe\u0301matique des Annales Scientifiques des Institutions Savantes de l\u2019Ukraine,", "citeRegEx": "Bernstein.,? \\Q1924\\E", "shortCiteRegEx": "Bernstein.", "year": 1924}, {"title": "On The Sample Complexity Of Reinforcement Learning", "author": ["S. Kakade"], "venue": "PhD thesis,", "citeRegEx": "Kakade.,? \\Q2003\\E", "shortCiteRegEx": "Kakade.", "year": 2003}, {"title": "Asymptotically efficient adaptive allocation rules", "author": ["T. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics,", "citeRegEx": "Lai and Robbins.,? \\Q1985\\E", "shortCiteRegEx": "Lai and Robbins.", "year": 1985}, {"title": "The sample complexity of exploration in the multi-armed bandit problem", "author": ["S. Mannor", "J. Tsitsiklis"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Mannor and Tsitsiklis.,? \\Q2004\\E", "shortCiteRegEx": "Mannor and Tsitsiklis.", "year": 2004}, {"title": "The variance of discounted Markov decision processes", "author": ["M. Sobel"], "venue": "Journal of Applied Probability,", "citeRegEx": "Sobel.,? \\Q1982\\E", "shortCiteRegEx": "Sobel.", "year": 1982}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["A. Strehl", "M. Littman"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "Strehl and Littman.,? \\Q2005\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2005}, {"title": "An analysis of model-based interval estimation for Markov decision processes", "author": ["A. Strehl", "M. Littman"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Strehl and Littman.,? \\Q2008\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2008}, {"title": "PAC model-free reinforcement learning", "author": ["A. Strehl", "L. Li", "E. Wiewiorac", "J. Langford", "M. Littman"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Strehl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2006}, {"title": "Reinforcement learning in finite MDPs: PAC analysis", "author": ["A. Strehl", "L. Li", "M. Littman"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Strehl et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2009}, {"title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "author": ["I. Szita", "C. Szepesv\u00e1ri"], "venue": "In ICML,", "citeRegEx": "Szita and Szepesv\u00e1ri.,? \\Q2010\\E", "shortCiteRegEx": "Szita and Szepesv\u00e1ri.", "year": 2010}, {"title": "T (A, \u01eb, \u03b4) if for all bandits the arm chosen on time-step T satisfies p(a\u2217)\u2212 p(aT ) \u2264 \u01eb with probability at least 1\u2212 \u03b4", "author": [], "venue": "(Mannor and Tsitsiklis,", "citeRegEx": "T,? \\Q2004\\E", "shortCiteRegEx": "T", "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "The performance of reinforcement learning algorithms in this setting can be measured in a number of ways, for instance by using regret or PAC bounds (Kakade, 2003).", "startOffset": 149, "endOffset": 163}, {"referenceID": 3, "context": "Many previous algorithms have been shown to be PAC with varying bounds (Kakade, 2003; Strehl and Littman, 2005; Strehl et al., 2006, 2009; Szita and Szepesv\u00e1ri, 2010; Auer, 2011).", "startOffset": 71, "endOffset": 178}, {"referenceID": 7, "context": "Many previous algorithms have been shown to be PAC with varying bounds (Kakade, 2003; Strehl and Littman, 2005; Strehl et al., 2006, 2009; Szita and Szepesv\u00e1ri, 2010; Auer, 2011).", "startOffset": 71, "endOffset": 178}, {"referenceID": 11, "context": "Many previous algorithms have been shown to be PAC with varying bounds (Kakade, 2003; Strehl and Littman, 2005; Strehl et al., 2006, 2009; Szita and Szepesv\u00e1ri, 2010; Auer, 2011).", "startOffset": 71, "endOffset": 178}, {"referenceID": 0, "context": "We modify the Upper Confidence Reinforcement Learning (UCRL) algorithm of Auer et al. (2010); Auer (2011); Strehl and Littman (2008) and, under the assumption that there are at most two possible next-states for each state/action pair, prove a PAC bound of", "startOffset": 74, "endOffset": 93}, {"referenceID": 0, "context": "We modify the Upper Confidence Reinforcement Learning (UCRL) algorithm of Auer et al. (2010); Auer (2011); Strehl and Littman (2008) and, under the assumption that there are at most two possible next-states for each state/action pair, prove a PAC bound of", "startOffset": 74, "endOffset": 106}, {"referenceID": 0, "context": "We modify the Upper Confidence Reinforcement Learning (UCRL) algorithm of Auer et al. (2010); Auer (2011); Strehl and Littman (2008) and, under the assumption that there are at most two possible next-states for each state/action pair, prove a PAC bound of", "startOffset": 74, "endOffset": 133}, {"referenceID": 11, "context": "This bound is an improvement1 on the previous best (Auer, 2011) and published best (Szita and Szepesv\u00e1ri, 2010), which are", "startOffset": 83, "endOffset": 111}, {"referenceID": 9, "context": "We also present a matching (up to logarithmic factors) lower bound that is both larger and more general than the previous best given by Strehl et al. (2009). The class of MDPs used in the counter-example satisfy the assumption used in the upper bound.", "startOffset": 136, "endOffset": 157}, {"referenceID": 4, "context": "The approach has been successful in obtaining uniform sample complexity (or regret) bounds in various domains where the exploration/exploitation problem is an issue (Lai and Robbins, 1985; Agrawal, 1995; Auer et al., 2002; Strehl and Littman, 2005; Auer and Ortner, 2007; Auer et al., 2010; Auer, 2011).", "startOffset": 165, "endOffset": 302}, {"referenceID": 7, "context": "The approach has been successful in obtaining uniform sample complexity (or regret) bounds in various domains where the exploration/exploitation problem is an issue (Lai and Robbins, 1985; Agrawal, 1995; Auer et al., 2002; Strehl and Littman, 2005; Auer and Ortner, 2007; Auer et al., 2010; Auer, 2011).", "startOffset": 165, "endOffset": 302}, {"referenceID": 7, "context": "Note that the existence of the function ExtendedValueIteration is proven and an algorithm given by Strehl and Littman (2008). Note that sa and sa are dependent on (s, a) and are known to the algorithm.", "startOffset": 99, "endOffset": 125}, {"referenceID": 0, "context": "The proof of Theorem 3 borrows components from the work of Auer et al. (2010), Strehl and Littman (2008) and Szita and Szepesv\u00e1ri (2010).", "startOffset": 59, "endOffset": 78}, {"referenceID": 0, "context": "The proof of Theorem 3 borrows components from the work of Auer et al. (2010), Strehl and Littman (2008) and Szita and Szepesv\u00e1ri (2010).", "startOffset": 59, "endOffset": 105}, {"referenceID": 0, "context": "The proof of Theorem 3 borrows components from the work of Auer et al. (2010), Strehl and Littman (2008) and Szita and Szepesv\u00e1ri (2010). 1.", "startOffset": 59, "endOffset": 137}, {"referenceID": 3, "context": "Also called the discounted future state distribution in Kakade (2003).", "startOffset": 56, "endOffset": 70}, {"referenceID": 6, "context": "The proof of part 2 is closely related to the approach taken by Strehl and Littman (2008). Recall that M\u0303 is chosen optimistically by extended value iteration.", "startOffset": 64, "endOffset": 90}, {"referenceID": 6, "context": "See the paper of Sobel (1982) for a proof.", "startOffset": 17, "endOffset": 30}, {"referenceID": 9, "context": "The approach is similar to that of Strehl et al. (2009), but we make two refinements to improve the bound to depend on 1/(1\u2212\u03b3)3 and remove the policy restrictions.", "startOffset": 35, "endOffset": 56}, {"referenceID": 5, "context": "We can then make use of a theorem of Mannor and Tsitsiklis (2004) on bandit sample-complexity to show that the number of times a\u2217 is not selected is at least", "startOffset": 37, "endOffset": 66}, {"referenceID": 9, "context": "Note that Strehl et al. (2009) proved their theorem for a specific class of policies while Theorem 15 holds for all policies.", "startOffset": 10, "endOffset": 31}, {"referenceID": 8, "context": "This assumption aside, the new upper bound improves on the previously best known bound of Auer (2011). If the assumption is dropped then the new proof can be used to construct an algorithm that is better than the bound of Auer (2011) in terms of 1/(1\u2212 \u03b3), but worse in |S|.", "startOffset": 0, "endOffset": 102}, {"referenceID": 8, "context": "This assumption aside, the new upper bound improves on the previously best known bound of Auer (2011). If the assumption is dropped then the new proof can be used to construct an algorithm that is better than the bound of Auer (2011) in terms of 1/(1\u2212 \u03b3), but worse in |S|.", "startOffset": 0, "endOffset": 234}, {"referenceID": 7, "context": "The lower bound, which comes without assumptions, improves on the work of Strehl et al. (2009) by being both larger and more general.", "startOffset": 74, "endOffset": 95}, {"referenceID": 7, "context": "We did not analyze the running time of our version of UCRL, but expect analysis similar to that of Strehl and Littman (2008) can be used to show that UCRL can be approximated to run in polynomial time with no cost to sample-complexity.", "startOffset": 99, "endOffset": 125}], "year": 2012, "abstractText": "We study upper and lower bounds on the sample-complexity of learning nearoptimal behaviour in finite-state discounted Markov Decision Processes (MDPs). For the upper bound we make the assumption that each action leads to at most two possible next-states and prove a new bound for a UCRL-style algorithm on the number of time-steps when it is not Probably Approximately Correct (PAC). The new lower bound strengthens previous work by being both more general (it applies to all policies) and tighter. The upper and lower bounds match up to logarithmic factors.", "creator": "LaTeX with hyperref package"}}}