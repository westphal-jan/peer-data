{"id": "1202.4002", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Feb-2012", "title": "Generalized Principal Component Analysis (GPCA)", "abstract": "This paper presents an algebro-geometric solution to the problem of segmenting an unknown number of subspaces of unknown and varying dimensions from sample data points. We represent the subspaces with a set of homogeneous polynomials whose degree is the number of subspaces and whose derivatives at a data point give normal vectors to the subspace passing through the point. When the number of subspaces is known, we show that these polynomials can be estimated linearly from data; hence, subspace segmentation is reduced to classifying one point per subspace. We select these points optimally from the data set by minimizing certain distance function, thus dealing automatically with moderate noise in the data. A basis for the complement of each subspace is then recovered by applying standard PCA to the collection of derivatives (normal vectors). Extensions of GPCA that deal with data in a high- dimensional space and with an unknown number of subspaces are also presented. Our experiments on low-dimensional data show that GPCA outperforms existing algebraic algorithms based on polynomial factorization and provides a good initialization to iterative techniques such as K-subspaces and Expectation Maximization. We also present applications of GPCA to computer vision problems such as face clustering, temporal video segmentation, and 3D motion segmentation from point correspondences in multiple affine views.", "histories": [["v1", "Fri, 17 Feb 2012 20:07:25 GMT  (864kb)", "http://arxiv.org/abs/1202.4002v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["rene vidal", "yi ma", "shankar sastry"], "accepted": false, "id": "1202.4002"}, "pdf": {"name": "1202.4002.pdf", "metadata": {"source": "CRF", "title": "Generalized Principal Component Analysis (GPCA)", "authors": ["Ren\u00e9 Vidal"], "emails": [], "sections": [{"heading": null, "text": "We represent the sub-spaces with a series of homogeneous polynomials, the degree of which is the number of sub-spaces and the derivatives of which result in normal vectors for the sub-space passing the point. If the number of sub-spaces is known, we show that these polynomials can be estimated linearly on the basis of the data; therefore, sub-space segmentation is reduced to the classification of one point per subspace. We optimally select these points from the data set by minimizing certain distance functions and thus automatically dealing with moderate noise in the data. A basis for supplementing each sub-space is then restored by applying standard PCA to the collection of derivatives (normal vectors). Extensions of GPCA, which deal with data in a high-dimensional space and with an unknown number of sub-spaces, are also presented. Our experiments on low-dimensional data show that GPCA surpasses existing algebraic algorithms based on polynomial factorization and provides good initialization of dynamic problems of motion."}, {"heading": "1 INTRODUCTION", "text": "PRINCIPAL Component Analysis (PCA) [12] refers to the problem of the arrangement of a linear partial space S IRD of unknown dimension d < D to N sample points fxjgNj \u00bc 1 in S. This problem is evident in a variety of applications in many areas, e.g., pattern recognition, data compression, regression, image processing, etc., and can be solved in a remarkably simple way from the single value decomposition (SVD) of the data matrix \u00bd x1; x2;.; xN 2 IRD, image processing, etc. This linear algebraic interpretation of minimizing the sum of square substances from the (noisy) data points xj to their projections ~ xj in S.xj In addition to these algebraic and geometric interpretations, PCA can be understood in a probable way."}, {"heading": "1.1 Previous Work on Subspace Segmentation", "text": "Subspace segmentation is a fundamental problem in many applications in computer vision (e.g. image / motion / video segmentation), image processing (e.g. image representation and compression), and system theory (e.g. hybrid system identification), which is usually considered \"chicken andegg.\" If data segmentation is known, one could easily insert a single subspace into each group of points with standard PCA. Conversely, if the subspace bases were known, one could easily find the data points that best fit each subspace. Since in practice neither the subspace bases nor the segmentation of the data are known, most existing methods choose a basis for each subspace and then iterate between data segmentation and subspace estimation. This can be done by, for example, segmenting the subspace base base base base base base base base base base segments, and segmenting the data segmentation of the subspace base base segments."}, {"heading": "1.2 Paper Organization and Contributions", "text": "In this paper, we propose an algebro-geometric approach to subspace segmentation called Generalized Principal Component Analysis (GPCA), based on the adaptation, differentiation, and division of polynomials. Unlike previous work, we do not limit ourselves to orthogonal, trivial overlaps, or known and equal dimensions. Instead, we address the most general case of an arbitrary number of subspaces with unknown and possibly different dimensions (e.g. Fig. 1) and arbitrary overlaps between subspaces."}, {"heading": "2 AN INTRODUCTORY EXAMPLE", "text": "Imagine that in IR3 we are given data from a line S1 \u00bc fx: x1 \u00bc x2 \u00bc 0g and a plane S2 \u00bc fx: x3 \u00bc 0g, as in Fig. 1. We can describe the two subspaces as S1 [S2 \u00bc fx: \u00f0x1 \u00bc x2 \u00bc 0\u0421IR1 \u00bc fx: \u00f01 \u00bc fx], although each subspace is de-scribed with polynomial sofdegreeons (linear equations), the mixture of two subspaces is described with two polynomials of degree two, namely p21\u00f0x2 \u00bc \u00f0x3 \u00bc 0 and p22\u00f0x3.we more generally consider that two linear subspaces can be represented in IR3, which satisfy some polynomials of the formulas."}, {"heading": "3 GENERALIZED PRINCIPAL COMPONENT ANALYSIS", "text": "In this section, we derive a constructive algebro-geometric solution to the problem of sub-space segmentation if the number of sub-spaces n is known; the case in which the number of sub-spaces is unknown is discussed in Section 4.2. Our algebrogeometric solution is summarized in the following theorem: Theorem 1 (Generalized Principal Component Analysis). A union of n sub-spaces of the IRD can be represented by a series of homogeneous degree n polynomials in D variables. These polynomials can be estimated linearly if there are enough samples in the sub-spaces. A basis for the completion of each sub-space can be derived from the derivatives of these polynomials to a point in each of the sub-spaces. Such points can be selected recursively by means of polynomial division. Therefore, the sub-space segmentation problem is mathematically equivalent to the matching, differentiation and division of a series of homogeneous polynomials."}, {"heading": "3.1 Notation", "text": "The space of all homogeneous degree n polynomials in D variables is a vector space of the dimension MnG1; a special basis for this space is given by all homogeneous degree n polynomials in D variables, i.e. the polynomials n in D variables are a vector space of the dimension MnG1;..; D, and n1."}, {"heading": "3.3 Fitting Polynomials to Data Lying in Multiple Subspaces", "text": "The problem of identifying subranges fSigni 1 from a set of data pointsX \u00bc: fxjgNj \u00bc 1 in the subranges is equivalent to solving the normal fundamentals fBign1 \u00bc 1 from the set of nonlinear equations in (6). Although these polynomial equations are not linear in each data point x, they are actually linear in the coefficient vector cn. We use the space of coefficient vectors cn of all homogenetic polynomialities that must be fulfilled in each data point, we have cTn in each data point 0 for all.; N use the space of coefficient vectors cn of all homogeneous polynomialisms that disappear on the n subranges."}, {"heading": "3.4 Obtaining a Basis and the Dimension of Each Subspace by Polynomial Differentiation", "text": "In this section we show that the bases fBigni \u00bc 1 can be obtained for the completion of the n subspaces and their dimensions fdigni \u00bc 1 by distinguishing all polynomials that can be obtained from the left zero of the embedded data matrix V n\u00f0 \u00bc. In this case there is only one vector bi 2 IRD normal to the subspace Si. Therefore, there is only one polynomial that represents the n hyperplanes, namely the pn\u00f0x2 dimensions of the n dimensions."}, {"heading": "3.5 Choosing One Point per Subspace by Polynomial Division", "text": "In this section, however, we show how to select these n points in the unmonitored learning scenarios where we do not know which of the data points we are using. To this end, we note that we can always select a point that is located on one of the sub-spaces, say Sn, by checking whether the pnffi points in the unmonitored learning scenery are selected. As we specify a number of data points in the sub-spaces, we could in principle select any one of the data points. However, in the presence of noise and outliers, we can be far away from the true sub-spaces. In Section 2, we have selected a point in the data that minimizes the problem."}, {"heading": "4 EXTENSIONS TO THE BASIC GPCA ALGORITHM", "text": "In this section we will discuss some extensions of GPCA that deal with practical situations, such as low-dimensional sub-spaces of a high-dimensional space and an unknown number of sub-spaces."}, {"heading": "4.1 Projection and Minimum Representation", "text": "If the dimension of ambient space D is large, the complexity of GPCA in this case is projected into two segments of space. However, since Mn\u00f0D\u00de is in the order nD, we are interested in modelling the data as a union of sub-spaces. Remember that we can only choose one basis for the zero space of V n\u00f0D\u00de, and that linear combinations of factorizable polynomials are not necessarily factorizable. In such cases, it seems rather redundant to use IRD to represent such a low-dimensional linear structure. One way to reduce dimensionality is to limit the data to a small-dimensional (sub-) space. An example is shown in Fig, where two lines L1 and L2 are projected onto a general plane in IR3."}, {"heading": "4.2 Identifying an Unknown Number of Subspaces of Unknown Dimensions", "text": "The solution of the partial space segmentation problem proposed in Section 3 requires prior knowledge of the number of sub-spaces. < In practice, however, the number of sub-spaces cannot be determined beforehand, so we cannot directly estimate the polynomials representing the sub-spaces. < From Section 3 we know that in this case there is a unique polynomium of degree n that disappears in Z \u00bc 1Si, in a union of n different hyperplanes Si five times. < from Section 3 we know that there is a unique polynomium of degree n that disappears in Z \u00bc 1Si. <"}, {"heading": "5 EXPERIMENTAL RESULTS AND APPLICATIONS IN", "text": "COMPUTER VISIONIn this section, we first evaluate the performance of the GPCA using synthetically generated data by comparing and combining it with the following approaches: 1. Polynomial factorization algorithm (PFA). However, this algorithm is only applicable to the case of hyperplanes. It calculates the normal vectors fbigni \u00bc 1 on the n hyperplanes by factoring the homogeneous polynomial algorithm \u00bc \u00f0bT1 x\u00debT2 x\u00de \u00f0bTnx\u00de into a product of linear factors. See [24] for more details. 2. K-subspaces. In view of an initial estimate for the subspace bases, this algorithm moves between clusters of data points using the distance to the different subspaces and calculating a basis for each subspace using standard PCA. See [10] for more details."}, {"heading": "5.1 Experiments on Synthetic Data", "text": "The experimental setup consists of selecting n \u00bc 2; 3; 4 collections of N \u00bc 200n points at randomly selected levels in IR3. Zero-mean Gaussian noise with s.t.d. of 0 percent to 5 percent along the subspace norms is added to the sample points. We perform 1,000 tests for each sound level. For each experiment, the error between the true (unit) normal vectors fbigni \u00bc 1 and their estimates for ig n \u00bc 1 is calculated as the mean error between the normal vectors: Error \u00bc: 1 n Xn i \u00bc 1 acos bTi b."}, {"heading": "5.2 Face Clustering under Varying Illumination", "text": "In view of a collection of blank images fIj 2 IRDgNj \u00bc 1 of n different faces taken under different illumination, we would like to group the image according to the face of the same person. For a Lambertian object, it was shown that the totality of all images taken under all light conditions forms a cone in the image space, which can be approximated by a low-dimensional subspace. [10] Therefore, we can first cluster the image collection by estimating the basis for each of these subspaces, since the images of different faces will be located in different subspaces. Since the number of pixels D in practice is large compared to the dimension of the subspaces, we apply PCA to project the images onto IRD 0 with D0 D (see Section 4.1). Specifically, we calculate the SVD of the data I1 I2 IN \u00bd D N \u00bc U V T and consider a genetic subsurface consisting of the first D0 columns of the IRD data."}, {"heading": "5.3 Temporal Segmentation of Video Sequences", "text": "Consider a news video sequence in which the camera switches between a small number of scenes. For example, the host could interview a guest and the camera could switch between the host, the guest and both, as shown in Fig. 7a. Given the frames fIj 2 IRDgNj \u00bc 1, we would like to group them according to the different scenes. We assume that all frames corresponding to the same scene live in a low-dimensional subspace of IRD and that different scenes correspond to different subspaces. As in the case of face clustering, we can segment the video sequence into different scenes by applying GPCA to the image data projected onto the first few main components. Figure shows the segmentation results for two video sequences. In both cases, perfect segmentation is achieved."}, {"heading": "5.4 Segmentation of Linearly Moving Objects", "text": "In this section we apply GPCA to the problem of the segmentation of the 3D movement of several objects subject to a purely translational movement. We refer the reader to [25], [26] where, in the case of arbitrary rotation and translation via the segmentation of a mixture of basic matrices, the scene can be modeled as a mixture of purely translational motion models, fTigni \u00bc 1, where Ti 2 represents the translation of objects relative to the camera between the two successive frames. In view of the images x1 and x2 of a point in object i in the first and second frame, or rays x1, x2 and Ti are coplanar. Therefore, x1, x2 and Ti must represent the known epipolar limitations for linear motion.xT2 \u00f0Ti x1\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441flifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifliflifli"}, {"heading": "5.5 Three-Dimensional Motion Segmentation from Multiple Affine Views", "text": "Let fxfp 2 IR2gp \u00bc 1; Nf \u00bc 1;...; F a collection of F images of N 3D dots fXp 2 IR3gNj \u00bc 1 taken by a moving affine camera. Under the affine camera model, which generates the orthographic, weak perspective and paraperspec projection, the images fulfill the equationxfp \u00bc AfXp; \u00f037G, where Af 2 IR2 4 is the affine camera matrix for frame f, which depends on the position and orientation of the camera as well as the internal calibration parameters. Therefore, if we stack all image measurements into a 2F N matrix W, we obtainW \u00bc MSTx11 x1N... xF1 xFN2 6643 775 2F N \u00bc A1.. AF 2 664 3 775 2F X1 XN \u00bd N N N: 338."}, {"heading": "6 CONCLUSIONS AND OPEN ISSUES", "text": "We have proposed an algebro-geometric approach to sub-space segmentation called Generalized Principal ComponentAnalysis (GPCA). Our approach is based on estimating the collection of polynomials from data and then evaluating their derivatives at a data point to provide a basis for the space passing through it. Our experiments showed that the GPCA causes about half the error taking into account existing algebraic algorithms based on polynomial factorization and significantly improves the performance of fiterative techniques such as K-sub-spaces and EM. We also demonstrated the performance of the GPCA on visual problems such as facial grouping and video / motion segmentation. Currently, the GPCA works well when the number and dimensions of sub-spaces are small, but performance deteriorates when the number of sub-spaces and the number of sub-spaces increases."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors would like to thank Drs. Jacopo Piazzi and Kun Huang for their contributions to this work and Drs. Frederik Schaffalitzky and Robert Fossum for their insightful discussions on this topic. This work was supported in part by Hopkins WSE Startup Funds, UIUC ECE Startup Funds and by grants NSF CAREER IIS-0347456, NSF CAREER IIS0447739, NSF CRS-EHS-0509151, ONR YIP N00014-05-10633, ONR N00014-00-1-0621, ONR N000140510836 and DARPA F33615-98-C-3614."}], "references": [{"title": "A New Approach to Dimensionality Reduction Theory and Algorithms", "author": ["D.S. Broomhead", "M. Kirby"], "venue": "SIAM J. Applied Math., vol. 60, no. 6, pp. 2114-2142, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "A Generalization of Principal Component Analysis to the Exponential Family", "author": ["M. Collins", "S. Dasgupta", "R. Schapire"], "venue": "Advances on Neural Information Processing Systems, vol. 14, 2001.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "A Multibody Factorization Method for Independently Moving Objects", "author": ["J. Costeira", "T. Kanade"], "venue": "Int\u2019l J. Computer Vision, vol. 29, no. 3, pp. 159-179, 1998.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "The Approximation of One Matrix by Another of Lower Rank", "author": ["C. Eckart", "G. Young"], "venue": "Psychometrika, vol. 1, pp. 211-218, 1936.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1936}, {"title": "RANSAC Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography", "author": ["M.A. Fischler", "R.C. Bolles"], "venue": "Comm. ACM, vol. 26, pp. 381-395, 1981.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1981}, {"title": "Zelevinsky, Discriminants, Resultants, and Multidimensional Determinants", "author": ["I.M. Gelfand", "M.M. Kapranov", "A.V"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Algebraic Geometry: A", "author": ["J. Harris"], "venue": "First Course. Springer-Verlag,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1992}, {"title": "Clustering Apperances of Objects under Varying Illumination Conditions", "author": ["J. Ho", "M.-H. Yang", "J. Lim", "K.-C. Lee", "D. Kriegman"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 1, pp. 11-18, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Minimum Effective Dimension for Mixtures of Subspaces: A Robust GPCA Algorithm and Its Applications", "author": ["K. Huang", "Y. Ma", "R. Vidal"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 2, pp. 631-638, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Principal Component Analysis", "author": ["I. Jolliffe"], "venue": "New York: Springer- Verlag,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1986}, {"title": "Motion Segmentation by Subspace Separation and Model Selection", "author": ["K. Kanatani"], "venue": "Proc. IEEE Int\u2019l Conf. Computer Vision, vol. 2, pp. 586-591, 2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Multi-Stage Optimization for Multi- Body Motion Segmentation", "author": ["K. Kanatani", "Y. Sugaya"], "venue": "Proc. Australia-Japan Advanced Workshop Computer Vision, pp. 335-349, 2003.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiple Eigenspaces", "author": ["A. Leonardis", "H. Bischof", "J. Maver"], "venue": "Pattern Recognition, vol. 35, no. 11, pp. 2613-2627, 2002.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Nonlinear Component Analysis as a Kernel Eigenvalue Problem", "author": ["B. Scholkopf", "A. Smola", "K.-R. Muller"], "venue": "Neural Computation, vol. 10, pp. 1299-1319, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "A Unified Computational Theory for Motion Transparency and Motion Boundaries Based on Eigenenergy Analysis", "author": ["M. Shizawa", "K. Mase"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, pp. 289-295, 1991.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1991}, {"title": "Probability and Random Processes with Applications to Signal Processing, third ed", "author": ["H. Stark", "J.W. Woods"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "Mixtures of Probabilistic Principal Component Analyzers,\u201dNeural", "author": ["M. Tipping", "C. Bishop"], "venue": "Computation, vol. 11,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Probabilistic Principal Component Analysis", "author": ["M. Tipping", "C. Bishop"], "venue": "J. Royal Statistical Soc., vol. 61, no. 3, pp. 611-622, 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "An Integrated Bayesian Approach to Layer Extraction from Image Sequences", "author": ["P. Torr", "R. Szeliski", "P. Anandan"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 23, no. 3, pp. 297-303, Mar. 2001.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Motion Segmentation with Missing Data by PowerFactorization and Generalized PCA", "author": ["R. Vidal", "R. Hartley"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. II, pp. 310-316, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "A New GPCA Algorithm for Clustering Subspaces by Fitting, Differentiating, and Dividing Polynomials", "author": ["R. Vidal", "Y. Ma", "J. Piazzi"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. I, pp. 510-517, 2004.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Generalized Principal Component Analysis (GPCA)", "author": ["R. Vidal", "Y. Ma", "S. Sastry"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. I, pp. 621-628, 2003.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Two-View Multibody Structure from Motion", "author": ["R. Vidal", "Y. Ma", "S. Soatto", "S. Sastry"], "venue": "Int\u2019l J. Computer Vision, to be published in 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "A Unified Algebraic Approach to 2-D and 3- D Motion Segmentation", "author": ["R. Vidal", "Y. Ma"], "venue": "Proc. European Conf. Computer Vision, pp. 1-15, 2004.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Multibody Grouping via Orthogonal Subspace Decomposition", "author": ["Y. Wu", "Z. Zhang", "T.S. Huang", "J.Y. Lin"], "venue": "Proc. IEEE Conf. Computer Vision and Pattern Recognition, vol. 2, pp. 252-257, 2001.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 9, "context": "PRINCIPAL Component Analysis (PCA) [12] refers to the problem of fitting a linear subspace S IR of unknown dimension d < D to N sample points fxjgNj1\u204441 in S.", "startOffset": 35, "endOffset": 39}, {"referenceID": 17, "context": "In Probabilistic PCA [20] (PPCA), the noise is assumed to be drawn from an unknown distribution and the problem becomes one of identifying the subspace and distribution parameters in a maximum-likelihood sense.", "startOffset": 21, "endOffset": 25}, {"referenceID": 1, "context": "When the noise distribution is Gaussian, the algebro-geometric and probabilistic interpretations coincide [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 1, "context": "noise distribution is non-Gaussian, the solution toPPCA is no longer linear, as shown in [2], where PCA is generalized to", "startOffset": 89, "endOffset": 92}, {"referenceID": 13, "context": "The standard solution toNLPCA [16] is based on first embedding the data into a higher-dimensional feature space F and then applying standard PCA to the embedded data.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "In the context of stochastic signal processing, PCA is also known as the Karhunen-Loeve transform [18]; in the applied statistics literature, SVD is also known as the Eckart and Young decomposition [4].", "startOffset": 98, "endOffset": 102}, {"referenceID": 3, "context": "In the context of stochastic signal processing, PCA is also known as the Karhunen-Loeve transform [18]; in the applied statistics literature, SVD is also known as the Eckart and Young decomposition [4].", "startOffset": 198, "endOffset": 201}, {"referenceID": 7, "context": ", K-subspaces [10], an extension of K-means to the case of subspaces, subspace growing and subspace selection [15], or Expectation Maximization (EM) for mixtures of PCAs [19].", "startOffset": 14, "endOffset": 18}, {"referenceID": 12, "context": ", K-subspaces [10], an extension of K-means to the case of subspaces, subspace growing and subspace selection [15], or Expectation Maximization (EM) for mixtures of PCAs [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": ", K-subspaces [10], an extension of K-means to the case of subspaces, subspace growing and subspace selection [15], or Expectation Maximization (EM) for mixtures of PCAs [19].", "startOffset": 170, "endOffset": 174}, {"referenceID": 18, "context": "Unfortunately, most iterative methods are, in general, very sensitive to initialization; hence, they may not converge to the global optimum [21].", "startOffset": 140, "endOffset": 144}, {"referenceID": 10, "context": "In [13] (see, also, [3]), it is shown that when the subspaces are orthogonal, of equal dimension d, and intersect only at the origin, which implies thatD nd, one can use the SVD of the data to define a similarity matrix from which the segmentation of the data can be obtained using spectral clustering techniques.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [13] (see, also, [3]), it is shown that when the subspaces are orthogonal, of equal dimension d, and intersect only at the origin, which implies thatD nd, one can use the SVD of the data to define a similarity matrix from which the segmentation of the data can be obtained using spectral clustering techniques.", "startOffset": 20, "endOffset": 23}, {"referenceID": 10, "context": "Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28].", "startOffset": 73, "endOffset": 77}, {"referenceID": 24, "context": "Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28].", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28].", "startOffset": 176, "endOffset": 180}, {"referenceID": 19, "context": "Unfortunately, thismethod is sensitive to noise in the data, as shown in [13], [27] where various improvements are proposed, and fails when the subspaces intersect arbitrarily [14], [22], [28].", "startOffset": 182, "endOffset": 186}, {"referenceID": 11, "context": "The latter case has been addressed in an ad hoc fashion by using clustering algorithms such as K-means, spectral clustering, or EM [14], [28] to segment the data and PCA to obtain a basis for each group.", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "The only algebraic approaches that deal with arbitrary intersections are [17], which studies the case of two planes in IR and [24] which studies the case of subspaces of codimension one, i.", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "The only algebraic approaches that deal with arbitrary intersections are [17], which studies the case of two planes in IR and [24] which studies the case of subspaces of codimension one, i.", "startOffset": 126, "endOffset": 130}, {"referenceID": 20, "context": "Our previous work [23] extended this framework to subspaces of unknown and possibly different dimensions under the additional assumption that the number of subspaces is known.", "startOffset": 18, "endOffset": 22}, {"referenceID": 21, "context": "This paper unifies the results of [24] and [23] and extends to the case inwhich both the number anddimensions of the subspaces are unknown.", "startOffset": 34, "endOffset": 38}, {"referenceID": 20, "context": "This paper unifies the results of [24] and [23] and extends to the case inwhich both the number anddimensions of the subspaces are unknown.", "startOffset": 43, "endOffset": 47}, {"referenceID": 6, "context": ";nDx n1 1 x n2 2 x nD D ; \u00f01\u00de where n : IR D ! IRMn\u00f0D\u00de is the Veronese map of degree n [7], also known as the polynomial embedding in machine learning, defined as n : 1\u20442x1; .", "startOffset": 87, "endOffset": 90}, {"referenceID": 8, "context": "An alternative way of selecting the correct linear model (in feature space) for noisy data can be found in [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "Many methods from robust statistics can be deployed to detect and reject the outliers [5], [11].", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "Many methods from robust statistics can be deployed to detect and reject the outliers [5], [11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 4, "context": "Alternatively, one can detect and reject outliers using Random Sample Consensus (RANSAC) [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 0, "context": "The reader may refer to [1] for alternative ways of choosing a projection.", "startOffset": 24, "endOffset": 27}, {"referenceID": 8, "context": "Onemay refer to [11] for a more detailed discussion on selecting the best multiple-subspace model from noisy data, using modelselection criteria such as MML, MDL, AIC, and BIC.", "startOffset": 16, "endOffset": 20}, {"referenceID": 21, "context": "See [24] for further details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "See [10] for further details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "See [19] for further details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 7, "context": "For aLambertian object, it has been shown that the set of all images taken under all lighting conditions forms a cone in the image space,which can bewell approximated by a lowdimensional subspace [10].", "startOffset": 196, "endOffset": 200}, {"referenceID": 22, "context": "We refer the reader to [25],", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "[26], where for the case of arbitrary rotation and translation", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "for these sequences are taken from [14] and consist of point", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "The table also shows results reported in [14] from existing", "startOffset": 41, "endOffset": 45}, {"referenceID": 11, "context": "Segmenting the point correspondences of sequences A (left), B (center), and C (right) in [14] for each pair of consecutive frames by segmenting subspaces in IR.", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "This is because GPCA starts by estimating a collection of polynomials in a linear fashion, thus neglecting the nonlinear constraints among the coefficients of those polynomials, the so-called Brill\u2019s equations [6].", "startOffset": 210, "endOffset": 213}], "year": 2005, "abstractText": "This paper presents an algebro-geometric solution to the problem of segmenting an unknown number of subspaces of unknown and varying dimensions from sample data points. We represent the subspaces with a set of homogeneous polynomials whose degree is the number of subspaces and whose derivatives at a data point give normal vectors to the subspace passing through the point. When the number of subspaces is known, we show that these polynomials can be estimated linearly from data; hence, subspace segmentation is reduced to classifying one point per subspace. We select these points optimally from the data set by minimizing certain distance function, thus dealing automatically with moderate noise in the data. A basis for the complement of each subspace is then recovered by applying standard PCA to the collection of derivatives (normal vectors). Extensions of GPCA that deal with data in a highdimensional space and with an unknown number of subspaces are also presented. Our experiments on low-dimensional data show that GPCA outperforms existing algebraic algorithms based on polynomial factorization and provides a good initialization to iterative techniques such as K-subspaces and Expectation Maximization. We also present applications of GPCA to computer vision problems such as face clustering, temporal video segmentation, and 3Dmotion segmentation from point correspondences in multiple affine views.", "creator": "3B2 Total Publishing System 7.51c/W"}}}