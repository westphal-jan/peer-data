{"id": "1202.5298", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Feb-2012", "title": "Min Max Generalization for Two-stage Deterministic Batch Mode Reinforcement Learning: Relaxation Schemes", "abstract": "We study the minmax optimization problem introduced in [22] for computing policies for batch mode reinforcement learning in a deterministic setting. First, we show that this problem is NP-hard. In the two-stage case, we provide two relaxation schemes. The first relaxation scheme works by dropping some constraints in order to obtain a problem that is solvable in polynomial time. The second relaxation scheme, based on a Lagrangian relaxation where all constraints are dualized, leads to a conic quadratic programming problem. We also theoretically prove and empirically illustrate that both relaxation schemes provide better results than those given in [22].", "histories": [["v1", "Thu, 23 Feb 2012 20:53:18 GMT  (560kb,D)", "http://arxiv.org/abs/1202.5298v1", null], ["v2", "Tue, 30 Oct 2012 16:29:38 GMT  (560kb,D)", "http://arxiv.org/abs/1202.5298v2", null]], "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["raphael fonteneau", "damien ernst", "bernard boigelot", "quentin louveaux"], "accepted": false, "id": "1202.5298"}, "pdf": {"name": "1202.5298.pdf", "metadata": {"source": "CRF", "title": "MIN MAX GENERALIZATION FOR DETERMINISTIC BATCH MODE REINFORCEMENT LEARNING: RELAXATION SCHEMES", "authors": ["R. FONTENEAU", "D. ERNST", "B. BOIGELOT"], "emails": [], "sections": [{"heading": null, "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "6.2. Protocol and Results.", "text": "D (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D) (D). (D). (D). (D). (D). (D). (D D). (D). (D). (D D D). (D). (D). (D D). (D D). (D D D D). (D D D (D D D D D D). (D D D). (D D D D D D D D (D D D D D D D). (D D D D D D D D (D D D D D D D D D D D (D D D D D D D D (D D D D D D D D D D D (D D D D D D D D D D (D D D D D D D D D D D D D D (D D D D D D D D D D D (D D D D D D D D D D (D D D D D D D D D D D D D D D D D D (D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D"}, {"heading": "Appendix A. Proof of Lemma 5.12.", "text": "We assume that x0 6 = x0 and x1 6 = y0, otherwise the problem is trivial. \u2022 Trust region solution (x1, r1, y1): 0, x2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0"}], "references": [{"title": "Adaptive two-stage designs in phase ii clinical trials", "author": ["A. BANERJEE", "A.A. TSIATIS"], "venue": "Statistics in medicine, 25 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "H\u221e-optimal control and related minimax design problems: a dynamic game approach", "author": ["T. BA\u015eAR", "P. BERNHARD"], "venue": "vol. 5, Birkhauser", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Strong duality in nonconvex quadratic optimization with two quadratic constraints", "author": ["A. BECK", "Y.C. ELDAR"], "venue": "SIAM Journal on Optimization, 17 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Robust model predictive control: A survey", "author": ["A. BEMPORAD", "M. MORARI"], "venue": "Robustness in Identification and Control, 245 ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1999}, {"title": "Lectures on Modern Convex Optimization", "author": ["A. BEN-TAL", "A.S. NEMIROVSKI"], "venue": "Siam", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Neuro-Dynamic Programming", "author": ["D.P. BERTSEKAS", "J.N. TSITSIKLIS"], "venue": "Athena Scientific", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1996}, {"title": "Introduction to Stochastic Programming", "author": ["J.R. BIRGE", "F. LOUVEAUX"], "venue": "Springer Verlag", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Numerical optimization", "author": ["JF BONNANS", "J.C. GILBERT", "C. LEMAR\u00c9CHAL", "C. SAGASTIZ\u00c1BAL"], "venue": "theoretical and numerical aspects", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex Optimization", "author": ["S.P. BOYD", "L. VANDENBERGHE"], "venue": "Cambridge Univ Pr", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. BRADTKE", "A.G. BARTO"], "venue": "Machine Learning, 22 ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Reinforcement Learning and Dynamic Programming using Function Approximators", "author": ["L. BUSONIU", "R. BABUSKA", "B. DE SCHUTTER", "D. ERNST"], "venue": "Taylor & Francis CRC Press", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Model Predictive Control", "author": ["E.F. CAMACHO", "C. BORDONS"], "venue": "Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Trust-region Methods", "author": ["A.R. CONN", "N.I.M. GOULD", "P.L. TOINT"], "venue": "vol. 1, Society for Industrial Mathematics", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "A two-stage stochastic programming with recourse model for determining robust planting plans in horticulture", "author": ["K. DARBY-DOWMAN", "S. BARKER", "E. AUDSLEY", "D. PARSONS"], "venue": "Journal of the Operational Research Society, ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Risk-aware decision making and dynamic programming", "author": ["B. DEFOURNY", "D. ERNST", "L. WEHENKEL"], "venue": "Selected for oral presentation at the NIPS-08 Workshop on Model Uncertainty and Risk in Reinforcement Learning, Whistler, Canada, ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Percentile optimization for Markov decision processes with parameter uncertainty", "author": ["E. DELAGE", "S. MANNOR"], "venue": "Operations Research, 58 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Tree-based batch mode reinforcement learning", "author": ["D. ERNST", "P. GEURTS", "L. WEHENKEL"], "venue": "Journal of Machine Learning Research, 6 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Reinforcement learning versus model predictive control: a comparison on a power system problem", "author": ["D. ERNST", "M. GLAVIC", "F. CAPITANESCU", "L. WEHENKEL"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics, 39 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Contributions to Batch Mode Reinforcement Learning", "author": ["R. FONTENEAU"], "venue": "PhD thesis, University of Li\u00e8ge", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Inferring bounds on the performance of a control policy from a sample of trajectories", "author": ["R. FONTENEAU", "S. MURPHY", "L. WEHENKEL", "D. ERNST"], "venue": "Proceedings of the 2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (IEEE ADPRL 09), Nashville, TN, USA", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "A cautious approach to generalization in reinforcement learning", "author": ["R. FONTENEAU", "S.A. MURPHY", "L. WEHENKEL", "D. ERNST"], "venue": "Proceedings of the Second International Conference on Agents and Artificial Intelligence ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards min max generalization in reinforcement learning", "author": ["R. FONTENEAU", "S.A. MURPHY", "L. WEHENKEL", "D. ERNST"], "venue": "Agents and Artificial Intelligence: International Conference, ICAART 2010, Valencia, Spain, January 2010, Revised Selected Papers. Series: Communications in Computer and Information Science (CCIS), vol. 129, Springer, Heidelberg", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Stochastic Two-stage Programming", "author": ["K. FRAUENDORFER"], "venue": "Springer", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1992}, {"title": "Robust Control and Model Uncertainty", "author": ["L.P. HANSEN", "T.J. SARGENT"], "venue": "American Economic Review, ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Convex Analysis and Minimization Algorithms: Fundamentals", "author": ["J.B. HIRIART-URRUTY", "C. LEMAR\u00c9CHAL"], "venue": "vol. 305, Springer-Verlag", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "Theory of Financial Decision Making", "author": ["J.E. INGERSOLL"], "venue": "Rowman and Littlefield Publishers, Inc.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1987}, {"title": "Minimax real-time heuristic search", "author": ["S. KOENIG"], "venue": "Artificial Intelligence, 129 ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2001}, {"title": "Least-squares policy iteration", "author": ["M.G. LAGOUDAKIS", "R. PARR"], "venue": "Jounal of Machine Learning Research, 4 ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["M.L. LITTMAN"], "venue": "Proceedings of the Eleventh International Conference on Machine Learning ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1994}, {"title": "Optimal two-stage group-sequential designs", "author": ["Y. LOKHNYGINA", "A.A. TSIATIS"], "venue": "Journal of Statistical Planning and Inference, 138 ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Estimation of survival distributions of treatment policies in two-stage randomization designs in clinical trials", "author": ["J.K. LUNCEFORD", "M. DAVIDIAN", "A.A. TSIATIS"], "venue": "Biometrics, ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Bias and variance in value function estimation", "author": ["S. MANNOR", "D. SIMESTER", "P. SUN", "J.N. TSITSIKLIS"], "venue": "Proceedings of the Twenty-first International Conference on Machine Learning ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Optimal dynamic treatment regimes", "author": ["S.A. MURPHY"], "venue": "Journal of the Royal Statistical Society, Series B, 65(2) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2003}, {"title": "An experimental design for the development of adaptive treatment strategies", "author": ["S.A. MURPHY"], "venue": "Statistics in Medicine, 24 ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust stochastic approximation approach to  Min Max Generalization for Deterministic Batch Mode Reinforcement Learning: Relaxation Schemes 33 stochastic programming", "author": ["A. NEMIROVSKI", "A. JUDITSKY", "G. LAN", "A. SHAPIRO"], "venue": "SIAM Journal on Optimization, 19 ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Interior point polynomial methods in convex programming", "author": ["Y. NESTEROV", "A. NEMIROVSKI"], "venue": "Studies in applied mathematics, 13 ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1994}, {"title": "Kernel-based reinforcement learning", "author": ["D. ORMONEIT", "S. SEN"], "venue": "Machine Learning, 49 ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2002}, {"title": "A framework for computing bounds for the return of a policy", "author": ["C. PADURARU", "D. PRECUP", "J. PINEAU"], "venue": "Ninth European Workshop on Reinforcement Learning (EWRL9)", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Computational Complexity", "author": ["C.H. PAPADIMITRIOU"], "venue": "John Wiley and Sons Ltd.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2003}, {"title": "Performance guarantees for individualized treatment rules", "author": ["M. QIAN", "S.A. MURPHY"], "venue": "Tech. Report 498, Department of Statistics, University of Michigan", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2009}, {"title": "Neural fitted Q iteration - first experiences with a data efficient neural reinforcement learning method", "author": ["M. RIEDMILLER"], "venue": "Proceedings of the Sixteenth European Conference on Machine Learning ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2005}, {"title": "Minimax search and reinforcement learning for adversarial tetris", "author": ["M. ROVATOUS", "M. LAGOUDAKIS"], "venue": "Proceedings of the 6th Hellenic Conference on Artificial Intelligence (SETN\u201910), Athens, Greece", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2010}, {"title": "Min-max feedback model predictive control for constrained linear systems", "author": ["P. SCOKAERT", "D. MAYNE"], "venue": "IEEE Transactions on Automatic Control, 43 ", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1998}, {"title": "A dynamic programming approach to adjustable robust optimization", "author": ["A. SHAPIRO"], "venue": "Operations Research Letters, 39 ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2011}, {"title": "Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones", "author": ["J.F. STURM"], "venue": "Optimization methods and software,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1999}, {"title": "Reinforcement Learning", "author": ["R.S. SUTTON", "A.G. BARTO"], "venue": "MIT Press", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1998}, {"title": "Optimal estimator for the survival distribution and related quantities for treatment policies in two-stage randomization designs in clinical trials", "author": ["A.S. WAHED", "A.A. TSIATIS"], "venue": "Biometrics, 60 ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 21, "context": "We study the min max optimization problem introduced in [22] for computing policies for batch mode reinforcement learning in a deterministic setting.", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": "We also theoretically prove and empirically illustrate that both relaxation schemes provide better results than those given in [22].", "startOffset": 127, "endOffset": 131}, {"referenceID": 45, "context": "Research in Reinforcement Learning (RL) [48] aims at designing computational agents able to learn by themselves how to interact with their environment to maximize a numerical reward signal.", "startOffset": 40, "endOffset": 44}, {"referenceID": 25, "context": "The techniques developed in this field have appealed researchers trying to solve sequential decision making problems in many fields such as Finance [26], Medicine [34, 35] or Engineering [42].", "startOffset": 148, "endOffset": 152}, {"referenceID": 32, "context": "The techniques developed in this field have appealed researchers trying to solve sequential decision making problems in many fields such as Finance [26], Medicine [34, 35] or Engineering [42].", "startOffset": 163, "endOffset": 171}, {"referenceID": 33, "context": "The techniques developed in this field have appealed researchers trying to solve sequential decision making problems in many fields such as Finance [26], Medicine [34, 35] or Engineering [42].", "startOffset": 163, "endOffset": 171}, {"referenceID": 40, "context": "The techniques developed in this field have appealed researchers trying to solve sequential decision making problems in many fields such as Finance [26], Medicine [34, 35] or Engineering [42].", "startOffset": 187, "endOffset": 191}, {"referenceID": 9, "context": "Since the end of the nineties, several researchers have focused on the resolution of a subproblem of RL: computing a high-performance policy when the only information available on the environment is contained in a batch collection of trajectories of the agent [10, 17, 28, 38, 42, 19].", "startOffset": 260, "endOffset": 284}, {"referenceID": 16, "context": "Since the end of the nineties, several researchers have focused on the resolution of a subproblem of RL: computing a high-performance policy when the only information available on the environment is contained in a batch collection of trajectories of the agent [10, 17, 28, 38, 42, 19].", "startOffset": 260, "endOffset": 284}, {"referenceID": 27, "context": "Since the end of the nineties, several researchers have focused on the resolution of a subproblem of RL: computing a high-performance policy when the only information available on the environment is contained in a batch collection of trajectories of the agent [10, 17, 28, 38, 42, 19].", "startOffset": 260, "endOffset": 284}, {"referenceID": 36, "context": "Since the end of the nineties, several researchers have focused on the resolution of a subproblem of RL: computing a high-performance policy when the only information available on the environment is contained in a batch collection of trajectories of the agent [10, 17, 28, 38, 42, 19].", "startOffset": 260, "endOffset": 284}, {"referenceID": 40, "context": "Since the end of the nineties, several researchers have focused on the resolution of a subproblem of RL: computing a high-performance policy when the only information available on the environment is contained in a batch collection of trajectories of the agent [10, 17, 28, 38, 42, 19].", "startOffset": 260, "endOffset": 284}, {"referenceID": 18, "context": "Since the end of the nineties, several researchers have focused on the resolution of a subproblem of RL: computing a high-performance policy when the only information available on the environment is contained in a batch collection of trajectories of the agent [10, 17, 28, 38, 42, 19].", "startOffset": 260, "endOffset": 284}, {"referenceID": 5, "context": "The dominant approach for generalizing this information is to combine BMRL algorithms with function approximators [6, 28, 17, 11].", "startOffset": 114, "endOffset": 129}, {"referenceID": 27, "context": "The dominant approach for generalizing this information is to combine BMRL algorithms with function approximators [6, 28, 17, 11].", "startOffset": 114, "endOffset": 129}, {"referenceID": 16, "context": "The dominant approach for generalizing this information is to combine BMRL algorithms with function approximators [6, 28, 17, 11].", "startOffset": 114, "endOffset": 129}, {"referenceID": 10, "context": "The dominant approach for generalizing this information is to combine BMRL algorithms with function approximators [6, 28, 17, 11].", "startOffset": 114, "endOffset": 129}, {"referenceID": 21, "context": "To overcome this problem, [22] propose a min max-type strategy for generalizing in deterministic, Lipschitz continuous environments with continuous state spaces, finite action spaces, and finite time-horizon.", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "This lower bound is derived from their previous work [20, 21] and has a tightness that depends on the sample dispersion.", "startOffset": 53, "endOffset": 61}, {"referenceID": 20, "context": "This lower bound is derived from their previous work [20, 21] and has a tightness that depends on the sample dispersion.", "startOffset": 53, "endOffset": 61}, {"referenceID": 21, "context": "In this paper, we propose to further investigate the min max generalization optimization problem that was initially proposed in [22].", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": "This results into a well known configuration called the trust-region subproblem [13].", "startOffset": 80, "endOffset": 84}, {"referenceID": 21, "context": "Trust-region better than [22]", "startOffset": 25, "endOffset": 29}, {"referenceID": 21, "context": "Lagrangian relaxation better than Trust-region thus better than [22] Convergence when the sample dispersion goes to 0 NP-hard", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "In stochastic frameworks, min max approaches are often successful for deriving robust solutions with respect to uncertainties in the (parametric) representation of the probability distributions associated with the environment [16].", "startOffset": 226, "endOffset": 230}, {"referenceID": 28, "context": "[29, 43].", "startOffset": 0, "endOffset": 8}, {"referenceID": 41, "context": "[29, 43].", "startOffset": 0, "endOffset": 8}, {"referenceID": 26, "context": "They have also received some attention for solving partially observable Markov decision processes [30, 27].", "startOffset": 98, "endOffset": 106}, {"referenceID": 21, "context": "The min max approach towards generalization, originally introduced in [22], implicitly relies on a methodology for computing lower bounds on the worst possible return (considering any compatible environment) in a deterministic setting with a mostly unknown actual environment.", "startOffset": 70, "endOffset": 74}, {"referenceID": 31, "context": "In this respect, it is related to other approaches that aim at computing performance guarantees on the returns of inferred policies [33, 41, 39].", "startOffset": 132, "endOffset": 144}, {"referenceID": 39, "context": "In this respect, it is related to other approaches that aim at computing performance guarantees on the returns of inferred policies [33, 41, 39].", "startOffset": 132, "endOffset": 144}, {"referenceID": 37, "context": "In this respect, it is related to other approaches that aim at computing performance guarantees on the returns of inferred policies [33, 41, 39].", "startOffset": 132, "endOffset": 144}, {"referenceID": 23, "context": "This includes Robust Control theory [24] with H\u221e methods [2], but also Model Predictive Control (MPC) theory - where usually the environment is supposed to be fully known [12, 18] - for which min max approaches have been used to determine an optimal sequence of actions with respect to the \u201cworst case\u201d disturbance sequence occurring [44, 4].", "startOffset": 36, "endOffset": 40}, {"referenceID": 1, "context": "This includes Robust Control theory [24] with H\u221e methods [2], but also Model Predictive Control (MPC) theory - where usually the environment is supposed to be fully known [12, 18] - for which min max approaches have been used to determine an optimal sequence of actions with respect to the \u201cworst case\u201d disturbance sequence occurring [44, 4].", "startOffset": 57, "endOffset": 60}, {"referenceID": 11, "context": "This includes Robust Control theory [24] with H\u221e methods [2], but also Model Predictive Control (MPC) theory - where usually the environment is supposed to be fully known [12, 18] - for which min max approaches have been used to determine an optimal sequence of actions with respect to the \u201cworst case\u201d disturbance sequence occurring [44, 4].", "startOffset": 171, "endOffset": 179}, {"referenceID": 17, "context": "This includes Robust Control theory [24] with H\u221e methods [2], but also Model Predictive Control (MPC) theory - where usually the environment is supposed to be fully known [12, 18] - for which min max approaches have been used to determine an optimal sequence of actions with respect to the \u201cworst case\u201d disturbance sequence occurring [44, 4].", "startOffset": 171, "endOffset": 179}, {"referenceID": 42, "context": "This includes Robust Control theory [24] with H\u221e methods [2], but also Model Predictive Control (MPC) theory - where usually the environment is supposed to be fully known [12, 18] - for which min max approaches have been used to determine an optimal sequence of actions with respect to the \u201cworst case\u201d disturbance sequence occurring [44, 4].", "startOffset": 334, "endOffset": 341}, {"referenceID": 3, "context": "This includes Robust Control theory [24] with H\u221e methods [2], but also Model Predictive Control (MPC) theory - where usually the environment is supposed to be fully known [12, 18] - for which min max approaches have been used to determine an optimal sequence of actions with respect to the \u201cworst case\u201d disturbance sequence occurring [44, 4].", "startOffset": 334, "endOffset": 341}, {"referenceID": 6, "context": "Finally, there is a broad stream of works in the field of Stochastic Programming [7] that", "startOffset": 81, "endOffset": 84}, {"referenceID": 14, "context": "have addressed the problem of safely planning under uncertainties, mainly known as \u201crobust stochastic programming\u201d or \u201crisk-averse stochastic programming\u201d [15, 45, 46, 36].", "startOffset": 155, "endOffset": 171}, {"referenceID": 43, "context": "have addressed the problem of safely planning under uncertainties, mainly known as \u201crobust stochastic programming\u201d or \u201crisk-averse stochastic programming\u201d [15, 45, 46, 36].", "startOffset": 155, "endOffset": 171}, {"referenceID": 34, "context": "have addressed the problem of safely planning under uncertainties, mainly known as \u201crobust stochastic programming\u201d or \u201crisk-averse stochastic programming\u201d [15, 45, 46, 36].", "startOffset": 155, "endOffset": 171}, {"referenceID": 22, "context": "In this field, the two-stage case has also been particularly well-studied [23, 14].", "startOffset": 74, "endOffset": 82}, {"referenceID": 13, "context": "In this field, the two-stage case has also been particularly well-studied [23, 14].", "startOffset": 74, "endOffset": 82}, {"referenceID": 21, "context": "The formalization was originally proposed in [22].", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "As shown in [22], this worst possible return can be computed by solving a finite-dimensional optimization problem over X T\u22121 \u00d7 R .", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "Many works in optimal sequential decision making have considered the two-stage case [23, 14], which relates to many applications, such as for instance medical applications where one wants to infer \u201csafe\u201d clinical decision rules from batch collections of clinical data [1, 31, 32, 49].", "startOffset": 84, "endOffset": 92}, {"referenceID": 13, "context": "Many works in optimal sequential decision making have considered the two-stage case [23, 14], which relates to many applications, such as for instance medical applications where one wants to infer \u201csafe\u201d clinical decision rules from batch collections of clinical data [1, 31, 32, 49].", "startOffset": 84, "endOffset": 92}, {"referenceID": 0, "context": "Many works in optimal sequential decision making have considered the two-stage case [23, 14], which relates to many applications, such as for instance medical applications where one wants to infer \u201csafe\u201d clinical decision rules from batch collections of clinical data [1, 31, 32, 49].", "startOffset": 268, "endOffset": 283}, {"referenceID": 29, "context": "Many works in optimal sequential decision making have considered the two-stage case [23, 14], which relates to many applications, such as for instance medical applications where one wants to infer \u201csafe\u201d clinical decision rules from batch collections of clinical data [1, 31, 32, 49].", "startOffset": 268, "endOffset": 283}, {"referenceID": 30, "context": "Many works in optimal sequential decision making have considered the two-stage case [23, 14], which relates to many applications, such as for instance medical applications where one wants to infer \u201csafe\u201d clinical decision rules from batch collections of clinical data [1, 31, 32, 49].", "startOffset": 268, "endOffset": 283}, {"referenceID": 46, "context": "Many works in optimal sequential decision making have considered the two-stage case [23, 14], which relates to many applications, such as for instance medical applications where one wants to infer \u201csafe\u201d clinical decision rules from batch collections of clinical data [1, 31, 32, 49].", "startOffset": 268, "endOffset": 283}, {"referenceID": 38, "context": "To prove it, we will do a reduction from the {0, 1}\u2212programming feasibility problem [40].", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "We show that this scheme provides bounds that are greater or equal to the CGRL bound introduced in [22].", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "We also prove that this relaxation scheme always gives better bounds than the first relaxation scheme mentioned above, and consequently, better bounds than [22].", "startOffset": 156, "endOffset": 160}, {"referenceID": 12, "context": "This problem is referred to in the literature as the trust-region subproblem [13].", "startOffset": 77, "endOffset": 81}, {"referenceID": 24, "context": "5) Observe that the optimal value of (P \u2032\u2032(u0,u1) LD ) is known to provide a lower bound on the optimal value of (P \u2032\u2032(u0,u1) 2 ) [25].", "startOffset": 130, "endOffset": 134}, {"referenceID": 4, "context": ", \u03bcn(u1) as well as a fractionalquadratic function ([5]), i.", "startOffset": 52, "endOffset": 55}, {"referenceID": 4, "context": "This type of problem is known as a rotated quadratic conic problem and can be formulated as a conic quadratic optimization problem ([5]) that can be solved in polynomial time using interior point methods [5, 37, 9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "This type of problem is known as a rotated quadratic conic problem and can be formulated as a conic quadratic optimization problem ([5]) that can be solved in polynomial time using interior point methods [5, 37, 9].", "startOffset": 204, "endOffset": 214}, {"referenceID": 35, "context": "This type of problem is known as a rotated quadratic conic problem and can be formulated as a conic quadratic optimization problem ([5]) that can be solved in polynomial time using interior point methods [5, 37, 9].", "startOffset": 204, "endOffset": 214}, {"referenceID": 8, "context": "This type of problem is known as a rotated quadratic conic problem and can be formulated as a conic quadratic optimization problem ([5]) that can be solved in polynomial time using interior point methods [5, 37, 9].", "startOffset": 204, "endOffset": 214}, {"referenceID": 20, "context": "The CGRL algorithm proposed in [21, 22] for addressing the min max problem uses the procedure described in [20] for computing a lower bound on the return of a policy given a sample of trajectories.", "startOffset": 31, "endOffset": 39}, {"referenceID": 21, "context": "The CGRL algorithm proposed in [21, 22] for addressing the min max problem uses the procedure described in [20] for computing a lower bound on the return of a policy given a sample of trajectories.", "startOffset": 31, "endOffset": 39}, {"referenceID": 19, "context": "The CGRL algorithm proposed in [21, 22] for addressing the min max problem uses the procedure described in [20] for computing a lower bound on the return of a policy given a sample of trajectories.", "startOffset": 107, "endOffset": 111}, {"referenceID": 2, "context": "Proofs of this lemma can be found in [3] and [8], but we also provide in Appendix A a proof in our particular case.", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "Proofs of this lemma can be found in [3] and [8], but we also provide in Appendix A a proof in our particular case.", "startOffset": 45, "endOffset": 48}, {"referenceID": 21, "context": "is derived from the formalization of the min max generalization problem introduced in [22].", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "The proof for the case where B01(F) = B01 CGRL (F) is given in [21].", "startOffset": 63, "endOffset": 67}, {"referenceID": 44, "context": "In all our experiments, the computation of the Lagrangian relaxations, which requires to solve a conic-quadratic program, are done using SeDuMi [47].", "startOffset": 144, "endOffset": 148}, {"referenceID": 0, "context": ", 15, we generate a sample of transitions Fci using a grid over [0, 1] \u00d7 U , as follows: \u2200u \u2208 U , F (u) ci = {([ i1 i ; i2 i ] , u, \u03c1 ([ i1 i ; i2 i ] , u ) , f ([ i1 i ; i2 i ] , u )) \u2223\u2223\u2223\u2223(i1, i2) \u2208 {1, .", "startOffset": 64, "endOffset": 70}, {"referenceID": 0, "context": ",Fci,100 using a uniform probability distribution over the space [0, 1] \u00d7 U .", "startOffset": 65, "endOffset": 71}, {"referenceID": 24, "context": "Since the Trust-region solution to (P \u2032\u2032(u0,u1) TR (k0, k1)) is optimal, and by property of the Lagrangian relaxation [25], one has:", "startOffset": 118, "endOffset": 122}], "year": 2017, "abstractText": "We study the min max optimization problem introduced in [22] for computing policies for batch mode reinforcement learning in a deterministic setting. First, we show that this problem is NP-hard. In the twostage case, we provide two relaxation schemes. The first relaxation scheme works by dropping some constraints in order to obtain a problem that is solvable in polynomial time. The second relaxation scheme, based on a Lagrangian relaxation where all constraints are dualized, leads to a conic quadratic programming problem. We also theoretically prove and empirically illustrate that both relaxation schemes provide better results than those given in [22].", "creator": "LaTeX with hyperref package"}}}