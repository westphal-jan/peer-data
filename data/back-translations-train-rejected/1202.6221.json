{"id": "1202.6221", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2012", "title": "Confusion Matrix Stability Bounds for Multiclass Classification", "abstract": "In this paper, we provide new theoretical results on the generalization properties of learning algorithms for multiclass classification problems. The originality of our work is that we propose to use the confusion matrix of a classifier as a measure of its quality; our contribution is in the line of work which attempts to set up and study the statistical properties of new evaluation measures such as, e.g. ROC curves. In the confusion-based learning framework we propose, we claim that a targetted objective is to minimize the size of the confusion matrix C, measured through its operator norm ||C||. We derive generalization bounds on the (size of the) confusion matrix in an extended framework of uniform stability, adapted to the case of matrix valued loss. Pivotal to our study is a very recent matrix concentration inequality that generalizes McDiarmid's inequality. As an illustration of the relevance of our theoretical results, we show how two SVM learning procedures can be proved to be confusion-friendly. To the best of our knowledge, the present paper is the first that focuses on the confusion matrix from a theoretical point of view.", "histories": [["v1", "Tue, 28 Feb 2012 14:03:11 GMT  (38kb)", "http://arxiv.org/abs/1202.6221v1", null], ["v2", "Thu, 24 May 2012 19:27:24 GMT  (50kb)", "http://arxiv.org/abs/1202.6221v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pierre machart", "liva ralaivola"], "accepted": false, "id": "1202.6221"}, "pdf": {"name": "1202.6221.pdf", "metadata": {"source": "CRF", "title": "Confusion Matrix Stability Bounds for Multiclass Classification", "authors": ["Pierre Machart", "Liva Ralaivola"], "emails": ["firstname.name@lif.univ-mrs.fr"], "sections": [{"heading": null, "text": "ar Xiv: 120 2.62 21v1 [cs.LG] 2 8Fe b20 12In this paper, we provide new theoretical results on the generalization properties of learning algorithms for multi-class classification problems. The originality of our work is that we propose to use the confusion matrix of a classifier as a measure of its quality; our contribution lies in the line in which we attempt to establish and investigate the statistical properties of new assessment measures such as ROC curves. In the confusion framework that we propose, we claim that one objective is to minimize the size of the confusion matrix C, measured by its operator standard. We deduce generalization limits to the (size of) confusion matrix in an extended framework of uniform stability adapted to the case of matrix loss. The pivotal point of our study is a very recent matrix concentration inequality that generalizes McDiarmids inequality."}, {"heading": "1 Introduction", "text": "The question of whether statistically relevant methods are available to learn reliable predictors is of particular interest nowadays, given the widespread demand for such predictors in the fields of information gathering, web mining, bioinformatics, or neuroscience. However, the literature on multiclass learning is not as extensive as that on binary classification, while this problem raises questions from an algorithmic, theoretical, and practical viewpoint. One of the most prominent questions is that on assessing the quality of a multiclass predictor. Here, we develop our results with the idea that the confusion matrix is a performance measure that should be investigated, as it provides a finer idea of the properties of a classifier than the mere misclassification rate. More specifically, building on very recent matrix-based predictors, the concentration inequalities provided by Tropp (2011) - sometimes referred to as non-commentary concentration inequalities - are shown as a stability awareness framework for learning."}, {"heading": "2 Confusion Matrix Awareness", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notation", "text": "As already mentioned, we focus on the problem of multi-class classification. The input space is q = q = q and the target space is Y = {1,.., Q}. The training sequence Z = {Zi = (Xi, Yi)} mi = 1 consists of m identical and independently of each other random pairs Zi = (Xi, Yi) distributed according to an unknown (but fixed) distribution D over X \u00b7 Y. The realization of input data is zi = (xi, yi) and z, x and y refer to the realization of the corresponding designations Y = {Yi} mi = 1, we can write Z = {X, Y}. The realization of Zi = (Xi, Yi) and Zy refer to the realization of the corresponding designations of random variables that are y. For a sequence y, \u00b7 \u00b7 \u00b7 \u00b7, ym} of m designations, mq (y), or simply mq."}, {"heading": "2.2 Confusion Matrix", "text": "It is not that we can apply the quality of a predictor H to its predictor class H (h) only if this could lead to erroneous conclusions regarding the quality of learning if the quality of learning is unbalanced in many situations. (h) It is important not to measure the quality of learning. (h) It can only lead to erroneous conclusions regarding the quality of learning. (h) This can lead to erroneous conclusions regarding the quality of learning. (h) In many situations where the quality of learning is unbalanced, it is important not to measure the quality of learning. (h) This can lead to erroneous conclusions regarding the quality of learning. (h) In many situations where the quality of learning is unbalanced, it is important not to measure the quality of learning. (h)"}, {"heading": "3 Deriving Stability Bounds on the Confusion Matrix", "text": "One of the most important questions in learning theory is to assess the real performance of a learning system. The usual approach is to examine how empirical metrics converge with their expectations. In traditional environments, it often boils down to setting boundaries that describe how empirical risk relates to the expected. In this paper, we show that similar techniques can be used to limit the operator norm of the confusion matrix."}, {"heading": "3.1 Stability", "text": "According to the early work of Vapnik (1982), risk has traditionally been estimated on the basis of its empirical measure and a measure of the complexity of the hypotheses class such as Vapnik-Chervonenkis (VC-dim), the fat-shaking dimension or the Rademacher complexity. In the last decade, a new and successful approach has emerged that relies on algorithmic stability to set some new boundaries. One of the highlights of this approach is the focus on some properties of the existing learning algorithm rather than the characterization of the hypotheses class. Roughly speaking, this enables the knowledge of how a particular algorithm actually explores the hypotheses space, which often leads to narrower boundaries. The main results of Bousquet and Elisseeff (2002) were achieved using the following definition of uniform stability: Definition 1 (Uniform of Stability Bousquet and Elisseeff (2002)."}, {"heading": "3.2 Non-Commutative McDiarmid", "text": "In Bousquet and Elisseeff (2002), the authors use some well-known concentration inequalities to derive boundaries from it. Specifically, they use a variation of Azuma's inequality based on McDiarmid McDiarmid (1989) to describe how a scalar function of independent random variables (the elements of our training set) normally focuses on its mean, with the variance depending on how the change of one of the random variables affects the value of the function. Some recent work Tropp (2011) extends McDiarmid's inequality to the matrix setting. For the sake of self-limitation, let us remember this non-commutative limit. Theorem 1 (Matrix-related difference (drop (2011), episode 7.5)). Let S and Si be defined as above and H be a function, the m-variables to a self-adrix adrix quantity Q.Consider a sequence {we} set Qi (Qi) values for Qi (Qi)."}, {"heading": "3.3 Stability Bound", "text": "Theorem 2 (Confusion bound) Let A (Confusion bound) be a learning algorithm. Suppose that all eligible loss functions take values in the range [0; M] into account. Let y (Confusion bound) be a fixed sequence of labels.If A has only confusion stability \u03b2 with respect to all loss matrices Lq (for q-Y), then the following applies with probability 1 \u2212 \u03b4 over the random draw of X-Y (A, X) \u2212 Cs (y) (A)."}, {"heading": "3.4 Proof of Theorem 2", "text": "Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z, Z,"}, {"heading": "4 Analysis of existing algorithms", "text": "Now that the main result on the stability limit has been determined, we will examine how some existing multi-class algorithms exhibit some stability characteristics and thus fall within the scope of our analysis. Specifically, we will analyze two well-known models of multi-class support vector machines and show that they favor small confusion errors. But, first, we will examine the more general stability of multi-class algorithms by using regularization in Reproductive Kernel Hilbert Spaces (RKHS)."}, {"heading": "4.1 Hilbert Space Regularized Algorithms", "text": "Many well-known and widely used algorithms have a minimization of a regulated objective function q = q = q = q = Tikhonov and Arsenin (1977). In the context of the Kernel MachinesChristianini and Shawe-Taylor (2000), this regularization mechanism can take the following form: B = q = q = q = q = 2k.where k: X \u00b7 X \u2192 R denotes the kernel associated with the RKHS H. In order to investigate the stability properties of algorithms, it minimizes a data-compatible term punished by such regulators, we must introduce a minor definition in our multi-class environment, which is an adjunct to Definition 19 of Bousquet and Elisseeff (2002). Definition 3. A loss function defined on HQ \u00d7 Y is in any case multi-admissible if it is permissible in relation to all of its initial arguments."}, {"heading": "4.2 Lee, Lin and Wahba model", "text": "One of the best-known and best-studied models for multi-class classification, in the context of QM, was proposed by Lee et al. (2004). In this paper, the authors propose the use of the following (punished) functional functions."}, {"heading": "4.3 Weston and Watkins model", "text": "One of the oldest models when it comes to multi-class SVM is based on Weston and Watkins (1998). They consider the following functions. (h, x, y) = \u2211 q 6 = y (1 \u2212 hy (x) + hq (x))) + The AWW algorithm minimizes the following functions: J (h) = 1mm \u00b2 k = 1 \u00b2 q 6 = yk (1 \u2212 hy (x) + hq (x) + + \u03bb Q \u00b2 < p = 1 \u00b2 hq \u2212 hp \u00b2 2, This time we will introduce the functions hpq = hp \u2212 hq. We can then classify J (h) asJ (h) = q \u00b2 q < n: yn = q1mq q q q (h, xn, q) + \u043aQ \u00b2 \u00b2 s as stable."}, {"heading": "5 Discussion and Conclusion", "text": "In this paper, we have proposed a new framework, namely algorithmic confusion stability, along with new boundaries for characterizing the generalization properties of multi-class learning algorithms. The crux of our study is to envision the confusion matrix as a performance measure that differs from commonly encountered approaches that examine the generalization properties of scalar-rated performance (such as accuracy).Some of the questions raised by this paper are the following: Is it possible to derive confusion stable algorithms that are precisely designed to control the norm of their confusion matrix? Are there algorithms other than those analyzed here that can be examined in our new framework? In particular, is this the case for k-nearest neighbors whose generalization analysis is possible thanks to classical stability arguments?"}], "references": [{"title": "Stability and generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Elisseeff,? \\Q2002\\E", "shortCiteRegEx": "Bousquet and Elisseeff", "year": 2002}, {"title": "An Introduction to Support Vector Machines: and Other Kernel-Based Learning Methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "Cristianini and Shawe.Taylor,? \\Q2000\\E", "shortCiteRegEx": "Cristianini and Shawe.Taylor", "year": 2000}, {"title": "Multicategory support vector machines", "author": ["Y. Lee", "Y. Lin", "G. Wahba"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Lee et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2004}, {"title": "On the method of bounded differences", "author": ["C. McDiarmid"], "venue": "In Surveys in Combinatorics,", "citeRegEx": "McDiarmid,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid", "year": 1989}, {"title": "Solutions of Ill-Posed Problems. Winston", "author": ["A.N. Tikhonov", "V.Y. Arsenin"], "venue": null, "citeRegEx": "Tikhonov and Arsenin,? \\Q1977\\E", "shortCiteRegEx": "Tikhonov and Arsenin", "year": 1977}, {"title": "User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics", "author": ["J.A. Tropp"], "venue": null, "citeRegEx": "Tropp,? \\Q2011\\E", "shortCiteRegEx": "Tropp", "year": 2011}, {"title": "Estimation of Dependences Based on Empirical Data", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "Vapnik,? \\Q1982\\E", "shortCiteRegEx": "Vapnik", "year": 1982}, {"title": "Multi-class support vector machines", "author": ["J. Weston", "C. Watkins"], "venue": null, "citeRegEx": "Weston and Watkins,? \\Q1998\\E", "shortCiteRegEx": "Weston and Watkins", "year": 1998}], "referenceMentions": [{"referenceID": 4, "context": "More precisely, building on very recent matrix-based concentration inequalities provided by Tropp (2011) \u2014sometimes referred to as noncommutative concentration inequalities\u2014 we establish a stability based framework for confusion-aware learning algorithm.", "startOffset": 92, "endOffset": 105}, {"referenceID": 0, "context": "In a sense, our framework and our results extend those of Bousquet and Elisseeff (2002), which are designed for scalar loss functions.", "startOffset": 58, "endOffset": 88}, {"referenceID": 5, "context": "1 Stability Following the early work of Vapnik (1982), the risk has traditionally been estimated through its empirical measure and a measure of the complexity of the hypothesis class such as Vapnik-Chervonenkis (VC-dim), fat-shattering dimension or Rademacher complexity.", "startOffset": 40, "endOffset": 54}, {"referenceID": 0, "context": "The main results in Bousquet and Elisseeff (2002) were obtained using the following definition of uniform stability.", "startOffset": 20, "endOffset": 50}, {"referenceID": 0, "context": "The main results in Bousquet and Elisseeff (2002) were obtained using the following definition of uniform stability. Definition 1 (Uniform stability Bousquet and Elisseeff (2002)).", "startOffset": 20, "endOffset": 179}, {"referenceID": 0, "context": "2 Non-Commutative McDiarmid In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds.", "startOffset": 31, "endOffset": 61}, {"referenceID": 0, "context": "2 Non-Commutative McDiarmid In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds. More specifically, they use a variation of Azuma\u2019s inequality, due to McDiarmid McDiarmid (1989). It describes how a scalar function of independent random variables (the elements of our training set) normally concentrates around its mean, with variance depending on how changing one of the random variables impacts the value of the function.", "startOffset": 31, "endOffset": 244}, {"referenceID": 0, "context": "2 Non-Commutative McDiarmid In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds. More specifically, they use a variation of Azuma\u2019s inequality, due to McDiarmid McDiarmid (1989). It describes how a scalar function of independent random variables (the elements of our training set) normally concentrates around its mean, with variance depending on how changing one of the random variables impacts the value of the function. Some more recent work Tropp (2011) extends McDiarmid\u2019s inequality to the matrix setting.", "startOffset": 31, "endOffset": 524}, {"referenceID": 0, "context": "2 Non-Commutative McDiarmid In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds. More specifically, they use a variation of Azuma\u2019s inequality, due to McDiarmid McDiarmid (1989). It describes how a scalar function of independent random variables (the elements of our training set) normally concentrates around its mean, with variance depending on how changing one of the random variables impacts the value of the function. Some more recent work Tropp (2011) extends McDiarmid\u2019s inequality to the matrix setting. For the sake of self-containedness, we recall this non-commutative bound. Theorem 1 (Matrix bounded difference (Tropp (2011), corollary 7.", "startOffset": 31, "endOffset": 703}, {"referenceID": 5, "context": "The choice of such a dilation is directly motivated by the following property, recalled in Tropp (2011). Lemma 1.", "startOffset": 91, "endOffset": 104}, {"referenceID": 3, "context": "1 Hilbert Space Regularized Algorithms Many well-known and widely-used algorithms feature a minimization of a regularized objective functionTikhonov and Arsenin (1977). In the context of kernel machinesCristianini and Shawe-Taylor (2000), this regularizer \u03a9(h) may take the following form:", "startOffset": 140, "endOffset": 168}, {"referenceID": 1, "context": "In the context of kernel machinesCristianini and Shawe-Taylor (2000), this regularizer \u03a9(h) may take the following form:", "startOffset": 33, "endOffset": 69}, {"referenceID": 0, "context": "In order to study the stability properties of algorithms, minimizing a data-fitting term, penalized by such regularizers, in our multi-class setting, we need to introduce a minor definition that is an addition to definition 19 of Bousquet and Elisseeff (2002). Definition 3.", "startOffset": 230, "endOffset": 260}, {"referenceID": 0, "context": "Roughly, the idea is to exploit definition 3 in order to apply the theorem 22 of Bousquet and Elisseeff (2002) for each loss lq.", "startOffset": 81, "endOffset": 111}, {"referenceID": 2, "context": "2 Lee, Lin and Wahba model One of the most well-known and well-studied model for multi-class classification, in the context of SVM, was proposed by Lee et al. (2004). In this work, the authors suggest the use of the following loss function.", "startOffset": 148, "endOffset": 166}, {"referenceID": 7, "context": "3 Weston and Watkins model One of the oldest models, when it comes to multi-class SVM is due to Weston and Watkins (1998). They consider the following loss functions.", "startOffset": 2, "endOffset": 122}], "year": 2017, "abstractText": "In this paper, we provide new theoretical results on the generalization properties of learning algorithms for multiclass classification problems. The originality of our work is that we propose to use the confusion matrix of a classifier as a measure of its quality; our contribution is in the line of work which attempts to set up and study the statistical properties of new evaluation measures such as, e.g. ROC curves. In the confusion-based learning framework we propose, we claim that a targetted objective is to minimize the size of the confusion matrix C, measured through its operator norm \u2016C\u2016. We derive generalization bounds on the (size of the) confusion matrix in an extended framework of uniform stability, adapted to the case of matrix valued loss. Pivotal to our study is a very recent matrix concentration inequality that generalizes McDiarmid\u2019s inequality. As an illustration of the relevance of our theoretical results, we show how two SVM learning procedures can be proved to be confusion-friendly. To the best of our knowledge, the present paper is the first that focuses on the confusion matrix from a theoretical point of view.", "creator": "LaTeX with hyperref package"}}}