{"id": "1202.6583", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Feb-2012", "title": "A Lexical Analysis Tool with Ambiguity Support", "abstract": "Lexical ambiguities naturally arise in languages. We present Lamb, a lexical analyzer that produces a lexical analysis graph describing all the possible sequences of tokens that can be found within the input string. Parsers can process such lexical analysis graphs and discard any sequence of tokens that does not produce a valid syntactic sentence, therefore performing, together with Lamb, a context-sensitive lexical analysis in lexically-ambiguous language specifications.", "histories": [["v1", "Wed, 29 Feb 2012 15:59:54 GMT  (68kb)", "http://arxiv.org/abs/1202.6583v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.FL", "authors": ["luis quesada", "fernando berzal", "francisco j cortijo"], "accepted": false, "id": "1202.6583"}, "pdf": {"name": "1202.6583.pdf", "metadata": {"source": "CRF", "title": "A Lexical Analysis Tool with Ambiguity Support", "authors": ["Luis Quesada"], "emails": ["lquesada@decsai.ugr.es,", "fberzal@decsai.ugr.es,", "cb@decsai.ugr.es"], "sections": [{"heading": null, "text": "ar Xiv: 120 2.65 83v1 [cs.CL] 2 9Fe b20 12"}, {"heading": "A Lexical Analysis Tool with Ambiguity Support", "text": "In fact, it is that you are able to play by the rules and that you are able to play by the rules that you have given yourself."}, {"heading": "II. BACKGROUND", "text": "The above-mentioned suggestions from politics and politics, which have come under criticism in recent years, are now being put into practice, in a direction in which they see themselves able to move, in a direction in which they have gone, in which they have gone, in which they have gone, in which they have gone, in which they have gone, in which they have gone, in which they have pushed themselves, in which they have pushed themselves, in which they have pushed themselves."}, {"heading": "III. LAMB", "text": "Unlike the techniques mentioned above, Lamb is able to detect and capture lexical ambiguities. Our proposed algorithm uses as input the string to be scanned and a list of tokens associated with their corresponding regular expressions. It generates as output a lexical analyzer graph in which each token is associated in the input sequence with its subsequent and preceding tokens. Our algorithm consists of two steps: the scan step, which detects all possible tokens in the input string, and the graph generation step, which calculates the sets of preceding and subsequent tokens for each token and builds the resulting lexical analyzer graph."}, {"heading": "A. The Scanning Step", "text": "The pseudo-code for the scan step is shown in Figure 2. Our algorithm gets an input string called input = > a list of matchers called matcherlist. Each matcher consists of a regular expression and its corresponding match method, a priority value, and a next value. The match method attempts to perform a match by specifying the input string and a starting position in it. The priority value specifies the match priority. Thevalue 0 is reserved for ignored patterns that represent text that does not match tokens. Then, priority values for relevant tokens start at 1, where the lower value of the priority is, the higher the priority. If two tokens share the same priority, the lexer captures both patterns when they overlap due to lexical ambiguities. If two tokens have different priority values and they start at the same position in the input string, only the larger priority marker is considered."}, {"heading": "B. The Graph Generation Step", "text": "The algorithm shown in Figure 3 traverses the list of detected tokens in reverse order and efficiently calculates the sets of previous and subsequent tokens for each token. The sets of preceding and subsequent tokens of the token x are defined in Equation 1, which are a, b, c tokens and xstart and xend are the start and end positions of the token x in the input string. b) The dominant start variable in the pseudo code avoids the need to wander through the token list to find out if a token exists between two specific tokens < bstart & c, cstart > aend, cend < bstart < bstart (1)."}, {"heading": "IV. COMPARISON", "text": "To make a formal comparison between traditional techniques and Lamb, we have implemented a simple (and inefficient) proof reader that supports ambiguities and enables lexical analysis guided by a syntactic rule set, which returns as many parse trees as you can get by applying a set of syntactic rules to a lexical analyzer, its pseudo-code is shown in Figure 4. It iteratively tries to match each rule that proceeds from each existing token and follows every possible token path, and adds the newly found tokens to the list until no new tokens are found in an iteration. Given a language specification that describes the tokens listed in Figure 10, the input string \"& 5.2 & / 25.20 /\" may correspond to the four different lexical analysis alternatives listed in Figure 11, depending on whether the sequences of digits are separated by dots."}, {"heading": "E ::= A B", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A ::= Ampersand Real Ampersand", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B ::= Slash Integer Point Integer Slash", "text": "Figure 12: Context-sensitive syntactical rules for resolving lexical ambiguities. Supporting lexical ambiguities leads to the only possible valid sentence, which in turn is based on the only valid lexical analysis, both of which are shown in Figure 9. Although statistical models such as Hidden Markov Models can provide correct results in similar situations, they cannot be used for this type of language specification, in which the specification specifies how each token is to be recognized, and their results may not always be accurate, making formal verification of its accuracy in a well-defined environment difficult."}, {"heading": "V. CONCLUSIONS", "text": "We have introduced a lexical analyzer, Lamb, that supports lexical ambiguities. It performs lexical analysis that efficiently captures all possible sequences of tokens for lexical, ambiguous languages and creates a lexical analyzer that describes them all. Lamb supports assigning priorities to tokens, as traditional techniques do, but unlike them, it does not enforce these priorities to be set and allows common distribution of priority values. Tokens with common priorities are considered valid alternatives rather than mutually exclusive options. The lexical graph can then be passed as input to a parser that discards any sequence of tokens that does not produce valid syntactical analysis. In summary, our proposal performs a context-sensitive lexical analysis guided by syntactic rules and supports lexical, ambiguous language specifications."}], "references": [{"title": "Compilers: Principles, Techniques, and Tools", "author": ["Alfred V. Aho", "Monica S. Lam", "Ravi Sethi", "Jeffrey D. Ullman"], "venue": "Addison Wesley,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Hidden markov processes", "author": ["Yariv Ephraim", "Neri Merhav"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "The hierarchical hidden markov model: Analysis and applications", "author": ["Shai Fine", "Yoram Singer", "Naftali Tishby"], "venue": "Machile Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Data Mining: Concepts and Techniques. The Morgan Kaufmann Series in Data Management Systems", "author": ["Jiawei Han", "Micheline Kamber", "Jian Pei"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition", "author": ["Daniel Jurafsky", "James H. Martin"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Extension of the limit theorems of probability theory to a sum of variables connected in a chain", "author": ["Andrey Andreyevich Markov"], "venue": "Dynamic Probabilistic Systems", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1971}, {"title": "Maximum entropy markov models for information extraction and segmentation", "author": ["Andrew McCallum", "Dayne Freitag", "Fernando Pereira"], "venue": "In Proc. of the 17th International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Conflict detection and resolution in a lexical analyzer generator", "author": ["J.R. Nawrocki"], "venue": "Information Processing Letters,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["Lawrence R. Rabiner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "From semi-syntactic lexical analyzer to a new compiler model", "author": ["Yuh-Huei Shyu"], "venue": "ACM SIGPLAN Notices,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1986}, {"title": "Introduction to the Theory of Computation", "author": ["Michael Sipser"], "venue": "Course Technology,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}], "referenceMentions": [{"referenceID": 7, "context": "Lexical ambiguities occur when an input string simultaneously corresponds to several token sequences [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 5, "context": "Statistical lexical analyzers also exist [7].", "startOffset": 41, "endOffset": 44}, {"referenceID": 0, "context": "As research in lexical analyzers sets the basis for the application of parsers, it inherits their application fields: the compilation or interpretation of source code written in programming languages [1], the interpretation and integration of data in data mining applications [4], and natural language processing [5].", "startOffset": 200, "endOffset": 203}, {"referenceID": 3, "context": "As research in lexical analyzers sets the basis for the application of parsers, it inherits their application fields: the compilation or interpretation of source code written in programming languages [1], the interpretation and integration of data in data mining applications [4], and natural language processing [5].", "startOffset": 276, "endOffset": 279}, {"referenceID": 4, "context": "As research in lexical analyzers sets the basis for the application of parsers, it inherits their application fields: the compilation or interpretation of source code written in programming languages [1], the interpretation and integration of data in data mining applications [4], and natural language processing [5].", "startOffset": 313, "endOffset": 316}, {"referenceID": 10, "context": "\u2022 Lex generates a lexer that takes as input a set of token types, associated regular expressions [12], and the string to be scanned; and produces the sequence of tokens found in the string.", "startOffset": 97, "endOffset": 101}, {"referenceID": 1, "context": "Statistical models as Hidden Markov Models [2, 7, 10], Hierarchical Hidden Markov Models [3], or Maximum Entropy Markov Models [8] consider the existence of implicit relationships between words, symbols, or characters that are close together in strings.", "startOffset": 43, "endOffset": 53}, {"referenceID": 5, "context": "Statistical models as Hidden Markov Models [2, 7, 10], Hierarchical Hidden Markov Models [3], or Maximum Entropy Markov Models [8] consider the existence of implicit relationships between words, symbols, or characters that are close together in strings.", "startOffset": 43, "endOffset": 53}, {"referenceID": 8, "context": "Statistical models as Hidden Markov Models [2, 7, 10], Hierarchical Hidden Markov Models [3], or Maximum Entropy Markov Models [8] consider the existence of implicit relationships between words, symbols, or characters that are close together in strings.", "startOffset": 43, "endOffset": 53}, {"referenceID": 2, "context": "Statistical models as Hidden Markov Models [2, 7, 10], Hierarchical Hidden Markov Models [3], or Maximum Entropy Markov Models [8] consider the existence of implicit relationships between words, symbols, or characters that are close together in strings.", "startOffset": 89, "endOffset": 92}, {"referenceID": 6, "context": "Statistical models as Hidden Markov Models [2, 7, 10], Hierarchical Hidden Markov Models [3], or Maximum Entropy Markov Models [8] consider the existence of implicit relationships between words, symbols, or characters that are close together in strings.", "startOffset": 127, "endOffset": 130}, {"referenceID": 9, "context": "The semi-syntactic lexical analyzer proposed in [11] brings some of the context information found in the syntactic rule set to the deterministic finite automaton that perform the lexical analysis.", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 7, "endOffset": 12}, {"referenceID": 1, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 7, "endOffset": 12}, {"referenceID": 2, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 7, "endOffset": 12}, {"referenceID": 3, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 7, "endOffset": 12}, {"referenceID": 4, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 7, "endOffset": 12}, {"referenceID": 5, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 7, "endOffset": 12}, {"referenceID": 6, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 7, "endOffset": 12}, {"referenceID": 7, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 7, "endOffset": 12}, {"referenceID": 0, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 29, "endOffset": 34}, {"referenceID": 1, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 29, "endOffset": 34}, {"referenceID": 2, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 29, "endOffset": 34}, {"referenceID": 3, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 29, "endOffset": 34}, {"referenceID": 4, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 29, "endOffset": 34}, {"referenceID": 5, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 29, "endOffset": 34}, {"referenceID": 6, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 29, "endOffset": 34}, {"referenceID": 7, "context": "(-|\\+)?[0-9]+ Integer (-|\\+)?[0-9]+\\.", "startOffset": 29, "endOffset": 34}, {"referenceID": 0, "context": "[0-9]+ Real", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[0-9]+ Real", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[0-9]+ Real", "startOffset": 0, "endOffset": 5}, {"referenceID": 3, "context": "[0-9]+ Real", "startOffset": 0, "endOffset": 5}, {"referenceID": 4, "context": "[0-9]+ Real", "startOffset": 0, "endOffset": 5}, {"referenceID": 5, "context": "[0-9]+ Real", "startOffset": 0, "endOffset": 5}, {"referenceID": 6, "context": "[0-9]+ Real", "startOffset": 0, "endOffset": 5}, {"referenceID": 7, "context": "[0-9]+ Real", "startOffset": 0, "endOffset": 5}], "year": 2012, "abstractText": "Lexical ambiguities naturally arise in languages. We present Lamb, a lexical analyzer that produces a lexical analysis graph describing all the possible sequences of tokens that can be found within the input string. Parsers can process such lexical analysis graphs and discard any sequence of tokens that does not produce a valid syntactic sentence, therefore performing, together with Lamb, a context-sensitive lexical analysis in lexically-ambiguous language specifications.", "creator": "LaTeX with hyperref package"}}}