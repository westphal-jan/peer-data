{"id": "1203.0160", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Mar-2012", "title": "Scaling Datalog for Machine Learning on Big Data", "abstract": "In this paper, we present the case for a declarative foundation for data-intensive machine learning systems. Instead of creating a new system for each specific flavor of machine learning task, or hardcoding new optimizations, we argue for the use of recursive queries to program a variety of machine learning systems. By taking this approach, database query optimization techniques can be utilized to identify effective execution plans, and the resulting runtime plans can be executed on a single unified data-parallel query processing engine. As a proof of concept, we consider two programming models--Pregel and Iterative Map-Reduce-Update---from the machine learning domain, and show how they can be captured in Datalog, tuned for a specific task, and then compiled into an optimized physical plan. Experiments performed on a large computing cluster with real data demonstrate that this declarative approach can provide very good performance while offering both increased generality and programming ease.", "histories": [["v1", "Thu, 1 Mar 2012 11:43:43 GMT  (1632kb,D)", "http://arxiv.org/abs/1203.0160v1", null], ["v2", "Fri, 2 Mar 2012 10:14:58 GMT  (1296kb,D)", "http://arxiv.org/abs/1203.0160v2", null]], "reviews": [], "SUBJECTS": "cs.DB cs.LG cs.PF", "authors": ["yingyi bu", "vinayak borkar", "michael j carey", "joshua rosen", "neoklis polyzotis", "tyson condie", "markus weimer", "raghu ramakrishnan"], "accepted": false, "id": "1203.0160"}, "pdf": {"name": "1203.0160.pdf", "metadata": {"source": "CRF", "title": "Scaling Datalog for Machine Learning on Big Data", "authors": ["Yingyi Bu", "Vinayak Borkar", "Michael J. Carey", "Joshua Rosen", "Neoklis Polyzotis", "Tyson Condie", "Markus Weimer", "Raghu Ramakrishnan"], "emails": [], "sections": [{"heading": "1. INTRODUCTION", "text": "In fact, most people are able to move to another world in which they are in the position in which they find themselves."}, {"heading": "2. PROGRAMMING MODELS FOR ML", "text": "The goal of machine learning (ML) is to turn observational data into a model that can be used to predict or explain previously unseen data. While the range of machine learning techniques is broad, most of them can be understood in terms of three complementary perspectives: \u2022 ML as a search for a good set of parameters based on objective function, a domain expert writes a program that contains the known data, millions of unknown parameters that together make up the model. \u2022 A runtime program searches for a good set of parameters based on objective function. A search strategy includes the parameter space to accurately capture the known data and predict exactly what the instances are that make up the model. \u2022 ML as an iterative refinement can close the gap between the model and objective function."}, {"heading": "2.1 Pregel", "text": "Pregel [22] is a graph analysis support system developed by Google. It discloses a message delivery interface in the form of two UDFs per vertex: Update the update function per vertex. It accepts the current vertex and incoming messages, and produces outgoing messages and an updated status. Combine (optionally) aggregated messages intended for a vertex. We omit other aspects of Pregel - graph mutation and global aggregators - because they are not necessary in many graph algorithms [22], and the machinery for global aggregators is captured later when we deal with iterative map reduction updates. The pregel runtime performs a sequence of iterations, called supersteps, through a Bulk Synchronous Processing (BSP) model. In a single superstep, the Pregel runtime UDF performs the update of the UDF to all active vertex points exactly once."}, {"heading": "2.2 Iterative Map-Reduce-Update", "text": "A large class of learning algorithms can be found in the statistical query model [20]. Statistical queries (e.g.: max, min, sum) decompose into a data-based map function and a subsequent aggregation with a reduced function [10] in which the functions with the same name are identified. Commercial and associative programming supports this function by the following three UDFs. The card receives only a global state worth reading and is applied to all training data."}, {"heading": "2.3 Discussion", "text": "From the data flow perspective, an important difference between different types of ML models is the relationship of the model to the observational data, both in size and structure. Certain models (e.g. regression, classification and clustering) are global for all observation data and therefore relatively small (think MB vs. GB). In others (e.g. theme models and matrix factorization), the model consists of interdependent parameters that are local for each observation and therefore of the same order of magnitude as the observation data. Any system that wants to efficiently support both classes of ML models must be able to recognize and optimize the nature of the task - global or local. The two programming frameworks that we consider to be a broad range of machine learning and graph analysis cover a broad range. Pregel is a well-known graph analysis platform that can be used to develop local models."}, {"heading": "3. DECLARATIVE REPRESENTATION", "text": "In this section, we present a translation of the two programming models into declarative Datalog programs and a formal analysis of the semantics and accuracy of that translation, providing information on the semantics and structure of the underlying data operations, which in turn allows us to think about possible optimizations (e.g. reordering operators or mapping logical operators to various possible implementations), thus generating efficient execution plans across a wide range of configurations. Datalog is a natural choice for this intermedia logical representation, as it can include the inherent recursion of algorithms. Before delving into the details of the translation of the two programming models, we present a brief overview of the most important concepts in Datalog. A Datalog program consists of a set of rules and an optional quantum theory. A Datalog rule has the form p (Y): - q1 (X1), qn."}, {"heading": "3.1 Pregel for Local Models", "text": "We start with the Datalog program in listing 1, which specifies the pregel programming model as it relates to the derivation of local models. A special time argument (the variable J) is used to track the current superstep number passed to the updated UDF call in rule L6, which is discussed below. Rule L1 then calls an initialization UDF init vertex (which accepts the (Id, date) variable as an argument and returns the (state) variable for each tuple in input: referenced by the data predicate. Rule L2 then initializes a send predicate with an activation message to be delivered to all vertices in iteration zero. Rule L3 implements the combination of messages intended for the same vertex. It performs a group-based aggregation via predicate sending, with the combined uteration function declaring the next aggregate state to be a global aggregate (section 2.1)."}, {"heading": "3.2 Iterative Map-Reduce-Update", "text": "iSe \"iW\" n, iSe \"iSe\" n, \"iSe\" iSe, \"iSe\" iSe, \"\" iSe, \"\" iSe, \"\" iSe, \"\" iSe, \"\" iSe, \"\" iSe, \"\" iSe, \"\" iSe, \"\", \"\", \"\" \",\" \"\", \"\" \",\" \"\" \"\", \"\" \"\" \",\" \"\" \"\", \"\" \"\" \",\" \"\" \"\", \"\" \"\", \"\" \",\" \",\" \",\" \"\", \"\" \",\" \"\", \"\", \"\" \",\" \",\" \",\" \",\", \"\", \"\", \",\" \",\", \",\" \",\" \"\", \",\" \"\", \"\", \"\", \"\" \",\" \",\" \"\", \"\", \"\", \"\" \",\" \"\", \"\", \"\" \"\", \",\" \",\", \"\" \"\", \"\", \"\" \"\" \",\", \"\" \",\", \"\" \"\" \"\", \",\", \",\", \"\", \"\" \"\" \"\", \",\", \",\" \"\", \",\" \",\" \"\", \"\" \"\", \"\" \",\", \"\" \",\", \"\", \"\" \"\" \",\" \"\" \",\", \",\" \"\", \"\", \"\", \"\", \"\", \",\" \"\", \",\" \"\" \"\", \"\" \"\" \",\", \",\" \"\" \"\" \"\" \"\", \",\" \"\" \"\", \"\" \",\" \",\" \"\", \"\", \"\", \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\", \"\", \"\" \"\" \"\", \"\" \"\", \",\" \"\" \"\", \"\" \"\" \",\" \"\" \"\" \"\" \",\" \"\", \",\" \"\" \"\" \""}, {"heading": "3.3 Semantics and Correctness", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country."}, {"heading": "4. PHYSICAL DATAFLOW", "text": "In this section we present the physical parallel data flow plans on which the Pregel and Iterative Map-Reduce-Update programming models are executed. We select the data parallel runtime of Hyracks [7] as the target platform for developing and executing these physical plans. First, we give a very brief overview of Hyracks (Section 4.1), then we describe the physical plans for Pregel (Section 4.2) and Iterative MapReduce-Update (Section 4.3)."}, {"heading": "4.1 Hyracks Overview", "text": "Hyracks is a data-parallel runtime in the same general space as Hadoop [3] and Dryad [18]. Tasks are presented in the form of directed acyclic graphs consisting of operators and connectors. Operators are responsible for the consumption of input partitions and the creation of output partitions. Connectors perform a redistribution of data between operators. Operators are characterized by an algorithm (e.g. filter, index join, hash group) and input / output data properties (e.g. ordered by or partitioned by a certain attribute). Connectors are classified according to a connection topology and an algorithm (e.g. one-to-one connector, aggregate connector, m-to-n-hash partition connector or m-to-n-hash partition connector) as well as according to a materialization policy (e.g. receiver side, or block side)."}, {"heading": "4.2 Pregel", "text": "Figure 4 shows the optimized physical plan for Pregel. The top data flow performs iteration 0 and is derived from the logical plans L1 and L2 in Figure 3. The file scan operator (O3) reads the partitions of the input (graph) data and writes the result to the vertex identifier (O1) and the sort operator (O4). The operator generates an initial activation message for the vertex, which sends the result to the vertex (O2). The sorted tuples are passed to the int vertex."}, {"heading": "4.3 Iterative Map-Reduce-Update", "text": "We describe how to build and optimize a physical plan for iterative map reduction update programming that aims to run Batch Gradient Descent (BGD). Figure 5 describes a physical plan that is generated by translating the logical query plan in Figure 2 to a physical data flow. Figure 0 leads to a logical plan G1. Here, we simply write the original model to HDFS based on the following iterations until the driver recognizes the termination. This dataflow corresponds to the logical plans G2-3. At the beginning of each iteration, the model is read by HDFS and paired with the dataflow iterations."}, {"heading": "5.1 Batch Gradient Descent", "text": "We start with a batch gradient descent (BGD) task on a Realworld dataset pulled from the web content recommendation domain. The data consists of 16, 557, 921 records sampled from Yahoo! News. Each record consists of a feature vector that describes a (user, content) pair and a label that indicates whether the user is consuming the content. The goal of the ML task is to learn a linear model that describes the probability of consumption for a previously unseen (user, content) pair. The total number of features in the dataset is 8, 368, 084, 005, and each feature vector is economical: users are only interested in a small subset of the content. The dataset is stored in HDFS and, before the job is executed, it is perfectly balanced across all the machines used in the experiment. That is, each machine is assigned to an equal number of records."}, {"heading": "5.1.1 Cost-optimal configuration for fixed-size data", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far, until it is so far."}, {"heading": "5.2 PageRank", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "5.3 Discussion", "text": "Before comparing our system to Hadoop, we also tried to compare it to three other \"obvious candidate systems,\" namely Giraph [2], Mahout [5] and Spark [30]. What we discovered (the hard way!) is that none of these systems was able to execute PageRank for the Yahoo! webmap record, even given all 6 racks (175 machines), due to design issues related either to memory management (Giraph and Spark) or to the implementation of algorithms (Mahout). An interesting observation regarding the Spark user model was the process involved in implementing a 1-level aggregation tree (175 machines). To perform preaggregation in Spark, we had to explicitly write such an intermediate step - in the user's code - \"reduceByKey,\" which then feeds the final (global) reduction step."}, {"heading": "6. RELATED WORK", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a region, in a country, in a country, in a country, in a country, in a city and in a country, in a country, in a country and in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a"}, {"heading": "7. CONCLUSION", "text": "The growing demand for machine learning is driving both industry and science to design new types of highly scalable iterative computer systems, such as Mahout, Pregel, Spark, Twister, HaLoop, and PrItr. However, today's specialized machine learning platforms tend to mix logical representations with physical implementations. As a result, today's platforms require their developers to rebuild critical components and hardcode optimization strategies, and to confine themselves to specific runtime settings that normally fit only (naturally) a limited subset of potential machine learning processes, leading to the current state of practice, with the implementation of new scalable machine learning algorithms being very labor-intensive and the entire data processing pipeline comprising several disparate tools associated with file and workflow-based glubs. In contrast, we have a clearer foundation on which specialized learning processes can fall back."}, {"heading": "8. REFERENCES", "text": "[1] Alekh Agarwal, Olivier Chapelle, Miroslav Dud\u0131 \u0301 k, and JohnLangford. A reliable effective terascale linear learning system. CoRR, abs / 1110.4198, 2011. [2] Giraph: Open-source implementation of Pregel. http: / / incubator.apache.org / giraph /. [3] Hadoop: Open-source implementation of MapReduce. http: / / hadoop.apache.org. [4] The Hive Project. http: / / hive.apache.org /. [5] The Mahout Project. http: / mahout.apache.org /. Ashima Atul. Compact implementation of distributedinference algorithms for network.'s thesis, EECS Department, University of California, Berkeley, Mar 2009."}, {"heading": "A. BATCH GRADIENT DESCENT", "text": "The question of whether the search for a new model is a search for a new model for the search for a new model is guided by a loss function. (fw) The function fw amounts to scanning the space of parameterized functions.) The parameters are also referred to as models and are typically related to asw. Hence, the search for fw is guided by a loss function l (fw), which indicates the divergence between a prediction fw (x) and a known class of machine learning problems. (fw)"}, {"heading": "B. MODEL(ING) SEMANTICS", "text": "In fact, it is so that we will be able to move to another world, in which we can move to another world, in which we are able to create another world, in which we are able to create another world, in which we are able to create another world, in which we are able to create a new world, in which we create a new world, in which we create a new world, in which we create a new world, in which we create a new world, in which we create a new world, in which we create a new world, in which we create a new world, in which we create a new world, in which we create a new world, in which we create a new world, in which we create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world, in which we want to create a new world we want to create a new world, in which we want to create a new world, in which we want to create a new world, we want to create a new world, in which we want to create a new world we want to create a new world, in which we want to create a new world we want to create a new world, we want to create a new world, in which we want to create a new world in which we want to create a new world."}], "references": [{"title": "A reliable effective terascale linear learning system", "author": ["Alekh Agarwal", "Olivier Chapelle", "Miroslav Dud\u0131\u0301k", "John Langford"], "venue": "CoRR, abs/1110.4198,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Compact implementation of distributed inference algorithms for network", "author": ["Ashima Atul"], "venue": "Master\u2019s thesis, EECS Department,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Hyracks: A flexible and extensible foundation for data-intensive computing", "author": ["Vinayak R. Borkar", "Michael J. Carey", "Raman Grover", "Nicola Onose", "Rares Vernica"], "venue": "In ICDE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Inside \u201cBig Data Management\u201d: Ogres, Onions, or Parfaits", "author": ["Vinayak R. Borkar", "Michael J. Carey", "Chen Li"], "venue": "In EDBT,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "HaLoop: Efficient iterative data processing on large", "author": ["Yingyi Bu", "Bill Howe", "Magdalena Balazinska", "Michael D. Ernst"], "venue": "clusters. PVLDB,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Map-reduce for machine learning on multicore", "author": ["Cheng-Tao Chu", "Sang Kyun Kim", "Yi-An Lin", "YuanYuan Yu", "Gary R. Bradski", "Andrew Y. Ng", "Kunle Olukotun"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Mad skills: New analysis practices for big data", "author": ["Jeffrey Cohen", "Brian Dolan", "Mark Dunlap", "Joseph M. Hellerstein", "Caleb Welton"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Mapreduce: Simplified data processing on large clusters", "author": ["Jeffrey Dean", "Sanjay Ghemawat"], "venue": "In OSDI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "The gamma database machine project", "author": ["David J. DeWitt", "Shahram Ghandeharizadeh", "Donovan A. Schneider", "Allan Bricker", "Hui-I Hsiao", "Rick Rasmussen"], "venue": "IEEE Trans. Knowl. Data Eng.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1990}, {"title": "Parallel database systems: The future of high performance database systems", "author": ["David J. DeWitt", "Jim Gray"], "venue": "Commun. ACM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Dyna: Extending Datalog for modern AI", "author": ["Jason Eisner", "Nathaniel W. Filardo"], "venue": "Datalog 2.0,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "An overview of the system software of a parallel relational database machine grace", "author": ["Shinya Fushimi", "Masaru Kitsuregawa", "Hidehiko Tanaka"], "venue": "In VLDB,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1986}, {"title": "Dryad: distributed data-parallel programs from sequential building blocks", "author": ["Michael Isard", "Mihai Budiu", "Yuan Yu", "Andrew Birrell", "Dennis Fetterly"], "venue": "In EuroSys,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Distributed data-parallel computing using a high-level programming language", "author": ["Michael Isard", "Yuan Yu"], "venue": "In SIGMOD Conference,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Efficient noise-tolerant learning from statistical queries", "author": ["Michael Kearns"], "venue": "In Journal of the ACM,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "GraphLab: A new framework for parallel machine learning", "author": ["Yucheng Low", "Joseph Gonzalez", "Aapo Kyrola", "Danny Bickson", "Carlos Guestrin", "Joseph M. Hellerstein"], "venue": "In UAI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Pregel: a system for large-scale graph processing", "author": ["Grzegorz Malewicz", "Matthew H. Austern", "Aart J.C. Bik", "James C. Dehnert", "Ilan Horn", "Naty Leiser", "Grzegorz Czajkowski"], "venue": "In SIGMOD Conference,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Tuffy: Scaling up statistical inference in markov logic networks using an RDBMS", "author": ["Feng Niu", "Christopher R\u00e9", "AnHai Doan", "Jude W. Shavlik"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Pig Latin: a not-so-foreign language for data processing", "author": ["Christopher Olston", "Benjamin Reed", "Utkarsh Srivastava", "Ravi Kumar", "Andrew Tomkins"], "venue": "In SIGMOD Conference,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "The pagerank citation ranking: Bringing order to the web", "author": ["Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd"], "venue": "Technical Report 1999-66,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1999}, {"title": "A survey of research on deductive database systems", "author": ["Raghu Ramakrishnan", "Jeffrey D. Ullman"], "venue": "Journal of Logic Programming,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1993}, {"title": "The genesis of a database computer", "author": ["Jack Shermer", "Philip M. Neches"], "venue": "IEEE Computer,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1984}, {"title": "A convenient framework for efficient parallel multipass algorithms", "author": ["Markus Weimer", "Sriram Rao", "Martin Zinkevich"], "venue": "In LCCC : NIPS 2010 Workshop on Learning on Cores, Clusters and Clouds,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Spark: cluster computing with working sets. HotCloud\u201910", "author": ["Matei Zaharia", "Mosharaf Chowdhury", "Michael J. Franklin", "Scott Shenker", "Ion Stoica"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Negation and aggregates in recursive rules: the LDL++ approach", "author": ["Carlo Zaniolo", "Natraj Arni", "KayLiang Ong"], "venue": "In DOOD,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1993}, {"title": "PrIter: a distributed framework for prioritized iterative computations", "author": ["Yanfeng Zhang", "Qixin Gao", "Lixin Gao", "Cuirong Wang"], "venue": "In SOCC,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2011}], "referenceMentions": [{"referenceID": 19, "context": ", PageRank [25] or Batch Gradient Descent.", "startOffset": 11, "endOffset": 15}, {"referenceID": 2, "context": "Second, we will demonstrate that an appropriately chosen data-intensive computing substrate, namely Hyracks [7], is able to handle the computational requirements of such programs through the application of dataflow processing techniques like those used in parallel databases [15].", "startOffset": 108, "endOffset": 111}, {"referenceID": 9, "context": "Second, we will demonstrate that an appropriately chosen data-intensive computing substrate, namely Hyracks [7], is able to handle the computational requirements of such programs through the application of dataflow processing techniques like those used in parallel databases [15].", "startOffset": 275, "endOffset": 279}, {"referenceID": 20, "context": "The fact that Datalog is well-suited for iterative computations and for graph-centric programming is well-known [26], and it has also been demonstrated that Datalog is well-suited to search problems [12].", "startOffset": 112, "endOffset": 116}, {"referenceID": 1, "context": "The natural fit between Datalog and ML programming has also been recognized by others [6, 16], but not at \u201cBig Data\u201d scale.", "startOffset": 86, "endOffset": 93}, {"referenceID": 10, "context": "The natural fit between Datalog and ML programming has also been recognized by others [6, 16], but not at \u201cBig Data\u201d scale.", "startOffset": 86, "endOffset": 93}, {"referenceID": 16, "context": "Pregel [22] is a system developed at Google for supporting graph analytics.", "startOffset": 7, "endOffset": 11}, {"referenceID": 16, "context": "We omit other aspects of Pregel\u2014graph mutation and global aggregators\u2014because they are not necessary in many graph algorithms [22], and the machinery for global aggregators is captured later when we address Iterative Map-Reduce-Update.", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "Example: PageRank [25] is a canonical example of a graph algorithm that is concisely captured by Pregel.", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "A large class of machine learning algorithms are expressible in the statistical query model [20].", "startOffset": 92, "endOffset": 96}, {"referenceID": 5, "context": ") themselves decompose into a data-local map function and a subsequent aggregation using a reduce function [10], where map and reduce refer to the functions by the same name from the functional programming literature.", "startOffset": 107, "endOffset": 111}, {"referenceID": 7, "context": "2 It is interesting to point out here that Google\u2019s MapReduce [13] programming model is not", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "Example: Convex Optimization A large class of machine learning\u2014including Support Vector Machines, Linear and Logistic Regression and structured prediction tasks such as machine translation\u2014can be cast as convex optimization problems, which in turn can be solved efficiently using an Iterative Map-ReduceUpdate approach [1, 28].", "startOffset": 319, "endOffset": 326}, {"referenceID": 22, "context": "Example: Convex Optimization A large class of machine learning\u2014including Support Vector Machines, Linear and Logistic Regression and structured prediction tasks such as machine translation\u2014can be cast as convex optimization problems, which in turn can be solved efficiently using an Iterative Map-ReduceUpdate approach [1, 28].", "startOffset": 319, "endOffset": 326}, {"referenceID": 20, "context": "The evaluation of a Datalog program reaches a fixpoint when no further deductions can be made based on the currently inferred facts [26].", "startOffset": 132, "endOffset": 136}, {"referenceID": 24, "context": "XY-stratified [31].", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "The proof can be found in Appendix B and is based on the machinery developed in [31].", "startOffset": 80, "endOffset": 84}, {"referenceID": 20, "context": "We describe the semantics of the two Datalog programs by translating them (using standard techniques from the deductive database literature [26]) into an extended relational algebra.", "startOffset": 140, "endOffset": 144}, {"referenceID": 2, "context": "We choose the Hyracks data-parallel runtime [7] as the target platform to develop and execute these physical plans.", "startOffset": 44, "endOffset": 47}, {"referenceID": 12, "context": "Hyracks is a data-parallel runtime in the same general space as Hadoop [3] and Dryad [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 23, "context": "In this section, we present experiments comparing the Datalogderived physical plans of Section 4 to implementations of the same tasks on two alternative systems: Spark [30] for BGD and Hadoop [3] for Pregel.", "startOffset": 168, "endOffset": 172}, {"referenceID": 23, "context": "three other \u201cobvious candidate\u201d systems, namely Giraph [2], Mahout [5], and Spark [30].", "startOffset": 82, "endOffset": 86}, {"referenceID": 8, "context": "Parallel database systems such as Gamma [14], Teradata [27], and GRACE [17] applied partitioned-parallel processing to data management, particularly query processing, over two decades ago.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "Parallel database systems such as Gamma [14], Teradata [27], and GRACE [17] applied partitioned-parallel processing to data management, particularly query processing, over two decades ago.", "startOffset": 55, "endOffset": 59}, {"referenceID": 11, "context": "Parallel database systems such as Gamma [14], Teradata [27], and GRACE [17] applied partitioned-parallel processing to data management, particularly query processing, over two decades ago.", "startOffset": 71, "endOffset": 75}, {"referenceID": 7, "context": "The introduction of Google\u2019s MapReduce system [13], based on similar principles, led to the recent flurry of work in MapReducebased data-intensive computing.", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "Systems like Dryad [18] and Hyracks [7] have successfully made the case for supporting a richer set of data operators beyond map and reduce as well as a richer set of data communication patterns.", "startOffset": 19, "endOffset": 23}, {"referenceID": 2, "context": "Systems like Dryad [18] and Hyracks [7] have successfully made the case for supporting a richer set of data operators beyond map and reduce as well as a richer set of data communication patterns.", "startOffset": 36, "endOffset": 39}, {"referenceID": 18, "context": "High-level language abstractions like Pig [24], Hive [4], and DryadLINQ [19] reduce the accidental complexity of programming in a lower-level dataflow paradigm (e.", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "High-level language abstractions like Pig [24], Hive [4], and DryadLINQ [19] reduce the accidental complexity of programming in a lower-level dataflow paradigm (e.", "startOffset": 72, "endOffset": 76}, {"referenceID": 4, "context": "Iterative extensions to MapReduce like HaLoop [9] and PrIter [32] were the first to identify and address the need for runtime looping constructs.", "startOffset": 46, "endOffset": 49}, {"referenceID": 25, "context": "Iterative extensions to MapReduce like HaLoop [9] and PrIter [32] were the first to identify and address the need for runtime looping constructs.", "startOffset": 61, "endOffset": 65}, {"referenceID": 16, "context": "Domain-specific programming models like Pregel [22], GraphLab [21], and Spark [30], go beyond one-off implementations for specific algorithms (e.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "Domain-specific programming models like Pregel [22], GraphLab [21], and Spark [30], go beyond one-off implementations for specific algorithms (e.", "startOffset": 62, "endOffset": 66}, {"referenceID": 23, "context": "Domain-specific programming models like Pregel [22], GraphLab [21], and Spark [30], go beyond one-off implementations for specific algorithms (e.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "[1, 28]), to general purpose systems that capture a specific class of ML tasks.", "startOffset": 0, "endOffset": 7}, {"referenceID": 22, "context": "[1, 28]), to general purpose systems that capture a specific class of ML tasks.", "startOffset": 0, "endOffset": 7}, {"referenceID": 17, "context": "In Tuffy [23], Markov Logic Networks are represented as declarative rules in first-order-logic, and from there, optimized into an efficient runtime plan by a RDBMS.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "MadLib [11] maps linear algebra operations, such as matrix multiplies, to SQL queries that are then compiled and optimized for a parallel database system.", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "Atul and Hellerstein [6] use Overlog\u2014a distributed Datalog-like declarative language\u2014to elegantly capture probabilistic inference algorithms; among them, a Junction Tree Running Intersection Property expressed in a mere seven Overlog rules.", "startOffset": 21, "endOffset": 24}, {"referenceID": 10, "context": "Dyna uses a Datalog extension to capture statistical Artificial Intelligence algorithms as systems of equations, which relate intensional and extensional data to form structured prediction models [16].", "startOffset": 196, "endOffset": 200}, {"referenceID": 3, "context": "We are currently developing the ScalOps query processing components required to automate the remaining translations steps from Figure 1; these include the Planner/Optimizer as well as a more general algebraic foundation based on extending the Algebricks query algebra and rewrite rule framework of ASTERIX [8].", "startOffset": 306, "endOffset": 309}], "year": 2017, "abstractText": "In this paper, we present the case for a declarative foundation for data-intensive machine learning systems. Instead of creating a new system for each specific flavor of machine learning task, or hardcoding new optimizations, we argue for the use of recursive queries to program a variety of machine learning systems. By taking this approach, database query optimization techniques can be utilized to identify effective execution plans, and the resulting runtime plans can be executed on a single unified data-parallel query processing engine. As a proof of concept, we consider two programming models\u2014Pregel and Iterative Map-Reduce-Update\u2014from the machine learning domain, and show how they can be captured in Datalog, tuned for a specific task, and then compiled into an optimized physical plan. Experiments performed on a large computing cluster with real data demonstrate that this declarative approach can provide very good performance while offering both increased generality and programming ease.", "creator": "LaTeX with hyperref package"}}}