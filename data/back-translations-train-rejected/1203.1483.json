{"id": "1203.1483", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2012", "title": "Learning Random Kernel Approximations for Object Recognition", "abstract": "Approximations based on random Fourier features have recently emerged as an efficient and formally consistent methodology to design large-scale kernel machines. By expressing the kernel as a Fourier expansion, features are generated based on a finite set of random basis projections, sampled from the Fourier transform of the kernel, with inner products that are Monte Carlo approximations of the original kernel. Based on the observation that different kernel-induced Fourier sampling distributions correspond to different kernel parameters, we show that an optimization process in the Fourier domain can be used to identify the different frequency bands that are useful for prediction on training data. Moreover, the application of group Lasso to random feature vectors corresponding to a linear combination of multiple kernels, leads to efficient and scalable reformulations of the standard multiple kernel learning model \\cite{Varma09}. In this paper we develop the linear Fourier approximation methodology for both single and multiple gradient-based kernel learning and show that it produces fast and accurate predictors on a complex dataset such as the Visual Object Challenge 2011 (VOC2011).", "histories": [["v1", "Wed, 7 Mar 2012 14:33:26 GMT  (58kb,D)", "http://arxiv.org/abs/1203.1483v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["eduard gabriel b\\u{a}z\\u{a}van", "fuxin li", "cristian sminchisescu"], "accepted": false, "id": "1203.1483"}, "pdf": {"name": "1203.1483.pdf", "metadata": {"source": "CRF", "title": "Learning Random Kernel Approximations for Object Recognition", "authors": ["Eduard Gabriel B\u0103z\u0103van", "Fuxin Li", "Cristian Sminchisescu"], "emails": ["eduard.bazavan@imar.ro", "fli@cc.gatech.edu", "cristian.sminchisescu@ins.uni-bonn.de"], "sections": [{"heading": "1. Introduction", "text": "The right choice of core function and its hyperparameters is critical to the success of applying core methods to practical applications. These selections involve a number of different problems: from selecting a single widescreen parameter in radial base kernels, scaling different features with different weight classes, to learning a linear or nonlinear combination of several cores (MKL). In complicated practical problems such as computer vision, the need for multiple cores naturally arises. Images can be represented by descriptors based on shape, color, and texture, and these descriptors have different routes that classify different categories."}, {"heading": "2. Related work", "text": "Approaches to kernel learning can generally be classified into methods that estimate the hyperparameters of a single kernel [25, 6, 15] and methods that learn the weights of a linear combination of nuclei and possibly their hyperparameters. [31, 2, 16, 36] A popular approach to a single kernel learning is the gradient-based method followed by Chapelle et al. [6, 15] Specify an efficient algorithm that oscillates between learning an SVM and optimizing the characteristics."}, {"heading": "3. Learning Kernels Parameters", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Random Fourier Approximations", "text": "Central to their use is the \"kernel trick,\" which makes it possible to design algorithms that depend only on the inner product of their arguments. This trick relies on the property of positively defined core functions k (\u00b7, \u00b7) to define a point product and a waiver money, so that the point product between raised data points can be efficiently calculated as \"x\" > \"x.\" This makes it possible to handle complex or infinite dimensional attribute distributions in space, and in fact, these usually provide the best results in visual recognition datasets [13, 18]. However, the very advantage that has popularized kernel methods requires the manipulation of matrices of pairs of examples. For large datasets, scaling kernel methods is at least square in the number of their direct elements."}, {"heading": "3.2. Single Kernel Learning", "text": "In order to achieve this goal, we need a learning criterion and an algorithm to optimize the hyperparameters. This efficiency of linear formulation also allows us to scale the number of parameters that are unattainable with classical methods. Our approach to kernel learning will optimize the hyperparameters in terms of error in terms of validation held, for models obtained on the training set to prevent overadjustment. Subsequently, we will prevent denote X and y from the matrix of inputs or code variations (in the order of objectives) and the matrix of objectives on the training set."}, {"heading": "3.3. Multiple Kernel Learning", "text": "In this section, we show that we can do exactly the opposite with the random Fourier methodology, and that this new formulation is equivalent to the multiple kernel learning formulation, and then compare the two methods with the random Fourier framework. We prove that this new formulation is equivalent to the multiple kernel learning formulation, and then compare the two methods with the random Fourier question, and the Lasso group is applied to it (RFF-GL)."}, {"heading": "3.3.1 Proof of equivalence", "text": "We now show that GMKL as optimization method with RFF-GL = > IDT = = IDT = IDT = IDT = IDT = IDT = IDT = IDT = IDT = IDT = IDT = IDT = IDT = IDT = IDT (IDT) = IDT (IDT) = IDT (IDT) = IDT (IDT) = IDT (IDT) = IT (IT) = IT (IT) (IT) = IT (IT) (IT) (IT) = IT (IT) (IT) (IT) = IT (IT) (IT) (IT) = IDT (IT) (IT) (IT) = IDT (IT) (IT) (IT) = IDT (IT) (IT) = IDT (IT) (IT) (IT) = IDT (IT) (IT) = IDT (IT) = IDT (IT) = IDT (IDT) = IDT (IDT) = IDT (IDT) = IDT (IDT) = IDT (IDT) = IDT (IDT) = IDT (IDT) = IDT (IDT) = IDT (IDT) = IDT (IDT) = IDT (IDT) = IDT = IDT (IDT) = IDT (IDT) = IDT (IDT) = IDT (IDT (IDT) = IDT (IDT = IDT) = IDT (IDT) = IDT (IDT (IDT)"}, {"heading": "3.3.2 Computational complexity", "text": "We assume an average time complexity for a support vector machine algorithm [5] to be O (N3sv), where Nsv is the number of support vectors. Since GMKL is based on several evaluations of the standard SVM solver, we can show the time complexity of the multiple kernel learning framework: O (rN2m2 + iGMKL (N 2r + N3sv) + rNsvNm 2), where r is the number of cores, N is the number of training samples, m is the size of the training input (we assume it is the same for all data for simplicity), and iGMKL is the maximum number of calls and SVM solvers. The complexity is dominated by the term O (rN2m2), which is the cost of calculating the kernel matrices (we assume it is formulated in the laser group so that we have complexity NmO + N2d)."}, {"heading": "4. Experiments", "text": "We present experiments for learning single cores and learning multiple cores by comparing the random linear Fourier methodology with its nonlinear counterparts in the kernel, both in terms of runtime and accuracy. We have experimented with the PASCAL VOC2011 segmentation challenge, which consists of 2223 images for training with the truth on the ground, divided into two halves for training and validation. A further 1111 images are given for testing without truth on the ground. Following the standard method, we have trained the methods on the training set (further divided, internally into training and validation for learning hyperparameters on the kernel) and tested on the PASCAL VOC2011 validation theorem. We have used the method presented in Li et al. [18] and relied on a segmentation algorithm consisting of Carreira and Sminchiseu [4 per 100] segments to filter the segum of approximately the pool."}, {"heading": "4.1. Single kernel learning", "text": "We performed a series of experiments for our Random Fourier Learning Technique (RFF-SKL) and compared it with the single-kernel learning technique introduced by Chapelle et al. [6] (KRR-GD) in terms of accuracy. We want to predict the class for more than 105 segments. We expect RFF-SKL to produce results comparable to KRR-GD in terms of accuracy. The results will be shown in Table 2. We vary the size of the training set from 103 to 105 and measure both the runtimes of RFF-SKL and accuracy. In Figure 2 we present how the accuracy depends on the size of the training data and the average time required to operate a class. We observe that RFF-SKL runtime scales are linear in the number of examples. The number of random Fourier samples we selected was d = 3000. This is consistent with our computational complexity discussed in Section 3.2."}, {"heading": "4.2. Multiple Kernel Learning", "text": "For our random Fourier characteristics within the Group Lasso Framework (RFF-GL), we approximated the kernel using the recommended settings as in [30]. In Figure 3, we compare the accuracy of predicting the correct class for the segments. We see that for a small number of training samples, GMKL is slightly superior, but RFF-GL is catching up due to its scalability. Working with kernel matrices greater than 104 was not feasible for GMKL. For the Lasso group, we adjusted the implementation presented in [21], while for comparisons with standard multi-kernel learning, we presented GMKL implementation by GMKL, which was represented by Varales and Babyales in an average four-class relationship."}, {"heading": "5. Conclusions", "text": "The Fourier methodology is a powerful and formally consistent class of linear approximation techniques for nonlinear kernel machines that promises to combine good model scalability and nonlinear predictive power. This has motivated research to expand the class of useful cores that can be approximated, such as Chi-square [35, 19], but leaves a lot of room for reformulating standard problems such as single or multi-kernel learning in the linear Fourier area. In this paper, we have developed grade-based methods for single and multi-kernel learning in the Fourier area and demonstrated that they are efficient and deliver accurate results on a complex computer vision dataset such as VOC2011."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "Approximations based on random Fourier features have<lb>recently emerged as an efficient and formally consistent<lb>methodology to design large-scale kernel machines [24].<lb>By expressing the kernel as a Fourier expansion, features<lb>are generated based on a finite set of random basis pro-<lb>jections, sampled from the Fourier transform of the kernel,<lb>with inner products that are Monte Carlo approximations of<lb>the original kernel. Based on the observation that different<lb>kernel-induced Fourier sampling distributions correspond<lb>to different kernel parameters, we show that an optimiza-<lb>tion process in the Fourier domain can be used to identify<lb>the different frequency bands that are useful for prediction<lb>on training data. Moreover, the application of group Lasso<lb>[37] to random feature vectors corresponding to a linear<lb>combination of multiple kernels, leads to efficient and scal-<lb>able reformulations of the standard multiple kernel learning<lb>model [33]. In this paper we develop the linear Fourier<lb>approximation methodology for both single and multiple<lb>gradient-based kernel learning and show that it produces<lb>fast and accurate predictors on a complex dataset such as<lb>the Visual Object Challenge 2011 (VOC2011).", "creator": "LaTeX with hyperref package"}}}