{"id": "1203.1858", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Mar-2012", "title": "Distributional Measures of Semantic Distance: A Survey", "abstract": "The ability to mimic human notions of semantic distance has widespread applications. Some measures rely only on raw text (distributional measures) and some rely on knowledge sources such as WordNet. Although extensive studies have been performed to compare WordNet-based measures with human judgment, the use of distributional measures as proxies to estimate semantic distance has received little attention. Even though they have traditionally performed poorly when compared to WordNet-based measures, they lay claim to certain uniquely attractive features, such as their applicability in resource-poor languages and their ability to mimic both semantic similarity and semantic relatedness. Therefore, this paper presents a detailed study of distributional measures. Particular attention is paid to flesh out the strengths and limitations of both WordNet-based and distributional measures, and how distributional measures of distance can be brought more in line with human notions of semantic distance. We conclude with a brief discussion of recent work on hybrid measures.", "histories": [["v1", "Thu, 8 Mar 2012 17:29:33 GMT  (69kb)", "http://arxiv.org/abs/1203.1858v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["saif m mohammad", "graeme hirst"], "accepted": false, "id": "1203.1858"}, "pdf": {"name": "1203.1858.pdf", "metadata": {"source": "CRF", "title": "Distributional Measures of Semantic Distance: A Survey", "authors": ["Saif Mohammad", "Graeme Hirst"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 120 3.18 58v1 [cs.CL] 8 Mar 201 2This is an unpublished manuscript that was first made available on the Web in 2006. It is a heavily revised version of an earlier manuscript entitled \"Distributional Measures as Proxy for Semantic Relationships,\" which was first made available on the Web in 2005. Distributional Measures of Semantic Distance: A SurveySaif Mohammad University of Toronto Graeme Hirst University of Toronto The ability to mimic human notions of semantic distance is widespread. Some measurements are based only on raw text (distributional measurements) and some are based on knowledge sources such as WordNet. Although extensive studies have been conducted to compare WordNet-based measurements with human judgment, the use of distributional measurements such as semantic distance has attracted little attention. Although they are traditionally weak compared to WordNet-based measurements, they claim both their applicability and their uniformity in certain languages."}, {"heading": "1. Introduction", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush, to rush."}, {"heading": "1.1 Semantic relatedness and semantic similarity", "text": "Semantic distance consists of two kinds: semantic similarity and semantic kinship. The former is a subset of the latter, but the two terms can be used interchangeably in certain contexts, making it all the more important to be aware of their distinction. Two concepts are considered semantically similar when there is a synonymy (or near-synonymy), hyponymy (hypernymia), antagonymy or troponymy relationship between them (examples are APPLES-BANANAS, DOCTOR-SURGEON, DARK-BRIGHT). Two senses of the word are considered semantically related if there is any lexical semantic relationship between them at all - classic or non-classical (examples are APPLES-BANANANAS, SURGEON-SCALPEL, TREE-SHADE)."}, {"heading": "1.2 Human judgments of semantic distance", "text": "In fact, most people are able to identify themselves and understand what they are supposed to do. (...) Most people in the world don't know what they are supposed to do. (...) Most people in the world don't know what they are supposed to do. (...) They don't know what they are supposed to do. (...) They don't know what they are supposed to do. (...) They don't know what they are supposed to do. (...) They don't know what they are supposed to do. (...) They don't know what they are supposed to do. (...) They don't know what they are supposed to do. (...) They don't know what they are supposed to do. (...) They don't know what they are supposed to do. (...). (...) They don't know what they are supposed to do. (...) They don't know what they are supposed to do. (...). (...) They don't do. (...). (...) They do. (...) They do. (...) They do. (...) They do. (...) They do. (...) They do. (...) They do. (...) They do. (...) They do. (...) They do. (...) They do. (...). (...) They do. (do. (...). (...) They do. (...) They do. (...). (do. (...). (do. (...). (do. (...). (do. (...)."}, {"heading": "1.3 Automatic measures of semantic distance", "text": "Automatic semantic distance measurements quantify the semantic distance between pairs of words. They give values within a certain range (for example, 0 to 1), so that one end of that range represents maximum proximity or synonymy, while the other end represents 3maximum distance. 1 Depending on which end is, semantic distance measurements can be classified as distance measurements (greater values indicate greater distance and proximity).1 Measurements of proximity can easily be converted into a distance measurement using an appropriate inverse function, or vice versa. Two classes of automatic methods have traditionally been used to determine semantic distance. Knowledgeable measurements of concept distance, such as those by Jiang and Conrath (1997), Leacock and Khodorow (1998), and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine distance."}, {"heading": "2. Knowledge-rich approaches to semantic distance", "text": "Before we begin our study of distributional scales, let us take a quick look at resource-based scales. In a way, they complement distributional scales, and thus the discussion will set the context for the analysis of distributional scales. The creation of electronically available ontologies and semantic networks like WordNet made it possible to use them to solve numerous natural language problems, including measuring semantic distances. Budanitsky and Hirst (2006), Hirst and Budanitsky (2005), and Patwardhan, Banerjee and Pedersen (2003) have compiled a comprehensive overview of the various WordNet-based scales whose comparisons1 A note on terminology: In many contexts, the term distance scales refers to the full scale (regardless of what the different ends of the range mean). In certain other contexts (as in this paragraph), distance scales refer only to those that signify greater distances."}, {"heading": "2.1 Measures that exploit WordNet\u2019s semantic network", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "2.2 Measures that rely on dictionaries and thesauri", "text": "Lesk (1986) introduced a method to perform word sense disambiguations using word glossaries (definitions), comparing the glosses of the senses of a target word with those of its context, and determining the number of word overlappings. Sense with the4 Only the verbs requiring a topic were selected, and the sub-categorization frameworks had to match each other. 6Mohammad and Hirst used semantic distance distributional yardsticks to determine the greatest number of overlaps as the intended sense of the target. Inspired by this approach, Banerjee and Pedersen (2003) proposed a semantic mapping unit that considers two concepts to be more semantically related when there is more overlap in their glosses. It is noteworthy that they overcome the problem of short-term word loss by giving the glosses of concepts related to the target terms a higher-order term syllabus more semantically related when there is more overlap in their glosses."}, {"heading": "2.3 Challenges", "text": "This year, it has come to the point where it will be able to leave the country without having to deal with a country in which it is a country in which it is a country in which it is a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "3. Knowledge-lean, distributional approaches to semantic distance", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The distributional hypotheses: the original and the revised", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to fight, to fight, to move, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "3.2 Corpus-based measures of distributional distance", "text": "We describe specific distributional measures based on the distribution hypotheses; depending on which specific hypotheses they apply, they imitate either semantic similarity or semantic relativism. (The number of dimensions is equal to the size of the word (SoA). (The number of words is equal to the size of the word (SoA, x).) (The vectors that match two words in this space are close together, and thus they get a low distribution result if they share many common words and the co-occurring10Mohammad and Hirst distributional measures of semantic distance words or less the same strength of association with the two target words."}, {"heading": "4. The anatomy of a distributional measure", "text": "Although there are numerous distribution scales, many of which seem dramatically different from each other, all distribution scales fulfill two functions: (1) create distribution profiles (DPs) and (2) calculate the distance between two DPs. The distribution profile of a word is the strength of association between it and each of the lexical, syntactic and / or semantic units that accompany it. Frequently used distribution scales of a word (lexical DPW) are conditional probability (0 to 1) and targeted mutual information (\u2212 \u221e to \u221e). Frequently used units of coexistence with the target are other words, and so we speak of the lexical distribution profile of a word (lexical DPW). Common occurring words can be all those in a predetermined window around the target, or limited to those that have a specific syntactical (e.g. distributive scale object) or semantic relationship."}, {"heading": "4.1 Simple co-occurrences versus syntactically related words", "text": "Harris (1968), one of the early advocates of the distribution hypothesis, used syntactically related words to represent the context of a word. However, the strength of the association of a word that appears in the context of the target words can be used to determine their distributional similarity. Dagan, Lee and Pereira (1997), Lee (1999) and Weeds (2003) represent the context of a noun with verbs whose object it is (single syntactical relationship), Hindle (1990) represents the context of a noun with verbs with which it shares the verb-object or subject-verb relationship, while Lin (1998b) uses words that refer to a noun by using one of the many distinct syntactical relationships underlying it to determine the distributional similarity. Sagittarius and Pedersen (1997) and Yoshida, Yukawa and Kuwabara (2003) use all of the syntactic words common in 2007 to determine the multiple tactical resemblance (although the resemblance is almost relatable)."}, {"heading": "4.2 Compositionality", "text": "The different measures of distributional similarity are not calculable, and the final distance cannot be distinguished. In certain measures, each word occurring side by side contributes to a limited, calculable distribution range between the target words. (The final measures of the distribution distance are the sum of these measures.) The relative entropy-based measures, L1 norm and L2 norm, fall into this category. On the other hand, the cosmic measure, along with Hindle and Lin, is in the category of what we call non-compositional measures. Each word occurring together contributes to a result shared by both target words contributing to the numerical system. (The words occurring in conjunction with only one of the two target words contribute only one of the two target words.) The ratio is calculated as soon as all words occurring together are considered."}, {"heading": "4.3 Predictors of Semantic Relatedness", "text": "Given a pair of target words, the vocabulary can be divided into three groups: (1) the group of words that (jointly) occur with both target words; (2) words that (jointly) occur with exactly one of the two target words; (3) words that (jointly) occur with neither target word. Hindle (1990) uses evidence only from words that (jointly) occur with both target words to determine the distribution similarity; all other measures discussed so far in this paper also use words that (jointly) occur with only one target word. It can be argued that the more common occurrences between two words, the more they are related to each other. For example, drink and sip can be considered related because they have a number of common occurrences, such as water, tea, and so on. Similarly, drink and chess can be considered unrelated as words that do not occur with the other."}, {"heading": "4.4 Capitalizing on asymmetry", "text": "Given a hypernym-hyponym pair (auto-car, say) asymmetric distribution measures such as the Kullback-Leibler divergence, \u03b1-skew divergence, and the CRMs generate different values such as the distribution distance of w1 with w2 compared to that of w2 with w1. Normally, if w1 is a more general concept than w2, the measures w1 find more distributionally similar than w2 than vice versa (see (Mirkin, Dagan, and Geffet 2007) for working on lexical contexts using the KullbackLeibler divergence).Weeds (2003) argues that this behavior is intuitive, since it is more often acceptable to replace a generic concept rather than a specific one, and substitutability is an indicator of semantic similarity."}, {"heading": "4.5 Summarizing the distributional measures", "text": "26M o h am m ad an d H irstD istrib u (wP) w (wP) w (wP) w (w1) w (w1) w (w2) p (w2) p (w1) p (w1) p (w1) p (w1) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (w2) p (wp) p (w2) p (w2) p (wp) p (w2) p (wp) p (w2) p (p) p (w2) p (p) p (w2) p (p) p (w2) p (p) p (w2) p (w2) p (p) p (w2) p (w2) p (w2) p (w2) p (w2) p (p (w2) p (w2) p (w2) p (p (w2) p (p (w2) p (w2) p (p (w2) p (p (w2) p (w2) p (p (w2) p (p (w2) p (p (w2) p (w2) p (p (p (w2) p (w2) p (p (w2) p (p (w2) p (w2) p (w2) p (p (w2) p (p (w2) p (p (p) p (p) p (p (w2) p (w2) p (w2) p (w2) p (p (p) p (w2) p (p (p) p (p (w2) p) p (w2) p (1) p (w2) p (w2) p (w2) p (p (1) p (1) p (1) p (1) p (1) p ("}, {"heading": "4.6 Challenges", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "5. A hybrid approach: Distributional measures of concept-distance", "text": "So far, we have looked at approaches that exploit the structure of a resource like WordNet, and corpus-based distribution approaches that make use of co-event statistics. A hybrid approach to semantic distance is one that reconciles the two by combining information about concepts explicitly encoded in a linguistic resource with information about words implicitly encoded by co-occurrences in the text. Mohammad and Hirst (2006b) and Mohammad et al. (2007) have proposed such an approach that combines corpus statistics with a published thesaurus to give the semantic distance between concepts (and not words)."}, {"heading": "5.1 The distributional hypothesis for concepts", "text": "As explained in Section 4.6.1, when used in different senses, words tend to hold different \"enterprises\" (adjacent words). Consider, for example, the constructed but plausible distribution profile of stars: space 0.21, film 0.16, famous 0.15, light 0.12, constellation 0.11, heat 0.08, rich 0.07, hydrogen 0.07,. Let us observe that there are words that occur with both the CELESTIAL-BODY sense and the CELEBRITY sense of the star. Thus, it is clear that different perceptions of deterioration can have very different distribution profiles (using a single DP for the word means the unification of these profiles. While this may be useful for certain applications, Mohammad and Hirst (2006b) argue that in a number of tasks (including estimating semantic distance) two different distribution profiles of PCs for different distribution capabilities arise."}, {"heading": "5.1.1 A suitable knowledge source and concept inventory.", "text": "Mohammad and Hirst (2006b) use the categories in the Macquarie Thesaurus, 812 overall, as very coarse-grained word-sense perceptions or concepts, as opposed to approaches that use WordNet or other similarly fine-grained sense inventory.11 Their approach to determining the word-concept occurrence (described in the next subsection) requires a set of potentially ambiguous words that collectively represent each concept - for which a thesaurus is a natural choice. Using categories in a thesaurus as concepts means that this approach requires a concept-concept distance matrix of just 812 x 812 x 812 x 812 - much smaller (less than 0.01%) than the matrix required by the WordNet-based and distributional measures."}, {"heading": "5.1.2 Estimating distributional profiles of concepts. A word\u2013category co-occurrence", "text": "It is not as if this is a word populated by a large body............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "5.2 Multilinguality", "text": "In fact, we see ourselves as being able to change the world, and we see ourselves as being able to change the world."}, {"heading": "5.3 Challenges", "text": "Distributional measures of concept-distance have many desirable features of both the knowledge-rich approaches and the strictly corpus-based approaches - they have the High34Mohammad and Hirst Distributional Measures of Semantic Distanceaccuracies of knowledge-rich approach, they can measure both semantic kinship and semantic similarity, and they have a strong corpus dependency that makes them domain adaptable. Furthermore, the approach can no longer measure the distance between words not listed in the thesaurus, which is especially a problem for domain-specific jargon that finds no place in a universally valid source of knowledge."}, {"heading": "6. Conclusion", "text": "A large number of important natural language problems, including machine translation, information extraction, and word sense decoding, can sometimes be considered semantic distance problems. There are numerous measures of semantic distance - those that use a source of knowledge and those that rely on corpora. However, their use in the real world is limited. In this paper, we explored how automatic measures can be more closely aligned with human notions of semantic distance, how they can be applied to a large number of natural language tasks, and how they can even be used for languages that are inadequate in high-quality linguistic resources. Although corpus-based distributions of distance have traditionally performed poorly compared to WordNet-based measures, we have shown that (1) there are a number of reasons that make distribution sizes uniquely attractive, and (2) that their potential cannot yet be fully realized. Distributions sizes can be considered to be applied to both (most of them) and (they can only be applied to most of the texts)."}, {"heading": "Acknowledgments", "text": "We thank Suzanne Stevenson, Gerald Penn and the Computational Linguistics Group of the University of Toronto for helpful discussions. This research is funded by the Natural Sciences and Engineering Research Council of Canada and the University of Toronto."}], "references": [{"title": "Clustering WordNet word senses", "author": ["Agirre", "Eneko", "Oier Lopez de Lacalle Lekuona"], "venue": "In Proceedings of the 1st International Conference on Recent", "citeRegEx": "Agirre et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Agirre et al\\.", "year": 2003}, {"title": "Extended gloss overlaps as a measure of semantic relatedness", "author": ["Banerjee", "Pedersen2003]Banerjee", "Satanjeev", "Ted Pedersen"], "venue": "In Proceedings of the Eighteenth International Joint Conference", "citeRegEx": "Banerjee et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2003}, {"title": "Evaluating WordNet-based measures of semantic distance", "author": ["Budanitsky", "Hirst2006]Budanitsky", "Alexander", "Graeme Hirst"], "venue": "Computational Linguistics,", "citeRegEx": "Budanitsky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Budanitsky et al\\.", "year": 2006}, {"title": "Word association norms, mutual information and lexicography", "author": ["Church", "Hanks1990]Church", "Kenneth W", "Patrick Hanks"], "venue": "Computational Linguistics,", "citeRegEx": "Church et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Church et al\\.", "year": 1990}, {"title": "From Distributional to Semantic Similarity", "author": ["R. James"], "venue": "Ph.D. thesis, School of Informatics,", "citeRegEx": "James,? \\Q2004\\E", "shortCiteRegEx": "James", "year": 2004}, {"title": "Similarity-based estimation of word cooccurrence probabilities", "author": ["Dagan", "Lee", "Pereira1994]Dagan", "Ido", "Lillian Lee", "Fernando Pereira"], "venue": "In Proceedings of the 32nd Annual Meeting of the Association", "citeRegEx": "Dagan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1994}, {"title": "Similarity-based methods for word sense disambiguation", "author": ["Dagan", "Lee", "Pereira1997]Dagan", "Ido", "Lillian Lee", "Fernando Pereira"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1997}, {"title": "Similarity-based models of cooccurrence probabilities.Machine Learning, 34(1\u20133):43\u201369", "author": ["Dagan", "Lee", "Pereira1999]Dagan", "Ido", "Lillian Lee", "Fernando Pereira"], "venue": null, "citeRegEx": "Dagan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1999}, {"title": "Contextual word similarity and estimation from sparse data", "author": ["Dagan", "Marcus", "Markovitch1995] Dagan", "Ido", "Shaul Marcus", "Shaul Markovitch"], "venue": "Computer Speech and Language,", "citeRegEx": "Dagan et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1995}, {"title": "A synopsis of linguistic theory 1930\u201355", "author": ["R. John"], "venue": null, "citeRegEx": "John,? \\Q1957\\E", "shortCiteRegEx": "John", "year": 1957}, {"title": "Scaling distributional similarity to large corpora", "author": ["Gorman", "Curran2006]Gorman", "James", "James R. Curran"], "venue": "In Proceedings of the 21st International Conference on Computational Linguistics", "citeRegEx": "Gorman et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Gorman et al\\.", "year": 2006}, {"title": "Sextant: Exploring unexplored contexts for semantic extraction from syntactic analysis", "author": ["Grefenstette1992]Grefenstette", "Gregory"], "venue": "In Proceedings of the 30th Annual Meeting of the Association", "citeRegEx": "Grefenstette1992.Grefenstette and Gregory.,? \\Q1992\\E", "shortCiteRegEx": "Grefenstette1992.Grefenstette and Gregory.", "year": 1992}, {"title": "Using the structure of a conceptual network in computing semantic relatedness", "author": ["Gurevych2005]Gurevych", "Iryna"], "venue": "In Proceedings of the 2nd International Joint Conference on Natural Language Processing", "citeRegEx": "Gurevych2005.Gurevych and Iryna.,? \\Q2005\\E", "shortCiteRegEx": "Gurevych2005.Gurevych and Iryna.", "year": 2005}, {"title": "Lexis as a linguistic level", "author": ["K. Michael A"], "venue": "In memory of J.R. Firth,", "citeRegEx": "A.,? \\Q1966\\E", "shortCiteRegEx": "A.", "year": 1966}, {"title": "Overview of the first text retrieval conference", "author": ["Harman1993]Harman", "Donna"], "venue": "In Proceedings of the 16th Annual International Association for Computing Machinery Special Interest Group", "citeRegEx": "Harman1993.Harman and Donna.,? \\Q1993\\E", "shortCiteRegEx": "Harman1993.Harman and Donna.", "year": 1993}, {"title": "Noun classification from predicate-argument structures", "author": ["Hindle1990]Hindle", "Donald"], "venue": "In Proceedings of the 28th Annual Meeting of the Association of Computational Linguistics", "citeRegEx": "Hindle1990.Hindle and Donald.,? \\Q1990\\E", "shortCiteRegEx": "Hindle1990.Hindle and Donald.", "year": 1990}, {"title": "Correcting real-word spelling errors by restoring lexical cohesion", "author": ["Hirst", "Budanitsky2005]Hirst", "Graeme", "Alexander Budanitsky"], "venue": "Natural Language Engineering,", "citeRegEx": "Hirst et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hirst et al\\.", "year": 2005}, {"title": "Lexical chains as representations of context for the detection and correction of malapropisms", "author": ["Hirst", "St-Onge1998]Hirst", "Graeme", "David St-Onge"], "venue": null, "citeRegEx": "Hirst et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Hirst et al\\.", "year": 1998}, {"title": "Acquiring collocations for lexical choice between near-synonyms", "author": ["Inkpen", "Hirst2002]Inkpen", "Diana", "Graeme Hirst"], "venue": "In SIGLEXWorkshop on Unsupervised Lexical Acquisition,", "citeRegEx": "Inkpen et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Inkpen et al\\.", "year": 2002}, {"title": "Roget\u2019s Thesaurus and semantic similarity", "author": ["Jarmasz", "Szpakowicz2003]Jarmasz", "Mario", "Stan Szpakowicz"], "venue": "In Proceedings of the International Conference on Recent Advances in Natural Language", "citeRegEx": "Jarmasz et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Jarmasz et al\\.", "year": 2003}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["Jiang", "Conrath1997]Jiang", "Jay J", "David W. Conrath"], "venue": "In Proceedings of International Conference on Research", "citeRegEx": "Jiang et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 1997}, {"title": "Introduction to latent semantic analysis", "author": ["Landauer", "Foltz", "Laham1998]Landauer", "Thomas K", "Peter W. Foltz", "Darrell Laham"], "venue": "Discourse Processes,", "citeRegEx": "Landauer et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1998}, {"title": "Combining local context and WordNet similarity for word sense identification", "author": ["Leacock", "Chodorow1998]Leacock", "Claudia", "Martin Chodorow"], "venue": null, "citeRegEx": "Leacock et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Leacock et al\\.", "year": 1998}, {"title": "Measures of distributional similarity", "author": ["Lee1999]Lee", "Lillian"], "venue": "In Proceedings of the 37th conference on Association for Computational Linguistics", "citeRegEx": "Lee1999.Lee and Lillian.,? \\Q1999\\E", "shortCiteRegEx": "Lee1999.Lee and Lillian.", "year": 1999}, {"title": "On the effectiveness of the skew divergence for statistical language analysis", "author": ["Lee2001]Lee", "Lillian"], "venue": "In Proceedings of the Eigth International Workshop on Artificial Intelligence and Statistics", "citeRegEx": "Lee2001.Lee and Lillian.,? \\Q2001\\E", "shortCiteRegEx": "Lee2001.Lee and Lillian.", "year": 2001}, {"title": "Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone", "author": ["Lesk1986]Lesk", "Michael"], "venue": "In Proceedings of the 5th Annual International Conference", "citeRegEx": "Lesk1986.Lesk and Michael.,? \\Q1986\\E", "shortCiteRegEx": "Lesk1986.Lesk and Michael.", "year": 1986}, {"title": "Identifying synonyms among distributionally similar words", "author": ["Lin et al.2003]Lin", "Dekang", "Shaojun Zhao", "Lijuan Qin", "Ming Zhou"], "venue": "In Proceedings of the 18th International Joint", "citeRegEx": "al.2003.Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al.2003.Lin et al\\.", "year": 2003}, {"title": "Unsupervised acquisition of predominant word senses", "author": ["McCarthy et al.2007]McCarthy", "Diana", "Rob Koeling", "Julie Weeds", "John Carroll"], "venue": "Computational linguistics,", "citeRegEx": "al.2007.McCarthy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al.2007.McCarthy et al\\.", "year": 2007}, {"title": "Contextual correlates of semantic similarity", "author": ["Miller", "Charles1991]Miller", "George A", "Walter G. Charles"], "venue": "Language and Cognitive Processes,", "citeRegEx": "Miller et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1991}, {"title": "Integrating pattern-based and distributional similarity methods for lexical entailment acquisition", "author": ["Mirkin", "Dagan", "Geffet2007]Mirkin", "Shachar", "Ido Dagan", "Maayan Geffet"], "venue": null, "citeRegEx": "Mirkin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mirkin et al\\.", "year": 2007}, {"title": "Cross-lingual distributional profiles of concepts for measuring semantic distance", "author": ["Mohammad et al.2007]Mohammad", "Saif", "Iryna Gurevych", "Graeme Hirst", "Torsten Zesch"], "venue": null, "citeRegEx": "al.2007.Mohammad et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al.2007.Mohammad et al\\.", "year": 2007}, {"title": "Determining word sense dominance using a thesaurus", "author": ["Mohammad", "Hirst2006a]Mohammad", "Saif", "Graeme Hirst"], "venue": "In Proceedings of the 11th Conference of the European Chapter of the Association", "citeRegEx": "Mohammad et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2006}, {"title": "Distributional measures of concept-distance: A task-oriented evaluation", "author": ["Mohammad", "Hirst2006b]Mohammad", "Saif", "Graeme Hirst"], "venue": "In Proceedings of the Conference", "citeRegEx": "Mohammad et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2006}, {"title": "Non-classical lexical semantic relations", "author": ["Morris", "Hirst2004]Morris", "Jane", "Graeme Hirst"], "venue": "In Proceedings of the Workshop on Computational Lexical Semantics, Human Language Technology", "citeRegEx": "Morris et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Morris et al\\.", "year": 2004}, {"title": "Using measures of semantic relatedness for word sense disambiguation", "author": ["Patwardhan", "Banerjee", "Pedersen2003] Patwardhan", "Siddharth", "Satanjeev Banerjee", "Ted Pedersen"], "venue": null, "citeRegEx": "Patwardhan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Patwardhan et al\\.", "year": 2003}, {"title": "Experience with a stack decoder-based HMM CSR and back-off n-gram language models", "author": ["B. Douglas"], "venue": "In Proceedings of the Speech and Natural Language Workshop,", "citeRegEx": "Douglas,? \\Q1991\\E", "shortCiteRegEx": "Douglas", "year": 1991}, {"title": "Distributional clustering of English words", "author": ["Pereira", "Tishby", "Lee1993]Pereira", "Fernando", "Naftali Tishby", "Lillian Lee"], "venue": "In Proceedings of the 31st Annual Meeting of the Association of Computational", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Development and application of a metric on semantic nets", "author": ["Rada et al.1989]Rada", "Roy", "Hafedh Mili", "Ellen Bicknell", "Maria Blettner"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "al.1989.Rada et al\\.,? \\Q1989\\E", "shortCiteRegEx": "al.1989.Rada et al\\.", "year": 1989}, {"title": "Word sense discovery based on sense descriptor dissimilarity", "author": ["Rapp2003]Rapp", "Reinhard"], "venue": "In Proceedings of the Machine Translation Summit IX,", "citeRegEx": "Rapp2003.Rapp and Reinhard.,? \\Q2003\\E", "shortCiteRegEx": "Rapp2003.Rapp and Reinhard.", "year": 2003}, {"title": "Using information content to evaluate semantic similarity", "author": ["Resnik1995]Resnik", "Philip"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence", "citeRegEx": "Resnik1995.Resnik and Philip.,? \\Q1995\\E", "shortCiteRegEx": "Resnik1995.Resnik and Philip.", "year": 1995}, {"title": "WordNet and class-based probabilities", "author": ["Resnik1998]Resnik", "Philip"], "venue": null, "citeRegEx": "Resnik1998.Resnik and Philip.,? \\Q1998\\E", "shortCiteRegEx": "Resnik1998.Resnik and Philip.", "year": 1998}, {"title": "Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language", "author": ["Resnik1999]Resnik", "Philip"], "venue": null, "citeRegEx": "Resnik1999.Resnik and Philip.,? \\Q1999\\E", "shortCiteRegEx": "Resnik1999.Resnik and Philip.", "year": 1999}, {"title": "Measuring verb similarity", "author": ["Resnik", "Diab2000]Resnik", "Philip", "Mona Diab"], "venue": "In Proceedings of the 22nd Annual Meeting of the Cognitive Science Society (CogSci", "citeRegEx": "Resnik et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Resnik et al\\.", "year": 2000}, {"title": "Contextual correlates of synonymy", "author": ["Rubenstein", "Goodenough1965] Rubenstein", "Herbert", "John B. Goodenough"], "venue": "Communications of the", "citeRegEx": "Rubenstein et al\\.,? \\Q1965\\E", "shortCiteRegEx": "Rubenstein et al\\.", "year": 1965}, {"title": "A cooccurrence-based thesaurus and two applications to information retreival", "author": ["Sch\u00fctze", "Pedersen1997]Sch\u00fctze", "Hinrich", "Jan O. Pedersen"], "venue": "Information Processing and Management,", "citeRegEx": "Sch\u00fctze et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Sch\u00fctze et al\\.", "year": 1997}, {"title": "Mining the Web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["Turney2001]Turney", "Peter"], "venue": "In Proceedings of the Twelfth European Conference on Machine Learning", "citeRegEx": "Turney2001.Turney and Peter.,? \\Q2001\\E", "shortCiteRegEx": "Turney2001.Turney and Peter.", "year": 2001}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Weeds", "Weir", "McCarthy2004]Weeds", "Julie", "David Weir", "Diana McCarthy"], "venue": "In Proceedings of the 20th International Conference", "citeRegEx": "Weeds et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}, {"title": "Measures and Applications of Lexical Distributional Similarity", "author": ["E. Julie"], "venue": "Ph.D. thesis, Department of Informatics,", "citeRegEx": "Julie,? \\Q2003\\E", "shortCiteRegEx": "Julie", "year": 2003}, {"title": "Measuring semantic similarity in the taxonomy of WordNet", "author": ["Yang", "Powers2005]Yang", "Dongqiang", "David Powers"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2005}, {"title": "Verb similarity on the taxonomy of WordNet", "author": ["Yang", "Powers2006]Yang", "Dongqiang", "David Powers"], "venue": "In Proceedings of the Third International WordNet Conference", "citeRegEx": "Yang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2006}, {"title": "Constructing and examining personalized cooccurrence-based thesauri on web", "author": ["Yoshida", "Yukawa", "Kuwabara2003] Yoshida", "Sen", "Takashi Yukawa", "Kazuhiro Kuwabara"], "venue": null, "citeRegEx": "Yoshida et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yoshida et al\\.", "year": 2003}, {"title": "Comparing Wikipedia and German WordNet by evaluating semantic relatedness on multiple datasets", "author": ["Zesch", "Gurevych", "M\u00fchlh\u00e4user2007] Zesch", "Torsten", "Iryna Gurevych", "Max M\u00fchlh\u00e4user"], "venue": null, "citeRegEx": "Zesch et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zesch et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 13, "context": "According to Cruse (1986), a lexical semantic relation is a relation between lexical units\u2014a surface form along with a sense.", "startOffset": 0, "endOffset": 26}, {"referenceID": 13, "context": "According to Cruse (1986), a lexical semantic relation is a relation between lexical units\u2014a surface form along with a sense. As he points out, the number of semantic relations that bind concepts is innumerable; but certain relations, such as hyponymy, meronymy, antonymy, and troponymy, are more systematic and have enjoyed more attention in the linguistics community. However, as Morris and Hirst (2004) point out, these relations are far out-numbered by others, which they call non-classical relations.", "startOffset": 0, "endOffset": 406}, {"referenceID": 13, "context": "Humans are adept at estimating semantic distance; but consider the following questions: How strongly will two people agree/disagree on distance estimates? Will the agreement vary over different sets of concepts? Are we equally good at estimating semantic similarity and semantic relatedness? In our minds, is there a clear distinction between related and unrelated concepts or are concept-pairs spread across the whole range from synonymous to unrelated? Some of the earliest work that begins to address these questions is by Rubenstein and Goodenough (1965). They conducted quantitative experiments with human subjects (51 in all) who were asked to rate 65 English word pairs on a scale from 0.", "startOffset": 212, "endOffset": 559}, {"referenceID": 13, "context": "Humans are adept at estimating semantic distance; but consider the following questions: How strongly will two people agree/disagree on distance estimates? Will the agreement vary over different sets of concepts? Are we equally good at estimating semantic similarity and semantic relatedness? In our minds, is there a clear distinction between related and unrelated concepts or are concept-pairs spread across the whole range from synonymous to unrelated? Some of the earliest work that begins to address these questions is by Rubenstein and Goodenough (1965). They conducted quantitative experiments with human subjects (51 in all) who were asked to rate 65 English word pairs on a scale from 0.0 to 4.0 as per their semantic distance. The word pairs chosen ranged from almost synonymous to unrelated. However, they were all noun pairs and those that were semantically close were also semantically similar; the dataset did not contain word pairs that are semantically related but not semantically similar. The subjects repeated the annotation after two weeks and the new distance values had a Pearson\u2019s correlation r of 0.85 with the old ones. Miller and Charles (1991) also conducted a similar study on 30 word pairs taken from the Rubenstein-Goodenough pairs.", "startOffset": 212, "endOffset": 1170}, {"referenceID": 13, "context": "Humans are adept at estimating semantic distance; but consider the following questions: How strongly will two people agree/disagree on distance estimates? Will the agreement vary over different sets of concepts? Are we equally good at estimating semantic similarity and semantic relatedness? In our minds, is there a clear distinction between related and unrelated concepts or are concept-pairs spread across the whole range from synonymous to unrelated? Some of the earliest work that begins to address these questions is by Rubenstein and Goodenough (1965). They conducted quantitative experiments with human subjects (51 in all) who were asked to rate 65 English word pairs on a scale from 0.0 to 4.0 as per their semantic distance. The word pairs chosen ranged from almost synonymous to unrelated. However, they were all noun pairs and those that were semantically close were also semantically similar; the dataset did not contain word pairs that are semantically related but not semantically similar. The subjects repeated the annotation after two weeks and the new distance values had a Pearson\u2019s correlation r of 0.85 with the old ones. Miller and Charles (1991) also conducted a similar study on 30 word pairs taken from the Rubenstein-Goodenough pairs. These annotations had a high correlation (r = 0.97) with the mean annotations of Rubenstein and Goodenough (1965). Resnik (1999) repeated these experiments and found the inter-annotator correlation (r) to be 0.", "startOffset": 212, "endOffset": 1376}, {"referenceID": 13, "context": "Humans are adept at estimating semantic distance; but consider the following questions: How strongly will two people agree/disagree on distance estimates? Will the agreement vary over different sets of concepts? Are we equally good at estimating semantic similarity and semantic relatedness? In our minds, is there a clear distinction between related and unrelated concepts or are concept-pairs spread across the whole range from synonymous to unrelated? Some of the earliest work that begins to address these questions is by Rubenstein and Goodenough (1965). They conducted quantitative experiments with human subjects (51 in all) who were asked to rate 65 English word pairs on a scale from 0.0 to 4.0 as per their semantic distance. The word pairs chosen ranged from almost synonymous to unrelated. However, they were all noun pairs and those that were semantically close were also semantically similar; the dataset did not contain word pairs that are semantically related but not semantically similar. The subjects repeated the annotation after two weeks and the new distance values had a Pearson\u2019s correlation r of 0.85 with the old ones. Miller and Charles (1991) also conducted a similar study on 30 word pairs taken from the Rubenstein-Goodenough pairs. These annotations had a high correlation (r = 0.97) with the mean annotations of Rubenstein and Goodenough (1965). Resnik (1999) repeated these experiments and found the inter-annotator correlation (r) to be 0.", "startOffset": 212, "endOffset": 1391}, {"referenceID": 13, "context": "1 Ameasure of closeness can be easily converted to a measure of distance by applying a suitable inverse function, or vice versa. Two classes of automatic methods have been traditionally used to determine semantic distance. Knowledge-rich measures of concept-distance, such as those of Jiang and Conrath (1997), Leacock and Chodorow (1998), and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it.", "startOffset": 2, "endOffset": 310}, {"referenceID": 13, "context": "1 Ameasure of closeness can be easily converted to a measure of distance by applying a suitable inverse function, or vice versa. Two classes of automatic methods have been traditionally used to determine semantic distance. Knowledge-rich measures of concept-distance, such as those of Jiang and Conrath (1997), Leacock and Chodorow (1998), and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it.", "startOffset": 2, "endOffset": 339}, {"referenceID": 13, "context": "1 Ameasure of closeness can be easily converted to a measure of distance by applying a suitable inverse function, or vice versa. Two classes of automatic methods have been traditionally used to determine semantic distance. Knowledge-rich measures of concept-distance, such as those of Jiang and Conrath (1997), Leacock and Chodorow (1998), and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it.", "startOffset": 2, "endOffset": 358}, {"referenceID": 13, "context": "1 Ameasure of closeness can be easily converted to a measure of distance by applying a suitable inverse function, or vice versa. Two classes of automatic methods have been traditionally used to determine semantic distance. Knowledge-rich measures of concept-distance, such as those of Jiang and Conrath (1997), Leacock and Chodorow (1998), and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it.2 Distributional measures of word-distance (knowledgelean measures), such as cosine and \u03b1-skew divergence (Lee 2001), rely on the distributional hypothesis, which states that two words tend to be semantically close if they occur in similar contexts (Firth 1957). Distributional measures rely simply on text (and possibly some shallow syntactic processing) and can give the distance between any two words that occur at least a few times. The various WordNet-based measures have been widely studied (Budanitsky and Hirst 2006; Patwardhan, Banerjee, and Pedersen 2003). The study of distributional measures on the whole has received much less attention.3 Even though, as Weeds (2003) and Mohammad and Hirst (2006b) show, they perform poorly when compared to WordNet-based measures, the distributional measures of word-distance have many attractive features, including their ability to measure both semantic similarity and semantic relatedness.", "startOffset": 2, "endOffset": 1163}, {"referenceID": 13, "context": "1 Ameasure of closeness can be easily converted to a measure of distance by applying a suitable inverse function, or vice versa. Two classes of automatic methods have been traditionally used to determine semantic distance. Knowledge-rich measures of concept-distance, such as those of Jiang and Conrath (1997), Leacock and Chodorow (1998), and Resnik (1995), rely on the structure of a knowledge source, such as WordNet, to determine the distance between two concepts defined in it.2 Distributional measures of word-distance (knowledgelean measures), such as cosine and \u03b1-skew divergence (Lee 2001), rely on the distributional hypothesis, which states that two words tend to be semantically close if they occur in similar contexts (Firth 1957). Distributional measures rely simply on text (and possibly some shallow syntactic processing) and can give the distance between any two words that occur at least a few times. The various WordNet-based measures have been widely studied (Budanitsky and Hirst 2006; Patwardhan, Banerjee, and Pedersen 2003). The study of distributional measures on the whole has received much less attention.3 Even though, as Weeds (2003) and Mohammad and Hirst (2006b) show, they perform poorly when compared to WordNet-based measures, the distributional measures of word-distance have many attractive features, including their ability to measure both semantic similarity and semantic relatedness.", "startOffset": 2, "endOffset": 1194}, {"referenceID": 13, "context": "1 A note about terminology: In many contexts, the term distance measures refers to the complete set of measures (irrespective of what the different ends of the range signify). In certain other contexts (as in this paragraph), distance measures refers only to those measures that give larger values to signify greater distance. The context, usually by its reference to this numeric property or lack thereof will make clear the intended meaning of the term. 2 The nodes in WordNet (synsets) represent word senses and edges between nodes represent semantic relations such as hyponymy and meronymy. 3 See Curran (2004) and Weeds, Weir, and McCarthy (2004) for other work that compares various distributional measures.", "startOffset": 2, "endOffset": 615}, {"referenceID": 13, "context": "1 A note about terminology: In many contexts, the term distance measures refers to the complete set of measures (irrespective of what the different ends of the range signify). In certain other contexts (as in this paragraph), distance measures refers only to those measures that give larger values to signify greater distance. The context, usually by its reference to this numeric property or lack thereof will make clear the intended meaning of the term. 2 The nodes in WordNet (synsets) represent word senses and edges between nodes represent semantic relations such as hyponymy and meronymy. 3 See Curran (2004) and Weeds, Weir, and McCarthy (2004) for other work that compares various distributional measures.", "startOffset": 2, "endOffset": 652}, {"referenceID": 13, "context": "As per Resnik\u2019s formula, given a particular lowest superordinate, the exact positions of the target nodes below it in the hierarchy do not have any effect on the semantic similarity. Intuitively, we would expect that word pairs closer to the lso are more semantically similar than those that are distant. Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms.", "startOffset": 0, "endOffset": 330}, {"referenceID": 13, "context": "As per Resnik\u2019s formula, given a particular lowest superordinate, the exact positions of the target nodes below it in the hierarchy do not have any effect on the semantic similarity. Intuitively, we would expect that word pairs closer to the lso are more semantically similar than those that are distant. Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms.", "startOffset": 0, "endOffset": 345}, {"referenceID": 13, "context": "As per Resnik\u2019s formula, given a particular lowest superordinate, the exact positions of the target nodes below it in the hierarchy do not have any effect on the semantic similarity. Intuitively, we would expect that word pairs closer to the lso are more semantically similar than those that are distant. Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms. The Jiang and Conrath (1997) measure (JC) determines how dissimilar each target concept is from the lso (IC(c1)\u2212 IC(lso(c1, c2)) and IC(c2)\u2212 IC(lso(c1, c2))).", "startOffset": 0, "endOffset": 469}, {"referenceID": 13, "context": "As per Resnik\u2019s formula, given a particular lowest superordinate, the exact positions of the target nodes below it in the hierarchy do not have any effect on the semantic similarity. Intuitively, we would expect that word pairs closer to the lso are more semantically similar than those that are distant. Jiang and Conrath (1997) and Lin (1997) incorporate this notion into their measures which are arithmetic variations of the same terms. The Jiang and Conrath (1997) measure (JC) determines how dissimilar each target concept is from the lso (IC(c1)\u2212 IC(lso(c1, c2)) and IC(c2)\u2212 IC(lso(c1, c2))). The final semantic distance between the two concepts is then taken to be the sum of these differences. Lin (1997) (like Resnik) points out that the lso is what is common between the two target concepts and that its information content is the common information between the two concepts.", "startOffset": 0, "endOffset": 713}, {"referenceID": 13, "context": "All of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs.", "startOffset": 0, "endOffset": 218}, {"referenceID": 13, "context": "All of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs.", "startOffset": 0, "endOffset": 245}, {"referenceID": 13, "context": "All of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs. Resnik and Diab (2000) ported several measures which are traditionally applied on the noun hypernymy/hyponymy network (edge counting, and the measures of Resnik (1995), and Lin (1997)) to the relatively shallow verb troponymy network.", "startOffset": 0, "endOffset": 313}, {"referenceID": 13, "context": "All of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs. Resnik and Diab (2000) ported several measures which are traditionally applied on the noun hypernymy/hyponymy network (edge counting, and the measures of Resnik (1995), and Lin (1997)) to the relatively shallow verb troponymy network.", "startOffset": 0, "endOffset": 458}, {"referenceID": 13, "context": "All of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs. Resnik and Diab (2000) ported several measures which are traditionally applied on the noun hypernymy/hyponymy network (edge counting, and the measures of Resnik (1995), and Lin (1997)) to the relatively shallow verb troponymy network.", "startOffset": 0, "endOffset": 474}, {"referenceID": 13, "context": "All of the approaches described above rely heavily (if not solely) on the hypernymy/hyponymy network in WordNet; they are designed for, and evaluated on, noun\u2013noun pairs. However, more recently, Resnik and Diab (2000) and Yang and Powers (2006) developed measures aimed at verb\u2013verb pairs. Resnik and Diab (2000) ported several measures which are traditionally applied on the noun hypernymy/hyponymy network (edge counting, and the measures of Resnik (1995), and Lin (1997)) to the relatively shallow verb troponymy network. The two information content\u2013based measures ranked a carefully chosen set of 48 verbs best in order of their semantic distance.4 Yang and Powers (2006) ported their earlier work on nouns (Yang and Powers 2005) to verbs.", "startOffset": 0, "endOffset": 676}, {"referenceID": 13, "context": "As Morris and Hirst (2004) pointed out, a large number of concept pairs, such as STRAWBERRY\u2013CREAM and DOCTOR\u2013SCALPEL, have a non-classical relation between them (STRAWBERRIES are usually eaten with CREAM and a DOCTOR uses a SCALPEL to make an incision).", "startOffset": 0, "endOffset": 27}, {"referenceID": 13, "context": "For example, SPACE and TIME are close in the domain of quantum mechanics but not so much in most others. Ontologies have beenmade for specific domains, which may be used to determine semantic similarity specific to these domains. However, the number of such ontologies is very limited. Some of the more successful WordNet-based measures, such as that of Jiang and Conrath (1997), that rely also on text, do indeed capture domain-specificity to some extent, but the distance values are still largely shaped by the underlying network, which is not domain-specific.", "startOffset": 15, "endOffset": 379}, {"referenceID": 13, "context": "The set of contexts of a target word is usually represented by the set of words in these contexts, their strength of association (SoA) with the target word, and possibly their syntactic relation with the target, for example verb\u2013object, subject\u2013verb, and so on. The strength of cooccurrence association between the target and another word quantifies howmuch more (or less) than chance the two words occur together in text. Commonly used measures of association are conditional probability (CP) and pointwise mutual information (PMI). The distance between the sets of contexts of two target words can be used as a proxy for their semantic distance, as words found in similar contexts tend to be semantically similar\u2014the distributional hypothesis (Firth 1957; Harris 1968). The hypothesis makes intuitive sense, as Budanitsky and Hirst (2006) point out: If two words have many co-occurring words in common, then similar things are being said about both of them and so they are likely to be semantically similar.", "startOffset": 132, "endOffset": 841}, {"referenceID": 13, "context": "The set of contexts of a target word is usually represented by the set of words in these contexts, their strength of association (SoA) with the target word, and possibly their syntactic relation with the target, for example verb\u2013object, subject\u2013verb, and so on. The strength of cooccurrence association between the target and another word quantifies howmuch more (or less) than chance the two words occur together in text. Commonly used measures of association are conditional probability (CP) and pointwise mutual information (PMI). The distance between the sets of contexts of two target words can be used as a proxy for their semantic distance, as words found in similar contexts tend to be semantically similar\u2014the distributional hypothesis (Firth 1957; Harris 1968). The hypothesis makes intuitive sense, as Budanitsky and Hirst (2006) point out: If two words have many co-occurring words in common, then similar things are being said about both of them and so they are likely to be semantically similar. Conversely, if two words are semantically similar, then they are likely to be used in a similar fashion in text and thus end up with many common co-occurrences. For example, the semantically similar bug and insect are expected to have a number of common co-occurring words such as crawl, squash, small, woods, and so on, in a large-enough text corpus. The distributional hypothesis only mentions semantic similarity and not semantic relatedness. This, coupled with the fact that the difference between semantic relatedness and semantic similarity is somewhat nuanced and can be missed, meant that almost all work employing the distributional hypothesis was labeled as estimating semantic similarity. However, it should be noted that distributional measures can be used to estimate both semantic similarity and semantic relatedness. Even though Sch\u00fctze and Pedersen (1997) and Landauer, Foltz, and Laham (1998), for example, use the term similarity and not relatedness, their LSA-based distance measures in fact estimate semantic relatedness and not semantic similarity.", "startOffset": 132, "endOffset": 1882}, {"referenceID": 13, "context": "The set of contexts of a target word is usually represented by the set of words in these contexts, their strength of association (SoA) with the target word, and possibly their syntactic relation with the target, for example verb\u2013object, subject\u2013verb, and so on. The strength of cooccurrence association between the target and another word quantifies howmuch more (or less) than chance the two words occur together in text. Commonly used measures of association are conditional probability (CP) and pointwise mutual information (PMI). The distance between the sets of contexts of two target words can be used as a proxy for their semantic distance, as words found in similar contexts tend to be semantically similar\u2014the distributional hypothesis (Firth 1957; Harris 1968). The hypothesis makes intuitive sense, as Budanitsky and Hirst (2006) point out: If two words have many co-occurring words in common, then similar things are being said about both of them and so they are likely to be semantically similar. Conversely, if two words are semantically similar, then they are likely to be used in a similar fashion in text and thus end up with many common co-occurrences. For example, the semantically similar bug and insect are expected to have a number of common co-occurring words such as crawl, squash, small, woods, and so on, in a large-enough text corpus. The distributional hypothesis only mentions semantic similarity and not semantic relatedness. This, coupled with the fact that the difference between semantic relatedness and semantic similarity is somewhat nuanced and can be missed, meant that almost all work employing the distributional hypothesis was labeled as estimating semantic similarity. However, it should be noted that distributional measures can be used to estimate both semantic similarity and semantic relatedness. Even though Sch\u00fctze and Pedersen (1997) and Landauer, Foltz, and Laham (1998), for example, use the term similarity and not relatedness, their LSA-based distance measures in fact estimate semantic relatedness and not semantic similarity.", "startOffset": 132, "endOffset": 1920}, {"referenceID": 13, "context": "As an additional requirement, the target words must have low PMI with words they do not both cooccur with. Second, Hindle uses the minimum of the PMI between each of the target words and the shared co-occurring word, while Lin uses the sum. Taking the sum has the drawback of not penalizing for a mismatch in strength of co-occurrence, as long as w1 and w2 both co-occur with a word. Hindle (1990) used a portion of the Associated Press news stories (6 million words) to classify the nouns into semantically related classes.", "startOffset": 0, "endOffset": 398}, {"referenceID": 13, "context": "As an additional requirement, the target words must have low PMI with words they do not both cooccur with. Second, Hindle uses the minimum of the PMI between each of the target words and the shared co-occurring word, while Lin uses the sum. Taking the sum has the drawback of not penalizing for a mismatch in strength of co-occurrence, as long as w1 and w2 both co-occur with a word. Hindle (1990) used a portion of the Associated Press news stories (6 million words) to classify the nouns into semantically related classes. Lin (1998b) used his measure to generate a distributional thesaurus from a 64-million-word corpus of theWall Street Journal, San Jose Mercury, and AP Newswire.", "startOffset": 0, "endOffset": 537}, {"referenceID": 13, "context": "Use of Kullback-Leibler distance as the semantic distance metric yielded a 20% improvement in perplexity on the Wall Street Journal and dictation corpora provided by ARPA\u2019s HLT program (Paul 1991). It should be noted here that the use of distributionally similar words to estimate unseen bigram probabilities will likely lead to erroneous results in case of less-preferred and strongly-preferred collocations (word pairs). Inkpen and Hirst (2002) point out that even though words like task and job are semantically very similar, the collocations they form with other words may have varying degrees of usage.", "startOffset": 166, "endOffset": 447}, {"referenceID": 13, "context": "Also, the measure retains the asymmetric nature of the Kullback-Leibler divergence. Lee (2001) shows that \u03b1-skew divergence performs better than Kullback-Leibler divergence in estimating word co-occurrence probabilities.", "startOffset": 0, "endOffset": 95}, {"referenceID": 13, "context": "Also, the measure retains the asymmetric nature of the Kullback-Leibler divergence. Lee (2001) shows that \u03b1-skew divergence performs better than Kullback-Leibler divergence in estimating word co-occurrence probabilities. Weeds (2003) achieves a correlation of 0.", "startOffset": 0, "endOffset": 234}, {"referenceID": 13, "context": "4 Latent Semantic Analysis. Landauer, Foltz, and Laham (1998) proposed Latent semantic analysis (LSA), which can be used to determine distributional distance betweenwords or between sets of words.", "startOffset": 18, "endOffset": 62}, {"referenceID": 13, "context": "4 Latent Semantic Analysis. Landauer, Foltz, and Laham (1998) proposed Latent semantic analysis (LSA), which can be used to determine distributional distance betweenwords or between sets of words.8 Unlike the various approaches described earlier where a word\u2013word co-occurrence matrix is created, the first step of LSA involves the creation of a word\u2013paragraph, word\u2013document, or similar such word-passage matrix, where a passage is some grouping of words. A cell for wordw and passage p is populated with the number of times w occurs in p or, for even better results, a function of this frequency that captures how much information the occurrence of the word in a text passage carries. Next, the dimensionality of this matrix is reduced by applying singular value decomposition (SVD), a standard matrix decomposition technique. This smaller set of dimensions represents abstract (unknown) concepts. Then the original word\u2013passage matrix is recreated, but this time from the reduced dimensions. Landauer, Foltz, and Laham (1998) point out that this results in new matrix cell values that are different from what they were before.", "startOffset": 18, "endOffset": 1029}, {"referenceID": 13, "context": "4 Latent Semantic Analysis. Landauer, Foltz, and Laham (1998) proposed Latent semantic analysis (LSA), which can be used to determine distributional distance betweenwords or between sets of words.8 Unlike the various approaches described earlier where a word\u2013word co-occurrence matrix is created, the first step of LSA involves the creation of a word\u2013paragraph, word\u2013document, or similar such word-passage matrix, where a passage is some grouping of words. A cell for wordw and passage p is populated with the number of times w occurs in p or, for even better results, a function of this frequency that captures how much information the occurrence of the word in a text passage carries. Next, the dimensionality of this matrix is reduced by applying singular value decomposition (SVD), a standard matrix decomposition technique. This smaller set of dimensions represents abstract (unknown) concepts. Then the original word\u2013passage matrix is recreated, but this time from the reduced dimensions. Landauer, Foltz, and Laham (1998) point out that this results in new matrix cell values that are different from what they were before. More specifically, words that are expected to occur more often in a passage than what the original cell values reflect are incremented. Then a standard vector distance measure, such as cosine, that captures the distance between distributions of the two target words is applied. LSA was used by Sch\u00fctze and Pedersen (1997), Turney (2001) and Rapp (2003) to measure distributional distance, with encouraging results.", "startOffset": 18, "endOffset": 1452}, {"referenceID": 13, "context": "4 Latent Semantic Analysis. Landauer, Foltz, and Laham (1998) proposed Latent semantic analysis (LSA), which can be used to determine distributional distance betweenwords or between sets of words.8 Unlike the various approaches described earlier where a word\u2013word co-occurrence matrix is created, the first step of LSA involves the creation of a word\u2013paragraph, word\u2013document, or similar such word-passage matrix, where a passage is some grouping of words. A cell for wordw and passage p is populated with the number of times w occurs in p or, for even better results, a function of this frequency that captures how much information the occurrence of the word in a text passage carries. Next, the dimensionality of this matrix is reduced by applying singular value decomposition (SVD), a standard matrix decomposition technique. This smaller set of dimensions represents abstract (unknown) concepts. Then the original word\u2013passage matrix is recreated, but this time from the reduced dimensions. Landauer, Foltz, and Laham (1998) point out that this results in new matrix cell values that are different from what they were before. More specifically, words that are expected to occur more often in a passage than what the original cell values reflect are incremented. Then a standard vector distance measure, such as cosine, that captures the distance between distributions of the two target words is applied. LSA was used by Sch\u00fctze and Pedersen (1997), Turney (2001) and Rapp (2003) to measure distributional distance, with encouraging results.", "startOffset": 18, "endOffset": 1467}, {"referenceID": 13, "context": "4 Latent Semantic Analysis. Landauer, Foltz, and Laham (1998) proposed Latent semantic analysis (LSA), which can be used to determine distributional distance betweenwords or between sets of words.8 Unlike the various approaches described earlier where a word\u2013word co-occurrence matrix is created, the first step of LSA involves the creation of a word\u2013paragraph, word\u2013document, or similar such word-passage matrix, where a passage is some grouping of words. A cell for wordw and passage p is populated with the number of times w occurs in p or, for even better results, a function of this frequency that captures how much information the occurrence of the word in a text passage carries. Next, the dimensionality of this matrix is reduced by applying singular value decomposition (SVD), a standard matrix decomposition technique. This smaller set of dimensions represents abstract (unknown) concepts. Then the original word\u2013passage matrix is recreated, but this time from the reduced dimensions. Landauer, Foltz, and Laham (1998) point out that this results in new matrix cell values that are different from what they were before. More specifically, words that are expected to occur more often in a passage than what the original cell values reflect are incremented. Then a standard vector distance measure, such as cosine, that captures the distance between distributions of the two target words is applied. LSA was used by Sch\u00fctze and Pedersen (1997), Turney (2001) and Rapp (2003) to measure distributional distance, with encouraging results.", "startOffset": 18, "endOffset": 1483}, {"referenceID": 13, "context": "Asymmetry in substitutability is intuitive, as in many cases it may be acceptable to substitute a word, say dog, with another, say animal, but the reverse is not acceptable as often. SinceWeeds uses substitutability as a measure of semantic similarity, she believes that distributional similarity between two words should reflect this property as well. Hence, like the Kullback-Leibler divergence, all her distributional similarity models are asymmetric. Weeds (2003) extracted verb\u2013object pairs of 2,000 nouns from the British National Corpus (BNC).", "startOffset": 0, "endOffset": 468}, {"referenceID": 13, "context": "Although Lin (1998b) shows that the use of multiple syntactic relations is more beneficial as compared to just one, McCarthy et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "Although Lin (1998b) shows that the use of multiple syntactic relations is more beneficial as compared to just one, McCarthy et al. (2007) show that results obtained using just word co-occurrences produced almost as good results as those obtained using syntactically related words.", "startOffset": 0, "endOffset": 139}, {"referenceID": 13, "context": "As Dagan, Marcus, and Markovitch (1995) point out, there is strong evidence for dissimilarity if the strength of association with the other target word is much lower than the maximum, and strong evidence of similarity if the strength of association with both target words is more or less the same.", "startOffset": 0, "endOffset": 40}, {"referenceID": 13, "context": "All the other measures discussed in this paper so far also use words that co-occur with just one target word. One can argue that the more there are common co-occurrences between two words, the more they are related. For example, drink and sip may be considered related as they have a number of common co-occurrences such as water, tea and so on. Similarly, drink and chess can be deemed unrelated as words that co-occur with one do not with the other. For example, water and tea do not usually co-occur with chess, while castle and move are not found close to drink. Measures that use all co-occurrences (common and exclusive) tap into this intuitive notion. However, certain strong exclusive co-occurrences can adversely effect the measure. Consider the classic strong coffee vs powerful coffee example (Halliday (1966)).", "startOffset": 0, "endOffset": 821}, {"referenceID": 31, "context": "Mohammad and Hirst (2006b) and Mohammad et al. (2007) have proposed one such approach that combines corpus statistics with a published thesaurus to give the semantic distance between concepts (rather than words).", "startOffset": 31, "endOffset": 54}, {"referenceID": 13, "context": "Observe that it haswords that co-occur both with star\u2019s CELESTIAL BODY sense and star\u2019s CELEBRITY sense. Thus, it is clear that different senses of a wordmay have very different distributional profiles. Using a single DP for the word will mean the union of those profiles. While this might be useful for certain applications, Mohammad and Hirst (2006b) argue that in a number of tasks (including estimating semantic distance), acquiring different DPs for the different senses is not only more intuitive, but also, as they show through experiments, more useful.", "startOffset": 63, "endOffset": 353}, {"referenceID": 13, "context": "11 It has been suggested for some time now that WordNet is much too fine-grained for certain natural language applications (Agirre and Lopez de Lacalle Lekuona (2003) and citations therein).", "startOffset": 124, "endOffset": 167}, {"referenceID": 31, "context": "Mohammad et al. (2007) showed how text in one language L1 can be combined with a knowledge source in another L2 using a bilingual lexicon L1\u2013L2 and a bootstrapping and concept-disambiguation algorithm to create cross-lingual distributional profiles of concepts.", "startOffset": 0, "endOffset": 23}, {"referenceID": 31, "context": "Given a German word wde in context, Mohammad et al. (2007) use a German\u2013English bilingual lexicon to determine its different possible English translations.", "startOffset": 36, "endOffset": 59}, {"referenceID": 31, "context": "Mohammad et al. (2007) evaluated the cross-lingual measures of semantic distance on two tasks: (1) estimating semantic distance between words and ranking the word pairs according to semantic distance, and (2) solving Reader\u2019s Digest \u2018Word Power\u2019 problems.", "startOffset": 0, "endOffset": 23}, {"referenceID": 31, "context": "Mohammad and Hirst (2006b) and Mohammad et al. (2007) have reported results using the categories of the thesaurus as very coarse word senses.", "startOffset": 31, "endOffset": 54}], "year": 2012, "abstractText": "The ability to mimic human notions of semantic distance has widespread applications. Some measures rely only on raw text (distributional measures) and some rely on knowledge sources such as WordNet. Although extensive studies have been performed to compare WordNet-based measures with human judgment, the use of distributional measures as proxies to estimate semantic distance has received little attention. Even though they have traditionally performed poorly when compared to WordNet-based measures, they lay claim to certain uniquely attractive features, such as their applicability in resource-poor languages and their ability to mimic both semantic similarity and semantic relatedness. Therefore, this paper presents a detailed study of distributional measures. Particular attention is paid to flesh out the strengths and limitations of both WordNet-based and distributional measures, and how distributional measures of distance can be brought more in line with human notions of semantic distance. We conclude with a brief discussion of recent work on hybrid measures.", "creator": "LaTeX with hyperref package"}}}