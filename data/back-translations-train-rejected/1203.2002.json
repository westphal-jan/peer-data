{"id": "1203.2002", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Mar-2012", "title": "Graph partitioning advance clustering technique", "abstract": "Clustering is a common technique for statistical data analysis, Clustering is the process of grouping the data into classes or clusters so that objects within a cluster have high similarity in comparison to one another, but are very dissimilar to objects in other clusters. Dissimilarities are assessed based on the attribute values describing the objects. Often, distance measures are used. Clustering is an unsupervised learning technique, where interesting patterns and structures can be found directly from very large data sets with little or none of the background knowledge. This paper also considers the partitioning of m-dimensional lattice graphs using Fiedler's approach, which requires the determination of the eigenvector belonging to the second smallest Eigenvalue of the Laplacian with K-means partitioning algorithm.", "histories": [["v1", "Fri, 9 Mar 2012 07:08:10 GMT  (245kb)", "http://arxiv.org/abs/1203.2002v1", "14 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.LG cs.DB", "authors": ["t soni madhulatha"], "accepted": false, "id": "1203.2002"}, "pdf": {"name": "1203.2002.pdf", "metadata": {"source": "CRF", "title": "GRAPH PARTITIONING ADVANCE CLUSTERING TECHNIQUE", "authors": ["T. Soni Madhulatha"], "emails": ["latha.gannarapu@gmail.com"], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijcses.2012.3109 91Clustering is a common technique for statistical data analysis, clustering is the process of grouping data into classes or clusters, so that objects within a cluster are very similar in comparison to each other, but objects in other clusters are very dissimilar. Differences are assessed by the attribute values describing the objects. Distance measurements are often used. Clustering is an unattended learning method in which interesting patterns and structures can be found directly from very large data sets with little or no background knowledge. In this paper, the partitioning of dimensional grid diagrams is also considered using Fiedler's approach, which requires determination of the eigenvector, which is the second-smallest inherent value of the laptop computer with K-mean partitioning algorithm.KEYWORDSClustering, K-Means, Iterative Shifting, Fiedler Approach, Symmetrical Matrix, Matrix, Elacic Values, Matrix."}, {"heading": "1. INTRODUCTION", "text": "Unlike classification and regression, which analyze class-marked data sets, clustering analyzes data objects without consulting class names. In many cases, class-marked data simply does not exist at the beginning [1]. Objects are bundled or grouped based on the principle of maximizing class similarity and minimizing class similarity. It is a collection of data objects that are similar within the same cluster and dissimilar to objects in other clusters. Any cluster thus formed can be considered an object class from which rules can be derived. In addition to the term data clustering, synonyms such as cluster analysis, automatic classification, numerical taxonomy, botrology, and typological analysis are used."}, {"heading": "1.1 Types of Data in cluster Analysis and variable types", "text": "Before turning to cluster algorithms, we must first examine the type of data that is common in cluster analysis. Main memory-based cluster algorithms work either by object-by-variable structure or by object-by-object structure. Object-by-variable is presented as a data matrix, with the object-by-object structure presented as a dissimilarity matrix. According to the cluster principle, we must calculate the inequality between the objects. Objects cited in the Data Mining textbook by Han and Kamber are described as follows: Interval scaled variables: The variables that are continuous measurements on a roughly linear scale. Example: Marks, age, height, etc. Binary variables: This variable has only two states, either 0 or 1. Nominal variable: Nominal is the generalization of binary variables that can capture more than two states. Example rainbow colors have scales that are IBO colors that are very useful for evaluating ors.These orative states are:"}, {"heading": "1.2 Categorization of clustering methods", "text": "There are a large number of cluster algorithms in the literature. The choice of cluster algorithm depends both on the type of data available and on its purpose and application. If cluster analysis is used as a descriptive or exploratory tool, it is possible to test several algorithms on the same data to see what the data can reveal. In general, important cluster methods can be categorized into the following category.1. Hierarchical 2. DensityThe above methods are not up-to-date method partitioning methods3. K-Means4. K-Medoids5. Markov Cluster Algorithm (MCL) 6. Non-negative matrix fractionation (NMF) 7. Singular Value Decomposition (SVD) The above require preliminary knowledge of the data in order to select some cluster algorithms that integrate the ideas of multiple cluster methods, so that it is sometimes difficult to classify a particular algorithm as a single cluster methodology."}, {"heading": "2. CLASSICAL PARTITIONING METHODS", "text": "The simplest and most basic version of cluster analysis is partitioning, which divides the object of a set into several exclusive groups or clusters. The most commonly used partitioning methods are: k-mean algorithmk-medoids algorithm and their variations"}, {"heading": "2.1 K-MEANS ALGORITHM", "text": "K means clustering algorithm was developed by J. McQueen and then by J. A. Hartigan and M. A. Wong based on 1087 Q = 1087 1087 1087 1087 1087 1087 = 1087 1087 1087 1087 = 1087 1087 1087 1087. The k mean algorithm takes the input parameter, k, and partitions a set of n objects into k clusters so that the resulting intra cluster similarity is high, whereas the inter-cluster similarity is low. Cluster similarity is measured in terms of the mean value of the objects in the cluster that can be considered the center of the cluster."}, {"heading": "3. FIEDLER\u2019S METHOD", "text": "Fiedler's approach to clustering, which theoretically determines the relationship between the size of the obtained clusters and the number of links intersected by this partitioning as a function of a threshold \u03b1 and graph properties such as the number of nodes and links. Applying Fiedler's beautiful results to the laplactic matrix Q of a graph, the eigenvector, the second smallest eigenvalue known as algebraic connectivity, must be calculated. In order to find the laplactic matrix, the construction of an A-adaptation matrix and a D-degree matrix is required, so that the laplactic matrix L is formed as follows: L = D - A ------------------------------------------------------------------------------- (2) For a simple graph G with n vertices, its laplactic matrix is defined as follows:"}, {"heading": "3.1 Adjacency Matrix", "text": "The adjacence matrix of a finite graph G of n vertices is the n \u00b7 n matrix, where the non-diagonal entry aij is the number of edges from the vertex i to the vertex j and the diagonal entry aii has either one or twice as many edges from the vertex i to itself."}, {"heading": "3.2 Degree matrix", "text": "In the mathematical domain of graph theory, the degree matrix is a diagonal matrix containing information about the degree of each vertex, which is the number of edges connecting a vertex v. If i \u0445 j then replaces the cell value with 0 other wise degree of the vertex vi."}, {"heading": "3.3 Laplacian matrix", "text": "Faced with a simple graph G with n vertices, its laplac matrix is defined as follows: L (i, j) = vertex vi, if i = j, if i-j and vi do not border on vj and fill in all other cases with 0."}, {"heading": "3.4 Fiedler method", "text": "This method divides the data set S into two sets S1 and S2 based on the own vector Vector = = 11gen = = the 2nd smallest intrinsic value of the laplac matrix. Consider the equations 3 (n1..., j n1k \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 in matrix notation we can write this as Y = AX, where Y is a column vector and A = (aij) is a matrix transformation. In many situations we have to put a vector into a scalmed multiple of itself.i.e. AX = X-------------------------------------------------------------------------------------------------------------------- (4), where II is a scalarming problem. Such problems are known as sovereignty value problems."}, {"heading": "4. METHODOLOGY OF EXPERIMENTATION", "text": "We observed several different eigenvectors, followed the Fiedler algorithm and then encoded in Matlab with eigs (), eig (), using small samples of well-known clusters.MATLAB CODE: Steps: 1. Enter the laplactic matrix in Matlab as: a = [2 0 0 0 0 0 0 0 0 0 -1 0 -1 0 -1 0 -1 0; 0 -1 0 -1 0 0 0 0 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0 -1 0, 0 -1 0 -1 0 -1 0 -1 0 -1, 0 -1 0 -1 0 -1, 0 -1 0 -1 0 -1 0, 0 -1 0, 0 -1 0 -1, 0 -1 0 -1, 0, 0 -1, 0, 0 -1, 0, 0 -1, 0 -0, 0 -1, 0, 0 -0 -0, 0, 0 -0, 0, 0, 0, 0, 0, 0, 0 -1, 0 -1, 0 -1, 0 -1, 0, 0, 0 -0, 0, 0 -0, 0, 0, 0, 0, 0, 0 -0, 0, 0, 0, 0 -0, 0, 0, 0, 0, 0, 0, 0 -0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 -0, 0, 0, 0, 0, 0, 0, 0"}, {"heading": "4. CONCLUSION", "text": "The advantage of the K-Means algorithm is its favorable execution time. Its disadvantage is that the user needs to know in advance how many clusters are to be searched for. It is observed that the K-Means algorithm is efficient only for smaller data sets. Feldder's method does not require prior knowledge of the number of clusters, but most cluster methods require symmetry of the matrices. Symmetry techniques either distort original information or considerably enlarge the data set, and there are many applications where the data is not symmetrical like hyperlinks on the web."}, {"heading": "5. REFERENCES", "text": "[1] Han, J. and Kamber, M. Data Mining: Concepts and Techniques, 2001 (Academic Press, San Diego, California, USA). [2] Comparison between Clustering Algorithms - Osama Abu Abbas [3] Pham, D.T. and Afify, A.A. Clustering techniques and their applications in Engineering.Submitted to Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science, 2006. [4] Jain, A.K. and Dubes, R.C. Algorithms for Clustering Data, 1988 (Prentice Hall, EnglewoodCliffs, New Jersey, USA). [5] Bottou, L. and Bengio, Y. Convergence properties of the k-means algorithm. [6] B. Mohar, The Laplacian Spectra of Graphs. Graph Theory, Combinatorics, New Jersey, USA)."}], "references": [{"title": "Clustering techniques and their applications in engineering", "author": ["D.T. Pham", "A.A. Afify"], "venue": "Submitted to Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Algorithms for Clustering Data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "The Laplacian Spectra of Graphs", "author": ["B. Mohar"], "venue": "Graph Theory, Combinatorics, and Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1991}, {"title": "An introduction to the Theory of Graph Spectra.Addison-Wesley", "author": ["D. Cvetkovi \u0301c", "P. Rowlinson", "S. Simi \u0301c"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Performance Analysis of Communications Networks and Systems", "author": ["P. Van Mieghem"], "venue": "She published papers in various National and International Journals and Conferences. She is a Life Member of ISTE , IAENG and APSMS", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}], "referenceMentions": [], "year": 2012, "abstractText": "Clustering is a common technique for statistical data analysis, Clustering is the process of grouping the data into classes or clusters so that objects within a cluster have high similarity in comparison to one another, but are very dissimilar to objects in other clusters. Dissimilarities are assessed based on the attribute values describing the objects. Often, distance measures are used. Clustering is an unsupervised learning technique, where interesting patterns and structures can be found directly from very large data sets with little or none of the background knowledge. This paper also considers the partitioning of mdimensional lattice graphs using Fiedler\u2019s approach, which requires the determination of the eigenvector belonging to the second smallest Eigen value of the Laplacian with K-means partitioning algorithm.", "creator": "PScript5.dll Version 5.2.2"}}}