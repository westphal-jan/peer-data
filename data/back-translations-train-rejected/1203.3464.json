{"id": "1203.3464", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Gibbs Sampling in Open-Universe Stochastic Languages", "abstract": "Languages for open-universe probabilistic models (OUPMs) can represent situations with an unknown number of objects and iden- tity uncertainty. While such cases arise in a wide range of important real-world appli- cations, existing general purpose inference methods for OUPMs are far less efficient than those available for more restricted lan- guages and model classes. This paper goes some way to remedying this deficit by in- troducing, and proving correct, a generaliza- tion of Gibbs sampling to partial worlds with possibly varying model structure. Our ap- proach draws on and extends previous generic OUPM inference methods, as well as aux- iliary variable samplers for nonparametric mixture models. It has been implemented for BLOG, a well-known OUPM language. Combined with compile-time optimizations, the resulting algorithm yields very substan- tial speedups over existing methods on sev- eral test cases, and substantially improves the practicality of OUPM languages generally.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (423kb)", "http://arxiv.org/abs/1203.3464v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nimar s arora", "rodrigo de salvo braz", "erik b sudderth", "stuart russell"], "accepted": false, "id": "1203.3464"}, "pdf": {"name": "1203.3464.pdf", "metadata": {"source": "CRF", "title": "Gibbs Sampling in Open-Universe Stochastic Languages", "authors": ["Nimar S. Arora", "Rodrigo de Salvo Braz"], "emails": [], "sections": [{"heading": null, "text": "Languages for Open Universal Probability Models (OUPMs) can represent situations with an unknown number of objects and identity uncertainty. While such cases occur in a variety of important real-world applications, existing universal inference methods for OUPMs are far less efficient than those available for more limited languages and model classes. In some ways, this contribution contributes to remedying this deficit by introducing and extending a generalization of Gibbs samples to sub-worlds with possibly different model structures. Our approach draws on earlier generic OUPM inference methods as well as additional variable samples for non-parametric mixing models. It was implemented for BLOG, a well-known OUPM language. Combined with compile-time optimizations, the resulting algorithm leads to very significant accelerations over existing methods in several test cases and significantly improves the practicality of OUPM languages in general."}, {"heading": "1 Introduction", "text": "General purpose probabilistic modeling languages are aimed at facilitating the development of complex models while providing effective, general sequencing methods so that the modeler does not have to rewrite model-specific sequence codes from scratch for each application. BUGS (Spiegelhalter et al., 1996) can represent directed graphical models of indexed sets of random variables and uses MCMC conclusions (in particular Gibbs sampling where possible). As the expressiveness of modeling languages increases, so does the bandwidth of representable problems. The class of first-class probability languages used in the open universe, including BLOG (Milch et al., 2005a) and Church (Goodman et al., 2008), handles cases where the number of objects (in BUGS, the index set) as a section is unknown and possibly unlimited, and the objective identity is uncertain. It is still possible to write a complete inference algorithm for BLOG based on the MLOG subset."}, {"heading": "2 Contingent Bayesian Networks", "text": "This section repeats itself, and in some cases generalizes definitions proposed by Milch et al. (2005b). A contingent Bayesian network (CBN) consists of a series of random variables V, and for each variable X-V, a domain (X), and a decision tree TX. The decision tree is a directed binary tree in which each node is a predicate on a subset of V. Each leaf of TX encodes a probability distribution parameterized by a subset of V, and defined on dom (X). Example 1. An aircraft of unknown wing type - helicopter or FixedWingPlane - is detected on a radar. Each leaf of TX encodes has an unknown rotorLength, and depending on that length, they could generate a characteristic pattern called BladeFlash (Tait, 2009) in the returned radar signal. A FixedWingPlane could generate a bladeFlash."}, {"heading": "3 Related Work", "text": "Milch and Russell (2006) have already demonstrated that the state space for the Markov chain Monte Carlo (MCMC) inference in CBNs can consist of minimal partial instances supporting the evidence, e, and query variables Q. This idea has been exploited to build the current, standard BLOG inference engine. Standard sample algorithms for non-parametric Dirichlet process mixing models use a related representation: they start with parameters for those mixing components that support evidence, as well as for some auxiliary components (Neal, 2000). Our new algorithm builds on these two methods."}, {"heading": "3.1 Parent-Conditional Sampling", "text": "In the absence of a model-specific request distribution supplied by the user, BLOG's existing inference is q = unique q engine (parental conditional suggestion), which randomly selects a variable X from all the non-evident variables in the current instance. If X was a switching variable in the current instance, we may then have to cancel new variables and unneeded (vars) in order to achieve a minimal and self-supporting value above Q. All new variables are associated with values from their parental conditional distribution. We say that any Y formula constructed using this method is achievable by X. The following properties are easy to accept as true."}, {"heading": "3.2 Gibbs Sampling", "text": "Equation (5) summarizes the main problem with parent-child relationship (X): If the proposed value for the sampled variable X does not apply to the children of X, the step is rejected. To avoid inappropriate assumptions, hierarchical statistical models are often scattered or \"vague,\" so that such parent-child models have extremely low acceptance probabilities. Originally, this method was proposed by Geman and Geman for inference in disordered fields, and later popularized as the general Bayesian method by Gelfand andSmith (1990)."}, {"heading": "4 Gibbs Sampling in Contingent Bayesian Networks", "text": "We are now developing a general extension of standard Gibbs samplers applicable to arbitrary switching variables with finite domains. (The proposal for a switch variable, X, is continued in three steps.) First, the instance \u03c3 is reduced to a subset of variables, core (\u03c3, X), which should exist in a minimal, self-supporting instantiation. (The proposal for a self-supporting instantiation constructed by \u03c3X = a, for any country (X). Second, we construct minimal self-supporting instantiations, i = 1,. (X) | 1, for any value in a country (X) \u2212 (X). These instantiations agree with core (X), but assign different values to X. All remaining variables in these configurations are derived from their parent-conditioned priors."}, {"heading": "5 BLOG Compiler", "text": "We have implemented our algorithm in a new implementation of the BLOG language, which we will call blogc1. The rough framework of our implementation is similar to that of Milch's Public Domain MetropolisHastings version, except in two essential respects. Firstly, for variables with (possibly unknown) finite domains we always use Gibbs sampling. By statically analyzing the structure of the model, we can determine which variables are the circuitry of variables that need to be re-recorded for each transition, etc. Based on the analysis, corresponding code is generated that shows the actual sampling and reporting.Let's take as an example the BLOG model in Figure 6. This model describes the previous distribution of two types of aircraft - fixed-wing aircraft and helicopters. These aircraft can generate an arbitrary number of blips on the radar (the fact that an aircraft generates a blip b is represented by setting the source (b), which allows us to continue to generate blips on the radar due to their interaction with the helicopters)."}, {"heading": "6 Experimental Results", "text": "We have compared the convergence speed and accuracy of blogc against the existing generic MetropolisHastings Inference Engine with BLOG, which we call BLOG-MH. Since a Gibbs and an MH sampler perform different amounts of work in each sample, we felt it was more appropriate to compare the two inference engines in terms of time. To control the compiler optimizations in blogc, we have implemented a version of BLOG-MH in blogc, which we will call blogc-MH. For some of the other experiments we have also implemented, a version of Gibbs sampling that is not untenable and resample variables are not at its core, which we call blogc-noblock.In the following three models, each inference engine is run for a different number of samples, where a sample is defined by this inference engine."}, {"heading": "7 Conclusions", "text": "We have shown a significant improvement in inference performance for models written in the BLOG language. Our Gibbs sampling algorithm for CBNs and our compiler techniques for generating efficient inference codes are generally applicable to all stochastic languages of the open universe."}, {"heading": "Acknowledgements", "text": "This work would not have been possible without the considerable help of Brian Milch to make the models presented here work in BLOG-MH. Matthew Can translated the Bayes Net alarm for BLOG. Finally, the first author would like to thank his family for their boundless patience and support during this work."}], "references": [{"title": "An introduction to MCMC for machine learning", "author": ["C. Andrieu", "N. de Freitas", "A. Doucet", "M.I. Jordan"], "venue": "Machine Learning,", "citeRegEx": "Andrieu et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Andrieu et al\\.", "year": 2003}, {"title": "The alarm monitoring system: A case study with two probabilistic inference techniques for belief networks", "author": ["I. Beinlich", "G. Suermondt", "R. Chavez", "G. Cooper"], "venue": "Proc. 2\u2019nd European Conf. on AI and Medicine. Springer-Verlag,", "citeRegEx": "Beinlich et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Beinlich et al\\.", "year": 1989}, {"title": "Challenge: Where is the impact of Bayesian networks in learning? IJCAI", "author": ["N. Friedman", "M. Goldszmidt", "D. Heckerman", "S. Russell"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1997}, {"title": "Samplingbased approaches to calculating marginal densities", "author": ["A.E. Gelfand", "A.F.M. Smith"], "venue": null, "citeRegEx": "Gelfand and Smith,? \\Q1990\\E", "shortCiteRegEx": "Gelfand and Smith", "year": 1990}, {"title": "Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images", "author": ["S. Geman", "D. Geman"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Geman and Geman,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman", "year": 1984}, {"title": "Church: a language for generative models. UAI", "author": ["N. Goodman", "V. Mansinghka", "D. Roy", "K. Bonawitz", "J. Tenenbaum"], "venue": null, "citeRegEx": "Goodman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2008}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["B. Milch", "B. Marthi", "S.J. Russell", "D. Sontag", "D.L. Ong", "A. Kolobov"], "venue": "IJCAI (pp", "citeRegEx": "Milch et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Milch et al\\.", "year": 2005}, {"title": "Approximate inference for infinite contingent Bayesian networks", "author": ["B. Milch", "B. Marthi", "D. Sontag", "S. Russell", "D.L. Ong", "A. Kolobov"], "venue": "In Proc. 10th AISTATS (pp. 238\u2013245)", "citeRegEx": "Milch et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Milch et al\\.", "year": 2005}, {"title": "General-purpose MCMC inference over relational structures", "author": ["B. Milch", "S. Russell"], "venue": "Proceedings of the Proceedings of the Twenty-Second Conference Annual Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Milch and Russell,? \\Q2006\\E", "shortCiteRegEx": "Milch and Russell", "year": 2006}, {"title": "Markov chain sampling methods for dirichlet process mixture models", "author": ["R.M. Neal"], "venue": "Journal of Computational and Graphical Statistics,", "citeRegEx": "Neal,? \\Q2000\\E", "shortCiteRegEx": "Neal", "year": 2000}, {"title": "BUGS: Bayesian inference using gibbs sampling, version", "author": ["D. Spiegelhalter", "A. Thomas", "N. Best", "W. Gilks"], "venue": null, "citeRegEx": "Spiegelhalter et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Spiegelhalter et al\\.", "year": 1996}, {"title": "Introduction to radar target recognition. The Institution of Engineering and Technology, United Kingdom", "author": ["P. Tait"], "venue": null, "citeRegEx": "Tait,? \\Q2009\\E", "shortCiteRegEx": "Tait", "year": 2009}], "referenceMentions": [{"referenceID": 10, "context": "For example, BUGS (Spiegelhalter et al., 1996) can represent directed graphical models over indexed sets of random variables and uses MCMC inference (in particular, Gibbs sampling where this is possible).", "startOffset": 18, "endOffset": 46}, {"referenceID": 5, "context": ", 2005a) and Church (Goodman et al., 2008), handles cases in which the number of objects (in BUGS, the index set) is unknown and perhaps unbounded, and object identity is uncertain.", "startOffset": 20, "endOffset": 42}, {"referenceID": 6, "context": "This section repeats, and in some cases generalizes, definitions proposed by Milch et al. (2005b). A contingent Bayesian network (CBN) consists of a set of random variables V, and for each variable X \u2208V, a domain dom(X) and decision tree TX .", "startOffset": 77, "endOffset": 98}, {"referenceID": 11, "context": "Helicopters have an unknown RotorLength, and depending on this length they might produce a characteristic pattern called a BladeFlash (Tait, 2009) in the returned radar signal.", "startOffset": 134, "endOffset": 146}, {"referenceID": 9, "context": "Standard sampling algorithms for nonparametric, Dirichlet process mixture models use a related representation: they instantiate parameters for those mixture components which support the evidence, as well as a few auxiliary components (Neal, 2000).", "startOffset": 234, "endOffset": 246}, {"referenceID": 0, "context": "The nature of this proposal distribution q(\u03c3 \u2192 \u03c3\u2032) makes it quite simple to compute the acceptance ratio for the Metropolis\u2013Hastings (MH) method (Andrieu et al., 2003), which takes the following form:", "startOffset": 145, "endOffset": 167}, {"referenceID": 4, "context": "This method was originally proposed by Geman and Geman (1984) for inference in undirected Markov random fields, and later popularized as a general Bayesian inference method by Gelfand and 1.", "startOffset": 39, "endOffset": 62}, {"referenceID": 9, "context": "One possible solution, proposed in the context of Dirichlet process (DP) mixture models by Neal (2000), augments \u03c3 with auxiliary variables chosen so that \u03c3 is self-supporting for all a\u2208 dom(X).", "startOffset": 91, "endOffset": 103}, {"referenceID": 1, "context": "First, we evaluate on the Alarm network of (Beinlich et al., 1989) available from the Bayes Network Repository (Friedman et al.", "startOffset": 43, "endOffset": 66}, {"referenceID": 2, "context": ", 1989) available from the Bayes Network Repository (Friedman et al., 1997).", "startOffset": 52, "endOffset": 75}], "year": 2010, "abstractText": "Languages for open-universe probabilistic models (OUPMs) can represent situations with an unknown number of objects and identity uncertainty. While such cases arise in a wide range of important real-world applications, existing general purpose inference methods for OUPMs are far less efficient than those available for more restricted languages and model classes. This paper goes some way to remedying this deficit by introducing, and proving correct, a generalization of Gibbs sampling to partial worlds with possibly varying model structure. Our approach draws on and extends previous generic OUPM inference methods, as well as auxiliary variable samplers for nonparametric mixture models. It has been implemented for BLOG, a well-known OUPM language. Combined with compile-time optimizations, the resulting algorithm yields very substantial speedups over existing methods on several test cases, and substantially improves the practicality of OUPM languages generally.", "creator": "TeX"}}}