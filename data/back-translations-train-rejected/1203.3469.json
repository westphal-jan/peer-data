{"id": "1203.3469", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Probabilistic Similarity Logic", "abstract": "Many machine learning applications require the ability to learn from and reason about noisy multi-relational data. To address this, several effective representations have been developed that provide both a language for expressing the structural regularities of a domain, and principled support for probabilistic inference. In addition to these two aspects, however, many applications also involve a third aspect-the need to reason about similarities-which has not been directly supported in existing frameworks. This paper introduces probabilistic similarity logic (PSL), a general-purpose framework for joint reasoning about similarity in relational domains that incorporates probabilistic reasoning about similarities and relational structure in a principled way. PSL can integrate any existing domain-specific similarity measures and also supports reasoning about similarities between sets of entities. We provide efficient inference and learning techniques for PSL and demonstrate its effectiveness both in common relational tasks and in settings that require reasoning about similarity.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (488kb)", "http://arxiv.org/abs/1203.3469v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["matthias brocheler", "lilyana mihalkova", "lise getoor"], "accepted": false, "id": "1203.3469"}, "pdf": {"name": "1203.3469.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Similarity Logic", "authors": ["Matthias Br\u00f6cheler", "Lilyana Mihalkova"], "emails": [], "sections": [{"heading": null, "text": "Many machine learning applications require the ability to learn from and understand noisy multirelational data. To achieve this, several effective representations have been developed that provide both a language to express the structural laws of a domain and a principal support for probabilistic conclusions. However, in addition to these two aspects, many applications also include a third aspect - the need to think about similarities - that is not directly supported in existing frameworks. In this paper, probabilistic similarity logic (PSL) is introduced, a universal framework for collective thinking about similarities in relational domains that includes probabilistic thinking about similarities and relational structures in a principled way. PSL can integrate all existing domain-specific similarity measures and also supports thinking about similarities between entities. We provide efficient inference and learning techniques for PSL and demonstrate their effectiveness both in shared relational tasks and in settings that require thinking about similarities."}, {"heading": "1 Introduction", "text": "A variety of machine learning programs require the ability to learn from and to reason, or uncertain, multi-relational data. This has led to the areas of statistical Relational Learning (SFL) and Multilateral Data Processing (SFL), which address the question of whether and to what extent it is a structural regularity that exists in a domain, and on the other hand, that they provide principled support for probable inference, e.g. [12, 8].In addition to the relative structure and probability of people interested in another world, there is a third aspect of interest - the need to think about similarities - that is not directly present in existing SRL frameworks.This paper leads to a probable similarity of logical similarities (PSL), an all-encompassing definition of relationships."}, {"heading": "2 PSL", "text": "This year, it has reached the point where it will be able to put itself at the top of the group that it is in."}, {"heading": "2.3 The Importance of Similarity", "text": "The ability to think about similarities in a relationship framework is a key novel feature of PSL =. This is where we motivate their importance. The most immediate advantage of arguing about similarity is that in PSL the many well-understood domain-specific measures of similarity that exist in the literature can easily be brought to bear in a relationship context (another advantage derives from the interaction of relationship structure and similarity; namely, PSL supports the argument about similarity not only between the attributes of two entities X and Y, but also between the respective entities related to X and Y, e.g. the sets of entities related to X and Y via the publisher relationship. Because support for fixed similarity is such an important aspect of PSL, we consider it further by contrasting SetFreePSL in which similarity is not permitted, up to full PSL."}, {"heading": "2.4 Inference", "text": "It is often necessary to draw a maximum conclusion (also called MPE conclusion) (11) (11) in order to derive the most likely values for a number of suggestions, given values for the remaining (evidence) suggestions. For example, in an ontology alignment task, we would like to predict the best matches of concepts from the other side. Formally, the task is then to find a truth matching that is most likely after the PSL program, given the evidence: IMAP (y) = argmax I (y) | I (x) = argmax I (y) 1 Z exp (\u2212 d3), P)."}, {"heading": "2.5 Weight learning", "text": "Weight learning in PSL is performed using standard techniques by optimizing the log probability of interpretation I: logP (I (G) = \u2212 \u03b4 (V (I), 0) \u2212 logZ (12) above, \u03b4 (V (I), 0) is as in definition 2. The gradient of the equation (12) with respect to weight wk depends on the function: MAP inference I0 (y) \u2190 all zeros assignment 1.1 R \u2190 all grounded rules activated by I (x), I0 (y) 1.2 whileR has been updated do1.3 i \u2190 current iteration1.4 O \u2190 generateConvexProb (R) 1.5 Ii (y) \u2190 Optimization (O) 1.6 for each proposition y y do1.7, if Ii (y) > Proposition (y) > Proposition (p = 0.01) > Proposition (p = 0.8) then1,8 Ry \u2190 activated rules containing the y R value \u2212 1.11 log1.12AP function depending on the respective inference in M1."}, {"heading": "2.6 The PSL System", "text": "We implemented PSL in Java using a relational database1 to store and efficiently query during rule grounding. Based on a set of rules and database handling, the system identifies any rules that may be potentially unsatisfactory, and builds a numerical model that complies with the rules that can be solved with any standard numerical optimization toolbox 2. If the optimal solution determined by the numerical solver changes the truth value of an atom, the system automatically determines all affected rules and explores any rules that may be unsatisfactory as a result. A set of data structures are maintained to efficiently determine such changes. Basic rule changes are reflected in the numerical model, which is maintained throughout the thought process and updated in the solver, allowing the solver to utilize knowledge of the previous optimal solution in order to quickly adapt to the new solution. The system will be available at http: / / psl.umiacumdu.e."}, {"heading": "3 Experiments", "text": "This section presents an empirical evaluation of PSL that addresses two questions: 1We used the freely available H2 database (http: / / www. h2database.com /) 2We used MOSEK (http: / / www.mosek.com) 1. Is PSL effective in modeling tasks related to relational inferences? 2. How useful are the new features provided by PSL? We examine these questions based on two different problems, namely (a) category prediction and similarity propagation for Wikipedia documents and (b) ontology alignment on a standard corpus of bibliographic ontologies. After describing the data and experimental methodology, we present results that demonstrate the effectiveness of PSL on relational inferences."}, {"heading": "3.1 Wikipedia Category Prediction", "text": "We collected all the Wikipedia articles that we identified as users in the period from October 7 to 21, 2009, and so we have 2460 documents at our disposal. We used featured articles because they are richly interconnected, both through their hyperlinks and through their network of human editors [3]. Some of the original categories that were similar were merged to ensure that each category contained enough documents; the data includes the Link relations (fromDoc, toDoc), which create a hyperlink between two documents; Talk, user, which determines that the user edits the \"talk\" page of the given document; 4 and HasCat (Document, Category), which states that the document has a specific category. We used the last two years of editing to the conversation pages."}, {"heading": "3.2 Ontology Alignment", "text": "In fact, it is such that most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "3.3 Similarity and Scalability", "text": "Finally, we discuss the similarity aspect of PSL in the context of the experiments presented. On the one hand, we have easily integrated a wide range of similarity metrics with a semantics centered around similarity. In our Wikipedia experiments, we integrated cosinal similarity and an existing implementation of Naive Bayes. For the ontology alignment, we used previously proposed string similarity metrics such as Levenshtein, Dice and others. However, since our gold standard valuation data did not include similarity values, but rather hard turkey statements, we were unable to evaluate the quality of the similarity values derived from PSL. Instead, we used some means of similarity after processing, e.g. in ontology alignment, two concepts as accurately aligned when their similarity was greater than 0.5. This raises the question of whether a discreet formulation of the problem within PSL would lead to a better result."}, {"heading": "4 Related Work", "text": "PSL builds on a large body of research in SRL, in which the relationship structure is parameterized to find an optimal solution."}, {"heading": "5 Conclusions and Future Work", "text": "We have introduced a new general framework that integrates probabilistic thinking about similarity in a relational context and demonstrates its effectiveness in two distinct tasks, including considerations of similarity. We have experimentally validated the usefulness of set constructs and similarity conclusions - two new features supported by PSL. Future directions include the study of different distances from satisfaction functions, such as the L2 distance, and the application of PSL to other areas, especially those where we can validate PSL using ground truth data with similarity values."}, {"heading": "Acknowledgment", "text": "We thank Avi Pfeffer, Kristian Kersting and the anonymous reviewers for their helpful comments and suggestions. This material is based on work supported by the National Science Foundation under grant number 0937094. LM is supported by a CI Fellowship under grant number 0937060 of the NSF to the Computing Research Association. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or the CRA."}], "references": [{"title": "Second-order cone programming", "author": ["F. Alizadeh", "D. Goldfarb"], "venue": "Mathematical Programming, 95(1):3\u201351", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Network analysis of collaboration structure in Wikipedia", "author": ["Ulrik Brandes", "Patrick Kenis", "J\u00fcrgen Lerner", "Denise van Raaij"], "venue": "In Proceedings of WWW-09,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Probabilistic similarity logic", "author": ["Matthias Br\u00f6cheler", "Lilyana Mihalkova", "Lise Getoor"], "venue": "Technical report,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "First results of the ontology alignment evaluation initiative 2008", "author": ["C. Caracciolo", "J. Euzenat", "L. Hollink", "R. Ichise", "A. Isaac", "V. Malaise", "C. Meilicke", "J. Pane", "P. Shvaiko", "H. Stuckenschmidt"], "venue": "ISWC-2008 Workshop on Ontology Matching", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "A survey on ontology mapping", "author": ["N. Choi", "I.Y. Song", "H. Han"], "venue": "ACM SIGMOD Record, 35(3):34\u201341", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proceedings of EMNLP-02,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Logical and Relational Learning", "author": ["Luc De Raedt"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Monoidal t-norm based logic: towards a logic for left-continuous tnorms", "author": ["Francesc Esteva", "Llus Godo"], "venue": "Fuzzy Sets and Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "Ontology matching", "author": ["J. Euzenat"], "venue": "Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning probabilistic models of link structure", "author": ["Lise Getoor", "Nir Friedman", "Daphne Koller", "Benjamin Taskar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Introduction to Statistical Relational Learning", "author": ["Lise Getoor", "Ben Taskar", "editors"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Bayesian logic programs", "author": ["K. Kersting", "L. De Raedt"], "venue": "Technical report, Albert-Ludwigs University", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Fuzzy Sets and Fuzzy Logic: Theory and Applications", "author": ["G.J. Klir", "B. Yuan"], "venue": "Prentice Hall", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}, {"title": "Fast learning of relational kernels", "author": ["Niels Landwehr", "Andrea Passerini", "Luc De Raedt", "Paolo Frasconi"], "venue": "Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Applications of second-order cone programming", "author": ["M.S. Lobo", "L. Vandenberghe", "S. Boyd", "H. Lebret"], "venue": "Linear Algebra and Its Applications, 284(1- 3):193\u2013228", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "FACTORIE: Probabilistic programming via imperatively defined factor graphs", "author": ["Andrew McCallum", "Karl Schultz", "Sameer Singh"], "venue": "In Proceedings of NIPS-", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning, 62(1):107\u2013136", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Improving the accuracy and efficiency of MAP inference for Markov logic", "author": ["S. Riedel"], "venue": "Proceedings of UAI-08", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Discriminative probabilistic models for relational data", "author": ["Ben Taskar", "Pieter Abbeel", "Daphne Koller"], "venue": "In Proceedings of UAI-02,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Hybrid Markov logic networks", "author": ["Jue Wang", "Pedro Domingos"], "venue": "In Proceedings of AAAI-08,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": ", [12, 8].", "startOffset": 2, "endOffset": 9}, {"referenceID": 6, "context": ", [12, 8].", "startOffset": 2, "endOffset": 9}, {"referenceID": 4, "context": ", [6, 10].", "startOffset": 2, "endOffset": 9}, {"referenceID": 8, "context": ", [6, 10].", "startOffset": 2, "endOffset": 9}, {"referenceID": 0, "context": "The functions s1, s2, and s3 can be arbitrary functions of a pair of the appropriate types of entities, or sets of entities, whose domain is [0, 1].", "startOffset": 141, "endOffset": 147}, {"referenceID": 0, "context": "However, unlike in binary logic, here the conjunction and implication operators need to combine similarities, which are real numbers in [0, 1].", "startOffset": 136, "endOffset": 142}, {"referenceID": 12, "context": "One set of such truth-combining operators that generalize their Boolean counterparts is provided by t-norms and their corresponding t-conorms [14].", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "Above, a, b \u2208 [0, 1] can be similarities or Boolean truth values.", "startOffset": 14, "endOffset": 20}, {"referenceID": 0, "context": ", I(g) \u2208 [0, 1].", "startOffset": 9, "endOffset": 15}, {"referenceID": 14, "context": "This algorithm works by transforming the grounded PSL program into a second-order cone program (SOCP) [16] (line 4) whose solution gives an assignment to the propositions in y (line 5).", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "To solve the SOCP, one can use any available technique [1].", "startOffset": 55, "endOffset": 58}, {"referenceID": 17, "context": "1 is in essence equivalent to cutting plane inference (CPI) [20], except that, unlike CPI, PSL programs are continuous constrained numeric optimization programs.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "1, as stated in Theorem 1, which relies on the standard complexity result for SOCP [16] by showing the equivalence of PSL inference to a certain numeric problem outlined above.", "startOffset": 83, "endOffset": 87}, {"referenceID": 2, "context": "A detailed derivation of the SOCP and proof of the theorem can be found in the extended version of the paper [4].", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "We experimented with two ways of optimizing the above gradient: we used BFGS, a popular quasi-Newton method [18], and the Perceptron algorithm [7], where in both cases the expectation was approximated with the value of \u2016Vk(I),0\u20161 in the MAP state, which is a frequently used approximation since computing the expectation is intractable.", "startOffset": 143, "endOffset": 146}, {"referenceID": 1, "context": "We used featured articles because they are richly connected, both by their hyperlinks and by their network of human editors [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "A large number of approaches have been proposed (see [6] and [10] for surveys).", "startOffset": 53, "endOffset": 56}, {"referenceID": 8, "context": "A large number of approaches have been proposed (see [6] and [10] for surveys).", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "The full set of rules is included in the extended version of this paper [4].", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "To evaluate the performance of our set of PSL rules, we conducted an experimental study using the OAEI benchmark [5].", "startOffset": 113, "endOffset": 116}, {"referenceID": 3, "context": "Figure 3 compares the F1 score of PSL against the reported scores of other systems that participated in the evaluation initiative [5] on the real-world ontologies included in the benchmark (300 level).", "startOffset": 130, "endOffset": 133}, {"referenceID": 11, "context": ", BLPs [13], PRMs", "startOffset": 7, "endOffset": 11}, {"referenceID": 9, "context": "[11], RMNs [21], MLNs [19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[11], RMNs [21], MLNs [19].", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "[11], RMNs [21], MLNs [19].", "startOffset": 22, "endOffset": 26}, {"referenceID": 19, "context": "In terms of expressivity, PSL is closest to Hybrid MLNs [22], which allow the use of numeric-valued predicates.", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "In contrast, because PSL restricts the numeric-valued predicates it allows to similarity predicates, which evaluate in the interval [0, 1], it can incorporate logical and similarity reasoning in a principled way by using t-(co)norms, thus making use of a well-developed theoretical framework, e.", "startOffset": 132, "endOffset": 138}, {"referenceID": 7, "context": ", [9].", "startOffset": 2, "endOffset": 5}, {"referenceID": 15, "context": "PSL is related to imperative frameworks, such as, IDFs [17] that use a programming language to define structural dependencies.", "startOffset": 55, "endOffset": 59}, {"referenceID": 13, "context": "proaches, such as kFoil [15], that base similarity computation with kernel functions on relational structure.", "startOffset": 24, "endOffset": 28}], "year": 2010, "abstractText": "Many machine learning applications require the ability to learn from and reason about noisy multi-relational data. To address this, several effective representations have been developed that provide both a language for expressing the structural regularities of a domain, and principled support for probabilistic inference. In addition to these two aspects, however, many applications also involve a third aspect\u2013the need to reason about similarities\u2013which has not been directly supported in existing frameworks. This paper introduces probabilistic similarity logic (PSL), a general-purpose framework for joint reasoning about similarity in relational domains that incorporates probabilistic reasoning about similarities and relational structure in a principled way. PSL can integrate any existing domainspecific similarity measures and also supports reasoning about similarities between sets of entities. We provide efficient inference and learning techniques for PSL and demonstrate its effectiveness both in common relational tasks and in settings that require reasoning about similarity.", "creator": "TeX"}}}