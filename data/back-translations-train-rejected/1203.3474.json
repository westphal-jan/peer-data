{"id": "1203.3474", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Distribution over Beliefs for Memory Bounded Dec-POMDP Planning", "abstract": "We propose a new point-based method for approximate planning in Dec-POMDP which outperforms the state-of-the-art approaches in terms of solution quality. It uses a heuristic estimation of the prior probability of beliefs to choose a bounded number of policy trees: this choice is formulated as a combinatorial optimisation problem minimising the error induced by pruning.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (411kb)", "http://arxiv.org/abs/1203.3474v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gabriel corona", "francois charpillet"], "accepted": false, "id": "1203.3474"}, "pdf": {"name": "1203.3474.pdf", "metadata": {"source": "CRF", "title": "Distribution over Beliefs for Memory Bounded Dec-POMDP Planning", "authors": ["Gabriel Corona", "Fran\u00e7ois Charpillet"], "emails": [], "sections": [{"heading": null, "text": "In Dec-POMDP, we propose a new point-based approach to approximate planning that surpasses current approaches to solution quality, using a heuristic estimate of the previous likelihood of convictions to select a limited number of policy trees: this choice is formulated as a combinatorial optimization problem that minimizes the error caused by the cut."}, {"heading": "1 Introduction", "text": "The problem of controlling a team of cooperative agents in interaction with a stochastic environment has a wide field of application: packet routing in a network, robot coordination, control of sensor networks, zone monitoring, etc. The formal framework of the Dec-POMDP (Decentralized Partially Observable Markov Decision Process) can be rigorously used to address this problem. Here, we focus on the particular case of planning: Given the rules of the environment and a target modeled as a reward function, we are looking for a policy that is assigned to each agent to maximize the rewards. Calculation of an optimal common policy is NEXP-complete within a limited horizon [Bernstein et al., 2000] when the horizon is smaller than the number of states. Accurate compilation is therefore not usable, except for very small problems (in particular, with a small number of observations) and for small horizons Dynamic-Dynamic-Up, several approaches have been proposed to solve POC-MNAP problem."}, {"heading": "2 Models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 MDP", "text": "The Markov Decision Process (MDP) framework models the decision problem of a single actor who tries to maximize rewards by interacting with a fully observable environment; time is discrete and in each step t the actor knows exactly the state of the environment and chooses an action At + 1. An MDP is a TupelM = < S, A, T, R > where: S et A are the states s of the environment and the actions a of the actor; the transition function T represents the dynamics of the system by defining the transition probabilities; T (s \u2032 | s, a) = Pr (St + 1 = s \u2032 | St = s, At + 1 = a), by state s by state s \u2032 when performing action a; then function R represents the target of the actor by defining the expectation R (s, a) = E (Rt + 1 = s = a)."}, {"heading": "2.2 POMDP", "text": "The POMDP framework (Partially Observable MDP) generalizes the MDP framework for partially observable environments: instead of observing the state St, the agent receives an observation Ot, which can be used to derive St. At any time, the agent selects an action. In given past observations O1... Ot \u2212 1 and actions A1.. In \u2212 1. A POMDP is a tuple M = < S, A, T, R, E, O > where: \"is the series of observations o that the agent can obtain; the observation function O defines the probabilities of observation O (o | s, a, s \u2032) = Pr (Ot + 1 = o | St = 1 = a, St + 1 = s\") of observation o in the transition from s to s \u00b2 with action. The probability distribution O (A probability distribution Bt) represents the belief of the agent, i.e. the Bayesian knowledge that it has traced back to the current state."}, {"heading": "2.3 Dec-POMDP", "text": "The Dec-POMDP framework generalizes the POMDP framework for a team of agents. A Dec-POMDP is a TupelM = < I, S, (Ai) i, I, (Ai) i, I, O >, where I am the finite set of agents i. Each agent has its own sets of Ai and i local actions ai and observations oi: a joint action a = (a1,... an) \"A\" is a tuple of local actions where A = i \"I\" Ai \"is the set of common actions; a joint observation o = (o1,.. on) is a tuple of local observations where B = i\" i \"i\" i \"i is the set of common observations. At any time, each agent selects a local action Ati, since the past local observations O1i..... O t \u2212 1 i.\" The original belief B0 is one prior to the collection of the POMDP by a common agent."}, {"heading": "2.4 Policy Trees", "text": "In the case of a finite horizon problem, the local deterministic strategies of a Dec-POMDP can be modeled as political trees (Figure 1), whose nodes are actions and whose branches are observations. Common political trees are tuples of local political trees (one for each agent), q = (q1),.., qn (on) of the common sub-policy trees, the Q = (aq1,.., aqn) of the joint action of q from the local strategy trees aqi and q (o) = (q1),., qn (on) of the common sub-strategy trees representing Qi (oi) of the local strategy trees Qi (oi). A common strategy tree q can be evaluated by its value."}, {"heading": "3 State of the Art", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Dynamic Programming", "text": "Dynamic Programming (DP) Dec-POMDP Planning builds political trees from bottom to top (Hansen et al., 2004) (see Algorithm 1). The DP operator (line 2) builds the sentences Qti political trees of horizon H \u2212 t (executed from time t to H) from the sentences Qt + 1i political trees of horizon H \u2212 t \u2212 1 (executed from time t + 1 to H). This operator normally consists of two phases: fuse and pruning (algorithm 2). Algorithm 1: Dynamic Programming Result: \"i\" Q0i \"and their\" V-vectors \"foreach i\" I do QHi \"(empty tree) 1 for t = H-1 to 0 do (Qt) i\" i \"i\" i \"DP\" Qt + 1i \"i\" (Qt + 1i) i \"c\" h \"i\" i \"h\" i \"i\" i \"c\" h \"i\" i \"i\" c \"i\" c \"i\" c \"i\" c \"h\" i \"i."}, {"heading": "3.2 MBDP", "text": "Memory Bounded Dynamic Programming (MBDP) [Seuken and Zilberstein, 2007b] has been proposed to solve Dec-POMDPs with a high horizon (algorithm 4). It replaces exact pruning with memory-bound pruning: for each agent i, a maximum of W policy trees from Q-Ti are kept in the specified Qti. W points (probability distribution to states) are generated using some heuristic guidelines. For each point b, the best common policy q = (q1,., qn) is selected (line 4) and the corresponding local trees qi are moved from Q-Ti to Qti (lines 5 and 6). Algorithm 4: DP operator for MBDP data: Bt, W-point data: i, Qt + 1i and their vectors are moved upwards by Q vectors Qti (lines 5 and 6)."}, {"heading": "3.3 PBIP", "text": "Point Based Incremental Pruning (PBIP) [Dibangoye et al., 2009] takes a different approach to solving the scalability problem of full backup (algorithm 5). It avoids full generation and enumeration of common strategies by using a phase of searching for a common tree instead of the two phases of backup and editing (algorithm 5). For each point b, an in-depth search for the optimal common tree is performed (line 5). Branch A search tree node q represents a set of common strategy trees q defined by a set of constraints. It can be considered a partially defined strategy tree. Each child of the search tree is a subset of its parent tree defined by an additional constraint on q, either by defining common strategies qA or by defining a common strategy tree q."}, {"heading": "4 Proposition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Intuitive Idea", "text": "For simplicity of presentation, our approach is described first in each case. (1) A first approach for algorithm 5: DP Operator for PBIP Data: Bt, W points Data: i, Qt + 1i and their \u03b1-vectors Result: i, Qti and their \u03b1-vectors, with Qti, W Sel, 1 foreach i, I do Qti + 1 foreach b, Bt do3Let the search space Qt + 1 = iAi \u00b7 (Q t + 1 i) i4 Search (branch and bound) q = (qi) i, I as5 arg maxq, Qt + 1\\ Sel, Vq (b) Sel, 6 foreach i, I do I, Qti, Qti, Qti, Qti, 7 the selection of trees consists of maximizing the mean of V t (b): maximise, V t, t, t, t, t, t, b."}, {"heading": "4.2 Criterion", "text": "A Dec-POMDP can be considered a POMDP with a limitation on the form of its policy: the policy must be decentralized, and this limitation is added to criterion (2). In view of points Bt, we are looking for a Qti set of at most W trees among the | Ai | W | i | generated by a complete backup: Maximizing b-Bt max q-i-I Qti Vq (b) (3).t. In order to better reflect the decentralized nature of the problem, we may want to work with the DecPOMDP beliefs: however, it is not clear how these beliefs should be treated, since each actor has his or her own beliefs."}, {"heading": "4.3 Algorithm", "text": "PSMBDP builds a policy with a memory-constrained DP operator: the DP operator uses an exhaustive backup phase and a cut-phase solution (3). Algorithm 6 is a general PSMBDP algorithm with a two-stage DP operator. In contrast to the MBDP-based approaches, the number of samples can be higher than the number W of trees, which makes it possible to cover the faith space better. In practice, a heuristic solution of (3) is sought first. In the current implementation, criterion (3) is solved with a heuristic greedy approach (algorithm 7). The common policy tree q to maximize this criterion is selected first (line 2): This can be done by means of a PBIP search for the mean faith, b = 1 | Bt |] n, Bt \u00b2 n, Bt \u00b2 n, Bt \u00b2, Bt \u00b2, Bt \u00b2, Bt \u00b2, Bt \u00b2, Bt \u00b2, its local trees are placed in the Qti, the Sets, Qti \u00b2, an algorithm, q \u00b2, then an algorithm, i \u00b2 i i, i, i, i, i, i, i, i, i, i \u00b2, and i \u00b2."}, {"heading": "4.4 Branch-and-Bound Search", "text": "The question that arises is whether it is a local tree, a local tree that has defined itself as a local tree, and a local tree that cannot be defined as a local tree, but rather as a local tree, whose local tree cannot be defined as a local tree. (The question is what defines the local tree as a local tree, but as a local tree whose local tree is defined not as a local tree, but as a local tree, whose local tree cannot be defined as a local tree.) The question is what defines the local tree as a local tree, is the question what defines the local tree as a local tree. (The question what defines the local tree as a local tree, is the local tree.) The question is what defines the local tree as a local tree can be defined as a local tree. (The question what defines the local tree as a local tree can be defined as a local tree, is the question whether the local tree can be defined as a local tree.) The question is whether the local tree can be defined as a local tree, is the question whether the local tree can be defined as a local tree. (The question whether the local tree can be defined as a local tree can be defined as a local tree.) The question is whether the local tree can be defined as a local tree be defined as a local tree is the local tree, is the local tree as a local tree, is the local tree."}, {"heading": "5 Experiments", "text": "This section contains the results of various algorithms for some benchmark problems, the average expected reward (AEV), its standard deviation (\u03c3) and the mean computation time (T) in seconds. Algorithms PBIP / BeFS is a variant of PBIP with the best initial search. PSMBDP and PSMBDP / BeFS are respectively exhaustive PSMBDP and PSMBDP with the best initial search. In some cases, the value of the optimal policy of the underlying MDP is also shown: it is a ceiling of the Dec-POMDP policy assessments. Heuristics PSMBDP takes into account the primacy of the beliefs of the underlying POMDP when using a heuristic policy and refers to example beliefs on these distributions: one half of the points is greedily used by the POMDP policy with respect to the optimal MDP value function; the other half is taken from the random policy. MGDP and PDP function use the heuristic policy of the states based on the previous half of the MDP policy."}, {"heading": "5.1 Dec-Tiger", "text": "Table 1 shows results for the Dec Tiger problem [Nair et al., 2003] for H = 100, with 50 executions and N = 100 for PSMBDP. The (b) variant uses the same heuristic method as MBDP. The use of POMDP belief before heuristics does not yield good results. It is not necessary to give the PSMBDP a high value of W to find a good solution, since a smaller (usually 5) number of political trees per time step of PSMBDP. The calculation times are lower for PSMBDP for this problem, because only a small number of trees per agent are obtained: the average width of local policy trees is 5 for all heuristics. The other trees are not kept because they do not increase the predetermined criterion. Therefore, the number of trees and \u03b1 vectors generated by exhaustive fuse is lower."}, {"heading": "5.2 Firefighters", "text": "The results for the problem of fire fighting [Oliehoek et al., 2008] with 2 agents, 4 houses and 3 fire stages are shown in Table 2 for Horizon 50 with W = 7. For PSMBDP with N = 100. PSMBDP finds solutions of better quality than MBDP."}, {"heading": "5.3 Modified Firefighters", "text": "To test problems with a larger number of observations, we use a modified fire-fighting problem, in which each actor observes the resulting fire intensity in the house he goes to. Table 3 shows results for this modified problem with | I | = 2, 4 houses, fires from 0 to 3, H = 100, averaged to 25 executions with W = 3 and N = 100 for PSMBDP. PSMBDP finds solutions of better quality than PGDP."}, {"heading": "5.4 Cooperative Box Pushing", "text": "The Cooperative Box Pushing Problem [Seuken and Zilberstein, 2007a] is a problem with a higher number of observations (5 observations per agent for two agents) and it has been suggested to name algorithms that can handle a larger number of observations. On average, the results shown in Table 4 are over 25 executions. PGDP time out for H = 50 (T > 20,000). PSMBDP finds slightly better solutions than PGDP for a comparable computing time."}, {"heading": "6 Conclusion", "text": "We have introduced a new optimization criterion to select the policy trees in a memory-limited approach of dynamic bottom-up programming, i.e. maximizing the expected reward for a heuristic policy versus beliefs. We have introduced PSMBDP, a planning algorithm for dec-POMDPs based on this principle. Simple implementation using an exhaustive list of strategies can be used for small problems. To solve problems with a higher number of observations, a version with implicit listing of common policy trees can be used by means of split and bound search. PSMBDP is able to find solutions of better quality than MBDP and PBIP. However, the results depend heavily on the problems and heuristics used. Our optimization scheme uses a simple greedy heuristics, so we will be able to find solutions of better quality through the use of better optimization methods."}], "references": [{"title": "Mixed integer linear programming for exact finite-horizon planning in decentralized POMDPs", "author": ["Aras et al", "R. 2007] Aras", "A. Dutech", "F. Charpillet"], "venue": "In Proceedings of the Thirteenth International Conference on Automated Planning and", "citeRegEx": "al. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "al. et al\\.", "year": 2007}, {"title": "The complexity of decentralized control of markov decision processes", "author": ["Bernstein et al", "D.S. 2000] Bernstein", "S. Zilberstein", "N. Immerman"], "venue": "In Mathematics of Operations Research,", "citeRegEx": "al. et al\\.,? \\Q2000\\E", "shortCiteRegEx": "al. et al\\.", "year": 2000}, {"title": "Value-based observation compression for DEC-POMDPs", "author": ["Carlin", "Zilberstein", "A. 2008] Carlin", "S. Zilberstein"], "venue": "Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems,", "citeRegEx": "Carlin et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Carlin et al\\.", "year": 2008}, {"title": "Point-based incremental pruning heuristic for solving finitehorizon dec-pomdps", "author": ["Dibangoye et al", "J.S. 2009] Dibangoye", "Mouaddib", "A.-I", "B. Chaib-draa"], "venue": "In AAMAS \u201909: Proceedings of The 8th International Conference on Au-", "citeRegEx": "al. et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al. et al\\.", "year": 2009}, {"title": "Dynamic programming for partially observable stochastic games", "author": ["Hansen et al", "E. 2004] Hansen", "D. Bernstein", "S. Zilberstein"], "venue": "In Proceedings of the Nineteenth National Conference on Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q2004\\E", "shortCiteRegEx": "al. et al\\.", "year": 2004}, {"title": "Taming Decentralized POMDPs: Towards Efficient Policy Computation for Multiagent Settings", "author": ["Nair et al", "R. 2003] Nair", "M. Tambe", "M. Yokoo", "D. Pynadath", "S. Marsella"], "venue": "Proceedings of the Twentieth International Joint Con-", "citeRegEx": "al. et al\\.,? \\Q2003\\E", "shortCiteRegEx": "al. et al\\.", "year": 2003}, {"title": "Optimal and approximate Q-value functions for decentralized POMDPs", "author": ["Oliehoek et al", "F.A. 2008] Oliehoek", "M.T.J. Spaan", "N. Vlassis"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "al. et al\\.,? \\Q2008\\E", "shortCiteRegEx": "al. et al\\.", "year": 2008}, {"title": "Improved Memory-Bounded Dynamic Programming for Decentralized POMDPs", "author": ["Seuken", "Zilberstein", "S. 2007a] Seuken", "S. Zilberstein"], "venue": "In Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligences (UAI-07),", "citeRegEx": "Seuken et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Seuken et al\\.", "year": 2007}, {"title": "Memory-Bounded Dynamic Programming for DEC-POMDPs", "author": ["Seuken", "Zilberstein", "S. 2007b] Seuken", "S. Zilberstein"], "venue": "Proceedings of the Twentieth International Joint Conference on Artificial Intelligences (IJCAI-07)", "citeRegEx": "Seuken et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Seuken et al\\.", "year": 2007}, {"title": "Point-based dynamic programming for DEC-POMDPs", "author": ["Szer", "Charpillet", "D. 2006] Szer", "F. Charpillet"], "venue": "In Proceedings of the TwentyFirst AAAI National Conference on Artificial Intelligence", "citeRegEx": "Szer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Szer et al\\.", "year": 2006}, {"title": "MAA*: A Heuristic Search Algorithm for Solving Decentralized POMDPs", "author": ["Szer et al", "D. 2005] Szer", "F. Charpillet", "S. Zilberstein"], "venue": "In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "al. et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al. et al\\.", "year": 2005}], "referenceMentions": [], "year": 2010, "abstractText": "We propose a new point-based method for approximate planning in Dec-POMDP which outperforms the state-of-the-art approaches in terms of solution quality. It uses a heuristic estimation of the prior probability of beliefs to choose a bounded number of policy trees: this choice is formulated as a combinatorial optimisation problem minimising the error induced by pruning.", "creator": "TeX"}}}