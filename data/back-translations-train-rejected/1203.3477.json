{"id": "1203.3477", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "A Scalable Method for Solving High-Dimensional Continuous POMDPs Using Local Approximation", "abstract": "Partially-Observable Markov Decision Processes (POMDPs) are typically solved by finding an approximate global solution to a corresponding belief-MDP. In this paper, we offer a new planning algorithm for POMDPs with continuous state, action and observation spaces. Since such domains have an inherent notion of locality, we can find an approximate solution using local optimization methods. We parameterize the belief distribution as a Gaussian mixture, and use the Extended Kalman Filter (EKF) to approximate the belief update. Since the EKF is a first-order filter, we can marginalize over the observations analytically. By using feedback control and state estimation during policy execution, we recover a behavior that is effectively conditioned on incoming observations despite the unconditioned planning. Local optimization provides no guarantees of global optimality, but it allows us to tackle domains that are at least an order of magnitude larger than the current state-of-the-art. We demonstrate the scalability of our algorithm by considering a simulated hand-eye coordination domain with 16 continuous state dimensions and 6 continuous action dimensions.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (291kb)", "http://arxiv.org/abs/1203.3477v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tom erez", "william d smart"], "accepted": false, "id": "1203.3477"}, "pdf": {"name": "1203.3477.pdf", "metadata": {"source": "CRF", "title": "A Scalable Method for Solving High-Dimensional Continuous POMDPs Using Local Approximation", "authors": ["Tom Erez", "William D. Smart"], "emails": ["etom@cse.wustl.edu", "wds@cse.wustl.edu"], "sections": [{"heading": null, "text": "Partially observable Markov decision-making processes (POMDPs) are typically solved by finding an approximate global solution for a corresponding faith MDP. In this paper, we offer a new planning algorithm for POMDPs with continuous state, action, and observation spaces. As such areas have an inherent notion of locality, we can find an approximate solution using local optimization methods. We parameterize the faith distribution as a Gaussian mixture and use the Advanced Kalman Filter (EKF) to approximate faith actualization. Since the EKF is a filter of the first order, we can analytically marginalize the observations. By using feedback control and state assessment during policy execution, we recover a behavior that, despite unconditional planning, is effectively dependent on incoming observations. Local optimization does not provide guarantees of global optimization, but it allows us to address areas larger than the current state of the art."}, {"heading": "1 INTRODUCTION", "text": "This year it is more than ever before."}, {"heading": "2 DEFINITIONS", "text": "We consider a discrete time-limited POMDP defined by a tuple < S, A, Z, T, E, R, N >, where S, A, and Z represent the state space, action space, or observation space; T (s) = Pr (s) = Pr (s) | s, a) is a transitional function that describes the probability of the next state in light of the current state and action; and R is a time-dependent reward function Ri (s, a) with a terminal reward RN (s). In this essay, we consider a discounted optimization criterion where the agent's goal is to maximize the expected cumulative reward within a fixed time horizon N. This formulation is a departure from the general focus on discounted horizons, and we adopt it because it is useful for the local optimal control algorithm (section 4)."}, {"heading": "3 THE BELIEF DOMAIN", "text": "The faith state b-B associated with a faith is a probability distribution over S, where bi (s) is the probability that the true state at the time is i. In order to construct the faith range of a given POMDP, we must find a representation for b and define the reward function and dynamics (faith actualization) over this space. In view of the current faith b, an act a and observation e.g., the actualized faith b (s) can be calculated by applying the Bayes Rule. To make this function mathematically traceable, we must apply a certain approximation b (s) to true faith b (s) and commit ourselves to a state treasure filter in order to actualize the approximate faith b (s). The reward associated with a faith is simply the expected value over this state distribution: Ri (a), b (a), b (b), b (b), b (b), (b), (b), (b), (b), (b, (b), (b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, (b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, (b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, (b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b, b"}, {"heading": "3.1 CONTINUOUS DYNAMICS", "text": "In this section, we focus on non-linear stochastic dynamics ds = f (s, a) dt + q (s, a) d \u00b2 s, which is a Viennese process. Integrating these dynamics over a small time step leads to a normal distribution over the next state s \u00b2 (s, s, a), (3) and the covariance Q = \u03c4qTq is a time scale of the continuous process qd \u00b2 s. Similarly, we focus on observation distributions of the form: (z, s, a) = N (s, a), W (s, a). The covariance Q = \u03c4qTq is a time scale of the continuous process qd \u00b2 s. Similarly, we focus on the observation distributions of the form: z, s, a) = N (s, a)."}, {"heading": "3.2 UNILATERAL CONSTRAINTS", "text": "In the previous section, we assumed that F and W can be active at any given time. However, this assumption may be too restrictive for some areas; in particular, it excludes discontinuous dynamics that occur due to unilateral constraints. In this section, we consider areas of interest to be areas of ambiguity due to contact, object manipulation and locomotion: ds = f (s, a) dt + Q (s, a) d (s) d (s) d (s) p (s) p (s) p (s) p (p) p (p) p (s) p (p) - (p) p (p) p (s) p (p) p (p) p (p) p (p) p) p (p) p (p) p (p) p (p) - \"p\" p \"p\" p \"s\" (s) p (p) s (p) - (p) p (p) s (p) p (p) - (p) p (s) p (p) - (p) p (s) p (p) p (p) - (p) p (p) p (p) p (p) p (p) - (p) p (p) p (p) p (p) p (p) p (p)."}, {"heading": "3.2.1 Truncation", "text": "In order to recalibrate the belief in constraint, we linearize the constraint function by analyzing the abbreviated normal distributions (Boutilier, 2002; Toussaint, 2009). We can rotate and scale the state space more linear to ensure that the constraint is manifold and that the uncertainty in this dimension is independent of the others. Therefore, we can focus our analysis on the one-dimensional case, assuming without loss of universality that the constraint does not affect any dimension, but k.Let us leave x-N (\u00b5, 2). If we are bound to an interval x-gate (l, u), its distribution becomes: Pr (x), 2-dimensional culture, u-distributions."}, {"heading": "3.2.2 Mixture Reduction", "text": "To maintain our form (a Gaussian without limitation, a Gaussian without limitation), we reduce this fourGaussian mixture to two and project the second Gaussian on compulsion. The reduction of a mixture of two Gaussians {\u03bd1, \u03bd2, \u03b1} results in a single Gaussian whose mean and covariance has the following values: s = 1 + (1 \u2212 \u03b1) s 2, (10a) 44, (1 \u2212 \u03b1) 44, 2 + (1 \u2212 \u03b1) (s \u00b2 1 \u2212 s 2) (s \u00b2 2) T (10b) Using these equations, we combine the two Gaussians over the compulsion to a single Gaussian constriction and the two Gaussians below the compulsion to a second."}, {"heading": "4 PLANNING", "text": "The belief actualization schemes of the previous section (together with (1)) define a problem of deterministic optimal control in a high-dimensional continuous space with nonlinear dynamics and non-square reward that can be solved by a variety of techniques; Platt et al. (2010), for example, uses the multi-recording method. We use a local optimization scheme called Differential Dynamic Programming (DDP), an algorithm that has been successfully applied to high-dimensional, non-linear control domains in the real world (e.g. Abbeel & Ng, 2005; Tassa et al., 2008). Interested readers can find a detailed description of the algorithm in Jacobson & Mayne (1970). The advantage of using DDP is that in addition to the locally optimal trajectory and the open action sequence that realizes this trajectory, it generates a sequence of linear feedback winning matrices."}, {"heading": "5 POLICY EXECUTION", "text": "Since a policy for a continuous POMDP is infinite, it also needs to be parameterized. In this paper, we focus on measures that are locally linear: \u03c0 (b, i) = a-i + Li (b-i) (13) for some parameterized states of belief b-1: N, actions a-1: N-1, and feedback gains matrices L1: N-1. This parameterization corresponds to the output of differential dynamic programming as described in the previous paragraph. Policy is implemented after planning, as the actor interacts with the environment. Incoming observations are filtered by government assessment, and the feedback control responds to changes in the perceived state and responds accordingly. Therefore, the behavior of the actor depends on observations received, even if they were marginalized during planning. At this stage, we are no longer obliged to faith actualization programs from Section 3, and a more precise filter (e.g., a particle filter that can be used for another one) that is less dangerous."}, {"heading": "6 RESULTS", "text": "First, we demonstrate the main features of our method using an example of planar navigation, which roughly corresponds to the areas considered by Roy & Thrun (1999) and Brooks (2009), and then demonstrate the scalability of our method by solving a 16-dimensional problem first presented by Erez & Smart (2009)."}, {"heading": "6.1 PLANAR NAVIGATION", "text": "The resulting optimal behavior (Figure 1 (a)) is found in less than a minute: the robot dodges obstacles by approaching the side wall, and then \"cuts\" the corner on its way to the target at the bottom wall. To investigate the effect of linearizing the constraint, we tested a case in which the agent interacts with the curved segment of the constraint. As Figure 1 (b) shows, the optimal path in this case follows without difficulty around the corner. The unambiguous property of the contact with the wall is referred to as \"coastal navigation\" (Roy & Thrun, 1999), and our algorithm is able to identify and use this feature as it emerges in the optimal solution. The unclear property of the contact with the wall is referred to as \"optimal coastal navigation.\""}, {"heading": "6.2 HAND-EYE COORDINATION", "text": "In fact, most people are able to move to another world, in which they are able, in which they are able to move."}, {"heading": "7 DISCUSSION", "text": "This paper offers a new perspective on solving the continuous POMDPs. Instead of using a global approach in a faith MDP, we marginalize the observations and throw the infinite dimensional, stochastic faith domain into a finite, optimal control. The approach we are presenting here applies to areas whose dynamics are smooth (and thus Gaussian beliefs represent a good approximation) or with a one-sided constraint (where we use a specifically structured two-dimensional mixture); however, our approximations cannot be held for areas with a more elaborate structure."}, {"heading": "Acknowledgements", "text": "This work was supported by the NSF-Prize BCS 0924609"}], "references": [{"title": "Exploration and apprenticeship learning in reinforcement learning", "author": ["Abbeel", "Pieter", "Ng", "Andrew Y"], "venue": "In International Conference on Machine Learning (ICML), pp", "citeRegEx": "Abbeel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Abbeel et al\\.", "year": 2005}, {"title": "A POMDP formulation of preference elicitation problems", "author": ["Boutilier", "Craig"], "venue": "In Eighteenth national conference on Artificial intelligence,", "citeRegEx": "Boutilier and Craig.,? \\Q2002\\E", "shortCiteRegEx": "Boutilier and Craig.", "year": 2002}, {"title": "Parametric POMDPs", "author": ["Brooks", "Alex"], "venue": "VDM Verlag,", "citeRegEx": "Brooks and Alex.,? \\Q2009\\E", "shortCiteRegEx": "Brooks and Alex.", "year": 2009}, {"title": "Continuous-state POMDPs with hybrid dynamics", "author": ["Brunskill", "Emma", "Kaelbling", "Leslie", "Lozano-Perez", "Tomas", "Roy", "Nicholas"], "venue": "In Tenth International Symposium on Artificial Intelligence and Mathematics (ISAIM),", "citeRegEx": "Brunskill et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Brunskill et al\\.", "year": 2008}, {"title": "Dictionary of Eye Terminology", "author": ["Cassin", "Barbara", "Rubin", "Melvin L"], "venue": "Triad Publishing Company (FL),", "citeRegEx": "Cassin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Cassin et al\\.", "year": 2001}, {"title": "Bipedal walking on rough terrain using manifold control", "author": ["T. Erez", "W.D. Smart"], "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),", "citeRegEx": "Erez and Smart,? \\Q2007\\E", "shortCiteRegEx": "Erez and Smart", "year": 2007}, {"title": "What does shaping mean for computational reinforcement learning", "author": ["Erez", "Tom", "Smart", "William D"], "venue": "IEEE International Conference on Development and Learning (ICDL),", "citeRegEx": "Erez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Erez et al\\.", "year": 2008}, {"title": "Coupling perception and action using minimax optimal control", "author": ["Erez", "Tom", "Smart", "William D"], "venue": "In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),", "citeRegEx": "Erez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Erez et al\\.", "year": 2009}, {"title": "Region-based incremental pruning for POMDPs", "author": ["Feng", "Zhengzhu", "Zilberstein", "Shlomo"], "venue": "In The 20th conference on Uncertainty in artificial intelligence (UAI),", "citeRegEx": "Feng et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2004}, {"title": "Solving POMDPs with continuous or large discrete observation spaces", "author": ["Hoey", "Jesse", "Poupart", "Pascal"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Hoey et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hoey et al\\.", "year": 2005}, {"title": "Grasping POMDPs", "author": ["Hsiao", "Kaijen", "Kaelbling", "Leslie Pack", "Lozano-Perez", "Tomas"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Hsiao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hsiao et al\\.", "year": 2007}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["Kaelbling", "Leslie Pack", "Littman", "Michael L", "Cassandra", "Anthony R"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "A POMDP framework for coordinated guidance of autonomous uavs for multitarget tracking", "author": ["Miller", "Scott A", "Harris", "Zachary A", "Chong", "Edwin K. P"], "venue": "EURASIP Journal of Advanced Signal Process,", "citeRegEx": "Miller et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2009}, {"title": "Belief space planning assuming maximum likelihood observations", "author": ["Platt", "Robert", "Tedrake", "Russ", "Kaelbling", "Leslie Pack", "Lozano-Perez", "Tomas"], "venue": "In Robotics: Science and Systems (R:SS),", "citeRegEx": "Platt et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Platt et al\\.", "year": 2010}, {"title": "Point-based value iteration for continuous POMDPs", "author": ["Porta", "Josep M", "Vlassis", "Nikos", "Spaan", "Matthijs T.J", "Poupart", "Pascal"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Porta et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Porta et al\\.", "year": 2006}, {"title": "The belief roadmap: Efficient planning in belief space by factoring the covariance", "author": ["S. Prentice", "N. Roy"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Prentice and Roy,? \\Q2009\\E", "shortCiteRegEx": "Prentice and Roy", "year": 2009}, {"title": "Coastal navigation with mobile robots", "author": ["Roy", "Nicholas", "Thrun", "Sebastian"], "venue": "In Advances in Neural Processing Systems (NIPS),", "citeRegEx": "Roy et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Roy et al\\.", "year": 1999}, {"title": "The Optimal Control of Partially Observable Markov Processes", "author": ["E.J. Sondik"], "venue": "PhD thesis,", "citeRegEx": "Sondik,? \\Q1971\\E", "shortCiteRegEx": "Sondik", "year": 1971}, {"title": "Planning with continuous actions in partially observable environments", "author": ["Spaan", "Matthijs T. J", "Vlassis", "Nikos A"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Spaan et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Spaan et al\\.", "year": 2005}, {"title": "Optimal Control and Estimation", "author": ["Stengel", "Robert F"], "venue": "Dover Publications,", "citeRegEx": "Stengel and F.,? \\Q1994\\E", "shortCiteRegEx": "Stengel and F.", "year": 1994}, {"title": "Rigid-body dynamics with friction and impact", "author": ["Stewart", "David E"], "venue": "SIAM Reviews,", "citeRegEx": "Stewart and E.,? \\Q2000\\E", "shortCiteRegEx": "Stewart and E.", "year": 2000}, {"title": "Policies based on trajectory libraries", "author": ["Stolle", "Martin", "Atkeson", "Chris"], "venue": "In IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "Stolle et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Stolle et al\\.", "year": 2006}, {"title": "Receding horizon differential dynamic programming", "author": ["Tassa", "Yuval", "Erez", "Tom", "Smart", "William"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Tassa et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tassa et al\\.", "year": 2008}, {"title": "LQR-trees: Feedback motion planning on sparse randomized trees", "author": ["Tedrake", "Russ"], "venue": "In Robotics: Science and Systems (R:SS),", "citeRegEx": "Tedrake and Russ.,? \\Q2009\\E", "shortCiteRegEx": "Tedrake and Russ.", "year": 2009}, {"title": "Monte carlo POMDPs", "author": ["S. Thrun"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Thrun,? \\Q2000\\E", "shortCiteRegEx": "Thrun", "year": 2000}, {"title": "Pros and cons of truncated gaussian ep in the context of approximate inference control", "author": ["Toussaint", "Marc"], "venue": "NIPS workshop on Probabilistic Approaches for Robotics and Control,", "citeRegEx": "Toussaint and Marc.,? \\Q2009\\E", "shortCiteRegEx": "Toussaint and Marc.", "year": 2009}], "referenceMentions": [{"referenceID": 14, "context": "edu interest in tackling continuous domains (Porta et al., 2006; Brooks, 2009).", "startOffset": 44, "endOffset": 78}, {"referenceID": 11, "context": "The standard approach to solving POMDPs is to find an approximate solution to the fully-observable belief -MDP, whose states are probability distributions over the state space of the original POMDP (Kaelbling et al., 1998).", "startOffset": 198, "endOffset": 222}, {"referenceID": 24, "context": "However, the belief space of a continuous POMDP is infinite-dimensional, and must be approximated (Thrun, 2000).", "startOffset": 98, "endOffset": 111}, {"referenceID": 17, "context": "The optimal value function of belief-MDPs is piecewiselinear and convex in the discrete case (Sondik, 1971), and this also holds for some cases of continuous state (Porta et al.", "startOffset": 93, "endOffset": 107}, {"referenceID": 14, "context": "The optimal value function of belief-MDPs is piecewiselinear and convex in the discrete case (Sondik, 1971), and this also holds for some cases of continuous state (Porta et al., 2006), as long as the observations and actions are discrete.", "startOffset": 164, "endOffset": 184}, {"referenceID": 3, "context": "This result was used to tackle domains with continuous hybrid-linear dynamics by Brunskill et al. (2008). Other combinations of the discrete and the continuous domains were also considered (Hoey & Poupart, 2005; Spaan & Vlassis, 2005).", "startOffset": 81, "endOffset": 105}, {"referenceID": 12, "context": "The method we present here is very similar to Miller et al. (2009), who used the term nominalbelief optimization, and Platt et al.", "startOffset": 46, "endOffset": 67}, {"referenceID": 12, "context": "The method we present here is very similar to Miller et al. (2009), who used the term nominalbelief optimization, and Platt et al. (2010), who used the term maximum-likelihood observations.", "startOffset": 46, "endOffset": 138}, {"referenceID": 22, "context": "We use a local optimization scheme called Differential Dynamic Programming (DDP), an algorithm that has been successfully applied to real-world high-dimensional, non-linear control domains (e.g., Abbeel & Ng, 2005; Tassa et al., 2008).", "startOffset": 189, "endOffset": 234}, {"referenceID": 13, "context": "This can be solved using a variety of techniques; Platt et al. (2010), for example, use the multiple shooting method.", "startOffset": 50, "endOffset": 70}, {"referenceID": 13, "context": "This can be solved using a variety of techniques; Platt et al. (2010), for example, use the multiple shooting method. We use a local optimization scheme called Differential Dynamic Programming (DDP), an algorithm that has been successfully applied to real-world high-dimensional, non-linear control domains (e.g., Abbeel & Ng, 2005; Tassa et al., 2008). The interested reader may find an in-depth description of the algorithm in Jacobson & Mayne (1970).", "startOffset": 50, "endOffset": 453}, {"referenceID": 24, "context": "First, we demonstrate key features of our method by considering an example of planar navigation, roughly corresponding to domains considered by Roy & Thrun (1999) and Brooks (2009).", "startOffset": 150, "endOffset": 163}, {"referenceID": 24, "context": "First, we demonstrate key features of our method by considering an example of planar navigation, roughly corresponding to domains considered by Roy & Thrun (1999) and Brooks (2009). Then, we demonstrate the scalability of our method by solving a 16-dimensional problem, first presented by Erez & Smart (2009).", "startOffset": 150, "endOffset": 181}, {"referenceID": 24, "context": "First, we demonstrate key features of our method by considering an example of planar navigation, roughly corresponding to domains considered by Roy & Thrun (1999) and Brooks (2009). Then, we demonstrate the scalability of our method by solving a 16-dimensional problem, first presented by Erez & Smart (2009).", "startOffset": 150, "endOffset": 309}, {"referenceID": 24, "context": "The disambiguating property of the contact with the wall is termed \u201ccoastal navigation\u201d (Roy & Thrun, 1999), and our algorithm is able to identify and leverage this feature as it emerges in the optimal solution. We cannot offer a direct comparison of our results with Brooks (2009), since his experiments were conducted on real robots.", "startOffset": 95, "endOffset": 282}, {"referenceID": 22, "context": "One natural extension of this work could employ local optimization from multiple starting points, creating a controller that uses a trajectory library (Stolle & Atkeson, 2006; Tassa et al., 2008).", "startOffset": 151, "endOffset": 195}], "year": 2010, "abstractText": "Partially-Observable Markov Decision Processes (POMDPs) are typically solved by finding an approximate global solution to a corresponding belief-MDP. In this paper, we offer a new planning algorithm for POMDPs with continuous state, action and observation spaces. Since such domains have an inherent notion of locality, we can find an approximate solution using local optimization methods. We parameterize the belief distribution as a Gaussian mixture, and use the Extended Kalman Filter (EKF) to approximate the belief update. Since the EKF is a first-order filter, we can marginalize over the observations analytically. By using feedback control and state estimation during policy execution, we recover a behavior that is effectively conditioned on incoming observations despite the unconditioned planning. Local optimization provides no guarantees of global optimality, but it allows us to tackle domains that are at least an order of magnitude larger than the current state-of-the-art. We demonstrate the scalability of our algorithm by considering a simulated hand-eye coordination domain with 16 continuous state dimensions and 6 continuous action dimensions.", "creator": "TeX"}}}