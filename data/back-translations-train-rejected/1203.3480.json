{"id": "1203.3480", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Learning Game Representations from Data Using Rationality Constraints", "abstract": "While game theory is widely used to model strategic interactions, a natural question is where do the game representations come from? One answer is to learn the representations from data. If one wants to learn both the payoffs and the players' strategies, a naive approach is to learn them both directly from the data. This approach ignores the fact the players might be playing reasonably good strategies, so there is a connection between the strategies and the data. The main contribution of this paper is to make this connection while learning. We formulate the learning problem as a weighted constraint satisfaction problem, including constraints both for the fit of the payoffs and strategies to the data and the fit of the strategies to the payoffs. We use quantal response equilibrium as our notion of rationality for quantifying the latter fit. Our results show that incorporating rationality constraints can improve learning when the amount of data is limited.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (175kb)", "http://arxiv.org/abs/1203.3480v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["xi alice gao", "avi pfeffer"], "accepted": false, "id": "1203.3480"}, "pdf": {"name": "1203.3480.pdf", "metadata": {"source": "CRF", "title": "Learning Game Representations from Data Using Rationality Constraints", "authors": ["Xi Alice Gao"], "emails": ["xagao@seas.harvard.edu", "apfeffer@cra.com"], "sections": [{"heading": null, "text": "One answer is to learn the representations from data. If you want to learn both the payouts and the strategies of the players, a naive approach is to learn both directly from the data. This approach ignores the fact that the players could play reasonably good strategies, so there is a link between the strategies and the data. The main contribution of this paper is to make this connection during learning. We formulate the learning problem as a weighted satisfaction problem with limitations both in terms of adjusting the payouts and strategies to the data and in adjusting the strategies to the payouts. We use quantum balance as our idea of rationality to quantify the latter fit. Our results show that the inclusion of rationality constraints can improve learning when the amount of data is limited."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Normal Form Games", "text": "Such a game G = [I, A, U] is given by a group of players I with | I | = N, a group of pure strategies A = \u00b7 i-IAi, in which Ai is the set of pure strategies for players i-I, and U = \u00b7 i-Iui, in which ui: A \u2192 < the payout function for players i-I. A mixed strategy of player i is denoted by \u03c3i: Ai \u2192 [0, 1], in which an \"Ai\" (a) = 1 is a probability distribution over all pure strategies of player i. We use \u2212 i to denote the set of all other players as players i. Thus a \u2212 i and ppi \u2212 i denote the common pure strategy and mixed strategy profile of the group of players \u2212 i. We focus on finite games for which the group of players I and the sets of pure strategies Ai for each player i-I. The game G is considered complete information in the sense that each player has the full knowledge of the other player."}, {"heading": "2.2 Equilibrium Notions", "text": "In our learning model, we look at two popular equilibrium terms: the mixed strategy Nash Equilibrium strategies and the quantum response Equilibrium functions. (Nash Equilibrium profile assumes that each player is perfectly rational. \u2212 In the Nash balance, each player i responds best to the mixed strategy profile of all other players. \u2212 i.Definition: 1 A common mixed strategy profile is Nash balance when each player i plays the best answer to the strategy profiles of his opponents \u2212 i, d.i.ui (\u03c3 - i, \u03c3 - i) is more equilibrium profile when each player i is the best answer to the strategy profile of his opponents \u2212 i, i.i.The definition implies that no player i can benefit from it in expectation by unilaterally deviating from the Nash balance strategy profile."}, {"heading": "2.3 Weighted Constraint Satisfaction Problems", "text": "A classic constraint satisfaction problem (CSP) is defined by a set of variables, each with a limited domain and a set of constraints. Each constraint consists of a subset of variables and a relationship to the variables in the constraint. A solution for a CSP is an assignment of values to the variables, so that each constraint is met, i.e. the projection of the assignment to the constraint variables is in the constraint relation. A weighted CSP (WCSP) is similar to a CSP except that it has two types of constraints. Hard constraints are like those in an ordinary CSP. Soft constraints consist of a set of variables and a cost function of these variables to non-negative numbers. The purpose of solving a CSP is to find an assignment that meets all hard constraints, while minimizing the sum of the cost of soft constraints for assignment."}, {"heading": "3 Problem Definition", "text": "We consider the generation of M-examples of the game of a One-Shot Simultaneous-Move Game G. G is a complete information game, so that players have complete knowledge of the payout matrix of the game. We assume that player i selects a fixed mixed strategy \u03c3i selected in advance and plays it for each sample generated. This is a simplistic assumption that allows us to treat the training instances as independent; relaxation is a topic for future work. For the k-th sample, player i decides to play some pure strategy ai-Ai and requires some real estimated payouts ui (A), where A the common pure strategy profile is chosen by all players; in our fruit supplier example, the players are the fruit providers, and the pure strategies of each player are the location of the stall, the types of fruit are sold, and the prizes are awarded. This game is played for many times by the fruit providers."}, {"heading": "4 Our WCSP Learning Model", "text": "Our WCSP model has a variable corresponding to each player (A), which is the probability of the pure strategy chosen by all players. Since the domains of the strategies and payout variables are continuous, we declare them multipliers of the real estimated discrediting parameters and the actual payout vector of the player. We also limit the payout values ui (A) to be within a pre-defined range [umin, umax] where the strategies, umax < are the minimum and maximum of the observed payout values. Now we define the four types of limits in our WCSP learning model."}, {"heading": "5 Complexity and Constraint Decomposition", "text": "If there are N players and K pure strategies per player, it will be in the order of NK variables, so the size of the constraint in NK will be exponential. Exponential costs in N are acceptable because the size of the normal form representation itself is exponential in the number of players. Furthermore, representations like graphical games provide a handle on how to deal with games with a large number of players. However, exponential costs in K would severely limit the practicality of the approach. Fortunately, we can avoid these costs by decomposing constraints that we are describing now. We will go through all four types of constraints in turn."}, {"heading": "5.1 Strategy Maximum Likelihood Constraints", "text": "The maximum probability limitations of the strategy are simple limitations. In particular, there is a limitation for each pure strategy of each player. For N-players and K-pure strategies for each player, our WCSP has maximum probability limitations of the NK strategy. If we use as discrediting parameters for strategy variables, then the complexity of the maximum probability limitations of the strategy is NK."}, {"heading": "5.2 Strategy Consistency Constraints", "text": "Each strategy consistency constraint includes K strategy variables, and there are N such constraints. If we use 1 K values as discretization parameters for the strategy variables, then there are 1 value choices for each variable in order of 1. If we directly implement this consistency constraint, then there are 1 K values from which a small positive real number must be searched. The complexity of the strategy consistency constraints is N K, which exponentially translates into K.However, for each constraint, we define i additional K variables t1,... tk and break down the multivariable hard constraint into K hard constraints as a consequence. t1 = \u03c3i (a1) t2 = t1 + \u03c3i (a2)... tk = tk \u2212 1 + \u03c3i (an) = 1 (10) In view of this decomposition, we only have to look for 3 possible value tuples from K. This is a significant reduction of this constraint as K. > 3"}, {"heading": "5.3 Payoff Maximum Likelihood Constraints", "text": "The maximum payout probability constraints are simple constraints. There is a constraint for each pure strategy for each player i, for total NKN constraints. If we use \u03b4 as the discrediting parameter for payout variables, then the complexity of the maximum payout probability constraint is NK N\u03b4."}, {"heading": "5.4 Rationality Constraints", "text": "There is a limitation for each player's pure strategy i. With N players and K-pure strategies for each player, we have NK rationality constraints. For each player i and each player's pure strategy ak i, we can resolve the rationality constraints in several steps. First, the term \u03c3 \u2212 i (a \u2212 i) ui (ak, a \u2212 i) can be split into (N + 1) ternary constraints as follows: t1 = 1 (a1)... ti \u2212 1 = ti \u2212 2\u03c3i \u2212 1 (ai \u2212 1) ti \u2212 1\u03c3i + 1 (ai \u2212 1) kun \u2212 kun rationality (an) tn \u2212 tn (an) tn + 1 = tnui (ak, a \u2212 i) (11) Use xi to denote the last equation in the decomposition of the ith product."}, {"heading": "6 Evaluation", "text": "In this section we first discuss some basic methods with which we compare our learning method, then we describe our implementation, present our experimental results and discuss their impact."}, {"heading": "6.1 Baseline Methods", "text": "A totally naive approach is to learn the strategies and payouts of the game separately from the data provided. We call this method the Naive Method. This method is an approach with maximum probability, in which the strategies and payouts with the highest log probabilities are selected based on the observations. In our environment, the payouts are observed noisily. If the number of samples is small, the observed frequency of any pure strategy may not reflect the underlying mixed strategy that is played. Therefore, we expect the Naive Method to perform poorly when the standard deviations of payouts are high and when the number of samples is small. A more sophisticated approach is to learn the payout values from the observed data and to derive the mixed strategies that are played in solving an equilibrium based on a particular solution concept. We use NaiveNash to refer to the method with the Nash Equilibrium Concept and NaiRE Method with the QRE to refer to QRE."}, {"heading": "6.2 Experimental Setup", "text": "We evaluate our learning methods with computational experiments on randomly generated 2-player normal-form games with 2 pure strategies for each player (hereinafter referred to as the 2-player game). Our implementation consists of the following steps: 1. Generate a random 2-player normal-form game. 2. Use Gambit (McKelvey et al., 2007) to calculate LQRE of the game with different \u03bb values. 3. Given certain LQRE strategies, generate data about the game, provided the players play according to the stated strategies, and the payouts are generated from a Gaussian distribution around the payouts stated in the game. 4. Generate and store a WCSP problem in a predefined XML format. This XML file defines the variables, the domains of the variables, and the constraints in terms of cost related to value assignment tuples. 5. Solve the WCSP file with the 2 toolbar."}, {"heading": "6.3 Results and Discussion", "text": "We evaluated our learning method in three different settings. First, we varied the training size and compared our approach with the Naive, NaiveLQRE and NaiveNash methods. Second, we varied the \u03bb values and made the same comparison. Finally, we examined the performance of our algorithm using false \u03bb. However, the error of each learning instance is measured by calculating the Euclidean distance between the combined vectors of the actual versus learned strategy and payout values. First, we consider the deviation of the training size, i.e. the number of samples of the game being observed. If the training size is small, learning is expected to accurately represent the game as the limited amount of data does not accurately reflect the mixed strategy chosen by the players. Even with a limited amount of data, the pairing values with noise will be too distorted to reflect the true mean and standard deviations of the observed payout values."}, {"heading": "7 Conclusion and Future Work", "text": "Although our experimental results are preliminary, we hope that this work opens the door to exploring efficient and scalable implementations and approaches to the idea. In future work, we would first like to generate data for games with more players, more actions for each player, and a larger number of samples. We also plan to experiment with games with interesting structures. There are several other specific future directions. It would be preferable to make a variable that is learned rather than treat it as a fixed parameter that needs to be set. Manufacturing a variable can be taken into account in our WCSP framework. Also, we may not want to assume that all agents use the same formula, and our LQRE formulation can be generalized in this way."}, {"heading": "Acknowledgements", "text": "This work was supported by the Office of Scientific Research of the Air Force under the MURI contract 5710002613."}], "references": [{"title": "Semiring-based constraint satisfaction and optimization", "author": ["S. Bistarelli", "U. Montanari", "F. Rossi"], "venue": "Journal of the ACM,", "citeRegEx": "Bistarelli et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Bistarelli et al\\.", "year": 1997}, {"title": "Toolbar: a state-of-the-art platform for WCSP. http://www.inra.fr/bia/t/degivry/toolbar.pdf", "author": ["S. Bouveret", "F. Heras", "S. de Givry", "J. Larrosa", "M. Sanchez", "T. Schiex"], "venue": null, "citeRegEx": "Bouveret et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bouveret et al\\.", "year": 2004}, {"title": "Behavioral game theory", "author": ["C.F. Camerer"], "venue": null, "citeRegEx": "Camerer,? \\Q2003\\E", "shortCiteRegEx": "Camerer", "year": 2003}, {"title": "Learning graphical game models", "author": ["Q. Duong", "Y. Vorobeychik", "S. Singh", "M.P. Wellman"], "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence", "citeRegEx": "Duong et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2009}, {"title": "Learning and solving many-player games through a clusterbased representation", "author": ["S. Ficici", "D.C. Parkes", "A. Pfeffer"], "venue": "In Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence", "citeRegEx": "Ficici et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ficici et al\\.", "year": 2008}, {"title": "Graphical models for game theory", "author": ["M.J. Kearns", "M.L. Littman", "S.P. Singh"], "venue": "In Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence (UAI", "citeRegEx": "Kearns et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kearns et al\\.", "year": 2001}, {"title": "Gambit: Software tools for game theory, version 0.2007.01.30", "author": ["R.D. McKelvey", "A.M. McLennan", "T.L. Turocy"], "venue": null, "citeRegEx": "McKelvey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McKelvey et al\\.", "year": 2007}, {"title": "Quantal response equilibria for normal form games", "author": ["R.D. McKelvey", "T.R. Palfrey"], "venue": "In Normal Form Games, Games and Economic Behavior,", "citeRegEx": "McKelvey and Palfrey,? \\Q1996\\E", "shortCiteRegEx": "McKelvey and Palfrey", "year": 1996}, {"title": "Constraint satisfaction algorithms for graphical games", "author": ["V. Soni", "S. Singh", "M.P. Wellman"], "venue": "In Proceedings of the 6th International Joint Conference on Autonomous Agents and Multiagent Systems", "citeRegEx": "Soni et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Soni et al\\.", "year": 2007}, {"title": "Learning payoff functions in infinite games", "author": ["Y. Vorobeychik", "M.P. Wellman", "S. Singh"], "venue": null, "citeRegEx": "Vorobeychik et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vorobeychik et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 5, "context": "For instance, one possible future work is to add additional constraints for learning the structures of graphical games (Kearns et al., 2001).", "startOffset": 119, "endOffset": 140}, {"referenceID": 3, "context": "Ficici et al. (2008) learned a reduced game form representation from data by clustering agents with similar strategic views of the game and used this representation to approximately solve asymmetric games of many players.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Ficici et al. (2008) learned a reduced game form representation from data by clustering agents with similar strategic views of the game and used this representation to approximately solve asymmetric games of many players. Vorobeychik et al. (2007) used regression learning techniques to learn payoff functions of infinite games.", "startOffset": 0, "endOffset": 248}, {"referenceID": 3, "context": "Duong et al. (2009) applied existing learning algorithms such as branch-and-bound, greedy, and local search algorithms to learning graphical games from data.", "startOffset": 0, "endOffset": 20}, {"referenceID": 7, "context": "We use the quantal response equilibrium notion to model the bounded rationality of agents (McKelvey and Palfrey, 1996).", "startOffset": 90, "endOffset": 118}, {"referenceID": 6, "context": "Use Gambit (McKelvey et al., 2007) to compute LQRE of the game with different \u03bb values.", "startOffset": 11, "endOffset": 34}, {"referenceID": 1, "context": "Solve the WCSP specified in the XML file using Toolbar2 (Bouveret et al., 2004) which is a stateof-the-art C++ solver for WCSP, Max-SAT, and Bayesian Networks.", "startOffset": 56, "endOffset": 79}], "year": 2010, "abstractText": "While game theory is widely used to model strategic interactions, a natural question is where do the game representations come from? One answer is to learn the representations from data. If one wants to learn both the payoffs and the players\u2019 strategies, a naive approach is to learn them both directly from the data. This approach ignores the fact the players might be playing reasonably good strategies, so there is a connection between the strategies and the data. The main contribution of this paper is to make this connection while learning. We formulate the learning problem as a weighted constraint satisfaction problem, including constraints both for the fit of the payoffs and strategies to the data and the fit of the strategies to the payoffs. We use quantal response equilibrium as our notion of rationality for quantifying the latter fit. Our results show that incorporating rationality constraints can improve learning when the amount of data is limited.", "creator": "LaTeX with hyperref package"}}}