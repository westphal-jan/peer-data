{"id": "1203.3482", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Formula-Based Probabilistic Inference", "abstract": "Computing the probability of a formula given the probabilities or weights associated with other formulas is a natural extension of logical inference to the probabilistic setting. Surprisingly, this problem has received little attention in the literature to date, particularly considering that it includes many standard inference problems as special cases. In this paper, we propose two algorithms for this problem: formula decomposition and conditioning, which is an exact method, and formula importance sampling, which is an approximate method. The latter is, to our knowledge, the first application of model counting to approximate probabilistic inference. Unlike conventional variable-based algorithms, our algorithms work in the dual realm of logical formulas. Theoretically, we show that our algorithms can greatly improve efficiency by exploiting the structural information in the formulas. Empirically, we show that they are indeed quite powerful, often achieving substantial performance gains over state-of-the-art schemes.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (216kb)", "http://arxiv.org/abs/1203.3482v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["vibhav gogate", "pedro domingos"], "accepted": false, "id": "1203.3482"}, "pdf": {"name": "1203.3482.pdf", "metadata": {"source": "CRF", "title": "Formula-Based Probabilistic Inference", "authors": ["Vibhav Gogate"], "emails": ["vgogate@cs.washington.edu", "pedrod@cs.washington.edu"], "sections": [{"heading": null, "text": "Calculating the probability of a formula taking into account the probabilities or weights associated with other formulas is a natural extension of the logical conclusion to the probabilistic setting. Surprisingly, little attention has been paid to this problem in the literature so far, especially considering that it includes many standard inference problems as special cases. In this paper, we propose two algorithms for this problem: formula composition and conditioning, which is an exact method, and formula sampling, an approximate method. The latter, to our knowledge, is the first application of model counting to approximate probable conclusion. Unlike conventional variable-based algorithms, our algorithms operate in the dual realm of logical formulas. Theoretically, we show that our algorithms can significantly increase efficiency by evaluating the structural information in the formulas. Empirically, we show that they are actually quite powerful and often achieve significant performance gains over modern systems."}, {"heading": "1 Introduction", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notation", "text": "Let X = {X1,. Xn} be a set of propositional variables that can be mapped from the set {0, 1} or {False, True}. Let F be a propositional formula via X.A model or a solution of F is a 0 / 1 truth mapping to all variables in X that F is a one-letter clause. Let's assume that F is in any case a constellation of clauses, namely a constellation of clauses, a clause that is a disjunction of letters. A letter of the unit is a one-letter clause. Propositional Satisfiability or SAT is the deciding problem of determining whether F is the canonical NP complete problem. Model counting is the problem of determining the number of models of F, it is a # P complete problem."}, {"heading": "3 Exact Formula-based Inference", "text": "We first explain how to react to the different problems and then show how to generalise them. (...) We consider the expression for ZM (...) as a way to combine the different types of terms. (...) We can consider the different types of terms (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...) as a kind of conceptualisation (...)"}, {"heading": "3.1 Related work", "text": "FDC generalizes variable conditioning programs such as recursive conditioning (Darwiche, 2001), AND / OR search (Dechter and Mateescu, 2007), and value imination (Bacchus et al., 2003) because we need to limit our conditioning to unit clauses. FDC also generalizes weighted model counts (WMC) such as ACE (Chavira and Darwiche, 2008) and Cachet (Sang et al., 2005). These weighted model counts introduce additional Boolean variables to model each soft clause. Conditioning of these Boolean variables corresponds to the conditioning of the soft clauses in PropMRF. Therefore, FDC can simulate WMC by limiting its conditioning not only to the standard clauses, but also to the soft clauses already present in the PropMRF. Finally, FDC is related to rationalized constraint reasoning (Gomelles, 2004)."}, {"heading": "4 Formula Importance Sampling", "text": "In this section, we generalize conventional variable-based meaning samples to formula meaning samples and show that our generalization leads to new sample schemes with less variance. First, we present background information on variable-based meaning samples."}, {"heading": "4.1 Variable-Based Importance Sampling", "text": "The importance test (Rubinstein, 1981) is a general scheme that can be used to approximate any quantity such as ZM, which can be expressed as the sum of a function over a domain. Main idea is to use a weight distribution Q, which expresses PM (x) > 0 \u21d2 Q (x) > 0 and ZM as follows: ZM = \u2211 x Sol (F) m \u00b2 i = 1\u03c6i (xV (\u03c6i)) \u00b7 Q (x) Q (x) = EQ [I (x) \u00b2 m i = 1 \u03c6i (xV)) Q (x) (9), where I (x) is an indicator function, which is 1 if x is a solution of FM and 0 otherwise. In N independent and identical (i.i.d.) samples (x (1),."}, {"heading": "4.2 Formula-based Importance Sampling", "text": "(H)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "4.3 Variance Reduction", "text": "The Z-N estimate of algorithm 2 is likely to have a smaller average square error than the Z-N estimate given in Eq.10. In particular, with a variable meaning distribution Q (X), we can always construct a formula-based meaning distribution U (H) from Q (X), so that the variance of Z-N is smaller than that of Z-N. Let us define: U (h) = \u2211 xh-Sol (Fh-FM) Q (xh) (14) Intuitively, any sample from U (H) from Eq.14 is heavy in the sense that it corresponds to # (FM-Fh) samples from Q (xh). Due to this larger sample size, the variance of Z-N is smaller than that of Z-N (provided that # (FM-Fh) can be efficiently calculated). The only caveat is that generating samples from U (H) is more expensive. Formally, the proof is provided in the extended version of the paper."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Exact Inference", "text": "We compared ourselves to the reactionary and reactionary policies that are able to play by the rules that they have shown in recent years."}, {"heading": "5.1.1 Random networks", "text": "Our first domain is that of random nets. Networks are created on the basis of the model (n, m, s), where n is the number of (Boolean) variables, m is the number of weighted sentences, and s is the size of each weighted sentence. Given n variables X = {X1,..., Xn}, each sentence Ci (for i = 1 to m) is generated by random selection s (different) random variables from X, each with a probability of 0.5. For our experiments, we set n = m and experiment with three values for n and m: n, m \u00b2 {40, 50, 60}. s was determined from 3 to 9 in steps of 2.A random problem (n, m, s) is referred to in Table 1 as n \u2212 m \u2212 s."}, {"heading": "5.1.2 Medical Diagnosis", "text": "Our second domain is a version of the medical diagnostic networks QMR-DT (Shwe et al., 1991) as used in Cachet (Sang et al., 2005). Each problem can be specified using a two-layer bipartite diagram in which the top layer consists of diseases and the bottom layer of symptoms. If a disease causes a symptom, there is an edge from disease to symptom. We have a weighted clause for each disease and a weighted clause for each symptom, which is simply a logical OR of the diseases it causes (in (Sang et al., 2005), this clause was hard. We attach an arbitrary weight to it to make the problem more difficult.) For our experiments, we varied the number of diseases and symptoms from 40 to 60. For each symptom, we varied the number of diseases they can cause from 5 to 11 in increments of 2. The diseases for each symptom are randomly selected."}, {"heading": "5.1.3 Relational networks", "text": "Our last domain is that of relationship networks. We experimented with the Friends and Smokers networks and the Entity Resolution Networks. In the Friends and Smokers Networks (FS), we have three predictors smoking (x), which indicate whether a person smokes, cancer (x), which indicates whether a person has cancer, and friends (x, y), which indicate who is friends with whom. The probability model is defined by assigning weight to two logical constraints, friends (x, y) smoking (x) \u21d2 smoking (y) and smoking (x) \u21d2 cancer (x). Given a domain for x and y, a propMRF can be generated from these two logical constraints, taking into account all possible foundations of each predicate. We experimented with different domain sizes for x and y, ranging from 25 to 34. From Table 1, we can see that the time needed by FDC is two-dimensional."}, {"heading": "5.2 Approximate Inference", "text": "We compared \"Formula Importance Sample (FIS)\" with \"Variable Importance Sample (VIS)\" and \"State of Art Schemes such as MC-SAT (Poon and Domingos, 2006) and Gibbs Sample available in Alchemy (Kok et al., 2004) in the three areas described above. For both VIS and FIS, we chose to construct the meaning distribution Q from the output of a Belief Propagation Scheme (BP) because BP has been shown to be more important than other approaches in previous studies (Yuan and Druzdzel, 2006; Gogate and Dechter, 2005). Next, we describe how the method described in (Gogateand Dechter, 2005) can be adapted to construct a meaning distribution via formulas. Here we first introduce BP (or Generalized Belief Propagation (Yedidia et al., 2004)))."}, {"heading": "6 Summary and Conclusion", "text": "In this paper, we introduced a new formula-based approach for performing exact and approximate formula conclusions in graphical models. Formula-based conclusions are attractive because: (a) they generalize standard variable-based conclusions, (b) they yield several new efficient algorithms that are not possible simply by thinking about the variables, and (c) they naturally fit in with recent research efforts to combine logical and probabilistic artificial intelligence. Our empirical evaluation shows that formula-based approaches are particularly suitable for domains with large clauses. Such clauses are one of the main reasons for using logic instead of tables to represent potentials. or2Exact counting was cited when the number of variables was less than 100, which was the case for most networks we experimented with, except for the relational benchmarks. CPTs in graphical models are one of the main reasons for using logic instead of tables."}, {"heading": "Acknowledgements", "text": "This research was partially funded by ARO grants W911NF08-1-0242, AFRL grants FA8750-09-C-0181, DARPA grants FA8750-05-2-0283, FA8750-07-D-0185, HR0011-06-C-0025, HR0011-07-C-0060 and NBCH-D030010, NSF grants IIS-0534881 and IIS-0803481 and ONR grants N00014-08-1-0670. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies of ARO, DARPA, NSF, ONR or the United States Government."}], "references": [{"title": "Value Elimination: Bayesian Inference via Backtracking Search", "author": ["Fahiem Bacchus", "Shannon Dalmao", "Toniann Pitassi"], "venue": "In UAI,", "citeRegEx": "Bacchus et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bacchus et al\\.", "year": 2003}, {"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier"], "venue": "In Uncertainty in Artificial Intelligence", "citeRegEx": "Boutilier.,? \\Q1996\\E", "shortCiteRegEx": "Boutilier.", "year": 1996}, {"title": "Rao-Blackwellisation of sampling schemes", "author": ["George Casella", "Christian P. Robert"], "venue": null, "citeRegEx": "Casella and Robert.,? \\Q1996\\E", "shortCiteRegEx": "Casella and Robert.", "year": 1996}, {"title": "On probabilistic inference by weighted model counting", "author": ["Mark Chavira", "Adnan Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Chavira and Darwiche.,? \\Q2008\\E", "shortCiteRegEx": "Chavira and Darwiche.", "year": 2008}, {"title": "Recursive conditioning", "author": ["Adnan Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Darwiche.,? \\Q2001\\E", "shortCiteRegEx": "Darwiche.", "year": 2001}, {"title": "New Advances in Compiling CNF into Decomposable Negation Normal Form", "author": ["Adnan Darwiche"], "venue": "In ECAI,", "citeRegEx": "Darwiche.,? \\Q2004\\E", "shortCiteRegEx": "Darwiche.", "year": 2004}, {"title": "AND/OR search spaces for graphical models", "author": ["R. Dechter", "R. Mateescu"], "venue": "Artificial Intelligence,", "citeRegEx": "Dechter and Mateescu.,? \\Q2007\\E", "shortCiteRegEx": "Dechter and Mateescu.", "year": 2007}, {"title": "Markov Logic: An Interface Layer for Artificial Intelligence", "author": ["Pedro Domingos", "Daniel Lowd"], "venue": null, "citeRegEx": "Domingos and Lowd.,? \\Q2009\\E", "shortCiteRegEx": "Domingos and Lowd.", "year": 2009}, {"title": "Stochastic relaxations, Gibbs distributions and the Bayesian restoration of images", "author": ["Stuart Geman", "Donald Geman"], "venue": "IEEE Transaction on Pattern analysis and Machine Intelligence,", "citeRegEx": "Geman and Geman.,? \\Q1984\\E", "shortCiteRegEx": "Geman and Geman.", "year": 1984}, {"title": "Introduction to Statistical Relational Learning", "author": ["Lise Getoor", "Ben Taskar"], "venue": null, "citeRegEx": "Getoor and Taskar.,? \\Q2007\\E", "shortCiteRegEx": "Getoor and Taskar.", "year": 2007}, {"title": "Approximate inference algorithms for hybrid Bayesian networks with discrete constraints", "author": ["Vibhav Gogate", "Rina Dechter"], "venue": "In UAI,", "citeRegEx": "Gogate and Dechter.,? \\Q2005\\E", "shortCiteRegEx": "Gogate and Dechter.", "year": 2005}, {"title": "SampleSearch: A scheme that Searches for Consistent Samples", "author": ["Vibhav Gogate", "Rina Dechter"], "venue": "Proceedings of the 11th Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Gogate and Dechter.,? \\Q2007\\E", "shortCiteRegEx": "Gogate and Dechter.", "year": 2007}, {"title": "Approximate counting by sampling the backtrack-free search space", "author": ["Vibhav Gogate", "Rina Dechter"], "venue": "In Proceedings of 22nd Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Gogate and Dechter.,? \\Q2007\\E", "shortCiteRegEx": "Gogate and Dechter.", "year": 2007}, {"title": "AND/OR Importance Sampling", "author": ["Vibhav Gogate", "Rina Dechter"], "venue": "Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Gogate and Dechter.,? \\Q2008\\E", "shortCiteRegEx": "Gogate and Dechter.", "year": 2008}, {"title": "Streamlined constraint reasoning", "author": ["Carla P. Gomes", "Meinolf Sellmann"], "venue": "In CP,", "citeRegEx": "Gomes and Sellmann.,? \\Q2004\\E", "shortCiteRegEx": "Gomes and Sellmann.", "year": 2004}, {"title": "From sampling to model counting", "author": ["Carla P. Gomes", "J\u00f6rg Hoffmann", "Ashish Sabharwal", "Bart Selman"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Gomes et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gomes et al\\.", "year": 2007}, {"title": "Local computation with probabilities on graphical structures and their application to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Lauritzen and Spiegelhalter.,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter.", "year": 1988}, {"title": "Using weighted max-sat engines to solve mpe", "author": ["James D. Park"], "venue": "In AAAI,", "citeRegEx": "Park.,? \\Q2002\\E", "shortCiteRegEx": "Park.", "year": 2002}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q1988\\E", "shortCiteRegEx": "Pearl.", "year": 1988}, {"title": "Inducing features of random fields", "author": ["Stephen Della Pietra", "Vincent J. Della Pietra", "John D. Lafferty"], "venue": "IEEE Transanctions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Pietra et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Sound and efficient inference with probabilistic and deterministic dependencies", "author": ["Hoifung Poon", "Pedro Domingos"], "venue": "In AAAI,", "citeRegEx": "Poon and Domingos.,? \\Q2006\\E", "shortCiteRegEx": "Poon and Domingos.", "year": 2006}, {"title": "Counting models using connected components", "author": ["Roberto J. Bayardo Jr.", "Joseph Daniel Pehoushek"], "venue": "In Proceedings of 17th National Conference on Artificial Intelligence (AAAI),", "citeRegEx": "Jr. and Pehoushek.,? \\Q2000\\E", "shortCiteRegEx": "Jr. and Pehoushek.", "year": 2000}, {"title": "Simulation and the Monte Carlo Method", "author": ["Reuven Y. Rubinstein"], "venue": null, "citeRegEx": "Rubinstein.,? \\Q1981\\E", "shortCiteRegEx": "Rubinstein.", "year": 1981}, {"title": "Heuristics for fast exact model counting", "author": ["Tian Sang", "Paul Beame", "Henry Kautz"], "venue": "In Eighth International Conference on Theory and Applications of Satisfiability Testing (SAT),", "citeRegEx": "Sang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2005}, {"title": "Probabilistic diagnosis using a reformulation of the internist- 1/qmr knowledge base i. the probabilistic model and inference algorithms", "author": ["M. Shwe", "B. Middleton", "D. Heckerman", "M. Henrion", "E. Horvitz", "H. Lehmann", "G. Cooper"], "venue": "Methods of Information in Medicine,", "citeRegEx": "Shwe et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Shwe et al\\.", "year": 1991}, {"title": "Minisat v1.13-a SAT solver with conflict-clause minimization", "author": ["Niklas Sorensson", "Niklas Een"], "venue": "In SAT 2005 competition,", "citeRegEx": "Sorensson and Een.,? \\Q2005\\E", "shortCiteRegEx": "Sorensson and Een.", "year": 2005}, {"title": "A new approach to model counting", "author": ["Wei Wei", "Bart Selman"], "venue": "In SAT,", "citeRegEx": "Wei and Selman.,? \\Q2005\\E", "shortCiteRegEx": "Wei and Selman.", "year": 2005}, {"title": "Constructing free energy approximations and generalized belief propagation algorithms", "author": ["Jonathan S. Yedidia", "William T. Freeman", "Yair Weiss"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2004}, {"title": "Importance sampling algorithms for Bayesian networks: Principles and performance", "author": ["Changhe Yuan", "Marek J. Druzdzel"], "venue": "Mathematical and Computer Modelling,", "citeRegEx": "Yuan and Druzdzel.,? \\Q2006\\E", "shortCiteRegEx": "Yuan and Druzdzel.", "year": 2006}], "referenceMentions": [{"referenceID": 19, "context": "Another problem is that in general a set of formula probabilities does not completely specify a distribution, but this is naturally solved by assuming the maximum entropy distribution consistent with the specified probabilities (Nilsson, 1986; Pietra et al., 1997).", "startOffset": 228, "endOffset": 264}, {"referenceID": 18, "context": "This contrasts with the large literature on inference for graphical models, which always specify unique and consistent distributions (Pearl, 1988).", "startOffset": 133, "endOffset": 146}, {"referenceID": 9, "context": "This issue has gained prominence in the field of statistical relational learning (SRL) (Getoor and Taskar, 2007), which seeks to learn models with both logical and probabilistic aspects.", "startOffset": 87, "endOffset": 112}, {"referenceID": 7, "context": "For example, Markov logic represents knowledge as a set of weighted formulas, which define a log-linear model (Domingos and Lowd, 2009).", "startOffset": 110, "endOffset": 135}, {"referenceID": 19, "context": "Formulas with probabilities with the maximum entropy assumption and weighted formulas are equivalent; the problem of converting the former to the latter is equivalent to the problem of learning the maximum likelihood weights (Pietra et al., 1997).", "startOffset": 225, "endOffset": 246}, {"referenceID": 17, "context": "Another reason to seek efficient inference procedures for probabilistic logic is that inference in graphical models can be reduced to it (Park, 2002).", "startOffset": 137, "endOffset": 149}, {"referenceID": 16, "context": "Standard inference schemes for graphical models such as junction trees (Lauritzen and Spiegelhalter, 1988) and bucket elimination (Dechter, 1999) have complexity exponential in the treewidth of the model, making them impractical for complex domains.", "startOffset": 71, "endOffset": 106}, {"referenceID": 3, "context": "However, treewidth can be overcome by exploiting structural properties like determinism (Chavira and Darwiche, 2008) and context-specific independence (Boutilier, 1996).", "startOffset": 88, "endOffset": 116}, {"referenceID": 1, "context": "However, treewidth can be overcome by exploiting structural properties like determinism (Chavira and Darwiche, 2008) and context-specific independence (Boutilier, 1996).", "startOffset": 151, "endOffset": 168}, {"referenceID": 23, "context": "Several highly efficient algorithms accomplish this by encoding a graphical models as sets of weighted formulas and applying logical inference techniques to them (Sang et al., 2005; Chavira and Darwiche, 2008).", "startOffset": 162, "endOffset": 209}, {"referenceID": 3, "context": "Several highly efficient algorithms accomplish this by encoding a graphical models as sets of weighted formulas and applying logical inference techniques to them (Sang et al., 2005; Chavira and Darwiche, 2008).", "startOffset": 162, "endOffset": 209}, {"referenceID": 6, "context": "FDC performs AND/OR search (Dechter and Mateescu, 2007) or recursive conditioning (Darwiche, 2001), with and without caching, over the space of formulas, utilizing several Boolean constraint propagation and pruning techniques.", "startOffset": 27, "endOffset": 55}, {"referenceID": 4, "context": "FDC performs AND/OR search (Dechter and Mateescu, 2007) or recursive conditioning (Darwiche, 2001), with and without caching, over the space of formulas, utilizing several Boolean constraint propagation and pruning techniques.", "startOffset": 82, "endOffset": 98}, {"referenceID": 15, "context": "These model counts can either be computed exactly, if it is feasible, or approximately using the recently introduced approximate model counters such as SampleCount (Gomes et al., 2007) and SampleSearch (Gogate and Dechter, 2007b).", "startOffset": 164, "endOffset": 184}, {"referenceID": 3, "context": "Our experiments show that as the number of variables in the formulas increases, formula-based schemes not only dominate their variable based counterparts but also state-of-the-art exact algorithms such as ACE (Chavira and Darwiche, 2008) and approximate schemes such as MC-SAT (Poon and Domingos, 2006) and Gibbs sampling (Geman and Geman, 1984).", "startOffset": 209, "endOffset": 237}, {"referenceID": 20, "context": "Our experiments show that as the number of variables in the formulas increases, formula-based schemes not only dominate their variable based counterparts but also state-of-the-art exact algorithms such as ACE (Chavira and Darwiche, 2008) and approximate schemes such as MC-SAT (Poon and Domingos, 2006) and Gibbs sampling (Geman and Geman, 1984).", "startOffset": 277, "endOffset": 302}, {"referenceID": 8, "context": "Our experiments show that as the number of variables in the formulas increases, formula-based schemes not only dominate their variable based counterparts but also state-of-the-art exact algorithms such as ACE (Chavira and Darwiche, 2008) and approximate schemes such as MC-SAT (Poon and Domingos, 2006) and Gibbs sampling (Geman and Geman, 1984).", "startOffset": 322, "endOffset": 345}, {"referenceID": 17, "context": "It is known that any discrete Markov random field or a Bayesian network can be encoded as a PropMRF (Park, 2002; Sang et al., 2005; Chavira and Darwiche, 2008).", "startOffset": 100, "endOffset": 159}, {"referenceID": 23, "context": "It is known that any discrete Markov random field or a Bayesian network can be encoded as a PropMRF (Park, 2002; Sang et al., 2005; Chavira and Darwiche, 2008).", "startOffset": 100, "endOffset": 159}, {"referenceID": 3, "context": "It is known that any discrete Markov random field or a Bayesian network can be encoded as a PropMRF (Park, 2002; Sang et al., 2005; Chavira and Darwiche, 2008).", "startOffset": 100, "endOffset": 159}, {"referenceID": 3, "context": "These and other ideas form the backbone of many state-of-the-art schemes such as ACE (Chavira and Darwiche, 2008) and Cachet (Sang et al.", "startOffset": 85, "endOffset": 113}, {"referenceID": 23, "context": "These and other ideas form the backbone of many state-of-the-art schemes such as ACE (Chavira and Darwiche, 2008) and Cachet (Sang et al., 2005).", "startOffset": 125, "endOffset": 144}, {"referenceID": 4, "context": "Another advancement that we can use is problem decomposition (Darwiche, 2001; Dechter and Mateescu, 2007).", "startOffset": 61, "endOffset": 105}, {"referenceID": 6, "context": "Another advancement that we can use is problem decomposition (Darwiche, 2001; Dechter and Mateescu, 2007).", "startOffset": 61, "endOffset": 105}, {"referenceID": 4, "context": "Note that this is a very important step and is the primary reason for efficiency of techniques such as recursive conditioning (Darwiche, 2001) and AND/OR search (Dechter and Mateescu, 2007).", "startOffset": 126, "endOffset": 142}, {"referenceID": 6, "context": "Note that this is a very important step and is the primary reason for efficiency of techniques such as recursive conditioning (Darwiche, 2001) and AND/OR search (Dechter and Mateescu, 2007).", "startOffset": 161, "endOffset": 189}, {"referenceID": 23, "context": "Second, we can augment FDC with component caching and clause learning as in Cachet (Sang et al., 2005) and use w-cutset conditioning (Dechter, 1999) in a straight forward manner.", "startOffset": 83, "endOffset": 102}, {"referenceID": 4, "context": "FDC generalizes variable-based conditioning schemes such as recursive conditioning (Darwiche, 2001), AND/OR search (Dechter and Mateescu, 2007) and value elimination (Bacchus et al.", "startOffset": 83, "endOffset": 99}, {"referenceID": 6, "context": "FDC generalizes variable-based conditioning schemes such as recursive conditioning (Darwiche, 2001), AND/OR search (Dechter and Mateescu, 2007) and value elimination (Bacchus et al.", "startOffset": 115, "endOffset": 143}, {"referenceID": 0, "context": "FDC generalizes variable-based conditioning schemes such as recursive conditioning (Darwiche, 2001), AND/OR search (Dechter and Mateescu, 2007) and value elimination (Bacchus et al., 2003) because all we have to do is restrict our conditioning to unit clauses.", "startOffset": 166, "endOffset": 188}, {"referenceID": 3, "context": "FDC also generalizes weighted model counting (WMC) approaches such as ACE (Chavira and Darwiche, 2008) and Cachet (Sang et al.", "startOffset": 74, "endOffset": 102}, {"referenceID": 23, "context": "FDC also generalizes weighted model counting (WMC) approaches such as ACE (Chavira and Darwiche, 2008) and Cachet (Sang et al., 2005).", "startOffset": 114, "endOffset": 133}, {"referenceID": 14, "context": "Finally, FDC is related to streamlined constraint reasoning (SCR) approach of (Gomes and Sellmann, 2004).", "startOffset": 78, "endOffset": 104}, {"referenceID": 22, "context": "Importance sampling (Rubinstein, 1981) is a general scheme which can be used to approximate any quantity such as ZM which can be expressed as a sum of a function over a domain.", "startOffset": 20, "endOffset": 38}, {"referenceID": 22, "context": "It is known (Rubinstein, 1981) that EQ[\u1e90N ] = ZM, namely it is unbiased.", "startOffset": 12, "endOffset": 30}, {"referenceID": 25, "context": "Algorithm 2 outlines a procedure for constructing such a distribution using a complete SAT solver (for example Minisat (Sorensson and Een, 2005)).", "startOffset": 119, "endOffset": 144}, {"referenceID": 26, "context": "In such cases, we can use state-of-the-art approximate counting techniques such as ApproxCount (Wei and Selman, 2005), SampleCount (Gomes et al.", "startOffset": 95, "endOffset": 117}, {"referenceID": 15, "context": "In such cases, we can use state-of-the-art approximate counting techniques such as ApproxCount (Wei and Selman, 2005), SampleCount (Gomes et al., 2007) and SampleSearch (Gogate and Dechter, 2007b).", "startOffset": 131, "endOffset": 151}, {"referenceID": 2, "context": "We can easily integrate FIS with other variance reduction schemes such as Rao-Blackwellisation (Casella and Robert, 1996) and AND/OR sampling (Gogate and Dechter, 2008).", "startOffset": 95, "endOffset": 121}, {"referenceID": 13, "context": "We can easily integrate FIS with other variance reduction schemes such as Rao-Blackwellisation (Casella and Robert, 1996) and AND/OR sampling (Gogate and Dechter, 2008).", "startOffset": 142, "endOffset": 168}, {"referenceID": 3, "context": "We compared \u201cFormula Decomposition and Conditioning (FDC)\u201d against \u201cVariable Decomposition and Conditioning (VDC)\u201d, variable elimination (VE) (Dechter, 1999) and ACE (Chavira and Darwiche, 2008) (which internally uses the C2D compiler (Darwiche, 2004)) for computing the partition function on benchmark problems from three domains: (a) Random networks, (b) medical diagnosis networks and (c) Relational networks.", "startOffset": 166, "endOffset": 194}, {"referenceID": 5, "context": "We compared \u201cFormula Decomposition and Conditioning (FDC)\u201d against \u201cVariable Decomposition and Conditioning (VDC)\u201d, variable elimination (VE) (Dechter, 1999) and ACE (Chavira and Darwiche, 2008) (which internally uses the C2D compiler (Darwiche, 2004)) for computing the partition function on benchmark problems from three domains: (a) Random networks, (b) medical diagnosis networks and (c) Relational networks.", "startOffset": 235, "endOffset": 251}, {"referenceID": 23, "context": "Also, similar to Cachet (Sang et al., 2005), we use component caching and similar to w-cutset conditioning (Dechter, 1999), we invoke bucket elimination at a node if the treewidth of the (remaining) PropMRF at the node is less than 16.", "startOffset": 24, "endOffset": 43}, {"referenceID": 3, "context": "We also tried a few other heuristics, both static and dynamic, such as (i) conditioning on a subclause C (and its negation) that causes the most unit propagations (but one has to perform unit propagations for each candidate clause, which can be quite expensive in practice) (ii) graph partitioning heuristics based on the min-fill, mindegree and hmetis orderings; these heuristics are used by solvers such as ACE (Chavira and Darwiche, 2008) and AND/OR search (Dechter and Mateescu, 2007) and (iii) Entropy-based heuristics.", "startOffset": 413, "endOffset": 441}, {"referenceID": 6, "context": "We also tried a few other heuristics, both static and dynamic, such as (i) conditioning on a subclause C (and its negation) that causes the most unit propagations (but one has to perform unit propagations for each candidate clause, which can be quite expensive in practice) (ii) graph partitioning heuristics based on the min-fill, mindegree and hmetis orderings; these heuristics are used by solvers such as ACE (Chavira and Darwiche, 2008) and AND/OR search (Dechter and Mateescu, 2007) and (iii) Entropy-based heuristics.", "startOffset": 460, "endOffset": 488}, {"referenceID": 24, "context": "Our second domain is a version of QMR-DT medical diagnosis networks (Shwe et al., 1991) as used in Cachet (Sang et al.", "startOffset": 68, "endOffset": 87}, {"referenceID": 23, "context": ", 1991) as used in Cachet (Sang et al., 2005).", "startOffset": 26, "endOffset": 45}, {"referenceID": 23, "context": "We have a weighted unit clause for each disease and a weighted clause for each symptom, which is simply a logical OR of the diseases that cause it (in (Sang et al., 2005), this clause was hard.", "startOffset": 151, "endOffset": 170}, {"referenceID": 20, "context": "We compared \u201cFormula importance sampling (FIS)\u201d against \u201cVariable importance sampling (VIS)\u201d and stateof-the-art schemes such as MC-SAT (Poon and Domingos, 2006) and Gibbs sampling available in Alchemy (Kok et al.", "startOffset": 136, "endOffset": 161}, {"referenceID": 28, "context": "For both VIS and FIS, we chose to construct the importance distribution Q from the output of a Belief propagation scheme (BP), because BP was shown to yield a better importance function than other approaches in previous studies (Yuan and Druzdzel, 2006; Gogate and Dechter, 2005).", "startOffset": 228, "endOffset": 279}, {"referenceID": 10, "context": "For both VIS and FIS, we chose to construct the importance distribution Q from the output of a Belief propagation scheme (BP), because BP was shown to yield a better importance function than other approaches in previous studies (Yuan and Druzdzel, 2006; Gogate and Dechter, 2005).", "startOffset": 228, "endOffset": 279}, {"referenceID": 10, "context": "We describe next, how the method described in (Gogate and Dechter, 2005) can be adapted to construct an importance distribution over formulas.", "startOffset": 46, "endOffset": 72}, {"referenceID": 27, "context": "Here, we first run BP (or Generalized Belief Propagation (Yedidia et al., 2004)) over a factor (or region) graph in which the nodes are the variables and the factors are the hard and the soft clauses.", "startOffset": 57, "endOffset": 79}, {"referenceID": 25, "context": "We used Minisat (Sorensson and Een, 2005) as our SAT solver.", "startOffset": 16, "endOffset": 41}], "year": 2010, "abstractText": "Computing the probability of a formula given the probabilities or weights associated with other formulas is a natural extension of logical inference to the probabilistic setting. Surprisingly, this problem has received little attention in the literature to date, particularly considering that it includes many standard inference problems as special cases. In this paper, we propose two algorithms for this problem: formula decomposition and conditioning, which is an exact method, and formula importance sampling, which is an approximate method. The latter is, to our knowledge, the first application of model counting to approximate probabilistic inference. Unlike conventional variable-based algorithms, our algorithms work in the dual realm of logical formulas. Theoretically, we show that our algorithms can greatly improve efficiency by exploiting the structural information in the formulas. Empirically, we show that they are indeed quite powerful, often achieving substantial performance gains over state-of-the-art schemes.", "creator": "gnuplot 4.2 patchlevel 4 "}}}