{"id": "1203.3490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Anytime Planning for Decentralized POMDPs using Expectation Maximization", "abstract": "Decentralized POMDPs provide an expressive framework for multi-agent sequential decision making. While fnite-horizon DECPOMDPs have enjoyed signifcant success, progress remains slow for the infnite-horizon case mainly due to the inherent complexity of optimizing stochastic controllers representing agent policies. We present a promising new class of algorithms for the infnite-horizon case, which recasts the optimization problem as inference in a mixture of DBNs. An attractive feature of this approach is the straightforward adoption of existing inference techniques in DBNs for solving DEC-POMDPs and supporting richer representations such as factored or continuous states and actions. We also derive the Expectation Maximization (EM) algorithm to optimize the joint policy represented as DBNs. Experiments on benchmark domains show that EM compares favorably against the state-of-the-art solvers.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (547kb)", "http://arxiv.org/abs/1203.3490v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["akshat kumar", "shlomo zilberstein"], "accepted": false, "id": "1203.3490"}, "pdf": {"name": "1203.3490.pdf", "metadata": {"source": "CRF", "title": "Anytime Planning for Decentralized POMDPs using Expectation Maximization", "authors": ["Akshat Kumar"], "emails": ["akshat@cs.umass.edu", "shlomo@cs.umass.edu"], "sections": [{"heading": null, "text": "Decentralized POMDPs provide an expressive framework for multi-stakeholder sequential decision-making. While DECPOMDPs with limited horizons have enjoyed significant success, progress is slow in the case with infinite horizons, mainly due to the inherent complexity of optimizing stochastic controllers that represent agent policies. We present a promising new class of algorithms for the case with infinite horizons that describe the optimization problem as inference in a mixture of DBNs. An attractive feature of this approach is the easy adoption of existing inference techniques in DBNs to solve DEC POMDPs and support richer representations such as factored or continuous states and measures. We also derive the Expectation Maximization (EM) algorithm to optimize the common policy presented as DBNs. Experiments on benchmark domains show that EM is favorable to modern solvers."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive on their own, and they are able to survive on their own."}, {"heading": "2 The DEC-POMDP model", "text": "In this section, we present the DEC-POMDP model for two agents [5]. Note, however, that DECPOMDPs with a finite horizon are also complete for two agents. The sentence S stands for the set of environmental states with a given initial state distribution b0. The plot of Agent 1 is set by A and Agent 2 by B. The finite set of observations for Agent 1 and Z for Agent 2. O (s, ab, yz) stands for the probability P (y, z | s, a) of Agent 1 for the common reward R (s, a, b). Y is the finite set of observations for Agent 1 and Z for Agent 2. O (s, ab, yz) denotes the probability P (s, a, b) of Agent 1 for the observation of Agent 1 Y and Agent 2 for the observation of Agent DEP."}, {"heading": "3 DEC-POMDPs as mixture of DBNs", "text": "In this section, we describe how DEC POMDPs can be reformulated as a mixture of DBNs (= 1), so that maximizing the probability of rewards (to be defined later) within this framework is tantamount to optimizing the common policy. Our approach is based on the framework proposed in [19, 20] to solve Markovian planning problems using a single DBN. First, we informally describe the intuition behind this reformulation (please refer to [19] for details), and then we describe in detail the steps that can be described specifically for DEC-POMDPs.A DEC-POMDP, with the reward sent at each time step. However, our approach describes an infinite mix of a specific type of DBNs in which reward is only sent at the end. For example, Fig. 1 (a) describes the DBN for the time t = 0."}, {"heading": "4 EM algorithm for DEC-POMDPs", "text": "This section describes the EM algorithm [7] to maximize the probability of rewards in the mix of DBNs representing DEC POMDPs. In the corresponding DBNs, only the binary reward is treated as observed (r = 1); all other variables are latent; while the probability is maximized, EM provides the common political parameters DEC-POMDPs. EM also has the desirable characteristic at all times, as the probability (and the political value that is proportional to probability) is guaranteed to increase per iteration to convergence. We note that EM is not guaranteed to converge with the global optima. However, in the experiments we show that EM almost always achieves values similar to the state-of-the-art NLP-based solver [1] and much better than DEC-BPI [4]. The main advantage of using EM is its ability to simply generalize far more comprehensive representations than are currently possible."}, {"heading": "4.1 E-step", "text": "In the E step, for the fixed parameter \u03b8, forward-looking messages \u03b1 = \u03b2 = q q q \u03b2 = q \u03b2 messages are disseminated. First, we define the following Markovian transitions over the (p, q, s) state in the DBN of image 1 (b). However, these transitions are independent of the time t due to the stationary common policy. We also adopt the convention that for any random variable v, v \u00b2 s refers to the next time period and v \u00b2 refers to the previous time period. For each group of variables v, Pt (v, v \u00b2) refers to P (vt = v \u00b2 s = v \u00b2 s). P (p \u00b2, s \u00b2, s \u00b2 s \u00b2, s \u00b2, s \u00b2 s \u00b2) then refers to the previous time period v, Pt \u00b2 qz \u00b2 qz \u00b2 qz \u00b2 s refers to the P (vt = v \u00b2 s)."}, {"heading": "4.1.1 Complexity", "text": "The calculation of Markov transitions on the (p, q, s) chain has the complexity O (N4S2A2Y 2), where N is the maximum number of nodes for a controller. Message distribution has the complexity O (TmaxN4S2). Techniques to effectively reduce this complexity without compromising accuracy will be discussed later."}, {"heading": "4.2 M-step", "text": "In the DBNs of Figure 1 (a, b), every variable is hidden except the reward variable. EM provides better estimates of these variables after each M step, increasing the probability L\u03b8 and thus the guideline value. For details on EM, refer to [7]. The estimated parameters are < \u03c0, \u03bb, \u03bd > for each agent. For a given DBN for time T, the following complete log probability for the DEC-POMDP-DBN mixing factor is maximized. Each variable denotes the latent, with each variable denoting a sequence of the length T. That is, P = p0: T. EM maximizes the following complete log probability for the DEC-POMDP-DBN mixing factor. The previous parameters and the solution? denotes new parameters q.Q (KA, TB?) = new parameters q."}, {"heading": "4.2.1 Action updates", "text": "The update of the action parameters \u03c0? ap for Agent 1 can be derived by deriving Q = q q q = q q q = q q = q = q = q = q = q = q = q = q \u00b2 s as follows: Q (2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001). By breaking the above summation between t = T and t = 0 to T \u2212 1, we get the answer to T = 0 p (2001, 2001, 2001, 2001, 2001, 2001). p (P) p (P) p (p) p (P) p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\" p \"p\""}, {"heading": "4.2.2 Controller node transition updates", "text": "The update of the transition parameter of the controller node for Agent 1 can be found by adding Q (1, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "4.2.3 Initial node distribution", "text": "The initial node distribution \u03bd for controller nodes of Agent 1 and 2 can be updated as follows. We do not show a complete derivative as it is similar to the other parameters."}, {"heading": "4.2.4 Complexity and implementation issues", "text": "The complexity of updating all the action parameters is O (N4S2AY 2). Updating the node transitions requires O (N4S2Y 2 + N2S2Y 2A2). This is relatively high compared to the POMDP updates that require O (N2S2AY), mainly due to the scale of interactions present in DEC-POMDPs. In our experimental environments, we observed that a relatively small controller (N \u2264 5) is sufficient to achieve good quality solutions. The main factor for complexity is the factor S2, as we have experimented with large domains with almost 250 states. The good news is that the structure of the E and M step equations provides a way to effectively reduce this complexity by a significant factor without sacrificing accuracy. For a certain state, joint action < a, b > and joint observation < y, z >, the next possible states can be calculated as follows (b), the ability (b) to be effective."}, {"heading": "5 Experiments", "text": "We experimented with several standard 2-agent DECPOMDP parameters, EM is always better than DEPI-like values. We experimented with several standard 2-agent DECPOMDP benchmarks with a discount factor of 0.9. Full details of these problems cannot be found in [1, 4]. We compare our approach with decentralized limited policy iteration (DEC-BPI) and a non-convex optimization solver (NLP). The DEC-BPI algorithm improves the parameters of a node using a linear program, while the other node parameters are fixed. The NLP approach describes the policy optimization problem as a non-linear program and uses an off-the-shelf solver solution, Snopt [9], to get a solution. We implemented the EM algorithm in JAVA. All of our experiments were on a Mac with 4GB RAM and an average of 2.4GHU each controller is a random point."}, {"heading": "6 Conclusion and future work", "text": "We present a new approach to solving DEC-POMDPs using conclusions in a mixture of DBNs. Even a simple implementation of the approach yields good results. Extensive experiments show that EM is always better than DEC-BPI and compares positively with the NLP solver of the state. The experiments also highlight two potential disadvantages of the EM approach: the negative effects of reward scaling on the solution quality and the slow convergence rate for large problems. In addition, we address the runtime problem by parallelizing the algorithm. For example, \u03b1 and \u03b2 can be propagated in parallel. Even the updating of the parameters of each node can be performed in parallel for each iteration. Furthermore, the structure of the EM update equations is very vulnerable to Google's map reduction paradigm [6], which allows each parameter to be realized by a number of machines."}, {"heading": "Acknowledgments", "text": "This work was supported in part by the National Science Foundation Grant IIS-0812149 and the Air Force Office of Scientific Research Grant FA9550-08-1-0181."}], "references": [{"title": "Optimizing fixed-size stochastic controllers for POMDPs and decentralized POMDPs", "author": ["C. Amato", "D.S. Bernstein", "S. Zilberstein"], "venue": "JAAMAS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Planning by probabilistic inference", "author": ["H. Attias"], "venue": "In Workshop on AISTATS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Solving transition independent decentralized markov decision processes", "author": ["R. Becker", "S. Zilberstein", "V. Lesser", "C.V. Goldman"], "venue": "JAIR, 22:423\u2013455,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Policy iteration for decentralized control of Markov decision processes", "author": ["D.S. Bernstein", "C. Amato", "E.A. Hansen", "S. Zilberstein"], "venue": "JAIR, 34:89\u2013132,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein"], "venue": "J. MOR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "MapReduce: a flexible data processing", "author": ["J. Dean", "S. Ghemawat"], "venue": "tool. CACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical society, Series B,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1977}, {"title": "Point-based incremental pruning heuristic for solving finite-horizon DEC-POMDPs", "author": ["J.S. Dibangoye", "A.-I. Mouaddib", "B. Chaib-draa"], "venue": "In AAMAS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "SNOPT: An SQP algorithm for large-scale constrained optimization. SIOPT", "author": ["P.E. Gill", "W. Murray", "M.A. Saunders"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "New inference strategies for solving Markov decision processes using reversible jump MCMC", "author": ["M. Hoffman", "H. Kueck", "N. de Freitas", "A. Doucet"], "venue": "In UAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Point based backup for decentralized POMDPs: Complexity and new algorithms", "author": ["A. Kumar", "S. Zilberstein"], "venue": "In AAMAS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Complexity of finite-horizon Markov decision process problems", "author": ["M. Mundhenk", "J. Goldsmith", "C. Lusena", "E. Allender"], "venue": "J. ACM,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs", "author": ["R. Nair", "P. Varakantham", "M. Tambe", "M. Yokoo"], "venue": "In AAAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Optimal and approximate Q-value functions for decentralized POMDPs", "author": ["F.A. Oliehoek", "M.T.J. Spaan", "N.A. Vlassis"], "venue": "JAIR, 32:289\u2013353,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Anytime pointbased approximations for large POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "JAIR, 27:335\u2013380,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Memory-bounded dynamic programming for DEC-POMDPs", "author": ["S. Seuken", "S. Zilberstein"], "venue": "In IJCAI, pages 2009\u20132015,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Heuristic search value iteration for POMDPs", "author": ["T. Smith", "R. Simmons"], "venue": "In UAI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Hierarchical POMDP controller optimization by likelihood maximization", "author": ["M. Toussaint", "L. Charlin", "P. Poupart"], "venue": "In UAI,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Probabilistic inference for solving (PO)MDPs", "author": ["M. Toussaint", "S. Harmeling", "A. Storkey"], "venue": "Technical Report EDIINF-RR-0934,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Probabilistic inference for solving discrete and continuous state markov decision processes", "author": ["M. Toussaint", "A.J. Storkey"], "venue": "In ICML,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}], "referenceMentions": [{"referenceID": 4, "context": "Decentralized partially observable MDPs (DECPOMDPs) have emerged in recent years as an important framework for modeling sequential decision making by a team of agents [5].", "startOffset": 167, "endOffset": 170}, {"referenceID": 2, "context": "Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13].", "startOffset": 94, "endOffset": 97}, {"referenceID": 13, "context": "Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 4, "context": "Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13].", "startOffset": 166, "endOffset": 169}, {"referenceID": 12, "context": "Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13].", "startOffset": 217, "endOffset": 221}, {"referenceID": 4, "context": "However, the rich model comes with a price\u2013optimally solving a finite-horizon DEC-POMDP is NEXP-Complete [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 11, "context": "In contrast, finite-horizon POMDPs are PSPACE-complete [12], a strictly lower complexity class that highlights the difficulty of solving DEC-POMDPs.", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "Recently, a multitude of point-based approximate algorithms have been proposed for solving finite-horizon DEC-POMDPs [11, 8, 16].", "startOffset": 117, "endOffset": 128}, {"referenceID": 7, "context": "Recently, a multitude of point-based approximate algorithms have been proposed for solving finite-horizon DEC-POMDPs [11, 8, 16].", "startOffset": 117, "endOffset": 128}, {"referenceID": 15, "context": "Recently, a multitude of point-based approximate algorithms have been proposed for solving finite-horizon DEC-POMDPs [11, 8, 16].", "startOffset": 117, "endOffset": 128}, {"referenceID": 14, "context": "However, unlike their point-based counterparts in POMDPs ([15, 17]), they cannot be easily adopted for the infinite-horizon case due to a variety of reasons.", "startOffset": 58, "endOffset": 66}, {"referenceID": 16, "context": "However, unlike their point-based counterparts in POMDPs ([15, 17]), they cannot be easily adopted for the infinite-horizon case due to a variety of reasons.", "startOffset": 58, "endOffset": 66}, {"referenceID": 3, "context": "[4] highlight, no analogous equation exists for DEC-POMDPs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "To alleviate such problems, most infinite-horizon algorithms represent agent policies as finite-state controllers [1, 4].", "startOffset": 114, "endOffset": 120}, {"referenceID": 3, "context": "To alleviate such problems, most infinite-horizon algorithms represent agent policies as finite-state controllers [1, 4].", "startOffset": 114, "endOffset": 120}, {"referenceID": 3, "context": "So far, only two algorithms have shown promise for effectively solving infinite-horizon DEC-POMDPs\u2013decentralized bounded policy iteration (DEC-BPI) [4] and a non-linear programming based approach (NLP) [1].", "startOffset": 148, "endOffset": 151}, {"referenceID": 0, "context": "So far, only two algorithms have shown promise for effectively solving infinite-horizon DEC-POMDPs\u2013decentralized bounded policy iteration (DEC-BPI) [4] and a non-linear programming based approach (NLP) [1].", "startOffset": 202, "endOffset": 205}, {"referenceID": 19, "context": "\u2019s approach of transforming the planning problem to its equivalent mixture of dynamic Bayes nets (DBNs) and using likelihood maximization in this framework to optimize the policy value [20, 19].", "startOffset": 185, "endOffset": 193}, {"referenceID": 18, "context": "\u2019s approach of transforming the planning problem to its equivalent mixture of dynamic Bayes nets (DBNs) and using likelihood maximization in this framework to optimize the policy value [20, 19].", "startOffset": 185, "endOffset": 193}, {"referenceID": 1, "context": "Earlier work on planning by probabilistic inference can be found in [2].", "startOffset": 68, "endOffset": 71}, {"referenceID": 18, "context": "Such approaches have been successful in solving MDPs and POMDPs [19].", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "They also easily extend to factored or hierarchical structures [18] and can handle continuous action and state spaces thanks to advanced probabilistic inference techniques [10].", "startOffset": 63, "endOffset": 67}, {"referenceID": 9, "context": "They also easily extend to factored or hierarchical structures [18] and can handle continuous action and state spaces thanks to advanced probabilistic inference techniques [10].", "startOffset": 172, "endOffset": 176}, {"referenceID": 4, "context": "In this section, we introduce the DEC-POMDP model for two agents [5].", "startOffset": 65, "endOffset": 68}, {"referenceID": 4, "context": "This added uncertainty about other agents in the system make DEC-POMDPs NEXP complete [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "We represent the stationary policy of each agent using a fixed size, stochastic finite-state controller (FSC) similar to [1].", "startOffset": 121, "endOffset": 124}, {"referenceID": 18, "context": "Our approach is based on the framework proposed in [19, 20] to solve Markovian planning problems using probabilistic inference.", "startOffset": 51, "endOffset": 59}, {"referenceID": 19, "context": "Our approach is based on the framework proposed in [19, 20] to solve Markovian planning problems using probabilistic inference.", "startOffset": 51, "endOffset": 59}, {"referenceID": 18, "context": "First we informally describe the intuition behind this reformulation (for details please refer to [19]) and then we describe in detail the steps specific to DEC-POMDPs.", "startOffset": 98, "endOffset": 102}, {"referenceID": 18, "context": "The proof is omitted as it is very similar to that of MDPs and POMDPs [19].", "startOffset": 70, "endOffset": 74}, {"referenceID": 6, "context": "This section describes the EM algorithm [7] for maximizing the reward likelihood in the mixture of DBNs representing DEC-POMDPs.", "startOffset": 40, "endOffset": 43}, {"referenceID": 0, "context": "However, in the experiments we show that EM almost always achieves similar values as the state-of-the-art NLP based solver [1] and much better than DEC-BPI [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "However, in the experiments we show that EM almost always achieves similar values as the state-of-the-art NLP based solver [1] and much better than DEC-BPI [4].", "startOffset": 156, "endOffset": 159}, {"referenceID": 18, "context": "It might appear that we need to propagate \u03b1 messages for each DBN separately, but as pointed out in [19], only one sweep is required as the head of the DBN is shared among all the mixture components.", "startOffset": 100, "endOffset": 104}, {"referenceID": 6, "context": "For details of EM, we refer to [7].", "startOffset": 31, "endOffset": 34}, {"referenceID": 18, "context": "We resolve the above time summation, as in [19], based on the fact that \u2211\u221e T=0 \u2211T\u22121 t=0 f(T \u2212 t \u2212 1)g(t) can be rewritten as \u2211\u221e t=0 \u2211\u221e T=t+1 f(T \u2212 t \u2212 1)g(t) and then setting \u03c4 = T \u2212 t\u2212 1 to get \u2211\u221e t=0 g(t) \u2211\u221e \u03c4=0 f(\u03c4).", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "Complete details of these problems can be found in [1, 4].", "startOffset": 51, "endOffset": 57}, {"referenceID": 3, "context": "Complete details of these problems can be found in [1, 4].", "startOffset": 51, "endOffset": 57}, {"referenceID": 3, "context": "We compare our approach with the decentralized bounded policy iteration (DEC-BPI) algorithm [4] and a non-convex optimization solver (NLP) [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "We compare our approach with the decentralized bounded policy iteration (DEC-BPI) algorithm [4] and a non-convex optimization solver (NLP) [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "The NLP approach recasts the policy optimization problem as a non-linear program and uses an off-the-shelf solver, Snopt [9], to obtain a solution.", "startOffset": 121, "endOffset": 124}, {"referenceID": 8, "context": "In contrast, the NLP solver [9] is an optimized package and therefore for larger problems is currently faster than the EM approach.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "Agents should coordinate to open the door leading to the treasure [1].", "startOffset": 66, "endOffset": 69}, {"referenceID": 18, "context": "This is a potential drawback of the EM approach, which applies to other Markovian planning problems too when using the technique of [19].", "startOffset": 132, "endOffset": 136}, {"referenceID": 5, "context": "Furthermore, the structure of EM\u2019s update equations is very amenable to Google\u2019s Map-Reduce paradigm [6], allowing each parameter to be computed by a cluster of machines in parallel using Map-Reduce.", "startOffset": 101, "endOffset": 104}, {"referenceID": 12, "context": "The ND-POMDP model [13] is a class of DECPOMDPs specifically designed to support large multiagent systems.", "startOffset": 19, "endOffset": 23}], "year": 2010, "abstractText": "Decentralized POMDPs provide an expressive framework for multi-agent sequential decision making. While finite-horizon DECPOMDPs have enjoyed significant success, progress remains slow for the infinite-horizon case mainly due to the inherent complexity of optimizing stochastic controllers representing agent policies. We present a promising new class of algorithms for the infinite-horizon case, which recasts the optimization problem as inference in a mixture of DBNs. An attractive feature of this approach is the straightforward adoption of existing inference techniques in DBNs for solving DEC-POMDPs and supporting richer representations such as factored or continuous states and actions. We also derive the Expectation Maximization (EM) algorithm to optimize the joint policy represented as DBNs. Experiments on benchmark domains show that EM compares favorably against the state-of-the-art solvers.", "creator": "TeX"}}}