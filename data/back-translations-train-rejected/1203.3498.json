{"id": "1203.3498", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Automated Planning in Repeated Adversarial Games", "abstract": "Game theory's prescriptive power typically relies on full rationality and/or self-play interactions. In contrast, this work sets aside these fundamental premises and focuses instead on heterogeneous autonomous interactions between two or more agents. Specifically, we introduce a new and concise representation for repeated adversarial (constant-sum) games that highlight the necessary features that enable an automated planing agent to reason about how to score above the game's Nash equilibrium, when facing heterogeneous adversaries. To this end, we present TeamUP, a model-based RL algorithm designed for learning and planning such an abstraction. In essence, it is somewhat similar to R-max with a cleverly engineered reward shaping that treats exploration as an adversarial optimization problem. In practice, it attempts to find an ally with which to tacitly collude (in more than two-player games) and then collaborates on a joint plan of actions that can consistently score a high utility in adversarial repeated games. We use the inaugural Lemonade Stand Game Tournament to demonstrate the effectiveness of our approach, and find that TeamUP is the best performing agent, demoting the Tournament's actual winning strategy into second place. In our experimental analysis, we show hat our strategy successfully and consistently builds collaborations with many different heterogeneous (and sometimes very sophisticated) adversaries.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (200kb)", "http://arxiv.org/abs/1203.3498v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["enrique munoz de cote", "archie c chapman", "adam m sykulski", "nicholas r jennings"], "accepted": false, "id": "1203.3498"}, "pdf": {"name": "1203.3498.pdf", "metadata": {"source": "CRF", "title": "Automated Planning in Repeated Adversarial Games", "authors": ["Enrique Munoz de Cote", "Archie C. Chapman", "Adam M. Sykulski", "Nicholas R. Jennings"], "emails": ["jemc@ecs.soton.ac.uk", "acc@ecs.soton.ac.uk", "adam.sykulski@imperial.ac.uk", "nrj@ecs.soton.ac.uk"], "sections": [{"heading": null, "text": "We use the first Lemonade Stand Game Tournament1 to demonstrate the effectiveness of our approach and find that TeamUP is the most powerful agent that degrades the actual winning strategy of the tournament to second place. In our experimental analysis, we show that our strategy is successful and consistently builds collaborations with many different heterogeneous (and sometimes very demanding) opponents."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to survive themselves if they do not put themselves in a position to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves."}, {"heading": "2 Background and Definitions", "text": "During this work we look at players facing each other and repeatedly playing a normal form game."}, {"heading": "A normal form game (NFG) is a n\u2013player", "text": "It is the best answer to the question of how players maximize their reward, and their best answer to the question of whether the best strategies they use are in the series of joint actions, and ri: A \u2192 R is the player's reward function. (A) An agent's goal is to maximize his reward, and his best answer to the question of whether the best strategies he will use are in the series of actions he will use. (A) An agent's goal is to maximize his reward, and his best answer to the question of what he will do is the series of best strategies he will use. (A) In our context, stable points are characterized by the amount of pure balances (NE) that are these common action profiles in which no single agent has an incentive to change his action."}, {"heading": "3 Planning in Adversarial Repeated Games", "text": "Calculating a strategy of the game in repeated NFGs is an opposing optimization problem, as explained earlier. And, the optimization capabilities of the planner are based on 3A changes in \u03c0i, which predict a change in k \u2212 i.e., how well he can predict the behavior of his opponents. However, against adaptive opponents, predicting their behavior means that one learns from history a mapping of the opponent's response strategies. Nevertheless, opposing games involving two or more players are equipped with an underlying structure that can help thinking by using a higher representation. In particular, it is possible to balance the (constant) sum in favor of the planner by working with other players to minimize the benefit of the excluded opponents. However, such cooperation is difficult to achieve without explicit communication, correlation or pre-game agreements, which we do not take into account. Instead, we focus on tactical collusion, in which teams of cooperating players are formed by repeating actions at the specific level of the game, in order to implement these more effective actions at the later stage."}, {"heading": "3.1 State Abstraction", "text": "Our goal is to work with other players by making them an offer that they cannot refuse, i.e. an action profile that consistently gives them a score above the Nash balance of the game. To do this, we measure the deviation of the planner's opponents from the ideal types that the planner can easily predict and work with. In this work, we use the ideas of leading Q-Learning algorithms (Littman & Stone, 2002) to generate our ideal types. We now formally describe these ideal types and then show how we generalize them by measuring the deviations of opponents from the ideal types, resulting in characteristics. Finally, we describe how the state space is constructed with a tuple of these characteristics. It is this abstraction that will help us build an automatic planner in the spirit of Littman and Stein's work."}, {"heading": "Ideal types", "text": "There are two obvious ways to initiate cooperation: First, a player could lead by simply sticking to his current strategy and waiting to see if any opponents follow him; second, a player could follow by changing his strategy to a strategy that is within the BR set. Based on these two game patterns, we define two ideal strategies that an agent could easily collaborate with: \u2022 A perfect lead strategy selects a starting strategy and does not move away during the duration of the game. \u2022 A perfect follow strategy always selects actions that are a BR to the previous action of the opponent being pursued."}, {"heading": "Features", "text": "The planner classifies the opponents based on their proximity to playing a lead or follow strategy, based on their previous actions. An opponent, who is classified as a player of a lead strategy, usually moves slowly or stationary and is therefore highly predictable. An opponent, who is classified as a player of a follow strategy, tends to play actions that lie within the best response that is set from the previous time step (or a correspondingly discounted average of recent time steps). In view of this, we will now discuss these ideal types and how we measure the deviation of an opponent from them, which form the basis for our state abstraction.To classify its opponents, the planner maintains a measure for a lead index, li, and a follow index, fij, (where j-N-i) to measure whether the player follows a player, j. The indices are calculated from the sequence of past actions of each player, Ai = (a. i, a follow, an indicator, where an indicator, j., an indicator, where an indicator is)."}, {"heading": "4 The TeamUP Algorithm", "text": "TeamUP is a model-based RL algorithm designed to learn and plan against unknown enemies. Essentially, it is reminiscent of R-max (Brafman & Tennenholtz, 2002) (using a model-based RL paradigm and implicit exploration), but also includes a cleverly constructed reward formula that treats exploration as an adversarial optimization problem. In practice, TeamUP tries to find allies with whom to tacitly collaborate on a common action plan that can consistently deliver a high level of benefit in contradictory replay games. Note that although we motivated this study of contradictory games, the basic features of our algorithmic approach are general enough to be useful in the broader class of repeated general sum games. In detail, TeamUP demonstrates from the metrics presented in the section that feature instances, oi, the values from the set of the Oj = FO, are stated in each subinstance {Fj = FO, which means that in the next subinstance they are equal."}, {"heading": "4.1 States and Transitions", "text": "At a glance, the state representation of TeamUP is based on high-level observations that classify each opponent (and himself) as either stationary (L), chasing another player (Fx) or unknown (O).4Using this abstraction, the algorithm learns state transitions and expected rewards. 4Note that we always use the index 0 to reference the planner himself.The state of the planner is composed of a tuple of the form s = (o0, {oi} i-N\\ 0) S = \u00d7 i-NOi (which captures the high-level behavior of all players in the game) and its action space is defined by the amount {L, Fi, Fj}. From Equation (2,3), consider how the discount factor is responsible for how \"high-level\" strategies are successfully modified."}, {"heading": "4.2 Social Reward Shaping Exploration", "text": "Explorative actions in a multi-agent context deserve a different treatment from those of a single-agent learning problem. This is because an agent's (typically random) exploratory actions, while unintentional, could be interpreted by his opponents as deliberate strategic actions so that they can contribute to the way his opponents react. This means that exploration in a MAL context is not only a heuristic that allows an agent to learn about his environment, but should also be skillfully designed to signal his opponents correctly. Moreover, exploratory actions could be costly. This fact is exacerbated in finite horizontal games where a player's chance of achieving high scores is lost. Social reward forms (Babes et al, 2008) are external rewards presented to an RL algorithm as an adjunct to the actual game rewards. Their purpose is to promote desirable behavior during the learning process."}, {"heading": "4.3 The Algorithm", "text": "TeamUP takes the parameters GP, GP, GP, GP and K and learns a model M of the environment by experiencing tuples < s, a, s, r > and then calculates an optimal policy in relation to this current model. Its implementation can be divided into two phases: Initialization: Start with an initial estimate for the model parameters, in which all state action pairs provide a reward based on their specific design function (Section 4.2) and all states most likely lead 1 into the fictitious state s0. Based on this current model, a call to VI (M) 8 calculates a new optimal discounted policy based on their current model M. For each stage game: (a) observe that a new joint action a = (a, a1,...) is observed and the new functions are calculated using the decision flow (Figure 1) which relativizes the next state."}, {"heading": "5 The Lemonade Stand Game", "text": "In fact, the players who are able to outdo themselves have to outdo themselves. In fact, it is not that they are able to outdo themselves. In fact, it is that they are able to outdo themselves. In fact, it is that they are able to outdo themselves. In fact, it is that they are able to outdo themselves. In fact, it is not that they are able to outdo themselves. In fact, it is that they are able to outdo themselves. In fact, it is not that they are able to outdo themselves."}, {"heading": "6 Results", "text": "In this section, we analyze and compare TeamUP with the other entries in the first LSG tournament to demonstrate the performance of the planner's resulting strategy against other participants in the original tournament. Testing TeamUP in this area means that our planner will be faced with two heterogeneous and unknown strategies in a game involving more than two action opponents. Successful strategies will therefore be those that can consistently balance the constant sum in their favor."}, {"heading": "6.1 The LSG Tournament", "text": "First, we have relaunched the tournament, but TeamUP (see Table 1) is also part of the overall strategy. Therefore, note that we do not include results for Brown, who was placed in the original Tournament 11. We are repeating the structure of the original tournament, which is a round-robin format with each triplet combination of agents simulated for multiple repetitions; the original tournament, which we completed with EA2, is likely to be the winner and Pujara and RL3 awarded a statistical tie for second place; however, in the revised version, TeamUP is the overall winner, who demoted EA2 to second place; the parameters we used for TeamUP in all reported experiments are: \u03b3 = 0.05, GOP = 0.3, K = 15 and these were chosen taking into account the length of the game; also note that for the LSG Rmax = 12 (for optimal states) and Rmin = 6 (for worst states)."}, {"heading": "7 Conclusions and Future Work", "text": "This paper presents a new way of thinking about multi-agent heterogeneous interactions. Specifically, our analyses and results are maintained within the mathematical framework of repetitive opposing games. We propose a new way of analyzing each repetitive game played between the planner and one or more opponents by abstracting important features about the interaction between the players themselves. Specifically, this paper proposes that such features be defined in terms such as \"leader\" and \"follower\" and classify the behavior of our opponents under these terms. We introduced TeamUP to show how an automatic planner, by arguing in a strategy space (as opposed to an action space), can generate strategies that achieve high usefulness in opposing games."}, {"heading": "Babes, M., Munoz de Cote, E., & Littman, M. L. (2008).", "text": "Proceedings of AAMAS (pp. 1389-1392).Banerjee, B., & Peng, J. (2005).Efficient learning of multistep best response. Proceedings of AAMAS (pp. 60-66).Netherlands: ACM."}, {"heading": "Brafman, R. I., & Tennenholtz, M. (2002). R-MAX - A", "text": "General polynomial time algorithm for near optimal reinforcement learning. Journal of Machine Learning Research, 3, 213-231."}, {"heading": "Ficici, S., Parkes, D., & Pfeffer, A. (2008). Learning and", "text": "Corvallis, Oregon: AUAI Press.Littman, M. L. (1994). Markov games as a framework for multiagent reinforcement learning. Proceedings of the Eleventh International Conference On Machine Learning (pp. 157-163). Littman, M. L., & Stone, P. (2002). Implicit negotiation in repetitive games, 393-404. LNCS. Springer."}, {"heading": "Munoz de Cote, E., & Jennings, N. R. (2010). Planning", "text": "Proceedings of AAMAS.Ng, A. Y., Harada, D., & Russell, S. (1999): Policy invariance under reward transformations: Theory and application to reward shaping. Proceedings of ICML (pp. 278-287).Puterman, M. L. (1994): Markov decision processes - discrete stochastic dynamic programming. New York, NY: John Wiley & Sons, Inc."}, {"heading": "Sykulski, A. M., Chapman, A. C., Munoz de Cote, E., &", "text": "Jennings, N. R. (2010).EA2: The winning strategy for the inaugural limonade stand game tournament. Proceedings of the European Conference on Artificial Intelligence.Wicks, J., & Greenwald, A. (2005): An algorithm for calculating stochastically stable distributions with applications for multi-agent learning in repeated games. Proceedings of UAI (pp. 623-632). Arlington, Virginia: AUAI Press."}], "references": [{"title": "Social reward shaping in the prisoner\u2019s dilemma", "author": ["M. Babes", "E. Munoz de Cote", "M.L. Littman"], "venue": "Proceedings of AAMAS (pp. 1389\u20131392)", "citeRegEx": "Babes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Babes et al\\.", "year": 2008}, {"title": "Efficient learning of multistep best response", "author": ["B. Banerjee", "J. Peng"], "venue": "Proceedings of AAMAS (pp. 60\u201366)", "citeRegEx": "Banerjee and Peng,? \\Q2005\\E", "shortCiteRegEx": "Banerjee and Peng", "year": 2005}, {"title": "R-MAX - A general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman and Tennenholtz,? \\Q2002\\E", "shortCiteRegEx": "Brafman and Tennenholtz", "year": 2002}, {"title": "Learning and solving many-player games through a cluster-based representation", "author": ["S. Ficici", "D. Parkes", "A. Pfeffer"], "venue": "Proceedings of UAI (pp. 188\u2013195)", "citeRegEx": "Ficici et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ficici et al\\.", "year": 2008}, {"title": "Markov games as a framework for multi-agent reinforcement learning", "author": ["M.L. Littman"], "venue": "Proceedings of the Eleventh International Conference On Machine Learning (pp. 157\u2013163)", "citeRegEx": "Littman,? \\Q1994\\E", "shortCiteRegEx": "Littman", "year": 1994}, {"title": "Implicit negotiation in repeated games, 393\u2013404", "author": ["M.L. Littman", "P. Stone"], "venue": null, "citeRegEx": "Littman and Stone,? \\Q2002\\E", "shortCiteRegEx": "Littman and Stone", "year": 2002}, {"title": "Planning against fictitious players in repeated normal form games", "author": ["E. Munoz de Cote", "N.R. Jennings"], "venue": "Proceedings of AAMAS", "citeRegEx": "Cote and Jennings,? \\Q2010\\E", "shortCiteRegEx": "Cote and Jennings", "year": 2010}, {"title": "Policy invariance under reward transformations: Theory and application to reward shaping", "author": ["A.Y. Ng", "D. Harada", "S. Russell"], "venue": "Proceedings of ICML (pp. 278\u2013287)", "citeRegEx": "Ng et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Ng et al\\.", "year": 1999}, {"title": "Markov decision processes\u2014 discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "EA: The winning strategy for the inaugural lemonade stand game tournament", "author": ["A.M. Sykulski", "A.C. Chapman", "E. Munoz de Cote", "N.R. Jennings"], "venue": "Proceedings of the 2010 European Conference on Artificial Intelligence", "citeRegEx": "Sykulski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sykulski et al\\.", "year": 2010}, {"title": "An algorithm for computing stochastically stable distributions with applications to multiagent learning in repeated games", "author": ["J. Wicks", "A. Greenwald"], "venue": "Proceedings of UAI (pp. 623\u2013632)", "citeRegEx": "Wicks and Greenwald,? \\Q2005\\E", "shortCiteRegEx": "Wicks and Greenwald", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "First, instead of assuming that our planner is facing the worst opponent, as (Littman, 1994; Brafman & Tennenholtz, 2002) do, our objective is to design a planner whose resulting strategy can do better than its security (max\u2013min) equilibrium strategy.", "startOffset": 77, "endOffset": 121}, {"referenceID": 0, "context": "Social reward shaping (Babes et al., 2008) are external rewards presented to a RL algorithm as an addition to the actual stage game rewards.", "startOffset": 22, "endOffset": 42}, {"referenceID": 7, "context": "6 Social shaping extends the well known potential\u2013based shaping framework (Ng et al., 1999) to a multiagent context.", "startOffset": 74, "endOffset": 91}, {"referenceID": 8, "context": "When trying to extend potential based shaping to stochastic games (a generalization of Markov decision processes (MDPs) (Puterman, 1994) that allows multiple agents), the potential of a state (i.", "startOffset": 120, "endOffset": 136}, {"referenceID": 8, "context": "Where VI(M) is a call to the standard value iteration algorithm (Puterman, 1994) on the current model M .", "startOffset": 64, "endOffset": 80}, {"referenceID": 9, "context": "We refer the reader to (Sykulski et al., 2010) for a throughout description and analysis of the game.", "startOffset": 23, "endOffset": 46}, {"referenceID": 3, "context": "Also, it would be interesting investigating if our abstraction could be generalized with the idea of clusterbased representations found in (Ficici et al., 2008).", "startOffset": 139, "endOffset": 160}], "year": 2010, "abstractText": "Game theory\u2019s prescriptive power typically relies on full rationality and/or self\u2013play interactions. In contrast, this work sets aside these fundamental premises and focuses instead on heterogeneous autonomous interactions between two or more agents. Specifically, we introduce a new and concise representation for repeated adversarial (constant\u2013sum) games that highlight the necessary features that enable an automated planing agent to reason about how to score above the game\u2019s Nash equilibrium, when facing heterogeneous adversaries. To this end, we present TeamUP, a model\u2013based RL algorithm designed for learning and planning such an abstraction. In essence, it is somewhat similar to R-max with a cleverly engineered reward shaping that treats exploration as an adversarial optimization problem. In practice, it attempts to find an ally with which to tacitly collude (in more than two\u2013player games) and then collaborates on a joint plan of actions that can consistently score a high utility in adversarial repeated games. We use the inaugural Lemonade Stand Game Tournament to demonstrate the effectiveness of our approach, and find that TeamUP is the best performing agent, demoting the Tournament\u2019s actual winning strategy into second place. In our experimental analysis, we show hat our strategy successfully and consistently builds collaborations with many different heterogeneous (and sometimes very sophisticated) adversaries.", "creator": "dvips(k) 5.97 Copyright 2008 Radical Eye Software"}}}