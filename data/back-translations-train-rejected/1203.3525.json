{"id": "1203.3525", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Learning Why Things Change: The Difference-Based Causality Learner", "abstract": "In this paper, we present the Difference- Based Causality Learner (DBCL), an algorithm for learning a class of discrete-time dynamic models that represents all causation across time by means of difference equations driving change in a system. We motivate this representation with real-world mechanical systems and prove DBCL's correctness for learning structure from time series data, an endeavour that is complicated by the existence of latent derivatives that have to be detected. We also prove that, under common assumptions for causal discovery, DBCL will identify the presence or absence of feedback loops, making the model more useful for predicting the effects of manipulating variables when the system is in equilibrium. We argue analytically and show empirically the advantages of DBCL over vector autoregression (VAR) and Granger causality models as well as modified forms of Bayesian and constraintbased structure discovery algorithms. Finally, we show that our algorithm can discover causal directions of alpha rhythms in human brains from EEG data.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (236kb)", "http://arxiv.org/abs/1203.3525v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mark voortman", "denver dash", "marek j druzdzel"], "accepted": false, "id": "1203.3525"}, "pdf": {"name": "1203.3525.pdf", "metadata": {"source": "CRF", "title": "Learning Why Things Change: The Difference-Based Causality Learner", "authors": ["Mark Voortman", "Marek J. Druzdzel"], "emails": ["mark@voortman.name", "denver.h.dash@intel.com", "marek@sis.pitt.edu"], "sections": [{"heading": null, "text": "In this paper, we present the Difference Based Causality Learner (DBCL), an algorithm for learning a class of discrete time dynamic models that represents all temporal causalities through differential equations that drive change in a system. We motivate this representation with real-world mechanical systems and prove that DBCL proves the correctness of learning structures from time series data, an endeavor complicated by the presence of latent derivatives that need to be detected. We also prove that under common assumptions for causal discovery, DBCL identifies the presence or absence of feedback loops, making the model more useful for predicting the effects of manipulating variables when the system is in equilibrium. We argue analytically and empirically demonstrate the advantages of DBCL over vector autoregression (VAR) and granular modality algorithms when the system is in equilibrium."}, {"heading": "1 INTRODUCTION AND MOTIVATION", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 DIFFERENCE-BASED CAUSAL MODELS", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "2.1 COMPARISON OF REPRESENTATIONS", "text": "Dynamic SEMs, such as Granger causality and VAR models, allow arbitrary edges over time. Unfortunately, this representation is too generic for many real physical systems. DBCMs, on the other hand, assume that all causalities work in the same way as in mechanical systems. This limitation represents a compromise between expressiveness and tractability. On the one hand, DBCMs can only represent mechanical systems that are Markovian of the first order. On the other hand, we know at least a little about these latent variables that are required to make the Markovian system. When we are confronted with data generated by differential equations, Markovian marvables may be missing, illustrating the distinction between DBCL and the other approaches."}, {"heading": "3 DBCM LEARNING", "text": "The DBCM learning problem can be posed as follows: Given time series data over a set of variables V, we derive a DBCM over a set of variables V-V-Y, where V-X contains differences over variables derived from the original data. In other words, DBCL does not assume all relevant derivatives or the order of these derivatives. Instead, it treats these missing derivatives as latent variables and attempts to detect them. We assume that apart from these derivatives, there are no other latent confusing variables. Note, for example, that this assumption also excludes the existence of structures of the form \u2206 X-X-Y, where \u2206 X and X-X are latent and Y is observable, because the X process forms a latent chain over time that Y can confuse at different times."}, {"heading": "3.1 THE ALGORITHM", "text": "DBCL consists of two steps: (1) Recognition of prime numbers (and integral) variables (V) and (2) Learning the contemporary structure. (1) The first step is accomplished by calculating numerical derivatives of all variables and then deciding which of them should be primary variables, based on the following theorem, which takes advantage of the fact that only primary variables can always be made independent of time by conditioning variables in V 1: theorem 1 (Detection of primary variables). (1) Let me imply the set of conditional independence relationships that are implied to a DBCM = < V, E > with V = 0 primary variables in V 1: E1 (Detection of primary variables). (2) Let me imply the set of conditional independence relationships that are applied to a fidelity implied to a DBCM < > EV with detection in V."}, {"heading": "3.2 IDENTIFICATION OF EMC VIOLATION", "text": "Considering a model output of DBCL, one might be interested in performing causal conclusions, i.e., predicting the effects of manipulating components of the system. < This operation is complicated by the presence of balances that may have occurred in the system. Dash [2003, 2005] shows that some dynamic systems do not obey equilibrium manipulation commutability (EMC), i.e. the causal graph that arises when an equilibrium model is manipulated may differ from the (true) graph that arises when the dynamic model is manipulated and then equilibrium. Dash points to two conditions that help in EMC identification: First, if a variable is self-regulating, which means that the X structure Pa (X) may be different, then when the dynamic model is manipulated and then equilibrium."}, {"heading": "4 RESULTS", "text": "For our empirical studies, we have generated data from real physical systems that are representative of the type of systems found in nature. We have also applied DBCL to real EEG brain data to reveal the causal propagation of alpha waves.1 The validation of DBCL is complicated by the fact that, as far as we know, there are few suitable baseline methods that are even in principle able to learn a DBCM when derivatives are unknown. As discussed in Section 2.1, when trying to learn causal relationships with the latent variables that are marginalized, an infinite order of Markov model results (Figure 1). FCI algorithms et al., which attempts to take latent variables into account, would also lead to causation."}, {"heading": "4.1 HARMONIC OSCILLATORS", "text": "In fact, most of them are able to assert themselves, that they are able to trump themselves, and that they are able to trump themselves, \"he said in an interview with the\" New York Times. \""}, {"heading": "4.2 EEG BRAIN DATA", "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "5 DISCUSSION", "text": "The main contribution of this paper is to present a new representation of learning models from time series. While DBCM has been discussed elsewhere in relation to the causality of dynamic systems, there has so far been no algorithm to learn from data. This paper presents such an algorithm and makes DBCM accessible to a wide range of practitioners in economics. Biology and AI, currently based on the causality of vectors, indicate that DBCM is particularly well suited for learning systems based on differential equations, and empirically show that such systems, in which the relevant derivatives are not known, do not exist either."}, {"heading": "Acknowledgments", "text": "Marek Druzdzel was partially supported by the National Institute of Health under funding number U01HL10106601. We thank the anonymous reviewers for their useful feedback."}, {"heading": "APPENDIX: PROOFS", "text": "There is only such a model if there is such a model, that there is such a model, that there is such a model, that there is such a model, that there is such a model, and only if there is such a model. (There is such a model that there is such a model) (There is such a model that there is such a model) (There is such a model that there is such a model) (There is such a model that there is such a model) (There is such a model that there is such a model). (There is such a model that there is such a model.) (There is such a model that there is such a model that there is such a model.) (There is such a model that there is such a model that there is such a model.) (There is such a model that there is such a model that there is such a model."}], "references": [{"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["Gregory F. Cooper", "Edward Herskovits"], "venue": "Machine Learning,", "citeRegEx": "Cooper and Herskovits.,? \\Q1992\\E", "shortCiteRegEx": "Cooper and Herskovits.", "year": 1992}, {"title": "Caveats for Causal Reasoning with Equilibrium Models", "author": ["Denver Dash"], "venue": "PhD thesis, Intelligent Systems Program,", "citeRegEx": "Dash.,? \\Q2003\\E", "shortCiteRegEx": "Dash.", "year": 2003}, {"title": "Restructuring dynamic causal systems in equilibrium. In AIStats, pages 81\u201388", "author": ["Denver Dash"], "venue": "Society for Artificial Intelligence and Statistics,", "citeRegEx": "Dash.,? \\Q2005\\E", "shortCiteRegEx": "Dash.", "year": 2005}, {"title": "Causal reasoning in graphical time series models", "author": ["Michael Eichler", "Vanessa Didelez"], "venue": "In UAI,", "citeRegEx": "Eichler and Didelez.,? \\Q2007\\E", "shortCiteRegEx": "Eichler and Didelez.", "year": 2007}, {"title": "Cointegration and error-correction: Representation, estimation, and testing", "author": ["Robert E. Engle", "Clive W.J. Granger"], "venue": null, "citeRegEx": "Engle and Granger.,? \\Q1987\\E", "shortCiteRegEx": "Engle and Granger.", "year": 1987}, {"title": "The Bayesian structural EM algorithm", "author": ["Nir Friedman"], "venue": "In UAI,", "citeRegEx": "Friedman.,? \\Q1998\\E", "shortCiteRegEx": "Friedman.", "year": 1998}, {"title": "Learning the structure of dynamic probabilistic networks. In UAI, pages 139\u2013147", "author": ["Nir Friedman", "Kevin Murphy", "Stuart Russell"], "venue": null, "citeRegEx": "Friedman et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1998}, {"title": "Learning Gaussian networks", "author": ["D. Geiger", "D. Heckerman"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "Geiger and Heckerman.,? \\Q1994\\E", "shortCiteRegEx": "Geiger and Heckerman.", "year": 1994}, {"title": "Measuring directional coupling between EEG sources", "author": ["Germ\u00e1n G\u00f2mez-Herrero", "Mercedes Atienza", "Karen Egiazarian", "Jose L. Cantero"], "venue": null, "citeRegEx": "G\u00f2mez.Herrero et al\\.,? \\Q2008\\E", "shortCiteRegEx": "G\u00f2mez.Herrero et al\\.", "year": 2008}, {"title": "Investigating causal relations by econometric models and cross-spectral methods", "author": ["Clive W.J. Granger"], "venue": null, "citeRegEx": "Granger.,? \\Q1969\\E", "shortCiteRegEx": "Granger.", "year": 1969}, {"title": "Causality and model abstraction", "author": ["Yumi Iwasaki", "Herbert A. Simon"], "venue": "Artificial Intelligence,", "citeRegEx": "Iwasaki and Simon.,? \\Q1994\\E", "shortCiteRegEx": "Iwasaki and Simon.", "year": 1994}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q2000\\E", "shortCiteRegEx": "Pearl.", "year": 2000}, {"title": "Evidential reasoning using stochastic simulation of causal models", "author": ["Judea Pearl"], "venue": "Artifical Intelligence,", "citeRegEx": "Pearl.,? \\Q1987\\E", "shortCiteRegEx": "Pearl.", "year": 1987}], "referenceMentions": [{"referenceID": 12, "context": ", Strotz and Wold, 1960], and Bayesian networks [Pearl, 1987] which started the paradigm shift of graphical models in AI and machine learning 20 years ago.", "startOffset": 48, "endOffset": 61}, {"referenceID": 6, "context": "In AI, there has been work on learning Dynamic Bayesian Networks (DBNs) [Friedman et al., 1998] and modified Granger causality [Eichler and Didelez, 2007].", "startOffset": 72, "endOffset": 95}, {"referenceID": 3, "context": ", 1998] and modified Granger causality [Eichler and Didelez, 2007].", "startOffset": 39, "endOffset": 66}, {"referenceID": 10, "context": "This paper considers Difference-Based Causal Models (DBCMs), a class of discrete-time dynamic models inspired by Iwasaki and Simon [1994] that models all causation across time by means of difference equations driving change in the system.", "startOffset": 113, "endOffset": 138}, {"referenceID": 8, "context": "This cross-temporal restriction makes DBCMs a subset of causal models as defined by Pearl [2000] and structural equation models similar to those discussed 50 years ago by Strotz and Wold [1960].", "startOffset": 84, "endOffset": 97}, {"referenceID": 8, "context": "This cross-temporal restriction makes DBCMs a subset of causal models as defined by Pearl [2000] and structural equation models similar to those discussed 50 years ago by Strotz and Wold [1960]. DBCM-like models were discussed by Iwasaki and Simon [1994] and Dash [2003, 2005] to analyze causality in dynamic systems, but to date no algorithm exists to learn them from data.", "startOffset": 84, "endOffset": 194}, {"referenceID": 8, "context": "DBCM-like models were discussed by Iwasaki and Simon [1994] and Dash [2003, 2005] to analyze causality in dynamic systems, but to date no algorithm exists to learn them from data.", "startOffset": 35, "endOffset": 60}, {"referenceID": 5, "context": "The structural EM algorithm [Friedman, 1998] does try to learn explicit latent variables and structure.", "startOffset": 28, "endOffset": 44}, {"referenceID": 7, "context": "It is possible to use a Bayesian approach without discretizing the data [Geiger and Heckerman, 1994], which we may explore in the future.", "startOffset": 72, "endOffset": 100}, {"referenceID": 8, "context": "One hypothesis to explain this is given by G\u00f2mez-Herrero et al. [2008] where they point out that conductivity of the", "startOffset": 43, "endOffset": 71}], "year": 2010, "abstractText": "In this paper, we present the DifferenceBased Causality Learner (DBCL), an algorithm for learning a class of discrete-time dynamic models that represents all causation across time by means of difference equations driving change in a system. We motivate this representation with real-world mechanical systems and prove DBCL\u2019s correctness for learning structure from time series data, an endeavour that is complicated by the existence of latent derivatives that have to be detected. We also prove that, under common assumptions for causal discovery, DBCL will identify the presence or absence of feedback loops, making the model more useful for predicting the effects of manipulating variables when the system is in equilibrium. We argue analytically and show empirically the advantages of DBCL over vector autoregression (VAR) and Granger causality models as well as modified forms of Bayesian and constraintbased structure discovery algorithms. Finally, we show that our algorithm can discover causal directions of alpha rhythms in human brains from EEG data.", "creator": "TeX"}}}