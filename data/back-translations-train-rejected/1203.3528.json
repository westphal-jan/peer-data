{"id": "1203.3528", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Rollout Sampling Policy Iteration for Decentralized POMDPs", "abstract": "We present decentralized rollout sampling policy iteration (DecRSPI) - a new algorithm for multi-agent decision problems formalized as DEC-POMDPs. DecRSPI is designed to improve scalability and tackle problems that lack an explicit model. The algorithm uses Monte- Carlo methods to generate a sample of reachable belief states. Then it computes a joint policy for each belief state based on the rollout estimations. A new policy representation allows us to represent solutions compactly. The key benefits of the algorithm are its linear time complexity over the number of agents, its bounded memory usage and good solution quality. It can solve larger problems that are intractable for existing planning algorithms. Experimental results confirm the effectiveness and scalability of the approach.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (212kb)", "http://arxiv.org/abs/1203.3528v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["feng wu", "shlomo zilberstein", "xiaoping chen"], "accepted": false, "id": "1203.3528"}, "pdf": {"name": "1203.3528.pdf", "metadata": {"source": "CRF", "title": "Rollout Sampling Policy Iteration for Decentralized POMDPs", "authors": ["Feng Wu", "Xiaoping Chen"], "emails": ["wufeng@mail.ustc.edu.cn", "shlomo@cs.umass.edu", "xpchen@ustc.edu.cn"], "sections": [{"heading": null, "text": "The algorithm uses MonteCarlo methods to generate a sample of achievable states of belief and then calculates a common policy for each state based on the rollout estimates. A new political representation allows us to present solutions in a concise manner. The main advantages of the algorithm are its linear time complexity compared to the number of actors, its limited memory usage and good solution quality. It can solve major problems that are insoluble for existing planning algorithms. Experimental results confirm the effectiveness and scalability of the approach."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Decentralized POMDPs", "text": "Formally, a finite horizon DEC-POMDP is a common observation space."}, {"heading": "3 The Rollout Sampling Method", "text": "In this section, we propose a new Rollout Sampling Policy Iteration (DecRSPI) for DEC POMDPs that generate heuristic stochastic strategies. & RSPI proposes a new Rollout Sampling Policy. & RSPI = Enhanced Strategies > POMDPs that generate heuristic stochastic strategies and use an approximate policy enhancement operator trained in Monte Carlo simulation. & RSPI = Enhanced Strategies > POMDPs that evaluate a common policy and then release the linear programs to induce a new enhanced policy. Similar to MBDP, DecRSPI creates point-based dynamic programming strategies that build strategies from bottom to top according to heuristic state distributions, the main difference being that DecRSPI improves strategies through simulation without knowing the precise transition function P, observation function, and reward function of the DER POC-MDP."}, {"heading": "3.1 Belief Sampling", "text": "In this paper, the doctrine b (S) is a distribution of probabilities across the states."}, {"heading": "3.2 Policy Improvement", "text": "In multi-agent settings, agents with only local information need to think about all the possible decisions of others and select the optimal common policy that maximizes the expected reward. A simple way to find the optimal common policy is simply to search across the entire space of possible policies, evaluate each one and select the policy with the highest value. Unfortunately, the number of possible common strategies is O (\"Ai\") and eliminates the dominated strategies in the early stages [11]. Instead of trawling through the entire political space, Dynamic Programming (DP) constructs strategies from the last step to the first and eliminates the dominated strategies in the early stages [11]."}, {"heading": "3.3 Rollout Evaluation", "text": "The rollout valuation is a Monte Carlo method for estimating the value of a policy ~ q at a state s (or state b), without requiring an explicit representation of the value function, as the DP algorithm does. A rollout for < s, ~ q > simulates a path from state s and the selection of measures according to the policy ~ q to horizon T. The observed total cumulative reward is averaged over K rollouts to estimate the value V (s, ~ q >). If a state b is given, it is easy to draw a state s from b and perform this simulation. A sketch of the rollout process is given in algorithm V. The accuracy of the expected value improves with the number of rollouts. Intuitively, the value can be considered a random variable starting from < s, ~ q >, the expectation of which is V (s, ~ q). Each rollout term is an average of this random sample and the V is the one of these random variables."}, {"heading": "3.4 Complexity Analysis", "text": "Note that the size of the policies of each agent with T levels and N decision nodes is given at each level. At each iteration, DecRSPI chooses an unimproved common policy and tries to improve the policy parameters (actions and node selection functions) of each agent. Therefore, the space required for m agents is in the order of O (mTN). Multiple rollouts are performed in the main process of each iteration, i.e. O (T 2). Theorem 3. The DecRSPI algorithm has a linear space and square time complexity relative to horizon T. It is clear that the amount of space grows linearly with the number of agents. At each iteration, the main loop chooses N common policies. At each common policy, the improvement process significantly selects agents relative to horizon T."}, {"heading": "4 Experiments", "text": "We conducted experiments on several common benchmark problems in the DEC-POMDP literature to evaluate the solution quality and runtime of DecRSPI. A larger domain of the distributed sensor network was used to test the scalability of DecRSPI in terms of the number of agents."}, {"heading": "4.1 Benchmark Problems", "text": "In fact, most of us are able to survive on our own, and they don't."}, {"heading": "4.2 Distributed Sensor Network", "text": "The distributed sensor network (DSN) problem, adapted to [20], consists of two chains with identical number of sensors as shown in Figure 3. The region between the chains is divided into cells and each cell is surrounded by four sensors, the two targets can move to the location, either move to an adjacent cell or remain in place with equal probability. Each target starts with an energy level of 2. A target is captured and removed when it reaches 0. Each sensor can take 3 actions (track-left, track-right and none) and has 4 observations (left and right cells are occupied or not), resulting in common spaces of 3 | I | actions and 4 | observations (e.g. 320 x 3.5 x 109 joint actions and 420 x 1012 joint observations for the 20 agent case). Each track action has a cost of 1. The energy of a target is reduced by 1 if it is tracked by at least three of the surrounding sensors at the same time."}, {"heading": "5 Related Work", "text": "The Distributed Gradient Descent (DGD) algorithm performs gradient-based policy searches independently of each agent's local controller using empirical data [15]. Zhang et al. [20] proposed an online algorithm that uses a natural actor-critic algorithm using conditional random fields (CRF). It can learn cooperative strategies with CRFs, but assumes that agents can communicate at every step and share their local observations. Melo [13] proposed a different algorithm with natural gradients, but it only works for transitional DEC POMDPs. In the contract, our algorithm learns cooperative strategies for the general DEC POMDP setting without any assumptions about communication. The rollout sampling method was introduced to learn MDP strategies without explicitly representing the value function [9, 10, 12]."}, {"heading": "6 Conclusion", "text": "In many applications, system dynamics are either too complex to model accurately, or too large to be explicitly represented. DecRSPI learns policies from experiences gained through mere environmental interaction; the strategies learned are fully decentralized, with no assumptions about communication or global observability; another advantage of DecRSPI is that it focuses the calculation only on achievable states. As the experiments show, little sampling is required in areas where agents have sparse interaction structures, and the solution quality, which is calculated by a small number of samples, is fairly close to the best existing planning algorithms. Most importantly, DecRSPI has linear time complexity across the number of agents, so DecRSPI can better plan problems with agents to use them as helpful in interacting with other agents."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the Air Force Office of Scientific Research under grant no. FA9550-08-1-0181, the National Science Foundation under grant no. IIS0812149, the Natural Science Foundations of China under grant no. 60745002, and the National Hi-Tech Project of China under grant no. 2008AA01Z150."}], "references": [{"title": "Incremental policy generation for finite-horizon DEC- POMDPs", "author": ["Chistopher Amato", "Jilles S. Dibangoye", "Shlomo Zilberstein"], "venue": "In Proc. of the 19th Int\u2019l Conf. on Automated Planning and Scheduling,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "The complexity of decentralized control of Markov decision processes", "author": ["Daniel S. Bernstein", "Shlomo Zilberstein", "Neil Immerman"], "venue": "In Proc. of the 16th Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Bounded policy iteration for decentralized POMDPs", "author": ["Daniel S. Bernstein", "Eric A. Hansen", "Shlomo Zilberstein"], "venue": "In Proc. of the 19th Int\u2019l Joint Conf. on Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Rollout algorithms for combinatorial optimization", "author": ["Dimitri P. Bertsekas", "John N. Tsitsiklis", "Cynara Wu"], "venue": "Journal of Heuristics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Parallel rollout for online solution of Dec-POMDPs", "author": ["Camille Besse", "Brahim Chaib-draa"], "venue": "In Proc. of the 21st Int\u2019l FLAIRS Conf.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["Lucian Busoniu", "Robert Babuska", "Bart D. Schutter"], "venue": "IEEE Trans. on SMC, Part C,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Parallel rollout for online solution of partially observable Markov decision processes", "author": ["Hyeong Soo Chang", "Robert Givan", "Edwin K.P. Chong"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Point-based incremental pruning heuristic for solving finite-horizon DEC-POMDPs", "author": ["Jilles S. Dibangoye", "Abdel-Illah Mouaddib", "Brahim Chaib-draa"], "venue": "In Proc. of the 8th Int\u2019l Joint Conf. on Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Rollout sampling approximate policy iteration", "author": ["Christos Dimitrakakis", "Michail G. Lagoudakis"], "venue": "Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Approximate policy iteration with a policy language bias", "author": ["Alan Fern", "Sung Wook Yoon", "Robert Givan"], "venue": "In Proc. of the 17th Conf. on Neural Info. Processing Systems,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Dynamic programming for partially observable stochastic games", "author": ["Eric A. Hansen", "Daniel S. Bernstein", "Shlomo Zilberstein"], "venue": "In Proc. of the 19th National Conf. on Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Reinforcement learning as classification: Leveraging modern classifiers", "author": ["Michail G. Lagoudakis", "Ronald Parr"], "venue": "In Proc. of the 20th Int\u2019l Conf. on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "Exploiting locality of interactions using a policy-gradient approach in multiagent learning", "author": ["Francisco S. Melo"], "venue": "In Proc. of the 18th European Conf. on Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings", "author": ["Ranjit Nair", "Milind Tambe", "Makoto Yokoo", "David V. Pynadath", "Stacy Marsella"], "venue": "In Proc. of the 18th Int\u2019l Joint Conf. on Artificial Intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Learning to cooperate via policy search", "author": ["Leonid Peshkin", "Kee-Eung Kim", "Nicolas Meuleau", "Leslie Pack Kaelbling"], "venue": "In Proc. of the 16th Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2000}, {"title": "Bounded finite state controllers", "author": ["Pascal Poupart", "Craig Boutilier"], "venue": "In Proc. of the 17th Annual Conference on Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Improved memorybounded dynamic programming for decentralized POMDPs", "author": ["Sven Seuken", "Shlomo Zilberstein"], "venue": "In Proc. of the 23rd Conf. on Uncertainty in Artificial Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Memory-bounded dynamic programming for DEC-POMDPs", "author": ["Sven Seuken", "Shlomo Zilberstein"], "venue": "In Proc. of the 20th Int\u2019ll Joint Conf. on Artificial Intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Reinforcement Learning: An Introduction", "author": ["Richard Sutton", "Andrew Barto"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Conditional random fields for multi-agent reinforcement learning", "author": ["Xinhua Zhang", "Douglas Aberdeen", "S.V.N. Vishwanathan"], "venue": "In Proc. of the 24th Int\u2019l Conf. on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": "These problems can be modeled as decentralized POMDPs (DEC-POMDPs) [2].", "startOffset": 67, "endOffset": 70}, {"referenceID": 17, "context": "When a complete model of the domain is available, DECPOMDPs can be solved using a wide range of optimal or approximate algorithms, particularly MBDP [18] and its descendants [1, 8, 17].", "startOffset": 149, "endOffset": 153}, {"referenceID": 0, "context": "When a complete model of the domain is available, DECPOMDPs can be solved using a wide range of optimal or approximate algorithms, particularly MBDP [18] and its descendants [1, 8, 17].", "startOffset": 174, "endOffset": 184}, {"referenceID": 7, "context": "When a complete model of the domain is available, DECPOMDPs can be solved using a wide range of optimal or approximate algorithms, particularly MBDP [18] and its descendants [1, 8, 17].", "startOffset": 174, "endOffset": 184}, {"referenceID": 16, "context": "When a complete model of the domain is available, DECPOMDPs can be solved using a wide range of optimal or approximate algorithms, particularly MBDP [18] and its descendants [1, 8, 17].", "startOffset": 174, "endOffset": 184}, {"referenceID": 1, "context": "This is not surprising given that finite-horizon DECPOMDPs are NEXP-complete [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 18, "context": "Incomplete domain knowledge is often addressed by reinforcement learning algorithms [19].", "startOffset": 84, "endOffset": 88}, {"referenceID": 5, "context": "However, most cooperative multi-agent reinforcement learning algorithms assume that the system state is completely observable by all the agents [6].", "startOffset": 144, "endOffset": 147}, {"referenceID": 18, "context": "In reinforcement learning, a class of useful techniques such as Monte-Carlo methods allows agents to choose actions based on experience [19].", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "While the policy execution is decentralized, planning or learning algorithms can operate offline and thus may be centralized [11, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 17, "context": "While the policy execution is decentralized, planning or learning algorithms can operate offline and thus may be centralized [11, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 0, "context": "The policies for finite-horizon DEC-POMDPs are often represented as a set of local policy trees [1, 8, 11, 17, 18].", "startOffset": 96, "endOffset": 114}, {"referenceID": 7, "context": "The policies for finite-horizon DEC-POMDPs are often represented as a set of local policy trees [1, 8, 11, 17, 18].", "startOffset": 96, "endOffset": 114}, {"referenceID": 10, "context": "The policies for finite-horizon DEC-POMDPs are often represented as a set of local policy trees [1, 8, 11, 17, 18].", "startOffset": 96, "endOffset": 114}, {"referenceID": 16, "context": "The policies for finite-horizon DEC-POMDPs are often represented as a set of local policy trees [1, 8, 11, 17, 18].", "startOffset": 96, "endOffset": 114}, {"referenceID": 17, "context": "The policies for finite-horizon DEC-POMDPs are often represented as a set of local policy trees [1, 8, 11, 17, 18].", "startOffset": 96, "endOffset": 114}, {"referenceID": 10, "context": "A dynamic programming (DP) algorithm was developed to build the policy trees optimally from the bottom up [11].", "startOffset": 106, "endOffset": 110}, {"referenceID": 0, "context": "These methods keep only a fixed number of trees at each iteration [1, 8, 17, 18].", "startOffset": 66, "endOffset": 80}, {"referenceID": 7, "context": "These methods keep only a fixed number of trees at each iteration [1, 8, 17, 18].", "startOffset": 66, "endOffset": 80}, {"referenceID": 16, "context": "These methods keep only a fixed number of trees at each iteration [1, 8, 17, 18].", "startOffset": 66, "endOffset": 80}, {"referenceID": 17, "context": "These methods keep only a fixed number of trees at each iteration [1, 8, 17, 18].", "startOffset": 66, "endOffset": 80}, {"referenceID": 15, "context": "It is quite similar to stochastic finite state controllers (FSC), used to solve infinite-horizon POMDPs [16] and DEC-POMDPs [3].", "startOffset": 104, "endOffset": 108}, {"referenceID": 2, "context": "It is quite similar to stochastic finite state controllers (FSC), used to solve infinite-horizon POMDPs [16] and DEC-POMDPs [3].", "startOffset": 124, "endOffset": 127}, {"referenceID": 10, "context": "Instead of searching over the entire policy space, dynamic programming (DP) constructs policies from the last step up to the first one and eliminates dominated policies at the early stages [11].", "startOffset": 189, "endOffset": 193}, {"referenceID": 17, "context": "Memory-bounded techniques have been developed to combine the top-down heuristics and the bottom-up dynamic programming together, keeping only a bounded number of policies at each iteration [18].", "startOffset": 189, "endOffset": 193}, {"referenceID": 16, "context": "This results in linear complexity over the horizon, but the one-step backup operation is still time-consuming [17].", "startOffset": 110, "endOffset": 114}, {"referenceID": 17, "context": "Our algorithm is based on the MBDP algorithm [18], but it approximates the backup operation with an alternating maximization process.", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "[14] and later refined by Bernstein et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "In the experiments, we compared our results with the distributed gradient descent (DGD) [15] with different horizons.", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "We also present the results of PBIP-IPG [1] \u2014 the best existing planning algorithm\u2014 for these domains.", "startOffset": 40, "endOffset": 43}, {"referenceID": 0, "context": "We experimented with three common DEC-POMDP benchmark problems, which are also used by PBIPIPG [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "The Meeting in a 3\u00d73 Grid problem [3] involves two robots that navigate in a 3\u00d73 grid and try to stay as much time as possible in the same cell.", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "[1], which has 81 states, 5 actions and 9 observations per robot.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "The Cooperative Box Pushing problem [17] involves two robots that cooperate with each other to push boxes to their destinations in a 3\u00d74 grid.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "The Stochastic Mars Rover problem [1] is", "startOffset": 34, "endOffset": 37}, {"referenceID": 19, "context": "The distributed sensor network (DSN) problem, adapted from [20], consists of two chains with identical number of sensors as shown in Figure 3.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "Most of the planning algorithms for general DEC-POMDPs have only been tested in domains with 2 agents [1, 8, 11, 17, 18].", "startOffset": 102, "endOffset": 120}, {"referenceID": 7, "context": "Most of the planning algorithms for general DEC-POMDPs have only been tested in domains with 2 agents [1, 8, 11, 17, 18].", "startOffset": 102, "endOffset": 120}, {"referenceID": 10, "context": "Most of the planning algorithms for general DEC-POMDPs have only been tested in domains with 2 agents [1, 8, 11, 17, 18].", "startOffset": 102, "endOffset": 120}, {"referenceID": 16, "context": "Most of the planning algorithms for general DEC-POMDPs have only been tested in domains with 2 agents [1, 8, 11, 17, 18].", "startOffset": 102, "endOffset": 120}, {"referenceID": 17, "context": "Most of the planning algorithms for general DEC-POMDPs have only been tested in domains with 2 agents [1, 8, 11, 17, 18].", "startOffset": 102, "endOffset": 120}, {"referenceID": 14, "context": "The distributed gradient descent (DGD) algorithm performs gradient-based policy search independently on each agent\u2019s local controller using the experience data [15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 19, "context": "[20] proposed an online natural actor-critic algorithm using conditional random fields (CRF).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Melo [13] proposed another actor-critic algorithm with natural gradient, but it only works for transition-independent DEC-POMDPs.", "startOffset": 5, "endOffset": 9}, {"referenceID": 8, "context": "The rollout sampling method has been introduced to learn MDP policies without explicitly representing the value function [9, 10, 12].", "startOffset": 121, "endOffset": 132}, {"referenceID": 9, "context": "The rollout sampling method has been introduced to learn MDP policies without explicitly representing the value function [9, 10, 12].", "startOffset": 121, "endOffset": 132}, {"referenceID": 11, "context": "The rollout sampling method has been introduced to learn MDP policies without explicitly representing the value function [9, 10, 12].", "startOffset": 121, "endOffset": 132}, {"referenceID": 3, "context": "The rollout technique is also widely used to perform lookahead and estimate the value of action in online methods [4, 5, 7].", "startOffset": 114, "endOffset": 123}, {"referenceID": 4, "context": "The rollout technique is also widely used to perform lookahead and estimate the value of action in online methods [4, 5, 7].", "startOffset": 114, "endOffset": 123}, {"referenceID": 6, "context": "The rollout technique is also widely used to perform lookahead and estimate the value of action in online methods [4, 5, 7].", "startOffset": 114, "endOffset": 123}], "year": 2010, "abstractText": "We present decentralized rollout sampling policy iteration (DecRSPI) \u2014 a new algorithm for multi-agent decision problems formalized as DEC-POMDPs. DecRSPI is designed to improve scalability and tackle problems that lack an explicit model. The algorithm uses MonteCarlo methods to generate a sample of reachable belief states. Then it computes a joint policy for each belief state based on the rollout estimations. A new policy representation allows us to represent solutions compactly. The key benefits of the algorithm are its linear time complexity over the number of agents, its bounded memory usage and good solution quality. It can solve larger problems that are intractable for existing planning algorithms. Experimental results confirm the effectiveness and scalability of the approach.", "creator": "TeX"}}}