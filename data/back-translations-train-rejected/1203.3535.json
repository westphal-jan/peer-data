{"id": "1203.3535", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Multi-Domain Collaborative Filtering", "abstract": "Collaborative filtering is an effective recommendation approach in which the preference of a user on an item is predicted based on the preferences of other users with similar interests. A big challenge in using collaborative filtering methods is the data sparsity problem which often arises because each user typically only rates very few items and hence the rating matrix is extremely sparse. In this paper, we address this problem by considering multiple collaborative filtering tasks in different domains simultaneously and exploiting the relationships between domains. We refer to it as a multi-domain collaborative filtering (MCF) problem. To solve the MCF problem, we propose a probabilistic framework which uses probabilistic matrix factorization to model the rating problem in each domain and allows the knowledge to be adaptively transferred across different domains by automatically learning the correlation between domains. We also introduce the link function for different domains to correct their biases. Experiments conducted on several real-world applications demonstrate the effectiveness of our methods when compared with some representative methods.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (299kb)", "http://arxiv.org/abs/1203.3535v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["yu zhang", "bin cao", "dit-yan yeung"], "accepted": false, "id": "1203.3535"}, "pdf": {"name": "1203.3535.pdf", "metadata": {"source": "CRF", "title": "Multi-Domain Collaborative Filtering", "authors": ["Yu Zhang", "Bin Cao", "Dit-Yan Yeung"], "emails": ["zhangyu@cse.ust.hk", "caobin@cse.ust.hk", "dyyeung@cse.ust.hk"], "sections": [{"heading": null, "text": "Collaborative filtering is an effective referral approach in which a user's preference for an item is predicted based on the preferences of other users with similar interests. A major challenge in using collaborative filtering methods is the problem of data sparseness, which often occurs because each user typically evaluates very few items and therefore the rating matrix is extremely sparse. In this paper, we address this problem by considering multiple collaborative filtering tasks in different domains at the same time and exploiting the relationships between domains. We speak of a collaborative multi-domain filtering problem (MCF). To solve the MCF problem, we propose a probabilistic framework that uses the factoring of the rating matrix to model the rating problem in each domain and transfer knowledge adaptably across different domains by automatically learning the correlation between domains. We also perform link functions to perform different domains to show the effects of their representative biases in some of our applications."}, {"heading": "1 Introduction", "text": "In this context, it should be noted that the solution to problems that have arisen in the past is not a solution, but a solution, which is a solution, which is primarily a solution."}, {"heading": "2 Multi-Domain Collaborative Filtering", "text": "Let Xi, mi \u00b7 ni set the rating matrix for the ith domain, where i = 1,. \u00b7, K. So for each domain we have users and ni items. Overall, we have users in all domains. Let's define the conditional distribution over the observed ratings on the ith domain as p (Xi, Vi, i) = m \u00b2, which represent the user-specific and item-specific latent characteristics vectors. Let's define the conditional distribution on the ith domain as p (Xi, Vi, i, i, i) = m \u00b2, M \u2212 rik = 1 (Xijk, 2i)] TVik, (Iijk, (1), where N (m, \u03a3) denotes the normal ratings on the ith domain as p (Xi, Vi, i)."}, {"heading": "2.1 Parameter Learning", "text": "The log posterior via {Ui} and {Vi} is presented as a problem ({Ui}, {Vi}, {Vi}, {Xi},, \u03a9) = \u2212 K [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], K, [K], K, [K], K, [K], K, [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K], [K, K], [K], [K], [K], [K, [K], [K], [K], [K], [K], K, [K,], K, [K], K, [K,], K, K, K, [,], K, [,], K, K, [,], K, [,], K, [,], [,], [,] K, [, K,], [,], [,], [, [,], [,], K, [,], [,], [, [,],], [, [,],], [, [,], [,], [,], [,], [,], [,], [,], [,], [,], [,], [,], [,"}, {"heading": "2.2 Discussions", "text": "In order to gain further insights into our method, we insert Eqs. (8), (10) and (11) into J ({Ui}, {Vi},,,,, \u03a9). By ignoring some constant terms, we can rephrase J ({Ui}, (Vi),,, \u03a9) asJ ({Ui}, {Vi},,,,, \u03a9) = K \u2211 i = 1 1 1 1 2 2 2 2i m \u00b2 j = 1 ln (m \u00b2 k = 1 (Uij) TUij) + K \u00b2 i = 1 dni 2 ln (ni \u00b2 k = 1 (Vik) TVik = 1 (md) K \u00b2 i = 1 ln (m \u00b2 j = 1 (Uij) TUij) + K \u00b2 i = 1 dni 2 ln (ni \u00b2 Ualize) the third (ni \u00b2 Ualize) and the fourth (Ualize)."}, {"heading": "3 Incorporation of Link Function", "text": "In the above models, the probability for the evaluation is in Eq = 1). However, since the evaluations are discrete integral values, the probability for the evaluation is not very high and therefore it can affect the performance of our model. Here, we are looking at a modification of our model that first transforms the original evaluations by a so-called link function and then applies the above model to the entire real evaluations. Otherwise, we will not present this modification after the transformation. The link function for the ith domain is denoted by gi (i), which is parameterized by i. We need a monotonic increase and mapping to the entire real line; otherwise, the probability measurement is not maintained. The transformed evaluations are defined by latent variables Zijk = i (X i), similar to those of Eq. (1), thelikelihood is defined on Zijk asp (Zi)."}, {"heading": "4 Related Work", "text": "In the case of the MCF, the method of collective matrix factorization requires a common latent user attribute matrix U, which is shared by all domains. However, in real-world applications where different domains have heterogeneous properties, this requirement does not make much sense. In this sense, our model can be considered a generalization of the collective matrix factorization method, where each domain has its own latent user attribute matrix and the correlation matrix between different domains is learned to improve the performance of all rating problems in all domains. In this sense, collective matrix factorization can be considered a special case of our model by restricting all Ui so that they are identical. In addition, [15] a transference-collaborative filter model has been proposed that aims to improve the performance of a rating problem with very sparse data, whereby the problem of another problem can be evaluated using another example, whereby all data can be evaluated simultaneously with 15."}, {"heading": "5 Experiments", "text": "In this section, we report on some experiments with two real-world datasets along with our analysis of the results."}, {"heading": "5.1 Experimental Settings", "text": "We test the proposed methods on two public domain recommendation records, in which the items come from different domains or subdomains."}, {"heading": "5.1.1 Datasets", "text": "We use two commonly used data sets in our experiments, including one of film ratings and one of book ratings. In both data sets, the articles can be divided into several heterogeneous ranges. \u2219 MovieLens2 is a widely used film recommendation data set. It contains 100,000 ratings on a scale of 1-5. Ratings are given by 943 users on 1682 movies. In addition to rating information, genre information about movies is also available. \u2219 Book-Crossing3 is a public book rating data set. A subset of data is used in our experiment, consisting of ratings of books with category information available on Amazon.com. The subset contains 56,148 ratings on a scale of 1-10 and these ratings are given by 28,503 users on 9,009 books. For the MovieLens data set, we use the five most popular genres to define the domains, while we use the five general book categories for the BookCrossing data set."}, {"heading": "5.1.2 Evaluation Metric", "text": "In this thesis, we use the Root Mean Square Error (RMSE) as a measure of performance evaluation: RMSE = \u221a \u2211 i, j (rij \u2212 r \u0109ij) 2N, (15) where rij denotes the ground truth rating of the user i for item j, r \u044bij the predicted rating and the denominator N the number of tested ratings. The lower the RMSE value, the better the performance."}, {"heading": "5.1.3 Baselines", "text": "We compare our proposed models with the following two methods: 2http: / / www.grouplens.org / 3http: / / www.informatik.uni-freiburg.de / \u0445 cziegler / BX / \u2219 Independent collaborative filtering using probabilistic matrix factorization (PMF), which deals independently with different rating prediction problems in different areas. \u2219 Model of collective matrix factorization (CMF) [24], which deals with problems with multiple matrix factorization tasks. In the following, we refer to our proposed method in section 2 as MCF and in section 3 as MCF-LF."}, {"heading": "5.2 Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Parameter Setting", "text": "The only parameter that needs to be set is the latent dimensionality of Figure 1, which shows the effect of the latent dimensionality on the performance of MCF for a subset of the MovieLens dataset. We can see that the performance in relation to RMSE does not change much after d has reached 10. Therefore, in the following experiments we set d to 10. Other parameters in PMF, CMF, MCF, MCF-LF are randomly generated."}, {"heading": "5.2.2 Results", "text": "Table 1 shows the experimental results of the MovieLens dataset. We can see that our proposed models perform best; the models that take into account multiple domains (CMF, MCF, MCF-LF) perform better than PMF, which treats different domains independently of each other. MCF, which can detect similarities between different rating prediction problems, performs better than CMF, which shows the effectiveness of using relationships between different domains. Comparing MCF and MCF-LF with their variant MCF-LF, which has the linking function, we can conclude that the linking function consistently improves performance across all domains. Table 2 shows the experimental results of the BookCrossing dataset. MCF and MCF-LF are also the best of all the compared methods. Unlike the situation in the MovieLens dataset, the domain of the MCF domain is worse than the MCF domain, although the MCF domain is worse than the MF domain with each other."}, {"heading": "5.2.3 Analysis on Correlation Matrix", "text": "Table 3 shows the correlation matrix between five areas learned from the MovieLens dataset, which seems to be consistent with intuition: for example, the genres \"comedy\" and \"thriller\" have the lowest correlation, while \"romance\" and \"drama\" have the highest correlation. For the genre \"action,\" we can see that the other genres are ranked in the order \"thriller,\" \"romance,\" \"drama\" and \"comedy,\" which is in line with our intuition. Table 4 shows the correlation matrix between five areas learned from the book-crossing dataset."}, {"heading": "6 Conclusion", "text": "We propose a probability model that takes into account the correlation between different areas when all assessment data is shared. Experiments carried out on several recommendation data sets show the effectiveness of our methods. Another way to alleviate the problem of data sparseness in CF is to apply active learning [3, 12]. Unlike many traditional machine learning methods, which passively wait for marked data to be provided to start the learning process, active learning takes a more active approach by selecting unmarked data points to consult an oracle or domain expert in order to reduce labelling costs. In our future work, we are interested in integrating active learning into our probability model in order to further increase learning performance."}, {"heading": "Acknowledgements", "text": "This research was supported by the General Research Fund 622209 of the Research Grants Council of Hong Kong."}], "references": [{"title": "Recommendation as classification: Using social and content-based information in recommendation", "author": ["C. Basu", "H. Hirsh", "W.W. Cohen"], "venue": "In Proceedings of the 15th National Conference on Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Learning collaborative information filters", "author": ["D. Billsus", "M.J. Pazzani"], "venue": "In Proceedings of the 15th International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Active collaborative filtering", "author": ["C. Boutilier", "R. Zemel", "B. Marlin"], "venue": "In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Empirical analysis of predictive algorithms for collaborative filtering", "author": ["J. Breese", "D. Heckerman", "C. Kadie"], "venue": "In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Collaborative prediction using ensembles of maximum margin matrix factorizations", "author": ["D. DeCoste"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "A rank minimization heuristic with application to minimum order system approximation", "author": ["M. Fazel", "H. Hindi", "S. Boyd"], "venue": "In Proceedings American Control Conference,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance matrices", "author": ["M. Fazel", "H. Hindi", "S. Boyd"], "venue": "In Proceedings American Control Conference,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Matrix variate distributions", "author": ["A.K. Gupta", "D.K. Nagar"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Dependency networks for collaborative filtering and data visualization", "author": ["D. Heckerman", "D. Chickering", "C. Meek", "R. Rounthwaite", "C. Kadie"], "venue": "In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "An algorithmic framework for performing collaborative filtering", "author": ["J.L. Herlocker", "J.A. Konstan", "A. Borchers", "J. Riedl"], "venue": "In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Preference-based graphic models for collaborative filtering", "author": ["R. Jin", "L. Si", "C. Zhai"], "venue": "In Proceedings of the 19th Conference in Uncertainty in Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "A bayesian approach toward active learning for collaborative filtering", "author": ["Rong Jin", "Luo Si"], "venue": "In Proceedings of the 20th Conference in Uncertainty in Artificial Intelligence,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Y. Koren"], "venue": "In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Non-linear matrix factorization with gaussian processes", "author": ["N.D. Lawrence", "R. Urtasun"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Transfer learning for collaborative filtering via a rating-matrix generative model", "author": ["B. Li", "Q. Yang", "X. Xue"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "A collaborative filtering algorithm and evaluation metric that accurately model the user experience", "author": ["M.R. McLaughlin", "J.L. Herlocker"], "venue": "In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Contentboosted collaborative filtering for improved recommendations", "author": ["P. Melville", "R.J. Mooney", "R. Nagarajan"], "venue": "In Proceedings of the 8th National Conference on Artificial intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Collaborative filtering by personality diagnosis: A hybrid memoryand model-based approach", "author": ["D. Pennock", "E. Horvitz", "S. Lawrence", "C. Giles"], "venue": "In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Probabilistic models for unified collaborative and content-based recommendation in sparse-data environments", "author": ["A. Popescul", "L. Ungar", "D. Pennock", "S. Lawrence"], "venue": "In Proceedings of the 17th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "author": ["J.D.M. Rennie", "N. Srebro"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2005}, {"title": "Probabilistic matrix factorization", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Itembased collaborative filtering recommendation algorithms", "author": ["B.M. Sarwar", "G. Karypis", "J.A. Konstan", "J. Riedl"], "venue": "In Proceedings of the 10th International World Wide Web Conference,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "Relational learning via collective matrix factorization", "author": ["A.P. Singh", "G.J. Gordon"], "venue": "In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "Maximummargin matrix factorization", "author": ["N. Srebro", "J.D.M. Rennie", "T.S. Jaakkola"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "A survey of collaborative filtering techniques", "author": ["X. Su", "T.M. Khoshgoftaar"], "venue": "Advances in Artificial Intelligence,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2009}, {"title": "Probabilistic principal component analysis", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Journal of the Royal Statistic Society, B,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1999}, {"title": "Ordinal boltzmann machines for collaborative filtering", "author": ["T. Truyen", "D. Phung", "S. Venkatesh"], "venue": "In Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Large-scale collaborative prediction using a nonparametric random effects model", "author": ["K. Yu", "J.D. Lafferty", "S. Zhu", "Y. Gong"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Collaborative ensemble learning: Combining collaborative and content-based information filtering via hierarchical bayes", "author": ["K. Yu", "A. Schwaighofer", "V. Tresp", "W.-Y. Ma", "H. Zhang"], "venue": "In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2003}, {"title": "Probabilistic memory-based collaborative filtering", "author": ["K. Yu", "A. Schwaighofer", "V. Tresp", "X. Xu", "H.-P. Kriegel"], "venue": "IEEE Transactions on Knowledge and Data Engeering,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Fast nonparametric matrix factorization for large-scale collaborative filtering", "author": ["K. Yu", "S. Zhu", "J.D. Lafferty", "Y. Gong"], "venue": "In Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}], "referenceMentions": [{"referenceID": 25, "context": "According to a survey on CF [26], different CF techniques can be classified into three categories: memorybased methods, model-based methods, and hybrid methods.", "startOffset": 28, "endOffset": 32}, {"referenceID": 9, "context": "Some representative methods are [10, 23, 16].", "startOffset": 32, "endOffset": 44}, {"referenceID": 22, "context": "Some representative methods are [10, 23, 16].", "startOffset": 32, "endOffset": 44}, {"referenceID": 15, "context": "Some representative methods are [10, 23, 16].", "startOffset": 32, "endOffset": 44}, {"referenceID": 3, "context": "Many learning models have been used for CF, such as Bayesian belief networks [4], graphical models [11, 28], and dependency networks [9].", "startOffset": 77, "endOffset": 80}, {"referenceID": 10, "context": "Many learning models have been used for CF, such as Bayesian belief networks [4], graphical models [11, 28], and dependency networks [9].", "startOffset": 99, "endOffset": 107}, {"referenceID": 27, "context": "Many learning models have been used for CF, such as Bayesian belief networks [4], graphical models [11, 28], and dependency networks [9].", "startOffset": 99, "endOffset": 107}, {"referenceID": 8, "context": "Many learning models have been used for CF, such as Bayesian belief networks [4], graphical models [11, 28], and dependency networks [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "Among all model-based CF methods, matrix factorization methods are perhaps the most popular one in recent years [2, 25, 20, 5, 21, 22, 14, 32, 29].", "startOffset": 112, "endOffset": 146}, {"referenceID": 24, "context": "Among all model-based CF methods, matrix factorization methods are perhaps the most popular one in recent years [2, 25, 20, 5, 21, 22, 14, 32, 29].", "startOffset": 112, "endOffset": 146}, {"referenceID": 19, "context": "Among all model-based CF methods, matrix factorization methods are perhaps the most popular one in recent years [2, 25, 20, 5, 21, 22, 14, 32, 29].", "startOffset": 112, "endOffset": 146}, {"referenceID": 4, "context": "Among all model-based CF methods, matrix factorization methods are perhaps the most popular one in recent years [2, 25, 20, 5, 21, 22, 14, 32, 29].", "startOffset": 112, "endOffset": 146}, {"referenceID": 20, "context": "Among all model-based CF methods, matrix factorization methods are perhaps the most popular one in recent years [2, 25, 20, 5, 21, 22, 14, 32, 29].", "startOffset": 112, "endOffset": 146}, {"referenceID": 21, "context": "Among all model-based CF methods, matrix factorization methods are perhaps the most popular one in recent years [2, 25, 20, 5, 21, 22, 14, 32, 29].", "startOffset": 112, "endOffset": 146}, {"referenceID": 13, "context": "Among all model-based CF methods, matrix factorization methods are perhaps the most popular one in recent years [2, 25, 20, 5, 21, 22, 14, 32, 29].", "startOffset": 112, "endOffset": 146}, {"referenceID": 31, "context": "Among all model-based CF methods, matrix factorization methods are perhaps the most popular one in recent years [2, 25, 20, 5, 21, 22, 14, 32, 29].", "startOffset": 112, "endOffset": 146}, {"referenceID": 28, "context": "Among all model-based CF methods, matrix factorization methods are perhaps the most popular one in recent years [2, 25, 20, 5, 21, 22, 14, 32, 29].", "startOffset": 112, "endOffset": 146}, {"referenceID": 0, "context": "Some examples are [1, 18, 19, 17, 30, 31, 13].", "startOffset": 18, "endOffset": 45}, {"referenceID": 17, "context": "Some examples are [1, 18, 19, 17, 30, 31, 13].", "startOffset": 18, "endOffset": 45}, {"referenceID": 18, "context": "Some examples are [1, 18, 19, 17, 30, 31, 13].", "startOffset": 18, "endOffset": 45}, {"referenceID": 16, "context": "Some examples are [1, 18, 19, 17, 30, 31, 13].", "startOffset": 18, "endOffset": 45}, {"referenceID": 29, "context": "Some examples are [1, 18, 19, 17, 30, 31, 13].", "startOffset": 18, "endOffset": 45}, {"referenceID": 30, "context": "Some examples are [1, 18, 19, 17, 30, 31, 13].", "startOffset": 18, "endOffset": 45}, {"referenceID": 12, "context": "Some examples are [1, 18, 19, 17, 30, 31, 13].", "startOffset": 18, "endOffset": 45}, {"referenceID": 25, "context": "the data sparsity problem [26] which means that the rating matrix is extremely sparse.", "startOffset": 26, "endOffset": 30}, {"referenceID": 20, "context": "Specifically, we propose a probabilistic framework which uses probabilistic matrix factorization (PMF) [21] to model the rating prediction problem in each domain and allows the knowledge to be adaptively transferred across different domains by automatically learning the correlation between domains.", "startOffset": 103, "endOffset": 107}, {"referenceID": 26, "context": "We place zero-mean spherical Gaussian priors [27] on the user features and item features as", "startOffset": 45, "endOffset": 49}, {"referenceID": 7, "context": "To learn the relationships between different domains, we place a matrix-variate normal distribution [8] on U = [vec(U), .", "startOffset": 100, "endOffset": 103}, {"referenceID": 24, "context": "which are related to the trace norms of U and V [25], the third and fourth terms in Eq.", "startOffset": 48, "endOffset": 52}, {"referenceID": 5, "context": "(12) penalize the ranks of U and V, respectively [6].", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "Moreover, according to [7], using the last term in Eq.", "startOffset": 23, "endOffset": 26}, {"referenceID": 23, "context": "The most related one is [24] which proposes a collective matrix factorization (CMF) method for CF.", "startOffset": 24, "endOffset": 28}, {"referenceID": 14, "context": "Moreover, a transfer collaborative filtering model was proposed in [15] which aims at improving the performance of a rating problem with very sparse data with the help of another rating problem which has denser rating data.", "startOffset": 67, "endOffset": 71}, {"referenceID": 14, "context": "However, the objective of [15] is different from ours.", "startOffset": 26, "endOffset": 30}, {"referenceID": 14, "context": "For example, the model in [15] is to improve one rating problem with the help of another rating problem, but in our case, we want to improve the performance of all rating problems in all domains simultaneously.", "startOffset": 26, "endOffset": 30}, {"referenceID": 14, "context": "Moreover, the model in [15] seems to work only for problems with two domains while our model can work for two or more domains in the same way.", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "\u2219 Collective matrix factorization (CMF) model [24], which handles problems involving multiple matrix factorization tasks.", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "Another way to alleviate the data sparsity problem in CF is to apply active learning [3, 12].", "startOffset": 85, "endOffset": 92}, {"referenceID": 11, "context": "Another way to alleviate the data sparsity problem in CF is to apply active learning [3, 12].", "startOffset": 85, "endOffset": 92}], "year": 2010, "abstractText": "Collaborative filtering is an effective recommendation approach in which the preference of a user on an item is predicted based on the preferences of other users with similar interests. A big challenge in using collaborative filtering methods is the data sparsity problem which often arises because each user typically only rates very few items and hence the rating matrix is extremely sparse. In this paper, we address this problem by considering multiple collaborative filtering tasks in different domains simultaneously and exploiting the relationships between domains. We refer to it as a multi-domain collaborative filtering (MCF) problem. To solve the MCF problem, we propose a probabilistic framework which uses probabilistic matrix factorization to model the rating problem in each domain and allows the knowledge to be adaptively transferred across different domains by automatically learning the correlation between domains. We also introduce the link function for different domains to correct their biases. Experiments conducted on several real-world applications demonstrate the effectiveness of our methods when compared with some representative methods.", "creator": "TeX"}}}