{"id": "1203.3538", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "RAPID: A Reachable Anytime Planner for Imprecisely-sensed Domains", "abstract": "Despite the intractability of generic optimal partially observable Markov decision process planning, there exist important problems that have highly structured models. Previous researchers have used this insight to construct more efficient algorithms for factored domains, and for domains with topological structure in the flat state dynamics model. In our work, motivated by findings from the education community relevant to automated tutoring, we consider problems that exhibit a form of topological structure in the factored dynamics model. Our Reachable Anytime Planner for Imprecisely-sensed Domains (RAPID) leverages this structure to efficiently compute a good initial envelope of reachable states under the optimal MDP policy in time linear in the number of state variables. RAPID performs partially-observable planning over the limited envelope of states, and slowly expands the state space considered as time allows. RAPID performs well on a large tutoring-inspired problem simulation with 122 state variables, corresponding to a flat state space of over 10^30 states.", "histories": [["v1", "Thu, 15 Mar 2012 11:25:52 GMT  (297kb)", "http://arxiv.org/abs/1203.3538v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["emma brunskill", "stuart russell"], "accepted": false, "id": "1203.3538"}, "pdf": {"name": "1203.3538.pdf", "metadata": {"source": "CRF", "title": "RAPID: A Reachable Anytime Planner for Imprecisely-sensed Domains", "authors": ["Emma Brunskill"], "emails": [], "sections": [{"heading": null, "text": "Despite the intractability of the generic optimal, partially observable Markov decision-making process planning, important problems exist with highly structured models. Previous researchers have used this insight to construct more efficient algorithms for fact domains and for domains with topological structure in the flat state dynamic model. In our work, motivated by insights from the educational community relevant to automated tutoring, we look at problems that have some form of topological structure in the fact dynamics model. Our Reachable Anytime Planner for Imprectly-sensed Domains (RAPID) uses this structure to efficiently calculate a good initial envelope of accessible states within the framework of optimal MDP policy in a time-linear manner in the number of state variables. RAPID performs partially observable planning over the limited envelope of states and slowly expands the considered state space. RAPID works well on a large tutoring-inspired problem space corresponding to a 1230 variable state space of 1030."}, {"heading": "1 INTRODUCTION", "text": "One of the most important questions in artificial intelligence research is how to make good decisions in large, stochastic, partially observable environments. Although generic optimal end-horizon planning is to be expected for partially observable Markov processes (POMDPs), it is only possible to capture different types of structures, luckily some of them have highly structured models. This insight has been used by previous researchers to develop more efficient POMDP algorithms that use different types of structures. Focusing on domains that have factored structures has led to POMDP planners solving some of the biggest POMDP problems in literature, including a hand-washing selector program (2005) and a RoboCup rescue task (Paquet et al)."}, {"heading": "2 RELATEDWORK", "text": "There has been significant progress recently in planning in partially observable stochastic areas. Two of the fastest generic POMDP planners are HSVI by Smith and Simmons (2005) and SARSOP by Kurniawati, Hsu and Lee (2008). None of the approaches uses Factored Structure (2005) and Symbolic HSVI (Sim et al., 2008) use Factored Structure (such as Boutlier, Dearden and Goldsmidt (2000). Symbolic Perseus (Boger et al.) and Symbolic HSVI (Sim et al., 2008) are two offline POMDP algorithms for factored state spaces that address major issues. In practice, both are quite similar."}, {"heading": "3 PROBLEM DESCRIPTION", "text": "We are interested in making decisions in POFUP that are partially observable, stochastic environments that have previously been described as single operators. (This can be done by the tuple < S, L, A, Z, b0, E, P, S, A),.. p (z, si), r (s, a), sG, sT >, where there is a set of states. The domain consists of L >. \u2022 A is a set of action variables s1, s2, s2,., and each state is an assignment of values (true or false) to all domain variables: s = < s1, s2, s2,., sL >. Each action aij is associated with a particular state si and has the potential to be variable trusts."}, {"heading": "4 ALGORITHM", "text": "This is often the case even when the attainable state space for certain initial states of belief is much smaller than the complete state space. Instead, we draw our inspiration from hull-based planning algorithms for large, fully observable MDPs and extend these ideas to our POFUPP domains. Dean et al. (1995) presented the idea of calculating a policy for fully observable, flat MDPs by planning only over a smaller state scope. Over time, the state scope was expanded to include more of the attainable state space. Algorithm 1 RAPID: REACHABLE ANYTIME PLANNING FOR IMPRECISELY-SENSED DOMAINS1: Sample an initial state from the initial belief 2: Construct an initial envelope using a deterministicMDP relaxation that can be solentable."}, {"heading": "4.1 INITIAL ENVELOPE CONSTRUCTION", "text": "Faced with a POFUPP process M, we must first construct an initial shell of states. Ideally, the shell would include states that have a reasonable probability of being visited by a good policy for the partially observable domain. States that are visited along the optimal MDP solution seem to be intuitively reasonable, since the MDP solution forms an upper limit for POMDP performance."}, {"heading": "4.2 ENVELOPE POMDP POLICY GENERATION", "text": "RAPID is derived from the definition of a POMDP P: \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \"\" \",\" \"\" \",\" \"\" \",\" \"\", \"\" \"\", \"\" \"\", \"\" \",\" \"\", \"\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \"\" \",\", \"\" \"\" \",\" \",\" \"\" \"\" \"\" \",\" \"\" \",\" \"\" \"\" \"\", \"\" \"\" \"\" \"\", \"\" \"\" \",\" \"\", \"\" \"\" \"\" \"\" \",\" \"\" \"\", \"\" \"\" \",\" \"\" \"\", \",\" \"\", \",\" \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\", \"\", \"\", \"\" \"\" \"\", \"\" \"\" \",\" \"\", \"\" \"\" \",\" \"\" \"\" \",\" \",\" \"\" \"\", \"\", \"\", \"\" \"\" \"\", \"\", \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \",\" \"\", \"\", \"\" \",\" \"\" \"\" \"\" \"\", \"\" \"\", \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\", \"\" \"\""}, {"heading": "4.3 ENVELOPE EXTENSION", "text": "There are numerous potential strategies for hull expansion, and in this initial work we used a simple but empirically effective approach. We look at three possible methods to identify a new state to expand the hull space; in other words, we try the first method and see if it identifies a new state to be added, if it does, we stop, otherwise we run the second method, etc. The first method tries a potential initial state s0i, which has a non-zero probability in the initial belief state b0, but is not yet part of the hull of the states. If all potential initial states are in the hull, the second method tries to find a new non-hull state by expanding the hull rings. This expansion is carried out by starting from a possible initial state and simulating a path."}, {"heading": "4.4 PERFORMANCE AND COMPUTATIONAL COMPLEXITY", "text": "First, for the sake of completeness, it should be noted that RAPID is guaranteed to converge to an \"optimal\" policy as long as an \"optimal\" POMDP planner is used, since RAPID guarantees that the envelope ultimately includes all states that are reachable from the original state of belief. Calculation of a state path from an initial state to the target state and the associated value estimations require a linear time in terms of the number of variables. The initial envelope will have at most O (L) states, which means that the initial POMDP planning is executed via a state space that is a linear function of the number of variables. The maximum number of states in the envelope is the achievable state space, which is typically much smaller than the potential 2L state space. The complexity of solving a POMDP depends on the respective technique. HSVI performs a \"depth first roll out\" and updates an explicit representation of an upper and lower range of both the POMDP value function and each one of the lower range of the POMDP value functions is actualized by updating a \"depth first roll out.\""}, {"heading": "4.5 UPPER BOUNDS FOR POFUPP PROBLEMS", "text": "We will soon prove that the path of states between a start and the destination state across the full space of juxtaposition of individual countries in terms of the actual cost of the POFUPP process in terms of this property efficiently calculates the fully observable optimal MDP value of states within the envelope, which can then be used to calculate an upper limit for the initial state of belief. Such limits can be useful for at least two reasons. Firstly, many POMDP solvers (including HSVI and SARSOP) use upper limits during planning. Typically, these limits are calculated by solving the MDP values known to be the POMDP values. However, solving the flat MDP values usually requires multiple backup operations, each requiring multiple backup operations in the number of states. Secondly, upper limits provide useful benchmarks for evaluating RAPID performance."}, {"heading": "5 EXPERIMENTS", "text": "Due to our interest in tutorial applications, we conducted simulation experiments in two areas inspired by the tutorial."}, {"heading": "5.1 DOMAINS", "text": "In both cases, the basic variable condition of graph construction was informed by literature from the educational communities: the transition probabilities, observation probabilities, and reward values were hand-selected. The first domain, SmallMath, consisted of 19 elementary mathematical skills that enabled a potential state space of 219 million 500,000 states. Graph prerequisite for the skills is shown in Figure 2. There are two possible observations and the prerequisites for these skills are fulfilled; the first act for a skill, a \"didactic\" action, has a high probability that the skills will lead to the transition to truth (p = 0.8), if it is not already present and the prerequisites for these skills are fulfilled; however, it does not provide feedback on whether the student has successfully acquired the skill. In our experiments, we set the probability of each observation is 0.5 for actions 1.3.5.,., 37. The second action for each skill (actions 2.4., 38) is loosely equivalent to an exercise with only 0.5 probability."}, {"heading": "5.2 SOLUTION PARAMETERS", "text": "The maximum horizon for SmallMath is set at -1000 for SmallMath and -100 for BigMath. Since there is typically a certain probability that the state will move into an out-of-envelope state, and both problems may require a long margin of maneuver to achieve the goal, the state reward was chosen loosely to prevent the transition to an out-of-envelope state without punishing the transition so severely that the calculated policy conservatively avoids adding any more skills. We did not optimize performance by varying this parameter, and other values could lead to further performance benefits. HSVI ends when a time limit is reached or a minimum gap between the upper and lower limits of the original belief is reached."}, {"heading": "5.3 EVALUATION METRICS", "text": "After each envelope expansion, we empirically evaluated the reward for the envelope policy over several episodes of the maximum horizon length of the respective problem. For SmallMath, we evaluated the empirical reward for 20 episodes after each expansion, and for BigMath, we evaluated the empirical reward for 5 episodes after each expansion: BigMath is much more computationally intensive due to the larger state space and longer problem horizon."}, {"heading": "5.4 BASELINES", "text": "Even the smaller of the two problems, SmallMath, still requires more than 500,000 states to list the exhaustive combinations of state variable combinations, limiting potential alternative algorithms for comparison against each other. SARSOP (Kurniawati et al., 2008) is a factored stateof the art generic POMDP solver that factored input files.Symbolic Perseus (Boger et al., 2005) is a factored statespace POMDP solver. Symbolic Perseus was used to calculate a good approximate solution for a factored hanging assistance problem with 13 variables, and over 50 states.In some cases, the achievable state space may be quite small, and so we first listed the achievable space, and then, using HSVI, implemented a POMDP policy on attainable states.Anderson implemented a simple, highly probable, heuristic Fixed Threshold, settings, settings, settings, settings, settings, settings, settability, getting old, settings, settings, settings, settings, settings."}, {"heading": "5.5 RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.5.1 SmallMath", "text": "We show the performance of RAPID on SmallMath in Figure 4a. RAPID was generally able to find a good solution quickly, and its consistency increased as computing time increased as expected. We represented SmallMath in SARSOP POMDPX format, but noted that initializing the SARSOP problem consistently attempted to exceed our limit of available memory (2 gigabytes). We believe this is because the current implementation still uses non-factorial dynamic representation and would require a full representation of SmallMath 219 x 219 x 38 centers. Symbolic Perseus requires specifying the number of tested beliefs that can be used for planning. If we specify a small number of beliefs (N = 20), the algorithm calculated a solution in 5150 s, but the resulting policy could never find a track to the goal."}, {"heading": "5.5.2 BigMath", "text": "Figure 4b & c shows the average performance after each envelope expansion for different runs compared to the cumulative runtime and after each envelope expansion, or after each envelope expansion. Based on our experience with SARSOP and Symbolic Perseus on SmallMath, we did not examine their use on BigMath, which is a much bigger problem. In BigMath, we compared FTNF to the performance of RAPID after 4 envelope extensions. Although FTNF is very fast, it generally performs much worse than RAPID over a wide bandwidth of thresholds (from 0.8 to 0.9999).FTNF with the performance of RAPID after 4 envelope extensions."}, {"heading": "6 CONCLUSION & FUTUREWORK", "text": "There are a number of important stochastic, partially observable problems that have a large amount of structure that can be used for efficient planning. In this paper, we focus on problems that have a form of topological structure in factored state space: domains that have such a structure include student tutoring, dialogue, and potentially assembly tasks. Our RAPID algorithm uses this structure to calculate an initial state envelope based on optimal MDP policies in a time-linear number of variables. RAPID then performs a standard POMDP planning over this limited envelope before expanding the envelope and resolving it at any time. Our experimental results show that RAPID can quickly produce a good policy for an extremely large factored problem where the problem structure is constructed using prior precondition diagrams from the educational community."}, {"heading": "Acknowledgements", "text": "The authors thank Sarah Finney, Jason Wolfe, Luke Zettlemoyer and the anonymous reviewers for their helpful comments. E.Brunskill was supported by an NSF Mathematical Sciences Postdoctoral Fellowship."}], "references": [{"title": "A decision-theoretic approach to task assistance for persons with dementia", "author": ["J. Boger", "P. Poupart", "J. Hoey", "C. Boutilier", "G. Fernie", "A. Mihailidis"], "venue": null, "citeRegEx": "Boger et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Boger et al\\.", "year": 2005}, {"title": "Stochastic dynamic programming with factored representations", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "Artificial Intelligence Journal", "citeRegEx": "Boutilier et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 2000}, {"title": "Structure and complexity in planning with unary operators", "author": ["R. Brafman", "C. Domshlak"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Brafman and Domshlak,? \\Q2003\\E", "shortCiteRegEx": "Brafman and Domshlak", "year": 2003}, {"title": "An analysis of the relationships among computation-related skills using a hierarchicalclustering technique", "author": ["J. Close", "F. Murtagh"], "venue": "Journal for Research in Mathematics Education,", "citeRegEx": "Close and Murtagh,? \\Q1986\\E", "shortCiteRegEx": "Close and Murtagh", "year": 1986}, {"title": "Knowledge tracing: Modeling the acquisition of procedural knowledge", "author": ["A. Corbett", "J. Anderson"], "venue": "User Modeling and User-Adapted Interaction,", "citeRegEx": "Corbett and Anderson,? \\Q1995\\E", "shortCiteRegEx": "Corbett and Anderson", "year": 1995}, {"title": "Introduction to algorithms", "author": ["T. Cormen", "C. Leiserson", "R. Rivest"], "venue": null, "citeRegEx": "Cormen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Cormen et al\\.", "year": 1999}, {"title": "Topological value iteration algorithm for Markov decision processes", "author": ["P. Dai", "J. Goldsmith"], "venue": null, "citeRegEx": "Dai and Goldsmith,? \\Q2007\\E", "shortCiteRegEx": "Dai and Goldsmith", "year": 2007}, {"title": "Planning under time constraints in stochastic domains", "author": ["T. Dean", "L.P. Kaelbling", "J. Kirman", "A. Nicholson"], "venue": "Artificial Intelligence,", "citeRegEx": "Dean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1995}, {"title": "Topological order planner for POMDPs", "author": ["J. Dibangoye", "G. Shani", "B. Chaib-draa", "A. Mouaddib"], "venue": null, "citeRegEx": "Dibangoye et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dibangoye et al\\.", "year": 2009}, {"title": "Principles of instructional design", "author": ["R. Gagn\u00e9", "L. Briggs"], "venue": null, "citeRegEx": "Gagn\u00e9 and Briggs,? \\Q1974\\E", "shortCiteRegEx": "Gagn\u00e9 and Briggs", "year": 1974}, {"title": "Envelope-based planning in relational MDPs", "author": ["N.H. Gardiol", "L.P. Kaelbling"], "venue": null, "citeRegEx": "Gardiol and Kaelbling,? \\Q2004\\E", "shortCiteRegEx": "Gardiol and Kaelbling", "year": 2004}, {"title": "Intelligent tutoring goes to school in the big city", "author": ["K.R. Koedinger", "J. Anderson", "W. Hadley", "M. Mark"], "venue": "International Journal of Artificial Intelligence in Education,", "citeRegEx": "Koedinger et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Koedinger et al\\.", "year": 1997}, {"title": "SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces", "author": ["H. Kurniawati", "D. Hsu", "W.S"], "venue": null, "citeRegEx": "Kurniawati et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kurniawati et al\\.", "year": 2008}, {"title": "Developed of a learning hierarchy for the computational skills of fractional number subtraction", "author": ["P. Miller", "E.R. Phillips"], "venue": "American Educational Research Association Meeting", "citeRegEx": "Miller and Phillips,? \\Q1974\\E", "shortCiteRegEx": "Miller and Phillips", "year": 1974}, {"title": "The complexity of Markov decision processes", "author": ["C. Papadimitriou", "J. Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "An online POMDP algorithm for complex multiagent environments. AAMAS", "author": ["S. Paquet", "L. Tobin", "B. Chaib-draa"], "venue": null, "citeRegEx": "Paquet et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Paquet et al\\.", "year": 2005}, {"title": "Symbolic heuristic search value iteration for factored pomdps", "author": ["H. Sim", "K. Kim", "J. Kim", "D. Chang", "M. Koo"], "venue": null, "citeRegEx": "Sim et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sim et al\\.", "year": 2008}, {"title": "Point-based POMDP algorithms: Improved analysis and implementation. UAI", "author": ["T. Smith", "R. Simmons"], "venue": null, "citeRegEx": "Smith and Simmons,? \\Q2005\\E", "shortCiteRegEx": "Smith and Simmons", "year": 2005}, {"title": "An intraconcept analysis of rational number addition: A validation study", "author": ["A.E. Uprichard", "E. Phillips"], "venue": "Journal for Research in Mathematics Education,", "citeRegEx": "Uprichard and Phillips,? \\Q1977\\E", "shortCiteRegEx": "Uprichard and Phillips", "year": 1977}], "referenceMentions": [{"referenceID": 0, "context": "Focussing on domains that exhibit factored structure has led to POMDP planners that solve some of the largest POMDP problems in the literature, including a hand washing assistance program (Boger et al., 2005) and a RoboCup rescue task (Paquet et al.", "startOffset": 188, "endOffset": 208}, {"referenceID": 15, "context": ", 2005) and a RoboCup rescue task (Paquet et al., 2005).", "startOffset": 34, "endOffset": 55}, {"referenceID": 8, "context": "Other recent work (Dai & Goldsmith, 2007; Dibangoye et al., 2009) has focused on domains where the flat state dynamics model limits the possible backtracking to earlier states, and showed that planning can be performed more efficiently when this topological structure is present.", "startOffset": 18, "endOffset": 65}, {"referenceID": 3, "context": "For example, some prior education studies coarsely approximate a student\u2019s knowledge as a factored set of binary variables, one for each skill, and infers a precondition graph structure among skills (known as a \u201clearning hierarchy\u201d) from student data: see for example Gagne\u00e9\u2019s and Briggs (1974) and Close and Murtagh (1986). Despite this structure, automated tutor action selection remains challenging as the factored state space may consist of hundreds of skills.", "startOffset": 299, "endOffset": 324}, {"referenceID": 17, "context": "Two of the fastest generic POMDP planners are HSVI by Smith and Simmons (2005) and SARSOP by Kurniawati, Hsu and Lee (2008).", "startOffset": 54, "endOffset": 79}, {"referenceID": 17, "context": "Two of the fastest generic POMDP planners are HSVI by Smith and Simmons (2005) and SARSOP by Kurniawati, Hsu and Lee (2008). Neither approach takes advantage of factored structure.", "startOffset": 54, "endOffset": 124}, {"referenceID": 0, "context": "Symbolic Perseus (Boger et al., 2005) and Symbolic HSVI (Sim et al.", "startOffset": 17, "endOffset": 37}, {"referenceID": 16, "context": ", 2005) and Symbolic HSVI (Sim et al., 2008) are two offline POMDP algorithms for factored state spaces which scale to large problems.", "startOffset": 26, "endOffset": 44}, {"referenceID": 0, "context": "Symbolic Perseus (Boger et al., 2005) and Symbolic HSVI (Sim et al., 2008) are two offline POMDP algorithms for factored state spaces which scale to large problems. In practice both perform fairly similarly to each other. Online, forward search POMDP planners can also leverage factored structure, and Paquet, Tobin and Chaib-draa (2005) used forward search to handle an extremely large, factored RoboCup rescue problem.", "startOffset": 18, "endOffset": 338}, {"referenceID": 6, "context": "Dai and Goldsmith (2007) leveraged the presence of layered positiveeffect state structure (certain clusters of states cannot be returned to) in their Topological Value Iteration (TVI) MDP algorithm.", "startOffset": 0, "endOffset": 25}, {"referenceID": 6, "context": "Dai and Goldsmith (2007) leveraged the presence of layered positiveeffect state structure (certain clusters of states cannot be returned to) in their Topological Value Iteration (TVI) MDP algorithm. Dibangoye et al. (2009) assumed a similar structure, and used this to create a heuristic Topological Order Planner (TOP) for POMDPs.", "startOffset": 0, "endOffset": 223}, {"referenceID": 7, "context": "To scale to very large, fully observable MDPs, Dean et al. (1995) proposed an anytime approach which initially restricts MDP planning to a smaller envelope of reachable states.", "startOffset": 47, "endOffset": 66}, {"referenceID": 7, "context": "To scale to very large, fully observable MDPs, Dean et al. (1995) proposed an anytime approach which initially restricts MDP planning to a smaller envelope of reachable states. Gardiol and Kaelbling (2004) extended this approach to be applicable in relational MDPs using action-based equivalence.", "startOffset": 47, "endOffset": 206}, {"referenceID": 7, "context": "Dean et al. (1995) presented the idea of computing a policy for fully observable, flat MDPs by planning only over a smaller envelope of states.", "startOffset": 0, "endOffset": 19}, {"referenceID": 5, "context": "As the precondition graph E is a directed acyclic graph (DAG), the topological order can be computed in time linear in the number of state variables and precondition conditions (Cormen et al., 1999).", "startOffset": 177, "endOffset": 198}, {"referenceID": 7, "context": "The definition of an out state follows prior work in the fully observable envelope literature (Dean et al., 1995).", "startOffset": 94, "endOffset": 113}, {"referenceID": 12, "context": "The fraction precondition graph was derived from Miller and Phillips (1974) and Uprichard and Phillips (1977).", "startOffset": 49, "endOffset": 76}, {"referenceID": 12, "context": "The fraction precondition graph was derived from Miller and Phillips (1974) and Uprichard and Phillips (1977). We combined the fraction precondition structure with the subtraction hierarchy from Gagn\u00e9 (1974), and the addition, subtraction, multiplication and division hierarchies from Close and Murtagh (1986).", "startOffset": 49, "endOffset": 110}, {"referenceID": 12, "context": "The fraction precondition graph was derived from Miller and Phillips (1974) and Uprichard and Phillips (1977). We combined the fraction precondition structure with the subtraction hierarchy from Gagn\u00e9 (1974), and the addition, subtraction, multiplication and division hierarchies from Close and Murtagh (1986).", "startOffset": 49, "endOffset": 208}, {"referenceID": 3, "context": "We combined the fraction precondition structure with the subtraction hierarchy from Gagn\u00e9 (1974), and the addition, subtraction, multiplication and division hierarchies from Close and Murtagh (1986). The full precondition graph is displayed in Figure 3 and consisted of 122 skills.", "startOffset": 174, "endOffset": 199}, {"referenceID": 12, "context": "This structure was derived by combining the Miller and Phillips (1974) and Uprichard and Phillips (1977) learning hierarchies for fractions, with Gagne\u00e9 and Briggs\u2019 (1974) subtraction hierarchy and Close and Murtagh\u2019s (1986) addition, subtraction, multiplication and division hierarchy.", "startOffset": 44, "endOffset": 71}, {"referenceID": 12, "context": "This structure was derived by combining the Miller and Phillips (1974) and Uprichard and Phillips (1977) learning hierarchies for fractions, with Gagne\u00e9 and Briggs\u2019 (1974) subtraction hierarchy and Close and Murtagh\u2019s (1986) addition, subtraction, multiplication and division hierarchy.", "startOffset": 44, "endOffset": 105}, {"referenceID": 12, "context": "This structure was derived by combining the Miller and Phillips (1974) and Uprichard and Phillips (1977) learning hierarchies for fractions, with Gagne\u00e9 and Briggs\u2019 (1974) subtraction hierarchy and Close and Murtagh\u2019s (1986) addition, subtraction, multiplication and division hierarchy.", "startOffset": 44, "endOffset": 172}, {"referenceID": 3, "context": "This structure was derived by combining the Miller and Phillips (1974) and Uprichard and Phillips (1977) learning hierarchies for fractions, with Gagne\u00e9 and Briggs\u2019 (1974) subtraction hierarchy and Close and Murtagh\u2019s (1986) addition, subtraction, multiplication and division hierarchy.", "startOffset": 198, "endOffset": 225}, {"referenceID": 12, "context": "SARSOP (Kurniawati et al., 2008) is a non-factored stateof-the-art generic POMDP solver which accepts factored input files.", "startOffset": 7, "endOffset": 32}, {"referenceID": 0, "context": "Symbolic Perseus (Boger et al., 2005) is a factored-statespace POMDP solver.", "startOffset": 17, "endOffset": 37}, {"referenceID": 11, "context": "We also implemented a simple, very fast, heuristic Fixed Threshold, No-Forgetting (FTNF) policy similar to policies used in prior intelligent tutoring systems (Corbett & Anderson, 1995; Koedinger et al., 1997).", "startOffset": 159, "endOffset": 209}], "year": 2010, "abstractText": "Despite the intractability of generic optimal partially observable Markov decision process planning, there exist important problems that have highly structured models. Previous researchers have used this insight to construct more efficient algorithms for factored domains, and for domains with topological structure in the flat state dynamics model. In our work, motivated by findings from the education community relevant to automated tutoring, we consider problems that exhibit a form of topological structure in the factored dynamics model. Our Reachable Anytime Planner for Imprecisely-sensed Domains (RAPID) leverages this structure to efficiently compute a good initial envelope of reachable states under the optimal MDP policy in time linear in the number of state variables. RAPID performs partially-observable planning over the limited envelope of states, and slowly expands the state space considered as time allows. RAPID performs well on a large tutoring-inspired problem simulation with 122 state variables, corresponding to a flat state space of over 10 states.", "creator": null}}}