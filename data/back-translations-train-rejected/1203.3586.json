{"id": "1203.3586", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Automated Text Summarization Base on Lexicales Chain and graph Using of WordNet and Wikipedia Knowledge Base", "abstract": "The technology of automatic document summarization is maturing and may provide a solution to the information overload problem. Nowadays, document summarization plays an important role in information retrieval. With a large volume of documents, presenting the user with a summary of each document greatly facilitates the task of finding the desired documents. Document summarization is a process of automatically creating a compressed version of a given document that provides useful information to users, and multi-document summarization is to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic. The lexical cohesion structure of the text can be exploited to determine the importance of a sentence/phrase. Lexical chains are useful tools to analyze the lexical cohesion structure in a text .In this paper we consider the effect of the use of lexical cohesion features in Summarization, And presenting a algorithm base on the knowledge base. Ours algorithm at first find the correct sense of any word, Then constructs the lexical chains, remove Lexical chains that less score than other, detects topics roughly from lexical chains, segments the text with respect to the topics and selects the most important sentences. The experimental results on an open benchmark datasets from DUC01 and DUC02 show that our proposed approach can improve the performance compared to sate-of-the-art summarization approaches.", "histories": [["v1", "Thu, 15 Mar 2012 22:56:29 GMT  (898kb)", "http://arxiv.org/abs/1203.3586v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["mohsen pourvali", "mohammad saniee abadeh"], "accepted": false, "id": "1203.3586"}, "pdf": {"name": "1203.3586.pdf", "metadata": {"source": "CRF", "title": "Automated Text Summarization Base on Lexicales Chain and graph Using of WordNet and Wikipedia Knowledge Base", "authors": ["Mohsen Pourvali", "Mohammad Saniee Abadeh"], "emails": [], "sections": [{"heading": "1. Introduction", "text": "This year, it is only a matter of time before a solution is found, until an agreement is reached."}, {"heading": "2. Related work", "text": "This year, it has come to the point where there is only one person who is able to take care of another person who is able to take care of another person."}, {"heading": "3. Word Sense Disambiguation", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "1 \u2208 1 2 \u2208 2 \u00a0 (1)", "text": "Then we find overlaps between glossy terms 1, 2 | 1 2 | (2) And all two concepts that have more similarities are selected as target words. Furthermore, we not only use overlaps in the unique (sequence of a word), but also overlaps in the bigram (sequence of two words). If there is a sense, the first word overlaps in the glossy concept of the second word, we give these two senses a special value. We do this because two concepts may have common words that are not related to their similarities, and it leads to an increase in the values of these two senses and makes an error in the selection of related words as a result. If we look at the meaning of the glossy concept of the second word, we can give this sense an additional chance to be selected in the process of selecting words for chains of words that are not semantically related."}, {"heading": "4. Clustring Lexical Chains", "text": "For each lexical chain LCi, a sentence occurrence vector Vi is formed.,...,,..., where n is the copyright (c) 2012 International Journal of Computer Science Issues. All Rights Reserved.number of sentence in the document. Each is the number of LCi members in sentence k. If sentence k has 3 members of LCi, then it is 3. Two lexical chains LCi and LCj go into the same cluster if their sentence occurrence vectors Vi and Vj are similar. Our cluster algorithm assumes an initial cluster distribution where each lexical chain is in its own cluster. Thus, our cluster algorithm begins with n clusters, where n is the number of lexical chains. Iteratively, the most similar cluster pair is found and merged into a single cluster, with each lexical chain standing in its own cluster."}, {"heading": "5. Sequence Extraction", "text": "In our algorithm, the text is segmented from the perspective of each lexical chain cluster, finding the hotspots for each topic. For each cluster, contiguous sequences of sentences are extracted as segments. Sentences that are connected usually speak about the same topic. For each lexical chain cluster Clj, we form sequences separately. For each sentence Sk, if the sentence Sk has a lexical chain link in Clj, a new sequence is started or the sentence is added to the sequence. If there is no cluster element in Sk, the sequence is terminated. By this procedure, the text is segmented relative to a cluster, identifying topic concentration points.Figure 5 is an example of text segmentation. Each sequence is evaluated using the formula in Equation (5)."}, {"heading": "6. Experiments and Results", "text": "In this section, we conduct experiments to test our summary empirically."}, {"heading": "7. Conclusion", "text": "Our algorithm relies on WordNet, which is theoretically domain-independent, and we have also used Wikipedia for some of the words that do not exist in WordNet. Our algorithm aimed to use more cohesion clues than other lexical chain-based summation algorithms. Our results were competitive with other summation algorithms and achieved good results. By having lexical chain links present at the same time, our algorithm tries to link technical terms and object terms in the text. With implicit segmentation, we tried to take advantage of lexical chains for text segmentation. It may be possible to use our algorithm as a text segmentator."}], "references": [{"title": "Summarization of text-based documents with a determination of latent topical sections and information-rich sentences", "author": ["R.M. Alguliev", "R.M. Alyguliev"], "venue": "Automatic Control and Computer Sciences ,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "QCS: A system for querying, clustering and summarizing documents", "author": ["D.M. Dunlavy", "D.P. O\u2019Leary", "J.M. Conroy", "J.D. Schlesinger"], "venue": "Information Processing and Management ,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Lexrank: Graph-based lexical centrality as salience in text summarization", "author": ["G. Erkan", "D.R. Radev"], "venue": "Journal of Artificial Intelligence Research ,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Automatic summarizing: The state of the art", "author": ["K.S. Jones"], "venue": "Information Processing and Management ,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "ROUGE: A package for automatic evaluation summaries", "author": ["Lin", "C. -Y"], "venue": "In Proceedings of the workshop on text summarization branches out,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Automatic evaluation of summaries using N-gram co-occurrence statistics. In Proceedings of the 2003 conference of the north american chapter of the association for computational linguistics on human language technology", "author": ["Lin", "C. -Y", "E.H. Hovy"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Explorations in automatic book summarization. In Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL 2007), (pp. 380\u2013389)", "author": ["R. Mihalcea", "H. Ceylan"], "venue": "Prague, Czech Republic.  IJCSI International Journal of Computer Science Issues,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "An Experimental Study of Graph Connectivity for Unsupervised Word Sense Disambiguation", "author": ["R. Navigli", "M. Lapata"], "venue": "IEEE Computer", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "An algorithm for suffix stripping", "author": ["M. Porter"], "venue": "Program ,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1980}, {"title": "Introduction to the special issue on summarization", "author": ["D. Radev", "E. Hovy", "K. McKeown"], "venue": "omputational Linguistics ,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Automatic text structuring and summarization", "author": ["G. Salton", "A. Singhal", "M. Mitra", "C. Buckley"], "venue": "Information Processing and Management ,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Document summarization using onditional random fields", "author": ["D. Shen", "Sun", "J. -T", "H. Li", "Q. Yang", "Z. Chen"], "venue": "In Proceedings of the 20th international joint conference on artificial intelligence (JCAI", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Enhancing single-document summarization by combining RankNet and third-party sources. In Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL", "author": ["K.M. Svore", "L. Vanderwende", "C.J.C. Burges"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Using only cross-document relationships for both generic and topic-focused multi-document summarizations", "author": ["X. Wan"], "venue": "Information Retrieval ,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Manifold-ranking based topic-focused multidocument summarization", "author": ["X. Wan", "J. Yang", "J. Xiao"], "venue": "In Proceedings of the 20th international joint conference on artificial intelligence (IJCAI", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Text summarization using a trainable summarizer and latent semantic analysis", "author": ["Yeh", "J-Y", "Ke", "H-R", "Yang", "W-P", "Meng", "I-H"], "venue": "Information Processing and Management ,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Summary in context: Searching versus browsing", "author": ["D.M. McDonald", "H. Chen"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "One story, one flow: Hidden Markov story models for multilingual multi document summarization", "author": ["P. Fung", "G. Ngai"], "venue": "ACM Transaction on Speech and Language Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "The Google similarity measure", "author": ["R.L. Cilibrasi", "P.M.B. Vitanyi"], "venue": "IEEE Transaction on Knowledge and Data Engineering,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}], "referenceMentions": [{"referenceID": 13, "context": "Text summarization is the process of automatically creating a compressed version of a given text that provides useful information to users, and multi-document summarization is to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic [14].", "startOffset": 309, "endOffset": 313}, {"referenceID": 9, "context": "Authors of the paper [10] provide the following definition for a summary: \u201cA summary can be loosely defined as a text that is produced from one or more texts that conveys important information in the original text(s), and that is no longer than half of the original text(s) and usually significantly less than that.", "startOffset": 21, "endOffset": 25}, {"referenceID": 3, "context": "The paper [4] reviews research on automatic summarizing over the last decade.", "startOffset": 10, "endOffset": 13}, {"referenceID": 10, "context": "In paper [11] proposed IJCSI International Journal of Computer Science Issues, Vol.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "edu/nlp/newsblaster/) usually needs information fusion, sentence compression and reformulation [14].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "Most commonly, such ranking approaches use some kind of similarity or centrality metric to rank sentences for inclusion in the summary \u2013 see, for example, [1].", "startOffset": 155, "endOffset": 158}, {"referenceID": 2, "context": "The centroid-based method [3] is one of the most popular extractive summarization methods.", "startOffset": 26, "endOffset": 29}, {"referenceID": 11, "context": "In paper [12] each document is considered as a sequence of sentences and the objective of extractive summarization is to label the sentences in the sequence with 1 and 0, where a label of 1 indicates that a sentence is a summary sentence while 0 denotes a non-summary sentence.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "In paper [15] proposed a novel extractive approach based on manifold\u2013ranking of sentences to query-based multi-document summarization.", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "After representing each sentence by a vector of features, the classification function can be trained in two different manners [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 15, "context": "One is in a discriminative way with well-known algorithms such as support vector machine (SVM) [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 2, "context": "Many unsupervised methods have been developed for document summarization by exploiting different features and relationships of the sentences \u2013 see, for example [3] and the references therein.", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "A query-based summary presents the information that is most relevant to the given queries [2] and [14] while a generic summary gives an overall sense of the document\u2019s content [2] , [4] , [12] , [14].", "startOffset": 90, "endOffset": 93}, {"referenceID": 13, "context": "A query-based summary presents the information that is most relevant to the given queries [2] and [14] while a generic summary gives an overall sense of the document\u2019s content [2] , [4] , [12] , [14].", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "A query-based summary presents the information that is most relevant to the given queries [2] and [14] while a generic summary gives an overall sense of the document\u2019s content [2] , [4] , [12] , [14].", "startOffset": 176, "endOffset": 179}, {"referenceID": 3, "context": "A query-based summary presents the information that is most relevant to the given queries [2] and [14] while a generic summary gives an overall sense of the document\u2019s content [2] , [4] , [12] , [14].", "startOffset": 182, "endOffset": 185}, {"referenceID": 11, "context": "A query-based summary presents the information that is most relevant to the given queries [2] and [14] while a generic summary gives an overall sense of the document\u2019s content [2] , [4] , [12] , [14].", "startOffset": 188, "endOffset": 192}, {"referenceID": 13, "context": "A query-based summary presents the information that is most relevant to the given queries [2] and [14] while a generic summary gives an overall sense of the document\u2019s content [2] , [4] , [12] , [14].", "startOffset": 195, "endOffset": 199}, {"referenceID": 1, "context": "The QCS system (Query, Cluster, and Summarize) [2] performs the following tasks in response to a query: retrieves relevant documents; separates the retrieved documents into clusters by topic, and creates a summary for each cluster.", "startOffset": 47, "endOffset": 50}, {"referenceID": 16, "context": "In paper [17] are developed a generic, a query-based, and a hybrid summarizer, each with IJCSI International Journal of Computer Science Issues, Vol.", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "The article [18] presents a multi-document, multi-lingual, theme-based summarization system based on modeling text cohesion (story flow).", "startOffset": 12, "endOffset": 16}, {"referenceID": 0, "context": "index == sp[1] tnew=Create New Nod(sp[2]) Call Hyp(ref tnew,level-1) Add_New_Nod_ToList(tnew)", "startOffset": 11, "endOffset": 14}, {"referenceID": 1, "context": "index == sp[1] tnew=Create New Nod(sp[2]) Call Hyp(ref tnew,level-1) Add_New_Nod_ToList(tnew)", "startOffset": 37, "endOffset": 40}, {"referenceID": 8, "context": "stop and the terms were stemmed using Porter\u2019s scheme [9], which is a commonly used algorithm for word stemming in English.", "startOffset": 54, "endOffset": 57}, {"referenceID": 11, "context": "This approach compares the candidate summary (denoted by Summcand) with the reference summary and computes the P, R and F1-measure values as shown in formula (8) [12].", "startOffset": 162, "endOffset": 166}, {"referenceID": 4, "context": "The second measure we use the ROUGE toolkit [5] , [6] for evaluation, which was adopted by DUC for automatically summarization evaluation.", "startOffset": 44, "endOffset": 47}, {"referenceID": 5, "context": "The second measure we use the ROUGE toolkit [5] , [6] for evaluation, which was adopted by DUC for automatically summarization evaluation.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "The measure is defined by formula (9) [5] , [6].", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "The measure is defined by formula (9) [5] , [6].", "startOffset": 44, "endOffset": 47}, {"referenceID": 11, "context": "We compared our method with four methods CRF [12], NetSum [13], Manifold\u2013Ranking [15] and SVM [16].", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "We compared our method with four methods CRF [12], NetSum [13], Manifold\u2013Ranking [15] and SVM [16].", "startOffset": 58, "endOffset": 62}, {"referenceID": 14, "context": "We compared our method with four methods CRF [12], NetSum [13], Manifold\u2013Ranking [15] and SVM [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "We compared our method with four methods CRF [12], NetSum [13], Manifold\u2013Ranking [15] and SVM [16].", "startOffset": 94, "endOffset": 98}], "year": 2012, "abstractText": "The technology of automatic document summarization is maturing and may provide a solution to the information overload problem. Nowadays, document summarization plays an important role in information retrieval. With a large volume of documents, presenting the user with a summary of each document greatly facilitates the task of finding the desired documents. Document summarization is a process of automatically creating a compressed version of a given document that provides useful information to users, and multi-document summarization is to produce a summary delivering the majority of information content from a set of documents about an explicit or implicit main topic. The lexical cohesion structure of the text can be exploited to determine the importance of a sentence/phrase. Lexical chains are useful tools to analyze the lexical cohesion structure in a text .In this paper we consider the effect of the use of lexical cohesion features in Summarization, And presenting a algorithm base on the knowledge base. Our algorithm at first find the correct sense of any word, Then constructs the lexical chains, remove Lexical chains that less score than other ,detects topics roughly from lexical chains, segments the text with respect to the topics and selects the most important sentences. The experimental results on an open benchmark datasets from DUC01 and DUC02 show that our proposed approach can improve the performance compared to sate-of-the-art summarization approaches.", "creator": "PScript5.dll Version 5.2.2"}}}