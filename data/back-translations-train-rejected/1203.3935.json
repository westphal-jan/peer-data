{"id": "1203.3935", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2012", "title": "Distributed Cooperative Q-learning for Power Allocation in Cognitive Femtocell Networks", "abstract": "In this paper, we propose a distributed reinforcement learning (RL) technique called distributed power control using Q-learning (DPC-Q) to manage the interference caused by the femtocells on macro-users in the downlink. The DPC-Q leverages Q-Learning to identify the sub-optimal pattern of power allocation, which strives to maximize femtocell capacity, while guaranteeing macrocell capacity level in an underlay cognitive setting. We propose two different approaches for the DPC-Q algorithm: namely, independent, and cooperative. In the former, femtocells learn independently from each other while in the latter, femtocells share some information during learning in order to enhance their performance. Simulation results show that the independent approach is capable of mitigating the interference generated by the femtocells on macro-users. Moreover, the results show that cooperation enhances the performance of the femtocells in terms of speed of convergence, fairness and aggregate femtocell capacity.", "histories": [["v1", "Sun, 18 Mar 2012 10:30:54 GMT  (168kb)", "http://arxiv.org/abs/1203.3935v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["hussein saad", "amr mohamed", "tamer elbatt"], "accepted": false, "id": "1203.3935"}, "pdf": {"name": "1203.3935.pdf", "metadata": {"source": "CRF", "title": "Distributed Cooperative Q-learning for Power Allocation in Cognitive Femtocell Networks", "authors": ["Hussein Saad", "Amr Mohamed", "Tamer ElBatt"], "emails": ["hussein.saad@nileu.edu.eg", "amrmg@qu.edu.qa", "telbatt@nileuniversity.edu.eg"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "A. Simulation Scenario", "text": "We consider a wireless network consisting of a macro cell supported by Nfemto femtocells. In the simulations, Nfemto extends between 4 and 15 femtocells. Each femtocell serves Uf = 1 femtocell user who happens to be in the femtocell coverage area. Both macro and femtocells share the same frequency band consisting of Nsub = 6 subcarriers where orthogonal downlink transmission is assumed. In the simulation, the channel gain between transmitter i and receiver j on subcarrier n is assumed to be loss-dominated and is specified by: h (n) ij = d (\u2212 k) ij (15), dij being the physical distance between transmitter i and receiver j and k the loss exponent. In the simulation, k = 2 is used. The distances are calculated according to the following assumptions: \u2022 The maximum distance between the MBS and the associated user is set to 1000 meters \u2022 IS the maximum distance between 800 \u2022 FBS and 800 \u2022 800 \u2022 FBS distance."}, {"heading": "B. Numerical Results", "text": "We refer to the reward functions defined by equations (12), (13) and (14) to achieve better cell resolution (F1 = 1), RF2 and RF3 respectively in all simulations. Figure (1) shows the convergence of macrocellular capacity on a specific subcarrier (C (n) o) using RF1 and RF2 with K = 80, K = 1000 and K = 10000. In addition, the figure shows that the value of K affects convergence, where K = 80 is better than K = 1000 than K = 10000, proving that the difference between positive and negative rewards is decreasing. Note that in the simulations the number of Qiterations 3000 was increased, whereas in the figure only 300 iterations are shown."}], "references": [{"title": "Femtocell networks: a survey,", "author": ["V. Chandrasekhar", "J. Andrews", "A. Gatherer"], "venue": "Communications Magazine, IEEE,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "S", "author": ["A.G.S. Saunders"], "venue": "Carlaw et al., Femtocells: Opportunities and Challenges for Business and Technology. Great Britain: John Wiley and Sons Ltd", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "and J", "author": ["P. Xia", "V. Chandrasekhar"], "venue": "G. Andrews, \u201cOpen vs closed access femtocells in the uplink,\u201d CoRR, vol. abs/1002.2964", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Cambridge MA, MIT press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Technical note Q-learning,", "author": ["C.J.C.H. Watkins", "P. Dayan"], "venue": "Journal of Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "Collaborative multiagent reinforcement learning by payoff propagation,", "author": ["J.R. Kok", "N. Vlassis"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Expertness based cooperative Qlearning,", "author": ["M. Ahmadabadi", "M. Asadpour"], "venue": "Systems, Man, and Cybernetics, Part B: IEEE Transactions on Cybernetics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Distributed Q-learning for aggregated interference control in cognitive radio networks,", "author": ["A. Galindo-Serrano", "L. Giupponi"], "venue": "Vehicular Technology, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Decentralized Q-learning for aggregated interference control in completely and partially observable cognitive radio networks,", "author": ["A. Galindo", "L. Giupponi"], "venue": "in proceedings of the Consumer Communications and Networking Conference (CCNC),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Distributed Q-learning for interference control in OFDMA-based femtocell networks,", "author": ["A. Galindo-Serrano", "L. Giupponi"], "venue": "Vehicular Technology Conference (VTC 2010-Spring),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Cognition and docition in OFDMA-based femtocell networks,\u201d in proceeding of GLOBE- COM", "author": ["A. Galindo-Serrano", "L. Giupponi", "M. Dohler"], "venue": "IEEE Global Telecommunications Conference,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Cooperative multi-agent learning: The state of the art,", "author": ["L. Panait", "S. Luke"], "venue": "Autonomous Agents and Multi-Agent Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Coordination and learning in cooperative multiagent systems,", "author": ["J.R. Kok"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "and W", "author": ["R. Jain", "D.-M. Chiu"], "venue": "Hawe, \u201cA quantitative measure of fairness and discrimination for resource allocation in shared computer systems,\u201d CoRR", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "Thus, their interference on macro-users and other femtocells is considered to be a daunting problem [1], [2].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "Thus, their interference on macro-users and other femtocells is considered to be a daunting problem [1], [2].", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "Based on these observations, in this paper we focus on closed access femtocells [3] working in the same bandwidth with macrocells (cognitive femtocells).", "startOffset": 80, "endOffset": 83}, {"referenceID": 3, "context": "We will use a distributed machine learning technique called reinforcement learning (RL) [4] to handle the interference problem generated by the femtocells on the macrocells\u2019 users.", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "One of the most popular RL techniques is Q-learning [5].", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "These features make Qlearning very suitable to be applied to the distributed femtocell setting in the form of the so called multi-agent Q-learning (MAQL) [6].", "startOffset": 154, "endOffset": 157}, {"referenceID": 5, "context": "The former assumes that agents are unaware of the other agents\u2019 actions while the latter allows the agents to share some knowledge while they are learning to enhance their performance [6], [7].", "startOffset": 184, "endOffset": 187}, {"referenceID": 6, "context": "The former assumes that agents are unaware of the other agents\u2019 actions while the latter allows the agents to share some knowledge while they are learning to enhance their performance [6], [7].", "startOffset": 189, "endOffset": 192}, {"referenceID": 7, "context": "In [8], [9], authors used IL Q-learning to perform power allocation in order to control the aggregate interference generated by multiple secondary users on the primary receiver of a digital TV (DTV) system.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [8], [9], authors used IL Q-learning to perform power allocation in order to control the aggregate interference generated by multiple secondary users on the primary receiver of a digital TV (DTV) system.", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "In [10], authors addressed the same goal of interference control but in the context of OFDMA-based femtocells.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "In [11], authors used IL Q-learning in the context of cognitive femtocells and introduced a new concept called docitive femtocells.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "A new reward function is introduced and compared to the reward function used in literature [10], [11].", "startOffset": 91, "endOffset": 95}, {"referenceID": 10, "context": "A new reward function is introduced and compared to the reward function used in literature [10], [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 4, "context": "In this section, the idea of Q-learning is presented by introducing the single agent case [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 9, "context": "Furthermore, equation (1) can be expressed as [10]:", "startOffset": 46, "endOffset": 50}, {"referenceID": 4, "context": "It was proved in [5], [12] that this update rule converges to the optimal Q-value under certain conditions.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "One of these conditions is that each stateaction pair must be visited infinitely often [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 11, "context": "Although, this may lead to oscillations and convergence problems, the IL paradigm showed good results in many applications [13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "1\u2264j\u2264N Qj(sj , aj) [14].", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "Based on this observation, in the next section we compare our reward function to the reward function used in [10]:", "startOffset": 109, "endOffset": 113}, {"referenceID": 9, "context": "Note that the authors in [10] defined the state for discrete power levels and this proves our point.", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "1 during the first 80% of the Q-iterations [8], [9] and [10].", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "1 during the first 80% of the Q-iterations [8], [9] and [10].", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "1 during the first 80% of the Q-iterations [8], [9] and [10].", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "fairness index [15]: f(x1, x2, \u00b7 \u00b7 \u00b7 , xn) = ( \u2211n i=1 xi) 2 n \u2211 n i=1 x 2 i where", "startOffset": 15, "endOffset": 19}], "year": 2012, "abstractText": "In this paper, we propose a distributed reinforcement learning (RL) technique called distributed power control using Q-learning (DPC-Q) to manage the interference caused by the femtocells on macro-users in the downlink. The DPC-Q leverages Q-Learning to identify the sub-optimal pattern of power allocation, which strives to maximize femtocell capacity, while guaranteeing macrocell capacity level in an underlay cognitive setting. We propose two different approaches for the DPC-Q algorithm: namely, independent, and cooperative. In the former, femtocells learn independently from each other while in the latter, femtocells share some information during learning in order to enhance their performance. Simulation results show that the independent approach is capable of mitigating the interference generated by the femtocells on macro-users. Moreover, the results show that cooperation enhances the performance of the femtocells in terms of speed of convergence, fairness and aggregate femtocell capacity.", "creator": "LaTeX with hyperref package"}}}