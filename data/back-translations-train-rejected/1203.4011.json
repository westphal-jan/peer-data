{"id": "1203.4011", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Mar-2012", "title": "Understanding Sampling Style Adversarial Search Methods", "abstract": "UCT has recently emerged as an exciting new adversarial reasoning technique based on cleverly balancing exploration and exploitation in a Monte-Carlo sampling setting. It has been particularly successful in the game of Go but the reasons for its success are not well understood and attempts to replicate its success in other domains such as Chess have failed. We provide an in-depth analysis of the potential of UCT in domain-independent settings, in cases where heuristic values are available, and the effect of enhancing random playouts to more informed playouts between two weak minimax players. To provide further insights, we develop synthetic game tree instances and discuss interesting properties of UCT, both empirically and analytically.", "histories": [["v1", "Thu, 15 Mar 2012 11:17:56 GMT  (636kb)", "http://arxiv.org/abs/1203.4011v1", "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["raghuram ramanujan", "ashish sabharwal", "bart selman"], "accepted": false, "id": "1203.4011"}, "pdf": {"name": "1203.4011.pdf", "metadata": {"source": "META", "title": "Understanding Sampling Style Adversarial Search Methods", "authors": ["Raghuram Ramanujan"], "emails": ["raghu@cs.cornell.edu", "sabhar@cs.cornell.edu", "selman@cs.cornell.edu"], "sections": [{"heading": null, "text": "UCT has recently proven to be an exciting new, hostile reasoning technology based on a smart balance between exploration and exploitation in a Monte-Carlo sampling environment. It has been particularly successful in the game of Go, but the reasons for its success are not well understood and attempts to replicate its success in other areas such as chess have failed. We offer an in-depth analysis of the potential of UCT in domain-independent environments, in cases where heuristic values are available, and of the effect of improving random playouts to more informed playouts between two weak Minimax players. To provide further insights, we develop examples of synthetic gaming trees and discuss interesting properties of UCT, both empirically and analytically."}, {"heading": "1 INTRODUCTION", "text": "This year it is more than ever before."}, {"heading": "2 BACKGROUND", "text": "In the past, the experts \"measures have been successfully applied to produce a game of incomplete information such as Bridge (Ginsberg, 1999) and Scrabble (Sheppard, 2002), but they have rarely outperformed traditional hostile planning techniques such as the Minimax algorithm in deterministic 2-player game constellations such as Chess. Recently, this has changed with the emergence of UCT, which was used to produce the first program capable of playing at the master level in 9x9 Go (Gelly and Silver, 2007, 2008), a domain that has so far proved challenging for Minimax, presumably due to a large branching factor and a lack of good heuristics. UCT has proven to be promising in new domains beyond the scope of any traditional planning techniques (Ciancarini and Favini, 2009), and in general gaming games (Finnsson and Bjo Burnsson, 2008)."}, {"heading": "3 DOMAIN-INDEPENDENT SETTINGS", "text": "We begin by exploring the extent to which UCT-like search methods can compete with Minimax searches in a completely domain-independent environment, for example, by considering quantified Boolean formulas (QBF), where all we have as input is a formula without information about the semantics of the variables or the specifics of the problem domain that the formula contains, as well as in the general game situation (Finnsson and Bjo B\u00f6rnsson, 2008).For our empirical research into the behavior of UCT and Minimax, we use the setting of chess, but modify Minimax to avoid the use of chess-specific heuristic information that pretends the domain is unknown."}, {"heading": "4 BOOSTING UCT WITH", "text": "HEURISTIC INFORMATION We are now looking at the attitude in which we deal primarily with the knowledge of domination in the world. We are interested in the extent to which this can be exploited to improve UCT. Heuristics have already produced promising results for Go. Typically, the heuristic value is used to initialize the value of the nodes to distort the selection process in the early iterations of the search. However, since the current heuristics in Go are not very strong, UCT will override the heuristic value with playout values fairly quickly once the node has been visited sufficiently (typically a few dozen times)."}, {"heading": "5 ENHANCING RANDOM PLAYOUTS", "text": "We focus our attention on one of the two key aspects of the UCT, random leads, and wonder whether such leads can provide useful information in areas such as chess, where we already have well-designed state evaluations. An interesting question in the context of leads is whether it is even possible to obtain useful information about a strong player by playing several plays between two weak players? We find that random leads tend to provide no more information than chess heuristics themselves, but a slightly more powerful lead between two MM-2 players - can surprisingly reveal information that is often only visible to a much deeper Minimax opponent, such as MM-8 players. We quantify this in terms of a strong correlation between the two leads. Such information, which is only relatively deep and systematically visible, can take the form of traps, as we have recently investigated (Ramujan et al has relatively low strategy, relatively low state relative to relatively low state strategy, relatively low state relative to relatively low state has a strategy)."}, {"heading": "6 INSIGHTS INTO UCT: SYNTHETIC SEARCH SPACES", "text": "To better understand its behavior, we look at synthetic opposing search rooms in which we vary key characteristics in a controlled manner that affect UCT's performance. We examine game trees with implanted winning strategies for the maximum player (referred to as Max) located at the root node. Winning strategies are parameterized by the number of critical decision nodes and their depths. If Max makes the right choice of action at each critical node, then, regardless of the actions chosen by each player at all other nodes, the path at the end of the game will be + 1. If Max chooses a wrong action at one of the critical nodes, the path at the end of the game will be drawn uniformly from {\u2212 1, 0, + 1}. This simple model captures the idea of winning plans that exist in many tactical games like Chess."}, {"heading": "6.1 EMPIRICAL OBSERVATIONS", "text": "Figure 3 illustrates how the time it takes UCT to \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "6.2 ANALYTICAL INSIGHTS", "text": "While a few attempts have been made to analyze bandit-based sampling methods in general and UCT in particular (e.g., Auer et al., 2002; Gelly and Silver, 2007; Coquelin and Munos, 2007), these analyses are based on the worst-case scenario and essentially boil down to the fact that an exponential (or even super-exponential) number of iterations are necessary and sufficient for UCT to converge to true Minimax values. These exponential time convergence results, while complicated and interesting, do not explain UCT's success in practice in areas like Go, with a virtually limited number of iterations available during the game. In contrast, our goal in this section is to provide a methodology for analyzing some of the simple scenarios in which UCT works and gain insight into its runtime behavior."}, {"heading": "6.2.1 Scenario A", "text": "For the sake of simplicity, let's start with the simplest case and build on it. Let's look at a binary game tree T with Max in play at the top node. Let's have TL and TR refer to the left and right sub-trees respectively as strategies of T. Let's assume that all the hands of TL are roughly referred to as + 1, i.e. Max has a certain victory if he makes the left move. Let's assume that a p fraction of the leaves where p-value [0, 1) is referred to by TR as + 1 and the rest as \u2212 1. This tree is shown in Figure 5 (a), with bold edges corresponding to the winning strategy. How long does it take for UCT to identify the left branch as a winning move. In a given round at the root node of T, a playout from the left child always leads to + 1, while a playout from the right child leads to + 1 with probability p."}, {"heading": "6.2.2 Scenario B", "text": "This example will also show that the depth of the critical nodes of a winning strategy exponentially influences the number of iterations necessary for convergence, in stark contrast to the K-step winning strategies in single player environments, where it is easy to argue that the depth of critical selection points is immaterial, and that all that matters is the number of critical selection points. Intuitively, the difference between the single player and two player settings is that in the previous case all decisions look equally good (or bad) at uncritical points, the player can arbitrarily \"freeze\" and further exploit it, while the opponent prevents this freezing by constantly forcing the winning player to avoid different areas of the search space in the hope of defeat. For example, for a depth d winning strategy, the loser can force the other player to explore exactly 2d / 2 ways."}, {"heading": "7 CONCLUSION", "text": "We studied UCT in areas such as chess, where traditional Minimax search is very effective, and our results show that UCT consistently beats Minimax in domain-independent environments, that it can be greatly enhanced by incorporating a state assessment function, and that more informed playouts can increase performance. Finally, our results on synthetic instances with implanted strategies showed an interesting pattern in UCT's convergence behavior."}, {"heading": "Acknowledgments", "text": "Supported by NSF (Expeditions in Computing award for Computational Sustainability, 0832782; IIS grant 0514429) and IISI, Cornell Univ. (AFOSR grant FA9550-04-1-0151)."}, {"heading": "P. Ciancarini and G. P. Favini. Monte Carlo tree search", "text": "Techniques in war games. In IJCAI-09, 2009.P.-A. Coquelin and R. Munos. Bandit algorithms for tree hunting. CoRR, abs / cs / 0703062, 2007."}, {"heading": "R. Coulom. Efficient selection and backup operators in", "text": "Monte Carlo Tree Search. In the 5th Intl. Conf. on Computer and Games, Volume 4360 of the LNCS, pp. 72-83, Turin, Italy, May 2006."}, {"heading": "H. Finnsson and Y. Bjo\u0308rnsson. Simulation-based approach", "text": "In AAAI-08, pp. 259-264. AAAI Press, 2008. ISBN 978-1-57735-368-3.S. Gelly and D. Silver. Combining online and offline knowledge in UCT. In 24th ICML, pp. 273-280, Corvallis, OR, June 2007. S. Gelly and D. Silver. Acielving master level play in 9 \u00d7 9 computer Go. In 23rd AAAI, pp. 1537-1540, Chicago, IL, July 2008.M. L. Ginsberg. GIB: Steps towards an expert-level bridgeplaying program. In IJCAI-99, pp. 584-589, 1999."}, {"heading": "L. Kocsis and C. Szepesva\u0301ri. Bandit based Monte-Carlo", "text": "Planning. In 17th ECML, Volume 4212 of the LNCS, pp. 282-293, Berlin, Germany, September 2006."}, {"heading": "D. S. Nau. Pathology on game trees revisited, and an", "text": "Alternative to Minimaxing. Artif. Intell., 21 (1-2): 221- 244, 1983."}, {"heading": "J. Pearl. On the nature of pathology in game searching.", "text": "Artif. Intell., 20 (4): 427-453, 1983."}, {"heading": "R. Ramanujan, A. Sabharwal, and B. Selman. On adversarial search spaces and sampling-based planning. In", "text": "20. ICAPS, pp. 242-245, Toronto, Canada, May 2010.B. Sheppard. World Championship Caliber Scrabble. Artif. Intell., 134 (1-2): 241-275, 2002."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "An adaptive sampling algorithm for solving Markov decision processes", "author": ["H.S. Chang", "M.C. Fu", "J. Hu", "S.I. Marcus"], "venue": "Operations Research,", "citeRegEx": "Chang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2005}, {"title": "Monte Carlo tree search techniques in the game of Kriegspiel", "author": ["P. Ciancarini", "G.P. Favini"], "venue": "In IJCAI-09,", "citeRegEx": "Ciancarini and Favini.,? \\Q2009\\E", "shortCiteRegEx": "Ciancarini and Favini.", "year": 2009}, {"title": "Bandit algorithms for tree search", "author": ["P.-A. Coquelin", "R. Munos"], "venue": "CoRR, abs/cs/0703062,", "citeRegEx": "Coquelin and Munos.,? \\Q2007\\E", "shortCiteRegEx": "Coquelin and Munos.", "year": 2007}, {"title": "Efficient selection and backup operators in Monte-Carlo tree search", "author": ["R. Coulom"], "venue": "In 5th Intl. Conf. on Computer and Games, vol. 4360 of LNCS,", "citeRegEx": "Coulom.,? \\Q2006\\E", "shortCiteRegEx": "Coulom.", "year": 2006}, {"title": "Simulation-based approach to general game playing", "author": ["H. Finnsson", "Y. Bj\u00f6rnsson"], "venue": "In AAAI-08,", "citeRegEx": "Finnsson and Bj\u00f6rnsson.,? \\Q2008\\E", "shortCiteRegEx": "Finnsson and Bj\u00f6rnsson.", "year": 2008}, {"title": "Combining online and offline knowledge in UCT", "author": ["S. Gelly", "D. Silver"], "venue": "In 24th ICML,", "citeRegEx": "Gelly and Silver.,? \\Q2007\\E", "shortCiteRegEx": "Gelly and Silver.", "year": 2007}, {"title": "GIB: Steps toward an expert-level bridgeplaying program", "author": ["M.L. Ginsberg"], "venue": "In IJCAI-99,", "citeRegEx": "Ginsberg.,? \\Q1999\\E", "shortCiteRegEx": "Ginsberg.", "year": 1999}, {"title": "Bandit based Monte-Carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "In 17th ECML, vol. 4212 of LNCS,", "citeRegEx": "Kocsis and Szepesv\u00e1ri.,? \\Q2006\\E", "shortCiteRegEx": "Kocsis and Szepesv\u00e1ri.", "year": 2006}, {"title": "Pathology on game trees revisited, and an alternative to minimaxing", "author": ["D.S. Nau"], "venue": "Artif. Intell.,", "citeRegEx": "Nau.,? \\Q1983\\E", "shortCiteRegEx": "Nau.", "year": 1983}, {"title": "On the nature of pathology in game searching", "author": ["J. Pearl"], "venue": "Artif. Intell.,", "citeRegEx": "Pearl.,? \\Q1983\\E", "shortCiteRegEx": "Pearl.", "year": 1983}, {"title": "On adversarial search spaces and sampling-based planning", "author": ["R. Ramanujan", "A. Sabharwal", "B. Selman"], "venue": "In 20th ICAPS,", "citeRegEx": "Ramanujan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ramanujan et al\\.", "year": 2010}, {"title": "World-championship-caliber Scrabble", "author": ["B. Sheppard"], "venue": "Artif. Intell.,", "citeRegEx": "Sheppard.,? \\Q2002\\E", "shortCiteRegEx": "Sheppard.", "year": 2002}], "referenceMentions": [{"referenceID": 8, "context": "The UCT algorithm (Kocsis and Szepesv\u00e1ri, 2006) is derived from a highly effective approach to solving the multi-armed bandit problem called UCB1 (Auer et al.", "startOffset": 18, "endOffset": 47}, {"referenceID": 0, "context": "The UCT algorithm (Kocsis and Szepesv\u00e1ri, 2006) is derived from a highly effective approach to solving the multi-armed bandit problem called UCB1 (Auer et al., 2002).", "startOffset": 146, "endOffset": 165}, {"referenceID": 6, "context": "We focus, in contrast to existing analysis (e.g., Auer et al., 2002; Gelly and Silver, 2007; Coquelin and Munos, 2007), on simple cases such as binary trees with implanted winning strategies of low complexity (i.", "startOffset": 43, "endOffset": 118}, {"referenceID": 3, "context": "We focus, in contrast to existing analysis (e.g., Auer et al., 2002; Gelly and Silver, 2007; Coquelin and Munos, 2007), on simple cases such as binary trees with implanted winning strategies of low complexity (i.", "startOffset": 43, "endOffset": 118}, {"referenceID": 0, "context": "We also allude to differences between single agent search as in UCB1 (Auer et al., 2002), which has been the motivation for the multi-agent UCT algorithm, and multiagent scenarios.", "startOffset": 69, "endOffset": 88}, {"referenceID": 7, "context": "Monte Carlo sampling techniques have been successfully applied in the past to produce expert-level play in games of incomplete information such as Bridge (Ginsberg, 1999) and Scrabble (Sheppard, 2002).", "startOffset": 154, "endOffset": 170}, {"referenceID": 12, "context": "Monte Carlo sampling techniques have been successfully applied in the past to produce expert-level play in games of incomplete information such as Bridge (Ginsberg, 1999) and Scrabble (Sheppard, 2002).", "startOffset": 184, "endOffset": 200}, {"referenceID": 2, "context": "UCT has also proved promising in new domains such as Kriegspiel that were beyond the scope of any traditional planning techniques (Ciancarini and Favini, 2009), and in general game playing (Finnsson and Bj\u00f6rnsson, 2008).", "startOffset": 130, "endOffset": 159}, {"referenceID": 5, "context": "UCT has also proved promising in new domains such as Kriegspiel that were beyond the scope of any traditional planning techniques (Ciancarini and Favini, 2009), and in general game playing (Finnsson and Bj\u00f6rnsson, 2008).", "startOffset": 189, "endOffset": 219}, {"referenceID": 5, "context": "This also happens in the general game playing setting (Finnsson and Bj\u00f6rnsson, 2008).", "startOffset": 54, "endOffset": 84}, {"referenceID": 11, "context": "Such information, visible only to relatively deep and systematic minimax searches, can take the form of traps as recently studied by us (Ramanujan et al., 2010), where making the \u201cwrong\u201d move leads to a state from which the opponent has a relatively simple winning strategy; such traps, even at surprisingly shallow depths, were found to be abundant even in grandmaster games of Chess.", "startOffset": 136, "endOffset": 160}, {"referenceID": 11, "context": "As a generalization of k-move winning strategies (Ramanujan et al., 2010), consider a heuristic state evaluation function h and a parameter \u2206.", "startOffset": 49, "endOffset": 73}, {"referenceID": 6, "context": "While a few attempts have been made to analyze bandit based sampling methods in general and UCT in particular (e.g., Auer et al., 2002; Gelly and Silver, 2007; Coquelin and Munos, 2007), these analyses are based on the worst case scenario and, in essence, boil down to showing that an exponential (or even superexponential (Coquelin and Munos, 2007)) number of iterations are necessary and sufficient for UCT to converge to true minimax values.", "startOffset": 110, "endOffset": 185}, {"referenceID": 3, "context": "While a few attempts have been made to analyze bandit based sampling methods in general and UCT in particular (e.g., Auer et al., 2002; Gelly and Silver, 2007; Coquelin and Munos, 2007), these analyses are based on the worst case scenario and, in essence, boil down to showing that an exponential (or even superexponential (Coquelin and Munos, 2007)) number of iterations are necessary and sufficient for UCT to converge to true minimax values.", "startOffset": 110, "endOffset": 185}, {"referenceID": 3, "context": ", 2002; Gelly and Silver, 2007; Coquelin and Munos, 2007), these analyses are based on the worst case scenario and, in essence, boil down to showing that an exponential (or even superexponential (Coquelin and Munos, 2007)) number of iterations are necessary and sufficient for UCT to converge to true minimax values.", "startOffset": 195, "endOffset": 221}, {"referenceID": 0, "context": "Auer et al. (2002) showed that this simpler variant of UCB1 also has similar good convergence properties (in the limit), as long as decreases linearly with the number of times the node is visited.", "startOffset": 0, "endOffset": 19}], "year": 2010, "abstractText": "UCT has recently emerged as an exciting new adversarial reasoning technique based on cleverly balancing exploration and exploitation in a Monte-Carlo sampling setting. It has been particularly successful in the game of Go but the reasons for its success are not well understood and attempts to replicate its success in other domains such as Chess have failed. We provide an in-depth analysis of the potential of UCT in domain-independent settings, in cases where heuristic values are available, and the effect of enhancing random playouts to more informed playouts between two weak minimax players. To provide further insights, we develop synthetic game tree instances and discuss interesting properties of UCT, both empirically and analytically.", "creator": "LaTeX with hyperref package"}}}