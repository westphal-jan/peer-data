{"id": "1203.4416", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2012", "title": "On Training Deep Boltzmann Machines", "abstract": "The deep Boltzmann machine (DBM) has been an important development in the quest for powerful \"deep\" probabilistic models. To date, simultaneous or joint training of all layers of the DBM has been largely unsuccessful with existing training methods. We introduce a simple regularization scheme that encourages the weight vectors associated with each hidden unit to have similar norms. We demonstrate that this regularization can be easily combined with standard stochastic maximum likelihood to yield an effective training strategy for the simultaneous training of all layers of the deep Boltzmann machine.", "histories": [["v1", "Tue, 20 Mar 2012 12:59:15 GMT  (269kb,D)", "http://arxiv.org/abs/1203.4416v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["guillaume desjardins", "aaron courville", "yoshua bengio"], "accepted": false, "id": "1203.4416"}, "pdf": {"name": "1203.4416.pdf", "metadata": {"source": "CRF", "title": "On Training Deep Boltzmann Machines", "authors": ["Guillaume Desjardins", "Aaron Courville", "Yoshua Bengio"], "emails": ["desjagui@iro.umontreal.ca", "courvila@iro.umontreal.ca", "bengioy@iro.umontreal.ca"], "sections": [{"heading": null, "text": "The Deep Boltzmann Machine (DBM) has been an important development in the search for powerful \"Deep\" probability models. So far, simultaneous or joint training of all layers of the DBM with existing training methods has been largely unsuccessful. We are introducing a simple regulation scheme that encourages the weight vectors associated with each hidden unit to have similar standards. We show that this regulation can easily be combined with the standard stochastic maximum probability to provide an effective training strategy for simultaneous training of all layers of the Deep Boltzmann Machine."}, {"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 Boltzmann Machines", "text": "A Boltzmann machine is defined as a network of symmetrically coupled binary stochastic units = hi-stochastic units (random variables), which can be divided into two groups: (1) the visible units v-0, 1-D, which represent the data, and (2) the hidden units h-0, 1-N, which mediate dependencies between the visible units through their interactions with each other; the pattern of interaction is defined by the energy function: EBM (v, h; \u03b8) = \u2212 1 2 vTUv \u2212 2 hTV h \u2212 vTWh \u2212 bT v \u2212 dTh, (1) where \u03b8 = {U, V, b, d} are the model parameters, each encoding the visible to visible interactions, the hidden to hidden interactions, the visible to hidden interactions, the visible to hidden interactions, the visible to hidden interactions, the visible to hidden interactions, the hidden interactions, the visible to hidden interactions, the hidden interactions, the hidden interactions, the visible to hidden, the hidden interactions, the hidden interactions, the hidden and the hidden self-links."}, {"heading": "2.1 Restricted Boltzmann Machines", "text": "The Restricted Boltzmann Machine (RBM) is probably the most popular subset of Boltzmann machines. They are defined by the limitation of interactions in the Boltzmann energy function, in Eq. 10, to only those between h and v, i.e. for ERBM, U = 0 and V = 0. As such, however, the RBM can be described as a split graph with the visibilities and hiddens forming two layers of depressions in the graph. With this limitation, the RBM possesses the useful property of factoring the conditional distribution over the hidden units, since the visibilities: P (h | v) = the maximum capability of P (hi | v) = the significant capability of Wjivj + di (7) Likewise, the conditional distribution over the visible units also possesses the factorizations: P (v | h) = the capability of P (vj | h) = j moid."}, {"heading": "3 Deep Boltzmann Machines", "text": "The Deep Boltzmann Machine (DBM) is another subset of the Boltzmann machine family of models in which the units are again arranged in layers. However, in contrast to the RBM, the DBM has several layers of hidden units, as shown in Figure 1. Regarding the Boltzmann energy function of Equation 10, the DBM corresponds to the setting U = 0 and a sparse connectivity structure in V and W. We can make the structure of the DBM clearer by specifying its energy function. For the 2-layer model, it is characterized as: EDBM (v, h (1), h (2); \u03b8) = \u2212 vTWh (1) \u2212 h (1) T V (2) \u2212 d (1) \u2212 d (2) T h (2) \u2212 bT v, (10) with the unit IS = {W, V, d (1), b}."}, {"heading": "3.1 Mean-field approximate inference", "text": "An essential starting point of the RBM is that the rear distribution over the hidden units (q) (v) (v) (v) is no longer traceable due to the interactions between the hidden units. Salakhutdinov and Hinton [7] use a middle field approach to the rear level. Specifically, in the case of the 2-layer model P (h), h (2) | v) with the factor distribution Qv (h), h (2)) = N1 j (h) = 1Qv (h) j (h) j (v) j) - N2 i = 1Qv (h) - i), so that the KL - DivergenceKL (h) - (h (v) - Qv (h1, h2) - Qv - (h) phase is minimized or equivalent, that a lower limit to the log probability is maximized: P (v) > L (Qv - Qv - h \""}, {"heading": "3.2 Training Deep Boltzmann Machines", "text": "Despite the intractability of the conclusion in the DBM, its training should theoretically not be much more complicated than that of the RBM. The main difference is that instead of directly maximizing the probability, we instead choose parameters to maximize the lower limit according to the probability given in Equation 11. The SML-based algorithm to maximize this lower limit is as follows: 1. Clamp the visible units to a training example. 2. Iterate via Equation (12-13) to convergence. 3. Generate negative phase samples v \u2212, h (1) \u2212 and h (2) \u2212 by SML. 4. Calculate those in steps 2-3. 5. Finally, update the model parameters with a step of slope."}, {"heading": "4 Joint Training of a DBM", "text": "While greedy layer-by-layer training has proved reasonably successful, joint training of a DBM would be highly desirable. It would not only be easier, but would open the door to local receptive field learning and more general architectures in which we would like the pattern of connectivity from above to influence the learning of lower-level traits. In this section, we present our simple proposal for a means of joint training of all layers of a DBM. Our strategy is based on the observation that standard SML-based joint training leads to large fluctuations in the weight vectors associated with each hidden unit - especially in the weight vectors of the first layer, which connect the visible units to the first hidden layer. Therefore, our proposal is to regulate the maximum probability target in order to encourage units to have similar standards (i) within a given layer and (ii) to a lesser extent, across adjacent layers."}, {"heading": "5 Experiments", "text": "To evaluate the effect of our regularization concept, we trained deep Boltzmann machines on the penetrating MNIST dataset [4]. All of our models were trained for 106 updates, using minibatches of size 50, which varied the learning rate in {10 \u2212 2, 10 \u2212 3, 10 \u2212 4}. Direct approach Our first model was a 3-layer DBM with [500,500,1000] hidden units in the first, second and third layer (respectively). The resulting filters are shown in Figure 2.As we can see, many of the filters of the first layer do not differ significantly from their random initialization, with very small standard. Based on the difference between these results and the successful training of an RBM, we speculate that the top-down interactions prevent the first layer from learning useful filters. We have confirmed that the high-frequency \"noise\" filters of the second layer are actually the filters with the top-level activation that appears to be the lowest."}, {"heading": "6 Discussion", "text": "We have introduced a simple regularization scheme that seems to prevent SML from falling into a poor local minimum of our objective function.The regularizer encourages units to learn filters that have similar standards, both within and across adjacent layers. We have empirically demonstrated the failure of joint training of all layers of a Boltzmann deep machine, which leaves many filters in the bottom layer of a 2-layer DBM with very small standard and consequently little contribution to data modeling. We have also empirically demonstrated the success of our regularization scheme by practicing both layers of a 2-layer DBM simultaneously. Although we have not shown it here, our scheme also seems to work well for DBMs with more than 2 layers. In future work we would like to determine how many layers can be learned simultaneously using similar basic regularizations."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the following agencies for research funding and computer support: NSERC, Calcul Que \u0301 bec and CIFAR. We would also like to thank the developers of Theano [1]."}], "references": [{"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Technical Report GCNU TR 2000-004,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Gradient based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Sparse deep belief net model for visual area V2", "author": ["H. Lee", "C. Ekanadham", "A. Ng"], "venue": "In NIPS\u201907,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Annealed importance sampling", "author": ["R.M. Neal"], "venue": "Statistics and Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS 2009),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "On the quantitative analysis of deep belief networks", "author": ["R. Salakhutdinov", "I. Murray"], "venue": "ICML 2008,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Training restricted Boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates", "author": ["L. Younes"], "venue": "Stochastics and Stochastic Reports,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}], "referenceMentions": [{"referenceID": 6, "context": "1 Introduction Since its introduction by Salakhutdinov and Hinton [7], the deep Boltzmann machine (DBM) has been one of the most ambitious attempts to build a probabilistic model with many layers of latent or hidden variables.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "The DBM shares some characteristics with the earlier deep belief network (DBN) [3].", "startOffset": 79, "endOffset": 82}, {"referenceID": 9, "context": "Straightforward applications of gradient-based methods such as stochastic maximum likelihood (SML) [10] (also known as persistent contrastive divergence [9]) appear to fall in poor local minima, and fail to adequately explore the space of model parameters.", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "Straightforward applications of gradient-based methods such as stochastic maximum likelihood (SML) [10] (also known as persistent contrastive divergence [9]) appear to fall in poor local minima, and fail to adequately explore the space of model parameters.", "startOffset": 153, "endOffset": 156}, {"referenceID": 6, "context": "One solution, as presented in [7], is based on a greedy layer-wise pretraining strategy which appears to overcome these poor local minima .", "startOffset": 30, "endOffset": 33}, {"referenceID": 4, "context": "This is a common principle that has previously been applied in the form of sparsity penalties for RBM and DBN training [5], where all hidden units are encouraged to be active over an equal proportion of the data.", "startOffset": 119, "endOffset": 122}, {"referenceID": 9, "context": "In training, we follow the stochastic maximum likelihood (SML) algorithm (also know as persistent contrastive divergence or PCD) [10, 9], i.", "startOffset": 129, "endOffset": 136}, {"referenceID": 8, "context": "In training, we follow the stochastic maximum likelihood (SML) algorithm (also know as persistent contrastive divergence or PCD) [10, 9], i.", "startOffset": 129, "endOffset": 136}, {"referenceID": 6, "context": "Salakhutdinov and Hinton [7] resort to a mean-field approximation to the posterior.", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "It can thus be estimated through SML or Contrastive Divergence [2] as in the RBM case.", "startOffset": 63, "endOffset": 66}, {"referenceID": 6, "context": "The failure of the SML joint training strategy was noted by Salakhutdinov and Hinton [7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 3, "context": "5 Experiments To evaluate the effect of our regularization term, we trained deep Boltzmann machines on the pervasive MNIST dataset [4].", "startOffset": 131, "endOffset": 134}], "year": 2012, "abstractText": "The deep Boltzmann machine (DBM) has been an important development in the quest for powerful \u201cdeep\u201d probabilistic models. To date, simultaneous or joint training of all layers of the DBM has been largely unsuccessful with existing training methods. We introduce a simple regularization scheme that encourages the weight vectors associated with each hidden unit to have similar norms. We demonstrate that this regularization can be easily combined with standard stochastic maximum likelihood to yield an effective training strategy for the simultaneous training of all layers of the deep Boltzmann machine.", "creator": "LaTeX with hyperref package"}}}