{"id": "1203.4598", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2012", "title": "Adaptive Mixture Methods Based on Bregman Divergences", "abstract": "We investigate adaptive mixture methods that linearly combine outputs of $m$ constituent filters running in parallel to model a desired signal. We use \"Bregman divergences\" and obtain certain multiplicative updates to train the linear combination weights under an affine constraint or without any constraints. We use unnormalized relative entropy and relative entropy to define two different Bregman divergences that produce an unnormalized exponentiated gradient update and a normalized exponentiated gradient update on the mixture weights, respectively. We then carry out the mean and the mean-square transient analysis of these adaptive algorithms when they are used to combine outputs of $m$ constituent filters. We illustrate the accuracy of our results and demonstrate the effectiveness of these updates for sparse mixture systems.", "histories": [["v1", "Tue, 20 Mar 2012 21:32:33 GMT  (339kb)", "http://arxiv.org/abs/1203.4598v1", "Submitted to Digital Signal Processing, Elsevier; IEEE.org"]], "COMMENTS": "Submitted to Digital Signal Processing, Elsevier; IEEE.org", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mehmet a donmez", "huseyin a inan", "suleyman s kozat"], "accepted": false, "id": "1203.4598"}, "pdf": {"name": "1203.4598.pdf", "metadata": {"source": "CRF", "title": "Adaptive Mixture Methods Based on Bregman Divergences", "authors": ["Mehmet A. Donmez", "Huseyin A. Inan", "Suleyman S. Kozat"], "emails": ["mdonmez@ku.edu.tr", "huseyin.inan@boun.edu.tr", "skozat@ku.edu.tr"], "sections": [{"heading": null, "text": "We use \"Bregman divergences\" and receive certain multiplicative updates to train the linear combination weights under affine constraint or without any constraints. We use unnormalized relative entropy and relative entropy to define two different Bregman divergences that generate an unnormalized, exponentialized gradient update and a normalized, exponentialized gradient update of the mixture weights. We then perform the mean and mean square transient analysis of these adaptive algorithms when used to combine the emissions of m-constituent filters. We demonstrate the accuracy of our results and demonstrate the effectiveness of these updates for sparse mixture systems. Keywords: Adaptive mixture, Bregman divergence, multipscolizative solutions for Donyint addresses A. @ Edulez (Signal: Suduez @ 2012)."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 System Description", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Notation", "text": "In this essay, all vectors are column vectors and are represented by lower case letters in bold. Matrices are represented by upper case letters in bold. For display purposes, we work only with real data. Since a vector w, w (i) denotes the ith entry of w, wT is the transposition of w, and w (w) and w (i) are the l1 norm. For a matrix W, diag (W) stands for the l2 norm. For a matrix W, tr (W) is the track. For two vectors v1 and v2, we define the concatenation [v1; v2]. For a random vector, diag (W) stands for a column vector containing the diagonal entries of W. For two vectors v1 and v2, we define the concatenation [v1; v2]. For a random vector, the expected vector (or vector) is the vector (expected)."}, {"heading": "2.2 System Description", "text": "The framework we are examining has two stages. In the first stage, we have m adaptive filters, which produce outputs y (t), i = 1,., m, which in parallel provide a desired signal y (t) as in Fig. 1. The second stage is the mixing stage, in which the outputs of the first stage filters are combined to improve the equilibrium state and / or transient performance through the constituent filters. We combine the outputs of the first stage filters to produce the final output as y (t) = wT (t) x (t), in the x (t) x (z), in the x (t).z), y (t), in which we generate the mixing weights with multiplicative updates (or exponentialized gradient updates). We point out that in order to meet the limitations and use the multiplicate updates [9] [20]."}, {"heading": "3 Adaptive Mixture Algorithms", "text": "In this section, we examine affinity-restricted and unrestricted mixtures that have been updated with the EGU algorithm and the EC algorithm."}, {"heading": "3.1 Affinely Constrained Mixture", "text": "If an affine constraint is imposed on the mixture, such that wT (t) 1 = 1, we can gety (t) = w (t) Tx (t), e (t) = y (t) \u2212 y (t), w (i) (t) = \u03bb (i) (t), i = 1,.., m \u2212 1, w (m) (t) = 1 \u2212 m \u2212 1 \u2211 i = 1\u03bb (i) (t) (t), where the m \u2212 1 dimensional vector \u03bb (t) = [\u03bb (1) (t),.., \u03bb (m \u2212 1) (t)] T is the unlimited weight vector, i.e., \u03bb (t). Rm \u2212 1. Using the m (t) as an unlimited weight vector, the error can be written as e (t) = [y (t) \u2212 y \u2212 m (t) \u2212 derive (t), where it (t) is an unlimited weight vector, not (t) (t), (t) (t), (t) (), (t)."}, {"heading": "3.1.1 Unnormalized Relative Entropy", "text": "Based on the unrestricted relative entropy as distance measure, we obtain: a (t + 1) = argmin \u03bb {2 (m \u2212 1) \u2211 i = 1 [\u03bb (i) ln (\u03bb (i) \u03bb (i) a (i) a (t) \u2212 \u03bb (i)] + \u00b5 [la (\u03bba (t)) + \u03bbla (\u03bb) T (i). According to some algebra, this results in: a (i) a (t + 1) = \u03bb (i) a (t) exp {\u00b5e (t) (y) i (t) \u2212 y m (t)}, i = 1,.., m \u2212 1, \u03bb (i) a (t + 1) = \u03bb (i) a (t) exp {\u2212 \u00b5e (t) (y) \u2212 y m (t)}, i = m (t), \u03bba (i)."}, {"heading": "3.1.2 Relative Entropy", "text": "Using relative entropy as distance measure, we obtain a multiplier (t) a (t) a (t) a (t) a (t) a (t) a (t) a (t) a (t) a (t). This results in a multiplier (t) a (t + 1) = u\u03bb (i) a (t) a (t) exp {\u00b5e (t) (t) \u2212 \u2212 i (t) p (t). \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 m (t) a (k) a (t) exp {\u00b5e (t) (y) k (t) (g) k (m), p (m), p (t), p (t), p (t), p (t), p (t), p (t)."}, {"heading": "3.2 Unconstrained Mixture", "text": "Without limitations on the combination weights, the mixing stage can be written in such a way that w (t) = wT (t) x (t), e (t) = y (t) \u2212 y (t), where w (t) and w2 (t) do not necessarily have to be negative, i.e. wi (t) Rm +, i = 1, 2. We then record the unlimited weights wa (t) = [w1 (t); w2 (t)] and define a function of the loss e (t) aslu (wa (t) = e2 (t)."}, {"heading": "3.2.1 Unnormalized Relative Entropy", "text": "The definition of the cost function similar to (4) and its minimization with regard to the yields from the tow results in (i) a (t + 1) = w (i) a (t) exp {\u00b5e (t) y (t)}, i = 1,..., m, w (i) a (t + 1) = w (i) a (t) exp {\u2212 \u00b5e (t) y \u0441i (t)}, i = m + 1,.., 2 m and provides the multiplicative update of wa (t)."}, {"heading": "3.2.2 Relative Entropy", "text": "Using relative entropy under the simple constraint of w, we obtain the updated data w (i) a (t + 1) = u w (i) a (t) exp {\u00b5e (t) y (t) i (t) m \u2211 k = 1 [w (k) a (t) exp {\u00b5e (t) y k (t)} + w (k + m) a (t) exp {\u2212 \u00b5e (t) y k (t)}], i = 1,.., m, w (i) a (t + 1) = u w (i) a (t) exp {\u2212 \u00b5e (t) y i (t)} m \u00b2 k = 1 [w (k) exp {\u00b5e (t) y k (t)}."}, {"heading": "4 Transient Analysis", "text": "In this section we will examine the mean and mean quadratic transient analysis of adaptive mixing methods. We will start with the affin-restricted combination."}, {"heading": "4.1 Affinely Constrained Mixture", "text": "We first perform the transient analysis of the mixture weights updated with the EGU algorithm and then proceed with the transient analysis of the mixture weights updated with the EC algorithm."}, {"heading": "4.1.1 Unconstrained Relative Entropy", "text": "For the mixture updated with the EGU algorithm, we have the multiplicative update (as\u03bb (i) 1 (t + 1) = \u03bb (i) 1 (t) exp {\u00b5e (t) (t) (y) \u2212 i (t) \u2212 kk!, (5) \u03bb (i) 2 (t) 2 (t) (t) (i) 2 (t) exp {\u2212 e (t) (y) (y) i (t) i (i) 2 (t) 2 (t) exp {\u2212 \u00b5e (t) (t) (t) (y) i (i) i (t) i (t) \u2212 y (t)} (t)."}, {"heading": "4.1.2 Relative Entropy", "text": "For the combination updated with the EG algorithm, we have the multiplicative updates (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n (n) (n) (n (n) (n (n) (n (n) (n (n) (n (n) (n (n) (n (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) (n) n) (n) (n) (n) (n) (n) n (n) (n) (n) (n) n (n (n) n) (n (n) n) n (n (n) n (n) n) n (n (n) n (n) (n) n (n) n) n (n (n) n (n (n) n) n (n) n (n) n (n (n) n (n (n) n) n (n (n) n (n) n) n (n) n (n (n) n (n (n) n) n (n (n) n (n) n (n) n) n (n) n (n) n (n (n) n (n) n (n) n (n) n) n (n (n) n (n) n) n) n (n (n) n)"}, {"heading": "4.2 Unconstrained Mixture", "text": "We use unrestricted relative entropy and relative entropy as distance measures to update unrestricted mixing weights. First, we perform a transient analysis of the mixing weights, which is updated using the EGU algorithm. Then, we proceed to the transient analysis of the mixing weights, which is updated using the EC algorithm. Note that since the unrestricted case is close to the affin-restricted case, we only make the necessary modifications to obtain the mean and the variance recursions for the transient analysis."}, {"heading": "4.2.1 Unconstrained Relative Entropy", "text": "For the unlimited combination updated with EGU (t) = [w1 (t)), we have the multiplicative updates asw (i) 1 (t) (t), (t), (t), (w), (2), (3), (3), (3), (4), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5), (5, (5), (5, (5), (5), (5, (5), (5), (5), (5), (5), (5, (5, (5), (5), (5, (5), (5, (5), (5, (5), (5), (5, (5), (5), (5, (5, (5), (5, (5), (5), (5, (5), (5), (5, (5), (5), (5, (5), (5, (5), (5), (5, (5), (5, (5), (5, ("}, {"heading": "4.2.2 Relative Entropy", "text": "For the unrestricted combination updated with the EC algorithm, we have the multiplicative updates asw (i) a (t + 1) = u w (i) a (t) exp {\u00b5e (t) y (t) i (t)} m \u00b2 k = 1 [w (k) a (t) exp {\u00b5e (t) y (t) k (t)]], i = 1,.., m, w (i) a (t + 1) = u w (i) a (t) a (t) exp {\u2212 \u00b5e (t) y (t) m \u00b2 k = 1 [w (k) a (t) exp {\u00b5e (t) y (t)."}, {"heading": "5 Simulations", "text": "In this section, we demonstrate the accuracy of our results and compare the performance of the various adaptive mixing methods through simulations. In our simulations, we observe that using the EC algorithm to form the mixture weights yields better performance than using the LMS algorithm or the EGU algorithm to train the mixture weights for combinations with more than two filters and if the combination prefers only a few of the constituent filters.The LMS algorithm and the EGU algorithm work similarly in our simulations when they are used to train the mixture weights. We also observe in our simulations that the mixture weights under the EC update convergence to the optimal combination vector are faster than the mixture weights under the LMS algorithm. To compare the performance of the EC and LMS algorithms and the accuracy of our results in (27), (28) and (29) from different algorithmic parameters, the system parameters as well as the desired signals are selected."}, {"heading": "6 Conclusion", "text": "In this paper, we examine adaptive mixing methods based on Bregman divergences and combine the results of m adaptive filters to model a desired signal. We use unnormalized relative entropy and relative entropy as distance measurements that generate exponential gradient updating with unnormalized weights (EGU) and exponential gradient updating with positive and negative weights (EC) to train the mixing weights under affine constraints or without limitations. We provide the transient analysis of these methods, which are updated with the EGU and EC algorithms. In our simulations, we compare the performance of the EC, EGU and LMS algorithms and find that the EC algorithm performs better than the EGU and LMS algorithms when the combination vector is sparse in the stationary state. We note that the GU- similar algorithms are used to form mixing algorithms and LMS."}], "references": [{"title": "A class of stochastic gradient algorithms with exponentiated error cost functions", "author": ["C. Boukis", "D. Mandic", "A.G. Constantinides"], "venue": "Digital Signal Processing", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "A comparison of new and old algorithms for a mixture estimation problem", "author": ["D.P. Helmbold", "R.E. Schapire", "Y. Singer", "M.K. Warmuth"], "venue": "Machine Learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Stochastic analysis of an error power ratio scheme applied to the affine combination of two lms adaptive filters", "author": ["J.C.M. Bermudez", "N.J. Bershad", "J.Y. Tourneret"], "venue": "Signal Processing", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Plant identification via adaptive combination of transversal filters", "author": ["J. Arenas-Garcia", "M. Martinez-Ramon", "A. Navia-Vazquez", "A.R. Figueiras- Vidal"], "venue": "Signal Processing", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Multi-stage adaptive signal processing", "author": ["S.S. Kozat", "A.C. Singer"], "venue": "Proceedings of SAM Signal Proc. Workshop,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Separate-variable adaptive combination of LMS adaptive filters for plant identification", "author": ["J. Arenas-Garcia", "V. Gomez-Verdejo", "M. Martinez-Ramon", "A.R. Figueiras- Vidal"], "venue": "in: Proc. of the 13th IEEE Int. Workshop Neural Networks Signal Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Multiple plant identifier via adaptive LMS convex combination", "author": ["J. Arenas-Garcia", "M. Martinez-Ramon", "V. Gomez-Verdejo", "A.R. Figueiras- Vidal"], "venue": "in: Proc. of the IEEE Int. Symp. Intel. Signal Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "New algorithms for improved adaptive convex combination of lms transversal filters", "author": ["J. Arenas-Garcia", "V. Gomez-Verdejo", "A.R. Figueiras-Vidal"], "venue": "IEEE Transactions on Instrumentation and Measurement", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M. Warmuth"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "On-line portfolio selection using multiplicative updates, Mathematical Finance", "author": ["D.P. Helmbold", "R.E. Schapire", "Y. Singer", "M.K. Warmuth"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1998}, {"title": "An affine combination of two LMS adaptive filters: Transient mean-square analysis", "author": ["N.J. Bershad", "J.C.M. Bermudez", "J. Tourneret"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Steady state MSE performance analysis of mixture approaches to adaptive filtering", "author": ["S.S. Kozat", "A.T. Erdogan", "A.C. Singer", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "The LMS, PNLMS, and Exponentiated Gradient algorithms", "author": ["J. Benesty", "Y.A. Huang"], "venue": "Proc. Eur. Signal Process. Conf. (EUSIPCO)", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Fundamentals of Adaptive Filtering", "author": ["A.H. Sayed"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Mean-square performance of a convex combination of two adaptive filters", "author": ["J. Arenas-Garcia", "A.R. Figueiras-Vidal", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Arenas-Garcia, A transient analysis for the convex combination of adaptive filters", "author": ["V.H. Nascimento", "J.M.T.M. Silva"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Transient analysis of adaptive affine combinations", "author": ["S.S. Kozat", "A.T. Erdogan", "A.C. Singer", "A.H. Sayed"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "A game of prediction with expert advice", "author": ["V. Vovk"], "venue": "Journal of Computer and System Sciences", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Adaptive algorithms for sparse echo cancellation", "author": ["P.A. Naylor", "J. Cui", "M. Brookes"], "venue": "Signal Processing", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Helmbold", "R.E. Schapire", "M.K. Warmuth"], "venue": "Journal of the ACM", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "1 Introduction In this paper, we study adaptive mixture methods based on \u201cBregman divergences\u201d [1,2] that combine outputs of m constituent filters running in parallel on the same task.", "startOffset": 95, "endOffset": 100}, {"referenceID": 1, "context": "1 Introduction In this paper, we study adaptive mixture methods based on \u201cBregman divergences\u201d [1,2] that combine outputs of m constituent filters running in parallel on the same task.", "startOffset": 95, "endOffset": 100}, {"referenceID": 2, "context": "The overall system has two stages [3\u20138].", "startOffset": 34, "endOffset": 39}, {"referenceID": 3, "context": "The overall system has two stages [3\u20138].", "startOffset": 34, "endOffset": 39}, {"referenceID": 4, "context": "The overall system has two stages [3\u20138].", "startOffset": 34, "endOffset": 39}, {"referenceID": 5, "context": "The overall system has two stages [3\u20138].", "startOffset": 34, "endOffset": 39}, {"referenceID": 6, "context": "The overall system has two stages [3\u20138].", "startOffset": 34, "endOffset": 39}, {"referenceID": 7, "context": "The overall system has two stages [3\u20138].", "startOffset": 34, "endOffset": 39}, {"referenceID": 8, "context": "We use Bregman divergences and obtain certain multiplicative updates [9], [2], [10] to train these linear combination weights under an affine constraint [11] or without any constraints [12].", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "We use Bregman divergences and obtain certain multiplicative updates [9], [2], [10] to train these linear combination weights under an affine constraint [11] or without any constraints [12].", "startOffset": 74, "endOffset": 77}, {"referenceID": 9, "context": "We use Bregman divergences and obtain certain multiplicative updates [9], [2], [10] to train these linear combination weights under an affine constraint [11] or without any constraints [12].", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "We use Bregman divergences and obtain certain multiplicative updates [9], [2], [10] to train these linear combination weights under an affine constraint [11] or without any constraints [12].", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "We use Bregman divergences and obtain certain multiplicative updates [9], [2], [10] to train these linear combination weights under an affine constraint [11] or without any constraints [12].", "startOffset": 185, "endOffset": 189}, {"referenceID": 1, "context": "We use unnormalized [2] and normalized relative entropy [9] to define two different Bregman divergences that produce the unnormalized exponentiated gradient update (EGU) and the exponentiated gradient update (EG) on the mixture weights [9], respectively.", "startOffset": 20, "endOffset": 23}, {"referenceID": 8, "context": "We use unnormalized [2] and normalized relative entropy [9] to define two different Bregman divergences that produce the unnormalized exponentiated gradient update (EGU) and the exponentiated gradient update (EG) on the mixture weights [9], respectively.", "startOffset": 56, "endOffset": 59}, {"referenceID": 8, "context": "We use unnormalized [2] and normalized relative entropy [9] to define two different Bregman divergences that produce the unnormalized exponentiated gradient update (EGU) and the exponentiated gradient update (EG) on the mixture weights [9], respectively.", "startOffset": 236, "endOffset": 239}, {"referenceID": 12, "context": "We emphasize that to the best of our knowledge, this is the first mean and mean-square transient analysis of the EGU algorithm and the EG algorithm in the mixture framework (which naturally covers the classical framework also [13,14]).", "startOffset": 226, "endOffset": 233}, {"referenceID": 13, "context": "We emphasize that to the best of our knowledge, this is the first mean and mean-square transient analysis of the EGU algorithm and the EG algorithm in the mixture framework (which naturally covers the classical framework also [13,14]).", "startOffset": 226, "endOffset": 233}, {"referenceID": 10, "context": "Adaptive mixture methods are utilized in a wide range of signal processing applications in order to improve the steady-state and/or convergence performance over the constituent filters [11,12,15].", "startOffset": 185, "endOffset": 195}, {"referenceID": 11, "context": "Adaptive mixture methods are utilized in a wide range of signal processing applications in order to improve the steady-state and/or convergence performance over the constituent filters [11,12,15].", "startOffset": 185, "endOffset": 195}, {"referenceID": 14, "context": "Adaptive mixture methods are utilized in a wide range of signal processing applications in order to improve the steady-state and/or convergence performance over the constituent filters [11,12,15].", "startOffset": 185, "endOffset": 195}, {"referenceID": 14, "context": "An adaptive convexly constrained mixture of two filters is studied in [15], where the convex combination is shown to be \u201cuniversal\u201d such that the combination performs at least as well as its best constituent filter in the steady-state [15].", "startOffset": 70, "endOffset": 74}, {"referenceID": 14, "context": "An adaptive convexly constrained mixture of two filters is studied in [15], where the convex combination is shown to be \u201cuniversal\u201d such that the combination performs at least as well as its best constituent filter in the steady-state [15].", "startOffset": 235, "endOffset": 239}, {"referenceID": 15, "context": "The transient analysis of this adaptive convex combination is studied in [16], where the time evolution of the mean and variance of the mixture weights is provided.", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "an affinely constrained mixture of adaptive filters using a stochastic gradient update is introduced in [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "The steady-state mean square error (MSE) of this affinely constrained mixture is shown to outperform the steady-state MSE of the best constituent filter in the mixture under certain conditions [11].", "startOffset": 193, "endOffset": 197}, {"referenceID": 16, "context": "The transient analysis of this affinely constrained mixture for m constituent filters is carried out in [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 11, "context": "The general linear mixture framework as well as the steady-state performances of different mixture configurations are studied in [12].", "startOffset": 129, "endOffset": 133}, {"referenceID": 17, "context": "We point out that the EG algorithm is widely used in sequential learning theory [18] and minimizes an approximate final estimation error while penalizing the distance between the new and the old filter weights.", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "In network and acoustic echo cancellation applications, the EG algorithm is shown to converge faster than the LMS algorithm [14, 19] when the system impulse response is sparse [13].", "startOffset": 124, "endOffset": 132}, {"referenceID": 18, "context": "In network and acoustic echo cancellation applications, the EG algorithm is shown to converge faster than the LMS algorithm [14, 19] when the system impulse response is sparse [13].", "startOffset": 124, "endOffset": 132}, {"referenceID": 12, "context": "In network and acoustic echo cancellation applications, the EG algorithm is shown to converge faster than the LMS algorithm [14, 19] when the system impulse response is sparse [13].", "startOffset": 176, "endOffset": 180}, {"referenceID": 10, "context": "Similarly, in our simulations, we observe that using the EG algorithm to train the mixture weights yields increased convergence speed compared to using the LMS algorithm to train the mixture weights [11, 12] when the combination favors only a few of the constituent filters in the steady state, i.", "startOffset": 199, "endOffset": 207}, {"referenceID": 11, "context": "Similarly, in our simulations, we observe that using the EG algorithm to train the mixture weights yields increased convergence speed compared to using the LMS algorithm to train the mixture weights [11, 12] when the combination favors only a few of the constituent filters in the steady state, i.", "startOffset": 199, "endOffset": 207}, {"referenceID": 1, "context": ", \u0177m(t)] T and train the mixture weights using multiplicative updates (or exponentiated gradient updates) [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 8, "context": "We point out that in order to satisfy the constraints and derive the multiplicative updates [9], [20], we use reparametrization of the mixture weights as w(t) = f (z(t)) and perform the update on z(t) as z(t + 1) = argmin z {", "startOffset": 92, "endOffset": 95}, {"referenceID": 19, "context": "We point out that in order to satisfy the constraints and derive the multiplicative updates [9], [20], we use reparametrization of the mixture weights as w(t) = f (z(t)) and perform the update on z(t) as z(t + 1) = argmin z {", "startOffset": 97, "endOffset": 101}, {"referenceID": 1, "context": "Since \u03bc is usually relatively small [2], we approximate (7) and (8) as \u03bb (i) 1 (t+ 1) = \u03bb (i) 1 (t) (", "startOffset": 36, "endOffset": 39}, {"referenceID": 16, "context": "(16) Taking expectation of both sides of (16) and using E [ \u03bcdiag ( \u03b4(t) ) \u03bb(t)e0(t) ] = E [ \u03bcdiag ( \u03b4(t) ) \u03bb(t) ] E[e0(t)] = 0, E [ 2\u03bcdiag ( \u03b4(t) ) \u03bb1(t)e0(t) ] = E [ 2\u03bcdiag ( \u03b4(t) ) \u03bb1(t) ] E[e0(t)] = 0, and assuming that \u03bb1(t) and \u03bb2(t) are independent of \u03b5(t) [17] yield E [", "startOffset": 264, "endOffset": 268}, {"referenceID": 15, "context": "Assuming convergence of R(t) and p(t) (which is true for a wide range of adaptive methods in the first stage [16], [14, 21]), we obtain limt\u2192\u221e E [", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "Assuming convergence of R(t) and p(t) (which is true for a wide range of adaptive methods in the first stage [16], [14, 21]), we obtain limt\u2192\u221e E [", "startOffset": 115, "endOffset": 123}, {"referenceID": 16, "context": "independent when i 6= j [17], [14], we get a recursion for E [ \u03bba(t)\u03bb T a (t) ]", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "independent when i 6= j [17], [14], we get a recursion for E [ \u03bba(t)\u03bb T a (t) ]", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Note that \u0393(t) and \u03b3(t) are derived for a wide range of adaptive filters [16], [14].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "Note that \u0393(t) and \u03b3(t) are derived for a wide range of adaptive filters [16], [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "= 0 for most adaptive filters in the first stage [14] and if \u03bc is chosen so that all the eigenvalues of E [ I \u2212 \u03bcdiag ( w1(t) + w2(t) ) x(t)x (t) ] have strictly less than unit magnitude for every t, then limt\u2192\u221eE [", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "147] , is chosen as in [17].", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "Therefore, in the steady-state, we obtain the optimum vector approximately as wo = [1, 0].", "startOffset": 83, "endOffset": 89}], "year": 2012, "abstractText": "We investigate adaptive mixture methods that linearly combine outputs of m constituent filters running in parallel to model a desired signal. We use \u201cBregman divergences\u201d and obtain certain multiplicative updates to train the linear combination weights under an affine constraint or without any constraints. We use unnormalized relative entropy and relative entropy to define two different Bregman divergences that produce an unnormalized exponentiated gradient update and a normalized exponentiated gradient update on the mixture weights, respectively. We then carry out the mean and the mean-square transient analysis of these adaptive algorithms when they are used to combine outputs of m constituent filters. We illustrate the accuracy of our results and demonstrate the effectiveness of these updates for sparse mixture systems.", "creator": "LaTeX with hyperref package"}}}