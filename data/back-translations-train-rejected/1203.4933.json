{"id": "1203.4933", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2012", "title": "Reduplicated MWE (RMWE) helps in improving the CRF based Manipuri POS Tagger", "abstract": "This paper gives a detail overview about the modified features selection in CRF (Conditional Random Field) based Manipuri POS (Part of Speech) tagging. Selection of features is so important in CRF that the better are the features then the better are the outputs. This work is an attempt or an experiment to make the previous work more efficient. Multiple new features are tried to run the CRF and again tried with the Reduplicated Multiword Expression (RMWE) as another feature. The CRF run with RMWE because Manipuri is rich of RMWE and identification of RMWE becomes one of the necessities to bring up the result of POS tagging. The new CRF system shows a Recall of 78.22%, Precision of 73.15% and F-measure of 75.60%. With the identification of RMWE and considering it as a feature makes an improvement to a Recall of 80.20%, Precision of 74.31% and F-measure of 77.14%.", "histories": [["v1", "Thu, 22 Mar 2012 09:50:51 GMT  (384kb)", "http://arxiv.org/abs/1203.4933v1", "15 pages, 4 tables, 2 figures, the linkthis http URLarXiv admin note: text overlap witharXiv:1111.2399"]], "COMMENTS": "15 pages, 4 tables, 2 figures, the linkthis http URLarXiv admin note: text overlap witharXiv:1111.2399", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kishorjit nongmeikapam", "lairenlakpam nonglenjaoba", "yumnam nirmal", "sivaji bandyopadhyay"], "accepted": false, "id": "1203.4933"}, "pdf": {"name": "1203.4933.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["MANIPURI POS TAGGER", "Kishorjit Nongmeikapam", "Lairenlakpam Nonglenjaoba", "Yumnam Nirmal", "Sivaji Bandhyopadhyay"], "emails": ["kishorjit.nongmeikapa@gmail.com", "nonglen.ran@gmail.com", "yumnamnirmal@gmail.com", "sivaji_cse_ju@yahoo.com"], "sections": [{"heading": null, "text": "This work is an attempt or experiment to make the previous work more efficient. Several new features are tried to run the CRF, and the Reduplicated Multiword Expression (RMWE) is tried again as another feature. CRF runs with RMWE because Manipuri is rich in RMWE, and the identification of RMWE becomes one of the necessities to boost the result of POS tagging. The new CRF system shows a recall of 78.22%, an accuracy of 73.15% and an F-measure of 75.60%. Identifying RMWE and viewing it as a feature improves a recall of 80.20%, an accuracy of 74.31% and an F-measure of 77.14%."}, {"heading": "1. INTRODUCTION", "text": "This year, it has come to the point where we will be able to go in search of a solution that is capable, that we are able, that we are able to find a solution, that we are able, that we are able to find a solution, that we are able to find a solution, that we are able, that we are able to find a solution, that we are able to find a solution, that we are able to find a solution, that we are able, that we are able to find a solution."}, {"heading": "2. ROOT AND AN EXAMPLE OF AN AGGLUTINATIVE MANIPURI WORD", "text": "In Manipuri, roots are different in different languages. In English, roots are almost free, which means that there are separate root groups that denote different grammatical categories such as nouns (pronouns, boy, bird, etc.), verbs (run, cry, attempt, etc.), adjectives (high, thin, large, etc.), etc. In Manipuri, there are no separate roots for adjectives and verbs in manipulation. In Manipuri, roots are of two types, they are free and bound roots. Free roots can stand alone without suffixes in a sentence, and bound roots take up other affixes except those with free roots. This language is highly agglutinative and to prove with this point that suffixes in the form of \"suffixes\" are an example word: \"(\" pusinha suffixes, \"etc.)."}, {"heading": "3. REDUPLICATED MULTIWORD EXPRESSIONS (RMWE)", "text": "In Manipuri, the process of reduplication is defined as follows: \"Reduplication is the repetition that results in a single word.\" These are the RMWE. The reduced MWEs in Manipuri are mainly divided into four different types: 1) Completely reduced MWEs, 2) Mixed reduced MWEs, 3) Echo reduced MWEs and 4) Partly reduced MWEs. Apart from these four types, there are also cases of a) Double reduced MWEs and b) Semantically reduced MWEs."}, {"heading": "3.1 Complete Reduplication MWEs", "text": "In fact, most of them are able to retaliate, \"he said.\" It's as if they are able to retaliate, \"he said.\" It's as if they are able to retaliate. \""}, {"heading": "3.2 Partial Reduplicated MWEs", "text": "In the case of partial reduplication, the second word carries part of the first word as an appendage to the second word, either as a suffix or as a prefix. For example, J =; J K (\"c\u043et-thok c\u043et-sin\") means \"going back and forth,\" @ F @ (\"sa-mi lan-mi\") means \"army.\""}, {"heading": "3.3 Echo Reduplicated MWEs", "text": "The second word has no dictionary meaning and is essentially an echo of the first word. For example = K: K (\"thk-si kha-si\") means \"good way.\" Here the first word has a dictionary meaning \"good way,\" but the second word has no dictionary meaning and is an echo of the first word."}, {"heading": "3.4 Mimic Reduplicated MWEs", "text": "In mimic reduplication, the words are complete reduplication, but the morphemes are tonomatopoeic sounds, usually emotional or natural."}, {"heading": "3.5 Double Reduplicated MWEs", "text": "In the double reduplication of MWE there are three words where the prefix or suffix of the first two words is reduced but the prefix or suffix is missing in the third word. An example of the double reduplication of prefixes is 9 5 9 5 5 (\"i-mun i-mun mun-ba\"), which means \"perfectly mature.\" It should be noted that the prefix is duplicated in the first two words, while in the following example the suffix reduction takes place, > B > B > B (\"\u03c9-srok \u03c9-srok \u03c9-ba\"), which means \"luminous white.\""}, {"heading": "3.6 Semantic Reduplicated MWEs", "text": "Both reduplication words have the same meaning as the MWE. This type of MWEs is very special to the Manipuri language. For example, A (\"pamba k\u0430y\") means \"tiger\" and each of the component words means \"tiger.\" Semantic reduplication abounds in Manipuri, as such words are derived from similar words used by seven clans in Manipur during the evolution of language."}, {"heading": "4. STEMMING OF MANIPURI WORDS", "text": "The root words, which are considered to be characteristics for the execution of the CRF, follow an algorithm mentioned in [25]. In this algorithm, manipuri words are contained by iterative stripping of suffixes. As mentioned in Section 2, a word is rich in suffixes and prefixes. To stop a word, an iterative method of stripping is performed using the acceptable list of prefixes (11 numbers) and suffixes (61 numbers), as mentioned in Table 1 and Table 2 above."}, {"heading": "4.1 The Algorithm", "text": "This trunk consists mainly of four algorithms, the first of which reads the prefixes, the second the suffixes, the third the root word that removes the prefixes, and the last algorithm the root word that removes the suffixes. Prefixes and suffixes are removed in an iterative approach, as shown in Algorithm 3 and Algorithm 4, until all suffixes and suffixes are removed from Manipuri. The root word is stored in stemwrd.Algorithm1: read _ prefixes () 1. Repeat 2 to 4 until all prefixes (pi) are read. wrltx _ list 2 until all suffixes d are removed. The root word is stored in stemwrd.Algorithm1: read _ prefixes () 1. Repeat 2 to 4 until all prefixes (pi) are read."}, {"heading": "5. CONCEPTS OF CRF", "text": "The concept of the conditional random field in [24] was developed to calculate the conditional probabilities of values on other designated entry nodes of undirected graphical models. CRF encodes a conditional probability distribution with a given set of characteristics. It is an unattended approach, in which the system learns by training and can be used to test other text. Conditional probability of a state sequence Y = (y1, y2,.. yT) in an observation sequence X = (x1, x2,.. xT) is calculated as: P (Y | X) = t) X, y, y (fexp (1t1-tT1tkkkk XZ). The values of this function can be calculated between - \u221e + - \u00b2 ---- where CRT (yt, X, t, t, t) is a function whose weight distribution is the ability associated with fk and can be learned by training."}, {"heading": "6. CRF MODEL AND FEATURE SELECTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 The CRF Model", "text": "The work of [19] also shows the use of CRF to mark the POS in a running text. It was the first attempt at POS marking with CRF and had low efficiency. This work is an attempt or experiment to make the previous work more efficient. In this work, the C + + -based CRF + + 0.53 package1, which is available as open source for segmenting or labeling sequential data, is used. The CRF model for manipuri POS marking (Figure 1) consists mainly of data training and data testing. The important processes required for POS marking using CRF are feature selection, pre-processing, which includes the arrangement of tokens or words in sentences with other notations, the creation of model files after the training and finally testing with the test corpus.The following subsections explain the overall process in detail: 1 http: / / crf.sourppge.net /"}, {"heading": "6.1 Feature Selection", "text": "The aforementioned hsci-eaJnlhsrdcnlhSrdcu nvo edn nlrcnhei-eaJnlhsrteeaeSrlhsrcnlhsrteeoiuiuiuiuiuuiuuuiuiuiuiuiuaeSrgcnh ni edn nlrteeSrteeu ufa nde nlrrrgne\u00fceaeFngn uaf nde nlrrrrrrrrrrrteeaeSn ni edn nlrteeSas the ihsc-eaJnlcnlrrrrrteeaeeaeFngn uiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiuiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}, {"heading": "6.2 Pre-processing and feature selection", "text": "A Manipuri text document serves as an input file. The training and test files consist of multiple tokens. In addition, each token consists of multiple (but specified) columns in which the columns are used by a template file. The template file gives a complete idea of the feature selection. Each token must be represented in a line, with the columns separated by spaces (spaces or tabular characters). A sequence of tokens is turned into a sentence. Prior to training and testing in the CRF, the input document is converted into a multi-token file with fixed columns, and the template file allows for the feature combination and selection specified in Section 6.1. Two default files with multiple tokens with fixed columns are created: one for training and another for testing. In the training file, the last column is manually marked with all identified POS tag2, while in the test file we can either use the same token for O only or for the same token. \""}, {"heading": "6.3 Training to get the Model File", "text": "For the training of the CRF we used a template file whose function is to select the features from the features list. The model file is the output file after the training. The model file is the learned file of the CRF system. We receive the model file by means of the training file after the training. This model file is a ready-made file of the CRF tool for the test process. There is no need to use the template file and the training file again, since the model file consists of the detailed information of the template file and training file."}, {"heading": "6.4 Testing", "text": "The test is performed on the basis of the model file generated during the training of the CRF system. One of the two gold standard files created before the execution of the CRF system is the test file. As mentioned in Section 6.2, this file must be created in the same format as the training file, i.e. with a fixed number of columns in the same field as the training file. Thus, after the test process, the output file is a new file with an additional column containing the POS tags."}, {"heading": "7. EXPERIMENT AND THE RESULT", "text": "It is time consuming, as already mentioned in Section 6.2, three linguistic experts from the Linguistics Department, Manipur University is hired to correct the spelling and syntax of a sentence. Also, they have performed the most important part of the POS marking on the corpus, which consists of 25,000 tokens. It is considered the gold standard, but later each line is divided into two files, one for the training file and another for the testing. As in Section 6, a Java program is written to identify the required features except the POS marking, since it is marked manually and the function is separated by a space. Thus, each line becomes a sentence. The last column of both the training and the Testing2 http: / / shiva.iit.ac.in / SPSAL2007 / iit _ tagset _ guidelines.pdffiles is separated by a space."}, {"heading": "7.1 Experiment for selection of best feature", "text": "As mentioned in Section 6.2, the pre-processing of the file is performed and a total of 25,000 words are divided into two files, one consisting of 20,000 words and the second file consisting of 5,000 words. The first file is for training and the second file is for testing. As already mentioned, a Java program is used to divide the sentence into an equal number of columns separated by a space with different characteristics. The experiment is performed with different combinations of characteristics. The characteristics are selected manually so that the result shows an improvement in the F-bar. Among the different experiments with different combinations, Table 4 lists some of the best combinations. Note that the combinations in the list are not the only ones to be tried, but belong to the best combinations.Table 3 explains the notations used in Table 4."}, {"heading": "7.2 Evaluation and best Feature", "text": "The disadvantage of manual selection is the combination of attributes. Rigidity in the CRF model is also the combination of attributes and selection of attributes. The best feature reported so far in the previous CRF-based model [19] is the following:"}, {"heading": "F= {Wi-2, Wi-1, Wi, Wi+1, |prefix|<=n, |suffix|<=n, Dynamic POS tag of the previous word, Digit information, Length of the word, Frequent word, Symbol feature}", "text": "The list consists of surrounding words, prefixes, suffixes, surrounding POS characters, number information, word length, word frequency and symbol characteristics. The prefixes and suffixes used are only a combination of n characters. They do not adopt the standard prefixes and suffixes. The model used here has a different list and the best function is selected according to all possible combinations.The best result is the one that shows the best efficiency among the results.This is done with the following characteristics: F = {Wi-2, W i-1, W i, W i + 1, SWi-1, SWi + 1, number of acceptable standard suffixes, number of acceptable standard suffixes, acceptable suffixes in the word, acceptable prefixes in the word, word length, word frequency, digit function, symbol characteristic} The best characteristic in the model gives all (R) of 2.7% (78.7%) and 73.19% (the previous measure) that"}, {"heading": "8. IMPROVEMENT USING RMWE", "text": "With the aim of improving the efficiency of POS marking in the CRF system, the concept of identification and inclusion of the RMWE as a feature is being tested."}, {"heading": "8.1. The RMWE identification Model", "text": "The algorithms and models proposed in [20] for locating reduced MWEs in the Manipuri text are used to identify reduced MWEs. The first model is used to identify the complete, mimic, partial, double and echo reduced MWEs. Figure 2 gives an idea of the model 1. The functions performed by the different parts of the proposed model are: a) Tokenizer, separates the words by spaces or special symbols to identify two consecutive words Wi and Wi + 1. b) Reduplication MWE identifier, it verifies the valid inversions in the words and also checks the semantics of Wi + 1 in the dictionary for the Echo dictionary. c) Valid inflection list, list of frequently used valid inversions is an important resource for MWE identification. d) Dictionary, it contains the lexicon and related semantics."}, {"heading": "8.1. The Algorithm for first model", "text": "Algorithm for identifying the types of reducing MWEs: 1. Repeat 2 to 21 until all the tokens (Wi) are read in the text where (i = 1 to n). 2. Check if Wi and Wi + 1 are the same word, if the same goes to 3, go to 14 3. check if Wi + 1 is in the dictionary when it is found, then identify as mimic reduction 4. Repeat 5 to 8 until the entire list of prefixes (Pk) is read where (k = 1 to m) 5. temp1 = (Wi) - Pk 6. Check if temp1 is a starting substring of Wi + 2, if it goes to 9 7. temp2 = (Wi) - Sj 8. Check if temp2 is a starting substring of Wi + 2, if it goes so to 9, otherwise go to Wi 9. Identify it as a double reduction of Wi + 2, if it goes to 9 7. temp2 = (Wi) - Sj 8. Check if temp2 is a starting substring of Wi + 2, if it goes to 9. (Wi + 2) - Wi + 1. Identify it as a double reduction of Wi + 2, if it goes to 11. Repeat all until 12, until the entire temple is read (Wj = Wi) where Wi + 1 is read (Wi = Wi)."}, {"heading": "8.1.2 The Algorithm for second model", "text": "Algorithm for identifying the semantic reduplication of MWEs: 1. Repeat 2 until all tokens (Wi) are read in the text, where (i = 1 to n). 2. Check in the dictionary whether Wi + 1 are semantically identical when the same asmantic reduplication is identified. 3. End."}, {"heading": "8.2 The improvement after using RMWE as a feature", "text": "After running the training and test files with the above model (Section 8.1), the output will be marked with B-RMWE for the beginning and I-RMWE for the rest of the RMWE and O for the non-RMWEs. This output will be placed as a new column in the multiple token file for both training and testing, and the training file will be run again with the CRF toolkit, which will output a new model file. This model file will be used to execute the test file, which after the learning process will add a new output column, which is the POS tag of the machine. This new marker will be used to compare with the previous output."}, {"heading": "9. CONCLUSION", "text": "The previous reported model of [19] has an accuracy of 72.04% when tested in the test kit with CRF system, and an accuracy of 74.38% when tested in the SVM system, but this purpose system is a better model. This model surpasses the previous model in terms of efficiency with the recall (R) of 78.22%, the precision (P) of 73.15% and the F-dimension (F) of 75.60%. With the RMWE as a feature, it even improves the efficiency of the recall (R) of 80.20%, the precision (P) of 74.31% and the F-dimension (F) of 77.14%."}], "references": [{"title": "A Meitei Grammar of Roots and Affixes, A Thesis, Unpublish, Manipur", "author": ["N Nonigopal Singh"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "A Simple Rule-based Part of Speech Tagger", "author": ["Brill", "Eric"], "venue": "In the Proceedings of Third International Conference on Applied Natural Language Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part of Speech Tagging", "author": ["Brill", "Eric"], "venue": "Computational Linguistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Decision tree models applied to labeling of texts with parts of speech", "author": ["E. Black", "F. Jelinek", "J. Lafferty", "R. Mercer", "S. Roukos"], "venue": "In the DARPA Workshop on Speech and Natural Language,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1992}, {"title": "A Practical Part of Speech Tagger", "author": ["D. Cutting", "J. Kupiec", "J. Pederson", "P. Sibun"], "venue": "In the Proceedings of the 3 ANLP Conference,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "A maximum entropy Parts- of- Speech Tagger", "author": ["A. Ratnaparakhi"], "venue": "In the Proceedings EMNLP 1, ACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "Part-of-speech tagging using a Hidden Markov Model", "author": ["R. Kupiec"], "venue": "In Computer Speech and Language,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1992}, {"title": "Discrimination oriented probabilistic tagging", "author": ["Lin", "Y.C.T.H. Chiang", "K.Y. Su"], "venue": "In the Proceedings of ROCLING V,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "HMM-based Part-of-Speech Tagging for Chinese Corpora", "author": ["C.H. Chang", "C.D. Chen"], "venue": "In the Proceedings of the Workshop on Very Large Corpora,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": " K", "author": ["C.J. Chen", "M.H. Bai"], "venue": "J. Chen, ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Part of Speech Tagging of Chinese Sentences Using Genetic Algorithm", "author": ["T. K"], "venue": "In the Proceedings of ICCC96, National University of Singapore,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "POS Tagging using HMM and Rule-based Chunking", "author": ["Ekbal", "Asif", "S Mondal", "B. Sivaji"], "venue": "In the Proceedings of SPSAL2007, IJCAI, India,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Bengali Part of Speech Tagging using Conditional Random Field, In the Proceedings 7th SNLP, Thailand", "author": ["Ekbal", "Asif", "R. Haque", "B. Sivaji"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Maximum Entropy based Bengali Part of Speech Tagging", "author": ["Ekbal", "Asif", "R. Haque", "B. Sivaji"], "venue": "Advances in Natural Language Processing and Applications, Research in Computing Science (RCS) Journal,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Morphological Richness offsets Resource Demand \u2013Experiences in constructing a POS tagger for Hindi", "author": ["Smriti Singh", "Kuhoo Gupta", "Manish Shrivastava", "Pushpak Bhattacharya"], "venue": "In the Proceedings of COLING- ACL,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Shallow Parsing with Conditional Random fields", "author": ["F. Sha", "F. Pereira"], "venue": "In the Proceedings of NAACL-HLT, Canada,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Morphology Driven Manipuri POS Tagger", "author": ["T. Doren Singh", "B. Sivaji"], "venue": "In the Proceeding of IJCNLP NLPLPL", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Manipuri POS tagging using CRF and SVM: A language independent approach", "author": ["T. Doren Singh", "A. Ekbal", "B. Sivaji"], "venue": "In the proceeding of 6th International conference on Natural Language Processing (ICON -2008),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Identification of Reduplicated MWEs in Manipuri: A Rule based Approached", "author": ["N. Kishorjit", "B. Sivaji"], "venue": "In the Proceeding of 23 International Conference on the Computer Processing of Oriental Languages", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Identification of Reduplicated Multiword Expressions Using CRF, A", "author": ["N. Kishorjit", "L. Dhiraj", "N. Bikramjit Singh", "Mayekleima Chanu", "Ng", "B. Sivaji"], "venue": "Gelbukh (Ed.):CICLing", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Identification of MWEs Using CRF in Manipuri and Improvement Using Reduplicated MWEs", "author": ["N. Kishorjit", "B. Sivaji"], "venue": "In the Proceedings of 8 International Conference on Natural Language (ICON-2010), IIT Kharagpur, India,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Web Based Manipuri Corpus for Multiword NER and Reduplicated MWEs Identification using SVM", "author": ["T. Doren Singh", "B. Sivaji"], "venue": "In the Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), the 23rd International Conference on Computational Linguistics (COLING), Beijing,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "In the Procceedings of the 18th International Conference on Machine Learning (ICML01),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "A Light Weight Manipuri Stemmer", "author": ["N. Kishorjit", "S. Bishworjit", "M. Romina", "Mayekleima Chanu", "Ng", "B. Sivaji"], "venue": "Proceedings of Natioanal Conference on Indian Language Computing (NCILC),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "In Manipuri, words are formed in three processes called affixation, derivation and compounding as mentioned in [1].", "startOffset": 111, "endOffset": 114}, {"referenceID": 1, "context": "Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8].", "startOffset": 162, "endOffset": 165}, {"referenceID": 2, "context": "Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8].", "startOffset": 210, "endOffset": 213}, {"referenceID": 3, "context": "Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8].", "startOffset": 303, "endOffset": 306}, {"referenceID": 4, "context": "Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8].", "startOffset": 324, "endOffset": 327}, {"referenceID": 5, "context": "Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8].", "startOffset": 353, "endOffset": 356}, {"referenceID": 6, "context": "Different attempts are seen for different major languages using different approaches such as for English a Simple Rule-based Part of Speech Tagger is reported in [3], transformation-based error-driven learning [4], decision trees models applied to labelling of texts with parts of speech is reported in [5], Markov model in [6], maximum entropy methods [7] and Part-of-speech tagging using a Hidden Markov Model (HMM) in [8].", "startOffset": 421, "endOffset": 424}, {"referenceID": 7, "context": "For Chinese, the works are found ranging from rule based, HMM to Genetic Algorithm [9]-[12].", "startOffset": 83, "endOffset": 86}, {"referenceID": 10, "context": "For Chinese, the works are found ranging from rule based, HMM to Genetic Algorithm [9]-[12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "For Indian languages like Bengali works are reported in [13]-[15] and for Hindi [16].", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "For Indian languages like Bengali works are reported in [13]-[15] and for Hindi [16].", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": "For Indian languages like Bengali works are reported in [13]-[15] and for Hindi [16].", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "Works of POS tagging using CRF can also be seen in [17].", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "Works of Manipuri POS tagging are reported in [18]-[19].", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "Works of Manipuri POS tagging are reported in [18]-[19].", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "For the identification of Reduplicated Multiword Expression (RMWE) is reported in [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 19, "context": "Identification of RMWE using CRF is reported in [21] and improvement of MWE using RMWE is reported in [22].", "startOffset": 48, "endOffset": 52}, {"referenceID": 20, "context": "Identification of RMWE using CRF is reported in [21] and improvement of MWE using RMWE is reported in [22].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": "Web Based Manipuri Corpus for Multiword NER and Reduplicated MWEs Identification using SVM is reported in [23].", "startOffset": 106, "endOffset": 110}, {"referenceID": 18, "context": "In Manipuri works for identification of reduplicated MWEs has been reported for the first time in [20].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "The Stem words which are considered as feature for running the CRF follows an algorithm mention in [25].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "The concept of Conditional Random Field in [24] is developed in order to calculate the conditional probabilities of values on other designated input nodes of undirected graphical models.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "The work of [19] also shows the use of CRF in order to tag the POS in a running text.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "The best feature so far reported in the previous CRF based model [19] is as follows:", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "The earlier model in [19] reports that the CRF based system shows 72.", "startOffset": 21, "endOffset": 25}, {"referenceID": 18, "context": "The Algorithms and models for finding reduplicated MWEs in Manipuri text as suggested in [20] is used for the identification of reduplicated MWEs.", "startOffset": 89, "endOffset": 93}, {"referenceID": 0, "context": "Feature R (in %) P (in %) FS (in %) W[-2,+1], SW[-1,+1], P[1], S[4], L, F, NS, NP, D, SF 78.", "startOffset": 58, "endOffset": 61}, {"referenceID": 2, "context": "Feature R (in %) P (in %) FS (in %) W[-2,+1], SW[-1,+1], P[1], S[4], L, F, NS, NP, D, SF 78.", "startOffset": 64, "endOffset": 67}, {"referenceID": 0, "context": "60 W[-2,+2], SW[-2,+1], P[1], S[4], L, F, NS, NP, D, SF 77.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "60 W[-2,+2], SW[-2,+1], P[1], S[4], L, F, NS, NP, D, SF 77.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "64 W[-2,+3], SW[-2,+2], P[1], S[4], L, F, NS, NP, D, SF 75.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "64 W[-2,+3], SW[-2,+2], P[1], S[4], L, F, NS, NP, D, SF 75.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "04 W[-3,+1], SW[-3,+1], P[1], S[4], L, F, NS, NP, D, SF 72.", "startOffset": 25, "endOffset": 28}, {"referenceID": 2, "context": "04 W[-3,+1], SW[-3,+1], P[1], S[4], L, F, NS, NP, D, SF 72.", "startOffset": 31, "endOffset": 34}, {"referenceID": 0, "context": "57 W[-3,+3], SW[-3,+2], P[1], S[5], L, F, NS, NP, D 61.", "startOffset": 25, "endOffset": 28}, {"referenceID": 3, "context": "57 W[-3,+3], SW[-3,+2], P[1], S[5], L, F, NS, NP, D 61.", "startOffset": 31, "endOffset": 34}, {"referenceID": 3, "context": "02 W[-3,+4], SW[-2,+3], P[2], S[5], L, F, NS, SF 53.", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": "37 W[-4,+1], SW[-4,+1], P[2], S[6], L, NP, D, SF 47.", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "78 W[-4,+3], SW[-3,+3], P[3], S[9], L, F, D, SF 38.", "startOffset": 25, "endOffset": 28}, {"referenceID": 7, "context": "78 W[-4,+3], SW[-3,+3], P[3], S[9], L, F, D, SF 38.", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "70 W[-4,+4], SW[-4,+4], P[3], S[10], NS, NP 34.", "startOffset": 25, "endOffset": 28}, {"referenceID": 8, "context": "70 W[-4,+4], SW[-4,+4], P[3], S[10], NS, NP 34.", "startOffset": 31, "endOffset": 35}, {"referenceID": 17, "context": "The previous reported model of [19] it has an accuracy of 72.", "startOffset": 31, "endOffset": 35}], "year": 2012, "abstractText": "This paper gives a detail overview about the modified features selection in CRF (Conditional Random Field) based Manipuri POS (Part of Speech) tagging. Selection of features is so important in CRF that the better are the features then the better are the outputs. This work is an attempt or an experiment to make the previous work more efficient. Multiple new features are tried to run the CRF and again tried with the Reduplicated Multiword Expression (RMWE) as another feature. The CRF run with RMWE because Manipuri is rich of RMWE and identification of RMWE becomes one of the necessities to bring up the result of POS tagging. The new CRF system shows a Recall of 78.22%, Precision of 73.15% and F-measure of 75.60%. With the identification of RMWE and considering it as a feature makes an improvement to a Recall of 80.20%, Precision of 74.31% and F-measure of 77.14%.", "creator": "PScript5.dll Version 5.2.2"}}}