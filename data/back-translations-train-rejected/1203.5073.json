{"id": "1203.5073", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2012", "title": "USFD at KBP 2011: Entity Linking, Slot Filling and Temporal Bounding", "abstract": "This paper describes the University of Sheffield's entry in the 2011 TAC KBP entity linking and slot filling tasks. We chose to participate in the monolingual entity linking task, the monolingual slot filling task and the temporal slot filling tasks. We set out to build a framework for experimentation with knowledge base population. This framework was created, and applied to multiple KBP tasks. We demonstrated that our proposed framework is effective and suitable for collaborative development efforts, as well as useful in a teaching environment. Finally we present results that, while very modest, provide improvements an order of magnitude greater than our 2010 attempt.", "histories": [["v1", "Thu, 22 Mar 2012 18:34:19 GMT  (116kb,D)", "http://arxiv.org/abs/1203.5073v1", "Proc. Text Analysis Conference (2011)"]], "COMMENTS": "Proc. Text Analysis Conference (2011)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amev burman", "arun jayapal", "sathish kannan", "madhu kavilikatta", "ayman alhelbawy", "leon derczynski", "robert gaizauskas"], "accepted": false, "id": "1203.5073"}, "pdf": {"name": "1203.5073.pdf", "metadata": {"source": "CRF", "title": "USFD at KBP 2011: Entity Linking, Slot Filling and Temporal Bounding", "authors": ["Amev Burman", "Arun Jayapal", "Sathish Kannan", "Madhu Kavilikatta", "Ayman Alhelbawy", "Leon Derczynski", "Robert Gaizauskas"], "emails": [], "sections": [{"heading": null, "text": "USFD at KBP 2011: Entity Linking, Slot Filling and Temporal BoundingAmev Burman, Arun Jayapal, Sathish Kannan, Madhu Kavilikatta Ayman Alhelbawy, Leon Derczynski, Robert GaizauskasNatural Language Processing Group Department of Computer ScienceUniversity of Sheffield, S1 4DP, UK"}, {"heading": "1 Introduction", "text": "This paper describes the entry of the University of Sheffield into the 2011 TAC KBP Programme for Linking and Completing Tasks (Ji et al., 2011). Our team consisted of five MSc students, two PhD students and another senior academic. For the MSc students, attending the course formed the core of their MSc dissertation project, which they began in February 2011 and finished at the end of August 2011. None of them had any prior experience in human language technologies or machine learning before starting their programme. For the two PhD students, participating in this project was relevant to their ongoing doctoral work. This team organisation enabled us to raise significant manpower without special external funding and within a limited period of time, but of course there were inevitable problems with coordinating effort and getting up to speed."}, {"heading": "2 Entity Linking Task", "text": "The liaison task of the entity is to link a mention of a named entity contextualized in a particular document with a node in a provided knowledge base that describes the same real entity. If there is no such node, the entity should be linked to Nile. In this task, there are three main challenges: the first challenge is the ambiguity and diversity of names: the same entity can occur in different contexts with different meanings (e.g. Norfolk can refer to a city in the United States or the United Kingdom); furthermore, the same entity can be designated with different strings, including acronyms (USA) and nicknames (Uncle Sam); the second challenge is that the requested named entity cannot be found in the knowledge base at all; and the final challenge is to cluster all linked Nile mentions."}, {"heading": "2.1 System Processing", "text": "Our system consists of four level models, as in Figure 1: 1. Candidate generation: At this stage, all KB nodes that may be associated with the query unit are determined. 2. Nile predictor: At this stage, a binary classifier is used to decide whether the query mention should be linked to a KB node or not. 3. Candidate selection: At this stage, for each query mention to be associated with the KB, a candidate from the candidate group is selected as the link for the query mention.ar Xiv: 120 3.50 73v1 [cs.CL] 2 2M ar2 0124. Nil Mention Clustering: At this stage, all Nile-linked query mentions are grouped so that each cluster contains all mentions that should be linked to a single KB node, i.e. refer to the same unit."}, {"heading": "2.1.1 Candidate Generation", "text": "The main objective of the candidate generation process is to reduce the search space of potential link targets from the full KB to a small subset of plausible candidate nodes within the node. Mentioning the query is used both as a single phrase and as a set of its constituent tokens to search for the query string in the titles and texts of the KB node. Variant name extraction We extracted different name forms for mentioning the same entity from a Wikipedia dump. Hyperlinks, redirection pages and disambiguity pages are used to link different entity mentions to the same entity (Reddy et al., 2010; Varma et al., 2009). This repository of proposed name variants is then used in the query extension to extend the queries related to a given entity to all possible names. As mentioning the entity is not yet clearly defined, it is not necessary to ask all of the suggested name variants."}, {"heading": "2.1.2 Nil Prediction", "text": "We must recognize these cases and mark them with an NIL link. NIL link is assigned after a candidate list has been created (see Varma et al. (2009), Radford et al. (2010). If the created candidate list is empty, then the query list is associated with NIL. If the candidate list is not empty, we use two techniques to find a candidate, the first simply selects the candidate with the highest score, i.e. the candidate with the highest score based on the Lucene similarity value. If the highest score is above a threshold, then the candidate is selected, and if it is below the threshold, the predictor links the mention to NIL. The second technique calculates the difference between the results of two candidates with the highest score value, then compares that difference with some thresholds \u03b2; if the difference exceeds the threshold, the candidate with the highest score is selected, otherwise the mention is associated with NIL."}, {"heading": "2.1.3 Candidate Selection", "text": "The phase of candidate selection runs only on a non-empty candidate list, because an empty candidate list means that the search query is linked to NIL. For each search request, the candidate with the highest score is selected as the correct candidate."}, {"heading": "2.1.4 Nil Clustering", "text": "The Levenshtein distance is measured between the different mentions, and if the distance is below a threshold \u03b1, the mentions are grouped in the same clusters. Two experiments are carried out and the results are presented in Table 3. How clustering according to string equality achieves better results than a distance of one. Data set: The TAC2011 data set contains 2250 cases, of which 1126 must be linked to \"Nile.\" In the gold standard, the 1126 Nile instances are bundled into 814 clusters. Only these 1126 cases are sent to the cluster module to check its performance separately, regardless of Nile predictor performance. Evaluation formula: \"All Pair Counting Measures\" are used to evaluate the similarity between the results of two cluster systems. This metric examines how likely it is that the algorithms group or separate a pair of data points in different clusters."}, {"heading": "2.2 Evaluation", "text": "In this section, we provide a brief description of the various passes and their results. All experiments are evaluated using B-Cubed + and micro-average scoring metrics. In our setup, a threshold \u03b1 = 5.9 for Nile clustering is used in Nile Predictor and Levenshtein distance = 0. The default scoreboard published by the TAC organizers is used to evaluate each pass, with results in Table 4. Different query schemes are used in different passes as follows. 1. Wiki text is not used, with searches limited exclusively to node titles. 3. Wiki text is used, the search scheme used in this pass uses only query mentions. 2. Wiki text is used. The search scheme used in this pass uses the query mentions and the different name variants for the query mentions. 3. Wiki text is used, the search scheme used in this pass uses the query mentions and the different name variants for the query mentions."}, {"heading": "3 Slot Filling Task", "text": "For an organization, you can talk about its leaders, its size, and its place of origin. For an individual, you can talk about their gender, age, or religious orientation. These characteristic types can be considered \"slots\" whose values can be used to describe a company. In filling slots, the task is to find values for a set of slots for each individual list of entities, based on a knowledge base of structured data and a source collection of millions of documents with unstructured text. In this section, we discuss our approach to filling slots."}, {"heading": "3.1 System Processing", "text": "Our system is built like a pipeline: for each entity / slot pair, we first select documents likely to carry slot values, using query formulations (Section 3.1.2) and then information queries (Section 3.1.1), then we examine the best-placed returned texts and try to extract, using learned classifiers, all default entity mentions plus mentions of other entity types that may occur as slot values (Section 3.1.3), then we run a learned slot-specific relation extractor over the sentences that contain an occurrence of the target enterprise and an entity of the type needed as a value for the requested slot, resulting in a list of slot values for candidates (Section 3.1.4), then we assign these slot values and, in the case of slot values, return a slot value or a list of slot values from the best candidates (Section 3.1.5)."}, {"heading": "3.1.1 Preprocessing and Indexing", "text": "For the tasks of Slot Filling (SF) and Entity Linking (EL), primarily two variants of IR were used in the SF task: Document Retrieval (DR) and Passage Retrieval (PR).The documents were analyzed to extract text and its parent elements using JDOM and then indexed using Lucene. We used Lucene's default analyzer for indexing and stop word removal. The parent element of the text is used as the field name, giving the flexibility to search the document with fields and document structure and only with the body (BaezaYates et al., 1999). Instead of returning the text of the document, the pointers or paths of the document were returned when a search is performed. Lucene's default settings were used for searching and ranking."}, {"heading": "3.1.2 Query Formulation", "text": "(...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (. (). (). (). (). (). (...). (...). (). (). (). (). (...). (). (). (). (). (). (). (). (). ("}, {"heading": "3.1.3 Entity Identification", "text": "This year, it has reached the stage where it will be able to leave the country in which it is located, where it is a country in which it is a country in which it is not a country, but a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country"}, {"heading": "3.1.4 Candidate Slot Value Extraction", "text": "In fact, the fact is that most of them are able to move to a different world in which they are able than in another world in which they are able to live, in which they are able to live."}, {"heading": "3.1.5 Slot Value Selection", "text": "In the last phase of our system, from the slot values extracted by the relation extractor in the previous phase, we select which slot values of the candidates (or slot values in the case of list values) are returned as the correct answer. To do this, we classify the candidates identified in the phase of extracting the slot value of the candidates. In evaluating the candidates, two factors are taken into account: (1) the number of occurrences as a ranking factor and (2) the confidence score provided to each candidate by the relation extractor classifier. If a value occurs more than three times in the list of possible slot values, the system uses the number of occurrences as a ranking factor. Otherwise, the system uses the confidence score as a ranking factor. In the first case, the slot values are sorted by the number of preoccurrences. In the second case, the values are sorted based on the confidence score of the ranking factor."}, {"heading": "3.2 Evaluation", "text": "We evaluated both the overall slot filling performance and the performance of our query formulation / IR components in providing suitable slot filling data."}, {"heading": "3.2.1 Overall", "text": "We submitted three passes: one with document retrieval, no coreference resolution and sack-of-word extractor functions; a second with document retrieval, coreference resolution and n-gram functions; and a third with passage-level retrieval, corefer resolution and n-gram functions. Our results are presented in Table 5."}, {"heading": "3.2.2 Query Formulation/Document Retrieval Evaluation", "text": "We evaluated the formulation of queries and the retrieval of documents against the capture and redundancy measures introduced by Roberts and Gaizauskas (2004), which were originally designed to answer questions. Coverage is the proportion of questions for which answers can be found from the retrieved documents or passages, while redundancy is the average number of documents or passages retrieved that contain answers to a particular question or query. These measures can be directly transferred to the fill-in task, where we treat each slot as a question. In the evaluation, the 2010 TAC-KBP data were used for all entities and slots; the results are used in Table 6. Strict and lenient versions of each measure, with the strict measure requiring both the document identification and the response chain to match those of the gold standard, while the compliant one only has to match the response chain, i.e. the amount must be correct, but the document that does not contain a slot that is correct."}, {"heading": "3.2.3 Entity Extraction and Coreference Evaluation", "text": "We evaluated our entity extractor as follows: We selected one entity and one slot for ORGANIZATION entities and one slot for PERSON entities, and collected the top 20 documents returned by our query formulation and document retrieval system for each of these entity slot pairs. We commented all candidate slot values manually over these two twenty sets of documents to provide a small gold standard test set. For candidates who complete documents that match the ORGANIZATION query, the F measure for the entity identifier was a total of 91.07%. We also manually evaluated our co-ferencing approach using the same two sets of documents, and came to an F measure for candidates that matches the PERSON query, an F measure of 73.07% for Korea references to the target document correlation to the ORGERATION and the target document reference to the correlation to the correlation to the ORGERATION."}, {"heading": "3.2.4 Candidate Slot Value Extraction Evaluation", "text": "In order to evaluate our applicant slot value extraction process, we did two different things: First, we evaluated the quality of the training data provided by our remote monitoring approach. As it was impossible to verify all manually produced training data, we randomly assigned 40 positive examples for each of four slot types (slots that expect DATEs, PERSONs, LOCATIONs and ORGANIZATIONs) and 40 negative examples for each of four slot types. The results of this evaluation are shown in Table 7. In addition to evaluating the quality of the training data we generate, we performed some evaluations to determine the optimal combination of feature set combinations. Ten times cross-validation numbers for the optimal feature sets over the training data are shown in the first row in Table 8, again for a selection of one slot from each of four slot types. Finally, we evaluated the slot value extraction capabilities from a small sample set, and selected them correctly from a small sample set of small sample sets that correctly evaluated the quantum of the small sample sets they were evaluated from."}, {"heading": "4 Temporal Filling Task", "text": "The task is to identify upper and lower limits at the start and end times of a state indicated by a triple time specification of entity relation and filler, resulting in four data for each unique fill value. There are two time tasks, a complete time limit task and a diagnostic time limit task. We supply the fill values for the complete task, and TAC supplies the fill values and source document for the diagnostic task. Our time component did not distinguish between the two tasks; for the complete task, we used output values generated by our slot fill component. We approached this task by writing down source documents in TimeML (Pustejovsky et al., 2003), a modern standard for temporal semantic annotations. It was a mixture of standard components and user-defined code. After the document was noted, we attempted to describe the absolute time component to best relate the three-time event to the fill component."}, {"heading": "4.1 System Processing", "text": "We divide our processing into three parts: initial comment, selection of an event corresponding to the persistence of the filler value, and time considerations to detect the initial and final limits of this state."}, {"heading": "4.1.1 TimeML Annotation", "text": "We are also interested in events, as they can mean the beginning, the end or the whole persistence of a triple. Finally, we need to be able to determine the nature of the relationship between these times and events; TimeML uses TLINKs to comment on these relationships. We used a newer version of HeidelTime (Stro t gen and Gertz, 2010) to create TimeML-compliant temporal expressions (or Timex) annotations on the selected document, which required a document creation time (DCT) that best relates to the function. To do this, we built a regular expression based on DCT extractor2 and used it to create a DCT database of each document in the source collection (this failed with one of the 1,777,888 documents; manual verification did not include any clues as to its origin time."}, {"heading": "4.1.2 Event Selection", "text": "We call this task \"event selection.\" Our approach was simple. First, we searched for a TimeML event whose text matched the filler. Otherwise, we searched for sentences that contained the filler and selected an event in the same sentence. If none was found, we took all the text of the document and tried to find a simplified version of the filler text somewhere in the document; then we returned the next event to each mention. Finally, we tried to find the closest point in time to the filler text. If there was nothing yet, we gave up the slot."}, {"heading": "4.1.3 Temporal Reasoning", "text": "Given a TimeML note and an event, our task now is to figure out what time periods exist immediately before and after the event. We can recognize the 5LDC catalog entry LDC2006T08. 6http: / / www.nltk.org / by taking advantage of the commutative and transitive nature of some types of temporal relationships. To ensure that as many relationships are created between events and times as possible, we perform a temporal closure via the initial automatic annotation with the CAVaT consistency tool, ignoring inconsistent configurations.Generating time terminations is computationally laborious, we have reduced the size of the dataset by generating isolated groups of related events and times with the CAVaT subgraph modules, and then calculating the closure via precisely these \"nodes.\" We now have an event that represents the fill value 06, merging the times and gradient relationships with the beginning of the season, and the time-based events."}, {"heading": "4.2 Evaluation", "text": "Test and sampling data were available for the temporal task 7, including query sets, time slot annotations, and a shortcut file describing which time periods were considered related to filler. Distribution of slots in these data is in Table 9. To test the effectiveness of the system, we evaluated output performance with the query sets provided using these temporal slot annotations. Results are included in Table 10, including performance per slot. Results for the full slot fill task are in Table 11. This is based on accurate slot values and time constraints. An analysis of our approach to the diagnostic time task, perhaps using a corpus such as TimeBank, remains for future work."}, {"heading": "5 Conclusion", "text": "This framework has been created and applied to several KBP tasks. We have shown that our proposed framework is effective and suitable for joint development efforts, as well as useful in a teaching environment. Finally, we present results that, while very modest, offer improvements on a scale greater than our 2010 attempt (Yu et al., 2010)."}], "references": [{"title": "Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit. O\u2019Reilly Media", "author": ["Bird et al.2009] S. Bird", "E. Klein", "E. Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Analysing Temporally Annotated Corpora with CAVaT", "author": ["Derczynski", "Gaizauskas2010] L. Derczynski", "R. Gaizauskas"], "venue": "In Proceedings of the 7th LREC,", "citeRegEx": "Derczynski et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Derczynski et al\\.", "year": 2010}, {"title": "Incorporating non-local information into information extraction systems by Gibbs sampling", "author": ["Finkel et al.2005] J.R. Finkel", "T. Grenager", "C. Manning"], "venue": "In Proceedings of ACL,", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Overview of the TAC2011 Knowledge Base Population Track", "author": ["Ji et al.2011] H. Ji", "R. Grishman", "H.T. Dang", "X.S. Li", "K. Griffit", "J. Ellis"], "venue": "In Proc. Text Analytics Conference", "citeRegEx": "Ji et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2011}, {"title": "Three approaches to learning TLINKS in TimeML", "author": ["Mani et al.2007] I. Mani", "B. Wellner", "M. Verhagen", "J. Pustejovsky"], "venue": "Technical report,", "citeRegEx": "Mani et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mani et al\\.", "year": 2007}, {"title": "Distant supervision for relation extraction without labeled data", "author": ["Mintz et al.2009] M. Mintz", "S. Bills", "R. Snow", "D. Jurafsky"], "venue": "In Proceedings of the Joint ACL-IJCNLP Conference,", "citeRegEx": "Mintz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Mintz et al\\.", "year": 2009}, {"title": "TimeML: Robust specification of event and temporal expressions in text", "author": ["J. Castano", "R. Ingria", "R. Saur\u0131", "R. Gaizauskas", "A. Setzer", "G. Katz", "D. Radev"], "venue": "In IWCS-5 Fifth International Workshop on Computational Se-", "citeRegEx": "Pustejovsky et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pustejovsky et al\\.", "year": 2003}, {"title": "Document-level Entity Linking: CMCRC at TAC", "author": ["Radford et al.2010] W. Radford", "B. Hachey", "J. Nothman", "M. Honnibal", "J.R. Curran"], "venue": "In Proc. Text Analytics Conference", "citeRegEx": "Radford et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Radford et al\\.", "year": 2010}, {"title": "Objective criteria for the evaluation of clustering methods", "author": ["W.M. Rand"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Rand.,? \\Q1971\\E", "shortCiteRegEx": "Rand.", "year": 1971}, {"title": "Linking Named Entities to a Structured Knowledge Base", "author": ["K. Reddy", "K. Kumar", "S. Krishna", "P. Pingali", "V. Varma"], "venue": "In Proceedings of 11th International Conference on Intelligent Text Processing and Computational Linguistics", "citeRegEx": "Reddy et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2010}, {"title": "Evaluating Passage Retrieval Approaches for Question Answering", "author": ["Roberts", "Gaizauskas2004] I. Roberts", "R. Gaizauskas"], "venue": "Advances in Information Retrieval: Proceedings of the 26th European", "citeRegEx": "Roberts et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Roberts et al\\.", "year": 2004}, {"title": "Passage retrieval for question answering", "author": ["A. Sanka"], "venue": null, "citeRegEx": "Sanka.,? \\Q2005\\E", "shortCiteRegEx": "Sanka.", "year": 2005}, {"title": "Evita: a robust event recognizer for QA systems", "author": ["Saur\u0131\u0301 et al.2005] R. Saur\u0131", "R. Knippen", "M. Verhagen", "J. Pustejovsky"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Saur\u0131\u0301 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Saur\u0131\u0301 et al\\.", "year": 2005}, {"title": "HeidelTime: High quality rule-based extraction and normalization of temporal expressions", "author": ["Str\u00f6tgen", "Gertz2010] J. Str\u00f6tgen", "M. Gertz"], "venue": "In Proceedings of SemEval-2010,", "citeRegEx": "Str\u00f6tgen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Str\u00f6tgen et al\\.", "year": 2010}, {"title": "SemEval-2010 task 13: TempEval-2", "author": ["Verhagen et al.2010] M. Verhagen", "R. Sauri", "T. Caselli", "J. Pustejovsky"], "venue": "In Proceedings of SemEval-2010,", "citeRegEx": "Verhagen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Verhagen et al\\.", "year": 2010}, {"title": "The University of Sheffield System at TAC KBP", "author": ["J. Yu", "O. Mujgond", "R. Gaizauskas"], "venue": "In Proc. Text Analytics Conference", "citeRegEx": "Yu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 3, "context": "This paper describes the University of Sheffield\u2019s entry in the 2011 TAC KBP entity linking and slot filling tasks (Ji et al., 2011).", "startOffset": 115, "endOffset": 132}, {"referenceID": 9, "context": "Hyper-links, redirect pages and disambiguation pages are used to associate different named entity mentions with the same entity (Reddy et al., 2010; Varma et al., 2009).", "startOffset": 128, "endOffset": 168}, {"referenceID": 2, "context": "The second strategy uses additional named entity (NE) mentions for query expansion: the Stanford NE recognizer (Finkel et al., 2005) is used to find NE mentions in the query document, and generates a query containing the query entity mention plus all the NE mentions found in the query document,", "startOffset": 111, "endOffset": 132}, {"referenceID": 7, "context": "(2009), Radford et al. (2010)).", "startOffset": 8, "endOffset": 30}, {"referenceID": 8, "context": "The Rand index (Rand, 1971) computes similarity between the system output clusters (output of the clustering algorithm) and the clusters found in a gold standard.", "startOffset": 15, "endOffset": 27}, {"referenceID": 2, "context": "Named Entity Recognition The Stanford Named Entity Recognition (NER) tool (Finkel et al., 2005) was used to find named entities.", "startOffset": 74, "endOffset": 95}, {"referenceID": 0, "context": "Because we needed a broader range of entity classes we re-trained the classifier using the MUC6 and MUC7 datasets 1 and NLTK (Bird et al., 2009) gazetteers.", "startOffset": 125, "endOffset": 144}, {"referenceID": 5, "context": ", Mintz et al. (2009)).", "startOffset": 2, "endOffset": 22}, {"referenceID": 11, "context": "This follows the original strict and lenient measures implemented in the tool we used to assist evaluation, IR4QA (Sanka, 2005).", "startOffset": 114, "endOffset": 127}, {"referenceID": 6, "context": "We approached this task by annotating source documents in TimeML (Pustejovsky et al., 2003), a modern standard for temporal semantic annotation.", "startOffset": 65, "endOffset": 91}, {"referenceID": 12, "context": "The only off-the-shelf TimeML event annotation tool found was Evita (Saur\u0131\u0301 et al., 2005), which requires some preprocessing.", "startOffset": 68, "endOffset": 89}, {"referenceID": 14, "context": "be linked into two sets, as per TempEval (Verhagen et al., 2010): intra-sentence event-time links, and inter-sentence event-event links with a 3-sentence window.", "startOffset": 41, "endOffset": 64}, {"referenceID": 4, "context": "Our feature set was the same used as Mani et al. (2007) which relied on surface data available from any TimeML annotation.", "startOffset": 37, "endOffset": 56}, {"referenceID": 15, "context": "Finally we present results that, while very modest, provide improvements an order of magnitude greater than our 2010 attempt (Yu et al., 2010).", "startOffset": 125, "endOffset": 142}], "year": 2012, "abstractText": "This paper describes the University of Sheffield\u2019s entry in the 2011 TAC KBP entity linking and slot filling tasks (Ji et al., 2011). We chose to participate in the monolingual entity linking task, the monolingual slot filling task and the temporal slot filling tasks. Our team consisted of five MSc students, two PhD students and one more senior academic. For the MSc students, their participation in the track formed the core of their MSc dissertation project, which they began in February 2011 and finished at the end of August 2011. None of them had any prior experience in human language technologies or machine learning before their programme started in October 2010. For the two PhD students participation was relevant to their ongoing PhD research. This team organization allowed us to muster considerable manpower without dedicated external funding and within a limited period time; but of course there were inevitable issues with co-ordination of effort and of getting up to speed. The students found participation to be an excellent and very enjoyable learning experience. Insofar as any common theme emerges from our approaches to the three tasks it is an effort to learn from and exploit data wherever possible: in entity linking we learn thresholds for nil prediction and acquire lists of name variants from data; in slot filling we learn entity recognizers and relation extractors; in temporal slot filling we use time and event annotators that are learned from data. The rest of this paper describes our approach and related investigations in more detail. Sections 2 and 3 describe in detail our approaches to the EL and SF tasks respectively, and Section 4 summarises our temporal slot filling approach.", "creator": "LaTeX with hyperref package"}}}