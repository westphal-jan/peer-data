{"id": "1203.5084", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Mar-2012", "title": "A Data Driven Approach to Query Expansion in Question Answering", "abstract": "Automated answering of natural language questions is an interesting and useful problem to solve. Question answering (QA) systems often perform information retrieval at an initial stage. Information retrieval (IR) performance, provided by engines such as Lucene, places a bound on overall system performance. For example, no answer bearing documents are retrieved at low ranks for almost 40% of questions.", "histories": [["v1", "Thu, 22 Mar 2012 19:19:02 GMT  (24kb)", "http://arxiv.org/abs/1203.5084v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["leon derczynski", "jun wang", "robert gaizauskas", "mark a greenwood"], "accepted": false, "id": "1203.5084"}, "pdf": {"name": "1203.5084.pdf", "metadata": {"source": "CRF", "title": "A Data Driven Approach to Query Expansion in Question Answering", "authors": ["Leon Derczynski", "Jun Wang", "Robert Gaizauskas"], "emails": ["m.greenwood}@shef.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 120 3.50 84v1 [cs.CL] 2 2M arIn this paper, response texts from previous QA reviews at the Text Retrieval Conferences (TREC) are paired with queries and analyzed to identify performance enhancing words, which are then used to evaluate the performance of a query enhancement method. Data-driven extension words that can be used to improve and evaluate query enhancement methods have been found in over 70% of the difficult questions, and simple blind relevance feedback (RF) has been correctly predicted as unlikely to improve overall performance, and a possible explanation for its low IR value for QA is given."}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Background and Related Work", "text": "The performance of an IR system can be quantified in many ways. We select and define measures that are relevant for quality assurance. An IR-specific relevance feedback for quality assurance has been worked on, where it has generally proved unhelpful. We outline the methods used in the past, extend them and provide means to validate and test QA relevance feedback."}, {"heading": "2.1 Measuring QA Performance", "text": "In this paper, two basic measures are used to describe the performance of the IR component. Coverage is defined as the proportion of questions where at least one answer with text appears in the retrieved group. Redundancy is the average number of texts bearing answers that are retrieved for each question (Roberts and Gaizauskas, 2004).Both measures have a fixed limit n for the number of texts retrieved by a search engine for a query. Since redundancy counts the number of texts that contain correct answers, and not the instances of the answer itself, it can never be greater than the number of texts retrieved. The TREC reference answers offer two ways to find a correct text, both with a regular expression and with a document ID. Retrieved hits (retrieval of answer-bearing documents) are those where the retrieved text matches the regular expression; strict hits occur when the document ID of the retrieved text is found to be incorrectly decled from the document, but some do not match the document as a dangerous expression."}, {"heading": "2.2 Relevance Feedback", "text": "Relevance feedback is a widely researched query enhancement technique. It is often performed using a specific metric to select terms based on a limited number of ranking documents of size r; the use of a larger set brings the term distribution closer to values across the entire corpus and away from values in documents relevant to query terms (Roussinov and Fan, 2005; Allan, 1996). In the context of QA, Pizzato (2006) uses blind RF using the AQUAINT corpus in an attempt to improve performance in answering factual questions about personal names. This is a similar approach to some of the content in this paper, although it is limited to studying named units and does not attempt to examine extensions from existing response files. Monz (2003) finds a negative result in applying blind feedback for QA in TREC 11 and a reduction of REC 11 and a neutral term for TREC 10 and TREC 10 tasks available."}, {"heading": "3 Methodology", "text": "First, we investigated the possibility of an IR component-specific error that could affect coverage by testing a variety of IR motors and configurations, and then identified difficult questions based on various power thresholds. Next, answers to these more difficult questions were checked for words that increased performance as the query expanded, and then we assessed the likelihood of success of an RF-based approach. Finally, blind RF was applied to the entire questionnaire, measuring IR performance and comparing the terms used for RF with those that proved helpful as extension terms."}, {"heading": "3.1 IR Engines", "text": "In this case, a QR system has been set up in which one person is able to transform into another, namely, into a group of people who are able to integrate into a group, and into a group of people who are able to integrate into a group."}, {"heading": "3.2 Identification of Difficult Questions", "text": "Once the performance of an IR configuration is known via a questionnaire, it is possible to create a simple report listing redundancy for each question. A performance report script accesses the database of the FA tool and lists all the questions in a given sentence with the strict and forgiving redundancy for selected motors and configurations. Motors can use configurations at the passage or document level. 1Save Terrier / TREC2004 / Passage Level retrieval with Terrier was very slow using our configuration and could not reliably be executed with the same terrier instance as document level retrieval. Data on the performance of the three motors is described in Table 2. As you can see, the coverage with Passage Level retrieval (which was often preferred because the AE component works best with a reduced amount of text)."}, {"heading": "3.3 Extension of Difficult Questions", "text": "The documents considered relevant by TREC must contain some useful texts that may help the IR engine performance. Such words should be disclosed by gaining redundancy when used to extend an initially difficult query, which is usually characterized by a change from zero to a non-zero value (meaning that relevant documents were found where none were previously). In an attempt to find out where the useful text is, the relevant documents were retrieved for each difficult question, and passages that correspond to the regular expression of the answer were identified. A script is then used to create a list of terms from each passage, removing words in the answer or its destination, words that occur in the answer, and stopwords (based on the indexing of the stopword list, and a set of strains common within the corpus). In later runs, numbers are also removed from the term list, their value being nebulous as often as the 1999 question is, as well as the question to be submitted."}, {"heading": "3.4 Relevance Feedback Performance", "text": "Now that we can find the helpful expansion terms (HEWs) described above, we are equipped to evaluate the expansion methods of the query. A simplistic approach could use blind RF to determine the candidate extensions and be considered potentially successful if these terms are found in the set of HEWs for a query. To this end, term frequencies can be measured using the Top-R documents retrieved using anaphora-resolved query Q. After stop word and question word removal, frequent terms are appended to Q, which are then re-evaluated. This has already been attempted on factual questions (Roussinov et al., 2005) and with a limited range of r-values (Monz, 2003), but not validated with the help of a set of data-driven terms. We investigated how likely Term Frequency (TF) based RF is to detect HEWs. To do this, the portion of the HEW was originally called, as well as the percentage of the HEW occurred."}, {"heading": "4 Results", "text": "Once we have HEWs, we can determine whether they will be of significant help as query extensions. We can also determine whether a query extension method is likely to be fruitful. Blind RF has been applied and evaluated against the helpful wordlist, as well as the effects of RF on coverage."}, {"heading": "4.1 Difficult Question Analysis", "text": "The number of difficult questions found at n = 20 is shown in Table 3. Document-level queries yielded much less difficult questions, since the amount of text retrieved was more likely to find forgiving matches. A comparison of the strict and forgiving matches can be found in Table 5. Extensions were then applied to difficult questions, with or without the goal. The performance of these extensions is shown in Table 6. Results show that a significant proportion (74.4%) of difficult questions can benefit from adding unanswerable words found in response texts."}, {"heading": "4.2 Applying Relevance Feedback", "text": "Using simple TF-based feedback (see Section 3.4), 5 terms were selected for each query that had some overlap (see Table 7) with the extension words set, suggesting that this RF may increase performance for previously invisible questions. However, only a small number of RF terms are likely to occur in the originally retrieved texts (IRT), although a perceptible proportion of IRT terms (up to 34.29%) contains at least one RF. However, these terms are unlikely to be very common in the documents and are unlikely to be selected using TF-based blind RF terms. The mean percentage of selected RF terms for which HEW was used was only 2.88%. Blind RF for answering questions fails here due to this small proportion. Strict measures are used for evaluation as we are interested in finding documents that were not retrieved previously, rather than changes in the distribution of key words, to find better terms in the RTF document and RF fail terms."}, {"heading": "5 Discussion", "text": "Most difficult questions can be answered by adding HEWs that are included in the answer, and the work to find a relationship between words found in the first query and those HEWs leads to increases. HEWs also provide an effective means of evaluating other RF methods that can be converted into a generic quick test tool for querying expansion techniques. TF-based RF is not effective in detecting extensions, and reduces overall IR performance. There has been no major performance change between motors and configurations. Strict paragraphs where coverage of expansion techniques has never been achieved."}, {"heading": "6 Conclusion and Future Work", "text": "The paragraph-level query seems to give a better idea of which questions are the most difficult, although there is the possibility of false negatives from answer lists and anaphorical solutions. Identifying HEWs has a potential benefit in query expansion. It could be used to evaluate RF approaches or associate them with question words and use them as extensions. Previous work has excluded relevance feedback using a single ranking in certain circumstances, although this is not based on analyzing responsive texts. The presence of HEWs in IRT for difficult questions shows that guided RF approaches can work, but this will be difficult to track a query."}], "references": [{"title": "The Lemur toolkit for language modeling and information retrieval", "author": ["J. Allan et al.2003] Allan", "J. Callan", "K. CollinsThompson", "B. Croft", "F. Feng", "D. Fisher", "J. Lafferty", "L. Larkey", "TN Truong", "P. Ogilvie"], "venue": null, "citeRegEx": "Allan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Allan et al\\.", "year": 2003}, {"title": "Incremental relevance feedback for information filtering", "author": ["J. Allan"], "venue": "In Research and Development in IR,", "citeRegEx": "Allan,? \\Q1996\\E", "shortCiteRegEx": "Allan", "year": 1996}, {"title": "What works better for question answering: Stemming or morphological query expansion", "author": ["M.W. Bilotti et al.2004] Bilotti", "B. Katz", "J. Lin"], "venue": "Proc. IR for QA Workshop at SIGIR", "citeRegEx": "Bilotti et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bilotti et al\\.", "year": 2004}, {"title": "The university of sheffield\u2019s trec 2006 q&a experiments", "author": ["M. Stevenson", "R. Gaizauskas"], "venue": "In Proc. 15th Text REtrieval Conf", "citeRegEx": "Greenwood et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Greenwood et al\\.", "year": 2006}, {"title": "Answerfinder: Question answering from your desktop", "author": ["M.A. Greenwood"], "venue": "In Proc. 7th Annual Colloquium for the UK SIG for Computational Linguistics", "citeRegEx": "Greenwood,? \\Q2004\\E", "shortCiteRegEx": "Greenwood", "year": 2004}, {"title": "Using pertainyms to improve passage retrieval for questions requesting information about a location", "author": ["M.A. Greenwood"], "venue": "In Proc. Workshop on IR for QA (SIGIR", "citeRegEx": "Greenwood,? \\Q2004\\E", "shortCiteRegEx": "Greenwood", "year": 2004}, {"title": "Building a reusable test collection for question answering", "author": ["Lin", "J. Katz2005] Lin", "B. Katz"], "venue": "J. American Society for Information Science and Technology", "citeRegEx": "Lin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2005}, {"title": "Pseudo-relevance feedback using named entities for question answering", "author": ["L.A. Pizzato et al.2006] Pizzato", "D. Molla", "C. Paris"], "venue": "Australasian Language Technology Workshop", "citeRegEx": "Pizzato et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pizzato et al\\.", "year": 2006}, {"title": "An Algorithm for Suffix Stripping Program", "author": ["M. Porter"], "venue": null, "citeRegEx": "Porter,? \\Q1980\\E", "shortCiteRegEx": "Porter", "year": 1980}, {"title": "Evaluating passage retrieval approaches for question answering", "author": ["Roberts", "I Gaizauskas2004] Roberts", "R Gaizauskas"], "venue": "In Proc. 26th European Conf. on IR", "citeRegEx": "Roberts et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Roberts et al\\.", "year": 2004}, {"title": "Discretization based learning approach to information retrieval", "author": ["Roussinov", "D. Fan2005] Roussinov", "W. Fan"], "venue": "In Proc. 2005 Conf. on Human Language Technologies", "citeRegEx": "Roussinov et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Roussinov et al\\.", "year": 2005}, {"title": "Building on redundancy: Factoid question answering, robust retrieval and the other", "author": ["M. Chau", "E. Filatova", "J.A. Robles-Flores"], "venue": "In Proc. 14th Text REtrieval Conf", "citeRegEx": "Roussinov et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Roussinov et al\\.", "year": 2005}, {"title": "Quantitative evaluation of passage retrieval algorithms for question answering", "author": ["S. Tellex et al.2003] Tellex", "B. Katz", "J. Lin", "A. Fernandes", "G. Marton"], "venue": "Proc. 26th Annual Int\u2019l ACM SIGIR Conf. on R&D in IR,", "citeRegEx": "Tellex et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Tellex et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "Techniques are used to identify phrases relevant to a query topic, in order to reduce noise (such as terms with a low corpus frequency that relate to only a single article) and query drift (Roussinov and Fan, 2005; Allan, 1996).", "startOffset": 189, "endOffset": 227}, {"referenceID": 0, "context": "The chosen additional engines were: Indri, based on the mature INQUERY engine and the Lemur toolkit (Allan et al., 2003); and Terrier, a newer engine designed to deal with corpora in the terabyte range and to back applications entered into TREC conferences (Ounis et al.", "startOffset": 100, "endOffset": 120}, {"referenceID": 8, "context": "The initial IR component configuration was with Lucene indexing the AQUAINT corpus at passage-level, with a Porter stemmer (Porter, 1980) and an augmented version of the CACM (Jones and van Rijsbergen, 1976) stopword list.", "startOffset": 123, "endOffset": 137}, {"referenceID": 3, "context": "AE components begin to fail due to excess noise when presented with over 20 texts, so this value is enough to encompass typical operating parameters and leave space for discovery (Greenwood et al., 2006).", "startOffset": 179, "endOffset": 203}, {"referenceID": 2, "context": "Not all documents containing answers are noted, only those checked by the NIST judges (Bilotti et al., 2004).", "startOffset": 86, "endOffset": 108}, {"referenceID": 12, "context": "Work has been done into creating synthetic corpora that include exhaustive answer sets (Bilotti, 2004; Tellex et al., 2003; Lin and Katz, 2005), but for the sake of consistency, and easy comparison with both parallel work and prior local results, the TREC judgements will be used to evaluate", "startOffset": 87, "endOffset": 143}, {"referenceID": 10, "context": "Addition of the target often leads to gains in performance (Roussinov et al., 2005), and may also aid in cases where anaphora resolution has failed.", "startOffset": 59, "endOffset": 83}, {"referenceID": 10, "context": "This has been previously attempted for factoid questions (Roussinov et al., 2005) and with a limited range of r values (Monz, 2003) but not validated", "startOffset": 57, "endOffset": 81}, {"referenceID": 4, "context": "This reconfirms work done by Greenwood (2004b).", "startOffset": 29, "endOffset": 47}], "year": 2012, "abstractText": "Automated answering of natural language questions is an interesting and useful problem to solve. Question answering (QA) systems often perform information retrieval at an initial stage. Information retrieval (IR) performance, provided by engines such as Lucene, places a bound on overall system performance. For example, no answer bearing documents are retrieved at low ranks for almost 40% of questions. In this paper, answer texts from previous QA evaluations held as part of the Text REtrieval Conferences (TREC) are paired with queries and analysed in an attempt to identify performance-enhancing words. These words are then used to evaluate the performance of a query expansion method. Data driven extension words were found to help in over 70% of difficult questions. These words can be used to improve and evaluate query expansion methods. Simple blind relevance feedback (RF) was correctly predicted as unlikely to help overall performance, and an possible explanation is provided for its low value in IR for QA.", "creator": "LaTeX with hyperref package"}}}