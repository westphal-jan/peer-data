{"id": "1203.5262", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Mar-2012", "title": "ASR Context-Sensitive Error Correction Based on Microsoft N-Gram Dataset", "abstract": "At the present time, computers are employed to solve complex tasks and problems ranging from simple calculations to intensive digital image processing and intricate algorithmic optimization problems to computationally-demanding weather forecasting problems. ASR short for Automatic Speech Recognition is yet another type of computational problem whose purpose is to recognize human spoken speech and convert it into text that can be processed by a computer. Despite that ASR has many versatile and pervasive real-world applications,it is still relatively erroneous and not perfectly solved as it is prone to produce spelling errors in the recognized text, especially if the ASR system is operating in a noisy environment, its vocabulary size is limited, and its input speech is of bad or low quality. This paper proposes a post-editing ASR error correction method based on MicrosoftN-Gram dataset for detecting and correcting spelling errors generated by ASR systems. The proposed method comprises an error detection algorithm for detecting word errors; a candidate corrections generation algorithm for generating correction suggestions for the detected word errors; and a context-sensitive error correction algorithm for selecting the best candidate for correction. The virtue of using the Microsoft N-Gram dataset is that it contains real-world data and word sequences extracted from the web which canmimica comprehensive dictionary of words having a large and all-inclusive vocabulary. Experiments conducted on numerous speeches, performed by different speakers, showed a remarkable reduction in ASR errors. Future research can improve upon the proposed algorithm so much so that it can be parallelized to take advantage of multiprocessor and distributed systems.", "histories": [["v1", "Fri, 23 Mar 2012 14:51:05 GMT  (832kb)", "http://arxiv.org/abs/1203.5262v1", "LACSC - Lebanese Association for Computational Sciences -this http URL"]], "COMMENTS": "LACSC - Lebanese Association for Computational Sciences -this http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["youssef bassil", "paul semaan"], "accepted": false, "id": "1203.5262"}, "pdf": {"name": "1203.5262.pdf", "metadata": {"source": "CRF", "title": "ASR Context-Sensitive Error Correction Based on Microsoft N-Gram Dataset", "authors": ["Youssef Bassil", "Paul Semaan"], "emails": [], "sections": [{"heading": null, "text": "In fact, most people are able to solve problems related to different fields and domains. ASR short for Automatic Speech Recognition has been an object of great focus and attention in recent years, as it has been studied and researched by various scientists, universities and research centers. ASR converts spoken words into text that can be processed by a computer. Speech-ToText (STT), Automated Telephone Services (ATS), Voice Interface (VUI), Voice-driven Home (VUI)."}, {"heading": "4 LIMITATIONS OF ASR BUILT-INDICTIONARIES", "text": "The reasons behind OOV are as follows: The first reason is a comprehensive dictionary that can cover every single word in its entirety, and there may be a turning point in determining the overall error rate of the system. Applications with fewer terms like \"yes\" and \"no,\" or numbers like \"1,2,3... 9\" are easier to handle than those with large terms like continuous dictation systems, which sometimes require the recognition of terms and words. Such systems are called LVCSR, which do not cover all words in the language, but usually have to distinguish between 20,000 and 60,000 terms, while achieving a good level of accuracy and a minimum of errors. Since ASR systems are often built on traditional words that do not all words in the language suffer, theories suffer."}, {"heading": "6.1 The Error Detection Algorithm", "text": "\"The errors are listed in the ASR output text list.\" \"Formally, these errors are E = {e1, e2, e3, ep}, where edenotes is a non-word error, and p denotes the total number of detected errors.\" The ASR output text is typed from A = {a1, a2, a3, at}, where a word or term is found in the ASRoutput text, and tis the total number of words. The algorithm works as follows: it starts validating each word in Aagainst Microsoft Web N-Gram Dataset; if an entry for ai is found in the dataset, then ai is spelled correctly in the dataset output text correction, and therefore no spell correction is required. In contrast, if no entry for the word aiin exists in the dataset, then it is assumed to be spelled incorrectly, and spelling correction is required."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was conducted by the Lebanese Association for Computational Sciences (LACSC), Beirut, Lebanon under the \"Web \u2010 Scale Speech Recognition Research Project - WSSRRP2011.\" REFERENCES [1] T.Dutoit, An Introduction to Text \u2010 to-Speech Synthesis (Text, Speech and Language Technology), Springer, 1sted, 1997. [2] L. Deng, X. Huang, \"Challenges in Adapting Speech Recognition,\" Communications of the ACM, vol. 47, no. 1, pp. 60- 75, 2004. M. Forsberg \"Why Speech Recognition is Difficult, J.J. Rudnicky, Alexander G. Hauptmann, Kaifu Lee\" Survey of Current Speech Technology, \"Communications of the ACM, vol. 37, pp. 57, 1994."}], "references": [{"title": "Challenges in adopting speech recognition", "author": ["L. Deng", "X. Huang"], "venue": "Communications of the ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Speech Recognition is Difficult", "author": ["M. Forsberg", "\u201cWhy"], "venue": "Chalmers University of Technology,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Survey of Current Speech Technology", "author": ["Alexander I. Rudnicky", "Alexander G. Hauptmann", "Kaifu Lee"], "venue": "Communications of the ACM,vol", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["Schuetze Manning"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Applied Speech and Audio Processing: With Matlab Examples", "author": ["Ian McLoughlin"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "A tutorial on Hidden Markov Models and selected applications in speech recognition", "author": ["Lawrence R. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77,no", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "Hidden Markov Models for Speech Recognition, Edinburgh", "author": ["X. Huang", "M. Jack", "Y. Ariki"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1990}, {"title": "Essaid\u2019unerecherchestatistiquesur le texte du roman \u201cEug\u00e8neOneguine\u201d", "author": ["A.A. Markov"], "venue": "Bull. Acad. Imper. Sci. St. Petersburg,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1913}, {"title": "Effect of Error Correction Strategy on Speech Dictation Throughput", "author": ["J.R. Lewis"], "venue": "Proceedings of the Human Factors and Ergonomics Society,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1999}, {"title": "Correcting recognition errors via discriminative utterance verification", "author": ["A.R. Setlur", "R.A. Sukkar", "J. Jacob"], "venue": "In Proceedings of the International Conference on Spoken Language Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "A multi\u2010pass error detection and correction framework for Mandarin LVCSR", "author": ["Z. Zhou", "H.M. Meng", "W.K. Lo"], "venue": "In Proceedings of the International Conference on Spoken Language Processing,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Error corrective mechanisms for speech recognition", "author": ["L. Mangu", "M. Padmanabhan"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "A method for correcting errors in speech recognition using the statistical features of character co\u2010occurrence", "author": ["S. Kaki", "E. Sumita", "H. Iida"], "venue": "In COLING\u2010ACL,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Speech recognition error correction using maximum entropy language model", "author": ["S. Jung", "M. Jeong", "G.G. Lee"], "venue": "In Proceedings of the International Conference on Spoken Language Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Context\u2010based speech recognition error detection and correction", "author": ["A. Sarma", "D.D. Palmer"], "venue": "In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2004}, {"title": "Error correction via a post\u2010 processor for continuous speech recognition", "author": ["E.K. Ringger", "J.F. Allen"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1996}, {"title": "A Fertility Channel Model for Post\u2010 Correction of Continuous Speech Recognition", "author": ["E.K. Ringger", "J.F. Allen"], "venue": "In Proceedings of the Fourth International Conference on Spoken Language Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "Improving speech recognition through text\u2010based linguistic post\u2010processing", "author": ["R.L.Brandow", "T. Strzalkowski"], "venue": "United States, Patent 6064957,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2000}], "referenceMentions": [{"referenceID": 0, "context": "These errors are often caused by the extreme noise in theenvironment, the bad quality of the speech, the fluctuating utterance of the dialogue, and the small size of the ASR vocabulary [2], [3].", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "These errors are often caused by the extreme noise in theenvironment, the bad quality of the speech, the fluctuating utterance of the dialogue, and the small size of the ASR vocabulary [2], [3].", "startOffset": 190, "endOffset": 193}, {"referenceID": 2, "context": "Numerous error-correction methods and algorithms were devised to help fight against ASR errors, some of themrely on post-processing the output text and correcting it manually;whereas others rely on building improved acoustic models to increase the precision of speech recognition [4].", "startOffset": 280, "endOffset": 283}, {"referenceID": 3, "context": "As defined by many textbooks [7], [8], and [9], automatic speech recognition systems also known as ASR, receive some speech signals as input and generate a corresponding readable text transcript as output.", "startOffset": 34, "endOffset": 37}, {"referenceID": 4, "context": "As defined by many textbooks [7], [8], and [9], automatic speech recognition systems also known as ASR, receive some speech signals as input and generate a corresponding readable text transcript as output.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Essentially, an ASR system is often implemented using a Hidden Markov Model  (HMM)  [10],  [11] based on  the notion  of  noisy  channel  [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 6, "context": "Essentially, an ASR system is often implemented using a Hidden Markov Model  (HMM)  [10],  [11] based on  the notion  of  noisy  channel  [12].", "startOffset": 91, "endOffset": 95}, {"referenceID": 7, "context": "P(W)  is  usually  calculated  using  the probabilistic  n\u2010gram model  [13] which predicts  the next word, letter, or phone in a given sequence.", "startOffset": 71, "endOffset": 75}, {"referenceID": 8, "context": "An early experiment conducted at IBM research labs [14] to  calculate  the  number  of  errors  generated  by  ASR systems,  showed  an  average  of  105  errors  being committed  per  minute.", "startOffset": 51, "endOffset": 55}, {"referenceID": 9, "context": "In  that  context,  Setlur,  Sukkar, andJacob[15]  proposed  an  algorithm  that  treats  each utterance of the spoken word as hypothesis and assigns it a confidence score during the recognition.", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "Likewise, Zhou, Meng,  and Lo[16] proposed  another  algorithm  to detect  and  correct misspellings  in ASR  systems.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "In  this  perspective,  Mangu  and Padmanabhan[17]  proposed  a  transformation\u2010based learning  algorithm  for  ASR  error  correction.", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "Iida[18] proposed an error correction algorithm based on  pattern  learning  to  detect  misspellings  and  on similarity  string  matching  algorithm  to  correct misspellings.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "Jung, Jeong, and Lee[19] employed the noisy channel model to detect error patterns  in the output text.", "startOffset": 20, "endOffset": 24}, {"referenceID": 14, "context": "Furthermore, Sarma  and Palmer[20] proposed  a method  for detecting errors based on  statistical  co\u2010occurrence of words  in  the output  transcript.", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "As  an  initial  attempt, Ringger  and Allen[21] proposed  a post\u2010processor model for  discovering  statistical  error  patterns  and  correct errors.", "startOffset": 44, "endOffset": 48}, {"referenceID": 16, "context": "On the other hand, Ringger and  Allen[22]  proposed  a  post\u2010editing  model  named SPEECHPP  to  correct  word  errors  generated  by  ASR systems.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "Another attempt was presented by Brandow and Strzalkowski[23] in whichthe text generated by the ASR system is collected and  aligned  with  the  correct  transcription  of  the  same text.", "startOffset": 57, "endOffset": 61}], "year": 2012, "abstractText": "At the present time, computers are employed to solve complex tasks and problems ranging from simple calculations to intensive digital image processing and intricate algorithmic optimization problems to computationally-demanding weather forecasting problems. ASR short for Automatic Speech Recognition is yet another type of computational problem whose purpose is to recognize human spoken speech and convert it into text that can be processed by a computer. Despite that ASR has many versatile and pervasive real-world applications,it is still relatively erroneous and not perfectly solved as it is prone to produce spelling errors in the recognized text, especially if the ASR system is operating in a noisy environment, its vocabulary size is limited, and its input speech is of bad or low quality. This paper proposes a post-editing ASR error correction method based on MicrosoftN-Gram dataset for detecting and correcting spelling errors generated by ASR systems. The proposed method comprises an error detection algorithm for detecting word errors; a candidate corrections generation algorithm for generating correction suggestions for the detected word errors; and a context-sensitive error correction algorithm for selecting the best candidate for correction. The virtue of using the Microsoft N-Gram dataset is that it contains real-world data and word sequences extracted from the web which canmimica comprehensive dictionary of words having a large and all-inclusive vocabulary. Experiments conducted on numerous speeches, performed by different speakers, showed a remarkable reduction in ASR errors. Future research can improve upon the proposed algorithm so much so that it can be parallelized to take advantage of multiprocessor and distributed systems.", "creator": "PScript5.dll Version 5.2.2"}}}