{"id": "1203.5443", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Mar-2012", "title": "Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA", "abstract": "An automated technique has recently been proposed to transfer learning in the hierarchical Bayesian optimization algorithm (hBOA) based on distance-based statistics. The technique enables practitioners to improve hBOA efficiency by collecting statistics from probabilistic models obtained in previous hBOA runs and using the obtained statistics to bias future hBOA runs on similar problems. The purpose of this paper is threefold: (1) test the technique on several classes of NP-complete problems, including MAXSAT, spin glasses and minimum vertex cover; (2) demonstrate that the technique is effective even when previous runs were done on problems of different size; (3) provide empirical evidence that combining transfer learning with other efficiency enhancement techniques can often provide nearly multiplicative speedups.", "histories": [["v1", "Sat, 24 Mar 2012 20:11:21 GMT  (79kb)", "https://arxiv.org/abs/1203.5443v1", "Submitted to Parallel Problem Solving from Nature (PPSN XII), 10 pages. arXiv admin note: substantial text overlap witharXiv:1201.2241"], ["v2", "Thu, 21 Jun 2012 12:47:30 GMT  (79kb)", "http://arxiv.org/abs/1203.5443v2", "Accepted at Parallel Problem Solving from Nature (PPSN XII), 10 pages. arXiv admin note: substantial text overlap witharXiv:1201.2241"]], "COMMENTS": "Submitted to Parallel Problem Solving from Nature (PPSN XII), 10 pages. arXiv admin note: substantial text overlap witharXiv:1201.2241", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["martin pelikan", "mark w hauschild", "pier luca lanzi"], "accepted": false, "id": "1203.5443"}, "pdf": {"name": "1203.5443.pdf", "metadata": {"source": "CRF", "title": "Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA", "authors": ["Martin Pelikan", "Mark W. Hauschild"], "emails": ["medal@medal-lab.org", "martin@martinpelikan.net", "mwh308@umsl.edu", "pierluca.lanzi@polimi.it"], "sections": [{"heading": null, "text": "ar Xiv: 120 3.54 43v2 [cs.NE] Recently, an automated technique was proposed to transfer learning to the hierarchical Bayesian optimization algorithm (hBOA) based on distance-based statistics, which enables practitioners to improve the efficiency of hBOA by collecting statistics from probable models of previous hBOA runs and using the obtained statistics to distort future hBOA runs to similar problems. The purpose of this paper is threefold. (1) Test the technique on several classes of NP-complete problems, including MAXSAT, spin glasses and minimal vertex coverage; (2) show that the technique is effective even when previous runs have been performed on problems of varying sizes; (3) provide empirical evidence that combining transfer learning with other techniques to increase efficiency can often lead to near multiplicate speeds."}, {"heading": "Keywords", "text": "Transfer Learning, Inductive Transfer, Learning from Experience, Estimation of Distribution Algorithms, Hierarchical Bayesian Optimization Algorithm, Detachable Problems, Efficiency Increase.Missouri Estimation of Distribution Algorithms Laboratory (MEDAL) Department of Mathematics and Computer Science, 321 ESH University of Missouri-St. Louis One University Blvd., St. Louis, MO 63121 E-mail: medal @ medal-lab.org WWW: http: / / medal-lab.org /"}, {"heading": "Transfer Learning, Soft Distance-Based Bias, and", "text": "the hierarchical BOA"}, {"heading": "Martin Pelikan", "text": "Missouri Estimation of Distribution Algorithms Laboratory (MEDAL), Department of Mathematics and Computer Science, 320 ESHUniversity of Missouri at St. Louis One University Blvd., St. Louis, MO 63121martin @ martinpelikan.nethttp: / / martinpelikan.net /"}, {"heading": "Mark W. Hauschild", "text": "Missouri Estimation of Distribution Algorithms Laboratory (MEDAL), Department of Mathematics and Computer Science, 321 ESHUniversity of Missouri at St. Louis One University Blvd., St. Louis, MO 63121mwh308 @ umsl.edu"}, {"heading": "Pier Luca Lanzi", "text": "Dipartimento di Elettronica e Informazione Politecnico di MilanoPiazza Leonardo da Vinci, 32 I-20133 Milano, Italy pierluca.lanzi @ polimi.it27. March 2012"}, {"heading": "Abstract", "text": "This method enables practitioners to improve the efficiency of the hBOA by collecting statistics from probable models of previous hBOA runs and using the obtained statistics to distort future hBOA runs into similar problems. The purpose of this work is threefold: (1) test the technique on multiple classes of NP-complete problems, including MAXSAT, spin glasses and minimal vertex coverage; (2) show that the technique is effective even when previous runs have been performed on problems of varying sizes; (3) provide empirical evidence that combining transfer learning with other techniques to increase efficiency can often lead to near-multiplicative accelerations. Keywords: transfer learning, inductive transfer, learning from experience, assessment of distribution algorithms, hierarchical Bayesian optimization algorithm, decomposable problems, efficiency improvement."}, {"heading": "1 Introduction", "text": "The evaluation of distribution algorithms (EDAs) [1, 2, 3, 4] guides the search for the optimum by building and sampling probability models of candidate solutions. The use of probability models in EDAs provides a basis for incorporating prior knowledge of the problem and learning from previous runs to solve new problem cases of a similar type with increased speed, accuracy and reliability. [5, 6] However, previous work in this area is based on handicraft limitations of probability models [7, 8, 9] which are difficult to design or even harmful to EDA efficiency and scalability [11]."}, {"heading": "2 Hierarchical BOA", "text": "The hierarchical optimization algorithms of the banks (hBOA) are able to be limited to the individual countries, both to the individual countries and to the overall regions of the individual countries. (...) The individual countries have to get involved with the individual countries. (...) The individual countries have to get involved with the individual countries. (...) The individual countries have to get involved with the individual countries. (...) The individual countries have to get involved with the individual countries. (...) The individual countries have to get involved with the individual countries. (...) The individual countries have to get involved with the individual countries. (...) The individual countries have to get involved with the individual countries. (...) The individual countries have to get involved with the individual countries. (...) The individual countries have to get involved with the individual countries. (...) The individual countries have to get involved with the individual countries. (...) The individual countries have to get involved with the individual countries. (...) The individual countries have to get involved with the individual countries. (...) The individual countries have to get involved with the individual countries."}, {"heading": "3 Learning from Experience using Distance-Based Bias", "text": "In hBOA and other EDAs based on complex probability models, building a precise probability model is critical to success [2, 3, 11, 19]. However, building complex probability models can be time-consuming, and it can require fairly large approaches to solving the problem [2, 3]. Learning from experience [5, 6, 12, 20, 21] provides an approach to solving this problem. The basic idea of learning from experience is to collect information about the problem by examining previous passes of the optimization algorithm and using the information obtained to distort the search for new problem cases. Using bias based on the results of other learning tasks is also common in machine learning, where it is called inductive transfer or transfer of learning variables [22, 23]. Since learning the model structure is often the most accountable task in modeling, learning experience models, which are often based on probability models of other probability pairs, is classified into probability pairs."}, {"heading": "3.1 Distance Metric for Additively Decomposable Functions", "text": "For many optimization problems, the objective function (fitness function) can be expressed as an additive decomposable function (ADF): f (X1,.., Xn) = m \u2211 i = 1fi (Si), (1) where (X1,.., Xn) are problem decision variables (string positions), fi is the subfunction of the problem, and Si \u0109i {X1, X2,.., Xn} is the subset of variables that contribute to fi. While there can often be several ways of decomposing the problem by means of additive decomposition, one would typically prefer decomposition variables that minimize the size of the subsets {Si}. Note that the difficulty of ADFs is not entirely determined by the order of the partial problems, but also by the definition of the partial problems and their interaction; even with partial problems of the order snoble variables only 2 or 3, the problem NP can be complete."}, {"heading": "3.2 Distance-Based Bias Based on Previous Runs of hBOA", "text": "This section describes the approach to learning from experiences developed by Pelikan and Hauschild [12], inspired mainly by the work of Hauschild et al. [6, 20, 21]. Let us assume that a number of hBOA models from previous hBOA runs are applied to similar problems. Before the bias is applied to previous runs in hBOA, the models in M are first processed to generate data that serve as the basis for the introduction of bias. Processing begins with the analysis of the models in M to determine the number s (m, d, j) of splits to any variable Xi, so that D (Xi, Xj) = d in a decision tree Tj for the variable Xj probability in a model m-M. Then the values s (m, d, j) are used to evaluate the probability of Pk (d, j) of a kth split on a variable at the distance d from Xj in a dependency tree in a p."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Test Problems and Experimental Setup", "text": "In fact, it is the case that most of them are able to survive themselves if they do not put themselves in the position in which they find themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves."}, {"heading": "4.2 Results", "text": "The results presented in Tables 1, 2 and 3 confirm the observation from Ref. [12] that the stronger the bias, the greater the benefit, at least for the area of bias examined and most of the issues raised; therefore, in the rest of this discussion we focus on \u0432 = 9. In all cases, distance-based bias resulted in significant accelerations of about 1.2 to 3.1. The best accelerations were achieved for minimal vertex coverage. In all cases, performance was strictly improved in at least 70% of problem cases in terms of execution time; in most cases, the improvements were observed in a much larger majority of cases. Accelerations were substantial even if the bias was based on previous runs of problem cases of different, smaller size; in fact, the accelerations achieved with such a bias were almost identical to the accelerations with the bias based on cases of the same size. Thus, the results provide clear empirical evidence that the bias was based on the distance from the main unit, even if the main unit is almost identical in the case of the same size."}, {"heading": "5 Summary and Conclusions", "text": "This paper augments the previous work on increasing efficiency of the Bayean hierarchical optimization algorithm (hBOA) using a distance-based bias derived from previous hBOA runs [12]. The paper showed that (1) distance-based bias causes significant acceleration in several previously untested classes of demanding, NP-complete problems, (2) the approach is applicable even when previous runs have been performed on problem cases of varying sizes, and (3) the approach, in combination with other techniques to increase efficiency, can achieve near-multiplicative acceleration. In summary, the results presented in this paper, together with previous work [12] clearly show that learning from experience with a distance-based bias has great potential to improve the efficiency of hBOA in particular and the evaluation of distribution algorithms (EDAs) in general. Several topics are central to future work. The approach should be adapted to other model-oriented optimization algorithms, including other optimization techniques."}, {"heading": "Acknowledgments", "text": "This project was sponsored by the National Science Foundation under the auspices of ECS-0547013 and IIS-1115352 and by the University of Missouri-St. Louis through the High Performance Computing Collaboratory under the auspices of Information Technology Services. Most of the experiments were conducted at the Beowulf Cluster maintained by ITS at the University of Missouri in St. Louis and the HPC resources of the Bioinformatics Consortium at the University of Missouri. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation."}], "references": [{"title": "An introduction and survey of estimation of distribution algorithms. Swarm and Evolutionary Computation", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Estimation of Distribution Algorithms: A New Tool for Evolutionary Computation", "author": ["P. Larra\u00f1aga", "J.A. Lozano", "eds"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "A survey of optimization by building and using probabilistic models. Computational Optimization and Applications", "author": ["M. Pelikan", "D.E. Goldberg", "F. Lobo"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Introduction to estimation of distribution algorithms", "author": ["M. Pelikan", "M.W. Hauschild", "F.G. Lobo"], "venue": "MEDAL Report No", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Hierarchical Bayesian optimization algorithm: Toward a new generation of evolutionary algorithms", "author": ["M. Pelikan"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Using previous models to bias structural learning in the hierarchical BOA", "author": ["M.W. Hauschild", "M. Pelikan", "K. Sastry", "D.E. Goldberg"], "venue": "Evolutionary Computation", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Schemata, distributions and graphical models in evolutionary optimization", "author": ["H. M\u00fchlenbein", "T. Mahnig", "A.O. Rodriguez"], "venue": "Journal of Heuristics", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Evolutionary optimization and the estimation of search distributions with applications to graph bipartitioning", "author": ["H. M\u00fchlenbein", "T. Mahnig"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Incorporating a priori knowledge in probabilistic-model based optimization", "author": ["S. Baluja"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "A problem-knowledge based evolutionary algorithm KBOA for hypergraph partitioning", "author": ["J. Schwarz", "J. Ocenasek"], "venue": "Proc. of the Fourth Joint Conf. on Knowledge-Based Software Engineering,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Analyzing probabilistic models in hierarchical BOA", "author": ["M.W. Hauschild", "M. Pelikan", "K. Sastry", "C.F. Lima"], "venue": "IEEE Transactions on Evolutionary Computation", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Distance-based bias in model-directed optimization of additively decomposable problems", "author": ["M. Pelikan", "M. Hauschild"], "venue": "MEDAL Report No", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Sporadic model building for efficiency enhancement of the hierarchical BOA", "author": ["M. Pelikan", "K. Sastry", "D.E. Goldberg"], "venue": "Genetic Programming and Evolvable Machines", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Escaping hierarchical traps with competent genetic algorithms", "author": ["M. Pelikan", "D.E. Goldberg"], "venue": "Genetic and Evol. Comp. Conf", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "A Bayesian approach to learning Bayesian networks with local structure", "author": ["D.M. Chickering", "D. Heckerman", "C. Meek"], "venue": "Technical Report MSR-TR-97-07,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1997}, {"title": "Finding multimodal solutions using restricted tournament selection", "author": ["G.R. Harik"], "venue": "Proc. of the Int. Conf. on Genetic Algorithms", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Learning Bayesian networks with local structure", "author": ["N. Friedman", "M. Goldszmidt"], "venue": "Graphical models. MIT Press", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E.H. Herskovits"], "venue": "Machine Learning", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1992}, {"title": "Model accuracy in the Bayesian optimization algorithm", "author": ["C.F. Lima", "F.G. Lobo", "M. Pelikan", "D.E. Goldberg"], "venue": "Soft Computing", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Enhancing efficiency of hierarchical BOA via distance-based model restrictions", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": "Parallel Problem Solving from Nature (PPSN X)", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Intelligent bias of network structures in the hierarchical BOA", "author": ["M.W. Hauschild", "M. Pelikan"], "venue": "Genetic and Evol. Comp. Conf", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Direct transfer of learned information among neural networks", "author": ["L.Y. Pratt", "J. Mostow", "C.A. Kamm", "A.A. Kamm"], "venue": "Proceedings of the Ninth National Conference on Artificial Intelligence", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1991}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Searching for ground states of Ising spin glasses with hierarchical BOA and cluster exact approximation", "author": ["M. Pelikan", "A.K. Hartmann"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "ed.: Spin glasses and random fields", "author": ["A. Young"], "venue": "World Scientific,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1998}, {"title": "Hybrid evolutionary algorithms on minimum vertex cover for random graphs", "author": ["M. Pelikan", "R. Kalapala", "A.K. Hartmann"], "venue": "Genetic and Evol. Comp. Conf", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Minimal vertex covers on finite-connectivity random graphs: A hardsphere lattice-gas picture", "author": ["M. Weigt", "A.K. Hartmann"], "venue": "Physical Review E", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Hierarchical BOA solves Ising spin glasses and maxsat", "author": ["M. Pelikan", "D.E. Goldberg"], "venue": "Genetic and Evol. Comp. Conf. (GECCO-2003) II", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Morphing: Combining structure and randomness", "author": ["I. Gent", "H.H. Hoos", "P. Prosser", "T. Walsh"], "venue": "Proc. of the American Association of Artificial Intelligence", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1999}, {"title": "Evaluation-relaxation schemes for genetic and evolutionary algorithms. Master\u2019s thesis, University of Illinois at Urbana-Champaign", "author": ["K. Sastry"], "venue": "Department of General Engineering,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "Estimation of distribution algorithms (EDAs) [1, 2, 3, 4] guide the search for the optimum by building and sampling probabilistic models of candidate solutions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 1, "context": "Estimation of distribution algorithms (EDAs) [1, 2, 3, 4] guide the search for the optimum by building and sampling probabilistic models of candidate solutions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 2, "context": "Estimation of distribution algorithms (EDAs) [1, 2, 3, 4] guide the search for the optimum by building and sampling probabilistic models of candidate solutions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 3, "context": "Estimation of distribution algorithms (EDAs) [1, 2, 3, 4] guide the search for the optimum by building and sampling probabilistic models of candidate solutions.", "startOffset": 45, "endOffset": 57}, {"referenceID": 4, "context": "The use of probabilistic models in EDAs provides a basis for incorporating prior knowledge about the problem and learning from previous runs in order to solve new problem instances of similar type with increased speed, accuracy and reliability [5, 6].", "startOffset": 244, "endOffset": 250}, {"referenceID": 5, "context": "The use of probabilistic models in EDAs provides a basis for incorporating prior knowledge about the problem and learning from previous runs in order to solve new problem instances of similar type with increased speed, accuracy and reliability [5, 6].", "startOffset": 244, "endOffset": 250}, {"referenceID": 6, "context": "However, much prior work in this area was based on hand-crafted constraints on probabilistic models [7, 8, 9, 10] which may be difficult to design or even detrimental to EDA efficiency and scalability [11].", "startOffset": 100, "endOffset": 113}, {"referenceID": 7, "context": "However, much prior work in this area was based on hand-crafted constraints on probabilistic models [7, 8, 9, 10] which may be difficult to design or even detrimental to EDA efficiency and scalability [11].", "startOffset": 100, "endOffset": 113}, {"referenceID": 8, "context": "However, much prior work in this area was based on hand-crafted constraints on probabilistic models [7, 8, 9, 10] which may be difficult to design or even detrimental to EDA efficiency and scalability [11].", "startOffset": 100, "endOffset": 113}, {"referenceID": 9, "context": "However, much prior work in this area was based on hand-crafted constraints on probabilistic models [7, 8, 9, 10] which may be difficult to design or even detrimental to EDA efficiency and scalability [11].", "startOffset": 100, "endOffset": 113}, {"referenceID": 10, "context": "However, much prior work in this area was based on hand-crafted constraints on probabilistic models [7, 8, 9, 10] which may be difficult to design or even detrimental to EDA efficiency and scalability [11].", "startOffset": 201, "endOffset": 205}, {"referenceID": 11, "context": "Recently, Pelikan and Hauschild [12] proposed an automated technique capable of learning from previous runs of the hierarchical Bayesian optimization algorithm (hBOA) in order to improve efficiency of future hBOA runs on problems of similar type.", "startOffset": 32, "endOffset": 36}, {"referenceID": 11, "context": "While the distance metric is strongly related to the problem being solved, the aforementioned study [12] described a rather general metric that can be applied to practically any problem with the objective function represented by an additively decomposable function.", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "However, the prior study [12] evaluated the proposed technique on only two classes of problems and it did not demonstrate several key features of this technique.", "startOffset": 25, "endOffset": 29}, {"referenceID": 11, "context": "[12] on other classes of challenging optimization problems, (2) demonstrate the ability of this technique to learn from problem instances of one size in order to introduce bias for instances of another size, and (3) demonstrate the potential benefits of combining this technique with other efficiency enhancement techniques, such as sporadic model building [13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[12] on other classes of challenging optimization problems, (2) demonstrate the ability of this technique to learn from problem instances of one size in order to introduce bias for instances of another size, and (3) demonstrate the potential benefits of combining this technique with other efficiency enhancement techniques, such as sporadic model building [13].", "startOffset": 357, "endOffset": 361}, {"referenceID": 11, "context": "The new results together with the results published in prior work [12] provide strong evidence of the broad applicability and great potential of this technique for learning from experience (transfer learning) in EDAs.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "Section 3 discusses efficiency enhancement of estimation of distribution algorithms using inductive transfer with main focus on hBOA and the distance-based bias [12].", "startOffset": 161, "endOffset": 165}, {"referenceID": 4, "context": "The hierarchical Bayesian optimization algorithm (hBOA) [5, 14] works with a population of candidate solutions represented by fixed-length strings over a finite alphabet.", "startOffset": 56, "endOffset": 63}, {"referenceID": 13, "context": "The hierarchical Bayesian optimization algorithm (hBOA) [5, 14] works with a population of candidate solutions represented by fixed-length strings over a finite alphabet.", "startOffset": 56, "endOffset": 63}, {"referenceID": 14, "context": "Next, hBOA (1) learns a Bayesian network with local structures [15] for the selected solutions and (2) generates new candidate solutions by sampling the distribution encoded by the built network.", "startOffset": 63, "endOffset": 67}, {"referenceID": 15, "context": "To maintain useful diversity in the population, the new candidate solutions are incorporated into the original population using restricted tournament selection (RTS) [16].", "startOffset": 166, "endOffset": 170}, {"referenceID": 14, "context": "hBOA represents probabilistic models of candidate solutions by Bayesian networks with local structures [15, 17].", "startOffset": 103, "endOffset": 111}, {"referenceID": 16, "context": "hBOA represents probabilistic models of candidate solutions by Bayesian networks with local structures [15, 17].", "startOffset": 103, "endOffset": 111}, {"referenceID": 13, "context": "To represent conditional probabilities of each variable given the variable\u2019s parents, hBOA uses decision trees [14, 15].", "startOffset": 111, "endOffset": 119}, {"referenceID": 14, "context": "To represent conditional probabilities of each variable given the variable\u2019s parents, hBOA uses decision trees [14, 15].", "startOffset": 111, "endOffset": 119}, {"referenceID": 14, "context": "Models are evaluated using the Bayesian-Dirichlet (BDe) metric with penalty for model complexity, which estimates the goodness of a Bayesian network structure given data D and background knowledge \u03be as p(B|D, \u03be) = cp(B|\u03be)p(D|B, \u03be), where c is a normalization constant [15, 18].", "startOffset": 268, "endOffset": 276}, {"referenceID": 17, "context": "Models are evaluated using the Bayesian-Dirichlet (BDe) metric with penalty for model complexity, which estimates the goodness of a Bayesian network structure given data D and background knowledge \u03be as p(B|D, \u03be) = cp(B|\u03be)p(D|B, \u03be), where c is a normalization constant [15, 18].", "startOffset": 268, "endOffset": 276}, {"referenceID": 14, "context": "The BayesianDirichlet metric estimates the term p(D|B, \u03be) by combining the observed and prior statistics for relevant combinations of variables [15].", "startOffset": 144, "endOffset": 148}, {"referenceID": 4, "context": "To favor simpler networks to the more complex ones, the prior probability p(B|\u03be) is often set to decrease exponentially fast with respect to the description length of the network\u2019s parameters [5, 17].", "startOffset": 192, "endOffset": 199}, {"referenceID": 16, "context": "To favor simpler networks to the more complex ones, the prior probability p(B|\u03be) is often set to decrease exponentially fast with respect to the description length of the network\u2019s parameters [5, 17].", "startOffset": 192, "endOffset": 199}, {"referenceID": 1, "context": "In hBOA and other EDAs based on complex probabilistic models, building an accurate probabilistic model is crucial to the success [2, 3, 11, 19].", "startOffset": 129, "endOffset": 143}, {"referenceID": 2, "context": "In hBOA and other EDAs based on complex probabilistic models, building an accurate probabilistic model is crucial to the success [2, 3, 11, 19].", "startOffset": 129, "endOffset": 143}, {"referenceID": 10, "context": "In hBOA and other EDAs based on complex probabilistic models, building an accurate probabilistic model is crucial to the success [2, 3, 11, 19].", "startOffset": 129, "endOffset": 143}, {"referenceID": 18, "context": "In hBOA and other EDAs based on complex probabilistic models, building an accurate probabilistic model is crucial to the success [2, 3, 11, 19].", "startOffset": 129, "endOffset": 143}, {"referenceID": 1, "context": "However, building complex probabilistic models can be time consuming and it may require rather large populations of solutions [2, 3].", "startOffset": 126, "endOffset": 132}, {"referenceID": 2, "context": "However, building complex probabilistic models can be time consuming and it may require rather large populations of solutions [2, 3].", "startOffset": 126, "endOffset": 132}, {"referenceID": 5, "context": "That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations [6, 8, 9, 20, 21].", "startOffset": 155, "endOffset": 172}, {"referenceID": 7, "context": "That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations [6, 8, 9, 20, 21].", "startOffset": 155, "endOffset": 172}, {"referenceID": 8, "context": "That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations [6, 8, 9, 20, 21].", "startOffset": 155, "endOffset": 172}, {"referenceID": 19, "context": "That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations [6, 8, 9, 20, 21].", "startOffset": 155, "endOffset": 172}, {"referenceID": 20, "context": "That is why much effort has been put into enhancing efficiency of model building in EDAs and improving quality of EDA models even with smaller populations [6, 8, 9, 20, 21].", "startOffset": 155, "endOffset": 172}, {"referenceID": 4, "context": "Learning from experience [5, 6, 12, 20, 21] represents one approach to addressing this issue.", "startOffset": 25, "endOffset": 43}, {"referenceID": 5, "context": "Learning from experience [5, 6, 12, 20, 21] represents one approach to addressing this issue.", "startOffset": 25, "endOffset": 43}, {"referenceID": 11, "context": "Learning from experience [5, 6, 12, 20, 21] represents one approach to addressing this issue.", "startOffset": 25, "endOffset": 43}, {"referenceID": 19, "context": "Learning from experience [5, 6, 12, 20, 21] represents one approach to addressing this issue.", "startOffset": 25, "endOffset": 43}, {"referenceID": 20, "context": "Learning from experience [5, 6, 12, 20, 21] represents one approach to addressing this issue.", "startOffset": 25, "endOffset": 43}, {"referenceID": 21, "context": "The use of bias based on the results of other learning tasks is also commonplace in machine learning where it is referred to as inductive transfer or transfer learning [22, 23].", "startOffset": 168, "endOffset": 176}, {"referenceID": 22, "context": "The use of bias based on the results of other learning tasks is also commonplace in machine learning where it is referred to as inductive transfer or transfer learning [22, 23].", "startOffset": 168, "endOffset": 176}, {"referenceID": 11, "context": "The key to make the learning from experience work is to ensure that the pairs of variables are classified into a set of categories so that the pairs in each category have a lot in common and can be expected to be either correlated or independent simultaneously [12].", "startOffset": 261, "endOffset": 265}, {"referenceID": 11, "context": "that [12], in which pairs of variables are classified into categories based on a predefined distance metric on variables.", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "[12] follows the work of Hauschild et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6, 11, 20].", "startOffset": 0, "endOffset": 11}, {"referenceID": 10, "context": "[6, 11, 20].", "startOffset": 0, "endOffset": 11}, {"referenceID": 19, "context": "[6, 11, 20].", "startOffset": 0, "endOffset": 11}, {"referenceID": 10, "context": "This observation has been confirmed with numerous experimental studies across a number of important problem domains from spin glasses distributed on a finite-dimensional lattice [11, 12] to NK landscapes [12].", "startOffset": 178, "endOffset": 186}, {"referenceID": 11, "context": "This observation has been confirmed with numerous experimental studies across a number of important problem domains from spin glasses distributed on a finite-dimensional lattice [11, 12] to NK landscapes [12].", "startOffset": 178, "endOffset": 186}, {"referenceID": 11, "context": "This observation has been confirmed with numerous experimental studies across a number of important problem domains from spin glasses distributed on a finite-dimensional lattice [11, 12] to NK landscapes [12].", "startOffset": 204, "endOffset": 208}, {"referenceID": 11, "context": "This section describes the approach to learning from experience developed by Pelikan and Hauschild [12] inspired mainly by the work of Hauschild et al.", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "[6, 20, 21].", "startOffset": 0, "endOffset": 11}, {"referenceID": 19, "context": "[6, 20, 21].", "startOffset": 0, "endOffset": 11}, {"referenceID": 20, "context": "[6, 20, 21].", "startOffset": 0, "endOffset": 11}, {"referenceID": 11, "context": "Pelikan and Hauschild [12] proposed to use the prior probability distribution p(B|\u03be) to introduce a bias based on distance-based statistics from previous hBOA runs represented by Pk(d, j) by setting", "startOffset": 22, "endOffset": 26}, {"referenceID": 23, "context": "The experiments were done for three problem classes known to be difficult for most genetic and evolutionary algorithms: (1) Three-dimensional Ising spin glasses were considered with \u00b1J couplings and periodic boundary conditions [24, 25]; two problem sizes were used, n = 6\u00d7 6\u00d7 6 = 216 spins and n = 7\u00d7 7\u00d7 7 = 343 spins with 1,000 unique problem instances for each n.", "startOffset": 228, "endOffset": 236}, {"referenceID": 24, "context": "The experiments were done for three problem classes known to be difficult for most genetic and evolutionary algorithms: (1) Three-dimensional Ising spin glasses were considered with \u00b1J couplings and periodic boundary conditions [24, 25]; two problem sizes were used, n = 6\u00d7 6\u00d7 6 = 216 spins and n = 7\u00d7 7\u00d7 7 = 343 spins with 1,000 unique problem instances for each n.", "startOffset": 228, "endOffset": 236}, {"referenceID": 25, "context": "(2) Minimum vertex cover was considered for random graphs of fixed ratio c of the number of edges and number of nodes [26, 27]; two ratios (c = 2 and c = 4) and two problem sizes (n = 150 and n = 200) were used with 1,000 unique problem instances for each combination of c and n.", "startOffset": 118, "endOffset": 126}, {"referenceID": 26, "context": "(2) Minimum vertex cover was considered for random graphs of fixed ratio c of the number of edges and number of nodes [26, 27]; two ratios (c = 2 and c = 4) and two problem sizes (n = 150 and n = 200) were used with 1,000 unique problem instances for each combination of c and n.", "startOffset": 118, "endOffset": 126}, {"referenceID": 27, "context": "(3) MAXSAT was considered for mapped instances of graph coloring with graphs created by combining regular ring lattices (with probability 1\u2212 p) and random graphs (with probability p) [28, 29]; 100 unique problem instances of n = 500 bits (propositions) were used for each considered value of p, from p = 2\u22128 (graphs nearly identical to a regular ring lattice) to p = 2\u22121 (graphs with half of the edges random).", "startOffset": 183, "endOffset": 191}, {"referenceID": 28, "context": "(3) MAXSAT was considered for mapped instances of graph coloring with graphs created by combining regular ring lattices (with probability 1\u2212 p) and random graphs (with probability p) [28, 29]; 100 unique problem instances of n = 500 bits (propositions) were used for each considered value of p, from p = 2\u22128 (graphs nearly identical to a regular ring lattice) to p = 2\u22121 (graphs with half of the edges random).", "startOffset": 183, "endOffset": 191}, {"referenceID": 23, "context": "[24, 26, 28].", "startOffset": 0, "endOffset": 12}, {"referenceID": 25, "context": "[24, 26, 28].", "startOffset": 0, "endOffset": 12}, {"referenceID": 27, "context": "[24, 26, 28].", "startOffset": 0, "endOffset": 12}, {"referenceID": 4, "context": "For each problem instance, we used bisection [5, 30] to ensure that the population size was within 5% of the minimum population size to find the optimum in 10 out of 10 independent runs.", "startOffset": 45, "endOffset": 52}, {"referenceID": 29, "context": "For each problem instance, we used bisection [5, 30] to ensure that the population size was within 5% of the minimum population size to find the optimum in 10 out of 10 independent runs.", "startOffset": 45, "endOffset": 52}, {"referenceID": 4, "context": "Bit-flip hill climbing (HC) [5] was incorporated into hBOA to improve its performance on all test problems except for the minimum vertex cover; HC was used to improve every solution in the population.", "startOffset": 28, "endOffset": 31}, {"referenceID": 25, "context": "[26] was incorporated instead.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Finally, we examine the combination of the distance-based bias based on prior runs and the sporadic model building [13].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "[13], and then we carry out a similar experiment using both the distance-based bias as well as the sporadic model building, recording the speedups with respect to the base case.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12] that the stronger the bias the greater the benefits, at least for the examined range of \u03ba \u2208 {1, 3, 5, 7, 9} and most problem settings; that is why in the remainder of this discussion we focus on \u03ba = 9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The results thus provide clear empirical evidence that the distance-based bias is applicable even when the problem instances vary in size, which was argued [12] to be one of the main advantages of the distance-based bias over prior work in the area but was not demonstrated.", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "This paper extended the prior work on efficiency enhancement of the hierarchical Bayesian optimization algorithm (hBOA) using a distance-based bias derived from prior hBOA runs [12].", "startOffset": 177, "endOffset": 181}, {"referenceID": 11, "context": "In summary, the results presented in this paper together with the prior work [12] provide clear evidence that learning from experience using a distance-based bias has a great potential to improve efficiency of hBOA in particular and estimation of distribution algorithms (EDAs) in general.", "startOffset": 77, "endOffset": 81}], "year": 2013, "abstractText": "An automated technique has recently been proposed to transfer learning in the hierarchical Bayesian optimization algorithm (hBOA) based on distance-based statistics. The technique enables practitioners to improve hBOA efficiency by collecting statistics from probabilistic models obtained in previous hBOA runs and using the obtained statistics to bias future hBOA runs on similar problems. The purpose of this paper is threefold: (1) test the technique on several classes of NP-complete problems, including MAXSAT, spin glasses and minimum vertex cover; (2) demonstrate that the technique is effective even when previous runs were done on problems of different size; (3) provide empirical evidence that combining transfer learning with other efficiency enhancement techniques can often yield nearly multiplicative speedups.", "creator": "LaTeX with hyperref package"}}}