{"id": "1203.5716", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Mar-2012", "title": "Credal Classification based on AODE and compression coefficients", "abstract": "Bayesian model averaging (BMA) is an approach to average over alternative models; yet, it usually gets excessively concentrated around the single most probable model, therefore achieving only sub-optimal classification performance. The compression-based approach (Boulle, 2007) overcomes this problem, averaging over the different models by applying a logarithmic smoothing over the models' posterior probabilities. This approach has shown excellent performances when applied to ensembles of naive Bayes classifiers. AODE is another ensemble of models with high performance (Webb, 2005), based on a collection of non-naive classifiers (called SPODE) whose probabilistic predictions are aggregated by simple arithmetic mean. Aggregating the SPODEs via BMA rather than by arithmetic mean deteriorates the performance; instead, we aggregate the SPODEs via the compression coefficients and we show that the resulting classifier obtains a slight but consistent improvement over AODE. However, an important issue in any Bayesian ensemble of models is the arbitrariness in the choice of the prior over the models. We address this problem by the paradigm of credal classification, namely by substituting the unique prior with a set of priors. Credal classifier automatically recognize the prior-dependent instances, namely the instances whose most probable class varies, when different priors are considered; in these cases, credal classifiers remain reliable by returning a set of classes rather than a single class. We thus develop the credal version of both the BMA-based and the compression-based ensemble of SPODEs, substituting the single prior over the models by a set of priors. Experiments show that both credal classifiers provide higher classification reliability than their determinate counterparts; moreover the compression-based credal classifier compares favorably to previous credal classifiers.", "histories": [["v1", "Mon, 26 Mar 2012 16:25:35 GMT  (88kb,S)", "https://arxiv.org/abs/1203.5716v1", null], ["v2", "Tue, 27 Mar 2012 10:27:30 GMT  (30kb)", "http://arxiv.org/abs/1203.5716v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["giorgio corani", "alessandro antonucci"], "accepted": false, "id": "1203.5716"}, "pdf": {"name": "1203.5716.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["giorgio@idsia.ch"], "sections": [{"heading": null, "text": "The compression-based approach (Boulle, 2007) overcomes this problem by evaluating the different models by applying a logarithmic smoothing of the posterior probabilities of the models. This approach has performed excellently when applied to ensembles of naive Bayes classifiers. AODE is another set of high-performance models (Webb et al., 2005): it consists of a collection of non-naive classifiers (called SPODE) whose likely predictions are aggregated by simple arithmetic means."}, {"heading": "1. Introduction", "text": "The problem is that BMA excessively focuses on the most likely model (Domingos, 2000; Minka, 2002): in particular, on large datasets \"which, using posterior probabilities, overcomes almost the same model as selecting the Boulle model\" (BMA). It is a solid solution to the uncertainty that characterizes the identification of the supposedly best model for a particular dataset, given a number of alternative models that are better to predict than any single model (Hoeting et al., 1999). However, such an assumption is generally inaccurate; for this reason, the real datasets BMA generally do not perform very well; see the discussion and references in Cerquides et al. (2005) for more details. The problem is that BMA excessively focuses on the only most likely model (Domingos, 2000; Minka, 2002): in particular on large datasets, \"using posterior probabilities.\""}, {"heading": "2. Methods", "text": "We consider a classification problem with k-characteristics; we call C the class variable (where values are taken in C) and A: = (A1,..., Ak) the set of characteristics, taking values in A1,..., Ak. For a generic variable A, we call P (A) the probability mass function above its values, and P (a) the probability that A = a. We assume that the data is complete and the training data contains D n instances. We learn the model parameters from the training data by adopting Dirichlet priors and setting the corresponding sample size to 1. Under 0-1 loss, a traditional probabilistic classifier returns, for a test instance a = 1,... whose class is unknown, the most likely class c: c: = argmax c: = C (c | a).Classifiers based on imprecise probabilities (credal classifier), c change this paradigm shift, c = max: menus: a)."}, {"heading": "2.1. From Naive Bayes to AODE", "text": "The Naive Bayes Classifier assumes the stochastic independence of the characteristics associated with the class; it therefore factorizes the common probability as follows: P (c, a): = P (c) \u00b7 k-j = 1P (aj | c), (1), according to the topology of Fig.1 (a). Despite the biased estimation of probabilities based on the above (so-called naive) assumption, naive Bayes performs well below 0-1 loss due to its low variance (Domingos and Pazzani, 1997); it is therefore a reasonable choice if the goal is a simple classification, without the need for precise probability estimates; it is particularly competitive on small and medium-sized datasets, thanks to its low variance (Friedman, 1997). To improve the model, weaker assumptions about the conditional independence of the characteristics must be taken into account; for example, Figure c, the tree-augmented naive classifier (TAN) is the ability."}, {"heading": "2.2. Bayesian Model Averaging (BMA) with SPODEs", "text": "Considering that each SPODE has the same number of variables, the same number of variables, the same number of variables, the same number of variables and the same in-degree1, we are adopting a uniform approach, that is, a uniform approach for each SPODE. Considering that each SPODE has the same number of variables, the same number of variables is also the same in-degree1, we are adopting a uniform approach, that is, a uniform approach for each SPODE. Considering that each SPODE is often adopted within the BMA, the maximum number of parents per node is: it is two approaches for each SPODE. In fact, the uniform approach is often applied within the BMA."}, {"heading": "2.2.1. Exponentiation of the Log-Likelihoods", "text": "Regardless of whether the marginal probability or the conditional probability is taken into account, it is customary to calculate the log probability rather than the probability to avoid numerical problems due to the multiplication of many probabilities. However, if the log probabilities are very negative, as is the case with large datasets, their exponentialization may also have numerical problems, a problem that was solved with high numerical precision in Yang et al. (2007): \"BMA often lead to an arithmetic overflow in the calculation of very large exponentials or factorials. One solution is the use of the Java class BigDecimal, which unfortunately can be very slow.\" Algorithm 1 describes a method for exposing the log probabilities that is both numerically robust and computationally fast. The method was communicated to us by D. Dash, who has published several papers on BMA (Dash and Cooper, 2004)."}, {"heading": "2.3. BMA-AODE*: Extending BMA-AODE to Sets of Probabilities", "text": "It is not easy to accurately distinguish the previous probability of each SPODE model between zero and one (vhs), however, this model would lead to vague conclusions that prevent learning from the data (Piatti et al., 2009). To obtain the non-vacuum inferences, we present a non-vacuumed lower probability for the previous probability of each individual SPODE model (vhs)."}, {"heading": "2.4. Compression-Based averaging", "text": "We will apply the classification efficiently, we will apply the classification efficiently, we will introduce averaging based on the marginal probabilities of the classes, we will apply the classification of the classification efficiently, we will therefore use it as a remedy against the tendency of the BMA classification to focus excessively on the most likely model that actually degrades performance (Boulle \u0301, 2007; Domingos, 2000), this approach replaces the posterior probabilities P (sj | D) of the models with smoother compression weights, which we call P \u2032. To present the method, we need some more notations from LLj, the protocol of conditional probability of the model sj. We will also introduce the zero classification as a Bayesian network without arcs, which represents the class as independent of the characteristics and their stabilistic classifications."}, {"heading": "2.5. COMP-AODE*: Extending COMP-AODE to Sets of Probabilities", "text": "We extend COMP-AODE to imprecise probabilities by collecting multiple specifications of the previous P (S) over the models (SJ) that flow into a null model (S) in which the previous probability of the SPODEs (S) can be freely varied. (S) We also consider the null model of a fixed previous probability here, while the previous probability of the SPODEs (S) is free to vary under the conditions of the BMA-AODE *; in this way we consider a condition of the previous ignorance. (S) The credally set Pc (S) is therefore adopted by the COMP-AODE *. (S) We have the condition of the previous ignorance. (sj) The credally used Pc-AODE (S) is free of the previous condition of the previous ignorance."}, {"heading": "2.6. Computational Complexity of the Classifiers", "text": "We must abide by the rules we have adopted, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "3. Experiments", "text": "We conduct experiments with 40 sets of data, the characteristics of which are given in the appendix (Table B.2). For each set of data, we perform 10 passes with 5-fold cross-validation. To obtain complete data, we replace missing values with the median and the mode for numerical and categorical characteristics respectively. We discredit numerical characteristics by the entropy-based method of (Fayyad and Irani, 1993). For the pair-wise comparison of classifiers over the collection of data sets, we use the non-parametric Wilcoxon Sign-Rank test. 4 The Wilcoxon Sign-Rank test is actually recommended to compare two classifiers on several sets of data (Demsar, 2006): Since it is not parametric, it avoids strong assumptions and handles outliers robustly."}, {"heading": "3.1. Determinate classifiers", "text": "We call the classifiers, which always return a single class, namely AODE, BMA-AODE and COMP-AODE. For certain classifiers, we use two indicators: Accuracy, namely the percentage of correct classifications, and Brier Loss 1ntente-i (1 \u2212 P (i) | a (i)) 2, which specifies the number of instances in the test group, while P (i) | a (i))) evaluates the probability estimated by the classification for the true class of instance. Brier Loss evaluates the quality of the estimated probabilities in a more sensitive way than accuracy."}, {"heading": "3.2. Credal classifiers", "text": "A credal classifier can be considered as separating the instances into two groups: the safe ones, for which it returns a single class, and the pre-dependent ones, for which there are two or more classes. Note that the previous dependence is not an intrinsic ownership of the instance: an instance can in fact be judged as pre-dependent on a particular credal classifier and as safe on another credal classifier. To characterize the performance of a credal classifier, the following four indicators are considered (Corani and Zaffalon, 2008b): \u2022 Determination:% of the instances that are recognized as safe, namely classified with a singleclass; \u2022 Accuracy, which is achieved via the instances, Accuracy: the accuracy achieved by returning a series of classes, via the pre-dependent instances; \u2022 Indefinite output size: the average number of classes returned on the previous instances."}, {"heading": "3.3. Utility-based Measures", "text": "In fact, it is in such a way that most people who are in a position to surpass themselves, to surpass themselves, to surpass themselves and to surpass themselves, to go to another world, in which they move into another world, in which they do not reinvent themselves, in which they do not reinvent themselves, in which they cannot reinvent themselves, in which they do not reinvent themselves, in which they cannot reinvent themselves, in which they do not reinvent themselves, in which they do not reinvent themselves, in which they cannot reinvent themselves, in which they do not reinvent themselves, in which they survive themselves, in which they cannot reinvent themselves, in which they are rooted, in which they are not reinvented, in which they are not reinvented, in which they are not reinvented, in which they are not reinvented, in which they do not reinvent themselves, in which they survive themselves, in which they cannot reinvent themselves, in which they are reinvented, in which they are rooted, in which they are not reinvented, in which they are not reinvented, in which they are refracted, in which they are fragmented, in which they are refracted, in which they are refracted, in which they are fragmented, in which they become refragment, in themselves, in which they are fragmented, in which they become refragment, in themselves, in which they are fragmented, in which they become refragment, in themselves, in which they are fragmented, in which they become refragment, in themselves, in which they are they are refragment, in which they are they are fragmented, in themselves, in which they are refragment, in which they are they are they become, in themselves, in which they are refragment, in themselves, in which they become fragmented, in which they are they become, in themselves, in which they are refragment, in themselves, in which they are refragment, in which they are refragment, in themselves they are refragment, in which they are refragment, in themselves they are refragment, in which they are refragment, in which they are refragment they are they are refracted, in themselves, in which they are refra"}, {"heading": "3.4. Comparison with previous credal classifiers", "text": "In this section, we compare COMP-AODE * with previous Credal classifiers. A well-known Credal classifier is the naive Credal classifier (NCC) (Corani and Zaffalon, 2008b), which is an extension of naive Bayes to an imprecise probability. However, we have operated NCC on the same collection of data sets after the experimental setup of Section 3; over time, algorithms have been developed that are more sophisticated than NCC, such as: \u2022 credal model averaging (Corani and Zaffalon, 2008a), namely a generation of BMA *."}, {"heading": "3.5. Some comments on credal classification vs reject option", "text": "& & & & & & & & & 10 & & & & 10 & & & & 10 & & & 10 & & & & 10 & & & & 10 & & & 10 & & & & 10 & & & 10 & & & 10; & & & 10 & & & 10 & & & 10; & & & & 10 & & & & 10; & & & 10; & & # 10; & # 10; & # 10; & # 10; & # 10; & & # 10; & & & # 10; & & & # 10; & & & & # 10; & & & & & & & 10; & & & & & # 10; & & & # 10; & & & & # 10; & & & & & & # 10 & & & & & & & & 10; & & & & & & & & & 10 & & & & & & & & & 10; & & & & & & & & & 10; & & & & & & & & & 10; & & & & & & & & & 10; & & & & & & & # 10; & & & & & & # 10; & & & # 10; & & & # 10; & & # 10; & & & & # 10; & & # 10; & & & & # 10; & & & # 10; & & & & & # 10; & & & # 10; & & & & & # 10; & & & & & # 10 & & & & & & & # 10; & & & & & & & & # 10 & & & & & & & & & # 10 & & & & & & & & & & & & & # 10; & & & & # 10 & & & & & & & & & & # 10 & & & & & & & & & & & & # 10 & & & & & & & & & & & & & & & # 10 & & & & & & & & & & & & & & # 10 & & & & & & & & & & & & & # 10 & & & & & & & & & & & & & & & & & & & # 10 & & & & & & & & & & & & & & & & # 10 & & & & & & & & & & & & & # 10 & & & & & & & & & & &"}, {"heading": "4. Conclusions", "text": "Applying the Bayesian model of averaging over SPODEs reduces classification performance compared to the standard AODE. Instead, the COMP-AODE classifier proposed here, which uses the compression-based approach over SPODEs, achieves slightly better overall classification performance than AODE; our results therefore extend the scope of (Boulle \u0301, 2007), where the compression-based approach was applied via an ensemble of naive Bayes classifiers. BMAAODE * and COMP-AODE * both credential classifiers automatically expand or extend BMA-AODE and COMP-AODE to an imprecise probability and replace the uniform prior approach to SPODEs with a credal set; both credential classifiers automatically identify the previously dependent instances and reliably manage them by returning a small but highly precise group of ODE knowledge to AMP as well as the subject to a strong decline in AMP knowledge."}, {"heading": "Acknowledgements", "text": "The research in this paper was partially supported by the Swiss NSF grants no. 200020-132252 and the Hasler Foundation no. 10030."}, {"heading": "Appendix A. Mapping linear-fractional programs to linear programs by the CharnesCooper transformation", "text": "In this appendix, we adapt the classic Charnes-Cooper transformation to the respective linearly broken program, which must be solved in order to test the dominance of the BMA AODE * as described in Section 2.3. Let us write the optimization variables as xj: = P (sj) (with j = 1,.., k) and the coefficients as: \u03b3i \u03b4i: = P (c \u00b2 | a, mj) P (c \u00b2 | a, mj) \u00b7 Lj. (A.1) The objective function therefore rewrites as: \u2211 kj = 1 \u03b3jxj \u2211 kj = 1 \u03b4jxj. (A.2) with j = 1,..., k. Let us actually change the variables as follows: yj: = xj \u0445 j \u0441jxj, (A.3) and insert the auxiliary variable as a constant. (A.4) After this non-linear transformation, the objective function takes jint jint, i.e. the linear jint, i.e. we write the original jint."}, {"heading": "Appendix B. Data sets list", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "Upper entropy of credal sets. applications to credal classification", "author": ["J. Abell\u00e1n", "S. Moral"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "Abell\u00e1n and Moral,? \\Q2005\\E", "shortCiteRegEx": "Abell\u00e1n and Moral", "year": 2005}, {"title": "Linear-fractional programming: theory, methods, applications and software", "author": ["E. Bajalinov"], "venue": null, "citeRegEx": "Bajalinov,? \\Q2003\\E", "shortCiteRegEx": "Bajalinov", "year": 2003}, {"title": "Compression-based averaging of selective naive bayes classifiers", "author": ["M. Boull\u00e9"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Boull\u00e9,? \\Q2007\\E", "shortCiteRegEx": "Boull\u00e9", "year": 2007}, {"title": "Robust Bayesian linear classifier ensembles", "author": ["J. Cerquides", "R De M\u00e0ntaras"], "venue": "Lecture notes in computer science", "citeRegEx": "Cerquides and M\u00e0ntaras,? \\Q2005\\E", "shortCiteRegEx": "Cerquides and M\u00e0ntaras", "year": 2005}, {"title": "Combining probability distributions from experts in risk analysis", "author": ["R. Clemen", "R. Winkler"], "venue": "Risk Analysis", "citeRegEx": "Clemen and Winkler,? \\Q1999\\E", "shortCiteRegEx": "Clemen and Winkler", "year": 1999}, {"title": "Bayesian networks with imprecise probabilities: Theory and application to classification", "author": ["G. Corani", "A. Antonucci", "M. Zaffalon"], "venue": "Data Mining: Foundations and Intelligent Paradigms", "citeRegEx": "Corani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Corani et al\\.", "year": 2012}, {"title": "Credal model averaging: an extension of Bayesian model averaging to imprecise probabilities", "author": ["G. Corani", "M. Zaffalon"], "venue": "Proc. 12th European Conference on Machine Learning (ECML-PKDD", "citeRegEx": "Corani and Zaffalon,? \\Q2008\\E", "shortCiteRegEx": "Corani and Zaffalon", "year": 2008}, {"title": "Learning reliable classifiers from small or incomplete data sets: the naive credal classifier 2", "author": ["G. Corani", "M. Zaffalon"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Corani and Zaffalon,? \\Q2008\\E", "shortCiteRegEx": "Corani and Zaffalon", "year": 2008}, {"title": "On searching for optimal classifiers among bayesian networks", "author": ["R. Cowell"], "venue": "Proceedings of the Eighth International Conference on Artificial Intelligence and Statistics", "citeRegEx": "Cowell,? \\Q2001\\E", "shortCiteRegEx": "Cowell", "year": 2001}, {"title": "Model Averaging for Prediction with Discrete Bayesian Networks", "author": ["D. Dash", "G. Cooper"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Dash and Cooper,? \\Q2004\\E", "shortCiteRegEx": "Dash and Cooper", "year": 2004}, {"title": "Learning nondeterministic classifiers", "author": ["J. del Coz", "A. Bahamonde"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Coz and Bahamonde,? \\Q2009\\E", "shortCiteRegEx": "Coz and Bahamonde", "year": 2009}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Demsar"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Demsar,? \\Q2006\\E", "shortCiteRegEx": "Demsar", "year": 2006}, {"title": "Bayesian averaging of classifiers and the overfitting problem", "author": ["P. Domingos"], "venue": "Proc. of the 17th International Conference on Machine Learning", "citeRegEx": "Domingos,? \\Q2000\\E", "shortCiteRegEx": "Domingos", "year": 2000}, {"title": "On the optimality of the simple Bayesian classifier under zero-one loss", "author": ["P. Domingos", "M. Pazzani"], "venue": "Machine Learning", "citeRegEx": "Domingos and Pazzani,? \\Q1997\\E", "shortCiteRegEx": "Domingos and Pazzani", "year": 1997}, {"title": "A synthetic view of belief revision with uncertain inputs in the framework of possibility theory", "author": ["D. Dubois", "H. Prade"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "Dubois and Prade,? \\Q1997\\E", "shortCiteRegEx": "Dubois and Prade", "year": 1997}, {"title": "Multi-interval discretization of continuous-valued attributes for classification learning. In: Proceedings of the 13th international joint conference on artificial intelligence", "author": ["U.M. Fayyad", "K.B. Irani"], "venue": null, "citeRegEx": "Fayyad and Irani,? \\Q1993\\E", "shortCiteRegEx": "Fayyad and Irani", "year": 1993}, {"title": "On bias, variance, 0/1 - loss, and the curse-of-dimensionality", "author": ["J. Friedman"], "venue": "Data Mining and Knowledge Discovery", "citeRegEx": "Friedman,? \\Q1997\\E", "shortCiteRegEx": "Friedman", "year": 1997}, {"title": "Bayesian networks classifiers", "author": ["N. Friedman", "D. Geiger", "M. Goldszmidt"], "venue": "Machine Learning", "citeRegEx": "Friedman et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Friedman et al\\.", "year": 1997}, {"title": "Classification with reject option", "author": ["R. Herbei", "M. Wegkamp"], "venue": "Canadian Journal of Statistics", "citeRegEx": "Herbei and Wegkamp,? \\Q2006\\E", "shortCiteRegEx": "Herbei and Wegkamp", "year": 2006}, {"title": "Bayesian Model Averaging: a Tutorial", "author": ["J. Hoeting", "D. Madigan", "A. Raftery", "C. Volinsky"], "venue": "Statistical Science", "citeRegEx": "Hoeting et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Hoeting et al\\.", "year": 1999}, {"title": "On supervised selection of bayesian networks", "author": ["P. Kontkanen", "P. Myllymaki", "T. Silander", "H. Tirri"], "venue": "Proc. of the Fifteenth Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Kontkanen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kontkanen et al\\.", "year": 1999}, {"title": "The Enterprise of Knowledge", "author": ["I. Levi"], "venue": null, "citeRegEx": "Levi,? \\Q1980\\E", "shortCiteRegEx": "Levi", "year": 1980}, {"title": "Approximating expected utility by a function of mean and variance", "author": ["H. Levy", "H. Markowitz"], "venue": "The American Economic Review", "citeRegEx": "Levy and Markowitz,? \\Q1979\\E", "shortCiteRegEx": "Levy and Markowitz", "year": 1979}, {"title": "Bayesian model averaging is not model combination", "author": ["T. Minka"], "venue": "Tech. rep., MIT Media Lab note. URL http://research.microsoft.com/ \u0303{}minka/papers/minka-bma-isnt-mc.pdf", "citeRegEx": "Minka,? \\Q2002\\E", "shortCiteRegEx": "Minka", "year": 2002}, {"title": "Limits of learning about a categorical latent variable under prior near-ignorance", "author": ["A. Piatti", "M. Zaffalon", "F. Trojani", "M. Hutter"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "Piatti et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Piatti et al\\.", "year": 2009}, {"title": "Decision making under uncertainty using imprecise probabilities", "author": ["M. Troffaes"], "venue": "International Journal of Approximate Reasoning", "citeRegEx": "Troffaes,? \\Q2007\\E", "shortCiteRegEx": "Troffaes", "year": 2007}, {"title": "Statistical Reasoning with Imprecise Probabilities", "author": ["P. Walley"], "venue": null, "citeRegEx": "Walley,? \\Q1991\\E", "shortCiteRegEx": "Walley", "year": 1991}, {"title": "Not so naive bayes: Aggregating onedependence estimators", "author": ["G. Webb", "J. Boughton", "Z. Wang"], "venue": "Machine Learning", "citeRegEx": "Webb et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Webb et al\\.", "year": 2005}, {"title": "To select or to weigh: A comparative study of linear combination schemes for superparentone-dependence estimators. Knowledge and Data Engineering", "author": ["Y. Yang", "G. Webb", "J. Cerquides", "K. Korb", "J. Boughton", "K. Ting"], "venue": "IEEE Transactions on", "citeRegEx": "Yang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2007}, {"title": "Utility-based accuracy measures to empirically evaluate credal classifiers", "author": ["M. Zaffalon", "G. Corani", "D. Mau\u00e1"], "venue": "ISIPTA\u201911: Proceedings of the Seventh International Symposium on Imprecise Probability: Theories and Applications. SIPTA,", "citeRegEx": "Zaffalon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zaffalon et al\\.", "year": 2011}, {"title": "Evaluating credal classifiers by utilitydiscounted predictive accuracy", "author": ["M. Zaffalon", "G. Corani", "D. Maua"], "venue": "Tech. Rep. IDSIA-03-12, IDSIA (Istituto Dalle Molle Intelligenza Artificiale", "citeRegEx": "Zaffalon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zaffalon et al\\.", "year": 2012}, {"title": "We have therefore mapped the original problem into a standard linear program and the solutions of the two problems are known to coincide (Bajalinov", "author": [], "venue": "Chap", "citeRegEx": "t.,? \\Q2003\\E", "shortCiteRegEx": "t.", "year": 2003}], "referenceMentions": [{"referenceID": 2, "context": "The compression-based approach (Boull\u00e9, 2007) overcomes this problem; it averages over the different models by applying a logarithmic smoothing over the models\u2019 posterior probabilities.", "startOffset": 31, "endOffset": 45}, {"referenceID": 27, "context": "AODE is another ensemble of models with high performance (Webb et al., 2005): it consists of a collection of non-naive classifiers (called SPODE) whose probabilistic predictions are aggregated by simple arithmetic mean.", "startOffset": 57, "endOffset": 76}, {"referenceID": 19, "context": "Introduction Bayesian model averaging (BMA) (Hoeting et al., 1999) is a sound solution to the uncertainty which characterizes the identification of the supposedly best model for a certain data set; given a set of alternative models, BMA weights the inferences produced by the various models, using the models\u2019 posterior probabilities as weights.", "startOffset": 44, "endOffset": 66}, {"referenceID": 19, "context": "BMA assumes the data to be generated by one of the considered models; under this assumption, it provides better predictive accuracy than any single model (Hoeting et al., 1999).", "startOffset": 154, "endOffset": 176}, {"referenceID": 12, "context": "The problem is that BMA gets excessively concentrated around the single most probable model (Domingos, 2000; Minka, 2002): especially on large data sets, \u201caveraging using the posterior probabilities to weight the models is almost the same as selecting the MAP model\u201d (Boull\u00e9, 2007).", "startOffset": 92, "endOffset": 121}, {"referenceID": 23, "context": "The problem is that BMA gets excessively concentrated around the single most probable model (Domingos, 2000; Minka, 2002): especially on large data sets, \u201caveraging using the posterior probabilities to weight the models is almost the same as selecting the MAP model\u201d (Boull\u00e9, 2007).", "startOffset": 92, "endOffset": 121}, {"referenceID": 2, "context": "The problem is that BMA gets excessively concentrated around the single most probable model (Domingos, 2000; Minka, 2002): especially on large data sets, \u201caveraging using the posterior probabilities to weight the models is almost the same as selecting the MAP model\u201d (Boull\u00e9, 2007).", "startOffset": 267, "endOffset": 281}, {"referenceID": 2, "context": "To overcome the problem of BMA getting excessively concentrated around the most probable model, a compression-based approach has been introduced in (Boull\u00e9, 2007); it computes more evenly-distributed weights, by applying a logarithmic smoothing to the models posterior probabilities.", "startOffset": 148, "endOffset": 162}, {"referenceID": 27, "context": "Another ensemble of Bayesian networks classifiers known for its good performance is AODE (Webb et al., 2005), which is instead based on a set of SPODE (SuperParent-", "startOffset": 89, "endOffset": 108}, {"referenceID": 17, "context": "Introduction Bayesian model averaging (BMA) (Hoeting et al., 1999) is a sound solution to the uncertainty which characterizes the identification of the supposedly best model for a certain data set; given a set of alternative models, BMA weights the inferences produced by the various models, using the models\u2019 posterior probabilities as weights. BMA assumes the data to be generated by one of the considered models; under this assumption, it provides better predictive accuracy than any single model (Hoeting et al., 1999). However, such an assumption is generally not true; for this reason, on real data sets BMA does not generally perform very well; see the discussion and the references in Cerquides et al. (2005) for more details.", "startOffset": 45, "endOffset": 717}, {"referenceID": 2, "context": "The problem is that BMA gets excessively concentrated around the single most probable model (Domingos, 2000; Minka, 2002): especially on large data sets, \u201caveraging using the posterior probabilities to weight the models is almost the same as selecting the MAP model\u201d (Boull\u00e9, 2007). To overcome the problem of BMA getting excessively concentrated around the most probable model, a compression-based approach has been introduced in (Boull\u00e9, 2007); it computes more evenly-distributed weights, by applying a logarithmic smoothing to the models posterior probabilities. The compression-based weights, which can be justified from an information-theoretic viewpoint, have been used in Boull\u00e9 (2007) to average over different naive Bayes classifiers, characterized by different feature sets, obtaining excellent rank in international competitions on classification.", "startOffset": 268, "endOffset": 694}, {"referenceID": 28, "context": "Alternative methods to aggregate SPODEs, more complex than AODE, have been considered (Yang et al., 2007), but AODE generally outperforms them: \u201cAODE, which simply linearly combines every SPODE without any selection or weighting, is actually more effective than the majority of rival schemes\u201d.", "startOffset": 86, "endOffset": 105}, {"referenceID": 28, "context": "As reported in (Cerquides et al., 2005; Yang et al., 2007), AODE outperforms aggregating SPODEs via BMA; in both (Yang et al.", "startOffset": 15, "endOffset": 58}, {"referenceID": 28, "context": ", 2007), AODE outperforms aggregating SPODEs via BMA; in both (Yang et al., 2007; Cerquides et al., 2005) the best results were instead obtained using an algorithm (called MAPLMG), which estimates the most probable linear mixture of SPODEs; this overcomes the problem of assuming a single SPODE to be the true model.", "startOffset": 62, "endOffset": 105}, {"referenceID": 5, "context": "We address this problem by adopting the paradigm of credal classification (Corani et al., 2012; Corani and Zaffalon, 2008b), namely drop-", "startOffset": 74, "endOffset": 123}, {"referenceID": 23, "context": "Alternative methods to aggregate SPODEs, more complex than AODE, have been considered (Yang et al., 2007), but AODE generally outperforms them: \u201cAODE, which simply linearly combines every SPODE without any selection or weighting, is actually more effective than the majority of rival schemes\u201d. As reported in (Cerquides et al., 2005; Yang et al., 2007), AODE outperforms aggregating SPODEs via BMA; in both (Yang et al., 2007; Cerquides et al., 2005) the best results were instead obtained using an algorithm (called MAPLMG), which estimates the most probable linear mixture of SPODEs; this overcomes the problem of assuming a single SPODE to be the true model. In this paper, we address this problem by means of the compression coefficients. As a preliminary step we develop BMA-AODE, namely BMA over SPODEs, with some computational differences with respect to the framework of Yang et al. (2007) and Cerquides et al.", "startOffset": 87, "endOffset": 898}, {"referenceID": 23, "context": "Alternative methods to aggregate SPODEs, more complex than AODE, have been considered (Yang et al., 2007), but AODE generally outperforms them: \u201cAODE, which simply linearly combines every SPODE without any selection or weighting, is actually more effective than the majority of rival schemes\u201d. As reported in (Cerquides et al., 2005; Yang et al., 2007), AODE outperforms aggregating SPODEs via BMA; in both (Yang et al., 2007; Cerquides et al., 2005) the best results were instead obtained using an algorithm (called MAPLMG), which estimates the most probable linear mixture of SPODEs; this overcomes the problem of assuming a single SPODE to be the true model. In this paper, we address this problem by means of the compression coefficients. As a preliminary step we develop BMA-AODE, namely BMA over SPODEs, with some computational differences with respect to the framework of Yang et al. (2007) and Cerquides et al. (2005); our results confirm however that BMA over SPODEs is outperformed by AODE.", "startOffset": 87, "endOffset": 926}, {"referenceID": 15, "context": "A common choice is to adopt a uniform mass function, as we do in both BMA-AODE and COMP-AODE; this however can be criticized from different standpoints; see for instance the rejoinder in Hoeting et al. (1999). In Boull\u00e9 (2007), a prior which favors simpler models over complex ones is adopted.", "startOffset": 187, "endOffset": 209}, {"referenceID": 2, "context": "In Boull\u00e9 (2007), a prior which favors simpler models over complex ones is adopted.", "startOffset": 3, "endOffset": 17}, {"referenceID": 21, "context": "ping the unique prior in favor of a set of priors (prior credal set) (Levi, 1980).", "startOffset": 69, "endOffset": 81}, {"referenceID": 13, "context": "Despite the biased estimate of probabilities due to the above (so-called naive) assumption, naive Bayes performs well under 01 loss (Domingos and Pazzani, 1997); it thus constitutes a reasonable choice if the goal is simple classification, without the need for accurate probability estimates; it is especially competitive on data sets of small and medium size , thanks to its low variance error (Friedman, 1997).", "startOffset": 132, "endOffset": 160}, {"referenceID": 16, "context": "Despite the biased estimate of probabilities due to the above (so-called naive) assumption, naive Bayes performs well under 01 loss (Domingos and Pazzani, 1997); it thus constitutes a reasonable choice if the goal is simple classification, without the need for accurate probability estimates; it is especially competitive on data sets of small and medium size , thanks to its low variance error (Friedman, 1997).", "startOffset": 395, "endOffset": 411}, {"referenceID": 17, "context": "Generally, TAN outperforms naive Bayes in classification (Friedman et al., 1997).", "startOffset": 57, "endOffset": 80}, {"referenceID": 27, "context": "The AODE classifier (Webb et al., 2005) is an ensemble of k SPODE (SuperParent One Dependence Estimator) classifiers; each SPODE is characterized by a certain super-parent feature, so that the other features are modeled as depending on both the class and the super-parent, as shown in in Fig.", "startOffset": 20, "endOffset": 39}, {"referenceID": 28, "context": "This computational schema has been adopted to implement BMA over SPODEs in (Cerquides et al., 2005; Yang et al., 2007), and has been outperformed by AODE.", "startOffset": 75, "endOffset": 118}, {"referenceID": 8, "context": "Therefore, a model can perform badly at classification despite having high marginal likelihood (Cowell, 2001; Kontkanen et al., 1999); for this reason, scoring rules more appropriate for classification should be considered.", "startOffset": 95, "endOffset": 133}, {"referenceID": 20, "context": "Therefore, a model can perform badly at classification despite having high marginal likelihood (Cowell, 2001; Kontkanen et al., 1999); for this reason, scoring rules more appropriate for classification should be considered.", "startOffset": 95, "endOffset": 133}, {"referenceID": 2, "context": "Following Boull\u00e9 (2007), we thus substitute the marginal likelihood with conditional likelihood: Lj := n", "startOffset": 10, "endOffset": 24}, {"referenceID": 14, "context": "We remove from the ensemble the SPODEs whose conditional likelihood is smaller than Lmax/10, where Lmax is the maximum conditional likelihood among all SPODEs; discarding models with very low posterior probability is in fact common when dealing with BMA; this procedure can be seen as a belief revision (Dubois and Prade, 1997).", "startOffset": 303, "endOffset": 327}, {"referenceID": 9, "context": "Dash, who published several works on BMA (Dash and Cooper, 2004).", "startOffset": 41, "endOffset": 64}, {"referenceID": 26, "context": "BMA-AODE*: Extending BMA-AODE to Sets of Probabilities By BMA-AODE* we extend BMA-AODE to imprecise probabilities (Walley, 1991), allowing multiple specifications of the prior mass function P (S); we denote the credal set containing such prior mass functions as P(S).", "startOffset": 114, "endOffset": 128}, {"referenceID": 24, "context": "Yet, this would generate vacuous posterior inferences, thus preventing learning from data (Piatti et al., 2009).", "startOffset": 90, "endOffset": 111}, {"referenceID": 25, "context": "This issue has been addressed in Yang et al. (2007) by means of high numerical precision: \u201cBMA often lead to arithmetic overflow when calculating very large exponentials or factorials.", "startOffset": 33, "endOffset": 52}, {"referenceID": 5, "context": "When dealing with prior-dependent instances, credal classifiers (Corani et al., 2012; Corani and Zaffalon, 2008b) become indeterminate, by returning a set of classes instead of a single class.", "startOffset": 64, "endOffset": 113}, {"referenceID": 25, "context": "We point the reader to (Troffaes, 2007) for a discussion of alternative criteria for taking decisions under imprecise probabilities.", "startOffset": 23, "endOffset": 39}, {"referenceID": 2, "context": "Compression-Based averaging Compression-based averaging has been introduced by (Boull\u00e9, 2007) as a remedy against the tendency of BMA at getting excessively concentrated around the most probable model, which indeed deteriorates the performances (Boull\u00e9, 2007; Domingos, 2000).", "startOffset": 79, "endOffset": 93}, {"referenceID": 2, "context": "Compression-Based averaging Compression-based averaging has been introduced by (Boull\u00e9, 2007) as a remedy against the tendency of BMA at getting excessively concentrated around the most probable model, which indeed deteriorates the performances (Boull\u00e9, 2007; Domingos, 2000).", "startOffset": 245, "endOffset": 275}, {"referenceID": 12, "context": "Compression-Based averaging Compression-based averaging has been introduced by (Boull\u00e9, 2007) as a remedy against the tendency of BMA at getting excessively concentrated around the most probable model, which indeed deteriorates the performances (Boull\u00e9, 2007; Domingos, 2000).", "startOffset": 245, "endOffset": 275}, {"referenceID": 2, "context": "It has been shown (Boull\u00e9, 2007) that LL0 = \u2212nH(C), where H(C) := \u2212 \u2211 c\u2208C P (c) logP (c) is the entropy 2 of the class.", "startOffset": 18, "endOffset": 32}, {"referenceID": 2, "context": "Following (Boull\u00e9, 2007), we keep in the ensemble only the feasible models, namely those with \u03c0j > 0; we instead discard the models with \u03c0j < 0.", "startOffset": 10, "endOffset": 24}, {"referenceID": 2, "context": "The compression coefficients can be justified as follows (Boull\u00e9, 2007): LLj+logP (sj) \u201crepresents the quantity of information required to encode the model plus the class values given the model.", "startOffset": 57, "endOffset": 71}, {"referenceID": 4, "context": "COMP-AODE performs a weighted linear combination of probabilities estimated by different models; in risk analysis, a weighted linear combination of probabilities estimated by different experts is referred to as linear opinion pool (Clemen and Winkler, 1999).", "startOffset": 231, "endOffset": 257}, {"referenceID": 15, "context": "We discretize numerical features by the entropy-based method of (Fayyad and Irani, 1993).", "startOffset": 64, "endOffset": 88}, {"referenceID": 11, "context": "4 The Wilcoxon signed-rank test is indeed recommended for comparing two classifiers on multiple data sets (Demsar, 2006): being non-parametric it avoids strong assumptions and robustly deals with outliers.", "startOffset": 106, "endOffset": 120}, {"referenceID": 28, "context": "The fact that AODE outperforms BMAAODE could be expected; the same finding was already given in (Yang et al., 2007) and in (Cerquides et al.", "startOffset": 96, "endOffset": 115}, {"referenceID": 2, "context": "These positive results with the compression-based approach broaden the scope of the experiments of (Boull\u00e9, 2007), in which the compression approach was applied to an ensemble of naive Bayes classifiers.", "startOffset": 99, "endOffset": 113}, {"referenceID": 29, "context": "A theoretical justification for discounted-accuracy has been given by Zaffalon et al. (2011) showing that, within a betting framework based on fairly general assumptions, discounted-accuracy is the only score which satisfies some fundamental properties for assessing both determinate and indeterminate classifications.", "startOffset": 70, "endOffset": 93}, {"referenceID": 29, "context": "A theoretical justification for discounted-accuracy has been given by Zaffalon et al. (2011) showing that, within a betting framework based on fairly general assumptions, discounted-accuracy is the only score which satisfies some fundamental properties for assessing both determinate and indeterminate classifications. Yet Zaffalon et al. (2011) also shows some severe limits of discounted-accuracy, which we illustrate by means of an example: we consider two different medical doctors, doctors random and doctor vacuous, who should decide whether a patient is healthy or", "startOffset": 70, "endOffset": 346}, {"referenceID": 22, "context": "In fact, under risk-aversion, the expected utility increases with expectation of the rewards and decreases with their variance (Levy and Markowitz, 1979).", "startOffset": 127, "endOffset": 153}, {"referenceID": 21, "context": "In fact, under risk-aversion, the expected utility increases with expectation of the rewards and decreases with their variance (Levy and Markowitz, 1979). To capture this point it is necessary introducing a utility function, to be then applied on the discounted-accuracy score assigned on each instance. In Zaffalon et al. (2011) the utility function is designed as follows: the utility of a correct and determinate classification (discounted-accuracy 1) is 1; the utility of a wrong classification (discounted-accuracy 0) is 0; the utility of an accurate but indeterminate classification consisting of two classes (discounted-accuracy 0.", "startOffset": 128, "endOffset": 330}, {"referenceID": 10, "context": "In del Coz and Bahamonde (2009) classifiers which return indeterminate classifications are scored through the F1-metric, originally designed for Information Retrieval tasks.", "startOffset": 7, "endOffset": 32}, {"referenceID": 10, "context": "In del Coz and Bahamonde (2009) classifiers which return indeterminate classifications are scored through the F1-metric, originally designed for Information Retrieval tasks. The F1 metric, when applied to indeterminate classifications, returns a score which is always comprised between u65 and u80, further confirming the reasonableness of both utility functions. More details on the links between F1, u65 and u80 are given in Zaffalon et al. (2012). We remark that in real applications the utility function should", "startOffset": 7, "endOffset": 450}, {"referenceID": 0, "context": "However, over time algorithms more sophisticated than NCC have been developed, such as: \u2022 credal model averaging (CMA) (Corani and Zaffalon, 2008a), namely a generalization of BMA (in the same spirit of BMA-AODE) for naive Bayes classifier; \u2022 credal decision tree (CDT) (Abell\u00e1n and Moral, 2005), namely an extension of classification trees to imprecise probability.", "startOffset": 270, "endOffset": 295}, {"referenceID": 11, "context": "We then compare CDT, CMA and COMP-AODE* via the Friedman test; this is the approach recommended by (Demsar, 2006) for comparing multiple classifiers on multiple data sets.", "startOffset": 99, "endOffset": 113}, {"referenceID": 18, "context": "Some comments on credal classification vs reject option Determinate classifiers can be equipped with a reject option (Herbei and Wegkamp, 2006), thus refusing to classify an instance if the posterior probability of the most probable class is less than a threshold.", "startOffset": 117, "endOffset": 143}, {"referenceID": 2, "context": "Instead the COMP-AODE classifier proposed here, which applies the compression-based approach over SPODEs, obtains overall slightly better classification performance than AODE; our results thus broadens the scope of (Boull\u00e9, 2007), in which the compression-based approach was applied over an ensemble of naive Bayes classifiers.", "startOffset": 215, "endOffset": 229}], "year": 2014, "abstractText": "Bayesian model averaging (BMA) is a common approach to average over alternative models; yet, it usually gets excessively concentrated around the single most probable model, therefore achieving only sub-optimal classification performance. The compression-based approach (Boull\u00e9, 2007) overcomes this problem; it averages over the different models by applying a logarithmic smoothing over the models\u2019 posterior probabilities. This approach has shown excellent performances when applied to ensembles of naive Bayes classifiers. AODE is another ensemble of models with high performance (Webb et al., 2005): it consists of a collection of non-naive classifiers (called SPODE) whose probabilistic predictions are aggregated by simple arithmetic mean. Aggregating the SPODEs via BMA rather than by arithmetic mean deteriorates the performance; instead, we propose to aggregate the SPODEs via the compression coefficients and we show that the resulting classifier obtains a slight but consistent improvement over AODE. However, an important issue in any Bayesian ensemble of models is the arbitrariness in the choice of the prior over the models. We address this problem by adopting the paradigm of credal classification, namely by substituting the unique prior with a set of priors. Credal classifier are able to automatically recognize the prior-dependent instances, namely the instances whose most probable class varies, when different priors are considered; in these cases, credal classifiers remain reliable by returning a set of classes rather than a single class. We thus develop the credal version Corresponding author: giorgio@idsia.ch Preprint submitted to Elsevier January 1, 2014 of both the BMA-based and the compression-based ensemble of SPODEs, substituting the single prior over the models by a set of priors. By experiments we show that both credal classifiers provide overall higher classification reliability than their determinate counterparts. Moreover, the compression-based credal classifier compares favorably to previous credal classifiers.", "creator": "LaTeX with hyperref package"}}}