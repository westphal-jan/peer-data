{"id": "1204.1637", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Apr-2012", "title": "Characterization of Dynamic Bayesian Network", "abstract": "In this report, we will be interested at Dynamic Bayesian Network (DBNs) as a model that tries to incorporate temporal dimension with uncertainty. We start with basics of DBN where we especially focus in Inference and Learning concepts and algorithms. Then we will present different levels and methods of creating DBNs as well as approaches of incorporating temporal dimension in static Bayesian network.", "histories": [["v1", "Sat, 7 Apr 2012 13:55:29 GMT  (367kb)", "http://arxiv.org/abs/1204.1637v1", "9 pages, (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 2, No. 7, 2011"]], "COMMENTS": "9 pages, (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 2, No. 7, 2011", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nabil ghanmy", "mohamed ali mahjoub", "najoua essoukri ben amara"], "accepted": false, "id": "1204.1637"}, "pdf": {"name": "1204.1637.pdf", "metadata": {"source": "CRF", "title": "Characterization of Dynamic Bayesian Network The Dynamic Bayesian Network as temporal network", "authors": ["Nabil Ghanmi", "Mohamed Ali Mahjoub", "Najoua Essoukri Ben Amara"], "emails": ["Nabil.ghnamy@gmail.com", "medali.mahjoub@ipeim.rnu.tn", "Najoua.benamara@eniso.rnu.tn"], "sections": [{"heading": null, "text": "In fact, it is the case that we will be able to be in a position to find a solution that is capable, that we are able to remain in the world."}, {"heading": "1) Exact Inference:", "text": "rE \"s rf\u00fc ide rf\u00fc ide f\u00fc die r die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f die f e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e"}, {"heading": "2) Approximate inference:", "text": "In fact, it is such that most of us are in a position to embark on a search for a solution that is in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a position, in which they are in a world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live,"}, {"heading": "IV. DIFFERENT APPROACHES FOR INCORPORATING TIME IN BAYESIAN NETWORK", "text": "Dynamic Bayesian networks (DBN) are an extension of Bayesian networks that represent the temporal or spatial evolution of random variables. There are several models for incorporating time into the representation of networks. These models can be divided into three broad categories: models that use static BNs and formal grammars to represent the temporal dimension (temporal probabilistic networks (TPNs) models that use a mixture of multiple probabilistic framework models Models that use time nodes in static BNs to represent time dependencies. The first two models are designed for specific purposes and have a very limited application, so we will focus on the thirtieth models. Probabilistic Temporal Networks (PTN)"}, {"heading": "1) Definition", "text": "A probabilistic temporal network (PTN) is defined as a model that represents time information while fully respecting probabilistic semantics. In a PTN, the nodes of the graph are the aggregates of time and the arcs are causal and / or temporal relationsThis type of network uses grammatical rules to express temporal dependencies in the structure of Bayesian networks: Preserving the structure of static Bayesian networks allows the reuse of powerful techniques to infer this particular type of networks. Grammar introduces temporal relationships between events."}, {"heading": "2) Temporal Reasoning", "text": "In PTN, the temporal reasoning is based on the interval algebra [8] introduced in 1983 by James F. Allen, a calculation that defines the possible relationships between time intervals and provides a composition table that can be used as a basis for arguing about descriptions of temporal events. The following 13 basic relationships capture possible relationships between two intervals and are presented in the following table: Bayesian Dynamic Networks Generalize Hidden Markov Models (HMM) and Linear Dynamic Systems (LDS) by presenting the hidden states (and seen) as state variables with complex interdependencies. HMMs are used to continuously represent discrete states (variables).The combination of these two structures to create a mixed state DBN. This type of model was introduced and applied to the recognition of human gestures [9]. Pure probabilistic DBN is considered in this section to represent a graph between two interdependencies (two nodes)."}, {"heading": "1) Extension of BNs toward DBNs", "text": "A static Bayesian network can be expanded in many ways to represent a time process; these extensions can be grouped into five categories: 1- Adding the history of a node to explicitly express the temporal aspect of the Bayesian network. 2- Selecting from a library of pre-developed Bayesian networks appropriate to the current state. 3- Changing the dynamics of the network structure. 4- Repeat the traditional network for each step by introducing Bayesian networks to represent events. 5- Repeat the classic Bayesian network by adding arcs that represent the temporal dependencies of a span of time.The networks of the first category can be considered purely static BNs to which an additional node is added to represent past information in time."}, {"heading": "2) Dynamic change in the structure of DBNs", "text": "Changes in the structure of a DBN can be: - Changing the network settings (values of the table of conditional probabilities CPT) of a time intersection to another - Adding or deleting new nodes and / or arcs to the structure of the BN. The structural changes of a DBN (addition or deletion of edges or nodes) are a complex problem and cannot simply be generalized. Below, we are interested in changing parameters (CPT) system. In [10] Zweig and Russell presented a model that uses decomposition techniques to represent dynamic situations in real life. These dynamic processes can be broken down into several sequences. Such decomposition can be used for speech recognition or recognition of manuscripts. They found it more appropriate to depict dynamic processes (time) that generate an RB (a subnet) at each stage of the development of the process in order to model the whole process through a single BN. Each subnetwork must be learned from given observations."}, {"heading": "3) DBNs for events representation", "text": "This year, it is more than ever before in the history of the city, where it has gone down in history as never before."}, {"heading": "A. Definition", "text": "In a coupled HMM, each hidden variable (the state) is connected to its own observation, and it is also connected to its two closest neighbors in the time span, with the exception of the following variables, which belong to the chain boundary and each have a single closest neighbor (see Figure 4)."}, {"heading": "B. Parameters of coupled HMMs", "text": "Let a CHMM model be formed with L-coupled HMMs. This model is fully described and specifies the following parameters: Initial probabilities: is the number of states (hidden nodes) of the chain transition probabilities: probability of observation"}, {"heading": "C. Extension of the forward-backword algorithm for CHMM:", "text": "As with conventional HMMs, we use the forward backword algorithm to calculate L-coupled HMMs. There, each observation is a vector. Since L-HMMs are coupled, the variables forward and backword should be defined jointly for all HMMs. In other words, we define the forward variable as follows: And the backword variable as follows: Therefore, we can calculate the two variables inductively as follows: And the probability function can be calculated as follows:"}, {"heading": "D. EM algorithm for learning parameters of CHMM", "text": "As in the case of traditional HMMs, the two basic steps of the TheEM algorithm, as described in [3]: Estimation step: Given the observations O, the parameters to estimateand the objective function, we construct anauxiliary function: that represent the expectation of all sequences of possible states, given the observationsO and the current parameters estimated9 | P a g ehttp: / ijacsa.thesai.org / Maximization step: In the exact EM algorithm, the role of this step is toestimate the new parameters as follows: VIII. MOTIVATION OF USING CHMMMMAccording to its definition, a coupled HMM is considered a collection of HMMs, one for each data stream, where the discrete nodes time t for each HMM are conditioned by the discrete nodes time."}], "references": [{"title": "Speech Recognition with Dynamic Bayesian networks,Bayesian networks", "author": ["G. ZWEIG"], "venue": "PhD thesis,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Variational Infererence in Probabilistic Models", "author": ["N. Lawrence"], "venue": "PhD thesis,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Markov chain monte carlo in practice", "author": ["W. Gilks", "S. Richardson", "D. Spiegelhalter"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Probabilistic reasoning in intelligent systems: Networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1991}, {"title": "S.Mian: Modelling Gene Expression Data using Dynamic Bayesian Natwork", "author": ["K. Muphy"], "venue": "Technical Report, Computer Science Division,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "An Introduction to Bayesian Network Theory and Usage", "author": ["T.A. Stephenson"], "venue": "IDIAP Research Report 00-03,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Time series classi_cation using mixed-state ynamic Bayesian networks", "author": ["V. Pavlovic", "B. Frey", "T.S. Huang"], "venue": "in Proc. IEEE CVPR,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Compositional Modelling With DPNs", "author": ["G.Zweig", "S. Russel"], "venue": "Report N. UCB/CSD-97-970,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Probabilistic Independence Networks for Hidden Markov Probability Models", "author": ["P. Smyth", "D. Heckerman", "M. Jordan"], "venue": "Technical Report MSR-TR-96-03, Microsoft Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "Dynamic Bayesian Networks: Representation, Inference and Leaning", "author": ["K. Murphy"], "venue": "PhD thesis Univesity of Califonia, Berkely,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Event-coupled hidden Markov models", "author": ["T.T. Kristjansson", "B.J. Frey", "T. Huang"], "venue": "In Proc. IEEE Int. Conf. On Multimedia and Exposition,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "R\u00e9seaux Bay\u00e9siens Dynamiques pour la V\u00e9rification du Locuteur", "author": ["E. Sanchez"], "venue": "PhD thesis Telecom Paris,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Input-Output HMMs for sequence processing", "author": ["Y. Bengio", "P. Frasconi"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "New approach using Bayesian Network to improve content based image classification systems", "author": ["K. Jayech", "MA Mahjoub"], "venue": "IJCSI International Journal of Computer Science Issues,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Clustering and Bayesian Network to improve content based image classification systems", "author": ["K. Jayech", "MA Mahjoub"], "venue": "International Journal of Advanced Computer Science and Applications- a Special Issue on Image Processing and Analysis,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "kalti \u201cSoftware Comparison dealing with Bayesian networks", "author": ["K. MA Mahjoub"], "venue": "Lecture Notes in Computer Science (LNCS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}], "referenceMentions": [{"referenceID": 13, "context": "They bring us four advantages as a data modeling tool [16,17,18] A dynamic Bayesian network can be defined as a repetition of conventional networks in which we add a causal one time step to another.", "startOffset": 54, "endOffset": 64}, {"referenceID": 14, "context": "They bring us four advantages as a data modeling tool [16,17,18] A dynamic Bayesian network can be defined as a repetition of conventional networks in which we add a causal one time step to another.", "startOffset": 54, "endOffset": 64}, {"referenceID": 15, "context": "They bring us four advantages as a data modeling tool [16,17,18] A dynamic Bayesian network can be defined as a repetition of conventional networks in which we add a causal one time step to another.", "startOffset": 54, "endOffset": 64}, {"referenceID": 0, "context": "The Junction Tree Algorithm [1] is an algorithm similar to the Baum-Welch algorithm used in HMM.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "a) Variational methods The simplest example is the approximation by the average (mean-field approximation) [2], which exploits the law of large numbers to approximate large sums of random variables by their average.", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "b) Monte Carlo The easiest Monte Carlo Method [3] is the Importance Sampling (IS) that produces a large number of samples x from the unconditional distribution of hidden variables) then we give weight to samples based on their likelihood (where y is the observation).", "startOffset": 46, "endOffset": 49}, {"referenceID": 3, "context": "c) Loopy Belief propagation We apply the algorithm of Pearl [4] to the original graph even if it contains loops.", "startOffset": 60, "endOffset": 63}, {"referenceID": 4, "context": "From these considerations, there are 4 possible cases of learning [5]:", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "To accomplish the task of structural learning, we need [6]: - an algorithm to find the different possible structures - a metric for comparing the possible structures to each other The structure learning algorithms can be classified into two broad categories.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "This type of model was introduced and applied to the recognition of human gestures [9]", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "In [10] Zweig and Russell presented a model that uses decomposition techniques to represent dynamic situations real.", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "The formalism and algorithms remain the same [11].", "startOffset": 45, "endOffset": 49}, {"referenceID": 9, "context": "If you change the tables of probability distributions (discrete tables) by continuous distributions (eg Gaussian), then it also becomes possible to represent models based on Kalman filters [12].", "startOffset": 189, "endOffset": 193}, {"referenceID": 10, "context": "Figure 4 (b) is a specific coupling of HMM described in [13] as an event coupled HMM.", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "Input / Output HMM (Figure 4 (c)) [15] represents a promising alternative to the use of a hidden Markov model.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "org/ The factorial HMM (Figure 3 (d)) [14] is a model used to represent systems in which the hidden states are made from a set of decoupled dynamical systems and with only one observation available.", "startOffset": 38, "endOffset": 42}, {"referenceID": 2, "context": "As in the case of traditional HMMs, the two basic steps of the EM algorithm as described in [3] are: \uf0b7 Estimation step: Given the observations O, the parameters to estimate and the objective function , we construct an auxiliary function:", "startOffset": 92, "endOffset": 95}], "year": 2011, "abstractText": "In this report, we will be interested at Dynamic Bayesian Network (DBNs) as a model that tries to incorporate temporal dimension with uncertainty. We start with basics of DBN where we especially focus in Inference and Learning concepts and algorithms. Then we will present different levels and methods of creating DBNs as well as approaches of incorporating temporal dimension in static Bayesian network. KeywordsDBN, DAG, Inference, Learning, HMM, EM Algorithm, SEM, MLE, coupled HMMs", "creator": "Conv2pdf.com"}}}