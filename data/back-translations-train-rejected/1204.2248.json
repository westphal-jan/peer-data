{"id": "1204.2248", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Apr-2012", "title": "Robust Spatio-Temporal Signal Recovery from Noisy Counts in Social Media", "abstract": "Many real-world phenomena can be represented by a spatio-temporal signal: where, when, and how much. Social media is a tantalizing data source for those who wish to monitor such signals. Unlike most prior work, we assume that the target phenomenon is known and we are given a method to count its occurrences in social media. However, counting is plagued by sample bias, incomplete data, and, paradoxically, data scarcity -- issues inadequately addressed by prior work. We formulate signal recovery as a Poisson point process estimation problem. We explicitly incorporate human population bias, time delays and spatial distortions, and spatio-temporal regularization into the model to address the noisy count issues. We present an efficient optimization algorithm and discuss its theoretical properties. We show that our model is more accurate than commonly-used baselines. Finally, we present a case study on wildlife roadkill monitoring, where our model produces qualitatively convincing results.", "histories": [["v1", "Tue, 10 Apr 2012 18:56:29 GMT  (908kb,D)", "http://arxiv.org/abs/1204.2248v1", "16 pages"]], "COMMENTS": "16 pages", "reviews": [], "SUBJECTS": "cs.AI cs.SI", "authors": ["jun-ming xu", "aniruddha bhargava", "robert nowak", "xiaojin zhu"], "accepted": false, "id": "1204.2248"}, "pdf": {"name": "1204.2248.pdf", "metadata": {"source": "CRF", "title": "Robust Spatio-Temporal Signal Recovery from Noisy Counts in Social Media", "authors": ["Jun-Ming Xu", "Aniruddha Bhargava", "Robert Nowak", "Xiaojin Zhu"], "emails": ["xujm@cs.wisc.edu,", "aniruddha@wisc.edu,", "nowak@ece.wisc.edu,", "jerryzhu@cs.wisc.edu"], "sections": [{"heading": "1 Introduction", "text": "Many real-world phenomena of interest to science are spatio-temporal in nature. They can be characterized by a real-rated intensity function f = R = 0, in which the value fs, t quantifies the prevalence of the phenomenon on the spot. Examples include animal mortality, algal blooms, hail damage, and seismic intensity. Direct instrumental recording of f is often difficult and expensive. Social media provide a unique way to perceive such spatio-temporal signals, in which users play the role of \"sensors\" by posting their experiences with a target phenomenon. For example, the user willingly posts their encounters with dead animals."}, {"heading": "2 The Socioscope", "text": "We propose Socioscope, a probabilistic model that robustly restores spatio-temporal signals from social media data. Formally, f is defined on discrete spatio-temporal papers. A trash can (s, t) could, for example, be a US state per day t or a county s per hour t. From the first stage, we get xs, t, the number of target social media posts within this trash. The task is to fs, t from xs, t. A commonly used estimate is f, t = xs, t. This estimate can be justified as the maximum probability estimate of a Poisson model x-Poisson (f). This idea underscores several emerging systems such as the earthquake damage monitoring of Twitter [12]. However, this estimate is unsatisfactory, since the counts xs, t themselves can be noisy: As mentioned above, the estimate ignores the bias of the population - more posts become a target when there are no users and no users."}, {"heading": "2.1 Correcting Human Population Bias", "text": "Suppose that each target post contains precise location and time metadata, which allows us to count xj, the number of target posts in bin j. Given xj, it is tempting to use the maximum probability estimate f-j = xj, which is based on a simple Poisson model xj-Poisson (fj). However, this model is too naive: even if fj-fk, for example, is equal to the level of squirrel activity in two containers, we would expect xj-xk if there are more people in bin j than in bin k, simply because more people see the same group of squirrels. To explain this population bias, we define an \"active population intensity of social media\" (loose term \"human population\" below) g = (g1,.., gn) > Rn-Rn-0. Let zj be the number of all social media posts in bin j, the vast majority of which does not relate to the target phenomenon."}, {"heading": "2.2 Handling Noisy and Incomplete Data", "text": "This would have been the end of the story if we could reliably assign each post to a source that we clearly identify as the source. Unfortunately, this is often not the case for social media. In this work, we focus on the problem of spatial uncertainty due to noisy or incomplete social media data. A prime example of spatial uncertainty is the lack of position metadata in Twitter posts (called tweets).1 In the most recent data we have collected, only 3% of tweets change automatically as the user travels and thus cannot be the true place where a tweet is posted. The remaining 50% contain no position at all. Clearly, we cannot reliably assign the latter two types of tweets to a spatial source. 2To address this problem, we borrow an idea of Positron Emission Tomography [28]."}, {"heading": "2.3 Socioscope: Penalized Poisson Likelihood Model", "text": "We observe the number of targets x = (x1,.., xm) in the detector containers. These are modelled as independently distributed Poisson random variables: xi \u0445 Poisson (hi), for i = 1.. M. (5) The log probability factors as \"(f) = log m, i = 1 hxii e \u2212 hi xi! = m, i = 1 (xi log, hi \u2212 hi) + c, (6) where c is a constant. In (6) we treat g as given. Target points can be scarce in some detector containers. In fact, we often have zero targets for the wildlife case study to be discussed later. This problem can be mitigated by the fact that many real-world phenomena are spatially smooth, i.e.,\" adjacent \"source containers in space or in time we have a similar intensity."}, {"heading": "2.4 Optimization", "text": "We solve the socioscopic optimization problem (9) with the BFGS, a quasi-Newton method [20]. The gradient can be easily calculated, where r = (r1... rm) is a ratio vector with ri = xi / hi and H is a diagonal matrix with Hjj = \u0443 (r \u2212 1), (10). We initialize with the following heuristics. Taking into account the count x and the transition matrix P, we calculate the least square projection between 0 and 2. This projection is easy to calculate. However, \u03b70 may contain negative components that are not suitable for the Poisson intensity. We force positivity by setting the maximum (10 \u2212 4, through0) element by element, with the bottom 10 \u2212 4 ensuring that the protocol is more efficient within 11 \u2212 15 seconds."}, {"heading": "2.5 Parameter Tuning", "text": "The selection of the regularization parameter \u03bb has a profound influence on the smoothness of the estimates. It may be possible to select these parameters on the basis of prior knowledge of certain problems, but for our experiments we select these parameters using a cross-validation (CV) procedure that provides us with a fully data-based and objective approach to regularization. CV is quite easy to implement in the Poisson setting. A hold-out set of data can be constructed uniformly randomly by simple sub-sampling of events from the total observation. This generates a partial set of a subset of counts that follows exactly the same distribution as the total set, modulo a reduction in the total intensity per subsampling. The addition of the hold-out set is what remains of the complete data set, and we call the training set that assumes exactly the same distribution of the total CV as the total amount of the sum of the sum of the sum of the probability we assume as a fraction of the specific Hold."}, {"heading": "2.6 Theoretical Considerations", "text": "The natural measure of signal noise in this problem is the number of counts in each recycle bin. The higher the counts, the more stable and \"less noisy\" our estimators will be. In fact, larger counts, due to greater underlying intensities, result in small errors on a relative scale. However, the accuracy of our recovery also depends on the regularity of the underlying function f. If it is very smooth, for example a constant function, then the error would be inversely proportional to the total number of counts, not the number in each recycle bin, f is determined by a single constant. To give some insight into the dependence of the estimate on the total number of counts, we assume that f is the underlying continuous intensity function of interest."}, {"heading": "3 Related Work", "text": "There is no comparable previous work that focuses on robust single recovery from social media (i.e., the \"second stage,\" as we mentioned in the introduction), but there has been considerable coherent work at the first stage, which we summarize below. Current research focuses on user-generated content on the Web and on spatio-temporal variation of topics based on their topics. Early work in this direction began with a message text transcribed from news wire and other media [1]. Recent research has focused on user-generated content on the Web and on spatio-temporal variation of topics. Latent Dirichlet Allocation (LDA) is a popular uncontrolled method for recognizing topics. Mei et al al al. [18] expanded LDA by considering the spatio-temporal context to identify subtopics from web logs you analyzed to enrich the topic-temporal pattern."}, {"heading": "4 A Synthetic Experiment", "text": "The synthetic experiment corresponds to the case study in the next section. There are 48 US continental states plus Washington DC, and T = 24 hours. This results in a sum of n = 1176 source data, and m = (2 x 49 + 1) T = 2376 detector doses. The transition matrix P is the same as in the case study that will be discussed later. The total number z is derived from actual Twitter data and g = z (1). We design the ground truth target signal f x to be constant in time but spatially vary. Figure 1 (a) shows the basic truth f spatial. It is a mixture of two Gaussian distributions that are dis-cretized at the state level. The modes are in Washington and New York, respectively. From P, f and g we generate the observed target mail for each detector using a Poisson random number."}, {"heading": "5 Case Study: Roadkill", "text": "We now turn to a real-life task of estimating the spatial and temporal intensity of the roadkill for several common wildlife species through Twitter posts. The study of the roadkill has values in the areas of ecology, nature conservation, and transportation safety. The target phenomenon consists of roadkill events for a specific species within the continental United States between September 22 and November 30, 2011. Our spatio-temporal source containers are state \u00b7 hour-of-the-day. We index the 48 continental U.S. states plus the District of Columbia. We aggregate the 10-week study period into 24 hours of one day. Target counts x are still sparse even when aggregated: For example, most state-hour combinations have zero counts for armadillos and the largest count in x (1) and x (2) is 3. Therefore, restoring the underlying signal f remains a challenge. Do not index the hours from 1 to 24. This results in s | | = 49, | = = 1 (source = = 1t = = 17m or 1)."}, {"heading": "5.1 Data Preparation", "text": "All tweets contain time metadata. However, most tweets do not contain location metadata, as discussed earlier. 5.1.1 Total counts z (1) and human population intensity g.To get the total count z, we collected tweets via the Twitter Stream API, supplying a subsample of all tweets (not just target posts) with geo-tag. Therefore, all of these tweets contain precise longitude and latitude in which they were created. By using a database with reverse geocoding (http: / / www.datasciencetoolkit.org), we assigned the coordinates to a US state. There are a large number of such tweets. Counting the number of tweets in each state hour bin gave us z (1), from which g is estimated. Figure 2 shows the estimated g. The x-axis is the hour of the day and the rigid axis of the states, ordered by the length of each state hour, which z (1) can be calculated from the second hour (1) of the first hour (1)."}, {"heading": "5.1.2 Identifying Target Posts to Obtain Counts x.", "text": "Although not part of Socioscope, we detail this pre-processing step here for reproducibility. In Step 1, we collected tweets using a keyword API. Each tweet must include the animal name (e.g. \"squirrel (s)\") and the phrase \"run over.\" We received 5,857 tweets from squirrels, 325 Chipmunk tweets, 180 opossum tweets and 159 Armadillo tweets during the study period. However, many such tweets did not actually contain roadkill events. For example, \"I almost ran over an Armadillo on my longboard, luckily my feline reflexes saved me.\" Clearly, the author did not kill the Armadillo tweets. In Step 2, we built a binary text classifier to identify the targets below them. Subsequently [26], the tweets were folded wide without any plugging or stop-word removal."}, {"heading": "5.1.3 Constructing the Transition Matrix P.", "text": "In this study, P characterizes the fraction of tweets that were actually generated in the source trash (s, t) and end up in the three detector trash: exact location st (1), potentially noisy location st (2) and missing location t (3). We define P as follows: P (s, t) (1), (s, t) = 0.03 and P (r, t) (1), (s, t) = 0 for location 6 = s, to reflect the fact that we know exactly 3% of the location of the target post. P (r, t) (2), (s, t) = 0.47Mr, s for all r, s.M is a 49 x 49 \"falsely self-declared\" matrix.Mr, s is the probability that a user declares in her profile that she is in state r, but her contribution is actually generated in state r."}, {"heading": "5.1.4 Specifying the Graph Regularizer.", "text": "Our diagram shows two types of edges: temporal edges connect source containers with the same state and adjacent hours by weight. Spatial edges connect source containers with the same hour and adjacent states by weight ws. The regularization weight \u03bb was absorbed in wt and ws. We matched the weights wt and ws with CV to the 2D grid {10 \u2212 3, 10 \u2212 2.5,..., 103} 2."}, {"heading": "5.2 Results", "text": "The results on four animals: armadillos, chipmunks, squirrels, opossums. Perhaps surprisingly, precise road kill intensities for these animals are unknown to science (This serves as a good example of the value the socioscope can provide to scientists). Instead, the experts could only provide one range map for each animal, see the left column in Figure 3. These maps indicate presence / absence and were extracted by NatureServe [21]. Furthermore, the experts defined Armadillo and opossum as nocturnal, chipmunk as diurnal, and squirrels as both crepuscular (active at dusk) and diurnal. Due to the lack of quantitative soil truth, our comparison will inevitably be qualitative in nature. Socioscope provides reasonable estimates on these animals."}, {"heading": "6 Future Work", "text": "Socioscope is a first step towards this goal. There are many open questions: 1. We treated target posts as safe. In reality, a natural language processing system can often provide confidence. For example, a tweet could only be considered a target post with a probability of 0.8. It will be interesting to explore ways in which this trust can be incorporated into our framework. 2. The time lag and spatial shift between the target event and the creation of a post is commonplace, as discussed in footnote 2. Assessing an appropriate transition matrix P of social media data to enable Socioscope to deal with such point-spread functions remains a dream of the future. 3. It may be necessary to incorporate psychological factors in order to better model the human \"sensors.\" For example, a person may not bother tweeting about a Chipmunk street killer, but may be eager to do so if he sees an elk street killer. 4. Instead of using space and time in order to improve continuity, a point will be used."}, {"heading": "7 Acknowledgments", "text": "We thank Megan K. Hines of the Wildlife Data Integration Network for providing area maps and guidelines for wildlife."}], "references": [{"title": "Topic Detection and Tracking: Event-Based Information Organization", "author": ["James Allan"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Influence and correlation in social networks", "author": ["Aris Anagnostopoulos", "Ravi Kumar", "Mohammad Mahdian"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Beyond trending topics: Real-world event identification on twitter", "author": ["Hila Becker", "Naaman Mor", "Luis Gravano"], "venue": "In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Emerging topic detection on twitter based on temporal and social terms evaluation", "author": ["Mario Cataldi", "Luigi Di Caro", "Claudio Schifanella"], "venue": "In Proceedings of the Tenth International Workshop on Multimedia Data Mining, MDMKDD", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Event detection from flickr data through wavelet-based spatial analysis", "author": ["Ling Chen", "Abhishek Roy"], "venue": "In Proceedings of the 18th ACM conference on Information and knowledge management,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Spectral graph theory, Regional Conference Series in Mathematics, No. 92", "author": ["F.R.K. Chung"], "venue": "American Mathematical Society,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Concentration inequalities of the cross-validation estimate for stable predictors", "author": ["M. Cornec"], "venue": "Arxiv preprint arXiv:1011.5133,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Feedback effects between similarity and social influence in online communities", "author": ["David Crandall", "Dan Cosley", "Daniel Huttenlocher", "Jon Kleinberg", "Siddharth Suri"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "Mark my words!: linguistic style accommodation in social media", "author": ["Cristian Danescu-Niculescu-Mizil", "Michael Gamon", "Susan Dumais"], "venue": "In Proceedings of the 20th international conference on World wide web,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Density estimation by wavelet thresholding", "author": ["D. Donoho", "I. Johnstone", "G. Kerkyacharian", "D. Picard"], "venue": "The Annals of Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}, {"title": "OMG earthquake! Can Twitter improve earthquake response", "author": ["P.S. Earle", "M. Guy", "C. Ostrum", "S. Horvath", "R.A. Buckmaster"], "venue": "Eos Transactions, American Geophysical Union, Fall Meeting Supplement,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "A latent variable model for geographic lexical variation", "author": ["Jacob Eisenstein", "Brendan O\u2019Connor", "Noah A. Smith", "Eric P. Xing"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Finding scientific topics", "author": ["Thomas L. Griffiths", "Mark Steyvers"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Finding hierarchy in directed online social networks", "author": ["Mangesh Gupte", "Pravin Shankar", "Jing Li", "S. Muthukrishnan", "Liviu Iftode"], "venue": "In Proceedings of the 20th international conference on World wide web,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Life in the network: the coming age of computational social science", "author": ["David Lazer", "Alex (Sandy) Pentland", "Lada Adamic", "Sinan Aral", "Albert Laszlo Barabasi", "Devon Brewer", "Nicholas Christakis", "Noshir Contractor", "James Fowler", "Myron Gutmann", "Tony Jebara", "Gary King", "Michael Macy", "Deb Roy", "Marshall Van Alstyne"], "venue": "Science (New York, NY),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Pet: a statistical model for popular events tracking in social communities", "author": ["Cindy Xide Lin", "Bo Zhao", "Qiaozhu Mei", "Jiawei Han"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "A probabilistic approach to spatiotemporal theme pattern mining on weblogs", "author": ["Qiaozhu Mei", "Chao Liu", "Hang Su", "ChengXiang Zhai"], "venue": "In Proceedings of the 15th international conference on World Wide Web, WWW", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Statistical inference and simulation for spatial point processes. Monographs on statistics and applied probability", "author": ["J. M\u00f8ller", "R.P. Waagepetersen"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Numerical optimization. Springer series in operations research", "author": ["J. Nocedal", "S.J. Wright"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Digital distribution maps of the mammals of the western hemisphere, version 3.0", "author": ["B.D. Patterson", "G. Ceballos", "W. Sechrest", "M.F. Tognelli", "T. Brooks", "L. Luna", "P. Ortega", "I. Salazar", "B.E. Young"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Detecting controversial events from twitter", "author": ["Ana-Maria Popescu", "Marco Pennacchiotti"], "venue": "In Proceedings of the 19th ACM international conference on Information and knowledge management,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Extracting events and event descriptions from twitter", "author": ["Ana-Maria Popescu", "Marco Pennacchiotti", "Deepa Paranjpe"], "venue": "In Proceedings of the 20th international conference companion on World wide web,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Mining blog stories using communitybased and temporal clustering", "author": ["Arun Qamra", "Belle Tseng", "Edward Y. Chang"], "venue": "In Proceedings of the 15th ACM international conference on Information and knowledge management,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Earthquake shakes twitter users: realtime event detection by social sensors", "author": ["Takeshi Sakaki", "Makoto Okazaki", "Yutaka Matsuo"], "venue": "In Proceedings of the 19th international conference on World wide web,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Closing the Loop: Fast, Interactive Semi-Supervised Annotation With Queries on Features and Instances", "author": ["B. Settles"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Unified cross-validation methodology for selection among estimators and a general cross-validated adaptive epsilon-net estimator: Finite sample oracle inequalities and examples", "author": ["M.J. Van Der Laan", "S. Dudoit"], "venue": "U.C. Berkeley Division of Biostatistics Working Paper Series,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "A statistical model for positron emission tomography", "author": ["Y. Vardi", "L.A. Shepp", "L. Kaufman"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1985}, {"title": "Jasmine: a real-time local-event detection system based on geolocation information propagated to microblogs", "author": ["Kazufumi Watanabe", "Masanao Ochi", "Makoto Okabe", "Rikio Onai"], "venue": "In Proceedings of the 20th ACM international conference on Information and knowledge management,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Event detection in twitter", "author": ["J. Weng", "B.S. Lee"], "venue": "In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media. AAAI,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Multiscale poisson intensity and density estimation", "author": ["R. Willett", "R. Nowak"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "A study of retrospective and on-line event detection", "author": ["Yiming Yang", "Tom Pierce", "Jaime Carbonell"], "venue": "In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1998}, {"title": "Geographical topic discovery and comparison", "author": ["Zhijun Yin", "Liangliang Cao", "Jiawei Han", "Chengxiang Zhai", "Thomas Huang"], "venue": "In Proceedings of the 20th international conference on World wide web,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}], "referenceMentions": [{"referenceID": 31, "context": "Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33].", "startOffset": 131, "endOffset": 142}, {"referenceID": 2, "context": "Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33].", "startOffset": 131, "endOffset": 142}, {"referenceID": 24, "context": "Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33].", "startOffset": 131, "endOffset": 142}, {"referenceID": 0, "context": "Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33].", "startOffset": 253, "endOffset": 264}, {"referenceID": 17, "context": "Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33].", "startOffset": 253, "endOffset": 264}, {"referenceID": 32, "context": "Sophisticated natural language processing techniques have been used to identify social media posts relevant to a topic of interest [32, 3, 25] and advanced machine learning tools have been proposed to discover popular or emerging topics in social media [1, 18, 33].", "startOffset": 253, "endOffset": 264}, {"referenceID": 11, "context": "This idea underlines several emerging systems such as earthquake damage monitoring from Twitter [12].", "startOffset": 96, "endOffset": 100}, {"referenceID": 27, "context": "2 To address this issue, we borrow an idea from Positron Emission Tomography [28].", "startOffset": 77, "endOffset": 81}, {"referenceID": 6, "context": "where L is the combinatorial graph Laplacian [7]: L = D\u2212W, and D is the diagonal degree matrix with Djj = \u2211n k=1wjk.", "startOffset": 45, "endOffset": 48}, {"referenceID": 19, "context": "We solve the Socioscope optimization problem (9) with BFGS, a quasi-Newton method [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 26, "context": "For theoretical reasons beyond the scope of this paper, we do not recommend leave-one-out CV [27, 8].", "startOffset": 93, "endOffset": 100}, {"referenceID": 7, "context": "For theoretical reasons beyond the scope of this paper, we do not recommend leave-one-out CV [27, 8].", "startOffset": 93, "endOffset": 100}, {"referenceID": 10, "context": "We recall the following minimax lower bound, which follows from the results in [11, 31].", "startOffset": 79, "endOffset": 87}, {"referenceID": 30, "context": "We recall the following minimax lower bound, which follows from the results in [11, 31].", "startOffset": 79, "endOffset": 87}, {"referenceID": 0, "context": "The early work in this direction began with news text streamed from newswire and transcribed from other media [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "Latent Dirichlet Allocation (LDA) [4, 14] is a popular unsupervised method to detect topics.", "startOffset": 34, "endOffset": 41}, {"referenceID": 13, "context": "Latent Dirichlet Allocation (LDA) [4, 14] is a popular unsupervised method to detect topics.", "startOffset": 34, "endOffset": 41}, {"referenceID": 17, "context": "[18] extended LDA by taking spatio-temporal context into account to identify subtopics from weblogs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[33] studied GPS-associated documents, whose coordinates are generated by Gaussian Mixture Model in their generative framework.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] proposed a feature-pivot method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Besides text, social network structure also provides important information for detecting community-based topics [24] and user interests [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "Besides text, social network structure also provides important information for detecting community-based topics [24] and user interests [17].", "startOffset": 136, "endOffset": 140}, {"referenceID": 31, "context": "[32] uses clustering algorithm to identify events from news stream.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Others tried to distinguish posts related to real world event from nonevents ones, such as describing daily life or emotions [3].", "startOffset": 125, "endOffset": 128}, {"referenceID": 5, "context": "Such kind of events were also detected in Flickr photos with meta information [6] and Twitter [30].", "startOffset": 78, "endOffset": 81}, {"referenceID": 29, "context": "Such kind of events were also detected in Flickr photos with meta information [6] and Twitter [30].", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "[22, 23] focused on the detection of controversial events which provoke a public debate in which audience members express opposing opinions.", "startOffset": 0, "endOffset": 8}, {"referenceID": 22, "context": "[22, 23] focused on the detection of controversial events which provoke a public debate in which audience members express opposing opinions.", "startOffset": 0, "endOffset": 8}, {"referenceID": 28, "context": "[29] studied smaller-scale local-events, such as sales at a supermarket.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] monitored Twitter to detect real-time events such as earthquakes and hurricanes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Another line of related work uses social media as a data source to answer scientific questions [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 12, "context": "[13] studied the geographic linguistic variation with geotagged social media.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] studied the psycholinguistic theory of communication accommodation with twitter conversations.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] studied social hierarchy and stratification in online social network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] and Anagnostopoulos et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] tried to understand the social influence through the interaction on social network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "Following [26], the tweets were case-folded without any stemming or stopword removal.", "startOffset": 10, "endOffset": 14}, {"referenceID": 20, "context": "These maps indicate presence/absence only, and were extracted from NatureServe [21].", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "Instead of discretizing space and time into bins, one may adopt a spatial point process model to learn a continuous intensity function instead [19].", "startOffset": 143, "endOffset": 147}], "year": 2012, "abstractText": "Many real-world phenomena can be represented by a spatio-temporal signal: where, when, and how much. Social media is a tantalizing data source for those who wish to monitor such signals. Unlike most prior work, we assume that the target phenomenon is known and we are given a method to count its occurrences in social media. However, counting is plagued by sample bias, incomplete data, and, paradoxically, data scarcity \u2013 issues inadequately addressed by prior work. We formulate signal recovery as a Poisson point process estimation problem. We explicitly incorporate human population bias, time delays and spatial distortions, and spatiotemporal regularization into the model to address the noisy count issues. We present an efficient optimization algorithm and discuss its theoretical properties. We show that our model is more accurate than commonly-used baselines. Finally, we present a case study on wildlife roadkill monitoring, where our model produces qualitatively convincing results.", "creator": "LaTeX with hyperref package"}}}