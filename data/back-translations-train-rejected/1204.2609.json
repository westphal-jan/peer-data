{"id": "1204.2609", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2012", "title": "Stochastic Feature Mapping for PAC-Bayes Classification", "abstract": "Probabilistic generative modeling of data distributions can potentially exploit hidden information which is useful for discriminative classification. This observation has motivated the development of approaches that couple generative and discriminative models for classification. In this paper, we propose a new approach to couple generative and discriminative models in an unified framework based on PAC-Bayes risk theory. We first derive the {\\em model-parameter-independent stochastic feature mapping} from a practical MAP classifier operating on generative models. Then we construct a linear stochastic classifier equipped with the feature mapping, and derive the {\\em explicit PAC-Bayes risk bounds} for such classifier for both supervised and semi-supervised learning. Minimizing the risk bound, using an EM-like iterative procedure, results in a new posterior over hidden variables (E-step) and the update rules of model parameters (M-step). The derivation of the posterior is always feasible due to {\\em the way of equipping feature mapping} and {\\em the explicit form of bounding risk}. The derived posterior allows the tuning of generative models and subsequently the feature mappings for better classification. {\\em The derived update rules of the model parameters are same to those of the uncoupled models} as the feature mapping is model-parameter-independent. Our experiments show that the coupling between data modeling generative model and the discriminative classifier via a stochastic feature mapping in this framework leads to a general classification tool with state-of-the-art performance.", "histories": [["v1", "Thu, 12 Apr 2012 03:49:15 GMT  (30kb)", "https://arxiv.org/abs/1204.2609v1", "6 pages, 3 figures"], ["v2", "Mon, 16 Apr 2012 02:44:25 GMT  (30kb)", "http://arxiv.org/abs/1204.2609v2", "6 pages, 3 figures"]], "COMMENTS": "6 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xiong li", "tai sing lee", "yuncai liu"], "accepted": false, "id": "1204.2609"}, "pdf": {"name": "1204.2609.pdf", "metadata": {"source": "CRF", "title": "Stochastic Feature Mapping for PAC-Bayes Classification", "authors": ["Xiong Li"], "emails": ["whomliu}@sjtu.edu.cn,", "tai@cs.cmu.edu"], "sections": [{"heading": null, "text": "[1], [21], [9], [9], the most complementary models of the two paradigms have been studied [19], which leads to several promising works [3], [21], [9], [9], the above observations have emerged from these works in the context of classification: (1), (2), (2), (2), (2), (2), (2), (2), (2), (3), (3), (3), (3), (3), (3), (3), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, \"(5), (5), (5), (5), (5), (5), (5), (5), (5), (5,\" (5), \"(5), (5),\" (5), \"(5),\" (5), \"(5,\" (5), \"(5),\" (5), \"(5,\" (5), \"(5),\" (5), \"(5),\" (5), \"(5,\" (5), \"(5),\" (5), \"(5,\" (5), \"(5),\" (5), \"(5,\" (5), \"(5),\" (5), \"(5,\" (5), \"(5),\" (5), \"(5),\" (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), ("}, {"heading": "A. PAC-Bayes bounds for stochastic feature mapping", "text": "An example is an input-output pair (x, y) in which x-X and y-Y. In a PAC-Bayes setting [13], each example (x, y) is converted from a fixed, but unknown, probability distribution factor D to X-Y. Let f (x, v) Y be some classifier with a series of variables. The learning task consists in selecting a posterior distribution Q over a space F of the classifiers and a space V of the variables so that the Q weight-majority classifier BQ = (f, v) is the least possible risk on the exercise example S = {x1, y1), \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 BQ}. The output of BQ is closely related to the output of the Gibbs classifiers GQ, which are classified first."}, {"heading": "B. Objective function and specification", "text": "Let us B (Q) = 11 \u2212 e \u2212 C [1 \u2212 exp {\u2212 J (Q) Q = 1 m \u00b2 (Q) Q = 1 m \u00b2 (Q) Q (Q) Q = 1 m \u00b2 (Q) Q (Q) Q = 1 m \u00b2 (Q) Q = 1 m \u00b2 (Q) Q (Q) P + 1 (P) P (Q, C) w.r.t. Note: The minimization of B (Q) w.r.t. Q corresponds to the minimization of J (Q) w.r.t. Q. Since unlabeled data is only available when estimating dS (GQ), J (Q) and C. to minimize ml samples and unlabeled data, we can asJ (Q) = C [eS l (GQ) + 1 dS u (GQ) + 1 m KL (Q)."}, {"heading": "A. Deriving a general classification tool", "text": "In the first experiment, we derive a general classification method by applying the proposed framework to a simple but general generative model, the Gaussian mixture model. Let x-Rd be the observed variable; z = {z1, \u00b7 \u00b7 \u00b7, zk} be the hidden binary vector for K mixture components and assume that the covariance matrix is diagonal; a = {a1, \u00b7 \u00b7 \u00b7, ak} be the approximate butt parameters of z. The elements of the characteristic representation of this model are {zi (xT, diag (xxT), 1), zi log ai} Ki = 1. The butt of z can easily be derived from Equation (13). The number of mixture components is configured to K = 4 throughout the experiment. We select 8 data sets from the UCI database for evaluation, with those without missing units being preferred. The number of classes of each class is between the 2 and the 15 of the FM number."}, {"heading": "B. Scene recognition", "text": "We evaluate our SFM method and compare its performance with comparable methods for a typical visual task, scene detection. In this task, visual words are used for image representation due to their robustness in relation to the subject and spatial variance.5We use latent Dirichlet Allocation (LDA) [1] to model the distributions of visual words, and derive a recognition tool under the proposed framework.As [15], we use the topic variable in a random manner using collapsed Gibbs scanning, rejecting examples according to the Equation Rule. (13) We fix the parameter \u03b1 and allow \u03b2 [15] to be updated. Let us specify w, z or word and topic, and g the parameter of the approximate rear image of e.g. The elements of the characteristic sketching of a model are {znk, wnznk, znk log \u03b3nk} n, i, k, k, k, where n, i, index word, theme and theme."}, {"heading": "C. Protein classification", "text": "In order to evaluate the capability of the suggested approach in dealing with variable length sequences, we apply the suggested framework in which it is possible to apply the quantum physics. The problem here assigns test protein sequences to the domain that is in the SCOP (1.53) Taxonomy tree after functions of Qi = Q = Q = Q = Q = Q Q = Q Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q = Q"}], "references": [{"title": "Latent dirichlet allocation", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "PAC-Bayesian learning of linear classifiers", "author": ["P. Germain", "A. Lacasse", "F. Laviolette", "M. Marchand"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Exploiting generative models in discriminative classifiers", "author": ["T. Jaakkola", "D. Haussler"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Introduction to variational methods for graphical models", "author": ["M. Jordan", "Z. Ghahramani", "J. Tommi", "S. Lawrence"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Tutorial on practical prediction theory for classification", "author": ["J. Langford"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "International Journal of Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "author": ["R. Neal", "G. Hinton"], "venue": "Learning in Graphical Models,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes", "author": ["A.Y. Ng", "M.I. Jordan"], "venue": "In NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Free energy score space", "author": ["A. Perina", "M. Cristani", "U. Castellani", "V. Murino", "N. Jojic"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "A tutorial on hidden Markov models and selected applications inspeech recognition", "author": ["L. Rabiner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1989}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "PAC-Bayes bounds for the risk of the majority vote and the variance of the Gibbs classifier", "author": ["A. Lacasse", "F. Laviolette", "M. Marchand"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Some PAC-Bayesian theorems", "author": ["D. McAllester"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Hybrid generative-discriminative classification using posterior divergence", "author": ["X. Li", "T. Lee", "Y. Liu"], "venue": "In CVPR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Finding scientifc topics", "author": ["T. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Multiple Kernels for Object Detection", "author": ["A. Vedaldi", "V. Gulshan", "M. Varma", "A. Zisserman"], "venue": "In ICCV,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "A desicion-theoretic generalization of online learning and an application to boosting", "author": ["Y. Freund", "R. Schapire"], "venue": "Computational Learning Theory,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "Localized Multiple Kernel Learning", "author": ["M. Gonen", "E. Alpaydin"], "venue": "In ICML,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Maximum entropy discrimination", "author": ["T. Jaakkola", "M. Meila", "T. Jebara"], "venue": "Technical Report AITR-1668,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Hybrid generative-discriminative visual categorization", "author": ["A. Holub", "M. Welling", "P. Perona"], "venue": "International Journal of Computer Vision,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Classificatin with hybrid generatve/discriminative models", "author": ["R. Raina", "Y. Shen", "A. Ng", "A. McCallum"], "venue": "In NIPS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains", "author": ["L. Baum", "T. Petrie", "G. Soules", "N. Weiss"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1970}, {"title": "Multi-conditional learning: Generative/discriminative training for clustering and classification", "author": ["A. Mccallum", "C. Pal", "G. Druck", "X. Wang"], "venue": "In AAAI,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}], "referenceMentions": [{"referenceID": 18, "context": "The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9].", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9].", "startOffset": 114, "endOffset": 117}, {"referenceID": 20, "context": "The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9].", "startOffset": 119, "endOffset": 123}, {"referenceID": 22, "context": "The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9].", "startOffset": 125, "endOffset": 129}, {"referenceID": 8, "context": "The complementarities of the two paradigms have been investigated [19], [8], resulting in several promising works [3], [21], [23], [9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "Generative score space methods [3], [9], [14] are motivated by the above observations.", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "Generative score space methods [3], [9], [14] are motivated by the above observations.", "startOffset": 36, "endOffset": 39}, {"referenceID": 13, "context": "Generative score space methods [3], [9], [14] are motivated by the above observations.", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "Maximum entropy discrimination [19] provides yet another framework to exploit generative models for classification under the large margin principle.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "In addition, the VC risk bound [11] utilized by this method is generally loose in comparison with the PAC-Bayes bounds [13], [5], [2].", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "In addition, the VC risk bound [11] utilized by this method is generally loose in comparison with the PAC-Bayes bounds [13], [5], [2].", "startOffset": 119, "endOffset": 123}, {"referenceID": 4, "context": "In addition, the VC risk bound [11] utilized by this method is generally loose in comparison with the PAC-Bayes bounds [13], [5], [2].", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "In addition, the VC risk bound [11] utilized by this method is generally loose in comparison with the PAC-Bayes bounds [13], [5], [2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 20, "context": "Also, there are some other efforts [21], [23] made to couple generative and discriminative models for classification.", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "Also, there are some other efforts [21], [23] made to couple generative and discriminative models for classification.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "This paper proposes an approach based on the PAC-Bayes theory [13], [5], [2] to integrate the complementary strengths of generative and discriminative models.", "startOffset": 62, "endOffset": 66}, {"referenceID": 4, "context": "This paper proposes an approach based on the PAC-Bayes theory [13], [5], [2] to integrate the complementary strengths of generative and discriminative models.", "startOffset": 68, "endOffset": 71}, {"referenceID": 1, "context": "This paper proposes an approach based on the PAC-Bayes theory [13], [5], [2] to integrate the complementary strengths of generative and discriminative models.", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "This is distinct from the current methods [3], [9], [14] which map a data point to a feature deterministically.", "startOffset": 42, "endOffset": 45}, {"referenceID": 8, "context": "This is distinct from the current methods [3], [9], [14] which map a data point to a feature deterministically.", "startOffset": 47, "endOffset": 50}, {"referenceID": 13, "context": "This is distinct from the current methods [3], [9], [14] which map a data point to a feature deterministically.", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "The derived feature mapping is functioning similar to [3], [9], [14].", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "The derived feature mapping is functioning similar to [3], [9], [14].", "startOffset": 59, "endOffset": 62}, {"referenceID": 13, "context": "The derived feature mapping is functioning similar to [3], [9], [14].", "startOffset": 64, "endOffset": 68}, {"referenceID": 6, "context": "We can resort to the following variational lower bound [7], [4]:", "startOffset": 55, "endOffset": 58}, {"referenceID": 3, "context": "We can resort to the following variational lower bound [7], [4]:", "startOffset": 60, "endOffset": 63}, {"referenceID": 18, "context": "(1)), we resort to the following tractable one [19], [9]", "startOffset": 47, "endOffset": 51}, {"referenceID": 8, "context": "(1)), we resort to the following tractable one [19], [9]", "startOffset": 53, "endOffset": 56}, {"referenceID": 3, "context": "Further, we assume that the approximate posterior of h, for the example x, takes the same from with its prior P(h) but with different parameter [4] Q(h) = exp{c(\u03b8\u2032 h) T (h) + S (h) + f (\u03b8\u2032 h)}.", "startOffset": 144, "endOffset": 147}, {"referenceID": 10, "context": "First, this classifier allows PAC-Bayes risk bounds that have explicit solutions for Q(h+, h\u2212) which can help tune the feature mapping for better classification; Second, the PACBayes risk bound for such a classifier can be tighter than VC bounds [11]; Third, the feature mapping is independent with model parameters \u03b8, making the solution of \u03b8 very simple.", "startOffset": 246, "endOffset": 250}, {"referenceID": 12, "context": "In a PAC-Bayes setting [13], each example (x, y) is drawn from a fixed, but unknown, probability distribution D on X \u00d7 Y.", "startOffset": 23, "endOffset": 27}, {"referenceID": 12, "context": "Among several PACBayes bounds [13], [5], [2], [12], the bound derived in [2] is quite tight and gives an explicit bound for the true risk R(GQ), which allows the derivation of the posterior Q, in contrast to most of the other implicit bounds over KL(Rs(GQ)\u2016R(GQ)).", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "Among several PACBayes bounds [13], [5], [2], [12], the bound derived in [2] is quite tight and gives an explicit bound for the true risk R(GQ), which allows the derivation of the posterior Q, in contrast to most of the other implicit bounds over KL(Rs(GQ)\u2016R(GQ)).", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "Among several PACBayes bounds [13], [5], [2], [12], the bound derived in [2] is quite tight and gives an explicit bound for the true risk R(GQ), which allows the derivation of the posterior Q, in contrast to most of the other implicit bounds over KL(Rs(GQ)\u2016R(GQ)).", "startOffset": 41, "endOffset": 44}, {"referenceID": 11, "context": "Among several PACBayes bounds [13], [5], [2], [12], the bound derived in [2] is quite tight and gives an explicit bound for the true risk R(GQ), which allows the derivation of the posterior Q, in contrast to most of the other implicit bounds over KL(Rs(GQ)\u2016R(GQ)).", "startOffset": 46, "endOffset": 50}, {"referenceID": 1, "context": "Among several PACBayes bounds [13], [5], [2], [12], the bound derived in [2] is quite tight and gives an explicit bound for the true risk R(GQ), which allows the derivation of the posterior Q, in contrast to most of the other implicit bounds over KL(Rs(GQ)\u2016R(GQ)).", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "2 of [2], and can be proved by replacing f with ( f , v) and reapplying its proof [2].", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "2 of [2], and can be proved by replacing f with ( f , v) and reapplying its proof [2].", "startOffset": 82, "endOffset": 85}, {"referenceID": 11, "context": "The semi-supervised bound is different with [12] whose bound is implicit and has no explicit solution of Q.", "startOffset": 44, "endOffset": 48}, {"referenceID": 4, "context": "To derive these expressions, as were done in [5], we assume that the prior of the weight is Gaussian P(w) = N(u0, I) and its posterior is also Gaussian except with a different mean, i.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "Using the assumption and Gaussian integrals [5], we have,", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "We now show how an EM-like iterative procedure [4] can be used to learn the stochastic feature space and the Gibbs classifier simultaneously.", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "In classification, similar with [2], we use the decision rule of majority vote \u0177 = sign[ n \u2211n j=1 EP(h+,h\u2212 |xi)Q(w)w \u00b7 \u03c6(xi, h+, h\u2212)] \u2243 sign[ n \u2211n j=1 u\u00b7\u03c6(xi, h+i j, h\u2212i j)] with n = 5 and (h+i j, h\u2212i j) being the j-th example drawn from P(h+, h\u2212|xi).", "startOffset": 32, "endOffset": 35}, {"referenceID": 2, "context": "Two related and general methods, Fisher score (FS) [3] and free energy score space (FESS) [9], and some other state-of-the-art methods are also tested for comparison.", "startOffset": 51, "endOffset": 54}, {"referenceID": 8, "context": "Two related and general methods, Fisher score (FS) [3] and free energy score space (FESS) [9], and some other state-of-the-art methods are also tested for comparison.", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "The strategy adopted by [2] is to perform the optimization for 10\u223c100 trials where a new random initial point within the range [\u221220, 20]d is used in each trial.", "startOffset": 24, "endOffset": 27}, {"referenceID": 16, "context": "We compare our method SFM with Adaboost [17], SVM [11], localized multiple kernel learning (LMKL) [18] and PAC-Bayes gradient descent PBGD3 [2].", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "We compare our method SFM with Adaboost [17], SVM [11], localized multiple kernel learning (LMKL) [18] and PAC-Bayes gradient descent PBGD3 [2].", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "We compare our method SFM with Adaboost [17], SVM [11], localized multiple kernel learning (LMKL) [18] and PAC-Bayes gradient descent PBGD3 [2].", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "We compare our method SFM with Adaboost [17], SVM [11], localized multiple kernel learning (LMKL) [18] and PAC-Bayes gradient descent PBGD3 [2].", "startOffset": 140, "endOffset": 143}, {"referenceID": 0, "context": "We use latent Dirichlet allocation (LDA) [1] to model the distributions of visual words, and derive a recognition tool under the proposed framework.", "startOffset": 41, "endOffset": 44}, {"referenceID": 14, "context": "Like [15], we sample the topic variable using collapsed Gibbs sampling and reject examples according to the rule for Eq.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "We fix the parameter \u03b1 and allow \u03b2 [15] to be updated.", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "For FS [3] and FESS [9], we extract features from the trained LDA model and deliver to SVM.", "startOffset": 7, "endOffset": 10}, {"referenceID": 8, "context": "For FS [3] and FESS [9], we extract features from the trained LDA model and deliver to SVM.", "startOffset": 20, "endOffset": 23}, {"referenceID": 5, "context": "For each image, dense SIFT descriptors [6] are extracted from 20\u00d7 20 grid patches over 4 scales.", "startOffset": 39, "endOffset": 42}, {"referenceID": 15, "context": "Our results compare well with PHOW [16] which is a state-of-the-art feature for scene recognition.", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "Hidden Markov model (HMM) [10] is used to model the distribution over protein sequences for its ability in handling sequences with variable length.", "startOffset": 26, "endOffset": 30}, {"referenceID": 21, "context": "With the hidden states of the input sequence inferred by BaumWelch algorithm [22], it is easy to estimate the posterior transition probabilities conditioned on x.", "startOffset": 77, "endOffset": 81}], "year": 2012, "abstractText": "Probabilistic generative modeling of data distributions can potentially exploit hidden information which is useful for discriminative classification. This observation has motivated the development of approaches that couple generative and discriminative models for classification. In this paper, we propose a new approach to couple generative and discriminative models in an unified framework based on PAC-Bayes risk theory. We first derive the model-parameter-independent stochastic feature mapping from a practical MAP classifier operating on generative models. Then we construct a linear stochastic classifier equipped with the feature mapping, and derive the explicit PAC-Bayes risk bounds for such classifier for both supervised and semi-supervised learning. Minimizing the risk bound, using an EM-like iterative procedure, results in a new posterior over hidden variables (Estep) and the update rules of model parameters (M-step). The derivation of the posterior is always feasible due to the way of equipping feature mapping and the explicit form of bounding risk. The derived posterior allows the tuning of generative models and subsequently the feature mappings for better classification. The derived update rules of the model parameters are same to those of the uncoupled models as the feature mapping is model-parameterindependent. Our experiments show that the coupling between data modeling generative model and the discriminative classifier via a stochastic feature mapping in this framework leads to a general classification tool with state-of-the-art performance.", "creator": "LaTeX with hyperref package"}}}