{"id": "1204.2742", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Apr-2012", "title": "Video In Sentences Out", "abstract": "We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases,spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the track-to-role assignments, and changing body posture.", "histories": [["v1", "Thu, 12 Apr 2012 14:47:44 GMT  (4210kb,D)", "http://arxiv.org/abs/1204.2742v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["andrei barbu", "alexander bridge", "zachary burchill", "dan coroian", "sven dickinson", "sanja fidler", "aaron michaux", "sam mussman", "siddharth narayanaswamy", "dhaval salvi", "lara schmidt", "jiangnan shangguan", "jeffrey mark siskind", "jarrell waggoner", "song wang", "jinlian wei", "yifan yin", "zhiqi zhang"], "accepted": false, "id": "1204.2742"}, "pdf": {"name": "1204.2742.pdf", "metadata": {"source": "CRF", "title": "Video In Sentences Out", "authors": ["Andrei Barbu", "a\u2217Alexander", "Zachary Burchill", "Dan Coroian", "Sven Dickinson", "Sanja Fidler", "Aaron Michaux", "Sam Mussman", "Siddharth Narayanaswamy", "Dhaval Salvi", "Lara Schmidt", "Jiangnan Shangguan", "Jeffrey Mark Siskind", "Jarrell Waggoner", "Song Wang", "Jinlian Wei", "Yifan Yin", "Zhiqi Zhang"], "emails": ["andrei@0xab.com."], "sections": [{"heading": null, "text": "We present a system that produces sentence descriptions of video: who did what with whom, where and how they did it. The action class is rendered as a verb, participant objects as noun phrases, properties of these objects as adjective modifiers in these noun phrases, spatial relationships between these participants as preposition phrases and event characteristics as preposition phrase additions and adverbial modifiers. To extract the information needed to render these linguistic units, an approach to event detection is needed that restores object tracks, track-to-roll assignments and body posture changes."}, {"heading": "1 Introduction", "text": "In fact, if they don't play by the rules, most of them will be able to survive on their own."}, {"heading": "2 The mind\u2019s eye corpus", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country and in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "3 Overall system architecture", "text": "We first apply detectors (Felzenszwalb et al., 2010a, b) for each object class to each image of each video. These detectors tend to yield many false positives, but only a few false negatives. The Kanade-Lucas-Tomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker is then used to project each detection five frames forward to magnify the series of detections and compensate for further false negatives in the raw detector output. A dynamic programming algorithm (Viterbi, 1971) is then used to select an optimal series of detections that is time coherent with the optical flow, resulting in a series of object tracks for each video. These tracks are then smoothed out and used to calculate a time series of feature vectors for each video."}, {"heading": "3.1 Object detection and tracking", "text": "In fact, most of them are able to play by the rules that they have set themselves, and they are able to play by the rules that they have set by the rules."}, {"heading": "3.2 Body-posture codebook", "text": "We recognize events through a combination of the movement of the event participants and the changing posture of the human participants. Posture information is derived using the partial structure produced as a by-product of the Felzenszwalb et al. Detectors. While this information is much louder and less accurate to the human eye than precisely articulated models (Andriluka et al., 2008; Bregler, 1997; Gavrila and Davis, 1995; Sigal et al., 2010; Yang and Ramanan, 2011) and appears incomprehensible to the human eye, it is sufficient to improve event recognition accuracy. Such information can be extracted much more robustly than possible from a large, uncommented corpus using precise articulated models. Posture information is derived from the partial structure in two ways. First, we calculate a vector of partial shifts, whereby each shift is decoded as a vector from the recognition center to the sub-center, with eukoook hierarchical parts forming the sub-area."}, {"heading": "3.3 Event classification", "text": "This year, we will be able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said."}, {"heading": "3.4 Generating sentences", "text": "We produce a sentence from a recorded action class along with the associated tracks that use the templates from Table 3. > In these submissions, the words in cursive letters are called fixed strings, words in bold denote the action category, X and Y are the subject and object noun phrases, and the categories Adv, PPendo and Pexo are the subject and thus the subject. The processes for generating these noun phrases are described below. Onetrack HMMs use this track as an agent and thus as the subject. For dual-track HMEs, we select the assignment of roles that are more likely to result in class class class class class class class class class and prepositional phrases."}, {"heading": "4 Experimental results", "text": "For each video, we created sentences that corresponded to the three most likely action classes. Fig. 7 shows key frames from four videos in our test set along with the sentence that was created for the most likely action class. Human judges rated each video set pair to determine whether the sentence matches the video and whether it describes a prominent event that is depicted in this video. 26.7% (601 / 2247) of the video set pairs were considered true and 7.9% (178 / 2247) of the video set pairs were considered outstanding. Restricting to only the sentence that corresponds to the most likely action class for each video, 25.5% (191 / 749) of the video set pairs were considered true and 8.4% (63 / 749) of the video set pairs were considered alienated."}, {"heading": "5 Conclusion", "text": "The integration of language and vision (Aloimonos et al., 2011; Barzialy et al., 2003; Darrell et al., 2011; McKevitt et al., 1994, 1995-1996) and the recognition of actions in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have long been of considerable interest. There is also work on generating sentential descriptions of static images (Farhadi et al., 2009; Kulkarni et al., 2011; Yao et al., 2010). However, we are not aware of any previous work that has produced such rich sentential descriptions as described here."}, {"heading": "Acknowledgments", "text": "This work has been supported in part by the NSF Grant CCF0438806, the Naval Research Laboratory under contract number N00173-10-1-G023, the Army Research Laboratory under cooperation agreement number W911NF-10-2-0060, and computing resources provided by Information Technology at Purdue through the Rosen Center for Advanced Computing. Any views, opinions, findings, conclusions or recommendations contained or expressed in this document or material are those of the author (s) and do not necessarily reflect the views or official guidelines of the NSF, the Naval Research Laboratory, the Office of Naval Research, the Army Research Laboratory or the U.S. Government."}], "references": [{"title": "AAAI Workshop on Language-Action Tools for Cognitive Artificial Agents: Integrating Vision", "author": ["Y. Aloimonos", "L. Fadiga", "G. Metta", "K. Pastra", "editors"], "venue": "Action and Language,", "citeRegEx": "Aloimonos et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Aloimonos et al\\.", "year": 2011}, {"title": "People-tracking-bydetection and people-detection-by-tracking", "author": ["M. Andriluka", "S. Roth", "B. Schiele"], "venue": "In CVPR, pages", "citeRegEx": "Andriluka et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Andriluka et al\\.", "year": 2008}, {"title": "Actions as space-time shapes", "author": ["M. Blank", "L. Gorelick", "E. Shechtman", "M. Irani", "R. Basri"], "venue": "In ICCV,", "citeRegEx": "Blank et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Blank et al\\.", "year": 2005}, {"title": "Learning and recognizing human dynamics in video sequences", "author": ["Christoph Bregler"], "venue": "In CVPR,", "citeRegEx": "Bregler.,? \\Q1997\\E", "shortCiteRegEx": "Bregler.", "year": 1997}, {"title": "Describing objects by their attributes", "author": ["Ali Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth"], "venue": "In CVPR,", "citeRegEx": "Farhadi et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2009}, {"title": "Cascade object detection with deformable part models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester"], "venue": "In CVPR,", "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Object detection with discriminatively trained part based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": null, "citeRegEx": "Felzenszwalb et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Felzenszwalb et al\\.", "year": 2010}, {"title": "Towards 3-d model-based tracking and recognition of human movement", "author": ["D.M. Gavrila", "L.S. Davis"], "venue": "In International Workshop on Face and Gesture Recognition,", "citeRegEx": "Gavrila and Davis.,? \\Q1995\\E", "shortCiteRegEx": "Gavrila and Davis.", "year": 1995}, {"title": "Logic and conversation", "author": ["H.P. Grice"], "venue": "Syntax and Semantics 3: Speech Acts,", "citeRegEx": "Grice.,? \\Q1975\\E", "shortCiteRegEx": "Grice.", "year": 1975}, {"title": "Object, scene and actions: Combining multiple features for human action recognition", "author": ["Nazli Ikizler-Cinibis", "Stan Sclaroff"], "venue": "In ECCV,", "citeRegEx": "Ikizler.Cinibis and Sclaroff.,? \\Q2010\\E", "shortCiteRegEx": "Ikizler.Cinibis and Sclaroff.", "year": 2010}, {"title": "Semantics and Cognition", "author": ["Ray Jackendoff"], "venue": null, "citeRegEx": "Jackendoff.,? \\Q1983\\E", "shortCiteRegEx": "Jackendoff.", "year": 1983}, {"title": "Baby talk: Understanding and generating simple image descriptions", "author": ["Girish Kulkarni", "Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C. Berg", "Tamara L. Berg"], "venue": "In CVPR,", "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Local velocity-adapted motion events for spatio-temporal recognition", "author": ["I. Laptev", "B. Caputo", "C. Schuldt", "T. Lindeberg"], "venue": "CVIU, 108(3):207\u201329,", "citeRegEx": "Laptev et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Laptev et al\\.", "year": 2007}, {"title": "Learning realistic human actions from movies", "author": ["I. Laptev", "M. Marszalek", "C. Schmid", "B. Rozenfeld"], "venue": "In CVPR,", "citeRegEx": "Laptev et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Laptev et al\\.", "year": 2008}, {"title": "Recognizing realistic actions from videos \u201cin the wild", "author": ["J. Liu", "J. Luo", "M. Shah"], "venue": "In CVPR, pages 1996\u20132003,", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Integration of Natural Language and Vision Processing, volume I\u2013IV", "author": ["P. McKevitt", "editor"], "venue": null, "citeRegEx": "McKevitt and editor.,? \\Q1996\\E", "shortCiteRegEx": "McKevitt and editor.", "year": 1996}, {"title": "Unsupervised learning of human action categories using spatial-temporal words", "author": ["J.C. Niebles", "H. Wang", "L. Fei-Fei"], "venue": null, "citeRegEx": "Niebles et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Niebles et al\\.", "year": 2008}, {"title": "A threshold selection method from gray-level histograms", "author": ["N. Otsu"], "venue": "IEEE Trans. on Systems, Man and Cybernetics,", "citeRegEx": "Otsu.,? \\Q1979\\E", "shortCiteRegEx": "Otsu.", "year": 1979}, {"title": "Learnability and Cognition", "author": ["Steven Pinker"], "venue": null, "citeRegEx": "Pinker.,? \\Q1989\\E", "shortCiteRegEx": "Pinker.", "year": 1989}, {"title": "Action MACH: A spatio-temporal maximum average correlation height filter for action recognition", "author": ["M.D. Rodriguez", "J. Ahmed", "M. Shah"], "venue": "In CVPR,", "citeRegEx": "Rodriguez et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Rodriguez et al\\.", "year": 2008}, {"title": "Recognizing human actions: A local SVM approach", "author": ["C. Schuldt", "I. Laptev", "B. Caputo"], "venue": "In ICPR, pages 32\u20136,", "citeRegEx": "Schuldt et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Schuldt et al\\.", "year": 2004}, {"title": "A 3-dimensional SIFT descriptor and its application to action recognition", "author": ["P. Scovanner", "S. Ali", "M. Shah"], "venue": "In International Conference on Multimedia,", "citeRegEx": "Scovanner et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Scovanner et al\\.", "year": 2007}, {"title": "Good features to track", "author": ["J. Shi", "C. Tomasi"], "venue": "In CVPR, pages 593\u2013600,", "citeRegEx": "Shi and Tomasi.,? \\Q1994\\E", "shortCiteRegEx": "Shi and Tomasi.", "year": 1994}, {"title": "HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human", "author": ["L. Sigal", "A. Balan", "M.J. Black"], "venue": "motion. IJCV,", "citeRegEx": "Sigal et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sigal et al\\.", "year": 2010}, {"title": "A maximum-likelihood approach to visual event classification", "author": ["J.M. Siskind", "Q. Morris"], "venue": "In ECCV,", "citeRegEx": "Siskind and Morris.,? \\Q1996\\E", "shortCiteRegEx": "Siskind and Morris.", "year": 1996}, {"title": "Realtime American sign language recognition using desk and wearable computer based video", "author": ["Thad Starner", "Joshua Weaver", "Alex Pentland"], "venue": null, "citeRegEx": "Starner et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Starner et al\\.", "year": 1998}, {"title": "Detection and tracking of point features", "author": ["C. Tomasi", "T. Kanade"], "venue": "Technical Report CMU-CS-91-132,", "citeRegEx": "Tomasi and Kanade.,? \\Q1991\\E", "shortCiteRegEx": "Tomasi and Kanade.", "year": 1991}, {"title": "Convolutional codes and their performance in communication systems", "author": ["A.J. Viterbi"], "venue": "IEEE Trans. on Communication,", "citeRegEx": "Viterbi.,? \\Q1971\\E", "shortCiteRegEx": "Viterbi.", "year": 1971}, {"title": "Human action recognition by semilatent topic models", "author": ["Y. Wang", "G. Mori"], "venue": "PAMI, 31(10):1762\u201374,", "citeRegEx": "Wang and Mori.,? \\Q2009\\E", "shortCiteRegEx": "Wang and Mori.", "year": 2009}, {"title": "Motion based event recognition using HMM", "author": ["Gu Xu", "Yu-Fei Ma", "HongJiang Zhang", "Shiqiang Yang"], "venue": "In ICPR,", "citeRegEx": "Xu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2002}, {"title": "Articulated pose estimation using flexible mixtures of parts", "author": ["Y. Yang", "D. Ramanan"], "venue": "In CVPR,", "citeRegEx": "Yang and Ramanan.,? \\Q2011\\E", "shortCiteRegEx": "Yang and Ramanan.", "year": 2011}, {"title": "I2t: Image parsing to text description", "author": ["B.Z. Yao", "Xiong Yang", "Liang Lin", "Mun Wai Lee", "Song-Chun Zhu"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Yao et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yao et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 12, "context": "This means that one cannot use common approaches to event recognition, such as spatiotemporal bags of words (Laptev et al., 2007; Niebles et al., 2008; Scovanner et al., 2007), spatiotemporal volumes (Blank et al.", "startOffset": 108, "endOffset": 175}, {"referenceID": 16, "context": "This means that one cannot use common approaches to event recognition, such as spatiotemporal bags of words (Laptev et al., 2007; Niebles et al., 2008; Scovanner et al., 2007), spatiotemporal volumes (Blank et al.", "startOffset": 108, "endOffset": 175}, {"referenceID": 21, "context": "This means that one cannot use common approaches to event recognition, such as spatiotemporal bags of words (Laptev et al., 2007; Niebles et al., 2008; Scovanner et al., 2007), spatiotemporal volumes (Blank et al.", "startOffset": 108, "endOffset": 175}, {"referenceID": 2, "context": ", 2007), spatiotemporal volumes (Blank et al., 2005; Laptev et al., 2008; Rodriguez et al., 2008), and tracked feature points (Liu et al.", "startOffset": 32, "endOffset": 97}, {"referenceID": 13, "context": ", 2007), spatiotemporal volumes (Blank et al., 2005; Laptev et al., 2008; Rodriguez et al., 2008), and tracked feature points (Liu et al.", "startOffset": 32, "endOffset": 97}, {"referenceID": 19, "context": ", 2007), spatiotemporal volumes (Blank et al., 2005; Laptev et al., 2008; Rodriguez et al., 2008), and tracked feature points (Liu et al.", "startOffset": 32, "endOffset": 97}, {"referenceID": 14, "context": ", 2008), and tracked feature points (Liu et al., 2009; Schuldt et al., 2004; Wang and Mori, 2009) that do not determine the class of participant objects and the roles that they play.", "startOffset": 36, "endOffset": 97}, {"referenceID": 20, "context": ", 2008), and tracked feature points (Liu et al., 2009; Schuldt et al., 2004; Wang and Mori, 2009) that do not determine the class of participant objects and the roles that they play.", "startOffset": 36, "endOffset": 97}, {"referenceID": 28, "context": ", 2008), and tracked feature points (Liu et al., 2009; Schuldt et al., 2004; Wang and Mori, 2009) that do not determine the class of participant objects and the roles that they play.", "startOffset": 36, "endOffset": 97}, {"referenceID": 10, "context": "A common assumption in Linguistics (Jackendoff, 1983; Pinker, 1989) is that verbs typically characterize the interaction between event participants in terms of the gross changing motion of these participants.", "startOffset": 35, "endOffset": 67}, {"referenceID": 18, "context": "A common assumption in Linguistics (Jackendoff, 1983; Pinker, 1989) is that verbs typically characterize the interaction between event participants in terms of the gross changing motion of these participants.", "startOffset": 35, "endOffset": 67}, {"referenceID": 2, "context": "For example, the WEIZMANN dataset (Blank et al., 2005) and the KTH dataset (Schuldt et al.", "startOffset": 34, "endOffset": 54}, {"referenceID": 20, "context": ", 2005) and the KTH dataset (Schuldt et al., 2004) depict events with a single human participant, not ones where people interact with other people or objects.", "startOffset": 28, "endOffset": 50}, {"referenceID": 19, "context": "Moreover, such datasets, as well as the SPORTS ACTIONS dataset (Rodriguez et al., 2008) and the YOUTUBE dataset (Liu et al.", "startOffset": 63, "endOffset": 87}, {"referenceID": 14, "context": ", 2008) and the YOUTUBE dataset (Liu et al., 2009), often make action-class distinctions that are irrelevant to the choice of verb, e.", "startOffset": 32, "endOffset": 50}, {"referenceID": 28, "context": "swing Other datasets, such as the BALLET dataset (Wang and Mori, 2009) and the UCF50 dataset (Liu et al.", "startOffset": 49, "endOffset": 70}, {"referenceID": 14, "context": "swing Other datasets, such as the BALLET dataset (Wang and Mori, 2009) and the UCF50 dataset (Liu et al., 2009), depict larger-scale activities that bear activity-class names that are not well suited to sentential description, e.", "startOffset": 93, "endOffset": 111}, {"referenceID": 22, "context": "The Kanade-Lucas-Tomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker", "startOffset": 30, "endOffset": 77}, {"referenceID": 26, "context": "The Kanade-Lucas-Tomasi (KLT) (Shi and Tomasi, 1994; Tomasi and Kanade, 1991) feature tracker", "startOffset": 30, "endOffset": 77}, {"referenceID": 27, "context": "A dynamicprogramming algorithm (Viterbi, 1971) is then used to select an optimal set of detections that is temporally coherent with optical flow, yielding a set of object tracks for each video.", "startOffset": 31, "endOffset": 46}, {"referenceID": 24, "context": "Hidden Markov Models (HMMs) are then employed as time-series classifiers to yield verb labels for each video (Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005), together with the object tracks of the participants in the action described by that verb along with the roles they play.", "startOffset": 109, "endOffset": 201}, {"referenceID": 25, "context": "Hidden Markov Models (HMMs) are then employed as time-series classifiers to yield verb labels for each video (Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005), together with the object tracks of the participants in the action described by that verb along with the roles they play.", "startOffset": 109, "endOffset": 201}, {"referenceID": 28, "context": "Hidden Markov Models (HMMs) are then employed as time-series classifiers to yield verb labels for each video (Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005), together with the object tracks of the participants in the action described by that verb along with the roles they play.", "startOffset": 109, "endOffset": 201}, {"referenceID": 17, "context": "The offset is then taken to be the minimum of the value that maximizes the between-class variance (Otsu, 1979) when bipartitioning this histogram and the trained acceptance threshold offset by a fixed, but small, amount (0.", "startOffset": 98, "endOffset": 110}, {"referenceID": 1, "context": "While such information is far noisier and less accurate than fitting precise articulated models (Andriluka et al., 2008; Bregler, 1997; Gavrila and Davis, 1995; Sigal et al., 2010; Yang and Ramanan, 2011) and appears unintelligible to the human eye, as shown in Section 3.", "startOffset": 96, "endOffset": 204}, {"referenceID": 3, "context": "While such information is far noisier and less accurate than fitting precise articulated models (Andriluka et al., 2008; Bregler, 1997; Gavrila and Davis, 1995; Sigal et al., 2010; Yang and Ramanan, 2011) and appears unintelligible to the human eye, as shown in Section 3.", "startOffset": 96, "endOffset": 204}, {"referenceID": 7, "context": "While such information is far noisier and less accurate than fitting precise articulated models (Andriluka et al., 2008; Bregler, 1997; Gavrila and Davis, 1995; Sigal et al., 2010; Yang and Ramanan, 2011) and appears unintelligible to the human eye, as shown in Section 3.", "startOffset": 96, "endOffset": 204}, {"referenceID": 23, "context": "While such information is far noisier and less accurate than fitting precise articulated models (Andriluka et al., 2008; Bregler, 1997; Gavrila and Davis, 1995; Sigal et al., 2010; Yang and Ramanan, 2011) and appears unintelligible to the human eye, as shown in Section 3.", "startOffset": 96, "endOffset": 204}, {"referenceID": 30, "context": "While such information is far noisier and less accurate than fitting precise articulated models (Andriluka et al., 2008; Bregler, 1997; Gavrila and Davis, 1995; Sigal et al., 2010; Yang and Ramanan, 2011) and appears unintelligible to the human eye, as shown in Section 3.", "startOffset": 96, "endOffset": 204}, {"referenceID": 24, "context": "all possible such mappings and selecting the one with the highest likelihood (Siskind and Morris, 1996).", "startOffset": 77, "endOffset": 103}, {"referenceID": 8, "context": "Following the Gricean Maxim of Quantity (Grice, 1975), we only generate color and person-pose adjectives if needed to prevent coreference of nonhuman event participants.", "startOffset": 40, "endOffset": 53}, {"referenceID": 2, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 13, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 14, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 19, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 20, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 24, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 25, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 28, "context": ", 2011; McKevitt, 1994, 1995\u20131996) and recognition of action in video (Blank et al., 2005; Laptev et al., 2008; Liu et al., 2009; Rodriguez et al., 2008; Schuldt et al., 2004; Siskind and Morris, 1996; Starner et al., 1998; Wang and Mori, 2009; Xu et al., 2002, 2005) have been of considerable interest for a long time.", "startOffset": 70, "endOffset": 267}, {"referenceID": 4, "context": "There has also been work on generating sentential descriptions of static images (Farhadi et al., 2009; Kulkarni et al., 2011; Yao et al., 2010).", "startOffset": 80, "endOffset": 143}, {"referenceID": 11, "context": "There has also been work on generating sentential descriptions of static images (Farhadi et al., 2009; Kulkarni et al., 2011; Yao et al., 2010).", "startOffset": 80, "endOffset": 143}, {"referenceID": 31, "context": "There has also been work on generating sentential descriptions of static images (Farhadi et al., 2009; Kulkarni et al., 2011; Yao et al., 2010).", "startOffset": 80, "endOffset": 143}], "year": 2012, "abstractText": "We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the track-to-role assignments, and changing body posture.", "creator": "LaTeX with hyperref package"}}}