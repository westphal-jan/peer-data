{"id": "1204.3516", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2012", "title": "When majority voting fails: Comparing quality assurance methods for noisy human computation environment", "abstract": "Quality assurance remains a key topic in human computation research. Prior work indicates that majority voting is effective for low difficulty tasks, but has limitations for harder tasks. This paper explores two methods of addressing this problem: tournament selection and elimination selection, which exploit 2-, 3- and 4-way comparisons between different answers to human computation tasks. Our experimental results and statistical analyses show that both methods produce the correct answer in noisy human computation environment more often than majority voting. Furthermore, we find that the use of 4-way comparisons can significantly reduce the cost of quality assurance relative to the use of 2-way comparisons.", "histories": [["v1", "Mon, 16 Apr 2012 15:12:22 GMT  (383kb)", "http://arxiv.org/abs/1204.3516v1", "Presented at Collective Intelligence conference, 2012 (arXiv:1204.2991)"]], "COMMENTS": "Presented at Collective Intelligence conference, 2012 (arXiv:1204.2991)", "reviews": [], "SUBJECTS": "cs.SI cs.AI", "authors": ["yu-an sun", "christopher dance"], "accepted": false, "id": "1204.3516"}, "pdf": {"name": "1204.3516.pdf", "metadata": {"source": "CRF", "title": "WHEN MAJORITY VOTING FAILS: COMPARINGQUALITY ASSURANCE METHODS FOR NOISY HUMAN COMPUTATION ENVIRONMENT", "authors": ["Yu-An Sun", "Christopher Dance"], "emails": ["yuan.sun@xerox.com", "chris.dance@xrce.xerox.com"], "sections": [{"heading": null, "text": "Quality assurance remains a key issue in research into human calculation. Previous work suggests that majority decisions are effective for tasks with little difficulty, but have limitations for more difficult tasks. This paper examines two methods for solving this problem: tournament selection and elimination selection, where 2-, 3- and 4-way comparisons are made between different answers to human arithmetic tasks. Our experimental results and statistical analysis show that both methods produce the correct answer more often than majority decisions in a noisy environment of human calculation. Furthermore, we find that the use of 4-way comparisons can significantly reduce the cost of quality assurance compared to the use of 2-way comparisons."}, {"heading": "INTRODUCTION", "text": "Human calculation is a growing field of research that promises that humans and computers will work seamlessly together to implement powerful systems.Algorithmically aggregated results from human calculation workers are key to such an integrated human-computer system (Little & Sun 2011).However, the nature of a human calculation system lies in workers selecting tasks themselves that they can work on. The results of such open calls are generally noisy with varying degrees of correctness and quality. Redundancy in majority votes (Bernstein et al. 2010) or independent issue agreements (von Ahn & Dabbish 2004) are commonly assumed to address this issue. However, Sun et al. (2011) and Law & von Ahn (2009) have both found that high-quality results lie in a minority of answers and are often not identified by majority votes. As highlighted in (Law & von Ahn 2011), the limitations of majority voting on the task involve workers who may have different concepts based on random.1) that the majority vote may differ from the majority vote."}, {"heading": "RELATED WORK", "text": "Quality control in Human Computation Independent match and filtering are the two most commonly used methods of quality control for human calculation. Independent agreement aims to select the best issue by majority vote. In the ESP game, the agreement mechanism may be input agreement or issue agreement (Law & von Ahn 2009). An issue agreement system only accepts captions agreed by two independent actors and no communication is allowed between them. We consider only the issue agreement mechanism as independent match and equivalent to majority votes. Games for a purpose that adopts an issue agreement include the ESP game (by Ahn & Dabbish 2004), HerdIt (Barrington et al. 2009) and Categorilla (Vickrey et al. 2008). Filtering bad output based on gold questions is another quality control technique (Le et al. 2010, Oleson et al. 2011)."}, {"heading": "PROBLEM DEFINITION", "text": "Given the m options, our goal is to find the option with the highest quality. However, this task needs to be solved economically in terms of the number of 2-, 3- or 4-way comparisons. Ideally, therefore, we want an algorithm that maximizes the likelihood that the highest quality item will be selected under a constraint that limits the maximum number of comparisons."}, {"heading": "OUR METHODS", "text": "This year, it is so far that it will be able to put itself at the top, \"he said.\" It is very important that we are able to put ourselves at the top, \"he said.\" But we are not yet in a position. \""}, {"heading": "RESULTS", "text": "First, we present results on a task to translate Chinese idioms, which show that the selection of tournaments and elimination of decisions can succeed where majority decisions fail. Then, we present a statistical analysis of human calculations on five tasks. This allows us to estimate the cost and error rates of our methods in the real world and answer the question of whether it is better to use 2-, 3- or 4-way comparisons. CHINESE IDIOM TRANSLATIONMajority Voting We conducted experiments to translate the five Chinese idioms listed in Figure 1. This is a challenging task as the literal meaning of idioms differs from their true interpretations. We performed these experiments with CrowdFlower, which performs the processing of online micro-tasks under a number of working channels including Amazon Mechanical Turk. Specific instructions were given to capture true interpretations in English and not to translate literal translations of Chinese idioms into one large number of answers (one in 30)."}, {"heading": "Tournament Selection Experiments", "text": "We conducted tournament selection procedures (Method A pairs of idiom translation comparisons. We started with outputs generated by the crowd. These outputs were as follows, with the correct translation being: 1. How a dog fails, a tiger2. Who are you? 3. None4. Attempt to do something beyond his abilities and mistakes 5. Painted Tiger Anti-DogAt the end of each round of tournament selection, the ratio of the above entries was calculated and the corresponding action is shown in Figure 4. Correct translation was initially a minority, but it gradually outperformed all the other candidates and emerged as the clear winner within five rounds. Thus, it seems that tournament selection is well suited for such translation tasks as it is easier to select the correct translation from a list than to produce the correct translation.) With the second Chinese, the unique flower workers."}, {"heading": "Elimination Selection Experiments", "text": "In this experiment, once a translation reaches 16 losses, it is removed from the pool. The final answer is produced when the pool has only one option left. Figure 5 shows the number of losses for each translation versus the number of correct translations, Option 4, is used as the final answer according to the 91stMODELLING # -WAY COMPARISON DATA To predict the cost and error rates of our methods in the real world, and to answer the question of whether it is better to use two comparisons, we must make assumptions. Therefore, we now match several statistical models to a series of 1273 # tasks consisting of a Chinese idiom translation and the following three puzzles 1: If you have an infinite supply of water 5 quart and 3 quart, how would you measure exactly 4 quart?"}, {"heading": "COMPARISON OF SELECTION METHODS", "text": "Our attachment to the error rate of elimination selection (see appendix) describes its performance for a more general family of comparison probabilities. However, we would like to understand the error rate and mean number of games that could result from applying both tournament and elimination selection to data observed in real-world experiments. In this section, we will take a Bayesian approach to these questions. If the data show an exact distribution of Bradley-Terry, in which the best-known position has the highest probability of winning, the algorithms as shown in Figure 6 would perform. In this graph, the error rate converts to zero as the number of comparisons increases and there is a clear performance improvement as we move from 2- to 3- to 4-way comparisons. However, the real data may not have an exact BradleyTerry distribution and the actual distribution may not be the assumptions under which elimination of comparisons increases."}, {"heading": "CONCLUSIONS", "text": "We proposed two methods for selecting the best answer for difficult human arithmetic tasks: tournament selection and elimination selection. Both methods successfully lead to the right answer when majority decisions fail by performing concept experiments. We performed a statistical analysis of multi-way comparisons obtained from five human arithmetic tasks. While a BradleyTerry model aggregates the data for each given method using multi-way comparisons, it is not possible to describe 2-, 3- and 4-way comparisons with a single common Bradley-Terry model at the same time. Using a Bayesian approach based on this analysis, we compared the costs and error rates of each method with the Condorcet vote. This comparison showed that four-way comparisons result in a lower error rate at the same cost than pairwise selection methods."}, {"heading": "FUTURE WORK", "text": "Our work demonstrated the applicability of tournament and elimination selection methods as quality assurance to human calculation systems. Future work will include exploring a Bradley-Terry blending model to better capture worker intentions, a hybrid method of both selection and filtering, and various types of human calculation tasks."}, {"heading": "Computation,\u201d Synthesis Lectures on Artificial Intelligence and Machine Learning", "text": "Bernstein, M. S., Little, G., Miller, R. C., Hartmann, B., Ackerman, M. S., Karger, D. R., Crowell, D., Panovich, K. (2010), \"Soylent: A Word Processor with a Crowd Inside,\" UIST '10, ACM PressLittle, G. and Sun, Y. (2011), \"Human OCR: Insights from a Complex Human Computation Process,\" Workshop on Crowdsourcing and Human Computation, Services, Studies and Platforms, ACM CHIvon Ahn, L. and Dabbish L. (2004), \"Labeling images with a computer game,\" ACM CHISun, Y., Roy, S., Little, G. (2011), \"Beyond Independent Agreement: A Tournament Selection Approach for Quality Assurance of Human Computation Tasks, HCOMPLaw, E. and von Ahn, L. (2009),\" Beyond Independent Agreement: A Tournament Selection for Quality Approach, COMPA, COMPA: A, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA: COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA: COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA: COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA, COMPA,"}, {"heading": "Data,\u201d Chapman & Hall", "text": "Adler, M., Gemmell, P., Harchol-Balter, M., Karp, R., Kenyon, C. (1994), \"Selection in the Presence of Noise: The Design of Playoff Systems,\" The Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, 564-573Ben-Akiva, M. and Lerman, S. R. (1985), \"Discrete Choice Analysis,\" The MIT PressBradley, R. A. and Terry, M. E. (1952), \"The Rank Analysis of Incomplete Block Designs,\" I. The Method of Paired Comparisons. Biometrics 39, 324- 345Luce, R. D. (1959), \"Individual Choice Behavior,\" WileyGoldberg, D. and Deb, K. (1991), \"A Comparative Analysis of Selection Schemes Used in Genetic Progorithms,\" Block of Genetic Algorithms, R. R. Fermandez, A., S., C. and C. Deb, K. (1991), The Analysis D. (1991)."}, {"heading": "APPENDIX: HOW OFTEN DOES # -WAY ELIMINATION SELECTION MAKE ERRORS?", "text": "We analyze the performance of the -way elimination method (or 0) when we win a game. (Adler et al. 1994) We conclude this sketch and extend it to the case of -way elimination method. Therefore, this appendix justifies the use of the -way elimination selection method and provides insight into the selection of parameters. First, we must ensure that the probability of a game winning a match coincides with some items identified as item 1. To do this, we assume a discriminatory assumption that is a generalization of the assumption made in [? Adler]. Say items V 6 1 and 1 are both agreed with some items that are to be compared. If 3 and $12.32 then we consider two items with items 1, 3 and items V, 3 and items V, 3 and items V, 3, whereas if $11.2, V2 then we consider only a game with items 1, 2 and V. Let (1) or if we lose a game with (0)."}], "references": [{"title": "Human Computation,", "author": ["E. Law", "L. von Ahn"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning", "citeRegEx": "Law and Ahn,? \\Q2011\\E", "shortCiteRegEx": "Law and Ahn", "year": 2011}, {"title": "Soylent: A Word Processor with a Crowd Inside,", "author": ["M.S. Bernstein", "G. Little", "R.C. Miller", "B. Hartmann", "M.S. Ackerman", "D.R. Karger", "D. Crowell", "K. Panovich"], "venue": null, "citeRegEx": "Bernstein et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2010}, {"title": "Human OCR: Insights from a Complex Human Computation Process,", "author": ["G. Little", "Y. Sun"], "venue": "Workshop on Crowdsourcing and Human Computation,", "citeRegEx": "Little and Sun,? \\Q2011\\E", "shortCiteRegEx": "Little and Sun", "year": 2011}, {"title": "Labeling images with a computer game,", "author": ["L. von Ahn", "Dabbish L"], "venue": null, "citeRegEx": "Ahn and L.,? \\Q2004\\E", "shortCiteRegEx": "Ahn and L.", "year": 2004}, {"title": "Beyond Independent Agreement: A Tournament Selection Approach for Quality Assurance of Human Computation Tasks,", "author": ["Y. Sun", "S. Roy", "G. Little"], "venue": null, "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}, {"title": "Input-agreement: A new mechanism for data collection using human computation games,", "author": ["E. Law", "L. von Ahn"], "venue": null, "citeRegEx": "Law and Ahn,? \\Q2009\\E", "shortCiteRegEx": "Law and Ahn", "year": 2009}, {"title": "CODA: Convergence Diagnosis and Output Analysis for MCMC,", "author": ["M. Plummer", "N. Best", "K. Cowles", "K. Vines"], "venue": "R News,", "citeRegEx": "Plummer et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Plummer et al\\.", "year": 2006}, {"title": "Ensuring quality in crowdsourced search relevance evaluation: The effects of training question distribution,", "author": ["J. Le", "A. Edmonds", "V. Hester", "L. Biewald"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Le et al\\.", "year": 2010}, {"title": "Programmatic Gold: Targeted and Scalable Quality Assurance in Crowdsourcing,", "author": ["D. Oleson", "A. Sorokin", "G. Laughlin", "V. Hester", "J. Le", "L. Biewald"], "venue": null, "citeRegEx": "Oleson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Oleson et al\\.", "year": 2011}, {"title": "User-Centered Design of a Social Game to Tag Music,", "author": ["L. Barrington", "D. O\u2019Malley", "D. Turnbull", "G. Lanckriet"], "venue": null, "citeRegEx": "Barrington et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Barrington et al\\.", "year": 2009}, {"title": "Online Word Games for Semantic Data Collection,", "author": ["D. Vickrey", "A. Bronzan", "W. Choi", "A. Kumar", "J. Turner-Maier", "A. Wang", "D. Koller"], "venue": null, "citeRegEx": "Vickrey et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Vickrey et al\\.", "year": 2008}, {"title": "Analyzing and Modeling Rank Data,", "author": ["J. Marden"], "venue": null, "citeRegEx": "Marden,? \\Q1995\\E", "shortCiteRegEx": "Marden", "year": 1995}, {"title": "Selection in the Presence of Noise: The Design of Playoff Systems", "author": ["M. Adler", "P. Gemmell", "M. Harchol-Balter", "R. Karp", "C. Kenyon"], "venue": "The Fifth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Adler et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Adler et al\\.", "year": 1994}, {"title": "Discrete Choice Analysis,", "author": ["M. Ben-Akiva", "S.R. Lerman"], "venue": null, "citeRegEx": "Ben.Akiva and Lerman,? \\Q1985\\E", "shortCiteRegEx": "Ben.Akiva and Lerman", "year": 1985}, {"title": "The Rank Analysis of Incomplete Block Designs, \u201c I", "author": ["R.A. Bradley", "M.E. Terry"], "venue": "The method of paired comparisons. Biometrika", "citeRegEx": "Bradley and Terry,? \\Q1952\\E", "shortCiteRegEx": "Bradley and Terry", "year": 1952}, {"title": "Individual Choice Behavior,", "author": ["R.D. Luce"], "venue": null, "citeRegEx": "Luce,? \\Q1959\\E", "shortCiteRegEx": "Luce", "year": 1959}, {"title": "A Comparative Analysis of Selection Schemes Used in Genetic Algorithms,", "author": ["D. Goldberg", "K. Deb"], "venue": "Foundations of Genetic Algorithms", "citeRegEx": "Goldberg and Deb,? \\Q1991\\E", "shortCiteRegEx": "Goldberg and Deb", "year": 1991}, {"title": "Genetic-Based Machine Learning for Rule Induction: Taxonomy, Experimental Study and State of the Art,", "author": ["A. Fermandez", "S. Garcia", "J. Luengo", "E. BernadoMansilla", "F. Herrera"], "venue": null, "citeRegEx": "Fermandez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Fermandez et al\\.", "year": 2010}, {"title": "Probability Models on Rankings", "author": ["H. Stern"], "venue": null, "citeRegEx": "Stern,? \\Q1993\\E", "shortCiteRegEx": "Stern", "year": 1993}], "referenceMentions": [{"referenceID": 1, "context": "Redundancy with majority voting (Bernstein et al. 2010) or independent output agreement (von Ahn & Dabbish 2004) is commonly adopted to address this issue.", "startOffset": 32, "endOffset": 55}, {"referenceID": 18, "context": "We simulate these methods to benchmark their time complexity, error rates and the costs associated with their deployment, in terms of number of comparisons required, and compare them with selection based on Condorcet voting (Stern 1993).", "startOffset": 224, "endOffset": 236}, {"referenceID": 1, "context": "Redundancy with majority voting (Bernstein et al. 2010) or independent output agreement (von Ahn & Dabbish 2004) is commonly adopted to address this issue. However, Sun et al (2011) and Law & von Ahn (2009) both identified that high quality results reside in a minority of the responses and are often not identified by majority voting.", "startOffset": 33, "endOffset": 182}, {"referenceID": 1, "context": "Redundancy with majority voting (Bernstein et al. 2010) or independent output agreement (von Ahn & Dabbish 2004) is commonly adopted to address this issue. However, Sun et al (2011) and Law & von Ahn (2009) both identified that high quality results reside in a minority of the responses and are often not identified by majority voting.", "startOffset": 33, "endOffset": 207}, {"referenceID": 9, "context": "Games with a purpose that adopt output agreement include the ESP game (von Ahn & Dabbish 2004), HerdIt (Barrington et al. 2009), and Categorilla (Vickrey et al.", "startOffset": 103, "endOffset": 127}, {"referenceID": 10, "context": "2009), and Categorilla (Vickrey et al. 2008).", "startOffset": 23, "endOffset": 44}, {"referenceID": 11, "context": "Statistical Modeling for Noisy Pair Comparison Pairwise and multi-way comparisons are commonly used for psychology experiments and image quality assessment in conjunction with statistical models such as the multinomial logit (Ben-Akiva & Lerman 1985), Bradley-Terry (Bradley & Terry 1952) and Plackett-Luce (Marden 1995, Ben-Akiva & Lerman 1985) models. Such work typically focuses on estimating the qualities of different items when the pairs to be compared are specified externally. In contrast, we wish to find an efficient algorithm for choosing which pairs to compare in order to select the best answer, as in Adler et al (1994). In other words, assuming there is a ground truth best answer, we are trying to determine which observations to make, whereas such statistical models are typically used to analyze the data after the observations have been made.", "startOffset": 308, "endOffset": 634}, {"referenceID": 6, "context": "To check that this was a sufficient number of iterations, we used the R-package CODA (Plummer et al. 2006).", "startOffset": 85, "endOffset": 106}], "year": 2012, "abstractText": "Quality assurance remains a key topic in human computation research. Prior work indicates that majority voting is effective for low difficulty tasks, but has limitations for harder tasks. This paper explores two methods of addressing this problem: tournament selection and elimination selection, which exploit 2-, 3and 4-way comparisons between different answers to human computation tasks. Our experimental results and statistical analyses show that both methods produce the correct answer in noisy human computation environment more often than majority voting. Furthermore, we find that the use of 4-way comparisons can significantly reduce the cost of quality assurance relative to the use of 2-way comparisons.", "creator": "PDFCreator Version 0.9.8"}}}