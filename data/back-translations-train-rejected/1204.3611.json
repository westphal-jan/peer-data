{"id": "1204.3611", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Apr-2012", "title": "Learning to Predict the Wisdom of Crowds", "abstract": "The problem of \"approximating the crowd\" is that of estimating the crowd's majority opinion by querying only a subset of it. Algorithms that approximate the crowd can intelligently stretch a limited budget for a crowdsourcing task. We present an algorithm, \"CrowdSense,\" that works in an online fashion to dynamically sample subsets of labelers based on an exploration/exploitation criterion. The algorithm produces a weighted combination of a subset of the labelers' votes that approximates the crowd's opinion.", "histories": [["v1", "Mon, 16 Apr 2012 19:39:13 GMT  (1100kb)", "http://arxiv.org/abs/1204.3611v1", "Presented at Collective Intelligence conference, 2012 (arXiv:1204.2991)"]], "COMMENTS": "Presented at Collective Intelligence conference, 2012 (arXiv:1204.2991)", "reviews": [], "SUBJECTS": "cs.SI cs.LG", "authors": ["seyda ertekin", "haym hirsh", "cynthia rudin"], "accepted": false, "id": "1204.3611"}, "pdf": {"name": "1204.3611.pdf", "metadata": {"source": "CRF", "title": "LEARNING TO PREDICT THE WISDOM OF CROWDS", "authors": ["Seyda Ertekin", "Haym Hirsh", "Cynthia Rudin"], "emails": ["seyda@mit.edu", "hirsh@cs.rutgers.edu", "rudin@mit.edu"], "sections": [{"heading": null, "text": "ar Xiv: 120 4.36 11v1 [cs.SI] 16 Apr 201 2"}, {"heading": "INTRODUCTION", "text": "This year, there will be no significant change."}, {"heading": "RELATED WORK", "text": "The low cost of crowdsourcing work has increasingly led to resources such as Amazon Mechanical Turk1 (AMT) 2011 being used to label data for machine learning purposes, where collecting multiple labels from non-professional annotators can produce results similar to those of experts. This cost-effective way of generating labels with AMT has also been used in several studies (Dakka & Ipeirotis 2008, Kaisers & Lowe 2008, Nakov 2008, O'Connor, Jurafsky & Ng 2008, Sorokin & Forsyth 2008, Nowak & Ru \ufffd ger 2010), while crowdsourcing is clearly very effective for simple tasks that require little to no training of labelers, the rate of inconsistency has been shown to increase with task difficulties (Gillick & Liu 2010, Sorokin & Forsyth 2008). Although a number of approaches are being developed to manage the varying reliability of crowdsourcing work (see Caldson-Burlison 2010)."}, {"heading": "CROWDSENSE", "text": "s look at the quality estimations of the labelers as a yardstick for their agreement with the audience. L = (l1, l2,.), lk: x (1, 1), lk: x (1), lk: x (2), lk: x (1), lk: x (2), lk: x (2), lk: x (1), lk: x (2), lk: x (1), lk: x (2), lk: x (2), lk: x (2), lk: x (2), lk: x (2), lk: x (2), lk: x (1), lk: x (1), lk: x (2), lk: x (1), lk: x (1), lk: x, lk: x, lk: 1, lk: x, lk: x (2), lk: x (1, lk: x), lk: x (1, lk: x), lk: x (1, lk: x), lk: x (1, lk: x), lk: x (1, lk: x (1), lk: x (1, lx), lk: x (1, lk: 1, lx: x), lk: x (1, lk: x (1), lk: x (1), lk: x (1, lk: x: x), lk: x (1, lk: x (1), lk: x: x (1), lk: x: x (1, lk: x), lk: x (1, lk: x (1), lk: x: x (1), lk: x: x (1), lk: x: x (1), lk: x (1, lk: x (1), lk: x: x (1), lk: x: x: x (1, lk: x), lk: x (1, lk: x: x), lk: x: x: x"}, {"heading": "DATASETS AND BASELINES", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, live, in which they, in fact, in which they, in fact, are able to move, are able to move, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, move, in which they, in which they are able to move, in which they are able to move, in which they"}, {"heading": "OVERALL PERFORMANCE", "text": "First, we compare CrowdSense with the baselines to demonstrate its ability to accurately approximate the set's tuning. In the next section, we present a modular view of CrowdSense and show the impact of each module on the algorithm's accuracy. Accuracy is calculated as the percentage of examples in which the algorithm matched the majority vote of the entire CrowdSense. Figure 2 shows that CrowdSense consistently achieved the highest accuracy over baselines across different values, indicating that CrowdSenseuses any set budget more effectively than IEThresh. Generally, CrowdSense's quality estimates better reflect the true accuracy of crowd members and are therefore able to identify and select a more representative subset of the crowd. The other baselines, which we did not consider to be the same level of performance as CrowdSense and IEThresh. On the subplots in Figure 2, the accuracy of the best point of the label is not weighted to a baseliner's line."}, {"heading": "SPECIFIC CHOICES FOR EXPLORATION AND", "text": "The algorithm template on which CrowdSense is based has three components that can be instantiated in different ways: (1) the composition of the initial seed quantity of the labellers (Step 4 (b) in the pseudocode), (2) how the following labellers are added to the sentence (Step 4 (c) and (3) the weighting scheme that influences the selection of the initial labellers, the way in which the additional labellers are included, and the strategy for combining the labellers \"voices (Step 4 (b) (c) (d). We tested the effect of the first component by conducting separate experiments that initiated the labeller with three (3Q), one (1Q) and none (0Q) labellers that had the highest quality of the labellers (Steps 4 (b) (c) (d)))."}, {"heading": "Initialization with Gold Standard", "text": "Gold standard examples are the \"actual\" opinion of the amount collected by taking the majority vote of all members. > Although collecting votes from the entire set initially increases the total cost of some of the examples, it could help us make better estimates in previous iterations, thus achieving a higher overall accuracy. In CrowdSense, initialization with gold standard is consistent with equation (3).In Figure 4, we present the effect of initialization with gold standard for CrowdSense and IEThresh. The gold rate contains four examples, two of which were chosen as + 1 by the majority of the crowd, and the other two voted as -1. The ongoing accuracy of the curves in Figure 4 begins after observing the gold data. As expected, gold data significantly improves CrowdSense's performance because the estimates are now initialized with perfect budgets."}, {"heading": "Effect of the \u03b5 parameter", "text": "Figure 6 illustrates the average mileage over 100 runs of CrowdSense for different Epsilon values. Each endpoint of each of the curves in Figure 6 corresponds to a single point in Figure 2. Figure 6 shows that increasing Epsilon over time leads to increased accuracy at a higher cost."}, {"heading": "Effect of the K parameter", "text": "The K parameter helps with both early stage exploration and later stage exploitation; the K parameter helps ensure that labellers who have seen fewer examples (smaller digit) are not considered more valuable than good labellers who have seen many examples; the quality estimate is a shrinkage estimate that lowers the probabilities when there is uncertainty; consider a labeller who has been asked to vote and vote correctly on just one example; his vote should not be counted as high as a voter who has seen 100 examples and correctly labeled 99 of them; this can be achieved with K sufficiently generously; Increasing K also makes the quality estimates more stable, helping to facilitate exploration; the quality estimates in the first few iterations in which the labeller is involved are often not very accurate and not very stable; and as a result, the algorithm does not take into account the accuracy of the early labels."}, {"heading": "CONCLUSIONS AND FUTURE WORK", "text": "In this context, we have discussed the exploration / exploitation that is necessary to estimate the qualities of labellers, and exploitation (i.e. the repeated use of the best labellers) is necessary to obtain a good estimate of majority votes. We have presented a modular sketch that CrowdSense follows, namely that a small pool of labellers first chooses, then labellers are gradually added until the estimation of the majority is more certain, then a weighted majority is adopted as an overall forecast, the most important of which is the choice of the total budget for labelling the entire data set. We have compared our results with several baselines, suggesting that CrowdSense, and the general exploration / exploitation of the ideas behind it, can be useful."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Thomas W. Malone for the helpful conversations. Support for this project comes from the MIT Intelligence Initiative (I2)."}, {"heading": "Vision and Pattern Recognition Workshop 0, 1\u20138.", "text": "Wallace, B. C., Small, K., Brodley, C. E. & Trikalinos, T. A. (2011), Who should label what? instance allocation in multiple expert active learning, in \"Proc. of the SIAM International Conference on Data Mining (SDM).\" Warfield, S. K., Zou, K. H. & Wells, W. M. (2004), \"Simultaneous truth and performance level estimation (staple): an algorithm for the validation of image segmentation.,\" IEEE Transactions on Medical Imaging (TMI) 23 (7), 903-21.Welinder, P., Branson, S., Belongie, S. & Perona, P. (2010), The multidimensional wisdom of crowd, in \"Advances in Neural Information Processing Systems (NIPS).\" Whitehill, J., Ruvolo, P., fan Wu, T., Bergsma, J. & Movellan, P. (2009), Whose vote should more: Optimal integration in Neural Information Processing Systems (NIPS). \"Whitehill, J., Ruvolo, J., Ruppill, P. (2009)."}, {"heading": "Machine Learning Research - Proceedings Track", "text": "(JMLR) 9, 932-939."}], "references": [{"title": "Creating speech and language data with amazon\u2019s mechanical turk, in \u2018Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk", "author": ["C. Callison-Burch", "M. Dredze"], "venue": null, "citeRegEx": "Callison.Burch and Dredze,? \\Q2010\\E", "shortCiteRegEx": "Callison.Burch and Dredze", "year": 2010}, {"title": "Automatic extraction of useful facet hierarchies from text databases, in \u2018Proceedings of the 24th International Conference on Data Engineering (ICDE)", "author": ["W. Dakka", "P.G. Ipeirotis"], "venue": null, "citeRegEx": "Dakka and Ipeirotis,? \\Q2008\\E", "shortCiteRegEx": "Dakka and Ipeirotis", "year": 2008}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Applied Statistics", "citeRegEx": "Dawid and Skene,? \\Q1979\\E", "shortCiteRegEx": "Dawid and Skene", "year": 1979}, {"title": "2009a), Good learners for evil teachers, in \u2018Proc. of the 26 Annual International Conference on Machine Learning (ICML)", "author": ["O. Dekel", "O. Shamir"], "venue": null, "citeRegEx": "Dekel and Shamir,? \\Q2009\\E", "shortCiteRegEx": "Dekel and Shamir", "year": 2009}, {"title": "Vox populi: Collecting high-quality labels from a crowd, in \u2018In", "author": ["O. Dekel", "O. Shamir"], "venue": "Proceedings of the 22nd Annual Conference on Learning Theory\u2019", "citeRegEx": "Dekel and Shamir,? \\Q2009\\E", "shortCiteRegEx": "Dekel and Shamir", "year": 2009}, {"title": "Efficiently learning the accuracy of labeling sources for selective sampling, in \u2018Proceedings of the 15th International Conference on Knowledge Discovery and Data Mining (KDD)", "author": ["P. Donmez", "J.G. Carbonell", "J. Schneider"], "venue": null, "citeRegEx": "Donmez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Donmez et al\\.", "year": 2009}, {"title": "Non-expert evaluation of summarization systems is risky, in \u2018Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon\u2019s Mechanical Turk\u2019, CSLDAMT", "author": ["D. Gillick", "Y. Liu"], "venue": null, "citeRegEx": "Gillick and Liu,? \\Q2010\\E", "shortCiteRegEx": "Gillick and Liu", "year": 2010}, {"title": "Creating a research collection of question answer sentence pairs with amazons mechanical turk, in \u2018Proceedings of the Sixth International Language Resources and Evaluation (LREC\u201908)\u2019, European Language Resources Association", "author": ["M. Kaisser", "J. Lowe"], "venue": null, "citeRegEx": "Kaisser and Lowe,? \\Q2008\\E", "shortCiteRegEx": "Kaisser and Lowe", "year": 2008}, {"title": "Cobayes: bayesian knowledge corroboration with assessors of unknown areas of expertise, in \u2018Proc", "author": ["G. Kasneci", "J.V. Gael", "D. Stern", "T. Graepel"], "venue": "ACM International Conference on Web Search and Data Mining (WSDM)\u2019,", "citeRegEx": "Kasneci et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kasneci et al\\.", "year": 2011}, {"title": "Human Computation, Synthesis Lectures on Artificial Intelligence and Machine Learning", "author": ["E. Law", "L. von Ahn"], "venue": null, "citeRegEx": "Law and Ahn,? \\Q2011\\E", "shortCiteRegEx": "Law and Ahn", "year": 2011}, {"title": "Noun compound interpretation using paraphrasing verbs: Feasibility study, in \u2018Proceedings of the 13th international conference on Artificial Intelligence: Methodology, Systems, and Applications", "author": ["P. Nakov"], "venue": "AIMSA", "citeRegEx": "Nakov,? \\Q2008\\E", "shortCiteRegEx": "Nakov", "year": 2008}, {"title": "How reliable are annotations via crowdsourcing: a study about inter-annotator agreement for multi-label image annotation, in \u2018Proceedings of the international conference on Multimedia information", "author": ["S. Nowak", "S. R\u00fcger"], "venue": null, "citeRegEx": "Nowak and R\u00fcger,? \\Q2010\\E", "shortCiteRegEx": "Nowak and R\u00fcger", "year": 2010}, {"title": "Human computation: a survey and taxonomy of a growing field, in \u2018Proceedings of the 2011", "author": ["A.J. Quinn", "B.B. Bederson"], "venue": "Annual Conference on Human Factors in Computing Systems (CHI)\u2019,", "citeRegEx": "Quinn and Bederson,? \\Q2011\\E", "shortCiteRegEx": "Quinn and Bederson", "year": 2011}, {"title": "Learning from crowds", "author": ["V.C. Raykar", "S. Yu", "L.H. Zhao", "G.H. Valadez", "C. Florin", "L. Bogoni", "L. Moy"], "venue": "Journal of Machine Learning Research (JMLR)", "citeRegEx": "Raykar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Raykar et al\\.", "year": 2010}, {"title": "Get another label? improving data quality and data mining using multiple, noisy labelers, in \u2018Proceeding of the 14 International Conference on Knowledge Discovery and Data Mining (KDD)", "author": ["V.S. Sheng", "F. Provost", "P.G. Ipeirotis"], "venue": null, "citeRegEx": "Sheng et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sheng et al\\.", "year": 2008}, {"title": "Knowledge discovery in large image databases: Dealing with uncertainties in ground truth, in \u2018KDD Workshop", "author": ["P. Smyth", "M.C. Burl", "U.M. Fayyad", "P. Perona"], "venue": null, "citeRegEx": "Smyth et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Smyth et al\\.", "year": 1994}, {"title": "Inferring ground truth from subjective labelling of venus images., in \u2018Advances in Neural Information Processing Systems (NIPS)", "author": ["P. Smyth", "U.M. Fayyad", "M.C. Burl", "P. Perona", "P. Baldi"], "venue": null, "citeRegEx": "Smyth et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Smyth et al\\.", "year": 1994}, {"title": "Cheap and fast\u2014but is it good?: evaluating non-expert annotations for natural language tasks, in \u2018Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)", "author": ["R. Snow", "B. O\u2019Connor", "D. Jurafsky", "A.Y. Ng"], "venue": null, "citeRegEx": "Snow et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2008}, {"title": "Utility data annotation with amazon mechanical turk", "author": ["A. Sorokin", "D. Forsyth"], "venue": "Computer Vision and Pattern Recognition Workshop", "citeRegEx": "Sorokin and Forsyth,? \\Q2008\\E", "shortCiteRegEx": "Sorokin and Forsyth", "year": 2008}, {"title": "Who should label what? instance allocation in multiple expert active learning, in \u2018Proc. of the SIAM International Conference on Data Mining (SDM)", "author": ["B.C. Wallace", "K. Small", "C.E. Brodley", "T.A. Trikalinos"], "venue": null, "citeRegEx": "Wallace et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Wallace et al\\.", "year": 2011}, {"title": "Simultaneous truth and performance level estimation (staple): an algorithm for the validation of image segmentation.", "author": ["S.K. Warfield", "K.H. Zou", "W.M. Wells"], "venue": "IEEE Transactions on Medical Imaging (TMI)", "citeRegEx": "Warfield et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Warfield et al\\.", "year": 2004}, {"title": "The multidimensional wisdom of crowds, in \u2018Advances in Neural Information Processing Systems (NIPS)", "author": ["P. Welinder", "S. Branson", "S. Belongie", "P. Perona"], "venue": null, "citeRegEx": "Welinder et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Welinder et al\\.", "year": 2010}, {"title": "Whose vote should count more: Optimal integration of labels from labelers of unknown expertise, in \u2018Advances in Neural Information Processing Systems (NIPS)", "author": ["J. Whitehill", "P. Ruvolo", "T. fan Wu", "J. Bergsma", "J. Movellan"], "venue": null, "citeRegEx": "Whitehill et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Whitehill et al\\.", "year": 2009}, {"title": "Modeling multiple annotator expertise in the semi-supervised learning", "author": ["Y. Yan", "R. Rosales", "G. Fung", "J. Dy"], "venue": "scenario, in \u2018Proceedings of the Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial", "citeRegEx": "Yan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2010}, {"title": "Modeling annotator expertise: Learning when everybody knows a bit of something", "author": ["Y. Yan", "R. Rosales", "G. Fung", "M.W. Schmidt", "G.H. Valadez", "L. Bogoni", "L. Moy", "J.G. Dy"], "venue": "Journal of Machine Learning Research - Proceedings Track", "citeRegEx": "Yan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yan et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 21, "context": "Dawid & Skene 1979, Kasneci, Gael, Stern & Graepel 2011, Smyth, Burl, Fayyad & Perona 1994, Smyth, Fayyad, Burl, Perona & Baldi 1994, Warfield, Zou & Wells 2004), sometimes in tandem with learning values for other latent variables such as task difficulty (Welinder, Branson, Belongie & Perona 2010, Whitehill, Ruvolo, fan Wu, Bergsma & Movellan 2009), classifier parameters (Dekel & Shamir 2009a, Raykar, Yu, Zhao, Valadez, Florin, Bogoni & Moy 2010, Yan, Rosales, Fung & Dy 2010, Yan, Rosales, Fung, Schmidt, Valadez, Bogoni, Moy & Dy 2010), or domain-specific information about the labeling task (Welinder et al. 2010).", "startOffset": 598, "endOffset": 620}, {"referenceID": 10, "context": "This cost-effective way of generating labeled collections using AMT has also been used in several studies (Dakka & Ipeirotis 2008, Kaisser & Lowe 2008, Nakov 2008, Snow, O\u2019Connor, Jurafsky & Ng 2008, Sorokin & Forsyth 2008, Nowak & R\u00fcger 2010). While crowdsourcing clearly is highly effective for easy tasks that require little to no training of the labelers, the rate of disagreement has been shown to increase with task difficulty (Gillick & Liu 2010, Sorokin & Forsyth 2008). Although a range of approaches are being developed to manage the varying reliability of crowdsourced labor (see, for example Callison-Burch & Dredze 2010, Law & von Ahn 2011, Quinn & Bederson 2011, Wallace, Small, Brodley & Trikalinos 2011), the most common method for using crowdsourcing to label data is to obtain multiple labels for each item from different labelers and treat the majority label as an item\u2019s true label. For example, Sheng, Provost & Ipeirotis (2008) demonstrated that repeated labeling can be preferable to single labeling in the presence of label noise, especially when the cost of data preprocessing is non-negligible.", "startOffset": 152, "endOffset": 950}, {"referenceID": 10, "context": "This cost-effective way of generating labeled collections using AMT has also been used in several studies (Dakka & Ipeirotis 2008, Kaisser & Lowe 2008, Nakov 2008, Snow, O\u2019Connor, Jurafsky & Ng 2008, Sorokin & Forsyth 2008, Nowak & R\u00fcger 2010). While crowdsourcing clearly is highly effective for easy tasks that require little to no training of the labelers, the rate of disagreement has been shown to increase with task difficulty (Gillick & Liu 2010, Sorokin & Forsyth 2008). Although a range of approaches are being developed to manage the varying reliability of crowdsourced labor (see, for example Callison-Burch & Dredze 2010, Law & von Ahn 2011, Quinn & Bederson 2011, Wallace, Small, Brodley & Trikalinos 2011), the most common method for using crowdsourcing to label data is to obtain multiple labels for each item from different labelers and treat the majority label as an item\u2019s true label. For example, Sheng, Provost & Ipeirotis (2008) demonstrated that repeated labeling can be preferable to single labeling in the presence of label noise, especially when the cost of data preprocessing is non-negligible. Dekel & Shamir (2009b) proposed an algorithm for pruning the labels of less reliable labelers in order to improve the accuracy of the majority vote of labelers.", "startOffset": 152, "endOffset": 1144}, {"referenceID": 13, "context": "Sheng et al. (2008) is one exception, performing active learning by reasoning about the value of seeking additional labels on data given the data obtained thus far.", "startOffset": 0, "endOffset": 20}, {"referenceID": 13, "context": "Sheng et al. (2008) is one exception, performing active learning by reasoning about the value of seeking additional labels on data given the data obtained thus far. Donmez, Carbonell & Schneider (2009) simultaneously estimate labeler accuracies and train a classifier using labelers\u2019 votes to apply active learning to select the next example for labeling.", "startOffset": 0, "endOffset": 202}, {"referenceID": 5, "context": "We discuss the approach taken by Donmez et al. (2009) later in the paper as one of the baselines to which we compare our results, because this is the only algorithm we know of aside from ours that can be naturally applied to approximating the crowd in the online setting.", "startOffset": 33, "endOffset": 54}, {"referenceID": 5, "context": "As for the comparative analysis of labeler selection based on quality estimates, we compare CrowdSense against IEThresh (Donmez et al. 2009).", "startOffset": 120, "endOffset": 140}], "year": 2012, "abstractText": "The problem of \u201capproximating the crowd\u201d is that of estimating the crowd\u2019s majority opinion by querying only a subset of it. Algorithms that approximate the crowd can intelligently stretch a limited budget for a crowdsourcing task. We present an algorithm, \u201cCrowdSense,\u201d that works in an online fashion to dynamically sample subsets of labelers based on an exploration/exploitation criterion. The algorithm produces a weighted combination of a subset of the labelers\u2019 votes that approximates the crowd\u2019s opinion.", "creator": "LaTeX with hyperref package"}}}