{"id": "1204.3968", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Apr-2012", "title": "Convolutional Neural Networks Applied to House Numbers Digit Classification", "abstract": "We classify digits of real-world house numbers using convolutional neural networks (ConvNets). ConvNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 94.85% accuracy on the SVHN dataset (45.2% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.", "histories": [["v1", "Wed, 18 Apr 2012 03:48:38 GMT  (750kb)", "http://arxiv.org/abs/1204.3968v1", "4 pages, 6 figures, 2 tables"]], "COMMENTS": "4 pages, 6 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["pierre sermanet", "soumith chintala", "yann lecun"], "accepted": false, "id": "1204.3968"}, "pdf": {"name": "1204.3968.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["sermanet@cs.nyu.edu", "soumith@cs.nyu.edu", "yann@cs.nyu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 120 4.39 68v1 [cs.CV] 1 8A pr2 01"}, {"heading": "1. Introduction", "text": "However, it is a more difficult problem in the context of complex nature scenes such as photos, where the best current methods lag behind human performance, mainly due to non-contrasting backgrounds, low resolution, defocused and motion-blurred images, and large differences in illumination (Figure 1). [8] Recently, a new digital classification dataset for house numbers was introduced that was extracted from street images. It is similar in format to the popular MNIST dataset [7] (10 digits, 32 x 32 inputs), but contains an order of magnitude larger (600,000 labeled digits), includes color information, and various natural backgrounds. Previous approaches to classifying characters and digits from natural images used several handmade features [3] and template matching [14]. In contrast, ConvNet's learning functions show overlap all the way from pixels to classifiers. [8] Previous approaches to classifying characters and digits from natural images used multiple hand-made features [14] and template matching [14]."}, {"heading": "2 Architecture", "text": "The ConvNet architecture consists of repeatedly stacked feature levels. Each level contains a folding module followed by a pooling / subsampling module and a normalization module. While traditional pooling modules in ConvNet are either average or maximum poolings, we use Lp pooling here. The normalization module is only subtractive and not subtractive and divisive, i.e. the mean of each neighborhood is subtracted to the performance of each level (but not divided by the standard deviation, as it reduces performance with this dataset). Finally, multilevel features are also used as opposed to single-level features."}, {"heading": "2.1 Lp-Pooling", "text": "Lp-pooling is a biologically inspired pooling layer that is modelled on complex cells [12, 5] and whose function can be summarised in Equation (1), where G is a Gaussian core, I is the input-feature map, and O is the output-feature map. It can be imagined that it gives increased weight to stronger features and suppresses weaker features. Noteworthy are two specific cases of Lp-pooling. P = 1 corresponds to a simple Gaussian averaging, while P = \u221e corresponds to max-pooling (i.e., only the strongest signal is activated). Lp-pooling was previously used in [6, 15] and a theoretical analysis of this method is described in [1]. O = (1, j) P \u00b7 G (i, j)) 1 / P (1) Figure 2 illustrates a simple example of L2pooling."}, {"heading": "2.2 Multi-Stage Features", "text": "Multi-level characteristics (MS) are obtained by incorporating the results of all levels into the classifier (Figure 3), providing richer representations than single-level characteristics (SS) by adding complementary information such as local textures and fine details lost through higher levels. MS characteristics have continuously improved performance in other work [4, 11, 9] and in this work as well (Figure 4). However, we observe minimal increases in this dataset compared to other object types such as pedestrians and traffic signs (Table 1). The likely explanation for this observation is that increases correlate with the amount of texture and multi-level characteristics of the interesting objects."}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Data Preparation", "text": "The SVHN classification dataset [8] contains 32 x 32 images with 3 color channels. The dataset is divided into three subsets: tension set, additional set and test set. The additional set consists of a large set of simple samples and the tension set is a smaller set of more difficult samples. As we do not receive information on how the sample of these images was conducted, we proceed from a random order to construct our validation set. We compose our validation set of 2 / 3 of training samples (400 per class) and 1 / 3 of additional samples (200 per class), resulting in a total of 6000 samples. This distribution allows us to measure the success of simple samples but places greater emphasis on difficult points. Samples are processed with a local contrast normalization (with a 7x7 core) on the Y channel of the YUV space, followed by a global contrast normalization across each channel."}, {"heading": "3.2 Architecture Details", "text": "The first layer of folding generates 16 characteristics with 5x5 folding filters, while the second layer of folding produces 512 characteristics with 7x7 filters.The output to the classifier also includes inputs from the first layer that provide local characteristics / motifs to enhance the global characteristics. The classifier is a non-linear 2-layer classifier with 20 hidden units. Hyperparameters such as learning rate, regularization constant and learning rate decay are matched to the validation set. We use stochastic gradient descent as our optimization method and mix our data set after each training session. For the pooling layers, we compare Lp-pooling for the value p = 1, 2, 4, 8, 12, 16, 32, 170 to the validation set and use the most powerful pooling on the final test."}, {"heading": "4 Results & Future Work", "text": "Our experiments show a clear advantage of Lp pooling with 1 < p < \u221e on this dataset in validation (Figure 5) and test (the average pooling is 3.58 points below the L2 pooling in Table 2).With L4 pooling, we achieve a state-of-the-art performance on the test set with an accuracy of 94.85% compared to the previous best of 90.6% (Table 2).We also show that the use of multi-level features produces only a small increase in performance compared to the increase in performance in other visual applications. Furthermore, it is important to note that our approach is only fully supervised, whereas the best methods to date are unsupervised learning methods (means, auto-encoders).In the future, we will conduct experiments with unsupervised learning to compare the improvement in accuracy that can be attributed to supervision. Figure 6 shows the validation samples with the highest energy. Many of these experiments appear to be more artificially addressable during the introduction of large problem variations."}], "references": [{"title": "A theoretical analysis of feature pooling in vision algorithms", "author": ["Y. Boureau", "J. Ponce", "Y. LeCun"], "venue": "In Proc. International Conference on Machine learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "A committee of neural networks for traffic sign classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "In International Joint Conference on Neural Networks,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Character recognition in natural images", "author": ["T.E. de Campos", "B.R. Babu", "M. Varma"], "venue": "In Proceedings of the International Conference on Computer Vision Theory and Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Human tracking using convolutional neural networks", "author": ["J. Fan", "W. Xu", "Y. Wu", "Y. Gong"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Complex cell pooling and the statistics of natural images", "author": ["A. Hyvrinen", "U. Kster"], "venue": "In Computation in Neural Systems,,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Learning invariant features through topographic filter maps", "author": ["K. Kavukcuoglu", "M. Ranzato", "R. Fergus", "Y. Le- Cun"], "venue": "In Proc. International Conference on Computer Vision and Pattern Recognition", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Y. Netzer", "T. Wang", "A. Coates", "A. Bissacco", "B. Wu", "A.Y. Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Traffic signs and pedestrians vision with multi-scale convolutional networks", "author": ["P. Sermanet", "K. Kavukcuoglu", "Y. LeCun"], "venue": "In Snowbird Machine Learning Workshop,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Eblearn: Open-source energy-based learning in c++", "author": ["P. Sermanet", "K. Kavukcuoglu", "Y. LeCun"], "venue": "In Proc. International Conference on Tools with Artificial Intelligence", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Traffic sign recognition with multi-scale convolutional networks", "author": ["P. Sermanet", "Y. LeCun"], "venue": "In Proceedings of International Joint Conference on Neural Networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "A model of neuronal responses in visual area", "author": ["E.P. Simoncelli", "D.J. Heeger"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "The German Traffic Sign Recognition Benchmark: A multi-class classification competition", "author": ["J. Stallkamp", "M. Schlipsing", "J. Salmen", "C. Igel"], "venue": "In IEEE International Joint Conference on Neural Networks,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Digit classification on signboards for telephone number recognition", "author": ["T. Yamaguchi", "Y. Nakano", "M. Maruyama", "H. Miyao", "T. Hananoi"], "venue": "In ICDAR,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "In in IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}], "referenceMentions": [{"referenceID": 6, "context": "[8] recently introduced a new digit classification dataset of house numbers extracted from street level images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Previous approaches in classifying characters and digits from natural images used multiple hand-crafted features [3] and template-matching [14].", "startOffset": 113, "endOffset": 116}, {"referenceID": 12, "context": "Previous approaches in classifying characters and digits from natural images used multiple hand-crafted features [3] and template-matching [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "[8] demonstrated the superiority of learned features over hand-designed ones.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "was also previously shown among others in a traffic sign classification challenge [13] where two independent teams obtained the best performance against various other approaches using ConvNets [11, 2].", "startOffset": 82, "endOffset": 86}, {"referenceID": 9, "context": "was also previously shown among others in a traffic sign classification challenge [13] where two independent teams obtained the best performance against various other approaches using ConvNets [11, 2].", "startOffset": 193, "endOffset": 200}, {"referenceID": 1, "context": "was also previously shown among others in a traffic sign classification challenge [13] where two independent teams obtained the best performance against various other approaches using ConvNets [11, 2].", "startOffset": 193, "endOffset": 200}, {"referenceID": 6, "context": "[8] also show superior results with unsupervised learning, we however only report results with fully-supervised training.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "We use the traditional ConvNet architecture augmented with different pooling methods and with multi-stage features [11].", "startOffset": 115, "endOffset": 119}, {"referenceID": 8, "context": "This work was implemented with the EBLearn 1 C++ open-source framework [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "Lp pooling is a biologically inspired pooling layer modelled on complex cells [12, 5] who\u2019s operation can be summarized in equation (1), where G is a Gaussian kernel, I is the input feature map and O is the output feature map.", "startOffset": 78, "endOffset": 85}, {"referenceID": 4, "context": "Lp pooling is a biologically inspired pooling layer modelled on complex cells [12, 5] who\u2019s operation can be summarized in equation (1), where G is a Gaussian kernel, I is the input feature map and O is the output feature map.", "startOffset": 78, "endOffset": 85}, {"referenceID": 5, "context": "Lp-pooling has been used previously in [6, 15] and a theoretical analysis of this method is described in [1].", "startOffset": 39, "endOffset": 46}, {"referenceID": 13, "context": "Lp-pooling has been used previously in [6, 15] and a theoretical analysis of this method is described in [1].", "startOffset": 39, "endOffset": 46}, {"referenceID": 0, "context": "Lp-pooling has been used previously in [6, 15] and a theoretical analysis of this method is described in [1].", "startOffset": 105, "endOffset": 108}, {"referenceID": 3, "context": "MS features have consistently improved performance in other work [4, 11, 9] and in this work as well (Figure 4).", "startOffset": 65, "endOffset": 75}, {"referenceID": 9, "context": "MS features have consistently improved performance in other work [4, 11, 9] and in this work as well (Figure 4).", "startOffset": 65, "endOffset": 75}, {"referenceID": 7, "context": "MS features have consistently improved performance in other work [4, 11, 9] and in this work as well (Figure 4).", "startOffset": 65, "endOffset": 75}, {"referenceID": 6, "context": "The SVHN classification dataset [8] contains 32x32 images with 3 color channels.", "startOffset": 32, "endOffset": 35}, {"referenceID": 7, "context": "Pedestrians detection (INRIA) [9] 14.", "startOffset": 30, "endOffset": 33}, {"referenceID": 9, "context": "Traffic Signs classification (GTSRB) [11] 1.", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "Insights from [1] tell us that the optimal value of p varies for different input spaces and there is no single globally optimal value for p.", "startOffset": 14, "endOffset": 17}, {"referenceID": 6, "context": "Performance reported by [8] with the additional Supervised ConvNet with state-of-the-art accuracy of 94.", "startOffset": 24, "endOffset": 27}], "year": 2012, "abstractText": "We classify digits of real-world house numbers using convolutional neural networks (ConvNets). ConvNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are handdesigned, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multistage features and by using Lp pooling and establish a new state-of-the-art of 94.85% accuracy on the SVHN dataset (45.2% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net.", "creator": "gnuplot 4.4 patchlevel 4"}}}