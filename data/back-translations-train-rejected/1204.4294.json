{"id": "1204.4294", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Apr-2012", "title": "Learning in Riemannian Orbifolds", "abstract": "Learning in Riemannian orbifolds is motivated by existing machine learning algorithms that directly operate on finite combinatorial structures such as point patterns, trees, and graphs. These methods, however, lack statistical justification. This contribution derives consistency results for learning problems in structured domains and thereby generalizes learning in vector spaces and manifolds.", "histories": [["v1", "Thu, 19 Apr 2012 09:29:10 GMT  (16kb)", "http://arxiv.org/abs/1204.4294v1", "arXiv admin note: substantial text overlap witharXiv:1001.0921"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1001.0921", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV", "authors": ["brijnesh j jain", "klaus obermayer"], "accepted": false, "id": "1204.4294"}, "pdf": {"name": "1204.4294.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["brijnesh.jain@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 120 4.42 94v1 [cs.LG] 1 9A pr2 012Learning in Riemannian OrbifoldsBrijnesh J. Jain and Klaus ObermayerTechnische Universit\u00e4t Berlin, Germany-mail: brijnesh.jain @ gmail.com Learning in Riemannian Orbifolds is motivated by existing machine learning algorithms that operate directly on finite combinatorial structures such as dot patterns, trees and graphs. However, these methods lack statistical justification. This paper derives consistency results for learning problems in structured domains and thus generalizes learning in vector spaces and manifolds."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 The Problem of Learning on Graphs", "text": "This section aims to outline the problem of learning on structured data in order to motivate learning in Riemannian Orbifolds. As an illustrative example, we consider the problem of estimating the mean of a distribution on associated graphs. We start by describing the structures we want to learn from. Let A be a set of attributes and let us be a distinguishable element that denotes the null or void element. An associated graph is a tuple X = (V, \u03b1) consisting of a finite nonempty set of vertices and an attribute function. Elements of the associated graphs E = (i, j) V \u00d7 V: i and \u03b1: a tuple X = (V, \u03b1) consisting of a finite nonempty set of vertices and an attribute function."}, {"heading": "3 Riemannian Orbifolds", "text": "In order to keep the treatment technically as clear as possible, we assume that X = Rn is the n-dimensional Euclidean space, and\u0441 is a permutation group acting on X. In this case, we can refer to [18] to prove the statements and assertions made in this section. However, in a more general environment, X can also be a Rieman multiplicity. In this case, we refer to [3] for further details."}, {"heading": "3.1 Riemannian Orbifolds", "text": "The binary operation \u00b7: \u2022 X \u2022 X, (\u03b3, x) 7 \u2022 \u03b3 (x) is a group action from \"X\" to X. For \"X,\" the orbit of \"X\" is the set defined by [x] = \"X.\" The quotient set \"X\" = \"X\" / \"X\" = \"X,\" which consists of all orbits, carries the structure of a Riemann orbit. Its orbifold diagram is the surjective continuous mapping of \"X,\" \"X,\" \"7\" [x], which projects each point x onto its orbit [x]. With \"X\" as a trivial permutation group, \"X\" is also an orbifold group. \"Therefore, generalized orbifold sequences generalize the notion of euclidean space and multiplicity. In the following, an orbifold is a triple Q = (X\" id \") consisting of a euclidean space, a permutation group that acts on X, forming\" X \"and\" X \"elements.\""}, {"heading": "3.2 The Riemannian Orbifold of Graphs", "text": "To identify graphs with points from some orbifolds, some technical assumptions are necessary to simplify the mathematical treatment. To do this, let (GA, d) be a graph distance space with a graph distance d (\u00b7 \u00b7 \u00b7 \u00b7). Then, we make the following assumptions: A1. There is a characteristic map: A \u2192 H of attributes in a finite dimensional Euclidean space H and a distance function dH: H \u00b7 H \u2192 R + such that it (\u03b5) = 0 \u0445 H anddA (a, a \u00b2) = dH (a \u00b2) of attributes in a finite dimensional Euclidean space H and a distance function dH: H \u00b7 R + such that it (\u03b5) s graph (a \u00b2) = dH (a \u00b2) is a graph (a \u00b2) of attributes in a finite order A.A2. All graphs are finite of the finite order n, where n is a sufficiently large number."}, {"heading": "3.3 Metric Structures", "text": "Note that in the case of graph orbifolds, intrinsic metrics is a special graph based on a generalization of the concept of the maximum common subgraph, which occurs in various forms as a common choice of the measure of proximity [1, 6, 7, 11, 24, 25]. Each internal product < \u00b7 > on X results in a maximizer k: k: X-X-X-R of the form (X, Y) = max {< x >: x-X, y, Y}. We call the core function k (\u00b7) the optimal alignment core generated by < \u00b7 -X-X-X. Note that the maximizer of a set of positive defined kernels is an indeterminate core."}, {"heading": "3.4 Orbifold Mappings", "text": "This section introduces mappings between orbifolds and examines local analytical concepts of orbital fold functions. We assume that Q = (X) and Q \"= (X) are orbital fold functions. An orbital mapping between Q and Q\" is a mapping f: (R, {id}, idR) between their underlying spaces. The elevator of f is a mapping f: X \u2192 X \"between their mapping spaces f: X.\" Since R is an orbital mapping of the form QR = (R, {id}, idR), we can have an orbital function between Q and QR as a function f: X. \"The elevator of f\" is a function f: X \"R.\" The elevator f \"(f) is an orbital f.\" The elevator f \"is independent of the group action of X."}, {"heading": "4 Generalized Gradients", "text": "This section extends the concept of generalized differentiation in the sense of Norkin. [22] We start with the introduction of generalized differentiable functions. Let's make X = Rn a finite-dimensional euclidean space. [22] A function f: X \u2192 R is generalizable in x-X, that is, if there is a multidimensional map f: x-x-x, then everyone is x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}, {"heading": "5 Stochastic Optimization", "text": "(W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W (W) (W) (W) (W) (W) (W) (W) (W) (W (W) (W) (W) (W) (W) (W) (W (W) (W) (W) (W) (W) (W) (W) (W (W) (W) (W) (W) (W) (W (W) (W) (W) (W) (W) (W) (W) (W) (W) (W"}, {"heading": "6 Examples", "text": "These sections include some typical examples of statistical data analysis and learning problems ranging from vector spaces to structured domains. (W) We assume that Q = (X, p) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x x x x (W) x x x (W) x x x (W) x x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W) x (W (W) x (W) x (W) x (W) x (W) x (W x (W) x (W) x (W) x (W x (W) x (W) x (W) x (W) x (W) x (W) x (W x (W) x (W) x (W) x (W x (W) x (W) x"}, {"heading": "7 Conclusion", "text": "This paper demonstrates the consistency of learning in structured areas by reducing it to stochastic generalized gradient learning on Rieman's Orbit Folds. The proposed framework is applicable to learning on combinatorial structures such as dot patterns, trees and graphs. In retrospect, the proposed results provide a theoretical basis and statistical justification for a number of existing learning methods that operate directly in the field of graphs. Furthermore, the Orbifold framework provides a generic technique for generalizing gradient-based learning methods to structured domains. Future work aims to generalize the theory to more general Rieman's Orbit Folds and process discontinuous graph editing functions.The authors are very grateful to Vladimir Norkin for his kind support and valuable comments."}], "references": [{"title": "A linear programming approach for the weighted graph matching problem", "author": ["H.A. Almohamad", "S.O. Duffuaa"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 15(5):522\u2013525", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1993}, {"title": "editors", "author": ["G. Bakir", "T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola", "B. Taskar"], "venue": "Predicting structured data. The MIT Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Riemannian geometry of orbifolds", "author": ["J.E. Borzellino"], "venue": "PhD thesis, University of California, Los Angelos", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "Stochastic learning", "author": ["L. Bottou"], "venue": "Advanced lectures on machine learning, pages 146\u2013 168", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "On a relation between graph edit distance and maximum common subgraph", "author": ["H. Bunke"], "venue": "Pattern Recognition Letters, 18(8):689 \u2013 694", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "L", "author": ["T.S. Caetano"], "venue": "Cheng, Q.V. Le, , and A.J. Smola. Learning graph matching. In International Conference on Computer Vision, ICCV 2007, pages 1\u20138", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Balanced graph matching", "author": ["T. Cour", "P. Srinivasan", "J. Shi"], "venue": "Advances in Neural Information Processing Systems, NIPS 2007, volume 19", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization", "author": ["Y.M. Ermoliev", "V.I. Norkin"], "venue": "Cybernetics and Systems Analysis, 34(2):196\u2013 215", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}, {"title": "Theory and algorithms on the median graph", "author": ["M. Ferrer"], "venue": "Application to graph-based classification and clustering. PhD thesis, Universitat Aut\u00f2noma de Barcelona", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "I", "author": ["M. Ferrer", "E. Valveny", "F. Serratosa"], "venue": "Bardaj\u0301\u0131, and H. Bunke. Graph-based k-means clustering: A comparison of the set median versus the generalized median graph. In Computer Analysis of Images and Patterns, CAIP 2009, pages 342\u2013350", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "A graduated assignment algorithm for graph matching", "author": ["S. Gold", "A. Rangarajan"], "venue": "Ieee Transactions On Pattern Analysis and Machine Intelligence, 18(4):377\u2013388", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1996}, {"title": "Learning with preknowledge: Clustering with point and graph matching distance measures", "author": ["S Gold", "A Rangarajan", "E Mjolsness"], "venue": "Neural Computation, 8(4):787\u2013 804", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1996}, {"title": "Self-organizing map for clustering in the graph domain", "author": ["S. G\u00fcnter", "H. Bunke"], "venue": "Pattern Recognition Letters, 23(4):405\u2013417", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Median graph computation for graph clustering", "author": ["A. Hlaoui", "S. Wang"], "venue": "Soft Computing-A Fusion of Foundations, Methodologies and Applications, 10(1):47\u201353", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "On the sample mean of graphs", "author": ["B. Jain", "K. Obermayer"], "venue": "International Joint Conference on Neural Networks, IJCNN 2008, pages 993\u20131000", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Algorithms for the sample mean of graphs", "author": ["B. Jain", "K. Obermayer"], "venue": "Computer Analysis of Images and Patterns, CAIP 2009, pages 351\u2013359", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Graph quantization", "author": ["B. Jain", "K. Obermayer"], "venue": "arXiv:1001.0921v1 [cs.AI]", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Structure spaces", "author": ["B. Jain", "K. Obermayer"], "venue": "Journal of Machine Learning Research, 10:2667\u20132714", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Central clustering of attributed graphs", "author": ["B. Jain", "F. Wysotzki"], "venue": "Machine Learning, 56(1-3):169\u2013207", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Structural perceptrons for attributed graphs", "author": ["B. Jain", "F. Wysotzki"], "venue": "Structural, Syntactic, and Statistical Pattern Recognition, SSPR/SPR 2004, pages 85\u201394", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "An median graphs: properties", "author": ["X. Jiang", "A. Munger", "H. Bunke"], "venue": "algorithms, and applications. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23(10):1144\u20131151", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Stochastic generalized-differentiable functions in the problem of nonconvex nonsmooth stochastic optimization", "author": ["V.I. Norkin"], "venue": "Cybernetics, 22(6):804\u2013809", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1986}, {"title": "Clustering of web documents using graph representations", "author": ["A. Schenker", "H. Bunke", "M. Last", "A. Kandel"], "venue": "Applied Graph Theory in Computer Vision and Pattern Recognition, volume 52 of Studies in Computational Intelligence, pages 247\u2013265. Springer", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "An eigendecomposition approach to weighted graph matching problems", "author": ["S. Umeyama"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 10(5):695\u2013703", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1988}, {"title": "A rkhs interpolator-based graph matching algorithm", "author": ["M.A. van Wyk", "T.S. Durrani", "B.J. van Wyk"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "Adaptive switching circuits", "author": ["B. Widrow", "M.E. Hoff"], "venue": "IRE WESCON Convention Record, volume 4, pages 96\u2013104", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1960}], "referenceMentions": [{"referenceID": 8, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 165, "endOffset": 180}, {"referenceID": 15, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 165, "endOffset": 180}, {"referenceID": 14, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 165, "endOffset": 180}, {"referenceID": 20, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 165, "endOffset": 180}, {"referenceID": 9, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 211, "endOffset": 239}, {"referenceID": 11, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 211, "endOffset": 239}, {"referenceID": 12, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 211, "endOffset": 239}, {"referenceID": 13, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 211, "endOffset": 239}, {"referenceID": 18, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 211, "endOffset": 239}, {"referenceID": 14, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 211, "endOffset": 239}, {"referenceID": 22, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 211, "endOffset": 239}, {"referenceID": 16, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 269, "endOffset": 273}, {"referenceID": 19, "context": "Examples from structural pattern recognition that learn on structured data include estimating central points of a distribution on graphs such as the mean and median [9, 16, 15, 21], central clustering of graphs [10, 12, 13, 14, 19, 15, 23], learning graph quantization [17], and multilayer perceptrons for graphs [20].", "startOffset": 313, "endOffset": 317}, {"referenceID": 17, "context": "In retrospect, the structure space framework proposed by [18] theoretically justifies the above approaches in the sense that they actually minimize an empirical risk function on structures.", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "For this we regard graphs as points of some structure space [18].", "startOffset": 60, "endOffset": 64}, {"referenceID": 17, "context": "In comparison to [18], the innovations are as follows: First, we extend the more suitable concept of generalized differentiability in the sense of Norkin [22] to functions on graphs.", "startOffset": 17, "endOffset": 21}, {"referenceID": 21, "context": "In comparison to [18], the innovations are as follows: First, we extend the more suitable concept of generalized differentiability in the sense of Norkin [22] to functions on graphs.", "startOffset": 154, "endOffset": 158}, {"referenceID": 7, "context": "Third, equipped with these results, we apply a consistency theorem by Ermoliev and Norkin [8] for generalized differentiable loss functions.", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "Fourth, since orbifolds generalize Euclidean spaces and manifolds, this framework not only establishes consistency for stochastic generalized gradient learning but also for standard stochastic gradient learning in Euclidean spaces (see [4]) under the unifying umbrella of learning on Riemannian orbifolds.", "startOffset": 236, "endOffset": 239}, {"referenceID": 10, "context": "Examples are geometric graph distance functions [11] and distances based on the maximum common subgraph including graph and subgraph isomorphism [5].", "startOffset": 48, "endOffset": 52}, {"referenceID": 4, "context": "Examples are geometric graph distance functions [11] and distances based on the maximum common subgraph including graph and subgraph isomorphism [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 21, "context": "We therefore address the following questions: (i) How can we extend gradient-based learning problems from Euclidean spaces to GA? (ii) How can we minimize the expected risk of a learning problem with structured input- and/or output-space GA in a statistically consistent way? The ansatz to answer both questions is to identify graphs as points of a Riemannian orbifold and to extend the concept of generalized differentiability in the sense of Norkin [22] in order to apply methods from stochastic optimization for non-differentiable and non-convex loss functions.", "startOffset": 451, "endOffset": 455}, {"referenceID": 17, "context": "In doing so, we can refer to [18] for proofs of statements and claims made in this section.", "startOffset": 29, "endOffset": 33}, {"referenceID": 2, "context": "In this case, we refer to [3] for more details.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "This graph metric occurs in various different guises as a common choice of proximity measure [1, 6, 7, 11, 24, 25].", "startOffset": 93, "endOffset": 114}, {"referenceID": 5, "context": "This graph metric occurs in various different guises as a common choice of proximity measure [1, 6, 7, 11, 24, 25].", "startOffset": 93, "endOffset": 114}, {"referenceID": 6, "context": "This graph metric occurs in various different guises as a common choice of proximity measure [1, 6, 7, 11, 24, 25].", "startOffset": 93, "endOffset": 114}, {"referenceID": 10, "context": "This graph metric occurs in various different guises as a common choice of proximity measure [1, 6, 7, 11, 24, 25].", "startOffset": 93, "endOffset": 114}, {"referenceID": 23, "context": "This graph metric occurs in various different guises as a common choice of proximity measure [1, 6, 7, 11, 24, 25].", "startOffset": 93, "endOffset": 114}, {"referenceID": 24, "context": "This graph metric occurs in various different guises as a common choice of proximity measure [1, 6, 7, 11, 24, 25].", "startOffset": 93, "endOffset": 114}, {"referenceID": 21, "context": "4 Generalized Gradients This section extends the concept of generalized differentiability in the sense of Norkin [22] to orbifold functions.", "startOffset": 113, "endOffset": 117}, {"referenceID": 21, "context": "Generalized differentiable functions have the following properties [22]: 1.", "startOffset": 67, "endOffset": 71}, {"referenceID": 7, "context": "Since the interchange of integral and generalized gradient is valid, that is \u2202WR(W ) = E [\u2202WL(Z,W )] under mild assumptions [8, 22], we can minimize the expected risk R(W ) according to the following stochastic generalized gradient (SGG) method: Wt+1 = \u03a0\u03a9 (Wt \u2212 \u03b7tSt), t \u2265 0, where W0 \u2208 \u03a9 and \u03a0\u03a9 is a projection operator on \u03a9.", "startOffset": 124, "endOffset": 131}, {"referenceID": 21, "context": "Since the interchange of integral and generalized gradient is valid, that is \u2202WR(W ) = E [\u2202WL(Z,W )] under mild assumptions [8, 22], we can minimize the expected risk R(W ) according to the following stochastic generalized gradient (SGG) method: Wt+1 = \u03a0\u03a9 (Wt \u2212 \u03b7tSt), t \u2265 0, where W0 \u2208 \u03a9 and \u03a0\u03a9 is a projection operator on \u03a9.", "startOffset": 124, "endOffset": 131}, {"referenceID": 7, "context": "Then by Ermoliev and Norkin\u2019s Theorem [8], the SGG method is consistent in the sense that the sequence (Wt)t\u22650 converges almost surely to points satisfying necessary extremum conditions \u03a9 = {W \u2208 \u03a9 : 0 \u2208 \u2202WR(W ) +N\u03a9(W )},", "startOffset": 38, "endOffset": 41}, {"referenceID": 3, "context": "online k-means) [4].", "startOffset": 16, "endOffset": 19}, {"referenceID": 25, "context": "Orbifold adaline generalizes the adaline proposed by [26].", "startOffset": 53, "endOffset": 57}, {"referenceID": 19, "context": "Non-standard examples include multi-layer perceptrons for adaptive processing of graphs [20] and learning to predict structured data [2].", "startOffset": 88, "endOffset": 92}, {"referenceID": 1, "context": "Non-standard examples include multi-layer perceptrons for adaptive processing of graphs [20] and learning to predict structured data [2].", "startOffset": 133, "endOffset": 136}, {"referenceID": 11, "context": "Examples include competitive learning [12, 13, 17] and k-means as well as k-medoids algorithms [10, 15, 23].", "startOffset": 38, "endOffset": 50}, {"referenceID": 12, "context": "Examples include competitive learning [12, 13, 17] and k-means as well as k-medoids algorithms [10, 15, 23].", "startOffset": 38, "endOffset": 50}, {"referenceID": 16, "context": "Examples include competitive learning [12, 13, 17] and k-means as well as k-medoids algorithms [10, 15, 23].", "startOffset": 38, "endOffset": 50}, {"referenceID": 9, "context": "Examples include competitive learning [12, 13, 17] and k-means as well as k-medoids algorithms [10, 15, 23].", "startOffset": 95, "endOffset": 107}, {"referenceID": 14, "context": "Examples include competitive learning [12, 13, 17] and k-means as well as k-medoids algorithms [10, 15, 23].", "startOffset": 95, "endOffset": 107}, {"referenceID": 22, "context": "Examples include competitive learning [12, 13, 17] and k-means as well as k-medoids algorithms [10, 15, 23].", "startOffset": 95, "endOffset": 107}], "year": 2012, "abstractText": "Learning in Riemannian orbifolds is motivated by existing machine learning algorithms that directly operate on finite combinatorial structures such as point patterns, trees, and graphs. These methods, however, lack statistical justification. This contribution derives consistency results for learning problems in structured domains and thereby generalizes learning in vector spaces and manifolds.", "creator": "LaTeX with hyperref package"}}}