{"id": "1204.5309", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Apr-2012", "title": "Analysis Operator Learning and Its Application to Image Reconstruction", "abstract": "Exploiting a priori known structural information lies at the core of many image reconstruction methods that can be stated as inverse problems. The synthesis model, which assumes that images can be decomposed into a linear combination of very few atoms of some dictionary, is now a well established tool for the design of image reconstruction algorithms. An interesting alternative is the analysis model, where the signal is multiplied by an analysis operator and the outcome is assumed to be the sparse. This approach has only recently gained increasing interest. The quality of reconstruction methods based on an analysis model severely depends on the right choice of the suitable operator.", "histories": [["v1", "Tue, 24 Apr 2012 08:56:42 GMT  (4344kb,D)", "http://arxiv.org/abs/1204.5309v1", "18 pages, 43 figures"], ["v2", "Sun, 16 Sep 2012 19:34:35 GMT  (3127kb,D)", "http://arxiv.org/abs/1204.5309v2", "12 pages, 7 figures"], ["v3", "Tue, 26 Mar 2013 11:51:49 GMT  (3217kb,D)", "http://arxiv.org/abs/1204.5309v3", "12 pages, 7 figures"]], "COMMENTS": "18 pages, 43 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["simon hawe", "martin kleinsteuber", "klaus diepold"], "accepted": false, "id": "1204.5309"}, "pdf": {"name": "1204.5309.pdf", "metadata": {"source": "CRF", "title": "Analysis Operator Learning and Its Application to Image Reconstruction", "authors": ["Simon Hawe", "Martin Kleinsteuber", "Klaus Diepold"], "emails": [], "sections": [{"heading": null, "text": "In this thesis, we present an algorithm for learning an analysis operator from training images. Our method is based on a \"p-standard minimization of the set of full-scale matrices with normalized columns. We carefully apply the applied conjugate gradient method to manifolds and explain the underlying geometry of the constraints. In addition, we compare our approach with the most modern methods for image denosis, inpainting and single-frame superresolution. Our numerical results show the competitive performance of our general approach in all presented applications compared to specialized state-of-the-art techniques."}, {"heading": "A. Problem Description", "text": "L INEAR inverse problems are ubiquitous in the field of image processing. Prominent examples are image processing [1], inpainting [2], superresolution [3], or image reconstruction from a few indirect measurements as in Compressive Sensing [4]. Essentially, the goal with all these problems is to reconstruct an unknown image s'Rn as accurately as possible from a series of indirect and possibly corrupt measurements y'Rm with n \u2265 m, see [5] for a detailed introduction to inverse problems. Formally, this measurement process can be simply written = As + e, (1) where the vector e'Rm models scan scan errors and noise and A'Rm'n is the measurement matrix that models the scanning process. In many cases, the reconstruction s by simply reversing the equation (1) is highly poorly positioned because either the exact measurement process and the deviation A'Rm'n is the measurement matrix that models the scanning process, which is the simple process of scanning the image reconstruction in comparison with the image reconstruction, or the resolution of the image reconstruction process is in front of many cases)."}, {"heading": "B. Synthesis Model and Dictionary Learning", "text": "In fact, most of them are able to survive on their own, and they see themselves able to survive on their own."}, {"heading": "C. Analysis Model", "text": "An alternative to the synthesis model (3) for the reconstruction of a signal is the solution? = arg min s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s, known as the analysis model [7]. In contrast to the synthesis model, in which a signal is completely described by the non-zero elements of x, the analysis model assumes that the zero elements of the analysed vector \u00b2 s describe the subspace containing the signal. To emphasize this difference, the term cosparity was introduced in [20], in which the number of zeros of x \u00b2 s is simply counted. Since the spareness in the synthesis model depends on the selected dictionary, the cosparity of an analysed signal depends on dictionary."}, {"heading": "D. Contributions", "text": "We are introducing a new algorithm to learn a patch-based analysis operator from a set of training samples, based on several constraints on the operator, one of which is in Section II-B. One of these constraints requires the analysis operator to have a complete rank and normalized rows. To enforce this constraint, we are proposing an efficient geometric conjugate flow method on the so-called oblique manifolds in Section III. In doing so, an initial operator is iteratively updated to minimize the \"p-pseudo-standard\" of the corresponding analyzed training samples while preventing revision to a subset of the training samples. In Section IV, we will also explain how to apply the local patch-based analysis operator to achieve global reconstruction results. We are using the learned analysis operator in Section V for the tasks of image denosis, inpainting and frame superresolution. The examples and numerical results show the broad and effective applicability of our approach."}, {"heading": "E. Notations", "text": "Matrices are written as calligraphic uppercase letters such as X, column vectors are denoted with bold lowercase letters such as x, while scalars are either uppercase letters or lowercase letters such as n, N. With vi we denote the ith element of the vector v, vij denotes the ith element in the jth column of a matrix V. The vector v:, i denotes the ith column of V, whereas vi,: the transposed ith column of V. With Eij we denote a matrix whose ith entry in the jth column is equal to one, and all the others equal to zero. Ik denotes the identity matrix of the dimension (k \u00b7 k), 0 denotes the zero matrix of the corresponding dimension, and ddiag (V) is the diagonal matrix whose entries in the jth column are equal to one, and all the others are zero. With the vector V, 2F = 2 j we denote the quadratic matrix of the corresponding dimension, and ddiag (V) is the diagonal matrix whose entries in the jth column are equal to zero."}, {"heading": "A. Prior Art", "text": "The subject of Analysis Operator Training has only recently been researched, and few previous papers exist. > Subsequently, we will apply the Analysis Operator Learning Methods applicable to image processing to a number of M-learning examples. > The objective of Analysis Operator Learning Processes is to find a matrix that is extracted from a set of sample images..., sM-Learning Examples is a matrix in which the training samples can constitute their columns., The goal of the analysis authors is a differentiated vectorized image analysis extracted from a set of sample images., sM Learning Examples is a matrix in which the training samples constitute their columns."}, {"heading": "B. Motivation of Our Approach", "text": "The learning algorithm presented here finds an analyzer that relates to a set of learning samples by solving a problem related to (6). < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "III. ANALYSIS OPERATOR LEARNING ALGORITHM", "text": "Since we know that the solution to problem (14) is limited to a smooth multiplicity, we can develop a method of geometric conjugate gradient (CG-) to learn the analysis operator. Geometric CG methods have proven efficient due to the combination of moderate computational complexity and good convergence properties in various applications, see e.g. [29] for a CG-like method on oblique multiplicity. To conclude this work in itself, we begin with a brief review of the general concepts of optimization on matrix multiplicity. Afterwards, we present the concrete formulas and implementation details for our optimization problem on oblique multiplicity. For a detailed introduction to optimization on matrix multiplicity, please refer the interested reader to [30]."}, {"heading": "A. Optimization on Matrix Manifolds", "text": "Release M, which is called an element of tangent vector at X. Each tangent vector space is associated with an inner product that differs from the surrounding Rn \u00b7 k, which allows the dimensions and angles to M.The Riemannian gradient of TXM, an element of tangent vector space on M.The Riemannian gradient of TXM is an element of tangent vector space on M.The Riemannian gradient of TXM is an element of tangent vector space on the surrounding Rn \u00b7 k, which allows the dimensions and angles to M.The Riemannian gradient of TXM, which is an element of tangent vector at X."}, {"heading": "B. Geometric Conjugate Gradient for Analysis Operator Learning", "text": "In this section, we derive all the ingredients to implement the geometric conjugate method as described in the previous section description. (1) The results regarding the geometry of OB (n, k) are derived to improve readability, and since the dimensions n and k are fixed in the rest of the work, the oblique manifold method of OB.H is further extended to the tangential space TXOB (Q) = Q (X > Q). (30) The orthogonal projection of a matrix Q (n) onto the tangential space TXOB is the solution of the arbitrary manifestations, meaning that its calculation is as good as possible. (30) Regarding geodesics, note that the solution is a second ordered equation."}, {"heading": "IV. ANALYSIS OPERATOR BASED IMAGE RECONSTRUCTION", "text": "In this section, we explain how the success of this approach depends to a large extent on the partitioning of the image."}, {"heading": "V. APPLICATIONS AND EXPERIMENTAL RESULTS", "text": "This shows different results for classic image reconstruction problems by using an analysis operator unfamiliar with our algorithms proposed in Section III. (We compare all our results with a total variation-based approach [34]. It serves as a general yardstick for all reconstruction applications considered, as it is the most commonly used analysis operator. (The current evaluation is limited to grayscale images, but the general concept can be extended directly to color images.) The following experimental results we selected were patches of size (7 \u00d7 7), i.e. n = 49. Note that we learned a general analysis of the operator, i.e. the results in good reconstruction performance are independent of the chosen patch size n that we chose, i.e. n = 49. Note that we learned from the five test images shown in Figure 2."}, {"heading": "C. Single Image Super-Resolution", "text": "In Single Frame Superresolution (SR), the goal is to reconstruct a high resolution image s-RN from an observed low resolution image y-Rm, assuming that y is a blurred and downsampled version of s. Mathematically, this process can be formulated as y = DBs + e, where D-Rm \u00b7 N is a decimation operator and B-RN \u00b7 N is a blurred operator. Therefore, the measurement matrix is given by A = DB. Ideally, the exact blur core is known or given an estimate. Here, we consider the more realistic case of an unknown blur core. To apply our approach to enlarging an image by a factor d in both vertical and horizontal dimensions, we model the blur over a Gaussian core of the dimension (2d \u2212 1) and with the standard deviation manifoldness of the image."}, {"heading": "VI. CONCLUSION", "text": "This paper dealt with the topic of learning the operator of analysis using sample image fields and its application for the precise solution of several inverse problems in imaging. To learn the operator, we motivated a \"minimization on the set of full-fledged matrices with normalized columns. To solve the optimization task occurring, a method of geometric conjugate gradient on the oblique manifold is proposed. Furthermore, we provided an invariant partitioning method to deploy the local patch-based analysis operator in order to achieve globally consistent reconstruction results. For the famous tasks of image denosis, image coloring and single image superresolution, we provide promising results that compete with the current state of the art. Similar to the synthesis signal reconstruction model with dictionaries, we expect that the performance of the analysis approach can be further increased depending on the application, by using the respective operator in relation to the specific training set."}], "references": [{"title": "Image denoising using scale mixtures of gaussians in the wavelet domain", "author": ["J. Portilla", "V. Strela", "M. Wainwright", "E. Simoncelli"], "venue": "IEEE Transactions on Image Processing, vol. 12, no. 11, pp. 1338\u20131351, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Image inpainting", "author": ["M. Bertalm\u00eco", "G. Sapiro", "V. Caselles", "C. Ballester"], "venue": "ACM SIGGRAPH, 2000, pp. 417\u2013424.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "Example-based super-resolution", "author": ["W.T. Freeman", "T.R. Jones", "E.C. Pasztor"], "venue": "IEEE Computer Graphics and Applications, vol. 22, no. 2, pp. 56\u201365, 2002.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information", "author": ["E.J. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 2, pp. 489\u2013509, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to the mathematical theory of inverse problems", "author": ["A. Kirsch"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1991}, {"title": "On the role of sparse and redundant representations in image processing", "author": ["M. Elad", "M.A.T. Figueiredo", "Y.M."], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 972\u2013982, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Analysis versus synthesis in signal priors", "author": ["M. Elad", "P. Milanfar", "R. Rubinstein"], "venue": "Inverse Problems, vol. 3, no. 3, pp. 947\u2013968, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Computational methods for sparse solution of linear inverse problems", "author": ["J.A. Tropp", "S.J. Wright"], "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 948\u2013958, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "A theory for multiresolution signal decomposition: the wavelet representation", "author": ["S. Mallat"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 11, no. 7, pp. 674\u2013693, 1989.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1989}, {"title": "Sparse geometric image representations with bandelets", "author": ["E. Le Pennec", "S. Mallat"], "venue": "IEEE Transactions on Image Processing, vol. 14, no. 4, pp. 423\u2013438, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "The curvelet transform for image denoising", "author": ["J.-L. Starck", "E.J. Cand\u00e8s", "D.L. Donoho"], "venue": "IEEE Transactions on Image Processing, vol. 11, no. 6, pp. 670\u2013684, 2002.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Image denoising via sparse and redundant representations over learned dictionaries", "author": ["M. Elad", "M. Aharon"], "venue": "IEEE Transactions on Image Processing, vol. 15, no. 12, pp. 3736\u20133745, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning Research, vol. 11, no. 1, pp. 19\u201360, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonparametric bayesian dictionary learning for analysis of noisy and incomplete images", "author": ["M. Zhou", "H. Chen", "J. Paisley", "L. Ren", "L. Li", "Z. Xing", "D. Dunson", "G. Sapiro", "L. Carin"], "venue": "IEEE Transactions on Image Processing, vol. 21, no. 1, pp. 130\u2013144, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Dictionary learning algorithms for sparse representation", "author": ["K. Kreutz-Delgado", "J.F. Murray", "B.D. Rao", "K. Engan", "T.W. Lee", "T.J. Sejnowski"], "venue": "Neural computation, vol. 15, no. 2, pp. 349\u2013396, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Method of optimal directions for frame design", "author": ["K. Engan", "S. Aase", "J. Hakon Husoy"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, 1999, pp. 2443\u20132446.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "K-svd: An algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Transactions on Signal Processing, vol. 54, no. 11, pp. 4311\u20134322, 2006.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Double sparsity: Learning sparse dictionaries for sparse signal approximation", "author": ["R. Rubinstein", "M. Zibulevsky", "M. Elad"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 3, pp. 1553\u20131564, 2010.  TECHNICAL REPORT, TECHNISCHE UNIVERSIT\u00c4T M\u00dcNCHEN  18", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Dictionary learning", "author": ["I. To\u0161i\u0107", "P. Frossard"], "venue": "IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 27\u201338, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Cosparse analysis modeling - uniqueness and algorithms", "author": ["S. Nam", "M. Davies", "M. Elad", "R. Gribonval"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, 2011, pp. 5804\u20135807.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparsity and smoothness via the fused lasso", "author": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "Journal of the Royal Statistical Society Series B, pp. 91\u2013108, 2005.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Signal restoration with overcomplete wavelet transforms: Comparison of analysis and synthesis priors", "author": ["I.W. Selesnick", "M.A.T. Figueiredo"], "venue": "In Proceedings of SPIE Wavelets XIII, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear total variation based noise removal algorithms", "author": ["L.I. Rudin", "S. Osher", "E. Fatemi"], "venue": "Physica D, vol. 60, no. 1-4, pp. 259\u2013268, 1992.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1992}, {"title": "Sequential minimal eigenvalues - an approach to analysis dictionary learning", "author": ["B. Ophir", "M. Elad", "N. Bertin", "M.D. Plumbley"], "venue": "European Signal Processing Conference, 2011, pp. 1465\u20131469.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "K-SVD dictionary-learning for the analysis sparse model", "author": ["R. Rubinstein", "T. Faktor", "M. Elad"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis operator learning for overcomplete cosparse representations", "author": ["M. Yaghoobi", "S. Nam", "R. Gribonval", "M.E. Davies"], "venue": "European Signal Processing Conference, 2011, pp. 1470\u20131474.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Noise aware analysis operator learning for approximately cosparse signals", "author": ["\u2014\u2014"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "A continuous-time approach to the oblique procrustes problem", "author": ["N.T. Trendafilov"], "venue": "Behaviormetrika, vol. 26, no. 2, pp. 167\u2013181, 1999.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Blind source separation with compressively sensed linear mixtures", "author": ["M. Kleinsteuber", "H. Shen"], "venue": "IEEE Signal Processing Letters, vol. 19, no. 2, pp. 107\u2013110, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimization Algorithms on Matrix Manifolds", "author": ["P.-A. Absil", "R. Mahony", "R. Sepulchre"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "An efficient hybrid conjugate gradient method for unconstrained optimization", "author": ["Y. Dai", "Y. Yuan"], "venue": "Annals of Operations Research, vol. 103, no. 1-4, pp. 33\u201347, 2001.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "Global convergence properties of conjugate gradient methods for optimization", "author": ["J.C. Gilbert", "J. Nocedal"], "venue": "SIAM Journal on Optimization, vol. 2, no. 1, pp. 21\u201342, 1992.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1992}, {"title": "Fields of experts: a framework for learning image priors", "author": ["S. Roth", "M. Black"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2005, pp. 860\u2013867.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Algorithms and software for total variation image reconstruction via first-order methods", "author": ["J. Dahl", "P.C. Hansen", "S. Jensen", "T.L. Jensen"], "venue": "Numerical Algorithms, vol. 53, no. 1, pp. 67\u201392, 2010.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}, {"title": "Cartoon-like image reconstruction via constrained `p-minimization", "author": ["S. Hawe", "M. Kleinsteuber", "K. Diepold"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing, 2012, pp. 717\u2013720.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Image denoising by sparse 3-d transform-domain collaborative filtering", "author": ["K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian"], "venue": "IEEE Transactions on Image Processing, vol. 16, no. 8, pp. 2080\u20132095, 2007.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "Image super-resolution via sparse representation", "author": ["J. Yang", "J. Wright", "T. Huang", "Y. Ma"], "venue": "IEEE Transactions on Image Processing, vol. 19, no. 11, pp. 2861\u20132873, 2010.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Prominent examples are image denoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4].", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "Prominent examples are image denoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4].", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Prominent examples are image denoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4].", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "Prominent examples are image denoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4].", "startOffset": 163, "endOffset": 166}, {"referenceID": 4, "context": "Basically, in all these problems the goal is to reconstruct an unknown image s \u2208 Rn as accurately as possible from a set of indirect and maybe corrupted measurements y \u2208 Rm with n \u2265 m, see [5] for a detailed introduction to inverse problems.", "startOffset": 189, "endOffset": 192}, {"referenceID": 5, "context": "[6], is that natural images admit a sparse representation x \u2208 Rd over some dictionary D \u2208 Rn\u00d7d with d \u2265 n.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "As the signal is synthesized from the sparse coefficients, the reconstruction model (3) is called the synthesis reconstruction model [7].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "For a broad overview of such algorithms, we refer the interested reader to [8].", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "Popular examples include Wavelets [9], Bandlets [10], and Curvlets [11] among several others, or a concatenation of various such bases/dictionaries.", "startOffset": 34, "endOffset": 37}, {"referenceID": 9, "context": "Popular examples include Wavelets [9], Bandlets [10], and Curvlets [11] among several others, or a concatenation of various such bases/dictionaries.", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "Popular examples include Wavelets [9], Bandlets [10], and Curvlets [11] among several others, or a concatenation of various such bases/dictionaries.", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "This is desirable for various image reconstruction applications as it readily improves their performance and accuracy [12]\u2013[14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 13, "context": "This is desirable for various image reconstruction applications as it readily improves their performance and accuracy [12]\u2013[14].", "startOffset": 123, "endOffset": 127}, {"referenceID": 14, "context": "Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18].", "startOffset": 125, "endOffset": 129}, {"referenceID": 15, "context": "Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18].", "startOffset": 175, "endOffset": 179}, {"referenceID": 17, "context": "Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18].", "startOffset": 304, "endOffset": 308}, {"referenceID": 18, "context": "For a comprehensive overview of dictionary learning techniques see [19].", "startOffset": 67, "endOffset": 71}, {"referenceID": 6, "context": "which is known as the analysis model [7].", "startOffset": 37, "endOffset": 40}, {"referenceID": 19, "context": "To emphasize this difference, the term cosparsity has been introduced in [20], which simply counts the number of zero elements of \u03a9s.", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "Different analysis operators proposed in the literature include the fused Lasso [21], the translation invariant wavelet transform [22], and probably best known the finite difference operator which is closely related to the total-variation [23].", "startOffset": 80, "endOffset": 84}, {"referenceID": 21, "context": "Different analysis operators proposed in the literature include the fused Lasso [21], the translation invariant wavelet transform [22], and probably best known the finite difference operator which is closely related to the total-variation [23].", "startOffset": 130, "endOffset": 134}, {"referenceID": 22, "context": "Different analysis operators proposed in the literature include the fused Lasso [21], the translation invariant wavelet transform [22], and probably best known the finite difference operator which is closely related to the total-variation [23].", "startOffset": 239, "endOffset": 243}, {"referenceID": 6, "context": "The question is: can the performance of analysis based signal reconstruction be improved upon when a learnt analysis operator is applied instead of a predefined one, as it is the case for the synthesis model where learnt dictionaries outperform analytic dictionaries? As it has been discussed in [7], the two models differ significantly, and the na\u00efve way of learning a dictionary and simply employing its transposed or its pseudo-inverse as the learnt analysis operator fails.", "startOffset": 296, "endOffset": 299}, {"referenceID": 23, "context": "In [24], an algorithm is proposed where the rows of the analysis operator are found sequentially by identifying directions that are orthogonal to a subset of the training samples.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "An adaption of the widely known K-SVD dictionary learning algorithm to the problem of analysis operator learning is presented in [25].", "startOffset": 129, "endOffset": 133}, {"referenceID": 23, "context": "Thereby, each row of \u03a9 is updated in a similar way as described in the previous paragraph for the method of [24].", "startOffset": 108, "endOffset": 112}, {"referenceID": 23, "context": "Interestingly, the operator learnt on piecewise constant image patches by [24] and [25] closely mimics the finite difference operator.", "startOffset": 74, "endOffset": 78}, {"referenceID": 24, "context": "Interestingly, the operator learnt on piecewise constant image patches by [24] and [25] closely mimics the finite difference operator.", "startOffset": 83, "endOffset": 87}, {"referenceID": 25, "context": "In [26], the authors employ g(\u03a9S) = \u2016\u03a9S\u20161 as the sparsity promoting function and suggest a constrained optimization technique that utilizes a projected subgradient method for iteratively solving (6).", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "These constraints motivate the consideration of the set of full rank matrices with normalized columns, which admits a manifold structure known as the oblique manifold [28] OB(n, k) := {X \u2208 Rn\u00d7k| rk(X ) = n, ddiag(X>X ) = Ik}.", "startOffset": 167, "endOffset": 171}, {"referenceID": 28, "context": "[29] for a CG-type method on the oblique manifold.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "For an in-depth introduction on optimization on matrix manifolds, we refer the interested reader to [30].", "startOffset": 100, "endOffset": 104}, {"referenceID": 29, "context": "in [30].", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "which has been suggested in [31].", "startOffset": 28, "endOffset": 32}, {"referenceID": 31, "context": "as proposed in [32].", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "To overcome these drawbacks, we introduce a method related to the patch based synthesis approach presented in [12] and the MRF approach from [33], which provides global support from local information.", "startOffset": 110, "endOffset": 114}, {"referenceID": 32, "context": "To overcome these drawbacks, we introduce a method related to the patch based synthesis approach presented in [12] and the MRF approach from [33], which provides global support from local information.", "startOffset": 141, "endOffset": 145}, {"referenceID": 33, "context": "We compare all our results with a total variation based approach [34].", "startOffset": 65, "endOffset": 69}, {"referenceID": 34, "context": "For all applications we reconstructed the images by solving the minimization problem (47) using the conjugate gradient method proposed in [35].", "startOffset": 138, "endOffset": 142}, {"referenceID": 35, "context": "(b) BM3D denoising [36], PSNR = 30.", "startOffset": 19, "endOffset": 23}, {"referenceID": 11, "context": "(c) K-SVD denoising [12], PSNR = 29.", "startOffset": 20, "endOffset": 24}, {"referenceID": 33, "context": "(d) TV denoising [34], PSNR = 28.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "Table I presents a comparison of the reconstruction results achieved by our method with (i) the K-SVD approach specifically designed for image denoising [12], (ii) the currently best denoising algorithm BM3D [36], and (iii) total-variation based denoising [34].", "startOffset": 153, "endOffset": 157}, {"referenceID": 35, "context": "Table I presents a comparison of the reconstruction results achieved by our method with (i) the K-SVD approach specifically designed for image denoising [12], (ii) the currently best denoising algorithm BM3D [36], and (iii) total-variation based denoising [34].", "startOffset": 208, "endOffset": 212}, {"referenceID": 33, "context": "Table I presents a comparison of the reconstruction results achieved by our method with (i) the K-SVD approach specifically designed for image denoising [12], (ii) the currently best denoising algorithm BM3D [36], and (iii) total-variation based denoising [34].", "startOffset": 256, "endOffset": 260}, {"referenceID": 1, "context": "Image Inpainting In image inpainting as originally proposed in [2], the goal is to fill up a set of damaged or disturbing pixels such that the resulting image is visually appealing.", "startOffset": 63, "endOffset": 66}, {"referenceID": 35, "context": "EACH CELL PRESENTS FOUR PSNR RESULTS IN DB ACHIEVED BY DENOISING THE RESPECTIVE IMAGE WITH FOUR DIFFERENT ALGORITHMS, WHICH ARE: TOP LEFT BM3D [36](THE CURRENTLY BEST RATED DENOISING ALGORITHM), TOP RIGHT TOTAL-VARIATION DENOISING (BEST WORKING ANALYSIS METHOD), BOTTOM LEFT K-SVD DENOISING [12], BOTTOM RIGHT PROPOSED METHOD.", "startOffset": 143, "endOffset": 147}, {"referenceID": 11, "context": "EACH CELL PRESENTS FOUR PSNR RESULTS IN DB ACHIEVED BY DENOISING THE RESPECTIVE IMAGE WITH FOUR DIFFERENT ALGORITHMS, WHICH ARE: TOP LEFT BM3D [36](THE CURRENTLY BEST RATED DENOISING ALGORITHM), TOP RIGHT TOTAL-VARIATION DENOISING (BEST WORKING ANALYSIS METHOD), BOTTOM LEFT K-SVD DENOISING [12], BOTTOM RIGHT PROPOSED METHOD.", "startOffset": 291, "endOffset": 295}, {"referenceID": 33, "context": "The second experiment demonstrates another typical inpainting problem taken from the literature [34], where a text overlaying an image has to be removed, see Figure 6.", "startOffset": 96, "endOffset": 100}, {"referenceID": 33, "context": "For comparison, we present the TV based results from [34] in Figure 6(c).", "startOffset": 53, "endOffset": 57}, {"referenceID": 36, "context": "In Figure 7(e) and (j), we show the reconstruction results of our method for magnifying the \"Girl\" and the \"Baboon\" image by d = 3 and compare it with bicubic interpolation Figure 7(b) and (g), TV based upsampling Figure 7(d) and (i), and the SR algorithm presented in [37] Figure 7(c) and (h).", "startOffset": 269, "endOffset": 273}, {"referenceID": 36, "context": "(c) Method [37], PSNR = 32.", "startOffset": 11, "endOffset": 15}, {"referenceID": 36, "context": "(h) Method [37], PSNR = 23.", "startOffset": 11, "endOffset": 15}, {"referenceID": 36, "context": "(d) Method [37], PSNR = 22.", "startOffset": 11, "endOffset": 15}], "year": 2017, "abstractText": "Exploiting a priori known structural information lies at the core of many image reconstruction methods that can be stated as inverse problems. The synthesis model, which assumes that images can be decomposed into a linear combination of very few atoms of some dictionary, is now a well established tool for the design of image reconstruction algorithms. An interesting alternative is the analysis model, where the signal is multiplied by an analysis operator and the outcome is assumed to be the sparse. This approach has only recently gained increasing interest. The quality of reconstruction methods based on an analysis model severely depends on the right choice of the suitable operator. In this work, we present an algorithm for learning an analysis operator from training images. Our method is based on an `p-norm minimization on the set of full rank matrices with normalized columns. We carefully introduce the employed conjugate gradient method on manifolds, and explain the underlying geometry of the constraints. Moreover, we compare our approach to state-of-the-art methods for image denoising, inpainting, and single image super-resolution. Our numerical results show competitive performance of our general approach in all presented applications compared to the specialized state-of-the-art techniques.", "creator": "LaTeX with hyperref package"}}}