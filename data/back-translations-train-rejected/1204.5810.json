{"id": "1204.5810", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2012", "title": "Geometry of Online Packing Linear Programs", "abstract": "We consider packing LP's with $m$ rows where all constraint coefficients are normalized to be in the unit interval. The n columns arrive in random order and the goal is to set the corresponding decision variables irrevocably when they arrive so as to obtain a feasible solution maximizing the expected reward. Previous (1 - \\epsilon)-competitive algorithms require the right-hand side of the LP to be Omega((m/\\epsilon^2) log (n/\\epsilon)), a bound that worsens with the number of columns and rows. However, the dependence on the number of columns is not required in the single-row case and known lower bounds for the general case are also independent of n.", "histories": [["v1", "Thu, 26 Apr 2012 02:06:44 GMT  (48kb,D)", "http://arxiv.org/abs/1204.5810v1", null]], "reviews": [], "SUBJECTS": "cs.DS cs.LG", "authors": ["marco molinaro", "r ravi"], "accepted": false, "id": "1204.5810"}, "pdf": {"name": "1204.5810.pdf", "metadata": {"source": "CRF", "title": "Geometry of Online Packing Linear Programs", "authors": ["Marco Molinaro", "R. Ravi"], "emails": [], "sections": [{"heading": null, "text": "n), a limit that worsens with the number of columns and rows. However, dependence on the number of columns is not required in the single-line case, and known lower limits for the general case are also independent of n.Our goal is to understand whether dependence on n is required in the multi-line case, making it generally more difficult than in the single-line version. We refute this by having an algorithm that is (1 \u2212) competitive as long as the right sides are blank (m 2 2 2 log m).Our techniques refine earlier PAClearning approaches that interpreted online decisions as a linear classification of columns based on patterned dual prices.The main component of our improvement stems from a non-standard coverage argument along with the realization that the columns of the LP belong to only a few single-line subranges that we can obtain small such covers; the limitation of the size of the cover is also based on the geometry of the classifier."}, {"heading": "1 Introduction", "text": "Traditional optimization models usually assume that input is considered a priori fixed and that the early solution has already been reached. However, in most applications, the data is either disclosed over time or only roughly revealed through input, which is often modeled in terms of a probability distribution. Consequently, much effort has been put into understanding the quality of solutions that can be achieved without full knowledge of input, which has led to the development of online and stochastic optimization [7, 6]. Emerging problems such as assignment of advertisers and yield management on the Internet are inherently online in nature and have further accelerated this development. Linear programming is arguably the most important and therefore well-studied optimization problem. Therefore, understanding the limitations of linear programs when complete data is not available is a fundamental theoretical problem with a number of applications, including ad allocation and yield management problems over. In fact, a simple seamless program with a unified problem is considered to be the first problem of limiting online."}, {"heading": "2 OTP for almost 1-dim columns", "text": "In this section we describe and analyze the algorithm OTP (One-Time Pricing) about LPs whose columns are contained in a few 1-dimensional subranges of Rm. The general goal is to find a suitable double (maybe not feasible) solution p for (LP) and to use it to classify the columns of the LP. Specifically, since p + Rm, we define x (p) t = 1 if p) and x (p) t = 0 otherwise. Therefore, x (p) is the result of classifying the columns (\u03c0t, at) s with the homogeneous hyperplane in Rm + 1 with normal (\u2212 1, p). The motivation behind this classification is that it selects the columns that have positive reduced costs in relation to the dual solution p, or alternatively, it solves the Lagrangic Relaxation with p as multipliers.Sampling LP's."}, {"heading": "2.1 Connection to PAC learning", "text": "& & # 8222; K & # 8222; K & # 8211; K & # 8222; K & # 8222; K & # 8211; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8220; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 8222; K & # 222; K & # 222; K & # 82K; 82K; 82K; 82K; 82K; 82K; 82K; 82K; 82K; 8222; K & # 8222; K; K & # 222; K & # 2K; K; 82K; 82K; K; 82K; 82K; 8222; K; K & # 8222; K; K & # 82K; K; 82K; 82K; K; 82K; 82K; K; 82K; 82K; K; 8222; K; K; K; 82K; 82K; K; 82K; 82K; K; 82K; K; 82K; K; 82K; K; 82K; K; K; 82K; K; 82K; K; 82K; K; K; K; 82K; K; K; K; 82K; K; 82K; K; K; 82K; K; 82K; K; K; K; K; 82K; K; 82K; K; K; K; 82K; K; K; 82K; K; K; 82K; K; K; 82K; K; K; 82K; K; K; K; 82K; K; K; K; 82K; K; K; 82K; K; K; K; K"}, {"heading": "2.2 Similarity via witnesses", "text": "First of all, we are able to engage in two areas, depending on why they are bad: for i (m), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x (x), x, x (x), x, x (x), x (x, x (x), x (x), x (x), x (x), x (x (x), x (x, x), x (x, x), x (x (x), x (x), x (x, x), x (x), x (x (x), x, x (x), x (x), x (x, x (x), x (x), x (x), x (x (x), x, x (x), x (x, x (x), x (x), x (x, x), x, x (x), x (x (x), x (x), x (x, x), x (x, x), x (x (x, x), x (x, x, x), x (x), x ("}, {"heading": "2.3 Small witness sets for almost 1-dim columns", "text": "In this case, X + i and X \u2212 i are the maximum set of X \u2212 i. We assume that we can lie in a few one-dimensional subspaces."}, {"heading": "3 Robust OTP", "text": "In this section we look at (LP) with columns that do not belong to a few 1-dimensional Q = columns. (Considering the results of the previous section, we would like to disturb the columns of this LP so that they belong to a few 1-dimensional Q = columns, and so that an approximate solution to this disturbed LP is also an approximate solution to the original. (More precisely: we obtain a series of vectorsQ \u2212 Rm and transform each column into a column a, which is a scaling of a vector in Q, and we leave the rewards unchanged.) The crucial observation is that the solutions of an LP \u2212 Rm are resilient to minor changes in the constraint matrix.Lemma 3.1 Let's look at real numbers in a column a, which is a scaling of a vector in Q, and we leave the rewards unchanged."}, {"heading": "4 Robust DPA", "text": "In this section, we describe our final algorithm, which has an improved dependency on 1 /. Following [1], the idea is to update the dual vector used in classification as new columns: We use the first 2i n columns to classify the columns 2i n + 1. This leads to improved generalization limits, which in turn reduces the dependence on 1 /. The Robust DPA algorithm (as column DPA) can be seen as a combination of solutions to multiple sampled LPs, achieved through a modification of OTP, which is indicated by (s, \u03b4) -OTP. This algorithm aims to solve the program (2s, 1) -LP and can be described as follows: it finds an optimal dual solution (p, \u03b1) for (s, (1 \u2212 3) -LP and sets x.x."}, {"heading": "5 Open problems", "text": "A very interesting open question is whether the techniques presented in this paper can be used to obtain improved algorithms for general mapping problems [14]. The difficulty with this problem is that the classifications of the columns are no longer linear; they essentially stem from a conjunction of linear classifiers. In view of this additional flexibility, it does not seem strong enough to have the columns in a few one-dimensional subspaces to impose strong properties in the classifications. It would be interesting to find the appropriate geometric structure of the columns in this case. A direct open question, of course, is to improve the lower or upper limit of the dependence on the right side B in order to obtain (1 \u2212) -competitive algorithms. One possibility is to investigate how much the techniques presented here can be advanced and what their limitations are. Another possibility is to analyze the performance of the algorithm from [10] under the random permutation model."}, {"heading": "A Bernstein inequality for sampling without replacement", "text": "Lemma A.1 (theorem 2.14.19 in [25]) Let Y = {Y1,.., Yn} be a series of real numbers in the interval [0, 1] and let 0 < < 1. Let S be a random subset of Y of size s and let YS = [1] S be Yi. Set \u00b5 = 1 n \u00b2 i Yi and \u03c32 = 1n \u00b2 i (Yi \u2212 \u00b5) 2, then we have this for each case > 0Pr (| YS \u2212 s\u00b5 | \u2265) \u2264 2 exp (\u2212 certaini | Yi | + \u0445) indication that since the Yi belong to the interval [0, 1] we can limit the variance upwards by the mean value as follows: \u03c32 \u2264 1 n \u00b2 | Yi \u2212 \u00b5 | \u2264 1 n (\u0432i | Yi | + \u0445 i | p |) = 2\u00b5.This results in the following corollarity A.2 Let us consider the conditions of the previous dimming."}, {"heading": "B Proof of Lemmas 2.4 and 2.5", "text": "Proof of lemma 2,4: Fix a scenario \u03c3 for the duration of the proof. By assuming, xS is feasible for (LP), so it is sufficient to show that it reaches a value of at least (1 \u2212 3) OPT. Let us consider (LP) with a modified right side: max n \u00b2 t = 1 \u03c0txtn \u00b2 t = 1 atixt \u2264 ai (xS). \u2212 ckn Let us note that xS is an optimal solution for maxx \u00b2. [0,1] n L (pS, x), which is at least the OPT (modLP), is the optimal value of LP (modLP), the optimal value of LP (modLP)."}, {"heading": "C Proof of Lemma 2.8", "text": "The following simple inequalities are helpful: Observation C.1 For, \u03b1, \u03b2 \u2265 0, 1 \u2212 \u03b1 1 + \u03b2 \u2265 1 \u2212 (\u03b1 + \u03b2) and 1 \u2212 \u03b1 1 \u2212 \u03b2 \u2264 1 \u2212 (\u03b1 \u2212 \u03b2). Combining equations (2.1), (2.2) and (2.1) and unification across all terms in the disjunction, we have that Pr (xS is bad) \u2264 i, w \u00b2 W + i Pr (skewm (, w) + \u2211 i, w \u00b2 W \u2212 i Pr (skewp (, w)). Thus, it is sufficient to show that for all w \u00b2 W + i (or w \u00b2 W \u2212 i) the event skewm (, w) (or skewp (, w))) most likely occurs 2 exp (\u2212 3B33). Let us take w \u00b2 W + i (w)."}, {"heading": "D Proof of Lemma 2.10", "text": "s not be the last index of Cj that belongs to x-Cj; this implies that we are for all t-Cj-pcj-pcj-pcj-pcj-pcj-pcj-pcj-pcj-pcj-pcj-pcj-pcj-pcj-pcj-pcj. From the arrangement of columns follows that x-Cj-t-Cj-t-pcj-t-pcj-t-pcj-pcj-pcj-pcj-pj-pj-cj-cj-cj-cj-cj-cj-cj-cj-cj-cj-cj-cj-cj-cj-pj-pj-pj-cj-cj-pj-cj-cj-cj-pj-pj-cj-cj-cj-pj-pj-cj-pj-cj-pj-cj-cj-cj-cj-cj-cj-cj-cj-pj-pj-pj-pj-pcj-pcj-pcj-pcj-pcj-pj-pcj-pcj-pcj-pcj-pj-pcj-pcj-pj-pcj-pj-pcj-pcj-pj-pcj-pcj pcj-pj-pcj-pj-pcj-pcj-pcj-pj-pcj-pj-pcj-pj-pcj-pcj-pj-pj-pcj-pj-pcj-pj-pcj-pcj-pcj-pj-pcj-pj-pj-pj-pcj-pj-pcj pcj-pj-pcj-pcj-pj-pcj-pj-pcj-pcj-pcj-pcj"}, {"heading": "E Proof of Lemma 2.11", "text": "We prove that W + i is a witness set for X + i; the proof that W \u2212 i is a witness set for X \u2212 i is analogous. First, we claim that for all x \u2212 X + i there is a witness, so that x \u00b2 x and ai (x \u00b2) x [B, B + m]. To see this, let p be such that x = x (p) and ai (x (p) p) = 0 (since columns with ati > 0 will have ai = i at some point in time). Based on the assumption that the input is in general position, ai (p0) > B (since x (p) x) x) is uncontinuous (as a function of x)."}, {"heading": "F Proof of Lemma 2.12", "text": "Remember the definitions of P v (for v-LK) and P'j (for j-m surfaces), where it is sufficient to prove that at most (O-K-log K) m of the families P's are not empty. Since x-p-Bvi, if and only if for all j-K surfaces x (p) | Cj-B vj i, j is present, it follows that P v = j P vj j. Suppose that B'i, j is the first index in Cj, so that the prefix {t-Cj: t-ig-J surfaces occupy the budget i to an extent in I. Using Lemma 2.10 and the fact that the at's are not negative, we obtain that B'i, j is the set of all prefixes of Cj that contain these surfaces but do not contain surfaces containing p '+ 1 surfaces. Furthermore, note that the set of x-p surfaces of the P, if the prefixes are silver, is only the prefixes containing P."}, {"heading": "G Proof of Lemma 3.1", "text": "LP1 denotes the LP with columns (\u03c0t, a-t) and rights (1 \u2212) B and LP2 denotes the LP with columns (\u03c0t, at) and right B. Let x be a -approximate solution for LP1. Note that we have the upper limit at \u2212 a-t-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta-ta"}, {"heading": "H Proof of Lemma 4.1", "text": "The proof uses the same ideas used in the analysis of OTP, although some definitions need to be slightly changed. (Remember that S = {\u03c3 (1), \u03c3 (2),. \u2212 \u2212 \u2212 We also use pS to name the dual vector used by (s) -OTP for its classification, and set xS = x (pS). With slight misuse in the notation, we often see xS as a (possibly unworkable) solution for (2s, 1) -LP, meaning that we coordinate the vector xS for the first 2s (1)."}, {"heading": "I Proof of Theorem 4.2", "text": "LP1 denotes the LP with the columns (\u03c0t, a-t) and right B (1-21.5) approximation for LP1, and the theorem follows from Lemma 3.1.First, we show that the returned solution x is feasible for LP1. To verify the value of the returned solution, we first show that we get the value of the returned solution for all i, j by linearity, aj (x) = [x) = OP2 [x) = OP2 [x) = OP2]."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "<lb>We consider packing LP\u2019s with m rows where all constraint coefficients are normalized to be in the<lb>unit interval. The n columns arrive in random order and the goal is to set the corresponding decision<lb>variables irrevocably when they arrive so as to obtain a feasible solution maximizing the expected reward.<lb>Previous (1\u2212 )-competitive algorithms require the right-hand side of the LP to be \u03a9(<lb>2 log n ), a bound<lb>that worsens with the number of columns and rows. However, the dependence on the number of columns<lb>is not required in the single-row case and known lower bounds for the general case are also independent<lb>of n.<lb>Our goal is to understand whether the dependence on n is required in the multi-row case, making it<lb>fundamentally harder than the single-row version. We refute this by exhibiting an algorithm which is<lb>(1\u2212 )-competitive as long as the right-hand sides are \u03a9(<lb>2<lb>2 log m ). Our techniques refine previous PAC-<lb>learning based approaches which interpret the online decisions as linear classifications of the columns<lb>based on sampled dual prices. The key ingredient of our improvement comes from a non-standard<lb>covering argument together with the realization that only when the columns of the LP belong to few 1-d<lb>subspaces we can obtain small such covers; bounding the size of the cover constructed also relies on the<lb>geometry of linear classifiers. General packing LP\u2019s are handled by perturbing the input columns, which<lb>can be seen as making the learning problem more robust.", "creator": "LaTeX with hyperref package"}}}