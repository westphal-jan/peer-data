{"id": "1204.5852", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Apr-2012", "title": "Context-sensitive Spelling Correction Using Google Web 1T 5-Gram Information", "abstract": "In computing, spell checking is the process of detecting and sometimes providing spelling suggestions for incorrectly spelled words in a text. Basically, a spell checker is a computer program that uses a dictionary of words to perform spell checking. The bigger the dictionary is, the higher is the error detection rate. The fact that spell checkers are based on regular dictionaries, they suffer from data sparseness problem as they cannot capture large vocabulary of words including proper names, domain-specific terms, technical jargons, special acronyms, and terminologies. As a result, they exhibit low error detection rate and often fail to catch major errors in the text. This paper proposes a new context-sensitive spelling correction method for detecting and correcting non-word and real-word errors in digital text documents. The approach hinges around data statistics from Google Web 1T 5-gram data set which consists of a big volume of n-gram word sequences, extracted from the World Wide Web. Fundamentally, the proposed method comprises an error detector that detects misspellings, a candidate spellings generator based on a character 2-gram model that generates correction suggestions, and an error corrector that performs contextual error correction. Experiments conducted on a set of text documents from different domains and containing misspellings, showed an outstanding spelling error correction rate and a drastic reduction of both non-word and real-word errors. In a further study, the proposed algorithm is to be parallelized so as to lower the computational cost of the error detection and correction processes.", "histories": [["v1", "Thu, 26 Apr 2012 07:44:18 GMT  (319kb)", "http://arxiv.org/abs/1204.5852v1", "LACSC - Lebanese Association for Computational Sciences -this http URL"]], "COMMENTS": "LACSC - Lebanese Association for Computational Sciences -this http URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["youssef bassil", "mohammad alwani"], "accepted": false, "id": "1204.5852"}, "pdf": {"name": "1204.5852.pdf", "metadata": {"source": "CRF", "title": "Context-sensitive Spelling Correction Using Google Web 1T 5-Gram Information", "authors": ["Youssef Bassil", "Mohammad Alwani"], "emails": ["youssef.bassil@lacsc.org"], "sections": [{"heading": null, "text": "Published by the Canadian Center of Science and Education 37Context-sensitive spelling correction using Google Web 1T 5-gram informationYoussef Bassil1 & Mohammad Alwani1"}, {"heading": "1 LACSC - Lebanese Association for Computational Sciences, Beirut, Lebanon Correspondence: Youssef Bassil, LACSC - Lebanese Association for Computational Sciences, Registered under", "text": "No. 957, 2011, Beirut, Lebanon. Email: youssef.bassil @ lacsc.orgReceived: January 9, 2012 Accepted: February 11, 2012 Online Published: May 1, 2012 doi: 10.5539 / cis.v5n3p37 error URL: http: / / dx.doi.org / 10.5539 / cis.v5n3p37 The research is being conducted by the Lebanese Association for Computational Sciences (LACSC), No. 957, 2011, under the title \"Web-Scale Spell-Checking Research Project - WSSCRP2011\" Abstract In computing, spell-checking is the process of recognizing and sometimes providing suggestions for spelling misspelled words in a text. Basically, a spell-checker is a computer program that uses a dictionary of words to perform spell-checking."}, {"heading": "1. Introduction", "text": "In recent years, this trend has changed radically as the IT industry and advances in computer technology have spawned a new type of computing application."}, {"heading": "2. State of the Art", "text": "In practice, spelling errors in written text vary between 1% and 3% (Grudin, 1983; Kukich, 1992), 80% of which are usually caused by trivial editing operations such as inserting, deleting, replacing, and transposing (Damerau, 1964). However, another study (Peterson, 1986) pointed out that 94% of spelling errors are typically caused by such editing operations. In fact, spelling error recognition and correction algorithms can only be divided into several types (Kukich, 1992): Wordless error detection, which consists of recognizing incorrect words that are not words, i.e. words that cannot be found in a dictionary; the isolated word error correction, which consists of correcting non-word errors but looking at them in isolation, regardless of their context; and context-dependent or context-sensitive error detection and correction, which consists of correcting the sentence and correction."}, {"heading": "2.1 Noisy Channel Model", "text": "The bottom line of this approach is that if one could know how the original word was distorted, then it would be easy to find the actual correction (Jurafsky & Martin, 2008). However, the noisy channel model is a special case of the Bayesian inference (Bayes, 1963), which is basically a classification-like model that verifies some observations and classifies them into appropriate classes and categories. Bledsoe and Browning (1959) and Mosteller and Wallace (1964) were the first among other researchers to apply the Bayean inference to the correction of P as a computer-generated text. Mathematically, the Bayesian model is a probability model based on statistical assumptions and probability theory, namely the previous probability P (w) and probability P (O | w), which is normally calculated by the following equation."}, {"heading": "2.2 N-Gram Model", "text": "In principle, the n-gram model is a probabilistic model originally proposed by Markov (1913) and later applied by Shannon (1948), Chomsky (1956), and Chomsky (1957) to predict the next word in a particular word sequence. In short, an n-gram is simply a word sequence that is n words long. For example, \"the cat\" is a 2-gram sequence, also known as bigram, \"the cat eats\" a 4-gram sequence. \"The cat eats food and drink\" is a 7-gram sequence of words that are n words long. Below are examples of 3-gram and 4-gram word sequences extracted from their frequencies: \"Google Web 1T n-gram data (Google Inc., 2006)."}, {"heading": "2.3 Minimum Edit Distance", "text": "The Minimum Edit Distance Algorithm was first conceived by Wagner and Fischer (1974), and it is defined as the minimum number of edits required to turn a string x into a string y. These operations are insertion, deletion, and substitution. In spelling correction, the purpose of the Minimum Edit Distance Algorithm is to reduce the number of candidate letters by eliminating candidates with maximum editing spacing, as they are considered fewer characters with the spelling error than other candidates. There are various editing ISSN 1913-8989 E-ISSN 1913-40distance algorithms: Levenshtein (Levenshtein, 1966), Hamming (Hamming, 1950), and the longest common subsequence (Allison & Dix, 1986) algorithms that are unable to distinguish the string between the strings, The Levenshtein Algorithm employs a weighting mechanism independent of its cost."}, {"heading": "2.4 Context-sensitive Spelling Error Correction", "text": "In fact, it is a real bug, which is usually not able to fix the mentioned bugs."}, {"heading": "3. Prop", "text": "This p-non-where mix o-non-where the google perform counts the p"}, {"heading": "3.1 Go", "text": "The iontS-ionS-ionrpnlrrpSrptpnlreeoipnlrptpnreeoipnlptpnlrpSrteeu-iiiiiipnlrpnrpnlreo-iiiipnlrrpnlrrrpnlrpnlrpnlrpnlrpnrpnlreo-iiiiipnlrrrrrrpnlrrrpnlrrrpnlrrrteeoS-ionv-iiiiipnlrlrpnlrrrrrlrrrrlpnrteepnS-ipnllrrrrrrrrrrrrrnrlS-ionS-ionrpnlrpnlrpnlrlrteepnrepnS-ipnlteepnS-ipnlrrrrrrnlrrrrrrnrrnS"}, {"heading": "3.2 Error Detection Algorithm", "text": "The proposed error detection algorithm detects non-word errors E = {e1, e2, e3, em} in the original text T = {w1, w2, w3, wn}, where e is not a misspelled word or simply an incorrect word, so m is the total number of detected errors, w is a word in the original text and n is the total number of words in the text. The process begins with the validation of each word wi in T against Google's unigram record; if wi is found, wi should be correct and no correction should take place. Otherwise, if the word wi is not found, wi is called wrong, and therefore a correction is required. Google's dataset of unigrams is already sorted alphabetically, so the binary search can be used immaculately to speed up the execution time of error detection."}, {"heading": "3.3 Candidate Spellings Generation Algorithm", "text": "\"The proposed candidates,\" according to the speaker, \"can be split into 2-gram character sequences.\" \"The proposed candidates,\" according to the speaker, \"are a list of possible spell corrections for each recorded non-word error.\" \"These candidate corrections are defined by C = {c11, c12, c13, c1r,..., cm1, cm3, cmq}, where a particular candidate specifies a certain character specification, m denotes the total number of recorded non-word character errors, and r and q denotes the total number of candidates generated for a particular error.\" Essentially, the algorithms are based on a character-based 2-gram model that searches for unigrams in Google, m denotes the total number of captured non-word sequences in 2-gram sequences with the error word. \""}, {"heading": "3.4 The Context-sensitive Error Correction Algorithm", "text": "The proposed contextsensitive spelling error correction algorithm takes each generated candidate cik with four of the words that precede the original error in the original text = 6 case, which results in Sk = \"wi-4wi-3wi-2wi-1 cik,\" where S denotes a 5-gram word sentence, w denotes a word that precedes the original error, c denotes a specific candidate that is spelled for a specific error, i denotes the ith word that precedes the original error, and k denotes the kth candidate spelling. Each constructed sentence Sk is then sketched with Google 5-gram word counts from Google Web 1T 5-gram record. The candidate cik, which belongs to the sentence Sk with the highest number, is selected as a substitute for the originally detected error word, and k denotes the kth candidate spelling. \"Each constructed sentence Sk is then sketched with Google 5-gram word counts from Google Web 1T 5-gram record.\" The candidate cik, which belongs to the sentence Sk with the highest number, \"is sketched as a replacement for the originally detected error word. Back to the previous example, the list of S-sentences can be sketched as follows: S1 =\" case, where only a ball is sketched \"S2 =\" case where only a sketch is sketch, \"S2 =\" case where only a sketch with the highest number, \"Sk is a sketch with the highest number,\" Sk is selected as a replacement for the originally detected error word, \"Sizk with the highest number,\" Sizk is sketch is sketch. \"case\" S1 = 4, \"Sizk is sketch case\" where S4 is sketch is sketch, \"where S4 is a single word is only an angle = 4, where S4 is only an angle is published, S4, where Sizk is an angle is only an angle = 4, where Sizk is an angle, S4 is only an angle, where Sizk = 4 is only an angle, Sizk is only an angle, Sizk is an angle, where S4 is a 4 is only an angle, where Sizk is"}, {"heading": "4. Experiments and Results", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...)"}, {"heading": "5. Conclusions and Future Work", "text": "The proposed algorithm is based on a 5-gram dataset from Google Web 1T, which contains an enormous volume of word sequences originally extracted from web pages. The aim of this new method was to improve the error correction rate of modern spelling corrections, especially context-sensitive error corrections. The proposed method outperformed when tested among other spelling corrections, notably GNU Aspell and the proprietary Ghotit dyslexia. In fact, 99% of the non-word errors and 70% of the real word errors were corrected by the proposed method, while the closest competitor, Ghotit, scored about 70% for non-word errors and 29% for real-word errors. Overall, 93% of the total errors were corrected by the Canadian Center of Science and Education 47 by the proposed method, while Ghotit scored 62%."}], "references": [{"title": "A bit-string longest common-subsequence algorithm", "author": ["L. Allison", "T.I. Dix"], "venue": "Information Processing Letters,", "citeRegEx": "Allison and Dix,? \\Q1986\\E", "shortCiteRegEx": "Allison and Dix", "year": 1986}, {"title": "Another Look at the Data Sparsity Problem", "author": ["B. Allison", "D. Guthrie", "L. Guthrie"], "venue": "Proceedings of the 9th International Conference on Text, Speech and Dialogue, Czech Republic", "citeRegEx": "Allison et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Allison et al\\.", "year": 2006}, {"title": "GNU Aspell Spell Checker", "author": ["K. Atkinson"], "venue": "Retrieved from http://aspell.net/", "citeRegEx": "Atkinson,? 2004", "shortCiteRegEx": "Atkinson", "year": 2004}, {"title": "Scaling to Very Very Large Corpora for Natural Language Disambiguation", "author": ["M. Banko", "E. Brill"], "venue": "In ACL,", "citeRegEx": "Banko and Brill,? \\Q2001\\E", "shortCiteRegEx": "Banko and Brill", "year": 2001}, {"title": "An Essay Toward Solving a Problem in the Doctrine of Chances", "author": ["T. Bayes"], "venue": "reprinted in Facsimiles of two papers by Bayes, Hafner Publishing Company, 53, New York.", "citeRegEx": "Bayes,? 1963", "shortCiteRegEx": "Bayes", "year": 1963}, {"title": "Pattern recognition and reading by machine", "author": ["W.W. Bledsoe", "I. Browning"], "venue": "Proceedings of the Eastern Joint Computer Conference,", "citeRegEx": "Bledsoe and Browning,? \\Q1959\\E", "shortCiteRegEx": "Bledsoe and Browning", "year": 1959}, {"title": "Memory-Based Context-Sensitive Spelling Correction at Web Scale", "author": ["A. Carlson", "I. Fette"], "venue": "Proceedings of the IEEE International Conference on Machine Learning and Applications (ICMLA)", "citeRegEx": "Carlson and Fette,? \\Q2007\\E", "shortCiteRegEx": "Carlson and Fette", "year": 2007}, {"title": "Three models for the description of language", "author": ["N. Chomsky"], "venue": "IRI Transactions on Information Theory, 2(3), 113-124. http://dx.doi.org/10.1109/TIT.1956.1056813", "citeRegEx": "Chomsky,? 1956", "shortCiteRegEx": "Chomsky", "year": 1956}, {"title": "Syntactic Structures", "author": ["N. Chomsky"], "venue": "Mouton, The Hague.", "citeRegEx": "Chomsky,? 1957", "shortCiteRegEx": "Chomsky", "year": 1957}, {"title": "A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams", "author": ["K.W. Church", "W.A. Gale"], "venue": "computer speech and language,", "citeRegEx": "Church and Gale,? \\Q1991\\E", "shortCiteRegEx": "Church and Gale", "year": 1991}, {"title": "Probability scoring for spelling correction. Statistics and Computing", "author": ["K.W. Church", "W.A. Gale"], "venue": null, "citeRegEx": "Church and Gale,? \\Q1991\\E", "shortCiteRegEx": "Church and Gale", "year": 1991}, {"title": "A technique for computer detection and correction of spelling errors", "author": ["F.J. Damerau"], "venue": "Communications of the ACM, 7(3), 171-176. http://dx.doi.org/10.1145/363958.363994", "citeRegEx": "Damerau,? 1964", "shortCiteRegEx": "Damerau", "year": 1964}, {"title": "Large-scale lexical semantics for speech recognition support", "author": ["G. Demetriou", "E. Atwell", "C. Souter"], "venue": "In EUROSPEECH,", "citeRegEx": "Demetriou et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Demetriou et al\\.", "year": 1997}, {"title": "Ghotit Dyslexia Contextual Spell Checker", "author": ["Ghotit ltd."], "venue": "Retrieved from http://www.ghotit.com/home.shtml", "citeRegEx": "ltd.,? 2011", "shortCiteRegEx": "ltd.", "year": 2011}, {"title": "1999).A winnow-based approach to context-sensitive spelling correction", "author": ["A.R. Golding", "D. Roth"], "venue": "Machine Learning,", "citeRegEx": "Golding and Roth,? \\Q1999\\E", "shortCiteRegEx": "Golding and Roth", "year": 1999}, {"title": "Google Web 1T 5-Gram Version 1", "author": ["Google Inc."], "venue": "Retrieved from http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13", "citeRegEx": "Inc.,? 2006", "shortCiteRegEx": "Inc.", "year": 2006}, {"title": "Error patterns in novice and skilled transcription typing", "author": ["J.T. Grudin"], "venue": "Cooper edition, Springer-Verlag, Cognitive Aspects of Skilled Typewriting, pp. 121-139.", "citeRegEx": "Grudin,? 1983", "shortCiteRegEx": "Grudin", "year": 1983}, {"title": "Error detecting and error correcting codes", "author": ["R.W. Hamming"], "venue": "Bell System Technical Journal, 29(2), 147-160.", "citeRegEx": "Hamming,? 1950", "shortCiteRegEx": "Hamming", "year": 1950}, {"title": "A comparison of standard spell checking algorithms and novel binary neural approach", "author": ["V.J. Hodge", "J. Austin"], "venue": "IEEE Trans. Know. Dat. Eng.,", "citeRegEx": "Hodge and Austin,? \\Q2003\\E", "shortCiteRegEx": "Hodge and Austin", "year": 2003}, {"title": "Real-Word Spelling Correction using Google Web 1T n-gram Data Set", "author": ["A. Islam", "D. Inkpen"], "venue": "Proceedings of the 18th ACM Conference on Information and Knowledge Management,", "citeRegEx": "Islam and Inkpen,? \\Q2009\\E", "shortCiteRegEx": "Islam and Inkpen", "year": 2009}, {"title": "Theory of Probability (2nd ed.)", "author": ["H. Jeffreys"], "venue": null, "citeRegEx": "Jeffreys,? \\Q1948\\E", "shortCiteRegEx": "Jeffreys", "year": 1948}, {"title": "Speech and Language Processing (2nd ed.)", "author": ["D. Jurafsky", "J. Martin"], "venue": null, "citeRegEx": "Jurafsky and Martin,? \\Q2008\\E", "shortCiteRegEx": "Jurafsky and Martin", "year": 2008}, {"title": "A spelling correction program base on a noisy channel model", "author": ["M.D. Kernighan", "K.W. Church", "W.A. Gale"], "venue": "In COLING-90,", "citeRegEx": "Kernighan et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Kernighan et al\\.", "year": 1990}, {"title": "Introduction to the Special Issue on the Web as a Corpus. journal of computational linguistics", "author": ["A. Kilgariff", "G. Grefenstette"], "venue": null, "citeRegEx": "Kilgariff and Grefenstette,? \\Q2003\\E", "shortCiteRegEx": "Kilgariff and Grefenstette", "year": 2003}, {"title": "A cache-based natural language model for speech recognition", "author": ["R. Kuhn", "M.R. De"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Kuhn and De,? \\Q1990\\E", "shortCiteRegEx": "Kuhn and De", "year": 1990}, {"title": "Techniques for automatically correcting words in text", "author": ["K. Kukich"], "venue": "ACM Computing Surveys, 24(4), 377-439. http://dx.doi.org/10.1145/146370.146380", "citeRegEx": "Kukich,? 1992", "shortCiteRegEx": "Kukich", "year": 1992}, {"title": "Binary codes capable of correcting deletions, insertions, and reversals", "author": ["V.I. Levenshtein"], "venue": "Cybernetics and Control Theory, 10(8), 707-710.", "citeRegEx": "Levenshtein,? 1966", "shortCiteRegEx": "Levenshtein", "year": 1966}, {"title": "Web text corpus for natural language processing", "author": ["V. Liu", "J.R. Curran"], "venue": "In EACL,", "citeRegEx": "Liu and Curran,? \\Q2006\\E", "shortCiteRegEx": "Liu and Curran", "year": 2006}, {"title": "An Introduction to Information Retrieval", "author": ["M. Raghavan"], "venue": null, "citeRegEx": "Raghavan,? \\Q2008\\E", "shortCiteRegEx": "Raghavan", "year": 2008}, {"title": "Essaid\u2019unerecherchestatistiquesur le texte du roman \u201cEug\u00e8neOneguine\u201d, Bull", "author": ["A.A. Markov"], "venue": "Acad. Imper. Sci., St. Petersburg. http://dx.doi.org/10.1017/S0269889706001074", "citeRegEx": "Markov,? 1913", "shortCiteRegEx": "Markov", "year": 1913}, {"title": "Context based spelling correction", "author": ["E. Mays", "F.J. Damerau", "R.L. Mercer"], "venue": "Information Processing and Management,", "citeRegEx": "Mays et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Mays et al\\.", "year": 1991}, {"title": "Inference and disputed authorship: The Federalist", "author": ["F. Mosteller", "D.L. Wallace"], "venue": null, "citeRegEx": "Mosteller and Wallace,? \\Q1964\\E", "shortCiteRegEx": "Mosteller and Wallace", "year": 1964}, {"title": "A variable-length Category-based n-gram language model", "author": ["T.R. Niesler", "P.C. Woodland"], "venue": "In IEEE ICASSP-96, Atlanta, GA,", "citeRegEx": "Niesler and Woodland,? \\Q1996\\E", "shortCiteRegEx": "Niesler and Woodland", "year": 1996}, {"title": "A note on undetected typing errors", "author": ["J.L. Peterson"], "venue": "Communications of the ACM, 29(7), 633-637. http://dx.doi.org/10.1145/6138.6146", "citeRegEx": "Peterson,? 1986", "shortCiteRegEx": "Peterson", "year": 1986}, {"title": "A mathematical theory of communication", "author": ["C.E. Shannon"], "venue": "Bell system Technical Journal, 27(3), 379-423.", "citeRegEx": "Shannon,? 1948", "shortCiteRegEx": "Shannon", "year": 1948}, {"title": "The string-to-string correction problem", "author": ["R.A. Wagner", "M.J. Fischer"], "venue": "Journal of the Association for Computing Machinery,", "citeRegEx": "Wagner and Fischer,? \\Q1974\\E", "shortCiteRegEx": "Wagner and Fischer", "year": 1974}], "referenceMentions": [{"referenceID": 16, "context": "Practically, spelling errors in type written text vary between 1% and 3% (Grudin, 1983; Kukich, 1992) where 80% of them are usually caused by trivial editing operations such as insertion, deletion, substitution, and transposition (Damerau, 1964).", "startOffset": 73, "endOffset": 101}, {"referenceID": 25, "context": "Practically, spelling errors in type written text vary between 1% and 3% (Grudin, 1983; Kukich, 1992) where 80% of them are usually caused by trivial editing operations such as insertion, deletion, substitution, and transposition (Damerau, 1964).", "startOffset": 73, "endOffset": 101}, {"referenceID": 11, "context": "Practically, spelling errors in type written text vary between 1% and 3% (Grudin, 1983; Kukich, 1992) where 80% of them are usually caused by trivial editing operations such as insertion, deletion, substitution, and transposition (Damerau, 1964).", "startOffset": 230, "endOffset": 245}, {"referenceID": 33, "context": "Nonetheless, a different study (Peterson, 1986) pointed out that 94% of spelling errors are typically caused by such editing operations.", "startOffset": 31, "endOffset": 47}, {"referenceID": 25, "context": "In fact, spelling error detection and correction algorithms can be merely broken down into several types (Kukich, 1992): The non-word error detection which consists of detecting error words that are non-words, that is, words that cannot be found in a dictionary; The isolated-word error correction which consists of correcting non-word errors but looking at them in isolation, independently of their context; And the context-dependent or context-sensitive error detection and correction which consists of detecting and correcting errors according to their context in the sentence.", "startOffset": 105, "endOffset": 119}, {"referenceID": 4, "context": "The noisy channel model is a special case of Bayesian inference (Bayes, 1963) which is principally a classification-type model that inspects some observations and ranks them into appropriate classesand categories.", "startOffset": 64, "endOffset": 77}, {"referenceID": 4, "context": "The noisy channel model is a special case of Bayesian inference (Bayes, 1963) which is principally a classification-type model that inspects some observations and ranks them into appropriate classesand categories. Bledsoe and Browning (1959), and Mosteller and Wallace (1964) were the first among other researches to apply the Bayesian inference to detect misspellings in computer generated text.", "startOffset": 45, "endOffset": 242}, {"referenceID": 4, "context": "The noisy channel model is a special case of Bayesian inference (Bayes, 1963) which is principally a classification-type model that inspects some observations and ranks them into appropriate classesand categories. Bledsoe and Browning (1959), and Mosteller and Wallace (1964) were the first among other researches to apply the Bayesian inference to detect misspellings in computer generated text.", "startOffset": 45, "endOffset": 276}, {"referenceID": 6, "context": "Published by Canadian Center of Science and Education 39 Experiments conducted by Kernighan, Church and Gale (1990) proved that the Bayesian model is not perfect as it can fail to correct spelling errors in some cases, for instance, falsely correcting the spelling error \u201cacress\u201d as \u201cacres\u201d, while the original word is \u201cactress\u201d.", "startOffset": 93, "endOffset": 116}, {"referenceID": 4, "context": "Published by Canadian Center of Science and Education 39 Experiments conducted by Kernighan, Church and Gale (1990) proved that the Bayesian model is not perfect as it can fail to correct spelling errors in some cases, for instance, falsely correcting the spelling error \u201cacress\u201d as \u201cacres\u201d, while the original word is \u201cactress\u201d. 2.2 N-Gram Model The n-gram model has been so far applied in many linguistics problems such as spelling correction, speech recognition, and word sequence prediction. Principally, the n-gram is a probabilistic model originally proposed by Markov (1913) and later applied by Shannon (1948), Chomsky (1956), and Chomsky (1957) to predict the next word in a particular sequence of words.", "startOffset": 132, "endOffset": 582}, {"referenceID": 4, "context": "Published by Canadian Center of Science and Education 39 Experiments conducted by Kernighan, Church and Gale (1990) proved that the Bayesian model is not perfect as it can fail to correct spelling errors in some cases, for instance, falsely correcting the spelling error \u201cacress\u201d as \u201cacres\u201d, while the original word is \u201cactress\u201d. 2.2 N-Gram Model The n-gram model has been so far applied in many linguistics problems such as spelling correction, speech recognition, and word sequence prediction. Principally, the n-gram is a probabilistic model originally proposed by Markov (1913) and later applied by Shannon (1948), Chomsky (1956), and Chomsky (1957) to predict the next word in a particular sequence of words.", "startOffset": 132, "endOffset": 618}, {"referenceID": 4, "context": "Published by Canadian Center of Science and Education 39 Experiments conducted by Kernighan, Church and Gale (1990) proved that the Bayesian model is not perfect as it can fail to correct spelling errors in some cases, for instance, falsely correcting the spelling error \u201cacress\u201d as \u201cacres\u201d, while the original word is \u201cactress\u201d. 2.2 N-Gram Model The n-gram model has been so far applied in many linguistics problems such as spelling correction, speech recognition, and word sequence prediction. Principally, the n-gram is a probabilistic model originally proposed by Markov (1913) and later applied by Shannon (1948), Chomsky (1956), and Chomsky (1957) to predict the next word in a particular sequence of words.", "startOffset": 132, "endOffset": 634}, {"referenceID": 4, "context": "Published by Canadian Center of Science and Education 39 Experiments conducted by Kernighan, Church and Gale (1990) proved that the Bayesian model is not perfect as it can fail to correct spelling errors in some cases, for instance, falsely correcting the spelling error \u201cacress\u201d as \u201cacres\u201d, while the original word is \u201cactress\u201d. 2.2 N-Gram Model The n-gram model has been so far applied in many linguistics problems such as spelling correction, speech recognition, and word sequence prediction. Principally, the n-gram is a probabilistic model originally proposed by Markov (1913) and later applied by Shannon (1948), Chomsky (1956), and Chomsky (1957) to predict the next word in a particular sequence of words.", "startOffset": 132, "endOffset": 654}, {"referenceID": 20, "context": "Several broader studies were investigated to improve the n-gram model from different perspectives; this may include but not limited to smoothing techniques suggested to solve the problem of zero-frequency of n-grams that never occurred in a corpus (Jeffreys, 1948; Church & Gale, 1991), the weighted n-gram model that more accurately estimates the n-grams based on their location in the context (Kuhn & Mori, 1990), and the variable length n-gram model (Niesler & Woodland, 1996) which varies the length of grams to attaint better overall system performance and compactness.", "startOffset": 248, "endOffset": 285}, {"referenceID": 35, "context": "The Minimum Edit Distance algorithm was first conceived by Wagner and Fischer (1974), and it is defined as the minimum number of edit operations needed to transform a string x into a string y.", "startOffset": 59, "endOffset": 85}, {"referenceID": 26, "context": "ISSN 1913-8989 E-ISSN 1913-8997 40 distance algorithms: Levenshtein (Levenshtein, 1966), Hamming (Hamming, 1950) and the Longest Common Subsequence (Allison & Dix, 1986) algorithms.", "startOffset": 68, "endOffset": 87}, {"referenceID": 17, "context": "ISSN 1913-8989 E-ISSN 1913-8997 40 distance algorithms: Levenshtein (Levenshtein, 1966), Hamming (Hamming, 1950) and the Longest Common Subsequence (Allison & Dix, 1986) algorithms.", "startOffset": 97, "endOffset": 112}, {"referenceID": 9, "context": "Mays, Damerau and Mercer (1991) proposed using the n-gram model to predict the actual correction for a real-word error.", "startOffset": 6, "endOffset": 32}, {"referenceID": 9, "context": "Church and Gale (1991) suggested the use of a noisy channel to predict the actual correction for a real-word error.", "startOffset": 0, "endOffset": 23}, {"referenceID": 9, "context": "Church and Gale (1991) suggested the use of a noisy channel to predict the actual correction for a real-word error. The technique harnesses a 100 million words corpus and n-gram statistics to correct errors according to their contextual information. Liu and Curran (2006) employed n-gram statistics to correct real-word errors using a big corpus of text collected from crawling the web.", "startOffset": 0, "endOffset": 272}, {"referenceID": 2, "context": "For comparison purposes, the GNU Aspell (Atkinson, 2004) and Ghotit Dyslexia (Ghotit ltd.", "startOffset": 40, "endOffset": 56}], "year": 2012, "abstractText": "In computing, spell checking is the process of detecting and sometimes providing spelling suggestions for incorrectly spelled words in a text. Basically, a spell checker is a computer program that uses a dictionary of words to perform spell checking. The bigger the dictionary is, the higher is the error detection rate. The fact that spell checkers are based on regular dictionaries, they suffer from data sparseness problem as they cannot capture large vocabulary of words including proper names, domain-specific terms, technical jargons, special acronyms, and terminologies. As a result, they exhibit low error detection rate and often fail to catch major errors in the text. This paper proposes a new context-sensitive spelling correction method for detecting and correcting non-word and real-word errors in digital text documents. The approach hinges around data statistics from Google Web 1T 5-gram data set which consists of a big volume of n-gram word sequences, extracted from the World Wide Web. Fundamentally, the proposed method comprises an error detector that detects misspellings, a candidate spellings generator based on a character 2-gram model that generates correction suggestions, and an error corrector that performs contextual error correction. Experiments conducted on a set of text documents from different domains and containing misspellings, showed an outstanding spelling error correction rate and a drastic reduction of both non-word and real-word errors. In a further study, the proposed algorithm is to be parallelized so as to lower the computational cost of the error detection and correction processes.", "creator": "PScript5.dll Version 5.2.2"}}}