{"id": "1205.0406", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2012", "title": "Minimax Classifier for Uncertain Costs", "abstract": "Many studies on the cost-sensitive learning assumed that a unique cost matrix is known for a problem. However, this assumption may not hold for many real-world problems. For example, a classifier might need to be applied in several circumstances, each of which associates with a different cost matrix. Or, different human experts have different opinions about the costs for a given problem. Motivated by these facts, this study aims to seek the minimax classifier over multiple cost matrices. In summary, we theoretically proved that, no matter how many cost matrices are involved, the minimax problem can be tackled by solving a number of standard cost-sensitive problems and sub-problems that involve only two cost matrices. As a result, a general framework for achieving minimax classifier over multiple cost matrices is suggested and justified by preliminary empirical studies.", "histories": [["v1", "Wed, 2 May 2012 12:38:11 GMT  (1506kb,D)", "http://arxiv.org/abs/1205.0406v1", "6 pages, more materials will be added into the manuscript"]], "COMMENTS": "6 pages, more materials will be added into the manuscript", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["rui wang", "ke tang"], "accepted": false, "id": "1205.0406"}, "pdf": {"name": "1205.0406.pdf", "metadata": {"source": "CRF", "title": "Minimax Classifier for Uncertain Costs", "authors": ["Rui Wang", "Ke Tang"], "emails": ["wrui1108@mail.ustc.edu.cn,", "tang@ustc.edu.cn."], "sections": [{"heading": null, "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "II. PRELIMINARIES AND RELATED WORKS", "text": "In this section, we first present the basic notations and backgrounds, and then discuss two papers closely related to this study: one is the work of Liu and Zhou [14], which also deals with the uncertain cost problem, but without Xiv: 120 5.04 06v1 [cs.LG] 2 May 201 22 different formulations and learning objectives, and the other focuses on finding the Minimax classifier for unsafe classes [15]."}, {"heading": "A. Preliminaries", "text": "In view of a dataset S = {(x1, y1),.., (xn, yn), xi = {x1i,.., xmi}, the attribute vector of the instance is (xi, yi), and yi, {0, 1} is the class name. Suppose that there is no cost with correct classification, a cost matrix C can be represented by two values c0 and c1, which denote the cost of misclassification of an instance from class 0 and / or class 1, respectively. Moreover, we use p0 and p1 = 1 \u2212 p0 to represent the class priorities, so that in each class there are n0 = np1 and n1 = np1 instances. For each classifier h from the hypotheses space H, its total cost is L = np0p10c0 + np1p01c1 = n0p10c0 + n1p01c1, (1), with p01 being the probability that the class (1) is (p1)."}, {"heading": "B. Learning with Cost Intervals", "text": "In a recent paper, Liu and Zhou [14] considered a specific form of uncertain cost problem where c0 is 1 and c1 is uncertain, but within a predetermined interval [cmin, cmax]. Their goal is to construct a classifier that performs well for all individual costs within [cmin, cmax].Technically, the problem has been transformed to find the best replacement costs to train with, i.e., their learning goal is min h, H L (h, S, cmax) s.t. p (L (h, S, c) <) > 1 \u2212 \u03b4 [cmin, cmax] cmin \u2264 cs (2) An SVM-based algorithm has been proposed that primarily minimizes the largest total costs (i.e. L (h, S, cmax) <) > 1 \u2212 \u03b4 [cmin, cmax] cmin \u2264 cmax (2).Although it primarily minimizes the largest total costs (h, cm, cmin, total cm, max, and \u00b5 (2), max, and \u00b5 (2), the second costs."}, {"heading": "C. Minimax Classifier for Uncertain Class Priors", "text": "In the many studies concerning the Minimax criterion [16], those who focused their attention on creating a Minimax total cost classifier for an unsafe class before [15] are of particular interest for this study. Formally, the Minimax classification problem is to find the following classifier, hp = argmin h, H max P L (h, P, C) (3) It is well known that the total cost of a fixed classifier is a linear function of the previous one, while the optimal total cost (i.e. Bayean cost) is a concave function of the previous one [17]. Therefore, suppose that the best classifier for a particular class before P, then the total cost function of h, w, w.r.t. would previously be a confused line of the Bayean total cost curve to P, with two algorithms proposed to find the Minimax classifier based on these elegant properties."}, {"heading": "III. MINIMAX CLASSIFIER FOR UNCERTAIN COST", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Problem Formulation", "text": "As mentioned above, this study focuses on minimizing the largest total cost over a limited set of possible cost matrices. Formally, given a set of cost matrices U = {C1,... Ck} where Ci = {ci0, ci1} is the i-th cost matrix, the learning goal is to find hU = argmin h-H max C-U L (h, S, C). (4) Since the uncertain cost is formulated directly as a set, the problem is widely applicable in practice, ready for future studies on multi-class problems and facilitates theoretical analysis. On the other hand, the best classifier selected according to the Minimax criterion is much more reliable."}, {"heading": "B. Problem Analysis", "text": "Similarly, the other two classifiers are HB and BG, if they are both classified in the category \"classification,\" also in the category \"classification,\" and in the category \"classification,\" i.e. in the category \"classification,\" \"classification,\" \"classification,\" \"classification,\" \"classification,\" \"classification,\" \"classification,\" \"classification,\" \"classification,\" \"classification,\" \"classification,\" \"classification,\" \"classification,\" etc., \"\" there is a cost matrix, \"\" \",\" \"\" all others in the category., \"\" \"there is a cost matrix Cd,\" \"all others in the category\" that is dominated by other costs, \"\" \"\" \"there,\" \"\" it, \"\" it, \"\", \"\" it, \"\", \",\" \"it,\" \",\" \",\", \"\", \"\", \"\", \",\" \",\" \",\", \"\" \"\", \",\" \",\" \"\" \"\", \",\" \"\" \"\" \",\", \"\" \"\" \",\", \"\" \"\", \"\" \"\" \",\" \"\", \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\", \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\", \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "IV. EXPERIMENTS", "text": "In the experiments, we compared three frameworks to solve the Minimax problem. The first is to create the minimum total cost classifier for each possible cost matrix, without first considering a compromise between cost matrices, and then select the Minimax classifier, the second is our framework described above, and the third is to build the Minimax classifier directly, considering all possible cost matrices at the same time. To cut a long story short, we refer to these three frameworks as S, SP, and M respectively. 5A. ImplementationAlthough there are many standard cost-sensitive learning methods, the effort to minimize the total cost of a cost matrix can be used to implement S and one part of SP, to the best of our knowledge there is no specific method that can be used to implement M or the other part of SP (i.e. Minimax the total cost of two or more cost matrices)."}, {"heading": "B. Experimental Setup", "text": "Ten sets of data from the UCI Machine Learning Repository [19] were used in the experiments. Brief information about these data sets is summarized in Table I. Most of these ten classification problems are originally real-world cost-sensitive problems, e.g. the Australian, crx and German problems are fraud problems, while the heart, fungal and wdbc problems are related to human health. In these problems, the cost matrix for misclassification is usually difficult, if not impossible, to specify by practitioners, so the experiments on them are appropriate. For each of the data sets, we compared the 3 frameworks to 4 sets of cost matrices of different cardinalities, which are sets of 3 cost matrices, 5 cost matrices, 10 cost matrices and 20 cost matrices. The value of each element of the matrices is randomly generated within [0, 10]. Furthermore, it is ensured in advance that there is no dominated cost matrix."}, {"heading": "C. Results", "text": "Table II and Table III present the comparisons over each data set on training and exam respectively S. In addition, the value in each cell is the average total cost over 20 times 5-fold, and the best performance for each (data set, cost set) configuration is in bold. In addition, the results of Wilcoxon's significant ranking test referred to as superscripts on the values of the S and M methods, a superscript of 1 shows the performance of SP is significantly better than that of the corresponding method, \u2212 1 for significantly worse, and no superscript means that there is no statistically significant difference between SP and the corresponding method. In summary, we can see that SP outperforms the other two methods in almost all cases and keeps statistically comparable for the rest of the cases. There is no case that SP is statistically worse (i.e., there is no \u2212 1 on the superscripts).Of course, it is not surprising that the SP defeated S completely in the experiments."}, {"heading": "V. CONCLUSIONS AND DISCUSSIONS", "text": "For many real-world cost-sensitive learning problems, the costs associated with misclassification are uncertain. Many existing cost-sensitive learning algorithms that require accurate cost information (e.g. a unique cost matrix) are not applicable to these problems. In this paper, we consider the situation in which cost information is provided as a set of cost matrices and seek to reach the Minimax classifier via cost matrices. Theoretically, it is proven that the Minimax total cost classifier is either the optimal classifier for a single cost matrix in the set, or the Minimax classifier via a pair of cost matrices in the set. This result immediately provides a framework for achieving the Minimax classifier via an arbitrary number of cost matrices. In addition, it is also applicable in the case that cost information is provided as an infinite set, e.g. by combining it with an appropriate sample of this discrepancy process, the cost information becomes the basis for the infinite set."}], "references": [{"title": "Statistical fraud detection: A review", "author": ["R.J. Bolton", "D.J. Hand"], "venue": "Statistical Science, vol. 17, no. 3, pp. 235\u2013255, 2002.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2002}, {"title": "The Foundations of Cost-Sensitive Learning", "author": ["C. Elkan"], "venue": "Proceedings of 17th International Joint Conference on Artificial Intelligence, 2001, pp. 973\u2013978.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "An iterative method for multiclass cost-sensitive learning", "author": ["N. Abe", "B. Zadrozny", "J. Langford"], "venue": "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2004, pp. 3\u201311.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "On Multi-Class Cost-Sensitive Learning", "author": ["Z.H. Zhou", "X.Y. Liu"], "venue": "Proceedings of the 21st National Conference on Artificial Intelligence, 2006, pp. 567\u2013572.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Maximum likelihood in cost-sensitive learning: Model specification, approximations, and upper bounds", "author": ["J.P. Dmochowski", "P. Sajda", "L.C. Parra"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 3313\u2013 3332, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Metacost: A general method for making classifiers costsensitive", "author": ["P. Domingos"], "venue": "Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining, 1999, pp. 155\u2013 164.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "An instance-weighting method to induce cost-sensitive trees", "author": ["K.M. Ting"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 14, no. 3, pp. 659\u2013665, 2002.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "Training cost-sensitive neural networks with methods addressing the class imbalance problem", "author": ["Z.H. Zhou", "X.Y. Liu"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 18, no. 1, pp. 63\u201377, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Cost-sensitive boosting", "author": ["H. Masnadi-shirazi", "N. Vasconcelos", "S. Member"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 2, pp. 294\u2013309, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, 2002, pp. 694\u2013699.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "On reoptimizing multi-class classifiers", "author": ["C. Bourke", "K. Deng", "S.D. Scott", "R.E. Schapire", "N.V. Vinodchandran"], "venue": "Machine Learning, vol. 71, no. 2-3, pp. 219\u2013242, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Robust classification for imprecise environments", "author": ["F. Provost", "T. Fawcett"], "venue": "Machine Learning, vol. 42, pp. 203\u2013231, 2001.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning and making decisions when costs and probabilities are both unknown", "author": ["B. Zadrozny", "C. Elkan"], "venue": "Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, 2001, pp. 204\u2013213.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning with cost intervals", "author": ["X.Y. Liu", "Z.H. Zhou"], "venue": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2010, pp. 403\u2013412.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Minimax classifiers based on neural networks", "author": ["R. Alaiz Rodriguez", "A. Guerrero Curieses", "J. Cid Sueiro"], "venue": "Pattern Recognition, vol. 38, no. 1, pp. 29\u201339, 2005.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Pattern Classification, 2nd ed", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "New York: John Wiley,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "d\u2019Economie Politique", "author": ["V. Pareto", "Cour"], "venue": "Geneve: Librarie Droz,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1964}, {"title": "UCI machine learning repository", "author": ["A. Asuncion", "D. Newman"], "venue": "http://www.ics.uci.edu/\u223cmlearn/MLRepository.html, 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "For example, in fraud detection problem, predicting a normal client as fraud will cut the profit, while predicting a fraud client as normal would usually lead to great loss [1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 1, "context": "This kind of problem is referred to as cost sensitive-learning problem [2], and has attracted many interests in recent years due to its wide applications in the real world [3], [4], [5].", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "This kind of problem is referred to as cost sensitive-learning problem [2], and has attracted many interests in recent years due to its wide applications in the real world [3], [4], [5].", "startOffset": 172, "endOffset": 175}, {"referenceID": 3, "context": "This kind of problem is referred to as cost sensitive-learning problem [2], and has attracted many interests in recent years due to its wide applications in the real world [3], [4], [5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 4, "context": "This kind of problem is referred to as cost sensitive-learning problem [2], and has attracted many interests in recent years due to its wide applications in the real world [3], [4], [5].", "startOffset": 182, "endOffset": 185}, {"referenceID": 5, "context": "This can be done by modifying the training data according to the cost matrix [6], [7], or by extending learning algorithms directly [8], [9].", "startOffset": 77, "endOffset": 80}, {"referenceID": 6, "context": "This can be done by modifying the training data according to the cost matrix [6], [7], or by extending learning algorithms directly [8], [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 7, "context": "This can be done by modifying the training data according to the cost matrix [6], [7], or by extending learning algorithms directly [8], [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 8, "context": "This can be done by modifying the training data according to the cost matrix [6], [7], or by extending learning algorithms directly [8], [9].", "startOffset": 137, "endOffset": 140}, {"referenceID": 9, "context": "This category of methods, including calibration methods [10], threshold moving [5] and its variants [11], typically post process the output of a classifier to optimize its performance with respect to a objective (e.", "startOffset": 56, "endOffset": 60}, {"referenceID": 4, "context": "This category of methods, including calibration methods [10], threshold moving [5] and its variants [11], typically post process the output of a classifier to optimize its performance with respect to a objective (e.", "startOffset": 79, "endOffset": 82}, {"referenceID": 10, "context": "This category of methods, including calibration methods [10], threshold moving [5] and its variants [11], typically post process the output of a classifier to optimize its performance with respect to a objective (e.", "startOffset": 100, "endOffset": 104}, {"referenceID": 11, "context": "In the context of ROC analysis [12], it is claimed that a classifier can be built without any cost information, while still performs well in the scenarios where the cost matrix changes.", "startOffset": 31, "endOffset": 35}, {"referenceID": 12, "context": "Zadrozny and Elkan [13] considered the scenario where example-based misclassification costs are static but unknown.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "More recently, Liu and Zhou [14] investigates the problem of learning with cost intervals.", "startOffset": 28, "endOffset": 32}, {"referenceID": 13, "context": "One is the work from Liu and Zhou [14], which also deals with the uncertain cost problem, but with ar X iv :1 20 5.", "startOffset": 34, "endOffset": 38}, {"referenceID": 14, "context": "different formulation and learning target, and the other focus on finding the minimax classifier for uncertain class prior [15].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "In a recent work, Liu and Zhou [14] considered a special form of the uncertain cost problem where c0 is 1, and c1 is uncertain but within a predefined interval [cmin, cmax].", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "Although this re-scaling process does no harm to traditional cost-sensitive learning as well as the study in [14], it makes the comparison of total costs across different cost matrices meaningless.", "startOffset": 109, "endOffset": 113}, {"referenceID": 13, "context": "Considering that it is generally hard or even impossible to find a classifier that performances well on all costs over the interval (as suggested by [14] itself), the best classifier they built may lead to very big total cost on original cost matrices for real-world problems.", "startOffset": 149, "endOffset": 153}, {"referenceID": 14, "context": "In the many studies involving the minimax criterion [16], those focused their attention on building minimax total cost classifier for uncertain class prior [15] are of particular interest to this study.", "startOffset": 156, "endOffset": 160}, {"referenceID": 15, "context": ", the Bayesian cost) is a concave function of prior [17].", "startOffset": 52, "endOffset": 56}, {"referenceID": 14, "context": "Based on these elegant properties, Alaiz-Rodriguez et al proposed two algorithms based on neural networks model to find the minimax classifier iteratively in [15].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "Following the concept in economics [18], the front formed by all non-dominated classifiers in H is named as the Pareto front (see Fig.", "startOffset": 35, "endOffset": 39}, {"referenceID": 11, "context": "Please refer to [12], particularly Theorem 7 there, for further details.", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "Ten datasets from the UCI machine learning repository [19] were used in the experiments.", "startOffset": 54, "endOffset": 58}], "year": 2012, "abstractText": "Many studies on the cost-sensitive learning assumed that a unique cost matrix is known for a problem. However, this assumption may not hold for many real-world problems. For example, a classifier might need to be applied in several circumstances, each of which associates with a different cost matrix. Or, different human experts have different opinions about the costs for a given problem. Motivated by these facts, this study aims to seek the minimax classifier over multiple cost matrices. In summary, we theoretically proved that, no matter how many cost matrices are involved, the minimax problem can be tackled by solving a number of standard cost-sensitive problems and sub-problems that involve only two cost matrices. As a result, a general framework for achieving minimax classifier over multiple cost matrices is suggested and justified by preliminary empirical studies.", "creator": "LaTeX with hyperref package"}}}