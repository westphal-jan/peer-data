{"id": "1205.0610", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2012", "title": "Greedy Multiple Instance Learning via Codebook Learning and Nearest Neighbor Voting", "abstract": "Multiple instance learning (MIL) has attracted great attention recently in machine learning community. However, most MIL algorithms are very slow and cannot be applied to large datasets. In this paper, we propose a greedy strategy to speed up the multiple instance learning process. Our contribution is two fold. First, we propose a density ratio model, and show that maximizing a density ratio function is the low bound of the DD model under certain conditions. Secondly, we make use of a histogram ratio between positive bags and negative bags to represent the density ratio function and find codebooks separately for positive bags and negative bags by a greedy strategy. For testing, we make use of a nearest neighbor strategy to classify new bags. We test our method on both small benchmark datasets and the large TRECVID MED11 dataset. The experimental results show that our method yields comparable accuracy to the current state of the art, while being up to at least one order of magnitude faster.", "histories": [["v1", "Thu, 3 May 2012 04:09:19 GMT  (384kb,D)", "http://arxiv.org/abs/1205.0610v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gang chen", "jason corso"], "accepted": false, "id": "1205.0610"}, "pdf": {"name": "1205.0610.pdf", "metadata": {"source": "CRF", "title": "Greedy Multiple Instance Learning via Codebook Learning and Nearest Neighbor Voting", "authors": ["Gang Chen", "Jason Corso"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, the demand for input / label pairs in the training data is surprisingly prohibitive, especially in terms of the number of instances that are included in the concept, and the bag is only negative if all instances are included in the concept, and the bag is negative if all instances in it are negative. MIL's goal is to rein in the pockets, but a bag is positive even if it falls into the concept, and the waist is negative if all instances in it are negative."}, {"heading": "2 Related work", "text": "One of the earliest multi-instance learning algorithms was developed by Dietterich et al. [1] for predicting drug activity. Their algorithm, the axial parallel rectangle (APR) method, expands or shrinks a hyperrectangle in the instance feature space with the aim of finding the smallest box that covers at least one instance out of each positive pocket and no instances out of each negative pocket. Following this groundbreaking work, there was a significant amount of research devoted to MIL problems with various learning models, such as DD [2], EM-DD [10], and advanced citation kNN [9].Due to the success of the SVM algorithm [5], and the various positive theoretical results behind it, are extremely popular in machine learning. In addition, in order to improve classification, many variations of SVM were affected by the modification of constraints, the objective functions, spatial projection, cores, etc."}, {"heading": "3 Greedy multiple instance learning", "text": "In this section, we present a greedy strategy for multiple learning. We maximize the density (histogram) ratio between positive and negative pockets to find code books for both targets and negative candidate centers that can be used to classify novel pockets. Maron and Lozano-Perez's notation for pockets and instances used in this essay [2]."}, {"heading": "3.1 Density ratio function", "text": "In fact, it is a matter of a way in which one sees oneself in a position that is capable, that is in a position, that is in a position, that is in a position, that is in a position and is in a position, that is in a position and is in a position, that is in a position and is in a position, that is in a position and is in a position, that is in a position and is in a position, that is in a position and is in a position, that is in a position and is in a position, that is in a position and is in a position, that is in a position and is in a position, that is in a position and is in a position, that is in a position and is in a position, that is in a position and is in a position, that is in a position and is in a position to be able and be in a position."}, {"heading": "3.2 Target codebook discovery", "text": "It is possible that the denominator in Equation. (5) could be equal to 0. We introduce a small positive constant to avoid such a situation. Furthermore, we would like larger bins (k) + in positive bins to have higher priority than target centers; we introduce the sigmoid function as weights and rephrase Equation (5) as follows: arg max kh (k) + h (k) \u2212 + + \u00d7 \u03c3 (bins (k) + \u2212 n / K n / K n / K), k = 1,..., K (6), where \u03c3 (x) = 11 + e \u2212 x, and n is the total number of positive bins B +. Remember that n / K is only the average number of positive bins in each bins, if there is only one positive instance in each positive bins. Roughly speaking, if x is the intersection of n bins, it should be summarized in a bins, and the number of positive bins we want to have should be greater than the number of positive bins, thus increasing the number of positive bins."}, {"heading": "3.3 Nearest neighbor for classification", "text": "Suppose we have learned data centers: C + with p-positive centers and C \u2212 with q-negative centers, we use the nearest neighbor to classify new bags. For each new bag, we calculate its distance from p-positive centers and q-negative centers, and find the minimum Hausdorff distance [28] as the distance between the bag and the learned code books. Let's consider two situations below. (1) Let's assume that each positive bag shares only one goal, namely p = 1 and q = 0. In this situation, we set a threshold to measure the \"proximity\" for a new, unlabeled bag B. In this paper, we define the mean distance from the pockets to the target. Such a threshold strategy is similar to the probability threshold in the DD model. (2) Let's take care that all bags that overlap at a single point are not necessary. We can assume more complicated concepts, for example p-1 and q-1 pockets go out {x = 1, for a new bag."}, {"heading": "3.4 Relationship with Diverse Density model", "text": "For a target-hypothesis problem, we derive that the density-ratio model is the low limit of the divergent-density model. Definition For two class problems with balanced training data (m = n), if the positive training data can be separated from the negative, we say that the multiple instance learning problem is well distributed. One of the well-known examples is the Gaussian distribution. To find the desired target, i.e. the section of the positive pockets, we hope that it obeys the Gaussian distribution and can be separated from the negative instances."}, {"heading": "3.5 Algorithm", "text": "In view of the fact that the traditional K-remedy depends on the initial clusters, we use more robust K-remedy + + [29] in our experiment."}, {"heading": "3.6 Complexity", "text": "The complexity of our algorithm consists of the formation of K-mean clusters and the calculation of histograms for both positive and negative pouches. The other training steps work only with constant numbers, so their corresponding time can be ignored. Note that the complexity of our method is dominated by K-means, which can be terminated in O (NK). For large data sets, sample 10,000 instances from the training data and use K-means + to divide them into K-clusters and then assign all other instances to the K-centers."}, {"heading": "4 Experiments", "text": "We performed experimental evaluation on five benchmark datasets, including the traditional MUSK datasets (Musk1 and Musk2) [1] and image datasets (Tiger, Elephant and Fox) 1. We also evaluate our method on a large dataset, TRECVID MED11 dataset 2. A tenfold cross-validation was used and the pro-fold average 1http: / / www.cs.columbia.edu / ~ andrews / mil / datasets.html 2http: / / www.nist.gov / itl / iad / med11.cfmAlgorithm 11: Initialize K, p, q, \u03c4; 2: Partition D = D1, D2,..., D10; / 10-fold cross-validation 3: for i = 1; i < = 10; i + do 4: Dt = D \u2212 \u2212 basunt for testing 5: Do averages + and testing 6 in the classification."}, {"heading": "4.1 Benchmark MIL datasets", "text": "The MUSK datasets are more widely used than the benchmark datasets for the MIL algorithms. The feature vector for both Musk1 and Musk2 is 166-dimensional. The MUSK1 contains a total of 92 bags (47 positive and 45 negative), with about 6 instances per bag. The Musk2 datasets contain 102 bags (39 positive / 63 negative), with about 6 instances per bag. The COREL image dataset is 230 dimensional, with three object categories: tiger, elephant, and fox. Each of the three categories consists of 200 bags (100 positive and 100 negative), with about 6 instances per bag. We compare our method with many other classical MIL algorithms (the Matlab codes 3 of DD, EM-DD, Citation-KNN, and mi-SVM are available)."}, {"heading": "4.2 Experiments on the TRECVID MED11", "text": "We use the first five events with 813 video clips with millions of images representing complex activities and events, see Figure (3) for sample images. We extract local features using HOG3D [31]. To represent videos with local features, we use a common bag-of-words model with a 1000-word codebook. Each image is presented as a histogram of the occurrence of codebook elements. As this data set requires large memory, we use it on a 24-core machine (Intel (R) Xeon (R) CPU X5650 @ 2.67 GHz) with 48GB memory. In this experiment, we only have video-level labels. Therefore, we treat event detection as a MIL problem. Each video can be considered as a bag, and its images in the corresponding bag as instances. To evaluate the performance of our method, we use MI-SVM as a baseline, because we require the SVM structural timing (SVM) indicator = SVM."}, {"heading": "5 Conclusion", "text": "Instead of maximizing a probability function, we use a greedy strategy to maximize the histogram ratio between positive and negative pockets. By learning code books for both target and negative centers, we can use them to classify novel pockets based on the strategy of the nearest neighbors. The primary contribution of this paper is to maximize the density ratio to speed up the learning process. Another contribution is the learning of both targets and negative candidate centers to reduce false positives. Experimental results show that our method is significantly faster and more effective compared to the state of the art. In future work, we will consider the weight to learn instances of clusters to learn the code books. For example, we can use the removal of mahalanobis as a distance measure in K-media. We also plan to study spectral cluster methods or kernel-k means methods to learn the code books."}], "references": [{"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artif. Intell", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "A framework for multiple-instance learning", "author": ["O. Maron", "T. Lozano-Prez"], "venue": "In: NIPS", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "TPAMI", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Multiple instance boosting for object detection", "author": ["P. Viola", "J.C. Platt", "C. Zhang"], "venue": "In: NIPS", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrews", "I. Tsochantaridis", "T. Hofmann"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Supervised versus multiple instance learning: An empirical comparison", "author": ["S. Ray", "M. Craven"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Multi-instance multilabel learning with application to scene classification", "author": ["Zhou", "Z.h", "Zhang", "M.l"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Multiple instance learning via margin maximization", "author": ["O.E. Kundakcioglu", "O. Seref", "P.M. Pardalos"], "venue": "Appl. Numer. Math", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Solving the multiple-instance problem: A lazy learning approach", "author": ["J. Wang", "J.D. Zucker"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Em-dd: An improved multiple-instance learning technique", "author": ["Q. Zhang", "S.A. Goldman"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Multiple instance learning for sparse positive bags", "author": ["R.C. Bunescu", "R.J. Mooney"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Multiple-instance learning via embedded instance selection", "author": ["Y. Chen", "J. Bi", "Wang", "J.Z.: Miles"], "venue": "TPAMI", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "A regularization framework for multiple-instance learning", "author": ["Cheung", "P.m", "J.T. Kwok"], "venue": "In: ICML", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Deterministic annealing for multiple-instance learning", "author": ["P.V. Gehler", "O. Chapelle"], "venue": "Journal of Machine Learning Research", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Svm-based generalized multiple-instance learning via approximate box counting", "author": ["Q. Tao", "S. Scott", "N.V. Vinodchandran", "T.T. Osugi"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Multi-instance learning by treating instances as non-i.i.d. samples", "author": ["Z.H. Zhou", "Y.Y. Sun", "Y.F. Li"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Integrated segmentation and recognition of hand-printed numerals", "author": ["J.D. Keeler", "D.E. Rumelhart", "W.K. Leow"], "venue": "In: NIPS", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1990}, {"title": "Multi instance neural networks", "author": ["J. Ramon", "L.D. Raedt"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2000}, {"title": "Multiple instance learning via disjunctive programming boosting", "author": ["S. Andrews", "T. Hofmann"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Bayesian multiple instance learning: automatic feature selection and inductive transfer", "author": ["V.C. Raykar", "B. Krishnapuram", "J. Bi", "M. Dundar", "R.B. Rao"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Solving multiple-instance and multiple-part learning problems with decision trees and decision rules. application to the mutagenesis", "author": ["J.D.Z. Yann"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Adaptive p-posterior mixture-model kernels for multiple instance learning", "author": ["H. Wang", "Q. Yang", "H. Zha"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "F.D.L.: Gaussian processes multiple instance learning", "author": ["M. Kim", "Torre"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A conditional random field for multiple-instance learning", "author": ["T. Deselaers", "V. Ferrari"], "venue": "ICML", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Multiple instance learning with manifold bags", "author": ["B. Babenko", "N. Varma", "P. Doll\u00e1r", "S. Belongie"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "In: NIPS", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "k-means++: the advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "SODA. SODA \u201907,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Multiple instance learning for computer aided diagnosis", "author": ["G. Fung", "M. Dundar", "B. Krishnapuram", "R.B. Rao"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2006}, {"title": "A spatio-temporal descriptor based on 3d-gradients", "author": ["A. Kl\u00e1ser", "M. Marszalek", "C. Schmid"], "venue": "Pr(x = t | B", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 187, "endOffset": 190}, {"referenceID": 2, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 209, "endOffset": 212}, {"referenceID": 3, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 226, "endOffset": 229}, {"referenceID": 4, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 251, "endOffset": 257}, {"referenceID": 5, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 251, "endOffset": 257}, {"referenceID": 1, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 283, "endOffset": 289}, {"referenceID": 6, "context": "This paradigm has been receiving much attention in the last several years, and has many useful applications in a number of fields, including drug activity prediction [1], stock selection [2], object detection [3] and tracking [4], text categorization [5, 6] and image categorization [2, 7].", "startOffset": 283, "endOffset": 289}, {"referenceID": 7, "context": "Recently research [8] also shows that the halfspaces finding problem for MIL is NP-complete.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "Our approach is inspired by the definition of training bags, as well as the Diverse Density (DD) [2] and Citation kNN models [9].", "startOffset": 97, "endOffset": 100}, {"referenceID": 8, "context": "Our approach is inspired by the definition of training bags, as well as the Diverse Density (DD) [2] and Citation kNN models [9].", "startOffset": 125, "endOffset": 128}, {"referenceID": 0, "context": "[1] for drug activity prediction.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Following this seminal work, there has been a significant amount of research devoted to MIL problems using different learning models, such as DD [2], EM-DD [10], and extended Citation kNN [9].", "startOffset": 145, "endOffset": 148}, {"referenceID": 9, "context": "Following this seminal work, there has been a significant amount of research devoted to MIL problems using different learning models, such as DD [2], EM-DD [10], and extended Citation kNN [9].", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "Following this seminal work, there has been a significant amount of research devoted to MIL problems using different learning models, such as DD [2], EM-DD [10], and extended Citation kNN [9].", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "Due to the success of the SVM algorithm [5], and the various positive theoretical results behind it, maximum margin methods have become extremely popular in machine learning.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "[5] combined MIL with SVM first to cope with MIL problems.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 94, "endOffset": 118}, {"referenceID": 11, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 94, "endOffset": 118}, {"referenceID": 12, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 94, "endOffset": 118}, {"referenceID": 13, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 94, "endOffset": 118}, {"referenceID": 14, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 94, "endOffset": 118}, {"referenceID": 15, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 94, "endOffset": 118}, {"referenceID": 16, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 136, "endOffset": 144}, {"referenceID": 17, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 136, "endOffset": 144}, {"referenceID": 18, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 155, "endOffset": 162}, {"referenceID": 3, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 155, "endOffset": 162}, {"referenceID": 5, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 197, "endOffset": 204}, {"referenceID": 19, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 197, "endOffset": 204}, {"referenceID": 20, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 221, "endOffset": 225}, {"referenceID": 21, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 248, "endOffset": 252}, {"referenceID": 22, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 273, "endOffset": 277}, {"referenceID": 23, "context": "Later methods focus more on applying discriminative models to solve MIL problems, such as SVM [11, 12, 13, 14, 15, 16], neural networks [17, 18], boosting [19, 4], regression for feature selection [6, 20], decision trees [21], mixtures of Gaussian [22], Gaussian processes [23], conditional random fields [24] and manifold", "startOffset": 305, "endOffset": 309}, {"referenceID": 24, "context": "learning [25].", "startOffset": 9, "endOffset": 13}, {"referenceID": 1, "context": "As mentioned in [2], the assumption that all bags intersect at a single point is not necessary, DD assumes more complicated concepts.", "startOffset": 16, "endOffset": 19}, {"referenceID": 25, "context": "Recently, discriminative dictionary learning [26] greatly improves accuracy for visual object recognition.", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "The notation used in this paper for bags and instances is the one introduced by Maron and Lozano-Perez [2].", "startOffset": 103, "endOffset": 106}, {"referenceID": 0, "context": "Assume we have only 4 variable, v 1 , v + 2 , v \u2212 1 and v \u2212 2 , where v + i \u2265 v \u2212 j , i, j \u2208 [1, 2], then we have to prove v 1 v + 2 v\u2212 1 v \u2212 2 \u2265 v1+v2 v3+v4 .", "startOffset": 93, "endOffset": 99}, {"referenceID": 1, "context": "Assume we have only 4 variable, v 1 , v + 2 , v \u2212 1 and v \u2212 2 , where v + i \u2265 v \u2212 j , i, j \u2208 [1, 2], then we have to prove v 1 v + 2 v\u2212 1 v \u2212 2 \u2265 v1+v2 v3+v4 .", "startOffset": 93, "endOffset": 99}, {"referenceID": 26, "context": "Considering that the traditional K-means depends on the initial clusters, we use more robust K-means++ [29] in our experiment.", "startOffset": 103, "endOffset": 107}, {"referenceID": 0, "context": "We conducted experimental evaluation on five benchmark datasets including the traditional MUSK datasets (Musk1 and Musk2) [1] and image datasets (Tiger, Elephant, and Fox).", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "The comparisons of different methods demonstrate again that no single MIL algorithm outperforms the others across all data sets [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 0, "context": "Method Accuracy for benchmark dataset Musk1 Musk2 Elephant Fox Tiger APR[1] 92.", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "2 N/A N/A N/A DD[2] 88.", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "5 N/A N/A N/A EM-DD[10] 84.", "startOffset": 19, "endOffset": 23}, {"referenceID": 8, "context": "1 Citation kNN[9] 92.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "3 N/A N/A N/A mi-SVM[5] 87.", "startOffset": 20, "endOffset": 23}, {"referenceID": 27, "context": "9 MICA[30] 84.", "startOffset": 6, "endOffset": 10}, {"referenceID": 13, "context": "0 MI-SVM + DA[14] 85.", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "0 PPMM Kernel[22] 95.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "2 miGraph[16] 90.", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "9 CRF-MIL[24] 88.", "startOffset": 9, "endOffset": 13}, {"referenceID": 22, "context": "0 GP-MIL[23] 89.", "startOffset": 8, "endOffset": 12}, {"referenceID": 8, "context": "Note that except Citation kNN [9], the results of all other methods are based on 10 fold cross-validation.", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "rank Code DD [2] 1725.", "startOffset": 13, "endOffset": 16}, {"referenceID": 9, "context": "9 >3600 >3600 >3600 >3600 Matlab EM-DD [10] 1693.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "3 >3600 >3600 >3600 >3600 Matlab Citation kNN [9] 18.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "5 4 Matlab MI-SVM [5] 3.", "startOffset": 18, "endOffset": 21}, {"referenceID": 15, "context": "1 2 Matlab/C++ miGraph [16] 21.", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "4 3 Matlab/C++ CRF-MIL[24] 200.", "startOffset": 22, "endOffset": 26}, {"referenceID": 28, "context": "We extract local features using HOG3D [31].", "startOffset": 38, "endOffset": 42}, {"referenceID": 4, "context": "Accuracy Time (Days) MI-SVM [5] 37.", "startOffset": 28, "endOffset": 31}], "year": 2012, "abstractText": "Multiple instance learning (MIL) has attracted great attention recently in machine learning community. However, most MIL algorithms are very slow and cannot be applied to large datasets. In this paper, we propose a greedy strategy to speed up the multiple instance learning process. Our contribution is two fold. First, we propose a density ratio model, and show that maximizing a density ratio function is the low bound of the DD model under certain conditions. Secondly, we make use of a histogram ratio between positive bags and negative bags to represent the density ratio function and find codebooks separately for positive bags and negative bags by a greedy strategy. For testing, we make use of a nearest neighbor strategy to classify new bags. We test our method on both small benchmark datasets and the large TRECVID MED11 dataset. The experimental results show that our method yields comparable accuracy to the current state of the art, while being up to at least one order of magnitude faster.", "creator": "LaTeX with hyperref package"}}}