{"id": "1205.1638", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2012", "title": "Document summarization using positive pointwise mutual information", "abstract": "The degree of success in document summarization processes depends on the performance of the method used in identifying significant sentences in the documents. The collection of unique words characterizes the major signature of the document, and forms the basis for Term-Sentence-Matrix (TSM). The Positive Pointwise Mutual Information, which works well for measuring semantic similarity in the Term-Sentence-Matrix, is used in our method to assign weights for each entry in the Term-Sentence-Matrix. The Sentence-Rank-Matrix generated from this weighted TSM, is then used to extract a summary from the document. Our experiments show that such a method would outperform most of the existing methods in producing summaries from large documents.", "histories": [["v1", "Tue, 8 May 2012 09:19:10 GMT  (232kb)", "http://arxiv.org/abs/1205.1638v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["aji s", "ramachandra kaimal"], "accepted": false, "id": "1205.1638"}, "pdf": {"name": "1205.1638.pdf", "metadata": {"source": "CRF", "title": "DOCUMENT SUMMARIZATION USING POSITIVE POINTWISE MUTUAL INFORMATION", "authors": ["Ramachandra Kaimal"], "emails": ["aji_12345@yahoo.com", "mrkaimal@yahoo.com"], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijcsit.2012.4204 47The degree of success of document summary processes depends on the performance of the method used to identify significant sentences in documents. Collecting unique words characterizes the main signature of the document and forms the basis for term sentence matrix (TSM). Positive Pointwise Mutual Information, which is well suited to measuring semantic similarity in the term sentence matrix, is used in our method to assign weights for each entry in the term sentence matrix. Subsequently, the Sentence Rank matrix generated from this weighted TSM is used to extract a summary from the document. Our experiments show that such a method would surpass most of the existing methods for generating summaries from large documents. KEYWORDSData Mining, Text Mining, Document Summary, Positive Terrix Mutrix"}, {"heading": "1. INTRODUCTION", "text": "According to the latest IDC report [1], global information doubles every two years; in 2011, more than 1.8 zettabytes of information were generated worldwide; by 2020, the world will generate 50 times as much information and 75 times as many \"information repositories\" as IT personnel to manage it; the report also points to the need for new \"information technologies\" for information processing and storage; and to speed up access, the flow of information must be filtered and systematically stored, making the work of information systems (IRS) more effective by pooling the entire collection of documents; and automatic text summaries can help by providing compressed versions of text documents."}, {"heading": "2. METHOD", "text": "In linguistics, morphology [7] deals with the arrangement and relationships between words in a document. In any kind of word processing application, the first step is morphological analysis. Tokenization, elimination of stop words [8] and stammering [9] are the sub-tasks that are followed in our method."}, {"heading": "2.1 Tokenization and stop words elimination", "text": "Although the letters are the smallest unit, words are considered useful and informative building blocks of a document for processing. As shown in Figure 1, the sentences in the document are separated and treated as examples niS i,... 1, = for the experiment. Words in iS are separated in the next step and the punctuation marks and other irrelevant notations are removed from these words. Stopwords are very commonly used words such as \"that,\" \"from,\" \"a,\" in, \"to,\" is, \"for,\" with, \"etc., which do not contribute to the information content of a document and can therefore be removed. These stopwords have much meaning in the processing techniques of natural language that evaluate grammatical structures, but they have less significance in statistical analysis."}, {"heading": "2.2 Stemming", "text": "In general, the morphological variants of words separated from a document have analogous semantic understandings and can be regarded as equivalent in the IR system. A few algorithms [Lovins Stemming, Porter Stemming] have been developed to reduce a word to its trunk or root. After the parentage process, the terms of a document are the trunks, not the original words. Stemming algorithms not only reduce a word in stem, but also reduce the size of the word list to be considered for analysis. We follow the Porter Stemming [11] method, which is a rules-based algorithm that works with both suffixes and prefixes. The algorithm defines five successive steps, each consisting of a set of rules for transformation.Here, a word is presented as a combination of consonants and vowels in the form [] [] [] [VCVCVCC- where the sequence of the document can be written individually as a document - Vigent (PS)."}, {"heading": "2.3 Term-Sentence-Matrix", "text": "The presence of t words in the document is represented by a term sentence matrix (TSM) of n columns and t rows, where t is the number of unique words and n is the number of sentences in the entire document. Each element ijF of the matrix measures the meaning of term i appropriately in relation to the sentence and the entire document."}, {"heading": "2.4 Weighting the Elements", "text": "TSM alone is not suitable for analyzing the characteristic of a document; terms that have a high frequency are not necessarily more compelling; a weight derived from the local and documentary context can give more information than a frequency. Mutual information (MI) [12] of an entry measures the amount of information provided by that entry throughout the document. Consider a pair of results x and y, say the occurrence of words x and y, the MI is defined as:) 5 () / (log) 4 () / (log) 3 () (), (log), (log), (log), (pxypxpyxpyxpyxpyxp yxMI = = = = = The measure is symmetric and can be positive or negative, but is null if x and y are independent. [log) (log), (logmin), (logp), (pyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpypyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpypyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpyxpypypypypypypypypypyxpyxpyxpyxpypy"}, {"heading": "2.5 Ranking the sentence", "text": "The total meaning of the kest sentence, sk, can be calculated from the PPMI matrix as follows:) 11 (. 1 kti kik psPPMIs \u2211 = =, where kps is the probability of the kest sentence in the context of the document to be grasped. Sentences throughout the documents are classified according to ks. Sentences with the required percentage weight are identified and arranged in the order as in the original document."}, {"heading": "3. EXPERIMENTAL RESULTS", "text": "The articles contain an average of 850 words and 45 sentences. These articles are stored as plaintext. The implementation strategy of our method is shown in Figure 1.Here we look at seven documents discussing the translation details. Figure 2 explains the status of the feature extraction after the first phase. Although the total number of words before and after parentage has a well-defined relation, the number of words after parentage in each document has decreased significantly. An average of 50% of the words is eliminated in each document in the first phase. Figure 2: The total number of words to be considered for the next phases decreases significantly after the first phase. Figure 1: There are three phases in the implementation; the document to be grasped is given to the tokenization process in the first phase. Figure M overhauls the summary of the document."}, {"heading": "4. EVALUATION", "text": "There is no clear and standardized explanation for what constitutes a good summary. Assessing summaries is a major challenge in summary systems. Researchers have been working over the last few decades to answer this complex question. A new method in this area is evaluation based on latent semantic analysis [15], which evaluates the quality of the summary by the similarity between the content of the document and its summary."}, {"heading": "4.1 Measure of Main Topic", "text": "In addition to the existing PPMI matrix, we have constructed another matrix, SMI, for the summary of PPMI. SMI consists of t-rows and l-columns, l being the number of sentences in the summary. SVD method splits PPMI into three components such as) 12 (TVdSdUdPPMI = and the SMI is called) 13 (TVsSsUsMI = The first left singular vector of Ud is called the main topic [18] of the article. In this approach, the main topic of both the summary and the document is calculated, these vectors are the most significant characteristics of the document and the summary. The classic cosine, inspcos, between the \"main topic vector\" of the document and the \"summary\" reveals the degree of quality of the summary."}, {"heading": "5. CONCLUSION", "text": "The proposed method of summary consists of three separate phases: the Porter-Stemming algorithm in the morphological analysis phase has considerably reduced the characteristic matrix; the technique of positive-point reciprocal information is used to determine the weight of sentences in a document; it is shown here that latent semantic analysis is a reliable summary evaluation mechanism; it is pointed out that the summary of a document already reaches its maximum result in the first phases of the experiments.The overall average value of \u03c6cos, the distance measurement between the main topics of the summary and the document, shows that the importance of positive-point reciprocal information is evident in the text data analysis and in particular in the summary process."}], "references": [{"title": "Multi-Document Summarization as Applied in Information Retrieval", "author": ["Zhou", "D. Lei Li"], "venue": "Natural Language Processing and Knowledge Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Methods for Automatically Evaluating Answers to Complex Questions", "author": ["Jimmy Lin", "Dina Demner-Fushman"], "venue": "Information Retrieval,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Automated Text Summarization", "author": ["I. Mani"], "venue": "John Benjimans, Amsterdam. Journals,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Co-occurrence vectors from corpora vs. distance vectors from dictionaries", "author": ["Y. Niwa", "Y. Nitta"], "venue": "In Proceedings of the 15th International Conference On Computational Linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Word association norms, mutual information, and lexicography", "author": ["K. Church", "P. Hanks"], "venue": "In Proceedings of the 27th Annual Conference of the Association of Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1989}, {"title": "Word recognition by morphological analysis", "author": ["Hankyu Lim", "Ungmo Kim"], "venue": "Intelligent Information Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "The importance of stop word removal on recall values in text categorization", "author": ["C. Silva", "B. Ribeiro"], "venue": "Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Information Retrieval: Data Structures and Algorithms", "author": ["Frakes", "R. Baeza-Yates", "(ed"], "venue": "Englewood Cliffs, NJ: Prentice-Hall,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "Stemming in the language modeling framework", "author": ["James Allan", "Giridhar Kumaran"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "An algorithm for suffix stripping, Program, 14(3) pp 130\u2212137", "author": ["M.F. Porter"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1980}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["Peter D. Turney", "Patrick Pantel"], "venue": "Journal of Arti_cial Intelligence Research", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Co-occurrence vectors from corpora vs. distance vectors from dictionaries", "author": ["Y. Niwa", "Y. Nitta"], "venue": "In Proceedings of the 15th International Conference On Computational Linguistics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1994}, {"title": "The Challenges of Automatic Summarization", "author": ["U. Hahn", "I. Mani"], "venue": "Journal of Computer Science & Information Technology (IJCSIT)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Generic Text Summarization Using Relevance Measure and Latent Semantic Analysis", "author": ["Y. Gong", "X. Liu"], "venue": "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Using Latent Semantic Analysis in Text Summarization and Summary Evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "Proceedings of ISIM", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "Expected summarization holds a list of applications like information extraction, document retrieval [2], evaluation of answer books [3], etc.", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "Expected summarization holds a list of applications like information extraction, document retrieval [2], evaluation of answer books [3], etc.", "startOffset": 132, "endOffset": 135}, {"referenceID": 12, "context": "Since the first study on text extraction by Luhn appeared, the text summarization process has attracted lot of research activities [14,16,17].", "startOffset": 131, "endOffset": 141}, {"referenceID": 2, "context": "Depending on the purpose and intended users, a summary can be generic or user-focused [4].", "startOffset": 86, "endOffset": 89}, {"referenceID": 3, "context": "In this paper we suggest a method based on Positive Pointwise Mutual Information (PPMI) [5] an extension of Pointwise Mutual Information PMI[6] which gives more importance to measure the semantic similarity between the words in a document for document summarization.", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "In this paper we suggest a method based on Positive Pointwise Mutual Information (PPMI) [5] an extension of Pointwise Mutual Information PMI[6] which gives more importance to measure the semantic similarity between the words in a document for document summarization.", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "METHOD In linguistics, morphology [7] deals with the arrangement and relationships between the words in a document.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Tokenization, stop words elimination [8] and stemming [9] are the sub tasks that are followed in our method.", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "Tokenization, stop words elimination [8] and stemming [9] are the sub tasks that are followed in our method.", "startOffset": 54, "endOffset": 57}, {"referenceID": 8, "context": "A couple of algorithms [Lovins Stemming, Porter Stemming] for stemming [10,11] have been developed to reduce a word to its stem or root.", "startOffset": 71, "endOffset": 78}, {"referenceID": 9, "context": "A couple of algorithms [Lovins Stemming, Porter Stemming] for stemming [10,11] have been developed to reduce a word to its stem or root.", "startOffset": 71, "endOffset": 78}, {"referenceID": 9, "context": "We are following the Porter Stemming [11] method, which is a rule based algorithm that works with both suffixes and prefixes.", "startOffset": 37, "endOffset": 41}, {"referenceID": 10, "context": "Mutual Information (MI)[12] of an entry measures the amount of information contributed by that entry in the entire document.", "startOffset": 23, "endOffset": 27}, {"referenceID": 10, "context": "The Positive PMI (PPMI) [12] is a modified version of PMI, in which all MI values that are less than zero are replaced with zero [13].", "startOffset": 24, "endOffset": 28}, {"referenceID": 11, "context": "The Positive PMI (PPMI) [12] is a modified version of PMI, in which all MI values that are less than zero are replaced with zero [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 13, "context": "Evaluation based on Latent Semantic Analysis[15] is new method in this area.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "The first left singular vector of Ud is called the main topic[18] of the article.", "startOffset": 61, "endOffset": 65}], "year": 2012, "abstractText": "The degree of success in document summarization processes depends on the performance of the method used in identifying significant sentences in the documents. The collection of unique words characterizes the major signature of the document, and forms the basis for Term-Sentence-Matrix (TSM). The Positive Pointwise Mutual Information, which works well for measuring semantic similarity in the TermSentence-Matrix, is used in our method to assign weights for each entry in the Term-Sentence-Matrix. The Sentence-Rank-Matrix generated from this weighted TSM, is then used to extract a summary from the document. Our experiments show that such a method would outperform most of the existing methods in producing summaries from large documents.", "creator": "PScript5.dll Version 5.2.2"}}}