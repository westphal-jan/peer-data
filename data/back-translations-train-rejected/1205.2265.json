{"id": "1205.2265", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2012", "title": "Efficient Constrained Regret Minimization", "abstract": "Online learning constitutes a mathematical framework to analyze sequential decision making problems in adversarial environments. The learner repeatedly chooses an action, the environment responds with an outcome, and then the learner receives a reward for the played action. The goal of the learner is to maximize his total reward. However, there are situations in which, in addition to maximizing the cumulative reward, there are some additional constraints/goals on the sequence of decisions that must be satisfied by the learner. For example, in \\textit{online marketing}, simultaneously maximizing the cumulative reward and the number of buyers to take advantage of word-of-mouth advertising for future marketing seems to be a more ambitious goal than only maximizing cumulative reward. As another example, learning from costly expert advice captures more realistic settings than the original setting in applications such as routing in networks with power constraint. In this paper we study an extension to the online learning where the learner aims to maximize the total reward given that some additional constraints need to be satisfied. We propose Lagrangian exponentially weighted average (\\textbf{LEWA}) algorithm, an efficient algorithm to solve constrained online learning, which is a primal dual variant of the well known exponentially weighted average algorithm and inspired by the theory of Lagrangian method in constrained optimization. We establish the regret and the violation of the constraint bounds in full information and bandit feedback models.", "histories": [["v1", "Tue, 8 May 2012 23:06:06 GMT  (31kb)", "https://arxiv.org/abs/1205.2265v1", null], ["v2", "Thu, 4 Oct 2012 06:49:29 GMT  (32kb)", "http://arxiv.org/abs/1205.2265v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mehrdad mahdavi", "tianbao yang", "rong jin"], "accepted": false, "id": "1205.2265"}, "pdf": {"name": "1205.2265.pdf", "metadata": {"source": "CRF", "title": "Efficient Constrained Regret Minimization", "authors": ["Mehrdad Mahdavi", "Tianbao Yang", "Rong Jin"], "emails": ["mahdavim@msu.edu", "yangtia1@msu.edu", "rongjin@msu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 120 5.22 65v2 [cs.LG] 4 Oct 201 2Keywords: online learning, bandit, repentance minimization, repeated playing, limited decision-making"}, {"heading": "1 Introduction", "text": "This year, it is more than ever before in the history of the city in which we find ourselves."}, {"heading": "2 Statement of the Problem", "text": "It is about the question to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question,"}, {"heading": "3 Related Works", "text": "In fact, if we want to solve the problems, we are able to get to grips with them."}, {"heading": "4 Full Information Constrained Regret Minimization", "text": "In this section, we present the basic algorithms for online learning with limitation problem and analyze their performance using the primary-dual method in the opposite setting. (A simple approach to tackling the problem is to change the learner's reward functions to include a punitive coefficient that adjusts the likelihood of actions if the limitation is violated.) This approach circumvents the problem of limited online learning by turning it into an unrestricted problem. However, a simple analysis shows that this simple punitive approach is not able to gradually overcome the limits of regret and violation of the limitations. The main difficulty arises from the fact that an adaptive adversary can play with the punitive coefficient in order to weaken the influence of the penalty parameters that lead to one of the measures, i.e. either regret or violation of the limitations."}, {"heading": "4.1 A High Probability Bound", "text": "The performance limits that have been established in the previous section for regretting and violating the restriction are valid only in anticipation (which can have enormous fluctuations around its mean).Here, with a simple trick, we present a modified version of the LEWA algorithm that overwhelmingly achieves similar limits. To achieve this, we modify the original LEWA algorithm slightly. Specifically, we use a modified estimate instead and add a trust that is bound to achieve a more accurate estimate of the restriction vector c. The following theorem limits the regret and violation of the restriction in high probability for the modified algorithm. Theorem 2: Let the probability (1 / 2) ln (T \u2212 1 / 2) ln (T \u2212 1 / 2) ln (T \u2212 1 / 2) ln (T \u2212 1 / 2) n (T \u2212 1 / 2) n (T \u2212 1 / 2) n (T \u2212 1 / 2) n (T \u2212 1 / 2), and the violation of the restriction."}, {"heading": "5 Bandit Constrained Regret Minimization", "text": "In this section, we generalize our results to the bandit setting for both rewards and limitations. In the bandit setting, we are forced to choose an action that we select from the pool of actions (K). Then, only the reward and limitations for the action that we show to the learner are revealed, i.e., this problem can be solved in this case by taking an interest in solving the problem as maxp and Exp3 algorithms. In the classical setting, i.e., without limitation, this problem can be solved in stochastic and adversarial settings of UCB and Exp3 algorithms. The algorithms are shown in BanditLEWA algorithms that use a similar idea for exploration and exploitation."}, {"heading": "6 Conclusions and Future Works", "text": "The proposed algorithm, namely LEWA, is a primary dual variant of the exponentially weighted average algorithm and is based on the theory of Lagrange's theory of restricted optimization. We set expected and high probability limits for regret and long-term violation of the restriction in complete information and bandit environments by means of novel theoretical analyses. Especially in the complete information environment, LEWA algorithms achieve optimal O-limits for regret and O-limits for violation of the restriction in expectation and with a simple trick in high probability. The present work leaves open a number of interesting directions for future work. In particular, the extension of the framework for handling multi-criterion-line decisions is left to future work. Turning the proposed algorithm towards the one that accurately fulfils the restriction in the long term is also an interesting problem. Finally, it would be interesting if it were possible to improve the restriction."}, {"heading": "7 References", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Proof of Theorem 4", "text": "Similar to the analysis of the Exp3.P algorithm in [18], we have the following two uppermost trust limits (T = 1r = 1r = 1l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l = 1 l ="}], "references": [{"title": "On stochastic and worst-case models for investing", "author": ["E. Hazan", "S. Kale"], "venue": "in: NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "The weighted majority algorithm, Inf. Comput", "author": ["N. Littlestone", "M.K. Warmuth"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Path kernels and multiplicative updates", "author": ["E. Takimoto", "M.K. Warmuth"], "venue": "Journal Machine Learnning Research", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Unified algorithms for online learning and competitive analysis, COLT", "author": ["J.S.N. Niv Buchbinder", "Shahar Chen", "O. Shamir"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Online learning with sample path constraints", "author": ["S. Mannor", "J.N. Tsitsiklis", "J.Y. Yu"], "venue": "Journal of Machine Learning Research", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J. Comput. Syst. Sci", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "A parameter-free hedging algorithm", "author": ["K. Chaudhuri", "Y. Freund", "D. Hsu"], "venue": "in: NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Epsilonfirst policies for budget-limited multi-armed bandits", "author": ["L. Tran-Thanh", "A.C. Chapman", "E.M. de Cote", "A. Rogers", "N.R. Jennings"], "venue": "in: AAAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Knapsack based optimal policies for budget-limited multi-armed bandits, CoRR abs/1204.1909", "author": ["L. Tran-Thanh", "A.C. Chapman", "A. Rogers", "N.R. Jennings"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1909}, {"title": "Repeated games against budgeted adversaries", "author": ["J. Abernethy", "M.K. Warmuth"], "venue": "in: NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "A characterization of stability in linear programming, Operations Research", "author": ["S.M. Robinson"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1977}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["J. Langford", "T. Zhang"], "venue": "in: NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "in: Proceedings of the 20th International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Finite-time analysis of the multiarmed bandit problem, Machine Learning", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Comput", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2002}, {"title": "Extracting certainty from uncertainty: Regret bounded by variation in costs", "author": ["E. Hazan", "S. Kale"], "venue": "in: COLT,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}], "referenceMentions": [], "year": 2012, "abstractText": "Online learning constitutes a mathematical and compelling framework to analyze sequential decision making problems in adversarial environments. The learner repeatedly chooses an action, the environment responds with an outcome, and then the learner receives a reward for the played action. The goal of the learner is to maximize his total reward. However, there are situations in which, in addition to maximizing the cumulative reward, there are some additional constraints on the sequence of decisions that must be satisfied on average by the learner. In this paper we study an extension to the online learning where the learner aims to maximize the total reward given that some additional constraints need to be satisfied. By leveraging on the theory of Lagrangian method in constrained optimization, we propose Lagrangian exponentially weighted average (LEWA) algorithm, which is a primal-dual variant of the well known exponentially weighted average algorithm, to efficiently solve constrained online decision making problems. Using novel theoretical analysis, we establish the regret and the violation of the constraint bounds in full information and bandit feedback models.", "creator": "LaTeX with hyperref package"}}}