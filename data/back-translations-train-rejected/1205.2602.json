{"id": "1205.2602", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "The Entire Quantile Path of a Risk-Agnostic SVM Classifier", "abstract": "A quantile binary classifier uses the rule: Classify x as +1 if P(Y = 1|X = x) &gt;= t, and as -1 otherwise, for a fixed quantile parameter t {[0, 1]. It has been shown that Support Vector Machines (SVMs) in the limit are quantile classifiers with t = 1/2 . In this paper, we show that by using asymmetric cost of misclassification SVMs can be appropriately extended to recover, in the limit, the quantile binary classifier for any t. We then present a principled algorithm to solve the extended SVM classifier for all values of t simultaneously. This has two implications: First, one can recover the entire conditional distribution P(Y = 1|X = x) = t for t {[0, 1]. Second, we can build a risk-agnostic SVM classifier where the cost of misclassification need not be known apriori. Preliminary numerical experiments show the effectiveness of the proposed algorithm.", "histories": [["v1", "Wed, 9 May 2012 18:46:51 GMT  (356kb)", "http://arxiv.org/abs/1205.2602v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jin yu", "s v n vishwanatan", "jian zhang"], "accepted": false, "id": "1205.2602"}, "pdf": {"name": "1205.2602.pdf", "metadata": {"source": "CRF", "title": "The Entire Quantile Path of a Risk-Agnostic SVM Classifier", "authors": ["Jin Yu", "S.V.N. Vishwanathan", "Jian Zhang"], "emails": ["jin.yu@anu.edu.au", "jianzhan}@stat.purdue.edu"], "sections": [{"heading": null, "text": "A quantitative binary classifier uses the rule: Classify x as + 1 if P (Y = 1 | X = x) \u2265 \u03c4 and as \u2212 1 otherwise for a fixed quantity parameter. It has been shown that support vector machines (SVMs) limit quantil classifiers to \u03c4 = 12. In this paper we show that by using asymmetric costs of misclassification, SVMs can be adequately extended to restore the quantitative binary classifier at the limit for any desired performance. Subsequently, we present a principal algorithm for solving the extended SVM classifier for all values simultaneously. This has two implications: firstly, one can restore the entire conditional distribution P (Y = 1 | X = x) = profiling experiments for the proposed algorithm. Secondly, we can build a risk-agnostic SVM classifier where the costs of misclassification need not be known apriori."}, {"heading": "1 Introduction", "text": "It is not as if we put the data into an RKHS card (x) and use the kernels k (x) (x), x), x), x), x), x), x), x), x), x), x), x), x), x \"(2), c), c). (3), c). (3), c). (3), c). (3), c). (3), c). (3), c). (3), c). (3), c). (3), c). (3), c). (3), c). (3), c). (3). (3), c). (3). (3). (3), c). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4).). (4). (4)."}, {"heading": "2 Statistical Underpinnings", "text": "Let (X, Y) be a pair of random variables with training examples X + X and define the costs Y + (resp. C \u2212) as the cost for the misclassification of an x as + 1 (resp. \u2212 1). The cost-sensitive classification risk of a decision function g: X \u2192 {\u00b1 1} is defined as R (g): = C + P (Y = \u2212 1, g (X) = 1) (5) + C \u2212 P (Y = 1, g (X) = \u2212 1).The following problem results from the elementary Bayes decision theory (see e.g. Section 2.2 of Duda et al., 2001).Lemma 2.1 For each decision function gR (\u03b7 (x) \u2212 1), where the misclassification is an x."}, {"heading": "3 Dual Formulation", "text": "Similar to the case of standard SVM, we can (3) as a limited optimization problem = > Q2 = > Q2 = > Q2 = > Q3 = > Q3 = > Q3 = > Q3 = > Q3 = > Q3 = \u2212 Q3 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q4 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q5 \u2212 Q"}, {"heading": "4 Finding the Dual Solution Path", "text": "From sentence 3.1 it follows that if we can find a series of quantil parameters: K: = {\u03c4k} Kk = 1, which divide the interval [0, 1] into regions so that within these regions \u03b1\u03c4 changes linearly with \u03c4, i.e. the index sentences: L, M and R remain unchanged. Then we can quickly recover \u03b1\u03c4 for any value of \u03c4 from an \u03b1\u03c4k via (17). In the following we present our algorithm (algorithm 1), which is able to identify all \u03c4k that we call kinks."}, {"heading": "4.1 The Algorithm", "text": "Our goal is to construct a sorted list of nodes at which one of the following events occurs: 1. Elements in N, i.e., not in M, move toward M, 2. Elements in M move toward L, 3. Elements in M move toward R, and the associated index sets to L, M, and R, we know from the definition (11) that we have all values toward M that change the affiliation to an index. This means that event 1 occurs when a > 0 deviation from Kiki becomes only an unequal element of the gradients to zero, i.e., we know with respect to iD (11) that the association iD (association) 6 = 0, and with respect to N. This means that event 1 occurs when a > 0 deviation from Kiki becomes only an unequal element of the gradients to zero, i.e., we, with respect to iD (association) = 0, ii."}, {"heading": "4.2 Complexity Analysis", "text": "The temporal complexity of algorithm 1 is dominated by the calculation of \"Q\" = \"Q\" (16), which involves solving a linear size system. A standard solver such as the conjugate gradient method converges to solving such a linear system in a maximum of O (r | M | 2) time, r is the rank of \"QMM.\" According to the calculation of \"M,\" the main costs of \"findingtoM\" (19) are the costs of matrix vector multiplication; and the algorithm \"1 Dual Path Finding\" (DPF) 1: data input {(xi, yi)} ni = 1 and regularization factor 2: output sorted list of kinks, corresponding dual solutions and index sets."}, {"heading": "5 Related Work", "text": "They have shown that solutions to an SVM training problem are a piecemeal linear function of \u03bb. Based on this observation, they have proposed an algorithm that finds SVM solutions for all values of \u03bb. Therefore, the regulatory constant controls the balance between the regulatory concept and the empirical risk in the objective function (1), in order to prevent a classifier from overadjusting the training data. Therefore, it plays an important role in improving the predictive accuracy on invisible data. The effect of the regulatory constant on the behavior of an SVM classifier is fundamentally different from the empirical risks in the objective function (1), in order to prevent overadjustment of the training data. Therefore, it plays an important role in improving the predictive accuracy on invisible data. The effect of the regulatory constant on the behavior of an SVM classifier is fundamentally different from the regulatory strategy in a sense that the trade-off between the positive TPR (true) and the false RPR (false) of the negative RR (false) of the positive Tasymmetric (false RR)."}, {"heading": "6 Experiments", "text": "We evaluate the generalization performance of a quantitative classifier for different values of \u03c4 and compare the temporal complexity of our DPF method (algorithm 1) with a state-of-the-art linear SVM classifier: LIBLINEAR version 1.32 (Hsieh et al., 2008). We used the LBFGSB method by Byrd et al. (1995) to solve the dual problem in step 3 of algorithm 1; and the conjugate gradient method was applied to find how the LBFGSB used the quasi-Newton method by Byrd et al. (1995) to solve the dual problem in step 3 of algorithm 1. (Asuncion and Newman, 2007), the spam dataset for task A of the ECML / PKDD 2006 discovery challenge, 2 and 3 \u00d7 104 m spleice samples from a biological dataset."}, {"heading": "7 Conclusions and Outlook", "text": "In this paper, we will first demonstrate that minimizing asymmetric hinged loss will result in a quantitative classifier that is optimal for asymmetric misclassification costs. Then, we will present an algorithm that basically finds the entire solution of a quantitative classifier. Considering the entire solution, we can very efficiently construct a classifier for any given asymmetric classification. Admittedly, our numerical experiments are preliminary. Repeated execution of conjugate gradients to determine phenomena is the biggest bottleneck in our DFP algorithm. We will investigate decomposition methods that can use warm starts to reduce the compression load. Future work will include expanding our algorithm to quantify regression and classification problems across multiple classes."}, {"heading": "Acknowledgements", "text": "NICTA is funded by the Australian Government's Backing Australia's Ability Programme and the Centre of Excellence. This work is also supported by the European Community's IST Programme within the FP7 Network of Excellence, ICT-216886-NOE."}, {"heading": "A Proof of Lemma 2.2", "text": "The proof Lx (f) is the risk caused by X = x: Lx (f) = \u03b2 (\u03b2) = E [((1 + Y) / 2 \u2212 \u2211 Y) (1 \u2212 Y f (X))) + | X \u2212 \u0430 = \u0430 (1 \u2212 \u03b7 (x)))) (1 + f (x))) + + (1 \u2212 \u03c4 (1 \u2212 f (x)) +, then we need only show that f \u00b2 (x) Lx (f) for each specified x. We show first that the minimizer f \u00b2 (x) then f \u00b2 (x) satisfies (x) = \u2212 1. Assumption not, that is, there is x0 such a situation (x0) < performance, but f \u00b2 (x0) 6 = \u2212 1. Let f \u00b2 (x) have the same quality as f \u00b2 (x), except that f \u00b2 f \u00b2 (x0) = \u2212 1. Use the abbreviation f \u00b2 (x0) = f \u00b2."}, {"heading": "C Proof of Proposition 3.1", "text": "Proof Let us assume that the index propositions (11) of \u03b1\u03c4 remain unchanged for all \u0432 (\u03c4k, \u03c4k + 1). The linearity of \u03b1\u03c4 in (\u03c4k, \u03c4k + 1) is directly derived from (17). Let us leave the index propositions (\u03c4k + 1 \u2212 \u03c4), calculate them (17), and let us select \u03c4k + 1 in such a way that the affiliation to an index i changes at its limit (17). This can only happen if \u03b1\u0442 + i takes its limit values: 0 or c \u0432 + yi with the number \"iD\" (\u03b1 +) = 0, which means that either an i-M is about to leave M, or an i / \u041aM simply passes into M, where M is the edge index set of \u043e\u0439. Let us now show that \u03b1\u0442 + is optimal. To show this, we just have to show that an i + i is optimal. By construction we have to show that an i-M is about to leave M, and an optimal as an index set of \u043e\u0439 is only an optimum as we have an index of K\u043eT."}], "references": [{"title": "A limited memory algorithm for bound constrained optimization", "author": ["R. Byrd", "P. Lu", "J. Nocedal", "C. Zhu"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "Byrd et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Byrd et al\\.", "year": 1995}, {"title": "Pattern Classification and Scene Analysis", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": null, "citeRegEx": "Duda et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Duda et al\\.", "year": 2001}, {"title": "The entire regularization path for the support vector machine", "author": ["T. Hastie", "S. Rosset", "R. Tibshirani", "J. Zhu"], "venue": "JMLR, 5:1391\u20131415,", "citeRegEx": "Hastie et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2004}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "In Proc. ACM Conf. Knowledge Discovery and Data Mining (KDD). ACM,", "citeRegEx": "Joachims.,? \\Q2006\\E", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Support vector machines and the bayes rule in classification", "author": ["Y. Lin"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "Lin.,? \\Q2002\\E", "shortCiteRegEx": "Lin.", "year": 2002}, {"title": "Algorithms for bound constrained quadratic programming problems", "author": ["J.J. Mor\u00e9", "G. Toraldo"], "venue": "Numerische Mathematik,", "citeRegEx": "Mor\u00e9 and Toraldo.,? \\Q1989\\E", "shortCiteRegEx": "Mor\u00e9 and Toraldo.", "year": 1989}, {"title": "Combining statistical learning with a knowledge-based approach - a case study in intensive care monitoring", "author": ["K. Morik", "P. Brockhausen", "T. Joachims"], "venue": "In Proc. Intl. Conf. Machine Learning,", "citeRegEx": "Morik et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Morik et al\\.", "year": 1999}, {"title": "Probabilities for SV machines", "author": ["J. Platt"], "venue": "Advances in Large Margin Classifiers,", "citeRegEx": "Platt.,? \\Q2000\\E", "shortCiteRegEx": "Platt.", "year": 2000}], "referenceMentions": [{"referenceID": 4, "context": "It has been shown (Lin, 2002) that the minimizer of the hinge loss is exactly sign(\u03b7(x)\u22121/2), where \u03b7(x) = P (Y = 1|X = x) is the probability of Y = 1 conditioned on X = x.", "startOffset": 18, "endOffset": 29}, {"referenceID": 7, "context": "To address such a problem, several methods have been proposed to convert the SVM output into well-calibrated probabilistic scores, such as (Platt, 2000).", "startOffset": 139, "endOffset": 152}, {"referenceID": 5, "context": "The dual problem is a quadratic problem with box constrains, which can be solved by various optimization techniques (see e.g., Byrd et al., 1995; Mor\u00e9 and Toraldo, 1989)", "startOffset": 116, "endOffset": 169}, {"referenceID": 2, "context": "Perhaps the closest in spirit to our paper is the work of Hastie et al. (2004), who studied the influence of the regularization constant \u03bb on the generalization performance of a binary SVM.", "startOffset": 58, "endOffset": 79}, {"referenceID": 6, "context": "Although SVM classifiers with built-in asymmetric misclassification costs have been applied to classification problems that are characterized by highly skewed training data and to problems arisen from medical diagnosis (Veropoulos et al., 1999; Morik et al., 1999; Grandvalet et al., 2006), no rigorous statistical properties were established.", "startOffset": 219, "endOffset": 289}, {"referenceID": 3, "context": "Hence, optimization software such as SVM (Joachims, 2006) and LIBLINEAR (Hsieh et al.", "startOffset": 41, "endOffset": 57}, {"referenceID": 0, "context": "We used the LBFGSB quasi-Newton method of Byrd et al. (1995) to solve the dual problem at Step 3 of the Algorithm 1; and the conjugate gradient method was applied to find \u2206\u03b1M at Step 13.", "startOffset": 42, "endOffset": 61}], "year": 2009, "abstractText": "A quantile binary classifier uses the rule: Classify x as +1 if P (Y = 1|X = x) \u2265 \u03c4 , and as \u22121 otherwise, for a fixed quantile parameter \u03c4 \u2208 [0, 1]. It has been shown that Support Vector Machines (SVMs) in the limit are quantile classifiers with \u03c4 = 1 2 . In this paper, we show that by using asymmetric cost of misclassification SVMs can be appropriately extended to recover, in the limit, the quantile binary classifier for any \u03c4 . We then present a principled algorithm to solve the extended SVM classifier for all values of \u03c4 simultaneously. This has two implications: First, one can recover the entire conditional distribution P (Y = 1|X = x) = \u03c4 for \u03c4 \u2208 [0, 1]. Second, we can build a risk-agnostic SVM classifier where the cost of misclassification need not be known apriori. Preliminary numerical experiments show the effectiveness of the proposed algorithm.", "creator": "TeX"}}}