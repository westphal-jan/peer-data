{"id": "1205.2606", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Exploring compact reinforcement-learning representations with linear regression", "abstract": "This paper presents a new algorithm for online linear regression whose efficiency guarantees satisfy the requirements of the KWIK (Knows What It Knows) framework. The algorithm improves on the complexity bounds of the current state-of-the-art procedure in this setting. We explore several applications of this algorithm for learning compact reinforcement-learning representations. We show that KWIK linear regression can be used to learn the reward function of a factored MDP and the probabilities of action outcomes in Stochastic STRIPS and Object Oriented MDPs, none of which have been proven to be efficiently learnable in the RL setting before. We also combine KWIK linear regression with other KWIK learners to learn larger portions of these models, including experiments on learning factored MDP transition and reward functions together.", "histories": [["v1", "Wed, 9 May 2012 18:40:40 GMT  (283kb)", "http://arxiv.org/abs/1205.2606v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["thomas j walsh", "istvan szita", "carlos diuk", "michael l littman"], "accepted": false, "id": "1205.2606"}, "pdf": {"name": "1205.2606.pdf", "metadata": {"source": "CRF", "title": "Exploring compact reinforcement-learning representations with linear regression", "authors": ["Thomas J. Walsh", "Istv\u00e1n Szita", "Carlos Diuk", "Michael L. Littman"], "emails": [], "sections": [{"heading": null, "text": "This paper introduces a new algorithm for linear online regression, whose efficiency meets the requirements of the KWIK (Knows What It Knows) Framework. The algorithm improves the complexity limits of the current state-of-the-art procedure in this environment. We examine several applications of this algorithm for learning compact reinforcement learning representations. We show that the linear regression of KWIK can be used to learn the reward function of a factored MDP and the probability of action outcomes in stochastic STRIPS and object-oriented MDPs, none of which has proven to be efficient in the RL environment. We also combine the linear regression of KWIK with other KWIK learners to learn larger parts of these models, including experiments for learning factored MDP transition and reward functions."}, {"heading": "1 Introduction", "text": "In this context, it should be noted that this is a very complex issue, a political project that aims to put the foundations of democracy and the rule of law first."}, {"heading": "2 KWIK Linear Regression", "text": "KWIK [11] (Knows What It Knows) is a framework for the study of supervised learning algorithms and is designed to standardize the analysis of model-based reinforcement learning algorithms. Formally, a KWIK learner works via an entrance room X and an output room Y (\"I don't know\"), which allows the learner to select the true text X and present it to the learner. If the learner can make an accurate prediction of this input, he can predict this, otherwise he must admit that he does not know by returning (\"I don't know\"), which allows him to see the true Yt or a noisy version. An algorithm is referred to as KWIK if and only if, with high probability (1 \u2212 3)."}, {"heading": "2.1 A New KWIK Linear Regression Algorithm", "text": "Suppose that a new query ~ x has little effect, and secondly that the fluctuations caused by the use of the system are very small. If we were able to solve the linear regression problem, it is difficult because: (1) if Dt is the solution of the smallest squares not unique and (2), even if we have a solution, we do not have information about their confidence. We can avoid the first problem by regulation, i.e. by augmenting the system with I\u03b8 = ~ v, where ~ v is some arbitrary vector. Regularization certainly distorts the solution, but this gives us a measure of confidence: if the distortion is large, the predictor should have low confidence and performance. On the other hand, if the distortion is low, it has two important consequences."}, {"heading": "2.2 Proof sketch of Theorem 2.1", "text": "The second term of the prediction error (1) is ~ xTQ\u03b8 \u043d, which can be limited by the first term of (1). (1) We now consider the first term of (1). (1) Fix a constant of (1). (1) Fix a constant of (1). (2) Fix a constant of (1). (2) Fix a constant of (1). (2) Fix a constant of (2). (2). (2). (2). Fix a constant of (2). (2). (2) Fix a constant of (2). (2). (2).). (2).).). We show that the traces of the Qt matrices are positive, decreasing monotonously, and decreasing when the number of (2). (Qt)."}, {"heading": "3 Application 1: Learning Rewards in a Factored-State MDP", "text": "A Markov decision-making process (MDP) [16] is characterized only by a quintuple (X, A, R, Q = Q, Q = 2), where X is a finite series of states; A is a finite series of actions; R: X \u00b7 A \u2192 R is the agent's reward function; P: X \u00b7 A \u00b7 X \u2192 [0, 1] is the transition function; and finally, the discount rate on future rewards is high. A: X \u00b7 A \u00b7 A \u00b7 R is the agent's reward function; P: X \u00b7 X \u2192 [0, 1] is the cartesian product of m smaller components: X = X1 \u00b7 X2 \u00b7 Xm. A function f is a local function when defined by a subspace X [Z] of the state where Z is a (probably smaller) index set."}, {"heading": "3.1 Experiments", "text": "We conducted reward experiments on the stock domain [14], with 3 sectors and 2 stocks per sector. Rewards were at the interval [0.5, 1.5] for owning a rising stock and randomly in [\u2212 1.5, \u2212 0.5] for owning a decreasing stock. We compared six algorithms: (1) algorithm 2; (2) algorithm 1, modified to output Rmax in unknown states; (3) the previous state-of-the-art KWIK-LR algorithm [15], modified to output Rmax in unknown states; (4) a flat tabular reward learner; and to demonstrate the need for efficient exploration; (5) linear regression without exploration; and (6) linear regression with epsilon-greedy exploration. Each algorithm was executed 20 times for 250 steps, with the model updated every 5 steps. For the stock = 1.2 and the algorithm = 1.2 for the domain (2) and we have (2) for the algorithm = 1.2 for the R2)."}, {"heading": "4 Learning Transition Probabilities", "text": "We will consider another novel application of linear regression of KWIK - learning action effects probabilities in environments where these effects can be ambiguous. (Specifically, one considers environments where actions act as stochastic action schemes (e.g. travel (X, Y) rather than travel (Paris, Rome) and the effects of these actions are stochastical.) The action (X, Y) can lead to the effects (Y) with probability. (Formally, any action that a = [DP] is derived from theform a = [(spectrum 0, p0) \u00b7 (spectrum n, pn)) in which each action is a possible effect. (When the action is taken, one of these effects occurs according to the probability distribution induced by the pis.) The schemes may also contain conditional effects whose nature is determined by the specific language (as discussed in the following sections)."}, {"heading": "4.1 Application 2: Stochastic STRIPS", "text": "STRIPS domains [8] consist of a set of objects O, a set of predicates P, and a set of actions A. The actions have conjunctive (via P) preconditions and effects specified by ADD and DELETE (DEL) lists, which determine which predicates are added and deleted from the world state when the action occurs. Stochastic STRIPS operators generalize this representation by looking at several possible action effects present by < ADD, DEL, PROB > tuples as in Table 1. Note that this representation is an example of the general action scheme defined above. While other similar operators have learned [12], they tried to learn the complete operators (including the structure) heuristically, and could not give any guarantees (as we do) for the behavior of their algorithm, nor do they identify an efficient algorithm for learning the probabilities as we do with algorithm 3. In order to make the planning effects well-defined by STRIPS, we are not familiar with the Stoic effects."}, {"heading": "4.2 Application 3: Stochastic Object Oriented RL", "text": "In fact, it is the case that one sees oneself in a position to be in, and that one sees oneself in a position to put oneself in a position to put oneself in a position, to put oneself in a position, to put oneself in a position, to put oneself in a position, to put oneself in a position, to put oneself in a position, to put oneself in a position, in a position to put oneself in the position in which they are in."}, {"heading": "5 Extensions", "text": "We will now discuss extensions to learn more about the compact representations discussed in this paper. We will outline how to learn complete fMDPs and larger parts of STRIPS and OOMDP models by combining KWIKLR with other KWIK learners, and provide empirical support in the fMDP case. We will also discuss a variant of KWIK-LR that can be used to learn stochastic plot outcomes."}, {"heading": "5.1 Combining with Other KWIK Learners", "text": "We follow this \"building blocks\" approach and show how to combine the fMDP reward learning algorithm (algorithm 2) with an implementation of the Noisy Union [11; 10] algorithm (which is also KWIK) to learn the transition structure, transition probabilities (P) and reward function (R) of an fMDP at once. The only knowledge given to the agent is the number of parents that a factor can have (| Z |) and the reward function structure, which results in the only algorithm to learn all these parameters efficiently to date. Experience is applied to both module algorithms in parallel. The reward learner gives an optimistic approximation of the reward function given to the Noisy Union, which then learns the transition structure and probabilities."}, {"heading": "5.2 Future Work: Learning Effects", "text": "Unfortunately, a relaxation of this assumption is unlikely in the stochastic case, since the effect-learning problem is known to be NP-heavy [12]. If the number of possible effects is very small or each of them is of constant magnitude, enumeration techniques could be applied. However, these assumptions are often violated, so that researchers have focused on heuristic solutions [12]. Here, we propose a novel heuristic that extends KWIK-LR for probability learning to the environment in which the entire action scheme (including \"a\") must be learned. We propose to apply a thriftification; we consider all possible effects and use KWIK-LR to learn its probabilities, with an additional restriction that the number of \"active\" (not zero) probabilities should be small. This minimization can be efficiently calculated by linear programming, and techniques such as slit generation can be used to compare these problems with most other heurical conditions."}, {"heading": "6 Related Work", "text": "Online linear regression has also been studied in the context of remorse minimization (see e.g. [1]). Applications for limited RL problems also exist [2], but with different limitations. Furthermore, the remorse analysis seems to lack the modularity (ability to combine different learners) of the KWIK framework. Previous work on linear regression for model-based RL focused on learning linear transition functions in continuous spaces. However, these approaches often lacked theoretical guarantees or constraints on the environment in terms of noise and reward structure [7]. In this work, we have improved both the current state of the art for linear regression in RL [15] and applied it in applications beyond conventional linear transition models. Our theoretical results and experiments illustrate the potential of KWIK-LR in model-based RL. In the future, we intend to identify other compact models that can enable this technique to be applied efficiently and implemented in model-based learning."}, {"heading": "Acknowledgements", "text": "We thank Alexander L. Strehl and the reviewers for their helpful comments. Support came from the Fullbright Foundation, DARPA IPTO FA8750-05-2-0249, FA8650-06-C-7606 and NSF IIS-0713435."}], "references": [{"title": "An improved on-line algorithm for learning linear evaluation functions", "author": ["P. Auer"], "venue": "In COLT,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Adaptive and self-confident on-line learning", "author": ["P. Auer", "N. Cesa-Bianchi", "C. Gentile"], "venue": "algorithms. JCSS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Least-squares temporal difference learning", "author": ["J.A. Boyan"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "R-MAX\u2013a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "JMLR, 3:213\u2013231,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "An object-oriented representation for efficient reinforcement learning", "author": ["C. Diuk", "A. Cohen", "M.L. Littman"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "The adaptive kmeteorologists problem and its application to structure learning and feature selection in reinforcement learning", "author": ["C. Diuk", "L. Li", "B. Leffler"], "venue": "In ICML,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "PAC adaptive control of linear systems", "author": ["C.-N. Fiechter"], "venue": "In COLT,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "STRIPS: A new approach to the application of theorem proving to problem solving", "author": ["R. Fikes", "N.J. Nilsson"], "venue": "Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1971}, {"title": "Efficient reinforcement learning in factored MDPs", "author": ["M.J. Kearns", "D. Koller"], "venue": "In IJCAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "A Unifying Framework for Computational Reinforcement Learning Theory", "author": ["L. Li"], "venue": "PhD thesis,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Knows what it knows: A framework for self-aware learning", "author": ["L. Li", "M.L. Littman", "T.J. Walsh"], "venue": "In ICML,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Learning symbolic models of stochastic domains", "author": ["H.M. Pasula", "L.S. Zettlemoyer", "L.P. Kaelbling"], "venue": "JAIR, 29:309\u2013352,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Improving action selection in MDP\u2019s via knowledge transfer", "author": ["A.A. Sherstov", "P. Stone"], "venue": "In AAAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Efficient structure learning in factored-state MDPs", "author": ["A.L. Strehl", "C. Diuk", "M.L. Littman"], "venue": "In AAAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Online linear regression and its application to model-based reinforcement learning", "author": ["A.L. Strehl", "M.L. Littman"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "The many faces of optimism: a unifying approach", "author": ["I. Szita", "A. L\u0151rincz"], "venue": "In ICML,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "The First Probabilistic Track of the International Planning Competition", "author": ["H.L.S. Younes", "M.L. Littman", "D. Weissman", "J. Asmuth"], "venue": "JAIR, 24:851\u2013887,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}], "referenceMentions": [{"referenceID": 15, "context": "While the field of Reinforcement Learning (RL) [16] has certainly made use of linear regression in approximating value functions [3], using online regression to learn parameters of a model has been limited to environments with linear dynamics (e.", "startOffset": 47, "endOffset": 51}, {"referenceID": 2, "context": "While the field of Reinforcement Learning (RL) [16] has certainly made use of linear regression in approximating value functions [3], using online regression to learn parameters of a model has been limited to environments with linear dynamics (e.", "startOffset": 129, "endOffset": 132}, {"referenceID": 6, "context": "[7]), and has often been unable to make guarantees about the behavior of the resulting learning agent without strict assumptions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "Recently, the introduction of the KWIK (Knows What It Knows) framework [11] has provided a characterization of sufficient conditions for a model-learning algorithm to induce sample-efficient behavior in a reinforcement-learning agent.", "startOffset": 71, "endOffset": 75}, {"referenceID": 14, "context": "One of the first algorithms developed for this framework was a KWIK linear regression algorithm [15], which was used to learn the transition function of an MDP with linear dynamics.", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Specifically, we use KWIK linear regression (KWIK-LR) to learn the reward function in a factored MDP and the transition probabilities in domains encoded using Stochastic STRIPS [12] or Object Oriented MDPs (OOMDP) [5].", "startOffset": 177, "endOffset": 181}, {"referenceID": 4, "context": "Specifically, we use KWIK linear regression (KWIK-LR) to learn the reward function in a factored MDP and the transition probabilities in domains encoded using Stochastic STRIPS [12] or Object Oriented MDPs (OOMDP) [5].", "startOffset": 214, "endOffset": 217}, {"referenceID": 10, "context": "KWIK [11] (Knows What It Knows) is a framework for studying supervised learning algorithms and was designed to unify the analysis of model-based reinforcement-learning algorithms.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "It has been shown [10] that in the model-based reinforcement-learning setting, if the underlying model learner is KWIK, then it is possible to build an RL agent around it by driving exploration of the \u22a5 areas using an R-max [4] style manipulation of the value function.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "It has been shown [10] that in the model-based reinforcement-learning setting, if the underlying model learner is KWIK, then it is possible to build an RL agent around it by driving exploration of the \u22a5 areas using an R-max [4] style manipulation of the value function.", "startOffset": 224, "endOffset": 227}, {"referenceID": 14, "context": "One of the first uses of the KWIK framework was in the analysis of an online linear regression algorithm used to learn linear transitions in continuous state MDPs [15].", "startOffset": 163, "endOffset": 167}, {"referenceID": 15, "context": "A Markov decision process (MDP) [16] is characterized by a quintuple (X, A,R, P, \u03b3), where X is a finite set of states; A is a finite set of actions; R : X \u00d7 A \u2192 R is the reward function of the agent; P : X\u00d7A\u00d7X \u2192 [0, 1] is the transition function; and finally, \u03b3 \u2208 [0, 1) is the discount rate on future rewards.", "startOffset": 32, "endOffset": 36}, {"referenceID": 0, "context": "A Markov decision process (MDP) [16] is characterized by a quintuple (X, A,R, P, \u03b3), where X is a finite set of states; A is a finite set of actions; R : X \u00d7 A \u2192 R is the reward function of the agent; P : X\u00d7A\u00d7X \u2192 [0, 1] is the transition function; and finally, \u03b3 \u2208 [0, 1) is the discount rate on future rewards.", "startOffset": 213, "endOffset": 219}, {"referenceID": 8, "context": "We make the standard assumption [9] that for each i there exist sets \u0393i of size O(1) such that ~xt+1[i] depends only on ~xt[\u0393i] and at.", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "Algorithms exist for learning transition probabilities [9] and dependency structure [14], but until now, no algorithm existed for learning the reward functions.", "startOffset": 55, "endOffset": 58}, {"referenceID": 13, "context": "Algorithms exist for learning transition probabilities [9] and dependency structure [14], but until now, no algorithm existed for learning the reward functions.", "startOffset": 84, "endOffset": 88}, {"referenceID": 3, "context": "We initialize all unknown rewards to some constant R0 (analogous to the common maximum reward parameter Rmax [4]).", "startOffset": 109, "endOffset": 112}, {"referenceID": 16, "context": "This property follows from standard arguments [17]: for unknown states, the noise term of the prediction error can be bounded by Azuma\u2019s inequality, and R0 can be set high enough so that the second term is positive and dominates.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "This form of optimistic initialization has proven consistently better than R-max in flat MDPs [17].", "startOffset": 94, "endOffset": 98}, {"referenceID": 10, "context": "1 we combine this algorithm with another KWIK learner that learns the transition dependency structure and probabilities [11] to learn the full fMDP model.", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "We carried out reward-learning experiments on the Stocks domain [14], with 3 sectors and 2 stocks per sector.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "We compared six algorithms: (1) Algorithm 2; (2) Algorithm 1 modified to output Rmax in unknown states; (3) the previous state-of-the-art KWIK-LR algorithm [15] modified to output Rmax in unknown states; (4) a flat tabular reward learner; and to demonstrate the need for efficient exploration, (5) linear regression without exploration and (6) linear regression with epsilon-greedy exploration.", "startOffset": 156, "endOffset": 160}, {"referenceID": 11, "context": "This form of generalization has been used to encode many different types of environments in RL, including stochastic STRIPS [12], Object Oriented MDPs (OOMDPs) [5], and typed dynamics [13].", "startOffset": 124, "endOffset": 128}, {"referenceID": 4, "context": "This form of generalization has been used to encode many different types of environments in RL, including stochastic STRIPS [12], Object Oriented MDPs (OOMDPs) [5], and typed dynamics [13].", "startOffset": 160, "endOffset": 163}, {"referenceID": 12, "context": "This form of generalization has been used to encode many different types of environments in RL, including stochastic STRIPS [12], Object Oriented MDPs (OOMDPs) [5], and typed dynamics [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 15, "context": "A modified version of value iteration [16] (Line 7) is then used to plan the optimal next action.", "startOffset": 38, "endOffset": 42}, {"referenceID": 7, "context": "STRIPS domains [8] are made up of a set of objects O, a set of predicates P , and a set of actions A.", "startOffset": 15, "endOffset": 18}, {"referenceID": 11, "context": "While others have looked at learning similar operators [12], their work attempted to heuristically learn the full operators (including structure), and could not give any guarantees (as we do) on the behavior of their algorithm, nor did they identify an efficient algorithm for learning the probabilities, as we have with Algorithm 3.", "startOffset": 55, "endOffset": 59}, {"referenceID": 17, "context": "To make the planning step well defined, we consider Stochastic STRIPS with rewards [18].", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "Object-oriented MDPs [5] consist of a set of objects O, a set of actions A that take elements of O as parameters, and (in the original deterministic description) a set of condition-effect pairs \u3008c, \u03c9\u3009 associated with each action.", "startOffset": 21, "endOffset": 24}, {"referenceID": 4, "context": "Previous work [5] presented an efficient algorithm for learning deterministic effects.", "startOffset": 14, "endOffset": 17}, {"referenceID": 10, "context": "Existing work [11] describes methods for combining simple KWIK agents to learn in increasingly com0 500 1000 1500 2000 2500 3000 3500 4000 \u22121000 0 1000 2000 3000 4000 5000 6000", "startOffset": 14, "endOffset": 18}, {"referenceID": 5, "context": "For instance, in stochastic STRIPS and OOMDPs, the preconditions of actions (STRIPS) or the conditional effects (OOMDP) can be learned using an existing KWIK adaptation of Noisy Union [6] as long as their size is bounded by some known constant.", "startOffset": 184, "endOffset": 187}, {"referenceID": 11, "context": "Unfortunately, relaxing this assumption in the stochastic case is unlikely, since the effect learning problem is known to be NP-Hard [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "But, these assumptions are often violated, so researchers have concentrated on heuristic solutions [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 11, "context": "It remains a matter for future work to compare this system to other heuristic solutions [12] for such problems.", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "[1]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "Applications to restricted RL problems also exist [2], but with different types of bounds.", "startOffset": 50, "endOffset": 53}, {"referenceID": 6, "context": "However, these approaches often lacked theoretical guarantees or placed restrictions on the environment regarding noise and the reward structure [7].", "startOffset": 145, "endOffset": 148}, {"referenceID": 14, "context": "In this paper, we have both improved on the current state-of-the-art algorithm for linear regression in RL [15], and used it in applications beyond the standard linear transition models.", "startOffset": 107, "endOffset": 111}], "year": 2009, "abstractText": "This paper presents a new algorithm for online linear regression whose efficiency guarantees satisfy the requirements of the KWIK (Knows What It Knows) framework. The algorithm improves on the complexity bounds of the current state-of-the-art procedure in this setting. We explore several applications of this algorithm for learning compact reinforcement-learning representations. We show that KWIK linear regression can be used to learn the reward function of a factored MDP and the probabilities of action outcomes in Stochastic STRIPS and Object Oriented MDPs, none of which have been proven to be efficiently learnable in the RL setting before. We also combine KWIK linear regression with other KWIK learners to learn larger portions of these models, including experiments on learning factored MDP transition and reward functions together.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}