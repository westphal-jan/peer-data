{"id": "1205.2611", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Ordinal Boltzmann Machines for Collaborative Filtering", "abstract": "Collaborative filtering is an effective recommendation technique wherein the preference of an individual can potentially be predicted based on preferences of other members. Early algorithms often relied on the strong locality in the preference data, that is, it is enough to predict preference of a user on a particular item based on a small subset of other users with similar tastes or of other items with similar properties. More recently, dimensionality reduction techniques have proved to be equally competitive, and these are based on the co-occurrence patterns rather than locality. This paper explores and extends a probabilistic model known as Boltzmann Machine for collaborative filtering tasks. It seamlessly integrates both the similarity and co-occurrence in a principled manner. In particular, we study parameterisation options to deal with the ordinal nature of the preferences, and propose a joint modelling of both the user-based and item-based processes. Experiments on moderate and large-scale movie recommendation show that our framework rivals existing well-known methods.", "histories": [["v1", "Wed, 9 May 2012 18:35:35 GMT  (255kb)", "http://arxiv.org/abs/1205.2611v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["tran the truyen", "dinh q phung", "svetha venkatesh"], "accepted": false, "id": "1205.2611"}, "pdf": {"name": "1205.2611.pdf", "metadata": {"source": "CRF", "title": "Ordinal Boltzmann Machines for Collaborative Filtering", "authors": ["Tran The Truyen", "Dinh Q. Phung", "Svetha Venkatesh"], "emails": ["t.tran2@curtin.edu.au", "d.phung@curtin.edu.au", "s.venkatesh@curtin.edu.au"], "sections": [{"heading": null, "text": "Collaborative Ltering is an effective recommendation technology where an individual's preference can potentially be predicted based on the preferences of other members. Early algorithms often relied on the strong locality in the preference data, i.e. it is sufficient to predict a user's preference for a particular object based on a small subset of other users with similar tastes or other objects with similar characteristics. More recently, dimensionality reduction techniques have proven to be equally competitive, and these are based on coexistence patterns rather than locality. This paper examines and expands a probabilistic model known as the Boltzmann machine for collaborative creeping tasks. It seamlessly integrates both the similarity and occurrence in principle. In particular, we examine parameterization options to deal with the ordinary nature of preferences, and propose a common modeling of both user and item-based processes based on our existing experiments prior to and large-scale experiments."}, {"heading": "1 INTRODUCTION", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "2 USER-CENTRIC MODELLING", "text": "In this section we present the most likely modeling from a single user perspective using Boltzmann machines (BMs)."}, {"heading": "2.1 ORDINAL FEATURES", "text": "In this paper, we consider the case where user preferences are expressed in the form of ordinal values, i.e., the set of weighted values S is a set of ordinal values, and let's mark it with S = {R1, R2,... Rn}. A simple approach is simply to ignore the ordinal property and treat the valuations as categorical variables. In particular, the input bias can simply be an identity function fs (ri) = I [ri \u2261 Rs], and the correlation feature can be treated as a similarity between the two adjacent valuations fb (ri, rj) = I [ri \u2261 rj]. Another option is to treat them as numerical values, for example as random Gaussian variables (after appropriate pre-processing, see Appendix A.2 for detailed treatment). However, the shortcoming is that this treatment is only useful if such a numerical interpretation of Rs exists. A better way is to express the ordinal characteristics > we get the rating expressed by Ru: If the rating is expressed by Ri..."}, {"heading": "2.2 LEARNING", "text": "The training data consists of weighting values for input variables. Let us refer to these evidences per user u (r) rather than r (u) to distinguish them from the unspecified r (u). Let us drop the index u (r) to provide clarity, and use the gradient in relation to the model parameters from which the L (r) -P (hk, 1 | r) -P (hk, 1) -P (r) -P (ri) -P (ri) -P (ri) -P (r) -P (r) -P (r) -P (r) -P (hk, 1 | r) -P (r) -P (r) -P (r) -P (ri) -P (r) -P (r) -k) -P (r) -ri-P (r) -P (r) (r) -P r (r) (P) -P r (r) -P r (r) -r (r) -P (r) -P (r) -r (r) -k (r) -k) -P (r) -P (r) -r (r) -P (r) -P (r) -P (r) -r (r) -P (r) -P (r) -P (r) -r -P (r) -P (r) -P (r -r) -P (r -r -r -P (r) -r -P (r) -P (r) -P (r) -P r -P (r) -r -P (r-r -r -P (r) -P (r) -P (r-r) -P (r-r-r-r) -P (r -P (r) -P (r-r-r-r) -P (r-r) -P (r -P (r-r-r) -P (r-r) -P (r-r) -P (r-r-r-r) -P (r-r) -P (r -P (r-r-r-r) -P (r-r-r) -P (r-r-r-r-r-r-r"}, {"heading": "2.3 RATING PREDICTION", "text": "Remember that invisible objects are not modeled during training, but are added as an additional, unobserved node in the visible plane during testing. Prediction of the new element j / I (u) is based on the MAP mapping 2r-J = arg max rj P (rj | r-J), where P (r-J) is the measure of predictive power. Given r-J, the model structure is based on a tree with root rj and leaves {hk} dk = 1. Thus, r-J can be evaluated in linear time. However, the prediction for online delivery is still expensive. Here, we suggest using a cheaper method based on intermediate approximation: P (rj, h-R-J) is the mathematical arrangement rj-K (hk-R-J)."}, {"heading": "2.4 ITEM RANKING", "text": "In a recommendation system, we are often interested in creating a recommendation list of items for each user. Essentially, this is a ranking problem, as we have to provide a numerical score for each item for a certain number of items and select an item with the highest rank. In our BMs framework, adding a new item j to the model can reduce the model state energy by approximately an amount of EQ (rj, r) (de ned in Equation 2). Remember that the probability of use in Equation 11It may seem that adding a new item can make the user model unspecified, but in fact, the item is already in other users \"models and the associated parameters have been learned.2Alternatively, we can use the expected rating as a prediction r-j = \u2211 rj P (rj | r-rj) rj. Improves when the model state reduces the energy that motivates us to use the Rj-rj (rj-rj) score."}, {"heading": "3 JOINT MODELLING OF USERS", "text": "In the previous section, we assumed that the ratings are generated by some user-centered process. (Since users and items play an equal role in the data, we can assume that there is some user-centered process that generates ratings.) This can be realized by combining these two modeling approaches into a single element, as shown speculatively in Figure 2.More, that each user and item is modeled with its own hidden level. (Let d \"the dimensionality of the hidden variables associated with items, there are Md + Kd\" hidden nodes in the common model (each rating is associated with two hidden levels, one per user and one per item). The number of input nodes is the number of ratings in the entire database."}, {"heading": "4 EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 SETTING", "text": "The second largest data set is extracted from the Net ix challenge5, as the first 3000 items are used, resulting in 208, 332 users and 13.6 million reviews. Ratings are integers in the 5-star scale. The two data sets include only those users who rated more than 20 movies, and these movies are rated by more than 20 users. For each user, about 80% of the ratings are used for training and the rest is used for evaluation. We implement three variants of the BMs: the categorical, the ordinal and the Gaussian. For the Gaussian BMs, we need to normalize the ratings to get random numbers that follow the standard normal distribution N (0; 1)."}, {"heading": "4.2 RATING PREDICTION", "text": "In the first series of experiments, we measure the performance of BM models based on the rating prediction task (Section 2.3).For comparison, we implement the Singular Value Decomposition (SVD) for incomplete data (see for example (Salakhutdinov et al., 2007) for a description).The SVD is currently one of the best methods for film predictions (7).The evaluation criterion is based on the popular Mean Absolute Error (MAE) measurement, i.e. MAE = 1 | r \u0445 J \u2212 r \u0432 j \u2212 j | / J. Figure 3 shows the learning curves of BM variants compared to the SVD, which are all evaluated on the 1M MovieLens dataset.The size of BM hidden layers and the rank of SVD are set at 20.The Gure clearly shows the positive effect of joint modelling, as well as the integration of dimensionality reduction and correlation."}, {"heading": "4.3 ITEM RANKING", "text": "In the second group of experiments, we evaluate the Ordinal BMs for the task of item ranking (Section 2,4). Let's remember that we first need a set of candidate items for each user. Here, we use the Pearson similarity between users, i.e. for each user, we select 50 most similar users, and then collect the items that those users previously rated. These items, except those previously rated by user u, are the candidates. For comparison, we evaluate the Ordinal BMs using a basic method of popularity, where the meaning of a candidate is based on the number of ratings by neighboring users. The methods are only tested on the MovieLens dataset, because the Net-ix data is not suitable for calculating user-based correlations. The evaluation criteria include the standard recall / precision measurements, and the ranking utility that is used by (Breese et et Utility, 1998) when the Utility is reduced to one half of its value, the Utility is adapted to the one half of its value."}, {"heading": "5 RELATED WORK", "text": "The Boltzmann machines examined in this paper are more general in that the original proposal in (Ackley et al., 1985) generalizes Restricted BMs instead of binary variables due to the use of general exponential family (see e.g. (Salakhutdinov et al., 2007). Work in (Salakhutdinov et al., 2007) applies Restricted BMs to collaborative ltering, but is limited to individual user modeling and categorical variables. Other graphic models have been used for collaborative ltering in a number of locations, including Bayesian networks (Breese et al., 1998) and dependency networks (Heckerman et al., 2001). BMs from Bayesian networks where BMs are undirected models leading Bayesian networks."}, {"heading": "6 CONCLUSION", "text": "We have introduced Boltzmann machines for collaborative ltering tasks. BMs are an expressive framework for integrating various aspects of the data, including the low-dimensional representation of item / user processes and the correlation between items / users. We examine parameterization for dealing with the ordinal nature of assessments and propose the integration of multiple BMs to jointly model user and item-based processes. We have shown empirically that BMs are competitive in the referral problem of movies. This work can be expanded in various ways. First, we need to gradually update parameters when new users or items are available. Second, it is about learning the structure of the BMs, including determining the number of hidden units and connectivity in the input layer. And third, the model should be expanded to include external information such as user pros and item content."}, {"heading": "A APPENDIX", "text": "It is not the first time that we are in a country in which we know the common potential of the system (h, r), as we know it (h, r). It is the first time that we understand the common potential of the system (h, r). It is the second time that we know the common goal of the LPL (r, r, r), which we know. It is the first time that we understand the common goal of the LPL (r, r), which we know. It is the first time that we know the common goal of the LPL (r, r, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i, i"}], "references": [{"title": "A learning algorithm for Boltzmann machines", "author": ["D. Ackley", "G. Hinton", "T. Sejnowski"], "venue": "Cognitive Science,", "citeRegEx": "Ackley et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Ackley et al\\.", "year": 1985}, {"title": "Unifying collaborative and content-based ltering", "author": ["J. Basilico", "T. Hofmann"], "venue": "In Proceedings of the 21st International Conference on Machine learning (ICML),", "citeRegEx": "Basilico and Hofmann,? \\Q2004\\E", "shortCiteRegEx": "Basilico and Hofmann", "year": 2004}, {"title": "Recommendation as classi cation: Using social and content-based information in recommendation", "author": ["C. Basu", "H. Hirsh", "W. Cohen"], "venue": "In Proceedings of the 15th National Conference on Arti cial Intelligence (AAAI),", "citeRegEx": "Basu et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Basu et al\\.", "year": 1998}, {"title": "Statistical analysis of non-lattice data", "author": ["J. Besag"], "venue": "The Statistician, 24(3),", "citeRegEx": "Besag,? \\Q1975\\E", "shortCiteRegEx": "Besag", "year": 1975}, {"title": "Learning collaborative information lters", "author": ["D. Billsus", "M. Pazzani"], "venue": "In Proceedings of the 15th International Conference on Machine Learning (ICML),", "citeRegEx": "Billsus and Pazzani,? \\Q1998\\E", "shortCiteRegEx": "Billsus and Pazzani", "year": 1998}, {"title": "Empirical analysis of predictive algorithms for collaborative ltering", "author": ["J. Breese", "D. Heckerman", "C Kadie"], "venue": "In Proceedings of the 14th Conference on Uncertainty in Arti cial Intelligence (UAI),", "citeRegEx": "Breese et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Breese et al\\.", "year": 1998}, {"title": "Dependency networks for inference, collaborative ltering, and data visualization", "author": ["D. Heckerman", "D. Chickering", "C. Meek", "R. Rounthwaite", "C. Kadie"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Heckerman et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 2001}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G. Hinton"], "venue": "Neural Computation,", "citeRegEx": "Hinton,? \\Q2002\\E", "shortCiteRegEx": "Hinton", "year": 2002}, {"title": "Latent semantic models for collaborative ltering", "author": ["T. Hofmann"], "venue": "ACM Transactions on Information Systems (TOIS), 22(1),", "citeRegEx": "Hofmann,? \\Q2004\\E", "shortCiteRegEx": "Hofmann", "year": 2004}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative ltering model", "author": ["Y. Koren"], "venue": "In KDD", "citeRegEx": "Koren,? \\Q2008\\E", "shortCiteRegEx": "Koren", "year": 2008}, {"title": "Modeling user rating pro les for collaborative ltering", "author": ["B. Marlin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Marlin,? \\Q2004\\E", "shortCiteRegEx": "Marlin", "year": 2004}, {"title": "GroupLens: An open architecture for collaborative ltering of netnews", "author": ["P. Resnick", "N. Iacovou", "M. Suchak", "P. Bergstorm", "J. Riedl"], "venue": "In Proceedings of ACM Conference on Computer Supported Cooperative Work,", "citeRegEx": "Resnick et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Resnick et al\\.", "year": 1994}, {"title": "Restricted Boltzmann machines for collaborative ltering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "In Proceedings of the 24th International Conference on Machine Learning (ICML),", "citeRegEx": "Salakhutdinov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2007}, {"title": "Item-based collaborative ltering recommendation algorithms", "author": ["B. Sarwar", "G. Karypis", "J. Konstan", "J. Reidl"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Sarwar et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sarwar et al\\.", "year": 2001}, {"title": "Exponential family harmoniums with an application to information retrieval", "author": ["M. Welling", "M. Rosen-Zvi", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Welling et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Welling et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 11, "context": "For example, if we want to predict how much the user likes a particular movie we may look for similar users who have rated the movie before (Resnick et al., 1994).", "startOffset": 140, "endOffset": 162}, {"referenceID": 13, "context": "Alternatively, the rating for this new movie can be based on ratings of other similar movies that the user has watched (Sarwar et al., 2001).", "startOffset": 119, "endOffset": 140}, {"referenceID": 12, "context": "More recent development has suggested that dimensionality reduction techniques like SVD (Salakhutdinov et al., 2007), PLSA (Hofmann, 2004) and LDA (Marlin, 2004) are also competitive.", "startOffset": 88, "endOffset": 116}, {"referenceID": 8, "context": ", 2007), PLSA (Hofmann, 2004) and LDA (Marlin, 2004) are also competitive.", "startOffset": 14, "endOffset": 29}, {"referenceID": 10, "context": ", 2007), PLSA (Hofmann, 2004) and LDA (Marlin, 2004) are also competitive.", "startOffset": 38, "endOffset": 52}, {"referenceID": 0, "context": "Speci cally, we explore the application of an undirected graphical model known as Boltzmann Machines (BMs) (Ackley et al., 1985) for the problem.", "startOffset": 107, "endOffset": 128}, {"referenceID": 9, "context": "Note that its probabilistic integration di ers from the current practice of blending multiple independent models (Koren, 2008).", "startOffset": 113, "endOffset": 126}, {"referenceID": 8, "context": "One way to deal with this issue is to approximate them by continuous variables as done in (Hofmann, 2004) but this is only meaningful for numerical ratings.", "startOffset": 90, "endOffset": 105}, {"referenceID": 7, "context": "Further, this paper studies approximate learning strategies for large BMs, including Contrastive Divergence (Hinton, 2002) (CD), a structural extension to Pseudo-Likelihood (Besag, 1975) (PL), and the combination of CD and PL for the joint model.", "startOffset": 108, "endOffset": 122}, {"referenceID": 3, "context": "Further, this paper studies approximate learning strategies for large BMs, including Contrastive Divergence (Hinton, 2002) (CD), a structural extension to Pseudo-Likelihood (Besag, 1975) (PL), and the combination of CD and PL for the joint model.", "startOffset": 173, "endOffset": 186}, {"referenceID": 7, "context": "We follow a sampling strategy called Contrastive-Divergence (CD) (Hinton, 2002), in that we start the sampling from the data distribution, and stop the random walks after a few steps.", "startOffset": 65, "endOffset": 79}, {"referenceID": 3, "context": "Another method is to utilise Pseudo-likelihood (PL) (Besag, 1975), and we approximate the model loglikelihood by", "startOffset": 52, "endOffset": 65}, {"referenceID": 13, "context": "see (Sarwar et al., 2001)), and keep only positively correlated pairs.", "startOffset": 4, "endOffset": 25}, {"referenceID": 12, "context": "For comparison, we implement the Singular Value Decomposition (SVD) for incomplete data (see, for example, (Salakhutdinov et al., 2007) for a description).", "startOffset": 107, "endOffset": 135}, {"referenceID": 5, "context": "includes the standard recall/precision measures, and the ranking utility adapted from (Breese et al., 1998).", "startOffset": 86, "endOffset": 107}, {"referenceID": 5, "context": "As suggested in (Breese et al., 1998), we choose \u03b1 = 5.", "startOffset": 16, "endOffset": 37}, {"referenceID": 0, "context": "The Boltzmann Machines explored in this paper are more general that the original proposal in (Ackley et al., 1985) due to the use of general exponential family instead of binary variables, in the same way that the Harmoniums (Welling et al.", "startOffset": 93, "endOffset": 114}, {"referenceID": 14, "context": ", 1985) due to the use of general exponential family instead of binary variables, in the same way that the Harmoniums (Welling et al., 2005) generalises the Restricted BMs (e.", "startOffset": 118, "endOffset": 140}, {"referenceID": 12, "context": "see (Salakhutdinov et al., 2007)).", "startOffset": 4, "endOffset": 32}, {"referenceID": 12, "context": "The work in (Salakhutdinov et al., 2007) applies Restricted BMs for collaborative ltering but it is limited to individual modelling of users and categorical variables.", "startOffset": 12, "endOffset": 40}, {"referenceID": 5, "context": "Other graphical models have been employed for collaborative ltering in a number of places, including Bayesian networks (Breese et al., 1998) and dependency networks (Heckerman et al.", "startOffset": 119, "endOffset": 140}, {"referenceID": 6, "context": ", 1998) and dependency networks (Heckerman et al., 2001).", "startOffset": 32, "endOffset": 56}, {"referenceID": 3, "context": "Our method resembles dependency networks when pseudo-likelihood (Besag, 1975) learning is employed and no hidden variables are modelled, but dependency networks are generally inconsistent.", "startOffset": 64, "endOffset": 77}, {"referenceID": 8, "context": "The dimensionality reduction capacity of the BMs is shared by other probabilistic models, including mixture models, probabilistic latent semantic analysis (PLSA) (Hofmann, 2004) and latent Dirichlet allocation (LDA) (Marlin, 2004).", "startOffset": 162, "endOffset": 177}, {"referenceID": 10, "context": "The dimensionality reduction capacity of the BMs is shared by other probabilistic models, including mixture models, probabilistic latent semantic analysis (PLSA) (Hofmann, 2004) and latent Dirichlet allocation (LDA) (Marlin, 2004).", "startOffset": 216, "endOffset": 230}, {"referenceID": 4, "context": "Machine learning (Billsus and Pazzani, 1998; Basu et al., 1998; Basilico and Hofmann, 2004) has also been successfully applied to the collaborative ltering problem.", "startOffset": 17, "endOffset": 91}, {"referenceID": 2, "context": "Machine learning (Billsus and Pazzani, 1998; Basu et al., 1998; Basilico and Hofmann, 2004) has also been successfully applied to the collaborative ltering problem.", "startOffset": 17, "endOffset": 91}, {"referenceID": 1, "context": "Machine learning (Billsus and Pazzani, 1998; Basu et al., 1998; Basilico and Hofmann, 2004) has also been successfully applied to the collaborative ltering problem.", "startOffset": 17, "endOffset": 91}, {"referenceID": 8, "context": "Since ratings are sometimes provided in a numerical scale, they can be approximated by continuous variables, as suggested in (Hofmann, 2004).", "startOffset": 125, "endOffset": 140}], "year": 2009, "abstractText": "Collaborative ltering is an e ective recommendation technique wherein the preference of an individual can potentially be predicted based on preferences of other members. Early algorithms often relied on the strong locality in the preference data, that is, it is enough to predict preference of a user on a particular item based on a small subset of other users with similar tastes or of other items with similar properties. More recently, dimensionality reduction techniques have proved to be equally competitive, and these are based on the co-occurrence patterns rather than locality. This paper explores and extends a probabilistic model known as Boltzmann Machine for collaborative ltering tasks. It seamlessly integrates both the similarity and cooccurrence in a principled manner. In particular, we study parameterisation options to deal with the ordinal nature of the preferences, and propose a joint modelling of both the user-based and item-based processes. Experiments on moderate and large-scale movie recommendation show that our framework rivals existing well-known methods.", "creator": "TeX"}}}