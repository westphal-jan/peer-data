{"id": "1205.2616", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Bisimulation-based Approximate Lifted Inference", "abstract": "There has been a great deal of recent interest in methods for performing lifted inference; however, most of this work assumes that the first-order model is given as input to the system. Here, we describe lifted inference algorithms that determine symmetries and automatically lift the probabilistic model to speedup inference. In particular, we describe approximate lifted inference techniques that allow the user to trade off inference accuracy for computational efficiency by using a handful of tunable parameters, while keeping the error bounded. Our algorithms are closely related to the graph-theoretic concept of bisimulation. We report experiments on both synthetic and real data to show that in the presence of symmetries, run-times for inference can be improved significantly, with approximate lifted inference providing orders of magnitude speedup over ground inference.", "histories": [["v1", "Wed, 9 May 2012 18:27:56 GMT  (250kb)", "http://arxiv.org/abs/1205.2616v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["prithviraj sen", "amol deshpande", "lise getoor"], "accepted": false, "id": "1205.2616"}, "pdf": {"name": "1205.2616.pdf", "metadata": {"source": "CRF", "title": "Bisimulation-based Approximate Lifted Inference", "authors": ["Prithviraj Sen", "Amol Deshpande", "Lise Getoor"], "emails": ["sen@cs.umd.edu", "amol@cs.umd.edu", "getoor@cs.umd.edu"], "sections": [{"heading": null, "text": "Recently, there has been a great deal of interest in methods for performing elevated inferences, but most of this work assumes that the first-order model is given as input into the system. At this point, we describe elevated inference algorithms that determine symmetries and automatically raise the probable model to accelerate inferences. In particular, we describe approximate elevated inference techniques that allow the user to trade inference accuracy for computational efficiency by using a handful of tunable parameters while the error remains limited. Our algorithms are closely related to the graph theory concept of bisimulation. We report on experiments with both synthetic and real data to show that in the presence of symmetries, the transit times for inferences can be significantly improved, with safe elevated inferences delivering orders of magnitude over ground inferences."}, {"heading": "1 Introduction", "text": "This year it is so far that it will be able to do the aforementioned for the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the ref the"}, {"heading": "2 Background: Exact Lifted Inference with the RV-Elim Graph", "text": "We start with a certain notation. Let X denote a random variable that can be assigned a value from a previously defined domain (each 1). Let f (X) specify a factor or a cliquenpotenzial that as arguments contains a set of random variables X = {X1,.. Xn}. We can define a common probability distribution over a set of random variables by multiplying all factors Pr (X) = 1Z. F (X f), so that X f. \"X.\" X. \"X.\" X. \"X.\" X. \"X.\" X. \"X.\" X. \"X.\" X. \"X.\" X. \"X.\" X. \"X.\". \"\" X. \"X.\". \"\" \"..\" \"..\".. \"..\".. \"..\".. \"..\".. \"..\".. \"..\".. \"..\". \"..\". \"..\".. \"..\".. \"..\". \"..\".. \".\".. \"..\". \"..\".. \".\".. \"..\".. \"..\".. \"..\". \"..\". \"..\".. \"..\".. \"..\".. \""}, {"heading": "3 Approximate Lifted Inference", "text": "While the above approach to performing accurate elevated inference can provide significant acceleration when the probability model contains moderate to large quantities of symmetry, in many cases we can do much better if we are willing to accept approximate values in the calculated marginal probability distributions. The main idea is to explore looser versions of Property 2.1 so that we can split the vertices of the rv-elim diagram into larger blocks and thus arrive at a smaller compressed rv-elim diagram. Below, we describe two separate and orthogonal generalizations of Property 2.1 that can be used to implement an approximate elevated inference. Afterwards, we discuss how to combine our techniques with constrained complexity inference algorithms, and finally, we discuss how to combine all of our suggested ideas into a approximate elevated elevated inference machine."}, {"heading": "3.1 Lifted Inference with Approximate Bisimulation", "text": "To introduce our first technique, we need some notation. Given a vertex, the edge label graph G = (V, E, LV, LE) is a simple distinction. (ms2) The edge label graph G = (V, ms2, LE) like an rv-elim graph, we leave v0 (and vice versa). We further say that a path or simply a path, l0 (l) l1 (l) l1 (l) 1). ln (l) n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n\" n \"n.\" We will now try to mark it in a way."}, {"heading": "3.2 Lifted Inference with Factor Binning", "text": "We start by having to deal with two other factors without actually being able to measure the \"distance\" between two factors belonging to the same block when the distance between them is zero. Note: the inversion is not true. This is possible for two internal depressions in the rv-elim graph that actually represent factors that include identical input-output mappings, but because their parents do not belong to the same blocks, or because the parent arguments do not overlap in the same way as they flow into the partition."}, {"heading": "3.3 Bounded Complexity Lifted Inference", "text": "The approximation techniques we have introduced so far do not alleviate the worst-case complexity of the factor i.e. In other words, these techniques would not help if the soil inference method is associated with high treewidth (along with structured probabilistic graphical models). Next, we will show how to include the mini bucket scheme [5], a limited complexity approximate (soil) inference algorithm, with our ideas. This allows us to maintain strict control over the complexity of the inference. The mini bucket scheme is a modification of the variable elimination algorithm [23] where at each step instead of eliminating a random variable, multiplying all the factors it appears as an argument, we develop a series of mini buckets each containing a (disjoint) subset of factors containing this variable as an argument and then dividing the variable from each mini bucket separately."}, {"heading": "3.4 Unified Lifted Inference Engine", "text": "By combining the various steps, it is possible to combine all the ideas we have presented in this section into a uniform, approximate inference motor. Our combined inference motor takes a set of eight parameters that define the combinations of techniques we want to apply (see Table 1). The experiments presented in the next section use this generic inference engine."}, {"heading": "4 Experimental Evaluation", "text": "We conducted experiments with synthetic and real data to determine how upscale inferences operate with approximate bisimulation and factor binning alone. We also report on experiments with our upscale inference machine, using both approaches in tandem. Each number we report is on average over 3 runs, our implementation is in JAVA, and our experiments were performed on a machine with a 3GHz Xeon processor and 3GB of RAM. We compare our results with two basic algorithms: a ground inference method, which is essentially variable elimination [23], so that we get all the marginals in a single pass, and the accurate upscale inference method, which is in Section 2. We report on two metrics for each experiment: runtimes generated by the different algorithms in seconds (time), and errors, measured by calculating the average number of marginal probabilities that are not within 10 \u2212 correct values (Inbs)."}, {"heading": "4.1 Synthetic Bayesian Network Generator", "text": "The generator creates BNs where the random variables are organized in layers and random variables from the first layer randomly select parents from the i \u2212 1st layer. For our experiments, we created BNs with 3 layers: The first layer contained 1000 random variables, 2,500 and 3,250. We introduced random variables for each variable in the first layer, every 25th layer before that was identical. The random variables in the last layer are our query variables, for which we calculated marginal probabilities. All random variables had a size 30 domain. To generate factors defining the dependence between random variables from the ith and the i \u2212 1st layer, we randomly selected 2 parents from the previous layer. Two children can choose the same parents, so that we do not generate tree-based structured BNs. All factors with children from the ith layer are identical. This problem is most likely to follow different probability factors across the different layers."}, {"heading": "4.2 Lifted Inference with Approximate Bisimulation", "text": "The results are shown in Figure 4 (a) and Figure 4 (d), which show that with increasing path length (x-axis in these diagrams), the time for inference (Figure 4 (a)) slowly increases, but the error decreases (Figure 4 (d)). The solid line with triangles shows the results of a raised inference with approximate bisimulation without a mini bucket, and with a path length set to 3 we see that the error is about 18%; the inference procedure took about 3 seconds, which is almost 3 times the acceleration over exact raised inference (which took 8.2 seconds) and almost 9 times the acceleration over ground inference (which took 25.95 sec). All other lines in the diagrams correspond to the raised inference with approximate Bible schemas. Among these mini bucket schemes, the inference is significantly higher than the non-hazardous point argument (3)."}, {"heading": "4.3 Lifted Inference with Factor Binning", "text": "The results are shown in Figure 4 (b) and Figure 4 (e). For these experiments, we used the mean square distance to compare two factors. Specifically, with two factors f1 and f2 having a common common domain D, the times for inferences decrease (Figure 4 (b)) and the error increases (Figure 4 (e)). In these experiments, the ground inference took about 33 seconds and the exact raised inference took 25.24 seconds, which means that factor binning without a mini bucket (fixed line with triangles) achieves an acceleration of about 3.5 times over exact raised inferences and an exact raised inference without a mini bucket (solid line with triangles)."}, {"heading": "4.4 Unified Lifted Inference Engine", "text": "In our last set of experiments, we used both approximate bisimulation (path length = 3) and factor binning (\u03b5 = 0.01) with mini buckets (limited by the argumentation number i = 3), and the results are shown in Figure 4 (c) and Figure 4 (f).As is clear from Figure 4 (c), all three inference procedures, ground inference, exact elevated inference and approximate elevated inference show an increase in runtime as the probabilistic model increases in size, but there is a size difference in time between elevated inference and exact elevated inference (which splits identical factors together) and another order of magnitude of elevated inference for exact elevated inference (which also holds nearly identical factors together), while keeping accuracy within limits."}, {"heading": "4.5 Experiments on Real-World Data", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5 Related Work", "text": "Poole [17] was one of the first to show that variable elimination [23] can be modified to work directly with first-rate representations of random variables and factors (or clique potentials) to avoid propositionalization. Subsequently, de Salvo Braz et al. [3] describe Poole's work further and refer to it as inversion elimination. They also introduce another method of inversion elimination that is more expensive than inversion elimination [4], but can be helpful in certain situations where the tree width of the soil model makes soil inferences unfeasible. It is easy to show that the bisimulation approach to reverse inversion elimination (and partial inversion [4]) involves inversion elimination (and partial inversion [4]), given the shape of the Y model Xi-i (Xi, Y) (all other factors are common), Xi-1 avoids the elimination of any complexity."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we described easy, universally applicable approximation algorithms for advanced conclusions based on the graph theory concept of bisimulation. Essentially, our techniques are workaround solutions for variable elimination [23] and can be used wherever variable elimination is applicable, including the calculation of conditional probabilities and MAP assignments (by switching from the sum product-oriented operator to the maximum product).An interesting way of future work is to look for other limited complexity inference algorithms (besides mini-buckets) that can be combined with the techniques introduced in this thesis. Other possibilities for future work are the automatic determination of the optimal values of the various parameters (path length and \u03b5) and the creation of the compressed RV-Elim diagram directly from the description of the first order of the probability model."}, {"heading": "Acknowledgements", "text": "This work was partially supported by NSF Grants No IIS0438866 and IIS-0546136. We would also like to thank the anonymous reviewers for their comments and suggestions and Parag Singla for sharing with us the Cora-ER MLN.References [1] http: / / www.cs.umass.edu / \u02dc mccallum / data / cora-refs.tar.gz. [2] J. Bar-Ilan, G. Kortsarz, and D. Peleg. How to allocate net-work centers. Journal of Algorithms, 1993. [3] R. de Salvo Braz, E. Amir, and D. Roth. Lifted first-orderprobabilistic inference. In IJCAI, 2005. R. de Salvo Braz, E. Amir, and D. Roth. MPE and partially in-version in lifted probabilistic variable elimination. In AAAI, 2006."}], "references": [{"title": "How to allocate network centers", "author": ["J. Bar-Ilan", "G. Kortsarz", "D. Peleg"], "venue": "Journal of Algorithms", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1993}, {"title": "Lifted first-order probabilistic inference", "author": ["R. de Salvo Braz", "E. Amir", "D. Roth"], "venue": "In IJCAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "MPE and partial inversion in lifted probabilistic variable elimination", "author": ["R. de Salvo Braz", "E. Amir", "D. Roth"], "venue": "In AAAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Mini-buckets: A general scheme for bounded inference", "author": ["R. Dechter", "I. Rish"], "venue": "Journal of the ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "A threshold of ln(n) for approximating set cover", "author": ["U. Feige"], "venue": "Journal of the ACM", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "editors", "author": ["L. Getoor", "B. Taskar"], "venue": "Introduction to Statistical Relational Learning. MIT Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Citeseer: An automatic indexing system", "author": ["C. Giles", "K. Bollacker", "S. Lawrence"], "venue": "ACM Digital Libraries", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Approximation schemes for covering and packing problems in image processing and VLSI", "author": ["D. Hochbaum", "W. Maass"], "venue": "Journal of the ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1985}, {"title": "An algorithmic approach to network location problems I: The p-centers", "author": ["O. Kariv", "S. Hakimi"], "venue": "SIAM Journal on Applied Mathematics", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1979}, {"title": "Graph triangulation: Algorithms giving small total state space", "author": ["U. Kjaerulff"], "venue": "Technical report, Univ. of Aalborg", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Automating the construction of internet portals with machine learning", "author": ["A. McCallum", "K. Nigam", "J. Rennie", "K. Seymore"], "venue": "Information Retrieval Journal", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Lifted probabilistic inference with counting formulas", "author": ["B. Milch", "L. Zettlemoyer", "K. Kersting", "M. Haimes", "L. Kaelbling"], "venue": "AAAI", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "SPOOK: A system for probabilistic object-oriented knowledge representation", "author": ["A. Pfeffer", "D. Koller", "B. Milch", "K. Takusagawa"], "venue": "UAI", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "Exploiting shared correlations in probabilistic databases", "author": ["P. Sen", "A. Deshpande", "L. Getoor"], "venue": "VLDB", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Collective classification in network data", "author": ["P. Sen", "G. Namata", "M. Bilgic", "L. Getoor", "B. Gallagher", "T. Eliassi-Rad"], "venue": "AI Magazine, 29(3)", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Lifted first-order belief propagation", "author": ["P. Singla", "P. Domingos"], "venue": "AAAI", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximation Algorithms", "author": ["V. Vazirani"], "venue": "Springer", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Generalized belief propagation", "author": ["J. Yedidia", "W. Freeman", "Y. Weiss"], "venue": "NIPS", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "A simple approach to bayesian network computations", "author": ["N. Zhang", "D. Poole"], "venue": "Canadian Conference on AI", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1994}], "referenceMentions": [{"referenceID": 1, "context": "While recent work in lifted inference [3, 15, 16, 17, 20] are promising steps towards developing efficient inference algorithms that can exploit the first-order structure provided by most first-order probabilistic models (see [8] for a survey), all of these techniques assume the first-order structure is provided in the input.", "startOffset": 38, "endOffset": 57}, {"referenceID": 11, "context": "While recent work in lifted inference [3, 15, 16, 17, 20] are promising steps towards developing efficient inference algorithms that can exploit the first-order structure provided by most first-order probabilistic models (see [8] for a survey), all of these techniques assume the first-order structure is provided in the input.", "startOffset": 38, "endOffset": 57}, {"referenceID": 12, "context": "While recent work in lifted inference [3, 15, 16, 17, 20] are promising steps towards developing efficient inference algorithms that can exploit the first-order structure provided by most first-order probabilistic models (see [8] for a survey), all of these techniques assume the first-order structure is provided in the input.", "startOffset": 38, "endOffset": 57}, {"referenceID": 15, "context": "While recent work in lifted inference [3, 15, 16, 17, 20] are promising steps towards developing efficient inference algorithms that can exploit the first-order structure provided by most first-order probabilistic models (see [8] for a survey), all of these techniques assume the first-order structure is provided in the input.", "startOffset": 38, "endOffset": 57}, {"referenceID": 5, "context": "While recent work in lifted inference [3, 15, 16, 17, 20] are promising steps towards developing efficient inference algorithms that can exploit the first-order structure provided by most first-order probabilistic models (see [8] for a survey), all of these techniques assume the first-order structure is provided in the input.", "startOffset": 226, "endOffset": 229}, {"referenceID": 13, "context": "In prior work [18], we showed how, using the symmetry present in the probabilistic model and methods closely related to the graph-theoretic concept of bisimulation, it is possible to compile a compressed version of the inference problem.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "\u2022 We review the results from [18] and show how they are applicable to general probabilistic graphical models.", "startOffset": 29, "endOffset": 33}, {"referenceID": 3, "context": ", mini-buckets [5]) \u2013 that allows us to extend the use of our techniques to domains with unbounded treewidth.", "startOffset": 15, "endOffset": 18}, {"referenceID": 13, "context": "In the next section, we review our earlier work on exact lifted inference [18], in Section 3 we present our techniques for approximate lifted inference, in Section 4 we evaluate our approaches on synthetic and real-world data, in Section 5 we review related work and we conclude with a few pointers for future work in Section 6.", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "This section reviews material from [18].", "startOffset": 35, "endOffset": 39}, {"referenceID": 0, "context": "2 1 {[1],[2,1,3],1}", "startOffset": 9, "endOffset": 16}, {"referenceID": 1, "context": "2 1 {[1],[2,1,3],1}", "startOffset": 9, "endOffset": 16}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c 1 a", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c 1 a", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 1, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 d", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 d", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2, 1,3],1}", "startOffset": 7, "endOffset": 15}, {"referenceID": 1, "context": "1 {[1],[2, 1,3],1}", "startOffset": 7, "endOffset": 15}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 1, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 0, "context": "{[1,2],[2],2} 1 a 2", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 1 a 2", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 1, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c [ fs1 , fs2 ] [ fi1 , fi2 , fi3 ] [ fs3 ]", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c [ fs1 , fs2 ] [ fi1 , fi2 , fi3 ] [ fs3 ]", "startOffset": 7, "endOffset": 10}, {"referenceID": 13, "context": "\u2217This is an example of a graphical model that may be constructed during query evaluation over probabilistic databases [18].", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "\u2022 begin constructing the label by going through each parent\u2019s arguments list and forming a tuple composed of the arguments\u2019 ids assigned in the previous step: since fs1 is the first multiplicand and fi1 the second, we form our label by concatenating \u201c[1]\u201d with \u201c[2,1,3]\u201d,", "startOffset": 262, "endOffset": 269}, {"referenceID": 1, "context": "\u2022 begin constructing the label by going through each parent\u2019s arguments list and forming a tuple composed of the arguments\u2019 ids assigned in the previous step: since fs1 is the first multiplicand and fi1 the second, we form our label by concatenating \u201c[1]\u201d with \u201c[2,1,3]\u201d,", "startOffset": 262, "endOffset": 269}, {"referenceID": 0, "context": "Thus, the complete label for ms1 is \u201c{[1], [2,1,3], 1}\u201d (Figure 2(a)).", "startOffset": 43, "endOffset": 50}, {"referenceID": 1, "context": "Thus, the complete label for ms1 is \u201c{[1], [2,1,3], 1}\u201d (Figure 2(a)).", "startOffset": 43, "endOffset": 50}, {"referenceID": 13, "context": "In [18] we observed that by adapting the graph-theoretic notion of bisimulation [11], one can determine the equivalence class partitioning of the vertices.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "One caveat about the above approach is that since factor Algorithm 1: Exact Bisimulation [18]", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "In [18], we proposed ordering the parents of a node using their assigned blockids before assigning it to a block.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "We then run a modified min-size heuristic [13] on the compressed graph, and replace the vertices with the corresponding sets of random variables to get an elimination order for inference.", "startOffset": 42, "endOffset": 46}, {"referenceID": 0, "context": "2 1 {[1],[2,1,3],1}", "startOffset": 9, "endOffset": 16}, {"referenceID": 1, "context": "2 1 {[1],[2,1,3],1}", "startOffset": 9, "endOffset": 16}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c 1 a", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c 1 a", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 1, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 d", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 d", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2, 1,3],1}", "startOffset": 7, "endOffset": 15}, {"referenceID": 1, "context": "1 {[1],[2, 1,3],1}", "startOffset": 7, "endOffset": 15}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "2 1 {[1],[2,1,3],1}", "startOffset": 9, "endOffset": 16}, {"referenceID": 1, "context": "2 1 {[1],[2,1,3],1}", "startOffset": 9, "endOffset": 16}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c 1 a", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c 1 a", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 1, "context": "1 {[1],[2,1,3],1}", "startOffset": 7, "endOffset": 14}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 d", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 d", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2, 1,3],1}", "startOffset": 7, "endOffset": 15}, {"referenceID": 1, "context": "1 {[1],[2, 1,3],1}", "startOffset": 7, "endOffset": 15}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 2 c", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "1 {[1],[2,1,3],1} [ms1 ,ms2 ]", "startOffset": 7, "endOffset": 14}, {"referenceID": 1, "context": "1 {[1],[2,1,3],1} [ms1 ,ms2 ]", "startOffset": 7, "endOffset": 14}, {"referenceID": 0, "context": "{[1,2],[2],2} 1 {[1],[2,1,3],1} [ms3 ] 1 a 1 b 2 c [ fs3 ] [ fi1 , fi2 , fi3 ] [ fs1 , fs2 ]", "startOffset": 1, "endOffset": 6}, {"referenceID": 0, "context": "{[1,2],[2],2} 1 {[1],[2,1,3],1} [ms3 ] 1 a 1 b 2 c [ fs3 ] [ fi1 , fi2 , fi3 ] [ fs1 , fs2 ]", "startOffset": 7, "endOffset": 10}, {"referenceID": 0, "context": "{[1,2],[2],2} 1 {[1],[2,1,3],1} [ms3 ] 1 a 1 b 2 c [ fs3 ] [ fi1 , fi2 , fi3 ] [ fs1 , fs2 ]", "startOffset": 21, "endOffset": 28}, {"referenceID": 1, "context": "{[1,2],[2],2} 1 {[1],[2,1,3],1} [ms3 ] 1 a 1 b 2 c [ fs3 ] [ fi1 , fi2 , fi3 ] [ fs1 , fs2 ]", "startOffset": 21, "endOffset": 28}, {"referenceID": 0, "context": "For instance in Figure 2(a) recall that ms1 \u223c=ms2 , which have the same set of incoming paths from the roots {\u201ca(1){[1], [2,1,3],1}\u2032\u2032,\u201cc(2){[1], [2,1,3],1}\u2032\u2032}, the matching vertex paths for ms1 are fs1 ,ms1 and fi1 ,ms1 , resp.", "startOffset": 121, "endOffset": 128}, {"referenceID": 1, "context": "For instance in Figure 2(a) recall that ms1 \u223c=ms2 , which have the same set of incoming paths from the roots {\u201ca(1){[1], [2,1,3],1}\u2032\u2032,\u201cc(2){[1], [2,1,3],1}\u2032\u2032}, the matching vertex paths for ms1 are fs1 ,ms1 and fi1 ,ms1 , resp.", "startOffset": 121, "endOffset": 128}, {"referenceID": 0, "context": "For instance in Figure 2(a) recall that ms1 \u223c=ms2 , which have the same set of incoming paths from the roots {\u201ca(1){[1], [2,1,3],1}\u2032\u2032,\u201cc(2){[1], [2,1,3],1}\u2032\u2032}, the matching vertex paths for ms1 are fs1 ,ms1 and fi1 ,ms1 , resp.", "startOffset": 145, "endOffset": 152}, {"referenceID": 1, "context": "For instance in Figure 2(a) recall that ms1 \u223c=ms2 , which have the same set of incoming paths from the roots {\u201ca(1){[1], [2,1,3],1}\u2032\u2032,\u201cc(2){[1], [2,1,3],1}\u2032\u2032}, the matching vertex paths for ms1 are fs1 ,ms1 and fi1 ,ms1 , resp.", "startOffset": 145, "endOffset": 152}, {"referenceID": 0, "context": "This sort of behaviour is, in fact, on display in our running example where \u03bci2 \u03bci3 simply because, of the three incoming paths to \u03bci3 , \u201cb(1){[1], [2,1,3],1}(1){[1,2], [2],2}\u2032\u2032 (matching fs3 ,ms3 ,\u03bci3 ) doesn\u2019t match any of \u03bci2 \u2019s incoming paths.", "startOffset": 148, "endOffset": 155}, {"referenceID": 1, "context": "This sort of behaviour is, in fact, on display in our running example where \u03bci2 \u03bci3 simply because, of the three incoming paths to \u03bci3 , \u201cb(1){[1], [2,1,3],1}(1){[1,2], [2],2}\u2032\u2032 (matching fs3 ,ms3 ,\u03bci3 ) doesn\u2019t match any of \u03bci2 \u2019s incoming paths.", "startOffset": 148, "endOffset": 155}, {"referenceID": 0, "context": "This sort of behaviour is, in fact, on display in our running example where \u03bci2 \u03bci3 simply because, of the three incoming paths to \u03bci3 , \u201cb(1){[1], [2,1,3],1}(1){[1,2], [2],2}\u2032\u2032 (matching fs3 ,ms3 ,\u03bci3 ) doesn\u2019t match any of \u03bci2 \u2019s incoming paths.", "startOffset": 162, "endOffset": 167}, {"referenceID": 0, "context": "This sort of behaviour is, in fact, on display in our running example where \u03bci2 \u03bci3 simply because, of the three incoming paths to \u03bci3 , \u201cb(1){[1], [2,1,3],1}(1){[1,2], [2],2}\u2032\u2032 (matching fs3 ,ms3 ,\u03bci3 ) doesn\u2019t match any of \u03bci2 \u2019s incoming paths.", "startOffset": 169, "endOffset": 172}, {"referenceID": 0, "context": "Note how, in this case, ms3 has been differentiated from ms1 and ms2 since ms3 has an incoming path \u201cb(1){[1], [2,1,3],1}\u2032\u2032 (matching fs3 ,ms3 ) of length 1 which doesn\u2019t match any incoming 1-length path of ms1 or ms2 .", "startOffset": 111, "endOffset": 118}, {"referenceID": 1, "context": "Note how, in this case, ms3 has been differentiated from ms1 and ms2 since ms3 has an incoming path \u201cb(1){[1], [2,1,3],1}\u2032\u2032 (matching fs3 ,ms3 ) of length 1 which doesn\u2019t match any incoming 1-length path of ms1 or ms2 .", "startOffset": 111, "endOffset": 118}, {"referenceID": 4, "context": "Further, Feige [6] showed that DS is not approximable to a factor of (1\u2212o(1))ln(|V|) unless NP has \u201cslightly super-polynomial time\u201d algorithms (or NP \u2282 DTIME(nlog(log(|V|)))).", "startOffset": 15, "endOffset": 18}, {"referenceID": 16, "context": "This gives us an ln(|V|)-approximation algorithm [21].", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "FB is also equivalent to the \u03c1dominating set problem [2], which, in turn, is the converse of the classic k-center problem [12] where we are given a graph from which we need to choose a subset of k vertices so that their distance from the other vertices is minimized.", "startOffset": 53, "endOffset": 56}, {"referenceID": 8, "context": "FB is also equivalent to the \u03c1dominating set problem [2], which, in turn, is the converse of the classic k-center problem [12] where we are given a graph from which we need to choose a subset of k vertices so that their distance from the other vertices is minimized.", "startOffset": 122, "endOffset": 126}, {"referenceID": 7, "context": "For instance, for euclidean spaces, near-optimal factor binning is possible [10], especially when the factor sizes are not large.", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "Next we show how to incorporate the mini-bucket scheme [5], a bounded complexity approximate (ground) inference algorithm, with our ideas.", "startOffset": 55, "endOffset": 58}, {"referenceID": 18, "context": "The mini-buckets scheme is a modification of the variable elimination algorithm [23] where at each step instead of eliminating a random variable by multiplying all factors it appears as argument in, one devises a set of mini-buckets each containing a (disjoint) subset of factors that contains that variable as argument and then eliminates the variable separately from each mini-bucket.", "startOffset": 80, "endOffset": 84}, {"referenceID": 3, "context": "Dechter and Rish [5] show how such a modification of the variable elimination algorithm provides an upper bound over the numbers produced in the resulting factors.", "startOffset": 17, "endOffset": 20}, {"referenceID": 18, "context": "We compare our results with two baseline algorithms: A ground inference procedure which is basically variable elimination [23] modified so that we obtain all marginals in a single pass, and the exact lifted inference procedure reviewed in Section 2.", "startOffset": 122, "endOffset": 126}, {"referenceID": 10, "context": "We first report results on the Cora [14] and CiteSeer [9] datasets.", "startOffset": 36, "endOffset": 40}, {"referenceID": 6, "context": "We first report results on the Cora [14] and CiteSeer [9] datasets.", "startOffset": 54, "endOffset": 57}, {"referenceID": 14, "context": "Note that, for these experiments, using the citations in the datasets we produce Markov networks with unbounded treewidth and then perform collective classification [19], so we compare against ground inference with mini-buckets restricted to 6 arguments.", "startOffset": 165, "endOffset": 169}, {"referenceID": 18, "context": "Poole [17] was one of the first to show that variable elimination [23] can be modified to directly work with first-order representations of random variables and factors (or clique potentials) to avoid propositionalization.", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "[3] further developed on Poole\u2019s work and referred to it as inversion elimination.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "It is straightforward to show that the bisimulation approach to lifted inference subsumes inversion elimination (and partial inversion [4]).", "startOffset": 135, "endOffset": 138}, {"referenceID": 15, "context": "In other related work, Singla and Domingos [20] propose an approach where they run a bisimulation-like algorithm on the factor graph representing the probabilistic model to find clusters of random variables that send and receive identical messages which helps speed up inference with loopy belief propagation (LBP) [22], a ground approximate inference algorithm.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "In other related work, Singla and Domingos [20] propose an approach where they run a bisimulation-like algorithm on the factor graph representing the probabilistic model to find clusters of random variables that send and receive identical messages which helps speed up inference with loopy belief propagation (LBP) [22], a ground approximate inference algorithm.", "startOffset": 315, "endOffset": 319}, {"referenceID": 18, "context": "Essentially, our techniques are wrap-arounds for variable elimination [23] and can be used whenever variable elimination is applicable, including computing joint conditional probabilities and MAP assignments (by switching from the sumproduct operator to max-product).", "startOffset": 70, "endOffset": 74}], "year": 2009, "abstractText": "There has been a great deal of recent interest in methods for performing lifted inference; however, most of this work assumes that the first-order model is given as input to the system. Here, we describe lifted inference algorithms that determine symmetries and automatically lift the probabilistic model to speedup inference. In particular, we describe approximate lifted inference techniques that allow the user to trade off inference accuracy for computational efficiency by using a handful of tunable parameters, while keeping the error bounded. Our algorithms are closely related to the graph-theoretic concept of bisimulation. We report experiments on both synthetic and real data to show that in the presence of symmetries, run-times for inference can be improved significantly, with approximate lifted inference providing orders of magnitude speedup over ground inference.", "creator": "TeX"}}}