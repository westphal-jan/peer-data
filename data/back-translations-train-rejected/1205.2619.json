{"id": "1205.2619", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Regret-based Reward Elicitation for Markov Decision Processes", "abstract": "The specification of aMarkov decision process (MDP) can be difficult. Reward function specification is especially problematic; in practice, it is often cognitively complex and time-consuming for users to precisely specify rewards. This work casts the problem of specifying rewards as one of preference elicitation and aims to minimize the degree of precision with which a reward function must be specified while still allowing optimal or near-optimal policies to be produced. We first discuss how robust policies can be computed for MDPs given only partial reward information using the minimax regret criterion. We then demonstrate how regret can be reduced by efficiently eliciting reward information using bound queries, using regret-reduction as a means for choosing suitable queries. Empirical results demonstrate that regret-based reward elicitation offers an effective way to produce near-optimal policies without resorting to the precise specification of the entire reward function.", "histories": [["v1", "Wed, 9 May 2012 18:23:30 GMT  (317kb)", "http://arxiv.org/abs/1205.2619v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kevin regan", "craig boutilier"], "accepted": false, "id": "1205.2619"}, "pdf": {"name": "1205.2619.pdf", "metadata": {"source": "CRF", "title": "Regret-based Reward Elicitation for Markov Decision Processes", "authors": ["Kevin Regan", "Craig Boutilier"], "emails": ["kmregan@cs.toronto.edu", "cebly@cs.toronto.edu"], "sections": [{"heading": null, "text": "Specifying a Markov Decision Process (MDP) can be difficult. Reward functionality is particularly problematic; in practice, it is often cognitively complex and time-consuming for users to pinpoint rewards precisely. This work raises the problem of defining rewards as preferences and aims to minimize the degree of precision with which a reward function needs to be set, while at the same time creating optimal or near-optimal strategies. We first discuss how robust strategies can be calculated for MDPs who receive only partial reward information based on the minimax remorse criterion. Then, we show how remorse can be reduced by efficiently determining reward information through tied queries, using remorse reduction as a means of selecting suitable queries. Empirical results show that reward determination based on remorse provides an effective way to develop near-perfect strategies without relying on the exact specifications."}, {"heading": "1 Introduction", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2 Notation and Problem Formulation", "text": "We are starting with the review of the Millennium Goals and the definition of the MDG minimax remorse criterion with imprecise rewards."}, {"heading": "2.1 Markov Decision Processes", "text": "We use ra and Pa to denote the obvious limitations of these measures. We define E as the nk \u00b7 n matrix with one row for each state action pair and one column per state, where Esa, t = Psa (t) and P are the n \u00b7 n transition matrix. We use ra and Pa to denote the obvious limitations of these measures. We define E as the nk \u00b7 n matrix with one row for each state action pair and one column per state, where Esa (t) denotes the n \u00b7 n transition matrix."}, {"heading": "2.2 Minimax Regret for Imprecise MDPs", "text": "A number of researchers have looked at the problem of inaccurately specified MDPs (see below). Here, we focus on solving the MDPs with inaccurate reward functions. Because it is difficult to fully specify reward functions, we are often confronted with the problem of incomplete reward specification. In fact, as we see below, we often wish to leave parts of the reward function unelicited (or otherwise unevaluated). Formally, we assume that the realizable reward R reflects current knowledge of the reward, which may refer to the previous limitations of a user or domain expert resulting from an elitization process (as discussed below); or limitations arising from observations of user behavior (as in the reverse RL [15]). In all of these situations, it is unlikely that we have complete reward information."}, {"heading": "2.3 Robust Optimization for Imprecise MDPs", "text": "Most robust optimization work for inaccurately specified MDPs adopts the maximin criterion and produces strategies with maximum safety level or worst-case value [1, 13, 14, 16]. Restricting attention to imprecise rewards, the maximin value is determined by: MMN (R) = max f-F min r-R \u00b7 f (7) Most models are defined for uncertainty in all MDP parameters, but algorithmic work focuses on uncertainty in the transition function, and the determination of information about transition functions or rewards is disregarded. Robust strategies can be calculated for insecure transition functions by using the maximin criterion by decompiling the problem over time steps and using dynamic programming and efficient suboptimization to find the worst-case transition function [1, 13, 16]. McMahan, Gordon and Blum [14] develop a linear approach to calculate maximum empirical value (we compare the MDP)."}, {"heading": "3 Minimax Regret Computation", "text": "This definition does not seem tenable, as it gives the opponent too much power by giving him a set of rewards that force a consistent selection of rewards in practice. (Assume viable reward is set, represented by a convex polytopic Cr \"s, which we assume will be bounded.) Restrictions on r\" arise \"as discussed above, elicitation or behavioral observation. Minimax remorse can then be expressed as the following Minimax program: Max g\" r \"fsubject to:\" E. \""}, {"heading": "4 Reward Elicitation", "text": "Similarly, the observed user behavior cannot be used to apply limitations to the reward function under the premise of user optimization. \"In this work, we focus on simple, limited questions, although our strategies can be adapted to more general questions. We assume that the reward gap between the upper and lower limits of user activity is given.\" Is r (s, a) b, \"where the upper and lower limits are between the upper and lower limits (s, a). While this requires a direct quantitative evaluation of the value / reward by the user, it can be reformulated as standard gambling [10], a device used in decision analysis to reduce it to preference query (one of which is stochastical)."}, {"heading": "5 Experiments", "text": "We evaluate the overall performance of our approach using a series of randomly generated MDPs and specific MDPs produced in an autonomous computer environment. We evaluate the scalability of our approach and the effectiveness of minimax regret as a driver of discovery. First, we look at randomly generated MDPs. We add a structure to the MDP by creating a semi-sparse transition function: for each (s, a) pair, logically achievable states are uniformly drawn and a Gaussian is used to generate transition probabilities. We use a uniform initial state distribution \u03b1 and a discount factor \u03b3 = 0.95. The true rewards of indifference (e.g. \"I am not sure\") can also be handled by setting boundaries within the query point."}, {"heading": "5.1 Computational Efficiency", "text": "To measure the performance of the minimax rue calculation, we first examine the method of constraint generation. Figure 1 shows the rue gap between the master problem value and the sub-problem value for each iteration compared to the time (in ms.) to achieve this iteration.The results are displayed for 20 randomly generated MDPs with ten states and five actions. Figure 2 shows how the minimax rue calculation time increases with the size of the MDP (5 actions, different number of states).Constraint generation using the MIP formulation scales superlinear, so the minimax rue calculation is only possible for small MDPs with this formulation; in comparison, linear relaxation is much more efficient."}, {"heading": "5.2 Approximation Error", "text": "In order to evaluate the linear relaxation scheme for maximum regret, we have generated random MDPs that vary the number of states. Figure 3 shows average relative errors over 100 runs. The approximation performs well and it is encouraging that the error does not increase with the size of the MDP. We also evaluate its impact on minimax remorse when used to generate violated constraints. Figure 3 also shows relative errors for minimax remorse to be small, well below 10% on average. 5CPLEX 11 is used for all MIPS and LPs, and all the code runs on a PowerEdge 2950 server with dual quad-core Intel E5355 CPUs.6Remarkably, the calculations shown here use initial reward uncertainty. As queries refine the reward pole, the calculation of regret will generally be faster. This has positive effects."}, {"heading": "5.3 Elicitation Effectiveness", "text": "In fact, most of them will be able to retaliate, while others will be able to retaliate and retaliate."}, {"heading": "6 Conclusions & Future Work", "text": "Minimax regret not only provides robust strategies in the face of reward uncertainty, but we have shown that it also enables you to draw attention to the most important aspects of the reward function. Although the calculation costs are significant, it is an extremely effective driver of reward uncertainty, thereby reducing the (more important) cognitive or computational costs of determining the reward. In addition, it makes sense to approach value and reward at all times. The somewhat preliminary nature of this work allows for many interesting directions for future research. Perhaps most interesting is the development of more informative and intuitive queries that capture the sequential nature of the reward problem. Direct comparison of strategies allows to distinguish value from reward, but is cognitively challenging. Trajector comparison differentiates value but can contain irrelevant details."}], "references": [{"title": "Solving uncertain Markov decision problems", "author": ["J. Bagnell", "A. Ng", "J. Schneider"], "venue": "Tech Report,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Partitioning procedures for solving mixedvariables programming problems", "author": ["J. Benders"], "venue": "Numerische Math.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1962}, {"title": "A POMDP formulation of preference elicitation problems", "author": ["C. Boutilier"], "venue": "AAAI-02, pp.239\u2013246, Edmonton", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Cooperative negotiation in autonomic systems using  incremental utility elicitation UAI-03", "author": ["C. Boutilier", "R. Das", "J.O. Kephart", "G. Tesauro", "W.E. Walsh"], "venue": "pp.89\u201397, Acapulco", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Decision theoretic planning: Structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "J. Artif. Intel. Res., 11:1\u201394", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Constraint-based optimization and utility elicitation using the minimax decision criterion", "author": ["C. Boutilier", "R. Patrascu", "P. Poupart", "D. Schuurmans"], "venue": "Artificial Intelligence, 170:686\u2013713", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Eliciting bid taker non-price preferences in (combinatorial) auctions", "author": ["C. Boutilier", "T. Sandholm", "R. Shields"], "venue": "AAAI-04, pp.204\u2013211, San Jose, CA", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Making rational decisions using adaptive utility elicitation", "author": ["U. Chajewska", "D. Koller", "R. Parr"], "venue": "AAAI-00, pp.363\u2013 369, Austin, TX", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Percentile optimization in uncertain markov decision processes with application to efficient exploration", "author": ["E. Delage", "S. Mannor"], "venue": "ICML-07, pp.225\u2013232, Corvalis, OR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Decision Theory: An Introduction to the Mathematics of Rationality", "author": ["S. French"], "venue": "Halsted Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning the structure of dynamic probabilistic networks", "author": ["N. Friedman", "K.Murphy", "S. Russell"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Efficient solution algorithms for factored mdps", "author": ["C. Guestrin", "D. Koller", "R. Parr", "S. Venkataraman"], "venue": "J. Artif. Intel. Res. 19:399-468", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Robust dynamic programming", "author": ["G. Iyengar"], "venue": "Mathematics of Operations Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Planning in the presence of cost functions controlled by an adversary", "author": ["H. McMahan", "G .Gordon", "A. Blum"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Algorithms for inverse reinforcement learning", "author": ["A. Ng", "S. Russell"], "venue": "ICML-00, pp.663\u2013670, Stanford, CA", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Robustness in Markov decision problems with uncertain transition matrices", "author": ["A. Nilim", "L. El Ghaoui"], "venue": "NIPS-03, Vancouver", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M. Puterman"], "venue": "Wiley, New York", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1994}, {"title": "The Foundations of Statistics", "author": ["L. Savage"], "venue": "Wiley", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1954}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Value-based policy teaching with active indirect elicitation", "author": ["H. Zhang", "D. Parkes"], "venue": "AAAI-08, pp.208\u2013214", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 10, "context": "For this reason, much work has been devoted to learning the dynamics of stochastic systems from transition data, both in offline [11] and online (i.", "startOffset": 129, "endOffset": 133}, {"referenceID": 18, "context": ", reinforcement learning) settings [19].", "startOffset": 35, "endOffset": 39}, {"referenceID": 9, "context": "As has been well-recognized in decision analysis, people find it extremely difficult to quantify their strength of preferences precisely using utility functions (and, by extension, reward functions) [10].", "startOffset": 199, "endOffset": 203}, {"referenceID": 4, "context": "Second, the requirement to assess rewards and costs for all states and actions imposes an additional burden (one that can be somewhat alleviated by the use of multiattributemodels in factoredMDPs [5]).", "startOffset": 196, "endOffset": 199}, {"referenceID": 5, "context": "Recent research in preference elicitation for non-sequential decision problems exploits the fact that optimal or nearoptimal decisions can often be made with relatively imprecise specification of a utility function [6, 8].", "startOffset": 215, "endOffset": 221}, {"referenceID": 7, "context": "Recent research in preference elicitation for non-sequential decision problems exploits the fact that optimal or nearoptimal decisions can often be made with relatively imprecise specification of a utility function [6, 8].", "startOffset": 215, "endOffset": 221}, {"referenceID": 5, "context": "Specifically, we adopt the minimax regret decision criterion [6, 18] and develop a formulation for MDPs: intuitively, this determines a policy that has minimum regret, or loss w.", "startOffset": 61, "endOffset": 68}, {"referenceID": 17, "context": "Specifically, we adopt the minimax regret decision criterion [6, 18] and develop a formulation for MDPs: intuitively, this determines a policy that has minimum regret, or loss w.", "startOffset": 61, "endOffset": 68}, {"referenceID": 0, "context": "Unlike other work on robust optimization for imprecisely specified MDPs, which focuses on the maximin decision criterion [1, 13, 14, 16], minimax regret determines superior policies in the presence of reward function uncertainty.", "startOffset": 121, "endOffset": 136}, {"referenceID": 12, "context": "Unlike other work on robust optimization for imprecisely specified MDPs, which focuses on the maximin decision criterion [1, 13, 14, 16], minimax regret determines superior policies in the presence of reward function uncertainty.", "startOffset": 121, "endOffset": 136}, {"referenceID": 13, "context": "Unlike other work on robust optimization for imprecisely specified MDPs, which focuses on the maximin decision criterion [1, 13, 14, 16], minimax regret determines superior policies in the presence of reward function uncertainty.", "startOffset": 121, "endOffset": 136}, {"referenceID": 15, "context": "Unlike other work on robust optimization for imprecisely specified MDPs, which focuses on the maximin decision criterion [1, 13, 14, 16], minimax regret determines superior policies in the presence of reward function uncertainty.", "startOffset": 121, "endOffset": 136}, {"referenceID": 16, "context": ", those satisfying [17]:", "startOffset": 19, "endOffset": 23}, {"referenceID": 16, "context": "where f = sup f rf [17].", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "These could reflect: prior bounds specified by a user or domain expert; constraints that emerge from an elicitation process (as discussed below); or constraints that arise from observations of user behavior (as in inverse RL [15]).", "startOffset": 225, "endOffset": 229}, {"referenceID": 17, "context": "We adopt theminimax regret criterion, originally suggested (though not endorsed) by Savage [18], and applied with some success in non-sequential decision problems [6, 7].", "startOffset": 91, "endOffset": 95}, {"referenceID": 5, "context": "We adopt theminimax regret criterion, originally suggested (though not endorsed) by Savage [18], and applied with some success in non-sequential decision problems [6, 7].", "startOffset": 163, "endOffset": 169}, {"referenceID": 6, "context": "We adopt theminimax regret criterion, originally suggested (though not endorsed) by Savage [18], and applied with some success in non-sequential decision problems [6, 7].", "startOffset": 163, "endOffset": 169}, {"referenceID": 5, "context": "Minimax regret has a variety of desirable properties relative to other robust decision criteria [6].", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "Compared to Bayesian methods that compute expected value using a prior overR [3, 8], minimax regret provides worst-case bounds on loss.", "startOffset": 77, "endOffset": 83}, {"referenceID": 7, "context": "Compared to Bayesian methods that compute expected value using a prior overR [3, 8], minimax regret provides worst-case bounds on loss.", "startOffset": 77, "endOffset": 83}, {"referenceID": 5, "context": "Finally, it has been shown to be a very effective criterion for driving elicitation in one-shot problems [6, 7].", "startOffset": 105, "endOffset": 111}, {"referenceID": 6, "context": "Finally, it has been shown to be a very effective criterion for driving elicitation in one-shot problems [6, 7].", "startOffset": 105, "endOffset": 111}, {"referenceID": 0, "context": "Most work on robust optimization for imprecisely specified MDPs adopts the maximin criterion, producing policies with maximum security level or worst-case value [1, 13, 14, 16].", "startOffset": 161, "endOffset": 176}, {"referenceID": 12, "context": "Most work on robust optimization for imprecisely specified MDPs adopts the maximin criterion, producing policies with maximum security level or worst-case value [1, 13, 14, 16].", "startOffset": 161, "endOffset": 176}, {"referenceID": 13, "context": "Most work on robust optimization for imprecisely specified MDPs adopts the maximin criterion, producing policies with maximum security level or worst-case value [1, 13, 14, 16].", "startOffset": 161, "endOffset": 176}, {"referenceID": 15, "context": "Most work on robust optimization for imprecisely specified MDPs adopts the maximin criterion, producing policies with maximum security level or worst-case value [1, 13, 14, 16].", "startOffset": 161, "endOffset": 176}, {"referenceID": 0, "context": "Robust policies can be computed for uncertain transition functions using the maximin criterion by decomposing the problem across time-steps and using dynamic programming and an efficient suboptimization to find the worst case transition function [1, 13, 16].", "startOffset": 246, "endOffset": 257}, {"referenceID": 12, "context": "Robust policies can be computed for uncertain transition functions using the maximin criterion by decomposing the problem across time-steps and using dynamic programming and an efficient suboptimization to find the worst case transition function [1, 13, 16].", "startOffset": 246, "endOffset": 257}, {"referenceID": 15, "context": "Robust policies can be computed for uncertain transition functions using the maximin criterion by decomposing the problem across time-steps and using dynamic programming and an efficient suboptimization to find the worst case transition function [1, 13, 16].", "startOffset": 246, "endOffset": 257}, {"referenceID": 13, "context": "McMahan, Gordon, and Blum [14] develop a linear programming approach to efficiently compute the maximin value of an MDP (we empirically compare this approach to ours below).", "startOffset": 26, "endOffset": 30}, {"referenceID": 8, "context": "Delage and Mannor [9] address the problem of uncertainty over reward functions (and transition functions) in the presence of prior information, using a percentile criterion, which can be somewhat less pessimistic than maximin.", "startOffset": 18, "endOffset": 21}, {"referenceID": 19, "context": "Zhang and Parkes ([20]) also adopt maximin in a model that assumes an inverse reinforcement learning setting for policy teaching.", "startOffset": 18, "endOffset": 22}, {"referenceID": 5, "context": "Following the formulations for non-sequential problems developed in [6, 7], we instead formulate the optimization using a series of linear (LPs) and mixed integer programs (MIPs) that enforce a consistent choice of reward across time.", "startOffset": 68, "endOffset": 74}, {"referenceID": 6, "context": "Following the formulations for non-sequential problems developed in [6, 7], we instead formulate the optimization using a series of linear (LPs) and mixed integer programs (MIPs) that enforce a consistent choice of reward across time.", "startOffset": 68, "endOffset": 74}, {"referenceID": 1, "context": "However, vertex enumeration is not feasible; so we apply Benders\u2019 decomposition [2] to iteratively generate constraints.", "startOffset": 80, "endOffset": 83}, {"referenceID": 14, "context": "Similarly, observed user behavior can be used to induce constraints on the reward function under assumptions of user \u201coptimality\u201d [15].", "startOffset": 130, "endOffset": 134}, {"referenceID": 9, "context": "While this appears to require a direct, quantitative assessment of value/reward by the user, it can be recast as a standard gamble [10], a device used in decision analysis to reduce this to preference query over two outcomes (one of which is stochastic).", "startOffset": 131, "endOffset": 135}, {"referenceID": 8, "context": "Unlike reward queries [9], which require a direct assessment of r(s, a), bound queries require only a yes-no response and are less cognitively demanding.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "We explore some simple myopic heuristic criteria that are very easy to compute, are based on criteria suggested in [6].", "startOffset": 115, "endOffset": 118}, {"referenceID": 13, "context": "We implemented a variation of the Double Oracle maximin algorithm developed by McMahan, Gordon & Blum [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "To further evaluate our approach we elicit the reward function for an autonomic computing scenario [4] in which we must allocate computing or storage resources to application servers as their client demands change over time.", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "Server i can respond only to queries about the utility it gets from a specific resource allocation level, and this requires intensive optimization and simulation on the part of the server [4]; hence minimizing the number of such queries is critical.", "startOffset": 188, "endOffset": 191}, {"referenceID": 19, "context": "Another direction for improving elicitation is to incorporate implicit information in a manner similar to policy teaching [20].", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "Inverse RL [15] can be also used to translate observed behavior into constraints on reward.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": "Some Bayesian models [6, 8] allow noisy query responses and adding this to our regret model is another important direction.", "startOffset": 21, "endOffset": 27}, {"referenceID": 7, "context": "Some Bayesian models [6, 8] allow noisy query responses and adding this to our regret model is another important direction.", "startOffset": 21, "endOffset": 27}, {"referenceID": 11, "context": "We are also exploring techniques that exploit factored MDP structure using LP approaches [12].", "startOffset": 89, "endOffset": 93}], "year": 2009, "abstractText": "The specification of a Markov decision process (MDP) can be difficult. Reward function specification is especially problematic; in practice, it is often cognitively complex and time-consuming for users to precisely specify rewards. This work casts the problem of specifying rewards as one of preference elicitation and aims to minimize the degree of precision with which a reward function must be specified while still allowing optimal or near-optimal policies to be produced. We first discuss how robust policies can be computed for MDPs given only partial reward information using the minimax regret criterion. We then demonstrate how regret can be reduced by efficiently eliciting reward information using bound queries, using regret-reduction as a means for choosing suitable queries. Empirical results demonstrate that regret-based reward elicitation offers an effective way to produce near-optimal policies without resorting to the precise specification of the entire reward function.", "creator": "dvips(k) 5.96 Copyright 2005 Radical Eye Software"}}}