{"id": "1205.2636", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Monolingual Probabilistic Programming Using Generalized Coroutines", "abstract": "Probabilistic programming languages and modeling toolkits are two modular ways to build and reuse stochastic models and inference procedures. Combining strengths of both, we express models and inference as generalized coroutines in the same general-purpose language. We use existing facilities of the language, such as rich libraries, optimizing compilers, and types, to develop concise, declarative, and realistic models with competitive performance on exact and approximate inference. In particular, a wide range of models can be expressed using memoization. Because deterministic parts of models run at full speed, custom inference procedures are trivial to incorporate, and inference procedures can reason about themselves without interpretive overhead. Within this framework, we introduce a new, general algorithm for importance sampling with look-ahead.", "histories": [["v1", "Wed, 9 May 2012 15:39:37 GMT  (131kb)", "http://arxiv.org/abs/1205.2636v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.PL cs.AI", "authors": ["oleg kiselyov", "chung-chieh shan"], "accepted": false, "id": "1205.2636"}, "pdf": {"name": "1205.2636.pdf", "metadata": {"source": "CRF", "title": "Monolingual Probabilistic Programming Using Generalized Coroutines", "authors": ["Oleg Kiselyov", "Chung-chieh Shan"], "emails": [], "sections": [{"heading": null, "text": "Probabilistic programming languages and modeling tools are two modular ways to build and reuse stochastic models and inference methods. By combining both strengths, we express models and conclusions as generalized coroutines in the same general purpose language. We leverage existing language possibilities, such as rich libraries, optimizing compilers, and types, to develop concise, declarative, and realistic models with competitive power to exact and approximate conclusions. In particular, a wide range of models can be expressed through memorization. As deterministic parts of models run at full speed, custom inference methods are trivial to integrate, and inference methods can think about themselves without interpretative overhead. Within this framework, we are introducing a new, generic predictive algorithm for importance tests."}, {"heading": "1 Introduction", "text": "Declarative programming is the division of what to do and how to do it in two modules that can be used separately from each other. In the case of probabilistic inference, that is what is the definition of a stochastic model and what is the implementation of an algorithm. Dividing the two makes it easier to understand the meaning of the model and the functioning of the algorithm, especially in complex domains where it is impossible to adapt an algorithm by hand coding."}, {"heading": "1.1 A simple example model", "text": "We begin to illustrate our monolingual approach with a tiny model (based on Figure 14.11 by Russell and Norvig 2003); on the left is an influence diagram in which each node represents a Boolean variable; on the right is a corresponding model, in the general language OCaml (Leroy et al. 2008), forcomputing Pr (rain | wet _ grass = true); this program uses the dist function, which represents a list of probability pairs at a randomly chosen value, and the function that a dummy argument fails and never returns because it observes an impossible event; both functions are ordinary OCaml values defined by our framework."}, {"heading": "1.2 The rest of this paper", "text": "In \u00a7 2, we introduce a larger example to show the increased expressivity achieved by storing stochastic models, deterministic calculations and inference procedures all in the same universal language. We analyze performance using the I / O setup of the language and then improve it by recursively invoking inferences from the model. In \u00a7 3, we describe the generalized coroutin facility that transfers control between the model and the inference procedure, allowing us to mature a model into a tree of decisions. This maturation allows the elimination of buckets and, in \u00a7 4, a new algorithm to capture meaning. We describe our competitive inference performance using realistic models (Jaeger et al. 2007; Pepper 2007b). We discuss related work in \u00a7 5. Our code is available at http: / / okmij.org / ftp / kakuritu /."}, {"heading": "2 Expressivity", "text": "We use Jaeger et al.'s (2007) hidden Markov Model (HMM) benchmark to further illustrate how the expressivity of a universal language helps us to write clear, fast codes. HMM is a one-dimensional random run with 8 states: 0 1 2 3 5 6 70.7 0.70.4 0.4 0.4 0.4 0.40.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.30.3The initial state is uniformly randomly selected. There are two observation symbols, L and R. The typical query is to determine the distribution of states by, say, 10 time steps, taking into account some previous observations."}, {"heading": "2.1 Types for knowledge representation", "text": "The model is specified by the number of states and the transition and observation probabilities. We represent states as integers and define a data type that better matches the observations L and R.type state = int let nstates = 8 type obs = L | RWhen states and observations become more complex, they should be represented by a structure like a tuple or object that better matches the problem. Our random variables can be of any type, whether custom like obs or built-in like bool, int, tuples, dictionaries and even functions. We can express distributions over values of all these types without encoding them as bit strings or numbers. Transitions are sparse, so that we store their probabilities compactly in an array transition _ prob of out-edge lists. For example, the array transition _ prob. (2) is the list [(0.4,2) and obst state: obst = observations."}, {"heading": "2.2 Higher-order functions for inference", "text": "We present observed evidence as a function that takes a state and time as arguments and may fail; the following function executes the model for n steps and returns the final state st, calling recursively to execute the first n-1 steps, then evolves to use the state transition used above in step n.let rec run = fun n evidence - > let st = if n = 1 then uniform nstateselse evolve (run (n-1) evidence) in evidence st; st (The function used above is defined in the sense of dist to derive examples from a discrete even distribution.) We can present the conditional query Pr (state 10 | Obs5 = L) as a thunkritical query 1, which, when called, is executed with a proof function that iff L is not observed, at the time 5: let query1 = fun () - > run 10 (fun st n - > if n = serve & obltst < > then L (fail)."}, {"heading": "2.3 I/O and self-interpretation for performance", "text": "Another way to express the models in a multi-purpose language is to increase performance with familiar tools in the language. For example, the library function Sys.time quickly determines that an exact conclusion from the run is exponential. Sys.time requires an exponential number of recursive calls, as any programmer can easily learn to use the OCaml profiler or add a line to increment a counter for each call. Sys.time function and the call counter are indistinguishable from other functions and integrators in the model, and the reason for the exponential time can be revealed with the OCaml profiler."}, {"heading": "2.4 Reasoning about inference procedures themselves", "text": "The ability of models to rely on a variety of inferences is unique in our monolingual approach. It is not only useful for performance, but also for thinking about inferences themselves, such as inferences from and about other actors that apply approximate inferences of their own. To illustrate this expressivity, we start with a trivial model: Choose a coin that is either fair or completely true with equal probability. Let's compare the probability that flipping the coin is true. What is the probability that p is at least 0.3? It is, of course, 1 because 0.5 \u2265 0.3 and 1 \u2265 0.3. In the model code below, the predicate is at least 0.3 true compared with the probability calculated in the probability tables."}, {"heading": "3 Reifying a model into a search tree", "text": "As shown above, we express models, including all observed evidence, such as sampling procedures that can fail. Even without checking the source code of models that are expressed this way, the sampling is simple: just to define a random selection and not to make an exception. In this section, we explain how to dist and how to support more efficient inference, and the result is that we can turn a model into a search tree of random decisions."}, {"heading": "3.1 Exploring random choices lazily", "text": "Because the model makes random decisions, it can get to the point where it becomes useless until it comes to a result."}, {"heading": "3.2 Generalizing coroutines to lightweight threads", "text": "POSIX processes and forks are rather heavy-duty devices used for probabilistic programming, and not all operating systems offer them. By comparison, user-level threads can be much more efficient; for example, Erlang programs routinely create millions of concurrent threads. User-level threads also facilitate memory management (unused threads can be collected as garbage) and prevent the final results from the model from flowing into the inference. Following Filinski (1994), our OCamel implementation uses a library (Felleisen et al.) and Filinski 1990, which generalizes coroutines and user threads, is Threads. (Library analogues are available for Haskell, Scheme, and some SML implementations.) The library offers two operations on the execution stack."}, {"heading": "4 Importance sampling with look-ahead", "text": "Considering a model that has been redefined as a search tree, we can only adapt the idea of repulsion by traversing the tree from the root to a leaf, using the probabilities given on each branch to make a random selection. If this traverse is so lucky that it does not fail, then the leaf that reaches it can be reported - that is, collected in a histogram - as an end result weighing 1.In many realistic models, the evidence observed is very unlikely, so the repulsion takes too long to produce enough samples. Import sampling (Fung and Chang 1990; Shachter and Peot 1990) is a known improvement. In order to realize the importance of sampling probability programs written in IBAL, Pepper (2007b) developed several sophisticated techniques that amount to call-byneed evaluation and evidence toward Choices.Pfeffer's techniques do not require an analysis of the source code because our own source code may not have it."}, {"heading": "4.1 Data structures with stochastic components", "text": "To facilitate looking ahead in our Importance Sampling Algorithm, any random selection must be observed shortly after it is made, and unnoticed random decisions should not be made at all. Therefore, our models should be encoded using lazy evaluation or Pfeffer's Delayed Evaluation (2007b). As shown in Section 1.1, lazy evaluation can be expressed in the form of memoirs. In addition, a composite data structure such as a tuple or a list should undergo a lazy evaluation of each of its components separately, so that, for example, two lists of independent coin changes can be appended without actually determining one of the flips. To this end, we avoid the built-in list type of OCaml and define our own type of lazy lists, the components of which consist of memory-like tunks: Type \"a llist = unit - >\" a lcons = LNil | LCons of (unit - > \"a lappons\" (an appled \"a we can = unst)."}, {"heading": "4.2 Inference performance on realistic models", "text": "This year it is so far that it will be able to do the aforementioned for the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green for the green the green the green the green the green the green the green the green the green the green the green the green the green the green the green"}, {"heading": "5 Related work", "text": "Our work is distinguished and motivated by the newfound expressiveness and power that is afforded by writing models and inferences procedures in the same language, especially for deterministic code. All our code is written in OCaml. In contrast, the previous modeling toolkits and probabilistic languages are not implemented in languages that handle them. That is, they are not self-interpreters. For example: \u2022 The Bayes Net Toolbox (Murphy 2007a) is a MATLAB library, but their models are not expressed in MATLAB with margin, so they cannot think about themselves. \u2022 IBAL (Pepper 2007a) is implemented in OCaml, but it cannot argue about OCaml code as itself. \u2022 Church (Goodman et al. 2008) and Probabilistic Scheme (Radul 2007) are both on Scheme and implemented with variable state, but they cannot think about their own implementations as about their own."}, {"heading": "Acknowledgments", "text": "We thank Olivier Danvy, Noah D. Goodman, Michael L. Littman, Vikash K. Mansinghka, Avi Pfeffer, Daniel Roy, Stuart Russell and Matthew Stone for the discussions."}], "references": [{"title": "Abstracting control", "author": ["Danvy", "Olivier", "Andrzej Filinski."], "venue": "Lisp and functional programming, 151\u2013160.", "citeRegEx": "Danvy et al\\.,? 1990", "shortCiteRegEx": "Danvy et al\\.", "year": 1990}, {"title": "Hierarchical Bayes compiler", "author": ["Daum\u00e9", "Hal", "III."], "venue": "http:// www.cs.utah.edu/~hal/HBC/.", "citeRegEx": "Daum\u00e9 et al\\.,? 2007", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2007}, {"title": "Bucket elimination: A unifying framework for probabilistic inference", "author": ["Dechter", "Rina."], "venue": "Learning in graphical models, ed. Michael I. Jordan. MIT Press.", "citeRegEx": "Dechter and Rina.,? 1999", "shortCiteRegEx": "Dechter and Rina.", "year": 1999}, {"title": "Markov logic: A unifying framework for statistical relational learning", "author": ["Domingos", "Pedro", "Matthew Richardson."], "venue": "Getoor and Taskar (2007), 339\u2013371.", "citeRegEx": "Domingos et al\\.,? 2007", "shortCiteRegEx": "Domingos et al\\.", "year": 2007}, {"title": "Beyond continuations", "author": ["Felleisen", "Matthias", "Daniel P. Friedman", "Bruce F. Duba", "John Merrill."], "venue": "Tech. Rep. 216, Computer Science Dept., Indiana Univ.", "citeRegEx": "Felleisen et al\\.,? 1987", "shortCiteRegEx": "Felleisen et al\\.", "year": 1987}, {"title": "Representing monads", "author": ["Filinski", "Andrzej."], "venue": "Principles of programming languages, 446\u2013457.", "citeRegEx": "Filinski and Andrzej.,? 1994", "shortCiteRegEx": "Filinski and Andrzej.", "year": 1994}, {"title": "AutoBayes: A system for generating data analysis programs from statistical models", "author": ["Fischer", "Bernd", "Johann Schumann."], "venue": "Journal of Functional Programming 13(3):483\u2013508.", "citeRegEx": "Fischer et al\\.,? 2003", "shortCiteRegEx": "Fischer et al\\.", "year": 2003}, {"title": "Weighing and integrating evidence for stochastic simulation in Bayesian networks", "author": ["Fung", "Robert", "Kuo-Chu Chang."], "venue": "UAI 5 (1989), 209\u2013220.", "citeRegEx": "Fung et al\\.,? 1990", "shortCiteRegEx": "Fung et al\\.", "year": 1990}, {"title": "Introduction to statistical relational learning", "author": ["Getoor", "Lise", "Ben Taskar", "eds"], "venue": null, "citeRegEx": "Getoor et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Getoor et al\\.", "year": 2007}, {"title": "Church: A language for generative models", "author": ["Goodman", "Noah D.", "Vikash K. Mansinghka", "Daniel Roy", "Keith Bonawitz", "Joshua B. Tenenbaum."], "venue": "UAI 24, 220\u2013229.", "citeRegEx": "Goodman et al\\.,? 2008", "shortCiteRegEx": "Goodman et al\\.", "year": 2008}, {"title": "Logic continuations", "author": ["Haynes", "Christopher T."], "venue": "Journal of Logic Programming 4(2):157\u2013176.", "citeRegEx": "Haynes and T.,? 1987", "shortCiteRegEx": "Haynes and T.", "year": 1987}, {"title": "Comparative evaluation of probabilistic logic languages and systems", "author": ["Jaeger", "Manfred", "Petr Lidman", "Juan L. Mateo."], "venue": "Proceedings of mining and learning with graphs. http://www.cs.aau.dk/~jaeger/plsystems/.", "citeRegEx": "Jaeger et al\\.,? 2007", "shortCiteRegEx": "Jaeger et al\\.", "year": 2007}, {"title": "Native delimited continuations in (bytecode) OCaml", "author": ["Kiselyov", "Oleg."], "venue": "http://okmij.org/ftp/Computation/ Continuations.html#caml-shift.", "citeRegEx": "Kiselyov and Oleg.,? 2006", "shortCiteRegEx": "Kiselyov and Oleg.", "year": 2006}, {"title": "Effective Bayesian inference for stochastic programs", "author": ["Koller", "Daphne", "David McAllester", "Avi Pfeffer."], "venue": "AAAI, 740\u2013747.", "citeRegEx": "Koller et al\\.,? 1997", "shortCiteRegEx": "Koller et al\\.", "year": 1997}, {"title": "The Objective Caml system, release", "author": ["Leroy", "Xavier", "Damien Doligez", "Jacques Garrigue", "Didier R\u00e9my", "J\u00e9r\u00f4me Vouillon"], "venue": null, "citeRegEx": "Leroy et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Leroy et al\\.", "year": 2008}, {"title": "Case-factor diagrams for structured probabilistic modeling", "author": ["McAllester", "David", "Michael Collins", "Fernando Pereira."], "venue": "Journal of Computer and System Sciences 74(1):84\u201396.", "citeRegEx": "McAllester et al\\.,? 2008", "shortCiteRegEx": "McAllester et al\\.", "year": 2008}, {"title": "BLOG: Probabilistic models with unknown objects", "author": ["Milch", "Brian", "Bhaskara Marthi", "Stuart Russell", "David Sontag", "Daniel L. Ong", "Andrey Kolobov."], "venue": "Getoor and Taskar (2007), 373\u2013398.", "citeRegEx": "Milch et al\\.,? 2007", "shortCiteRegEx": "Milch et al\\.", "year": 2007}, {"title": "Bayes Net Toolbox for Matlab", "author": ["Murphy", "Kevin."], "venue": "http: //www.cs.ubc.ca/~murphyk/Software/BNT/bnt.html.", "citeRegEx": "Murphy and Kevin.,? 2007a", "shortCiteRegEx": "Murphy and Kevin.", "year": 2007}, {"title": "The design and implementation of IBAL: A general-purpose probabilistic language", "author": ["Pfeffer", "Avi."], "venue": "Getoor and Taskar (2007), 399\u2013432.", "citeRegEx": "Pfeffer and Avi.,? 2007a", "shortCiteRegEx": "Pfeffer and Avi.", "year": 2007}, {"title": "Artificial intelligence: Foundations of computational agents", "author": ["Poole", "David", "Alan Mackworth."], "venue": "Cambridge Univ. Press.", "citeRegEx": "Poole et al\\.,? 2009", "shortCiteRegEx": "Poole et al\\.", "year": 2009}, {"title": "Report on the probabilistic language Scheme", "author": ["Radul", "Alexey."], "venue": "DLS \u201907: Proceedings of the 2007 symposium on dynamic languages, 2\u201310. New York: ACM Press.", "citeRegEx": "Radul and Alexey.,? 2007", "shortCiteRegEx": "Radul and Alexey.", "year": 2007}, {"title": "Artificial intelligence: A modern approach", "author": ["Russell", "Stuart", "Peter Norvig."], "venue": "2nd ed. Prentice-Hall.", "citeRegEx": "Russell et al\\.,? 2003", "shortCiteRegEx": "Russell et al\\.", "year": 2003}, {"title": "A glimpse of symbolic-statistical modeling by PRISM", "author": ["Sato", "Taisuke."], "venue": "Journal of Intelligent Information Systems 31(2): 161\u2013176.", "citeRegEx": "Sato and Taisuke.,? 2008", "shortCiteRegEx": "Sato and Taisuke.", "year": 2008}, {"title": "Simulation approaches to general probabilistic inference on belief networks", "author": ["Shachter", "Ross D.", "Mark A. Peot."], "venue": "UAI 5 (1989), 221\u2013234.", "citeRegEx": "Shachter et al\\.,? 1990", "shortCiteRegEx": "Shachter et al\\.", "year": 1990}], "referenceMentions": [{"referenceID": 14, "context": "To the right is a corresponding model, expressed as a program in the general-purpose language OCaml (Leroy et al. 2008), for KISELYOV & SHAN UAI 2009 285", "startOffset": 100, "endOffset": 119}, {"referenceID": 9, "context": "Thus, we can use memoization to express nonparametric models such as Dirichlet processes (Goodman et al. 2008).", "startOffset": 89, "endOffset": 110}, {"referenceID": 11, "context": "We describe our competitive inference performance on realistic models (Jaeger et al. 2007; Pfeffer 2007b).", "startOffset": 70, "endOffset": 105}, {"referenceID": 11, "context": "We use Jaeger et al.\u2019s (2007) hidden Markov model (HMM) benchmark to further illustrate how the expressivity of a general-purpose language helps us write clear, fast code.", "startOffset": 7, "endOffset": 30}, {"referenceID": 11, "context": "This exact inference strategy handles Jaeger et al.\u2019s benchmarks (2007) in at most a few seconds each.", "startOffset": 38, "endOffset": 72}, {"referenceID": 9, "context": "As with Goodman et al.\u2019s nested query (2008), the outer and inner models may each invoke fail to express observations at different levels.", "startOffset": 8, "endOffset": 45}, {"referenceID": 15, "context": "It is also possible to represent independent choices compactly using AND nodes in the search tree (McAllester et al. 2008); the use of self-interpretation to express bucket elimination in \u00a72.", "startOffset": 98, "endOffset": 122}, {"referenceID": 4, "context": "Following Filinski (1994), our OCaml implementation uses a library (Kiselyov 2006) of delimited control operators (Felleisen et al. 1987; Danvy and Filinski 1990), which generalize coroutines and user-level threads.", "startOffset": 114, "endOffset": 162}, {"referenceID": 16, "context": "Besides the music model, we reimplemented Milch et al.\u2019s model of radar blips for aircraft tracking (2007). In this model, a 10\u00d7 10 radar screen monitors a region in the air with an unknown number of planes that move and turn randomly.", "startOffset": 42, "endOffset": 107}, {"referenceID": 9, "context": "\u2022 Church (Goodman et al. 2008) and Probabilistic Scheme (Radul 2007) are both based on Scheme and implemented using Scheme with mutable state, but they cannot reason about their own implementations, such as about their own inference accuracy.", "startOffset": 9, "endOffset": 30}, {"referenceID": 16, "context": "This choice sets our monolingual approach apart from PRISM (Sato 2008), BLOG (Milch et al. 2007), AILog (Poole and Mackworth 2009), and Markov logic (Domingos and Richardson 2007).", "startOffset": 77, "endOffset": 96}], "year": 2009, "abstractText": "Probabilistic programming languages and modeling toolkits are two modular ways to build and reuse stochastic models and inference procedures. Combining strengths of both, we express models and inference as generalized coroutines in the same general-purpose language. We use existing facilities of the language, such as rich libraries, optimizing compilers, and types, to develop concise, declarative, and realistic models with competitive performance on exact and approximate inference. In particular, a wide range of models can be expressed using memoization. Because deterministic parts of models run at full speed, custom inference procedures are trivial to incorporate, and inference procedures can reason about themselves without interpretive overhead. Within this framework, we introduce a new, general algorithm for importance sampling with look-ahead.", "creator": "TeX"}}}