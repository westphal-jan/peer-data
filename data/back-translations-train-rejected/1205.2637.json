{"id": "1205.2637", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Counting Belief Propagation", "abstract": "A major benefit of graphical models is that most knowledge is captured in the model structure. Many models, however, produce inference problems with a lot of symmetries not reflected in the graphical structure and hence not exploitable by efficient inference techniques such as belief propagation (BP). In this paper, we present a new and simple BP algorithm, called counting BP, that exploits such additional symmetries. Starting from a given factor graph, counting BP first constructs a compressed factor graph of clusternodes and clusterfactors, corresponding to sets of nodes and factors that are indistinguishable given the evidence. Then it runs a modified BP algorithm on the compressed graph that is equivalent to running BP on the original factor graph. Our experiments show that counting BP is applicable to a variety of important AI tasks such as (dynamic) relational models and boolean model counting, and that significant efficiency gains are obtainable, often by orders of magnitude.", "histories": [["v1", "Wed, 9 May 2012 15:37:58 GMT  (382kb)", "http://arxiv.org/abs/1205.2637v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kristian kersting", "babak ahmadi", "sriraam natarajan"], "accepted": false, "id": "1205.2637"}, "pdf": {"name": "1205.2637.pdf", "metadata": {"source": "CRF", "title": "Counting Belief Propagation", "authors": ["Kristian Kersting", "Babak Ahmadi", "Sriraam Natarajan"], "emails": [], "sections": [{"heading": null, "text": "A major advantage of graphical models is that most of the knowledge is captured in the model structure. In this paper, we present a new and simple BP algorithm that takes advantage of such additional symmetries. Starting from a given factor graph, BP first constructs a compressed factor graph of cluster nodes and cluster factors, according to a series of nodes and factors that cannot be distinguished based on the evidence, and then executes a modified BP algorithm on the compressed graph that matches BP on the original factor graph. Our experiments show that counting BP applies to a variety of important AI tasks such as (dynamic) relative models and Boolean model counting, and that significant efficiencies can be achieved, often by orders of magnitude."}, {"heading": "1 Introduction", "text": "In this context, the key contribution is the introduction of BP, which represents an approach that often efficiently calculates symmetric properties of probability distributions. However, many graphical models generate inference problems with many symmetries that are not reflected in the graphical structure and are therefore not exploited by BP. One of the most prominent examples of this is primary and relational probability models such as Markov logic networks [14]. In addition to relational probability models, however, there are also traditional, i.e. propositional probability models that often produce inference problems with a lot of symmetries. In this paper, we will show this for the classical model, which has problems in calculating the number of solutions of a given proposition formula. This problem generalizes the NP complete problem of propositional satisfaction, and hence is both highly useful and extremely expensive to solve in practice."}, {"heading": "2 Related Work", "text": "This is because predictive factors are directly consistent with the superlatives we present in this paper. Singla and Domingos \"cancelled first order belief propagation (LFOBP) is built on [7] and also groups random variables, i.e. nodes that send and receive identical messages. However, CBP differs from LFOBP in two important respects. Firstly, CBP is conceptually simpler than LFOBP. This is noteworthy because efficient sequencing approaches for first-class and relational probability models are typically complex. Secondly, LFOBP requires the specification of the probability model as input in stored logical form. Only nodes with the same predicate can be grouped to form so-called supernodes. This means that LFOBP coincides with standard BP for propositional MLNs, i.e., MLNs that contain propositional variables."}, {"heading": "3 Belief Propagation", "text": "Let X = (X1, X2,.., Xn) is a set of n discretely weighted random variables and let xi represent the possible realizations of the Xi random variables. Graphic models compactly represent a common distribution over X as a product of factors [12], i.e., P (X = x) > 0 for all common configurations x, the distribution can be equivalent as a loglinear model: P (X = x) = 1Z exp [4], and Z is a normalization constant. As long as P (X = x) > 0 for all common configurations x, the distribution f x can be equivalent as a loglinear model: P (X = x) = 1Z exp [4 \u00b7 gi (x)], where the factors gi (x) are arbitrary functions of (a subset of) the configuration x.Graphical models can be represented as factor graphs."}, {"heading": "4 Counting Belief Propagation", "text": "Although we are already very efficient, many graphical models produce factor graphs with many symmetries that are not reflected in the graphical structure. Let's look at the factor graphics in Figure 1. The associated potentials are identical. In other words, although the factors involved on the surface are different, they actually share a whole range of information. In contrast, we cannot fall back on this information that we will present now. We will now discuss every step that uses fracture letters such as G, X and F to name compressed graphs."}, {"heading": "5 Dynamic Relational Domains", "text": "Traditionally, graphical models such as Bayesian dynamic networks [5] are used to represent uncertain processes over time. DBNs represent the state of the world as a set of variables and model the probable dependencies of variables within and between time steps. While DBNs can often provide compact representations, many real-world domains cannot be represented compactly with them: domains can contain several types of objects as well as several types of relationships among each other. Formalities that can represent objects and relationships, unlike purely random variables, have a long history in artificial intelligence. Significant advances have been made recently in combining them with a principled treatment of uncertainty [6, 2]. First-order probability models essentially combine first-order graphical models with elements of firmware logic by defining template factors (such as pool parameters [13]) that are applicable to whole groups of objects at the same time. [14] A simple and powerful language is Markov."}, {"heading": "5.1 Dynamic Markov Logic Networks", "text": "At a time when most people are able to understand and understand the world as it is, and as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it is, as it, as it is, as it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what it is, what is, what it is, what it is, what it is, what is, what it is, how it is, what it is, what it is, what is, what is,"}, {"heading": "5.2 Lifted First-Order Factored Frontier", "text": "In contrast to the case of static MLNs, however, we need approximation even for sparse models: Random variables correlate slightly over time due to common influences in the past. The classical approaches to performing approximate inference in DBNs are the Boyen-Koller (BK) algorithm [1] and Murphy and Wei\u00df \"Factored Frontier (FF) algorithm [10]. Both approaches have been shown to be equivalent to an iteration of BP, but on different diagrams [10]. However, BK is an exact inference that is extremely complex for probable logic models, has not been scaled to realistic ranges and has therefore only been applied to rather small artificial problems. In contrast, FF is a more aggressive approach. It is equivalent to (logic) BP on the regular factor diagram with a forward-facing message protocol: Each node first sends a message\" from left \"to\" and then \"to\" FF. \""}, {"heading": "5.3 Experimental Evaluation", "text": "We used the social network DMLN in Table 1 (below). There were 20 people in the domain who were most frequently used for the most commonly used method. Fractions of r are thought to be unknown for other friendship relationships. Cancer (x, t) is unknown for all persons x and all time steps. \"observed\" persons were randomly selected, but the query predicate was cancer. In the first experiment we investigated the compression ratio between standard FF and LFOFF in 10 and 15 time steps. Fig. 3 (left) shows the results for 10 time steps. Results for 15 were similar and were omitted here. As you can see, the size of the factor chart and the number of messages sent is significantly lower than the ability of LFOFF. In the second experiment we compared the \"forward\" message protocol with the \"flood method.\""}, {"heading": "6 Model Counting", "text": "Model counting is the classic problem of calculating the number of solutions to a given formula. It generalizes the NP-complete problem of Proposi-1http: / / alchemy.cs.washington.edu / tional satisfiability enormously and is therefore both highly useful and extremely expensive to solve in practice. Interesting applications include multi-agent reasoning, hostile reasoning and chart coloring, among others. In this section, we present a new approach to calculating a probable lower limit of model counting based on BP."}, {"heading": "6.1 Counting using Belief Propagation", "text": "Our approach, called CBPCOUNT, is based on BPCOUNT for the calculation of a probable lower limit based on the model number of a Boolean formula F recently introduced by Kroc et al. [8] The basic idea is to efficiently obtain a rough estimate of the \"margins\" of propositional variables using beliefs with attenuation. If this information is calculated accurately enough, it is sufficient to recursively count the number of solutions of only one of the \"F with u = true\" and \"F with u = false.\" Kroc et al. have shown empirically that BPCOUNT has good quality limits in an earlier sample-based method.BCOUNT works as follows."}, {"heading": "6.2 Experimental Evaluation", "text": "We have (C) BPCOUNT based on SAMPLECOUNT 2 with our own Python (C) BP implementation.2www.cs.cornell.edu / \u02dc sabhar / samplecount / We run BPCOUNT and CBPCOUNT on the circuit synthesis problem 2bitmax 6 with damping factor 0.5 and convergence threshold 10 \u2212 8. The formula has 192 variables, 766 clauses and a true number of 2.1 \u00d7 1029. The resulting factor has 192 variable nodes, and 1800 edges. The statistics of the running (C) BPCount is shown in Fig. 4. As can be seen, a significant improvement in efficiency is achieved when the marginal estimates are calculated using CBP instead of BP."}, {"heading": "7 Conclusions", "text": "The most important contribution of this paper is the introduction of BP counting, a novel, scalable approach to belief propagation. CBP constructs a compressed factor graph of cluster variables and cluster factors that corresponds to a set of nodes and factors that are indistinguishable in the face of evidence, and applies a modified belief propagation to these factor graphs. Two novel algorithms have been implemented for demanding AI tasks: a Lifted Factor Frontier algorithm for approximate inference in dynamic Markov logic networks, and an efficient approach to calculating a lower limit for model counting Boolean formulas. A number of experiments have shown that significant efficiency gains are achievable when counting BP instead of standard BP, often by orders of magnitude. This work suggests several lines of future work, such as approximate grouping nodes and factors, developing generalized modelling CBP variants (the use of BP relational variants)."}], "references": [{"title": "Tractable inference for complex stochastic processes", "author": ["X. Boyen", "D. Koller"], "venue": "Proc. of the Conf. on Uncertainty in Artificial Intelligence (UAI-98), pages 33\u201342", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1998}, {"title": "editors", "author": ["L. De Raedt", "P. Frasconi", "K. Kersting", "S.H. Muggleton"], "venue": "Probabilistic Inductive Logic Programming, volume 4911 of Lecture Notes in Computer Science. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Lifted First Order Probabilistic Inference", "author": ["R. de Salvo Braz", "E. Amir", "D. Roth"], "venue": "In Proc. of the 19th In-  ternational Joint Conference on Artificial Intelligence", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "MPE and Partial Inversion in Lifted Probabilistic Variable Elimination", "author": ["R. de Salvo Braz", "E. Amir", "D. Roth"], "venue": "In Proc. of the 21st AAAI Conf. on Artificial Intelligence", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "A Model for Reasoning about Persistence and Causation", "author": ["T. Dean", "K. Kanazawa"], "venue": "Computational Intelligence, 5:142\u2013150", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1989}, {"title": "editors", "author": ["L. Getoor", "B. Taskar"], "venue": "An Introduction to Statistical Relational Learning. MIT Press", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Templatebased inference in symmetric relational Markov random fields", "author": ["A. Jaimovich", "O. Meshi", "N. Friedman"], "venue": "Proc. of the Conf. on Uncertainty in Artificial Intelligence (UAI-07), pages 191\u2013199", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Leveraging Belief Propagation", "author": ["L. Kroc", "A. Sabharwal", "B. Selman"], "venue": "Backtrack Search, and Statistics for Model Counting. In Proc. of the 5th Int. Conf. on the Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems (CPAIOR-08), pages 127\u2013141", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Lifted Probabilistic Inference with Counting Formulas", "author": ["B. Milch", "L. Zettlemoyer", "K. Kersting", "M. Haimes", "L. Pack Kaelbling"], "venue": "In Proc. of the 23rd AAAI Conf. on Artificial Intelligence", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "The Factored Frontier Algorithm for Approximate Inference in DBNs", "author": ["K.P. Murphy", "Y. Weiss"], "venue": "Proc. of the Conf. on Uncertainty in Artificial Intelligence (UAI-01), pages 378\u2013385", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Loopy Belief Propagation for Approximate Inference: An Empirical Study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": "Proc. of the Conf. on Uncertainty in Artificial Intelligence (UAI-99), pages 467\u2013475", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1999}, {"title": "Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann, 2. edition", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1991}, {"title": "First-Order Probabilistic Inference", "author": ["D. Poole"], "venue": "Proc. of the 18th International Joint Conference on Artificial Intelligence (IJCAI-05), pages 985\u2013991", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Markov Logic Networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning, 62:107\u2013136", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Combining component caching and clause learning for effective model counting", "author": ["T. Sang", "F. Bacchus", "P. Beame", "H. Kautz", "T. Pitassi"], "venue": "Proc. of the 7th Int. Conf. on Theory and Applications of Satisfiability Testing (SAT-04)", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Exploiting Shared Correlations in Probabilistic Databases", "author": ["P. Sen", "A. Deshpande", "L. Getoor"], "venue": "Proc. of the Intern. Conf. on Very Large Data Bases (VLDB-08)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Lifted First-Order Belief Propagation", "author": ["P. Singla", "P. Domingos"], "venue": "In Proc. of the 23rd AAAI Conf. on Artificial Intelligence (AAAI-08),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}], "referenceMentions": [{"referenceID": 13, "context": "One of the most prominent examples are first-order and relational probabilistic models such as Markov logic networks [14].", "startOffset": 117, "endOffset": 121}, {"referenceID": 16, "context": "The closest work to CBP is the recent work by Singla and Domingos [17].", "startOffset": 66, "endOffset": 70}, {"referenceID": 6, "context": "Singla and Domingos\u2019s lifted first-order belief propagation (LFOBP) builds upon [7] and also groups random variables, i.", "startOffset": 80, "endOffset": 83}, {"referenceID": 15, "context": "[16] recently presented another \u201cclustered\u201d inference approach based on bisimulation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Others such as Poole [13], Braz et al.", "startOffset": 21, "endOffset": 25}, {"referenceID": 2, "context": "[3, 4], and Milch et al.", "startOffset": 0, "endOffset": 6}, {"referenceID": 3, "context": "[3, 4], and Milch et al.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "[9] have developed lifted versions of the variable elimination algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Graphical models compactly represent a joint distribution over X as a product of factors [12], i.", "startOffset": 89, "endOffset": 93}, {"referenceID": 10, "context": "Although this loopy belief propagation has no guarantees of convergence or of giving the correct result, in practice it often does, and can be much more efficient than other methods [11].", "startOffset": 182, "endOffset": 186}, {"referenceID": 16, "context": "The theorem generalizes the theorem of Singla and Domingos [17] but can essentially be proven along the same ways.", "startOffset": 59, "endOffset": 63}, {"referenceID": 4, "context": "Traditionally, graphical models such as dynamic Bayesian networks [5] have been used to represent uncertain processes over time.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "Recently, significant progress has been made in combining them with a principled treatment of uncertainty [6, 2].", "startOffset": 106, "endOffset": 112}, {"referenceID": 1, "context": "Recently, significant progress has been made in combining them with a principled treatment of uncertainty [6, 2].", "startOffset": 106, "endOffset": 112}, {"referenceID": 12, "context": "First-order probabilistic models essentially combine graphical models with elements of firstorder logic by defining template factors (such as Poole\u2019s parfactors [13]) that apply to whole sets of objects at once.", "startOffset": 161, "endOffset": 165}, {"referenceID": 13, "context": "A simple and powerful such language is Markov logic [14].", "startOffset": 52, "endOffset": 56}, {"referenceID": 16, "context": "Table 1: (Top) Example of a social network Markov logic network inspired by [17].", "startOffset": 76, "endOffset": 80}, {"referenceID": 0, "context": "Classical approaches to perform approximate inference in DBNs are the Boyen-Koller (BK) algorithm [1] and Murphy and Weiss\u2019s factored frontier (FF) algorithm [10].", "startOffset": 98, "endOffset": 101}, {"referenceID": 9, "context": "Classical approaches to perform approximate inference in DBNs are the Boyen-Koller (BK) algorithm [1] and Murphy and Weiss\u2019s factored frontier (FF) algorithm [10].", "startOffset": 158, "endOffset": 162}, {"referenceID": 9, "context": "Both approaches have been shown to be equivalent to one iteration of BP but on different graphs [10].", "startOffset": 96, "endOffset": 100}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "An exact counter such as CACHET [15] is called when the formula is sufficiently simplified.", "startOffset": 32, "endOffset": 36}], "year": 2009, "abstractText": "A major benefit of graphical models is that most knowledge is captured in the model structure. Many models, however, produce inference problems with a lot of symmetries not reflected in the graphical structure and hence not exploitable by efficient inference techniques such as belief propagation (BP). In this paper, we present a new and simple BP algorithm, called counting BP, that exploits such additional symmetries. Starting from a given factor graph, counting BP first constructs a compressed factor graph of clusternodes and clusterfactors, corresponding to sets of nodes and factors that are indistinguishable given the evidence. Then it runs a modified BP algorithm on the compressed graph that is equivalent to running BP on the original factor graph. Our experiments show that counting BP is applicable to a variety of important AI tasks such as (dynamic) relational models and boolean model counting, and that significant efficiency gains are obtainable, often by orders of magnitude.", "creator": "TeX"}}}