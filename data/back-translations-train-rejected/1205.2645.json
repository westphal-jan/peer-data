{"id": "1205.2645", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Distributed Parallel Inference on Large Factor Graphs", "abstract": "As computer clusters become more common and the size of the problems encountered in the field of AI grows, there is an increasing demand for efficient parallel inference algorithms. We consider the problem of parallel inference on large factor graphs in the distributed memory setting of computer clusters. We develop a new efficient parallel inference algorithm, DBRSplash, which incorporates over-segmented graph partitioning, belief residual scheduling, and uniform work Splash operations. We empirically evaluate the DBRSplash algorithm on a 120 processor cluster and demonstrate linear to super-linear performance gains on large factor graph models.", "histories": [["v1", "Wed, 9 May 2012 15:23:28 GMT  (505kb)", "http://arxiv.org/abs/1205.2645v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["joseph e gonzalez", "yucheng low", "carlos e guestrin", "david o'hallaron"], "accepted": false, "id": "1205.2645"}, "pdf": {"name": "1205.2645.pdf", "metadata": {"source": "CRF", "title": "Distributed Parallel Inference on Large Factor Graphs", "authors": ["Joseph E. Gonzalez", "Yucheng Low", "Carlos Guestrin"], "emails": ["jegonzal@cs.cmu.edu", "ylow@cs.cmu.edu", "guestrin@cs.cmu.edu", "david.ohallaron@intel.com"], "sections": [{"heading": null, "text": "With the proliferation of computer clusters and the growing size of AI problems, there is an increasing need for efficient parallel inference algorithms. We are looking at the problem of parallel inferences on large factor graphs in distributed memory setting of computer clusters. We are developing a new efficient parallel inference algorithm, DBRSplash, which includes over-segmented graph partitioning, residual scheduling, and uniform workflows. We are evaluating the DBRSplash algorithm empirically on a 120-processor cluster and showing linear to superlinear performance gains on large factor graph models."}, {"heading": "1 INTRODUCTION", "text": "In this context, it should be noted that the measures proposed by the government are not only measures, but also measures relating to the management of the problem, both in terms of managing the problem itself and in terms of managing the problem itself. (...) In this context, it should be noted that the measures taken in recent years are not measures taken in recent years, but rather measures taken in recent years. (...) In this context, it should be noted that the measures taken in recent years have primarily been taken by politics. (...) The measures taken in the last ten years have been taken by measures taken in recent years, in order to take measures to take the measures taken in the next ten years to take the necessary measures. (...) The measures taken in the next ten years will be taken. (...) The measures we will take in the next ten years. (...)"}, {"heading": "2 BELIEF PROPAGATION", "text": "Many important probability models can be represented by factorised distributions of the form: P (x1,.., xn). (x3). (x3). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (5). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (x4). (5). (x4). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5)."}, {"heading": "2.1 THE RESIDUAL SPLASH ALGORITHM", "text": "The splash method shown in Fig. 1 generalizes the optimal forward-backward sequential update sequence used in acyclic graphical models. If it is applied to a vertex v in the factor diagram, the splash method first constructs a defined volume width initial search (BFS) that is rooted to v. Then, starting with the leaves, the vertex is updated sequentially until the root is reached, and then the process is reversed. If a vertex is updated, all outgoing messages from a vertex are recalculated using the current incoming messages. In an acyclic subgraph, the splash method corresponds to a forward-backward BP. The ResidualSplash algorithm applies a residual lag deviation if the residual value is not changed."}, {"heading": "3 DISTRIBUTING STATE", "text": "In this section, we will discuss the challenges associated with distributing the status of the ResidualSplash algorithm to p processors. In the shared memory setting, each processor has efficient direct access to all data structures and memory, enabling a single shared scheduling queue and a message set. As a result, the time and resources required to access the queue and update messages are symmetrical for all processors. Conversely, the access times in the distributed memory setting are not symmetrical. Instead, each processor can only directly access its local memory and must pass messages to communicate with other processors."}, {"heading": "3.1 FACTOR GRAPH AND MESSAGES", "text": "We start with partitioning factor-restriction and the above localization restrictions, we define the problem of retention, accountability and communication, we have to minimize the communication and make sure that the data needed for message calculation is locally available. We define a partitioning of factor-restriction via p-processors as set B = {B1,..., Bp} of disjuncture Bk-V Restrictions so that any Vertex-update is a local procedure in which the processor maps all factor-restrictions associated with the Restriction-processor Restrictions-Restrictions-Restrictions-Restrictions-Restrictions. Finally, we save any message on the processor that contains the target text. Therefore, any Vertex-update is a local when the Resttex-processor is updated, which can read factors and all incoming messages without communication. To keep the locality invariant, the new output message is calculated, it is transferred to the processor that can read the messages.By the locality-restrictions mentioned above."}, {"heading": "3.1.1 Update Counts Ui", "text": "In practice, we find that the ResidualSplash algorithm updates the vertices in a highly inconsistent manner; a key feature of dynamic planning that allows for more frequent updates of slow converging messages. To illustrate the difficulty in estimating the update numbers for each vertex, we introduce the synthetic denoization task. The input shown in Figure 2 (a) is a grayscale image with independent Gaussian noise N (0, \u03c32) added to each pixel. The factor graph (Figure 2 (b) corresponds to the paired Markov Random Field grid, which was constructed to have an inconsistent pattern by introducing a latent random variable for each pixel and linking adjacent variables by factors that encode a similarity preference."}, {"heading": "3.1.2 Uninformed Partitioning", "text": "Surprisingly, in practice, we find that an uninformed cut achieved by setting the number of updates to a constant (i.e. Ui = 1) achieves partitions with comparable communication costs and labor balance as achieved by using the true update numbers. In Table 1, we construct uninformed p = 120 partitions B * with U-i = 1 on multiple charts and report on the communication costs and balance sheet. Costs = tic B-B (u-B, v-B), E-wuv-B (u-B, v-B), E-wuvRel. Labor Balance = p-V-V wv-max B-B-B-B-W-V in relation to the ideal cut B-B-W achieved with the true update numbers Ui. We find that uninformed cuts have lower communication costs at the expense of increased imbalance. This discrepancy results from the need to satisfy the balance requirements in relation to true Ui communication costs at higher levels."}, {"heading": "3.1.3 Over-Partitioning", "text": "Since uninformed partitions tend to reduce communication costs and have a greater labor imbalance relative to informed partitions, we propose over-partitioning to improve the overall balance of the work with a small increase in communication costs. If we partition the graph with an uninformed cut, a frequently updated subgraph can be placed within a single partition. To reduce the likelihood of such an event, we can over-partition the graph into k \u00b7 p balanced partitions and then randomly distribute the partitions across the original p processors. By assigning the graph finer and randomly regions to different processors, we distribute more uneven update patterns that improve the overall balance of the work. However, over-partitioning also increases the number of edges that traverse the cut and thus the communication costs. By over-partitioning in the denoise task, we are able to improve the work balance (shown in Figure 2) with the cost of communication shown in Figure 2."}, {"heading": "3.2 DISTRIBUTING THE PRIORITY QUEUE", "text": "The ResidualSplash algorithm is based on a common global priority queue. However, in the cluster computing setting, a centralized sequence is inefficient. Instead, each processor constructs a local priority queue and iteratively applies the splash process to the top element of its local queue. In each round, the world's highest remaining vertex is at the top of one of the local queues. Unfortunately, the remaining p \u2212 1 highest vertex points are not guaranteed to be at the top of the remaining queues, so we do not recover from the original shared memory scheduling. However, any processor with not yet converged vertex points will need to update these final results and can therefore always make progress by updating the vertex at the top of its local queue. In Section 5.1, we show that the collection of local queues is sufficient to maintain the original optimization properties of the ResidualSplash algorithm."}, {"heading": "3.3 DISTRIBUTED TERMINATION", "text": "In the distributed environment, where there is no synchronized common state, it is difficult to identify the world's largest element and stop the algorithm when it falls below the termination boundary. This is the well-studied problem of distributed termination [Matocha and Camp, Mattern, 1987]. We implement a variation of the algorithm described in Misra [1983] by defining a token ring across all nodes in which a marker is guided in one direction around the ring. The marker progresses as soon as the node to which the marker belongs converges and stops execution. A node can continue execution when it receives a message whose maximum residuality exceeds the termination threshold."}, {"heading": "4 IMPROVED SCHEDULING", "text": "The fixed volume splash operation used in the ResidualSplash algorithm and the message-based residual scheduling pose several key challenges in scaling the algorithm to large factor charts. Specifically, both assume that all vertices require the same amount of work to be updated. However, complex factors and variables involved in many factors often take much longer to update. Meanwhile, the residual scheduling of the message assumes that a significant change in an incoming message implies a significant change in belief and outgoing messages. Conversely, the use of message residuals as a convergence condition assumes that a small change in all incoming messages will cause only a small change in belief and outgoing messages. If the factor chart is large with high vertibles, this can lead to unbalanced convergence and an affinity for updating high vertibles with little improvement in accuracy."}, {"heading": "4.1 BALANCED SPLASH", "text": "When planning splash operations, residual theuristics assigns a \"value\" to each splash update that ignores the cost of calculating the splash. However, if the graph structure is regular, the size of each splash and the resulting costs are likely to be similar, allowing the residual schedule to focus on minimizing residual costs. However, if the cost of calculating a splash is significantly different, as is the case with large irregular graphs, residual theuristics will not take costs into account. As a result, residual theuristics skips relatively high residual corners at a low cost in favor of the highest residual point at a much higher cost. Furthermore, high-grade, costly vertices associated with many other processors are likely to be included in many BFS traversals and thus updated disproportionately more frequently than other less strongly connected vertices. This problem is further frustrated in cluster setting, as high-grade vertices associated with many other processors significantly increase network traffic."}, {"heading": "4.2 NONUNIFORM CONVERGENCE", "text": "The use of message residuals as a termination criterion leads to uneven convergence in doctrines. Small changes in individual messages can combine at high vertices, resulting in large changes in doctrines and asymmetric convergence. We demonstrate this behavior by looking at a variable Xi with d = | i | incoming messages {m1,..., md}. Suppose all incoming messages converged to {m \u2032 1,.,.., m \u2032 d}, so that the resulting residual amount is smaller than \u03b2 (i.e., k | m \u2032 k \u2212 mk \u2212 1 \u2264 \u03b2). Suppose that the convergence criterion in equation \u2032 (2.4) converges the messages converged. However, the effective change in faith depends linearly on the degree and can therefore be far removed from convergence. Suppose that m1, m., m., m., d = 12} (If the residual values are \u2212 1 and \u2212 2} (\u2212 1), the residual messages are: \u2212 1."}, {"heading": "4.3 BELIEF RESIDUALS", "text": "In fact, it is such that it is a \"real\" process in which the two parties have agreed on a common denominator. (...) In fact, it is such that both sides have agreed on a common denominator. (...) In fact, it is such that both sides have agreed on a common denominator. (...) In fact, it is such that \"both sides have agreed on a common denominator.\" \"In the second half of the second half of the second half of the second half of the second half of the second part (...\" real denominator \") In fact, both sides have agreed on a common denominator.\" In the second half of the second half of the second half of the second half of the second half of the second part of the second part (... \"real denominator\") In the second half of the second part of the second part of the second part of the second part of the second part (... \"real denominator\") In the second half of the second part of the second part of the second part of the second part of the third part of the second part of the second part of the third part of the second part of the third part (third part) In the third part of the third part of the second part of the second part of the second part of the second part of the second part of the second part of the second part of the second part."}, {"heading": "5 THE DBRSPLASH ALGORITHM", "text": "We now present our Distributed Belief Residual Splash Algorithm (DBRSplash shown in Alg. 1), which combines the ideas presented in previous sections. Execution can be divided into two phases, setup and inference.In the setup phase, in line 1, we segment the input factor graph into kp messages using the METIS algorithms. Note that this could be achieved in parallel using ParMETIS, with our implementation using the sequential version for simplicity. Then, in line 2, we randomly assign k parts to each of the p processors. Parallel, each processor collects its factors and variables (line 3)."}, {"heading": "5.1 PRESERVING SPLASH CHAIN OPTIMALITY", "text": "In Gonz\u00e1lez et al. [2009], we introduced \u03c4 notation as a theoretical measure of the effective distance between V and V, where nodes are assumed to be almost independent. More formally, for all nodes, the minimum radius for which the propagation of faith on the subgraph centered on that node yields beliefs that are maximally removed from beliefs that are achieved using the entire diagram. By increasing the value of the, we reduce the sequential dependence structure and increase the chance of parallelism. We showed that the ResidualSplash algorithm, when applied to chain diagrams under the approximate inference setting, reaches the optimal lower limit. We now show that DBRSplash maintains optimality in the distributed setting. Theorem 5.1 (Splash Chain Optimality), when applied to chain diagrams under the approximate inference setting, achieves the optimal lower limit."}, {"heading": "6 EXPERIMENTS", "text": "We implemented an MPI-based version of DBRSplash in C + + using MPICH2. The splash size, superpartitioning factor and scheduling method (i.e. message and belief-based) were parameterized for comparison. We relied on the weighted kmetis partitioning routine from the METIS software library for graph partitioning. All partitions were calculated in less than 10 seconds. To ensure numerical stability and convergence, log-space message calculations and 0.6 attenuation were used. The convergence limit was set to \u03b2 = 10 \u2212 5. Cluster experiments were compiled with GCC 4.2.4 and computed on a cluster of 15 64-bit Linux blades with dual quad-core Intel Xeon 2.33GHZ (E5345) processors running Gigabit Ethernet.We evaluated the performance of DBRSplash Markov on multiple DomLogic Networks (2008) based on several probable factors."}, {"heading": "6.1 PARALLEL PERFORMANCE", "text": "The runtime and acceleration of DBRSplash was assessed on the uw systems and uw languages MLNs using various overpartitioning factors. In Fig. 4 (a) and Fig. 4 (b), DBRSplash achieves linear to superlinear runtimes and accelerations of up to 120 processors on the larger uw systems MLN. Superlinear acceleration can be attributed to increasing cache efficiency and memory bandwidth. Increasing the overpartitioning factor initially improves performance, but the performance gains are gradually dampened by increased communication costs as more processors are used. Meanwhile, the much smaller uw-language MLN shows only linear to superlinear performance gains of up to 20 processors (Fig. 5 (a) and Fig. 5 (b)))). With a total runtime of less than 10 seconds, the work is not sufficient to efficiently deploy more than 20 processors."}, {"heading": "6.2 OVER-PARTITIONING", "text": "In order to directly assess the effects of overpartitioning on the work balance and network traffic, we used the denoising task introduced in Fig. 3.1.1 on a 500 x 500 pixel 5-color image. Using 60 processors, we tested several overpartitioning factors and recorded both the current CPU load (Fig. 6 (a)) and cumulative network traffic (Fig. 6 (b). Without overpartitioning, the calculation is unbalanced, leading to a gradual reduction in the number of active processors. Increasing the overpartitioning factor reduces runtime and ensures that all processors remain active until convergence. However, overpartitioning, as suggested, increases network activity. We perform a similar analysis on both MLNs. CPU aging activity Fig. 4 (d) for underwater systems, performance (underwater systems MLN) decreases with increasing UW partition."}, {"heading": "6.3 ACCURACY ASSESSMENT", "text": "To evaluate the accuracy of DBRSplash faith predictions, we compare them to faith predictions obtained from Gibbs sampling. We generated chains of 125 thousand samples, dropped the first 25 thousand samples (burn-in), and then used remaining samples to estimate the true beliefs. We compared faith predictions by calculating the L1 difference across all variables in the model. We found that repeated chains starting at different random states converged into beliefs that differed on average by less than 0.05 in L1 per variable. In Figure 4 (c), we plot the accuracy of DBRSplash with 60 processors on UW systems as a function of vertex updates. In Figure 5 (c), we do the same for UW languages, but with a single processor, as the runtime is too short. In both cases, DBRSplash quickly achieves a high function of BRSplash accuracy from D4 to the precision of D4 (4)."}, {"heading": "6.4 IMPROVED SCHEDULING", "text": "To provide a common basis for comparison, we evaluate the accuracy of beliefs as discussed in Fig. 6.3 (Fig. 4 (c), Fig. 5 (c), Fig. 4 (f) and Fig. 5 (f)). These diagrams compare the accuracy of DBRSplash using Belief Residuals and Message Residuals. In Uw systems, the faith residuals achieve faster convergence to a precise solution. The Uw languages MLN exhibit an almost identical accuracy convergence using both planning methods.In addition, we have found that in several MLNs, message-based planning does not converge while faith-based planning converges. One such MLN, Cora-1, is characterized by extremely high degree variables (e.g. 59 variables with a degree of more than 100 and 3 variables with a degree of more than 1000)."}, {"heading": "7 CONCLUSIONS", "text": "To address the problem of state partitioning, we reduced the allocation of factors and messages and the calculation to draw sections with edge and vertex weights. In accurately estimating the weights, we required knowledge of update schedules and showed that uninformed cuts work relatively well in practice. Since uniformed cuts tend to have lower communication costs and greater imbalances than informed cuts, we proposed overpartitioning to improve the balance at the expense of increased communication costs. We found that overpartitioning can reduce total runtime as long as communication costs do not dominate. To support the determination of distributed memory and improve the performance of complex irregular graphs, we proposed a new scheduling that addresses the limitations of residual splash scheduling while maintaining the parallel optimization characteristics of processors. Using a distributed collection of effects, we made sure that spematialized schedules would be used more often."}, {"heading": "Acknowledgements", "text": "This work is supported by the ONR Young Investigator Program N00014-08-1-0752, the ARO under MURI W911NF0810242, DARPA IPTO FA8750-09-1-0141, the NSF under the grants NeTS-NOSS and CNS-0625518, and Joseph Gonzalez is supported by the AT & T Labs Fellowship. We thank Intel Research for the cluster time."}], "references": [{"title": "Distributed inference for latent dirichlet allocation", "author": ["D. Newman", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Newman et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Newman et al\\.", "year": 2007}, {"title": "Asynchronous distributed learning of topic models", "author": ["A. Asuncion", "P. Smyth", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "Asuncion et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Asuncion et al\\.", "year": 2008}, {"title": "A robust architecture for distributed inference in sensor networks", "author": ["M. Paskin", "C. Guestrin", "J. McFadden"], "venue": "In IPSN,", "citeRegEx": "Paskin et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Paskin et al\\.", "year": 2005}, {"title": "Distributed inference in dynamical systems", "author": ["S. Funiak", "C. Guestrin", "M. Paskin", "R. Sukthankar"], "venue": "In NIPS,", "citeRegEx": "Funiak et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Funiak et al\\.", "year": 2006}, {"title": "Residual splash for optimally parallelizing belief propagation", "author": ["J. Gonzalez", "Y. Low", "C. Guestrin"], "venue": "In AISTATS,", "citeRegEx": "Gonzalez et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gonzalez et al\\.", "year": 2009}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl.,? \\Q1988\\E", "shortCiteRegEx": "Pearl.", "year": 1988}, {"title": "Turbo decoding as an instance of Pearl\u2019s belief propagation algorithm", "author": ["R.J. McEliece", "D.J.C. MacKay", "J.F. Cheng"], "venue": "J-SAC,", "citeRegEx": "McEliece et al\\.,? \\Q1998\\E", "shortCiteRegEx": "McEliece et al\\.", "year": 1998}, {"title": "Stereo matching using belief propagation", "author": ["J. Sun", "N.N. Zheng", "H.Y. Shum"], "venue": "ITPAM,", "citeRegEx": "Sun et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2003}, {"title": "Understanding belief propagation and its generalizations", "author": ["J.S. Yedidia", "W.T. Freeman", "Y. Weiss"], "venue": "In Exploring artificial intelligence in the new millennium,", "citeRegEx": "Yedidia et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2003}, {"title": "Approximate inference and protein folding", "author": ["C. Yanover", "Y. Weiss"], "venue": "In NIPS,", "citeRegEx": "Yanover and Weiss.,? \\Q2002\\E", "shortCiteRegEx": "Yanover and Weiss.", "year": 2002}, {"title": "Residual belief propagation: Informed scheduling for asynchronous message passing", "author": ["G. Elidan", "I. Mcgraw", "D. Koller"], "venue": "In UAI,", "citeRegEx": "Elidan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Elidan et al\\.", "year": 2006}, {"title": "Multilevel k-way partitioning scheme for irregular graphs", "author": ["G. Karypis", "V. Kumar"], "venue": "J. Parallel Distrib. Comput.,", "citeRegEx": "Karypis and Kumar.,? \\Q1998\\E", "shortCiteRegEx": "Karypis and Kumar.", "year": 1998}, {"title": "Randomized static load balancing for tree-shaped computations", "author": ["P. Sanders"], "venue": "In Workshop on Parallel Processing,", "citeRegEx": "Sanders.,? \\Q1994\\E", "shortCiteRegEx": "Sanders.", "year": 1994}, {"title": "Algorithms for distributed termination detection", "author": ["F. Mattern"], "venue": "Distributed Computing,", "citeRegEx": "Mattern.,? \\Q1987\\E", "shortCiteRegEx": "Mattern.", "year": 1987}, {"title": "Detecting termination of distributed computations using markers", "author": ["J. Misra"], "venue": "In SIGOPS,", "citeRegEx": "Misra.,? \\Q1983\\E", "shortCiteRegEx": "Misra.", "year": 1983}, {"title": "Markov logic: A unifying language for structural and statistical pattern recognition", "author": ["P. Domingos", "S. Kok", "D. Lowd", "H.F. Poon", "M. Richardson", "P. Singla", "M. Sumner", "J. Wang"], "venue": "SSPR,", "citeRegEx": "Domingos et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Domingos et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Work by Newman et al. [2007] and Asuncion et al.", "startOffset": 8, "endOffset": 29}, {"referenceID": 0, "context": "Work by Newman et al. [2007] and Asuncion et al. [2008] in parallel inference for latent topic models and by Paskin et al.", "startOffset": 8, "endOffset": 56}, {"referenceID": 0, "context": "Work by Newman et al. [2007] and Asuncion et al. [2008] in parallel inference for latent topic models and by Paskin et al. [2005] and Funiak et al.", "startOffset": 8, "endOffset": 130}, {"referenceID": 0, "context": "Work by Newman et al. [2007] and Asuncion et al. [2008] in parallel inference for latent topic models and by Paskin et al. [2005] and Funiak et al. [2006] in distributed inference for sensor networks adopt a message based asynchronous computation model to address the important task of distributed graphical model inference.", "startOffset": 8, "endOffset": 155}, {"referenceID": 0, "context": "Work by Newman et al. [2007] and Asuncion et al. [2008] in parallel inference for latent topic models and by Paskin et al. [2005] and Funiak et al. [2006] in distributed inference for sensor networks adopt a message based asynchronous computation model to address the important task of distributed graphical model inference. However their approaches are specialized to particular models or settings. Alternatively, in Gonzalez et al. [2009] we explored", "startOffset": 8, "endOffset": 441}, {"referenceID": 4, "context": "\u201d This differs from the shared memory setting of Gonzalez et al. [2009], where every processor has direct access to all available memory.", "startOffset": 49, "endOffset": 72}, {"referenceID": 5, "context": "Belief Propagation (BP), or the Sum-Product algorithm, is a commonly used approximate inference algorithm originally proposed by Pearl [1988]. In BP, \u201cmessages\u201d (parameters), are iteratively computed along edges in the factor graph until convergence and then used to estimate marginals.", "startOffset": 129, "endOffset": 142}, {"referenceID": 4, "context": "However, in Gonzalez et al. [2009] we showed that efficient parallel inference in the shared memory setting is limited by the sequential dependencies among messages.", "startOffset": 12, "endOffset": 35}, {"referenceID": 11, "context": "Here we use the collection of multilevel graph partitioning algorithms in the METIS [Karypis and Kumar, 1998] graph partitioning library.", "startOffset": 84, "endOffset": 109}, {"referenceID": 12, "context": "If at each split the work is divided into two parts of proportion X and 1 \u2212 X where E [X] = 12 and Var [X] = \u03c3 (\u03c3 \u2264 1 2 ]), Sanders [1994] shows that we can obtain work balance with high probability if we select k at least \u03a9 ( p(log( 1 \u03c3+1/2 )) \u22121) .", "startOffset": 124, "endOffset": 139}, {"referenceID": 13, "context": "This is the well studied distributed termination problem [Matocha and Camp, Mattern, 1987]. We implement a variation of of the algorithm described in Misra [1983] by defining a token ring over all the nodes, in which a marker is passed in one direction around the ring.", "startOffset": 76, "endOffset": 163}, {"referenceID": 4, "context": "In Gonzalez et al. [2009] we introduced the \u03c4 notation as a theoretical measure for the effective distance \u03c4 at which vertices are assumed to be almost independent.", "startOffset": 3, "endOffset": 26}, {"referenceID": 4, "context": "The proof is essentially identical to the method used in Gonzalez et al. [2009]. We assume that the chain graph is optimally sliced into p connected pieces of |V | /p vertices each.", "startOffset": 57, "endOffset": 80}, {"referenceID": 15, "context": "We assessed the performance of DBRSplash on Markov Logic Networks (MLNs) [Domingos et al., 2008], a probabilistic extension to first-order logic obtained by attaching weights to logical clauses.", "startOffset": 73, "endOffset": 96}], "year": 2009, "abstractText": "As computer clusters become more common and the size of the problems encountered in the field of AI grows, there is an increasing demand for efficient parallel inference algorithms. We consider the problem of parallel inference on large factor graphs in the distributed memory setting of computer clusters. We develop a new efficient parallel inference algorithm, DBRSplash, which incorporates over-segmented graph partitioning, belief residual scheduling, and uniform work Splash operations. We empirically evaluate the DBRSplash algorithm on a 120 processor cluster and demonstrate linear to super-linear performance gains on large factor graph models.", "creator": "TeX"}}}