{"id": "1205.2646", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Censored Exploration and the Dark Pool Problem", "abstract": "We introduce and analyze a natural algorithm for multi-venue exploration from censored data, which is motivated by the Dark Pool Problem of modern quantitative finance. We prove that our algorithm converges in polynomial time to a near-optimal allocation policy; prior results for similar problems in stochastic inventory control guaranteed only asymptotic convergence and examined variants in which each venue could be treated independently. Our analysis bears a strong resemblance to that of efficient exploration/ exploitation schemes in the reinforcement learning literature. We describe an extensive experimental evaluation of our algorithm on the Dark Pool Problem using real trading data.", "histories": [["v1", "Wed, 9 May 2012 15:16:31 GMT  (241kb)", "http://arxiv.org/abs/1205.2646v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.LG cs.GT", "authors": ["kuzman ganchev", "michael kearns", "yuriy nevmyvaka", "jennifer wortman vaughan"], "accepted": false, "id": "1205.2646"}, "pdf": {"name": "1205.2646.pdf", "metadata": {"source": "CRF", "title": "Censored Exploration and the Dark Pool Problem", "authors": ["Kuzman Ganchev", "Michael Kearns", "Yuriy Nevmyvaka", "Jennifer Wortman Vaughan"], "emails": [], "sections": [{"heading": null, "text": "We introduce and analyze a natural algorithm for multi-venue exploration based on censored data motivated by the dark pool problem of modern quantitative finance. We demonstrate that our algorithm converges in polynomial time to a near-optimal allocation policy; previous results for similar problems in stochastic inventory control only guaranteed asymptotic convergence and examined variants in which each venue could be treated independently; our analysis shows strong similarity to efficient exploration / exploitation schemes in the literature for amplification; and we describe a comprehensive experimental assessment of our algorithm on the dark pool problem based on real trade data."}, {"heading": "1 Introduction", "text": "This year, it is closer than ever before to being able to take the lead."}, {"heading": "1.1 Related Work", "text": "The problem that is perhaps closest to our environment is the well-researched problem of the newsagent from business research literature. In this problem, a player (representing a newsagent owner) selects the quantity of V of newspapers he buys at a fixed price per unit over each period of time and tries to maximize profit in the face of demand uncertainty in a single place (his newsagent). On this issue, there is a large and varied literature; see Huh et al. (2009) and the quotes contained therein. In this paper, the authors are the first to consider the use of the Kaplan-Meier estimator for easily perishable inventory problems. They use an estimation loop similar to ours and show asymptotic convergence to near-optimal behavior in a single place. Managing the distribution of an exotically specified Volume V to multiple venues (these are the important aspects of the dark pool problem, where the volume to be handled is specified by a customer, and there are many dark pools."}, {"heading": "2 Preliminaries", "text": "We consider the following problem. At each point in time, a quantity or volume V t = 1, \u00b7 \u00b7 \u00b7, V} of units is presented to a learner, where V t is queried from an unknown distribution Q. The learner must decide to allocate these shares to a series of K known locations, where vti = 0, \u00b7 \u00b7, V t} is queried for each i = 1, \u00b7 \u00b7, K}, and \u2211 K i = 1 v t i = Vt. The learner is then told the number of units that rti will be consumed at each venue i. Here rti = min {sti, vti}, where sti is the maximum consumption level of the venue i at a time t sampled independently of a fixed but unknown distribution Pi. If rti = v t i, we say that the algorithm will receive a censored observation because it is possible to derive only this rti \u2264 sti. If rti < vti, we say that the algorithm will receive an empirical step, each time we must estimate the other for an observation i = i."}, {"heading": "3 Greedy Allocation is Optimal", "text": "In this section, we show that a simple greedy algorithm can maximize the (estimated) expected number of units consumed in a single time step. (The greedy algorithm assigns one unit at a time. (The location to which the next unit is assigned is chosen to maximize the estimated probability that the unit will be consumed; if vi \u2212 units have already been assigned to location i, then the estimated probability that the next assigned unit will be consumed is simply T vi (vi + 1). A formal description is given as algorithm 1 below. Theorem 1 The allocation returned by Greedy maximizes the expected number of units consumed in a sin algorithm 1: Optimal algorithm I: Optimal algorithm I: Input: Volume V, Tail probability estimates Ki = 1 Output: An allocation vv \u00b2 0; for V doi algorithm i is the expected time."}, {"heading": "4 Censored Exploration Algorithm", "text": "In this area, we are able to set out in search of new paths, in search of new ways and ways. (...) We have the opportunity to set out in search of new ways. (...) We have to set out in search of new ways. (...) We have to set out in search of new ways. (...) We have to set out in search of new ways. (...) We have to set out in search of new ways. (...) We have to set out in search of new ways. (...) We have to set out in search of new ways. (...) We have to set out in search of new ways. (...) We have to set out in search of new ways. (...) We have to set out in search of new ways. (...) We have to set out in search of new ways. (...) We have to set out in search of new ways. (...) We have to set out in search of new ways."}, {"heading": "4.1 Convergence of Kaplan-Meier Estimators", "text": "Let's start by describing the standard Kaplan Meier maximum probability estimate for censored data (Kaplan and Meier, 1958, Peterson, 1983), which limits our attention to a single location. Let's start by describing the standard Kaplan Meier maximum probability for censored data (Kaplan and Meier, 1958, Peterson, 1983) by limiting our attention to a single location. Let's let zi, s is the true probability that the demand in that location is exactly s-units, since the demand is at least s-1, it's easy to verify that for each s-0, Ti (s) = 1 s-1 s-1 s-conditional probability that there is a demand of at least s-1, it's an estimate of zi, s for each s-1, and then use these estimates to calculate an estimate of Ti (s). Let's be an indicator function that takes the value 1 if its input is true and 0 otherwise. Let's Dti, Meier as the first estimate of zi."}, {"heading": "4.2 Modifying Kaplan-Meier", "text": "In Algorithm 3, we describe the slight modification of Kaplan-Meier that is necessary for our analysis. As described above (Step 1), the value cti in this algorithm can intuitively be regarded as a \"cut-off point\" up to which we are guaranteed to have sufficient data to accurately estimate the tail probabilities using KaplanMeier. (This is formalized in Lemma 2 below.) Thus, for each quantity s \u2264 CTI, simply the tail probability of T + 1 (s) is exactly the Kaplan-Meier estimate as in Equation 1. However, we place the value of T (c t i + 1) optimistically on the Kaplan-Meier estimate of the tail probability at cti + 1). This optimistic modification is necessary to ensure that the greedy algorithm exploration (i.e.) has a chance to make progress toward an increase of at least one cut-off value."}, {"heading": "4.3 Exploitation and Exploration Lemmas", "text": "With these two lemmas in place, we are ready to explain our main exploration lemmas (Step 2) (Step 2), which formalizes the idea that a sufficient amount of exploration has occurred, the allocation of the greedy algorithms will therefore be -optimal. Proof of this problem is where the requirement that T-i + 1) must be optimistically set. In particular, because the optimistic setting of T-i (c-i + 1), we know that if the greedy policy allocates precisely cti units to avenue i, it could not gain too much by redistributing additional units from another location to another location. In this sense, we create a \"buffer\" over each cut-off, which guarantees that it is not necessary to proceed further as long as one of the two conditions in the situation is met. Lemma 4 (Exploitation Lemma) Assume that at the time t, the high probability holds in Lemma 2."}, {"heading": "4.4 Putting It All Together", "text": "With the exploitation and exploration of lemmas in place, we are finally ready to specify our most important theorems. Full proof is omitted due to the lack of space, but we outline the most important ideas successive.Theorem 3 (main law) For each > 0 and \u03b4 > 0, with the probability of 1 \u2212 \u03b4 (about the randomness of the drawings of Q and {Pi}), after running for a time polynomial in K, V, 1 /, and ln (1 / \u03b4), algorithm 2 makes a -optimal allocation in each subsequent time step with the probability of at least 1 \u2212.Proof sketch: Suppose that the algorithm runs for R fraction steps, where R is a polynomial in K, V, 1 /, and ln (1 / \u03b4) is an optimal algorithm allocation in each subsequent time step with the probability of 1 \u2212.Proof sketch: Suppose that the algorithm runs for R fraction steps, where we can argue a polynomial in K, 1 / \u03b4 is optimal for an optimal time plan, and ln (where \u2212 1 is a large time plan \u2212 1)."}, {"heading": "5 Application: Dark Pool Problem", "text": "We describe the application that provided the original inspiration for the development of our framework and algorithm. As mentioned in the introduction, dark pools are a particular and relatively young type of stock exchange for listed stocks. While the exact details are beyond the scope of this paper, the biggest challenge in executing high-volume trades in traditional (\"light\") exchanges is that it is difficult to \"conceal\" such trades, and their disclosure generally leads to negative effects on the price (e.g. the presence of a high-volume buyer leads to an increase in the price against that buyer). If the volume is sufficiently large, this difficulty of obfuscation persists, even if one tries to slowly break the trade over time. Dark pools were created precisely to address the problems faced by large traders and to emphasize the concealment of prices (Wikipedia, 2009, Bogoslaw, 2007)."}, {"heading": "5.1 Summary of Dark Pool Data", "text": "Our data set is derived from the internal dark pool order flow for a large U.S. broker-dealer. Each (possibly censored) observation in this data corresponds exactly to the form discussed throughout the paper - a triple of the name of the dark pool, the number of shares sent to that pool, and the number of shares subsequently executed within a short period of time. It is important to highlight some limitations of the data set. First, it should be noted that the data set mixes the policy that the broker uses for allocation in the dark pools with the liquidity that is available in the pools themselves. In our data set, the current policy was very similar to the banditstyle approach that we discuss below. Second, the \"parent orders,\" which determine the total volume to be allocated to the pools, were determined by the trading needs of the broker and are similarly beyond our control. The data set contains submissions and executions for four active pools E. Dark Pools: an average of five million shares, a dozen of Automated Trading BIDS, and a dozen of Automated Stocks each."}, {"heading": "5.2 Parametric Models for Dark Pools", "text": "The theory and algorithm that we have developed for censored exploration allow for a general (non-parametric) form for the Pi distributions, in which it is reasonable to ask whether the data allow a simple parametric form for these distributions. For each of these distributions, the basic methodology was the same. For each of the 4 \u00d7 12% share pairs, the response to this pairing was evenly distributed, and the training data was used to determine the maximum probability for the distributions."}, {"heading": "5.3 Data-Based Simulation Results", "text": "As with any control problem, the data of the dark pool in our possession is unfortunately insufficient to evaluate and compare different algorithms for the simulation. (This is due to the above fact that the volumes submitted to each location were determined by the specific policy that generated the data, and therefore we cannot explore alternative options - if our algorithm decides to submit 1000 shares in one place, but only 500 shares were submitted in the data, from which we simply cannot infer the result of our desired subordination. So, instead, we use the raw data to derive a simulator with which we can evaluate different approaches. In light of the modeling results of Section 5.2, the Simulator for Shares S was constructed as follows. For each dark pool, we have estimated all the data for the maximum probability of Zero Bin + Power Law Distribution. (Note that there is no need for a training test split, as we have already validated the choice of the distribution model separately)."}, {"heading": "Acknowledgments", "text": "We are grateful to Curtis Pfeiffer and Andrew Westhead for valuable conversations and Bobby Kleinberg for introducing us to the literature on the problem of news sellers."}, {"heading": "D. Bogoslaw. Big traders dive into dark pools.", "text": "Business Week article, available at: http: / / www.businessweek.com / investor / content / oct2007 / pi2007102 _ 394204.h% tm, 2007."}, {"heading": "R. Brafman and M. Tennenholtz. R-MAX - A general polynomial time algorithm for near-optimal reinforcement", "text": "Learning. JMLR, 3: 213-231, 2003.N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006."}, {"heading": "A. Foldes and L. Rejto. Strong uniform consistency for nonparametric survival curve estimators from randomly", "text": "The Annals of Statistics, 9 (1): 122-129, 1981."}, {"heading": "W. T. Huh, R. Levi, P. Rusmevichientong, and J. Orlin. Adaptive data-driven inventory control policies", "text": "Based on the Kaplan Meier estimator. Preprint available at http: / / legacy.orie.cornell.edu / ~ paatrus / psfiles / km-myopic.pdf, 2009."}, {"heading": "E. L. Kaplan and P. Meier. Nonparametric estimation from", "text": "incomplete observations. JASA, 53: 457-481, 1958. M. Kearns and S. Singh. Almost optimal amplification learning in polynomial time. MLJ, 49: 209-232, 2002.A. V. Peterson. Kaplan Meier estimator. In Encyclopedia of Statistical Sciences. Wiley, 1983.Wikipedia. Dark pools of liquidity. http: / / en.wikipedia. org / wiki / Dark _ pools _ of _ liquidity, 2009."}], "references": [{"title": "The Probabilistic Method, 2nd Edition", "author": ["N. Alon", "J. Spencer"], "venue": null, "citeRegEx": "Alon and Spencer.,? \\Q2000\\E", "shortCiteRegEx": "Alon and Spencer.", "year": 2000}, {"title": "Big traders dive into dark pools", "author": ["D. Bogoslaw"], "venue": "Business Week article, available at: http: //www.businessweek.com/investor/content/ oct2007/pi2007102_394204.h%tm,", "citeRegEx": "Bogoslaw.,? \\Q2007\\E", "shortCiteRegEx": "Bogoslaw.", "year": 2007}, {"title": "R-MAX - A general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R. Brafman", "M. Tennenholtz"], "venue": "JMLR, 3:213\u2013231,", "citeRegEx": "Brafman and Tennenholtz.,? \\Q2003\\E", "shortCiteRegEx": "Brafman and Tennenholtz.", "year": 2003}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "By Azuma\u2019s inequality (see, for example,Alon and Spencer (2000)), for any \u03b3,", "startOffset": 40, "endOffset": 64}, {"referenceID": 3, "context": "Finally, note that the bandit algorithm is the crudest type of weight-based allocation scheme of the type that abounds in the no-regret literature (Cesa-Bianchi and Lugosi, 2006); we are effectively forcing our problem into a 0/1 loss setting corresponding to \u201cno shares\u201d and \u201csome shares\u201d being executed.", "startOffset": 147, "endOffset": 178}], "year": 2009, "abstractText": "We introduce and analyze a natural algorithm for multi-venue exploration from censored data, which is motivated by the Dark Pool Problem of modern quantitative finance. We prove that our algorithm converges in polynomial time to a near-optimal allocation policy; prior results for similar problems in stochastic inventory control guaranteed only asymptotic convergence and examined variants in which each venue could be treated independently. Our analysis bears a strong resemblance to that of efficient exploration/exploitation schemes in the reinforcement learning literature. We describe an extensive experimental evaluation of our algorithm on the Dark Pool Problem using real trading data.", "creator": "dvips(k) 5.95a Copyright 2005 Radical Eye Software"}}}