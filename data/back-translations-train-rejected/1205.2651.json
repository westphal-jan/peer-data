{"id": "1205.2651", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "Seeing the Forest Despite the Trees: Large Scale Spatial-Temporal Decision Making", "abstract": "We introduce a challenging real-world planning problem where actions must be taken at each location in a spatial area at each point in time. We use forestry planning as the motivating application. In Large Scale Spatial-Temporal (LSST) planning problems, the state and action spaces are defined as the cross-products of many local state and action spaces spread over a large spatial area such as a city or forest. These problems possess state uncertainty, have complex utility functions involving spatial constraints and we generally must rely on simulations rather than an explicit transition model. We define LSST problems as reinforcement learning problems and present a solution using policy gradients. We compare two different policy formulations: an explicit policy that identifies each location in space and the action to take there; and an abstract policy that defines the proportion of actions to take across all locations in space. We show that the abstract policy is more robust and achieves higher rewards with far fewer parameters than the elementary policy. This abstract policy is also a better fit to the properties that practitioners in LSST problem domains require for such methods to be widely useful.", "histories": [["v1", "Wed, 9 May 2012 15:08:18 GMT  (354kb)", "http://arxiv.org/abs/1205.2651v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["mark crowley", "john nelson", "david l poole"], "accepted": false, "id": "1205.2651"}, "pdf": {"name": "1205.2651.pdf", "metadata": {"source": "CRF", "title": "Seeing the Forest Despite the Trees: Large Scale Spatial-Temporal Decision Making", "authors": ["Mark Crowley", "John Nelson", "David Poole"], "emails": [], "sections": [{"heading": null, "text": "We are introducing a challenging real-world planning problem, where action must be taken at all times in any location in a spatial area. As a motivational application, we are using forestry planning. In large-scale planning problems, state and action are defined as cross-products of many local state and action areas that extend over a large spatial area such as a city or forest. These problems have state uncertainties, have complex utility functions that entail spatial constraints, and we generally need to rely on simulations rather than an explicit transition model. We are defining LSST problems as aggravated learning problems and presenting a solution based on policy gradients. We are comparing two different policy formulations: an explicit policy that identifies each location in the space and the actions to be taken there; and an abstract policy that defines the proportion of actions that need to be taken in all places in the space. We are showing that abstract policy is more robust and achieves higher rewards with far fewer parameters than the basic policies."}, {"heading": "1 INTRODUCTION", "text": "In the containment of infectious diseases, decisions must be made about the allocation of medicines to thousands or millions of people distributed across the space based on needs, costs, transportation, or any number of other variables. In forestry planning, it depends on whether every tree is felled or not, or whether a different activity is carried out at every point in the forest. Problems of this kind we call Large Scale Spatial Temporal (LSST) planning problems. Having further motivated the problem with details from forestry planning, we introduce a general definition of LSST planning as a reinforcement problem (Sutton and Barto, 1998) and discuss the characteristics that a solution must possess. We show how political gradients (Williams, 1992) can satisfy many of these characteristics for LSST problems. We compare two policy formulations: a policy that better identifies these actions at the level, so that the actions at the abstraction level and the actions we take at the other level coincide with the abstractions."}, {"heading": "2 FORESTRY BACKGROUND", "text": "Forestry is a very important industry in British Columbia, generating 12% of the province's GDP and employing about 200,000 people. Large areas of forest up to several hundred thousand hectares are outsourced by the government to forestry companies to cut down trees and sell lumber. The government imposes many restrictions on management activities: establishing a maximum annual permitted cut, indicating areas that are out of bounds, indicating spatial restrictions to avoid high density of cutting surfaces, and protecting wildlife migratory routes and habitats. Violations of these restrictions are enforced with heavy fines and a possible revocation of the license. Suppose you are the senior forester responsible for planning for one of these companies, and your interest is to maximize your return and minimize the penalties you incur, which can be achieved by providing a steady supply of logs and maintaining a healthy forest."}, {"heading": "2.1 SOLUTION CONSIDERATIONS", "text": "In a complex field such as forestry, there are many researchers who have developed sophisticated simulation models for the entire landscape planning process, from tree growth to forest growth. The explicit transition models underlying these simulations are too complex and varied to be used as a valuable resource. In many LSST planning areas, a distinction is made between strategic, tactical and operational measures. Operational planning refers to the immediate implementation of a low plan, so that all simulation data can be used as a valuable resource."}, {"heading": "2.2 CURRENT SOLUTIONS IN FORESTRY", "text": "Many existing planning solutions in forestry have relied heavily on the assumption of spatial independence between cells in a forest, and the deterministic optimization models commonly used, such as linear programming, collapse when confronted with uncertainty about the state of the forest, and cannot use information about spatial relationships between cells. This is a problem in forestry (J.P.Kimmins et al., 2005), as MPB breaks the assumption of spatial independence and makes the problem uncertain. MPB can fly between nearby cells in the forest, so that the immediate neighborhood is always relevant when it comes to reducing the spread of pests and quickly saving trees that are killed by them. Other solution methods commonly used in forestry planning are simulation modeling and meta-heuristics. Simulation models are an interactive approach in which the user determines maps, constraints and preferences for various actions."}, {"heading": "3 LSST DEFINITIONS", "text": "In fact, it is as if it is a matter of a way in which people are able to determine themselves how they behave. (...) In fact, it is as if people are able to determine themselves how they behave. (...) In fact, it is as if people are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are in a position. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves. (...) It is as if they are able to determine themselves."}, {"heading": "3.1 POLICY DEFINITION", "text": "The probability of taking action in a cell in state s is given by \u03c0 (s, a, \u03b8). The political parameters, \u03b8, are an A \u00b7 F matrix of real numbers used to define the policy as a Gibbs distribution of weighted state characteristics: \u03c0 (s, a, \u03b8) = e\u03b8 [a] s \u0445 b \u0445 A e\u03b8 [b] s (1) Where \u03b8 [a] is a vector of feature weights combined as a point product with the cell state characteristic vector, s.2If it is not relevant, the path k is removed from the state and action names."}, {"heading": "3.2 LANDSCAPE POLICY", "text": "The landscape parameters \u03b8 define parameters for each local cell policy. The landscape policy is calculated as a product of the probabilities for all cells specified by the corresponding cell policies. We will provide two parameters for the landscape policy, \u03c0C and \u03c01, shown below. The first formulation, \u03c0C, defines an explicit policy by maintaining separate parameters for each cell and each time step, successC: C \u2192 \u03b8. \u03c0C (s, a, \u03b8C) = \u0445C \u03c0 (s [c], \u0445C [c]) (2) The second formulation, shown in (3), describes an abstract policy in which a single set of parameters, \u041a1, is used for all cells in the landscape at this time."}, {"heading": "4 POLICY GRADIENTS", "text": "Political Gradient (PG) methods seek to find optimal strategies by following the gradient in relation to the political parameters of a function. (PG) Methods that describe the value of current policies (PG researchers have recently made significant progress in the types of amplification of learning problems that can be solved in the short term (Kersting and Driessens, 2008; Riedmiller et al., 2007; Sutton et al., 2000; Williams, 1992). PG methods require stochastic, parameterized strategies and work well when the state space is very large and the transition dynamics are not available. These properties correspond well to LSST planning problems, so that PG methods appear to be a promising place to start looking for solutions. Political gradient algorithms are based on the observation that the expected value of a stochastic policy can be calculated using previously sampled trajectories by weighting the actual results obtained during each trajectory."}, {"heading": "4.1 REDUCING THE VARIANCE OF\u2207\u03b8V \u03b8", "text": "The difference in magnitude of R (k) can lead to a high deviation in the estimation of the development that impedes learning. A part of the deviation can be eliminated by subtracting from each occurrence of R (k) in equations (4) and (8) a constant baseline b. The optimal baseline for our problem (shown here for \u03c0C) is calculated for each political parameter: bt [\u03b1, f] for each level A and f [F] as follows: bt [\u03b1, f] = p = p (Riedmiller et al., 2007)."}, {"heading": "4.2 SOLVING LSST PROBLEMS USING POLICY GRADIENTS", "text": "The formulation for this sub-derivative in (8) requires us to know that this element is \u03c0 (s, a, \u03b8), and our choice of political parameters allows us to express this analytically. We calculate the sub-derivative \u03b1fV \u03b8 with respect to the parameters \u03b8 [\u03b1, f] for each \u03b1-A and f-F: The logbook \u03c0 (s, a, \u03b8) = the sub-derivative protocol (e\u03b8 [a] s [b] s [b] s \u2212 f log [a] s \u2212 f log [b] s [b] s \u2212 f log [b] s [a] s \u2212 f x]. This sub-derivative differs depending on whether the action for this element is a \u2212 s \u2212 s \u2212 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 b (b] s \u00b2 s \u00b2 s \u00b2 s \u00b2 b) s \u00b2 s (b) s \u00b2 s \u00b2 s (b) s \u00b2 s \u00b2 s \u00b2 s (b) s \u00b2 s \u00b2 s (s) s \u00b2 s \u00b2 s (s) s \u00b2 s \u00b2 s (s) s \u00b2 s \u00b2 s \u00b2 s (s)."}, {"heading": "4.3 LSST POLICY GRADIENT ALGORITHM", "text": "Combine all these elements and you arrive at the political gradient algorithm that iteratively generates new trajectories and updates politics based on the current set of trajectories: Algorithm: LSST-PG (s0) initialized randomly."}, {"heading": "5 TWO ALGORITHM VARIANTS", "text": "In Section 2.1, we outlined two main requirements for a good LSST planning solution, which deal with the lack of an explicit transition model and the definition of a strategic policy that does not focus too much on low level details. Policy differences provide a way to meet the first requirement, since they do not require a model and only follow the gradient of political value. A fairly obvious approach is the definition of a landscape policy using \u03c0C, which maintains parameters for every aspect of the state, where \u03b8Ct [c] is defined for each cell. We will call this algorithm LSST-PGC and it is simply the LSST-PG algorithm described above, where \u03c0C performs the function of \u03c0 in the code."}, {"heading": "5.1 ABSTRACT ACTIONS", "text": "The LSST-PGC algorithm has two major problems: firstly, it gives us an enormous number of parameters to look for with the dimensions | A | | F | \u00d7 T \u00d7 | C |. As we have already mentioned, | C | could be in the order of 100,000, whereas | A |, | F | and T in general are less than 100. This enormous space makes convergence to an optimal policy very difficult. Secondly, LSST-PGC does not give us a strategic policy, but a very low operational policy. Optimal strategic policy should not distinguish between certain cells. Politics should treat cells interchangeably and define a pattern of action that applies the proportion of cells of each action to the entire landscape. We do not want to require a commitment to specific actions for certain cells in our strategic policy. Politics is, essentially, that we focus too much on the \"trees\" to ever find the \"forest\" and these patterns of action."}, {"heading": "6 EXPERIMENTS", "text": "The goal of our research is to develop a planning algorithm that can leverage existing simulations from the LSST domains in a scalable manner to find high-quality strategic strategies. There is a wide variety of forestry simulators, each requiring extensive expertise to set up and integrate them. We chose this phase of our research to develop our own simple forest simulator to evaluate the performance of parentage of young trees after deforestation. Our simulator includes state characteristics for the distribution of tree species and age groups, the level of MPBs in a cell and its adjacent cells. Dynamics include the birth, growth and death of young trees after deforestation, the killing of trees by MPB and the spread of MPBs to nearby cells year after year. We have introduced the two algorithms, LSST-PG1 and LSSTPGC, into Matlab and conducted all tests on a dual processor to run P2PC 3.2GHP with Windows XP."}, {"heading": "7 RESULTS", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "8 RELATED AND FUTUREWORK", "text": "To improve realism, it would be best to switch to a simulator for application by forestry planners, using the tools used for simulation planning, as discussed in Section 2.2. We are working with forestry researchers to combine our algorithms with more of our own data and simulations to experiment on a larger scale required for results to be useful for forestry planning experts. Our work builds on research on model-free learning (Sutton and Barto, 1998) and policy gradient methods (Riedmiller et al., 2007). Sutton et al. (2000) describes the use of policy gradients within an RL framework. The algorithm presented here includes some enhancements to basic PG such as reward baselining and Rprop. Further advanced PG techniques are available as natural gradients."}, {"heading": "9 CONCLUSIONS", "text": "We have described how FD techniques can be applied to these problems and have shown a way to apply policy gradient methods to find good strategies for a simulated forestry planning problem. We have shown that the use of spatially stationary policy formulation significantly reduces the parameter space to be searched and improves the value of the resulting policy. We hope that raising awareness of these specific issues will benefit both the UAE research community and the many researchers and planners in real planning areas with LSST structures who are looking for a way to make their very complex problems more manageable."}, {"heading": "Acknowledgements", "text": "We would like to thank Matt Hoffman for his help with PG methods and the feedback he has received from Peter Carbonetto, Michael Chiang, Albert Jiang, Jacek Kisyn \u0301 ski and the very helpful advice of the anonymous reviewers."}], "references": [{"title": "Recent advances in hierarchical reinforcement learning", "author": ["A. Barto", "S. Mahadevan"], "venue": "Technical report, University of Massachusetts,", "citeRegEx": "Barto and Mahadevan.,? \\Q2003\\E", "shortCiteRegEx": "Barto and Mahadevan.", "year": 2003}, {"title": "Spatial forest planning: A review", "author": ["E.Z. Baskent", "S. Keles"], "venue": "Ecological Modelling,", "citeRegEx": "Baskent and Keles.,? \\Q2005\\E", "shortCiteRegEx": "Baskent and Keles.", "year": 2005}, {"title": "Provincial level projection of the current mountain pine beetle outbreak", "author": ["M. Eng", "A. Fall", "J. Hughes", "T. Shore", "B. Riel", "P. Hall"], "venue": "Technical report, NRC-CFS-PFC,", "citeRegEx": "Eng et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Eng et al\\.", "year": 2004}, {"title": "Coordinated reinforcement learning", "author": ["C. Guestrin", "M. Lagoudakis", "R. Parr"], "venue": "In ICML, pages 227\u2013234,", "citeRegEx": "Guestrin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2002}, {"title": "Possible forest futures", "author": ["J.P.Kimmins"], "venue": "Technical Report 2005-11,", "citeRegEx": "J.P.Kimmins,? \\Q2005\\E", "shortCiteRegEx": "J.P.Kimmins", "year": 2005}, {"title": "Non-parametric policy gradients: A unified treatment of propositional and relational domains", "author": ["K. Kersting", "K. Driessens"], "venue": "In ICML,", "citeRegEx": "Kersting and Driessens.,? \\Q2008\\E", "shortCiteRegEx": "Kersting and Driessens.", "year": 2008}, {"title": "Model-free least-squares policy iteration", "author": ["M. Lagoudakis", "R. Parr"], "venue": "In NIPS, pages 1547\u20131554,", "citeRegEx": "Lagoudakis and Parr.,? \\Q2001\\E", "shortCiteRegEx": "Lagoudakis and Parr.", "year": 2001}, {"title": "Examining the performance of six heuristic optimisation techniques in different forest planning problems", "author": ["T. Pukkala", "M. Kurttila"], "venue": "Silva Fennica,", "citeRegEx": "Pukkala and Kurttila.,? \\Q2005\\E", "shortCiteRegEx": "Pukkala and Kurttila.", "year": 2005}, {"title": "Evaluation of policy gradient methods and variants on the cart-pole benchmark", "author": ["M. Riedmiller", "J. Peters", "S. Schaal"], "venue": "In IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning,", "citeRegEx": "Riedmiller et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Riedmiller et al\\.", "year": 2007}, {"title": "Formal models and algorithms for decentralized decision making under uncertainty. JAAMAS", "author": ["S. Seuken", "S. Zilberstein"], "venue": null, "citeRegEx": "Seuken and Zilberstein.,? \\Q2008\\E", "shortCiteRegEx": "Seuken and Zilberstein.", "year": 2008}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R.S. Sutton", "D. Mcallester", "S. Singh", "Y. Mansour"], "venue": "In NIPS,", "citeRegEx": "Sutton et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2000}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine Learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}], "referenceMentions": [{"referenceID": 10, "context": "After further motivating the problem with details from the example of forestry planning we introduce a general definition of LSST planning as a reinforcement learning (Sutton and Barto, 1998) problem and discuss the properties a solution needs to possess.", "startOffset": 167, "endOffset": 191}, {"referenceID": 12, "context": "We demonstrate how policy gradients (Williams, 1992) can satisfy many of these properties for LSST problems.", "startOffset": 36, "endOffset": 52}, {"referenceID": 2, "context": "One major impact on forest health is insect infestation such as theMountain Pine Beetle (MPB)(Eng et al., 2004).", "startOffset": 93, "endOffset": 111}, {"referenceID": 7, "context": "The final set of methods in common use are stochastic local search methods such as tabu search, genetic algorithms and simulated annealing (Pukkala and Kurttila, 2005).", "startOffset": 139, "endOffset": 167}, {"referenceID": 1, "context": "Increasingly, there are efforts in forestry planning to improve modelling of uncertainty and complex, dynamic processes in the forest, such as fire or pest infestations (Baskent and Keles, 2005).", "startOffset": 169, "endOffset": 194}, {"referenceID": 5, "context": "PG researchers have recently achieved significant gains in the types of reinforcement learning problems that can be solved (Kersting and Driessens, 2008; Riedmiller et al., 2007; Sutton et al., 2000; Williams, 1992).", "startOffset": 123, "endOffset": 215}, {"referenceID": 8, "context": "PG researchers have recently achieved significant gains in the types of reinforcement learning problems that can be solved (Kersting and Driessens, 2008; Riedmiller et al., 2007; Sutton et al., 2000; Williams, 1992).", "startOffset": 123, "endOffset": 215}, {"referenceID": 11, "context": "PG researchers have recently achieved significant gains in the types of reinforcement learning problems that can be solved (Kersting and Driessens, 2008; Riedmiller et al., 2007; Sutton et al., 2000; Williams, 1992).", "startOffset": 123, "endOffset": 215}, {"referenceID": 12, "context": "PG researchers have recently achieved significant gains in the types of reinforcement learning problems that can be solved (Kersting and Driessens, 2008; Riedmiller et al., 2007; Sutton et al., 2000; Williams, 1992).", "startOffset": 123, "endOffset": 215}, {"referenceID": 11, "context": "Policy gradient algorithms are founded on the observation that the expected value of a stochastic policy can be computed using previously sampled trajectories by weighting the rewards actually received during each trajectory by the probability of that trajectory given the current policy (Sutton et al., 2000) (Riedmiller et al.", "startOffset": 288, "endOffset": 309}, {"referenceID": 8, "context": ", 2000) (Riedmiller et al., 2007):", "startOffset": 8, "endOffset": 33}, {"referenceID": 8, "context": "However, it turns out that computing the gradient of the value function with respect to the policy parameters,\u2207\u03b8V \u03b8, does not require knowing V \u03b8 or P (Riedmiller et al., 2007; Sutton et al., 2000):", "startOffset": 151, "endOffset": 197}, {"referenceID": 11, "context": "However, it turns out that computing the gradient of the value function with respect to the policy parameters,\u2207\u03b8V \u03b8, does not require knowing V \u03b8 or P (Riedmiller et al., 2007; Sutton et al., 2000):", "startOffset": 151, "endOffset": 197}, {"referenceID": 8, "context": "Riedmiller et al. (2007) describe a few techniques called optimal base-lining and Rprop to counteract these difficulties which we use.", "startOffset": 0, "endOffset": 25}, {"referenceID": 8, "context": "This is valid since \u2207\u03b8 \u222b p(k|\u03b8)dk = \u2207\u03b81 = 0 (Riedmiller et al., 2007).", "startOffset": 44, "endOffset": 69}, {"referenceID": 8, "context": "Another technique used to improve policy gradient performance is called Rprop (Riedmiller et al., 2007) which replaces scaled updating using the full gradient as in eq (9) with an update-value, \u2206\u03b8, which has the same direction as \u2207\u03b8V \u03b8 but a magnitude that is unrelated to the gradient.", "startOffset": 78, "endOffset": 103}, {"referenceID": 8, "context": "See (Riedmiller et al., 2007) for more details.", "startOffset": 4, "endOffset": 29}, {"referenceID": 8, "context": "1 which has been found to be reasonable for many problems (Riedmiller et al., 2007).", "startOffset": 58, "endOffset": 83}, {"referenceID": 10, "context": "Our work builds upon research on model-free reinforcement learning (RL)(Sutton and Barto, 1998) and policy gradient methods (Riedmiller et al.", "startOffset": 71, "endOffset": 95}, {"referenceID": 8, "context": "Our work builds upon research on model-free reinforcement learning (RL)(Sutton and Barto, 1998) and policy gradient methods (Riedmiller et al., 2007).", "startOffset": 124, "endOffset": 149}, {"referenceID": 8, "context": "Our work builds upon research on model-free reinforcement learning (RL)(Sutton and Barto, 1998) and policy gradient methods (Riedmiller et al., 2007). Sutton et al. (2000) described the basis for using policy gradients within an RL framework.", "startOffset": 125, "endOffset": 172}, {"referenceID": 8, "context": "Our work builds upon research on model-free reinforcement learning (RL)(Sutton and Barto, 1998) and policy gradient methods (Riedmiller et al., 2007). Sutton et al. (2000) described the basis for using policy gradients within an RL framework. The algorithm presented here includes some extensions to basic PG such as reward baselining and Rprop. More advanced PG techniques are available such as natural gradients, which have been shown by Riedmiller et al. (2007) to significantly improve performance by computing the reward baseline using the Fisher information matrix of the gradient.", "startOffset": 125, "endOffset": 465}, {"referenceID": 6, "context": "Policy Iteration (LSPI) (Lagoudakis and Parr, 2001), another RL approach that uses a parameterized policy and learns without a transition model by using a stored history of sampled trajectories.", "startOffset": 24, "endOffset": 51}, {"referenceID": 9, "context": "New research into decentralized (PO)MDPs (Seuken and Zilberstein, 2008) brings together various threads in coordinated multi-agent planning into one language.", "startOffset": 41, "endOffset": 71}, {"referenceID": 3, "context": "Guestrin et al. (2002) applied LSPI to multi-agent problems to find an initial estimate of the value function and specify a policy in proportion to the values of each action.", "startOffset": 0, "endOffset": 23}, {"referenceID": 5, "context": "Recently, Kersting and Driessens (2008) introduced a non-parametric policy gradient approach that might be useful for LSST planning.", "startOffset": 10, "endOffset": 40}, {"referenceID": 0, "context": "Thus, planning over time periods of varying lengths, such as is done in the SMDP literature (Barto and Mahadevan, 2003), could be useful.", "startOffset": 92, "endOffset": 119}], "year": 2009, "abstractText": "We introduce a challenging real-world planning problem where actions must be taken at each location in a spatial area at each point in time. We use forestry planning as the motivating application. In Large Scale Spatial-Temporal (LSST) planning problems, the state and action spaces are defined as the cross-products of many local state and action spaces spread over a large spatial area such as a city or forest. These problems possess state uncertainty, have complex utility functions involving spatial constraints and we generally must rely on simulations rather than an explicit transition model. We define LSST problems as reinforcement learning problems and present a solution using policy gradients. We compare two different policy formulations: an explicit policy that identifies each location in space and the action to take there; and an abstract policy that defines the proportion of actions to take across all locations in space. We show that the abstract policy is more robust and achieves higher rewards with far fewer parameters than the elementary policy. This abstract policy is also a better fit to the properties that practitioners in LSST problem domains require for such methods to be widely useful.", "creator": "TeX"}}}