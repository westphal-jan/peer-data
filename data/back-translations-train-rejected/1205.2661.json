{"id": "1205.2661", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs", "abstract": "We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of ~O(HSpAT). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds.", "histories": [["v1", "Wed, 9 May 2012 14:47:06 GMT  (177kb)", "http://arxiv.org/abs/1205.2661v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["peter l bartlett", "ambuj tewari"], "accepted": false, "id": "1205.2661"}, "pdf": {"name": "1205.2661.pdf", "metadata": {"source": "CRF", "title": "REGAL: A Regularization based Algorithm for Reinforcement Learning in Weakly Communicating MDPs", "authors": ["Peter L. Bartlett", "Ambuj Tewari"], "emails": [], "sections": [{"heading": null, "text": "We provide an algorithm that achieves the optimal remorse rate in an unknown, poorly communicating Markov Decision Process (MDP). The algorithm works in episodes in which it selects a policy based on the range of the optimal bias vector for each episode. In an MDP with S-states and A-actions, whose optimal bias vector spans a range of H, we show a remorse limit limited by O-value (HS \u221a AT)."}, {"heading": "1 INTRODUCTION", "text": "In Learn Amplification, an agent interacts with an environment while trying to maximize the overall reward it currently accumulates. Markov Decision Processes (MDPs) is the most commonly used model for the environment. To each MDP, a state space S and an action space A. Suppose there are S states and A actions, the parameters of the MDP then consist of S \u00b7 A State Transition Distributions Ps, a and S \u00b7 A rewards r (s, a). If the agent receives a reward, he receives a reward r (s, a) and the probability of moving to the state s is Ps, a (s). The agent does not know the parameters Ps, a and r (s, a) of the MDP in advance, but must learn them by interacting directly with the environment, thereby facing exploration vs. the exploitation trade-off that Kearns and Singh successfully describe [2002]."}, {"heading": "1.1 RELATED WORK", "text": "The problem of simultaneous estimation and control of MDPs has been explored in control theory [Kumarand Varaiya, 1986], operational research [Burnetas and Katehakis, 1997], and machine learning communities. In machine learning literature, finite time limits for undiscounted MDPs were pushed by Kearns and Singh [2002]. Their groundbreaking work produced a long thread of research [Brafman and Tennenholtz, 2002, Kakade, 2003, Strehl and Littman, 2005, Strehl et al., 2006, Auer and Ortner, 2007, Tewari and Bartlett, 2008, Auer et al., 2009a]. These efforts improved the S and A dependence of their boundaries, explored lower boundaries, and explored other ways to study exploration vs. exploitation tradability.The results of Auer and Ortner, MDp. [2007] and Bartlett, 2008] focused on unmanageable MDGPs."}, {"heading": "2 PRELIMINARIES", "text": "To simplify, we assume that the rewards r (s, a) and only the transition probabilities Ps, a are known, and only the transition probabilities Ps, a are unknown. This is not a restrictive assumption, since the rewards can also be estimated at the expense of increasing the regret caused by a constant factor. At the heart of the problem is not knowing the transition probabilities. Remember the definition of the dynamic programmer T, (T V) (s): = max a) A (r (s, a) + P > s, aV). Let V n (s) specify the maximum possible expected reward (stationary or non-stationary across all policies) that can be achieved in n increments starting from s. This can be calculated by simply recursion V 0 (s) = 0V n + 1 = T V V n n."}, {"heading": "3 REGAL", "text": "In this section we introduce Regal, an algorithm (see algorithm 1) inspired by the Ucrl2 algorithm by Auer et al. [2009a]. Before describing the algorithm, let us make a notation, then let us make an estimate of the probability of state at the time of the change of state, be a (s) sequence of the probability of state (s) = N (s) = N (s) max. (s) max. (s) max. (s) max. (s) max. (s) max. (s) max. (s) max. (s) max. (s) max. (s) max. (s)"}, {"heading": "4 SPAN AND DIAMETER", "text": "That is, when we define a sequence of vectors that vn + 1 = T vn starting from an arbitrary v0, thenlim n \u2192 nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; bsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; nbsp; n"}, {"heading": "5 LOWER BOUND", "text": "In this section, we provide a lower limit that corresponds to the upper limit, with the exception of the dependence on S. Based on the results in section 4, we can therefore show that the lower limit is narrow. Theory 6. There is a universal constant c0 that applies to all S, A, dow and each algorithm G, there is an MDP M with Dow (M). Therefore, we assume that the lower limit is narrow. Theory 6. There is a universal constant c0 that applies to all S, A, dow and each algorithm G, there is an MDP M with Dow (M)."}, {"heading": "6 ANALYSIS", "text": "In this section, we will first set up a notation and retrieve key terms from the technical report by Auer et al. [2009b] that will be useful to prove all three theorems. Let M denote the true (but unknown) underlying MDP. The quantities \u03bb? and h? will apply to this MDP during this section. Let \"k = \u2211 s, a vk (s, a) denote the length of the episode. Let\" k, \"h? k denote the minimum component equal to 0\" (Mk), h? (Mk), respectively. Let the transition matrices of the episodes be denoted by P (s, a) in Mk and M. Let's further assume that h? k? k has its minimum component equal to 0, \"so that sp (h?) = h?) and sp? k (h? k) of the following episode are the same."}, {"heading": "6.1 PROOF OF THEOREM 1", "text": "Consider an episode k-k (k) k-k (k) k-k (k) k-k (k) k-k (k) k-k (k) k-k (k) k-k (k) k-k (k) k-k (k) k-k (k) k-k (k) k-k (k). (9) Hence, k-k (s, a) k-k (s, a) vk (s, a) [s, a-k (s, a) [v-k) s (s, a-k) sp (h? k) -sp (h) \u2212 sp (h) \u2212 sp (s, a-k) s-sp (s, a-sp), a-sp (s-sp)."}, {"heading": "6.2 PROOF OF THEOREM 2", "text": "Consider an episode k: G: Then we have \u03bb? k: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D"}, {"heading": "6.3 PROOF OF THEOREM 3", "text": "In this case, we have episodes that consist of several sub-episodes, whose lengths increase in geometric progression. Let vk, j (s, a) the number of times (s, a) is visited during the sub-episode j of episode k. Thus, the transition matrices of k, j (vk, j (s, a). Let f, j, h? k, j (s). Let vk, j, rk, j (s). Let vk, j (s)."}, {"heading": "Acknowledgments", "text": "We thank DARPA for its support under the FA8750-05-2-0249 premium."}, {"heading": "A. N. Burnetas and M. N. Katehakis. Optimal adaptive", "text": "Mathematics of Operations Research, 22 (1): 222-255, 1997. Nicolo Cesa-Bianchi and Ga \"bor Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.Sham Cockade. On the Sample Complexity of Reinforcement Learning. Doctoral Thesis, Gatsby Computational Neuroscience Unit, University College London, 2003.Michael Kearns and Satinder Singh. Near-optimal Amplification Learning in polynomial time. Machine Learning, 49: 209-232, 2002.P. R. Kumar and P. Varaiya. Stochastic Systems: Estimation, Identification, and Adaptive Control. Prentice Hall, 1986.Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, 1994.Alexander L. Strehl and Michael Littman. A Theoretical Analysis of Model-Based Systems."}], "references": [{"title": "Logarithmic online regret bounds for undiscounted reinforcement learning", "author": ["Peter Auer", "Ronald Ortner"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Auer and Ortner.,? \\Q2007\\E", "shortCiteRegEx": "Auer and Ortner.", "year": 2007}, {"title": "Nearoptimal regret bounds for reinforcement learning", "author": ["Peter Auer", "Thomas Jaksch", "Ronald Ortner"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Auer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2009}, {"title": "Near-optimal regret bounds for reinforcement learning (full version), 2009b. URL: http://institute.unileoben.ac.at/infotech/publications/ ucrl2.pdf", "author": ["Peter Auer", "Thomas Jaksch", "Ronald Ortner"], "venue": null, "citeRegEx": "Auer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2009}, {"title": "R-MAX \u2013 a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["Ronen I. Brafman", "Moshe Tennenholtz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brafman and Tennenholtz.,? \\Q2002\\E", "shortCiteRegEx": "Brafman and Tennenholtz.", "year": 2002}, {"title": "Optimal adaptive policies for Markov decision processes", "author": ["A.N. Burnetas", "M.N. Katehakis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Burnetas and Katehakis.,? \\Q1997\\E", "shortCiteRegEx": "Burnetas and Katehakis.", "year": 1997}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "G\u00e1bor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "On the Sample Complexity of Reinforcement Learning", "author": ["Sham Kakade"], "venue": "PhD thesis, Gatsby Computational Neuroscience Unit,", "citeRegEx": "Kakade.,? \\Q2003\\E", "shortCiteRegEx": "Kakade.", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["Michael Kearns", "Satinder Singh"], "venue": "Machine Learning,", "citeRegEx": "Kearns and Singh.,? \\Q2002\\E", "shortCiteRegEx": "Kearns and Singh.", "year": 2002}, {"title": "Stochastic systems: Estimation, identification, and adaptive control", "author": ["P.R. Kumar", "P.P. Varaiya"], "venue": null, "citeRegEx": "Kumar and Varaiya.,? \\Q1986\\E", "shortCiteRegEx": "Kumar and Varaiya.", "year": 1986}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["Martin L. Puterman"], "venue": null, "citeRegEx": "Puterman.,? \\Q1994\\E", "shortCiteRegEx": "Puterman.", "year": 1994}, {"title": "A theoretical analysis of model-based interval estimation", "author": ["Alexander L. Strehl", "Michael Littman"], "venue": "In Proceedings of the Twenty-Second International Conference on Machine Learning,", "citeRegEx": "Strehl and Littman.,? \\Q2005\\E", "shortCiteRegEx": "Strehl and Littman.", "year": 2005}, {"title": "PAC model-free reinforcement learning", "author": ["Alexander L. Strehl", "Lihong Li", "Eric Wiewiora", "John Langford", "Michael L. Littman"], "venue": "In Proceedings of the Twenty-Third International Conference on Machine Learning,", "citeRegEx": "Strehl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2006}, {"title": "Optimistic linear programming gives logarithmic regret for irreducible MDPs", "author": ["Ambuj Tewari", "Peter L. Bartlett"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Tewari and Bartlett.,? \\Q2008\\E", "shortCiteRegEx": "Tewari and Bartlett.", "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "exploitation trade-off that Kearns and Singh [2002] succinctly describe as,", "startOffset": 28, "endOffset": 52}, {"referenceID": 8, "context": "The problem of simultaneous estimation and control of MDPs has been studied in the control theory [Kumar and Varaiya, 1986], operations research [Burnetas and Katehakis, 1997] and machine learning communities.", "startOffset": 98, "endOffset": 123}, {"referenceID": 4, "context": "The problem of simultaneous estimation and control of MDPs has been studied in the control theory [Kumar and Varaiya, 1986], operations research [Burnetas and Katehakis, 1997] and machine learning communities.", "startOffset": 145, "endOffset": 175}, {"referenceID": 0, "context": "The problem of simultaneous estimation and control of MDPs has been studied in the control theory [Kumar and Varaiya, 1986], operations research [Burnetas and Katehakis, 1997] and machine learning communities. In the machine learning literature, finite time bounds for undiscounted MDPs were pioneered by Kearns and Singh [2002]. Their seminal work spawned a long thread of research [Brafman and Tennenholtz, 2002, Kakade, 2003, Strehl and Littman, 2005, Strehl et al.", "startOffset": 146, "endOffset": 329}, {"referenceID": 0, "context": ", 2006, Auer and Ortner, 2007, Tewari and Bartlett, 2008, Auer et al., 2009a]. These efforts improved the S and A dependence of their bounds, investigated lower bounds and explored other ways to study the exploration vs. exploitation trade-off. The results of Auer and Ortner [2007] and Tewari and Bartlett [2008] applied to unichain and ergodic MDPs respectively.", "startOffset": 8, "endOffset": 283}, {"referenceID": 0, "context": ", 2006, Auer and Ortner, 2007, Tewari and Bartlett, 2008, Auer et al., 2009a]. These efforts improved the S and A dependence of their bounds, investigated lower bounds and explored other ways to study the exploration vs. exploitation trade-off. The results of Auer and Ortner [2007] and Tewari and Bartlett [2008] applied to unichain and ergodic MDPs respectively.", "startOffset": 8, "endOffset": 314}, {"referenceID": 0, "context": ", 2006, Auer and Ortner, 2007, Tewari and Bartlett, 2008, Auer et al., 2009a]. These efforts improved the S and A dependence of their bounds, investigated lower bounds and explored other ways to study the exploration vs. exploitation trade-off. The results of Auer and Ortner [2007] and Tewari and Bartlett [2008] applied to unichain and ergodic MDPs respectively. Recently, Auer et al. [2009a] gave an algorithm for communicating MDPs whose expected regret after T steps is, with high probability, \u00d5(D(M)S \u221a AT ) .", "startOffset": 8, "endOffset": 395}, {"referenceID": 1, "context": "So, we not only extend the result of Auer et al. [2009a] to weakly communicating MDPs but also replace D by a smaller quantity, sp(h).", "startOffset": 37, "endOffset": 57}, {"referenceID": 1, "context": "In this section, we present Regal, an algorithm (see Algorithm 1) inspired by the Ucrl2 algorithm of Auer et al. [2009a]. Before we describe the algorithm, let us set up some notation.", "startOffset": 101, "endOffset": 121}, {"referenceID": 9, "context": "In such MDPs, value iteration is known to converge [Puterman, 1994].", "startOffset": 51, "endOffset": 67}, {"referenceID": 9, "context": "Now, if the MDP M is periodic apply the following aperiodicity transform [Puterman, 1994] to get a new MDP M\u0303", "startOffset": 73, "endOffset": 89}, {"referenceID": 2, "context": "Using the results in Section 4, it is possible to show that the algorithms of Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] enjoy a regret bound of \u00d5(Dow(M) \u221a SAT ) for ergodic MDPs.", "startOffset": 78, "endOffset": 108}, {"referenceID": 2, "context": "Using the results in Section 4, it is possible to show that the algorithms of Burnetas and Katehakis [1997] and Tewari and Bartlett [2008] enjoy a regret bound of \u00d5(Dow(M) \u221a SAT ) for ergodic MDPs.", "startOffset": 78, "endOffset": 139}, {"referenceID": 1, "context": "We modify the MDP used by Auer et al. [2009a] to prove their lower bound.", "startOffset": 26, "endOffset": 46}, {"referenceID": 1, "context": "We will first set up notation and recall key lemmas from the technical report of Auer et al. [2009b] that will be useful for proving all three theorems.", "startOffset": 81, "endOffset": 101}, {"referenceID": 1, "context": "The following bound on the numberm of episodes until time T was proved in Auer et al. [2009b]. Lemma 7.", "startOffset": 74, "endOffset": 94}, {"referenceID": 1, "context": "The following result from Auer et al. [2009b] assures us that the contribution from \u201cbad\u201d episodes (in which the true MDP M does not lie in M) is small.", "startOffset": 26, "endOffset": 46}, {"referenceID": 1, "context": "Equation (9) in Auer et al. [2009b] gives, \u2211", "startOffset": 16, "endOffset": 36}], "year": 2009, "abstractText": "We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of \u00d5(HS \u221a AT ). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds.", "creator": "TeX"}}}