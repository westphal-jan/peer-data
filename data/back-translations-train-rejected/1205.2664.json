{"id": "1205.2664", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2012", "title": "A Bayesian Sampling Approach to Exploration in Reinforcement Learning", "abstract": "We present a modular approach to reinforcement learning that uses a Bayesian representation of the uncertainty over models. The approach, BOSS (Best of Sampled Set), drives exploration by sampling multiple models from the posterior and selecting actions optimistically. It extends previous work by providing a rule for deciding when to resample and how to combine the models. We show that our algorithm achieves nearoptimal reward with high probability with a sample complexity that is low relative to the speed at which the posterior distribution converges during learning. We demonstrate that BOSS performs quite favorably compared to state-of-the-art reinforcement-learning approaches and illustrate its flexibility by pairing it with a non-parametric model that generalizes across states.", "histories": [["v1", "Wed, 9 May 2012 14:42:20 GMT  (262kb)", "http://arxiv.org/abs/1205.2664v1", "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI2009)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["john asmuth", "lihong li", "michael l littman", "ali nouri", "david wingate"], "accepted": false, "id": "1205.2664"}, "pdf": {"name": "1205.2664.pdf", "metadata": {"source": "CRF", "title": "A Bayesian Sampling Approach to Exploration in Reinforcement Learning", "authors": ["John Asmuth", "Lihong Li", "Michael L. Littman", "Ali Nouri", "David Wingate"], "emails": [], "sections": [{"heading": null, "text": "The approach, BOSS (Best of Sampled Set), pushes exploration forward by sampling multiple models from the back and optimistically selecting measures. It extends previous work by providing a rule for deciding when and how to re-evaluate the models. We show that our algorithm is highly likely to achieve a near-optimal reward with sample complexity that is low relative to the speed at which the rear distribution converges during learning. We show that BOSS performs reasonably well compared to modern approaches to enhanced learning, and illustrate its flexibility by combining it with a non-parametric model that generalizes across states."}, {"heading": "1 INTRODUCTION", "text": "It is as if it were an experiment that did not exist, but an experiment that did not exist. (...) It is as if it had been an experiment. (...) It is as if it had been an experiment. (...) It is as if it had been an experiment. (...) It is as if it had been an experiment. (...) It is as if it had been an experiment. (...) It is as if it had been an experiment. (...) It is as if it had been an experiment. (...) It is as if it had been an experiment. (...)"}, {"heading": "2 BOSS: BEST OF SAMPLED SET", "text": "The idea of posterior scanning for decision-making has been around for decades (Thompson, 1933). Several newer algorithms have used this technique for Bayesian RL (Strens, 2000; Wilson et al., 2007). In this context, the posterior areas of the Markov Decision Process Space are maintained (MDPs) and the posterior area scanning requires drawing a complete MDP from this distribution process. Each scanning method must address a few key questions: 1) How many models will be scanned via the Markov Decision Process Space (MDPs) and the posterior area scanning requires complete scanning after each T question. There are challenges in selecting the correct value of T, but it can lead to a \"thrashing\" behavior in which the agent quickly switches exploration plans and ultimately makes little progress. Large T can lead to slower learning than new information required for planning in the 2000 posterior areas."}, {"heading": "3 ANALYSIS", "text": "This section provides a formal analysis of the efficiency of BOSS exploration. We consider the algorithm as a non-stationary policy for which a value function can be defined. Accordingly, the value of the state s when visited by algorithm A at the time t designated by V At (st) is the expected discounted sum of future rewards that the algorithm will collect after visiting s at the time t. Our goal is to show that if the parameters K and B are appropriately and with high probability selected, V At (st) will be nearly optimal, with the exception of a polynomic number of steps (Theorem 3.1). Our goal and some of our techniques closely follow the work within the PAC-MDP (Kakade, 2003; Strehl et al., 2006)."}, {"heading": "3.1 A GENERAL SAMPLE COMPLEXITY BOUND FOR BOSS", "text": "If possible, we refer to quantities related to this MDP, such as V-M function (V-1), according to its abstracts, V-1. By adoption, the true MDP-M-1 model is drawn from the previous distribution, and so after observing a sequence of transition processes, m-1 can, however, be considered drawn from the rear distribution. (Tagma 3.1 Let s0 be a solid state, p-2 the rear distribution via MDPs, and so after considering a sequence of transition processes, m-1 ln 1), then with a probability of at least 1 \u2212 1, a model among these K models is optimistic compared to m-1: maxi V-2 (s0). (s0). Proof (sketch). For each fixed, true model m-1, we define P as the probability of an optimistic model according to p-1: P."}, {"heading": "3.2 THE BAYESIAN CONCENTRATION SAMPLE COMPLEXITY", "text": "Theorem 3.1 depends on the Bayian concentration sample complexity f. A complete analysis of the f concentration is outside the scope of this work. Generally, f depends on certain properties of the model space as well as the previous distribution. While it is likely that a more accurate estimate of f can be obtained in special cases, we will use a fairly general result from Zhang (2006) to pre-determine our sample complexity of exploration in Theorem 3.1 on certain properties of Bayesian. Future work may instantiate this general result on special MDP classes and previous distributions. We need two key parameters introduced by Zhang (2006; Section 5.2). The first is the critical radius before mass, \u03b5p, n, which characterizes how dense the previous distribution p is around the true model (smaller values imply denser priors). The second is the critical upper radius with the coefficient 2 / 3, the denoted primary distribution, the decay of which is considered to be large (the consistency)."}, {"heading": "4 EXPERIMENTS", "text": "This section presents arithmetic experiments with BOSS, evaluating its performance on a simple literature domain to allow comparison with other published approaches. Consider the well-researched 5-state chain problem (Strens, 2000; Poupart et al., 2006). However, the agent has two actions: Action 1 moves the agent along the chain forward, and Action 2 resets the agent to the first node. Action 1, when taken from the last node, leaves the agent where it is and gives a reward of 10 - all other rewards are 0. Action 2 always has a reward of 2. However, with a 0.2 probability, the results will be reversed. Optimal behavior is to always choose Action 1 to achieve the high reward at the end of the chain. Slip probability 0.2 is the same for all state pair of shareholders. Action 2 always has a reward of 2."}, {"heading": "5 BAYESIAN MODELING OF STATE CLUSTERS", "text": "We say that two states are in the same cluster if their probability distributions of relative outcomes are the same regardless of an action. In Chain, for example, the results advance along the chain or start at the beginning. Both actions produce the same distribution of these two outcomes regardless of the state, Action 1 is 0.8 / 0.2 and Action 2 is 0.2 / 0.8, so that Chain can be considered a cluster. We present a variant of the chain example, the two-cluster Chain2, which includes an additional state cluster. Cluster 1 - States 1, 3 and 5 - behaves identically to the cluster in Chain. Cluster 2 - States 2 and 4 - has roughly the reverse distribution (Action 1 0.3 / 0.7, Action 2 0.7 / 0.3). RAM-RMAX can take advantage of the cluster structure, but only if it is known in advance. In this section, we show how BOSS can learn with a previously unknown structure to expand it into a cluster."}, {"heading": "5.1 A NON-PARAMETRIC MODEL OF STATE CLUSTERING", "text": "We derive a non-parametric cluster model that can simultaneously use observed transition outcomes to determine which parameters can bind and estimate their values. We first assume that the observed outcomes are generated independently for each state in a cluster c, but assume a common multinomial distribution of data within a cluster to improve our estimates of the associated transition probabilities. The generative model is a generative model in which all states in a particular cluster are brought together, meaning that we can use all observed outcomes from states in a cluster to improve our estimates of the associated transition probabilities. The generative model is a generative model in which CRP (s) is a generative model."}, {"heading": "5.2 BOSS WITH CLUSTERING PRIOR", "text": "We ranked BOSS in a factor design where we varied the environment (Chain vs. Chain2) and the previous one (Tied, Full, vs. Cluster, where Cluster is the model described in the previous subsection), and for our experiments, BOSS used a discounting factor \u03b3 = 0.95, knowledge parameter B = 10, and a sample size of K = 5. Cluster CRP used \u03b1 = 0.5, and whenever a sample was required, the Gibbs sampler ran over a burning period of 500 sweeps with 50 sweeps between each sample.Figure 1 shows the results of running BOSS with different priors in Chain and Chain2. The top row of the chart corresponds to the results for Chain. Moving from left to right, BOSS is operated with weaker priors - Tied, Cluster, and Full. Unsurprisingly, performance decreases with weaker priors."}, {"heading": "5.3 VARYING K", "text": "The experiments described in the previous section used model samples of size K = 5. Our next experiment should show how variation in sample size impacts. Note that Bayesian DP BOSS is very similar at K = 1, so it is important to quantify the effects of this parameter to understand the relationship between these algorithms. Figure 2 shows the result of running BOSS on Chain2 with the same parameters as in the previous section. Note that performance generally improves with K. The difference between K = 1 and K = 10 is statistically significant (t-test p < 0.001)."}, {"heading": "5.4 6x6 MARBLE MAZE", "text": "To demonstrate the exploration behavior of our algorithm, we developed a 6x6 grid-world domain with greater dynamics = For this standard dynamics (Russell & Norvig, 1994).In this environment, the four actions, N, S, E and W., guide the agent through the labyrinth on his way to the target. Each action has its intended effect with probability.8, and the rest of the time the agent travels to one of the two perpendicular states with equal probability. If there is a wall in the direction the agent is trying to go, it will stay where it is. Each step has a cost of 0.001, and the rest of the time the agent will be in \u2212 1 and + 1 states in case of falling into a pit or reaching the target, respectively. The map of the domain, along with its optimal policies, is illustrated in Figure 3.The dynamics of this environment are such that each local pattern of walls (at most 16) can be modelled as a separate cluster."}, {"heading": "5.5 COMPUTATIONAL COMPLEXITY", "text": "The computational time required by BOSS depends on two different factors: First, the time required for perstep planning using value titeration scales with the number of recorded MDPs, K. Second, the time required for sampling new MDPs depends linearly on K and the type of previously used MDPs. For a simple pre-stage, such as Full, samples can be taken extremely quickly. For a more complex pre-stage, such as clusters, samples can take longer. In the 6x6 marble maze, samples were taken at a rate of about one every ten seconds. It is worth noting that sampling can be done in parallel."}, {"heading": "6 CONCLUSIONS", "text": "We have compared the algorithm with several standardized exploration approaches and demonstrated that it is as good as the most common algorithm in any scenario tested. We have also derived a non-parametric Bayesian cluster model and shown how BOSS could use it to learn faster than non-generalizing comparison algorithms. In future work, we plan to analyze the more general environment in which priors are only approximate indicators of real distribution across environments. We are also interested in hierarchical approaches that can learn more precise priors in a transfer-like environment than non-generalizing comparison algorithms. Highly related work in this direction has been presented by Wilson et al. (2007). An interesting direction for future research is to consider extensions of our grouped state model, in which clustering in space occurs."}, {"heading": "Acknowledgements", "text": "We thank Josh Tenenbaum, Tong Zhang and the reviewers. This work was supported by DARPA IPTO FA8750-05-2-0249 and NSF IIS-0713435."}], "references": [{"title": "Exchangeability and related topics", "author": ["D. Aldous"], "venue": null, "citeRegEx": "Aldous,? \\Q1985\\E", "shortCiteRegEx": "Aldous", "year": 1985}, {"title": "Infinite latent feature models and the Indian buffet process", "author": ["T.L. Griffiths", "Z. Ghahramani"], "venue": "Neural Information Processing Systems (NIPS)", "citeRegEx": "Griffiths and Ghahramani,? \\Q2006\\E", "shortCiteRegEx": "Griffiths and Ghahramani", "year": 2006}, {"title": "On the sample complexity of reinforcement learning", "author": ["S.M. Kakade"], "venue": "Doctoral dissertation,", "citeRegEx": "Kakade,? \\Q2003\\E", "shortCiteRegEx": "Kakade", "year": 2003}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M.J. Kearns", "S.P. Singh"], "venue": "Machine Learning,", "citeRegEx": "Kearns and Singh,? \\Q2002\\E", "shortCiteRegEx": "Kearns and Singh", "year": 2002}, {"title": "Efficient reinforcement learning with relocatable action models", "author": ["B.R. Leffler", "M.L. Littman", "T. Edmunds"], "venue": "Proceedings of the Twenty-Second Conference on Artificial Intelligence (AAAI-07)", "citeRegEx": "Leffler et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Leffler et al\\.", "year": 2007}, {"title": "An analytic solution to discrete Bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "Proceedings of the 23rd International Conference on Machine Learning (pp. 697\u2013704)", "citeRegEx": "Poupart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Artificial intelligence: A modern approach", "author": ["S.J. Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell and Norvig,? \\Q1994\\E", "shortCiteRegEx": "Russell and Norvig", "year": 1994}, {"title": "Incremental model-based learners with formal learning-time guarantees", "author": ["A.L. Strehl", "L. Li", "M.L. Littman"], "venue": "Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence (UAI", "citeRegEx": "Strehl et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Strehl et al\\.", "year": 2006}, {"title": "A Bayesian framework for reinforcement learning", "author": ["M.J.A. Strens"], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning (ICML", "citeRegEx": "Strens,? \\Q2000\\E", "shortCiteRegEx": "Strens", "year": 2000}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": null, "citeRegEx": "Thompson,? \\Q1933\\E", "shortCiteRegEx": "Thompson", "year": 1933}, {"title": "The role of exploration in learning control", "author": ["S.B. Thrun"], "venue": null, "citeRegEx": "Thrun,? \\Q1992\\E", "shortCiteRegEx": "Thrun", "year": 1992}, {"title": "Bayesian sparse sampling for on-line reward optimization", "author": ["T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans"], "venue": "ICML \u201905: Proceedings of the 22nd international conference on Machine Learning (pp. 956\u2013963)", "citeRegEx": "Wang et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2005}, {"title": "Multitask reinforcement learning: A hierarchical Bayesian approach", "author": ["A. Wilson", "A. Fern", "S. Ray", "P. Tadepalli"], "venue": "Machine Learning, Proceedings of the TwentyFourth International Conference (ICML", "citeRegEx": "Wilson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2007}, {"title": "From \u03b5-entropy to KL-entropy: Analysis of minimum information complexity density estimation", "author": ["T. Zhang"], "venue": "The Annals of Statistics,", "citeRegEx": "Zhang,? \\Q2006\\E", "shortCiteRegEx": "Zhang", "year": 2006}], "referenceMentions": [{"referenceID": 5, "context": "A state-of-the-art belief-lookahead approach is BEETLE (Poupart et al., 2006), which plans in the continuous belief space defined by the agent\u2019s uncertainty.", "startOffset": 55, "endOffset": 77}, {"referenceID": 11, "context": "Myopic (Wang et al., 2005) approaches make decisions to reduce uncertainty, but they do not explicitly consider how this reduced uncertainty will impact future reward.", "startOffset": 7, "endOffset": 26}, {"referenceID": 10, "context": "Undirected (Thrun, 1992) approaches take exploratory actions, but without regard to what parts of their environment models remain uncertain.", "startOffset": 11, "endOffset": 24}, {"referenceID": 8, "context": "A sophisticated approach that falls into this category is Bayesian DP (Strens, 2000).", "startOffset": 70, "endOffset": 84}, {"referenceID": 9, "context": "The idea of sampling from the posterior for decision making has been around for decades (Thompson, 1933).", "startOffset": 88, "endOffset": 104}, {"referenceID": 8, "context": "Several recent algorithms have used this technique for Bayesian RL (Strens, 2000; Wilson et al., 2007).", "startOffset": 67, "endOffset": 102}, {"referenceID": 12, "context": "Several recent algorithms have used this technique for Bayesian RL (Strens, 2000; Wilson et al., 2007).", "startOffset": 67, "endOffset": 102}, {"referenceID": 8, "context": "Strens (2000) advocates a T approximating the depth of exploratory planning required.", "startOffset": 0, "endOffset": 14}, {"referenceID": 2, "context": "Our objective, and some of our techniques, closely follow work in the PAC-MDP framework (Kakade, 2003; Strehl et al., 2006).", "startOffset": 88, "endOffset": 123}, {"referenceID": 7, "context": "Our objective, and some of our techniques, closely follow work in the PAC-MDP framework (Kakade, 2003; Strehl et al., 2006).", "startOffset": 88, "endOffset": 123}, {"referenceID": 7, "context": "The proof relies on a general PACMDP theorem by Strehl et al. (2006) by verifying their three required conditions hold.", "startOffset": 48, "endOffset": 69}, {"referenceID": 13, "context": "While it is likely that a more accurate estimate of f can be obtained in special cases, we make use of a fairly general result by Zhang (2006) to relate our sample complexity of exploration in Theorem 3.", "startOffset": 130, "endOffset": 143}, {"referenceID": 13, "context": "2 of Zhang (2006) to solve for n.", "startOffset": 5, "endOffset": 18}, {"referenceID": 2, "context": "1 provides a performance guarantee similar to the PAC-MDP result for RMAX (Kakade, 2003).", "startOffset": 74, "endOffset": 88}, {"referenceID": 8, "context": "Consider the well-studied 5-state chain problem (Chain) (Strens, 2000; Poupart et al., 2006).", "startOffset": 56, "endOffset": 92}, {"referenceID": 5, "context": "Consider the well-studied 5-state chain problem (Chain) (Strens, 2000; Poupart et al., 2006).", "startOffset": 56, "endOffset": 92}, {"referenceID": 5, "context": "Poupart et al. (2006) consider the impact of encoding this constraint as a strong prior on the transition dynamics.", "startOffset": 0, "endOffset": 22}, {"referenceID": 4, "context": "RAM-RMAX (Leffler et al., 2007) ASMUTH ET AL.", "startOffset": 9, "endOffset": 31}, {"referenceID": 5, "context": "Results for BEETLE and exploit are due to Poupart et al. (2006). All runs used a discount factor of \u03b3 = 0.", "startOffset": 42, "endOffset": 64}, {"referenceID": 5, "context": "Poupart et al. (2006) point out that BEETLE (a belief-lookahead approach) is more effective than exploit (an undirected approach) in the Semi scenario, which requires more careful exploration to perform well.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "A similarly positive result (3158) in Full is obtained by Bayesian DP (Strens, 2000).", "startOffset": 70, "endOffset": 84}, {"referenceID": 0, "context": "Here, CRP is a Chinese Restaurant Process (Aldous, 1985), a flexible distribution that allows us to infer both the number of clusters and the assignment of states to clusters.", "startOffset": 42, "endOffset": 56}, {"referenceID": 12, "context": "Highly related work in this direction was presented by Wilson et al. (2007).", "startOffset": 55, "endOffset": 76}], "year": 2009, "abstractText": "We present a modular approach to reinforcement learning that uses a Bayesian representation of the uncertainty over models. The approach, BOSS (Best of Sampled Set), drives exploration by sampling multiple models from the posterior and selecting actions optimistically. It extends previous work by providing a rule for deciding when to resample and how to combine the models. We show that our algorithm achieves nearoptimal reward with high probability with a sample complexity that is low relative to the speed at which the posterior distribution converges during learning. We demonstrate that BOSS performs quite favorably compared to state-of-the-art reinforcement-learning approaches and illustrate its flexibility by pairing it with a non-parametric model that generalizes across states.", "creator": "TeX"}}}