{"id": "1205.2930", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2012", "title": "Density Sensitive Hashing", "abstract": "Nearest neighbors search is a fundamental problem in various research fields like machine learning, data mining and pattern recognition. Recently, hashing-based approaches, e.g., Locality Sensitive Hashing (LSH), are proved to be effective for scalable high dimensional nearest neighbors search. Many hashing algorithms found their theoretic root in random projection. Since these algorithms generate the hash tables (projections) randomly, a large number of hash tables (i.e., long codewords) are required in order to achieve both high precision and recall. To address this limitation, we propose a novel hashing algorithm called {\\em Density Sensitive Hashing} (DSH) in this paper. DSH can be regarded as an extension of LSH. By exploring the geometric structure of the data, DSH avoids the purely random projections selection and uses those projective functions which best agree with the distribution of the data. Extensive experimental results on real-world data sets have shown that the proposed method achieves better performance compared to the state-of-the-art hashing approaches.", "histories": [["v1", "Mon, 14 May 2012 02:27:52 GMT  (63kb)", "http://arxiv.org/abs/1205.2930v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["yue lin", "deng cai", "cheng li"], "accepted": false, "id": "1205.2930"}, "pdf": {"name": "1205.2930.pdf", "metadata": {"source": "CRF", "title": "Density Sensitive Hashing", "authors": ["Yue Lin", "Cheng Li"], "emails": ["linyue29@gmail.com,"], "sections": [{"heading": null, "text": "ar Xiv: 120 5.29 30v1 [cs.IR] 1 4M ay2 012 1Index Terms - Locality Sensitive Hashing, Random Projection, Clustering."}, {"heading": "1 INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2 BACKGROUND AND RELATED WORK", "text": "The generic hashing problem is based on random projection and illumination. The generic hashing problem is based on the following data: X = x1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "3 DENSITY SENSITIVE HASHING", "text": "In this section, we give the detailed description of our proposed Density Sensitive Hashing (DSH), which aims to overcome the disadvantages of both random and learning-based hashing approaches. To ensure that performance increases with increasing code length, DSH uses a framework similar to LSH. Unlike LSH, which generates the projections randomly, DSH uses the geometric structure of the data to guide the selection of the projections. Figure 1 is a toy example to illustrate the basic idea of our approach. There are four Gaussians in a three-dimensional plane and one is asked to encode the data with two-bit hash codes. LSH [8] generates the projections randomly and it is very possible that the data from the same Gaussian approach is encoded by different hash codes. PCA Hashing [34] uses the principal directions of the projectors as a vector for generating all the data from our GSIM structure (in our example, all the GSIM)."}, {"heading": "3.1 Minimum Distortion Quantization", "text": "The first step of the DSH is the quantization of the data. Recently, Pauleve \u0301 et al. [29] have shown that a quantified process for the data points can significantly improve the performance of the search for the closest neighbors. Motivated by this result, we use the k mean algorithm, one of the most popular quantization approaches, to partition the n points into k (k < n) groups. Let S = {S1, \u00b7 \u00b7, Sk} denote a specific quantization result. The distortion, also known as the sum of square error groups (SSE), can be used to measure the quality of the given quantization: SSE = k \u00b2 Sp \u00b2 x \u2212 p \u00b2 p \u00b2 p \u00b2 2 (4) The \u00b5p is the representative point of the p-th group Sp.By noting the SSE \u00b2 group for quantization, the quantization is high."}, {"heading": "3.2 Density Sensitive Projections Generation", "text": "Now we have the quantization result, which is called k-groups S1, \u00b7 \u00b7 \u00b7, Sk, and the i-th group has the center \u00b5i. Instead of generating random projections like LSH, our DSH tries to use this quantization result to guide the process of generating projections. We define the r-next neighbor matrix W of the groups as follows: Definition 1: r-next neighbor matrix W of the groups. With this definition, we can then define r-adjacent groups: Definition 2: r-adjacent groups: Group Si and Group Sj: Group Si and Group Sj are called r-adjacent groups, if and only if Wij = 1. Instead of a random projection, it is more natural to select those groups that adjacent to the neighboring groups well (Si-group): Group Si and Group Sj are called r-adjacent groups, if and only if Wij = 1. Instead of a random projection, it is possible to select those groups that adjacent groups are adjacent to each other (Si-group)."}, {"heading": "3.3 Entropy Based Projections Selection", "text": "Given the k groups, the previous step can generate about 1 2 kr projections. Since k = \u03b1L, our DSH 1 generates 2 \u03b1rL projections | | so far | |. Each projection results in a bit in the code and the usual setting of parameters \u03b1, r yields 12 \u03b1rL > L. Therefore, our DSH requires a projection selection step aimed at selecting L projections from the candidate set containing 12 \u03b1rL projections. From an information theory perspective, a \"good\" binary code should maximize the information / entropy provided by each bit [38]. Using the maximum entropy principle, a binary bit that provides a balanced partitioning of the data points delivers maximum information [32]. Therefore, we calculate the entropy of each candidate character and select the projections that can divide the data most evenly."}, {"heading": "3.4 Computational Complexity Analysis", "text": "Given n data points with dimensionality d, the computational complexity of DSH in the training phase is as follows: 1) O (\u03b1Lpnd): k means with p-iterations to generate \u03b1L groups (Step 1 in Alg. 1). 2) O (((\u03b1L) 2 (d + r): Find all r-adjacent groups (Step 2 in Alg. 1). 3) O (\u03b1Lrd): For each pair of adjacent groups generate the projection and the intercept (Step 3 in Alg. 1). 4) Calculate the entropy for all candidate projections, the O ((\u03b1L) 2dr) (Step 4 in Alg. 1). 5) The uppermost L projections can be found within O (\u03b1Lr log (\u03b1Lr): The binary codes for data points can be found in O (Lnd) (Step 5 in Alg. 1)."}, {"heading": "4 EXPERIMENT", "text": "In this section, we evaluate our DSH algorithm based on the high-dimensional search problem of our closest neighbors. Three large real-world datasets are used in our experiments. \u2022 GIST1M: It contains one million GIST characteristics and each characteristic is represented by a 960 dim vector. This dataset is publicly available2.2. http: / / corpus-texMex.irisa.fr \u2022 Flickr1M: We collect one million images from the Flickr and use a feature extraction code3 to extract one GIST characteristic for each picture.Each image is represented by a 512 dim GIST characteristic vector. This dataset is publicly available.4 \u2022 SIFT1M: It contains one million SIFT characteristics and each characteristic is represented by a 128 dim vector.This dataset is represented by a 512 dim characteristic vector, which is publicly available.We select one data set at random for each dataset query."}, {"heading": "4.1 Compared Algorithms", "text": "Seven state-of-the-art hashing algorithms for high dimensional nearest neighbors search are compared as this as both: \u2022 Locality Sensitive Hashing (LSH) [8], which is based on the random projection. \u2022 The projective vectors are randomly selected from a p-stable distribution (e.g., Gaussian). We implement the algorithm ourselves and make it publicly available. \u2022 Kernelized Locality Sensitive Hashing (KLSH) [22], which generalizes the LSH method to the kernel space. We use the code provided by the authors 7.3. http: / / www.vision.ethz.ch / \u0445 zhuji / felib.html 4. http: / www.cad.zju.edu.cn / home / dengcai / Data / NNSData.html 5. http: / / corpus-texmexico.irisa.fr. http: / / / www.cad.zju.e.edu.edu.e.home / deng.we / denghti /.."}, {"heading": "4.2 Experimental Results", "text": "In fact, the three random projection methods (LSH, KLSH and SIKH) show a high level of satisfaction as the code length increases, but they are unable to achieve significant improvements as the performance of all three methods continues to increase."}, {"heading": "4.3 Parameter Selection", "text": "Our DSH has three parameters: p (the number of iterations in k averages), \u03b1 (the parameter controlling the number of groups) and r (the parameter for r-adjacentTABLE 4 training time (s) of DSH compared to the number of iterations of k averages (p) at 64 bits.Data set p = 1 p = 2 p = 3 p = 4 p = 5 p = 6 GIST1M 18.8 37.2 56.5 76.2 94.1 111.7 23.5 35.8 48.1 62.6 76.4 SIFT1M 4.8 9.1 15.5 21.2 25.5 31.90.5 1 2.5 2 2.5 3 0.050.10,150.25\u03b1M ean Ave Pre cisi onLSH KLSH KDSH AGH 1DSH (a)."}, {"heading": "5 CONCLUSION", "text": "In this work, we have developed a novel hashing algorithm called Density Sensitive Hashing (DSH) for high-dimensional searching for the nearest neighbors. Unlike random, projection-based hashing approaches, such as Locality Sensitive Hashing, DSH uses the geometric structure of the data to guide the selection of projections. As a result, DSH can generate hashing codes with more discriminatory performance. Empirical studies on three large datasets show that the proposed algorithm adapts well to the data size and significantly outperforms the most modern hashing methods in terms of retrieval accuracy."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Commun. ACM, 51(1):117\u2013122,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "The Priority R-tree: a practically efficient and worst-case optimal R-tree", "author": ["L. Arge", "M. Berg", "H. Haverkort", "K. Yi"], "venue": "SIGMOD,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["K. B", "T. Darrell"], "venue": "In The Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J. Bentley"], "venue": "Communications of the ACM, 18:509\u2013517,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1975}, {"title": "When is nearest neighbor meaningful", "author": ["K. Beyer", "J. Goldstein", "R. Ramakrishnan", "U. Shaft"], "venue": "In ICDT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Transform coding for fast approximate nearest neighbor search in high dimensions", "author": ["J. Brandt"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 1815\u20131822,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Spectral Regression: A Regression Framework for Efficient Regularized Subspace Learning", "author": ["D. Cai"], "venue": "PhD thesis, Department of Computer Science, University of Illinois at Urbana-Champaign, May", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M. Charikar"], "venue": "STOC, pages 380\u2013388,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Fast locality-sensitive hashing", "author": ["A. Dasgupta", "R. Kumar", "T. Sarls"], "venue": "IEEE International Conference on Knowledge Discovery and Data Mining, pages 1073\u20131081,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Localitysensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokni"], "venue": "Symposium on Computational Geometry 2004, pages 253\u2013262,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Continuous visible nearest neighbor query processing in spatial databases", "author": ["Y. Gao", "B. Zheng", "G. Chen", "Q. Li", "X. Guo"], "venue": "VLDB J., 20(3):371\u2013396,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "International Conference on Very Large Data Bases,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 817\u2013824,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalable similarity search with optimized kernel hashing", "author": ["J. He", "W. Liu", "S.-F. Chang"], "venue": "IEEE International Conference on Knowledge Discovery and Data Mining, pages 1129\u20131138,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Product quantization for nearest neighbor search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 33(1):117\u2013128,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Semi-supervised simhash for efficient document similarity search", "author": ["Q. Jiang", "M. Sun"], "venue": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 93\u2013101,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W. Johnson", "J. Lindenstrauss"], "venue": "Contemporary mathematics, 26:189\u2013206,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1984}, {"title": "A posteriori multi-probe locality sensitive hashing", "author": ["A. Joly", "O. Buisson"], "venue": "ACM Multimedia, pages 209\u2013218,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Random maximum margin hashing", "author": ["A. Joly", "O. Buisson"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 873\u2013880,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Voronoi-based k nearest neighbor search for spatial network databases", "author": ["M.R. Kolahdouzan", "C. Shahabi"], "venue": "International Conference on Very Large Data Bases, pages 840\u2013851,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Fast nearest neighbor search in medical image databases", "author": ["F. Korn", "N. Sidiropoulos", "C. Faloutsos", "E. Siegel", "Z. Protopapas"], "venue": "International Conference on Very Large Data Bases, pages 215\u2013226,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Kernelized locality-sensitive hashing for scalable image search", "author": ["B. Kulis", "K. Grauman"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Hashing algorithms for large-scale learning", "author": ["P. Li", "A. Shrivastava", "J. Moore", "C. Konig"], "venue": "The Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "International Conference on Machine Learning,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-probe lsh: Efficient indexing for high-dimensional similarity search", "author": ["Q. Lv", "W. Josephson", "Z. Wang", "M. Charikar", "K. Li"], "venue": "International Conference on Very Large Data Bases, pages 950\u2013961,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Weakly-supervised hashing in kernel space", "author": ["Y. Mu", "J. Shen", "S. Yan"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 3344\u20133351,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Minimal loss hashing for compact binary codes", "author": ["M. Norouzi", "D.J. Fleet"], "venue": "International Conference on Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Entropy based nearest neighbor search in high dimensions", "author": ["R. Panigrahy"], "venue": "SODA, pages 1186\u20131195,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Locality sensitive hashing: A comparison of hash function types and querying mechanisms", "author": ["L. Paulev\u00e9", "H. J\u00e9gou", "L. Amsaleg"], "venue": "Pattern Recognition Letters, 31(11):1348\u20131358,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "The Neural Information Processing Systems,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "International Journal of Approximate Reasoning, 50(7):969\u2013978,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Semi-supervised hashing for scalable image retrieval", "author": ["J. Wang", "O. Kumar", "S.-F. Chang"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 3424\u20133431,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Sequential projection learning for hashing with compact codes", "author": ["J. Wang", "S. Kumar", "S.-F. Chang"], "venue": "International Conference on Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Annosearch: Image auto-annotation by search", "author": ["X.-J. Wang", "L. Zhang", "F. Jing", "W.-Y. Ma"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, pages 1483\u20131490,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "The Neural Information Processing Systems, pages 1753\u20131760,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Complementary hashing for approximate nearest neighbor search", "author": ["H. Xu", "J. Wang", "Z. Li", "G. Zeng", "N. Yu", "S. Li"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Laplacian co-hashing of terms and documents", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "ECIR, pages 577\u2013580,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Self-taught hashing for fast similarity search", "author": ["D. Zhang", "J. Wang", "D. Cai", "J. Lu"], "venue": "SIGIR, pages 18\u201325,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Nearest Neighbors (NN) search is a fundamental problem and has found applications in many data mining tasks [9], [11], [14].", "startOffset": 108, "endOffset": 111}, {"referenceID": 10, "context": "Nearest Neighbors (NN) search is a fundamental problem and has found applications in many data mining tasks [9], [11], [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "Nearest Neighbors (NN) search is a fundamental problem and has found applications in many data mining tasks [9], [11], [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 3, "context": "KD-tree [4] and R-tree [2]), have been proposed for nearest neighbors search.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "KD-tree [4] and R-tree [2]), have been proposed for nearest neighbors search.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "Unfortunately, these approaches perform worse than a linear scan when the dimensionality of the space is high [5].", "startOffset": 110, "endOffset": 113}, {"referenceID": 0, "context": "Given the intrinsic difficulty of exact nearest neighbors search, many hashing algorithms are proposed for Approximate Nearest Neighbors (ANN) search [1], [8], [10].", "startOffset": 150, "endOffset": 153}, {"referenceID": 7, "context": "Given the intrinsic difficulty of exact nearest neighbors search, many hashing algorithms are proposed for Approximate Nearest Neighbors (ANN) search [1], [8], [10].", "startOffset": 155, "endOffset": 158}, {"referenceID": 9, "context": "Given the intrinsic difficulty of exact nearest neighbors search, many hashing algorithms are proposed for Approximate Nearest Neighbors (ANN) search [1], [8], [10].", "startOffset": 160, "endOffset": 164}, {"referenceID": 0, "context": "One of the most popular methods is Locality Sensitive Hashing(LSH) [1], [8], [10], [12].", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "One of the most popular methods is Locality Sensitive Hashing(LSH) [1], [8], [10], [12].", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "One of the most popular methods is Locality Sensitive Hashing(LSH) [1], [8], [10], [12].", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "One of the most popular methods is Locality Sensitive Hashing(LSH) [1], [8], [10], [12].", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "Given a database with n samples, LSH makes no prior assumption about the data distribution and offers probabilistic guarantees of retrieving items within (1 + \u01eb) times the optimal similarity, with query times that are sub-linear with respect to n [22], [27].", "startOffset": 247, "endOffset": 251}, {"referenceID": 26, "context": "Given a database with n samples, LSH makes no prior assumption about the data distribution and offers probabilistic guarantees of retrieving items within (1 + \u01eb) times the optimal similarity, with query times that are sub-linear with respect to n [22], [27].", "startOffset": 253, "endOffset": 257}, {"referenceID": 16, "context": "However, according to the Jonson Lindenstrauss Theorem [17], LSH needs O(lnn/\u01eb) random projections to preserve the pairwise distances, where \u01eb is the relative error.", "startOffset": 55, "endOffset": 59}, {"referenceID": 5, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 95, "endOffset": 98}, {"referenceID": 14, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 100, "endOffset": 104}, {"referenceID": 15, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 106, "endOffset": 110}, {"referenceID": 30, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 112, "endOffset": 116}, {"referenceID": 31, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 118, "endOffset": 122}, {"referenceID": 34, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 124, "endOffset": 128}, {"referenceID": 37, "context": "Aiming at making full use of the structure of the data, many learning-based hashing algorithms [6], [15], [16], [31], [32], [35], [38] are proposed.", "startOffset": 130, "endOffset": 134}, {"referenceID": 18, "context": "Despite the success of these approaches for relatively small codes, they often fail to make significant improvement as the code length increases [19].", "startOffset": 145, "endOffset": 149}, {"referenceID": 32, "context": "For the linear projection-based hashing, we have [33]", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "One of the most popular hashing algorithms is Locality Sensitive Hashing (LSH) [1], [8], [10], [12].", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "One of the most popular hashing algorithms is Locality Sensitive Hashing (LSH) [1], [8], [10], [12].", "startOffset": 84, "endOffset": 87}, {"referenceID": 9, "context": "One of the most popular hashing algorithms is Locality Sensitive Hashing (LSH) [1], [8], [10], [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 11, "context": "One of the most popular hashing algorithms is Locality Sensitive Hashing (LSH) [1], [8], [10], [12].", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": "Using this hash function, two points\u2019 hash bits match with the probability proportional to the cosine of the angle between them [8].", "startOffset": 128, "endOffset": 131}, {"referenceID": 21, "context": "Specifically, for any two points xi,xj \u2208 R, we have [22]:", "startOffset": 52, "endOffset": 56}, {"referenceID": 21, "context": "Based on this nice property, LSH have the probabilistic guarantees of retrieving items within (1 + \u01eb) times the optimal similarity, with query times that are sub-linear with respect to n [22], [27].", "startOffset": 187, "endOffset": 191}, {"referenceID": 26, "context": "Based on this nice property, LSH have the probabilistic guarantees of retrieving items within (1 + \u01eb) times the optimal similarity, with query times that are sub-linear with respect to n [22], [27].", "startOffset": 193, "endOffset": 197}, {"referenceID": 0, "context": "Empirical studies [1] showed that the LSH is significantly more efficient than the methods based on hierarchical tree decomposition.", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21].", "startOffset": 69, "endOffset": 72}, {"referenceID": 13, "context": "It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21].", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21].", "startOffset": 96, "endOffset": 100}, {"referenceID": 33, "context": "It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21].", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21].", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "It has been successfully used in various applications in data mining [9], [14], computer vision [32], [34] and database [20], [21].", "startOffset": 126, "endOffset": 130}, {"referenceID": 17, "context": "There are many extensions for LSH [18], [22], [25], [28].", "startOffset": 34, "endOffset": 38}, {"referenceID": 21, "context": "There are many extensions for LSH [18], [22], [25], [28].", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "There are many extensions for LSH [18], [22], [25], [28].", "startOffset": 46, "endOffset": 50}, {"referenceID": 27, "context": "There are many extensions for LSH [18], [22], [25], [28].", "startOffset": 52, "endOffset": 56}, {"referenceID": 27, "context": "Entropy based LSH [28] and Multi-Probe LSH [25], [18] are proposed to reduce the space requirement in LSH but need much longer time to deal with the query.", "startOffset": 18, "endOffset": 22}, {"referenceID": 24, "context": "Entropy based LSH [28] and Multi-Probe LSH [25], [18] are proposed to reduce the space requirement in LSH but need much longer time to deal with the query.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "Entropy based LSH [28] and Multi-Probe LSH [25], [18] are proposed to reduce the space requirement in LSH but need much longer time to deal with the query.", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "To address this limitation, Kernelized Locality Sensitive Hashing is introduced in [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "It suggests to approximate a normal distribution in the kernel space using only kernel comparisons [19].", "startOffset": 99, "endOffset": 103}, {"referenceID": 29, "context": "In addition, the Shift Invariant Kernels Hashing [30], which is a distribution-free method based on the random features", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "This method has theoretical convergence guarantees and performs well for relatively large code sizes [13].", "startOffset": 101, "endOffset": 105}, {"referenceID": 16, "context": "According to the Jonson Lindenstrauss Theorem [17], O(lnn/\u01eb) projective vectors are needed to preserve the pairwise distances of a database with size n for the random projection, where \u01eb is the relative error.", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 69, "endOffset": 72}, {"referenceID": 5, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 74, "endOffset": 77}, {"referenceID": 12, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 97, "endOffset": 101}, {"referenceID": 22, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 103, "endOffset": 107}, {"referenceID": 23, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 109, "endOffset": 113}, {"referenceID": 25, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 115, "endOffset": 119}, {"referenceID": 26, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 121, "endOffset": 125}, {"referenceID": 28, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 127, "endOffset": 131}, {"referenceID": 30, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 133, "endOffset": 137}, {"referenceID": 31, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 139, "endOffset": 143}, {"referenceID": 34, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 145, "endOffset": 149}, {"referenceID": 35, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 151, "endOffset": 155}, {"referenceID": 36, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 157, "endOffset": 161}, {"referenceID": 37, "context": "To address the above limitation, many learning-based hashing methods [3], [6], [13], [15], [16], [19], [23], [24], [26], [27], [29], [31], [32], [35], [36], [37], [38] are proposed.", "startOffset": 163, "endOffset": 167}, {"referenceID": 33, "context": "PCA Hashing [34] might be the simplest one.", "startOffset": 12, "endOffset": 16}, {"referenceID": 23, "context": "Many other algorithms [24], [33], [35], [38] exploit the spectral properties of the data affinity (i.", "startOffset": 22, "endOffset": 26}, {"referenceID": 32, "context": "Many other algorithms [24], [33], [35], [38] exploit the spectral properties of the data affinity (i.", "startOffset": 28, "endOffset": 32}, {"referenceID": 34, "context": "Many other algorithms [24], [33], [35], [38] exploit the spectral properties of the data affinity (i.", "startOffset": 34, "endOffset": 38}, {"referenceID": 37, "context": "Many other algorithms [24], [33], [35], [38] exploit the spectral properties of the data affinity (i.", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "The spectral analysis of the data affinity matrix is usually time consuming [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 34, "context": "[35] made a strong assumption that data is uniformly distributed and proposed a Spectral Hashing method (SpH).", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "Anchor Graph Hashing (AGH) [24] is a recently proposed method to overcome this shortcoming.", "startOffset": 27, "endOffset": 31}, {"referenceID": 30, "context": "Some other learning based hashing methods include Semantic Hashing [31] which uses the stacked Restricted Boltzmann Machine (RBM) to generate the compact binary codes; Semi-supervised Sequential Projection Hashing (S3PH) [33] which can incorporate supervision information.", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": "Some other learning based hashing methods include Semantic Hashing [31] which uses the stacked Restricted Boltzmann Machine (RBM) to generate the compact binary codes; Semi-supervised Sequential Projection Hashing (S3PH) [33] which can incorporate supervision information.", "startOffset": 221, "endOffset": 225}, {"referenceID": 18, "context": "Despite the success of these learning based hashing approaches for relatively small codes, they often fail to make significant improvement as the code length increases [19].", "startOffset": 168, "endOffset": 172}, {"referenceID": 7, "context": "(a) Locality Sensitive Hashing [8] 00 01 10 11", "startOffset": 31, "endOffset": 34}, {"referenceID": 33, "context": "(b) PCA Hashing [34] 00 01 10 11", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "LSH [8] generates the projections randomly and it is very possible that the data points from the same Gaussian will be encoded by different hash codes.", "startOffset": 4, "endOffset": 7}, {"referenceID": 33, "context": "PCA Hashing [34] uses the principle directions of the data as the projective vectors.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "[29] show that a quantized preprocess for the data points can significantly improve the performance of the nearest neighbors search.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "From the information theoretic point of view, a \u201dgood\u201d binary codes should maximize the information/entropy provided by each bit [38].", "startOffset": 129, "endOffset": 133}, {"referenceID": 31, "context": "Using maximum entropy principle, a binary bit that gives balanced partitioning of the data points provides maximum information [32].", "startOffset": 127, "endOffset": 131}, {"referenceID": 32, "context": "We use the same criterion as in [33], [36], that a returned point is considered to be a true neighbor if it lies in the top 2 percentile points closest (measured by the Euclidian distance in the original space) to the query.", "startOffset": 32, "endOffset": 36}, {"referenceID": 35, "context": "We use the same criterion as in [33], [36], that a returned point is considered to be a true neighbor if it lies in the top 2 percentile points closest (measured by the Euclidian distance in the original space) to the query.", "startOffset": 38, "endOffset": 42}, {"referenceID": 32, "context": "We evaluate the retrieval results by the Mean Average Precision (MAP) and the precision-recall curve [33].", "startOffset": 101, "endOffset": 105}, {"referenceID": 7, "context": "\u2022 Locality Sensitive Hashing (LSH) [8], which is based on the random projection.", "startOffset": 35, "endOffset": 38}, {"referenceID": 21, "context": "\u2022 Kernelized Locality Sensitive Hashing (KLSH) [22], which generalizes the LSH method to the kernel space.", "startOffset": 47, "endOffset": 51}, {"referenceID": 29, "context": "\u2022 Shift-Invariant Kernel Hashing (SIKH) [30], which is a distribution-free method based on the random features mapping for approximating shift-invariant kernels.", "startOffset": 40, "endOffset": 44}, {"referenceID": 33, "context": "\u2022 Principle Component Analysis Hashing (PCAH) [34], which directly uses the top principal directions as the projective vectors to obtain the binary codes.", "startOffset": 46, "endOffset": 50}, {"referenceID": 34, "context": "\u2022 Spectral Hashing (SpH) [35], which is based on quantizing the values of analytical eigenfunctions computed along PCA directions of the data.", "startOffset": 25, "endOffset": 29}, {"referenceID": 23, "context": "\u2022 Anchor Graph Hashing (AGH) [24], which constructs an anchor graph to speed up the spectral analysis procedure.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "AGH with two-layer is used in our comparison for its superior performance over AGH with one-layer [24].", "startOffset": 98, "endOffset": 102}, {"referenceID": 23, "context": "We use the code provided by the authors and the number of anchors is set to be 300 and the number of nearest neighbors is set to be 2 as suggested in [24].", "startOffset": 150, "endOffset": 154}, {"referenceID": 7, "context": "LSH [8] 0.", "startOffset": 4, "endOffset": 7}, {"referenceID": 21, "context": "1\u00d7 10 KLSH [22] 27.", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "5\u00d7 10 SIKH [30] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "9\u00d7 10 PCAH [34] 31.", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "2\u00d7 10 SpH [35] 42.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "1 \u00d7 10 AGH [24] 340.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "LSH [8] 0.", "startOffset": 4, "endOffset": 7}, {"referenceID": 21, "context": "6\u00d7 10 KLSH [22] 18.", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "1\u00d7 10 SIKH [30] 1.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "3\u00d7 10 PCAH [34] 16.", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "9\u00d7 10 SpH [35] 22.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "6 \u00d7 10 AGH [24] 232.", "startOffset": 11, "endOffset": 15}, {"referenceID": 7, "context": "LSH [8] 0.", "startOffset": 4, "endOffset": 7}, {"referenceID": 21, "context": "4\u00d7 10 KLSH [22] 10.", "startOffset": 11, "endOffset": 15}, {"referenceID": 29, "context": "7\u00d7 10 SIKH [30] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "0\u00d7 10 PCAH [34] 3.", "startOffset": 11, "endOffset": 15}, {"referenceID": 34, "context": "5\u00d7 10 SpH [35] 11.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "9 \u00d7 10 AGH [24] 135.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "This is consistent with previous work [13], [33] and is probably because that most of the data variance is contained in the top few principal directions so that the later bits are calculated using the low-variance projections, leading to the poorly discriminative codes [33].", "startOffset": 38, "endOffset": 42}, {"referenceID": 32, "context": "This is consistent with previous work [13], [33] and is probably because that most of the data variance is contained in the top few principal directions so that the later bits are calculated using the low-variance projections, leading to the poorly discriminative codes [33].", "startOffset": 44, "endOffset": 48}, {"referenceID": 32, "context": "This is consistent with previous work [13], [33] and is probably because that most of the data variance is contained in the top few principal directions so that the later bits are calculated using the low-variance projections, leading to the poorly discriminative codes [33].", "startOffset": 270, "endOffset": 274}], "year": 2012, "abstractText": "Nearest neighbors search is a fundamental problem in various research fields like machine learning, data mining and pattern recognition. Recently, hashing-based approaches, e.g., Locality Sensitive Hashing (LSH), are proved to be effective for scalable high dimensional nearest neighbors search. Many hashing algorithms found their theoretic root in random projection. Since these algorithms generate the hash tables (projections) randomly, a large number of hash tables (i.e., long codewords) are required in order to achieve both high precision and recall. To address this limitation, we propose a novel hashing algorithm called Density Sensitive Hashing (DSH) in this paper. DSH can be regarded as an extension of LSH. By exploring the geometric structure of the data, DSH avoids the purely random projections selection and uses those projective functions which best agree with the distribution of the data. Extensive experimental results on real-world data sets have shown that the proposed method achieves better performance compared to the state-of-the-art hashing approaches.", "creator": "LaTeX with hyperref package"}}}