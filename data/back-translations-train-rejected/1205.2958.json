{"id": "1205.2958", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2012", "title": "b-Bit Minwise Hashing in Practice: Large-Scale Batch and Online Learning and Using GPUs for Fast Preprocessing with Simple Hash Functions", "abstract": "In this paper, we study several critical issues which must be tackled before one can apply b-bit minwise hashing to the volumes of data often used industrial applications, especially in the context of search.", "histories": [["v1", "Mon, 14 May 2012 08:28:10 GMT  (148kb)", "http://arxiv.org/abs/1205.2958v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.DB cs.LG", "authors": ["ping li", "anshumali shrivastava", "arnd christian konig"], "accepted": false, "id": "1205.2958"}, "pdf": {"name": "1205.2958.pdf", "metadata": {"source": "CRF", "title": "b-Bit Minwise Hashing in Practice: Large-Scale Batch and Online Learning and Using GPUs for Fast Preprocessing with Simple Hash Functions", "authors": ["Ping Li", "Arnd Christian K\u00f6nig"], "emails": ["pingli@cornell.edu", "anshu@cs.cornell.edu", "chrisko@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 120 5.29 58v1 [cs.IR] 1 4M ay2 01 (b-bit) Minwise hashing requires an expensive pre-processing step in which k (e.g. 500) minimum values are calculated after applying the corresponding permutations to each data vector. Note that the required k for classification tasks is often much greater than for double detections (which mainly involve very similar pairs).We developed a parallelization scheme using GPUs and observed that the pre-processing time can be reduced by a factor of 20-80 and becomes much smaller than the loading time of the data. Reducing the pre-processing time is extremely beneficial in practice, e.g. for detecting duplicate web pages (where minwise hashing is an important step in the creeping pipeline) or for increasing the test speed of online classifiers. A big advantage of b-bit hashing is that it can significantly reduce memory wise hashing, as it can significantly reduce the learning required for batch."}, {"heading": "1. INTRODUCTION", "text": "The recent development of b-bit minwise hashing [26] has resulted in a significant improvement in estimation accuracy and speed by proposing a new estimator that stores only the lowest bits of each hashed value. More recently, [27] has suggested the use of b-bit minwise hashing [26] in the context of learning algorithms such as SVM or logistic regression to large binary tasks (typical for web classification tasks). bbit minwise hashing can enable scalable learning where otherwise massive (and expensive) parallel architectures would have been required, with negligible reduction in learning quality."}, {"heading": "1.2 Linear Learning Algorithms", "text": "We focus on linear learning because many high-dimensional datasets used in the search are of course suitable for linear algorithms. Realistically, for industrial applications \"almost all large impact algorithms work in pseudo-linear or better time.\" [1] Linear algorithms such as linear SVM and logistic regression are now very powerful and extremely popular. Representative software packages include SVMperf [21], Pegasos [32], Bottou's SGD SVM [4] and LIBLINEAR [15]. Given a dataset {(xi, yi)} ni = 1, xi-R D, yi-1, yi-1, yi-1, 1}, the L2 regulated linear SVM must solve the following optimization problem: min w1 2 w + Cn."}, {"heading": "1.3 Issue 1: Expensive Preprocessing", "text": "(B-bit) Minwise hashing requires a very expensive pre-processing step (much more expensive than loading the data) to calculate k (e.g. k = 500) minimum values (after permutation) for each data vector. Note that in previous studies we were generally not too large for duplicate detection patterns (i.e., 200), mainly because duplicate detectors affect very similar pairs (e.g. R > 0.5). b-bit minwise hashing, we need to use larger k values after analyzing in [26] even in the context of duplicated detectors. Note that classification tasks are very different from duplicated detectors."}, {"heading": "1.4 Issue 2: Online Learning", "text": "Online learning has become increasingly popular in the context of web search and advertising [3,12,23,31] because it only requires loading one feature vector at a time, thus avoiding the hassle of storing a potentially very large dataset in memory, or the complexity and cost of parallel learning architectures. In this paper, we show that the resulting reduction in overall learning accuracy that we see in the experiments can also be very beneficial for online learning, because reducing the data size significantly reduces the loading time that normally dominates training costs, especially when many training periods are needed. In addition, b-bit-minwise hashing can also serve as an effective dimensionality reduction program, which can be important in the context of web search, since (i) machine learning based on text-n-gram features usually has a very large number of features and therefore can use significant memory and (ii) machine-based text (typically 38 user-dependent search models that require very different textual models to each other)."}, {"heading": "1.5 Issue 3: Massive Permutation Matrix", "text": "If the data dimension (D) is not too large, e.g. millions, the implementation of b-bit minwise hashing is straightforward for learning. Basically, we can assume a \"completely random permutation matrix\" of the size D \u00b7 k, which defines k permutation mappings. Thus, researchers (e.g. Matlab) use simulations to verify theoretical results on the assumption of completely random permutations. Unfortunately, when the dimension is in the order of billions (let alone 264), it becomes impractical (or too expensive) to store such a permutation matrix. Therefore, we have to resort to simple hash functions, such as various forms of 2-universal (2U) hashing (e.g. [13]). Now, the question is how reliable these hash functions are in the context of learning with b-bit minwise hashing.There have been previous studies on the effects of limited randomness on the accuracy of 64-bit minwise hashing."}, {"heading": "2. SIMPLE HASH FUNCTIONS", "text": "As previously discussed, it is often impossible to assume perfect random permutations in large-scale industrial practice. If, for example, D = 230 (about 1 billion) and k = 500, a matrix of D \u00b7 k integers (4 bytes each) would be needed to overcome the difficulty of achieving perfect permutations, the common practice is to use the so-called universal hashing [9]. A standard 2-universal (2U) hash function is used for j = 1 to k, h (2U) j (t) = {a1, j + a2, j t mod} mod D, (8) where p > D is a prime number and a1, j, j, uniformly chosen from {0,..., p \u2212 1}. To increase the randomness, you can also use the following 4-universal (4U) hash function: h (h) h (h) j (t) = 1ai, jt,} (1 mov, 1 mov, 1 mov, 1 mol, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1 mov, 1, 1 mov, 1 mov, 1 mov, 1 mov, 1, 1 mov, 1 mov, 1 mov, 1 mov, 1, 1 mov, 1 mov, 1 mov, 1, 1 mov, 1 mov, 1, 1 mov, 1, 1, 1 mov, 1 mov, 1 mov, 1, 1 mov, 1."}, {"heading": "3. GPU FOR FAST PREPROCESSING", "text": "In this section, we describe and analyze the use of graphics processors (GPUs) to quickly calculate minwise hashes. First, we outline the relevant characteristics of GPUs in general, and then describe the extent to which minwise hashing computation is suitable for execution on this architecture. Afterwards, we describe our implementation and analyze the resulting performance improvements compared to a CPU-based implementation."}, {"heading": "3.1 Introduction", "text": "The use of GPUs as multi-purpose co-processors is relatively new and primarily due to their high computing power at a comparatively low cost. GPUs offer significantly higher computing speed and memory bandwidth than traditional CPUs. However, since GPUs are designed for graphics processing, the programming model (which includes massively parallel single-instruction multiple data (SIMD) processing and limited bus speeds for data transfers to / from main memory) is not suitable for any data processing application [19]. GPUs consist of a number of SIMD multi-processors. At each cycle, all processors in a multi-processor execute identical commands, but on different portions of the data. Thus, GPUs can use spatial locality when accessing data and group access to successive memory addresses into a single access; this is known as coalessual access."}, {"heading": "3.2 Our Approach", "text": "Given the characteristics of GPU processing, our GPU algorithm for calculating B-bit hash values operates in three distinct phases: First, we read from disk to main memory in chunks of 10K sets and write them into GPU memory. Then, we compute the hash values and corresponding minima by applying all the hash functions to the data currently in the GPU, retaining memory latency for each hash function and corresponding minima. Finally, we write the resulting minima back into main memory and repeat the process. This batch-like computation has a number of advantages. As we transfer larger blocks of data, the main memory latency is reduced by using pre-calls of main memory. Furthermore, because the calculation within the GPU itself scans through consecutive blocks of data in GPU internal memory, and the PU internal calculation patterns (as opposed to random memory access patterns with other hash patterns) perform the computation."}, {"heading": "3.3 Avoid Modulo Operations in 2U Hashing", "text": "To avoid modulo operations in 2U hashing, we use a common trick [14]. Here, for the sake of simplicity, we assume D = 2s < 232 (note that D = 230 corresponds to about one billion characters). It is known that the following hash function is essentially 2U [14]: h (s) j (t) = {a1, j + a2, j t mod 2 32} mod 2s, (10) where a1, j is evenly selected from {0, 1,..., 232 \u2212 1} and a2, j is evenly selected from {1, 3,..., 232 \u2212 1} (i.e. a2, j is strange). This scheme is much faster because we can effectively use the integer overflow mechanism and the efficient bit shift operation."}, {"heading": "3.4 Avoid Modulo Operations in 4U Hashing", "text": "It is somewhat difficult to avoid modulo operations when evaluating 4U hash functions. Suppose, D < p = 231 \u2212 1 (a prime), we set the C # code to calculate v mod p with p = 231 \u2212 1: private static ulong BitMod (ulong v) {ulong p = 2147483647; / / p = 2 ^ 31-1 v = (v > 31) + (v & p); if (v > = 2 * p) v = (v > 31) + (v & p); if (v > = p) return v - p; elsereturn v;} To better understand the code, we must take into account that mod p = x and v mod 231 = \u21d2 v = p \u00d7 Z + x = 231 \u00d7 S + y = \u21d2 x = \u21d2 x = 231 (S \u2212 Z + yfor two integers S and y can be efficiently evaluated using bit operations: > S = x and > sick = 1 billion and > sip = 1 (1)."}, {"heading": "3.5 Experiments: Datasets", "text": "Table 1 summarizes the two data sets used in this evaluation: Webspam and rcv1. Since the web spam data set (24 GB in LibSVM format) may be too small compared to data sets used in industrial practice, we also present in this paper an empirical study of the extended rcv1 data set [4], which we created using the original characteristics + all pairs of characteristics + 1 / 30 of 3-way combinations (products) of characteristics. Note that for rcv1 we did not include the original test set in [4], which contains only 20242 examples. To ensure reliable test results, we randomly split our extended rcv1 data set in half to train and test it."}, {"heading": "3.6 Experiments: Platform", "text": "The GPU platform we use in our experiments is the NVIDIA Tesla C2050, which has 15 simultaneous multiprocessors (SMs), each with 2 groups of 16 scalar processors (hence 2 sets of 16 element-wide SIMD units), and the maximum (single-precision) Gflops of this GPU are 1030, with a maximum memory bandwidth of 144 GB / s. In comparison, the values for an Intel Xeon X5670 processor (Westmere) are 278 Gflops and 60 GB / s."}, {"heading": "3.7 Experiments: CPU Results", "text": "For these experiments, we use the setting k = 500. Table 2 shows the overhead of the CPU-based implementation, divided into the time it takes to load the data into memory, and the time it takes to compute the hash. For 2U, we always use the 2U hash function (10). For 4U (mod), we use the 4U hash function (9), which requires the modulo operation. For 4U (bit), we use the implementation in Section 3.4, which has converted the modulo operation into bit operations. Note that for rcv1 records, only the experimental results for 2U hash are reported. Table 2 shows that pre-processing with CPUs (even for 2U) can be very expensive, much more than loading data. 4U hash with modulo operations can take an order of magnitude more time than 2U hash hashing."}, {"heading": "3.8 Experiments: GPU results", "text": "The total cost of GPU-based processing for batch size = 10K is summarized in Table 3, which shows the significant time saving compared to CPU-based processing in Table 2. For example, the cost of 2U processing in the web spam dataset is reduced from 4100 seconds to 51 seconds, an 80-fold reduction. We also see improvements of similar magnitude for 4U processing (both modulo and bit versions) in web spam. For the rcv1 dataset, the time saving of the GPU-based implementation is about 20 times, compared to the CPU-based implementation. For both datasets, the cost of GPU-based preprocessing is considerably smaller than the loading time of the data. So while a further reduction in pre-processing costs is achieved, it is still interesting because we have to load the data once in the learning process time. The Figures 1 to 3 represent the breakdown of the GPU for the implementation."}, {"heading": "4. VALIDATION OF THE USE OF 2U/4U HASH FUNCTIONS FOR LEARNING", "text": "Since storing a completely random permutation matrix is not practical, we need to rely on simple hash functions such as 2U or 4U hash families for large-scale industrial applications, but before we can recommend them to practitioners, we must first validate on a smaller dataset that the use of such hash functions will not affect learning performance. To our knowledge, this section is the first empirical study of the effects of hash functions on machine learning with b-bit minwise hash. In addition, Appendix A offers a number of other experiments to estimate similarities with b-bit minwise hash functions. These experiments show that, as long as the data is not too dense, the use of 2U hash produces estimates very similar to the use of completely random permutations. This set of experiments could help to understand the experimental results in this section. Note that both datasets listed in Table 1 are extremely economical."}, {"heading": "4.1 Experimental Results", "text": "We experimented with both 2U and 4U hash schemes for the formation of linear SVM and logistic regression. We experimented with 30 values for regularization parameter C at the interval [10 \u2212 3, 100]. We experimented with 11 k values from k = 10 to k = 500 and for 7 b values: b = 1, 2, 4, 6, 8, 10, 16. Each experiment was repeated 50 times. Overall, the number of training experiments turned out to be 2 \u00d7 2 \u00d7 30 \u00d7 11 \u00d7 7 \u00d7 50 = 462000. To maximize repeatability, we always want to present the detailed experimental results for all parameters, rather than, for example, reporting only the best results or cross-validated results. In this subsection, we present only the results for webspam datasets using linear SVM data, as the results for logistic regression lead to the same conclusions. Figure 4 represents the SVM accuracies (above 50) on average."}, {"heading": "4.2 Experimental Results on VW Algorithm", "text": "The scope of B-bit hashing is limited to binary data, while other hashing algorithms such as random projections and the Vowpal wabbit hashing algorithm [33, 37] are not limited to binary data. [27] shows that VW and random projections have essentially the same deviations as in [25]. In this study, we are also conducting some experiments on the VW hashing algorithm, because we expect practitioners will find applications that are more suitable for VW than B-bit hashing. Since there will be no room to explain the details of the VW algorithm, please refer interested readers to [27, 33, 37]. Specifically, in this subsection, we present a comparison between the accuracy of VW when using a completely random implementation and when using 2U hashing functions. Figure 5 shows the experimental results for both linear SVM and for logistic regapsion, which means that the two hashing contexts are virtually identical, as we can see the two previous 2U hashing contexts are."}, {"heading": "5. LEARNING ON RCV1 DATA (200GB)", "text": "Compared to webspam, the size of the extended rcv1 dataset may be closer to the training datasets used in industrial applications. We report on the experiments on linear SVM and logistic regression as well as comparisons with the VW hash algorithm."}, {"heading": "5.1 Experiments on Linear SVM", "text": "Figure 6 and Figure 7 respectively provide the test accuracy and training times for linear SVM. We cannot report the initial line because the original dataset exceeds the storage capacity. By mere k = 30 and b = 12, our method can achieve > 90% test accuracy; by k \u2265 300, we can achieve > 95% test accuracy."}, {"heading": "5.2 Experiments on Logistic Regression", "text": "Figure 8 and Figure 9 show the test accuracies and training times for logistic regression. Here, too, our method can achieve a test accuracy of > 90% with just k = 30 and b = 12. With k \u2265 300, we achieve > 95% test accuracies. To better understand the significance of these results, we will next provide a comparative study using the VW hash algorithm [33, 37]."}, {"heading": "5.3 Comparisons with VW Algorithm", "text": "The Vowpal Wabbit (VW) algorithm [33, 37] is an influential hashing method for data / dimension reduction. Since we [27] only compared b-bit minwise hashing with VW on a small dataset, it is more revealing to compare the two algorithms on this much larger dataset (200GB). We experimented with VW with k = 25 to 214 hash containers (sample size). Note that 214 = 16384. It is difficult to train LIBLINEAR with k = 215, because the training size of the hashed data from VW is almost 48GB when k = 215.Figure 10 and Figure 11 record the test accuracy for SVM or logistic regression. In each figure, each panel has the same set of solid curves for VW, but a different set of dashed curves for different values of b-bit minwise hashing. Since the ranking of k is very large, we select values from here by 0.0b against VW."}, {"heading": "6. ONLINE LEARNING", "text": "Batch-by-batch learning algorithms (e.g. the LIBLINEAR package) face a challenge when the data does not fit into memory. In the context of the search, training data sets often exceed the storage capacity of a single machine. A common solution (e.g. [10, 39]) is to divide the data into blocks that are repeatedly loaded into memory to update the model coefficients. However, this does not solve the computational bottleneck, since loading the data blocks for many iterations involves a large number of hard disk I / O. b-bit minwise hashing provides a simple solution for high-dimensional data by significantly reducing the data size. Another increasingly popular solution is online learning, where only one feature vector needs to be loaded simultaneously. Here, we follow the notation convention used in online learning literature (and the SGD code [4])."}, {"heading": "6.1 SGD SVM Results on Webspam", "text": "Figure 14 shows the test accuracies compared to the regularization parameter \u03bb, in the last (100th) epoch. If b \u2265 8 and k \u2265 200 are used, b-bit hashing can achieve similar test accuracies as with the original data. Figure 15 illustrates the test accuracies compared to epochs for two selected \u03bb values. Perhaps 20 epochs are sufficient to achieve sufficient accuracy with b-bit hashing. Figure 16 shows the training time and loading time for both the use of the original data and for the use of 8-bit hashing. On average, the use of the original data takes about 10 times longer than the use of 8-bit hashing data, as reflected in Table 4. Also, the loading time of the data clearly dominates the cost. Since the loading time of the data dominates the cost of online learning, we have always converted the data in our SGD experiments into a binary format first, unlike the data used in the batch learning format we would be grateful for using in this IBM-based communication."}, {"heading": "6.2 SGD SVM Results on Rcv1", "text": "Figure 17 shows the test accuracy of SGD SVM on rcv1 in the 100th epoch, both for the original data and for the B-bit data (b = 8 and b = 12). At k \u2265 500 and b = 12, b-bit minwise hashing achieves similar accuracies as when using the original data. Figure 18 shows the training time and the loading time of the data. As explicitly calculated in Table 4, using the original data takes 30 times more time than using 12-bit minwise hashing. Again, the loading time of the data dominates the costs."}, {"heading": "6.3 Averaged SGD (ASGD)", "text": "While writing this paper in 2011, Leon Bottou kindly informed us that there has recently been a major update of the SGD code implementing ASGD (Averaged SGD) [38]. Therefore, we also offer experiments on ASGD in the web spam dataset, as shown in Figure 19. Compared to the SGD results, it seems that ASGD has some noticeable improvements over SGD. Nevertheless, ASGD still needs more than one epoch (perhaps 10 to 20) to approach the best accuracy, and B-bit hashing continues to perform very well in terms of accuracy and training time reduction."}, {"heading": "7. CONCLUSION", "text": "In fact, it is the case that most of them are able to abide by the rules they have imposed on themselves. (...) Most of them are able to abide by the rules. (...) Most of them are able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...)"}, {"heading": "Acknowledgement", "text": "We thank Leon Bottou for the very helpful communication."}, {"heading": "8. REFERENCES", "text": "[1]..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "A. RESEMBLANCE ESTIMATION USING SIMPLE HASH FUNCTIONS", "text": "In this section, we will examine the impact of using the 2U / 4U hashing function instead of (fully) random permutation matrices on the accuracy of estimating similarity by b-bit minwise hashing. This will give us a better understanding of why the learning outcomes (for SVM and logistic regression) with b-bit minwise hashing will not be noticeably affected by replacing the fully random permutation matrix with 2U / 4U hash functions. As we will see, as long as the original data is not too dense, the use of 2U / 4U hash functions will not result in a loss of estimation accuracy. As we have observed, the results from 2U and 4U are essentially indistinguishable, we will report only on the 2U experiments. The task we are examining here is the estimation of word associations. The data association extracted from commercial web crawls consists of 9 pairs of sets (S18 words each containing at least one set of hash Ds)."}], "references": [{"title": "Adaptive product normalization: Using online learning for record linkage in comparison shopping", "author": ["Mikhail Bilenko", "Sugato Basu", "Mehran Sahami"], "venue": "In ICDM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["Leon Bottou"], "venue": "In COMPSTAT,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "On the resemblance and containment of documents", "author": ["Andrei Z. Broder"], "venue": "In the Compression and Complexity of Sequences,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Min-wise independent permutations (extended abstract)", "author": ["Andrei Z. Broder", "Moses Charikar", "Alan M. Frieze", "Michael Mitzenmacher"], "venue": "In STOC,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Syntactic clustering of the web", "author": ["Andrei Z. Broder", "Steven C. Glassman", "Mark S. Manasse", "Geoffrey Zweig"], "venue": "In WWW,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Universal classes of hash functions (extended abstract)", "author": ["J. Lawrence Carter", "Mark N. Wegman"], "venue": "In STOC,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1977}, {"title": "Selective block minimization for faster convergence of limited memory large-scale linear models", "author": ["Kai-Wei Chang", "Dan Roth"], "venue": "In KDD,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Applying syntactic similarity algorithms for enterprise information management", "author": ["Ludmila Cherkasova", "Kave Eshghi", "Charles B. Morrey III", "Joseph Tucek", "Alistair C. Veitch"], "venue": "In KDD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Online learning from click data for sponsored search", "author": ["Massimiliano Ciaramita", "Vanessa Murdock", "Vassilis Plachouras"], "venue": "In WWW Conference,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2008}, {"title": "Universal hashing and k-wise independent random variables via integer arithmetic without primes", "author": ["Martin Dietzfelbinger"], "venue": "In Proceedings of the 13th Annual Symposium on Theoretical Aspects of Computer Science,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1996}, {"title": "A reliable randomized algorithm for the closest-pair problem", "author": ["Martin Dietzfelbinger", "Torben Hagerup", "Jyrki Katajainen", "Martti Penttonen"], "venue": "Journal of Algorithms,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Liblinear: A library for large linear classification", "author": ["Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "Xiang-Rui Wang", "Chih-Jen Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "A large-scale study of the evolution of web pages", "author": ["Dennis Fetterly", "Mark Manasse", "Marc Najork", "Janet L. Wiener"], "venue": "In WWW,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Efficient detection of large-scale redundancy in enterprise file systems", "author": ["George Forman", "Kave Eshghi", "Jaap Suermondt"], "venue": "SIGOPS Oper. Syst. Rev.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Precomputing search features for fast and accurate query classification", "author": ["Venkatesh Ganti", "Arnd Christian K\u00f6nig", "Xiao Li"], "venue": "In WSDM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "A Small Approximately Min-wise Independent Family of Hash Functions", "author": ["Piotr Indyk"], "venue": "J. Algorithms,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2001}, {"title": "Training linear svms in linear time", "author": ["Thorsten Joachims"], "venue": "In KDD,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2006}, {"title": "Fast: fast architecture sensitive tree search on modern cpus and gpus", "author": ["Changkyu Kim", "Jatin Chhugani", "Nadathur Satish", "Eric Sedlar", "Anthony D. Nguyen", "Tim Kaldewey", "Victor W. Lee", "Scott A. Brandt", "Pradeep Dubey"], "venue": "In SIGMOD,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Online Phishing Classification using Adversarial Data Mining and Signaling", "author": ["Gaston L\u2019Huillier", "Richard Weber", "Nicolas Figueroa"], "venue": "Games. SIGKDD Explor. Newsl.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Very sparse random projections", "author": ["Ping Li", "Trevor J. Hastie", "Kenneth W. Church"], "venue": "In KDD,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "Theory and applications b-bit minwise hashing", "author": ["Ping Li", "Arnd Christian K\u00f6nig"], "venue": "Commun. ACM,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Hashing algorithms for large-scale learning", "author": ["Ping Li", "Anshumali Shrivastava", "Joshua Moore", "Arnd Christian K\u00f6nig"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Detecting Near-Duplicates for Web-Crawling", "author": ["Gurmeet Singh Manku", "Arvind Jain", "Anish Das Sarma"], "venue": "In WWW,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Nearest-neighbor caching for content-match applications", "author": ["Sandeep Pandey", "Andrei Broder", "Flavio Chierichetti", "Vanja Josifovski", "Ravi Kumar", "Sergei Vassilvitskii"], "venue": "In WWW,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "On the k-independence required by linear probing and minwise independence", "author": ["Mihai P\u0103tra\u015fcu", "Mikkel Thorup"], "venue": "In ICALP,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Relaxed online SVMs for Spam Filtering", "author": ["D. Sculley", "Gabriel M. Wachman"], "venue": "In SIGIR,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2007}, {"title": "Pegasos: Primal estimated sub-gradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro"], "venue": "In ICML,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Hash kernels for structured data", "author": ["Qinfeng Shi", "James Petterson", "Gideon Dror", "John Langford", "Alex Smola", "S.V.N. Vishwanathan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Tabulation based 5-universal hashing and linear probing", "author": ["Mikkel Thorup", "Yin Zhang"], "venue": "In ALENEX,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "Lessons learned developing a practical large scale machine learning system", "author": ["Simon Tong"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Tracking web spam with html style similarities", "author": ["Tanguy Urvoy", "Emmanuel Chauveau", "Pascal Filoche", "Thomas Lavergne"], "venue": "ACM Trans. Web,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "Feature hashing for large scale multitask learning", "author": ["Kilian Weinberger", "Anirban Dasgupta", "John Langford", "Alex Smola", "Josh Attenberg"], "venue": "In ICML,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Towards optimal one pass large scale learning with averaged  stochastic gradient descent", "author": ["Wei Wu"], "venue": "Technical report,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Large linear classification when data cannot fit in memory", "author": ["Hsiang-Fu Yu", "Cho-Jui Hsieh", "Kai-Wei Chang", "Chih-Jen Lin"], "venue": "In KDD,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}], "referenceMentions": [{"referenceID": 21, "context": "The recent work [27] demonstrated a potential use of b-bit minwise hashing [26] for batch learning on large data.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "The recent work [27] demonstrated a potential use of b-bit minwise hashing [26] for batch learning on large data.", "startOffset": 75, "endOffset": 79}, {"referenceID": 2, "context": "INTRODUCTION Minwise hashing [6\u20138] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.", "startOffset": 29, "endOffset": 34}, {"referenceID": 3, "context": "INTRODUCTION Minwise hashing [6\u20138] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.", "startOffset": 29, "endOffset": 34}, {"referenceID": 4, "context": "INTRODUCTION Minwise hashing [6\u20138] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.", "startOffset": 29, "endOffset": 34}, {"referenceID": 23, "context": "INTRODUCTION Minwise hashing [6\u20138] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.", "startOffset": 212, "endOffset": 216}, {"referenceID": 13, "context": "INTRODUCTION Minwise hashing [6\u20138] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.", "startOffset": 269, "endOffset": 273}, {"referenceID": 7, "context": "INTRODUCTION Minwise hashing [6\u20138] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.", "startOffset": 345, "endOffset": 349}, {"referenceID": 30, "context": "INTRODUCTION Minwise hashing [6\u20138] is a standard technique for efficiently computing set similarities in the context of search, with further applications in the context of content matching for online advertising [29], detection of redundancy in enterprise file systems [17], syntactic similarity algorithms for enterprise information management [11], Web spam [36], etc.", "startOffset": 360, "endOffset": 364}, {"referenceID": 20, "context": "The recent development of bbit minwise hashing [26] provided a substantial improvement in the estimation accuracy and speed by proposing a new estimator that stores only the lowest b bits of each hashed value.", "startOffset": 47, "endOffset": 51}, {"referenceID": 21, "context": "More recently, [27] proposed the use of b-bit minwise hashing in the context of learning algorithms such as SVM or logistic regression on large binary data (which is typical in Web classification tasks).", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "In [27], experiments showed this for the webspam dataset which has 16 million features with a total disk size of 24GB in standard LibSVM format.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": ", min(\u03c0(S1)), using 64 bits [16].", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "The storage (and computational) cost is prohibitive in industrial applications [28].", "startOffset": 79, "endOffset": 83}, {"referenceID": 20, "context": "The recent work of b-bit minwise hashing [26] provides a simple solution by storing only the lowest b bits of each hashed value.", "startOffset": 41, "endOffset": 45}, {"referenceID": 20, "context": "[26] Assume D is large.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "For example, [35] discusses training datasets with (on average) n = 10 items and D = 10 distinct features.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "[37] experimented with a dataset of potentially D = 16 trillion (1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Representative software packages include SVM [21], Pegasos [32], Bottou\u2019s SGD SVM [4], and LIBLINEAR [15].", "startOffset": 45, "endOffset": 49}, {"referenceID": 26, "context": "Representative software packages include SVM [21], Pegasos [32], Bottou\u2019s SGD SVM [4], and LIBLINEAR [15].", "startOffset": 59, "endOffset": 63}, {"referenceID": 11, "context": "Representative software packages include SVM [21], Pegasos [32], Bottou\u2019s SGD SVM [4], and LIBLINEAR [15].", "startOffset": 101, "endOffset": 105}, {"referenceID": 2, "context": "Note that in prior studies for duplicate detection [6], k was usually not too large (i.", "startOffset": 51, "endOffset": 54}, {"referenceID": 20, "context": "With b-bit minwise hashing, we have to use larger k values according to the analysis in [26] even in the context of duplicate detections.", "startOffset": 88, "endOffset": 92}, {"referenceID": 17, "context": "GPUs offer, compared to current CPUs, higher instruction parallelism and very low latency access to the internal GPU memory, but comparatively slow latencies when accessing the main memory [22].", "startOffset": 189, "endOffset": 193}, {"referenceID": 0, "context": "4 Issue 2: Online Learning Online learning has become increasingly popular in the context of web search and advertising [3,12,23,31] as it only requires loading one feature vector at a time and thus avoids the overhead of storing a potentially very large dataset in memory or the complexity and cost of parallel learning architectures.", "startOffset": 120, "endOffset": 132}, {"referenceID": 8, "context": "4 Issue 2: Online Learning Online learning has become increasingly popular in the context of web search and advertising [3,12,23,31] as it only requires loading one feature vector at a time and thus avoids the overhead of storing a potentially very large dataset in memory or the complexity and cost of parallel learning architectures.", "startOffset": 120, "endOffset": 132}, {"referenceID": 18, "context": "4 Issue 2: Online Learning Online learning has become increasingly popular in the context of web search and advertising [3,12,23,31] as it only requires loading one feature vector at a time and thus avoids the overhead of storing a potentially very large dataset in memory or the complexity and cost of parallel learning architectures.", "startOffset": 120, "endOffset": 132}, {"referenceID": 25, "context": "4 Issue 2: Online Learning Online learning has become increasingly popular in the context of web search and advertising [3,12,23,31] as it only requires loading one feature vector at a time and thus avoids the overhead of storing a potentially very large dataset in memory or the complexity and cost of parallel learning architectures.", "startOffset": 120, "endOffset": 132}, {"referenceID": 32, "context": ", [38]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 9, "context": ", [13]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": ", [20, 30].", "startOffset": 2, "endOffset": 10}, {"referenceID": 24, "context": ", [20, 30].", "startOffset": 2, "endOffset": 10}, {"referenceID": 5, "context": "To overcome the difficulty in achieving perfect permutations, the common practice is to use the so-called universal hashing [9].", "startOffset": 124, "endOffset": 127}, {"referenceID": 10, "context": "3 Avoid Modulo Operations in 2U Hashing To avoid the modulo operations in 2U hashing, we adopt a common trick [14].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "It is known that the following hash function is essentially 2U [14]:", "startOffset": 63, "endOffset": 67}, {"referenceID": 28, "context": "A recent paper [34] implemented a similar trick for p = 2\u22121, which was simpler than ours because with p = 2 \u2212 1 there is no need to check the condition \u201cif (v >= 2 * p)\u201d.", "startOffset": 15, "endOffset": 19}, {"referenceID": 21, "context": "The webspam dataset was used in the recent paper [27].", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "We tried out 30 values for the regularization parameter C in the interval [10, 100].", "startOffset": 74, "endOffset": 83}, {"referenceID": 27, "context": "2 Experimental Results on VW Algorithm The application domains of b-bit minwise hashing are limited to binary data, whereas other hashing algorithms such as random projections and Vowpal Wabbit (VW) hashing algorithm [33, 37] are not restricted to binary data.", "startOffset": 217, "endOffset": 225}, {"referenceID": 31, "context": "2 Experimental Results on VW Algorithm The application domains of b-bit minwise hashing are limited to binary data, whereas other hashing algorithms such as random projections and Vowpal Wabbit (VW) hashing algorithm [33, 37] are not restricted to binary data.", "startOffset": 217, "endOffset": 225}, {"referenceID": 21, "context": "It is shown in [27] that VW and random projections have essentially the same variances as reported in [25].", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "It is shown in [27] that VW and random projections have essentially the same variances as reported in [25].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": "Since there won\u2019t be space to explain the details of VW algorithm, interested readers please refer to [27, 33, 37].", "startOffset": 102, "endOffset": 114}, {"referenceID": 27, "context": "Since there won\u2019t be space to explain the details of VW algorithm, interested readers please refer to [27, 33, 37].", "startOffset": 102, "endOffset": 114}, {"referenceID": 31, "context": "Since there won\u2019t be space to explain the details of VW algorithm, interested readers please refer to [27, 33, 37].", "startOffset": 102, "endOffset": 114}, {"referenceID": 27, "context": "To help understand the significance of these results, next we provide a comparison study with the VW hashing algorithm [33, 37].", "startOffset": 119, "endOffset": 127}, {"referenceID": 31, "context": "To help understand the significance of these results, next we provide a comparison study with the VW hashing algorithm [33, 37].", "startOffset": 119, "endOffset": 127}, {"referenceID": 27, "context": "3 Comparisons with VW Algorithm The Vowpal Wabbit (VW) algorithm [33, 37] is an influential hashing method for data/dimension reduction.", "startOffset": 65, "endOffset": 73}, {"referenceID": 31, "context": "3 Comparisons with VW Algorithm The Vowpal Wabbit (VW) algorithm [33, 37] is an influential hashing method for data/dimension reduction.", "startOffset": 65, "endOffset": 73}, {"referenceID": 21, "context": "Since [27] only compared b-bit minwise hashing with VW on a small dataset, it is more informative to conduct a comparison of the two algorithms on this 10 \u22123 10 \u22122 10 \u22121 10 0 10 1 10 2 50 60 70 80 90 100", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "The prior work [10] experimented with the VW online learning platform on the webspam dataset and reported an accuracy of 98.", "startOffset": 15, "endOffset": 19}, {"referenceID": 6, "context": ", [10, 39]) is to partition the data into blocks, which are repeatedly loaded into memory, to update the model coefficients.", "startOffset": 2, "endOffset": 10}, {"referenceID": 33, "context": ", [10, 39]) is to partition the data into blocks, which are repeatedly loaded into memory, to update the model coefficients.", "startOffset": 2, "endOffset": 10}, {"referenceID": 1, "context": "The stochastic gradient descent (SGD) [4, 5] is a very popular algorithm for online learning.", "startOffset": 38, "endOffset": 44}, {"referenceID": 32, "context": "3 Averaged SGD (ASGD) In the course of writing this paper in 2011, Leon Bottou kindly informed us that there was a recent major upgrade of the SGD code, which implemented ASGD (Averaged SGD) [38].", "startOffset": 191, "endOffset": 195}, {"referenceID": 21, "context": "CONCLUSION (b-bit) Minwise Hashing is a standard technique for similarity computation which has also recently been shown [27] to be a valuable data reduction technique in (batch) machine learning, where it can reduce both the computational overhead as well as the required infrastructure and energy consumption by orders of magnitude, at often negligible reduction in learning accuracy.", "startOffset": 121, "endOffset": 125}, {"referenceID": 2, "context": ", [6]) demonstrated that it would be sufficient to use about k \u2248 200 permutations.", "startOffset": 2, "endOffset": 5}, {"referenceID": 20, "context": "However, b-bit minwise hashing (for small values of b) does require more permutations than the original minwise hashing, as explained in [26], for example, by increasing k by a factor of 3 when using b = 1 and the resemblance threshold is R = 0.", "startOffset": 137, "endOffset": 141}], "year": 2012, "abstractText": "ABSTRACT Minwise hashing is a standard technique in the context of search for approximating set similarities. The recent work [27] demonstrated a potential use of b-bit minwise hashing [26] for batch learning on large data. However, several critical issues must be tackled before one can apply b-bit minwise hashing to the volumes of data often used industrial applications, especially in the context of search.", "creator": "LaTeX with hyperref package"}}}