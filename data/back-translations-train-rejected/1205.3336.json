{"id": "1205.3336", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-May-2012", "title": "Distribution of the search of evolutionary product unit neural networks for classification", "abstract": "This paper deals with the distributed processing in the search for an optimum classification model using evolutionary product unit neural networks. For this distributed search we used a cluster of computers. Our objective is to obtain a more efficient design than those net architectures which do not use a distributed process and which thus result in simpler designs. In order to get the best classification models we use evolutionary algorithms to train and design neural networks, which require a very time consuming computation. The reasons behind the need for this distribution are various. It is complicated to train this type of nets because of the difficulty entailed in determining their architecture due to the complex error surface. On the other hand, the use of evolutionary algorithms involves running a great number of tests with different seeds and parameters, thus resulting in a high computational cost", "histories": [["v1", "Tue, 15 May 2012 11:51:02 GMT  (206kb)", "http://arxiv.org/abs/1205.3336v1", "8 pages, 2 figures, in Proc. IADIS International Conference Applied Computing 2007 (AC 2007), ISBN 978-972-8924-30-0, pp. 266-273, Spain. Note: \"This is a reprint from a paper published in the Proceedings of the IADIS International Conference Applied Comupting 2007,this http URL\""]], "COMMENTS": "8 pages, 2 figures, in Proc. IADIS International Conference Applied Computing 2007 (AC 2007), ISBN 978-972-8924-30-0, pp. 266-273, Spain. Note: \"This is a reprint from a paper published in the Proceedings of the IADIS International Conference Applied Comupting 2007,this http URL\"", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CV", "authors": ["a j tall\\'on-ballesteros", "p a guti\\'errez-pe\\~na", "c herv\\'as-mart\\'inez"], "accepted": false, "id": "1205.3336"}, "pdf": {"name": "1205.3336.pdf", "metadata": {"source": "CRF", "title": "DISTRIBUTION OF THE SEARCH OF EVOLUTIONARY PRODUCT UNIT NEURAL NETWORKS FOR CLASSIFICATION", "authors": ["Antonio J. Tall\u00f3n-Ballesteros", "Pedro A. Guti\u00e9rrez-Pe\u00f1a", "C\u00e9sar Herv\u00e1s-Mart\u00ednez"], "emails": ["atallon@us.es", "zamarck@yahoo.es,", "chervas@uco.es"], "sections": [{"heading": null, "text": "This work deals with distributed processing in the search for an optimal classification model using evolutionary product units of neural networks. For this distributed search, we used a cluster of computers. Our goal is to obtain a more efficient design than those network architectures that do not use distributed processes and therefore lead to simpler designs. In order to obtain the best classification models, we use evolutionary algorithms to train and design neural networks, which require a very time-consuming calculation. Reasons for the necessity of this distribution are manifold. It is complicated to train this type of networks, as their architecture is difficult due to the complex error interface. On the other hand, the use of evolutionary algorithms requires the execution of a large number of tests with different seeds and parameters, resulting in high computational costs.KEYWORDSNeural networks, product units, classification, distributed processing, evolutionary algorithms."}, {"heading": "1. INTRODUCTION", "text": "This research is on the distribution of processing involved in the search for the best product unit Neural Network (PUNN) models [Durbin, 1990] [Mart\u00ednez-Estudillo, 2006A], using evolutionary algorithms, EAs. A cluster of computers [Buyya, 1999] is used to perform the distribution of this process.Many different types of neural network architectures have been used, but the most popular has been the single-hidden-layer feedback network. In addition to the numerous approaches that neural networks use in classification problems, we focus our attention on evolutionary artificial neural networks (EANNs). EANNs have been a key field of research over the past decade, which provides an improved platform for optimizing network performance and architecture (number of hidden nodes and number of connections) at the same time. [Miller, 1989] suggested that evolutionary computation was a very good candidate for finding the space associated with the fitness function of this architectures."}, {"heading": "2. DESCRIPTION", "text": "The proposed methodology consists of the use of an EA as a tool for learning the architecture and weights of a PUNN ([Mart\u00ednez-Estudillo, 2006A], [Mart\u00ednez-Estudillo, 2006B]) This class of multiplicative neural networks includes types such as sigma-pi networks and product unit networks. Some advantages of PUNNs are increased information capacities and the ability to form combinations of higher order inputs [Durbin, 1990]. Furthermore, it is possible to obtain the upper limits of the VC dimension of neural product unit networks that resemble those for sigmoidal neural networks [Schmitt, 2001]. Finally, it is a direct consequence of the Stein-Weierstrass theorem to prove that product unit neural networks are universal approximators [Mart\u00ednez-Estudillo, 2006A]."}, {"heading": "2.1 Evolutionary Algorithm", "text": "We use an EA to design the structure and learn the weights of the PUNNs. The search begins with a random initial population (EA 1), and for each iteration the population is updated using a population update algorithm. Therefore, the population is subjected to operations of replication and mutation. Crossover is not used due to its potential disadvantages in developing artificial networks. If we use the algorithm in the classification problems, we will only consider the construction of the suitability function and the changes in the mutation operators. A standard Softmax activation function is normally used for each node of the base layer, which is defined by: () 1 () jifj l fieg exx x (1), where l is the number of classes, () jf x is the output of the node j for the pattern x and () jg x is the probability of this pattern."}, {"heading": "3. IMPLEMENTED DISTRIBUTED MODELS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Experimental design distribution", "text": "In this first case, some parameters of the EA are distributed over all the nodes in the cluster: the number of neurons in the hidden layer (new), the maximum number of generations (gen) and optionally the value of the output variance (\u03b12). We will consider a basic configuration that will be modified by each of the processing nodes. Therefore, each node will have a specific mission and it will always set the same parameters. Changes have been defined relatively, and so they depend on the base configuration. Once the changes have been made, each of the nodes will conduct experiments with the new configuration. There are 8 nodes, so that in the case of distributing 3 parameters each of them will take 2 different values and in the case of 2 parameters one will have 4 different values and the other will have 2 values. For this distribution of processing we have used the following data: Equilibrium, Krebs, Pimpothyrothia, Hypothyrothia (all from the cistern: http / positive)."}, {"heading": "3.1.1 Validation technique and parameters used", "text": "In our case, the size of the tension set is 3n / 4 and n / 4 for the test set, where n is the number of patterns within the problem. The results of the experiments in this proposal were obtained with the following common parameters / characteristics of the configuration.The basic configuration values depend on the database and the number of parameters to be distributed. There will be different configurations, depending on the weather 2 / 3 parameters are distributed. Waveform, the value of \u03b12 is not distributed."}, {"heading": "3.1.2 Results", "text": "In this section we present the results obtained in the validation set of each of the groups described with 8 configurations. We have described the basic configuration as the first of each group. Among some configurations, the topology (number of neurons in the hidden layer) and the parameters (variance -\u03b12-, maximum number of generations) change and, among other things, only the topology. The best configuration is shown here in bold. We show the averages of the generalization CCR of the best obtained model, its standard deviation, the best and the worst value. Topologies are presented in this format: Number of inputs: Number of hidden layer nodes: Number of output layers. Medium-size datasets (Balance, Pima and Krebs).- The following results were obtained:"}, {"heading": "3.1.3 Statistical Analysis", "text": "We have performed a statistical analysis to compare the basic configuration and the best results for each of the first three databases mentioned above = 25,000 results. (In this comparison, we have used the CCR and the number of connections achieved in the generalization process. In balance, once the algorithm has been run 30 times, using the standard parameters (base configuration) and using those who have achieved the best results, we present the following considerations: a) using the Kolmogorov-Smirnov (K-S) tests, however, we conclude that the CCR distributions of the validation set and the number of connections in the best network model for each run are greater than 0.05, b) under the normality hypothesis, we have variance equivalence contrasts (Levene test) and means equality (with equal or different deviations depending on the previous Levene test results), if the algorithms are independent of the 30 best student test results."}, {"heading": "3.1.4 Comparison to previous results", "text": "Although the generalization of CCR values appears very good, we compare them with those previously achieved in standard PUs that do not use distribution models. In Table 8, the mean of generalization of CCR of the best model that was achieved with the best configuration using standard PUs and distributed PUs appears in bold. As we see, the average of the results achieved in the best distributed PU models in all cases exceeds the results achieved with standard PUs. In equilibrium, there is an improvement of 0.23%; a decrease in variance is also achieved, indicating greater homogeneity of the solutions. In cancer, there is a generalization of CCR increase of about 0.04% and the variance slightly decreases. Pima exhibits a more significant improvement (0.82%); however, the variance increases slightly. In hyphotyroid, the generalization of CCR increases to about 2.03% and the improvement is greater than in 1.32%."}, {"heading": "3.2 Processing distribution", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 Description", "text": "In this case, we divide the execution number on some of the nodes. Our goal is to get a measure of the optimal number of nodes taking into account some of the previous databases. We used the best configurations of the first three databases in the previous proposal."}, {"heading": "3.2.2 Performance Analysis", "text": "Since a cluster is a parallel or distributed architecture [Buyya, 1999], we use metrics to evaluate the performance of parallel algorithms ([Kumar, 1994], [Wilkinson, 1999], [Ortega, 2004]), such as acceleration and efficiency. Acceleration is defined as the ratio between the execution time in a processor and the parallel time with P equal to processors. This definition can be applied to our case, bearing in mind that we refer to nodes and not processors. S = T1 / TPIn general, acceleration in a processor and parallel time is equal to processors with P. We get the number of processors from which this saturation occurs. Acceleration takes into account the execution time and it is not always the best measure to evaluate algorithm throughput."}, {"heading": "4. CONCLUSIONS", "text": "Once we have received the results of the various experiments, we can come to the following conclusion: a) two proposals have been implemented that distribute different characteristics of the EA and the topology or the experimental process; b) after carrying out the tests, we can mention that the efficiency and effectiveness of the proposal are acceptable; c) we have conducted experiments with medium-sized classification databases to demonstrate the basic behaviour of the model; d) through the model distribution, we have overcome the previous results."}], "references": [{"title": "An evolutionary algorithm that constructs recurrent neural networks", "author": ["Angeline", "P. J"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Angeline and J,? \\Q1994\\E", "shortCiteRegEx": "Angeline and J", "year": 1994}, {"title": "Neural Networks for Pattern Recognition", "author": ["M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q1995\\E", "shortCiteRegEx": "Bishop", "year": 1995}, {"title": "High Performance Cluster Computing: Architectures and Systems, Vol. 1. Prentice Hall PTR, NJ", "author": ["R. Buyya"], "venue": "United States of America", "citeRegEx": "Buyya,? \\Q1999\\E", "shortCiteRegEx": "Buyya", "year": 1999}, {"title": "Weather forecasting with adaptive time-delay neural networks: A case study", "author": ["A Carpintero"], "venue": "In Proc. of Int. Conference On Neural Network Processing. Seul, Korea", "citeRegEx": "Carpintero,? \\Q1994\\E", "shortCiteRegEx": "Carpintero", "year": 1994}, {"title": "Product Units: A computationally powerful and biologically plausible extension to backpropagation networks", "author": ["R. Durbin", "D. Rumelhart"], "venue": "Neural Computation,", "citeRegEx": "Durbin and Rumelhart,? \\Q1990\\E", "shortCiteRegEx": "Durbin and Rumelhart", "year": 1990}, {"title": "Multiobjetive cooperative coevolution of artificial neural networks", "author": ["N Garc\u00eda-Pedrajas"], "venue": "Neural Networks,", "citeRegEx": "Garc\u00eda.Pedrajas,? \\Q2002\\E", "shortCiteRegEx": "Garc\u00eda.Pedrajas", "year": 2002}, {"title": "Global optimization algorithms for training product unit neural networks", "author": ["A. Ismail", "A.P. Engelbrecht"], "venue": "In International Joint Conference on Neural Networks", "citeRegEx": "Ismail and Engelbrecht,? \\Q2000\\E", "shortCiteRegEx": "Ismail and Engelbrecht", "year": 2000}, {"title": "Training product unit neural networks with genetic algorithms", "author": ["D.J. Janson", "J.F. Frenzel"], "venue": "IEEE Expert,", "citeRegEx": "Janson and Frenzel,? \\Q1993\\E", "shortCiteRegEx": "Janson and Frenzel", "year": 1993}, {"title": "Introduction to Parallel Computing. Design and Analysis of Algorithms", "author": ["V Kumar"], "venue": null, "citeRegEx": "Kumar,? \\Q1994\\E", "shortCiteRegEx": "Kumar", "year": 1994}, {"title": "Evolutionary Product Unit based Neural Networks for Regression", "author": ["Mart\u00ednez-Estudillo", "A. C"], "venue": "Neural Networks,", "citeRegEx": "Mart\u00ednez.Estudillo and C,? \\Q2006\\E", "shortCiteRegEx": "Mart\u00ednez.Estudillo and C", "year": 2006}, {"title": "Hybridization of evolutionary algorithms and local search by means of a clustering method", "author": ["Mart\u00ednez-Estudillo", "A. C"], "venue": "IEEE Transaction on Systems, Man and Cybernetics, Part. B: Cybernetics,", "citeRegEx": "Mart\u00ednez.Estudillo and C,? \\Q2006\\E", "shortCiteRegEx": "Mart\u00ednez.Estudillo and C", "year": 2006}, {"title": "Arquitectura de Computadores", "author": ["J Ortega"], "venue": null, "citeRegEx": "Ortega,? \\Q2004\\E", "shortCiteRegEx": "Ortega", "year": 2004}, {"title": "Adaptative Pattern Recognition and Neural Networks", "author": ["Y.H. Pao"], "venue": null, "citeRegEx": "Pao,? \\Q1989\\E", "shortCiteRegEx": "Pao", "year": 1989}, {"title": "Proben1\u2014A set of neural network benchmark problems and benchmarking rules. Fakult\u00e4t f\u00fcr Informatik, Univ. Karlsruhe, Karlsruhe", "author": ["L. Prechelt"], "venue": null, "citeRegEx": "Prechelt,? \\Q1994\\E", "shortCiteRegEx": "Prechelt", "year": 1994}, {"title": "Evolutionsstrategien: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution", "author": ["I. Rechenberg"], "venue": "Holzboog Verlag. Stuttgart,", "citeRegEx": "Rechenberg,? \\Q1973\\E", "shortCiteRegEx": "Rechenberg", "year": 1973}, {"title": "Law discovery from financial data using neural networks", "author": ["K Saito"], "venue": "Proceedings of IEEE / IAFE / INFORMS. Conference on Computational Intelligence for Financial Engineering (CIFEr00)", "citeRegEx": "Saito,? \\Q2000\\E", "shortCiteRegEx": "Saito", "year": 2000}, {"title": "On the Complexity of Computing and Learning with Multiplicative Neural Networks", "author": ["M. Schmitt"], "venue": "Neural Computation,", "citeRegEx": "Schmitt,? \\Q2001\\E", "shortCiteRegEx": "Schmitt", "year": 2001}, {"title": "Parallel Programming Techniques and applications using networked workstations and parallel computers", "author": ["B. Wilkinson", "M. Allen"], "venue": "Prentice-Hall. USA.", "citeRegEx": "Wilkinson and Allen,? 1999", "shortCiteRegEx": "Wilkinson and Allen", "year": 1999}, {"title": "A new evolutionary system for evolving artificial neural networks", "author": ["X. Yao", "Y. Liu"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Yao and Liu,? \\Q1997\\E", "shortCiteRegEx": "Yao and Liu", "year": 1997}], "referenceMentions": [{"referenceID": 2, "context": "A cluster of computers [Buyya, 1999] will be used to carry out the distribution of this processing.", "startOffset": 23, "endOffset": 36}, {"referenceID": 5, "context": "Since then, many evolutionary programming methods have been developed to evolve artificial neural networks, for instance [Yao, 1997] and [Garc\u00eda-Pedrajas, 2002].", "startOffset": 137, "endOffset": 160}, {"referenceID": 16, "context": "Besides that, it is possible to obtain the upper bounds of the VC dimension of product-unit neural networks similar to those obtained for sigmoidal neural networks [Schmitt, 2001].", "startOffset": 164, "endOffset": 179}, {"referenceID": 14, "context": "We use the 1/5 success rule of Rechenberg [Rechenberg, 1973].", "startOffset": 42, "endOffset": 60}, {"referenceID": 13, "context": "1 Validation technique and parameters used We have considered a cross validation experimental design called hold-out [Prechelt, 1994], that consists of splitting the data in two sets: train and test.", "startOffset": 117, "endOffset": 133}, {"referenceID": 2, "context": "Since a cluster is a parallel or distributed architecture [Buyya, 1999], we will use performance evaluation measures of parallel algorithms ([Kumar, 1994], [Wilkinson, 1999], [Ortega, 2004]), like the speedup and the efficiency.", "startOffset": 58, "endOffset": 71}, {"referenceID": 8, "context": "Since a cluster is a parallel or distributed architecture [Buyya, 1999], we will use performance evaluation measures of parallel algorithms ([Kumar, 1994], [Wilkinson, 1999], [Ortega, 2004]), like the speedup and the efficiency.", "startOffset": 141, "endOffset": 154}, {"referenceID": 11, "context": "Since a cluster is a parallel or distributed architecture [Buyya, 1999], we will use performance evaluation measures of parallel algorithms ([Kumar, 1994], [Wilkinson, 1999], [Ortega, 2004]), like the speedup and the efficiency.", "startOffset": 175, "endOffset": 189}], "year": 2012, "abstractText": "This paper deals with the distributed processing in the search for an optimum classification model using evolutionary product unit neural networks. For this distributed search we used a cluster of computers. Our objective is to obtain a more efficient design than those net architectures which do not use a distributed process and which thus result in simpler designs. In order to get the best classification models we use evolutionary algorithms to train and design neural networks, which require a very time consuming computation. The reasons behind the need for this distribution are various. It is complicated to train this type of nets because of the difficulty entailed in determining their architecture due to the complex error surface. On the other hand, the use of evolutionary algorithms involves running a great number of tests with different seeds and parameters, thus resulting in a high computational cost.", "creator": "PScript5.dll Version 5.2.2"}}}