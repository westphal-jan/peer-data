{"id": "1205.3981", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2012", "title": "kLog: A Language for Logical and Relational Learning with Kernels", "abstract": "kLog is a logical and relational language for kernel-based learning. It allows users to specify logical and relational learning problems at a high level in a declarative way. It builds on simple but powerful concepts: learning from interpretations, entity/relationship data modeling, logic programming and deductive databases (Prolog and Datalog), and graph kernels. kLog is a statistical relational learning system but unlike other statistical relational learning models, it does not represent a probability distribution directly. It is rather a kernel-based approach to learning that employs features derived from a grounded entity/relationship diagram. These features are derived using a novel technique called graphicalization: first, relational representations are transformed into graph based representations; subsequently, graph kernels are employed for defining feature spaces. kLog can use numerical and symbolic data, background knowledge in the form of Prolog or Datalog programs (as in inductive logic programming systems) and several statistical procedures can be used to fit the model parameters. The kLog framework can -- in principle -- be applied to tackle the same range of tasks that has made statistical relational learning so popular, including classification, regression, multitask learning, and collective classification.", "histories": [["v1", "Thu, 17 May 2012 17:00:00 GMT  (423kb,D)", "https://arxiv.org/abs/1205.3981v1", null], ["v2", "Fri, 18 May 2012 12:46:57 GMT  (429kb,D)", "http://arxiv.org/abs/1205.3981v2", null], ["v3", "Fri, 22 Jun 2012 12:06:52 GMT  (424kb,D)", "http://arxiv.org/abs/1205.3981v3", null], ["v4", "Mon, 17 Feb 2014 11:47:55 GMT  (511kb,D)", "http://arxiv.org/abs/1205.3981v4", null], ["v5", "Mon, 28 Jul 2014 13:41:00 GMT  (512kb,D)", "http://arxiv.org/abs/1205.3981v5", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.PL", "authors": ["paolo frasconi", "fabrizio costa", "luc de raedt", "kurt de grave"], "accepted": false, "id": "1205.3981"}, "pdf": {"name": "1205.3981.pdf", "metadata": {"source": "CRF", "title": "kLog: A Language for Logical and Relational Learning with Kernels", "authors": ["Paolo Frasconia", "Fabrizio Costab", "Luc De Raedtc", "Kurt De Gravec"], "emails": ["p-f@dsi.unifi.it", "costa@informatik.uni-freiburg.de", "Luc.DeRaedt@cs.kuleuven.be", "Kurt.DeGrave@cs.kuleuven.be"], "sections": [{"heading": null, "text": "We present kLog, a novel approach to statistical, relational learning. Unlike standard approaches, kLog does not represent a probability distribution directly. Rather, it is a language for performing kernel-based learning on expressive logical and relational representations. kLog allows users to declare learning problems. kLog is built on simple but powerful concepts: learning from interpretations, entity / relationship data modeling, logical programming, and deductive databases. The kernel's access to the rich representation is mediated by a technique we call graphicalization: Relational representation is first transformed into a graph - in particular a grounded entity / relationship diagram. Subsequently, a graph kernel choice defines the functional space. kLog supports mixed numerical and symbolic data, as well as background knowledge in the form of prolog or database programs, as in inductive programming systems."}, {"heading": "1. Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2. A kLog example", "text": "Before delving into the technical details of kLog, we illustrate the various steps using a real-life example from the UW-CSE dataset created by Domingos et al. to demonstrate the capabilities of MLNs. [7] Anonymous data was collected by the Department of Computer Science and Engineering at the University of Washington. Basic units include people (students or professors), academic papers, and academic courses. Available relationships determine, for example, whether a person was the author of a particular paper or whether they participated in the teaching activities of a particular course. The learning task consists of predicting the student advisors, i.e. predicting the binary relationship recommended by students and professors."}, {"heading": "2.1. Data format", "text": "Since (first order logic) functions are not allowed in language, a soil atom is essentially like a tuple in a relational database, for example, taught by (course170, person211, winter _ 0102). In this example, there are five interpretations: ai, graphics, languages, systems and theories that correspond to different research groups in the department. For example, a fragment of interpretation ai is advised in Listing 1.1 _ by (person21, person211). advise _ by (person211, person2personalized personalizations). 2 has _ position (personalized 211, faculty). has _ position (personalized 407, faculty). 3 in _ phase (personalized 014, personalized) personalized."}, {"heading": "2.2. Data modeling and learning", "text": "The first step in kLog modeling is to describe the domain using a classical database tool: Entity relationship diagrams are intensified. We start by modeling two entity types: student and professor, two inconsistent relationships: in _ phase and has _ position, and a binary relationship: advise _ by (that is the goal in this example).The diagram is shown in Figure 1. the kLog data model is written with the fragment of the listing code 2.1 Signature hat _ position (Professor _ id::: Professor, Position: property): extended. 2 Signature advise _ by (p1:: Student, p2:: Professor): extended. 3 Signature student (student _ id:: Self): extended. 4 Signature Professor _ id:: extended signature."}, {"heading": "2.3. Graphicalization", "text": "Graphic is our approach to grasping the relational structure of the data by means of a graph. It allows the use of core methods in SRL via a graph nucleus that measures the similarity between two graphs. It maps a set of base atoms into an undirected two-sided graph whose nodes are real base atoms and whose edges connect an entity atom to a kinship atom when the identifier of the former appears as an argument in the latter. The graph resulting from the graphing of the ai interpretation is shown in Figure 22. From this graph, kLog generates statement characteristics (based on a graph nucleus) for use in the learning process. The details of the graphing process and the kernel are given in Sections 6 and 6.2. Now that we have specified the inputs to the learning system, we have yet to determine the learning problem."}, {"heading": "3. Statistical setting", "text": "Our general approach to constructing a statistical model is based on learning interpretation settings = > Emissions. An interpretation, or logical world, is a set of basic atoms. We assume that interpretations are considered identical and independent of a fixed and unknown distribution. The goal is to use a data sample to draw some form of (direct or indirect) conclusions about this distribution (e.g., the first n natural numbers) that can be thought of as interpretation identifiers. As in other statistical learning systems, the goal is to draw some form of (direct or indirect) conclusions about this distribution. For simplicity, we focus in this paper mainly on supervised learning. In this case, it is customary to think of data as coming from two separate portions: inputs (so-called predictors or independent variables in statistics) and outputs (so-called answers or dependent variables). In our framework, this distinction is reflected in a given interpretation."}, {"heading": "4. The kLog language", "text": "A kLog program consists of: \u2022 a set of basic facts embedded in a standard prolog database that represent the data of the learning problem (see e.g. Listing 1); \u2022 a set of signature explanations (e.g. Listing 2); \u2022 a set of prolog predicates associated with intensive signatures (e.g. Listing 3); \u2022 a standard prolog program that specifies the learning problem and calls kLog predicates."}, {"heading": "4.1. Data modeling and signatures", "text": "In fact, it will be able to come out on top in the way that it has come out on top."}, {"heading": "4.2. Supervised learning jobs", "text": "rE \"s tis rf\u00fc ide rf\u00fc ide rf\u00fc the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green"}, {"heading": "4.3. Implementation", "text": "kLog is currently embedded in Yap Prolog [21] and consists of three main components: (1) a domain-specific interpreter, (2) a database loader, and (3) a library of predicates that are used to define the learning task, declare the graph core and learning model, and perform training, predictions, and performance assessments. kLog analyzes the signature declarations (see Section 4.1), possibly enriched with intensive and additional predicates. The database loader reads in a file with extensive basic facts and generates a graph for each interpretation in accordance with the procedure described in Section 6. the library contains common utilities for training and testing. Most of kLog is written in Prolog, except for the Featurevector generation and statistical learners written in C + +. kLog contains SVM 22 [and SVM] derivatives [and SVM] (grades)."}, {"heading": "5. Examples", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "6. Graphicalization and feature generation", "text": "The goal is to map an interpretation z = (x, y) into a feature vector \u03c6 (z) = \u03c6 (x, y) \u0432F (see section 3), which enables the application of several supervised learning algorithms that construct linear functions in the feature space F. In this context \u03c6 (z) can be calculated either explicitly or implicitly via a kernel function K (z, z) = < \u03c6 (z), \u03c6 (z) >. Kernel-based solutions are very popular, sometimes even faster in terms of computation, and can yield infinitely dimensional feature spaces. On the other hand, the explicit feature map construction can offer advantages in our setting, especially when dealing with large learning problems (many interpretations) and structured output tasks (exponentially many possible predictions). Our framework is based on two steps: first, an interpretation z is mapped into an undirected graph Gz; then a feature vector () is mapped out of a feature vector."}, {"heading": "6.1. Graphicalization procedure", "text": "Considering an interpretation of z, we construct a two-part graph Gz ([Vz, Fz], Ez) as follows (see Appendix B for notational conventions and Figure 2 for an example).Vertices: There is a vertex in Vz for each soil atom of each E relationship, and there is a vertex in Fz for each soil atom of each R relationship. Vertices are indicated by the predicate name of the soil atom, followed by the list of property values. However, identifiers in a soil atom do not appear in the labels, but they clearly identify vertices. Tupel Ids (v) mark the identifiers in the soil atom mapped in Vertex v. Edges: uv-Ez if and only if u-Vz, v-Fz, and Ids (u) -Ids of the world (v) vertices (v) are indicated by the role under which the identification of the soil atom appears."}, {"heading": "6.2. Graph kernel", "text": "While, in principle, any graph kernel can be used, there are several requirements that the chosen kernel must meet in practice. On the one hand, computing power is very important, both in terms of the number of graphs and in terms of graph size, since the grounding phase in the graphicalization procedure can yield very large graphs. On the other hand, we need a general purpose core with a flexible bias to adapt to a variety of application scenarios. In the case of independent interpretations (as in Examples 5.1 and 5.2), any graph kernel (several exist in the literature, see e.g. [37, 38, 39, 40, 41, 11, 42] and references to it) can be applied directly to the result of the graphicalization procedure (as in Examples 5.1 and 5.2), since there is exactly one graph kernel for each interpretation (several exist in the literature, see e.g. [37, 38, 39, 41, 11, 42, and references to it)."}, {"heading": "6.2.1. Kernel definition and notation", "text": "The NSPDK is an instance of a decomposition nucleus where \"parts\" are pairs of subgraphs (for more details on decomposition nuclei see Appendix C. For a given graph G = (V, E), and an integer r \u2265 0, let N vr (G) be the shortest path between x and v 9. A neighborhood is therefore a topological sphere with center v. Let us also introduce the following neighborhood-pair relationship: Rr, d = {(N vr) \u2264 r}, (G): d? (u, v) = d} (3), that is, relationship Rr, d), d) whose roots are exactly at the distance d."}, {"heading": "6.2.2. Subgraph kernels", "text": "Applying the graphing method to different relational datasets can potentially produce graphs with significantly different properties. In some cases (discrete property ranges), an exact agreement between neighborhood graphs is appropriate, but in other cases (continuous property ranges), it is more appropriate to use a soft conception of association. In the following sections, we present variants of a coupled subgraph that are used when the atoms in the relational dataset can have at most one discrete or continuous property, or when more general tuples of properties are allowed."}, {"heading": "6.2.3. Exact graph matching", "text": "An important case is when the atoms assigned to the vertex theorem of the resulting graph by the graphical method can exhibit a maximum of one discrete property. In this case, an atom r (c) becomes a vertex v, the designation of which is obtained by concatenating the signature name and the attribute value. In this case, the cape subgraph has the following form: \u03basubgraph (((A, B), (A \u2032, B \u2032) = 1A \u0445 = A \u2032 1B \u0445 = B \u2032 (8), where 1 denotes the indicator function and \u0445 = isomorphism between the graphs. Note that 1A \u0445 = A \u2032 is a valid kernel between graphs under the characteristic map transforming A \u2032 1B \u0445 = B \u2032 (8), a sequence of all zeros with the exception of the i-th element equal to 1 in accordance with the identifier for the brocanonic representation of A 43, which requires an isomatic evaluation of the K8."}, {"heading": "6.2.4. Soft matches", "text": "The idea of counting exact neighborhood subgraphs to express the similarity of the graphs is appropriate if the graphs are sparse (i.e., if the edge and vertex are of equal size) and if the maximum vertex is low. However, in these cases, a better solution is to loosen the all-or-nothing. A concrete example is when text information associated with a document is explicitly modeled (i.e., when word units are associated with a document unit: in this case, the ground type of match so that subgraphs partially or softly match. Although there are several graph cores based on this type of match, they generally suffer from very high computational costs."}, {"heading": "6.2.5. Tuples of properties", "text": "A standard assumption in graph kernels is that vertex and edge labels are elements of a discrete domain (B \u2032 V \u2032 11). However, in kLog the information associated with vertices is a tuple that can contain both discrete and real values. Here, we expand NSPDK to allow both hard and soft agreement between properties via graphics with properties that can be discrete, real, or a mixture of the two types. While similar extensions can be designed in principle for other graphs, the literature mainly focuses on properties with individual categorical names. The key idea is to use the canonical vertex (introduced in Section 6.2.3) to further characterize each property: in this way, the kernel is defined only between tuples of vertices that are isomorphisomorphism.We are the general structure of the kernel written on the subgraph as: (A, B \u2032 = V)."}, {"heading": "6.2.6. Domain knowledge bias via kernel points", "text": "Sometimes it is convenient to explicitly select the neighborhood subgraphs for efficiency or to inject domain knowledge into the kernel. We offer a declarative way to do this by introducing the set of key points, a subset of V (G) that contains all the vertices associated with the basic atoms of some specially marked signatures. We then redefine the relationship Rr, d (A, B, G) used in Equation 4, as in Section 6.2.1, but with the additional constraints that the roots of A and B must be key points. Key points are typically corner points that are assumed to represent information of high importance for the task at hand. Key points that are not key points only contribute to the core calculation when they occur in the neighborhood of key points. In kLog, key points are declared as a list of domain relationships: All vertibles that correspond with basic atoms of these relationships become key points."}, {"heading": "6.3. Viewpoints", "text": "The above approach effectively defines a kernel by InterpretationK (z, z) = K (Gz, Gz), where Gz is the result of the graphics applied to interpretation. For learning jobs such as classification or regression to interpretations (see Table 2), this kernel is directly usable in conjunction with simple kernel machines such as SVM. When we switch to more complex jobs such as classification of entities or tuples of entities, the kernel induces a feature vector (x, y) suitable for the application of a structured output technique where f (x) = argmaxy w > p (x, y).Alternatively, we can convert the structured output problem into a series of independent sub-problems as follows. For simplicity, we assume that the learning job consists of a single relationship r of relational activity. We call each soil atom of r a case."}, {"heading": "6.4. Parameterization in kLog", "text": "The cores presented in this section, together with the graphics method, form a statistical model that operates in a potentially very high-dimensional feature space. Although learners have a great deal of leeway to offer a degree of robustness to high-dimensional representations, it is still the responsibility of the user to choose suitable kernel parameters (r * and d *) to avoid overfitting. It should be noted that sub-graphics in the NSPDK effectively act as templates that are matched with graphics interpretations for new data. As identifiers do not occur even as a result of the graphics process, the same template may coincide several times with a form of parameter binding, as with other methods of statistical relational learning."}, {"heading": "7. kLog in practice", "text": "In this section we will demonstrate the use of kLog in various applications. All experimental results reported in this section were obtained with LibSVM in binary classification, multiclass or regression mode."}, {"heading": "7.1. Predicting a single property of one interpretation", "text": "In fact, it is that one is able to feel as if one is able to move to another world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, and in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to create a new world, in which one is able to enter a new world."}, {"heading": "7.2. Link prediction", "text": "We report here on experimental results on the UW-CSE domain, which we have already described in detail in Section 2. To evaluate the behavior of kLog, we evaluated the accuracy of the prediction according to the \"leave-one-research-group-out setup\" principle of [7].The whole 5-fold procedure runs in approximately 20 seconds on a single core of a 2.5 GHz Core i7 CPU. Comparative results in terms of Markov logic are shown in Figure 6 (MLN results published in [7]).The whole 5-fold procedure runs in a single core of 2.5 GHz Core i7 CPU. Compared to MLNs, kLog in the current implementation is the disadvantage of not performing collective mapping, but the advantage of defining more powerful features thanks to the graphical kernel."}, {"heading": "7.3. Entity classification", "text": "It consists of academic web pages composed of four computer science departments and the task is to identify the category (such as the student page, the course page, the professors page, etc.) Figure 8 shows the E / R diagram used in kLog. One of the most important relationships in this area is that words are associated to web pages. After the graphics, the depressions that represent web pages would have a high degree (at least the number of words), making the standard NSPDK of [11] completely inadequate: even by setting the maximum distance d: 1 and the maximum radius r: the hard match would essentially produce a unique property for each page. In this area, we can therefore appreciate the flexibility of the kernel defined in Section 6.2. In particular, the soft match of the kernel creates histograms of the word occurrences on the page that are very similar to the purses."}, {"heading": "7.4. Domains with a single interpretation", "text": "The Internet Movie Database (IMDb) collects information about movies and their cast, persons and companies working in the motion picture industry. (We focus on predicting whether their first weekend box office receipts exceed $2 million, a learning task previously defined in [30, 64]. (The learning environment defined so far (learning from independent interpretations) is not directly applicable, as train and test data must necessarily occur within the same interpretation. (The notion that slicing in kLog allows us to overcome this difficulty is a division of the true soil atoms in a given interpretation: z = (i1),., z (in)} where the disjoint sets z (j) are mentioned and the index set I = {i1,. (in}) is endowed with a total company accounting."}, {"heading": "8. Related work", "text": "This year, the time has come for an agreement to be reached in just a few days."}, {"heading": "9. Conclusions", "text": "We have introduced a novel language for logical and relationship-based learning called kLog, but there are unanswered and interesting open questions for collective research. It integrates logical and relationship-oriented learning with core methods and provides a basic framework for statistical learning based on core methods rather than graphical models. It uses a representation that is based on E / R modelling but is close to representations used by modern statistical learners. We have shown that the kLog framework can be used to formulate and address a wide range of learning tasks that are at least comparable to the state of the art of statistical relational learning techniques, and also that it can be used as a programming language for machine learning. The system presented in this paper is a first step towards a kernel-based language for relational learning, but there are unanswered and interesting open questions for collective research."}, {"heading": "Acknowledgments", "text": "We thank Tom Schrijvers for his dependency analysis code. PF was partly supported by the KU Leuven SF / 09 / 014 and partly by the Italian Ministry of University and Research PRIN 2009LNP494. KDG was partly supported by the KU Leuven GOA / 08 / 008 and partly by the ERC Starting Grant 240186 \"MiGraNT.\" Finally, we would like to thank the anonymous reviewers and the co-editor for their very useful and constructive comments."}, {"heading": "Appendix A. Syntax of the kLog domain declaration section", "text": "A kLog program consists of prolog code supplemented by a domain declaration section delimited by the two keywords begin _ domain and end _ domain and one or more signature declarations. A signature declaration consists of a signature header followed by one or more prolog clauses. Clauses in a signature declaration form the declaration of signature predicates and are automatically associated with the current signature heade. kLog also provides a library of prolog predicates with a special meaning for kLog, as discussed in this paragraph. A brief BNF description of the grammar of kLog domains can be found in Figure A.9.In addition, kLog provides a library of prolog predicates for handling data, learning, and performance measurement."}, {"heading": "Appendix B. Definitions", "text": "A diagram G = (V, E) consists of two sets V and E. The notation V (G) and E (G) is used when G is not the only graph to be considered. The elements of V are called vertices and the elements of E are called edges. Each edge has a set of two elements in V, which are called their endpoints, which we designate by concatenating the vertices variables, for example, we represent the edge between the vertices u and v with uv. In this case, we say that u is adjazent to v and that uv is an incident to u and v. The degree of a vertex is the number of edges that occur on it. A diagram is divided in two if its vertix set is divided into two subsets X and Y, so that each edge has an end in X and the other in Y."}, {"heading": "Appendix C. Decomposition kernels", "text": "We follow the notation in [92]. Given a setX and a function K: X > X > nel (R), we believe that K is a kernel on X \u00b7 X if K is symmetrical, i.e. if for each x-and y-X-K (x, y) = K (y, x), and if K is positive-semidefinitive, i.e. if for each N \u2265 1 and each x1,..... cN-R or equivalent if all its eigenvalues are not negative. It is easy to see that ifeach x-X (xi, xj) is positively semidefined, i.e. ij cicjKij \u2265 0 for all c1,."}, {"heading": "Appendix D. Graph invariant pseudo-identifier computation", "text": "In fact, it is so that it will be able to hide in order to erenen.n ndU \"n, he says,\" it is able to be in a position to eren.lcihsrcehncsrteeSi mentioned. \""}], "references": [{"title": "Structured machine learning: the next ten years", "author": ["T. Dietterich", "P. Domingos", "L. Getoor", "S. Muggleton", "P. Tadepalli"], "venue": "Mach Learn 73 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "R", "author": ["L. De Raedt", "B. Demoen", "D. Fierens", "B. Gutmann", "G. Janssens", "A. Kimmig", "N. Landwehr", "T. Mantadelis", "W. Meert"], "venue": "Rocha, et al., Towards digesting the alphabet-soup of statistical relational learning ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "S", "author": ["L. De Raedt", "P. Frasconi", "K. Kersting"], "venue": "Muggleton (Eds.), Probabilistic inductive logic programming: theory and applications, Vol. 4911 of Lecture notes in computer science, Springer, Berlin", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "B", "author": ["L. Getoor"], "venue": "Taskar (Eds.), Introduction to statistical relational learning, MIT Press, Cambridge, Mass.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast learning of relational kernels", "author": ["N. Landwehr", "A. Passerini", "L. De Raedt", "P. Frasconi"], "venue": "Machine learning 78 (3) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Max-margin Markov networks", "author": ["B. Taskar", "C. Guestrin", "D. Koller"], "venue": "in: S. Thrun, L. Saul, B. Sch\u00f6lkopf (Eds.), Proceedings of Neural Information Processing Systems, MIT Press, Cambridge, MA", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos"], "venue": "Machine Learning 62 ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning probabilistic relational models", "author": ["N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer"], "venue": "in: International Joint Conference on Artificial Intelligence", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Logical and relational learning", "author": ["L. De Raedt"], "venue": "1st Edition, Cognitive technologies, Springer, New York", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Probabilistic entity-relationship models", "author": ["D. Heckerman", "C. Meek", "D. Koller"], "venue": "PRMs, and plate models, in: L. Getoor, B. Taskar (Eds.), Introduction to Statistical Relational Learning, The MIT Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast neighborhood subgraph pairwise distance kernel", "author": ["F. Costa", "K. De Grave"], "venue": "in: Proc. of the 27th International Conference on Machine Learning ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["I. Tsochantaridis", "T. Joachims", "T. Hofmann", "Y. Altun"], "venue": "Journal of Machine Learning Research 6 (2) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "On discriminative vs", "author": ["A.Y. Ng", "M.I. Jordan"], "venue": "generative classifiers: A comparison of logistic regression and naive bayes, in: T. Dietterich, S. Becker, Z. Ghahramani (Eds.), Advances in Neural Information Processing Systems (NIPS) 14", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "An introduction to conditional random fields for relational learning", "author": ["C. Sutton", "A. McCallum"], "venue": "in: Getoor and Taskar", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Hidden markov support vector machines", "author": ["Y. Altun", "I. Tsochantaridis", "T. Hofmann"], "venue": "in: Twentieth International Conference on Machine Learning ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Applications of stochastic context-free grammars using the inside-outside algorithm", "author": ["K. Lari", "S. Young"], "venue": "Computer speech & language 5 (3) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1991}, {"title": "Stochastic logic programs", "author": ["S. Muggleton"], "venue": "in: L. De Raedt (Ed.), Advances in Inductive Logic Programming, IOS Press", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Discriminative probabilistic models for relational data", "author": ["B. Taskar", "P. Abbeel", "D. Koller"], "venue": "in: A. Darwiche, N. Friedman (Eds.), Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann, San Fransisco, CA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2002}, {"title": "Improving Prolog programs: Refactoring for Prolog", "author": ["A. Serebrenik", "T. Schrijvers", "B. Demoen"], "venue": "Theory and Practice of Logic Programming 8 (2) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning 73 (3) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "The Yap Prolog system", "author": ["V.S. Costa", "R. Rocha", "L. Damas"], "venue": "Theory and Practice of Logic Programming 12 (1-2) ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "in: Y. Lechevallier, G. Saporta (Eds.), Proceedings of the 19th International Conference on Computational Statistics ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Mutagenesis: ILP experiments in a non-determinate biological domain", "author": ["A. Srinivasan", "S.H. Muggleton", "R. King", "M. Sternberg"], "venue": "in: Proceedings of the 4th International Workshop on Inductive Logic Programming, volume 237 of GMD-Studien", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1994}, {"title": "A new atom-additive method for calculating partition coefficients", "author": ["R. Wang", "Y. Fu", "L. Lai"], "venue": "J. Chem. Inf. Comput. Sci 37 (3) ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research 6 (1) ", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "S", "author": ["M. Craven", "D. DiPasquo", "D. Freitag", "A. McCallum", "T. Mitchell", "K. Nigam"], "venue": "Slattery, Learning to extract symbolic knowledge from the world wide web ", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1998}, {"title": "S", "author": ["G.H. Bakir", "T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola", "B. Taskar"], "venue": "V. N. Vishwanathan (Eds.), Predicting Structured Data, Neural Information Processing, The MIT Press", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning to Classify Text using Support Vector Machines: Methods", "author": ["T. Joachims"], "venue": "Theory, and Algorithms, Kluwer Academic Publishers, Dordrecht", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Collective classification with relational dependency networks", "author": ["J. Neville", "D. Jensen"], "venue": "in: Workshop on Multi-Relational Data Mining ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Feature discovery with type extension trees", "author": ["P. Frasconi", "M. Jaeger", "A. Passerini"], "venue": "in: Proc. of the 18th Int. Conf. on Inductive Logic Programming, Springer", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}, {"title": "Global protein function prediction from protein-protein interaction networks", "author": ["A. Vazquez", "A. Flammini", "A. Maritan", "A. Vespignani"], "venue": "Nature biotechnology 21 (6) ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2003}, {"title": "Kernelbased data fusion and its application to protein function prediction in yeast", "author": ["G. Lanckriet", "M. Deng", "N. Cristianini", "M. Jordan", "W. Noble"], "venue": "in: Proceedings of the Pacific Symposium on Biocomputing, Vol. 9", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-P\u00e9rez"], "venue": "Artificial Intelligence 89 (1-2) ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "Adaptive computation and machine learning, MIT Press, Cambridge, MA", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "A general framework for adaptive processing of data structures", "author": ["P. Frasconi", "M. Gori", "A. Sperduti"], "venue": "IEEE Transactions on Neural Networks 9 (5) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1998}, {"title": "Cyclic pattern kernels for predictive graph mining", "author": ["T. Horv\u00e1th", "T. G\u00e4rtner", "S. Wrobel"], "venue": "in: Proceedings of KDD 04, ACM Press", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2004}, {"title": "Graph kernels for chemical informatics", "author": ["L. Ralaivola", "S. Swamidass", "H. Saigo", "P. Baldi"], "venue": "Neural Networks 18 (8) ", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2005}, {"title": "Graph kernels for molecular structure-activity relationship analysis with support vector machines", "author": ["P. Mahe", "N. Ueda", "T. Akutsu", "J. Perret", "J. Vert"], "venue": "J. Chem. Inf. Model 45 (4) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2005}, {"title": "Kernels for structured data", "author": ["T. G\u00e4rtner"], "venue": "Vol. 72 of Series in machine perception and artificial intelligence, World Scientific, Singapore", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2008}, {"title": "Graph kernels", "author": ["S.V.N. Vishwanathan", "N.N. Schraudolph", "R. Kondor", "K.M. Borgwardt"], "venue": "J. Mach. Learn. Res. 99 ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Weisfeiler-lehman graph kernels", "author": ["N. Shervashidze", "P. Schweitzer", "E.J. Van Leeuwen", "K. Mehlhorn", "K.M. Borgwardt"], "venue": "The Journal of Machine Learning Research 12 ", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2011}, {"title": "Practical graph isomorphism", "author": ["B.D. McKay"], "venue": "Congressus Numerantium 30 ", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1981}, {"title": "J", "author": ["X. Yan"], "venue": "Han., gSpan: Graph-based substructure pattern mining, in: Proc. 2002 Int. Conf. Data Mining (ICDM \u030102)", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2002}, {"title": "Isomorphism of graphs of bounded valence can be tested in polynomial time", "author": ["E.M. Luks"], "venue": "J. Comput. Syst. Sci 25 ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1982}, {"title": "A parametric filtering algorithm for the graph isomorphism problem", "author": ["S. Sorlin", "C. Solnon"], "venue": "Constraints 13 ", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Weighted decomposition kernels", "author": ["S. Menchetti", "F. Costa", "P. Frasconi"], "venue": "in: L. De Raedt, S. Wrobel (Eds.), Proceedings of the 22nd International Conference on Machine Learning, Vol. 119 of ACM International Conference Proceeding Series, ACM, New York, NY", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2005}, {"title": "Stacked graphical models for efficient inference in markov random fields", "author": ["Z. Kou", "W. Cohen"], "venue": "in: Proceedings of the Seventh SIAM International Conference on Data Mining", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "ADMET in silico modelling: towards prediction paradise", "author": ["H. van de Waterbeemd", "E. Gifford"], "venue": "Nat Rev Drug Discov", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2003}, {"title": "A survey of the predictive toxicology challenge 2000\u20132001", "author": ["C. Helma", "S. Kramer"], "venue": "Bioinformatics 19 (10) ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2003}, {"title": "Predictive Toxicology", "author": ["C. Helma"], "venue": "1st Edition, CRC Press", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2005}, {"title": "Classification of small molecules by two-and three-dimensional decomposition kernels", "author": ["A. Ceroni", "F. Costa", "P. Frasconi"], "venue": "Bioinformatics 23 (16) ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2007}, {"title": "Derivation and validation of toxicophores for muta- genicity prediction", "author": ["J. Kazius", "R. McGuire", "R. Bursi"], "venue": "J. Med. Chem 48 (1) ", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2005}, {"title": "W", "author": ["H. Blockeel", "S. Dzeroski", "B. Kompare", "S. Kramer", "B. Pfahringer"], "venue": "Laer, Experiments in Predicting Biodegradability., Applied Artificial Intelligence 18 (2) ", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2004}, {"title": "Discovering H-Bonding Rules in Crystals with Inductive Logic Programming", "author": ["H.Y. Ando", "L. Dehaspe", "W. Luyten", "E.V. Craenenbroeck", "H. Vandecasteele", "L.V. Meervelt"], "venue": "Mol. Pharm. 3 (6) ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2006}, {"title": "Molecular graph augmentation with rings and functional groups", "author": ["K. De Grave", "F. Costa"], "venue": "Journal of Chemical Information and Modeling 50 (9) ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2010}, {"title": "Stacked generalization", "author": ["D. Wolpert"], "venue": "Neural networks 5 (2) ", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1992}, {"title": "A perspective on inductive databases", "author": ["L. De Raedt"], "venue": "SIGKDD Explorations 4 (2) ", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2002}, {"title": "Constraint-based data mining", "author": ["J.-F. Boulicaut", "B. Jeudy"], "venue": "in: O. Maimon, L. Rokach (Eds.), Data Mining and Knowledge Discovery Handbook, 2nd Edition, Springer", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "The discipline of machine learning", "author": ["T. Mitchell"], "venue": "Tech. Rep. CMU-ML- 06- 108, Carnegie-Mellon University ", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards programming languages for machine learning and data mining (extended abstract)", "author": ["L. De Raedt", "S. Nijssen"], "venue": "in: Marzena Kryszkiewicz and Henryk Rybinski and Andrzej Skowron and Zbigniew W. Ras (Ed.), Foundations of Intelligent Systems - 19th International Symposium, IS- MIS 2011, Warsaw, Poland, June 28-30, 2011. Proceedings, Vol. 6804 of Lecture Notes in Computer Science, Springer", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning based java for rapid development of nlp systems", "author": ["N. Rizzolo", "D. Roth"], "venue": "in: Nicoletta Calzolari and Khalid Choukri and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis and Mike Rosner and Daniel Tapias (Ed.), Proceedings of the International Conference on Language Resources and Evaluation, LREC 2010, 17-23 May 2010, Valletta, Malta", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient weight learning for markov logic networks", "author": ["D. Lowd", "P. Domingos"], "venue": "in: Knowledge Discovery in Databases: PKDD 2007, Springer", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2007}, {"title": "A simple relational classifier", "author": ["S.A. Macskassy", "F. Provost"], "venue": "in: Proc. of the 2nd Workshop on Multi-Relational Data Mining", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2003}, {"title": "Inverse entailment and Progol", "author": ["S. Muggleton"], "venue": "New Generation Computing 13 (3\u20134) ", "citeRegEx": "65", "shortCiteRegEx": null, "year": 1995}, {"title": "Top-down induction of first order logical decision trees", "author": ["H. Blockeel", "L. De Raedt"], "venue": "Artificial Intelligence 101 (1-2) ", "citeRegEx": "67", "shortCiteRegEx": null, "year": 1998}, {"title": "Probabilistic similarity logic", "author": ["M. Br\u00f6cheler", "L. Mihalkova", "L. Getoor"], "venue": "in: Proc. of the 26th Conference on Uncertainty in Artificial Intelligence ", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian logic programming: theory and tool", "author": ["K. Kersting", "L. De Raedt"], "venue": "in: L. Getoor, B. Taskar (Eds.), An Introduction to Statistical Relational Learning, MIT Press", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2007}, {"title": "ProbLog: A probabilistic Prolog and its application in link discovery", "author": ["L. De Raedt", "A. Kimmig", "H. Toivonen"], "venue": "in: Proceedings of the 20th international joint conference on artificial intelligence", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2007}, {"title": "Lifted probabilistic inference", "author": ["K. Kersting"], "venue": "in: Proceedings of the 20th European Conference on Artificial Intelligence, Vol. 242 of Frontiers in Artificial Intelligence and Applications, IOS Press", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2012}, {"title": "Counting belief propagation", "author": ["K. Kersting", "B. Ahmadi", "S. Natarajan"], "venue": "in: Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI \u201909, AUAI Press, Arlington, Virginia, United States", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey of kernels for structured data", "author": ["T. G\u00e4rtner"], "venue": "SIGKDD Explorations", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 2003}, {"title": "Learning from interpretations: a rooted kernel for ordered hypergraphs", "author": ["G. Wachman", "R. Khardon"], "venue": "in: Proceedings of the 24th international conference on Machine learning", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2007}, {"title": "Propositionalization approaches to relational data mining", "author": ["S. Kramer", "N. Lavra\u010d", "P. Flach"], "venue": "in: S. D\u017eeroski, N. Lavra\u010d (Eds.), Relational Data Mining, Springer", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning logical definitions from relations", "author": ["J.R. Quinlan"], "venue": "Machine Learning 5 ", "citeRegEx": "76", "shortCiteRegEx": null, "year": 1990}, {"title": "Transforming graph data for statistical relational learning", "author": ["R.A. Rossi", "L.K. McDowell", "D.W. Aha", "J. Neville"], "venue": "Journal of Artificial Intelligence Research 45 (1) ", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2012}, {"title": "Relational retrieval using a combination of pathconstrained random walks", "author": ["N. Lao", "W.W. Cohen"], "venue": "Machine learning 81 (1) ", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2010}, {"title": "Mining graph data", "author": ["D.J. Cook", "L.B. Holder"], "venue": "Wiley-Interscience", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2006}, {"title": "Mining heterogeneous information networks: principles and methodologies", "author": ["Y. Sun", "J. Han"], "venue": "Synthesis Lectures on Data Mining and Knowledge Discovery 3 (2) ", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning and inference with constraints", "author": ["M. Chang", "L. Ratinov", "N. Rizzolo", "D. Roth"], "venue": "in: Proceedings of AAAI", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2008}, {"title": "FACTORIE: Probabilistic programming via imperatively defined factor graphs", "author": ["A. McCallum", "K. Schultz", "S. Singh"], "venue": "in: Neural Information Processing Systems (NIPS)", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2009}, {"title": "A statistical relational learning approach to identifying evidence based medicine categories", "author": ["M. Verbeke", "V. Van Asch", "R. Morante", "P. Frasconi", "W. Daelemans", "L. De Raedt"], "venue": "in: Proc. of EMNLP-CoNLL", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2012}, {"title": "Collective stability in structured prediction: Generalization from one example", "author": ["B. London", "B. Huang", "B. Taskar", "L. Getoor"], "venue": "in: S. Dasgupta, D. Mcallester (Eds.), Proceedings of the 30th International Conference on Machine Learning (ICML-13), JMLR Workshop and Conference Proceedings", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2013}, {"title": "On the implementation of the probabilistic logic programming language problog", "author": ["A. Kimmig", "B. Demoen", "L. De Raedt", "V.S. Costa", "R. Rocha"], "venue": "Theory and Practice of Logic Programming 11 (2-3) ", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernel-based logical and relational learning with klog for hedge cue detection", "author": ["M. Verbeke", "P. Frasconi", "V. Van Asch", "R. Morante", "W. Daelemans", "L. De Raedt"], "venue": "in: H. Muggleton, S. amd Watanabe, J. Tamaddoni- Nezhad A., Chen (Eds.), Inductive Logic Programming: Revised Selected Papers from the 21st International Conference ", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2011}, {"title": "Spatial relation extraction using relational learning", "author": ["P. Kordjamshidi", "P. Frasconi", "M. Van Otterlo", "M. Moens", "L. De Raedt"], "venue": "in: H. Muggleton, S. amd Watanabe, J. Tamaddoni-Nezhad A., Chen (Eds.), Inductive Logic Programming: Revised Selected Papers from the 21st International Conference ", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2011}, {"title": "A relational kernel-based framework for hierarchical image understanding", "author": ["L. Antanas", "P. Frasconi", "F. Costa", "T. Tuytelaars", "L. De Raedt"], "venue": "in: G. L. Gimel\u2019farb, E. R. Hancock, A. I. Imiya, A. Kuijper, M. Kudo, S. Shinichiro Omachi, T. Windeatt, K. Yamada (Eds.), Lecture Notes in Computer Science\u201e Springer", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2012}, {"title": "A relational kernel-based approach to scene classification", "author": ["L. Antanas", "M. Hoffmann", "P. Frasconi", "T. Tuytelaars", "L.D. Raedt"], "venue": "in: WACV", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2013}, {"title": "Employing logical languages for image understanding", "author": ["L. Antanas", "P. Frasconi", "T. Tuytelaars", "L. De Raedt"], "venue": "IEEE Workshop on Kernels and Distances for Computer Vision, International Conference on Computer Vision, Barcelona, Spain, 13 November 2011 ", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2011}, {"title": "Graph Theory and Its Applications", "author": ["J.L. Gross", "J. Yellen"], "venue": "Second Edition (Discrete Mathematics and Its Applications), Chapman & Hall/CRC", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2005}, {"title": "Convolution kernels on discrete structures", "author": ["D. Haussler"], "venue": "Tech. Rep. 99- 10, UCSC-CRL ", "citeRegEx": "92", "shortCiteRegEx": null, "year": 1999}, {"title": "Hash kernels for structured data", "author": ["Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A. Smola", "S. Vishwanathan"], "venue": "Journal of Machine Learning Research 10 ", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2009}, {"title": "Predictive quantitative structure-activity relationship models and their use for the efficient screening of molecules", "author": ["K. De Grave"], "venue": "Ph.D. thesis, Katholieke Universiteit Leuven ", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The field of statistical relational learning (SRL) is populated with a fairly large number of models and alternative representations, a state of affairs often referred to as the \u201cSRL alphabet soup\u201d [1, 2].", "startOffset": 198, "endOffset": 204}, {"referenceID": 1, "context": "The field of statistical relational learning (SRL) is populated with a fairly large number of models and alternative representations, a state of affairs often referred to as the \u201cSRL alphabet soup\u201d [1, 2].", "startOffset": 198, "endOffset": 204}, {"referenceID": 2, "context": "Even though there are many differences between these approaches, they typically extend a probabilistic representation (most often, a graphical model) with a logical or relational one [3, 4].", "startOffset": 183, "endOffset": 189}, {"referenceID": 3, "context": "Even though there are many differences between these approaches, they typically extend a probabilistic representation (most often, a graphical model) with a logical or relational one [3, 4].", "startOffset": 183, "endOffset": 189}, {"referenceID": 4, "context": "But this type of learning system has \u2014 with a few notable exceptions to relational prediction [5, 6] \u2014 not yet received a lot of attention in the SRL literature.", "startOffset": 94, "endOffset": 100}, {"referenceID": 5, "context": "But this type of learning system has \u2014 with a few notable exceptions to relational prediction [5, 6] \u2014 not yet received a lot of attention in the SRL literature.", "startOffset": 94, "endOffset": 100}, {"referenceID": 6, "context": "Furthermore, while it is by now commonly accepted that frameworks like Markov logic networks (MLNs) [7], probabilistic relational models (PRMs) [8], or probabilistic programming [3, 4] are general logical and relational languages that support a wide range of learning tasks, there exists today no such language for kernel-based learning.", "startOffset": 100, "endOffset": 103}, {"referenceID": 7, "context": "Furthermore, while it is by now commonly accepted that frameworks like Markov logic networks (MLNs) [7], probabilistic relational models (PRMs) [8], or probabilistic programming [3, 4] are general logical and relational languages that support a wide range of learning tasks, there exists today no such language for kernel-based learning.", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "Furthermore, while it is by now commonly accepted that frameworks like Markov logic networks (MLNs) [7], probabilistic relational models (PRMs) [8], or probabilistic programming [3, 4] are general logical and relational languages that support a wide range of learning tasks, there exists today no such language for kernel-based learning.", "startOffset": 178, "endOffset": 184}, {"referenceID": 3, "context": "Furthermore, while it is by now commonly accepted that frameworks like Markov logic networks (MLNs) [7], probabilistic relational models (PRMs) [8], or probabilistic programming [3, 4] are general logical and relational languages that support a wide range of learning tasks, there exists today no such language for kernel-based learning.", "startOffset": 178, "endOffset": 184}, {"referenceID": 8, "context": "kLog adopts, as many other logical and relational learning systems, the learning from interpretations framework [9].", "startOffset": 112, "endOffset": 115}, {"referenceID": 9, "context": "At this level, the description consists of an E/R-model describing the structure of the data and the data itself, which is similar to that of traditional SRL systems [10].", "startOffset": 166, "endOffset": 170}, {"referenceID": 10, "context": "In the current implementation of kLog that we describe in this paper, we employ the neighborhood subgraph pairwise distance kernel (NSPDK) [11] but the reader should keep in mind that other graph kernels can be incorporated.", "startOffset": 139, "endOffset": 143}, {"referenceID": 5, "context": "In Section 3, we formalize the assumed statistical setting for supervised learning from interpretations, provide some background on statistical modeling from a relational learning point of view, and position kLog more clearly in the context of related systems such as Markov logic, MN [6], etc.", "startOffset": 285, "endOffset": 288}, {"referenceID": 6, "context": "for demonstrating the capabilities of MLNs [7].", "startOffset": 43, "endOffset": 46}, {"referenceID": 8, "context": "introduce aggregated attributes [9].", "startOffset": 32, "endOffset": 35}, {"referenceID": 8, "context": "Our general approach to construct a statistical model is based on the learning from interpretations setting [9].", "startOffset": 108, "endOffset": 111}, {"referenceID": 11, "context": ", [12]), which may be developed without associating a probabilistic interpretation to F .", "startOffset": 2, "endOffset": 6}, {"referenceID": 12, "context": "The last two models are often cited as an example of generativediscriminative conjugate pairs because of the above reasons [13].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "When moving up to a slightly richer data type like sequences (perhaps the simplest case of relational data), the three models have well known extensions: naive Bayes extends to hidden Markov models (HMMs), logistic regression extends to conditional random fields (CRFs) [14], and SVM extends to structured output SVM for sequences [15, 12].", "startOffset": 270, "endOffset": 274}, {"referenceID": 14, "context": "When moving up to a slightly richer data type like sequences (perhaps the simplest case of relational data), the three models have well known extensions: naive Bayes extends to hidden Markov models (HMMs), logistic regression extends to conditional random fields (CRFs) [14], and SVM extends to structured output SVM for sequences [15, 12].", "startOffset": 331, "endOffset": 339}, {"referenceID": 11, "context": "When moving up to a slightly richer data type like sequences (perhaps the simplest case of relational data), the three models have well known extensions: naive Bayes extends to hidden Markov models (HMMs), logistic regression extends to conditional random fields (CRFs) [14], and SVM extends to structured output SVM for sequences [15, 12].", "startOffset": 331, "endOffset": 339}, {"referenceID": 13, "context": ", [14] for a detailed discussion).", "startOffset": 2, "endOffset": 6}, {"referenceID": 15, "context": "Among generative models, one natural extension of HMMs is stochastic context free grammars [16], which in turn can be extended to stochastic logic programs [17].", "startOffset": 91, "endOffset": 95}, {"referenceID": 16, "context": "Among generative models, one natural extension of HMMs is stochastic context free grammars [16], which in turn can be extended to stochastic logic programs [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 7, "context": "More expressive systems include probabilistic relational models (PRMs) [8] and Markov logic networks (MLNs) [7], when trained generatively.", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "More expressive systems include probabilistic relational models (PRMs) [8] and Markov logic networks (MLNs) [7], when trained generatively.", "startOffset": 108, "endOffset": 111}, {"referenceID": 11, "context": "Generalizations of SVM for relational structures akin to context free grammars have also been investigated [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "Among discriminative models, CRFs can be extended from linear chains to arbitrary relations [14], for example in the form of discriminative", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "Markov networks [18] and discriminative Markov logic networks [7].", "startOffset": 16, "endOffset": 20}, {"referenceID": 6, "context": "Markov networks [18] and discriminative Markov logic networks [7].", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "The use of SVM-like loss functions has also been explored in max-margin Markov networks (MN) [6].", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "The ability to specify intensional predicates through clauses (see an example in Listing 3) is most useful for introducing background knowledge in the learning process and common practice in inductive logic programming [9].", "startOffset": 219, "endOffset": 222}, {"referenceID": 18, "context": "The partition is inferred by analyzing the dependency graphs of Prolog predicates defining intensional relations, using an algorithm reminiscent of the call graph computation in ViPReSS [19].", "startOffset": 186, "endOffset": 190}, {"referenceID": 19, "context": ", [20]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 20, "context": "Implementation kLog is currently embedded in Yap Prolog [21] and consists of three main components: (1) a domain-specific interpreter, (2) a database loader, and (3) a library of predicates that are used to specify the learning task, to declare the graph kernel and the learning model, and to perform training, prediction, and performance evaluation.", "startOffset": 56, "endOffset": 60}, {"referenceID": 21, "context": "kLog incorporates LibSVM [22] and Stochastic gradient descent [23] and can interface with arbitrary (external) SVM solvers by coding appropriate data conversion wrappers.", "startOffset": 25, "endOffset": 29}, {"referenceID": 22, "context": "kLog incorporates LibSVM [22] and Stochastic gradient descent [23] and can interface with arbitrary (external) SVM solvers by coding appropriate data conversion wrappers.", "startOffset": 62, "endOffset": 66}, {"referenceID": 23, "context": "For the sake of concreteness, let us consider the problem of small molecule classification as pioneered in the relational learning setting in [24].", "startOffset": 142, "endOffset": 146}, {"referenceID": 24, "context": "to predict a real-valued property associated with a molecule, such as its biological activity or its octanol/water partition coefficient (logP) [25].", "startOffset": 144, "endOffset": 148}, {"referenceID": 25, "context": "Multitask learning can be handled trivially by learning independent predictors; alternatively, more sophisticated algorithms that take into account correlations amongst tasks (such as [26]) could be used.", "startOffset": 184, "endOffset": 188}, {"referenceID": 26, "context": "We illustrate this case using the classic WebKB domain [27].", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "an instance of a supervised structured output problem [28], that in this case might also be referred to as collective classification [18].", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "an instance of a supervised structured output problem [28], that in this case might also be referred to as collective classification [18].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": ", [29]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 29, "context": "Following the setup in [30], the problem is to predict \u201cblockbuster\u201d movies, i.", "startOffset": 23, "endOffset": 27}, {"referenceID": 30, "context": "When predicting the class of future movies, data about past movies\u2019 receipts can be used to construct features (indeed, the count of blockbuster movies produced by the same studio is one of the most informative features [31]).", "startOffset": 220, "endOffset": 224}, {"referenceID": 31, "context": "Assuming data for just a single organism is available, there is one entity set (protein) and a binary relation interact expressing protein-protein interaction [32, 33].", "startOffset": 159, "endOffset": 167}, {"referenceID": 32, "context": "Assuming data for just a single organism is available, there is one entity set (protein) and a binary relation interact expressing protein-protein interaction [32, 33].", "startOffset": 159, "endOffset": 167}, {"referenceID": 8, "context": "The use of an intermediate graphicalized representation is novel in the context of propositionalization, a well-known technique in logical and relational learning [9] that transforms graph-based or relational data directly into an attribute-value learning format, or possibly into a multi-instance learning one8.", "startOffset": 163, "endOffset": 166}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "8In multi-instance learning [34], the examples are sets of attribute-value tuples or sets of feature vectors.", "startOffset": 28, "endOffset": 32}, {"referenceID": 34, "context": "There are several other examples in the literature where a graph template is expanded into a ground graph, including the plate notation in graphical models [35], encoding networks in neural networks for learning data structures [36], and the construction of Markov networks in Markov logic [7].", "startOffset": 156, "endOffset": 160}, {"referenceID": 35, "context": "There are several other examples in the literature where a graph template is expanded into a ground graph, including the plate notation in graphical models [35], encoding networks in neural networks for learning data structures [36], and the construction of Markov networks in Markov logic [7].", "startOffset": 228, "endOffset": 232}, {"referenceID": 6, "context": "There are several other examples in the literature where a graph template is expanded into a ground graph, including the plate notation in graphical models [35], encoding networks in neural networks for learning data structures [36], and the construction of Markov networks in Markov logic [7].", "startOffset": 290, "endOffset": 293}, {"referenceID": 36, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 37, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 38, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 39, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 40, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 10, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 41, "context": "[37, 38, 39, 40, 41, 11, 42] and references therein) can be directly applied to the result of the graphicalization procedure since there is exactly one graph for each interpretation.", "startOffset": 0, "endOffset": 28}, {"referenceID": 10, "context": "In the current implementation, we use an extension of NSPDK [11].", "startOffset": 60, "endOffset": 64}, {"referenceID": 42, "context": "Note that 1A\u223c=A\u2032 is a valid kernel between graphs under the feature map \u03c6cl that transforms A into \u03c6cl(A), a sequence of all zeros except the i-th element equal to 1 in correspondence to the identifier for the canonical representation of A [43, 44].", "startOffset": 240, "endOffset": 248}, {"referenceID": 43, "context": "Note that 1A\u223c=A\u2032 is a valid kernel between graphs under the feature map \u03c6cl that transforms A into \u03c6cl(A), a sequence of all zeros except the i-th element equal to 1 in correspondence to the identifier for the canonical representation of A [43, 44].", "startOffset": 240, "endOffset": 248}, {"referenceID": 42, "context": "Algorithms that are in the worst case exponential but that are fast in practice do exist [43, 44].", "startOffset": 89, "endOffset": 97}, {"referenceID": 43, "context": "Algorithms that are in the worst case exponential but that are fast in practice do exist [43, 44].", "startOffset": 89, "endOffset": 97}, {"referenceID": 44, "context": "For special graph classes, such as bounded degree graphs [45], there exist polynomial time algorithms.", "startOffset": 57, "endOffset": 61}, {"referenceID": 45, "context": ", cases with very high vertex degree are possible as in general an entity atom may play a role in an arbitrary number of relationship atoms), we prefer an approximate solution with efficiency guarantees based on topological distances similar in spirit to [46].", "startOffset": 255, "endOffset": 259}, {"referenceID": 40, "context": "Although there exist several graph kernels that are based on this type of match, they generally suffer from very high computational costs [41].", "startOffset": 138, "endOffset": 142}, {"referenceID": 46, "context": "To ensure efficiency, we use an idea introduced in the Weighted Decomposition Kernel [47]: given a subgraph, we consider only the multinomial distribution (i.", "startOffset": 85, "endOffset": 89}, {"referenceID": 47, "context": "For example, Prolog predicates in intensional signatures can effectively be used as expressive relational templates for stacked graphical models [48] where input features for one instance are computed from predictions on other related instances.", "startOffset": 145, "endOffset": 149}, {"referenceID": 48, "context": "Predicting the biological activity of small molecules is a major task in chemoinformatics and can help drug development [49] and toxicology [50, 51].", "startOffset": 120, "endOffset": 124}, {"referenceID": 49, "context": "Predicting the biological activity of small molecules is a major task in chemoinformatics and can help drug development [49] and toxicology [50, 51].", "startOffset": 140, "endOffset": 148}, {"referenceID": 50, "context": "Predicting the biological activity of small molecules is a major task in chemoinformatics and can help drug development [49] and toxicology [50, 51].", "startOffset": 140, "endOffset": 148}, {"referenceID": 36, "context": ", [37, 38, 39, 41, 52]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 37, "context": ", [37, 38, 39, 41, 52]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 38, "context": ", [37, 38, 39, 41, 52]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 40, "context": ", [37, 38, 39, 41, 52]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 51, "context": ", [37, 38, 39, 41, 52]).", "startOffset": 2, "endOffset": 22}, {"referenceID": 52, "context": "The Bursi data set [53] consists of 4,337 molecular structures with associated mutagenicity labels (2,401 mutagens and 1,936 nonmutagens) obtained from a short-term in vitro assay that detects genetic damage.", "startOffset": 19, "endOffset": 23}, {"referenceID": 53, "context": "The Biodegradability data set [54] contains 328 compounds and the regression task is to predict their half-life for aerobic aqueous biodegradation starting from molecular structure and global molecular measurements.", "startOffset": 30, "endOffset": 34}, {"referenceID": 54, "context": "Relevant predicates in the extensional database are a/2, b/3 (atoms and bonds, respectively, extracted from the chemical structure), sub/3 (functional groups, computed by DMax Chemistry Assistant [55, 56]), fused/3, connected/4 (direct connection between two functional groups), linked/4 (connection between functional groups via an aliphatic chain).", "startOffset": 196, "endOffset": 204}, {"referenceID": 55, "context": "Relevant predicates in the extensional database are a/2, b/3 (atoms and bonds, respectively, extracted from the chemical structure), sub/3 (functional groups, computed by DMax Chemistry Assistant [55, 56]), fused/3, connected/4 (direct connection between two functional groups), linked/4 (connection between functional groups via an aliphatic chain).", "startOffset": 196, "endOffset": 204}, {"referenceID": 10, "context": "As shown in Table 3, results are relatively stable with respect to the choice of kernel hyperparameter (maximum radius and distance) and SVM regularization and essentially match the best results reported in [11] (AUROC 0.", "startOffset": 207, "endOffset": 211}, {"referenceID": 10, "context": "These results are not surprising since the graphs generated by kLog are very similar in this case to the expanded molecular graphs used in [11].", "startOffset": 139, "endOffset": 143}, {"referenceID": 55, "context": "Augmenting the language with the functional groups from [56] unexpectedly gave worse results in Tilde compared to a plain atom-bond language.", "startOffset": 56, "endOffset": 60}, {"referenceID": 53, "context": "We estimated prediction performance by repeating five times a ten-fold cross validation procedure as described in [54] (using exactly the same folds in each trial).", "startOffset": 114, "endOffset": 118}, {"referenceID": 4, "context": "04 (kFOIL was shown to outperform Tilde and S-CART in [5]).", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "To assess kLog behavior we evaluated prediction accuracy according to the leave-one-research-group-out setup of [7], using the domain description of Listings 2 and 3, together with a NSPDK kernel with distance 2, radius 2, and soft match.", "startOffset": 112, "endOffset": 115}, {"referenceID": 6, "context": "Comparative results with respect to Markov logic are reported in Figure 6 (MLN results published in [7]).", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "Professor) are unknown, as in [7]).", "startOffset": 30, "endOffset": 33}, {"referenceID": 56, "context": "Our procedure is reminiscent of stacked generalization [57].", "startOffset": 55, "endOffset": 59}, {"referenceID": 6, "context": "Results are reported in Figure 7 (MLN results published in [7]).", "startOffset": 59, "endOffset": 62}, {"referenceID": 57, "context": "Thus kLog satisfies what has been called the closure principle in the context of inductive databases [58, 59]; it is also this principle together with the embedding of kLog inside a programming language (Prolog) that turns kLog into a true programming language for machine learning [60, 61, 62].", "startOffset": 101, "endOffset": 109}, {"referenceID": 58, "context": "Thus kLog satisfies what has been called the closure principle in the context of inductive databases [58, 59]; it is also this principle together with the embedding of kLog inside a programming language (Prolog) that turns kLog into a true programming language for machine learning [60, 61, 62].", "startOffset": 101, "endOffset": 109}, {"referenceID": 59, "context": "Thus kLog satisfies what has been called the closure principle in the context of inductive databases [58, 59]; it is also this principle together with the embedding of kLog inside a programming language (Prolog) that turns kLog into a true programming language for machine learning [60, 61, 62].", "startOffset": 282, "endOffset": 294}, {"referenceID": 60, "context": "Thus kLog satisfies what has been called the closure principle in the context of inductive databases [58, 59]; it is also this principle together with the embedding of kLog inside a programming language (Prolog) that turns kLog into a true programming language for machine learning [60, 61, 62].", "startOffset": 282, "endOffset": 294}, {"referenceID": 61, "context": "Thus kLog satisfies what has been called the closure principle in the context of inductive databases [58, 59]; it is also this principle together with the embedding of kLog inside a programming language (Prolog) that turns kLog into a true programming language for machine learning [60, 61, 62].", "startOffset": 282, "endOffset": 294}, {"referenceID": 59, "context": "According to Mitchell [60], the development of such languages is a long outstanding research question.", "startOffset": 22, "endOffset": 26}, {"referenceID": 26, "context": "Entity classification The WebKB data set [27] has been widely used to evaluate relational methods for text categorization.", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "After graphicalization, vertices representing webpages have large degree (at least the number of words), making the standard NSPDK of [11] totally inadequate: even by setting the maximum distance d\u2217 = 1 and the maximum radius r\u2217 = 2, the hard match would essentially create a distinct feature for every page.", "startOffset": 134, "endOffset": 138}, {"referenceID": 0, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 1, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 2, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 3, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 4, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 5, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 6, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 7, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 8, "context": "The additional signature cs_in_url embodies common sense background knowledge that many course web pages contains the string \u201ccs\u201d followed by some digits and is intensionally defined using a Prolog predicate that holds true when the regular expression :cs(e*)[0-9]+: matches the page URL.", "startOffset": 259, "endOffset": 264}, {"referenceID": 62, "context": "For learning we used the preconditioned scaled conjugate gradient approach described in [63] and we", "startOffset": 88, "endOffset": 92}, {"referenceID": 62, "context": "The best results, reported in Table 6, used the trick of averaging MLN weights across all iterations as in [63].", "startOffset": 107, "endOffset": 111}, {"referenceID": 29, "context": "We focus on predicting, for each movie, whether its first weekend box-office receipts are over US$2 million, a learning task previously defined in [30, 64].", "startOffset": 147, "endOffset": 155}, {"referenceID": 63, "context": "We focus on predicting, for each movie, whether its first weekend box-office receipts are over US$2 million, a learning task previously defined in [30, 64].", "startOffset": 147, "endOffset": 155}, {"referenceID": 64, "context": "First, the underlying representation of the data that kLog employs at the first level is very close to that of standard inductive logic programming systems such as Progol [65], Aleph [66], and Tilde [67] in the sense that the input is essentially (a variation of) a Prolog program for specifying the data and the background knowledge.", "startOffset": 171, "endOffset": 175}, {"referenceID": 65, "context": "First, the underlying representation of the data that kLog employs at the first level is very close to that of standard inductive logic programming systems such as Progol [65], Aleph [66], and Tilde [67] in the sense that the input is essentially (a variation of) a Prolog program for specifying the data and the background knowledge.", "startOffset": 199, "endOffset": 203}, {"referenceID": 9, "context": "in [10].", "startOffset": 3, "endOffset": 7}, {"referenceID": 8, "context": "The signatures play a similar role as the notion of a declarative bias in inductive logic programming [9].", "startOffset": 102, "endOffset": 105}, {"referenceID": 64, "context": "This contrasts with some inductive logic programming systems such as Progol [65] and Aleph [66].", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "Second, kLog is related to many existing statistical relational learning systems such as Markov logic [7], probabilistic similarity logic [68], probabilistic relational models [8], Bayesian logic programs [69], and ProbLog [70] in that the representations of the inputs and outputs are essentially the same, that is, both in kLog and in statistical relational learning systems inputs are partial interpretations which are completed by predictions.", "startOffset": 102, "endOffset": 105}, {"referenceID": 66, "context": "Second, kLog is related to many existing statistical relational learning systems such as Markov logic [7], probabilistic similarity logic [68], probabilistic relational models [8], Bayesian logic programs [69], and ProbLog [70] in that the representations of the inputs and outputs are essentially the same, that is, both in kLog and in statistical relational learning systems inputs are partial interpretations which are completed by predictions.", "startOffset": 138, "endOffset": 142}, {"referenceID": 7, "context": "Second, kLog is related to many existing statistical relational learning systems such as Markov logic [7], probabilistic similarity logic [68], probabilistic relational models [8], Bayesian logic programs [69], and ProbLog [70] in that the representations of the inputs and outputs are essentially the same, that is, both in kLog and in statistical relational learning systems inputs are partial interpretations which are completed by predictions.", "startOffset": 176, "endOffset": 179}, {"referenceID": 67, "context": "Second, kLog is related to many existing statistical relational learning systems such as Markov logic [7], probabilistic similarity logic [68], probabilistic relational models [8], Bayesian logic programs [69], and ProbLog [70] in that the representations of the inputs and outputs are essentially the same, that is, both in kLog and in statistical relational learning systems inputs are partial interpretations which are completed by predictions.", "startOffset": 205, "endOffset": 209}, {"referenceID": 68, "context": "Second, kLog is related to many existing statistical relational learning systems such as Markov logic [7], probabilistic similarity logic [68], probabilistic relational models [8], Bayesian logic programs [69], and ProbLog [70] in that the representations of the inputs and outputs are essentially the same, that is, both in kLog and in statistical relational learning systems inputs are partial interpretations which are completed by predictions.", "startOffset": 223, "endOffset": 227}, {"referenceID": 6, "context": "For statistical relational learning methods such as Markov logic [7], probabilistic relational models [8], and Bayesian logic programs [69] the knowledge-based model construction process will result in a graphical model (Bayesian or Markov network) for each instance representing a class of probability distributions, while in kLog the process of graphicalization results in a graph representing an instance by unrolling the E/R-diagram.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "For statistical relational learning methods such as Markov logic [7], probabilistic relational models [8], and Bayesian logic programs [69] the knowledge-based model construction process will result in a graphical model (Bayesian or Markov network) for each instance representing a class of probability distributions, while in kLog the process of graphicalization results in a graph representing an instance by unrolling the E/R-diagram.", "startOffset": 102, "endOffset": 105}, {"referenceID": 67, "context": "For statistical relational learning methods such as Markov logic [7], probabilistic relational models [8], and Bayesian logic programs [69] the knowledge-based model construction process will result in a graphical model (Bayesian or Markov network) for each instance representing a class of probability distributions, while in kLog the process of graphicalization results in a graph representing an instance by unrolling the E/R-diagram.", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "A combination of structured-output learning [12] and iterative approaches (as incorporated in the EM algorithm) can form the basis for further work in", "startOffset": 44, "endOffset": 48}, {"referenceID": 69, "context": "Lifted inference has been the focus of a lot of attention in statistical relational learning; see [71] for an overview.", "startOffset": 98, "endOffset": 102}, {"referenceID": 70, "context": ", [72].", "startOffset": 2, "endOffset": 6}, {"referenceID": 71, "context": "kLog builds also upon the many results on learning with graph kernels, see [73] for an overview.", "startOffset": 75, "endOffset": 79}, {"referenceID": 72, "context": "In this regard, kLog is close in spirit to the work of [74], who define a kernel on hypergraphs, where hypergraphs are used to represent relational interpretations.", "startOffset": 55, "endOffset": 59}, {"referenceID": 73, "context": "The graphicalization approach introduced in kLog is closely related to the notion of propositionalization, a commonly applied technique in logical and relational learning [75, 9] to generate features from a relational representation.", "startOffset": 171, "endOffset": 178}, {"referenceID": 8, "context": "The graphicalization approach introduced in kLog is closely related to the notion of propositionalization, a commonly applied technique in logical and relational learning [75, 9] to generate features from a relational representation.", "startOffset": 171, "endOffset": 178}, {"referenceID": 4, "context": "kFOIL [5] is one such propositionalization technique that has been tightly integrated with a kernel-based method.", "startOffset": 6, "endOffset": 9}, {"referenceID": 74, "context": "It greedily derives a (small) set of features in a way that resembles the rule-learning algorithm of FOIL [76].", "startOffset": 106, "endOffset": 110}, {"referenceID": 75, "context": ", [77, 78, 79, 80].", "startOffset": 2, "endOffset": 18}, {"referenceID": 76, "context": ", [77, 78, 79, 80].", "startOffset": 2, "endOffset": 18}, {"referenceID": 77, "context": ", [77, 78, 79, 80].", "startOffset": 2, "endOffset": 18}, {"referenceID": 78, "context": ", [77, 78, 79, 80].", "startOffset": 2, "endOffset": 18}, {"referenceID": 61, "context": "Learning based Java [62] was designed to specifically address applications in natural language processing.", "startOffset": 20, "endOffset": 24}, {"referenceID": 79, "context": "It builds on the concept of data-driven compilation to perform feature extraction and nicely exploits the constrained conditional model framework [81] for structured output learning.", "startOffset": 146, "endOffset": 150}, {"referenceID": 80, "context": "FACTORIE [82] allows users to concisely define features used in a factor graph and, consequently, arbitrarily connected conditional random fields.", "startOffset": 9, "endOffset": 13}, {"referenceID": 81, "context": "For example, if dependencies have a regular sequential structure, dynamic programming can be used for this step, exactly as in conditional random fields (indeed, collective classification has been succesfully exploited within kLog in an application to natural language test segmentation [83]).", "startOffset": 287, "endOffset": 291}, {"referenceID": 82, "context": "Better understanding of generalization for structured prediction models has begun to emerge (see [84] and references therein) and a theoretical analysis of learning within the present kLog setting is another potential direction for future research.", "startOffset": 97, "endOffset": 101}, {"referenceID": 39, "context": "The implementation of a library of kernels suitable for different types of graphs, as well as the integration of other existing graph kernels (such as [40, 41, 42]) in the kLog framework, is therefore an important direction for future development.", "startOffset": 151, "endOffset": 163}, {"referenceID": 40, "context": "The implementation of a library of kernels suitable for different types of graphs, as well as the integration of other existing graph kernels (such as [40, 41, 42]) in the kLog framework, is therefore an important direction for future development.", "startOffset": 151, "endOffset": 163}, {"referenceID": 41, "context": "The implementation of a library of kernels suitable for different types of graphs, as well as the integration of other existing graph kernels (such as [40, 41, 42]) in the kLog framework, is therefore an important direction for future development.", "startOffset": 151, "endOffset": 163}, {"referenceID": 6, "context": "Many of these are similar to those employed in statistical relational learning systems such as Alchemy [7] and ProbLog [85].", "startOffset": 103, "endOffset": 106}, {"referenceID": 83, "context": "Many of these are similar to those employed in statistical relational learning systems such as Alchemy [7] and ProbLog [85].", "startOffset": 119, "endOffset": 123}, {"referenceID": 84, "context": "We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90].", "startOffset": 79, "endOffset": 91}, {"referenceID": 81, "context": "We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90].", "startOffset": 79, "endOffset": 91}, {"referenceID": 85, "context": "We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90].", "startOffset": 79, "endOffset": 91}, {"referenceID": 86, "context": "We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90].", "startOffset": 115, "endOffset": 127}, {"referenceID": 87, "context": "We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90].", "startOffset": 115, "endOffset": 127}, {"referenceID": 88, "context": "We are currently exploring applications of kLog in natural language processing [86, 83, 87] and in computer vision [88, 89, 90].", "startOffset": 115, "endOffset": 127}, {"referenceID": 89, "context": "We closely follow the notation in [91].", "startOffset": 34, "endOffset": 38}, {"referenceID": 90, "context": "We follow the notation in [92].", "startOffset": 26, "endOffset": 30}, {"referenceID": 90, "context": "In [92] it is demonstrated that, if there exist a kernel Kd over Xd \u00d7 Xd for each d = 1, .", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "The algorithm was first described in [11] but we present it again here for self-sufficiency.", "startOffset": 37, "endOffset": 41}, {"referenceID": 37, "context": ", for creating compact representations of molecular structures [38], and has been advocated as a tool for compressing very high-dimensional feature spaces [93].", "startOffset": 63, "endOffset": 67}, {"referenceID": 91, "context": ", for creating compact representations of molecular structures [38], and has been advocated as a tool for compressing very high-dimensional feature spaces [93].", "startOffset": 155, "endOffset": 159}], "year": 2014, "abstractText": "We introduce kLog, a novel approach to statistical relational learning. Unlike standard approaches, kLog does not represent a probability distribution directly. It is rather a language to perform kernel-based learning on expressive logical and relational representations. kLog allows users to specify learning problems declaratively. It builds on simple but powerful concepts: learning from interpretations, entity/relationship data modeling, logic programming, and deductive databases. Access by the kernel to the rich representation is mediated by a technique we call graphicalization: the relational representation is first transformed into a graph \u2014 in particular, a grounded entity/relationship diagram. Subsequently, a choice of graph kernel defines the feature space. kLog supports mixed numerical and symbolic data, as well as background knowledge in the form of Prolog or Datalog programs as in inductive logic programming systems. The kLog framework can be applied to tackle the same range of tasks that has made statistical relational learning so popular, including classification, regression, multitask learning, and collective classification. We also report about empirical comparisons, showing PF was a visiting professor at KU Leuven and FC a postdoctoral fellow at KU Leuven while this work was initiated \u2217Corresponding author Email addresses: p-f@dsi.unifi.it (Paolo Frasconi), costa@informatik.uni-freiburg.de (Fabrizio Costa), Luc.DeRaedt@cs.kuleuven.be (Luc De Raedt), Kurt.DeGrave@cs.kuleuven.be (Kurt De Grave) Preprint submitted to Artificial Intelligence July 29, 2014 ar X iv :1 20 5. 39 81 v5 [ cs .A I] 2 8 Ju l 2 01 4 that kLog can be either more accurate, or much faster at the same level of accuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at http://klog.dinfo.unifi.it along with tutorials.", "creator": "LaTeX with hyperref package"}}}