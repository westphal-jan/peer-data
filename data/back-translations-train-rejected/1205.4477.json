{"id": "1205.4477", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2012", "title": "Streaming Algorithms for Pattern Discovery over Dynamically Changing Event Sequences", "abstract": "Discovering frequent episodes over event sequences is an important data mining task. In many applications, events constituting the data sequence arrive as a stream, at furious rates, and recent trends (or frequent episodes) can change and drift due to the dynamical nature of the underlying event generation process. The ability to detect and track such the changing sets of frequent episodes can be valuable in many application scenarios. Current methods for frequent episode discovery are typically multipass algorithms, making them unsuitable in the streaming context. In this paper, we propose a new streaming algorithm for discovering frequent episodes over a window of recent events in the stream. Our algorithm processes events as they arrive, one batch at a time, while discovering the top frequent episodes over a window consisting of several batches in the immediate past. We derive approximation guarantees for our algorithm under the condition that frequent episodes are approximately well-separated from infrequent ones in every batch of the window. We present extensive experimental evaluations of our algorithm on both real and synthetic data. We also present comparisons with baselines and adaptations of streaming algorithms from itemset mining literature.", "histories": [["v1", "Mon, 21 May 2012 01:46:57 GMT  (872kb,D)", "http://arxiv.org/abs/1205.4477v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DB", "authors": ["debprakash patnaik", "naren ramakrishnan", "srivatsan laxman", "badrish chandramouli"], "accepted": false, "id": "1205.4477"}, "pdf": {"name": "1205.4477.pdf", "metadata": {"source": "CRF", "title": "Streaming Algorithms for Pattern Discovery over Dynamically Changing Event Sequences", "authors": ["Debprakash Patnaik", "Naren Ramakrishnan", "Srivatsan Laxman", "Badrish Chandramouli"], "emails": ["patnaik@vt.edu", "naren@vt.edu", "slaxman@microsoft.com", "badrishc@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "The problem of discovering interesting patterns from large datasets has been well researched in the form of pattern clauses such as item sets, sequential patterns and episodes with time constraints, but most of these techniques deal with static datasets that perform multiple passports. However, in many areas such as telecommunications and computer security, it is becoming increasingly difficult to store and process data at speeds comparable to their generation rate. A few minutes of call log data on a telecommunications network can easily run into millions of datasets. Such data is referred to as data streams [12]. A data stream is an unlimited sequence in which new data points or events arrive continuously and often at very high rates. Many traditional data mining algorithms become useless in this context because one cannot hope to store and then process all the data."}, {"heading": "2 Preliminaries", "text": "In the context of frequent episodes [9], an event sequence is referred to as < (e1, \u03c41),.., (en, \u03c4n) >, where (ei, \u03c4i) represents the ith event; ei is drawn from a finite alphabet E of the symbols (referred to as event types) and \u03c4i denotes the timestamp of the ith event, where V\u03b1 = {v1,..., v '} is a collection of \"nodes, < \u03b1 is a partial sequence of V\u03b1 and g\u03b1: V\u03b1 \u2192 E is a map that assigns an event-like g\u03b1 (v) episode to find episodes, where V\u03b1 = {v1,."}, {"heading": "3 Problem Statement", "text": "The available data (referred to as event stream) are a potentially infinite sequence of events: D = < (e1, \u03c41), (e2, \u03c42),., (ei, \u03c4i),.., (en, \u03c4n),.. > (1) Our goal is to find all episodes that were common in the recent past, and for this purpose we consider a sliding window for the window of the user's interest. In this model, the user wants to determine episodes that are frequent in a fixed-size window and end at the current time. As new events arrive in the stream, the window of the user's interest shifts and the task of data mining is to report the frequent episodes in the new window of interest.Typically, the window of interest is very large and cannot be stored and processed in memory."}, {"heading": "4 Method", "text": "In general, the top k episodes over a window are very different from the top k episodes in the individual episodes that form the window. This is illustrated by an example in the figure as the episodes in each episode B1,.., B4. The episodes in each episode with the corresponding batch frequencies in the figure. 2. The corresponding window frequencies (sum of individual episodes) are listed in Table 1. The top 2 episodes in B1 are (PQRS) and (WXYZ). Likewise (EFGH) and (IJKL) are the top 2 episodes in B2, and so on. (ABCD) 2We assume that the number of events in each batch above and that we have enough memory to store and process all events that occur in a batch."}, {"heading": "4.2 Incremental Algorithm", "text": "In this section, we present an efficient algorithm for incremental degradation patterns with frequency. (fsk \u2212) These frequent degradation patterns are frequent degradation patterns. From our formalism, the value of \u03b8 is determined by the type of degradation patterns we want to degrade. (v, k) Therefore, the frequent degradation patterns of the frequent degradation patterns are more common than (m \u2212 v). (f s \u2212 1) The goal of our degradation task is to report frequent degradation patterns of size. \"After processing the data in the batch Bs \u2212 1, we want all patterns with frequency greater than (f s \u2212 1). (n) This is achieved algorithmically by first setting a high frequency threshold and using the degradation method for patterns with the classical level wisely Apriori method. (2) If the number of size patterns is reduced below the support threshold and the support threshold is repeated until the degradation threshold is repeated, using the classical level and a high frequency threshold. (f)"}, {"heading": "4.3 Estimating \u2206 dynamically", "text": "In the streaming setting, the characteristics of the data may change over time. Therefore, a preset value of \u2206 cannot be provided in an intuitive way. Therefore, we estimate \u2206 from the frequency of episodes of \"size in successive windows. We calculate the differences in the frequency of episodes occurring in successive lots. In particular, we consider the value in the 75th percentile to be an estimate of \u0445. We avoid the maximum change because it tends to be noisy. Some patterns that exhibit large changes in frequency can distort the estimate and adversely affect the mining process."}, {"heading": "5 Results", "text": "In this section, we present results both on synthetic data and on data from real neuroscience experiments. We compare the performance of the proposed streaming episode mining algorithm on synthetic data to quantify the impact of various parameter selections and data characteristics on the quality of the top k episodes reported by each method. Finally, we show the quality of the results obtained on neuroscience data. To compare the quality of the results, we set up the following six variants of mining episodes: Alg 0: This is the naive brute force top k mining algorithm that loads an entire window of events at a time and lists the top k episodes by repeatedly lowering the frequency threshold for mining. When a new batch arrives, events from the oldest batch are withdrawn and the mining process repeats itself from scratch. This method serves as the basis for comparing all other algorithms in terms of accuracy and recovery of frequency."}, {"heading": "5.1 Synthetic Datasets", "text": "The data sets we used for experimental evaluation are in Table 2. The name of the data set is listed in column 1, the length of the data set (or the number of time periods in the data sequence) in column 2, the size of the alphabet (or the total number of event types) in column 3, the average rest rate in column 4, and the number of embedded patterns in column 5. In these data sets, the data length varies from - millions to - millions of events, the alphabet size varies from 1000 to 5000, the rest rate from 10.0 to 25.0, and the number of patterns embedded in the data from 25 to 50. Data generation model: The data generation model for synthetic data is based on the inhomogeneous Poisson process model for evaluating the algorithm for learning exactive dynamic networks [13]."}, {"heading": "5.2 Comparison of algorithms", "text": "In Fig. 5, we compare the five algorithms - Alg 1 to Alg 5 - that report the frequent episodes above the window by looking at one batch at a time with the base algorithm Alg 0 that stores and processes the entire window at each slider. Results are averaged across all 9 datasets shown in Table 2. We expect to marginalize the data characteristics and give a more general picture of each algorithm. Parameter settings for the experiments are shown in Table 3. Fig. 5 (a) records the precision of the output of each algorithm compared to that of Alg 0 (treated as down-to-earth truth). Likewise, Fig. 5 (b) shows the callback. Since the size of the output of each algorithm is roughly k, the corresponding precision and memory numbers are almost identical. Average runtimes are shown in Fig. 5 (c) and average memory requirements in MB."}, {"heading": "5.2.1 Performance over time", "text": "Next, we look at a dataset A2 with the number of event types = 1000, the average idle rate than 10 Hz, and the number of embedded patterns = 50. On this data, we show how the performance of the five algorithms changes over time. Window size is set to m = 10, and batch size = 105 sec. Fig. 6 shows the comparison of 50 contiguous batches. Fig. 6 (a) and b) show the way precision and retrieval evolve over time. Fig. 6 (c) and (d) show the matching memory usage and runtimes.The data generation model allows the episode frequencies to change slowly over time. In the datasets used in the comparison, we change the frequency of embedded episodes in two time intervals: Batch 15 to 20 and Batch 35 to 42. In Fig., the batch and batch frequency of the batch frequency we have (6) and (b) are shown by the specified batch line."}, {"heading": "5.2.2 Effect of Data Characteristics", "text": "In this section, we present results on synthetic data with different properties, namely number of event types (or alphabet size), noise level and number of patterns embedded in the data. In Fig. 7, we report on the effects of alphabet size on the quality of the results of the different algorithms. In the A1, A2 and A3 data sets, the alphabet size, i.e. the number of different event types, varies from 500 to 5000. We observe that for smaller alphabet sizes, the performance is better. Alg 1 and 2 consistently perform worse that the other algorithm is embedded for different alphabet sizes. In this experiment, we find that the quality of the results for the proposed algorithms is not very sensitive to the alphabet size. The precision and memory numbers fall by only 2-4%. This is significantly different from the pattern-mining setting in which the user provides a frequency threshold."}, {"heading": "5.2.3 Effect of Parameters", "text": "In this section, we will look at two important parameters of the algorithms, namely the batch size Tb and the number of batches that make up a window, m.In Fig. 10, the quality and performance indicators are plotted for three different batch sizes: 103, 104 and 105 (in sec). Batch size appears to have a significant impact on precision and recall. There is a 10% decrease in both precision and recall when batch size is reduced from 105 to 103 sec. However, note that a 100-fold decrease in batch size only changes the quality of the results by 10%. It is not hard to imagine that for smaller batch sizes, episode statistics can exhibit higher variability in different batches, resulting in lower precision and recall over the window. As the size of the batch grows, the top batch starts."}, {"heading": "5.3 Multi-neuronal Data", "text": "Multi-electrode arrays provide high-throughput recordings of spiking activity in neural tissue and are therefore rich sources of event data where events correlate with activation of specific neurons. We used data from dissociated cortical cultures collected over several days from Steve Potter's lab at Georgia Tech [15]. It is an extensive collection of recordings from a 64-electrode MEA setup. We show the result of frequent degradation phases in the data collected over several days from culture 6 [15]. We use a stack size of 150 seconds and all other parameters for degradation are the same as used for synthetic data.The diagrams in Fig. 12 show the performance of the various algorithms over time. Alg 1 and 2 provide very low precision values, implying that the top k in a stack is very different from the top k in the window."}, {"heading": "6 Related work", "text": "Most previous work in the K set is related to frequent mining and sequence patterns. [6, 5, 8, 3] Most previous work in the K set is related to frequent mining patterns. [6, 8, 3] Some interesting algorithms have also been proposed for processing mining processes in time series. [6] In this section, we discuss some of the existing methods for mining patterns, sequential patterns, and motifs. Karp et al proposed a one-time streaming algorithm series for frequent events occurring in a certain range of mining patterns. Algorithms that maintain a set of event types and their associated counts at any given time are initially empty. If an event is read from the input sequence when the event type exists in the set, then its number is increased. Otherwise, the event type is inserted into the K set with the number 1."}, {"heading": "7 Conclusions", "text": "We have uncovered an interesting aspect of temporal data mining, where the data owner wants results over a period of time in data that do not fit into memory or can be processed faster than the rate of data generation. We have proposed a new sliding window model that slides forward in lots. At any given time, only one amount of data is available for processing. We have investigated this problem and identified the theoretical guarantees that can be given and the assumptions necessary to support it. In many real-world applications, we find the need to characterize patterns not only by their frequency, but also by their tendency to persist over time. In neuroscience in particular, the network structure underlying a neuron ensemble is changing very slowly compared to the culture-wide periods in which the phenomenon bursts. Thus, separating the persistent patterns from the fragile patterns can give us more insight into the underlying network connectivity map."}], "references": [{"title": "Discovering injective episodes with general partial orders", "author": ["A. Achar", "S. Laxman", "R. Viswanathan", "P.S. Sastry"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Fast Algorithms for Mining Association Rules in Large Databases", "author": ["R. Agrawal", "R. Srikant"], "venue": "In Proceedings of the 20th International Conference on Very Large Databases", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Mining frequent itemsets in a stream", "author": ["T. Calders", "N. Dexters", "B. Goethals"], "venue": "Proceedings of the 2007 Seventh IEEE International Conference on Data Mining,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "A survey on algorithms for mining frequent itemsets over data streams", "author": ["J. Cheng", "Y. Ke", "W. Ng"], "venue": "Knowledge and Information Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "An algorithm for in-core frequent itemset mining on streaming data", "author": ["R. Jin", "G. Agrawal"], "venue": "Proceedings of the Fifth IEEE International Conference on Data Mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "A simple algorithm for finding frequent elements in streams and bags", "author": ["R.M. Karp", "S. Shenker", "C.H. Papadimitriou"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Discovering frequent episodes: Fast algorithms, Connections with HMMs and generalizations", "author": ["S. Laxman"], "venue": "PhD thesis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "Approximate frequency counts over data streams", "author": ["G.S. Manku", "R. Motwani"], "venue": "In Proceedings of the 28th international conference on Very Large Data Bases,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Discovery of frequent episodes in event sequences", "author": ["H. Mannila", "H. Toivonen", "A. Verkamo"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Stream sequential pattern mining with precise error bounds", "author": ["L. Mendes", "B. Ding", "J. Han"], "venue": "In Data Mining,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Online discovery and maintenance of time series motif", "author": ["A. Mueen", "E. Keogh"], "venue": "In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Data streams: Algorithms and applications", "author": ["S. Muthukrishnan"], "venue": "Foundations and Trends in Theoretical Computer Science,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Discovering excitatory relationships using dynamic bayesian networks", "author": ["D. Patnaik", "S. Laxman", "N. Ramakrishnan"], "venue": "Knowledge and Information Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Prefixspan: Mining sequential patterns efficiently by prefix-projected pattern growth", "author": ["J. Pei", "J. Han", "B. Mortazavi-Asl", "H. Pinto"], "venue": "In Proceedings of the 17th International Conference on Data Engineering,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}, {"title": "An extremely rich repertoire of bursting patterns during the development of cortical cultures", "author": ["D.A. Wagenaar", "J. Pine", "S.M. Potter"], "venue": "BMC Neuroscience,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Mining top-k frequent itemsets from data streams", "author": ["R.C.-W. Wong", "A.W.-C. Fu"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}], "referenceMentions": [{"referenceID": 11, "context": "Such data are referred to as data streams [12].", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "see [16]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 8, "context": "In the framework of frequent episodes [9], an event sequence is denoted as \u3008(e1, \u03c41), .", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Two occurrences of an episode are non-overlapped [7] if no event corresponding to one appears in-between the events corresponding to the other.", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "Given a frequency threshold, Apriori-style level-wise algorithms [9, 1] can be used to obtain the frequent episodes in the event sequence.", "startOffset": 65, "endOffset": 71}, {"referenceID": 0, "context": "Given a frequency threshold, Apriori-style level-wise algorithms [9, 1] can be used to obtain the frequent episodes in the event sequence.", "startOffset": 65, "endOffset": 71}, {"referenceID": 3, "context": "The current Streaming patterns literature has also considered other models, such as the landmark and time-fading models [4], but we do not consider them in this paper.", "startOffset": 120, "endOffset": 123}, {"referenceID": 1, "context": "Algorithmically this is achieved by first setting a high frequency threshold and mining for patterns using the classical level wise Apriori method [2].", "startOffset": 147, "endOffset": 150}, {"referenceID": 12, "context": "Data generation model: The data generation model for synthetic data is based on the inhomogeneous Poisson process model for evaluating the algorithm for learning excitatory dynamic networks [13].", "startOffset": 190, "endOffset": 194}, {"referenceID": 14, "context": "We used the data from dissociated cortical cultures gathered by Steve Potter\u2019s laboratory at Georgia Tech [15] over several days.", "startOffset": 106, "endOffset": 110}, {"referenceID": 14, "context": "We show the result of mining frequent episodes in the data collected over several days from Culture 6 [15].", "startOffset": 102, "endOffset": 106}, {"referenceID": 5, "context": "Most prior work in streaming pattern mining is related to frequent itemsets and sequential patterns [6, 5, 8, 3].", "startOffset": 100, "endOffset": 112}, {"referenceID": 4, "context": "Most prior work in streaming pattern mining is related to frequent itemsets and sequential patterns [6, 5, 8, 3].", "startOffset": 100, "endOffset": 112}, {"referenceID": 7, "context": "Most prior work in streaming pattern mining is related to frequent itemsets and sequential patterns [6, 5, 8, 3].", "startOffset": 100, "endOffset": 112}, {"referenceID": 2, "context": "Most prior work in streaming pattern mining is related to frequent itemsets and sequential patterns [6, 5, 8, 3].", "startOffset": 100, "endOffset": 112}, {"referenceID": 10, "context": "Some interesting algorithms have also been proposed for streaming motif mining in time-series data [11].", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "Karp et al proposed a one pass streaming algorithm for finding frequent events in an item sequence [6].", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "In [5], this approach was extended to mine frequent itemsets.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "Lossy counting constitutes another important class of streaming algorithms proposed by Manku and Motwani in 2002 [8].", "startOffset": 113, "endOffset": 116}, {"referenceID": 9, "context": "In [10], the pattern growth algorithm - PrefixSpan [14] for mining sequential patterns was extended to incorporate the idea of lossy counting.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In [10], the pattern growth algorithm - PrefixSpan [14] for mining sequential patterns was extended to incorporate the idea of lossy counting.", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "In [3], the authors propose a new frequency measure for itemsets over data streams.", "startOffset": 3, "endOffset": 6}, {"referenceID": 10, "context": "In [11] an online algorithm for mining time series motifs was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "In [13] we proposed an information theoretic principle for determining the frequency threshold that is ultimately used in learning a dynamic Bayesian network model for the data.", "startOffset": 3, "endOffset": 7}], "year": 2012, "abstractText": "Discovering frequent episodes over event sequences is an important data mining task. In many applications, events constituting the data sequence arrive as a stream, at furious rates, and recent trends (or frequent episodes) can change and drift due to the dynamical nature of the underlying event generation process. The ability to detect and track such the changing sets of frequent episodes can be valuable in many application scenarios. Current methods for frequent episode discovery are typically multipass algorithms, making them unsuitable in the streaming context. In this paper, we propose a new streaming algorithm for discovering frequent episodes over a window of recent events in the stream. Our algorithm processes events as they arrive, one batch at a time, while discovering the top frequent episodes over a window consisting of several batches in the immediate past. We derive approximation guarantees for our algorithm under the condition that frequent episodes are approximately well-separated from infrequent ones in every batch of the window. We present extensive experimental evaluations of our algorithm on both real and synthetic data. We also present empirical comparisons with baselines and adaptations of streaming algorithms from itemset mining literature.", "creator": "LaTeX with hyperref package"}}}