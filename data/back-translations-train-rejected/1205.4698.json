{"id": "1205.4698", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-May-2012", "title": "The Role of Weight Shrinking in Large Margin Perceptron Learning", "abstract": "We introduce into the classical perceptron algorithm with margin a mechanism that shrinks the current weight vector as a first step of the update. If the shrinking factor is constant the resulting algorithm may be regarded as a margin-error-driven version of NORMA with constant learning rate. In this case we show that the allowed strength of shrinking depends on the value of the maximum margin. We also consider variable shrinking factors for which there is no such dependence. In both cases we obtain new generalizations of the perceptron with margin able to provably attain in a finite number of steps any desirable approximation of the maximal margin hyperplane. The new approximate maximum margin classifiers appear experimentally to be very competitive in 2-norm soft margin tasks involving linear kernels.", "histories": [["v1", "Mon, 21 May 2012 19:19:49 GMT  (106kb)", "https://arxiv.org/abs/1205.4698v1", "15 pages"], ["v2", "Thu, 7 Feb 2013 19:10:14 GMT  (107kb)", "http://arxiv.org/abs/1205.4698v2", "15 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["constantinos panagiotakopoulos", "petroula tsampouka"], "accepted": false, "id": "1205.4698"}, "pdf": {"name": "1205.4698.pdf", "metadata": {"source": "CRF", "title": "The Role of Weight Shrinking in Large Margin Perceptron Learning", "authors": ["Constantinos Panagiotakopoulos"], "emails": ["costapan@eng.auth.gr,", "petroula@gen.auth.gr"], "sections": [{"heading": null, "text": "ar Xiv: 120 5.46 98v2 [cs.LG] 7 Feb 2"}, {"heading": "1 Introduction", "text": "It is only a matter of time before it will happen, until it will happen."}, {"heading": "2 The Algorithms", "text": "Let us consider a linear, separable training. (xk, lk) mk = 1, with vectors xk and labels lk = 1, -1, -1,. This training can be either the original dataset or the result of a mapping into a feature space of higher dimensionality. [23, 2] By moving in the same position in an additional dimension, i.e. by extending xk to [xk,] we construct an embedding of our data in the so-called extended space [4]. In this way we construct hyperplanes that have bias in the non-augmented feature space. After augmentation, a reflection is performed on the origin of the negatively designated patterns, allowing a uniform treatment of both categories of patterns."}, {"heading": "3 Theoretical Analysis", "text": "We start with the analysis of margin perception with a constant shrinkage. Theorem 1. The margin perception with constant shrinkage converges into a finite number."}, {"heading": "4 Implementation and Experiments", "text": "However, the parameter c \u00b2, which multiplies the threshold of the misclassification condition when this condition is used to select the points of the first level of active margin, is given the value c \u00b2 = 1.01. The parameters that determine the number of times the active records are appended to the algorithm are set to the values Nep1 = Nep2 = 5.An additional mechanism that provides a significant improvement in computational efficiency is the one that performs several updates [14-16] as soon as a data point of the algorithms is presented. It is understood that a multiple update of a certain number of updates that should be equivalent as a result of the repeated presentation of the data in question. Thus, the maximum multiplicity of such an update is determined by the requirement that meets the pattern of the misclassification."}, {"heading": "5 Conclusions", "text": "Motivated by the presence of weight contraction in most attempts to solve the L1-SVM problem by stochastic gradient descendence, we introduced this feature into the classical Perceptron margin algorithm. In the case of constant weight loss \u03bb and constant learning rate parameters, we demonstrated that convergence to solutions with near-maximum margin requires approaching a margin-dependent maximum permitted value. Variable shrinkage scenarios were also considered and proved not to be subject to such limitations. Theoretical analysis was confirmed by an experimental study with massive data sets looking for large margin solutions in an extended feature space, a problem consistent with the 2-standard soft margin. As a final conclusion of our study, we can say that the shrinkage of the current weight vector is the first step in updating the ability to elevate the online perceptor to a very effective large margin."}, {"heading": "A Proof of Lemma 1", "text": "Proof. We start from the induction in the integer. Because t = 1 inequality (20) is reduced to (n + 1) \u2264 2n, which since 2n = (1 + 1) n \u2265 1 + n. Let us now assume that (20) is valid and prove that (n + 1) \u2211 t + 1 k = 1 k \u2264 (t + 1) (t + 2) still (n + 1) (((t + 1) n + 1) n \u2264 (t + 1) n or that (t + 2) n \u2265 (t + 1) n \u2212 1 (n + 1 + t) n = (t + 1) n \u2264 (t + 1) (t + 2) n or that (t + 1) n \u2265 (t + 1) n \u2265 (n + 1) n \u2212 1 (n + 1 + 1) n (n + 1 + 1) n (n + 1)."}, {"heading": "B Proof of Lemma 2", "text": "Proof. We assume an induction in the integer t = 1 inequality (21) is reduced to (n + 1) (2n + 1) (2n + 1) (2n + 1) \u2265 2n (1 \u2212 n (n \u2212 2)) which holds n + 0. Let us now assume that (21) t + 1 k = 1 k = 1 k n \u2265 1 + 1 (t + 2) n + 1 (t + 1) n + 1 \u2212 n + 1 (t + 1) n + 1 k + 1 k + 1 k = (t + 1) n (t + 1) l = 1 (n + 1) n \u2265 (t + 1) n + 1 (t + 1) n + 1 (n + 1) n + 1 (n + 1) l = 1 (n + 1)."}, {"heading": "C Proof of Lemma 3", "text": "The proof: We assume an induction in the integer t, because t = 1 inequality (22) is reduced to 2n + 1 \u2264 (n + 1) 2, which obviously holds n = 0. Let us now assume that (22) is valid and prove that (2n + 1) (t + 1) \u2211 t + 1 k = 1 k2n \u2264 (n + 1) 2 (t + 1 k = 1 kn) 2. Use (22) we (2n + 1) + 1) \u2211 t + 1 k = 1 k 2n = (2n + 1) (t + 1) (t + 1) (t + 1) 2n = (t + 1) 2n \u2212 n (t + 1) 2n + 1) (t + 1 t = 1 t = 1 t (2n)."}], "references": [{"title": "Tracking the best hyperplane with a simple budget perceptron", "author": ["N. Cesa-Bianchi", "C. Gentile"], "venue": "COLT, pp. 483-498", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "An introduction to support vector machines", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": "Cambridge University Press, Cambridge", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "The forgetron: A kernel-based perceptron on a fixed budget", "author": ["O. Dekel", "S. Shalev-Shwartz", "Singer. Y."], "venue": "NIPS, 18 pp. 259-266", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Pattern classsification and scene analysis", "author": ["R.O. Duda", "P.E. Hart"], "venue": "Wiley, Chichester", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1973}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Y. Freund", "R.E. Shapire"], "venue": "Machine Learning 37(3), 277-296", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "A new approximate maximal margin classification algorithm", "author": ["C. Gentile"], "venue": "Journal of Machine Learning Research 2, 213-242", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Making large-scale SVM learning practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods-Support Vector Learning. MIT Press, Cambridge", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "KDD pp. 217-226", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "A dual coordinate descent method for large-scale linear SVM", "author": ["Hsieh", "C.-J.", "Chang", "K.-W.", "Lin", "C.-J.", "S.S. Keerthi", "S. Sundararajan"], "venue": "ICML pp. 408-415", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Online learning with kernels", "author": ["J. Kivinen", "A. Smola", "R. Williamson"], "venue": "IEEE Transactions on Signal Processing 52(8), 2165-2176", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning algorithms with optimal stability in neural networks", "author": ["W. Krauth", "M. M\u00e9zard"], "venue": "Journal of Physics A20, L745-L752", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1987}, {"title": "The relaxed online maximummargin algorithm", "author": ["Y. Li", "P. Long"], "venue": "Machine Learning, 46(1-3), 361-387", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "On convergence proofs on perceptrons", "author": ["A.B.J. Novikoff"], "venue": "Proc. Symp. Math. Theory Automata, vol. 12, pp. 615-622", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1962}, {"title": "The margin perceptron with unlearning", "author": ["C. Panagiotakopoulos", "P. Tsampouka"], "venue": "ICML pp. 855\u2013862", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "The margitron: A generalized perceptron with margin", "author": ["C. Panagiotakopoulos", "P. Tsampouka"], "venue": "IEEE Transactions on Neural Networks 22(3), 395-407", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "The perceptron with dynamic margin", "author": ["C. Panagiotakopoulos", "P. Tsampouka"], "venue": "Kivinen, J., et. al. (eds.) ALT 2011. LNCS (LNAI) vol. 6925, pp. 204-218. Springer, Heidelberg", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "author": ["J.C. Platt"], "venue": "Microsoft Res. Redmond WA, Tech. Rep. MSR-TR-98-14", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological Review, 65(6), 386-408", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1958}, {"title": "Pegasos: Primal estimated sub-gradient solver for SVM", "author": ["S. Shalev-Schwartz", "Y. Singer", "N. Srebro"], "venue": "ICML pp. 807-814", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Analysis of generic perceptron-like large margin classifiers", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "Gama, J., et. al. (eds.) ECML 2005. LNCS (LNAI) vol. 3720, pp. 750-758. Springer, Heidelberg", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Constant rate approximate maximum margin algorithms", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "F\u00fcrnkranz, J., et. al. (eds.) ECML 2006. LNCS (LNAI) vol. 4212, pp. 437-448. Springer, Heidelberg", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Approximate maximum margin algorithms with rules controlled by the number of mistakes", "author": ["P. Tsampouka", "J. Shawe-Taylor"], "venue": "ICML pp. 903\u2013910", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Statistical learning theory", "author": ["V. Vapnik"], "venue": "Wiley, Chichester", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 22, "context": "It is widely accepted that the generalization ability of learning machines improves as the margin of the solution hyperplane increases [23].", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "The simplest online learning algorithm for binary linear classification, the perceptron [18, 13], does not aim at any margin.", "startOffset": 88, "endOffset": 96}, {"referenceID": 12, "context": "The simplest online learning algorithm for binary linear classification, the perceptron [18, 13], does not aim at any margin.", "startOffset": 88, "endOffset": 96}, {"referenceID": 22, "context": "The problem, instead, of finding the optimal separating hyperplane is central to Support Vector Machines (SVMs) [23, 2].", "startOffset": 112, "endOffset": 119}, {"referenceID": 1, "context": "The problem, instead, of finding the optimal separating hyperplane is central to Support Vector Machines (SVMs) [23, 2].", "startOffset": 112, "endOffset": 119}, {"referenceID": 16, "context": "To overcome this obstacle decomposition methods [17, 7] were developed that apply optimization only to a subset of the training set.", "startOffset": 48, "endOffset": 55}, {"referenceID": 6, "context": "To overcome this obstacle decomposition methods [17, 7] were developed that apply optimization only to a subset of the training set.", "startOffset": 48, "endOffset": 55}, {"referenceID": 7, "context": "Only recently the so-called linear SVMs [8, 9, 14] by making partial use of primal notation in the case of linear kernels managed to successfully deal with massive datasets.", "startOffset": 40, "endOffset": 50}, {"referenceID": 8, "context": "Only recently the so-called linear SVMs [8, 9, 14] by making partial use of primal notation in the case of linear kernels managed to successfully deal with massive datasets.", "startOffset": 40, "endOffset": 50}, {"referenceID": 13, "context": "Only recently the so-called linear SVMs [8, 9, 14] by making partial use of primal notation in the case of linear kernels managed to successfully deal with massive datasets.", "startOffset": 40, "endOffset": 50}, {"referenceID": 3, "context": "The first algorithm of the kind is the perceptron with margin [4] the solutions of which provably possess only up to 1/2 of the maximum margin [11].", "startOffset": 62, "endOffset": 65}, {"referenceID": 10, "context": "The first algorithm of the kind is the perceptron with margin [4] the solutions of which provably possess only up to 1/2 of the maximum margin [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 11, "context": "For ROMMA [12] such a rule is the result of a relaxed optimization which reduces all constraints to just two.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "In contrast, ALMA [6] and much later CRAMMA [21] and MICRA [22] employ a \u201cprojection\u201d mechanism to restrict the length of the weight vector and adopt a learning rate and margin threshold in the condition which both follow specific rules involving the number of updates.", "startOffset": 18, "endOffset": 21}, {"referenceID": 20, "context": "In contrast, ALMA [6] and much later CRAMMA [21] and MICRA [22] employ a \u201cprojection\u201d mechanism to restrict the length of the weight vector and adopt a learning rate and margin threshold in the condition which both follow specific rules involving the number of updates.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "In contrast, ALMA [6] and much later CRAMMA [21] and MICRA [22] employ a \u201cprojection\u201d mechanism to restrict the length of the weight vector and adopt a learning rate and margin threshold in the condition which both follow specific rules involving the number of updates.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "Very recently, the margitron [15] and the perceptron with dynamic margin (PDM) [16] using modified conditions managed to approximately reach maximum margin solutions while maintaining the original perceptron update.", "startOffset": 29, "endOffset": 33}, {"referenceID": 15, "context": "Very recently, the margitron [15] and the perceptron with dynamic margin (PDM) [16] using modified conditions managed to approximately reach maximum margin solutions while maintaining the original perceptron update.", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "Notable representatives of this approach are the pioneer NORMA [10] and Pegasos [19].", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "Notable representatives of this approach are the pioneer NORMA [10] and Pegasos [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 2, "context": "Shrinking has also been employed by algorithms which do not have their origin in stochastic gradient descent as an accompanying mechanism in perceptron-based budget scenarios for classification [3] or tracking [1].", "startOffset": 194, "endOffset": 197}, {"referenceID": 0, "context": "Shrinking has also been employed by algorithms which do not have their origin in stochastic gradient descent as an accompanying mechanism in perceptron-based budget scenarios for classification [3] or tracking [1].", "startOffset": 210, "endOffset": 213}, {"referenceID": 22, "context": "This training set may be either the original dataset or the result of a mapping into a feature space of higher dimensionality [23, 2].", "startOffset": 126, "endOffset": 133}, {"referenceID": 1, "context": "This training set may be either the original dataset or the result of a mapping into a feature space of higher dimensionality [23, 2].", "startOffset": 126, "endOffset": 133}, {"referenceID": 3, "context": ", by extending xk to [xk, \u03c1], we construct an embedding of our data into the socalled augmented space [4].", "startOffset": 102, "endOffset": 105}, {"referenceID": 19, "context": "The maximum directional margin \u03b3d is upper bounded by the maximum geometric margin \u03b3 in the non-augmented space and tends to it as \u03c1 \u2192 \u221e [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 0, "context": "A variable shrinking factor t/(t+ \u03bb) is also employed by SPA [1] in which b = 0.", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": "To reduce the computational cost we adopt a two-member nested sequence of reduced \u201cactive sets\u201d of data points as described in detail in [15].", "startOffset": 137, "endOffset": 141}, {"referenceID": 13, "context": "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14\u201316] once a data point is presented to the algorithm.", "startOffset": 134, "endOffset": 141}, {"referenceID": 14, "context": "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14\u201316] once a data point is presented to the algorithm.", "startOffset": 134, "endOffset": 141}, {"referenceID": 15, "context": "An additional mechanism providing a substantial improvement of the computational efficiency is the one of performing multiple updates [14\u201316] once a data point is presented to the algorithm.", "startOffset": 134, "endOffset": 141}, {"referenceID": 4, "context": "For inseparable data, instead, a space extended by m dimensions, as many as the instances, is considered where each instance is placed at a distance \u2206 from the origin in the corresponding dimension [5].", "startOffset": 198, "endOffset": 201}, {"referenceID": 1, "context": "This extension generates a margin of at least\u2206/ \u221a m and its employment relies on the well-known equivalence between the hard margin optimization in the extended space and the soft margin optimization in the initial instance space with objective function \u2016w\u2016 +\u2206 \u2211 i\u03bei 2 involving the weight vector w and the 2-norm of the slacks \u03bei [2].", "startOffset": 331, "endOffset": 334}, {"referenceID": 15, "context": "Further details may be found in [16].", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "The experiments, like the ones of [16], were conducted on a 2.", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "Therefore, the runtimes reported here can be directly compared to the ones of [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 15, "context": "More important, though, is a comparison with the results obtained with other large margin classifiers as reported in [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 6, "context": "We see that MPCS and MPVS are orders of magnitude faster than ROMMA and SVM [7], faster than PDM and of comparable speed or at most about 2 times slower than the linear SVM algorithms DCD [9] and MPU [14].", "startOffset": 76, "endOffset": 79}, {"referenceID": 8, "context": "We see that MPCS and MPVS are orders of magnitude faster than ROMMA and SVM [7], faster than PDM and of comparable speed or at most about 2 times slower than the linear SVM algorithms DCD [9] and MPU [14].", "startOffset": 188, "endOffset": 191}, {"referenceID": 13, "context": "We see that MPCS and MPVS are orders of magnitude faster than ROMMA and SVM [7], faster than PDM and of comparable speed or at most about 2 times slower than the linear SVM algorithms DCD [9] and MPU [14].", "startOffset": 200, "endOffset": 204}], "year": 2013, "abstractText": "We introduce into the classical perceptron algorithm with margin a mechanism that shrinks the current weight vector as a first step of the update. If the shrinking factor is constant the resulting algorithm may be regarded as a margin-error-driven version of NORMA with constant learning rate. In this case we show that the allowed strength of shrinking depends on the value of the maximum margin. We also consider variable shrinking factors for which there is no such dependence. In both cases we obtain new generalizations of the perceptron with margin able to provably attain in a finite number of steps any desirable approximation of the maximal margin hyperplane. The new approximate maximum margin classifiers appear experimentally to be very competitive in 2-norm soft margin tasks involving linear kernels.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}