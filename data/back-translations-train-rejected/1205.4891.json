{"id": "1205.4891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2012", "title": "Clustering is difficult only when it does not matter", "abstract": "Numerous papers ask how difficult it is to cluster data. We suggest that the more relevant and interesting question is how difficult it is to cluster data sets {\\em that can be clustered well}. More generally, despite the ubiquity and the great importance of clustering, we still do not have a satisfactory mathematical theory of clustering. In order to properly understand clustering, it is clearly necessary to develop a solid theoretical basis for the area. For example, from the perspective of computational complexity theory the clustering problem seems very hard. Numerous papers introduce various criteria and numerical measures to quantify the quality of a given clustering. The resulting conclusions are pessimistic, since it is computationally difficult to find an optimal clustering of a given data set, if we go by any of these popular criteria. In contrast, the practitioners' perspective is much more optimistic. Our explanation for this disparity of opinions is that complexity theory concentrates on the worst case, whereas in reality we only care for data sets that can be clustered well.", "histories": [["v1", "Tue, 22 May 2012 12:25:01 GMT  (18kb,D)", "http://arxiv.org/abs/1205.4891v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["amit daniely", "nati linial", "michael saks"], "accepted": false, "id": "1205.4891"}, "pdf": {"name": "1205.4891.pdf", "metadata": {"source": "CRF", "title": "Clustering is difficult only when it does not matter\u2217", "authors": ["Amit Daniely", "Michael Saks"], "emails": ["amit.daniely@math.huji.ac.il", "nati@cs.huji.ac.il", "saks@math.rutgers.edu."], "sections": [{"heading": null, "text": "We present a theoretical framework of clustering in metric spaces, which revolves around a notion of \"good clustering.\" We show that if good clustering exists, it can be found efficiently in many cases. Contrary to popular belief, cluster analysis should not be considered a hard task. Keywords: cluster analysis, hardness of clustering, theoretical framework for clustering, stability. Recognition for this title goes to Tali Tishby, who explained this in a conversation with one of us many years ago. \u2020 Faculty of Mathematics, Hebrew University, Jerusalem 91904, Israel. Partly supported by a binational Israel-USA scholarship 2008368. amit.daniely @ math.huji.ac.il \"School of Computer Science and Engineering, Hebrew University, Jerusalem 91904, Israel. Partly supported by a binational Israel-USA scholarship 2008368. nati @ cs.hujiac.il"}, {"heading": "1 Introduction", "text": "Despite several recent attempts to develop a theory of clustering (e.g. [1, 4, 9]), our basic understanding of matter is still quite unsatisfactory. Rather, the cluster problem is concerned with a series of objects X, but with an additional structure, such as a difference (or similarity) of the function w: X \u00d7 X \u2192 [0, \u221e). Informal, we seek a division of X into clusters, so that objects in the same cluster are placed if they are sufficiently similar. Here are some concrete popular manifestations of this general problem. 1. A very popular optimization criterion is k-means. Besides X and w, a division of X into clusters is sought, so that objects in the same cluster are placed if they are sufficiently similar. Here are some concrete popular manifestations of this general problem."}, {"heading": "1.1 A Theoretical Framework for Clustering in Metric Spaces", "text": "There are numerous concepts of clusters in datasets and cluster methods that are found in the literature. Although these methods are not necessarily explicitly mentioned, they are guided by an ideal (in the Platonic sense) that is the notion of a good cluster in a room X. This is a subset of C \"X,\" so if x is \"C and y\" C, then x is much closer to C than y. \"To exclude trivialities, we usually need C to be large enough than any other cluster. The problem is specified in relation to a trivial singleton cluster. Even more emphasis is placed on problems of cluster formation. Here, we are looking for partitions of space X in clusters so that each x\" X \"is closer to the cluster containing it than to any other cluster. The problem is specified in relation to a proximity measurement (x, A) between elements x and subsets X. Numerous natural choices for X."}, {"heading": "1.2 An overview", "text": "Our discussion splits according to the value of the parameter \u03b1. If \u03b1 is limited from zero, we work with an exhaustive sample (e.g. as in [2]). We first sample a small number of points S out of space. Since | S | is small (logarithmic in an error parameter), it is mathematically feasible to associate all possible partitions of S. For each partition of S, we associate a cluster formation that can be regarded as the corresponding \"voronoi diagram.\" If the space has a (\u03b1) clustering C, then let us look at the partition of S, which is in accordance with C. We show that the \"voronoi diagram\" of Harry Potter almost coincides with C, provided that this part of 1. is delimited. Specifically, Lemma 2.2 controls the distances between points located in different clusters and clusters of C. Together with Hoeffding's inequality, Lemma 2.3 and Coroll4 result that we have a specific space of two (2.5) clusters."}, {"heading": "2 Clustering into Few Clusters \u2013 \u03b1 is bounded away", "text": "To avoid confusion, other probability measures used everywhere are defined by Pr. We define a metric d between two subsets of X, say C = {C1,.., Ck} and C \u00b2 = {C \u2032 1,., C \u00b2 k \u00b2 k \u00b2 k \u00b2. The definition of d (C, C \u00b2) naturally extends to the case where C and C \u00b2 l have sets and, say, l \u2264 k. The only change is that we are now both elements equal: [l] \u2192 [k] is 1: 1. We also define on the sets."}, {"heading": "Algorithmic Aspects", "text": "We will now show that a (\u03b1, \u03b3) clustering method can be approached efficiently. By lemma 2.4, (\u03b1, \u03b3) clustering we can approximate by a small sample, approximating with respect to the symmetric difference metric. A big mistake of this approximation scheme is that we do not have a verification method to accompany it. We do not know how to verify whether a given partition is close to a (\u03b1, \u03b3) clustering w.r.t. of the symmetric difference. To this end, we present a different idea of approximation. A family of subsets of X, C = {C1,., Ck} is an aggregation, if \u2022 For each i-idea [k], P (Ci)."}, {"heading": "3 Clustering into Many Clusters", "text": "To simplify matters, we consider only finite metric spaces, which are equipped with a uniform probability distribution."}, {"heading": "X into such clusters.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Algorithmic Aspects", "text": "Next, we will discuss several algorithmic aspects of clustering in any number of clusters. Our input consists of a finite metric space X and the parameter \u03b1 > 0. Lemma 3.1 suggests an algorithm to search (\u03b1, 3 +) clusters and to divide the space into (\u03b1, 3 +) clusters. The runtime of this algorithm is polynomic in | X | and independent of \u03b1. The second part of the problem suggests how to find all (\u03b1, 3 +) clusters. As the first part of the problem shows, the inclusion relationship between the (\u03b1, 3 +) clusters has a tree structure. Therefore, we can apply dynamic programming to find a division of the space into (\u03b1, 3 +) clusters, provided that such a partition exists. This proves the positive part of theorem 1.2.To correspond to the above positive result, we show theorem 3.4. The following problems are NP-Hard.1. (\u03b1, 2.5-CLUER-X, STING-\u03b1 and STING-STING-Part 2.5)."}, {"heading": "4 Conclusion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Relation to other work", "text": "As we explain below, our work is inspired by the classic VC / PAC theory. Furthermore, we refer to several recent papers that contribute to the development of a theory of clustering."}, {"heading": "VC/PAC theory", "text": "The VC / PAC setting provides the following formal description of the classification problem. We are dealing with a space X of instances. The problem is to recover an unknown member in a known class H of hypotheses. Here, H-YX, where Y is a finite set of labels. We are trying to recover the unknown h * by observing a sample S = {(xi, h * (xi)} mi = 1 x \u00d7 Y. These samples come from a fixed but unknown distribution over X. Our description of the cluster problem is similar. We are looking at a space X of instances and a class G of good clusters (P, C) of X, where P is a probability measurement over X and C is a partition of X. We get a sample {X1,... Xm} X coming from an unknown P, where (P, C) and a class G is for some good clusters (P, C) of X, and our purpose is to restore concrete C. Here is a metric space X."}, {"heading": "Alternative Notions of Good Clustering", "text": "These works assume that the space under consideration exhibits clustering with some structural properties and show how to find it efficiently. In particular, a key term in these works is the \u03b3-average attraction property, which is conceptually similar to our concept of clustering. In the case of a partition C = {C1,..., Ck} of a room X, it is possible to compare clusters additively or by multiplication with each other. In [4], the requirement that for each x-Ci and j-6-i value (x, Cj) applies that \u2206 (x, Ci) + \u2264 (x, Cj) applies to each x-Ci and j-6 = i, whereas our condition is a clear advantage of our conception is the scale invariance. On the other hand, their algorithms work even if X is not a metric space and is only equipped with an arbitrary dissimilarity."}, {"heading": "Stability", "text": "The concept of instance stability was introduced in [5] (see also [3]). An instance of an optimization problem is considered stable if the optimal solution does not change (or changes only slightly) in case of a minor input disturbance. It is noted that cases of cluster problems are of practical interest only if they are stable. The concept of a (\u03b1, \u03b3) cluster has a similar stability property. Namely, if we slightly disturb a metric, cluster formation is still (\u03b1, \u03b3 \") cluster formation for a (\u03b1, \u03b1) problem. We remember that good cluster formation remains a good cluster formation with a slight input disturbance. This work is a result of our work on stable instances of MAXCUT, which we consider a cluster problem. We remember that the input on the MAXCUT problem is a n \u00d7 non-symmetrical matrix."}, {"heading": "4.2 Future Work and Open Questions", "text": "In view of this article and of essays such as [1, 8, 4], it is clear that there is still much interest in new ideas of good clustering and relevant algorithms. Nevertheless, several natural questions remain open regarding the topics discussed here. 1. We believe that it should be possible to improve the dependence on \u03b1 and \u03b3 of the runtime of the algorithm. 3. As Lemma 3.1 shows, (3 +) clusters are only balls. It is not difficult to see that Lemma 2.3 implies that in view of the (\u03b1, \u03b3) clustering of an n-point of metric space, it is possible to find such a division into 2.5 clusters. 3. As Lemma 3.1 shows, (3 +) clusters are only balls. It is not difficult to see that Lemma 2.3 implies that in a (TA) clustering of an n-point of metric space, it is possible to find such a point."}, {"heading": "A Proofs omitted from the text", "text": "The Nachweis. (von Lemma \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "Numerous papers ask how difficult it is to cluster data. We suggest that the<lb>more relevant and interesting question is how difficult it is to cluster data sets<lb>that can be clustered well. More generally, despite the ubiquity and the great<lb>importance of clustering, we still do not have a satisfactory mathematical theory<lb>of clustering. In order to properly understand clustering, it is clearly necessary to<lb>develop a solid theoretical basis for the area. For example, from the perspective<lb>of computational complexity theory the clustering problem seems very hard.<lb>Numerous papers introduce various criteria and numerical measures to quantify<lb>the quality of a given clustering. The resulting conclusions are pessimistic, since<lb>it is computationally difficult to find an optimal clustering of a given data set, if<lb>we go by any of these popular criteria. In contrast, the practitioners\u2019 perspective<lb>is much more optimistic. Our explanation for this disparity of opinions is that<lb>complexity theory concentrates on the worst case, whereas in reality we only care<lb>for data sets that can be clustered well.<lb>We introduce a theoretical framework of clustering in metric spaces that<lb>revolves around a notion of \u201dgood clustering\u201d. We show that if a good clustering<lb>exists, then in many cases it can be efficiently found. Our conclusion is that<lb>contrary to popular belief, clustering should not be considered a hard task.<lb>", "creator": "LaTeX with hyperref package"}}}