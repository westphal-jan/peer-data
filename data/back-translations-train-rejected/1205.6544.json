{"id": "1205.6544", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2012", "title": "A Brief Summary of Dictionary Learning Based Approach for Classification (revised)", "abstract": "This note presents some representative methods which are based on dictionary learning (DL) for classification. We do not review the sophisticated methods or frameworks that involve DL for classification, such as online DL and spatial pyramid matching (SPM), but rather, we concentrate on the direct DL-based classification methods. Here, the \"so-called direct DL-based method\" is the approach directly deals with DL framework by adding some meaningful penalty terms. By listing some representative methods, we can roughly divide them into two categories, i.e. (1) directly making the dictionary discriminative and (2) forcing the sparse coefficients discriminative to push the discrimination power of the dictionary. From this taxonomy, we can expect some extensions of them as future researches.", "histories": [["v1", "Wed, 30 May 2012 05:07:55 GMT  (33kb)", "http://arxiv.org/abs/1205.6544v1", "a note revised from a withdrawn one"]], "COMMENTS": "a note revised from a withdrawn one", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["shu kong", "donghui wang"], "accepted": false, "id": "1205.6544"}, "pdf": {"name": "1205.6544.pdf", "metadata": {"source": "CRF", "title": "A Brief Summary of Dictionary Learning Based Approach for Classification", "authors": [], "emails": ["dhwang}@zju.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 120 5.65 44v1 [cs.CV]"}, {"heading": "1 Introduction", "text": "Dictionary Learning (DL), as a particular sparse signal model, aims to learn a set of atoms, or as visual words in the computer vision community, where a few atoms can be combined linearly to approximate a particular signal well. From the compression scanning point of view, it was originally designed to learn an adaptive code book to accurately represent the signals with constraints. In recent years, researchers have applied DL frameworks to other applications, achieving state-of-the-art achievements such as image reduction [4], clustering [2, 9], classification [1, 6], etc. It is known that the conventional DL framework is not adapted to classification as a result that the dictionary learned is only used for signal reconstruction."}, {"heading": "1.1 Sparse Representation-Based Classification", "text": "Wright et al. [10] propose the sparse representation-based classification method (SRC) for robust facial recognition and achieve very impressive results. Suppose there were C classes of individual faces, leave D = [X1,.., Xc,..., XC]. R \u00b7 N would be the set of original training samples, where Xc and R \u00b7 Nc would be the subset of all Nc vector represented training samples from class c. SRC treats the original data set as a comprehensive dictionary. Name x-Rd a curious facial image, then SRC x identifies the following two-step procedure: 1. Thinly code x via X via 1-norm minimizationa = argmin a-x-Da 2 + 1, (1) where it is a scalar facial image, then SRC x identifies as the following two-step procedure: 1. Thinly code spationmix over 1-normia = 1."}, {"heading": "1.2 Dictionary Learning Framework", "text": "Learning an adaptive dictionary (possible completion) aims to provide a basic pool in which some basics can be combined linearly to approximate a novel signal. Suppose there are a series of signals designated by X = [x1,..., xN], where xi is the ith signal. Then, the conventional dictionary learns the dictionary as follows: {A, D} = argmin D, Rd \u00b7 KA, RK \u00b7 NN, i = 1,.., N, (3) where A = [a1,.., aN] is the efficient matrix, and vice versa."}, {"heading": "2 Track I: Directly Making the Dictionary Discriminative", "text": "The methods of Track I use the reconstruction error for the final classification, so the dictionary learned should be as differentiated as possible. Inspired by SRC, Yang et al. suggest Meta-Face-Learning [12] to learn an adaptive dictionary for each class, and Ramirez et al. add a sophisticated term to derive finer classification-oriented dictionaries. Now, we present the two methods."}, {"heading": "2.1 Meta-Face Learning", "text": "SRC directly adopts the original facial images as a dictionary, but as discussed in [12], this predefined dictionary will contain a lot of redundancy as well as noise and trivial information that can negatively affect facial recognition. As training data grows, calculating sparse encodings will become a major obstacle. To focus on this problem, Yang et al. [12] propose a metaface learning method to learn a class-specific dictionary for each object: Di = argmin Di-Xi \u2212 DiAi-2 + 0-Ai-1, s.t. [12] a metaface learning method in which the matrix Xi-R-Ni contains all training images from the ith class as columns, dij is the j. column of the ith class-specific subdictionary Di = [d i-1,."}, {"heading": "2.2 Dictionary Learning with Structured Incoherence", "text": "Ramirez et al. note that the learned sub-dictionaries can share some common bases, i.e. some visual words from different sub-dictionaries can be very coherent [8]. Undoubtedly, the coherence of the atoms can be used interchangeably to reconstruct the query image, and the reconstruction error-based classifier will fail to identify some queries. To get around this problem, they add an incoherence term to make the dictionaries associated with different classes as independent as possible. The incoherence term is referred to as Q (Di, Dj): D i Dj 2 F. It is easy to see that this term causes the atoms from different sub-dictionaries to be as independent / incoherent as possible."}, {"heading": "3 Track II: Making the Coefficients Discriminative", "text": "Track II differs from Track I in the type of discrimination. Unlike Track I, it forces the sparse coefficients to be discriminatory, and propagates the discriminatory power indirectly into the general dictionary. Track II only needs to learn a general dictionary, rather than class-specific dictionaries. In this section, we list several recently proposed methods that belong to Track II."}, {"heading": "3.1 Supervised Dictionary Learning", "text": "Before introducing this method, we must clarify that the SupervisedDL (SupervisedDL) method is a specific approach proposed in [6], independently of other possible monitored DL frameworks. Mairal et al. propose combining logistic regression with the conventional dictionary learning framework as follows: (A, D) = argmin \u03b8D * Rd \u00b7 K A * RK \u00b7 NN * i = 1 (C (yif (xi, ai, \u03b8)) + \u03bb0 * xi \u2212 Dai * 2 2 + \u03bb1 * ai * 1) + \u03bb2 * zip2, s.t. \u00b2 2 2 2 \u2264 1, for Di = 1,..., N, (6) where C is the logistic loss function (C (x) = log (1 + e \u2212 x), which has properties similar to those of the hinges loss from the SVM literature, while distinguishable, and is a regulation parameter that prevents overpass."}, {"heading": "3.2 Discriminative K-SVD for Dictionary Learning", "text": "Zhang and Li propose discriminatory K-SVD (D-KSVD) to simultaneously achieve a desired dictionary that has good representational power and supports optimal class discrimination [13]. D-KSVD adds a simple linear regression to the conventional DL framework as a punitive term: (D, W, A) = argmin D, W, A, X \u2212 DA, 2F + 0, H \u2212 WA, 2 F + 0, 0]: The position of a non-zero element indicates the class. (7) where H = [h1,.., hN] is the parameter of the classifier, while hn = [0,.., 0, 1, 0,... 0]: the position of a non-zero element indicates the class, and W is the parameter of the classifier, \u03bb1, 2, and \u03bb3 are scalars that control the relative contribution of the corresponding terms."}, {"heading": "3.3 Label Consistent K-SVD", "text": "Jiang et al. propose a label consistent K-SVD (LC-KSVD) method to learn a discriminatory dictionary for sparse coding [5], introduce a consistent label restriction called a \"discriminatory sparse code error,\" and combine it with the reconstruction error and the classification error to form a uniform objective function as follows: (D, W, A) = argmin D, (W) = argmin D, (8), where H and W are equal to the D-KSVD code described in the previous subsection, Q = [q1,. qN]."}, {"heading": "3.4 Fisher Discriminant Dictionary Learning", "text": "The structured dictionary is called D = [D1,.., DC], where Dc is the class-specific subdictionary associated with the cth class. Name the dataset X \u2212 \u2212 Xi,., XC], where Xc is the subset of samples from the cth class. Then, solve the following formula using the dictionary and coefficients to derive the desired discriminatory dictionary: (D, A) = argmin D, Rd, KA, RK, NC (X, D) +, A, A, A, A, A, A, A, A, A, A, A."}, {"heading": "4 Summary", "text": "In the previous two sections, we review some representative DL-based classification approaches, both track I and track II. Obviously, it is intuitive but effective to add some sophisticated discrimination terms to the conventional DL framework in order to derive a well-learned dictionary for classification from it. If we examine these methods, we can anticipate a general framework here: min D, W, A C (Y, X, D, A) + \u03b7f (W, A, Y) + \u03bbAhA (A) + \u03bbWhW (W) s.t. Restriction of D, (13) where C (Y, D, A) is the conventional DL framework, f (W, A, Y) is the discrimination term for the sparse coefficients, hA and hW are the lagrange constraints for the sparse coefficient matrix A and the projector W, and the projector W, the projector Y and projector Y are necessarily weights, but rather than a W."}], "references": [{"title": "Differentiable sparse coding", "author": ["D.M. Bradley", "J.A. Bagnell"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Learning with l1-graph for image analysis", "author": ["B. Cheng", "J. Yang", "S. Yan", "Y. Fu", "T.S. Huang"], "venue": "IEEE Trans. Img. Proc.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Image denoising via learned dictionaries and sparse representation", "author": ["M. Elad", "M. Aharon"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "On the role of sparse and redundant representations in image processing", "author": ["M. Elad", "M. Figueiredo", "Y. Ma"], "venue": "proceedings of IEEE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Learning a discriminative dictionary for sparse coding via label consistent k-svd", "author": ["Z. Jiang", "Z. Lin", "L.S. Davis"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Self-taught learning: transfer learning from unlabeled data", "author": ["R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Classification and clustering via dictionary learning with structured incoherence and shared features", "author": ["I. Ramirez", "P. Sprechmann", "G. Sapiro"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "sparse representation for computer vision and pattern recognition", "author": ["J. Wright", "Y. Ma", "J. Mairal", "G. Sapiro", "T. Huang", "S. Yan"], "venue": "proceedings of IEEE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Fisher discrimination dictionary learning for sparse representation", "author": ["M. Yang", "L. Zhang", "X. Feng", "D. Zhang"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "metaface learning for sparse representation based face recognition", "author": ["M. Yang", "L. Zhang", "J. Yang", "D. Zhang"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Discriminative k-svd for dictionary learning in face recognition", "author": ["Q. Zhang", "B. Li"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.", "startOffset": 164, "endOffset": 167}, {"referenceID": 1, "context": "In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.", "startOffset": 180, "endOffset": 186}, {"referenceID": 8, "context": "In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.", "startOffset": 180, "endOffset": 186}, {"referenceID": 0, "context": "In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.", "startOffset": 203, "endOffset": 209}, {"referenceID": 5, "context": "In recent years, researchers have applied DL framework to other applications and achieved state-of-the-art performances, such as image denoising [3] and inpainting [4], clustering [2, 9], classification [1, 6], etc.", "startOffset": 203, "endOffset": 209}, {"referenceID": 11, "context": "Category Representative Approaches Track I Meta-face learning [12], DLSI [8] Track II SupervisedDL [6], D-KSVD [13], LC-KSVD [5], Fisher DL [11]", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "Category Representative Approaches Track I Meta-face learning [12], DLSI [8] Track II SupervisedDL [6], D-KSVD [13], LC-KSVD [5], Fisher DL [11]", "startOffset": 73, "endOffset": 76}, {"referenceID": 5, "context": "Category Representative Approaches Track I Meta-face learning [12], DLSI [8] Track II SupervisedDL [6], D-KSVD [13], LC-KSVD [5], Fisher DL [11]", "startOffset": 99, "endOffset": 102}, {"referenceID": 12, "context": "Category Representative Approaches Track I Meta-face learning [12], DLSI [8] Track II SupervisedDL [6], D-KSVD [13], LC-KSVD [5], Fisher DL [11]", "startOffset": 111, "endOffset": 115}, {"referenceID": 4, "context": "Category Representative Approaches Track I Meta-face learning [12], DLSI [8] Track II SupervisedDL [6], D-KSVD [13], LC-KSVD [5], Fisher DL [11]", "startOffset": 125, "endOffset": 128}, {"referenceID": 10, "context": "Category Representative Approaches Track I Meta-face learning [12], DLSI [8] Track II SupervisedDL [6], D-KSVD [13], LC-KSVD [5], Fisher DL [11]", "startOffset": 140, "endOffset": 144}, {"referenceID": 11, "context": "Track I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11].", "startOffset": 36, "endOffset": 40}, {"referenceID": 7, "context": "Track I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11].", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "Track I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11].", "startOffset": 117, "endOffset": 120}, {"referenceID": 12, "context": "Track I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 4, "context": "Track I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11].", "startOffset": 173, "endOffset": 176}, {"referenceID": 10, "context": "Track I includes Meta-face learning [12] and DL with structured incoherence [8], and Track II contains supervised DL [6], discriminative K-SVD [13], label consistence K-SVD [5] and Fisher discrimination DL [11].", "startOffset": 206, "endOffset": 210}, {"referenceID": 9, "context": "In the end of this section, we review an important method called sparse representation-based classification [10], then introduce the general dictionary learning framework with notations used in this note.", "startOffset": 108, "endOffset": 112}, {"referenceID": 11, "context": "In Section 2, we introduce Meta-face learning [12] and DLSI [8] as two specific examples of Track I, which uses the reconstruction error for the final classification like what SRC does.", "startOffset": 46, "endOffset": 50}, {"referenceID": 7, "context": "In Section 2, we introduce Meta-face learning [12] and DLSI [8] as two specific examples of Track I, which uses the reconstruction error for the final classification like what SRC does.", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "Track II, will presented in Section 3, including SupervisedDL [6], D-KSVD [13], LC-KSVD [5] and FisherDL [11].", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": "Track II, will presented in Section 3, including SupervisedDL [6], D-KSVD [13], LC-KSVD [5] and FisherDL [11].", "startOffset": 74, "endOffset": 78}, {"referenceID": 4, "context": "Track II, will presented in Section 3, including SupervisedDL [6], D-KSVD [13], LC-KSVD [5] and FisherDL [11].", "startOffset": 88, "endOffset": 91}, {"referenceID": 10, "context": "Track II, will presented in Section 3, including SupervisedDL [6], D-KSVD [13], LC-KSVD [5] and FisherDL [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "[10] propose the sparse representation based classification (SRC) method for robust face recognition, and achieve very impressive results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "propose meta-face learning [12] to learn an adaptive dictionary for each class, and Ramirez et al.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "1 Meta-Face Learning SRC directly adopts the original facial images as the dictionary, however, as discussed in [12], this pre-defined dictionary will incorporate much redundancy as well as noise and trivial information that can can be negative to the face recognition.", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "[12] propose a Metaface learning method to learn a class-specific dictionary for each object: Di = argmin Di \u2016Xi \u2212DiAi\u2016 2 2 + \u03bb\u2016Ai\u20161, s.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "some visual words from different sub-dictionaries can be very coherent [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "They empirically note that even though the incoherence term is imposed in the dictionaries, atoms representing common features in all classes tend to appear repeated almost exactly in dictionaries corresponding to different classes [8].", "startOffset": 232, "endOffset": 235}, {"referenceID": 5, "context": "1 Supervised Dictionary Learning Before presenting this method, we have to clarify that the Supervised DL (SupervisedDL) method is a specific approach proposed in [6], regardless of other possible supervised DL framework.", "startOffset": 163, "endOffset": 166}, {"referenceID": 6, "context": "This is the approach chosen in [7].", "startOffset": 31, "endOffset": 34}, {"referenceID": 12, "context": "2 Discriminative K-SVD for Dictionary Learning Zhang and Li propose discriminative K-SVD (D-KSVD) to simultaneously achieve a desired dictionary which has good representation power while supporting optimal discrimination of the classes [13].", "startOffset": 236, "endOffset": 240}, {"referenceID": 12, "context": "Note that the first two terms can be fused into one, and the term \u2016W\u20162F can be dropped during computation owing to the protocol of the original K-SVD algorithm(details in [13]).", "startOffset": 171, "endOffset": 175}, {"referenceID": 4, "context": "propose a label consistent K-SVD (LC-KSVD) method to learn a discriminative dictionary for sparse coding [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 12, "context": "At the same time, the linear regression term \u2016H\u2212WA\u20162F is added, which is the same as that of D-KSVD [13].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "propose Fisher discrimination dictionary learning (FisherDL) method based on the Fisher criterion to learn a structured dictionary [11], whose atom has correspondence to the class label.", "startOffset": 131, "endOffset": 135}, {"referenceID": 10, "context": "There are some crucial issues related to their model, such as the convexity of f(A) and sparse coding, and they discuss these issue in depth [11].", "startOffset": 141, "endOffset": 145}], "year": 2012, "abstractText": "This note presents some representative methods which are based on dictionary learning (DL) for classification. We do not review the sophisticated methods or frameworks that involve DL for classification, such as online DL and spatial pyramid matching (SPM), but rather, we concentrate on the direct DL-based classification methods. Here, the \u201cso-called direct DL-based method\u201d is the approach directly deals with DL framework by adding some meaningful penalty terms. By listing some representative methods, we can roughly divide them into two categories, i.e. (1) directly making the dictionary discriminative and (2) forcing the sparse coefficients discriminative to push the discrimination power of the dictionary. From this taxonomy, we can expect some extensions of them as future researches.", "creator": "LaTeX with hyperref package"}}}