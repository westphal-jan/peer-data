{"id": "1206.0042", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-May-2012", "title": "Language Acquisition in Computers", "abstract": "This project explores the nature of language acquisition in computers, guided by techniques similar to those used in children. While existing natural language processing methods are limited in scope and understanding, our system aims to gain an understanding of language from first principles and hence minimal initial input. The first portion of our system was implemented in Java and is focused on understanding the morphology of language using bigrams. We use frequency distributions and differences between them to define and distinguish languages. English and French texts were analyzed to determine a difference threshold of 55 before the texts are considered to be in different languages, and this threshold was verified using Spanish texts. The second portion of our system focuses on gaining an understanding of the syntax of a language using a recursive method. The program uses one of two possible methods to analyze given sentences based on either sentence patterns or surrounding words. Both methods have been implemented in C++. The program is able to understand the structure of simple sentences and learn new words. In addition, we have provided some suggestions regarding future work and potential extensions of the existing program.", "histories": [["v1", "Thu, 31 May 2012 22:02:32 GMT  (1470kb,D)", "http://arxiv.org/abs/1206.0042v1", "39 pages, 10 figures and 6 tables"]], "COMMENTS": "39 pages, 10 figures and 6 tables", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["megan belzner", "sean colin-ellerin", "jorge h roman"], "accepted": false, "id": "1206.0042"}, "pdf": {"name": "1206.0042.pdf", "metadata": {"source": "CRF", "title": "Language Acquisition in Computers", "authors": ["Megan Belzner", "Sean Colin-Ellerin", "Jorge H. Roman"], "emails": ["belzner@mit.edu", "seancolinellerin@gmail.com", "jhr@lanl.gov"], "sections": [{"heading": null, "text": "This project examines the nature of language acquisition in computers, guided by techniques similar to those used in children. While the existing methods of processing natural language are limited in scope and comprehension, our system aims to achieve an understanding of the language based on initial principles and thus minimal input. The first part of our system was implemented in Java and focuses on understanding the morphology of language by means of bigrams. We use frequency distributions and differences between them to define and distinguish languages. English and French texts were analyzed to determine a difference threshold of 55 before the texts are considered to be in different languages, and this threshold was verified with Spanish texts. The second part of our system focuses on understanding the syntax of a language by means of a recursive method. The program uses one of two possible methods to analyze given sentences either on the basis of sentence patterns or surrounding words. Both methods were implemented in C + +. The program is able to understand the structure of existing sentences and to make some potential suggestions for future work."}, {"heading": "1 Introduction 3", "text": "1.1. History..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "2 Morphology 11", "text": "2.1 Algorithm..........................................................................................................................."}, {"heading": "3 Syntax 17", "text": "3.1 Algorithm.........................................................................................................................."}, {"heading": "4 Analysis 23", "text": "4.1 Comparison with existing programmes........................................................................................................................................................................................................................................................................................."}, {"heading": "5 Extension 25", "text": "5.1 Other sentence structures................................................................................................"}, {"heading": "6 Conclusion 31", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Sample Texts 34", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Morphology Code 35", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C Syntax Code 37", "text": "Page 2 of 39"}, {"heading": "1 Introduction", "text": "Natural language processing is a broad and diverse sub-area of artificial intelligence. The question of how best to give a computer an intuitive understanding of speech is one with many possible answers, but one that has not yet been satisfactorily answered. Most existing programs work only to a limited extent, and in most cases it is not realistic to say that the computer actually understands the language in question. This project seeks to give a computer a truly intuitive understanding of a particular language by developing methods that allow the computer to learn the language on its own terms with a minimum of external input. We have developed methods to teach the computer both the morphology and syntax of a language, and have made some suggestions on language acquisition at the semantic level."}, {"heading": "1.1 History", "text": "The idea of processing natural language comes from Alan Turing, a British computer scientist who has formulated a hypothetical test known as the \"Turing Test.\" The \"Turing Test\" suggests that the question \"Can machines think?\" can be answered if a computer is indistinguishable from a human in all facets of thinking, such as conversation; object identification based on given characteristics, etc. [1] Turing says that many attempts have been made to develop software to process natural language, particularly through the sound recognition currently used in mobile phones, most competently in the iPhone 4S Siri system. However, most of these programs do not have high-level semantic capabilities, but only a very limited number of operations for which keywords are assigned. For example, the Siri system can send an email or text message."}, {"heading": "1.1.1 Statistical Parsers", "text": "There have also been some more technical, less commercially effective attempts at processing natural language, such as statistical language parsers, which have been extensively developed by many educational institutions. Parsers are a set of algorithms that determine the language parts of words in a given sentence. Currently, parsers use a set of human-analyzed sentences that generate a probability distribution that is then used as a statistical model for parsing other sentences. Stanford University and the University of California Berkeley use probabilistic context-free grammars (PCFG) statistical parsers that are currently used with 86.36% and 88% accuracy, respectively. [4] The different parts of the speech are separated as shown in Figure 1: NN = noun, NP = noun phrase, S = subject, VP = verb phrase, and the other symbols represent more specific parts of the speech."}, {"heading": "1.1.2 Other NLP Programs", "text": "In addition to statistical parsers that only determine the syntax of a sentence, some elementary programs have been written to evaluate the semantics of a given body of text. There is a system called FRUMP that organizes news reports by finding keywords in an article that correspond to a set of scripts, and then assigning the article to a specific category, in page 5 of 39, where it is grouped with other articles of similar content. SCISOR summarizes a given news article by analyzing the events using three different sets of knowledge: semantic knowledge, abstract knowledge, and event knowledge. As the program scans the body of text, certain words trigger different pieces of knowledge that are put together to get the best understanding of that word or sequence of words. The resulting meanings can then be organized and rewritten using similar meanings that are equally balanced between the three sets of knowledge as the original piece of information. Similarly, TOPIC summarizes a given text by separating the nouns and the meanings, and then dividing the meanings by the nouns."}, {"heading": "1.2 Common Language Properties", "text": "Certain elements of language are often used both in the processing of natural language and in the general analysis of language, such as bigrams and recursion, two characteristics that play an important role in this project."}, {"heading": "1.2.1 Bigrams", "text": "A n-gram is a sequence of n letters. The most commonly used forms of n-gram are bigrams and trigrams, because they provide a specific clue to a set of information without marking extremely rare and complex aspects of the topic. A good example of this is the use of bigrams in cryptography. A common method of decoding a message encrypted using a keyword such as the Vigenere cipher encryption is to calculate the spacing in letters between two of the same bigrams to determine the length of the keyword and then the keyword itself [8]. Similarly, if any n-grams are used where n is greater or equal to 4 or even in some cases equal to 3, then the number of the same n-gram for a Gift Page 6 of 39message would be very rare and would make it increasingly difficult to determine the length of the keyword. Similarly, in natural language processing, bigrams are used, which is the understanding of a word based on an unknown set of exrats."}, {"heading": "1.2.2 Recursion", "text": "The principle of recursion is an essential aspect of human language and is considered one of the primary ways in which children learn a language. For a sentence with a certain pattern, a word with a certain part of the speech can be exchanged for another word of the same part of the speech, indicating that the two words have the same part of the speech. For example, given the sentence: \"The boy wears a hat,\" it can be stated that \"the\" and \"one\" are the same part of the speech, by reconstructing the sentence as \"A boy wears a hat.\" In addition, the word \"boy\" can be exchanged for the word \"girl,\" indicating that it is also the same type of word, expanding the lexicon of the child. Moreover, the words of a sentence can remain unchanged while the pattern is changed, introducing a new part of the speech. If we have the sentence \"The boy wears a hat\" or \"A C A B,\" if it is presented as a pattern of parts of the speech, then we can add a part of the speech."}, {"heading": "1.3 Linguistic Interpretation", "text": "There is a basic tripartite chain in the structure of language. Phonetics is the most basic structure formed into meaning by units known as words. Units are then syntactically arranged to form sentences that in turn form a broader meaning, formally called semantics [12]. It is fundamental to learning a language that a computer understands the connections in page 8 of the phonetics-syntax-semantics chain and the structure and calculation of each one, with the exception of phonetics. Phonetics and its formulation can be disregarded because these refer more to the connections of the brain with external sounds than to the core structure of language. In fact, the entire outer world can be ignored, since there is no need to draw conclusions between external objects in its human sense perception and its representation as language formally known as pragmatics or in Chomsky's linguistics as e-language."}, {"heading": "1.4 Project Definition", "text": "This project explores the nature of language acquisition in computers by applying techniques similar to those used by children for language acquisition. We have focused mainly on morphology and syntax, and have developed methods that allow a computer to acquire knowledge of these aspects of the language. We have developed programs in both C + + and Java. With regard to morphology, the program is able to analyze the word structure of certain languages and distinguish between languages in different text examples using bigram frequencies, and we have explored the usefulness and limitations of this method in the context of existing methods.With this technique, we have developed computationally comprehensible definitions of English, French and Spanish morphologies. We have also described a novel technique for understanding the syntax of a language and have fundamentally implemented it on page 9 of 39, using a minimum of input and recursive methods to learn both approximate meanings of words and valid sentence structures."}, {"heading": "2 Morphology", "text": "To analyze the morphology of a particular language, bigrams can be used to define and compare languages. As the frequency distribution of a group of bigrams is unique for a particular language (via a sufficiently large sample text), this can be used with minimal effort as a precise identifier of a language."}, {"heading": "2.1 Algorithm", "text": "The program was originally developed in C + + and then translated to Java to take advantage of non-standard characters, and consists essentially of two parts. The first step is to generate a table of frequency values from a file, and the second step is to compare the two tables and set the degree of similarity. To create a frequency table, the program will generate a two-dimensional numerical array from the set of valid characters, so that the array contains a space for each possible bigram, setting the initial value for each word found to zero. For each word from the input file, the program checks each pair of characters and inserts the corresponding position in the frequency table. Once the end of the file is reached, each frequency number is divided by the total number of found bigrams to specify a percentage frequency. This process is shown in Figure 2.This results in an array similar to Table 1, which can be analyzed separately to distinguish between specific frequencies for one language and another, and to specify an unusual language."}, {"heading": "2.2 Limitations", "text": "However, this method has certain limitations. As the program is a bigram (although it can be easily tricked into using n-grams for all n greater than 1), words with only one letter are not taken into account. While this does not have a large overall effect, it does lead to some inaccuracies in analyzing the frequencies for a particular language. A more significant limitation is the requirement that all \"legal\" characters must be defined before the program is run. Although it would be relatively easy to dynamically determine the character set based on the input files, this creates problems where the fonts are not the same for each file, making it difficult, if not impossible, to accurately compare the two files."}, {"heading": "2.3 Results", "text": "Page 13 of 39The program was conducted using a series of files in English, French and Spanish as a whole. Initial frequency tables for analyzing each language were created using the EU Charter in each language [13], creating the frequency distributions in Figures 4, 5 and 6 for English, French and Spanish, respectively. These frequency charts show that each language has a handful of extremely common bigrams (in addition to some that occur little or no at all), but in English this includes \"th\" and \"er\" with percentage frequencies of 2.9 and 2.8 words, respectively, along with \"on\" and \"ti,\" both above 2.5%. These data are slightly distorted by the text used, although \"th\" and \"er\" are in fact the most common bigrams in English. A study conducted with a sample of 40,000 words [14] gave the two frequencies of 1.5 and 1.3, respectively, although the nightly text is not as common as in the example."}, {"heading": "3 Syntax", "text": "To analyze the syntax of a language, C + + implements a \"recursive learning method.\" Since the program would ideally require an absolute minimum of initial information, this method takes a small set of initial words and builds on it by alternately using known words to learn new sentence structures, and using familiar sentence structures to learn new words, as seen below. {cat, man, has, has} \u2192 \"The man has a cat\" \u2192 \"The x y a z\" \u2192 \"The man wore a hat\" \u2192 {cat, man, has, wear, hat}"}, {"heading": "3.1 Algorithm", "text": "There are two main elements to understand the recursive learning system < < < < p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p p > p > p p p p p > p > p > p p > p > p p > p p > p > p p p p p p > p p p p p > p p p p p p > p p p p p p p"}, {"heading": "3.2 Limitations", "text": "With proper development, this method could be used to learn many types of sentences. However, it still has some limitations. Although it is sufficient for simpler grammatical constructs, complicated words and patterns could cause confusion. In particular, words that have multiple meanings with different parts of the language would confuse the program on page 19 of 39 enormously, and currently it also has no way to combine related words (such as plural or possessive forms of nouns). Furthermore, this method requires carefully crafted training sets. Although, unlike most existing systems, it can work with a minimum of initial information, it still needs to learn the constructs somewhat sequentially and avoid introducing too many new concepts at once. This could be partially remedied by implementing a method that stores sentences that the program does not yet have the tools to analyze them later, so that a more general text could be used as training material."}, {"heading": "3.3 Results", "text": "To prove the concept of these methods, a series of three training sentences were entered and analyzed by the program to learn a handful of new words and concepts. Although the program begins with far less information than many existing programs, it still requires initial input - in this case, three words that together with \"a\" and \"those\" form the opening sentence, as in Table 3. The first sentence entered into the program is \"the man has a hat,\" which is analyzed with the word-based second method. From here, two new words are learned - \"a\" and \"those\" that are defined on the basis of the surrounding words. The sentence pattern is also catalogued, and the familiar information reads as in Table 4. Presently, although \"that\" only needs to appear before a noun, \"it is assumed that one\" page 20 of 39 previous verbs are required. \"To define\" a \"correctly, the next sentence is\" one man has the hat. \"While the two known grammatical particles\" invert the \"the program,\" the two is the two-word."}, {"heading": "4 Analysis", "text": "In this project, we have developed methods that allow a computer to understand and learn both the morphology and syntax of a language. Using novel techniques or applications, we have designed and implemented these methods and their ability to learn the language. Although the use of bigrams in language analysis is not a new idea, we have implemented them in a novel way by working on it as a defining quality and learning mechanism for the processing of natural language.The method proves to be very useful, if only to a certain extent, for understanding the morphology of a language. It is extremely effective when it uses a sufficiently large sample text, but for smaller sample texts it is no longer able to accurately compare language.Thus, although it creates a computationally effective \"definition\" of a language, its actual ability as a learning mechanism is limited. Used in combination with other methods, the Bigram method may prove to be extremely powerful. The recursive learning method, which has a very high level of understanding that can be implemented for both a very large and very small sentence."}, {"heading": "4.1 Comparison to Existing Programs", "text": "The use of bigrams to understand and analyze different parts of a given language has essentially been studied and implemented. For example, there are programs that calculate the frequency of bigrams to assess the morphology of a language. However, unlike our program, no one has yet used the differences in bigram frequencies between two languages to distinguish one language from the next. Page 23 of 39The system in the program used to determine the parts of the language in a sentence has rarely been tried, and when it has been used, only partially and in conjunction with other methods. Most natural language processing programs have been designed to be as efficient and effective as possible. As a result, many large databases of initial data that the program then analyzes and uses for subsequent input are used. As previously discussed, the most common and successful programs of this type are statistical parsers. On the contrary, our program uses the recursive principle to learn new vocabulary and forms of syntax for a given language, reciting only a very small initial set of data."}, {"heading": "5 Extension", "text": "The program in its current form can learn and understand a number of language elements, including morphology and simple sentence patterns. However, there is considerable scope for further research, both by developing techniques for learning syntax to allow a more complete range of sentence patterns, and by developing methods for a computational understanding of semantics."}, {"heading": "5.1 Other Sentence Structures", "text": "In fact, most of them will be able to play by the rules they have given themselves. \"That's the most important thing for us,\" he says. \"We have to play by the rules,\" he says. \"We have to play by the rules,\" he says. \"We have to play by the rules we have given ourselves.\""}, {"heading": "5.2 Semantics", "text": "In fact, most people who are able to survive themselves are not able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I believe that we will be able to change the world, and that we will be able to change the world, and that we will be able to change the world. \""}, {"heading": "6 Conclusion", "text": "We have developed techniques that allow a computer to learn and analyze the morphology of any language, thus understanding differences between two languages. We have also developed a recursive learning system for understanding sentence patterns and constructs that uses a minimum of initial information. Currently, the program can interpret many basic sentences and we have also provided opportunities and suggestions to expand the capabilities of the program. This approach is unique compared to conventional systems for processing natural language due to this lack of significant input and its recursive design, and could have great potential in the field of processing natural language.page 31 of 39"}, {"heading": "A Sample Texts", "text": "The following sample texts were used for testing: http: / / www.europarl.europa.eu / charter / text _ en.pdf) French EU Charter (available at http: / / www.europarl.pdf) French EU Charter (available at http: / / www.europarl.europa.eu / charter / pdf / text _ es.pdf) English Wikipedia article on \"Philosophy\" (available at http: / / en.wikipedia.org / wiki / Philosophy) English Wikipedia article on \"Encyclopedias\" (available at http: / en.wikipedia.org / Wikipedia \"Wikipedia\".wikipedia.org / Wikipedia \"Wikipedia\".wikipedia.org \"English Wikipedia article on\" Encyclopedia.org \"(available at http: / en.wikipedia.org / en.wikipedia.org) English Wikipedia article on\" Encyclopedia \"Wikipedia\" Wikipedia \"Wikipedia\" Wikipedia \"Wikipedia\" Wikipedia \"(available at http: / en.wikipedia.org / en.wikipedia.org / en.wikipedia.org)"}, {"heading": "C Syntax Code", "text": "To begin with, types must be created and initial input defined, and the sentence must be divided into its array; Array (Array) = (Array); Array (Array) = (Array); Array (Array) = (Array); Array (Array) = (Array); Array (Array) = (Array); Array (Array) = (Array); Array (Array) = (Array) = (Array); Array (Array) = (Array); Array (Array) = (Array) = Array (Array); Array (Array) = (Array); (=) ((); (=) ((); (=) ((); (=); (=) (); (=) (=); (=) (=); (=) (=); (=) (=); (=) (=) (=); (=) (=) (=) (=); (=) (=) (=) (=) (=); (=) (=) (=) (=) (=) (=) (=); (=) (=) (=) (=) (=); (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (=) (= (=) (=); (=) (=) (=); (=) (=) (=) (=) (= (= (=); (=); (=) (= (=); (=) (=); (=) (=) (=) (=) (=) (=); (=) (= (=) (= (=) (=); (=); (=); (=); (=) (=) (=) (= (= (=); (= (=); (=); (=); (=) (=) (=) (="}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "This project explores the nature of language acquisition in computers, guided by<lb>techniques similar to those used in children. While existing natural language processing<lb>methods are limited in scope and understanding, our system aims to gain an under-<lb>standing of language from first principles and hence minimal initial input. The first<lb>portion of our system was implemented in Java and is focused on understanding the<lb>morphology of language using bigrams. We use frequency distributions and differences<lb>between them to define and distinguish languages. English and French texts were an-<lb>alyzed to determine a difference threshold of 55 before the texts are considered to be<lb>in different languages, and this threshold was verified using Spanish texts. The second<lb>portion of our system focuses on gaining an understanding of the syntax of a language<lb>using a recursive method. The program uses one of two possible methods to analyze<lb>given sentences based on either sentence patterns or surrounding words. Both methods<lb>have been implemented in C++. The program is able to understand the structure of<lb>simple sentences and learn new words. In addition, we have provided some suggestions<lb>regarding future work and potential extensions of the existing program. 1<lb>ar<lb>X<lb>iv<lb>:1<lb>20<lb>6.<lb>00<lb>42<lb>v1<lb>[<lb>cs<lb>.C<lb>L<lb>]<lb>3<lb>1<lb>M<lb>ay<lb>2<lb>01<lb>2 Language Acquisition in Computers<lb>Megan Belzner and Sean Colin-Ellerin", "creator": "LaTeX with hyperref package"}}}