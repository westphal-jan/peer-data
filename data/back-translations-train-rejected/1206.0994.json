{"id": "1206.0994", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Apr-2012", "title": "An Optimization Framework for Semi-Supervised and Transfer Learning using Multiple Classifiers and Clusterers", "abstract": "Unsupervised models can provide supplementary soft constraints to help classify new, \"target\" data since similar instances in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place, as in transfer learning settings. This paper describes a general optimization framework that takes as input class membership estimates from existing classifiers learnt on previously encountered \"source\" data, as well as a similarity matrix from a cluster ensemble operating solely on the target data to be classified, and yields a consensus labeling of the target data. This framework admits a wide range of loss functions and classification/clustering methods. It exploits properties of Bregman divergences in conjunction with Legendre duality to yield a principled and scalable approach. A variety of experiments show that the proposed framework can yield results substantially superior to those provided by popular transductive learning techniques or by naively applying classifiers learnt on the original task to the target data.", "histories": [["v1", "Fri, 20 Apr 2012 01:58:40 GMT  (3895kb)", "http://arxiv.org/abs/1206.0994v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ayan acharya", "eduardo r hruschka", "joydeep ghosh", "sreangsu acharyya"], "accepted": false, "id": "1206.0994"}, "pdf": {"name": "1206.0994.pdf", "metadata": {"source": "CRF", "title": "An Optimization Framework for Semi-Supervised and Transfer Learning using Multiple Classifiers and Clusterers", "authors": ["Ayan Acharya", "Eduardo R. Hruschka", "Joydeep Ghosh", "Sreangsu Acharyya"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 120 6.09 94v1 [cs.LG] 2 0A pr2 01201"}, {"heading": "An Optimization Framework for Semi-Supervised and Transfer Learning using Multiple Classifiers and Clusterers", "text": "Ayan Acharya, University of Texas at Austin, USA Eduardo R. Hruschka, University of Texas at Austin, USA; University of Sao Paulo at Sao Carlos, Brazil Joydeep Ghosh, University of Texas at Austin, USA Sreangsu Acharyya, University of Texas at Austin, USA Unsupervised models can be supplemented soft constraints to help classify new, \"target\" data since similar cases in the target set are likely to share the same class label. Such models can also help identify possible differences between education and target allocations, which is useful in applications where concept drift can occur, such as in Transfer Learning Settings. This paper describes a general optimization framework that takes as input class membership estimates from existing classifiers who have learned from previously encountered \"source data,\" as well as a similarity matrix from a cluster group based solely on the target data to be classified."}, {"heading": "1. INTRODUCTION", "text": "In several data mining applications, ranging from the identification of different control systems in complex facilities to the characterization of different types of stocks in terms of price and volume movements, an initial classification model must be built that must be applied to unmarked data. As the statistics of the underlying phenomena often change over time, these classifiers also need to be rebuilt occasionally when performance exceeds an acceptable level. In such sit-This work has been supported by NSF grants (IIS-1016614) and by the Brazilian research agencies FAPESP and CNPq. The addresses of the author are: A. Acharya, Department of Electrical and Computer Engineering, University of Texas at Austin. Permission to make digital or hard copies of part or all of this work is granted for personal or classroom use provided that copies are not made or distributed."}, {"heading": "2. RELATED WORK", "text": "This paper uses the theory of classification and clustering to solve transfer and semi-supervised learning problems, and the underlying optimisation system inherits properties from the alternative way of optimising algorithms. In this section, a brief introduction to each of these different areas of research is offered. Combining several single and base classifiers to generate a more powerful ensemble system has been an active area of research for the past two decades. [Kunz 2004; Oza and Tumer 2008] provide both theoretical results and empirical evidence of the usefulness of such approaches for solving difficult classification problems, such as an analytical framework for mathematically quantifying improvements in classification outcomes by combining several models in [Tumer and Ghosh 1996]. An investigation of traditional real-world ensembles - including their applications such as remote sensing, person recognition and medicine - is presented in [Oza and Tumer 2008]."}, {"heading": "3. DESCRIPTION OF OAC3", "text": "The proposed framework, which combines classifiers and clusters to produce a more consolidated classification, is presented in Figure 1. It is assumed that a set of classifiers (consisting of one or more classifiers) was previously induced from a training set. Such classifiers may have been derived from labeled and unlabeled data, and they are part of the framework used to classify new data - i.e., instances of ACM transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Release Date: April 2012. the target X = {xi} ni = 1. The target sentence is a test set not used to build the classifiers. The classifiers are used to estimate initial class probabilities for each instance xi-X-X. These probability distributions are stored as a set of vectors representing the ACS-X constellation."}, {"heading": "3.1. Optimization Algorithm \u2014 OAC3", "text": "Consider that r1 (r1) classifiers, indexed by q1, and r2 (r2) clusters, indexed by q2, are used to obtain a consolidated classification, and the following steps (I-III) outline the proposed approaches. Steps I and II can be considered preliminary steps to obtain the input factors for OAC3, while Step III is the optimization algorithm that will be discussed in more detail. 1C, with an overload of notation, is used here to denote a collection of classes and should not be confused with Ck, which is used to determine the smoothness of a functionality. ACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Release Date: April 2012.Step I - Obtain input from classifiers q1 for instance xi is a dimensional class vector (q1) i i i i i i."}, {"heading": "3.2. Time Complexity Analysis of OAC3", "text": "In view of the fact that a trained ensemble of classifiers is available, the calculation of the vector set {\u03c0i} ni = 1 O (nr1k), where n is the number of instances in the target group, r1 is the number of components of the classifier ensemble, and k is the number of class names. The calculation of the similarity matrix S is O (r2n2), where r2 is the number of components of the cluster ensemble. Finally, the calculation costs (per iteration) of OAC3 are O (kn2), where r2 is the number of components of the cluster ensemble. In fact, the calculation bottleneck of OAC3 is not the optimization algorithm itself, whose main steps (1 and 2) can be parallelized (this can be determined by a careful examination of Equation (6) and (10)), but the calculation of the similarity matrix itself."}, {"heading": "4. CONVERGENCE ANALYSIS OF OAC3", "text": "We claim that the objective J in Eq. 4 leads to some uniqueminimizer deviating functions when Bregman uses deviations from the following properties as loss functions: (a) d\u03c6 (p, q) is strictly convergent in p and q separately. (b) d\u03c6 (p, q) is completely inconsistent in p and q. (e) If the level sets (pt, qt) \u2192 0 and qt, then pt \u2192 qt \u2192 pt and qt \u2192 pt. (f) If p) S and qt \u2192 p, then they are equal. (p, qt) \u2192 0.Bregman deviations that satisfy the above properties include a large number of useful functions such as the known loss."}, {"heading": "5. ANALYSIS OF RATE OF CONVERGENCE FOR OAC3", "text": "In practical applications, the convergence rate of all optimization algorithms is of great importance. To analyze them, we use some formulations derived in [Bezdek and Hathaway 2003] to characterize the local convergence rate of alternating minimization types of algorithms in general. In this section, we first explain the tools and then show that the analysis is seamlessly applicable to the objective function J. However, the details of the tools are skipped here and only the most important terms and theorems are provided."}, {"heading": "5.1. Tools for Analyzing Local Rate of Convergence", "text": "\u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "5.2. Hessian Calculation of J", "text": "From the theorems and lemmata presented in the previous subsection, it can be deduced that the Hessian of the object that is positive is a critical condition. Therefore, we will try to show that this 2J is positive for some of the Bregman divergences. According to Equation (4), the Hessian includes the following terms: \"y (l) iJ,\" \"n,\" \"j,\" \"j,\" \"y (r),\" y (r) j \"),\" i \",\" i, \"\" \"i,\" \"i\" i, \"\" \"i\" i, \"\" \"\" i."}, {"heading": "5.3. Hessian Calculation for KL and Generalized I divergence", "text": "We are now able to show that the Hessian of target J is clearly positive if KL or I-divergence is used as Bregman-divergence. Remember Table I that the generating functions \u03c6 (.)'s differ for KL and I-divergence only by a linear term and therefore the Hessian of target J would be equal for these two cases. We list here different terms of Hessian: 2y (r) i, y (l) iJ = n j = 1; j 6 = isij + \u03bb diag (((1 / y (l) i) k 1) (16) 2y (r) j, y (r) jJ = diag n i = 1; j 6 = isijy (l) iqy (l) iJ = n j = 1; j y (r) j2 (l) j2 (l) i (l) j (l) k 1) y y (y y (j) y (j) y (1 j jy y y (y) y (1 j-j j j (l) y y (1 jy y y y y (y) y (1 j-j j j (l) y y (1) y (j j j j j j j (l) y (1 j j y y y (j) y (1 j j j y y y (l) y (1 j j j j j j (l) y (1 j j j j j y y (l) y (1 j j j j j j y y (l) y (1 j j j j j j j y y (l) j j j j j (1) j j j j j j j j (1 (1 j j y y y y (1) j j j j j j (1) j j j j j j j (1 (1 j y y y y (1) j j j j j j y (1) j j j j j j j j (1 (1) j j j j j j j (1 (l) j y y (1 (1) j j j j j j j y y y (1 (1) j y y y y (1) j j j j j j j (1 (1) j j j j j j (1 (1 (l) j j j"}, {"heading": "5.4. Convergence Rate of OAC3 with KL and I-divergence", "text": "From Lemma A.1 we have that J is strictly convex in common and therefore has a unique minimizer. From the same Lemma J is separately strictly convex w.r.t for each of its arguments. Therefore, with other variables fixed to a certain value, a unique minimizer w.r.t has a certain variable. Therefore, all the conditions for J mentioned in Lemma 5.1 are strictly convex w.r.t in its entire range. Therefore, according to Theorem 5.4, we can conclude that J uses a unique minimizer w.r.t with other variables fixed to a certain value."}, {"heading": "6. EXPERIMENTAL EVALUATION", "text": "First, we provide a simple pedagogical example that illustrates how the supplementary constraints of cluster algorithms can be useful in improving the generalization capability of classifiers. Section 6.2 reports on sensitivity analyses of the OAC3 parameters. Then, in Section 6.3, we compare the performance of OAC3 with the recently proposed BGCM [Gao et al. 2009; Gao et al. 2011]. This comparison is straightforward and fair as it uses the same data sets as well as the same results of the base models kindly provided by the authors of this paper. Compared to other semi-monitored methods, the design space is much larger, as we are now faced with a variety of classification and cluster algorithms from which we can select as base models in OAC3, as with a variety of semi-monitored methods with which we can compare. Given the space available, in Section 6.4, we use simple (basic) 2006 Sempettings Sempettings Vi-Vi-Vemptor."}, {"heading": "6.1. Pedagogical Example", "text": "Consider the two-dimensional dataset known as Half-Moon, which has two classes, each of which is represented by 400 instances. From this dataset, 2% of the instances are used for training, while the remaining instances are used for testing (target). A classifier ensemble is selected, which is made up of three known classifiers (Decision Tree, Linear Discriminant, and Generalized Logistic Regression). To obtain a cluster ensemble, a uniform (hierarchical) cluster algorithm is selected. The cluster ensemble is then obtained from five data partitions represented in the dendrogram, which are intersected for different numbers of clusters (from 4 to 8). Figure 2 shows the target data class names derived from the standalone use of the classifier ensemble, while Figure 3 shows the corresponding results achieved by OAC3. Parameter values were set by AC3."}, {"heading": "6.2. Sensitivity Analysis", "text": "We perform a sensitivity analysis of the OAC3 parameters using the same classification data sets used in [Gao et al. 2009]. These data sets represent eleven classification tasks across three real-world application areas (20 newsgroups, Cora and DBLP). There are six data sets (News1 - News6) for 20 newsgroups and four data sets (Cora1 - Cora4) for Cora. In each task there is a target set on which the class labels should be predicted. In [Gao et al. 2009] two monitored models and two unmonitored models were used to obtain class and cluster labels (on the target groups), respectively. TheseACM Transactions on Knowledge Discovery from Data, Vol. 2, No. 4, Article 01, Release Date: April 2012.The same class and cluster labels are used as inputs to OAC3."}, {"heading": "6.3. Comparison with BGCM", "text": "As discussed in section 2, BGCM did not perform the algorithm because the raw data is not needed for availability. We evaluate OAC3 on the same data basis as in April 2004. We evaluate OAC3 on the same data basis as in April 2004. We evaluate OAC3 on the same data basis as in April 2010, using two verified models (M1 and M2) and two unsupervised models (M3 and M4) to obtain class and cluster names (at the target groups), each taking into account the results of two verified models (M1 and M2) and two unattended models (M3 and M4). These same labels are used as inputs to OAC3. Comparisons are made between OAC3 and BGCM, which were trained in the same data bases 4. In other words, bothOAC3 andBGCM receive the4For these datasets, comparisons with raw S3Vi and Kethi data are not provided."}, {"heading": "6.4. Comparison with S3VM", "text": "This algorithm is essentially a Transductive Linear Support Vector Machine (SVM), which can be considered a large-scale implementation of the algorithm introduced in [Joachims 1999b]. To handle unlabeled data, an additional term is appended to the SVM lens function, whose role is to push the classification hyper level toward low data density regions [Sindhwani and Keerthi 2006]. Default parameter values were chosen for S3VM. Six sets of data are used in our experiments: Half Moon function (see Section 6.1), Circles (this is a synthetic dataset that contains two-dimensional instances forming two concentric circles - one for each class), and four datasets from the library supporting Vector Machines5."}, {"heading": "6.5. Transfer Learning", "text": "In fact, it is one of the greatest challenges in the history of mankind that has been tackled in recent years."}, {"heading": "7. CONCLUDING REMARKS", "text": "The optimization algorithm assumes updates in a closed form, facilitates parallelization and is therefore extremely convenient to handle data on a large scale - especially with a linear convergence rate. The evidence for convergence is fairly new and generalizes a variety of Bregman divergences, facilitating appropriate divergence measurement based on the application domain and subsuming many other existing graph-based semi-monitored learning algorithms as special cases. It has been shown that the proposed framework empirically outperforms a variety of algorithms [Gao et al. 2011; Sindhwani and Keerthi 2006; Gao et al. 2008] in both semi-monitored and transferred learning problems. There are few aspects that can be further investigated. For example, the impact of the number of classifiers and clusters in OAC3 merits further investigation."}, {"heading": "A. PROOFS FOR CONVERGENCE OF OAC3", "text": "LEMMA A.1. The objective function J used in Eq. (4) is separate and common y (J) y (w) strictly convex over Sn \u00b7 Sn. Also J is common semi-continuous w.r.t y (l) and y (r).PROOF. (a) From the property (a) (a) in Section 4 it can be seen that J strictly convex w.r.t. y (l) and y (r) are separate. From the same property the first term f1 (r) = n (n) can be represented. i = 1dp (i = 1dp) (r) (r) i (r) i (r) i) in J strictly convex w.r.t. y (r) and 3. Terms in the ob-objective function can be represented collectively by f2 (l), y (r), y (r). This function is common convex by property (b), but not necessarily convex."}, {"heading": "B. PROOF FOR ANALYSIS OF RATE OF CONVERGENCE", "text": "LEMMA B.1. H = 2J = 2J = 1k = 1k = 1p = 1p = 1p = 1p = 1p = 1p = 0 when KL or generalized I-divergence is used as Bregman divergence. PROOF. Let's assume that z = (y (l) i = 1, (y (r) i = 1). Now z \u2020 Hz (37) = n (l) i = 1y (l) i, y (l) iy (l) i (l) i \u2212 siy (l) i = 1y (r) j \u2212 y (r) j (r) jy (r) j + 2n i, j = 1; i6 = jy (l) i \u2020 y (l) i (l) i (l) i \u2212 siy (l) i, y (r) jy (jy) jy (l) jj = 1p = 1p = 1p = 1p (1p) jj = 1p = 1p = 1p = 1p (1p) (1p = 1p)."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank Luiz F. S. Coletta for conducting the experiments described in Section 6.2, and Ambuj Tewari and Ali Jalali for pointing us to relevant literature to analyze the convergence rate of the optimization framework."}], "references": [{"title": "C3E: A Framework for Combining Ensembles of Classifiers and Clusterers", "author": ["A. ACHARYA", "E.R. HRUSCHKA", "J. GHOSH", "S. ACHARYYA"], "venue": "MCS. 269\u2013278.", "citeRegEx": "ACHARYA et al\\.,? 2011", "shortCiteRegEx": "ACHARYA et al\\.", "year": 2011}, {"title": "Clustering with Bregman Divergences", "author": ["A. BANERJEE", "S. MERUGU", "I.S. DHILLON", "J. GHOSH"], "venue": "JMLR 6, 1705\u20131749.", "citeRegEx": "BANERJEE et al\\.,? 2005", "shortCiteRegEx": "BANERJEE et al\\.", "year": 2005}, {"title": "On Manifold Regularization", "author": ["M. BELKIN", "P. NIYOGI", "V. SINDHWANI"], "venue": "AISTAT.", "citeRegEx": "BELKIN et al\\.,? 2005", "shortCiteRegEx": "BELKIN et al\\.", "year": 2005}, {"title": "Label Propagation and Quadratic Criterion", "author": ["Y. BENGIO", "O. DELALLEAU", "N. LE ROUX"], "venue": "SemiSupervised Learning, O. Chapelle, B. Sch\u00f6lkopf, and A. Zien, Eds. MIT Press, 193\u2013216.", "citeRegEx": "BENGIO et al\\.,? 2006", "shortCiteRegEx": "BENGIO et al\\.", "year": 2006}, {"title": "Some notes on alternating optimization", "author": ["J. BEZDEK", "R. HATHAWAY"], "venue": "Advances in Soft Computing AFSS 2002, N. Pal and M. Sugeno, Eds. Lecture Notes in Computer Science Series, vol. 2275. Springer Berlin / Heidelberg, 187\u2013195.", "citeRegEx": "BEZDEK and HATHAWAY,? 2002", "shortCiteRegEx": "BEZDEK and HATHAWAY", "year": 2002}, {"title": "Convergence of alternating optimization", "author": ["J.C. BEZDEK", "R.J. HATHAWAY"], "venue": "Neural, Parallel Sci. Comput. 11, 4, 351\u2013368.", "citeRegEx": "BEZDEK and HATHAWAY,? 2003", "shortCiteRegEx": "BEZDEK and HATHAWAY", "year": 2003}, {"title": "On-line algorithms in machine learning", "author": ["A. BLUM"], "venue": "Online Algorithms: The State of the Art, Fiat and Woeginger, Eds. LNCS Vol.1442, Springer.", "citeRegEx": "BLUM,? 1998", "shortCiteRegEx": "BLUM", "year": 1998}, {"title": "Knowledge transfer mechanisms for characterizing image datasets", "author": ["K.D. BOLLACKER", "J. GHOSH"], "venue": "Soft Computing and Image Processing. Physica-Verlag, Heidelberg.", "citeRegEx": "BOLLACKER and GHOSH,? 2000", "shortCiteRegEx": "BOLLACKER and GHOSH", "year": 2000}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["S. BOYD", "N. PARIKH", "E. CHU", "B. PELEATO", "J. ECKSTEIN"], "venue": "Tech Report.", "citeRegEx": "BOYD et al\\.,? 2011", "shortCiteRegEx": "BOYD et al\\.", "year": 2011}, {"title": "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming", "author": ["L.M. BREGMAN"], "venue": "USSR Computational Mathematics and Mathematical Physics 7, 3, 200 \u2013 217.", "citeRegEx": "BREGMAN,? 1967", "shortCiteRegEx": "BREGMAN", "year": 1967}, {"title": "A simultaneous learning framework for clustering and classification", "author": ["W. CAI", "S. CHEN", "D. ZHANG"], "venue": "Pattern Recognition 42, 1248\u20131259.", "citeRegEx": "CAI et al\\.,? 2009", "shortCiteRegEx": "CAI et al\\.", "year": 2009}, {"title": "Multitask learning", "author": ["R. CARUANA"], "venue": "Mach. Learn. 28, 41\u201375.", "citeRegEx": "CARUANA,? 1997", "shortCiteRegEx": "CARUANA", "year": 1997}, {"title": "Parallel Optimization: Theory, Algorithms and Applications", "author": ["Y.A. CENSOR", "S.A. ZENIOS"], "venue": "Oxford University Press.", "citeRegEx": "CENSOR and ZENIOS,? 1997", "shortCiteRegEx": "CENSOR and ZENIOS", "year": 1997}, {"title": "Semi-Supervised Learning", "author": ["O. CHAPELLE", "B. SCH\u00d6LKOPF", "A. ZIEN"], "venue": "MIT Press.", "citeRegEx": "CHAPELLE et al\\.,? 2006", "shortCiteRegEx": "CHAPELLE et al\\.", "year": 2006}, {"title": "Semi-supervised classification based on clustering ensembles", "author": ["S. CHEN", "G. GUO", "L. CHEN"], "venue": "Proc. of AICI \u201909. Springer-Verlag, 629\u2013638.", "citeRegEx": "CHEN et al\\.,? 2009", "shortCiteRegEx": "CHEN et al\\.", "year": 2009}, {"title": "Proximity maps for convex sets", "author": ["W. CHENEY", "A.A. GOLDSTEIN"], "venue": "Proceedings of the American Mathematical Society 10, 3, pp. 448\u2013450.", "citeRegEx": "CHENEY and GOLDSTEIN,? 1959", "shortCiteRegEx": "CHENEY and GOLDSTEIN", "year": 1959}, {"title": "On information regularization", "author": ["A. CORDUNEANU", "T. JAAKKOLA"], "venue": "UAI. 151\u2013158.", "citeRegEx": "CORDUNEANU and JAAKKOLA,? 2003", "shortCiteRegEx": "CORDUNEANU and JAAKKOLA", "year": 2003}, {"title": "Information geometry and alternatingminimization procedures", "author": ["I. CSISZ\u00c1R", "G. TUSN\u00c1DY"], "venue": "Statistics aand Decisions, Supplement Issue 1, 1, 205\u2013237.", "citeRegEx": "CSISZ\u00c1R and TUSN\u00c1DY,? 1984", "shortCiteRegEx": "CSISZ\u00c1R and TUSN\u00c1DY", "year": 1984}, {"title": "Co-clustering based classification for out-of-domain documents", "author": ["DAI W.", "XUE G.", "YANG Q.", "YU", "Y."], "venue": "Proc. of KDD. New York, NY, USA, 210\u2013219.", "citeRegEx": "W. et al\\.,? 2007", "shortCiteRegEx": "W. et al\\.", "year": 2007}, {"title": "Statistical comparison of classifiers over multiple data sets", "author": ["J. DEMSAR"], "venue": "Journal of Machine Learning Research 7, 7, 1\u201330.", "citeRegEx": "DEMSAR,? 2006", "shortCiteRegEx": "DEMSAR", "year": 2006}, {"title": "On em-like algorithms for minimum distance estimation", "author": ["P. EGGERMONT", "V. LARICCIA"], "venue": "Unpublished manuscript, University of Delaware.", "citeRegEx": "EGGERMONT and LARICCIA,? 1998", "shortCiteRegEx": "EGGERMONT and LARICCIA", "year": 1998}, {"title": "Solving cluster ensemble problems by bipartite graph partitioning", "author": ["X. FERN", "C. BRODLEY"], "venue": "Proc. of ICML. 281\u2013288.", "citeRegEx": "FERN and BRODLEY,? 2004", "shortCiteRegEx": "FERN and BRODLEY", "year": 2004}, {"title": "Collaborative clustering with background knowledge", "author": ["G. FORESTIER", "P. GAN\u00c7ARSKI", "C. WEMMERT"], "venue": "Data Knowl. Eng. 69, 211\u2013228.", "citeRegEx": "FORESTIER et al\\.,? 2010", "shortCiteRegEx": "FORESTIER et al\\.", "year": 2010}, {"title": "Knowledge transfer via multiple model local structure mapping", "author": ["GAO J.", "FAN W.", "JIANG J.", "HAN", "J."], "venue": "Proc. of KDD. 283\u2013291.", "citeRegEx": "J. et al\\.,? 2008", "shortCiteRegEx": "J. et al\\.", "year": 2008}, {"title": "Graph-based consensus maximization among multiple supervised and unsupervised models", "author": ["GAO J.", "LIANG F.", "FAN W.", "SUN Y.", "HAN", "J."], "venue": "Proc. of NIPS. 1\u20139.", "citeRegEx": "J. et al\\.,? 2009", "shortCiteRegEx": "J. et al\\.", "year": 2009}, {"title": "A graph-based consensus maximization approach for combining multiple supervised and unsupervised models", "author": ["GAO J.", "LIANG F.", "FAN W.", "SUN Y.", "HAN", "J."], "venue": "IEEE Transactions on Knowledge and Data Engineering accepted for publication.", "citeRegEx": "J. et al\\.,? 2011", "shortCiteRegEx": "J. et al\\.", "year": 2011}, {"title": "Cluster ensembles", "author": ["J. GHOSH", "A. ACHARYA"], "venue": "WIREs Data Mining and Knowledge Discovery 1, 1\u201312.", "citeRegEx": "GHOSH and ACHARYA,? 2011", "shortCiteRegEx": "GHOSH and ACHARYA", "year": 2011}, {"title": "Convergence theorems for generalized alternating minimization procedures", "author": ["A. GUNAWARDANA", "W. BYRNE"], "venue": "J. Mach. Learn. Res. 6, 2049\u20132073.", "citeRegEx": "GUNAWARDANA and BYRNE,? 2005", "shortCiteRegEx": "GUNAWARDANA and BYRNE", "year": 2005}, {"title": "Making large-scale SVM learning practical", "author": ["T. JOACHIMS"], "venue": "Advances in Kernel Methods: Support Vector Learning, C. B. B. Scholkopf and A. Smola, Eds. MIT Press, 169\u2013184.", "citeRegEx": "JOACHIMS,? 1999a", "shortCiteRegEx": "JOACHIMS", "year": 1999}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. JOACHIMS"], "venue": "Proc. of ICML. 200\u2013209.", "citeRegEx": "JOACHIMS,? 1999b", "shortCiteRegEx": "JOACHIMS", "year": 1999}, {"title": "Transductive learning via spectral graph partitioning", "author": ["T. JOACHIMS"], "venue": "Proceedings of the Twentieth International Conference on Machine Learning (ICML-2003).", "citeRegEx": "JOACHIMS,? 2003", "shortCiteRegEx": "JOACHIMS", "year": 2003}, {"title": "Best-bases feature extraction algorithms for classification of hyperspectral data", "author": ["S. KUMAR", "J. GHOSH", "M.M. CRAWFORD"], "venue": "IEEE TGRS 39, 7, 1368\u201379.", "citeRegEx": "KUMAR et al\\.,? 2001", "shortCiteRegEx": "KUMAR et al\\.", "year": 2001}, {"title": "Combining Pattern Classifiers: Methods and Algorithms", "author": ["L.I. KUNCHEVA"], "venue": "Wiley, Hoboken, NJ.", "citeRegEx": "KUNCHEVA,? 2004", "shortCiteRegEx": "KUNCHEVA", "year": 2004}, {"title": "Classifier ensembles: Select real-world applications", "author": ["N.C. OZA", "K. TUMER"], "venue": "Inf. Fusion 9, 4\u201320.", "citeRegEx": "OZA and TUMER,? 2008", "shortCiteRegEx": "OZA and TUMER", "year": 2008}, {"title": "A survey on transfer learning", "author": ["S.J. PAN", "Q. YANG"], "venue": "IEEE TKDE 22, 1345\u20131359.", "citeRegEx": "PAN and YANG,? 2010", "shortCiteRegEx": "PAN and YANG", "year": 2010}, {"title": "Bootstrap-inspired techniques in computational intelligence", "author": ["R. POLIKAR"], "venue": "IEEE SIGNAL PROCESSING MAGAZINE.", "citeRegEx": "POLIKAR,? 2007", "shortCiteRegEx": "POLIKAR", "year": 2007}, {"title": "Consensus based ensembles of soft clusterings", "author": ["K. PUNERA", "J. GHOSH"], "venue": "Applied Artificial Intelligence. Vol. 22. 109\u2013117.", "citeRegEx": "PUNERA and GHOSH,? 2008", "shortCiteRegEx": "PUNERA and GHOSH", "year": 2008}, {"title": "Exploiting class hierarchies for knowledge transfer in hyperspectral data", "author": ["S. RAJAN", "J. GHOSH", "M.M. CRAWFORD"], "venue": "IEEE TGRS 44, 11, 3408\u20133417.", "citeRegEx": "RAJAN et al\\.,? 2006", "shortCiteRegEx": "RAJAN et al\\.", "year": 2006}, {"title": "Guest editor\u2019s introduction: special issue on inductive transfer learning", "author": ["D.L. SILVER", "K.P. BENNETT"], "venue": "Mach. Learn. 73, 215\u2013220.", "citeRegEx": "SILVER and BENNETT,? 2008", "shortCiteRegEx": "SILVER and BENNETT", "year": 2008}, {"title": "Large scale semi-supervised linear SVMs", "author": ["V. SINDHWANI", "S.S. KEERTHI"], "venue": "Proc. of the 29th Annual International ACM SIGIR Conf. on Research and Development in Information Retrieval. NY, USA, 477\u2013484.", "citeRegEx": "SINDHWANI and KEERTHI,? 2006", "shortCiteRegEx": "SINDHWANI and KEERTHI", "year": 2006}, {"title": "Cluster ensembles \u2013 a knowledge reuse framework for combining multiple partitions", "author": ["A. STREHL", "J. GHOSH"], "venue": "JMLR 3, 583\u2013617.", "citeRegEx": "STREHL and GHOSH,? 2002", "shortCiteRegEx": "STREHL and GHOSH", "year": 2002}, {"title": "Semi-supervised learning with measure propagation", "author": ["A. SUBRAMANYA", "J. BILMES"], "venue": "Journal of Machine Learning. Research 12, 3311\u20133370.", "citeRegEx": "SUBRAMANYA and BILMES,? 2011", "shortCiteRegEx": "SUBRAMANYA and BILMES", "year": 2011}, {"title": "Entropic graph regularization in non-parametric semisupervised classification", "author": ["A. SUBRAMANYA", "J.A. BILMES"], "venue": "Proc. of NIPS. Vancouver, Canada.", "citeRegEx": "SUBRAMANYA and BILMES,? 2009", "shortCiteRegEx": "SUBRAMANYA and BILMES", "year": 2009}, {"title": "Learning To Learn", "author": ["S. THRUN", "L. PRATT"], "venue": "Kluwer Academic, Norwell, MA.", "citeRegEx": "THRUN and PRATT,? 1997", "shortCiteRegEx": "THRUN and PRATT", "year": 1997}, {"title": "Propagating distributions on a hypergraph by dual information regularization", "author": ["K. TSUDA"], "venue": "Proceedings of the 22nd international conference on Machine learning. ICML \u201905. ACM, New York, NY, USA, 920\u2013927.", "citeRegEx": "TSUDA,? 2005", "shortCiteRegEx": "TSUDA", "year": 2005}, {"title": "Analysis of decision boundaries in linearly combined neural classifiers", "author": ["K. TUMER", "J. GHOSH"], "venue": "Pattern Recognition 29, 2, 341\u2013348.", "citeRegEx": "TUMER and GHOSH,? 1996", "shortCiteRegEx": "TUMER and GHOSH", "year": 1996}, {"title": "Bayesian cluster ensembles", "author": ["H. WANG", "H. SHAN", "A. BANERJEE"], "venue": "Statistical Analysis and Data Mining 1, 1\u201317.", "citeRegEx": "WANG et al\\.,? 2011", "shortCiteRegEx": "WANG et al\\.", "year": 2011}, {"title": "Learning continuous latent variable models with bregman divergences", "author": ["S. WANG", "D. SCHUURMANS"], "venue": "2842, 190\u2013204.", "citeRegEx": "WANG and SCHUURMANS,? 2003a", "shortCiteRegEx": "WANG and SCHUURMANS", "year": 2003}, {"title": "Learning latent variable models with Bregman divergences", "author": ["S. WANG", "D. SCHUURMANS"], "venue": "IEEE International Symposium on Information Theory.", "citeRegEx": "WANG and SCHUURMANS,? 2003b", "shortCiteRegEx": "WANG and SCHUURMANS", "year": 2003}, {"title": "On the convergence properties of the EM algorithm", "author": ["WU C.F.J."], "venue": "Annals of Statistics.", "citeRegEx": "J.,? 1982", "shortCiteRegEx": "J.", "year": 1982}, {"title": "Nonlinear Programming: a Unified Approach", "author": ["W. ZANGWILL"], "venue": "Prentice-Hall International Series in Management, Englewood Cliffs: N.J.", "citeRegEx": "ZANGWILL,? 1969", "shortCiteRegEx": "ZANGWILL", "year": 1969}, {"title": "Linear prediction models with graph regularization for web-page categorization", "author": ["T. ZHANG", "A. POPESCUL", "DOM", "B."], "venue": "Proc. of the 12th ACM SIGKDD. ACM, New York, NY, USA, 821\u2013826.", "citeRegEx": "ZHANG et al\\.,? 2006", "shortCiteRegEx": "ZHANG et al\\.", "year": 2006}, {"title": "Semi-supervised learning with graphs", "author": ["ZHU X."], "venue": "Ph.D. thesis, Pittsburgh, PA, USA. Chair-Lafferty, John and Chair-Rosenfeld, Ronald.", "citeRegEx": "X.,? 2005", "shortCiteRegEx": "X.", "year": 2005}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["X. ZHU", "Z. GHAHRAMANI"], "venue": "Tech. rep., Carnegie Mellon University.", "citeRegEx": "ZHU and GHAHRAMANI,? 2002", "shortCiteRegEx": "ZHU and GHAHRAMANI", "year": 2002}, {"title": "Introduction to Semi-Supervised Learning", "author": ["X. ZHU", "A.B. GOLDBERG"], "venue": "Morgan & Claypool Publishers.", "citeRegEx": "ZHU and GOLDBERG,? 2009", "shortCiteRegEx": "ZHU and GOLDBERG", "year": 2009}], "referenceMentions": [{"referenceID": 8, "context": "General properties of a large class of loss functions described by Bregman divergences are exploited in this framework in conjunction with Legendre duality and a notion of variable splitting that is also used in alternating direction method of multipliers [Boyd et al. 2011]) to yield a principled and scalable solution.", "startOffset": 256, "endOffset": 274}, {"referenceID": 38, "context": "Note that the setting described above is different from transductive learning setups where both labeled and unlabeled data are available at the same time for model building [Silver and Bennett 2008], as well as online methods where decisions are made on one new example at a time, and after each such decision, the true label of the example is obtained and used to update the model parameters [Blum 1998].", "startOffset": 173, "endOffset": 198}, {"referenceID": 6, "context": "Note that the setting described above is different from transductive learning setups where both labeled and unlabeled data are available at the same time for model building [Silver and Bennett 2008], as well as online methods where decisions are made on one new example at a time, and after each such decision, the true label of the example is obtained and used to update the model parameters [Blum 1998].", "startOffset": 393, "endOffset": 404}, {"referenceID": 0, "context": "This particular algorithm has been briefly introduced in [Acharya et al. 2011].", "startOffset": 57, "endOffset": 78}, {"referenceID": 32, "context": "The combination of multiple single or base classifiers to generate a more capable ensemble classifier has been an active area of research for the past two decades [Kuncheva 2004; Oza and Tumer 2008].", "startOffset": 163, "endOffset": 198}, {"referenceID": 33, "context": "The combination of multiple single or base classifiers to generate a more capable ensemble classifier has been an active area of research for the past two decades [Kuncheva 2004; Oza and Tumer 2008].", "startOffset": 163, "endOffset": 198}, {"referenceID": 45, "context": "Several papers provide both theoretical results [Tumer and Ghosh 1996] and empirical evidence showing the utility of such approaches for solving difficult classification problems.", "startOffset": 48, "endOffset": 70}, {"referenceID": 45, "context": "For instance, an analytical framework to mathematically quantify the improvements in classification results due to combining multiple models has been addressed in [Tumer and Ghosh 1996].", "startOffset": 163, "endOffset": 185}, {"referenceID": 33, "context": "all recognition, and medicine \u2014 is presented in [Oza and Tumer 2008].", "startOffset": 48, "endOffset": 68}, {"referenceID": 46, "context": ", see [Wang et al. 2011; Ghosh and Acharya 2011] and references therein.", "startOffset": 6, "endOffset": 48}, {"referenceID": 26, "context": ", see [Wang et al. 2011; Ghosh and Acharya 2011] and references therein.", "startOffset": 6, "endOffset": 48}, {"referenceID": 40, "context": "More specifically, cluster ensembles can be used to generate more robust and stable clustering results (compared to a single clustering approach), perform distributed computing under privacy or sharing constraints, or reuse existing knowledge [Strehl and Ghosh 2002].", "startOffset": 243, "endOffset": 266}, {"referenceID": 35, "context": "We note however that: \u2022Like single classifiers/clusterers, with very few exceptions [Polikar 2007], ensemble methods assume that the test or scoring data comes from the same underlying distribution as the training (and validation) data.", "startOffset": 84, "endOffset": 98}, {"referenceID": 13, "context": "\u2022There is relatively little work in incorporating both labeled and unlabeled data while building ensembles, in contrast to the substantial amount of recent interest in semi-supervised learning - including semi-supervised clustering, semi-supervised classification, clustering with constraints and transductive learning methods - using a single model [Chapelle et al. 2006; Zhu and Goldberg 2009; Cai et al. 2009; Forestier et al. 2010; Chen et al. 2009].", "startOffset": 350, "endOffset": 453}, {"referenceID": 54, "context": "\u2022There is relatively little work in incorporating both labeled and unlabeled data while building ensembles, in contrast to the substantial amount of recent interest in semi-supervised learning - including semi-supervised clustering, semi-supervised classification, clustering with constraints and transductive learning methods - using a single model [Chapelle et al. 2006; Zhu and Goldberg 2009; Cai et al. 2009; Forestier et al. 2010; Chen et al. 2009].", "startOffset": 350, "endOffset": 453}, {"referenceID": 10, "context": "\u2022There is relatively little work in incorporating both labeled and unlabeled data while building ensembles, in contrast to the substantial amount of recent interest in semi-supervised learning - including semi-supervised clustering, semi-supervised classification, clustering with constraints and transductive learning methods - using a single model [Chapelle et al. 2006; Zhu and Goldberg 2009; Cai et al. 2009; Forestier et al. 2010; Chen et al. 2009].", "startOffset": 350, "endOffset": 453}, {"referenceID": 22, "context": "\u2022There is relatively little work in incorporating both labeled and unlabeled data while building ensembles, in contrast to the substantial amount of recent interest in semi-supervised learning - including semi-supervised clustering, semi-supervised classification, clustering with constraints and transductive learning methods - using a single model [Chapelle et al. 2006; Zhu and Goldberg 2009; Cai et al. 2009; Forestier et al. 2010; Chen et al. 2009].", "startOffset": 350, "endOffset": 453}, {"referenceID": 14, "context": "\u2022There is relatively little work in incorporating both labeled and unlabeled data while building ensembles, in contrast to the substantial amount of recent interest in semi-supervised learning - including semi-supervised clustering, semi-supervised classification, clustering with constraints and transductive learning methods - using a single model [Chapelle et al. 2006; Zhu and Goldberg 2009; Cai et al. 2009; Forestier et al. 2010; Chen et al. 2009].", "startOffset": 350, "endOffset": 453}, {"referenceID": 34, "context": ", see [Pan and Yang 2010; Silver and Bennett 2008] and references therein), with much work done in the past 15 years [Thrun and Pratt 1997].", "startOffset": 6, "endOffset": 50}, {"referenceID": 38, "context": ", see [Pan and Yang 2010; Silver and Bennett 2008] and references therein), with much work done in the past 15 years [Thrun and Pratt 1997].", "startOffset": 6, "endOffset": 50}, {"referenceID": 43, "context": ", see [Pan and Yang 2010; Silver and Bennett 2008] and references therein), with much work done in the past 15 years [Thrun and Pratt 1997].", "startOffset": 117, "endOffset": 139}, {"referenceID": 11, "context": "The tasks may be learnt simultaneously [Caruana 1997] or sequentially [Bollacker and Ghosh 2000].", "startOffset": 39, "endOffset": 53}, {"referenceID": 7, "context": "The tasks may be learnt simultaneously [Caruana 1997] or sequentially [Bollacker and Ghosh 2000].", "startOffset": 70, "endOffset": 96}, {"referenceID": 3, "context": "Semi-supervised learning is a domain of machine learning where both labeled and unlabeled data are used to train a model \u2013 typically with lot of unlabeled data and only a small amount of labeled data (see [Bengio et al. 2006; Zhu and Goldberg 2009] and the references therein for more details).", "startOffset": 205, "endOffset": 248}, {"referenceID": 54, "context": "Semi-supervised learning is a domain of machine learning where both labeled and unlabeled data are used to train a model \u2013 typically with lot of unlabeled data and only a small amount of labeled data (see [Bengio et al. 2006; Zhu and Goldberg 2009] and the references therein for more details).", "startOffset": 205, "endOffset": 248}, {"referenceID": 51, "context": "There are several graph-based semisupervised algorithms that use either the graph structure to spread labels from labeled to unlabeled samples, or optimize a loss function that includes a smoothness constraint derived from the graph [Zhang et al. 2006; Subramanya and Bilmes 2009; Subramanya and Bilmes 2011].", "startOffset": 233, "endOffset": 308}, {"referenceID": 42, "context": "There are several graph-based semisupervised algorithms that use either the graph structure to spread labels from labeled to unlabeled samples, or optimize a loss function that includes a smoothness constraint derived from the graph [Zhang et al. 2006; Subramanya and Bilmes 2009; Subramanya and Bilmes 2011].", "startOffset": 233, "endOffset": 308}, {"referenceID": 41, "context": "There are several graph-based semisupervised algorithms that use either the graph structure to spread labels from labeled to unlabeled samples, or optimize a loss function that includes a smoothness constraint derived from the graph [Zhang et al. 2006; Subramanya and Bilmes 2009; Subramanya and Bilmes 2011].", "startOffset": 233, "endOffset": 308}, {"referenceID": 53, "context": "A majority of previously proposed graph-based semi-supervised algorithms [Zhu and Ghahramani 2002; Joachims 2003; Belkin et al. 2005; Bengio et al. 2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 73, "endOffset": 153}, {"referenceID": 30, "context": "A majority of previously proposed graph-based semi-supervised algorithms [Zhu and Ghahramani 2002; Joachims 2003; Belkin et al. 2005; Bengio et al. 2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 73, "endOffset": 153}, {"referenceID": 2, "context": "A majority of previously proposed graph-based semi-supervised algorithms [Zhu and Ghahramani 2002; Joachims 2003; Belkin et al. 2005; Bengio et al. 2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 73, "endOffset": 153}, {"referenceID": 3, "context": "A majority of previously proposed graph-based semi-supervised algorithms [Zhu and Ghahramani 2002; Joachims 2003; Belkin et al. 2005; Bengio et al. 2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 73, "endOffset": 153}, {"referenceID": 41, "context": "2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 53, "endOffset": 81}, {"referenceID": 16, "context": "2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 110, "endOffset": 140}, {"referenceID": 44, "context": "2006] are based on minimizing squared-loss, while in [Subramanya and Bilmes 2011] (Measure Propagation \u2013 MP), [Corduneanu and Jaakkola 2003] and [Tsuda 2005], the authors used KL divergence.", "startOffset": 145, "endOffset": 157}, {"referenceID": 12, "context": "OAC uses certain Bregman divergences [Censor and Zenios 1997], among which the KL divergence and squared loss constitute just a subset (further details are provided later, in Section 4).", "startOffset": 37, "endOffset": 61}, {"referenceID": 8, "context": "Additionally, the techniques of variable splitting [Boyd et al. 2011] and alternating minimization procedure [Bezdek and Hathaway 2002] are invoked to provide a more scalable solution.", "startOffset": 51, "endOffset": 69}, {"referenceID": 4, "context": "2011] and alternating minimization procedure [Bezdek and Hathaway 2002] are invoked to provide a more scalable solution.", "startOffset": 45, "endOffset": 71}, {"referenceID": 40, "context": "In order to capture the similarities between the instances of X , OAC also takes as input a similarity matrix S, which can be computed from a cluster ensemble, in such a way that each matrix entry corresponds to the relative co-occurrence of two instances in the same cluster [Strehl and Ghosh 2002] \u2014 considering all the data partitions that form the cluster ensemble induced from X .", "startOffset": 276, "endOffset": 299}, {"referenceID": 40, "context": ", the CSPA algorithm in [Strehl and Ghosh 2002].", "startOffset": 24, "endOffset": 47}, {"referenceID": 9, "context": "1 ([Bregman 1967], [Banerjee et al.", "startOffset": 3, "endOffset": 17}, {"referenceID": 1, "context": "1 ([Bregman 1967], [Banerjee et al. 2005]).", "startOffset": 19, "endOffset": 41}, {"referenceID": 36, "context": ", see [Punera and Ghosh 2008].", "startOffset": 6, "endOffset": 29}, {"referenceID": 1, "context": "In that case, however, one can work in the (Legendre) dual space, where the optimal solution has a simple form \u2014 see [Banerjee et al. 2005] for details.", "startOffset": 117, "endOffset": 139}, {"referenceID": 1, "context": "2 [Banerjee et al. 2005] that is stated below.", "startOffset": 2, "endOffset": 24}, {"referenceID": 1, "context": "2 ([BANERJEE ET AL. 2005]).", "startOffset": 3, "endOffset": 25}, {"referenceID": 1, "context": "(8) One can show that \u2200yi,yj \u2208 int(dom(\u03c6)), d\u03c6(yi,yj) = d\u03c8(\u2207\u03c6(yj),\u2207\u03c6(yi)) \u2014 see [Banerjee et al. 2005] for more details.", "startOffset": 80, "endOffset": 102}, {"referenceID": 41, "context": "In that case, one needs to use another Lagrangian multiplier to make sure that the returned values lie on simplex as has been done in [Subramanya and Bilmes 2011].", "startOffset": 134, "endOffset": 162}, {"referenceID": 47, "context": "Bregman divergences that satisfy the above properties include a large number of useful loss functions such as the well-known squared loss, KL-divergence, generalized I-divergence, logistic loss, Itakura-Saito distance and Bose-Einstein entropy [Wang and Schuurmans 2003a].", "startOffset": 244, "endOffset": 271}, {"referenceID": 15, "context": "Some authors [Cheney and Goldstein 1959; Zangwill 1969; Wu 1982; Bezdek and Hathaway 2003] have shown that the convergence guarantee of alternating optimization can be analyzed using the topological properties of the objective and the space over which it is optimized.", "startOffset": 13, "endOffset": 90}, {"referenceID": 50, "context": "Some authors [Cheney and Goldstein 1959; Zangwill 1969; Wu 1982; Bezdek and Hathaway 2003] have shown that the convergence guarantee of alternating optimization can be analyzed using the topological properties of the objective and the space over which it is optimized.", "startOffset": 13, "endOffset": 90}, {"referenceID": 5, "context": "Some authors [Cheney and Goldstein 1959; Zangwill 1969; Wu 1982; Bezdek and Hathaway 2003] have shown that the convergence guarantee of alternating optimization can be analyzed using the topological properties of the objective and the space over which it is optimized.", "startOffset": 13, "endOffset": 90}, {"referenceID": 48, "context": "Others have used information geometry [Csisz\u00e1r and Tusn\u00e1dy 1984; Wang and Schuurmans 2003b; Subramanya and Bilmes 2011] to analyze the convergence as well as a combination of both information geometry and topological properties", "startOffset": 38, "endOffset": 119}, {"referenceID": 41, "context": "Others have used information geometry [Csisz\u00e1r and Tusn\u00e1dy 1984; Wang and Schuurmans 2003b; Subramanya and Bilmes 2011] to analyze the convergence as well as a combination of both information geometry and topological properties", "startOffset": 38, "endOffset": 119}, {"referenceID": 27, "context": "of the objective [Gunawardana and Byrne 2005].", "startOffset": 17, "endOffset": 45}, {"referenceID": 53, "context": "The algorithms in [Zhu and Ghahramani 2002; Belkin et al. 2005] are based on minimizing squared-loss and are only suitable for binary classification problems.", "startOffset": 18, "endOffset": 63}, {"referenceID": 2, "context": "The algorithms in [Zhu and Ghahramani 2002; Belkin et al. 2005] are based on minimizing squared-loss and are only suitable for binary classification problems.", "startOffset": 18, "endOffset": 63}, {"referenceID": 41, "context": "MP [Subramanya and Bilmes 2011], on the other hand, is suitable for multi-class problems and additionally provides guard against degenerate solutions (those that assign equal confidence to all classes).", "startOffset": 3, "endOffset": 31}, {"referenceID": 41, "context": "In [Subramanya and Bilmes 2011], the authors also proved that their algorithm converges but the convergence rate (for KL divergence) is not proven and only empirical evidence is given for a linear rate.", "startOffset": 3, "endOffset": 31}, {"referenceID": 41, "context": "In this paper, apart from generalizing these algorithms with a larger class of Bregman divergences, we provide proofs for linear rate of convergence for generalized I divergence and KL divergence (the proof for squared loss follows directly from the analysis of [Subramanya and Bilmes 2011]).", "startOffset": 262, "endOffset": 290}, {"referenceID": 2, "context": "Manifold regularization [Belkin et al. 2005] is a general framework in which a parametric loss function is defined over the labeled samples and is regularized by graph smoothness term defined over both the labeled and unlabeled samples.", "startOffset": 24, "endOffset": 44}, {"referenceID": 16, "context": "Information regularization [Corduneanu and Jaakkola 2003], in essence, works on the same intuition as OAC, but does not provide any proof of convergence and one of the steps of the optimization does not have a closed form solution \u2013 a concern for large data applications.", "startOffset": 27, "endOffset": 57}, {"referenceID": 44, "context": "[Tsuda 2005] extended the works of [Corduneanu and Jaakkola 2003] to hyper-graphs and used closed form solutions in both steps of the alternating minimization procedure which, surprisingly, can be seen as a special case of MP.", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "[Tsuda 2005] extended the works of [Corduneanu and Jaakkola 2003] to hyper-graphs and used closed form solutions in both steps of the alternating minimization procedure which, surprisingly, can be seen as a special case of MP.", "startOffset": 35, "endOffset": 65}, {"referenceID": 41, "context": "In [Subramanya and Bilmes 2011], the authors followed the procedure of [Csisz\u00e1r and Tusn\u00e1dy 1984] to prove the convergence of a slightly different objective that involves KL-divergence as a loss function.", "startOffset": 3, "endOffset": 31}, {"referenceID": 47, "context": "This Bregman divergence also satisfies the properties (a) to (f), which then allows one to use the convergence tools developed by [Wang and Schuurmans 2003a].", "startOffset": 130, "endOffset": 157}, {"referenceID": 5, "context": "To analyze the same, we use some formulations that were derived in [Bezdek and Hathaway 2003] to characterize the local convergence rate of alternating minimization type of algorithms in general.", "startOffset": 67, "endOffset": 93}, {"referenceID": 5, "context": "Before presenting the main theorem from [Bezdek and Hathaway 2003], the formal definition of q-linear rate of convergence is provided below.", "startOffset": 40, "endOffset": 66}, {"referenceID": 41, "context": "there is no need to maintain left and right copies) and the q-linear rate of convergence of the objective J can be proved following the same method as done in [Subramanya and Bilmes 2011].", "startOffset": 159, "endOffset": 187}, {"referenceID": 39, "context": "4 we use simple (linear) base methods, and pick the popular Semi-Supervised Linear Support Vector Machine (SVM) [Sindhwani and Keerthi 2006] for comparison.", "startOffset": 112, "endOffset": 140}, {"referenceID": 39, "context": "4For these datasets, comparisons with SVM [Sindhwani and Keerthi 2006] have not been performed because the raw data required for learning is not available.", "startOffset": 42, "endOffset": 70}, {"referenceID": 40, "context": ",M4), forBGCM, and for two well-known cluster ensemble approaches\u2014MCLA [Strehl and Ghosh 2002] and HBGF [Fern and Brodley 2004] \u2014 are reproduced here for comparison purposes.", "startOffset": 71, "endOffset": 94}, {"referenceID": 21, "context": ",M4), forBGCM, and for two well-known cluster ensemble approaches\u2014MCLA [Strehl and Ghosh 2002] and HBGF [Fern and Brodley 2004] \u2014 are reproduced here for comparison purposes.", "startOffset": 104, "endOffset": 127}, {"referenceID": 19, "context": "In order to provide some reassurance about the validity and non-randomness of the obtained results, the outcomes of statistical tests, following the study in [Demsar 2006],", "startOffset": 158, "endOffset": 171}, {"referenceID": 39, "context": "Comparison with SVM We also compare OAC to a popular semi-supervised algorithm known as SVM [Sindhwani and Keerthi 2006].", "startOffset": 92, "endOffset": 120}, {"referenceID": 29, "context": "This algorithm is essentially a Transductive Linear Support Vector Machine (SVM) which can be viewed as a large scale implementation of the algorithm introduced in [Joachims 1999b].", "startOffset": 164, "endOffset": 180}, {"referenceID": 39, "context": "For dealing with unlabeled data, it appends an additional term in the SVM objective function whose role is to drive the classification hyperplane towards low data density regions [Sindhwani and Keerthi 2006].", "startOffset": 179, "endOffset": 207}, {"referenceID": 19, "context": "In addition, OAC shows better accuracies than both SVM and BGCM \u2014 from the adopted statistical procedure [Demsar 2006], OAC exhibits significantly better accuracies at a significance level of 10%.", "startOffset": 105, "endOffset": 118}, {"referenceID": 38, "context": "Transfer Learning Transfer learning emphasizes the transfer of knowledge across domains, tasks, and distributions that are similar but not the same [Silver and Bennett 2008].", "startOffset": 148, "endOffset": 173}, {"referenceID": 34, "context": "The real-world datasets employed in our experiments are: a) Text Documents \u2014 [Pan and Yang 2010]: From the well-known text collections 20 newsgroup and Reuters-21758, nine cross-domain learning tasks are generated.", "startOffset": 77, "endOffset": 96}, {"referenceID": 37, "context": "b) Botswana \u2014 [Rajan et al. 2006]: This is an application of transfer learning to the pixel-level classification of remotely sensed images, which provides a real-life scenario where such learning will be useful \u2014 in contrast to the contrived setting of text classification, which is chosen as it has been used previously in [Dai et al.", "startOffset": 14, "endOffset": 33}, {"referenceID": 28, "context": "We also compare OAC with two transfer learning algorithms from the literature \u2014 Transductive Support Vector Machines (TSVM) [Joachims 1999a] and the Locally Weighted Ensemble (LWE) [Gao et al.", "startOffset": 124, "endOffset": 140}, {"referenceID": 31, "context": "For the hyperspectral data, we use two baseline classifiers: the well-known Na\u0131\u0308ve Bayes Wrapper (NBW) and the Maximum Likelihood (ML) classifier, which performs well when used with a best bases feature extractor [Kumar et al. 2001].", "startOffset": 213, "endOffset": 232}, {"referenceID": 39, "context": "The proposed framework has been empirically shown to outperform a variety of algorithms [Gao et al. 2011; Sindhwani and Keerthi 2006; Gao et al. 2008] in both semi-supervised and transfer learning problems.", "startOffset": 88, "endOffset": 150}, {"referenceID": 1, "context": "2 ([BANERJEE ET AL. 2005]).", "startOffset": 3, "endOffset": 25}, {"referenceID": 47, "context": "The proof is based on the works of [Wang and Schuurmans 2003a].", "startOffset": 35, "endOffset": 62}, {"referenceID": 47, "context": "The proof here follows the same line of argument as given in [Wang and Schuurmans 2003a] and [Eggermont and LaRiccia 1998].", "startOffset": 61, "endOffset": 88}, {"referenceID": 20, "context": "The proof here follows the same line of argument as given in [Wang and Schuurmans 2003a] and [Eggermont and LaRiccia 1998].", "startOffset": 93, "endOffset": 122}, {"referenceID": 41, "context": "There is another interesting aspect of J that was discovered in [Subramanya and Bilmes 2011] for a slightly different objective function with KL divergence used as a loss function.", "startOffset": 64, "endOffset": 92}], "year": 2012, "abstractText": "Unsupervised models can provide supplementary soft constraints to help classify new, \u201ctarget\u201d data since similar instances in the target set are more likely to share the same class label. Such models can also help detect possible differences between training and target distributions, which is useful in applications where concept drift may take place, as in transfer learning settings. This paper describes a general optimization framework that takes as input class membership estimates from existing classifiers learnt on previously encountered \u201csource\u201d data, as well as a similarity matrix from a cluster ensemble operating solely on the target data to be classified, and yields a consensus labeling of the target data. This framework admits a wide range of loss functions and classification/clustering methods. It exploits properties of Bregman divergences in conjunction with Legendre duality to yield a principled and scalable approach. A variety of experiments show that the proposed framework can yield results substantially superior to those provided by popular transductive learning techniques or by na\u0131\u0308vely applying classifiers learnt on the original task to the target data.", "creator": "LaTeX with hyperref package"}}}