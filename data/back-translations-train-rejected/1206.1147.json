{"id": "1206.1147", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2012", "title": "Memory-Efficient Topic Modeling", "abstract": "As one of the simplest probabilistic topic modeling techniques, latent Dirichlet allocation (LDA) has found many important applications in text mining, computer vision and computational biology. Recent training algorithms for LDA can be interpreted within a unified message passing framework. However, message passing requires storing previous messages with a large amount of memory space, increasing linearly with the number of documents or the number of topics. Therefore, the high memory usage is often a major problem for topic modeling of massive corpora containing a large number of topics. To reduce the space complexity, we propose a novel algorithm without storing previous messages for training LDA: tiny belief propagation (TBP). The basic idea of TBP relates the message passing algorithms with the non-negative matrix factorization (NMF) algorithms, which absorb the message updating into the message passing process, and thus avoid storing previous messages. Experimental results on four large data sets confirm that TBP performs comparably well or even better than current state-of-the-art training algorithms for LDA but with a much less memory consumption. TBP can do topic modeling when massive corpora cannot fit in the computer memory, for example, extracting thematic topics from 7 GB PUBMED corpora on a common desktop computer with 2GB memory.", "histories": [["v1", "Wed, 6 Jun 2012 08:34:43 GMT  (2235kb)", "https://arxiv.org/abs/1206.1147v1", "20 pages, 7 figures"], ["v2", "Fri, 8 Jun 2012 14:07:26 GMT  (2239kb)", "http://arxiv.org/abs/1206.1147v2", "20 pages, 7 figures"]], "COMMENTS": "20 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["jia zeng", "zhi-qiang liu", "xiao-qin cao"], "accepted": false, "id": "1206.1147"}, "pdf": {"name": "1206.1147.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Jia Zeng", "Xiao-Qin Cao"], "emails": ["j.zeng@ieee.org"], "sections": [{"heading": null, "text": "ar Xiv: 120 6.11 47v2 [cs.LG] Keywords: Topic models, latent Dirichlet allocation, tiny believe propagation, nonnegative matrix factorization, memory use."}, {"heading": "1. Introduction", "text": "This is a three-layered hierarchical model for probable subject modeling, computer vision and computer-based biology (Lead, 2012).The collections of documents can be presented as a document-word-co-occurrence matrix, in which each element is the number of words in the specific document. Modelling each document as a mixture of topics and each topic as a mixture of vocabulary words, LDA assigns thematic terms to explain non-zeros elements in the document-word matrix segments observed words into several thematic groups, which are referred to as themes.From the common probability of latent names and observed terms of the LDA, approximately the posterior probability of subject labeling of the subject label of the description, and estimation of multicultured terms."}, {"heading": "2. The Message Passing Algorithms for Training LDA", "text": "LDA assigns a set of semantic subject labels, z = {zkw, d} to explain non-zero elements in the document-word coexistence matrix \u00b7 D = {xw, d}, where 1 \u2264 w \u2264 W denotes the word index in the vocabulary, 1 \u2264 d \u2264 D denotes the document memory index in the corpus, and 1 \u2264 k \u2264 K denotes the subject index. Usually, the number of topics K is provided by the user. The subject title satisfies zkW, d = {0, 1}, \u2211 K = 1 z k w, d = 1. After deriving the subject label configuration from the document-word matrix, LDA estimates two matrices of multinomic parameters: topic distributions via the fixed vocabulary, W \u00b7 K = {zepte, k} using the number combinations from the topic x, the number combinations from the document-word matrix are used."}, {"heading": "2.1 Collapsed Gibbs Sampling (GS)", "text": "????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "2.2 Loopy Belief Propagation (BP)", "text": "The basic idea is to integrate the multinomial parameters that facilitate the BP algorithm for approximate conclusions and parameter estimation. Unlike GS, BP, this results in a slight probability in collapsed space {z, \u03b1, \u03b2}. The collapsed messages can be represented by a factor graph that contains all the uncertainties of the messages that enable the BP algorithm to approximate conclusions and parameter estimates. Unlike BP, this results in the following messages, \u00b5w (k) = p (zkw, d = 1), without sampling, in order to obtain all the uncertainties of the messages. Message updating is based on the equation is\u00b5w, d \u2212 p \u2212 s \u2212 w, d \u2212 s, k, k, k, k."}, {"heading": "2.3 Variational Bayes (VB)", "text": "In contrast to BP in collapsed space, VB (Lead et al., 2003; Winn and Bishop \u03b2 q, 2005) carries out variation messages by minimizing KL divergence, KL (p) = p (z k w, d = 1). The variation message updates the equation is\u00b5 w, d (k) + \u03b2 (p), (10), whereby the \u00b5 \u00b7, d (k) + x (p) x (p) x (p), x (p), x (p), x (p), x (p), x (p), x (k), x (p), x (p), x (p), x (p), x (p), x (p), x (k), x (k), x (k), x), x (k), x (k), x (k), x (k), x (k), x), and x (k) are the variation messages."}, {"heading": "2.4 Synchronous and Asynchronous Message Passing", "text": "Message forwarding algorithms for LDA first randomly initialize messages and then forward messages according to two schedules: the synchronous and asynchronous update schedule (Tap and Freeman, 2003).The synchronous message forwarding plan uses all messages in t \u2212 1 training iteration to update current messages in t-training iteration, while the asynchronous schedule immediately uses the updated messages to update other remaining messages within the same t-training iteration. Empirical results show that the asynchronous timetable for topic modeling is slightly more efficient than the synchronous timetable (Zeng et al., 2011).However, the synchronous timetable for parallel calculations is much easier to expand.GS is, of course, an asynchronous algorithm for message forwarding. The sampled theme label directly affects the sampling process for the next word token. Both synchronous and asynchronous timetables for BP messages work much more easily in terms of message forwarding with respect to the modeling accuracy of VB, but the timing of VB is equally good with the asynchronous."}, {"heading": "3. Tiny Belief Propagation", "text": "In this section, we propose that TBP store the message storage and data storage usage of BP in Section 2.2. Generally, BP's parameter memory takes up a relatively small amount of memory when the number of topics K is low. For example, the parameter \u03b8K \u00b7 D occupies about 0.6 GB of memory for 7 GB of PUBMED records (D = 8, 200, 000 and W = 141043) for K = 10, while the parameter \u03c6W \u00b7 K occupies about 0.01 GB of memory. For simplicity, we assume that the parameter memory is sufficient for topic modeling."}, {"heading": "3.1 Message Memory", "text": "The algorithmic contribution of TBP is to reduce BP's message memory to almost zero during the message transmission process. (6), (8) and (9) result in the approximate message update of Eq. (6) Note that such an approach does not distort the message update very much, since the current message from Eqs, d (k) is significantly smaller than the current message from other messages in both areas. (6) Note that such an approach distorts the message update very much, since the message from \u00b5w, d (k) is substantially smaller than the current message from other messages in both areas. Eq. (13) has the following intuitive explanation. If the growth word has a higher probability in the topic and the topic k has a greater share in the dth document, then the topic k has a higher probability of being associated with the element xw, d, i.e."}, {"heading": "3.2 Data Memory", "text": "We assume that the hard disk is large enough to store the Corpus file as a block. Lately, reading data from the hard disk into memory as a block is a promising way to solve such problems (Yu et al., 2010). We can expand the TBP algorithms in Figures 2 and 3 to read the Corpus file as a block and optimize each block sequentially. For example, we can read each document in the Corpus file into memory at once and perform the TBP algorithms to refine the matrices. After scanning all documents in the Corpus data file, TBP terminates an iteration of the training in the Figures. 2 and 3. Likewise, we can store the matrices in thumbnails."}, {"heading": "3.3 Relationship to Previous Algorithms", "text": "The intrinsic relationship between probabilistic topic models (Hofmann, 2001; Lead et al., 2003) and NMF topic (Lee and Seung, 2001) has been extensively discussed in several previous papers (Buntine, 2002; Gaussier and Goutte, 2005; Girolami et al., 2003; Wahabzada and Kersting, 2011; Wahabzada et al., 2011; Zeng et al., 2011). Recent work shows that NMF learning topic models have a polynomial time (Arora et al., 2012). Generally, learning topic models can be formulated within the news transmission framework in Section 2; Zeng et al., 2011). Recent work shows that NMF learning topic models have a polynomial time (Arora et al., 2012)."}, {"heading": "4. Experimental Results", "text": "Our experiments aim to confirm the lower memory usage of TBP compared to the most advanced batch learning algorithms such as VB (Lead et al., 2003), GS (Griffiths and Steyvers, 2004), and BP (Zeng et al., 2011). We use four publicly available datasets (Porteous et al., 2008; Hoffman et al., 2010): ENRON, NYTIMES, PUBMED, and WIKI. Previous studies (Porteous et al., 2008) showed that the result of subject modeling is relatively insensitive to the total number of documents in the corpus. Due to the memoric limitation for GS, BP, and VB algorithms, we select 15,000 documents from the original NYTIMES dataset, 80,000 documents from the original PUBMED dataset, and 10,000 documents from the original WI data."}, {"heading": "4.1 Comparison with Batch Learning Algorithms", "text": "We compare TBP with other batch learning algorithms such as GS, BP and VB. For all datasets, we have the same hyperparameters as \u03b1 = 2 / K, \u03b2 = 0.01 (Porteous et al., 2008). CPU time per iteration is apparently measured after sweeping the entire dataset. We report on the average CPU time per iteration according to T = 500 iterations, which virtually ensures that GS, BP and VB match in terms of training perplexity. For a fair comparison, we use the same random initialization to examine all algorithms with 500 iterations. To repeat our experiments, we have made all source codes and datasets publicly available (Zeng, 2012). These experiments are performed on the Sun Fire X4270 M2 server with two 6-core 3.46 GHz CPUs and 128 GB RAMs.Table 2 compares message storage during training."}, {"heading": "4.2 Comparison with Online Algorithms", "text": "The complete 7GB PUBMED dataset (Porteous et al., 2008) contains a total of D = 820 000 000 documents with a vocabulary size W = 141, 043. Currently, only TBP and online theme modeling methods can process 7GB dataset with 2GB of memory. OVB (Hoffman et al., 2010) uses the following standard parameters: \u0432 = 0.5, \u03c40 = 1024, and the mini-batch size S = 1024. We randomly reserve 40 000 documents as a test set and use the rest 8, 160 000 documents as a training set. The number of topics K = 10. The hyperparameters T = 2 / K = 0.05 and \u03b2 = 01.01."}, {"heading": "5. Conclusions", "text": "The TBP algorithm reduces the message storage required by conventional message forwarding algorithms such as GS, BP and VB. We also discuss the intrinsic relationship between the proposed TBP and NMF (Lee and Seung, 2001) with KL divergence. We note that TBP can roughly be considered a special NMF algorithm to minimize perplexity metrics, a widely used evaluation method for various LDA training algorithms. Furthermore, we confirm the overarching theme that models the accuracy of TBP in terms of predictive perplexity to large-scale experiments. Compared with the state-of-the-art online theme modeling algorithm OVB, the proposed TBP is faster and more accurate to extract 10 topics from the 7GB PUBMED corpus using a desktop computer with 2GB micro."}, {"heading": "Acknowledgments", "text": "This work is supported by the NSFC (grant no. 61003154), the Shanghai Key Laboratory of Intelligent Information Processing, China (grant no. IIPL-2010-009) and a grant no. Baidu to JZ as well as a grant no. GRF of the RGC UGC Hong Kong (grant no. 9041574) and a grant no. City University of Hong Kong (grant no. 7008026) to ZQL."}], "references": [{"title": "Residual belief propagation: Informed", "author": ["Gal Elidan", "Ian McGraw", "Daphne Koller"], "venue": "via the EM algorithm. Journal of the Royal Statistical Society,", "citeRegEx": "Elidan et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Elidan et al\\.", "year": 1977}, {"title": "Online learning for latent Dirichlet allocation", "author": ["M. Hoffman", "D. Blei", "F. Bach"], "venue": null, "citeRegEx": "Hoffman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2004}, {"title": "Comparison of graph cuts with belief", "author": ["F. Tappen", "William T. Freeman"], "venue": null, "citeRegEx": "Tappen and Freeman.,? \\Q2003\\E", "shortCiteRegEx": "Tappen and Freeman.", "year": 2003}, {"title": "Learning topic models by belief propagation", "author": ["Jia Zeng", "William K. Cheung", "Jiming Liu"], "venue": null, "citeRegEx": "Zeng et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": ", 2003) and the two-layer factor graph for the collapsed LDA Zeng et al. (2011). Recently, there have been three types of message passing algorithms for training LDA: GS, BP and VB.", "startOffset": 61, "endOffset": 80}, {"referenceID": 2, "context": "4 Synchronous and Asynchronous Message Passing Message passing algorithms for LDA first randomly initialize messages, and then pass messages according to two schedules: the synchronous and the asynchronous update schedules (Tappen and Freeman, 2003).", "startOffset": 223, "endOffset": 249}], "year": 2012, "abstractText": "As one of the simplest probabilistic topic modeling techniques, latent Dirichlet allocation (LDA) has found many important applications in text mining, computer vision and computational biology. Recent training algorithms for LDA can be interpreted within a unified message passing framework. However, message passing requires storing previous messages with a large amount of memory space, increasing linearly with the number of documents or the number of topics. Therefore, the high memory usage is often a major problem for topic modeling of massive corpora containing a large number of topics. To reduce the space complexity, we propose a novel algorithm without storing previous messages for training LDA: tiny belief propagation (TBP). The basic idea of TBP relates the message passing algorithms with the non-negative matrix factorization (NMF) algorithms, which absorb the message updating into the message passing process, and thus avoid storing previous messages. Experimental results on four large data sets confirm that TBP performs comparably well or even better than current state-of-the-art training algorithms for LDA but with a much less memory consumption. TBP can do topic modeling when massive corpora cannot fit in the computer memory, for example, extracting thematic topics from 7GB PUBMED corpora on a common desktop computer with 2GB memory.", "creator": "LaTeX with hyperref package"}}}