{"id": "1206.1208", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2012", "title": "Cumulative Step-size Adaptation on Linear Functions: Technical Report", "abstract": "The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation, where the step size is adapted measuring the length of a so-called cumulative path. The cumulative path is a combination of the previous steps realized by the algorithm, where the importance of each step decreases with time. This article studies the CSA-ES on composites of strictly increasing with affine linear functions through the investigation of its underlying Markov chains. Rigorous results on the change and the variation of the step size are derived with and without cumulation. The step-size diverges geometrically fast in most cases. Furthermore, the influence of the cumulation parameter is studied.", "histories": [["v1", "Wed, 6 Jun 2012 13:03:31 GMT  (171kb,D)", "http://arxiv.org/abs/1206.1208v1", null], ["v2", "Fri, 29 Jun 2012 18:56:20 GMT  (157kb,D)", "http://arxiv.org/abs/1206.1208v2", "Parallel Problem Solving From Nature (2012)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["alexandre adrien chotard", "anne auger", "nikolaus hansen"], "accepted": false, "id": "1206.1208"}, "pdf": {"name": "1206.1208.pdf", "metadata": {"source": "CRF", "title": "Cumulative Step-size Adaptation on Linear Functions", "authors": ["Alexandre Chotard", "Anne Auger", "Nikolaus Hansen"], "emails": ["firstname.lastname@lri.fr"], "sections": [{"heading": null, "text": "Keywords: CSA, cumulative path, evolutionary path, evolutionary strategies, gradual adaptation"}, {"heading": "1 Introduction", "text": "Evolutionary strategies (ESs) are continuous stochastic optimization algorithms in search of the optimum of a real evaluated function f: Rn \u2192 R. In the (1, \u03bb) -ES, in each iteration, \u03bb new children are generated from a single parent pointX-Rn by adding a random Gaussian vector to the parent, X: Rn 7 \u2192 X + \u03c3N (0, C).Here \u03c3 R: + is designated as step size and C is a covariance matrix. The best thing about the children, according to f, becomes the parent of the next normal iteration. In order to achieve reasonably fast convergence, step size and covariance matrix must be adjusted during the iterations of the algorithm. In this thesis, C is the identity and we examine the so-called cumulative step-size adaptation (CSA) [11,9]. In the CSA, a cumulative path is introduced, which is a combination of all the steps of the algorithm."}, {"heading": "2 The CSA-ES", "text": "In fact, the parent subdivider participating children generate the following: Y t, i = Xt + \u03c3t\u0432t, i with i [1, \u03bb], and vice versa, i \u0445 N (0, In), (2, i) i [1, \u03bb], i.i.d. Based on the (1, \u03bb) selection scheme chosen by these children, the one minimizing function f is selected: Xt + 1 = argmin {f (Y), Y [Y], 1, Y [1, \u03bb]], this latter equation will implicitly define the random variable distribution. (1) To adjust the step size, the cumulative path is defined as pt + 1 = (1 \u2212 c) pt. (2 \u2212 c) pt. (1) pt."}, {"heading": "3 Divergence rate of CSA-ES without cumulation", "text": "In this section we examine the (1, \u03bb) -CSA-ES without cumulation, i.e. c = 1. In this case, the path is always the same to the chosen step, i.e. for all t we have pt + 1 = 0? t. We have proven in term 1 that we are not usually. This allows us to apply the standard law of large numbers to find the limit of 1t ln (\u03c3t / \u03c30) and calculate the expected log step resizing.Suggestion 1. Let us know: = 12d\u03c3n (E (N 21: \u03bb) \u2212 1)."}, {"heading": "ES without cumulation satisfies (i) almost surely limt\u2192\u221e 1t ln (\u03c3t/\u03c30) = \u2206\u03c3 , and (ii)", "text": "In term 1 we have found that the first coordinate of the first stage of the second stage is distributed randomly according to N1: \u03bb and the other coordinates according to N (0, 1), i.e. E (2), n (1), n (2), n (2), n (1), n (1), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n (2), n, n (2)."}, {"heading": "4 Divergence rate of CSA-ES with cumulation", "text": "We now examine the (1) -CSA-ES with cumulation, i.e. 0 < c < c < 1. The path P is then a Markov chain, and in contrast to the case where c = 1 we cannot apply LLN for independent variables to Eq. (5) To prove the almost certain geometric divergence, LN exists just as well for Markov chains, provided that the Markov chain fulfills some stability properties: especially if the Markov chain P is \"irreducible,\" i.e., there is a measurement that states that each Borel proposition A of Rn exists with a positive probability, which can be achieved in a finite number of steps by assuming P from any p0-Rn. Furthermore, the chain P (i) must be positive, i.e. the chain permits an invariant probability measurement, i.e., for every borelian A in which we find ourselves."}, {"heading": "5 Study of the variations of ln (\u03c3t+1/\u03c3t)", "text": "The proof for Theorem 2 shows that the increment of the step on the right side of Eq. (7), for t \u2192 \u221e. If the dimension increases this increment to zero, which also indicates that it is more likely that \u03c3t + 1 is smaller than \u03c3t. To analyze this behavior, we examine the variance of ln (\u03c3t + 1 / \u03c3t) as a function of c and the dimension.Theorem 3. The variance of ln (\u03c3t + 1 / \u03c3t) corresponds to toVar (ln (\u03c3t + 1))))) = c2 4d\u03c32n2 (E ([pt + 1] 4) \u2212 E ([pt + 1) 2 \u2212 2 + 2 (n \u2212 1))) (n \u2212 2 + 2 (n \u2212 1)). (10) Further, E ([pt + 1), E ([pt + 1) (\u03c3t + 1)) -1), [t + 1), [t + 1), [t + 1, [t + 1), [t + 1, 2 + 1, 2 (n \u2212 1) (n \u2212 1))) (n \u2212 2 (n \u2212 1). Further, E (n \u2212 2 (n \u2212 2) corresponds to toVar (ln \u2212 2 + 2) (\u03c3t (\u03c3t + 2)))). (10) also corresponds to toVar (ln (ln \u2212 2 + 1 (\u03c3t (\u03c3t + 1)), E (ln (\u03c3t + 1 \u03c3t + 1 \u03c3t)))) = c2, E (= c2, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p), 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p), 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2p, 2"}, {"heading": "6 Summary", "text": "In this paper, we examine the (1, \u03bb) -CSA-ES for affine linear functions, which consist of strictly increasing transformations. We find the boundary distribution for ln (\u03c3t / \u03c30) / t in theorem 2 and rigorously prove the desired behaviour of \u03c3 with \u03bb \u2265 2: The depth varies geometrically quickly. In contrast, a random walk on ln (\u03c3) occurs without cumulation (c = 1) and with \u03bb = 2, as with (1, 2) -\u03c3SA-ES [8] (and also for the same symmetry reason). We derive an expression for the variance of the increment of step sizes. On linear functions, if c is kept constant and for n \u2192 \u221e, the standard deviation is approximately n times greater than the increment of step size. However, the standard deviation remains at c \u2264 1 / n1 / 3 for n \u2192."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the ANR-2010-COSI-002 grant (SIMINOLE) of the French Research Agency and the ANR COSINUS project ANR-08-COSI007-12."}], "references": [{"title": "Performance analysis of evolutionary optimization with cumulative step length adaptation", "author": ["D.V. Arnold", "H.-G. Beyer"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "On the behaviour of evolution strategies optimising cigar functions", "author": ["D.V. Arnold", "H.-G. Beyer"], "venue": "Evolutionary Computation,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Cumulative step length adaptation on ridge functions", "author": ["D.V. Arnold"], "venue": "In Parallel Problem Solving from Nature PPSN IX,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Random dynamics optimum tracking with evolution strategies", "author": ["D.V. Arnold", "H.G. Beyer"], "venue": "In Parallel Problem Solving from Nature PPSN VII,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Optimum tracking with evolution strategies", "author": ["D.V. Arnold", "H.G. Beyer"], "venue": "Evolutionary Computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Evolution strategies with cumulative step length adaptation on the noisy parabolic ridge", "author": ["D.V. Arnold", "H.G. Beyer"], "venue": "Natural Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Cumulative step-size adaptation on linear functions: Technical report", "author": ["A. Chotard", "A. Auger", "N. Hansen"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "An analysis of mutative \u03c3-self-adaptation on linear fitness functions", "author": ["N. Hansen"], "venue": "Evolutionary Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation", "author": ["N. Hansen", "A. Ostermeier"], "venue": "In International Conference on Evolutionary Computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Markov chains and stochastic stability", "author": ["S.P. Meyn", "R.L. Tweedie"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Step-size adaptation based on non-local use of selection information", "author": ["A. Ostermeier", "A. Gawelczyk", "N. Hansen"], "venue": "In Proceedings of Parallel Problem Solving from Nature \u2014 PPSN III,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}], "referenceMentions": [{"referenceID": 10, "context": "In this paper, C is the identity and we investigate the so-called Cumulative Step-size Adaptation (CSA) [11,9].", "startOffset": 104, "endOffset": 110}, {"referenceID": 8, "context": "In this paper, C is the identity and we investigate the so-called Cumulative Step-size Adaptation (CSA) [11,9].", "startOffset": 104, "endOffset": 110}, {"referenceID": 0, "context": "Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 1, "context": "Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 2, "context": "Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 5, "context": "Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].", "startOffset": 82, "endOffset": 91}, {"referenceID": 3, "context": "Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "Arnold and Beyer studied the behavior of CSA on sphere, cigar and ridge functions [1,2,3,6] and on dynamical optimization problems where the optimum moves randomly [4] or linearly [5].", "startOffset": 180, "endOffset": 183}, {"referenceID": 3, "context": "A simplification of the update considers the squared length of the path [4]:", "startOffset": 72, "endOffset": 75}, {"referenceID": 6, "context": "(see [7] for the full proof) The idea of the proof is to use the symmetry of the normal distribution to show that for two random variables U \u223c \u03a81:\u03bb+1 and V \u223c \u03a81:\u03bb, for every event E1 where U < V , there exists another event E2 counterbalancing the effect of E1, i.", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "In [7] we extend this result on the step-size to |[Xt]1|, which diverges geometrically almost surely at the same rate, given E(exp(\u2212(\u2016xi\u2016/n\u22121)/(2d\u03c3))) < 1 with xi \u223c \u03be.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "The path P satisfies the conditions of Lemma 3 and exhibits an invariant measure [7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 6, "context": "We refer to [7] for the proof that P satisfies the right assumptions.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "We refer to [7] for the development of E( [ pt+1 ]4 1 ), since limits of space in the paper prevents us to present it here.", "startOffset": 12, "endOffset": 15}, {"referenceID": 7, "context": "In contrast, without cumulation (c = 1) and with \u03bb = 2, a random walk on ln(\u03c3) occurs, like for the (1, 2)-\u03c3SA-ES [8] (and also for the same symmetry reason).", "startOffset": 114, "endOffset": 117}], "year": 2017, "abstractText": "The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation, where the step size is adapted measuring the length of a so-called cumulative path. The cumulative path is a combination of the previous steps realized by the algorithm, where the importance of each step decreases with time. This article studies the CSA-ES on composites of strictly increasing with affine linear functions through the investigation of its underlying Markov chains. Rigorous results on the change and the variation of the step size are derived with and without cumulation. The step-size diverges geometrically fast in most cases. Furthermore, the influence of the cumulation parameter is studied.", "creator": "LaTeX with hyperref package"}}}