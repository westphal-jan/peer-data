{"id": "1206.1458", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Jun-2012", "title": "Dispelling Classes Gradually to Improve Quality of Feature Reduction Approaches", "abstract": "Feature reduction is an important concept which is used for reducing dimensions to decrease the computation complexity and time of classification. Since now many approaches have been proposed for solving this problem, but almost all of them just presented a fix output for each input dataset that some of them aren't satisfied cases for classification. In this we proposed an approach as processing input dataset to increase accuracy rate of each feature extraction methods. First of all, a new concept called dispelling classes gradually (DCG) is proposed to increase separability of classes based on their labels. Next, this method is used to process input dataset of the feature reduction approaches to decrease the misclassification error rate of their outputs more than when output is achieved without any processing. In addition our method has a good quality to collate with noise based on adapting dataset with feature reduction approaches. In the result part, two conditions (With process and without that) are compared to support our idea by using some of UCI datasets.", "histories": [["v1", "Thu, 7 Jun 2012 11:52:21 GMT  (317kb)", "http://arxiv.org/abs/1206.1458v1", "11 Pages, 5 Figure, 7 Tables; Advanced Computing: An International Journal (ACIJ), Vol.3, No.3, May 2012"]], "COMMENTS": "11 Pages, 5 Figure, 7 Tables; Advanced Computing: An International Journal (ACIJ), Vol.3, No.3, May 2012", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["shervan fekri ershad", "sattar hashemi"], "accepted": false, "id": "1206.1458"}, "pdf": {"name": "1206.1458.pdf", "metadata": {"source": "CRF", "title": "Dispelling Classes Gradually to Improve Quality of Feature Reduction Approaches", "authors": ["Shervan Fekri Ershad", "Sattar Hashemi"], "emails": ["shfekri@shirazu.ac.ir", "S_hashemi@shirazu.ac.ir"], "sections": [{"heading": null, "text": "DOI: 10.5121 / acij.2012.3310 95Feature reduction is an important concept that is used to reduce the dimensions to reduce the computational complexity and the timing of the classification. Since then, many approaches have been proposed to solve this problem, but almost all have only submitted one fixed output for each input data set, some of which are not considered satisfactory cases for classification. In this context, we proposed an approach to process the input data set to increase the accuracy of each feature extraction method. Initially, a new concept called Disdispelling Classes Gradual (DCG) is proposed to increase the separability of classes based on their designations. Next, this method is used to process input data sets of the feature reduction approaches in order to reduce the misclassification error rate of their results more strongly than if the output is achieved without any processing. Furthermore, our method has good quality to work with noise reduction based on the idea of matching data sets with two result sets (Im)."}, {"heading": "Keywords", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Feature Reduction, Dispelling Classes Gradually, Feature Extraction, Classes Separability", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1. INTRODUCTION", "text": "Classification methods have been too important since the mid-twentieth century, when artificial intelligence was established."}, {"heading": "1.1. PAPER ORGANIZION", "text": "The recollection of this paper is organized as follows: Section two refers to the description of the Dispelling Classes Studually technique (DCG) and the method of this estimation; Section three refers to the proposed approach; Section four is valid in terms of the quality of the proposed approach; Section five refers to the adjustment of necessary parameters; Section six refers to the description of the noise insensitivity of the proposed approach; and finally, the results and conclusions are included."}, {"heading": "2. DCG CONCEPT", "text": "In numerical data sets, the error rate in misclassification decreases with increasing distance between classes [14]. Therefore, it is necessary and advantageous to increase these distances. In basic mathematics, it is stated that if two different numbers are subtracted like two other fixed numbers, and this action is repeated for \u03b1 times, in extreme cases it is authentic that the distance between the two will be more than the first generation. This principle is described in Figure (1). If we add class names to numerical names and then assign class names, the separability of classes in extreme situations increases by subtracting each class name from its sample value in available characteristics. This topic is described in Figure (1). X \u03b1L X \u03b1L X! # X (1) Where Xi is the ith sample and Xj is the jth sample and L! is the class name of Xj and L! is the class name of Xj. Figure (2) UCL (the results of the application (Iris) \u03b1 are all the same."}, {"heading": "3. PROPOSED APPROACH", "text": "According to earlier part, DCG is too advantageous to increase the separability of classes. But DCG does its process based on class labels. So on test set, DCG is not useful because we do not know sample labels. But set for train, it can be useful. According to the introductory part, all feature extraction methods try to describe an accurate approach to creating projection matrix W based on train set. So, if DCG increases the separability of the train set before you create projection matrix and then dimensionally reduced output dataset is calculated, then misclassification error rate of output datasets decreases and output dataset Y classifier gets slightly more than the output of the original input set.To implement this method, we assume as matrix C = [$...]% & where N is the number of samples and '. (The number of labels we set and the results for the vector is 1)."}, {"heading": "4. JUSTFICATION", "text": "The main question mentioned in this article is why applying the DCG concept to input data sets could increase the quality of approaches to reducing characteristics. (A) The first signification is that new dimensions are smaller than the original dimensions (B) The second signification is that the separability of class capability is stored as much as possible. As described in section two after processing the input data set by DCG, both the slope of each eigenvector and the variance of each class remain constant, and this was because the transfer quantity of each sample is equal to the transfer quantity of the samples of the same class. Consequently, the main part of the approaches to reducing characteristics that find the reduction of eigenvectors, and the variance of the classes will remain constant, and only the mean of each class will be changed."}, {"heading": "5. PARAMETER SELECTION", "text": "The number of the DCG loop called \u03b1 is an important parameter. A should be tuned correctly to meet our goals. To select \u03b1, there are two important points to pay attention to. (I) According to the DCG part, DCG recognizes the separability of classes in an infinite situation. So, there are some values for \u03b1 on each dataset that \u03b1 does not disperse the classes sufficiently and it is possible that some of its values reduce the separability of classes. As the alignments of classes are hidden for distribution, the motion orientation of one class is like another and of course its velocities are never the same, so it is possible that classes come close to each other and their distances are less than primitive situations. This range of \u03b1 is referred to as the problem maker area (LPMR), so we need the values that are outside this range."}, {"heading": "SRDA", "text": "According to Table (1), we plan Figure (5) on the basis of numbers \u03b1 between 1 and 60. According to Figure (5), the accuracy of the output accuracy of the trait extraction has a tolerance, so there are some local Optima and a global optimum. The authors suggest mountaineering algorithms or a simple genetic algorithm (J.Hollands 1970s) with a fitness function based on the misclassification error rate of the output set to calculate the best value of \u03b1 (global optimum). Figure.5. Tolerance of the accuracy of the output accuracy of the trait extraction on the basis of different \u03b1"}, {"heading": "6. NOISE INSENSITIVITY", "text": "One of the greatest advantages of the proposed approach is noise insensitivity. Depending on the problems encountered, some of the numerical values in the dataset change, it is called noise. Thus, noise occurring certainly reduces the quality of the approaches to reducing characteristics. By using the proposed approach, after performing some DCG loops, the noise samples approach other samples bearing the same designations, so that the effect of noise samples will decrease in the performance of approaches to reducing characteristics."}, {"heading": "7. RESULTS", "text": "In this part, we compare the misclassification error rate of our idea and the original methods. In each table of this part, we first calculate the misclassification error rate of the output of one of the original UCI datasets, which is represented in the second columns of each row. Thus, the attribute reduction approaches used are SRDA [2], PCA [1], and REDA-SRDA [8]. Then, the DCG is applied to the input datasets. Thus, the attribute reduction approaches are applied to their processed input datasets and then classified. Misclassification rates are represented in the fourth column. Furthermore, the parameter \u03b1 is calculated with an SGA (simple genetic algorithm) for each dataset and highlighted in the third columns of each row. All misclassification error rates calculated with k-nearest neighborhood classifiers [12], and we have tried to use the best number of k for each dataset to show the error rate to reduce the SVM to the result."}, {"heading": "Huber-Man (UCI Dataset)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Lung-Cancer (UCI Dataset)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Glass (UCI Dataset)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Breast-Cancer (UCI Dataset)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8. CONCLUSION", "text": "The results showed that the results of characteristic reduction methods have an error rate if the input is processed less than the original dataset. Next, we presented some ways to calculate optimal numbers of DCG loops and mentioned some reasons for this. Results showed the quality of our idea based on various classifications. Some of the advantages of the proposed approach are: 1) suitability with almost all processing-level classification methods 2) noise immunity in the result of using labels 3) low computing complexity compared to some previous approaches 4) Accuracy rate can never be lower when using DCG than when using original datasets."}, {"heading": "9. FUTURE WORK", "text": "According to the fifth section, an interesting future direction of research is to study how to calculate the numbers of the DCG loop without any algorithms using only formula, which will reduce time and computational complexity."}], "references": [{"title": "Graph embedding and extensions: A general framework for dimensionality reduction", "author": ["S. Yan", "D. Xu", "B. Zhang", "H. Zhang", "Q. Yang", "S. Lin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "An efficient algorithm for large-scale discriminant analysis", "author": ["D. Cai", "X. He", "J. Han"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Robust Euclidean Embedding", "author": ["L. Cayton", "S. Dasgupta"], "venue": "In the Proc of the International Conference on Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Introduction to statistical pattern recognition", "author": ["K. Fukunnaga"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1991}, {"title": "Locality preserving projections", "author": ["X. He", "P. Niyogi"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Feature extraction using information theoretic learning", "author": ["K. Hild-II", "D. Erdogmus", "K. Torkkola", "C. Principe"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Kernel maximum entropy data transformation and an enhanced spectral clustering algorithm", "author": ["P. Huber", "R. Jenssen W", "T. Eltoft", "Girolami M", "D. Erdogmus"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "robust feature extraction via information theoretic learning", "author": ["Y. Xiao-Tong", "H. Bao-gang"], "venue": "In the Proc of the International Conference on machine learning", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Correntropy: Properties and applications in nongaussian signal processing", "author": ["W. Liu", "P.P. Pokharel", "J.C. Principe"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Breakdown points of Cauchy regression-scale estimators", "author": ["I. Mizera", "C. Muller"], "venue": "Statistics and Probability Letters,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "On measures of information and entropy", "author": ["A. Renyi"], "venue": "Proc of the 4th Berkeley Symposium on Mathematics, Statistics and Probability,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1961}, {"title": "Color texture classification approach based on combination of primitive pattern units and statistical features", "author": ["Fekri Ershad", "Sh"], "venue": "International journal of Multimedia and Its application\",", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Feature extraction by nonparametric mutual information maximization", "author": ["K. Torkkola"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "To increase quality of feature reduction approaches based on processing input dataset", "author": ["Fekri Ershad", "Sh", "S. Hashemi"], "venue": "In Proc of IEEE 3rd International Conference on Communication Software and Networks (ICCSN),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "So a new concept called feature reduction was inducted in the literature of AI (Fukunnaga 1991) [4].", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "Another popular technique called Locality Preserving Projections (LPP) (He & Niyogi, 2004) [5], has been proposed for linear feature extraction by preserving the local relationships within the data set.", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": ", 2007) [1], many classical linear feature extraction techniques are unified into", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": ", 2008) [2], was proposed based on ridge regression.", "startOffset": 8, "endOffset": 11}, {"referenceID": 13, "context": "DCG CONCEPT In the numerical datasets, misclassification error rate decreases with increasing the distance between classes[14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 7, "context": "For example one of the new feature extraction methods that is presented in ICML 2009 by XiaoTong and Bao-Gang is REDA [8].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "The feature reduction approaches which are used are SRDA [2], PCA [1] and REDA-SRDA [8].", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "The feature reduction approaches which are used are SRDA [2], PCA [1] and REDA-SRDA [8].", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "The feature reduction approaches which are used are SRDA [2], PCA [1] and REDA-SRDA [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 11, "context": "All of the misclassification error rates computed with k-nearest neighborhood classifier [12] and we tried to use best number of k for each dataset to decrease the error rate.", "startOffset": 89, "endOffset": 93}], "year": 2012, "abstractText": "Feature reduction is an important concept which is used for reducing dimensions to decrease the computation complexity and time of classification. Since now many approaches have been proposed for solving this problem, but almost all of them just presented a fix output for each input dataset that some of them aren\u2019t satisfied cases for classification. In this we proposed an approach as processing input dataset to increase accuracy rate of each feature extraction methods. First of all, a new concept called dispelling classes gradually (DCG) is proposed to increase separability of classes based on their labels. Next, this method is used to process input dataset of the feature reduction approaches to decrease the misclassification error rate of their outputs more than when output is achieved without any processing. In addition our method has a good quality to collate with noise based on adapting dataset with feature reduction approaches. In the result part, two conditions (With process and without that) are compared to support our idea by using some of UCI datasets.", "creator": "PScript5.dll Version 5.2.2"}}}