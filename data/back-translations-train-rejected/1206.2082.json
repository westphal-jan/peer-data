{"id": "1206.2082", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2012", "title": "Dimension Independent Similarity Computation", "abstract": "We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high dimensional sparse vectors. All of our results are provably independent of dimension, meaning apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension, thus the dimension can be very large. We study Cosine, Dice, Overlap, Conditional, and the Jaccard similarity measures. For Jaccard similiarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems at large scale using data from the social networking site Twitter.", "histories": [["v1", "Mon, 11 Jun 2012 02:19:27 GMT  (32kb)", "http://arxiv.org/abs/1206.2082v1", null], ["v2", "Thu, 29 Nov 2012 04:40:52 GMT  (35kb)", "http://arxiv.org/abs/1206.2082v2", null], ["v3", "Tue, 2 Apr 2013 03:54:28 GMT  (35kb)", "http://arxiv.org/abs/1206.2082v3", null], ["v4", "Thu, 23 May 2013 07:56:18 GMT  (34kb)", "http://arxiv.org/abs/1206.2082v4", null]], "reviews": [], "SUBJECTS": "cs.DS cs.AI cs.DC", "authors": ["reza bosagh zadeh", "ashish goel"], "accepted": false, "id": "1206.2082"}, "pdf": {"name": "1206.2082.pdf", "metadata": {"source": "CRF", "title": "Dimension Independent Similarity Computation", "authors": ["Reza Bosagh Zadeh", "Ashish Goel"], "emails": ["rezab@stanford.edu", "ashishg@stanford.edu"], "sections": [{"heading": null, "text": "ar Xiv: 120 6.20 82v1 [cs.DS] 1We present a series of algorithms for Dimension Independent Similarity Computation (DISCO) to calculate all the pairwise similarities between very high-dimensional sparse vectors. All our results are demonstrably independent of dimension, i.e. apart from the initial cost of trivial data reading, all subsequent operations are independent of dimension, so the dimension can be very large. We study Cosinus, Dice, Overlap, Conditional and the Jaccard similarity measurements. For Jaccard similarity, we include an improved version of MinHash. Our results are aligned with the MapReduce framework. We empirically validate our theorems on a large scale using data from the social networking site Twitter.Keywords: Cosinus, Jaccard, Overlap, Dice, Similarity, MapReduce, dimensionally independent."}, {"heading": "1. Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2. Formal Preliminaries", "text": "In fact, it is so that most of us are able to outtrump ourselves, and that they are able to outtrump themselves. (...) Most of us have not been able to outtrump ourselves. (...) Most of us have not been able to outtrump ourselves. (...) Most of us have not been able to outtrump ourselves. (...) Most of us have not been able to outtrump ourselves. (...) \"Most of us have been able to outtrump ourselves. (...)\" (...) \"(...) We have done it. (...)\" (...) Most of us have been able to outtrump ourselves. \"(...)\" Most of us have been able to outtrump ourselves. \"(...)\" (... \"We have done it.\" (...). \"We have done it.\" (...). \"We have done it.\" (...). \"We have done it.\" (... \"We have done it.\" (...). \"We have done it.\" (... \"We have done it.\" (...). \"We have done it.\" (... \"We have done it.\" We have done it. \"(...).\" We have done it. \"We have done it.\" (... \"We have done it.\" We have done it. \"(...).\" We have done. \"We have done it.\" We have done. \"We have done it.\" (... \"We have done.\" We have done. \"We have done it.\" We have done. \"(...\"). \"We have done.\" We have done. \"We have done it.\" We have done. \"(...\"). \"We have done it.\" We have done. \"We have done.\" (... \").\""}, {"heading": "3. Related Work", "text": "Previously, the all-pair similarity problem was investigated in (Broder et al., 1997; Broder et al., 1997), in connection with the identification of nearly duplicate web pages (Broder et al., 1997). We describe MinHash later in Section 4.2. We are improving the vanilla implementation of MinHash on MapReduce to achieve better shuffle sizes without (effectively) a loss of accuracy. We prove these results in Section 4.2, and verify them experimentally. There is much work that discusses all pairs similarity calculation problem. In (Elsayed et al., 2008), the MapReduce framework is targeted, but there is still a linear dependence on dimensionality and no proven results. In (Lin, 2009), all pairs are calculated on MapReduce, but there is a focus on the life sciences."}, {"heading": "4. Results", "text": "The naive algorithm for calculating similarities first calculates dot products, then simply divides by what is necessary to obtain a similarity scale, i.e. in an implementation of MapReduce: 1. Specified document t, card with NaiveMapper (algorithm 1) 2. Reduce the use of the NaiveReducer (algorithm 2) algorithm 1 NaiveMapper (t) for all pairs (w1, w2) in t do emit ((((w1, w2) \u2192 1) End for algorithm 2 NaiveReducer ((((w1, w2), < r1,..., rR >) a = \u2211 R i = 1 rioutput a \u221a # (w1) # (w2) The above steps calculate all Dot products, which are then scaled by corresponding factors for each of the similarity. Instead of using naive algorithms, we modify the mapper of the naive algorithm."}, {"heading": "4.1 Cosine Similarity", "text": "To eliminate the dependence on N, we replace the emission function with algorithm 5.Algorithm 5 CosineSampleEmit (w1, w2) with the highest probability (w1, w2). (w1, w2, w2) Note that the more popular word is, the less likely it is to be output. (w1, w2) This is the key observation that leads to a shuffle size regardless of dimension. (w1, w2) We use a slightly different reducer that instead of the calculation # (w1, w2), computes cos (w1, w2) directly without intermediate steps. The exact estimator is in the proof of theorem 1.Since the CosineSampleEmit algorithms only guarantee to produce the correct similarity in expectation, we must show that the expected value will most likely be achieved."}, {"heading": "4.2 Jaccard Similarity", "text": "We improve the MinHash scheme to be much more efficient with a smaller shuffle size.Let h (t) be a hash function that maps documents to distinct numbers in [0, 1], and for each word w we define g (w1) (as MinHash of w) to be the minimum value of h (t) over all t that contain w. Then g (w1) = g (w2) exactly when the minimum value of the union # (w1) + # (w2) \u2212 # (w1, w2) is in the intersection # (w1, w2).ThusPr [g (w1) = g (w2) = (w1, w2) # (w1) # (w1) # (w2) \u2212 jp (w2) \u2212 jp) is in the intersection # (w1, w2)."}, {"heading": "5. Cosine Similarity in Streaming Model", "text": "We briefly leave the MapReduce framework and instead work in any \"streaming\" frame. (In this setting, the data is streamed from dimension to dimension by a single machine that can answer questions of form at any time. (In this setting, we will only describe the similarity between points x and y taking into account all previous inputs?) The most important performance metric is how much memory the machine uses and requests are answered in constant time. (We describe the algorithm only for cosmic similarity, and an almost identical algorithm will work for dice, overlaps, and conditional similarity. (Our algorithm will be very similar to the map reduction, but in place of emitting pairs for a reduced phase, we will instead insert them into a hash map H marked by pairs of words, holding a bag of emissions with each entry. H is used to track emissions by storing them in a single machine."}, {"heading": "6. Correctness and Shuffle Size Proofs for other Similarity Measures", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Overlap Similarity", "text": "The overlap similarity follows the same pattern as we have used for cosmic similarity, so we only explain the parts that are different, and the emission function changes to algorithm 8.Algorithm 8 OverlapSampleEmit (w1, w2) Probably we emit (w1, w2). The shuffle size for OverlapSampleEmit results from the following theory.Theorem 7 The expected shuffle size for OverlapSampleEmit is almost identical to cosmic similarity (w1, w2).Proof The expected contribution of each word pair yields the shuffle size: D-i = 1D-j = i + 1p\u0445 # (wi, wj) min (wj), # (wj) \u2264 p-D = 1D-unequal (j) = 1 (wi) = unequal (wi)."}, {"heading": "6.2 Dice Similarity", "text": "The emission function changes to algorithm 9.Algorithm 9 DiceSampleEmit (w1, w2) Probably we emit (w1) + # (w2) (w1, w2) \u2192 1) The proof of correctness is almost identical to cosmic similarity, so we do not repeat it. The shuffle size for DiceSampleEmit is derived from the following theorem. Theorem 8 The expected shuffle size for DiceSampleEmit is O (DL log (D) / year. The expected contribution of each word pair is the shuffle size: 2 D = 1D = j = i + 1pigung # (wi) # (wi, wj) \u2264 i = 1D = 1 # (wi) The highest probability (wi) is that the first (vival) (wi) (highest probability) is # wi (wi)."}, {"heading": "6.3 Conditional Similarity", "text": "The conditional similarity follows the same pattern we have used for cosmic similarity, so we only explain the parts that are different. the emission function changes to algorithm 10 and the reducer to algorithm 11. Algorithm 10 ConditionalSampleEmit (w1, w2) Probability N # (w1) # (w2) emit ((w1, w2) \u2192 1) The proof of correctness is almost identical to cosmic similarity, so we do not reformulate it. The shuffle size for ConditionalSampleEmit is derived from the following theorem, which slightly assumes algorithm 11 DISCOCondReducer (w1, w2), < r1,., rR >) a = \u2211 R i = 1 rioutput adifferent from the previous similarity (w1), since it requires an independent Co-occurences # assumption. Theorem 9 Assuming independent co-urence # wwww1 (w1) www1 (w1) (w1) = 1 (w1)."}, {"heading": "7. Experiments", "text": "We use data from the social network Twitter. Twitter is currently a very popular social networking platform. Users interact with Twitter via a web interface, instant messaging clients or sending mobile text messages. Public updates by users are available worldwide, and the vast majority of Twitter accounts are public. These public tweets provide a large real-time corpus of what is happening in the world. The data we use in this study was created by using N = 198, 134, 530 public tweets. The number of dimensions N = 198, 134, 530 in our data corresponds to the number of tweets, and each tweet is a document with a maximum size of 140 characters, which is a small cap for L. These documents are ideal for our framework as our shuffle size depends on L, which in this case is very small."}, {"heading": "7.1 Shuffle Size vs Accuracy", "text": "We have to optimize two parameters to achieve compromise accuracy and shuffle size: B and P. However, since they only occur as a p / B ratio in our algorithms, we simply use this as a compromise parameter. The reason we separated p and B was that the theorems go through well, but in reality we see only one optimization parameter. We increase p / A exponentially on the x-axis and plot the ratio of DISCO shuffle size to naive implementation. In all cases, we can achieve a 90% reduction in shuffle size without losing much of accuracy, as in Figures 2, 4, and 6. The accuracy we report is similar in terms of real cosines, cubes, and overlaps."}, {"heading": "7.2 Error vs Similarity Magnitude", "text": "To see this empirically, we will record the average error of all pairs that have a true similarity, in Figures 1, 3, 5, 9, 7, and 8. Note that the reason that large parts of the error in these diagrams are constant is because there are very few pairs with very great similarities, and therefore the error remains constant, while the difference between two pairs of similarities is so great."}, {"heading": "8. Conclusions and Future Directions", "text": "All our results are demonstrably independent of the dimension, i.e. apart from the initial cost of trivial data input, all subsequent operations are independent of the dimension, so the dimension can be very large. Although we use the models MapReduce (Dean and Ghemawat, 2008) and Streaming to discuss shuffle size and memory, the sampling strategy we use can be generalized to other frames. We assume that the DISCO sampling strategy will be useful when calculating a number between 0 and 1 by taking the ratio of an unknown number (the dot product in our case) to a known number (e.g. for cosmic similarity). This is a description at a very high level, and we give five concrete examples along with proofs and experiments."}], "references": [{"title": "Keyword generation for search engine advertising using semantic similarity between terms", "author": ["V. Abhishek", "K. Hosanagar"], "venue": "In EC", "citeRegEx": "Abhishek and Hosanagar.,? \\Q2007\\E", "shortCiteRegEx": "Abhishek and Hosanagar.", "year": 2007}, {"title": "Efficient exact set-similarity joins", "author": ["A. Arasu", "V. Ganti", "R. Kaushik"], "venue": "In VLDB", "citeRegEx": "Arasu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Arasu et al\\.", "year": 2006}, {"title": "Agglomerative clustering of a search engine query log", "author": ["D. Beeferman", "A. Berger"], "venue": "In SIGKDD", "citeRegEx": "Beeferman and Berger.,? \\Q2000\\E", "shortCiteRegEx": "Beeferman and Berger.", "year": 2000}, {"title": "The hadoop distributed file system: Architecture and design", "author": ["D. Borthakur"], "venue": "Hadoop Project Website,", "citeRegEx": "Borthakur.,? \\Q2007\\E", "shortCiteRegEx": "Borthakur.", "year": 2007}, {"title": "Semantic similarity between search engine queries using tem", "author": ["S. Chien", "N. Immorlica"], "venue": null, "citeRegEx": "Chien and Immorlica.,? \\Q2006\\E", "shortCiteRegEx": "Chien and Immorlica.", "year": 2006}, {"title": "Similarity search in high dimensions via hashing", "author": ["A. Gionis", "P. Indyk", "R. Motwani"], "venue": "In VLDB", "citeRegEx": "Gionis et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Gionis et al\\.", "year": 1999}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "STOC", "citeRegEx": "Indyk and Motwani.,? \\Q1998\\E", "shortCiteRegEx": "Indyk and Motwani.", "year": 1998}, {"title": "Brute force and indexed approaches to pairwise document similarity comparisons with mapreduce", "author": ["J. Lin"], "venue": "SIGIR", "citeRegEx": "Lin.,? \\Q2009\\E", "shortCiteRegEx": "Lin.", "year": 2009}, {"title": "Web-scale distributional similarity and entity set expansion", "author": ["P. Pantel", "E. Crestan", "A. Borkovsky", "A.M. Popescu", "V. Vyas"], "venue": "EMNLP", "citeRegEx": "Pantel et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pantel et al\\.", "year": 2009}, {"title": "Predicting click-through rate using keyword clusters", "author": ["M. Regelson", "D. Fain"], "venue": "In Proceedings of the Second Workshop on Sponsored Search Auctions,", "citeRegEx": "Regelson and Fain.,? \\Q2006\\E", "shortCiteRegEx": "Regelson and Fain.", "year": 2006}, {"title": "A web-based kernel function for measuring the similarity of short text snippets", "author": ["M. Sahami", "T.D. Heilman"], "venue": "WWW", "citeRegEx": "Sahami and Heilman.,? \\Q2006\\E", "shortCiteRegEx": "Sahami and Heilman.", "year": 2006}, {"title": "Efficient set joins on similarity predicates", "author": ["S. Sarawagi", "A. Kirpal"], "venue": "In ACM SIGMOD", "citeRegEx": "Sarawagi and Kirpal.,? \\Q2004\\E", "shortCiteRegEx": "Sarawagi and Kirpal.", "year": 2004}, {"title": "Evaluating similarity measures: a large-scale study in the orkut social network", "author": ["E. Spertus", "M. Sahami", "O. Buyukkokten"], "venue": "In SIGKDD", "citeRegEx": "Spertus et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Spertus et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 9, "context": "There are many examples, including \u2022 Advertiser keyword suggestions: When targeting advertisements via keywords, it is useful to expand the manually input set of keywords by other similar keywords, requiring finding all keywords more similar than a high threshold (Regelson and Fain, 2006).", "startOffset": 264, "endOffset": 289}, {"referenceID": 0, "context": "\u2022 Web Search: Rewriting queries given to a search engine is a common trick used to expand the coverage of the search results (Abhishek and Hosanagar, 2007).", "startOffset": 125, "endOffset": 155}, {"referenceID": 10, "context": "We focus on 5 different similarity measures, including cosine similarity which is very popular and produces high quality results across different domains (Chien and Immorlica, 2005; Chuang and Chien, 2005; Sahami and Heilman, 2006; Spertus et al., 2005).", "startOffset": 154, "endOffset": 253}, {"referenceID": 12, "context": "We focus on 5 different similarity measures, including cosine similarity which is very popular and produces high quality results across different domains (Chien and Immorlica, 2005; Chuang and Chien, 2005; Sahami and Heilman, 2006; Spertus et al., 2005).", "startOffset": 154, "endOffset": 253}, {"referenceID": 3, "context": "Our results and theorems hold across any MapReduce implementation such as Hadoop (Borthakur, 2007)(Gates et al.", "startOffset": 81, "endOffset": 98}, {"referenceID": 7, "context": "In (Lin, 2009), all pairs are on computed on MapReduce, but there is a focus on the life sciences domain.", "startOffset": 3, "endOffset": 14}, {"referenceID": 1, "context": "The all-pairs similarity search problem has also been addressed in the database community, where it is known as the similarity join problem (Arasu et al., 2006; Chaudhuri et al., 2006; Sarawagi and Kirpal, 2004).", "startOffset": 140, "endOffset": 211}, {"referenceID": 11, "context": "The all-pairs similarity search problem has also been addressed in the database community, where it is known as the similarity join problem (Arasu et al., 2006; Chaudhuri et al., 2006; Sarawagi and Kirpal, 2004).", "startOffset": 140, "endOffset": 211}, {"referenceID": 5, "context": "There is large body of work on the nearest neighbors problem, which is the problem of finding the k nearest neighbors of a given query point(Charikar, 2002; Fagin et al., 2003; Gionis et al., 1999; Indyk and Motwani, 1998).", "startOffset": 140, "endOffset": 222}, {"referenceID": 6, "context": "There is large body of work on the nearest neighbors problem, which is the problem of finding the k nearest neighbors of a given query point(Charikar, 2002; Fagin et al., 2003; Gionis et al., 1999; Indyk and Motwani, 1998).", "startOffset": 140, "endOffset": 222}, {"referenceID": 8, "context": "In (Pantel et al., 2009), the authors propose a highly scalable term similarity algorithm, implemented in the MapReduce framework, and deployed over a 200 billion word crawl of the Web to compute pairwise similarities between terms.", "startOffset": 3, "endOffset": 24}, {"referenceID": 2, "context": "Other related work includes clustering of web data (Beeferman and Berger, 2000; Chien and Immorlica, 2005; Sahami and Heilman, 2006; Spertus et al., 2005).", "startOffset": 51, "endOffset": 154}, {"referenceID": 10, "context": "Other related work includes clustering of web data (Beeferman and Berger, 2000; Chien and Immorlica, 2005; Sahami and Heilman, 2006; Spertus et al., 2005).", "startOffset": 51, "endOffset": 154}, {"referenceID": 12, "context": "Other related work includes clustering of web data (Beeferman and Berger, 2000; Chien and Immorlica, 2005; Sahami and Heilman, 2006; Spertus et al., 2005).", "startOffset": 51, "endOffset": 154}], "year": 2012, "abstractText": "We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high dimensional sparse vectors. All of our results are provably independent of dimension, meaning apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension, thus the dimension can be very large. We study Cosine, Dice, Overlap, Conditional, and the Jaccard similarity measures. For Jaccard similiarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems at large scale using data from the social networking site Twitter.", "creator": "LaTeX with hyperref package"}}}