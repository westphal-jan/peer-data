{"id": "1206.2190", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2012", "title": "Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation", "abstract": "This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA). Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture. Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf's law to reduce the total communication cost in PBP. Extensive experiments on different data sets demonstrate that CE-PBP achieves a higher topic modeling accuracy and reduces more than 80% communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm.", "histories": [["v1", "Mon, 11 Jun 2012 13:00:51 GMT  (1064kb)", "http://arxiv.org/abs/1206.2190v1", "9 pages, 5 figures"]], "COMMENTS": "9 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jian-feng yan", "zhi-qiang liu", "yang gao", "jia zeng"], "accepted": false, "id": "1206.2190"}, "pdf": {"name": "1206.2190.pdf", "metadata": {"source": "CRF", "title": "Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation", "authors": ["Jian-Feng Yan"], "emails": ["j.zeng@ieee.org"], "sections": [{"heading": null, "text": "ar Xiv: 120 6.21 90v1 [cs.L"}, {"heading": "1 Introduction", "text": "Thematic modeling for massive datasets has recently aroused intense research interest as large-scale datasets such as the collections of images and documents are becoming increasingly common in Hong Kong. [1, 2, 3, 4] Online and parallel thematic modeling algorithms were two important strategies for massive datasets. The former process the massive flow of data through mini-batches and discard the processed mini-batch at a glance [2, 4]. The latter use the parallel architecture to accelerate modeling by multi-core / processor and more storage resources [1, 3]. Although online theme modeling algorithms use less computing resources, their topic modeling accuracy depends on several heuristic parameters, including mini-batch size [2, 4] and is often comparable or less than batch learning algorithms. In practice, online algorithms are often 2 x 5 times faster than batch algorithms during 700 [4]."}, {"heading": "2 Parallel Belief Propagation (PBP)", "text": "LDA assigns a series of semantic subject descriptions to, z = {zkw, d \u00b2, to explain non-zero elements in the document-word-co-occurrence-matrix (cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. on this cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf. cf."}, {"heading": "3 Reduce Communication Costs by Zipf\u2019s Law", "text": "Zipf's Law [10] reveals that the word frequency r has the following relation to the word frequency f in many natural language records (32)."}, {"heading": "4 Experiments", "text": "We use four datasets 12: KOS, NIPS, ENRON and WIKI in Table 1. Since KOS is a relatively smaller dataset, we use it for parameter optimization in CE-PBP. The number of topics, K = 100, is specified in all experiments, except for special instructions. The number of training iterations T = 500. We use the same hyperparameters \u03b1 = \u03b2 = 0.01 for CE-PBP and PGS. Due to limited computing resources, we use only 32 processors to compare the performance of CE-PBP and PGS. We find that the communication costs follow the equations (8) and (11), so that our results can be safely generalized to more processors in the parallel architecture."}, {"heading": "4.1 Parameters for CE-PBP", "text": "The piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piqutery of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piquancy of the Piqutery of the Piqutery of the Piquancy of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the Piqutery of the"}, {"heading": "5 Conclusions", "text": "In order to reduce the communication costs that greatly impair the scalability of parallel subject modeling, we have proposed CE-PBP, which combines parallel belief dissemination (PBP) with Zipf's law solution for different communication rates. Extensive experiments with different data sets confirm that CE-PBP is faster, more accurate and more efficient than the state-of-the-art PGS algorithm. Since many data types studied in the physical and social sciences can be approximated by Zipf's law, our approach could provide a new way to accelerate other parallel algorithms. In future work, we will investigate how to reduce the size K of the global parameter matrix p? W \u00b7 K in communication. We also plan to expand the CE-PBP algorithm to learn more complicated topic models such as the hierarchical dirichlet process (HDP) [1]."}], "references": [{"title": "Distributed algorithms for topic models", "author": ["D. Newman", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Online inference of topics with latent dirichlet allocation", "author": ["K.R. Canini", "L. Shi", "T.L. Griffiths"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Using variational inference and mapreduce to scale topic modeling", "author": ["Ke Zhai", "Jordan L. Boyd-Graber", "Nima Asadi"], "venue": "CoRR, abs/1107.3765,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Online learning for latent dirichlet allocation", "author": ["M.D. Hoffman", "D.M. Blei", "F. Bach"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Learning topic models by belief propagation", "author": ["Jia Zeng", "William K. Cheung", "Jiming Liu"], "venue": "CoRR, abs/1109.3437,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Power laws, pareto distributions and zipf\u2019s law", "author": ["M.E.J. Newman"], "venue": "Contemporary physics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Plda: Parallel latent dirichlet allocation for large-scale applications", "author": ["Y. Wang", "H. Bai", "M. Stanton", "W.Y. Chen", "E. Chang"], "venue": "Algorithmic Aspects in Information and Management,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proc. Natl. Acad. Sci.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Human behavior and the principle of least effort", "author": ["G.K. Zipf"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1949}, {"title": "Distributed algorithms for topic models", "author": ["D. Newman", "A. Asuncion", "P. Smyth", "M. Welling"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Plda+: Parallel latent dirichlet allocation with data placement and pipeline processing", "author": ["Z. Liu", "Y. Zhang", "E.Y. Chang", "M. Sun"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Topic modeling for massive data sets has attracted intensive research interests recently, because large-scale data sets such as collections of images and documents are becoming increasingly common [1, 2, 3, 4].", "startOffset": 197, "endOffset": 209}, {"referenceID": 1, "context": "Topic modeling for massive data sets has attracted intensive research interests recently, because large-scale data sets such as collections of images and documents are becoming increasingly common [1, 2, 3, 4].", "startOffset": 197, "endOffset": 209}, {"referenceID": 2, "context": "Topic modeling for massive data sets has attracted intensive research interests recently, because large-scale data sets such as collections of images and documents are becoming increasingly common [1, 2, 3, 4].", "startOffset": 197, "endOffset": 209}, {"referenceID": 3, "context": "Topic modeling for massive data sets has attracted intensive research interests recently, because large-scale data sets such as collections of images and documents are becoming increasingly common [1, 2, 3, 4].", "startOffset": 197, "endOffset": 209}, {"referenceID": 1, "context": "The former processes the massive data stream by mini-batches, and discards the processed mini-batch after one look [2, 4].", "startOffset": 115, "endOffset": 121}, {"referenceID": 3, "context": "The former processes the massive data stream by mini-batches, and discards the processed mini-batch after one look [2, 4].", "startOffset": 115, "endOffset": 121}, {"referenceID": 0, "context": "The latter uses the parallel architecture to speed up the topic modeling by multi-core/processor and more memory resources [1, 3].", "startOffset": 123, "endOffset": 129}, {"referenceID": 2, "context": "The latter uses the parallel architecture to speed up the topic modeling by multi-core/processor and more memory resources [1, 3].", "startOffset": 123, "endOffset": 129}, {"referenceID": 1, "context": "Although online topic modeling algorithms use less computational resources, their topic modeling accuracy depends on several heuristic parameters including the mini-batch size [2, 4], and is often comparable or less than batch learning algorithms.", "startOffset": 176, "endOffset": 182}, {"referenceID": 3, "context": "Although online topic modeling algorithms use less computational resources, their topic modeling accuracy depends on several heuristic parameters including the mini-batch size [2, 4], and is often comparable or less than batch learning algorithms.", "startOffset": 176, "endOffset": 182}, {"referenceID": 3, "context": "In practice, online algorithms are often 2 \u223c 5 times faster than batch algorithms [4], while parallel algorithms can get 700 times faster under 1024 processors [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "In practice, online algorithms are often 2 \u223c 5 times faster than batch algorithms [4], while parallel algorithms can get 700 times faster under 1024 processors [1].", "startOffset": 160, "endOffset": 163}, {"referenceID": 4, "context": "In this paper, we propose a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA) [5], one of the simplest topic models.", "startOffset": 152, "endOffset": 155}, {"referenceID": 5, "context": "we extend the synchronous BP algorithm [6] to PBP on the parallel architecture for training LDA.", "startOffset": 39, "endOffset": 42}, {"referenceID": 6, "context": "Second, to reduce extensive communication/synchrononization delays, we use Zipf\u2019s law [7] to determine the communication rate of synchronizing global parameters in PBP.", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "Extensive experiments confirm that CE-PBP reduces around 85% communication time, and achieves a much higher topic modeling accuracy than the state-of-the-art parallel Gibbs sampling algorithm (PGS) [1, 8].", "startOffset": 198, "endOffset": 204}, {"referenceID": 7, "context": "Extensive experiments confirm that CE-PBP reduces around 85% communication time, and achieves a much higher topic modeling accuracy than the state-of-the-art parallel Gibbs sampling algorithm (PGS) [1, 8].", "startOffset": 198, "endOffset": 204}, {"referenceID": 8, "context": "For simplicity, we consider the smoothed LDA with fixed symmetric hyperparameters provided by users [9].", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "The collapsed Gibb sampling (GS) [9] is a Markov Chain Monte Carlo (MCMC) sampling technique to infer the marginal distribution or message, \u03bcw,d,i(k) = p(z k w,d,i = 1), where 1 \u2264 i \u2264 xw,d is the word token index.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "Unlike GS, BP [6] infers messages, \u03bcw,d(k) = p(z w,d = 1), without sampling in order to keep all uncertainties of messages.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "So, BP is often faster than GS by scanning a significantly less number of elements (NNZ \u226a \u2211 w,d xw,d) at each training iteration [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 5, "context": "(7) Because PBP follows the synchronous schedule, it produces exactly the same results of the synchronous BP [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 0, "context": "Notice that the parallel Gibbs sampling algorithm (PGS) [1, 8] is an approximate solution to GS in (1), because GS uses an asynchronous schedule for message passing [6].", "startOffset": 56, "endOffset": 62}, {"referenceID": 7, "context": "Notice that the parallel Gibbs sampling algorithm (PGS) [1, 8] is an approximate solution to GS in (1), because GS uses an asynchronous schedule for message passing [6].", "startOffset": 56, "endOffset": 62}, {"referenceID": 5, "context": "Notice that the parallel Gibbs sampling algorithm (PGS) [1, 8] is an approximate solution to GS in (1), because GS uses an asynchronous schedule for message passing [6].", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "(8) Notice that the communication cost of PGS [1, 8] can be also calculated as (8), but with the following major difference.", "startOffset": 46, "endOffset": 52}, {"referenceID": 7, "context": "(8) Notice that the communication cost of PGS [1, 8] can be also calculated as (8), but with the following major difference.", "startOffset": 46, "endOffset": 52}, {"referenceID": 0, "context": "Because this communication cost is still a bottleneck, to reduce (8), PGS changes the communication rate by running (6) and (7) at every T \u2032 > 1 training iterations [1], so that the total communication cost can be reduced to a fraction 1/T \u2032 of (8).", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "However, the low communication rate slows down the convergence and degrades the overall topic modeling performance of PGS [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 9, "context": "Zipf\u2019s law [10] reveals that the word frequency rank r has the following relationship with the word frequency f in many natural language data sets ,", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "1 to investigate both training time and predictive perplexity, where the predictive perplexity on test data set is calculated as in [11, 6].", "startOffset": 132, "endOffset": 139}, {"referenceID": 5, "context": "1 to investigate both training time and predictive perplexity, where the predictive perplexity on test data set is calculated as in [11, 6].", "startOffset": 132, "endOffset": 139}, {"referenceID": 5, "context": "Such results are consistent with previous results on comparing BP and GS [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "Recently, Google reports an improved version of PGS called PGS+[12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "Also, we plan to extend CE-PBP algorithm to learn more complicated topic models such as hierarchical Dirichlet process (HDP) [1].", "startOffset": 125, "endOffset": 128}], "year": 2012, "abstractText": "This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA). Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture. Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf\u2019s law to reduce the total communication cost in PBP. Extensive experiments on different data sets demonstrate that CE-PBP achieves a higher topic modeling accuracy and reduces more than 80% communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm.", "creator": "LaTeX with hyperref package"}}}