{"id": "1206.3099", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2012", "title": "Sparse Distributed Learning Based on Diffusion Adaptation", "abstract": "This article proposes diffusion LMS techniques for distributed estimation over adaptive networks that are able to exploit sparsity in the underlying system model. The approach relies on convex regularization, common in compressive sensing, to enhance the detection of sparsity via a diffusive process over the network. The resulting algorithms endow networks with learning abilities and allow them to learn the sparse structure from the incoming data in real-time, and also to track variations in the sparsity of the model. We provide convergence and mean-square performance analysis of the proposed method and show under what conditions it outperforms the unregularized diffusion version. We also show how to adaptively select the regularization parameter. Simulation results illustrate the advantage of the proposed filters for sparse data recovery.", "histories": [["v1", "Thu, 14 Jun 2012 13:10:35 GMT  (91kb)", "https://arxiv.org/abs/1206.3099v1", null], ["v2", "Mon, 12 Nov 2012 23:33:32 GMT  (972kb)", "http://arxiv.org/abs/1206.3099v2", "to appear in IEEE Trans. on Signal Processing, 2013"]], "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["paolo di lorenzo", "ali h sayed"], "accepted": false, "id": "1206.3099"}, "pdf": {"name": "1206.3099.pdf", "metadata": {"source": "CRF", "title": "Sparse Distributed Learning Based on Diffusion Adaptation", "authors": ["Paolo Di Lorenzo"], "emails": ["pubs-permissions@ieee.org.", "dilorenzo@infocom.uniroma1.it,", "sayed@ee.ucla.edu."], "sections": [{"heading": null, "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "II. SPARSE DISTRIBUTED ESTIMATION OVER ADAPTIVE NETWORKS", "text": "We assume that the data {dk (i), uk, i} collected by the various nodes are connected with an unknown sparse vector where via the linear model: dk (i) = uk, iw o + vk (i) (1), where vk (i) is a zero mean random variable with variance \u03c32v, k, independent of ul, j for all l and j and independent of vl (j) for l 6 = k and i 6 = j. Linear models of form (1) frequently occur in applications, November 14, 2012 DRAFT5 and are able to represent many cases of interest.The cooperative sparse estimation problem is cast as a search for the optimal estimator, which in a fully distributed way minimizes the following cost function: Jglob (w) = N \u0445 k = 1E | dk (i) \u2212 uk, iw | f (2), where the node is transformed, and f (w) is a function consolidated between the individual by a parameter."}, {"heading": "A. Adaptive Diffusion Strategy", "text": "We follow the approach of Jglob (w) = J lock (w), [11] to derive the distributed strategies for minimizing Jglob (w) (w). (3) We start by being able to specify the cost of avoiding nodes and avoiding nodes near nodes and nodes near nodes and nodes. (3) The coefficients of the nodes are, of course, zero when nodes and nodes are connected. (3) The coefficients of the global cost function in (2) can be expressed as: Jglob (w). (3)"}, {"heading": "B. Sparse Regularization", "text": "Before proceeding with the discussion, we will comment on the regulation function f (w) in (2). A sparse vector where generally there are only a few relatively large coefficients interspersed between many negligible ones, and the location of the non-zero elements is often unknown beforehand. However, in some applications we can consider some upper limits in terms of the number of non-zero elements. As the 0 standard in (23) is not convex, we cannot use it directly. Therefore, motivated by LASSO [28] and work on compressing elements [29], we will first consider the following 1-standard convex choice for a regulation function: f1 (w) = 0 standard."}, {"heading": "III. MEAN-SQUARE PERFORMANCE ANALYSIS", "text": "From now on we consider the estimates wk, i as realizations of a random process wk, i > l, i and analyze the performance of the sparse diffusion algorithm in terms of its behavior in the mean square. To do this, we perform the error quantities w-k, i = wo-wk, i... o-k, i = wo-k, i (29) We also perform the block diagonal matrixM = diag {\u00b51IM,., i, w-i = w-1, i... and the advanced block weighting matricesC = C IM, A = A IM (31), where f-icisions (29) we perform the block diagonal matrixM = diag {\u00b51IM,.,. o-sistic,."}, {"heading": "A. Convergence in the Mean", "text": "Letter D, EDi = diag (N) < l = 1cl, 1Ru, l,. (N), l = 1cl, NRu, l) (38) Taking into account the expectations of both sides of (37) and assumption 1, we come to the conclusion that the mean error vector develops according to the following dynamics: Ew, i = AT [I \u2212 MD], Ew, i \u2212 1 + \u03b3ATME, f (wi \u2212 1) (39) The following theorem guarantees the asymptotic mean stability of the diffusion strategy (21) and provides a closed form expression for the weight distortion due to the use of the regularization term. Theorem 1 (stability in the mean) and assumption 1 apply to the asymptotic mean stability of the diffusion strategy (21)."}, {"heading": "B. Convergence in Mean-Square", "text": "We investigate the behavior of the continuous state measurement quadrature deviation, E-W-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D-D"}, {"heading": "C. Comparison with Unregularized ATC Diffusion", "text": "We will now examine under what conditions the sparse diffusion filter (21) with respect to meansquare performance (61) will be its unregulated counterpart (64) with respect to the MSD expression (58) with respect to the k-ten node, we will note that the first term on the RHS matches the MSD of the default diffusion algorithm (64) with respect to the default diffusion algorithm (64) with respect to the second term with respect to the (58) with respect to regulation. Then the first term on the RHS > 0 and 0 < p < p < p < p p p; p; p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "D. Adaptation of the Regularization Parameter", "text": "In order to equip networks with the capacity for adaptive exploitation and to track the sparseness of the system model, we now propose a systematic approach to the selection of the regulation parameter (73). Thus, we allow the sparseness parameter to be considered as an approximate case, i.e. that steps similar to those in Section III.B are performed, we can replace (51) with the conditional relationship in which the sparseness parameters (51) are replaced by (45) and the sparseness parameters (1). (1) + Tr [ATMGTMA] + [66), in which the sparseness parameters are given by (45) and the sparseness parameters (2), i (2)."}, {"heading": "IV. SIMULATION RESULTS", "text": "In this section, we provide some numerical examples to illustrate the performance of the sparse diffusion algorithm. In the first example, we compare the performance of the sparse diffusion strategy with respect to the standard diffusion, taking into account consistent values of the regulation parameter \u03b3. The second example shows the advantages of adjusting the sparsity parameter to (80). Numerical example 1: Performance: We are looking at a connected network consisting of 20 nodes. The topology of the network is shown in Fig. 1. Regressors uk, i are size M = 50 and are zero-average white Gaussians with covariance matrices Ru, k = \u03c32u, kIM 2 u, with the top of Fig. 2. The background noise performance of each node is shown in the same way. The first example aims to show the tracking and constant performance of the diffusion algorithms for diffusion."}, {"heading": "V. CONCLUSION", "text": "In this thesis, we have proposed a class of diffusion LMS strategies, regulated by convex, economical penalties, for distributed estimation over adaptive networks. Two different tightening functions have been used: the \"1 standard,\" which attracts all vector elements evenly to zero, and a reweighted function, which better corresponds to the \"0 standard\" and selectively shrinks only those elements with small size. Convergence and Mean Square Analysis of the sparse adaptive diffusion filter show under which conditions we have the dominance of the proposed method in relation to its unregulated counterpart in terms of state stability. Further analyses lead to a procedure for updating the regulation parameters of the algorithm to ensure the dominance of the sparse diffusion filter in relation to its unregulated version. In this way, the network can adjust the system parameters in real time to improve the estimated performance according to the underlying vector economy."}, {"heading": "APPENDIX A", "text": "PROOF OF THEOREM 1LP (83) \u00b7 b). \u00b7 b). \u00b7 b). (2). (3). (3). (3). (3). (4). (4). (4). (4). (4). (4). (4). (4). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5).). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5). (5).). (5). (5). (5).). (5)."}, {"heading": "APPENDIX B", "text": "(49). (49)..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "APPENDIX C", "text": "Let us consider the limited random vector ci in (98), which is independent of the sound sequence (i) for all k, i. Let us leave B = AT (I \u2212 MD) and bi = \u03b3ATM \u2202 f (wi \u2212 1), of (37), we obtain the initial condition. If we follow the same steps as in Appendix A, if the step variables meet condition (40), the first term on the RHS will converge from (108) to zero. Furthermore, since the vector value of November 14, 2012 DRAFT30 sequence ci is limited, we will be able to apply the comparison of 51 [51] to the limit value of (14] to the limit value of 108 (finite), similar to what we did in (89)."}], "references": [{"title": "A new class of incremental gradient methods for least squares problems", "author": ["D. Bertsekas"], "venue": "SIAM J. Optim., vol. 7, no. 4, pp. 913\u2013926, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Incremental subgradient methods for nondifferentiable optimization", "author": ["A. Nedic", "D. Bertsekas"], "venue": "SIAM J. Optim., vol. 12, no. 1, pp. 109\u2013138, 2001.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Quantized incremental algorithms for distributed optimization", "author": ["M.G. Rabbat", "R.D. Nowak"], "venue": "IEEE J. Sel. Areas Commun., vol. 23, no. 4, pp. 798\u2013808, Apr. 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Incremental adaptive strategies over distributed networks", "author": ["C. Lopes", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 55, no. 8, pp. 4064\u20134077, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Distributed estimation over an adaptive incremental network based on the affine projection algorithm", "author": ["L. Li", "J. Chambers", "C. Lopes", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 1, pp. 151\u2013164, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Reducibility among combinational problems", "author": ["R.M. Karp"], "venue": "Complexity of Computer Computations (R. E. Miller and J. W. Thatcher, eds.), pp. 85\u2013104, 1972.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1972}, {"title": "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis", "author": ["C.G. Lopes", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 3122\u20133136, July 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion LMS strategies for distributed estimation", "author": ["F.S. Cattivelli", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 58, pp. 1035\u20131048, March 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Diffusion adaptation over networks", "author": ["A.H. Sayed"], "venue": "to appear in E-Reference Signal Processing, R. Chellapa and S. Theodoridis, editors, Elsevier, 2013. Also available on arXiv at http://arxiv.org/abs/1205.4220, May 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Diffusion least-mean squares with adaptive combiners: Formulation and performance analysis", "author": ["N. Takahashi", "I. Yamada", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 9, pp. 4795\u20134810, Sep. 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Diffusion adaptation strategies for distributed optimization and learning over networks", "author": ["J. Chen", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 8, pp. 4289-4305, August 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Modeling bird flight formations using diffusion adaptation", "author": ["F. Cattivelli", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing, vol. 59, no. 5, pp. 2038-2051, May 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Mobile adaptive networks", "author": ["S-Y. Tu", "A.H. Sayed"], "venue": "IEEE J. Sel.Topics on Signal Processing, vol. 5, no. 4, pp. 649-664, August 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Bacterial motility via diffusion adaptation", "author": ["J. Chen", "X. Zhao", "A.H. Sayed"], "venue": "Proc. 44th Asilomar Conference on Signals, Systems and Computers, Pacific Grove, CA, Nov. 2010, pp. 1930-1934.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Bio-inspired swarming for dynamic radio access based on diffusion adaptation", "author": ["P. Di Lorenzo", "S. Barbarossa", "A.H. Sayed"], "venue": "Proc. European Signal Processing Conference (EUSIPCO), Aug. 2011, Barcelona, pp.402\u2013406.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Collaborative learning of mixture models using diffusion adaptation", "author": ["Z. Towfic", "J. Chen", "A.H. Sayed"], "venue": "Proc. IEEE Workshop on Machine Learning for Signal Processing, Beijing, China, Sept. 2011, pp. 1\u20136.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive learning in a world of projections", "author": ["S. Theodoridis", "K. Slavakis", "I. Yamada"], "venue": "IEEE Signal Processing Magazine, vol. 28, no. 1, pp. 97\u2013123, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Adaptive robust distributed learning in diffusion sensor networks", "author": ["S. Chouvardas", "K. Slavakis", "S. Theodoridis"], "venue": "IEEE Transactions on Signal Processing, vol. 59, no. 10, pp. 4692\u20134707, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Trading off communications bandwidth with accuracy in adaptive diffusion networks", "author": ["S. Chouvardas", "K. Slavakis", "S. Theodoridis"], "venue": "Proc. IEEE International Conference on Acoustics Speech and Signal Processing, May 2011, Prague, Czeck Rep., pp. 2048\u20132051.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "A TAP selection algorithm for adaptive filters", "author": ["S. Kawamura", "M. Hatori"], "venue": "Proc. of IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 1986, vol. 11, Tokyo, Japan, pp. 2979\u20132982.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1986}, {"title": "LMS estimation via structural detection", "author": ["J. Homer", "I. Mareels", "R.R. Bitmead", "B. Wahlberg", "A. Gustafsson"], "venue": "IEEE Transactions on Signal Processing, vol. 46, pp. 2651\u20132663, October 1998. November 14, 2012  DRAFT  31", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Parallel NLMS filters with stochastic active taps and step-sizes for sparse system identification", "author": ["Y. Li", "Y. Gu", "K. Tang"], "venue": "Proc. of IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), vol. 3, pp. III, 2006, Toulouse.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Identification of sparse impulse response systems using an adaptive delay filter", "author": ["D.M. Etter"], "venue": "Pro. of IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), 1985, Tampa, Florida, pp. 1169\u20131172.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1985}, {"title": "Partial update LMS algorithms", "author": ["M. Godavarti", "A.O. Hero"], "venue": "IEEE Trans. on Signal Proc., vol. 53, no. 7, pp. 2382\u20132399, July, 2005.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "Proportionate normalized least-mean-squares adaptation in echo cancelers", "author": ["D. Duttweiler"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 8, no. 5, pp. 508\u2013518, Sep. 2000.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "An improved PNLMS algorithm", "author": ["J. Benesty", "S. Gay"], "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2002, Orlando, Florida, pp. 1881\u20131884.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Compressed sensing", "author": ["D. Donoho"], "venue": "IEEE Transactions on Information Theory, vol. 52, no. 4, pp. 1289\u20131306, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["R. Tibshirani"], "venue": "J. Royal Statistical Society: Series B, vol. 58, pp. 267\u2013288, 1996.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1996}, {"title": "Compressive sensing", "author": ["R. Baraniuk"], "venue": "IEEE Signal Processing Magazine, vol. 25, pp. 21\u201330, March 2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Enhancing sparsity by reweighted l1 minimization", "author": ["E.J. Candes", "M. Wakin", "S. Boyd"], "venue": "Journal of Fourier Analysis and Applications, vol. 14, pp.877\u2013905, 2007.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Stable signal recovery from incomplete and inaccurate measurements", "author": ["E. Candes", "J. Romberg", "T. Tao"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "From sparse solutions of systems of equations to sparse modeling of signals and images", "author": ["A. Bruckstein", "D. Donoho", "M. Elad"], "venue": "SIAM Review, vol. 51, no. 1, pp. 34\u201381, 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimally tuned iterative reconstruction algorithms for compressed sensing", "author": ["A. Maleki", "D. Donoho"], "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, no. 2, pp. 330\u2013341, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse LMS for system identification", "author": ["Y. Chen", "Y. Gu", "A.O. Hero"], "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, pp. 3125\u20133128, Taipei, May 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Convergence analysis of sparse LMS algorithms with l1-norm penalty based on white imput signal,", "author": ["K. Shi", "P. Shi"], "venue": "Signal Processsing,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Online adaptive estimation of sparse signals: where RLS meets the l1-norm", "author": ["D. Angelosante", "J.A. Bazerque", "G.B. Giannakis"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 7, pp. 3436\u20133447, July, 2010.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "SPARLS: The sparse RLS algorithm", "author": ["B. Babadi", "N. Kalouptsidis", "V. Tarokh"], "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 8, pp. 4013\u20134025, Aug., 2010.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Online sparse system identification and signal reconstruction using projections onto weighted l1 balls", "author": ["Y. Kopsinis", "K. Slavakis", "S. Theodoridis"], "venue": "IEEE Transactions on Signal Processing, vol. 59, no. 3, pp. 936\u2013952, March, 2010.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "A sparse adaptive filtering using time-varying soft-thresholding techniques", "author": ["Y. Murakami", "M. Yamagishi", "M. Yukawa", "I. Yamada"], "venue": "Proc. IEEE International Conference on Acoustics Speech and Signal Processing (ICASSP), Dallas, USA, 2010, pp. 3734\u20133737.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed basis pursuit", "author": ["J. Mota", "J. Xavier", "P. Aguiar", "M. Puschel"], "venue": "Arxiv preprint arXiv:1009.1128, 2010.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed sparse linear regression", "author": ["G. Mateos", "J.A. Bazerque", "G.B. Giannakis"], "venue": "IEEE Transactions on Signal Processing, vol 58, No. 10, pp. 5262\u20135276, Oct. 2010.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse diffusion LMS for distributed adaptive estimation", "author": ["P. Di Lorenzo", "S. Barbarossa", "A.H. Sayed"], "venue": "Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, Kyoto, Japan, March 2012.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "A sparsity-promoting adaptive algorithm for distributed learning", "author": ["S. Chouvardas", "K. Slavakis", "Y. Kopsinis", "S. Theodoridis"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 10, pp. 5412\u20135425, Oct. 2012.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2012}, {"title": "Diffusion sparse least-mean squares over networks", "author": ["Y. Liu", "C. Li", "Z. Zhang"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 8, pp. 4480\u20134485, Aug. 2012.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2012}, {"title": "Distributed spectrum sensing for cognitive radio networks by exploiting sparsity", "author": ["J.A. Bazerque", "G.B. Giannakis"], "venue": "IEEE Trans. on Signal Processing, vol 58, No. 3, pp. 1847\u20131862, March 2010.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1847}, {"title": "Distributed LMS for consensus-based in-network adaptive processing", "author": ["I.D. Schizas", "G. Mateos", "G.B. Giannakis"], "venue": "IEEE Trans. Signal Process., vol. 57, no. 6, pp. 2365\u20132382, Jun. 2009.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2009}, {"title": "Stochastic Approximation Algorithms and Applications", "author": ["H.J. Kushner", "G.G. Yin"], "venue": "New York: Springer-Verlag,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1997}, {"title": "Adaptive Filters, Wiley, NJ", "author": ["A.H. Sayed"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2008}], "referenceMentions": [{"referenceID": 41, "context": "Part of this work was presented at the 2012 IEEE International Conference on Acoustic, Speech and Signal Processing, Kyoto, March 2012 [42].", "startOffset": 135, "endOffset": 139}, {"referenceID": 0, "context": "One typical strategy is the incremental approach [1]-[5], where each node communicates only with one neighbor at a time over a cyclic path.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "One typical strategy is the incremental approach [1]-[5], where each node communicates only with one neighbor at a time over a cyclic path.", "startOffset": 53, "endOffset": 56}, {"referenceID": 5, "context": "However, determining a cyclic path that covers all nodes is an NP-hard problem [6] and, in addition, cyclic trajectories are prone to link and node failures.", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "To address these difficulties, adaptive diffusion techniques were proposed and studied in [7], [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "To address these difficulties, adaptive diffusion techniques were proposed and studied in [7], [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 11, "context": "In view of their robustness and adaptation properties, diffusion networks have been applied to model a variety of self-organized behavior encountered in nature, such as birds flying in formation [12], fish foraging for food [13] or bacteria motility [14].", "startOffset": 195, "endOffset": 199}, {"referenceID": 12, "context": "In view of their robustness and adaptation properties, diffusion networks have been applied to model a variety of self-organized behavior encountered in nature, such as birds flying in formation [12], fish foraging for food [13] or bacteria motility [14].", "startOffset": 224, "endOffset": 228}, {"referenceID": 13, "context": "In view of their robustness and adaptation properties, diffusion networks have been applied to model a variety of self-organized behavior encountered in nature, such as birds flying in formation [12], fish foraging for food [13] or bacteria motility [14].", "startOffset": 250, "endOffset": 254}, {"referenceID": 14, "context": "Diffusion adaptation has also been used to solve dynamic resource allocation problems in cognitive radios [15], to perform robust system modeling [18], and to implement distributed learning over mixture models in pattern recognition applications [16].", "startOffset": 106, "endOffset": 110}, {"referenceID": 17, "context": "Diffusion adaptation has also been used to solve dynamic resource allocation problems in cognitive radios [15], to perform robust system modeling [18], and to implement distributed learning over mixture models in pattern recognition applications [16].", "startOffset": 146, "endOffset": 150}, {"referenceID": 15, "context": "Diffusion adaptation has also been used to solve dynamic resource allocation problems in cognitive radios [15], to perform robust system modeling [18], and to implement distributed learning over mixture models in pattern recognition applications [16].", "startOffset": 246, "endOffset": 250}, {"referenceID": 26, "context": "Any prior information about the sparsity of wo can be exploited to help improve the estimation performance, as demonstrated in many recent efforts in the area of compressive sensing (CS) [27]-[29].", "startOffset": 187, "endOffset": 191}, {"referenceID": 28, "context": "Any prior information about the sparsity of wo can be exploited to help improve the estimation performance, as demonstrated in many recent efforts in the area of compressive sensing (CS) [27]-[29].", "startOffset": 192, "endOffset": 196}, {"referenceID": 27, "context": "Such schemes are useful in several contexts such as in the analysis of prostate cancer data [28], [41], spectrum sensing in cognitive radio [45], and spectrum estimation in wireless sensor networks [46].", "startOffset": 92, "endOffset": 96}, {"referenceID": 40, "context": "Such schemes are useful in several contexts such as in the analysis of prostate cancer data [28], [41], spectrum sensing in cognitive radio [45], and spectrum estimation in wireless sensor networks [46].", "startOffset": 98, "endOffset": 102}, {"referenceID": 44, "context": "Such schemes are useful in several contexts such as in the analysis of prostate cancer data [28], [41], spectrum sensing in cognitive radio [45], and spectrum estimation in wireless sensor networks [46].", "startOffset": 140, "endOffset": 144}, {"referenceID": 45, "context": "Such schemes are useful in several contexts such as in the analysis of prostate cancer data [28], [41], spectrum sensing in cognitive radio [45], and spectrum estimation in wireless sensor networks [46].", "startOffset": 198, "endOffset": 202}, {"referenceID": 19, "context": "Some of the early works that mix adaptation with sparsity-aware constructions include methods that rely on the heuristic selection of active taps [20]-[22], and on sequential partial updating techniques [23]November 14, 2012 DRAFT", "startOffset": 146, "endOffset": 150}, {"referenceID": 21, "context": "Some of the early works that mix adaptation with sparsity-aware constructions include methods that rely on the heuristic selection of active taps [20]-[22], and on sequential partial updating techniques [23]November 14, 2012 DRAFT", "startOffset": 151, "endOffset": 155}, {"referenceID": 22, "context": "Some of the early works that mix adaptation with sparsity-aware constructions include methods that rely on the heuristic selection of active taps [20]-[22], and on sequential partial updating techniques [23]November 14, 2012 DRAFT", "startOffset": 203, "endOffset": 207}, {"referenceID": 23, "context": "3 [24]; some other methods assign proportional step-sizes to different taps according to their magnitudes, such as the proportionate normalized LMS (PNLMS) algorithm and its variations [25]-[26].", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": "3 [24]; some other methods assign proportional step-sizes to different taps according to their magnitudes, such as the proportionate normalized LMS (PNLMS) algorithm and its variations [25]-[26].", "startOffset": 185, "endOffset": 189}, {"referenceID": 25, "context": "3 [24]; some other methods assign proportional step-sizes to different taps according to their magnitudes, such as the proportionate normalized LMS (PNLMS) algorithm and its variations [25]-[26].", "startOffset": 190, "endOffset": 194}, {"referenceID": 27, "context": "In subsequent studies, motivated by the LASSO technique [28] and by connections with compressive sensing [29], [30], several algorithms for sparse adaptive filtering have been proposed based on LMS [34]-[35], RLS [36], [37], and projection-based methods [38]-[39].", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "In subsequent studies, motivated by the LASSO technique [28] and by connections with compressive sensing [29], [30], several algorithms for sparse adaptive filtering have been proposed based on LMS [34]-[35], RLS [36], [37], and projection-based methods [38]-[39].", "startOffset": 105, "endOffset": 109}, {"referenceID": 29, "context": "In subsequent studies, motivated by the LASSO technique [28] and by connections with compressive sensing [29], [30], several algorithms for sparse adaptive filtering have been proposed based on LMS [34]-[35], RLS [36], [37], and projection-based methods [38]-[39].", "startOffset": 111, "endOffset": 115}, {"referenceID": 33, "context": "In subsequent studies, motivated by the LASSO technique [28] and by connections with compressive sensing [29], [30], several algorithms for sparse adaptive filtering have been proposed based on LMS [34]-[35], RLS [36], [37], and projection-based methods [38]-[39].", "startOffset": 198, "endOffset": 202}, {"referenceID": 34, "context": "In subsequent studies, motivated by the LASSO technique [28] and by connections with compressive sensing [29], [30], several algorithms for sparse adaptive filtering have been proposed based on LMS [34]-[35], RLS [36], [37], and projection-based methods [38]-[39].", "startOffset": 203, "endOffset": 207}, {"referenceID": 35, "context": "In subsequent studies, motivated by the LASSO technique [28] and by connections with compressive sensing [29], [30], several algorithms for sparse adaptive filtering have been proposed based on LMS [34]-[35], RLS [36], [37], and projection-based methods [38]-[39].", "startOffset": 213, "endOffset": 217}, {"referenceID": 36, "context": "In subsequent studies, motivated by the LASSO technique [28] and by connections with compressive sensing [29], [30], several algorithms for sparse adaptive filtering have been proposed based on LMS [34]-[35], RLS [36], [37], and projection-based methods [38]-[39].", "startOffset": 219, "endOffset": 223}, {"referenceID": 37, "context": "In subsequent studies, motivated by the LASSO technique [28] and by connections with compressive sensing [29], [30], several algorithms for sparse adaptive filtering have been proposed based on LMS [34]-[35], RLS [36], [37], and projection-based methods [38]-[39].", "startOffset": 254, "endOffset": 258}, {"referenceID": 38, "context": "In subsequent studies, motivated by the LASSO technique [28] and by connections with compressive sensing [29], [30], several algorithms for sparse adaptive filtering have been proposed based on LMS [34]-[35], RLS [36], [37], and projection-based methods [38]-[39].", "startOffset": 259, "endOffset": 263}, {"referenceID": 39, "context": "A couple of distributed algorithms implementing LASSO over ad-hoc networks have also been considered before, although their main purpose has been to use the network to solve a batch processing problem [40], [41].", "startOffset": 201, "endOffset": 205}, {"referenceID": 40, "context": "A couple of distributed algorithms implementing LASSO over ad-hoc networks have also been considered before, although their main purpose has been to use the network to solve a batch processing problem [40], [41].", "startOffset": 207, "endOffset": 211}, {"referenceID": 41, "context": "Investigations on adaptive and distributed solutions appear in [42],[43], and [44].", "startOffset": 63, "endOffset": 67}, {"referenceID": 42, "context": "Investigations on adaptive and distributed solutions appear in [42],[43], and [44].", "startOffset": 68, "endOffset": 72}, {"referenceID": 43, "context": "Investigations on adaptive and distributed solutions appear in [42],[43], and [44].", "startOffset": 78, "endOffset": 82}, {"referenceID": 41, "context": "In [42], we employed diffusion techniques that are able to identify and track sparsity over networks in a distributed manner; the techniques relied on the use of convenient convex regularization terms.", "startOffset": 3, "endOffset": 7}, {"referenceID": 42, "context": "In the related work [43], the authors employ projection techniques onto hyperslabs and weighted l1-balls to develop a useful sparsity-aware algorithm for distributed learning over diffusion networks.", "startOffset": 20, "endOffset": 24}, {"referenceID": 43, "context": "In [44], the authors use the same formulation of [42] and the techniques of [7][8] to independently arrive at useful diffusion strategies except that they limit the function f(\u00b7) in (2) to choices of the form \u2016w\u2016p, for particular selections of p-vector norms; they also include the regularization factor into the combination step of their algorithm rather than in the adaptation step, as done further ahead in this work.", "startOffset": 3, "endOffset": 7}, {"referenceID": 41, "context": "In [44], the authors use the same formulation of [42] and the techniques of [7][8] to independently arrive at useful diffusion strategies except that they limit the function f(\u00b7) in (2) to choices of the form \u2016w\u2016p, for particular selections of p-vector norms; they also include the regularization factor into the combination step of their algorithm rather than in the adaptation step, as done further ahead in this work.", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "In [44], the authors use the same formulation of [42] and the techniques of [7][8] to independently arrive at useful diffusion strategies except that they limit the function f(\u00b7) in (2) to choices of the form \u2016w\u2016p, for particular selections of p-vector norms; they also include the regularization factor into the combination step of their algorithm rather than in the adaptation step, as done further ahead in this work.", "startOffset": 76, "endOffset": 79}, {"referenceID": 7, "context": "In [44], the authors use the same formulation of [42] and the techniques of [7][8] to independently arrive at useful diffusion strategies except that they limit the function f(\u00b7) in (2) to choices of the form \u2016w\u2016p, for particular selections of p-vector norms; they also include the regularization factor into the combination step of their algorithm rather than in the adaptation step, as done further ahead in this work.", "startOffset": 79, "endOffset": 82}, {"referenceID": 41, "context": "The algorithms proposed here and in [42] are more general in a couple of respects: they allow for broader choices of the regularization function f(\u00b7), they allow for sharing of both data and weight estimates among the nodes (and not only estimates) by allowing for the use of two sets of combinations weights {al,k, cl,k} instead of only one set, and the resulting mean-square and stability analyses are more demanding due to these generalizations; see, e.", "startOffset": 36, "endOffset": 40}, {"referenceID": 41, "context": "In summary, in this paper we extend our preliminary work in [42] to develop adaptive networks running diffusion algorithms subject to constraints that enforce sparsity.", "startOffset": 60, "endOffset": 64}, {"referenceID": 7, "context": "Adaptive Diffusion Strategy We follow the approach proposed in [8], [11] to derive distributed strategies for the minimization of Jglob(w) in (2).", "startOffset": 63, "endOffset": 66}, {"referenceID": 10, "context": "Adaptive Diffusion Strategy We follow the approach proposed in [8], [11] to derive distributed strategies for the minimization of Jglob(w) in (2).", "startOffset": 68, "endOffset": 72}, {"referenceID": 10, "context": "This substitution is also reasonable in view of the fact that norms are equivalent and that each of the weighted norms in (9) can be bounded as \u03bbmin(\u0393l) \u00b7 \u2016w \u2212 w l \u2016 \u2264 \u2016w \u2212 w l \u20162\u0393l \u2264 \u03bbmax(\u0393l) \u00b7 \u2016w \u2212 w l \u2016 (10) Substitutions of this kind are common in the stochastic approximation literature where Hessian matrices, such as \u0393l, are replaced by multiples of the identity matrix; such approximations allow the use of simpler steepest-descent iterations in place of Newton-type iterations [11].", "startOffset": 486, "endOffset": 490}, {"referenceID": 0, "context": "This step is reminiscent of an incremental-type substitution [1]-[5].", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "This step is reminiscent of an incremental-type substitution [1]-[5].", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": ", [8]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "It was argued in [8] that ATC strategies generally outperform CTA strategies.", "startOffset": 17, "endOffset": 20}, {"referenceID": 42, "context": "Compared with the strategies proposed in [43] and [44], the diffusion algorithm (21) exploits data in the neighborhood more fully; the adaptation step aggregates data {dl(i), ul,i} from the neighbors, and the diffusion step aggregates estimates {\u03c8l,i} from the same neighbors.", "startOffset": 41, "endOffset": 45}, {"referenceID": 43, "context": "Compared with the strategies proposed in [43] and [44], the diffusion algorithm (21) exploits data in the neighborhood more fully; the adaptation step aggregates data {dl(i), ul,i} from the neighbors, and the diffusion step aggregates estimates {\u03c8l,i} from the same neighbors.", "startOffset": 50, "endOffset": 54}, {"referenceID": 42, "context": "The implementation in [43] uses a different algorithmic structure with C = I so that data {dl(i), ul,i} from the neighbors are not directly used.", "startOffset": 22, "endOffset": 26}, {"referenceID": 43, "context": "Compared with [44], observe that the effect of the regularization factor in (21) influences the adaptation step, and not the combination step as in [44].", "startOffset": 14, "endOffset": 18}, {"referenceID": 43, "context": "Compared with [44], observe that the effect of the regularization factor in (21) influences the adaptation step, and not the combination step as in [44].", "startOffset": 148, "endOffset": 152}, {"referenceID": 43, "context": "10 for the exchange of data {dl(i), ul,i} among the nodes through the use of the coefficients {cl,k}, whereas [44] uses C = I as well.", "startOffset": 110, "endOffset": 114}, {"referenceID": 27, "context": "Thus, motivated by LASSO [28] and work on compressive sensing [29], we first consider the following l1-norm convex choice for a regularization function: f1(w) = \u2016w\u20161 , M \u2211", "startOffset": 25, "endOffset": 29}, {"referenceID": 28, "context": "Thus, motivated by LASSO [28] and work on compressive sensing [29], we first consider the following l1-norm convex choice for a regularization function: f1(w) = \u2016w\u20161 , M \u2211", "startOffset": 62, "endOffset": 66}, {"referenceID": 29, "context": "The ZA update uniformly shrinks all components of the vector, and does not distinguish between zero and nonzero elements [30], [34].", "startOffset": 121, "endOffset": 125}, {"referenceID": 33, "context": "The ZA update uniformly shrinks all components of the vector, and does not distinguish between zero and nonzero elements [30], [34].", "startOffset": 127, "endOffset": 131}, {"referenceID": 29, "context": "Motivated by the idea of reweighting in compressive sampling [30], [34],[38], we also consider the following approximation: \u2016w\u20160 \u2243 M \u2211", "startOffset": 61, "endOffset": 65}, {"referenceID": 33, "context": "Motivated by the idea of reweighting in compressive sampling [30], [34],[38], we also consider the following approximation: \u2016w\u20160 \u2243 M \u2211", "startOffset": 67, "endOffset": 71}, {"referenceID": 37, "context": "Motivated by the idea of reweighting in compressive sampling [30], [34],[38], we also consider the following approximation: \u2016w\u20160 \u2243 M \u2211", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "11 which, for very small positive values of \u03b5, is a better approximation for the l0-norm of a vector w than the l1-norm [30], thus enhancing sparse recovery by the algorithm.", "startOffset": 120, "endOffset": 124}, {"referenceID": 29, "context": ", [30], [34], [38], [42], [44].", "startOffset": 2, "endOffset": 6}, {"referenceID": 33, "context": ", [30], [34], [38], [42], [44].", "startOffset": 8, "endOffset": 12}, {"referenceID": 37, "context": ", [30], [34], [38], [42], [44].", "startOffset": 14, "endOffset": 18}, {"referenceID": 41, "context": ", [30], [34], [38], [42], [44].", "startOffset": 20, "endOffset": 24}, {"referenceID": 43, "context": ", [30], [34], [38], [42], [44].", "startOffset": 26, "endOffset": 30}, {"referenceID": 46, "context": "Several studies in the literature, especially on stochastic approximation theory [47]\u2013[48], indicate that the performance expressions obtained using this assumption match well the actual performance of stand-alone filters for sufficiently small step-sizes.", "startOffset": 81, "endOffset": 85}, {"referenceID": 47, "context": "Several studies in the literature, especially on stochastic approximation theory [47]\u2013[48], indicate that the performance expressions obtained using this assumption match well the actual performance of stand-alone filters for sufficiently small step-sizes.", "startOffset": 86, "endOffset": 90}, {"referenceID": 6, "context": "Following the energy conservation framework of [7], [8] and under Assumption 1, we can establish the following variance relation: E\u2016w\u0303i\u2016\u03a3 = E\u2016w\u0303i\u22121\u2016\u03a3\u2032 + E[giMA\u03a3AMgi] + 2\u03b3E\u2202f(wi\u22121)MA\u03a3A (I \u2212MD) w\u0303i\u22121 + \u03b3E\u2016\u2202f(wi\u22121)\u2016MA\u03a3ATM (44) where \u03a3 is any Hermitian nonnegative-definite matrix that we are free to choose, and \u03a3 = E(I \u2212DiM)A\u03a3A (I \u2212MDi) (45) Relations (44)-(45) can be derived directly from (37) if we compute the weighted norm of both sides of the equality and use the fact that gi is independent of w\u0303i\u22121 and wi\u22121.", "startOffset": 47, "endOffset": 50}, {"referenceID": 7, "context": "Following the energy conservation framework of [7], [8] and under Assumption 1, we can establish the following variance relation: E\u2016w\u0303i\u2016\u03a3 = E\u2016w\u0303i\u22121\u2016\u03a3\u2032 + E[giMA\u03a3AMgi] + 2\u03b3E\u2202f(wi\u22121)MA\u03a3A (I \u2212MD) w\u0303i\u22121 + \u03b3E\u2016\u2202f(wi\u22121)\u2016MA\u03a3ATM (44) where \u03a3 is any Hermitian nonnegative-definite matrix that we are free to choose, and \u03a3 = E(I \u2212DiM)A\u03a3A (I \u2212MDi) (45) Relations (44)-(45) can be derived directly from (37) if we compute the weighted norm of both sides of the equality and use the fact that gi is independent of w\u0303i\u22121 and wi\u22121.", "startOffset": 52, "endOffset": 55}, {"referenceID": 10, "context": "Now, since A is left-stochastic, it can be verified that the above F is stable if (I \u2212DM) is stable [11], [9]; this latter condition is guaranteed by (40).", "startOffset": 100, "endOffset": 104}, {"referenceID": 8, "context": "Now, since A is left-stochastic, it can be verified that the above F is stable if (I \u2212DM) is stable [11], [9]; this latter condition is guaranteed by (40).", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "Expression (55) is a useful result: it allows us to derive several performance metrics through the proper selection of the free weighting parameter \u03c3 (or \u03a3), as was done in [8].", "startOffset": 173, "endOffset": 176}, {"referenceID": 7, "context": "Considering the MSD expression (58) at the k-th node, we notice that the first term on the RHS coincides with the MSD of the standard diffusion algorithm when \u03b3 = 0 (compare with (48) in [8]), whereas the second term in (58) is due to the regularization.", "startOffset": 187, "endOffset": 190}, {"referenceID": 27, "context": "For example, the l1-norm of wo can be upper bounded by some constant value [28].", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "2, we report the learning curves in terms of network MSD for 6 different adaptive filters: ATC diffusion LMS [8], ZA-ATC diffusion described by (21) and (25) and RZA-ATC diffusion described by (21) and (28), and the non-cooperative approach from [34].", "startOffset": 109, "endOffset": 112}, {"referenceID": 33, "context": "2, we report the learning curves in terms of network MSD for 6 different adaptive filters: ATC diffusion LMS [8], ZA-ATC diffusion described by (21) and (25) and RZA-ATC diffusion described by (21) and (28), and the non-cooperative approach from [34].", "startOffset": 246, "endOffset": 250}, {"referenceID": 33, "context": "2: Transient network MSD for the non-cooperative approaches LMS, ZA-LMS [34], RZA-LMS [34], and the diffusion techniques ATC [8], ZA-ATC given by (21)-(25), RZA-ATC given by (21)-(28).", "startOffset": 72, "endOffset": 76}, {"referenceID": 33, "context": "2: Transient network MSD for the non-cooperative approaches LMS, ZA-LMS [34], RZA-LMS [34], and the diffusion techniques ATC [8], ZA-ATC given by (21)-(25), RZA-ATC given by (21)-(28).", "startOffset": 86, "endOffset": 90}, {"referenceID": 7, "context": "2: Transient network MSD for the non-cooperative approaches LMS, ZA-LMS [34], RZA-LMS [34], and the diffusion techniques ATC [8], ZA-ATC given by (21)-(25), RZA-ATC given by (21)-(28).", "startOffset": 125, "endOffset": 128}, {"referenceID": 33, "context": "We also notice the gain of diffusion schemes with respect to the non-cooperative approaches from [34].", "startOffset": 97, "endOffset": 101}, {"referenceID": 7, "context": "Iteration index M S D ( dB ) ATC diffusion from [8] ZA ATC diffusion in (21) RZA ATC diffusion in (21) ATC l 1 \u2212LMS from [44] ATC l 1 \u2212RWLMS from [44]", "startOffset": 48, "endOffset": 51}, {"referenceID": 43, "context": "Iteration index M S D ( dB ) ATC diffusion from [8] ZA ATC diffusion in (21) RZA ATC diffusion in (21) ATC l 1 \u2212LMS from [44] ATC l 1 \u2212RWLMS from [44]", "startOffset": 121, "endOffset": 125}, {"referenceID": 43, "context": "Iteration index M S D ( dB ) ATC diffusion from [8] ZA ATC diffusion in (21) RZA ATC diffusion in (21) ATC l 1 \u2212LMS from [44] ATC l 1 \u2212RWLMS from [44]", "startOffset": 146, "endOffset": 150}, {"referenceID": 7, "context": "400 500 600 \u221234 \u221232 \u221230 \u221228 \u221226 \u221224 ATC diffusion from [8] ZA ATC diffusion in (21) RZA ATC diffusion in (21) Projection\u2212based learning from [43]", "startOffset": 55, "endOffset": 58}, {"referenceID": 42, "context": "400 500 600 \u221234 \u221232 \u221230 \u221228 \u221226 \u221224 ATC diffusion from [8] ZA ATC diffusion in (21) RZA ATC diffusion in (21) Projection\u2212based learning from [43]", "startOffset": 141, "endOffset": 145}, {"referenceID": 7, "context": "4: (Left) Transient network MSD for the diffusion techniques ATC [8], ZA-ATC described by (21) and (25), RZA-ATC described by (21) and (28), and the sparse diffusion algorithms from [44].", "startOffset": 65, "endOffset": 68}, {"referenceID": 43, "context": "4: (Left) Transient network MSD for the diffusion techniques ATC [8], ZA-ATC described by (21) and (25), RZA-ATC described by (21) and (28), and the sparse diffusion algorithms from [44].", "startOffset": 182, "endOffset": 186}, {"referenceID": 7, "context": "(Right) Transient network MSD for the diffusion techniques ATC [8], ZA-ATC described by (21) and (25), RZA-ATC described by (21) and (28), and the projection based distributed learning technique from [43].", "startOffset": 63, "endOffset": 66}, {"referenceID": 42, "context": "(Right) Transient network MSD for the diffusion techniques ATC [8], ZA-ATC described by (21) and (25), RZA-ATC described by (21) and (28), and the projection based distributed learning technique from [43].", "startOffset": 200, "endOffset": 204}, {"referenceID": 42, "context": "Finally, we compare our proposed sparse diffusion schemes with the sparsity promoting adaptive algorithms for distributed learning recently proposed in [43] and in [44].", "startOffset": 152, "endOffset": 156}, {"referenceID": 43, "context": "Finally, we compare our proposed sparse diffusion schemes with the sparsity promoting adaptive algorithms for distributed learning recently proposed in [43] and in [44].", "startOffset": 164, "endOffset": 168}, {"referenceID": 42, "context": "At the best of our knowledge, the works in [43] and [44] are the only two present in the literature that exploit sparsity processing data both in an adaptive and distributed fashion.", "startOffset": 43, "endOffset": 47}, {"referenceID": 43, "context": "At the best of our knowledge, the works in [43] and [44] are the only two present in the literature that exploit sparsity processing data both in an adaptive and distributed fashion.", "startOffset": 52, "endOffset": 56}, {"referenceID": 7, "context": "4 (left), we compare the steady-state performance, averaged over 100 independent simulations, of five adaptive filters: ATC diffusion LMS [8], ZA-ATC diffusion described by (21) and (25), RZA-ATC diffusion described by (21) and (28), the ATC l1-LMS and the ATC l1-RWLMS algorithms from [44].", "startOffset": 138, "endOffset": 141}, {"referenceID": 43, "context": "4 (left), we compare the steady-state performance, averaged over 100 independent simulations, of five adaptive filters: ATC diffusion LMS [8], ZA-ATC diffusion described by (21) and (25), RZA-ATC diffusion described by (21) and (28), the ATC l1-LMS and the ATC l1-RWLMS algorithms from [44].", "startOffset": 286, "endOffset": 290}, {"referenceID": 43, "context": "1, for both our methods and the algorithms from [44].", "startOffset": 48, "endOffset": 52}, {"referenceID": 43, "context": "4 (left), the proposed methods outperform the algorithms from [44] in terms of steady-state MSD.", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "4 (right), we compare the transient network MSD of four adaptive filters: ATC diffusion LMS [8], ZA-ATC diffusion described by (21) and (25), RZA-ATC diffusion described by (21) and (28), and the projection based sparse learning from [43].", "startOffset": 92, "endOffset": 95}, {"referenceID": 42, "context": "4 (right), we compare the transient network MSD of four adaptive filters: ATC diffusion LMS [8], ZA-ATC diffusion described by (21) and (25), RZA-ATC diffusion described by (21) and (28), and the projection based sparse learning from [43].", "startOffset": 234, "endOffset": 238}, {"referenceID": 42, "context": "23 RZA-ATC diffusion algorithms are the same of the previous simulation, whereas the parameters of the algorithm from [43] are chosen in order to have similar steady-state MSD with respect to the RZA-ATC diffusion method.", "startOffset": 118, "endOffset": 122}, {"referenceID": 42, "context": "Using the same notation adopted in [43], the parameters of the projection based filter are: \u03b5 = 1.", "startOffset": 35, "endOffset": 39}, {"referenceID": 42, "context": "Indeed, while our methods have an LMS type complexity O(3M), the projection-based method from [43] has a complexity equal to O(M(3 + q + logM)), due to the presence of q projections onto the hyperslabs and 1 projection on the weighted l1 ball per iteration.", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "5 (left), we report the learning curves in terms of network MSD for 3 different adaptive filters: ATC diffusion LMS [8], ZA-ATC diffusion described by (21) and (25)) and RZA-ATC diffusion described by (21) and (28), when the regularization parameter \u03b3i is chosen locally at each node according to the adaptive rule (80).", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "5: (Left) Transient network MSD for the the diffusion techniques ATC [8], ZA-ATC described by (21) and (25), RZA-ATC described by (21) and (28) with adaptive selection of the regularization parameter \u03b3i.", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "To proceed, we call upon results from [10], [11], [9].", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "To proceed, we call upon results from [10], [11], [9].", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "To proceed, we call upon results from [10], [11], [9].", "startOffset": 50, "endOffset": 53}, {"referenceID": 9, "context": "b,\u221e = 1 in view of the fact that A is a left-stochastic matrix [10].", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "(87) Now, we recall a result from [11], [48] on the block maximum norm of a block diagonal and Hermitian matrix X with M \u00d7M blocks {Xk}, which states that \u2016X\u2016b,\u221e = max k=1,.", "startOffset": 34, "endOffset": 38}, {"referenceID": 47, "context": "(87) Now, we recall a result from [11], [48] on the block maximum norm of a block diagonal and Hermitian matrix X with M \u00d7M blocks {Xk}, which states that \u2016X\u2016b,\u221e = max k=1,.", "startOffset": 40, "endOffset": 44}, {"referenceID": 0, "context": "Alternatively, we convert (100) into an equality recursion as follows: E\u2016w\u0303i\u2016\u03c3 = \u03b8iE\u2016w\u0303i\u22121\u2016F\u03c3 + \u03b8i(1 + \u03c5)r\u03c3 (101) for some coefficient \u03b8i \u2208 [0, 1] that depends on both E\u2016w\u0303i\u2016\u03c3 and E\u2016w\u0303i\u22121\u2016F\u03c3 .", "startOffset": 140, "endOffset": 146}, {"referenceID": 0, "context": "\u2223 \u2264 \u2016F \u03c3\u2016b,\u221e \u2264 \u2016F \u2016b,\u221e\u2016\u03c3\u2016b,\u221e (104) where the second inequality in (104) holds because the coefficients \u03bel(i) \u2208 [0, 1] for all i, whereas the third inequality in (104) holds because the block maximum norm of a vector is greater equal than the largest absolute value of its entries.", "startOffset": 111, "endOffset": 117}], "year": 2012, "abstractText": "This article proposes diffusion LMS strategies for distributed estimation over adaptive networks that are able to exploit sparsity in the underlying system model. The approach relies on convex regularization, common in compressive sensing, to enhance the detection of sparsity via a diffusive process over the network. The resulting algorithms endow networks with learning abilities and allow them to learn the sparse structure from the incoming data in real-time, and also to track variations in the sparsity of the model. We provide convergence and mean-square performance analysis of the proposed method and show under what conditions it outperforms the unregularized diffusion version. We also show how to adaptively select the regularization parameter. Simulation results illustrate the advantage of the proposed filters for sparse data recovery.", "creator": "LaTeX with hyperref package"}}}