{"id": "1206.3204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jun-2012", "title": "Improved Spectral-Norm Bounds for Clustering", "abstract": "Aiming to unify known results about clustering mixtures of distributions under separation conditions, Kumar and Kannan[2010] introduced a deterministic condition for clustering datasets. They showed that this single deterministic condition encompasses many previously studied clustering assumptions. More specifically, their proximity condition requires that in the target $k$-clustering, the projection of a point $x$ onto the line joining its cluster center $\\mu$ and some other center $\\mu'$, is a large additive factor closer to $\\mu$ than to $\\mu'$. This additive factor can be roughly described as $k$ times the spectral norm of the matrix representing the differences between the given (known) dataset and the means of the (unknown) target clustering. Clearly, the proximity condition implies center separation -- the distance between any two centers must be as large as the above mentioned bound.", "histories": [["v1", "Thu, 14 Jun 2012 18:23:46 GMT  (38kb)", "http://arxiv.org/abs/1206.3204v1", null], ["v2", "Fri, 15 Jun 2012 18:11:27 GMT  (42kb)", "http://arxiv.org/abs/1206.3204v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["pranjal awasthi", "or sheffet"], "accepted": false, "id": "1206.3204"}, "pdf": {"name": "1206.3204.pdf", "metadata": {"source": "CRF", "title": "Improved Spectral-Norm Bounds for Clustering", "authors": ["Pranjal Awasthi"], "emails": ["pawasthi@cs.cmu.edu", "osheffet@cs.cmu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 120 6,32 04v1 [cs.LG] 1 4Ju n20 12In this work we improve the work of Kumar and Kannan [KK10] along several axes. Firstly, we weaken the mean separation bound by a factor \u221a k, and secondly, we weaken the state of proximity by a factor k (in other words, the revised separation condition is independent of k). With these weaker limits, we still achieve the same guarantees when all points meet the state of proximity. Under the same weaker limits, we achieve even better guarantees if only (1 \u2212) fractions of the points meet the condition. Specifically, we unify all points except one (year + O (1 / c4) fraction of the points correctly, compared with O (k2\u0435) fractions of the [KK10] fractions of the [McK10] points, which are significant even in the special setting if they have a constant and k = island separation (1)."}, {"heading": "1 Introduction", "text": "\"We must ask ourselves the question to what extent we are able to solve the problems of the world.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"\" We. \"\" \"\". \"\" \"\" We. \"\" \"\" \"We.\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\" \"\". \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\" \".\" \"\". \"\" \"\" \"\" \".\" \"\". \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\""}, {"heading": "1.1 Our Contribution", "text": "In this work, the majority of our analyses are based on the following quantitatively weaker version of our work: \"We assume that the results of our work, which we have compiled in the last two years in the last three years, are much better than the results of our work.\" Even if we show that (1) only a few points meet the condition, our analysis (for most parts) does not divide the dataset into good and bad points, based on the condition or non-satisfactory condition of the proximity condition. (1) Analysis of the proximity of the dataset into good and bad points, based on the condition or non-satisfactory condition of the proximity condition. (2) Instead, our analysis is based on basic tools and the triangular angle condition in which we divide the quality of the dataset into good and bad points, based on the condition or non-satisfactory condition of the proximity condition condition condition."}, {"heading": "1.2 Notations and Preliminaries", "text": "It is a well known fact that if the number of M & # 246; rters & # 10; M & # 8222; n & # 8220; n & # 8222; n & # 8220; n & # 8222; n & # 8220; n & # 8222; n & # 8220; n & # 8222; n & # 8220; n & # 8222; n & # 8222; n & # 8220; n & # 8222; n & # 8222; n & # 8220; n & # 8222; n & # 8220; n & # 8220;. & # 8220; n & # 8222; s & # 8220; n & # 8222; n & # 8222; n & # 8220; n & # 8222; n & # 8222; n & # 8222; n & # 8222; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8222; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8222; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8220; n & # 8222; n & # 8220; n & # 8220; n."}, {"heading": "1.3 Formal Description of the Algorithm and Our Theorems", "text": "Once we have established the notation, we now present our algorithm in Figure 1. The goal of our algorithm is threefold: (a) to find a partition that identifies with the target grouped on the majority of the points, (b) to have the k mean cost of that partition comparable to the target, and (c) to output k centers that are close to the true centers. Each part requires stronger assumptions that allow us to demonstrate stronger guarantees. \u2022 If we only assume the mean separation of (1), Part I results in clustering that (a) is correct in at least 1 \u2212 O (c \u2212 2) fractions of the points from each target cluster (Theorem 3.1), and (b) has k mean cost less than (1 + O (1 / c))."}, {"heading": "1.4 Organization and Proofs Overview", "text": "However, the analysis of Part II of the algorithm is in Section 4. The first part of our analysis is a direct application of facts 1.1 and 1.2. Our assumption dictates that the distance between two centers is large (Part I of the Algorithm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm-Assignorm"}, {"heading": "2 Related Work", "text": "The work of [Das99] was the first to provide theoretical guarantees for the problem of learning a mixture of Gaussians under separation conditions. It showed that one can learn a mixture of k spherical Gaussians, provided that the separation between the cluster agents by [DS07] and the mixture weights is not too small. (\u03c32r denotes the maximum variance of the cluster r along any direction. This separation was improved by [DS07] n1 / 4). Arora and Kannan [SK01] extended these results to the case of the general Gaussians. In the case of the spherical Gaussians [VW02] showed that one can learn under a much weaker separation from (DS07) n1 / 4."}, {"heading": "3 Part I of the Algorithm", "text": "In this section we will consider only Part I of our algorithm. Our goal in this section is to show that Z is correct on all but a small constant fraction of the points, and that the k-mean cost of Z is no more than (1 + O (1 / c) times the k-mean cost of the target cluster. 4For constant k, [KSS04] there is a PTAS for the k-mean cost of Z. Theorem 3.1. There is a matching (given by Fact 1.2) between the target clusters T = {Zr} r, in which Zr = {i: \"A-points-i\" which meet the following properties: \u2022 For each cluster Ts0 in the target cluster we are no more than 1 / 2."}, {"heading": "3.1 Application: The ORSS-Separation", "text": "A simple application of theorem 3.2 concerns the datasets considered by Ostrovsky et al. [ORSS06], where the optimal k mean cost is a fraction of the optimal k \u2212 1 mean cost. Ostrovsky et al. have demonstrated that for such datasets a variant of the Lloyd method converges into a good solution in polynomial time. Kumar and Kannan have shown that datasets that fulfill the ORSSseparation also have the property that fulfills most points of their proximity condition. Their analysis is not immediate and provides a (1 + O) approximation. At this point, we provide a \"one-line\" proof that part I of the algorithm, cluster, yields one (1 + O) approximation for each k. Suppose that we have a dataset that fulfills the ORSS separation condition, so that each (k \u2212 1) partition of the dataset yields a (1 + C) approximation for each k."}, {"heading": "4 Part II of the Algorithm", "text": "In this section, our goal is to show that part II of our algorithm is very close to the target clusters. We should note that from this point on we assume that we are in the non-degenerated case in which we are in the number of SR-2-2-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-5-4-4-4-5-5-4-4-4-4-5-4-4-4-5-5-4-4-4-4-4-4-4-4-5-4-4-4-4-4-4-4-4-4-4-5-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4"}, {"heading": "4.1 The Proximity Condition \u2013 Part III of the Algorithm", "text": "It is obvious that this cluster correctly classifies the majority of the points. It shows that at most Os (c \u2212 2) fractions of the points we cannot satisfy with most Os (c \u2212 2) fractions of the points. To have a direct comparison with the Kumar-Kannan analysis, we now have the number of misclassified points w.r.t The number of points we satisfy."}, {"heading": "5 Applications", "text": "It is not as if there were a derogation from any distribution in any direction and the weight of any distribution is such that for each derogation there is a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a deviation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a derogation from a dero"}, {"heading": "6 An Open Problem", "text": "Our work presents an algorithm that successfully collects a data set, provided that the distance between two cluster centers meets a certain lower limit. We would like to point a certain direction to improve this limit. Note that our center separation depends on the size of the entire data set. It would be nice to deal with the case where the separation condition between micro and micro depends exclusively on Tr and Ts. That is, if we define that most of our analyses (and in particular Lemma 4,5) are based only on the ratio between micro and micro - micro - micro, it is possible to successfully separate clusters."}, {"heading": "A Some Basic Lemmas", "text": "(KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (KV08). (K08). (K08). (K08). (K08). (K08). (KV08). (KV08). (KV08). (KV08). (KV08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (V08). (K08). (K08). (K08). (K08). (K08). (K08). (V08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (V08). (K08). (K08). (K08). (K08). (K08). (K08). (K08). (K08)."}], "references": [{"title": "Stability yields a PTAS for k-median and k-means clustering", "author": ["Pranjal Awasthi", "Avrim Blum", "Or Sheffet"], "venue": "In FOCS,", "citeRegEx": "Awasthi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Awasthi et al\\.", "year": 2010}, {"title": "A spectral technique for coloring random 3-colorable graphs", "author": ["Noga Alon", "Nabil Kahale"], "venue": "In SIAM Journal on Computing,", "citeRegEx": "Alon and Kahale.,? \\Q1994\\E", "shortCiteRegEx": "Alon and Kahale.", "year": 1994}, {"title": "Finding a large hidden clique in a random graph", "author": ["Noga Alon", "Michael Krivelevich", "Benny Sudakov"], "venue": null, "citeRegEx": "Alon et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Alon et al\\.", "year": 1998}, {"title": "On spectral learning of mixtures of distributions", "author": ["Dimitris Achlioptas", "Frank McSherry"], "venue": "In COLT,", "citeRegEx": "Achlioptas and McSherry.,? \\Q2005\\E", "shortCiteRegEx": "Achlioptas and McSherry.", "year": 2005}, {"title": "k-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "In SODA,", "citeRegEx": "Arthur and Vassilvitskii.,? \\Q2007\\E", "shortCiteRegEx": "Arthur and Vassilvitskii.", "year": 2007}, {"title": "Approximate clustering without the approximation", "author": ["Maria-Florina Balcan", "Avrim Blum", "Anupam Gupta"], "venue": "In SODA,", "citeRegEx": "Balcan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2009}, {"title": "Approximate clustering via coresets", "author": ["Mihai B\u0101doiu", "Sariel Har-Peled", "Piotr Indyk"], "venue": "In STOC,", "citeRegEx": "B\u0101doiu et al\\.,? \\Q2002\\E", "shortCiteRegEx": "B\u0101doiu et al\\.", "year": 2002}, {"title": "Polynomial learning of distribution families", "author": ["Mikhail Belkin", "Kaushik Sinha"], "venue": "Computing Research Repository,", "citeRegEx": "Belkin and Sinha.,? \\Q2010\\E", "shortCiteRegEx": "Belkin and Sinha.", "year": 2010}, {"title": "Isotropic pca and affine-invariant clustering", "author": ["S. Charles Brubaker", "Santosh Vempala"], "venue": "In FOCS,", "citeRegEx": "Brubaker and Vempala.,? \\Q2008\\E", "shortCiteRegEx": "Brubaker and Vempala.", "year": 2008}, {"title": "Graph partitioning via adaptive spectral techniques", "author": ["Amin Coja-Oghlan"], "venue": "Comb. Probab. Comput.,", "citeRegEx": "Coja.Oghlan.,? \\Q2010\\E", "shortCiteRegEx": "Coja.Oghlan.", "year": 2010}, {"title": "Learning to match and cluster large highdimensional data sets for data integration", "author": ["William W. Cohen", "Jacob Richman"], "venue": "In KDD,", "citeRegEx": "Cohen and Richman.,? \\Q2002\\E", "shortCiteRegEx": "Cohen and Richman.", "year": 2002}, {"title": "Beyond gaussians: Spectral methods for learning mixtures of heavy-tailed distributions", "author": ["Kamalika Chaudhuri", "Satish Rao"], "venue": "In COLT,", "citeRegEx": "Chaudhuri and Rao.,? \\Q2008\\E", "shortCiteRegEx": "Chaudhuri and Rao.", "year": 2008}, {"title": "Learning mixtures of product distributions using correlations and independence", "author": ["Kamalika Chaudhuri", "Satish Rao"], "venue": "In COLT,", "citeRegEx": "Chaudhuri and Rao.,? \\Q2008\\E", "shortCiteRegEx": "Chaudhuri and Rao.", "year": 2008}, {"title": "Learning mixtures of gaussians", "author": ["Sanjoy Dasgupta"], "venue": "In FOCS,", "citeRegEx": "Dasgupta.,? \\Q1999\\E", "shortCiteRegEx": "Dasgupta.", "year": 1999}, {"title": "Spectral clustering with limited independence", "author": ["Anirban Dasgupta", "John Hopcroft", "Ravi Kannan", "Pradipta Mitra"], "venue": "In SODA,", "citeRegEx": "Dasgupta et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2007}, {"title": "Approximation schemes for clustering problems", "author": ["W. Fernandez de la Vega", "Marek Karpinski", "Claire Kenyon", "Yuval Rabani"], "venue": "In STOC,", "citeRegEx": "Vega et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Vega et al\\.", "year": 2003}, {"title": "A probabilistic analysis of em for mixtures of separated, spherical gaussians", "author": ["Sanjoy Dasgupta", "Leonard Schulman"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Dasgupta and Schulman.,? \\Q2007\\E", "shortCiteRegEx": "Dasgupta and Schulman.", "year": 2007}, {"title": "Deterministic clustering with data", "author": ["Michelle Effros", "Leonard J. Schulman"], "venue": "nets. ECCC,", "citeRegEx": "Effros and Schulman.,? \\Q2004\\E", "shortCiteRegEx": "Effros and Schulman.", "year": 2004}, {"title": "Simpler analyses of local search algorithms for facility location", "author": ["Anupam Gupta", "Kanat Tangwongsan"], "venue": "CoRR, abs/0809.2554,", "citeRegEx": "Gupta and Tangwongsan.,? \\Q2008\\E", "shortCiteRegEx": "Gupta and Tangwongsan.", "year": 2008}, {"title": "Matrix computations (3rd ed.)", "author": ["Gene H. Golub", "Charles F. Van Loan"], "venue": null, "citeRegEx": "Golub and Loan.,? \\Q1996\\E", "shortCiteRegEx": "Golub and Loan.", "year": 1996}, {"title": "On coresets for k-means and k-median clustering", "author": ["Sariel Har-Peled", "Soham Mazumdar"], "venue": "In STOC,", "citeRegEx": "Har.Peled and Mazumdar.,? \\Q2004\\E", "shortCiteRegEx": "Har.Peled and Mazumdar.", "year": 2004}, {"title": "Clustering with spectral norm and the k-means algorithm", "author": ["A. Kumar", "R. Kannan"], "venue": "In FOCS,", "citeRegEx": "Kumar and Kannan.,? \\Q2010\\E", "shortCiteRegEx": "Kumar and Kannan.", "year": 2010}, {"title": "A local search approximation algorithm for k-means clustering", "author": ["Tapas Kanungo", "David M. Mount", "Nathan S. Netanyahu", "Christine D. Piatko", "Ruth Silverman", "Angela Y. Wu"], "venue": "In Proc. 18th Symp. Comp. Geom.,", "citeRegEx": "Kanungo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kanungo et al\\.", "year": 2002}, {"title": "Efficiently learning mixtures of two gaussians", "author": ["Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant"], "venue": "In STOC\u201910,", "citeRegEx": "Kalai et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kalai et al\\.", "year": 2010}, {"title": "A simple linear time (1 + \u01eb)approximation algorithm for k-means clustering in any dimensions", "author": ["Amit Kumar", "Yogish Sabharwal", "Sandeep Sen"], "venue": "In FOCS,", "citeRegEx": "Kumar et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2004}, {"title": "The spectral method for general mixture models", "author": ["Ravindran Kannan", "Hadi Salmasian", "Santosh Vempala"], "venue": "SIAM J. Comput.,", "citeRegEx": "Kannan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Kannan et al\\.", "year": 2008}, {"title": "Spectral algorithms. Found", "author": ["Ravindran Kannan", "Santosh Vempala"], "venue": "Trends Theor. Comput. Sci.,", "citeRegEx": "Kannan and Vempala.,? \\Q2009\\E", "shortCiteRegEx": "Kannan and Vempala.", "year": 2009}, {"title": "Least squares quantization in pcm", "author": ["Stuart P. Lloyd"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Lloyd.,? \\Q1982\\E", "shortCiteRegEx": "Lloyd.", "year": 1982}, {"title": "Scop: a structural classification of proteins database for the investigation of sequences and structures", "author": ["A GMurzin", "S E Brenner", "T Hubbard", "C Chothia"], "venue": "Journal of Molecular Biology,", "citeRegEx": "GMurzin et al\\.,? \\Q1995\\E", "shortCiteRegEx": "GMurzin et al\\.", "year": 1995}, {"title": "Spectral partitioning of random graphs", "author": ["F. McSherry"], "venue": "In FOCS,", "citeRegEx": "McSherry.,? \\Q2001\\E", "shortCiteRegEx": "McSherry.", "year": 2001}, {"title": "Settling the polynomial learnability of mixtures of gaussians", "author": ["Ankur Moitra", "Gregory Valiant"], "venue": "In FOCS\u201910,", "citeRegEx": "Moitra and Valiant.,? \\Q2010\\E", "shortCiteRegEx": "Moitra and Valiant.", "year": 2010}, {"title": "Polynomial time approximation schemes for geometric k-clustering", "author": ["R. Ostrovsky", "Y. Rabani"], "venue": "In FOCS,", "citeRegEx": "Ostrovsky and Rabani.,? \\Q2000\\E", "shortCiteRegEx": "Ostrovsky and Rabani.", "year": 2000}, {"title": "The effectiveness of lloyd-type methods for the k-means problem", "author": ["Rafail Ostrovsky", "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "venue": "In FOCS,", "citeRegEx": "Ostrovsky et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ostrovsky et al\\.", "year": 2006}, {"title": "Clustering for edge-cost minimization (extended abstract)", "author": ["Leonard J. Schulman"], "venue": "In STOC, pages 547\u2013555,", "citeRegEx": "Schulman.,? \\Q2000\\E", "shortCiteRegEx": "Schulman.", "year": 2000}, {"title": "Learning mixtures of arbitrary gaussians", "author": ["Arora Sanjeev", "Ravi Kannan"], "venue": "In STOC,", "citeRegEx": "Sanjeev and Kannan.,? \\Q2001\\E", "shortCiteRegEx": "Sanjeev and Kannan.", "year": 2001}], "referenceMentions": [], "year": 2017, "abstractText": "Aiming to unify known results about clustering mixtures of distributions under separation conditions, Kumar and Kannan [KK10] introduced a deterministic condition for clustering datasets. They showed that this single deterministic condition encompasses many previously studied clustering assumptions. More specifically, their proximity condition requires that in the target k-clustering, the projection of a point x onto the line joining its cluster center \u03bc and some other center \u03bc, is a large additive factor closer to \u03bc than to \u03bc. This additive factor can be roughly described as k times the spectral norm of the matrix representing the differences between the given (known) dataset and the means of the (unknown) target clustering. Clearly, the proximity condition implies center separation \u2013 the distance between any two centers must be as large as the above mentioned bound. In this paper we improve upon the work of Kumar and Kannan [KK10] along several axes. First, we weaken the center separation bound by a factor of \u221a k, and secondly we weaken the proximity condition by a factor of k (in other words, the revised separation condition is independent of k). Using these weaker bounds we still achieve the same guarantees when all points satisfy the proximity condition. Under the same weaker bounds, we achieve even better guarantees when only (1\u2212\u01eb)-fraction of the points satisfy the condition. Specifically, we correctly cluster all but a (\u01eb + O(1/c))-fraction of the points, compared to O(k\u01eb)-fraction of [KK10], which is meaningful even in the particular setting when \u01eb is a constant and k = \u03c9(1). Most importantly, we greatly simplify the analysis of Kumar and Kannan. In fact, in the bulk of our analysis we ignore the proximity condition and use only center separation, along with the simple triangle and Markov inequalities. Yet these basic tools suffice to produce a clustering which (i) is correct on all but a constant fraction of the points, (ii) has k-means cost comparable to the k-means cost of the target clustering, and (iii) has centers very close to the target centers. Our improved separation condition allows us to match the results of the Planted Partition Model of McSherry [McS01], improve upon the results of Ostrovsky et al [ORSS06], and improve separation results for mixture of Gaussian models in a particular setting.", "creator": "LaTeX with hyperref package"}}}