{"id": "1206.3234", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Adaptive Inference on General Graphical Models", "abstract": "Many algorithms and applications involve repeatedly solving variations of the same inference problem; for example we may want to introduce new evidence to the model or perform updates to conditional dependencies. The goal of adaptive inference is to take advantage of what is preserved in the model and perform inference more rapidly than from scratch. In this paper, we describe techniques for adaptive inference on general graphs that support marginal computation and updates to the conditional probabilities and dependencies in logarithmic time. We give experimental results for an implementation of our algorithm, and demonstrate its potential performance benefit in the study of protein structure.", "histories": [["v1", "Wed, 13 Jun 2012 14:16:36 GMT  (154kb)", "http://arxiv.org/abs/1206.3234v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.DS cs.AI", "authors": ["umut a acar", "alexander t ihler", "ramgopal mettu", "ozgur sumer"], "accepted": false, "id": "1206.3234"}, "pdf": {"name": "1206.3234.pdf", "metadata": {"source": "CRF", "title": "Adaptive Inference on General Graphical Models", "authors": ["Umut A. Acar", "Alexander T. Ihler", "U.C. Irvine", "Ramgopal R. Mettu"], "emails": ["umut@tti-c.org", "ihler@ics.uci.edu", "mettu@ecs.umass.edu", "osumer@cs.uchicago.edu"], "sections": [{"heading": null, "text": "Many algorithms and applications involve repeatedly solving variations of the same inference problem; for example, we would like to introduce new evidence for the model or perform conditional dependency updates. Adaptive inference aims to take advantage of the advantages obtained in the model and draw conclusions more quickly than from scratch. In this paper, we describe techniques for adaptive inference to general graphs that support boundary calculation and updating conditional probabilities and dependencies in logarithmic time. We provide experimental results for implementing our algorithm and demonstrate its potential performance advantage in studying protein structure."}, {"heading": "1 Introduction", "text": "It is common in many applications to repeatedly draw conclusions about variations in the essentially same graphical model. However, in view of a number of learning problems, we can use observation data to modify a part of the model (e.g. to represent an observed marginal distribution) and then recalculate various moments of the new model before further updating the model [8]. Another example is the study of protein structures, in which a graphical model can be used to represent the conformation space of a protein structure [15, 9]. The most likely configuration in this model then corresponds to the minimum energy conformation for the corresponding protein. An application of interest in this setting is the carrying out of amino acid mutations in the protein to determine the effect of these mutations on the function of the protein. Of course, the changes described in the examples above can be treated by incorporating them into the model. A. Acar is supported by a gift from Intel. \u2020 R. Mettu is supported by a National FoundationScience Award 0637."}, {"heading": "2 Background", "text": "Graphic models provide a practical formalism for describing the structure within a function g (X) defined by a set of variables X = [x1,..., xn] (usually a common probability distribution or energy function via the xi).Graphic models use this structure to organize calculations with g (\u00b7) and construct efficient algorithms for many consequence tasks, including optimization, to find a Max-imum a posteriori (MAP) configuration, to marginalize or calculate the probability of the observed data. For the purposes of this work, we assume that each variable xi takes values from a limited set and focuses primarily on the problem of marginalization."}, {"heading": "2.1 Factor Graphs", "text": "Factor graphs [10] describe the factorization structure of the function g (X) using a bipartite graph consisting of factor nodes and variable nodes. Specifically, suppose such a graph G consists of factor nodes F = {f1,..., fm} and variable nodes X = {x1,..., xn}, and let Xj X designate the neighbors of the factor node fj. Then it is said that G is compatible with a function g (\u00b7) if and only with ifg (x1,., xn) = premij (Xj). In a common misuse of notation, we have used the same symbols to specify both each variable node and its associated variable xi, as well as for each factor node and its associated function fj. Frequently, it is convenient to refer to vertices without specifying whether they are variable or factor nodes. For this purpose, we define a series of \"artificial factors\" (both Xv = factors)."}, {"heading": "2.2 Marginalization", "text": "A classic consequence problem is the marginalization of the function g (X). Specifically, however, for some or all of the factors of xi we are interested in calculating the marginal functiongi (xi) = \u2211 X\\ xig (X). If the representation of the factor diagram of g (X) is individually connected (tree-structured), the marginalization can be efficiently performed with sum product [10]. In tree-structure graphs, sum product is typically formulated as a dual sequence: root formation of the tree at a node v, messages are sent up (leaves to the root), then down, according to which one can calculate the marginality for each node in the diagram. In more general diagrams (diagrams with cycles), the exact inference is less straightforward. One solution is to use a node tree [11]; this leads first to a tree-structured hypergraph of G, then essentially the same process of calculating margins."}, {"heading": "2.3 RC-Trees for Adaptive Inference", "text": "In [2] an algorithm for adaptive inferences to factor trees is described using a random selection method and the functions are stored at each node in the RC tree, which represents sufficient statistics for its sub-tree. It has been shown that creating the RC tree data structure requires time and space that are linear in the number of vertices n of the factor diagram, and produces a balanced tree with the expected height O (log n).The sufficient statistics stored in the RC tree can be used to \"query\" or calculate the boundary distributions in the factor tree by passing information downwards, taking time for the most expected O-points (kdk + 2 log n), where k is the maximum degree of the factor tree (log n), and d is the maximum dimension of each variable. In addition, changes to the tree can also be included in the expected O (kdk + 2 log n) time, with k representing the properties of the boundary tree we choose during the boundary structure of the factor tree."}, {"heading": "3 Hierarchical Clustering and Inference", "text": "We start with an idea of hierarchical clustering in factor diagrams that is compatible with, but more general than, the clustering induced by RC trees. We then describe how this clustering can be used to calculate the boundary distribution at each vertex of the factor diagram.X\\ Xc g \u00b7 h = \u2211 x g (u, v, x) \u00b7 h (x, y)."}, {"heading": "3.1 Hierarchical Clustering", "text": "For a factor graphic G = (X + F, E), a cluster C = (S + E), a cluster C = (S + E), a cluster C = (S + E), a cluster (S + E), a cluster (S + E), a cluster (S + E), a cluster (S + E), a cluster (S + E), a cluster E (S + E), a cluster E (S + E), a cluster E (S + E), a cluster (S + E), a cluster E (S + E), a cluster (S + E), a cluster (S + E), a cluster E (S + E), a cluster E (S + E), a cluster E (S + E), a cluster (S + E), a cluster E (S + E), a cluster (S + E), a cluster E (S + E), a cluster E (S + E), a cluster E (S + E + E), a cluster (S + E), a cluster E + E (S + E), a cluster E + E (S + E + E), a cluster (S + E + E + E, a cluster (S + E + E), a cluster E + E cluster E + E + E (S + E + E), a cluster E + E + E (S + E + E), a cluster E cluster E + E + E + E + E + E cluster (S + E + E + E + E + E), A cluster (S + E + E + E + E + E + E + E + E + E), A cluster (S + E + E cluster E + E + E + E + E + E cluster E + E + E + E + E + E + E + E cluster (S + E (S + E + E + E), A cluster E + E + E + E + E + E + E cluster E + E + E + E + E + E + E + E + E cluster E cluster E + E + E + E + E + E + E"}, {"heading": "3.2 Computing Marginal Distributions", "text": "As with bucket removal, the root of the cluster tree provides the boundary function for all variables that are removed last. In addition, it is also easy to calculate the boundary values at any other vertex by passing information down through the cluster tree. We calculate the boundary distribution of a node v as a sequence. We calculate the order from v to the root (v1 = v, vn of the root). We calculate a downward trend of marginalization functions from vn to v2 asMv, and leave v1,. We calculate the order from v to the root (v1 = v, vn of the root)."}, {"heading": "4 A Cluster Tree Data Structure", "text": "This year, as never before in the history of the city, where it has come to the point where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place."}, {"heading": "4.1 Interface and Efficiency", "text": "The interface supports the following operations: Cluster (G, T), Query (v), replaceFactor (old, new), insertTreeEdge (e), deleteTreeEdge (e), insertTreeEdge (e), deleteTree (e), deleteTree (e), deleteTree (e), deleteTree (e), deleteFactor (e) Operation replaces a factor with another factor. The rest of the operations insert or delete edges in the input factor graph. To analyze the efficiency of our data structure, we define a term for measuring a factor and its voltage tree."}, {"heading": "5 Experimental Results", "text": "We compare the performance of a Matlab implementation of our algorithm with the standard implementation of a Junction Tree-based Summary Product algorithm provided by the Bayes' Net Toolbox (BNT) [12]. We examine acceleration through adaptive conclusions in two scenarios: synthetic data that provide some control over the diagram size and tree width of the problems, and graphical models built from known protein backbone structures."}, {"heading": "5.1 Synthetic Data", "text": "For our synthetic dataset, we randomly generated factor diagrams with n variables and m factors, where 50 \u2264 n \u2264 1000 and m = n \u2212 1. We initialize each input graph as a simple Markov chain, where each factor fi depends on the variables xi and xi + 1, where 1 \u2264 i < n. This chain includes the set of tree edges in our algorithm. Then, we add k and cycles to the given parameters, adding the edges outside the tree as follows: If i is a multiple of k, we add the variable xi to the factor fi + \u2212 1 to create a length cycle. This results in a fairly structured but loop-like graph with limited tree width. The results for these synthetic experiments are shown in Fig. 5. We first calculate the (wall clock) time needed to construct the cluster tree of the diagram (using k = 2 and 2 =)."}, {"heading": "5.2 Application to Protein Structure", "text": "Graphic models constructed from protein structures were used to successfully predict structural properties [15] as well as free energy [9]. These models are typically constructed by using each node as an amino acid whose state represents rotamere [7], and conditional probabilities are based on a physical energy function (e.g. [14, 4]). A typical goal of these models is to efficiently calculate the most likely (i.e. energy-saving) conformation of the protein in its natural environment. However, actualizing factors allow us to study, for example, the effects of amino acid mutations, and the addition and removal of edges correspond directly with the enabling of spine movements in the protein. Moreover, the effect of these updates on the model in logarithmic time, which was not possible in previous approaches. To test the feasibility of our algorithm for these applications, we constructed factor diagrams from five benchmark-based heliosphere proteins [4] that were drawn from the SCELW4."}, {"heading": "6 Conclusion", "text": "We describe an efficient algorithm for adaptive conclusions in general graphical models. Our algorithm constructs a balanced representation of an overarching tree of the graphical input model and presents cycles in the model by commenting on this data structure. We can support all updates and boundary calculations in the expected O (\u03b1\u03b2 log n) time, where \u03b1 is a constant and \u03b2 is the size of a given graph section. Our experiments show that this approach provides significant acceleration of both synthetic and real protein data."}], "references": [{"title": "Dynamizing static algorithms with applications to dynamic trees and history independence", "author": ["U. Acar", "G. Blelloch", "R. Harper", "J. Vittes", "M. Woo"], "venue": "In ACM-SIAM Symposium on Discrete Algorithms (SODA),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Adaptive Bayesian inference", "author": ["U. Acar", "A.T. Ihler", "R.R. Mettu", "\u00d6 S\u00fcmer"], "venue": "In Proc. NIPS. MIT Press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "An experimental analysis of change propagation in dynamic trees", "author": ["U.A. Acar", "G. Blelloch", "J. Vittes"], "venue": "In Proc. 7th ACM-SIAM W. on Algorithm Eng. and Exp\u2019ts,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "A graph-theory algorithm for rapid protein side-chain prediction", "author": ["A.A. Canutescu", "A.A. Shelenkov", "R.L. Dunbrack Jr."], "venue": "Protein Sci,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Bucket elimination: A unifying framework for probabilistic inference", "author": ["R. Dechter"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Logarithmic-time updates and queries in probabilistic networks", "author": ["A.L. Delcher", "A.J. Grove", "S. Kasif", "J. Pearl"], "venue": "J. Artificial Intelligence Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Rotamer libraries in the 21st century", "author": ["R.L. Dunbrack Jr."], "venue": "Curr Opin Struct Biol,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "An iterative procedure for estimation in contingency tables", "author": ["S.E. Fienberg"], "venue": "Ann. Math. Stat.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1970}, {"title": "Free energy estimates of all-atom protein structures using generalized belief propagation", "author": ["H. Kamisetty", "E. P Xing", "C.J. Langmead"], "venue": "In Proc. 11th Ann. Int\u2019l Conf. Research in Computational Molecular Biology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F. Kschischang", "B. Frey", "H.-A. Loeliger"], "venue": "IEEE Trans. Inform. Theory,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Local computations with probabilities on graphical structures and their applications to expert systems", "author": ["S. Lauritzen", "D. Spiegelhalter"], "venue": "J. Royal Stat. Society, Ser. B,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1988}, {"title": "The Bayes net toolbox for Matlab", "author": ["K. Murphy"], "venue": "Computing Science and Statistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1988}, {"title": "A new force field for the molecular mechanical simulation of nucleic acids and proteins", "author": ["S.J. Weiner", "P.A. Kollman", "D.A. Case", "U.C. Singh", "G. Alagona", "S. Profeta Jr.", "P. Weiner"], "venue": "J. Am. Chem. Soc.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1984}, {"title": "Approximate inference and protein folding", "author": ["C. Yanover", "Y. Weiss"], "venue": "In Proc. NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2002}], "referenceMentions": [{"referenceID": 7, "context": ", fitting an observed marginal distribution), and then recompute various moments of the new model before updating the model further [8].", "startOffset": 132, "endOffset": 135}, {"referenceID": 14, "context": "Another example is in the study of protein structures, where a graphical model can be used to represent the conformation space of a protein structure [15, 9].", "startOffset": 150, "endOffset": 157}, {"referenceID": 8, "context": "Another example is in the study of protein structures, where a graphical model can be used to represent the conformation space of a protein structure [15, 9].", "startOffset": 150, "endOffset": 157}, {"referenceID": 5, "context": "[6] studied this problem under a set of fairly restrictive conditions, requiring that the graph be tree-structured and supporting only changes to the observed evidence in the model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] gave a method of supporting more general changes to the model so long as the model remains tree-structured.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "In principle, we can perform adaptive inference on loopy graphs by construcing their junction tree [13] and applying existing frameworks to the junction tree itself [6, 2].", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "In principle, we can perform adaptive inference on loopy graphs by construcing their junction tree [13] and applying existing frameworks to the junction tree itself [6, 2].", "startOffset": 165, "endOffset": 171}, {"referenceID": 1, "context": "In principle, we can perform adaptive inference on loopy graphs by construcing their junction tree [13] and applying existing frameworks to the junction tree itself [6, 2].", "startOffset": 165, "endOffset": 171}, {"referenceID": 1, "context": "Compared to previous work on factor trees [2], we also simplify marginal computations.", "startOffset": 42, "endOffset": 45}, {"referenceID": 9, "context": "Factor graphs [10] describe the factorization structure of the function g(X) using a bipartite graph consisting of factor nodes and variable nodes.", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "When the factor graph representation of g(X) is singlyconnected (tree-structured), marginalization can be performed efficiently using sum-product [10].", "startOffset": 146, "endOffset": 150}, {"referenceID": 10, "context": "One solution is to use a junction tree [11]; this first constructs a tree-structured hypergraph of G, then runs essentially the same inference process to compute marginals.", "startOffset": 39, "endOffset": 43}, {"referenceID": 4, "context": "An alternate but essentially equivalent view of exact inference is given by the bucket elimination algorithm [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "Bucket elimination is closely related to junction tree based inference, and an equivalent junction tree may be defined implicitly by its specified elimination ordering [5].", "startOffset": 168, "endOffset": 171}, {"referenceID": 1, "context": "In [2], an algorithm for adaptive inference in factor trees is described using \u201crake and compress\u201d trees (RC-trees).", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "In the previous work [2], the combination of G being treestructured and the selection criteria for creating clusters via rake or compress operations ensured that the computational complexity of each of these calculations was limited.", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "To compute and maintain a balanced clustering, we use the RC-Tree (Rake-and-Compress) tree data structure [1, 3].", "startOffset": 106, "endOffset": 112}, {"referenceID": 2, "context": "To compute and maintain a balanced clustering, we use the RC-Tree (Rake-and-Compress) tree data structure [1, 3].", "startOffset": 106, "endOffset": 112}, {"referenceID": 0, "context": "We then insert/delete (u, f) from the spanning tree and use the change-propagation method supplied by the RC-Tree to update the clustering [1].", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "Proof: It is known that the cluster tree can be computed in expected O(n) time, independent of the cluster functions and boundaries [1, 3], and that the depth of the cluster tree is O(log n) in expectation.", "startOffset": 132, "endOffset": 138}, {"referenceID": 2, "context": "Proof: It is known that the cluster tree can be computed in expected O(n) time, independent of the cluster functions and boundaries [1, 3], and that the depth of the cluster tree is O(log n) in expectation.", "startOffset": 132, "endOffset": 138}, {"referenceID": 11, "context": "We compare the performance of a Matlab implementation of our algorithm to a standard implementation of a junction tree-based sum-product algorithm provided by the Bayes\u2019 Net Toolbox (BNT) [12].", "startOffset": 188, "endOffset": 192}, {"referenceID": 14, "context": "Graphical models constructed from protein structures have been used to successfully predict structural properties [15]", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "as well as free energy [9].", "startOffset": 23, "endOffset": 26}, {"referenceID": 6, "context": "These models are typically constructed by taking each node as an amino acid whose states represent rotamers [7], and basing conditional probabilities on a physical energy function (e.", "startOffset": 108, "endOffset": 111}, {"referenceID": 13, "context": ", [14, 4]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 3, "context": ", [14, 4]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 3, "context": "To test the feasibility of our algorithm for these applications, we constructed factor graphs from five moderatelysized proteins drawn from the SCWRL benchmark [4].", "startOffset": 160, "endOffset": 163}], "year": 2008, "abstractText": "Many algorithms and applications involve repeatedly solving variations of the same inference problem; for example we may want to introduce new evidence to the model or perform updates to conditional dependencies. The goal of adaptive inference is to take advantage of what is preserved in the model and perform inference more rapidly than from scratch. In this paper, we describe techniques for adaptive inference on general graphs that support marginal computation and updates to the conditional probabilities and dependencies in logarithmic time. We give experimental results for an implementation of our algorithm, and demonstrate its potential performance benefit in the study of protein structure.", "creator": null}}}