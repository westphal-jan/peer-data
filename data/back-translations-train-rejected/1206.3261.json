{"id": "1206.3261", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Learning When to Take Advice: A Statistical Test for Achieving A Correlated Equilibrium", "abstract": "We study a multiagent learning problem where agents can either learn via repeated interactions, or can follow the advice of a mediator who suggests possible actions to take. We present an algorithmthat each agent can use so that, with high probability, they can verify whether or not the mediator's advice is useful. In particular, if the mediator's advice is useful then agents will reach a correlated equilibrium, but if the mediator's advice is not useful, then agents are not harmed by using our test, and can fall back to their original learning algorithm. We then generalize our algorithm and show that in the limit it always correctly verifies the mediator's advice.", "histories": [["v1", "Wed, 13 Jun 2012 15:33:53 GMT  (367kb)", "http://arxiv.org/abs/1206.3261v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.GT cs.AI cs.MA", "authors": ["greg hines", "kate larson"], "accepted": false, "id": "1206.3261"}, "pdf": {"name": "1206.3261.pdf", "metadata": {"source": "CRF", "title": "Learning When to Take Advice: A Statistical Test for Achieving A Correlated Equilibrium", "authors": ["Greg Hines"], "emails": ["ggdhines@cs.uwaterloo.ca", "klarson@cs.uwaterloo.ca"], "sections": [{"heading": null, "text": "We investigate a multi-agent learning problem where the facilitators can either learn through repeated interactions or follow the advice of a mediator who suggests possible action. We present an algorithm that any facilitator can use so that they are highly likely to check whether the mediator's advice is useful or not. In particular, if the mediator's advice is useful, the facilitators will achieve a correlated balance, but if the mediator's advice is not useful, the facilitators will not be harmed by our test and will be able to revert to their original learning algorithm."}, {"heading": "1 Introduction", "text": "In an environment where agents interact with each other over and over again (for example, through a replay), there are great opportunities for learning, as agents are able to adapt their strategies in light of the history of the game. This issue has attracted a lot of attention from several research communities, including the AI community and the game theory community. While many criteria have been proposed in this paper for measuring the success of learning approaches, a commonly used metric is whether agents learn how to best respond to the strategies played by the others. That is, the learning process converges into a balance. In this paper, we examine the problem of interaction between agents in a replay situation, but we present a third-party facilitator or consultant who makes policy suggestions for the agents. Ideally, by following the facilitator's suggestions, agents will be able to play against each other, and possibly even achieve mutually beneficial results that are not possible without mediation."}, {"heading": "2 Background", "text": "In this section, we present the key concepts and assumptions contained in this paper.A n-agent stage game is a tuple G = < N, A = A1 \u00b7.. \u00b7 An, u1,.., un >, where N = {1,.., n} is the set of agents, Ai is the set of possible actions i and A is the set of possible joint actions, and ui: A \u2192 R is the utility function for agent i. Without loss of generality, we assume that all utilities are greater or equal to 0. A specific action for agent i is ai Ai, and a joint action is a set of possible joint actions, and a joint action is one (a1,., an). We assume that A is public knowledge, but the utility functions of agents are private.Each agent chooses his actions according to a strategy. A strategy for agent i, \u0441\u0442i, is a distribution of probabilities over Ai, stating the probability with which the agent will play the possible action."}, {"heading": "3 Setup", "text": "The setting for our work is a repetitive game with a mediator, M. As is illustrated for the two agents case in Figure 1, the time t will begin with the mediator giving each agent a proposed action, sti. Agents will then simultaneously choose their action, ati, which may or may not be sti. If the agent i decides not to follow M's signal, he can instead use a learning algorithm, Li, which we assume is independent of M's signals, to select an action. Based on the actual common action, each agent will then receive a benefit and the process repeats itself. The signal from the mediator to each agent is a private information known only to this agent and the mediator, just as the benefit function of the agent set for each agent is public knowledge, just like the action taken by each agent during a spin. The signals from the mediator to each agent are correlated information known only to this agent and the mediator, just like the benefit function of the agent set for each agent is public knowledge, just like the action taken by each agent during a spin and the process repeats itself. The signals from the mediator to each agent are correlated information known only to this agent and the mediator, just like the benefit function of the agent set for each agent is public knowledge, just like the action taken by each agent during a spin, and the process repeats itself."}, {"heading": "4 The Initial Algorithm", "text": "In this section, we will describe how our initial algorithm works against the percentages predicted by the difference between these two significant strategies. In this case, Agent i will use a \"fallback\" strategy to see if M's signals will continue to be tracked throughout the sampling test.Since the benefits for each agent, as well as the signals they receive, are each private, there can be no way to prove or refute Condition 2 with absolute certainty during the game.The best solution we can do is to reach a probable conclusion. As common actions are public knowledge, we can compare the empirical percentages of the game for the duration of the sampling tests."}, {"heading": "4.1 Examples", "text": "In this section we provide two examples to illustrate how our test would work. Example 1: Consider the game in Figure 2.Let A = {(a1,1, a2,1), (a1,1, a2,2), (a2,1, a2,1), (a1,2, a2,2). Suppose M announces a correlated strategy, \u03c3MA = {1 / 18, 5 / 18, 2 / 18, 10 / 18}. Note: \u03c3MA is a correlated equilibrium. Suppose the agents choose p = 0,1 and \u03b4 = 0,01. Agents must now determine the critical value for rejection, c (\u03b1), and the length of the sampling test, lT."}, {"heading": "5 Repeated Testing", "text": "The limitation of our basic test is that there is always a certain positive error probability. This is due to the need to select values for 1 \u2212 p and \u03b4, both of which are greater than 0. Therefore, since we can select such values for 1 \u2212 p and \u03b4, this is not a great practical limitation, but we can hope to achieve a stronger theoretical result. Our goal is to have agents that approach the game if there is a correlated equilibrium. If this number is not a correlated equilibrium, then the usefulness of the agents for using our algorithm should not be worse. This leads to the idea of repeated tests in which agents will use several iterations of the repetition phase throughout the repetition phase. The amount of repeated sampling tests is R = {R1, R2,.}, where Rj = {bRj}, bRj} is the first time span Rj, and lj is the length of the Rj."}, {"heading": "6 Conclusion", "text": "We presented a test that the agents could use so that they could determine with a high probability whether the mediator's proposal was a correlated balance. So, if the mediator proposes a correlated balance, the agents will approach it, and otherwise the test will not be worse in the long run because he has used our algorithm. We envision several directions for future research. First, it might be possible to expand our algorithm to work in radically decoupled environments where the agents are unaware of the existence of others. This would significantly reduce the knowledge requirements of our test so that such a balance is possible."}, {"heading": "7 Acknowledgements", "text": "We thank Gord Hines for his statistical advice."}, {"heading": "A Proof of Lemma 1", "text": "The proof. If we consider disturbances in relation to an \"A,\" which is indicated by \"A\" = \"A.\" Since \"J\" is represented for all, \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" (F) = \"F\" \u2212 \"(F) \u2212\" (F) \u2212 \"\u2212\" (F) \u2212 \"\u2212\" (F) \u2212 \"\u2212\" (F) \u2212 \"\u2212\" (F) \u2212 \"(F) \u2212\" (F) = \"F\" (F) \u2212 \"(F) \u2212\" \u2212 \"(F) \u2212\" (F) \u2212 \"(F) \u2212\" (F) = \"F\" (F) \u2212 \"(F) \u2212\" (F) = \"F\" (F) \u2212 \"(F) \u2212\" \u2212 \"(F) \u2212\" \u2212 \"(F) \u2212\" \u2212 \"(F) \u2212\" \u2212 \"\u2212\" \u2212 \"(F) \u2212\" \u2212 \"\u2212\" \u2212 \"\u2212\" (F) \u2212 \"\u2212\" \u2212 \"\u2212\" (F) \u2212 \"\u2212\" \u2212 \"(F) \u2212\" \u2212 \"\u2212\" \u2212 \"(F) \u2212\" \u2212 \"(F) \u2212\" \u2212 \"(F) \u2212\" (F) = \"F\" (F) = \"F\" (F) = \"F (F) \u2212\" \u2212 \u2212 \"\u2212\" \u2212 \"\u2212\" \u2212 \"\u2212\" \u2212 \"(F\" \u2212 \"(F) \u2212\" \u2212 \"(F) \u2212\" \u2212 \"(F) \u2212\" \u2212 \"(F) \u2212\" (F) \u2212 \"(F) \u2212\" (F) \u2212 \"(F) \u2212\" \u2212 \"(F\" (F) \u2212 \"(F) \u2212\" (F \"(F) \u2212\" (F) \u2212 (F) \u2212 \"(F) \u2212\" \u2212 \"(F\" (F) \u2212 \"(F) \u2212\" (F \"(F) \u2212\" (F \"(F) \u2212\" (F"}], "references": [{"title": "Subjectivity and correlation in randomized strategies", "author": ["R. Aumann"], "venue": "Journal of Mathematical Economics, 1:67\u201396,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1974}, {"title": "Statistical Power Analysis for the Behavioral Sciences", "author": ["J. Cohen"], "venue": "2nd edition,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1988}, {"title": "Combining expert advice in reactive environments", "author": ["D.P.D. Farias", "N. Megiddo"], "venue": "Journal of the ACM, 53(5):762\u2013799,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Calibrated learning and correlated equilibrium", "author": ["D.P. Foster", "R. Vohra"], "venue": "Games and Economic Behavior, 21:40\u201355,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning, hypothesis testing, and Nash equilibrium", "author": ["D.P. Foster", "H.P. Young"], "venue": "Games and Economic Behavior, 45:73\u201396,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Network games", "author": ["A. Galeotti", "S. Goyal", "M.O. Jackson", "F. Vega- Redondo", "L. Yariv"], "venue": "Unpublished, Jan", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Correlated Q-learning", "author": ["A. Greenwald", "K. Hall"], "venue": "Proceedings of ICML-2003, pages 242\u2013249, Washington, DC, USA,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "A simple adaptive procedure leading to correlated equilibrium", "author": ["S. Hart", "A. Mas-Colell"], "venue": "Econometrica, 68:1127\u20131150,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Continuous Univariate Distributions, volume", "author": ["N. Johnson", "S. Kotz", "N. Balakrishnan"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1995}, {"title": "Correlated equilibria in graphical games", "author": ["S. Kakade", "M. Kearns", "J. Langford", "L. Ortiz"], "venue": "EC \u201903: Proceedings of the 4th ACM Conference on Electronic Commerce, pages 42\u201347, New York, NY, USA,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Strong mediated equilibrium", "author": ["D. Monderer", "M. Tennenholtz"], "venue": "Proceedings of the 21st American Association of Artificial Intelligence Conference, Boston, MA, USA,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "All of Statistics", "author": ["L. Wasserman"], "venue": "Springer,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "That is, our goal is for the agents to learn and adapt so that they find a correlated equilibrium [1].", "startOffset": 98, "endOffset": 101}, {"referenceID": 4, "context": "While hypothesis testing has been proposed in the multiagent learning literature as a tool that agents might use to learn how to play Nash equilibria [5], to the best of our knowledge it has never been applied for validating a mediator\u2019s advice.", "startOffset": 150, "endOffset": 153}, {"referenceID": 10, "context": "Note that our type of a mediator is different than Monderer and Tennenholtz\u2019s, where agents must agree to follow the mediator\u2019s suggested actions before knowing what they are [11].", "startOffset": 175, "endOffset": 179}, {"referenceID": 0, "context": "for all ai \u2208 Ai [1].", "startOffset": 16, "endOffset": 19}, {"referenceID": 3, "context": "Thus, our algorithm is differentiated from algorithms that achieve convergence to the set of correlated equilibrium, for example [4, 8].", "startOffset": 129, "endOffset": 135}, {"referenceID": 7, "context": "Thus, our algorithm is differentiated from algorithms that achieve convergence to the set of correlated equilibrium, for example [4, 8].", "startOffset": 129, "endOffset": 135}, {"referenceID": 11, "context": "where A is any subset of A such that |A| = |A| \u2212 1, X(a) = lT\u03c3 hist(lT ) A (a) is the actual frequency of play of a \u2208 A during the sampling test, E(a) = lT\u03c3 A (a) is the expected frequency of play according to \u03c3 A , and where lT is the length of the sampling period [12].", "startOffset": 266, "endOffset": 270}, {"referenceID": 8, "context": "The Pearson\u2019s \u03c72 test has (in the limit) a probability distribution function of \u03c7df + \u03c7 2 NCP,1, (6) where the first distribution has df = |A|\u22122 degrees of freedom, and the second distribution has 1 degree of freedom and a non-centrality parameter of NCP [9].", "startOffset": 255, "endOffset": 258}, {"referenceID": 8, "context": "then the probability of a Type 2 error is bounded by some value \u03b2(\u03b4\u0302) < 1, whose value is normally found via numerical computation [9].", "startOffset": 131, "endOffset": 134}, {"referenceID": 8, "context": "In practice, lT (\u03b1,\u03b2, \u03b4) would now be solved by some method of numerical computation [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "For simplicity, we used the tables in Cohen to obtain a value of lT = 2100 [2].", "startOffset": 75, "endOffset": 78}, {"referenceID": 11, "context": "The p-value is the smallest \u03b1 value that would still allow us to reject the hypothesis [12].", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "We assume that Li is flexible at the beginning of each free period [3].", "startOffset": 67, "endOffset": 70}, {"referenceID": 10, "context": "It may also be interesting to apply our approach to other solution concepts such as mediated equilibria [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 6, "context": "Thus, our approach could be combined with methods such as Q-learning [7].", "startOffset": 69, "endOffset": 72}, {"referenceID": 9, "context": "Correlated equilibria have also been used in graphical games, which can be used to model many different settings [10].", "startOffset": 113, "endOffset": 117}, {"referenceID": 5, "context": "For example, network games use graphical games to help represent a variety of problems, from public good provision and trade to information collection [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 5, "context": "These models can be hindered by a \u201cfundamental theoretical problem: even the simplest games played on networks have multiple equilibrium[sic] which display a bewildering range of possible outcomes\u201d [6].", "startOffset": 198, "endOffset": 201}], "year": 2008, "abstractText": "We study a multiagent learning problem where agents can either learn via repeated interactions, or can follow the advice of a mediator who suggests possible actions to take. We present an algorithm that each agent can use so that, with high probability, they can verify whether or not the mediator\u2019s advice is useful. In particular, if the mediator\u2019s advice is useful then agents will reach a correlated equilibrium, but if the mediator\u2019s advice is not useful, then agents are not harmed by using our test, and can fall back to their original learning algorithm. We then generalize our algorithm and show that in the limit it always correctly verifies the mediator\u2019s advice.", "creator": "dvips(k) 5.96.1 Copyright 2007 Radical Eye Software"}}}