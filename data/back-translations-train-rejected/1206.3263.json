{"id": "1206.3263", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Sparse Stochastic Finite-State Controllers for POMDPs", "abstract": "Bounded policy iteration is an approach to solving infinite-horizon POMDPs that represents policies as stochastic finite-state controllers and iteratively improves a controller by adjusting the parameters of each node using linear programming. In the original algorithm, the size of the linear programs, and thus the complexity of policy improvement, depends on the number of parameters of each node, which grows with the size of the controller. But in practice, the number of parameters of a node with non-zero values is often very small, and does not grow with the size of the controller. Based on this observation, we develop a version of bounded policy iteration that leverages the sparse structure of a stochastic finite-state controller. In each iteration, it improves a policy by the same amount as the original algorithm, but with much better scalability.", "histories": [["v1", "Wed, 13 Jun 2012 15:34:42 GMT  (112kb)", "http://arxiv.org/abs/1206.3263v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eric a hansen"], "accepted": false, "id": "1206.3263"}, "pdf": {"name": "1206.3263.pdf", "metadata": {"source": "CRF", "title": "Sparse Stochastic Finite-State Controllers for POMDPs", "authors": ["Eric A. Hansen"], "emails": ["hansen@cse.msstate.edu"], "sections": [{"heading": null, "text": "Bounded Policy Iteration is an approach to solving POMDPs with an infinite horizon that presents policies as stochastic finite state controllers and iteratively improves a controller by adjusting the parameters of each node using linear programming. In the original algorithm, the size of linear programs, and thus the complexity of policy improvement, depends on the number of parameters of each node growing with the size of the controller. In practice, however, the number of parameters of a node with non-zero values is often very small and does not grow with the size of the controller. Based on this observation, we are developing a version of bounded policy iteration that uses the sparse structure of a stochastic finite state controller. In each iteration, it improves a policy by the same amount as the original algorithm, but with much better scalability."}, {"heading": "1 Introduction", "text": "Partially observable Markov decision-making processes (POMDPs) provide a framework for decision-theoretical planning problems where actions must be taken on the basis of imperfect state information. Many researchers have shown that a policy for an infinite horizon POMDP can be represented by a finite state controller, but in some cases this is a deterministic controller in which a single action is associated with each node, and an observation leads to a deterministic transition to a successor node (Kaelbling, Littman, & Cassandra, 1998; Meuleau, Kim, Kaelbling, & Cassandra, 1999a). In other cases it is a stochastic controller in which actions are selected on the basis of a probability distribution associated with each node, and an observation leads to a probable transition to a successor node (Platzman, 1981; Meuleau, Peshkin, Kim, Kim, Kim, Bartlett, 2001, Kaelbling, Baxlett, 1999b; Bouxlier)."}, {"heading": "2 Background", "text": "We consider a discrete temporally infinite horizon POMDP with a finite set of states, S, a finite set of actions, A, and a finite set of observations, Z. Each time the environment is in a state in which the agent performs an action, A, the environment makes a transition to the state S with the probability of P (s), and the agent observes z (z) Z with the probability of P (z | s). In addition, the agent receives an immediate reward with the expected value R (s, a). We assume that the goal is to maximize the expected discounted reward overall, where \u03b2 (0, 1) is the discount factor. Since the state of the environment cannot be directly observed, we call b a | S | dimensional vector of state probabilities, which is called a state of faith, where b (s) denotes the probability that the system is in state s."}, {"heading": "2.1 Policy representation and evaluation", "text": "A policy for a POMDP can be represented by a finite state controller (FSC). A stochastic finite state control is a tuple < N, \u0432 >, where N is a finite set of nodes, is an action selection function that indicates the probability that the FSC will make a transition from node n \u00b2 N to node n \u00b2 N, and that it is a node transition function that indicates the probability \u03b7n (a, z, n \u00b2) = P (n \u00b2, a, z) that the FSC will make a transition from node n \u00b2 N to node n \u00b2 N after taking a measure. The value function of a policy represented by an FSC is piecemeal and convex, and can be calculated precisely by solving the following system of linear equations."}, {"heading": "2.2 Bounded policy iteration", "text": "In fact, it is the case that most of us are able to abide by the rules which they have imposed on themselves, and that they are able to abide by the rules which they have imposed on themselves. (...) It is not the case that they abide by the rules. (...) \"It is not the case that they abide by the rules.\" (...) \"It is so.\" (...) \"It is not the case that\" It is so. \"(...)\" (...) \"(...)\" (...) \"(...)\" (It is. \"(...)\" (...) \"(\") \"(It is not so.\" (...) \"(...)\" (...) \"(...\") \"(\") \"(\" (It is.) \"(...) (\" (\") ((\") ((\"). (\") ((\") (\") ((\") ((\"). (\") (\" (\") (\") (\") ((\") (). (\"(\") (\") (\") (). (\"(\") (\") (). (\" () (). (\"() () () () (). (\" () () () () (). () () (() () (). () () ((() (). () () () (() (). () () () () ((). () (() () () () () ((() (). () () () () () () (() () (() () () (() (() ((). () () (() () () (() () ((() (() () () (() ((() (() ().) (() (() ((() () () (() () () ((() () ((() () () (() (() () (() () (() ((() (() (() () () (() (() (() ((("}, {"heading": "3 Sparse bounded policy iteration", "text": "In this section, we describe a modified version of BPI, which we call Sparse BPI. In each iteration, it improves an FSC by the same amount as the original algorithm, but with significantly improved scalability. To motivate our approach, we begin with a discussion of the sparse structure of stochastic finitestate controllers found by BPI."}, {"heading": "3.1 Sparse stochastic finite-state controllers", "text": "As we have seen, every iteration of BPI | N | linear programs solves, and every linear program has | A | + | A | Z | N | variables and | S | + | A | Z | constraints (in addition to the constraints that the variables have non-negative values). Even for small FSCs, the number of variables in the linear programs can be very large, and the fact that the number of variables with the number of nodes in the controller significantly limits the scalability of BPI. However, if we look at the controllers produced by BPI, we find that most of the parameters in each node (i.e., most of the variables in the linear program solutions) have values of zero. Table 2 illustrates this vividly for four benchmark POMDPs that are not intended for the nodes."}, {"heading": "3.2 Sparse policy improvement algorithm", "text": "We will start by describing the main idea of the algorithm. We have seen that the number of parameters of a node with non-zero probabilities in the solution of the linear program in Table 1 is very small if the number of secured parameters is limited. However, if we could somehow identify the useful parameters of a node, we could improve a node by solving a linear program that includes only these variables. We will call a linear program that contains only some of the parameters of a reduced linear program. Our approach will be to solve a series of reduced linear programs in which the last one is guaranteed to include all useful parameters of the node. This approach will achieve the same result as a solution of the full linear program in Table 1, but will be more efficient if the reduced linear programs are much smaller. Next, we will describe a small method of identifying parameters that includes all useful parameters of the useful parameters."}, {"heading": "4 Experimental results", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "5 Conclusion", "text": "The new algorithm takes advantage of the sparse structure of stochastic finite-state controllers found by a limited policy iteration. Each iteration of the algorithm results in the identical improvement of a controller, which produces an iteration of the originally limited policy iteration algorithm, but with improved scalability. While the time the original algorithm takes to improve a single node grows with the size of the controller, the time the new algorithm takes to improve a single node is typically independent of the size of the controller. This makes it convenient to use limited policy iteration to find larger controllers for POMDPs."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the Air Force Office of Scientific Research under Award No. 05-003202A05."}], "references": [{"title": "Solving POMDPs using quadratically constrained linear programs", "author": ["C. Amato", "D. Bernstein", "S. Zilberstein"], "venue": "In Proceedings of the 20th International Joint Conference on Artificial Intelligence", "citeRegEx": "Amato et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Amato et al\\.", "year": 2007}, {"title": "Infinite-horizon policygradient estimation", "author": ["J. Baxter", "P. Bartlett"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Baxter and Bartlett,? \\Q2001\\E", "shortCiteRegEx": "Baxter and Bartlett", "year": 2001}, {"title": "Bounded policy iteration for decentralized POMDPs", "author": ["D. Bernstein", "E. Hansen", "S. Zilberstein"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence(IJCAI-05),", "citeRegEx": "Bernstein et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Bernstein et al\\.", "year": 2005}, {"title": "Introduction to Linear Optimization", "author": ["D. Bertsekas", "J. Tsitsiklis"], "venue": "Athena Scientific", "citeRegEx": "Bertsekas and Tsitsiklis,? \\Q1997\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis", "year": 1997}, {"title": "Tony\u2019s POMDP file repository page.. http://pomdp.org/pomdp/examples/index.shtml", "author": ["A. Cassandra"], "venue": null, "citeRegEx": "Cassandra,? \\Q2004\\E", "shortCiteRegEx": "Cassandra", "year": 2004}, {"title": "Approximate planning for factored POMDPs", "author": ["Z. Feng", "E. Hansen"], "venue": "In Proceedings of the 6th European Conference on Planning", "citeRegEx": "Feng and Hansen,? \\Q2001\\E", "shortCiteRegEx": "Feng and Hansen", "year": 2001}, {"title": "Solving POMDPs by searching in policy space", "author": ["E. Hansen"], "venue": "In Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Hansen,? \\Q1998\\E", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Point-based policy iteration", "author": ["S. Ji", "R. Parr", "H. Li", "X. Liao", "L. Carin"], "venue": "In Proceedings of the 22nd National Conference on Artificial Intelligence", "citeRegEx": "Ji et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2007}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L. Kaelbling", "M. Littman", "A. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Solving POMDPs by searching the space of finite policies", "author": ["N. Meuleau", "K. Kim", "L. Kaelbling", "A. Cassandra"], "venue": "In Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Meuleau et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 1999}, {"title": "Learning finite-state controllers for partially observable environments", "author": ["N. Meuleau", "L. Peshkin", "K. Kim", "L. Kaelbling"], "venue": "In Proceedings ofthe 15th Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Meuleau et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 1999}, {"title": "Point-based value iteration: An anytime algorithm for POMDPs", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In Proceedings of the 18th International Joint Conference on Artificial Intelligence", "citeRegEx": "Pineau et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2003}, {"title": "A feasible computational approach to infinite-horizon partially-observed Markov decision problems", "author": ["L. Platzman"], "venue": "Tech. rep., Georgia Institute of Technology. Reprinted in Working Notes of the AAAI Fall Symposium on Planning using Partially Observable", "citeRegEx": "Platzman,? \\Q1981\\E", "shortCiteRegEx": "Platzman", "year": 1981}, {"title": "Exploiting Structure to Efficiently Solve Large Scale Partially Observable Markov Decision Processes", "author": ["P. Poupart"], "venue": "Ph.D. thesis,", "citeRegEx": "Poupart,? \\Q2005\\E", "shortCiteRegEx": "Poupart", "year": 2005}, {"title": "Bounded finite state controllers", "author": ["P. Poupart", "C. Boutilier"], "venue": "In Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference,", "citeRegEx": "Poupart and Boutilier,? \\Q2004\\E", "shortCiteRegEx": "Poupart and Boutilier", "year": 2004}, {"title": "VDCBPI: An approximate scalable algorithm for large POMDPs", "author": ["P. Poupart", "C. Boutilier"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Poupart and Boutilier,? \\Q2005\\E", "shortCiteRegEx": "Poupart and Boutilier", "year": 2005}], "referenceMentions": [{"referenceID": 6, "context": "In some cases, this is a deterministic controller in which a single action is associated with each node, and an observation results in a deterministic transition to a successor node (Kaelbling, Littman, & Cassandra, 1998; Hansen, 1998; Meuleau, Kim, Kaelbling, & Cassandra, 1999a).", "startOffset": 182, "endOffset": 280}, {"referenceID": 12, "context": "In other cases, it is a stochastic controller in which actions are selected based on a probability distribution associated with each node, and an observation results in a probabilistic transition to a successor node (Platzman, 1981; Meuleau, Peshkin, Kim, & Kaelbling, 1999b; Baxter & Bartlett, 2001; Poupart & Boutilier, 2004; Amato, Bernstein, & Zilberstein, 2007).", "startOffset": 216, "endOffset": 366}, {"referenceID": 6, "context": "BPI is related to an exact policy iteration algorithm for POMDPs due to Hansen (1998), but differs from it by providing an elegant and effective approach to approximation in which bounding the size of the controller allows a tradeoff between planning time and plan quality.", "startOffset": 72, "endOffset": 86}, {"referenceID": 6, "context": "Hansen (1998) proposed a policy iteration algorithm for POMDPs that represents a policy as a deterministic finite-state controller.", "startOffset": 0, "endOffset": 14}, {"referenceID": 5, "context": "Feng and Hansen (2001) describe a method for performing approximate dynamic programming updates that has the effect of reducing the number of vectors added to a value function in value iteration, or the number of nodes added to a controller in policy iteration.", "startOffset": 0, "endOffset": 23}, {"referenceID": 13, "context": "For policy iteration, Poupart and Boutilier (2004) propose another approach that also has the benefit of avoiding an expensive dynamic programming update.", "startOffset": 22, "endOffset": 51}, {"referenceID": 4, "context": ") The four test POMDPs are from (Cassandra, 2004).", "startOffset": 32, "endOffset": 49}, {"referenceID": 13, "context": "For example, Poupart and Boutilier (2005) describe a technique called value-directed compression and report that it allows BPI to solve POMDPs with millions of states.", "startOffset": 13, "endOffset": 42}, {"referenceID": 4, "context": "Table 2 illustrates this vividly for four benchmark POMDPs from (Cassandra, 2004).", "startOffset": 64, "endOffset": 81}], "year": 2008, "abstractText": "Bounded policy iteration is an approach to solving infinite-horizon POMDPs that represents policies as stochastic finite-state controllers and iteratively improves a controller by adjusting the parameters of each node using linear programming. In the original algorithm, the size of the linear programs, and thus the complexity of policy improvement, depends on the number of parameters of each node, which grows with the size of the controller. But in practice, the number of parameters of a node with non-zero values is often very small, and does not grow with the size of the controller. Based on this observation, we develop a version of bounded policy iteration that leverages the sparse structure of a stochastic finite-state controller. In each iteration, it improves a policy by the same amount as the original algorithm, but with much better scalability.", "creator": " TeX output 2008.05.15:0148"}}}