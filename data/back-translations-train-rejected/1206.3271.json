{"id": "1206.3271", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Learning Arithmetic Circuits", "abstract": "Graphical models are usually learned without regard to the cost of doing inference with them. As a result, even if a good model is learned, it may perform poorly at prediction, because it requires approximate inference. We propose an alternative: learning models with a score function that directly penalizes the cost of inference. Specifically, we learn arithmetic circuits with a penalty on the number of edges in the circuit (in which the cost of inference is linear). Our algorithm is equivalent to learning a Bayesian network with context-specific independence by greedily splitting conditional distributions, at each step scoring the candidates by compiling the resulting network into an arithmetic circuit, and using its size as the penalty. We show how this can be done efficiently, without compiling a circuit from scratch for each candidate. Experiments on several real-world domains show that our algorithm is able to learn tractable models with very large treewidth, and yields more accurate predictions than a standard context-specific Bayesian network learner, in far less time.", "histories": [["v1", "Wed, 13 Jun 2012 15:38:26 GMT  (383kb)", "http://arxiv.org/abs/1206.3271v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daniel lowd", "pedro domingos"], "accepted": false, "id": "1206.3271"}, "pdf": {"name": "1206.3271.pdf", "metadata": {"source": "CRF", "title": "Learning Arithmetic Circuits", "authors": ["Daniel Lowd"], "emails": ["lowd@cs.washington.edu", "pedrod@cs.washington.edu"], "sections": [{"heading": null, "text": "We propose an alternative: learning models with a score function that directly penalizes the cost of inference. Specifically, we learn arithmetic circuits with a penalty for the number of edges in the circuit (where the cost of inference is linear); our algorithm corresponds to learning a Bayesian network with context-specific independence by greedily splitting distributions, evaluating candidates at each step by assembling the resulting network into an arithmetic circuit and using its size as a penalty; we show how this can be done efficiently without recompiling a circuit from scratch for each candidate; experiments in several areas of the real world show that our algorithm is capable of learning tractable models with very large tree widths and providing more accurate predictions than a context-specific Bayesian network, far less time."}, {"heading": "1 INTRODUCTION", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "2 BAYESIAN NETWORKS", "text": "A Bayesian network encodes the common probability distribution of a set of n variables, {X1,.., Xn}, as a directed acyclic graph and a series of conditional probability distributions (CPDs) (Pearl, 1988). Each node corresponds to a variable, and the associated CPD indicates the probability of each state of the variable in the face of any possible combination of states of its parents. Therefore, the common distribution of the variables is determined by P (X1,., Xn) = 1 P (Xi) = 1 P (Xi) = 1 P (Xi).For discrete domains, the simplest form of CPD is a conditional probability table."}, {"heading": "2.1 LOCAL STRUCTURE", "text": "A more scalable approach is to use decision trees as CPDs, taking advantage of context-specific dependencies (i.e., a child variable is independent of some of its parents to whom some of the values of the others are assigned) (Boutilier et al., 1996; Friedman & Goldszmidt, 1996; Chickering et al., 1997).The algorithm we present in this paper learns arithmetic circuits that are of this type of Bayesian network.In a CPD for variable Xi decision tree, each inner node is labeled with one of the parent variables, and each of its starting edges is labeled with a value of this variable.1 Each leaf node is a multinomical one representing the marginal distribution of Xi and variable values of Baye.The following two definitions will be useful when we set the variable distribution of Xi to the parent values."}, {"heading": "3 ARITHMETIC CIRCUITS", "text": "The probability distribution represented by a Bayesian network can be represented by a multilinear function known as network polynomia (Darwiche, 2003): P (X1 = x1, Xn = xn) = [Xn] i = 1I (Xi = xi) P (Xi = xi | \u03c0i), where the sum spans all possible instances of the variables, I () is the indicator function (1 if the argument is true, otherwise 0), and P (Xi | i) are the parameters of the Bayesian network. The probability of partial instantiation of the variables can now be easily calculated by setting all indicators consistent with the instantiation, and all others to 0, allowing arbitrary marginal and conditional queries to be answered linearly in time in the size of the polynomia."}, {"heading": "4 LEARNING ARITHMETIC CIRCUITS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 SCORING AND SEARCHING", "text": "Instead of learning a Bayesian network and then compiling it into a circuit, we induce an arithmetic circuit directly from the data with a score function that penalizes circuits with more edges.The score of an arithmetic circuit C on an i.i.d. training sample T isscore (C, T) = log P (T | C) \u2212 kpnp (C), where the first term is the log probability of the training data, P (T | C) = x T P (X | C), ke \u2265 0 is the pro-edge penalty, ne (C) is the number of edges in the circuit, kp \u2265 0 is the pro-parameter penalty, and np (C) is the number of parameters in the circuit. The last two allow us to combine our inference cost penalty with a conventional one."}, {"heading": "4.2 SPLITTING DISTRIBUTIONS", "text": "It is about the question to what extent it is about a way in which people are able to change the world and to change the world, and in which people are able to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change the world, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change the world, to change the world, to change the world, to change the world, to change the world, to change, to change, to change, to change, to change, to change, to change the world, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to"}, {"heading": "4.3 OPTIMIZATIONS", "text": "This year, it is more than ever before that it will be able to take the lead."}, {"heading": "5 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 DATASETS", "text": "The KDD Cup 2000 Clickstream Prediction dataset (Kohavi et al., 2000) consists of web session data from an online retailer. Using the subset of Hulten and Domingos (2002), each example consists of 65 Boolean variables that correspond to the question of whether a particular session visited a web page that corresponds to a particular category or not. Anonymous MSWeb are visitor data for 294 areas (vroots) of the Microsoft website collected during a week in February 1998. They can be found in the UCI Machine Learning Repository (Blake & Merz, 2000). EachMovie3 is a collaborative filter dataset in which users rate movies they have seen. We took a 10% sample of the original dataset that focused on the 500 most frequently rated movies, and reduced each variable to \"rated\" or \"not rated.\" For the KDD Cup and MSWeb, we used the data sets provided with the 10% for the training tests and the% for each match."}, {"heading": "5.2 LEARNING", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "5.3 INFERENCE", "text": "For each set of data, we used the test data to generate queries with different numbers of randomly selected query and evidence variables. Each query asked for the probability of configuring the query variables in the test example due to the configuration of the evidence variables in the same test example. We estimate accuracy as the mean log probability of the test sample configuration in all test examples. This is an approximation (up to an additive constant) of the Kullback-Leibler divergence between the inferred distribution and the true one estimated from the test samples. For KDD Cup and MSWeb, we generated queries from 1000 test examples; for EachMovie, we did not finish Gen-5AC-Greedy in the maximum allowed time of 72h. As a result, it has fewer edges and lower probability than AC-Quick.erated queries from all 593 test examples."}, {"heading": "6 CONCLUSION", "text": "In the past, learning and deduction were largely separated in graphical models, leading to the somewhat paradoxical conclusion that a great deal of effort is put into learning accurate models, only to lead to less accurate predictions when approximate conclusions are required. Our work seeks to improve this through closer integration of learning and deduction. Specifically, we introduced an algorithm for learning arithmetic circuits by maximizing the likelihood of having a penalty for circuit size. This ensures efficient deductions while providing great flexibility in modeling. In real-world experiments, our algorithm outperformed standard Bavarian network learning both in terms of the accuracy of query responses and the speed of deduction. Guidelines for future work include: studying other algorithms for learning arithmetic circuits; expanding our approach to dealing with missing data and hidden variables learning; applying Markov networks, continuous domains, and representations, etc."}, {"heading": "Acknowledgements", "text": "The authors thank Mark Chavira, Adnan Darwiche and Knot Pipatsrisawat for their help in using c2d in our Bayesian networks. This research was partially funded by a Microsoft Research Fellowship awarded to the first author. DARPA contracts NBCH-D030010 / 02000225, FA8750-07-D-0185 and HR0011-07-C-0060, DARPA grants FA8750-05-2-0283, NSF grants IIS-0534881 and ONR grants N-00014-05-1-0313. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official guidelines of DARPA, NSF, ONR or the United States Government."}, {"heading": "APPENDIX: PROOF SKETCH FOR", "text": "THEOREM 1Lemma 2: With each iteration of LearnAC (D, V), the resulting circuit is equivalent, decomposable, and deterministic. This can be proved by invoking SplitAC in each iteration. It is therefore easy to verify that the initial circuit is smooth, decomposable, and deterministic. Verifying that these properties are obtained through each invocation of SplitAC involves a second induction on the structure of the circuit that works from the sheet nodes. Evidence can be found in Lowd and Domingos (2008). Verifying that these properties are obtained through each invocation of SplitAC. (Induction on the number of splits performed) The initial circuit is a product of marginal distributions, equivalent to a Bayesian network without arcs, so that the base case is satisfactory. Assuming the circuit C was equivalent to a network split after the last IB, which we must show after the last IB application."}], "references": [{"title": "Complexity of finding embeddings in a k-tree", "author": ["S. Arnborg", "D.W. Corneil", "A. Proskurowski"], "venue": "SIAM J. Algebraic & Discrete Methods,", "citeRegEx": "Arnborg et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Arnborg et al\\.", "year": 1987}, {"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "Proc. UAI-96", "citeRegEx": "Boutilier et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1996}, {"title": "Efficient principled learning of thin junction trees", "author": ["A. Chechetka", "C. Guestrin"], "venue": null, "citeRegEx": "Chechetka and Guestrin,? \\Q2008\\E", "shortCiteRegEx": "Chechetka and Guestrin", "year": 2008}, {"title": "A Bayesian approach to learning Bayesian networks with local structure", "author": ["D. Chickering", "D. Heckerman", "C. Meek"], "venue": "Proc. UAI-97", "citeRegEx": "Chickering et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Chickering et al\\.", "year": 1997}, {"title": "The WinMine toolkit", "author": ["D.M. Chickering"], "venue": "(Tech. Rept. MSR-TR-2002-103). Microsoft,", "citeRegEx": "Chickering,? \\Q2002\\E", "shortCiteRegEx": "Chickering", "year": 2002}, {"title": "A logical approach to factoring belief networks", "author": ["A. Darwiche"], "venue": "Proc. KR-02", "citeRegEx": "Darwiche,? \\Q2002\\E", "shortCiteRegEx": "Darwiche", "year": 2002}, {"title": "A differential approach to inference in Bayesian networks", "author": ["A. Darwiche"], "venue": "J. ACM,", "citeRegEx": "Darwiche,? \\Q2003\\E", "shortCiteRegEx": "Darwiche", "year": 2003}, {"title": "Learning Bayesian networks with local structure", "author": ["N. Friedman", "M. Goldszmidt"], "venue": "Proc. UAI-96", "citeRegEx": "Friedman and Goldszmidt,? \\Q1996\\E", "shortCiteRegEx": "Friedman and Goldszmidt", "year": 1996}, {"title": "Mining complex models from arbitrarily large databases in constant time", "author": ["G. Hulten", "P. Domingos"], "venue": "Proc. KDD-02", "citeRegEx": "Hulten and Domingos,? \\Q2002\\E", "shortCiteRegEx": "Hulten and Domingos", "year": 2002}, {"title": "Learning probabilistic decision graphs", "author": ["M. Jaeger", "J. Nielsen", "T. Silander"], "venue": "Intl. J. Approx. Reasoning,", "citeRegEx": "Jaeger et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Jaeger et al\\.", "year": 2006}, {"title": "organizers\u2019 report: Peeling the onion", "author": ["R. Kohavi", "C. Brodley", "B. Frasca", "L. Mason", "Z. Zheng"], "venue": "KDD-Cup", "citeRegEx": "Kohavi et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Kohavi et al\\.", "year": 2000}, {"title": "Naive Bayes models for probability estimation", "author": ["D. Lowd", "P. Domingos"], "venue": "Proc. ICML-05", "citeRegEx": "Lowd and Domingos,? \\Q2005\\E", "shortCiteRegEx": "Lowd and Domingos", "year": 2005}, {"title": "Learning arithmetic circuits (Tech", "author": ["D. Lowd", "P. Domingos"], "venue": "Rept.). Dept. CSE,", "citeRegEx": "Lowd and Domingos,? \\Q2008\\E", "shortCiteRegEx": "Lowd and Domingos", "year": 2008}, {"title": "Learning with mixtures of trees", "author": ["M. Meila", "M. Jordan"], "venue": "J. Mach. Learn. Research,", "citeRegEx": "Meila and Jordan,? \\Q2000\\E", "shortCiteRegEx": "Meila and Jordan", "year": 2000}, {"title": "Probabilistic reasoning in intelligent systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "On the hardness of approximate reasoning", "author": ["D. Roth"], "venue": "Artif. Intel.,", "citeRegEx": "Roth,? \\Q1996\\E", "shortCiteRegEx": "Roth", "year": 1996}, {"title": "Maximum likelihood Markov networks: An algorithmic approach", "author": ["N. Srebro"], "venue": "Master\u2019s thesis,", "citeRegEx": "Srebro,? \\Q2000\\E", "shortCiteRegEx": "Srebro", "year": 2000}, {"title": "Let L be the logical image of C and L\u2032 be the logical image of C \u2032. From Lemma 2 and the discussion of logical images, we know", "author": ["L C"], "venue": null, "citeRegEx": "C et al\\.,? \\Q2002\\E", "shortCiteRegEx": "C et al\\.", "year": 2002}], "referenceMentions": [{"referenceID": 6, "context": "Our method takes advantage of recent advances in exact inference by compilation to arithmetic circuits (Darwiche, 2003).", "startOffset": 103, "endOffset": 119}, {"referenceID": 10, "context": ": Meila and Jordan (2000); Lowd and Domingos (2005)) and graphical models with thin junction trees (e.", "startOffset": 2, "endOffset": 26}, {"referenceID": 10, "context": ": Meila and Jordan (2000); Lowd and Domingos (2005)) and graphical models with thin junction trees (e.", "startOffset": 27, "endOffset": 52}, {"referenceID": 10, "context": ": Meila and Jordan (2000); Lowd and Domingos (2005)) and graphical models with thin junction trees (e.g.: Srebro (2000); Chechetka and Guestrin (2008)).", "startOffset": 27, "endOffset": 120}, {"referenceID": 2, "context": ": Srebro (2000); Chechetka and Guestrin (2008)).", "startOffset": 17, "endOffset": 47}, {"referenceID": 9, "context": "The prior work most closely related to ours is Jaeger et al.\u2019s (2006). Jaeger et al.", "startOffset": 47, "endOffset": 70}, {"referenceID": 14, "context": ", Xn}, as a directed acyclic graph and a set of conditional probability distributions (CPDs) (Pearl, 1988).", "startOffset": 93, "endOffset": 106}, {"referenceID": 0, "context": "Finding the minimum-treewidth junction tree is NP-hard (Arnborg et al., 1987).", "startOffset": 55, "endOffset": 77}, {"referenceID": 15, "context": "Inference in Bayesian networks is #Pcomplete (Roth, 1996).", "startOffset": 45, "endOffset": 57}, {"referenceID": 1, "context": ", a child variable is independent of some of its parents given some values of the others) (Boutilier et al., 1996; Friedman & Goldszmidt, 1996; Chickering et al., 1997).", "startOffset": 90, "endOffset": 168}, {"referenceID": 3, "context": ", a child variable is independent of some of its parents given some values of the others) (Boutilier et al., 1996; Friedman & Goldszmidt, 1996; Chickering et al., 1997).", "startOffset": 90, "endOffset": 168}, {"referenceID": 3, "context": "For simplicity, we limit our discussion to the case in which each edge has a single label, which Chickering et al. (1997) refer to as a complete split.", "startOffset": 97, "endOffset": 122}, {"referenceID": 3, "context": "A number of other methods have also been proposed, such as merging leaves to obtain decision graphs (Chickering et al., 1997) or searching through Bayesian network structures and inducing decision trees conditioned on the global structure (Friedman & Goldszmidt, 1996).", "startOffset": 100, "endOffset": 125}, {"referenceID": 3, "context": "This is one version of Chickering et al.\u2019s algorithm (1997). A number of other methods have also been proposed, such as merging leaves to obtain decision graphs (Chickering et al.", "startOffset": 23, "endOffset": 60}, {"referenceID": 6, "context": "The probability distribution represented by a Bayesian network can be equivalently represented by a multilinear function known as the network polynomial (Darwiche, 2003):", "startOffset": 153, "endOffset": 169}, {"referenceID": 5, "context": "Darwiche (2003) describes one way to do this, by encoding the network into a special logical form, factoring the logical form, and extracting the corresponding arithmetic circuit.", "startOffset": 0, "endOffset": 16}, {"referenceID": 5, "context": "SplitAC maintains a consistent probability distribution by preserving three properties, analogous to those defined by Darwiche (2002) for logical circuits.", "startOffset": 118, "endOffset": 134}, {"referenceID": 10, "context": "The KDD Cup 2000 clickstream prediction dataset (Kohavi et al., 2000) consists of web session data taken from an online retailer.", "startOffset": 48, "endOffset": 69}, {"referenceID": 8, "context": "Using the subset of Hulten and Domingos (2002), each example consists of 65 Boolean variables, corresponding to whether or not a particular session visited a web page matching a certain category.", "startOffset": 20, "endOffset": 47}, {"referenceID": 4, "context": "We used the WinMine Toolkit (Chickering, 2002) as a baseline.", "startOffset": 28, "endOffset": 46}, {"referenceID": 3, "context": "WinMine implements the algorithm for learning Bayesian networks with local structure described in Section 2 (Chickering et al., 1997), and has a number of other state-of-the-art features.", "startOffset": 108, "endOffset": 133}], "year": 2008, "abstractText": "Graphical models are usually learned without regard to the cost of doing inference with them. As a result, even if a good model is learned, it may perform poorly at prediction, because it requires approximate inference. We propose an alternative: learning models with a score function that directly penalizes the cost of inference. Specifically, we learn arithmetic circuits with a penalty on the number of edges in the circuit (in which the cost of inference is linear). Our algorithm is equivalent to learning a Bayesian network with context-specific independence by greedily splitting conditional distributions, at each step scoring the candidates by compiling the resulting network into an arithmetic circuit, and using its size as the penalty. We show how this can be done efficiently, without compiling a circuit from scratch for each candidate. Experiments on several real-world domains show that our algorithm is able to learn tractable models with very large treewidth, and yields more accurate predictions than a standard context-specific Bayesian network learner, in far less time.", "creator": "TeX"}}}