{"id": "1206.3272", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Improving Gradient Estimation by Incorporating Sensor Data", "abstract": "An efficient policy search algorithm should estimate the local gradient of the objective function, with respect to the policy parameters, from as few trials as possible. Whereas most policy search methods estimate this gradient by observing the rewards obtained during policy trials, we show, both theoretically and empirically, that taking into account the sensor data as well gives better gradient estimates and hence faster learning. The reason is that rewards obtained during policy execution vary from trial to trial due to noise in the environment; sensor data, which correlates with the noise, can be used to partially correct for this variation, resulting in an estimatorwith lower variance.", "histories": [["v1", "Wed, 13 Jun 2012 15:38:50 GMT  (238kb)", "http://arxiv.org/abs/1206.3272v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gregory lawrence", "stuart russell"], "accepted": false, "id": "1206.3272"}, "pdf": {"name": "1206.3272.pdf", "metadata": {"source": "CRF", "title": "Improving Gradient Estimation by Incorporating Sensor Data", "authors": ["Gregory Lawrence"], "emails": ["gregl@cs.berkeley.edu", "russell@cs.berkeley.edu"], "sections": [{"heading": null, "text": "An efficient political search algorithm should estimate the local gradient of objective function in terms of policy parameters from as few studies as possible. While most political search methods estimate this gradient by observing the rewards obtained during political studies, we show, both theoretically and empirically, that taking sensor data into account enables better gradient estimates and thus faster learning, because the rewards obtained during policy implementation from environmental noise vary from study to study; sensor data that correlate with noise can be used to partially correct this variation, resulting in an estimator with less variance."}, {"heading": "1 INTRODUCTION", "text": "These methods therefore work by adjusting the parameters of a policy to improve its value, i.e., the expected sum of rewards (possibly without taking into account) achieved during the execution of the policy. To do this, the algorithms will repeatedly estimate the gradient of value in relation to the parameters, using information observed during political experiments, and then adjust the parameters in the \"uphill\" direction. Since studies can be expensive, especially in physical environments, a number of authors may have presented techniques to reduce the number of required studies - mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 11, 13].Generally speaking, these methods appreciate the gradient of the policy parameters settings on each study and the actual sum of rewards."}, {"heading": "2 INCORPORATING SENSOR INFORMATION", "text": "The optimal policy \u03c0 maximizes the value function V (\u03c0) = E [f (h) | \u03c0], in which the story h is generated from \u03c0. In this essay, we perform the policy search by climbing through a space of parameterized policies \u03c0 Rd. We climb through this space by adjusting our current policy \u03c00 towards the gradient \u03c0V (\u03c0) | \u03c0 = \u03c00. At each gradient, the gradient is estimated on the basis of n policy studies, in which we vary the parameters of each study for exploration purposes."}, {"heading": "2.1 TOY EXAMPLE", "text": "In order to illustrate the main contributions of this paper, we will examine the problem of toy guns (Figure 1)."}, {"heading": "2.2 VARIANCE REDUCTION", "text": "In this section, we compare the variance of a gradient estimator that includes sensor data with an estimator that ignores it. We assume that the score function is an unknown linear function of both political parameters and sensor data. Gradient estimators take political studies from the current increase and provide an estimate of the impact of noise. To obtain an unbiased estimate, sensor data must be uncorrelated with the results of the scores. In Section 2.2.3, terms for the bias and deviation of a gradient estimator in the correlated setting that correlates with the noise-related disturbances in the score are used. To obtain an unbiased estimate, sensor data must be estimated incorrectly, not correlated with the results of the score."}, {"heading": "2.2.1 Ignore Sensor Data", "text": "From the point of view of a gradient estimator who ignores the sensor data, additional noise is added to the values that are not explained by interference in the sensor data. Let v represent this noise where each element is given by the equation v = sTAs + w. The variance of each entry in v is given by the expression AsT\u0432 sAs + \u03c32. The score function f1 (\u03c0, v) = \u03c0TA\u03c0 + b + v is equivalent to f2. We can learn the linear relationship between the political parameters and the score by performing linear regression on the set of n policy experiments. In other words, we find a suitable estimate for A\u03c0 \u2212 b + v is equivalent to f2. (2) We are interested in the gradient of E [f1 (\u03c0, v)."}, {"heading": "2.2.2 Include Sensor Data", "text": "A linear model that predicts the score as a function of both the political parameters and the sensor data has noise on the output partially explained by the sensor data. An estimate of the score as a linear function of the political parameters and sensor data can be called [g2 (S, f) \u03b22 (gs, S, f)] = [postostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostostosto"}, {"heading": "2.2.3 Correlated Sensors", "text": "If the sensors are correlated with the political parameters, then the gradient estimator, which includes the sensor data, is distorted. In this situation, we can represent the distribution via the sensors as linear Gaussian distribution s-N (A\u03c0, sT\u03c0 + b\u03c0, s). If we insert it into the score function f2, the following equation results: f3 (\u03c0, s, w, w\u03c0, s) = \u03c0TA\u03c0 + \u03c0TA\u03c0, sAs + b\u03c0, s TAs + wElsewhere, s is a Gaussian random variable with a variance from 0 to 0. The gradient with respect to \u03c00 is written as \u03940E [f3, s, w, w\u03c0, s) | \u03c00] = A\u03c0 + A\u03c0, sAs, which means that the gradient estimator g2 of A\u03c0, sAs \u2212 sAs is distorted whenever the sensors are correlated with the political parameters."}, {"heading": "2.3 FINDING GOOD SENSOR ENCODINGS", "text": "In the previous subsection, we saw that S should be independent of \u0432 in order to provide an unbiased estimate of the gradient, which often does not apply to many problems. Furthermore, the analyses suggest that in order to obtain an improved gradient estimator, we should prefer low-dimensional sensor encodings that have a large impact on the score. This section represents a heuristics that can be used to find good sensor encodings that provide performance gains on problems where the assumptions are incorrect. We also want sensor data not to correlate with the political parameters that lie in a high-dimensional space and project them down to a low-dimensional subspace. Our approach is to find possible sensor encodings to find the optimal projection at each rise. We use cross-validation to measure the quality of each projection."}, {"heading": "3 RESULTS", "text": "In this section, we describe how an agent can use his sensor data to improve his learning performance on a dart throwing task [6] and a quadruple motion task. These partially observable sequential tasks are partly due to the fact that the transition and the sensor models and their structures are unknown. These characteristics are characteristic of a wide range of real tasks.Figure 4 (a) shows a single frame of the dart throwing problem, in which the aim is to throw a dart with minimal average square errors (measured from where the dart hits the wall in the middle of the dartboard).The arm is modeled as a tripartite rigid body with dimensions based on biological measurements [2].The linkages correspond to the upper arm, forearm, and hand and are connected to a single degree of free rotation joints. The upper arm is connected to the shoulder in a fixed place."}, {"heading": "3.1 SENSORS FOR MOTOR CONTROL", "text": "The systems described in the previous subsection are able to measure the state of the observable joint angles during each political experiment. In these two tasks, the observable joints are the same as the controllable joints. In the quadruple chain, this means that the agent can perceive the positions of each leg relative to the body, but does not have access to the absolute position and rotation of the torso. Our task is to take the measured trajectories and transform the values into something that gives us improvements in our gradient estimator. Sensor encodings should be independent of the political parameters, so we try to find the difference between the observed movement of the system and the expected movement in each step of time. The idea of using sensory data to cancel out the effect of one's motion is also present in biology. [14] We approach this difference by learning a rough approximation of the dynamic system in a pre-processing phase."}, {"heading": "3.2 LEARNING PERFORMANCE", "text": "Figure 5 shows the learning curves for the dart throwing problem. We get a significant improvement in learning performance when we use an algorithm that incorporates sensor data, compared to an algorithm that ignores sensor data. At each step in mountaineering, we have drawn a single sample from 12 different guidelines and we have an average of over 48 mountain runs. Guidelines for each step in mountaineering have been extracted from a Gaussian distribution for exploratory purposes. Figure 6 shows the learning curves for the four-legged locomotion problem. We get an improvement in learning performance when we use an algorithm that incorporates sensor data, compared to an algorithm that ignores sensor data. At each step in mountaineering, we have drawn a single sample from 30 different guidelines and we have an average of 48 mountain runs. Guidelines for each step in mountaineering have been extracted from a Gaussian distribution for exploratory purposes."}, {"heading": "4 DISCUSSION", "text": "We have shown that the degree of improvement depends on the amount of information that the sensors provide about noise-induced interference in the score. We have also shown that performance gains depend on the dimensionality of sensor data. It is important to choose sensor encodings that are independent of political parameters to minimize distortion. We have also presented a technique to find good sensor encodings for problems where these assumptions (low dimensionality and statistical uncertainty) do not apply. Finally, we presented learning curves showing improvements in learning performance for a toy cannon problem, a throwing task, and a quadruple motion task. In this work, the distribution of each random variable was approximated by using a linear Gaussian relationship. Improvements in performance can be realized if we use non-linear mappings. Other improvements could arise from incorporating our problem into the actual distance that we often do not know from the physical results."}], "references": [{"title": "Infinite-horizon policygradient estimation", "author": ["J. Baxter", "P.L. Bartlett"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "A kinematic model of the upper limb based on the visible human project (vhp) image dataset", "author": ["B. Garner", "M. Pandy"], "venue": "Computer Methods in Biomechanics and Biomedical Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Variance reduction techniques for gradient estimates in reinforcement learning", "author": ["Evan Greensmith", "Peter L. Bartlett", "Jonathan Baxter"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "Signaldependent noise determines motor", "author": ["Christopher Harris", "Daniel Wolpert"], "venue": "planning. Nature,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Machine learning for fast quadrupedal locomotion", "author": ["N. Kohl", "P. Stone"], "venue": "In Proceedings of the Nineteenth National Conference on Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Efficient gradient estimation for motor control learning", "author": ["Gregory Lawrence", "Noah Cowan", "Stuart Russell"], "venue": "In Proceedings of the Nineteenth International Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Pegasus: A policy search method for large MDPs and POMDPs", "author": ["Andrew Y. Ng", "Michael Jordan"], "venue": "In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Autonomous helicopter flight via reinforcement learning", "author": ["Andrew Y. Ng", "H. Jin Kim", "Michael Jordan", "Shankar Sastry"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "Learning from scarce experience", "author": ["Leon Peshkin", "Christian R. Shelton"], "venue": "In Proceedings of the 19th International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Natural actor-critic", "author": ["Jan Peters", "Sethu Vijayakumar", "Stefan Schaal"], "venue": "In Proceedings of the 16th European Conference on Machine Learnin,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Policy improvement for POMDPs using normalized importance sampling", "author": ["Christian R. Shelton"], "venue": "In Proceedings of the Seventeenth International Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Optimal control as a theory of motor coordination", "author": ["Emanuel Todorov", "Michael I. Jordan"], "venue": "Nature Neuroscience,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "The optimal reward baseline for gradient-based reinforcement learning", "author": ["Lex Weaver", "Nigel Tao"], "venue": "In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Perspectives and problems in motor learning", "author": ["D. Wolpert", "Z. Ghahramani", "J. Flanagan"], "venue": "Trends in Cognitive Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2001}], "referenceMentions": [{"referenceID": 7, "context": "Successful applications include helicopter flight [8], quadruped locomotion [5], and baseball hitting [10].", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "Successful applications include helicopter flight [8], quadruped locomotion [5], and baseball hitting [10].", "startOffset": 76, "endOffset": 79}, {"referenceID": 9, "context": "Successful applications include helicopter flight [8], quadruped locomotion [5], and baseball hitting [10].", "startOffset": 102, "endOffset": 106}, {"referenceID": 0, "context": "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials\u2014mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].", "startOffset": 214, "endOffset": 241}, {"referenceID": 2, "context": "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials\u2014mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].", "startOffset": 214, "endOffset": 241}, {"referenceID": 5, "context": "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials\u2014mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].", "startOffset": 214, "endOffset": 241}, {"referenceID": 6, "context": "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials\u2014mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].", "startOffset": 214, "endOffset": 241}, {"referenceID": 9, "context": "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials\u2014mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].", "startOffset": 214, "endOffset": 241}, {"referenceID": 8, "context": "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials\u2014mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].", "startOffset": 214, "endOffset": 241}, {"referenceID": 10, "context": "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials\u2014mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].", "startOffset": 214, "endOffset": 241}, {"referenceID": 12, "context": "Because trials can be expensive, especially in physical environments, a number of authors have presented techniques to reduce the number of required trials\u2014mainly by reducing the variance of the gradient estimator [1, 3, 6, 7, 10, 9, 11, 13].", "startOffset": 214, "endOffset": 241}, {"referenceID": 5, "context": "One might also interpret the approach of [6] as using observed perturbations to improve gradient estimation\u2014but only for the restricted case of perfect sensing of noise applied directly to the policy parameters.", "startOffset": 41, "endOffset": 44}, {"referenceID": 5, "context": "Following [6], we assume the policy itself is perturbed by noise to give the actual controls u = (uv, u\u03b8) .", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "Figure 3 shows different learning curves for the cannon problem as we increase the level of actuator noise (\u03a3u = [ 1 0 0 4 ] ).", "startOffset": 113, "endOffset": 124}, {"referenceID": 3, "context": "Figure 3 shows different learning curves for the cannon problem as we increase the level of actuator noise (\u03a3u = [ 1 0 0 4 ] ).", "startOffset": 113, "endOffset": 124}, {"referenceID": 5, "context": "In this section we describe how an agent can use its sensor data to improve its learning performance on a dart throwing task [6] and a quadruped locomotion task.", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "The arm is modeled as a three-link rigid body with dimensions based on biological measurements [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "Multiplicative noise has been shown to explain some aspects of biological motion [4, 12].", "startOffset": 81, "endOffset": 88}, {"referenceID": 11, "context": "Multiplicative noise has been shown to explain some aspects of biological motion [4, 12].", "startOffset": 81, "endOffset": 88}, {"referenceID": 13, "context": "The idea of using sensory data to cancel out the effect of one\u2019s own motion is also present in the biology literature [14].", "startOffset": 118, "endOffset": 122}], "year": 2008, "abstractText": "An efficient policy search algorithm should estimate the local gradient of the objective function, with respect to the policy parameters, from as few trials as possible. Whereas most policy search methods estimate this gradient by observing the rewards obtained during policy trials, we show, both theoretically and empirically, that taking into account the sensor data as well gives better gradient estimates and hence faster learning. The reason is that rewards obtained during policy execution vary from trial to trial due to noise in the environment; sensor data, which correlates with the noise, can be used to partially correct for this variation, resulting in an estimator with lower variance.", "creator": "TeX"}}}