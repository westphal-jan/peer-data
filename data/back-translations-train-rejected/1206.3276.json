{"id": "1206.3276", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Explanation Trees for Causal Bayesian Networks", "abstract": "Bayesian networks can be used to extract explanations about the observed state of a subset of variables. In this paper, we explicate the desiderata of an explanation and confront them with the concept of explanation proposed by existing methods. The necessity of taking into account causal approaches when a causal graph is available is discussed. We then introduce causal explanation trees, based on the construction of explanation trees using the measure of causal information ow (Ay and Polani, 2006). This approach is compared to several other methods on known networks.", "histories": [["v1", "Wed, 13 Jun 2012 15:41:30 GMT  (1079kb)", "http://arxiv.org/abs/1206.3276v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ulf nielsen", "jean-philippe pellet", "r\\'e elisseeff"], "accepted": false, "id": "1206.3276"}, "pdf": {"name": "1206.3276.pdf", "metadata": {"source": "CRF", "title": "Explanation Trees for Causal Bayesian Networks", "authors": ["Ulf H. Nielsen", "Jean-Philippe Pellet", "Andr\u00e9 Elissee"], "emails": ["uln@zurich.ibm.com", "jep@zurich.ibm.com", "ael@zurich.ibm.com"], "sections": [{"heading": null, "text": "Bayean networks can be used to extract explanations of the observed state of a subset of variables. In this paper, we explain the desiderata of an explanation and confront it with the concept of explanation proposed by existing methods. We discuss the need to consider causal approaches when a causal graph is available. We then present causal explanation trees based on the construction of explanation trees using the measure of causal information (Ay and Polani, 2006). This approach is compared with several other methods on known networks."}, {"heading": "1 INTRODUCTION", "text": "A Bayesian network (BN, Pearl, 1988) is an algebraic tool for compactly representing the common probability distribution of a set of variables V by exploiting conditional independence between variables. It represents all variables in a directed acyclic graph (DAG), in which the absence of arcs between the nodes means (conditional) independence. In addition to graphing the structure of dependencies between variables, BNs allow a more precise solution of inference tasks. In this paper we discuss the extraction of explanations in causal BNs (Pearl, 2000; Spirtes et al., 2001) BNs, in which the arcs represent direct correlations between variables. Generally, explanations in BNs can be classified into three categories (Lacave and Diez, 2002), depending on the focus of the explanation: \u2022 Explanation of evidence as meaningful correlations between variables."}, {"heading": "NOTATION", "text": "Bold uppercase letters denote sets of random variables or nodes in a graph, depending on the context. V is the set of all variables in the analysis. Italic uppercase letters such as X, Xi, Y are random variables or nodes and elements of V; calligraphic uppercase letters such as X, Y are their respective domains. Vectors are lowercase, such as e or p; italic scalars. Unless otherwise specified, scalars x, y are assumed to be the value of their respective uppercase variables. The probability distribution of a random variable X is denoted by p (X), and we write p (X = x) or p (x) the probability of x. We only work with discrete variables."}, {"heading": "2 AN IDEALIZED EXPLANATION", "text": "In fact, the fact is that most of them will be able to feel as if they are able to move."}, {"heading": "3 EXISTING METHODS", "text": "In this section some of the most important techniques for nd explanations are discussed and discussed."}, {"heading": "3.1 MOST PROBABLE EXPLANATION & VARIANTS", "text": "A common level of explanatory force is the conditional probability of the explanatory variable H, which select the explanatory pattern. The most likely explanation (MPE) is the approach (Pearl, 1988), which is then able to complete the set of observations O, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o"}, {"heading": "3.2 SE ANALYSIS", "text": "In the SE analysis, Jensen (2001) additionally considers the sensitivity of an explanation h in relation to the explanatory pattern. Less sensitive explanations ensure that small changes in the parameters of the network do not lead to difficult-to-understand explanations, so that the explanation is stable in relation to the specification of the network. 1The difference between observation and intervention is fundamental to causality and can best be described by the example of Simpson's paradox in Pearl (2000), Chapter 6.SE Analysis, by comparing two explanations hi and hj (hj), usually with Bayes \"factor or probability ratio (Je reys, 1961): Bayes\" factor = posterior ratioprior ratio = p (hi | e) / p (hj) p (hj) = p (e | hi) p (e | hj) p (e | hj) p (e | hj) p (e | hj) p (e | hj) p (e) p (e) p (e) p (e) p (e) p (e) p (e) p (e)."}, {"heading": "3.3 EXPLANATION TREES", "text": "The Flores method (2005) constructs a series of best explanations, while at the same time giving a preference for precise explanations that summarize the results of the analysis in an explanation tree. We describe this method in more detail, because the causal information tree method (described in Section 4) is based on a similar representation. (De nition 1) An explanation tree for an explanation is then a tree in which each node X is an explanatory variable (with X-V / E), and each branch of X is a spectral instantiation x-X of X. A path from the root to a leaf is then a series of assignments X = x, Y = y, Z = z, summarized as P = p, which represents a complete explanation.Flores (2005) algorithm, summarized in Algorithm 1, builds such an explanation tree."}, {"heading": "4 CAUSAL EXPLANATION TREES", "text": "Like the previous method, causal explanation trees use the structure of the causal diagram to evaluate the intervention. The tree has grown to ensure that explanations are causal in each pathway: variables can only be selected as explanatory patterns if they are causally incorporated into the explanation. Before applying the causal criterion used in this approach, we need to interpret the concept of post-interventional distribution (Pearl, 2000, p. 72) to indicate the probability (or probability density) of e when X = x is observed. However, it does not represent the probability of the causal distribution of e when we are causally forced to have value x. Causally, we are interested in the intervention on X, which we call by do (X = x), rather the observation of x. In causal BNs, the tool used to evaluate these conditions, is Pearl's (1995) docalculus, which uses the structure of the triggering diagram to evaluate the intervention."}, {"heading": "5 EXPERIMENTS", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "6 CONCLUSION", "text": "We have presented an explanatory approach in causal BNs, causal explanation trees. Explanations are presented as a tree that presents several explanations compactly and makes them more readable than a (possibly long) list. Provided that the BN is causal, it allows us to use the criterion of causal information to build the tree. This leads to more meaningful explanations by explaining a given state only with variables that can explain it causally. The approach explicitly distinguishes between observation and explanation, allowing the user to enter all available knowledge about the network as an observation, while simultaneously focusing on explaining one of them and selecting the observed variables as part of a good explanation. The algorithm labels the sheets to check how a proposed explanation alters the probability of the explanation, making the tree easy to interpret. Causal explanation trees, unlike other techniques, do not place a condition on the explanation tree to explain the probability of the explanatory maxip."}, {"heading": "Ay, N. & Polani, D. (2006). Information ows in causal", "text": "Technical Report, Max Planck Institute for Mathematics in the Sciences. Chajewska, U. & Halpern, J. Y. (1997) Declaration in probabilistic systems. In: UAI-97, p. 62 71. Morgan Kaufmann."}, {"heading": "Chan, H. & Darwiche, A. (2006). On the robustness of", "text": "In: UAI-06.de Campos, L. M., G\u00e9mez, J. A., & Moral, S. (2001): Simplifying explanations in Bayesian faith networks. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 9.Flores, M. J. (2005): Bayesian networks Inference: Advanced algorithms for triangulation and partial abduction. Dissertation, Universidad De Castilla-La Mancha.Frey, B. J., Kschischang, F. R., & Loeliger, H.-A. (2001): Factor diagrams and the sum-product algorithm. IEEE Transactions on Information Theory, 47."}, {"heading": "Halpern, J. Y. & Pearl, J. (2005). Causes and explanations:", "text": "A structural-model approach. Part II: Explanations. The British Journal for the Philosophy of Science.Henrion, M. & Druzdzel, M. J. (1990). Qualitative Propagation and Scenario-based Scheme for Exploiting probabilistic reasoning. In: UAI, p. 17 32.Je reys, H. (1961). Theory of Probability. Oxford University Press.Jensen, F. V. (2001). Bayesian Networks and Decision Graphs. Springer.Lacave, C. & Diez, F. J. (2002). A review of explain methods of Bayesian networks. Knowledge Engineering Review, 17 (2): 107 127.Lauritzen, S. L. & Spiegelhalter, D. J. (1988). Local computations with probabilities on graphical structures and their application to expert systems. Journal of the Royal Statistical Society, Series B, 50: 157 224.Nielsen, U. H. (2007)."}, {"heading": "Yuan, C. & Lu, T.-C. (2007). Finding explanations in", "text": "In: The 18th International Workshop on Principles of Diagnosis."}], "references": [{"title": "Information ows in causal networks. Technical report, Max Planck Institute for Mathematics in the Sciences", "author": ["N. Ay", "D. Polani"], "venue": null, "citeRegEx": "Ay and Polani,? \\Q2006\\E", "shortCiteRegEx": "Ay and Polani", "year": 2006}, {"title": "De ning explanation in probabilistic systems", "author": ["U. Chajewska", "J.Y. Halpern"], "venue": "In: UAI-97,", "citeRegEx": "Chajewska and Halpern,? \\Q1997\\E", "shortCiteRegEx": "Chajewska and Halpern", "year": 1997}, {"title": "On the robustness of most probable explanations", "author": ["H. Chan", "A. Darwiche"], "venue": null, "citeRegEx": "Chan and Darwiche,? \\Q2006\\E", "shortCiteRegEx": "Chan and Darwiche", "year": 2006}, {"title": "Simplifying explanations in Bayesian belief networks", "author": ["L.M. de Campos", "J.A. G\u00e9mez", "S. Moral"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems", "citeRegEx": "Campos et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Campos et al\\.", "year": 2001}, {"title": "Bayesian networks Inference: Advanced algorithms for triangulation and partial abduction", "author": ["M.J. Flores"], "venue": "PhD thesis,", "citeRegEx": "Flores,? \\Q2005\\E", "shortCiteRegEx": "Flores", "year": 2005}, {"title": "Factor graphs and the sum-product algorithm", "author": ["B.J. Frey", "F.R. Kschischang", "Loeliger", "H.-A"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Frey et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Frey et al\\.", "year": 2001}, {"title": "Causes and explanations: A structural-model approach. Part II: Explanations", "author": ["J.Y. Halpern", "J. Pearl"], "venue": "The British Journal for the Philosophy of Science", "citeRegEx": "Halpern and Pearl,? \\Q2005\\E", "shortCiteRegEx": "Halpern and Pearl", "year": 2005}, {"title": "Qualitative propagation and scenario-based scheme for exploiting probabilistic reasoning", "author": ["M. Henrion", "M.J. Druzdzel"], "venue": "UAI, pages", "citeRegEx": "Henrion and Druzdzel,? \\Q1990\\E", "shortCiteRegEx": "Henrion and Druzdzel", "year": 1990}, {"title": "Theory of Probability", "author": ["H. Je reys"], "venue": null, "citeRegEx": "reys,? \\Q1961\\E", "shortCiteRegEx": "reys", "year": 1961}, {"title": "Bayesian Networks and Decision Graphs", "author": ["F.V. Jensen"], "venue": null, "citeRegEx": "Jensen,? \\Q2001\\E", "shortCiteRegEx": "Jensen", "year": 2001}, {"title": "A review of explanation methods of Bayesian networks", "author": ["C. Lacave", "F.J. Diez"], "venue": "Knowledge Engineering Review,", "citeRegEx": "Lacave and Diez,? \\Q2002\\E", "shortCiteRegEx": "Lacave and Diez", "year": 2002}, {"title": "Local computations with probabilities on graphical structures and their application to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Lauritzen and Spiegelhalter,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "On causal explanations in Bayesian networks. Master's thesis, IT University of Copenhagen", "author": ["U.H. Nielsen"], "venue": null, "citeRegEx": "Nielsen,? \\Q2007\\E", "shortCiteRegEx": "Nielsen", "year": 2007}, {"title": "Map complexity results and approximation methods", "author": ["J.D. Park"], "venue": "In: UAI-02,", "citeRegEx": "Park,? \\Q2002\\E", "shortCiteRegEx": "Park", "year": 2002}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Causal diagrams for empirical research", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1995\\E", "shortCiteRegEx": "Pearl", "year": 1995}, {"title": "Causality: Models, Reasoning, and Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q2000\\E", "shortCiteRegEx": "Pearl", "year": 2000}, {"title": "Explanation, irrelevance, and statistical independence", "author": ["S.E. Shimony"], "venue": "In: AAAI,", "citeRegEx": "Shimony,? \\Q1991\\E", "shortCiteRegEx": "Shimony", "year": 1991}, {"title": "Causation, Prediction, and Search, Second Edition", "author": ["P. Spirtes", "C. Glymour", "R. Scheines"], "venue": null, "citeRegEx": "Spirtes et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Spirtes et al\\.", "year": 2001}, {"title": "Finding explanations in bayesian networks", "author": ["C. Yuan", "Lu", "T.-C"], "venue": "The 18th International Workshop on Principles of Diagnosis", "citeRegEx": "Yuan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yuan et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "We then introduce causal explanation trees, based on the construction of explanation trees using the measure of causal information ow (Ay and Polani, 2006).", "startOffset": 134, "endOffset": 155}, {"referenceID": 16, "context": "In this paper, we discuss the extraction of explanations in causal BNs (Pearl, 2000; Spirtes et al., 2001) BNs where the arcs depict direct cause e ect relationships between variables.", "startOffset": 71, "endOffset": 106}, {"referenceID": 18, "context": "In this paper, we discuss the extraction of explanations in causal BNs (Pearl, 2000; Spirtes et al., 2001) BNs where the arcs depict direct cause e ect relationships between variables.", "startOffset": 71, "endOffset": 106}, {"referenceID": 10, "context": "Generally, explanations in BNs can be classi ed in three categories (Lacave and Diez, 2002) depending on the focus of the explanation: \u2022 Explanation of evidence.", "startOffset": 68, "endOffset": 91}, {"referenceID": 1, "context": "We insist on the distinction between the explanandum e and the observations o (Chajewska and Halpern, 1997).", "startOffset": 78, "endOffset": 107}, {"referenceID": 6, "context": "As Halpern and Pearl (2005) discuss, explanations need to be causal to be consistent with users' knowledge of the mechanisms of the system.", "startOffset": 3, "endOffset": 28}, {"referenceID": 16, "context": "In addition to assuming that the relationships between the variables V can be represented by a fully oriented causal BN, we assume that the corresponding joint probability distribution is faithful and causally su cient (Pearl, 2000; Spirtes et al., 2001).", "startOffset": 219, "endOffset": 254}, {"referenceID": 18, "context": "In addition to assuming that the relationships between the variables V can be represented by a fully oriented causal BN, we assume that the corresponding joint probability distribution is faithful and causally su cient (Pearl, 2000; Spirtes et al., 2001).", "startOffset": 219, "endOffset": 254}, {"referenceID": 14, "context": "The most probable explanation (MPE) approach (Pearl, 1988) then considers h\u2217 = arg maxh p(h | e) as the best explanation (or, alternatively, looks for the k best explanations by maximizing this probability).", "startOffset": 45, "endOffset": 58}, {"referenceID": 17, "context": "In the partial abduction approach (Shimony, 1991), the set of explanatory variables is a strict subset H ( V \\ E.", "startOffset": 34, "endOffset": 49}, {"referenceID": 17, "context": "Automatically selecting the relevant explanatory variable is a nontrivial issue (Shimony, 1991).", "startOffset": 80, "endOffset": 95}, {"referenceID": 3, "context": "Further e orts to make explanations more concise include de Campos et al. (2001), where the k most probable explanations are found and then simpli ed based on relevance and probabilistic criteria; and Henrion and Druzdzel (1990), where also partial assignments are allowed but only within a prede ned tree that limits the set of possible explanations.", "startOffset": 60, "endOffset": 81}, {"referenceID": 3, "context": "Further e orts to make explanations more concise include de Campos et al. (2001), where the k most probable explanations are found and then simpli ed based on relevance and probabilistic criteria; and Henrion and Druzdzel (1990), where also partial assignments are allowed but only within a prede ned tree that limits the set of possible explanations.", "startOffset": 60, "endOffset": 229}, {"referenceID": 1, "context": "There are several concerns with these approaches MPE/MAP or scenario-based maximizing some conditional probability of the explanatory variables (Chajewska and Halpern, 1997).", "startOffset": 144, "endOffset": 173}, {"referenceID": 2, "context": "MPEs and, to a lesser extent, MAP model explanations, are not robust: little changes in the network will often change the result of the analysis, even though the changes occur in parts of the network largely independent of the explanandum (Chan and Darwiche, 2006).", "startOffset": 239, "endOffset": 264}, {"referenceID": 9, "context": "In SE analysis, Jensen (2001) additionally considers the sensitivity of an explanation h with respect to the explanandum.", "startOffset": 16, "endOffset": 30}, {"referenceID": 13, "context": "The di erence between observation and intervention is fundamental to causality and is best described with the example of Simpson's paradox in Pearl (2000), chap.", "startOffset": 142, "endOffset": 155}, {"referenceID": 8, "context": "The empirical interpretation of Bayes' factor given by Je reys (1961) is that if it is less than 1 it is in favor of hj , if less than 3 it is a slight support for hi.", "startOffset": 58, "endOffset": 70}, {"referenceID": 16, "context": "An exhaustive search is performed over all subsets of the hypothesis, and the explanations are shown to be more concise in a sample network than MPE, Shimony's (1991) MAP, and the simpli cations described by de Campos et al.", "startOffset": 150, "endOffset": 167}, {"referenceID": 3, "context": "An exhaustive search is performed over all subsets of the hypothesis, and the explanations are shown to be more concise in a sample network than MPE, Shimony's (1991) MAP, and the simpli cations described by de Campos et al. (2001).", "startOffset": 211, "endOffset": 232}, {"referenceID": 4, "context": "The method of Flores (2005) constructs a set of best explanations while at the same time giving a preference for concise explanations, summarizing the results of the analysis in an explanation tree.", "startOffset": 14, "endOffset": 28}, {"referenceID": 4, "context": "Algorithm 1 Flores's (2005) Explanation Tree 1: function T = ExplanationTree(H, e,p; \u03b1, \u03b2) Input: H : set of explanatory variables E = e : explanandum P = p : path of variable assignments \u03b1, \u03b2 : stopping criteria Output: T : an explanation tree 2: X\u2217 \u2190 arg maxX\u2208H P Y \u2208H Inf(X;Y | e,p) 3: if max Y \u2208H\\X\u2217 Inf(X;Y | e,p) < \u03b1 or p(p | e) < \u03b2 then 4: return \u2205 5: end if 6: T \u2190 new tree with root X\u2217 7: for each x \u2208 domain(X\u2217) do 8: T \u2032 \u2190 ExplanationTree(H \\X\u2217, e,p \u222a {x}) 9: add a branch x to T with subtree T \u2032 and 10: assign it the label p(p, x | e) 11: end for 12: return T", "startOffset": 12, "endOffset": 28}, {"referenceID": 4, "context": "Flores (2005) also argues that explanations as constructed by Algorithm 1 are reasonable and more sensible than (k-)MPE in the sense that on simple networks, the returned explanations are those that we expect.", "startOffset": 0, "endOffset": 14}, {"referenceID": 4, "context": "First, on line 2 of Algorithm 1, variables are added to the tree in order of how much information they provide See Flores (2005) for additional cases where max at line 3 is replaced by min or avg, and Inf is the Gini index.", "startOffset": 115, "endOffset": 129}, {"referenceID": 14, "context": "the e ect of these conditionings is Pearl's (1995) docalculus, which uses the structure of the causal graph to evaluate the postintervention distribution.", "startOffset": 36, "endOffset": 51}, {"referenceID": 0, "context": "With this concept, we can now de ne the causal information ow (Ay and Polani, 2006), which will be our measure of causal contribution of explanatory variables towards our explanandum.", "startOffset": 62, "endOffset": 83}, {"referenceID": 5, "context": "The inference steps were implemented using the factor graph message-passing algorithm (Frey et al., 2001).", "startOffset": 86, "endOffset": 105}, {"referenceID": 4, "context": "For comparison, Flores's (2005) approach is O(nd).", "startOffset": 16, "endOffset": 32}, {"referenceID": 12, "context": "A more extended version of these experiments and comments can be found in Nielsen (2007).", "startOffset": 74, "endOffset": 89}, {"referenceID": 11, "context": "This network (Lauritzen and Spiegelhalter, 1988) models the relationships between two indicators, X-ray results and dyspnea, of severe diseases for a person.", "startOffset": 13, "endOffset": 48}], "year": 2008, "abstractText": "Bayesian networks can be used to extract explanations about the observed state of a subset of variables. In this paper, we explicate the desiderata of an explanation and confront them with the concept of explanation proposed by existing methods. The necessity of taking into account causal approaches when a causal graph is available is discussed. We then introduce causal explanation trees, based on the construction of explanation trees using the measure of causal information ow (Ay and Polani, 2006). This approach is compared to several other methods on known networks.", "creator": "LaTeX with hyperref package"}}}