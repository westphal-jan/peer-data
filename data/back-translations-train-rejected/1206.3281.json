{"id": "1206.3281", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Model-Based Bayesian Reinforcement Learning in Large Structured Domains", "abstract": "Model-based Bayesian reinforcement learning has generated significant interest in the AI community as it provides an elegant solution to the optimal exploration-exploitation tradeoff in classical reinforcement learning. Unfortunately, the applicability of this type of approach has been limited to small domains due to the high complexity of reasoning about the joint posterior over model parameters. In this paper, we consider the use of factored representations combined with online planning techniques, to improve scalability of these methods. The main contribution of this paper is a Bayesian framework for learning the structure and parameters of a dynamical system, while also simultaneously planning a (near-)optimal sequence of actions.", "histories": [["v1", "Wed, 13 Jun 2012 15:43:32 GMT  (332kb)", "http://arxiv.org/abs/1206.3281v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["stephane ross", "joelle pineau"], "accepted": false, "id": "1206.3281"}, "pdf": {"name": "1206.3281.pdf", "metadata": {"source": "CRF", "title": "Model-Based Bayesian Reinforcement Learning in Large Structured Domains", "authors": ["St\u00e9phane Ross", "Joelle Pineau"], "emails": [], "sections": [{"heading": null, "text": "Model-based Bayesian reinforcement learning has aroused great interest in the AI community as it provides an elegant solution for the optimal exploration and exploitation compromise in classical reinforcement learning. Unfortunately, the applicability of this approach is limited to small areas due to the high complexity of the common posterior level reasoning versus model parameters. In this paper, we consider the use of factored representations combined with online planning techniques to enhance the scalability of these methods. The most important contribution of this paper is a Bayesian framework for learning the structure and parameters of a dynamic system while simultaneously planning a (near) optimal sequence of actions."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Background", "text": "A Markov Decision Process (MDP) is a general framework for decision-making in stochastic systems (Bellman, 1957). It is often used to represent affirmation problems (Sutton & Barto, 1998).We consider an unknown system represented by an MDP model in factored form (S, A, T, R), where: \u2022 S: S1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Sn, which is the (discrete) set of states of the system; S1,.., Sn corresponds to the domain of the n state variables (features). \u2022 A, the (discrete) set of actions that can be executed by the agent. \u2022 T: S \u00b7 A \u00b7 Value \u2192 [0, 1] the transition function in which T (s, s \u2032) = Gr (s \u2032 s \u2032 s \u2032) represents the probability that an optimal state exists between these variables (features)."}, {"heading": "2.1 Bayesian Reinforcement Learning", "text": "While the MDP framework makes it possible to calculate the optimal policy for each stochastic system, it requires complete knowledge of transition dynamics, which is a strong assumption in practice. Bayes's model-based RL weakens this assumption by instead maintaining a probability distribution of the possible settings of each unknown parameter (Dearden et al., 1999). It assumes an initial prior distribution of these parameters and uses Bayes's rule to update the posterior distribution whenever transitions are observed in the course of interactions between the agent and the environment. Given that transition parameters are usually modeled using multinomial distributions, a natural choice to specify this posterior distribution is the dirichlet distribution. The dirichlet is specified by \"count\" parameters in the course of interactions between the agent and the environment."}, {"heading": "2.2 Learning Bayes Nets", "text": "Bayesian networks (BNs) have been extensive to build compact predictive models of multivariate data (q = q =). A BN models the joint distribution of multivariate data compactly by Exploiting conditional Independence Relations between Xi. It is defined by a set of variables X, a directed acyclic graph (DAG) structure G over variables in X, and parameters \u03b8G, when the structure G is known), or simultaneously learning the structure G and parameters G. For our purposes, we are largely interested in Bayesian approaches that learn both structure and parameters (Heckerman et al., 1995; Friedman & Koller, 2003; Eaton & Murphy, 2007)."}, {"heading": "3 Bayesian RL in Factored MDPs", "text": "We consider the problem of optimal action in a system presented as factored MDP, in which both the structure and the parameters of the DBNs that define the transition function T are unknown. We assume that the state has S1,..., Sn, the action set A and the reward function R. Our work extends trivially to the case in which R is unknown, but we omit this for the sake of simplicity of presentation."}, {"heading": "3.1 Factored Bayesian RL model", "text": "We consider the transition function T to be a hidden variable of the system, which is partially observed when there are transitions between the individual states. In this perspective, the decision problem can be considered as partially observable MDP (POMDP) \"(Kaelbling, Littman, & Cassandra, 1998).\" The state of this POMDP covers both the actual system state as well as the DBNs, which define T for each action. \"Formally, this POMDP is defined by the tuple (S \u2032, A \u2032, Z \u2032, DP, T \u2032, O \u2032, R \u2032):\" S \"(S)\" S \"S\" \"S\" \"S\" \"S\" \"S\" \"S\" S \"\" S \"\" \"S\" \"\" S \"\" \"for each action.\" Formally, \"POMDP\" is defined by the tuple (S \u2032, A \u2032, Z \u2032, \"DP, T \u2032,\" O \u2032, \"R \u2032):\" S \"(S)\" S \"\" S \"\" \"S\" \"S\" \"S\" S \"\" S \"S\" \"S\" S \"S\" S \"S\" S \"for each action.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"for\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"for\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"for each action.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" for \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" (one per action). \"S\" S \"S\" S \"S\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"(one per action).\" S. \"S.\" S. \"S.\" S. (S. S. \"S.\" S. \"GDP. S. S. S. S.\" S. S. \"S.\" S. S. \"(S. S. S. S. S. S.\")."}, {"heading": "3.2 Online Monte Carlo Planning Algorithm", "text": "To solve the planning problem outlined above, we need efficient alignment methods, and in particular, we turn to online sampling techniques to overcome the curse of dimensionality. (D) First, as mentioned above, we maintain the following graphs for each action. (D) Then we also have a product of the Dirichlet priors at the level of the previous P (Ga). Whenever a transition (s) occurs, the probability of graph G (1K) is updated. (D) For each sampled graph, we also have a product of the Dirichlet priors at the level of the previous P (s). Whenever a transition (s) occurs, the probability of graph G (1K) is updated."}, {"heading": "3.3 Resampling DBNs", "text": "The current approach is not particularly effective if the initial set of captured DBN structures is bad, as we merely update the weights and therefore do not change the structure. This can be solved by re-capturing new DBNs from the current rear P (G | h) to obtain more likely structures after observing the history. We implement this using an MCMC algorithm as described in Section 2.2. Generally, it may not be appropriate to re-capitalize charts too often. A useful criterion for deciding when to use new charts is the general probability of La of our current DBNs for a particular action a. This can be calculated directly from the normalization constant \u03b7 (Eq.8). Assuming that we can simply update t = 0, La = 1, L \u2032 a = La at each step."}, {"heading": "4 Experiments", "text": "To validate our approach, we are experimenting with instances of network administration in a running state (Guestrin et al., 2003). We are assuming that a network consists of n computers connected by some topology. Each computer is in either running or failed mode. A running computer has a certain probability of migrating to an error, regardless of its neighbors in the network; this probability is increased for each neighbor in error mode. The goal of the operator is to maximize the number of running computers while minimizing the reboot actions. The initial state assumes that all computers are running. In our experiments, we are assuming a probability that a running computer will go into failed mode and a probability that a failed computer will fail in any of its neighbors."}, {"heading": "4.1 Linear Network", "text": "In the linear network experiment, random K = 10 diagrams and resampling is performed whenever lnLa < \u2212 100. Online planning is performed with depth D = 2 and branching factor N = 5 for each action. As we use the immediate reward at the edge of the search tree, this corresponds to an approximate planning over a horizon of 3 steps. The same parameters are also used for planning with the known structure and across the entire common probability table. Results are shown in Figures 3-5. These figures show that our approach (referred to as Structure Learning) achieves similar yields as if the structure is known in advance (referred to as Known Structure). Both cases achieve an optimal yield (referred to as Known MDP1) very quickly, within 200 steps. Our approach is also able to learn transition dynamics as quickly as if the structure is known a priori."}, {"heading": "4.2 Ternary Tree Network", "text": "In the ternary tree network experiment, we use random K = 8 diagrams and whenever lnLa < \u2212 150. For planning, we use a depth D = 2 for each action and sample N = 4 next states. The results are shown in Figures 6-8. The results are similar to those of the linear network experiment. Importantly, this is a much more difficult problem for the unstructured approach, which has not improved even after 1500 learning steps. In contrast to our approach, which achieves a similar performance as if the structure is a priori known, and achieves optimal performance after a few hundred learning steps, these results are obtained even though the priors we offer are very weak. Average planning time per action is 153ms for structure learning and 29ms for the two fixed structure approaches."}, {"heading": "4.3 Dense Network", "text": "In the dense network experiment, we randomly select K = 8 graphs and fall back on them whenever lnLa < \u2212 120. For planning, we start from D = 2 and N = 4. The results are shown in Figures 9-11. In this area, we find a surprising result: Our approach to structural learning is able to learn the dynamics of the system much faster than if the structure is known in advance (see Figure 10), although the learned structures are still far from being correct (see Figure 11 and 4.1). This is an area where there are many dependencies between state variables, so there are many parameters to be learned (whether the structure is known or not). In such a case, our structural learning approach is advantageous because it can foster simpler structures that resemble the dynamics relatively well from very few state variables early on in learning (e.g. < 250)."}, {"heading": "5 Conclusion", "text": "This paper presents a novel Bayesian framework for learning both the structure and parameters of a Factored MDP, while optimizing the selection of measures to resolve a trade-off between model research and exploitation. It is important to note that both the use of factor representation and the use of online planning are key to facilitating our approach to scaling to large areas. By learning factor representation, we facilitate a strong generalization between states with similar characteristics, allowing for a more efficient use of data by learning the model. It is particularly interesting to note that our approach to structural learning is a useful method to accelerate RL even in areas with very weak structures."}, {"heading": "Acknowledgements", "text": "This research was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Que \u0301 be \u0301 cois de la Recherche sur la Nature et les Technologies (FQRNT) fund."}, {"heading": "Boutilier, C., Dearden, R., & Goldszmidt, M. (2000).", "text": "Stochastic dynamic programming with factored representations. Artif. Intel., 121 (1-2), 49-107."}, {"heading": "Dearden, R., Friedman, N., & Andre, D. (1999). Model", "text": "Bayesian structure learning using dynamic programming and MCMC. In UAI, pp. 150-159.Duff, M. (2002) Optimal Learning: Calculation Methods for Bayes-adaptive Markov Decision Processes. Doctoral Thesis, University of Massachusetts Amherst.Eaton, D., & Murphy, K. (2007) Bayesian structure learning using dynamic programming and MCMC. In UAI."}, {"heading": "Friedman, N., & Koller, D. (2003). Being Bayesian about", "text": "Bayesian Network Structure: A Bayesian Approach to Structure Finding in Bayesian Networks. Machine Learning, 50 (1-2), 95-125."}, {"heading": "Guestrin, C., Koller, D., Parr, R., & Venkataraman, S.", "text": "(2003) Efficient solution algorithms for factored MDPs. Journal of Artificial Intelligence Research, 19, 399-468."}, {"heading": "Heckerman, D., Geiger, D., & Chickering, D. M. (1995).", "text": "Learning bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20 (3), 197-243."}, {"heading": "Kaelbling, L. P., Littman, M. L., & Cassandra, A. R.", "text": "(1998). Planning and action in partially observable stochastic areas. Artificial Intelligence, 101 (1-2), 99-134."}, {"heading": "McAllester, D., & Singh, S. (1999). Approximate Planning", "text": "for Factored POMDPs by Faith State Simplification. In UAI, pp. 409-416."}, {"heading": "Poupart, P., Vlassis, N., Hoey, J., & Regan, K. (2006). An", "text": "In ICML, pp. 697-704. Sutton, R., & Barto, A. (1998): Reinforcement Learning: An Introduction. MIT Press."}], "references": [{"title": "A markovian decision process", "author": ["R. Bellman"], "venue": "Journal of Mathematics and Mechanics,", "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Stochastic dynamic programming with factored representations", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "Artif. Intel.,", "citeRegEx": "Boutilier et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 2000}, {"title": "Model based bayesian exploration", "author": ["R. Dearden", "N. Friedman", "D. Andre"], "venue": "In UAI,", "citeRegEx": "Dearden et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1999}, {"title": "Optimal learning: Computational procedures for Bayes-adaptive Markov decision processes", "author": ["M. Duff"], "venue": "Ph.D. thesis,", "citeRegEx": "Duff,? \\Q2002\\E", "shortCiteRegEx": "Duff", "year": 2002}, {"title": "Bayesian structure learning using dynamic programming and MCMC", "author": ["D. Eaton", "K. Murphy"], "venue": "In UAI", "citeRegEx": "Eaton and Murphy,? \\Q2007\\E", "shortCiteRegEx": "Eaton and Murphy", "year": 2007}, {"title": "Being Bayesian about Bayesian network structure: A Bayesian approach to structure discovery in Bayesian networks", "author": ["N. Friedman", "D. Koller"], "venue": "Machine Learning,", "citeRegEx": "Friedman and Koller,? \\Q2003\\E", "shortCiteRegEx": "Friedman and Koller", "year": 2003}, {"title": "Efficient solution algorithms for factored MDPs", "author": ["C. Guestrin", "D. Koller", "R. Parr", "S. Venkataraman"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Guestrin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Guestrin et al\\.", "year": 2003}, {"title": "Learning bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Machine Learning,", "citeRegEx": "Heckerman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1995}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Approximate Planning for Factored POMDPs using Belief State Simplification", "author": ["D. McAllester", "S. Singh"], "venue": "In UAI,", "citeRegEx": "McAllester and Singh,? \\Q1999\\E", "shortCiteRegEx": "McAllester and Singh", "year": 1999}, {"title": "An analytic solution to discrete Bayesian reinforcement learning", "author": ["P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan"], "venue": "In ICML,", "citeRegEx": "Poupart et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Poupart et al\\.", "year": 2006}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}], "referenceMentions": [{"referenceID": 3, "context": "Model-based Bayesian RL methods have successfully addressed these issues by maintaining a posterior distribution over unknown model parameters and acting such as to maximize long-term expected rewards with respect to this posterior (Dearden, Friedman, & Andre, 1999; Duff, 2002; Poupart, Vlassis, Hoey, & Regan, 2006).", "startOffset": 232, "endOffset": 317}, {"referenceID": 0, "context": "A Markov Decision Process (MDP) is a general framework for decision making in stochastic systems (Bellman, 1957).", "startOffset": 97, "endOffset": 112}, {"referenceID": 1, "context": "This can be represented efficiently by a DBN for each action, exploiting conditional independence relations that exist between state features (Boutilier et al., 2000).", "startOffset": 142, "endOffset": 166}, {"referenceID": 6, "context": "However, approximate algorithms exist to compute V \u2217 more efficiently by exploiting the factored representation (Guestrin et al., 2003).", "startOffset": 112, "endOffset": 135}, {"referenceID": 2, "context": "Model-based Bayesian RL weakens this assumption by instead maintaining a probability distribution over the possible settings of each unknown parameter (Dearden et al., 1999).", "startOffset": 151, "endOffset": 173}, {"referenceID": 3, "context": "The resulting decision problem is the following: given the agent is in state s with information state \u03c6, how should it behave such as to maximize its future expected rewards? This new decision problem can be modeled by an extended MDP model, called Bayes-Adaptive MDP (BAMDP), where the counts \u03c6 are included in the state space, and the transition function models how these parameters evolve given a particular state transition (Duff, 2002).", "startOffset": 428, "endOffset": 440}, {"referenceID": 7, "context": "For our purposes, we are mostly interested in Bayesian approaches that learn both the structure and parameters (Heckerman et al., 1995; Friedman & Koller, 2003; Eaton & Murphy, 2007).", "startOffset": 111, "endOffset": 182}, {"referenceID": 7, "context": "Under previous assumptions concerning the prior P (\u03b8G|G), P (D|G) can be computed in closed form and corresponds to the likelihood-equivalence Bayesian Dirichlet score metric (BDe) (Heckerman et al., 1995).", "startOffset": 181, "endOffset": 205}, {"referenceID": 6, "context": "To validate our approach, we experiment with instances of the network administration domain (Guestrin et al., 2003).", "startOffset": 92, "endOffset": 115}], "year": 2008, "abstractText": "Model-based Bayesian reinforcement learning has generated significant interest in the AI community as it provides an elegant solution to the optimal exploration-exploitation tradeoff in classical reinforcement learning. Unfortunately, the applicability of this type of approach has been limited to small domains due to the high complexity of reasoning about the joint posterior over model parameters. In this paper, we consider the use of factored representations combined with online planning techniques, to improve scalability of these methods. The main contribution of this paper is a Bayesian framework for learning the structure and parameters of a dynamical system, while also simultaneously planning a (near-)optimal sequence of actions.", "creator": "dvips(k) 5.96dev Copyright 2007 Radical Eye Software"}}}