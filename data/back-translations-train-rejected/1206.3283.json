{"id": "1206.3283", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Observation Subset Selection as Local Compilation of Performance Profiles", "abstract": "Deciding what to sense is a crucial task, made harder by dependencies and by a nonadditive utility function. We develop approximation algorithms for selecting an optimal set of measurements, under a dependency structure modeled by a tree-shaped Bayesian network (BN). Our approach is a generalization of composing anytime algorithm represented by conditional performance profiles. This is done by relaxing the input monotonicity assumption, and extending the local compilation technique to more general classes of performance profiles (PPs). We apply the extended scheme to selecting a subset of measurements for choosing a maximum expectation variable in a binary valued BN, and for minimizing the worst variance in a Gaussian BN.", "histories": [["v1", "Wed, 13 Jun 2012 15:44:14 GMT  (281kb)", "http://arxiv.org/abs/1206.3283v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yan radovilsky", "solomon eyal shimony"], "accepted": false, "id": "1206.3283"}, "pdf": {"name": "1206.3283.pdf", "metadata": {"source": "CRF", "title": "Observation Subset Selection as Local Compilation of Performance Profiles", "authors": ["Yan Radovilsky"], "emails": ["yanr@cs.bgu.ac.il", "shimony@cs.bgu.ac.il"], "sections": [{"heading": null, "text": "We develop approximation algorithms to select an optimal scale, taking into account a dependency structure modelled by a tree-shaped Bayesian network (BN). Our approach is to generalize the creation of an algorithm available at all times, represented by conditional power profiles, by loosening the input monotonicity assumption and extending the local compilation technique to more general classes of power profiles (PPs). We apply the extended scheme to selecting a subset of measurements to select a maximum expectation variable in a binary BN and minimize the worst variance in a Gaussian BN."}, {"heading": "1 Introduction", "text": "A typical diagnostic system consists of two types of variables: tests (observable) and hypotheses (unobservable), with statistical dependencies between variables. Each test, when performed, consumes resources (time or money) and provides a measurement of one or more test variables. Upon receipt of the values of the selected tests, the distribution of the model is updated. An objective function specifies a reward that is given to the system for posterior distribution. In this paper, we develop approximation algorithms for some settings of the OSS problem, a hard problem in the general case. Observation Subset Selection (OSS) is a limited version of this problem, in which all measurements must be selected in advance. In this paper, we develop approximation algorithms for some settings of the OSS problem for tree-like dependency structures. To address this problem, we present OSS as a variant of the following known meta-argumentation problem."}, {"heading": "2 Background", "text": "There are several alternative terms used to refer to flexible calculation methods in the research literature: a statistical model known as the Performance Profile (PP) is used; the simplest version of such a PP refers to an expected Performance Profile (EPP), a mapping of consumed time to an expected quality, Q: T \u2192 Q. Given the usability and time-dependent usage function described by EPP QA U: T \u00b7 Q \u2022 R, optimal time location."}, {"heading": "3 Generalized Local Compilation", "text": "This model generalizes the idea of a performance profile to an attainable performance profile and adapts the LC technology to the extended performance profile. (QP) The definition (QP) is able to obtain a complete quality profile for all CCs of the system. (QP) The definition (QP) is able to obtain a complete quality profile for all CCs of the system. (QP) The definition (QP) is able to obtain a complete quality profile for all CCs of the system. (QP) The definition (QP) is able to obtain a complete quality profile for all CCs of the system. (QP) The definition (QP) is able to obtain a complete quality profile for all CCs of the system. (QP) The definition 3.2 (QP) is attainable performance profile (RPP) of a CC is attainable by a number of CPs."}, {"heading": "4 Observation Selection in BNs.", "text": "In this section we describe how our framework can be applied to OSS in BNs. We use the following notation: \u2022 X = {Xi: 1 \u2264 i \u2264 n} - set of all state variables; \u2022 XH X - set of hypothesis state variables; \u2022 XM X - set of measurable state variables; \u2022 Y = {Yi: Xi-XM} - set of test variables; \u2022 N - BN via X-Y; \u2022 R: Pr (XH) \u2022 R - reward function; \u2022 T (E) = number of measurable state variables; \u2022 B - time budget (maximum observation time). Definition 4.1 (OSS optimization problem). The OSS optimization problem is: For a tuple (N, R, T, B) we select a subset of observation variables E Y that maximizes the expected reward: R (XH | E) is a subset of Xs (XH, T, T, E)."}, {"heading": "4.1 OSS in discrete BNs", "text": "We refer to this version of OSS as Boolean OSS (BOSS).The BN can be specified by the following parameters: Xi (Xi): if (Xi) XH), 0: otherwise. (8) We refer to this version of OSS as Boolean OSS (BOSS).The BN can be specified by the following parameters: Xi (Xi): if (Xi) XH), 0: otherwise. (8) We refer to this version of OSS as Boolean OSS (BOSS).The BN can be specified by the following parameters: Xi (Xi), 0: otherwise. (10) We refer to this version of OSS as Boolean OSS (BOSS).The BN can be specified by the following parameters: Xi (Xi), 0: otherwise. (8) We refer to this version of OSS as Boolean OSS (BOSS).The BN can be specified by the following parameters: Xi (Xi)."}, {"heading": "4.2 OSS in Gaussian Bayesian networks", "text": "Gaussian OSS (GOSS) we consider the following reward function: R (A | E) = min Xi Xi Xi Xi (A) = min Xi i (B), whereby the conditional probability distributions of variables (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B) (B (B) (B) (B) (B) (B (B) (B) (B) (B (B) (B) (B (B) (B) (B (B) (B) (B) (B (B) (B) (B (B)) (B (B (B)) (B (B) (B (B (B))) (B (B (B (B), B (B) (B (B (B)) (B (B (B)) (B (B (B) (B (B)) (B (B (B (B))) (B (B (B)) (B (B (B (B))))) (B (B (B (B (B))) (B (B (B)))) (B (B (B (B))) (B (B (B (B))))) (B (B (B))) (B (B (B (B (B (B)))) (B (B (B)"}, {"heading": "5 Summary", "text": "In this paper, we expanded the concept of CPP and presented an efficient technique for assembling a composite system beyond the monotonicity assumption, applying the extended scheme to optimize a series of measurements in two different constellations (to select a maximum expectation variable in a binary BN and to minimize the worst variance in a Gaussian BN), presenting polynomial time methods for both problems, and determining the quality of the approximation in theory. Application of our framework to real areas as an empirical evaluation is underway, and further broadening the framework to address more general system topologies are comprehensible strategies for actively monitoring possible directions for future research."}, {"heading": "Acknowledgements", "text": "Partially supported by the IMG4 consortium (under the MAGNET program of Israel's Ministry of Industry, Trade and Labor), the Lynn and William Frankel Center for Computer Sciences, and the Paul Ivanier Center for Robotics."}], "references": [{"title": "Solving time-dependent planning problems", "author": ["M. Boddy", "T. Dean"], "venue": "In IJCAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Incremental Pruning: A simple, fast, exact method for partially observable Markov decision processes", "author": ["A. Cassandra", "M.L. Littman", "N.L. Zhang"], "venue": "In Proceedings of (UAI\u201397),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Reasoning about beliefs and actions under computational resource constraints", "author": ["E. Horvitz"], "venue": "In UAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1987}, {"title": "Models of continual computation", "author": ["E. Horvitz"], "venue": "In AAAI/IAAI, pages 286\u2013293,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Continual computation policies for allocating offline and real-time resources", "author": ["E. Horvitz"], "venue": "In IJCAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Near-optimal nonmyopic value of information in graphical models", "author": ["A. Krause", "C. Guestrin"], "venue": "In UAI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2005}, {"title": "Optimal nonmyopic value of information in graphical models - efficient algorithms and theoretical limits", "author": ["A. Krause", "C. Guestrin"], "venue": "In IJCAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Knowledgebased anytime computation", "author": ["A.I. Mouaddib", "S. Zilberstein"], "venue": "In IJCAI, pages 775\u2013783,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}, {"title": "Efficient deterministic approximation algorithm for nonmyopic value of information in graphical models", "author": ["Y. Radovilsky", "G. Shattah", "E.S. Shimony"], "venue": "In SMC Conference,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Operational rationality through compilation of anytime algorithms", "author": ["S. Zilberstein"], "venue": "Technical report, Computer Science Division, University of California at Berkeley,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "Optimizing decision quality with contract algorithms", "author": ["S. Zilberstein"], "venue": "In IJCAI, pages 1576\u20131582,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Using anytime algorithms in intelligent systems", "author": ["S. Zilberstein"], "venue": "AI Magazine,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}], "referenceMentions": [{"referenceID": 10, "context": "This task is usually referred to in the research literature as the meta-level resource allocation (MRA) problem (see for example [11]).", "startOffset": 129, "endOffset": 133}, {"referenceID": 9, "context": "Zilberstein [10, 11, 12], the technique of local compilation (LC).", "startOffset": 12, "endOffset": 24}, {"referenceID": 10, "context": "Zilberstein [10, 11, 12], the technique of local compilation (LC).", "startOffset": 12, "endOffset": 24}, {"referenceID": 11, "context": "Zilberstein [10, 11, 12], the technique of local compilation (LC).", "startOffset": 12, "endOffset": 24}, {"referenceID": 2, "context": "Flexible computation refers to procedures that allow a graceful trade-off to be made between the quality of results and allocation of costly resources, such as time, memory, or information [3].", "startOffset": 189, "endOffset": 192}, {"referenceID": 3, "context": "Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].", "startOffset": 184, "endOffset": 190}, {"referenceID": 4, "context": "Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].", "startOffset": 184, "endOffset": 190}, {"referenceID": 7, "context": "Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].", "startOffset": 212, "endOffset": 215}, {"referenceID": 0, "context": "Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].", "startOffset": 240, "endOffset": 251}, {"referenceID": 9, "context": "Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].", "startOffset": 240, "endOffset": 251}, {"referenceID": 11, "context": "Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].", "startOffset": 240, "endOffset": 251}, {"referenceID": 9, "context": "An intuitive approach to tackle the MRA problem has been proposed in [10].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "In general, the task of (global) compilation is computationally hard even for approximate solution [10].", "startOffset": 99, "endOffset": 103}, {"referenceID": 9, "context": "Zilberstein [10] proposed an efficient algorithm based on the local compilation (LC) technique, summarized as RLC in Algorithm 1.", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": "In [7] the authors proved that the OSS problem is NP -hard even for tree-structured BNs.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [9] a similar technique was used beyond the exact observation assumption, and determined a theoretical bound for the worst-case loss in expected reward.", "startOffset": 3, "endOffset": 6}, {"referenceID": 5, "context": "Another approximation method based on greedy selection of test nodes is applicable when the reward function exhibits property of sub-modularity [6].", "startOffset": 144, "endOffset": 147}, {"referenceID": 1, "context": "The idea of pruning irrelevant CPs is very similar to one used in the Incremental Pruning algorithm [2] for filtering irrelevant \u03b1-vectors.", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "Since solving this problem for the general setting (even for the tree-structured topology) is proved to be NP -hard [7], we restrict our focus to BNs with boolean variables (Dom(Xi) = Dom(Yi) = {0, 1}), and consider the following reward function, defined for an arbitrary set of nodes A: R(A|E = e) = max Xi\u2208A Ri(Xi|E = e), where eachRi : Pr(Xi) \u2192 R is a local reward function:", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "is a number of intervals of size \u01ebp in the range [0, 1].", "startOffset": 49, "endOffset": 55}, {"referenceID": 0, "context": "To define Dp we use its projection D \u2032 p to the [0, 1] interval (Dp is defined exactly asDp in the BOSS case): Dp = {p : loga,b(p) \u2208 D \u2032 p}, (32) We express the discretization function \u03bbp through its projected version \u03bbp (which is defined as \u03bbp in BOSS): \u03bbp(p) = \u03bb \u2032 p(loga,b(p)).", "startOffset": 48, "endOffset": 54}], "year": 2008, "abstractText": "Deciding what to sense is a crucial task, made harder by dependencies and by a nonadditive utility function. We develop approximation algorithms for selecting an optimal set of measurements, under a dependency structure modeled by a tree-shaped Bayesian network (BN). Our approach is a generalization of composing anytime algorithm represented by conditional performance profiles. This is done by relaxing the input monotonicity assumption, and extending the local compilation technique to more general classes of performance profiles (PPs). We apply the extended scheme to selecting a subset of measurements for choosing a maximum expectation variable in a binary valued BN, and for minimizing the worst variance in a Gaussian BN.", "creator": null}}}