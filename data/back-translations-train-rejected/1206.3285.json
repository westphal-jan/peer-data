{"id": "1206.3285", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping", "abstract": "We consider the problem of efficiently learning optimal control policies and value functions over large state spaces in an online setting in which estimates must be available after each interaction with the world. This paper develops an explicitly model-based approach extending the Dyna architecture to linear function approximation. Dynastyle planning proceeds by generating imaginary experience from the world model and then applying model-free reinforcement learning algorithms to the imagined state transitions. Our main results are to prove that linear Dyna-style planning converges to a unique solution independent of the generating distribution, under natural conditions. In the policy evaluation setting, we prove that the limit point is the least-squares (LSTD) solution. An implication of our results is that prioritized-sweeping can be soundly extended to the linear approximation case, backing up to preceding features rather than to preceding states. We introduce two versions of prioritized sweeping with linear Dyna and briefly illustrate their performance empirically on the Mountain Car and Boyan Chain problems.", "histories": [["v1", "Wed, 13 Jun 2012 15:45:04 GMT  (273kb)", "http://arxiv.org/abs/1206.3285v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.SY", "authors": ["richard s sutton", "csaba szepesvari", "alborz geramifard", "michael p bowling"], "accepted": false, "id": "1206.3285"}, "pdf": {"name": "1206.3285.pdf", "metadata": {"source": "CRF", "title": "Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping", "authors": ["Richard S. Sutton", "Csaba Szepesv\u00e1ri", "Alborz Geramifard", "Michael Bowling"], "emails": [], "sections": [{"heading": null, "text": "This paper develops an explicitly model-based approach that extends the Dyna architecture to linear functional approximation. Dynasty planning continues by generating imaginary experiences from the world model and then applying model-free learning algorithms for the imaginary state transitions. Our key findings are the proof that Dyna-style linear planning converges into a unique solution under natural conditions regardless of generational distribution. In the context of policy evaluation, we demonstrate that the boundary point is the least square (LSTD) solution. One implication of our results is that prioritized sweeping can be solidly extended to the linear approximation case and is based on previous characteristics rather than previous states. We introduce two versions of prioritized linear Dyna switches and briefly illustrate their performance empirically on the Mountain Car and Boyan Chain problems."}, {"heading": "1 Online learning and planning", "text": "In fact, most of them are in a position to put themselves in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live in which they live in which they live, in which they live, in which they live in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they live, in which they, in which they live."}, {"heading": "2 Notation", "text": "We use the standard framework for reinforcement learning with linear functional approximation (Sutton & Barto 1998), in which experience consists of the temporally indexed stream s0, a0, r1, s1, a1, r2, s2,.. where st-S is a state in which rt-R is an action and rt-R is a reward. Actions are selected by a learning agent, and the states and rewards are selected from a stationary environment. The agent has no direct access to the states, but only through a corresponding trait vector. Rn = \u03c6 (st) The agent selects actions according to a policy. The value function is approximated as a linear function with parameter vector distribution."}, {"heading": "3 Theory for policy evaluation", "text": "A Dyna policy assessment algorithm goes through a sequence of planning steps, each of which generates an initial feature vector \u03c6 according to a probability distribution \u00b5, and then a next feature vector \u03c6. \"In view of this imaginary experience, for example, according to the linear TD (0) algorithm (Sutton 1988), a conventional model-free update using a linear TD (0) algorithm (Sutton 1988) is performed:................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3.1 Convergence and fixed point", "text": "There are two important theoretical questions about the TD (0) or residual gravitation (1) and (2): under what conditions are they convergence? And what do they do? Both of these questions turn out to be interesting answers. First, if the convergence of (1) is in question because it is known that the distribution of the initial states during training could be severely restricted if the distribution of the initial states does not correspond to the normal dynamics of the system, that is, if TD (0) is used outside politics. This suggests that the sampling distribution used here could be severely limited to keep the iteration stable. On the other hand, the data here is from the model, and the model is not a general system: it is deterministic1 and linear. This particular case could be much better behaved."}, {"heading": "3.2 Convergence to the LSTD solution", "text": "So far, we have discussed the convergence of planning on the basis of a model, but we have not said anything about the relationship of the model to the data or about the quality of the resulting solution. Suppose the model is the best linear fit to a finite dataset of observed feature vector-to-feature vector transitions with accompanying rewards. In this case, we can show that the fixed point of Dyna updates is the solution with the least time difference. This is the solution for which the mean TD (0) update is zero and also the solution found by the LSTD (0) algorithm (Barto & Bradtke 1996). Theorem 3.3. Given a training dataset of feature, reward, next-state feature triples D = [\u03c61, r1, \u03c6 \u2032 1, \u03c6 \u2032 1,...,...,..., that the same solution (s) are."}, {"heading": "F> = C\u22121D and b = C\u22121 r.", "text": "The insertion of C, D in (6) and the exclusion of \u03b8 shows that each solution of (6) is also a solution of (7). (8) If we multiply both sides of (8) by C \u2212 1 from the left, we obtain (7). Therefore, each solution of (6) is also a solution of (7). Since all the steps of the above derivation are reversible, we obtain that also the opposite statement applies. Algorithm 2: Linear Dyna with PWMA prioritized sweeping (policy evaluation) Obtain initial results, results, results, results, results, results, results, results, results. For each step: Act according to the policy. Obtain r, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, results, (7). (8) If we multiply both sides of (8) by C \u2212 1 from the left, we obtain (7)."}, {"heading": "4 Linear prioritized sweeping", "text": "We have shown that the convergence and the fixed point of policy evaluation by linear dyna are not influenced by the way in which the starting properties are selected vectors, which opens up the possibility of cleverly selecting them to accelerate the convergence of the planning process. A natural idea - the idea behind the prioritized timpani - is to work backwards from states that have changed in value to the states that lead to them. Lead-in states, in turn, are given priority for updating, because it is likely to change the value of the state (because they lead to a state that has changed in value). If a lead-in state is updated and its value is changed, then its lead-in states are given priority for updating, and so the context of the table lookup in which this idea was developed (Moore & Atkeson 1993; Peng 1993; see also Wingate & Seppi 2005) will be more up-to date, but there may be many states that just change before anyone."}, {"heading": "5 Theory for Control", "text": "We now turn to the complete case of control, in which separate models Fa > ba are learned and then available for each action a > are constructed so that Fa\u03c6 and b > a \u03c6 can be used as an estimate of the trait vector and the reward that will follow \u03c6 when the action a is taken. A linear Dyna algorithm for the control case goes through a sequence of planning steps, selecting one initial trait vector \u03c6 and one action a, and then a next trait vector \u03c6 \u2032 = Fa\u03c6 and the next reward r = ba\u03c6 is generated from the model. In light of this imaginary experience, a conventional model-free update is carried out. The simplest case is to reapply (1). A complete algorithm, including prioritized reversations, is applied in algorithm 4.The theory for the control case is less clear than for the policy assessment. The main problem is the stability of the \"forward matrix\" mixture."}, {"heading": "6 Empirical results", "text": "In this section we illustrate the empirical behavior of the four Dyna algorithms and make comparisons to model-free methods using variations of two standard test problems: Boyan Chain and Mountain Car. Our Boyan Chain environment is therefore an extension of what Boyan (1999, 2002) does from 13 to 98 states and from 4 to 25 characteristics (Geramifard, Bowling & Sutton 2006). Figure 1 describes this environment in general terms. Each episode starts at state N = 98 and ends in state 0. For all states s > 2 there is the same probability that it will result in states s \u2212 1 or s \u2212 2 with a reward of \u2212 3. From states 2 and 1 there are deterministic transitions to states 1 and 0 with respective rewards of \u2212 2 and 0. Our mountain car environment is exactly as described by Sutton (1996; Sutton & Barto 1998), newly implemented in Matlab."}, {"heading": "7 Conclusion", "text": "In this paper, we have taken important steps to establish the theoretical and algorithmic foundations of Dyna Planning with Linear Functional Approximation. We have found that Dyna Planning converges with the known rules of Reinforcement Learning under weak conditions, which in some cases roughly correspond to the existence of a finite solution to the planning problem, and that Convergence is a unique solution with the fewest squares, regardless of the distribution used to generate hypothetical experience. These results enable our second major contribution: the introduction of algorithms that expand prioritized sweeting to linear functional approximation, with correctness guarantees. Our empirical results illustrate the use of these algorithms and their potential to accelerate reinforcement learning. Overall, our findings support the conclusion that Dyna Planning can be a practical and competitive approach to achieving rapid online control for stochastic sequential decision-making problems with large states."}, {"heading": "Acknowledgements", "text": "The authors gratefully acknowledge the significant contributions of Cosmin Paduraru and Mark Ring to the early stages of this work, which was supported by iCORE, NSERC and Alberta Ingenuity."}], "references": [{"title": "Using local trajectory optimizers to speed up global optimization in dynamic programming", "author": ["C. Atkeson"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Atkeson,? \\Q1993\\E", "shortCiteRegEx": "Atkeson", "year": 1993}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L.C. Baird"], "venue": "In Proceedings of the Twelfth International Conference on Machine Learning,", "citeRegEx": "Baird,? \\Q1995\\E", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "Stochastic dynamic programming with factored representations", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "Artificial Intelligence", "citeRegEx": "Boutilier et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 2000}, {"title": "Sigma point policy iteration", "author": ["M. Bowling", "A. Geramifard", "D. Wingate"], "venue": "In Proceedings of the Seventh International Conference on Autonomous Agents and Multiagent Systems", "citeRegEx": "Bowling et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bowling et al\\.", "year": 2008}, {"title": "Least-squares temporal difference learning", "author": ["J.A. Boyan"], "venue": "In Proceedings of the Sixteenth International Conference on Machine Learning,", "citeRegEx": "Boyan,? \\Q1999\\E", "shortCiteRegEx": "Boyan", "year": 1999}, {"title": "Technical update: Least-squares temporal difference learning", "author": ["J.A. Boyan"], "venue": "Machine Learning,", "citeRegEx": "Boyan,? \\Q2002\\E", "shortCiteRegEx": "Boyan", "year": 2002}, {"title": "Learning the structure of factored markov decision processes in reinforcement learning problems", "author": ["T. Degris", "O. Sigaud", "P. Wuillemin"], "venue": "Proceedings of the 23rd International Conference on Machine Learning", "citeRegEx": "Degris et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Degris et al\\.", "year": 2006}, {"title": "General results on the convergence of stochastic algorithms", "author": ["B. Delyon"], "venue": "IEEE Transactions on Automatic Control,", "citeRegEx": "Delyon,? \\Q1996\\E", "shortCiteRegEx": "Delyon", "year": 1996}, {"title": "Incremental least-square temporal difference learning", "author": ["A. Geramifard", "M. Bowling", "R.S. Sutton"], "venue": "Proceedings of the National Conference on Artificial Intelligence,", "citeRegEx": "Geramifard et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Geramifard et al\\.", "year": 2006}, {"title": "Model-based reinforcement learning with an approximate, learned model", "author": ["L. Kuvayev", "R.S. Sutton"], "venue": "Proceedings of the Ninth Yale Workshop on Adaptive and Learning Systems,", "citeRegEx": "Kuvayev and Sutton,? \\Q1996\\E", "shortCiteRegEx": "Kuvayev and Sutton", "year": 1996}, {"title": "Least squares policy iteration", "author": ["M. Lagoudakis", "R. Parr"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Lagoudakis and Parr,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr", "year": 2003}, {"title": "Fast exact planning in Markov decision processes", "author": ["B. McMahan H", "J. Gordon G"], "venue": "Proceedings of the 15th International Conference on Automated Planning and Scheduling", "citeRegEx": "H. and G.,? \\Q2005\\E", "shortCiteRegEx": "H. and G.", "year": 2005}, {"title": "Prioritized sweeping: Reinforcement learning with less data and less real time", "author": ["A.W. Moore", "C.G. Atkeson"], "venue": "Machine Learning,", "citeRegEx": "Moore and Atkeson,? \\Q1993\\E", "shortCiteRegEx": "Moore and Atkeson", "year": 1993}, {"title": "Planning with Approximate and Learned Models of Markov Decision Processes", "author": ["C. Paduraru"], "venue": "MSc thesis,", "citeRegEx": "Paduraru,? \\Q2007\\E", "shortCiteRegEx": "Paduraru", "year": 2007}, {"title": "Efficient learning and planning within the Dyna framework, Adaptive Behavior", "author": ["J. Peng", "R.J. Williams"], "venue": null, "citeRegEx": "Peng and Williams,? \\Q1993\\E", "shortCiteRegEx": "Peng and Williams", "year": 1993}, {"title": "Natural actor-critic", "author": ["J. Peters", "S. Vijayakumar", "S. Schaal"], "venue": "Proceedings of the 16th European Conference on Machine Learning,", "citeRegEx": "Peters et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2005}, {"title": "Temporal difference learning applied to a high-performance game-playing program", "author": ["J. Schaeffer", "M. Hlynka", "V. Jussila"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence,", "citeRegEx": "Schaeffer et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Schaeffer et al\\.", "year": 2001}, {"title": "Reinforcement learning of local shape in the game of Go", "author": ["D. Silver", "R.S. Sutton", "M. M\u00fcller"], "venue": "Proceedings of the 20th International Joint Conference on Artificial Intelligence,", "citeRegEx": "Silver et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2007}, {"title": "Reinforcement learning with a hierarchy of abstract models", "author": ["S.P. Singh"], "venue": "Proceedings of the Tenth National Conference on Artificial Intelligence,", "citeRegEx": "Singh,? \\Q1992\\E", "shortCiteRegEx": "Singh", "year": 1992}, {"title": "Learning to predict by the method of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["R.S. Sutton"], "venue": "Proceedings of the Seventh International Conference on Machine Learning,", "citeRegEx": "Sutton,? \\Q1990\\E", "shortCiteRegEx": "Sutton", "year": 1990}, {"title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "author": ["R.S. Sutton"], "venue": "Advances in Neural Information Processing Systems: Proceedings of the 1995 Conference,", "citeRegEx": "Sutton,? \\Q1996\\E", "shortCiteRegEx": "Sutton", "year": 1996}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Prioritization methods for accelerating MDP solvers", "author": ["D. Wingate", "K.D. Seppi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Wingate and Seppi,? \\Q2005\\E", "shortCiteRegEx": "Wingate and Seppi", "year": 2005}], "referenceMentions": [{"referenceID": 20, "context": "The Dyna architecture (Sutton 1990) provides an effective and flexible approach to incremental planning while maintaining responsiveness.", "startOffset": 22, "endOffset": 35}, {"referenceID": 10, "context": "Paduraru (2007) treated this case, focusing mainly on sampling stochastic models of a cascading linear form, but also briefly discussing deterministic linear models.", "startOffset": 0, "endOffset": 16}, {"referenceID": 10, "context": "Paduraru (2007) treated this case, focusing mainly on sampling stochastic models of a cascading linear form, but also briefly discussing deterministic linear models. Degris, Sigaud and Wuillemin (2006) developed a version of Dyna based on approximations in the form of dynamic Bayes networks and decision trees.", "startOffset": 0, "endOffset": 202}, {"referenceID": 10, "context": "Paduraru (2007) treated this case, focusing mainly on sampling stochastic models of a cascading linear form, but also briefly discussing deterministic linear models. Degris, Sigaud and Wuillemin (2006) developed a version of Dyna based on approximations in the form of dynamic Bayes networks and decision trees. Their system, SPITI, included online learning and planning based on an incremental version of structured value iteration (Boutilier, Dearden & Goldszmidt 2000). Singh (1992) developed a version of Dyna for variable resolution but still tabular models.", "startOffset": 0, "endOffset": 486}, {"referenceID": 0, "context": "Finally, Atkeson (1993) and others have explored linear, learned models with off-line planning methods suited to low-dimensional continuous systems.", "startOffset": 9, "endOffset": 24}, {"referenceID": 19, "context": "Given this imaginary experience, a conventional modelfree update is performed, for example, according to the linear TD(0) algorithm (Sutton 1988):", "startOffset": 132, "endOffset": 145}, {"referenceID": 1, "context": "or according to the residual gradient algorithm (Baird 1995):", "startOffset": 48, "endOffset": 60}, {"referenceID": 7, "context": "Moreover, building on a result by Delyon (1996), one can show that the result continues to hold even if the sequence of features is generated in an algorithmic manner, again provided that some ergodicity conditions are met.", "startOffset": 34, "endOffset": 48}, {"referenceID": 12, "context": "The first, due simultaneously to Peng and Williams (1993) and to Moore and Atkeson (1993), which we call PWMA prioritized sweeping, adds the predecessors of every state encountered in real experience to the priority queue whether or not the value of the encountered state was significantly changed.", "startOffset": 33, "endOffset": 58}, {"referenceID": 0, "context": "The first, due simultaneously to Peng and Williams (1993) and to Moore and Atkeson (1993), which we call PWMA prioritized sweeping, adds the predecessors of every state encountered in real experience to the priority queue whether or not the value of the encountered state was significantly changed.", "startOffset": 75, "endOffset": 90}, {"referenceID": 0, "context": "The first, due simultaneously to Peng and Williams (1993) and to Moore and Atkeson (1993), which we call PWMA prioritized sweeping, adds the predecessors of every state encountered in real experience to the priority queue whether or not the value of the encountered state was significantly changed. The second form of prioritized sweeping, due to McMahan and Gordon (2005), and which we call MG prioritized sweeping, puts each encountered state on the queue, but not its predecessors.", "startOffset": 75, "endOffset": 373}, {"referenceID": 4, "context": "In this section we illustrate the empirical behavior of the four Dyna algorithms and make comparisons to model-free methods using variations of two standard test problems: Boyan Chain and Mountain Car. Our Boyan Chain environment is an extension of that by Boyan (1999, 2002) from 13 to 98 states, and from 4 to 25 features (Geramifard, Bowling & Sutton 2006). Figure 1 depicts this environment in the general form. Each episode starts at state N = 98 and terminates in state 0. For all states s > 2, there is an equal probability of transitioning to states s\u2212 1 or s\u22122 with a reward of\u22123. From states 2 and 1, there are deterministic transitions to states 1 and 0 with respective rewards of\u22122 and 0. Our Mountain Car environment is exactly as described by Sutton (1996; Sutton & Barto 1998), re-implemented in Matlab. An underpowered car must be driven to the top of a hill by rocking back and forth in a valley. The state variables are a pair (position,velocity) initialized to (\u22120.5, 0.0) at the beginning of each episode. The reward is\u22121 per time step. There are three discrete actions (accelerate, reverse, and coast). We used a value function representation based on tile-coding feature vectors exactly as in Sutton\u2019s (1996) experiments, with 10 tilings over the combined (position, velocity) pair, and with the tiles hashed down to 10,000 features.", "startOffset": 172, "endOffset": 1231}], "year": 2008, "abstractText": "We consider the problem of efficiently learning optimal control policies and value functions over large state spaces in an online setting in which estimates must be available after each interaction with the world. This paper develops an explicitly model-based approach extending the Dyna architecture to linear function approximation. Dynastyle planning proceeds by generating imaginary experience from the world model and then applying model-free reinforcement learning algorithms to the imagined state transitions. Our main results are to prove that linear Dyna-style planning converges to a unique solution independent of the generating distribution, under natural conditions. In the policy evaluation setting, we prove that the limit point is the least-squares (LSTD) solution. An implication of our results is that prioritized-sweeping can be soundly extended to the linear approximation case, backing up to preceding features rather than to preceding states. We introduce two versions of prioritized sweeping with linear Dyna and briefly illustrate their performance empirically on the Mountain Car and Boyan Chain problems. 1 Online learning and planning Efficient decision making when interacting with an incompletely known world can be thought of as an online learning and planning problem. Each interaction provides additional information that can be used to learn a better model of the world\u2019s dynamics, and because this change could result in a different action being best (given the model), the planning process should be repeated to take this into account. However, planning is inherently a complex process; on large problems it not possible to repeat it on every time step without greatly slowing down the response time of the system. Some form of incremental planning is required that, though incomplete on each step, still efficiently computes optimal actions in a timely manner. The Dyna architecture (Sutton 1990) provides an effective and flexible approach to incremental planning while maintaining responsiveness. There are two ideas underlying the Dyna architecture. One is that planning, acting, and learning are all continual, operating as fast as they can without waiting for each other. In practice, on conventional computers, each time step is shared between planning, acting, and learning, with proportions that can be set arbitrarily according to available resources and required response times. The second idea underlying the Dyna architecture is that learning and planning are similar in a radical sense. Planning in the Dyna architecture consists of using the model to generate imaginary experience and then processing the transitions of the imaginary experience by model-free reinforcement learning algorithms as if they had actually occurred. This can be shown, under various conditions, to produce exactly the same results as dynamic-programming methods in the limit of infinite imaginary experience. The original papers on the Dyna architecture and most subsequent extensions (e.g., Singh 1992; Peng & Williams 1993; Moore & Atkeson 1993; Kuvayev & Sutton 1996) assumed a Markov environment with a tabular representation of states. This table-lookup representation limits the applicability of the methods to relatively small problems. Reinforcement learning has been combined with function approximation to make it applicable to vastly larger problems than could be addressed with a tabular approach. The most popular form of function approximation is linear function approximation, in which states or state-action pairs are first mapped to feature vectors, which are then mapped in a linear way, with learned parameters, to value or next-state estimates. Linear methods have been used in many of the successful large-scale applications of reinforcement learning (e.g., Silver, Sutton & M\u00fcller 2007; Schaeffer, Hlynka & Jussila 2001). Linear function approximation is also simple, easy to understand, and possesses some of the strongest convergence and performance guarantees among function approximation methods. It is natural then to consider extending Dyna for use with linear function approximation, as we do in this paper. There has been little previous work addressing planning with linear function approximation in an online setting. Paduraru (2007) treated this case, focusing mainly on sampling stochastic models of a cascading linear form, but also briefly discussing deterministic linear models. Degris, Sigaud and Wuillemin (2006) developed a version of Dyna based on approximations in the form of dynamic Bayes networks and decision trees. Their system, SPITI, included online learning and planning based on an incremental version of structured value iteration (Boutilier, Dearden & Goldszmidt 2000). Singh (1992) developed a version of Dyna for variable resolution but still tabular models. Others have proposed linear least-squares methods for policy evaluation that are efficient in the amount of data used (Bradtke & Barto 1996; Boyan 1999, 2002; Geramifard, Bowling & Sutton 2006). These methods can be interpreted as forming and then planning with a linear model of the world\u2019s dynamics, but so far their extensions to the control case have not been well suited to online use (Lagoudakis & Parr 2003; Peters, Vijayakumar & Schaal 2005; Bowling, Geramifard, & Wingate 2008), whereas our linear Dyna methods are naturally adapted to this case. We discuss more specifically the relationship of our work to LSTD methods in a later section. Finally, Atkeson (1993) and others have explored linear, learned models with off-line planning methods suited to low-dimensional continuous systems.", "creator": "TeX"}}}