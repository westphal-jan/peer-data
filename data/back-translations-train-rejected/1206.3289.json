{"id": "1206.3289", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Efficient inference in persistent Dynamic Bayesian Networks", "abstract": "Numerous temporal inference tasks such as fault monitoring and anomaly detection exhibit a persistence property: for example, if something breaks, it stays broken until an intervention. When modeled as a Dynamic Bayesian Network, persistence adds dependencies between adjacent time slices, often making exact inference over time intractable using standard inference algorithms. However, we show that persistence implies a regular structure that can be exploited for efficient inference. We present three successively more general classes of models: persistent causal chains (PCCs), persistent causal trees (PCTs) and persistent polytrees (PPTs), and the corresponding exact inference algorithms that exploit persistence. We show that analytic asymptotic bounds for our algorithms compare favorably to junction tree inference; and we demonstrate empirically that we can perform exact smoothing on the order of 100 times faster than the approximate Boyen-Koller method on randomly generated instances of persistent tree models. We also show how to handle non-persistent variables and how persistence can be exploited effectively for approximate filtering.", "histories": [["v1", "Wed, 13 Jun 2012 15:46:24 GMT  (237kb)", "http://arxiv.org/abs/1206.3289v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tomas singliar", "denver dash"], "accepted": false, "id": "1206.3289"}, "pdf": {"name": "1206.3289.pdf", "metadata": {"source": "CRF", "title": "Efficient inference in persistent Dynamic Bayesian Networks", "authors": ["Tom\u00e1\u0161 \u0160ingliar"], "emails": [], "sections": [{"heading": null, "text": "Numerous temporal sequencing tasks, such as error monitoring and anomaly detection, have persistence properties: for example, if something breaks, it remains broken until intervention. If it is modeled as a dynamic Bayesian network, persistence adds dependencies between adjacent time periods, which often proves insoluble over time using standardized inference algorithms. However, we show that persistence implies a regular structure that can be used for efficient conclusions. We present three successively more general classes of models: persistent causal chains (PCCs), persistent causal trees (PCTs), and persistent polytrees (PPTs), and the accompanying exact inferential algorithms that exploit persistence. We show that analytical asymptotic boundaries for our algorithms can treat causal chains favorably with causal chains (CCTs) and persistent polytrees (PTs), and the associated exact inferential algorithms that exploit persistence."}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before we reach an agreement."}, {"heading": "2 Notation and changepoints", "text": "Consider a Bayesian network (BN) with N binary variables Xi; we refer to this network as a prototype. The corresponding dynamic BN with M slices are created by replicating the prototype M times, and combine some of the variables with their copies in the next slice. In our notation, the upper indexes extend over the time slices of the DBN; lower indexes reach over variables in any time span. Colon notation is used to denote sentences and sequences. For example, X1: M4 denotes the entire temporal sequence of the values of X4 from time to time M: variables without an upper index refer to their respective counterparts in the prototype. We say that a variable Xk is persistent if P (Xtk = 1, U t) = {P (Xk | U) = {P (Xk | U) if Xt = 01, if Xt \u2212 1k = 01, (1) where U = Xk are parents."}, {"heading": "3 PCC-DBN inference", "text": "To simplify the exposure, we will now focus on a specific prototype, a persistent causal chain DBN = = 11 (PCCDBN). This is a chain with Pa (Xi) = {Xi \u2212 1}, i = 1,..., N and Pa (O) = XN (so it has N + 1 node). Let us further assume that the leaves are not persistent and observed, while the causes (X nodes) are all persistent and hidden. The network is shown in Figure 1a and its transformed version in Figure 1b.Consider the problem of calculating P (O). This is generally one of the most difficult inference problems, where one has to integrate all the hidden state variables and is implicit in most inference queries: P (O1: M) = X1: M1: NP (O1: M | X1: M1: N) \u00b7 P (X1: N)."}, {"heading": "3.1 Upward Recursion Relations", "text": "As a limit condition for recursion, we assume that we have calculated for all 0 \u2264 k \u2264 M + 1. We show how to do this in the time O (M) in section 3.2. Furthermore, this algorithm requires the precalculation and caching of P-Iks for 0 \u2264 k \u2264 N and 0 \u2264 i \u2264 M, which can be dosed in O (MN) time and space. If you take a closer look at equation 7, it should be easy to recognize that for 0 \u2264 i \u2264 M in O (M) time with the following recursion you can calculate the equation N for 0 \u2264 i \u2264 M in the recursion. You can also repeat the equation for 0 \u2264 i \u2264 M with the recursion."}, {"heading": "3.2 Computing \u03a3iN+1", "text": "To prove conclusively, we must show how to calculate ElectriN + 1 (probability of observations for a given configuration i of X1: MN) for all 0 \u2264 i \u2264 M in time O (M). Since the parent of each Oj is given for each i, this calculation is simply the product of the observations: P (O1: M | jN = i) = M \u30fb iK = 1P (Ok | XkN, jN =) (13). Using our existing notation, we define '+ 1 = P (O k | XkN = 1), (14),' kkn = 1 '(1),' kn = 1 '(14),' kn = 1), 'kn = 1' (14), 'kn = 1),' (14), 'kn = 1),' (14), 'kn = 1),' (14), 'n = 1),' (14), 'n = 1),' (14), '(14),' n = 1), '(14),' (13), '(14),' (13), '(13),' (13), '(13),' (13), '(13)."}, {"heading": "3.3 Downward Recurrences", "text": "The discussion above completes the description of the \u03bb-pass of the PCC-DBN algorithm. Similar considerations can be applied to obtain the \u03c0-pass repetitions that we now give without a complete derivative. Analogous to the procedure is the semantics of the procedure of the procedure p (Xk = j | O + k), where O + k is the subset of proofs that can be obtained by Xk through his descent."}, {"heading": "3.4 PCC-DBN and belief propagation", "text": "We have just defined PCC-DBN, a version of the propagation of faith that first collects the evidence by passing the \u03bb messages to the root of the chain and passing the information on the \u03c0 messages to the leaves. Once the propagation is complete, we can recall any rear asp (X-k | O) = \u0430k + 1 \u00b7 \u0435k k k. (21) It is now useful to recall the types of potentials involved in Pearl's algorithm [8] and their relationship to the aforementioned quantities. For each node X, there are local potentials \u03c0X (x) \u0445 def p (X = x | e + X) and \u03bbX (x) \u0445 def p (e \u2212 X | X = x), where e + X and e \u2212 X respectively contain the evidences atable by the parents and the evidences atable by X \"downwards.\" There are two types of messages in Pearl's algorithm: \u0432X \u2212 Yi, which are sent by X to their children, the parents, and the children themselves."}, {"heading": "3.5 Simple Generalizations and Causal Trees", "text": "While PCC DBNs are useful to illustrate the general concepts of how to handle multiple probability distributions resulting from the changepoint transformation, they form a rather limited class of networks, and the inference query we have conducted has also been limited. Here, we briefly give a series of simple changes that allow this algorithm to be loosened in various ways: General Evidence Patterns We can have observations anywhere in the network, in any time span. If we consider the conclusion as a faith propagation, we can give the answer to any probability variable as Equation 21 with one caveat: An observation like X33 = 1 does not tell us with certainty the position of the changepoint, but it merely provides evidence that j3 < 3, so we cannot simply set the intersection point variable to state 3. Rather, the potentials corresponding to such evidence must be multiplied to messages as they are spelled by the faith propagation algorithm (see equality 22)."}, {"heading": "4 Further Generalizations", "text": "In this section we describe three other important generalizations of PC DBNs: polytrees, non-persistent nodes, and finally an approximate algorithm for general DAGs. These relaxations are more involved than those in Section 3.5 and therefore require more elaboration."}, {"heading": "4.1 Polytrees", "text": "In polytree changepoint networks, the structure in the conditional probability table P (X = x | U) can be used to store a multiplicative factor of M + 1, just as we have shown for tree nets. In this case, the recurrent structures are passed over the first parent variable, while the remaining parents are added by raw force. Similarly, the recurrent structures are executed over the parents to whom the message is addressed. In this case, the definition of the evolutionary processes is replaced by the temporal processes of P. The definition of the temporal processes of P. < M. Z = 1P. (z) k. \u00b7 [jk. Z = L + 1P. (z) k."}, {"heading": "4.2 Non-persistent nodes", "text": "We now show how an occasional non-persistent variable in the network can be handled even in polynomial time. We assume that the non-persistent variable is isolated, that is, that all of its neighbors are persistent. We start from this assumption to avoid having to call an embedded generic DBN inference algorithm like 2TBN to handle connected non-persistent variables. It is feasible, but it quickly becomes complex and inelegant. An easy way to deal with connected non-persistent nodes is to combine them into a single common node. Obviously, this solution leads to exponential growth in the state space of the connected nodes, making it somewhat unattractive."}, {"heading": "4.2.1 A simple example", "text": "To illustrate how an isolated non-persistent node would be handled, we first take a simple structure as shown in Figure 2. Then we can efficiently calculate P (Y = j) by moving the sums inwards: P (Y = j) = \u2211 XP (X) P (Y = j | X) = \u2211 X1,... XM [M] k = 1P (Xk | Xk \u2212 1)] [M) k = 1 \u03c6kj] = \u2211 X1 P (X1) \u03c61j [X2P (X2 | X1) \u03c62j."}, {"heading": "4.2.2 The general case", "text": "In faith multiplication (BP), the cliques correspond to the edges of the original polytree, and the dividing lines consist of individual nodes. Suppose we have a situation like Figure 3. Since variable C is not persistent, the size of the induced separator C is 2M. However, we can conceptually work with a larger clique BCD. The propagation of the message then requires summing up all C1: M, which we can do without actually repeating the clique potential as in Equation 27. The complete repeatability of variable B + is the variable that results in variable B."}, {"heading": "5 Experimental evaluation", "text": "We have implemented our algorithms in Matlab and compare them with the exact and approximate algorithms implemented in the Bayesian Network Toolbox (BNT) [11], because we compare them with the Boyen-Koller (BK) algorithm [1] in its 1) exact and 2) fully factorized setting. Although BK is reduced in its exact form to the incremental junction tree algorithm, we found that in practice it was faster than the 2TBN implementation, so the 2TBN algorithm is not included in the evaluation. Matlab runtime is not the ideal measure of the complexity of the algorithm, as it is probably more sensitive to the quality of implementation compared to other languages."}, {"heading": "5.1 Speed of the tree algorithm", "text": "To compare the inference velocity, a network with the structure of a whole binary tree with N nodes was generated. Among the possible observations of the MN, 10% of the variables were set to a random value (subject to persistence constraints, so that P (E) 6 = 0). We measured the execution time of query p (X-1 | E) - the posterior probability above the root node - for each algorithm. This process was added 100 times for each M, N combination and the respective times. In fact, the exact algorithm soon runs out of memory (around N = 20) and only the approximate version remains in existence. Exact PCT inconsistency, even if the actual number of points exceeds the number of points, is generated at the same time. We measured the execution time of query p (X-1 | E) - the posterior probability above the root node - for each algorithm."}, {"heading": "5.2 Speed of the polytree algorithm", "text": "However, the asymptotic time complexity of PPT-DBN may be less favorable than that of incremental approaches as the gradient velocity increases. However, its lower memory complexity is very beneficial, as documented in the following experiment. We have created a network in which most non-root nodes have exactly 2 parents, and have measured the time for the three inference algorithms. A square scaling with M is expected for PPT-DBN in such a network.Figure 6 shows that the exact PPT-DBN algorithm is many times faster than the approximate, fully factored Boyen-Koller algorithm with an M = 20 time interference window. Looking forward in Figure 7 suggests that the time performance at M = 70 time intervals would be approximately identical. The junction tree algorithm does not scale beyond 20 nodes based on the memory consumption rate of PT-7 clearly shows that Palih exceeds PPT-17."}, {"heading": "5.3 Fixed-window approximation", "text": "A slight disadvantage of the PPT-DBN algorithm is that it is not yet able to draw online conclusions. Therefore, monitoring a process M increases, and thus the computing time. In practice, only a fixed number of current observations is usually taken into account, with older observations falling out of the \"window.\" Therefore, we evaluate whether an appropriate precision can be achieved with small window sizes in which PPT-DBN dominates. Figure 8 shows that for several time windows t the mean square error of the calculated rear marginal errorBK = \u221a N-Slick i = 1 [PBK (Xti | O1: t) \u2212 Pex (Xti | O1: t) 2 can be caused by the fully factored Boyen-Koller method, and the same error for PPT-DBN, which ignores the smoothness of older than W."}, {"heading": "6 Conclusions and future work", "text": "We presented an algorithm for PC DBNs, a way to exploit the special structure of the DBN probability distribution when many variables are persistent. Unlike forward-looking approaches to the DBN conclusion that work slice-to-slice, we collapse the entire time progression and perform inferences in the original prototype network structure. For trees, the algorithm is many times faster than state-of-the-art universal universal exact and approximate DBN inference algorithms, while we have a spatial complexity of only O (MN), including generalization with inference window lengths running into the hundreds. While this method does not directly produce an incremental filtering algorithm, we show that a smoothing version of PC DBN inferences can perform approximate filtering of the approximate time that we can use faster and more persistent filtering algorithms than we have presented."}], "references": [{"title": "Tractable inference for complex stochastic processes", "author": ["X. Boyen", "D. Koller"], "venue": "In Proceedings of UAI-98,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Compiling Bayesian networks with local stucture", "author": ["Mark Chavira", "Adnan Darwiche"], "venue": "In Proceedings of IJCAI 2005,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "Compiling Bayesian networks using variable elimination", "author": ["Mark Chavira", "Adnan Darwiche"], "venue": "In Proceedings of IJCAI 2007,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Bayesian biosurveillance of disease outbreaks", "author": ["Gregory Cooper", "Denver Dash", "John Levander", "Weng- Keen Wong", "William Hogan", "Michael Wagner"], "venue": "In Proceedings of UAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "A model for reasoning about persistence and causation", "author": ["Thomas Dean", "Keiji Kanazawa"], "venue": "Computational Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Continuous time Markov networks", "author": ["Tal El-Hay", "Nir Friedman", "Daphne Koller", "Raz Kupferman"], "venue": "In Proceedings of UAI-06. AUAI Press,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "A new approach to linear filtering and prediction problems. Transactions of the ASME", "author": ["R.E. Kalman"], "venue": "Journal of Basic Engineering,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1960}, {"title": "A computational model for combined causal and diagnostic reasoning in inference systems", "author": ["Jin H. Kim", "Judea Pearl"], "venue": "In Proceedings IJCAI-83 (Karlsruhe,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1983}, {"title": "Bayesian inference in the presence of determinism", "author": ["David Larkin", "Rina Dechter"], "venue": "In Proceedings of Workshop on AI and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "The Bayes Net Toolbox for Matlab", "author": ["Kevin Murphy"], "venue": "Computing Science and Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Dynamic Bayesian Networks: Representation, Inference and Learning", "author": ["Kevin Murphy"], "venue": "PhD thesis, EECS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "The factored frontier algorithm for approximate inference in DBNs", "author": ["Kevin Murphy", "Yair Weiss"], "venue": "In Proceedings of 12 NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Survey of Bayesian models for modelling of stochastic temporal processes", "author": ["Brenda M. Ng"], "venue": "Technical Report UCRL-TR-225272, Lawrence Livermore National Laboratory,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Exploiting Locality in Probabilistic Inference", "author": ["Mark Andrew Paskin"], "venue": "PhD thesis, EECS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Fusion, propagation, and structuring in belief networks", "author": ["Judea Pearl"], "venue": "Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1986}, {"title": "Fusion and propagation with multiple observations in belief networks", "author": ["Mark A. Peot", "Ross D. Shachter"], "venue": "Artificial Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1991}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "In Proceedings of the IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1989}, {"title": "Affine algebraic decision diagrams (AADDs) and their application to structured probabilistic inference", "author": ["Scott Sanner", "David McAllester"], "venue": "In Proceedings of IJCAI", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Learning to detect adverse traffic events from noisily labeled data", "author": ["Tom\u00e1\u0161 \u0160ingliar", "Milo\u0161 Hauskrecht"], "venue": "In Proceedings of PKDD", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Understanding belief propagation and its generalizations", "author": ["Jonathan S. Yedidia", "William T. Freeman", "Yair Weiss"], "venue": "In Exploring Artificial Intelligence in the New Millennium,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}], "referenceMentions": [{"referenceID": 18, "context": "For instance, vehicular accidents cause obstructions on the road that last much longer than the required detection time and are thus persistent for the purpose of detection [20].", "startOffset": 173, "endOffset": 177}, {"referenceID": 3, "context": "Another example is outbreak detection [4], where an infected population stays infected much longer than the desired detection time.", "startOffset": 38, "endOffset": 41}, {"referenceID": 4, "context": "Dynamic Bayesian Networks (DBNs) [5] are a general formalism for modeling temporal systems under uncertainty.", "startOffset": 33, "endOffset": 36}, {"referenceID": 16, "context": "Many standard time-series methods are special cases of DBNs, including Hidden Markov Models [18] and Kalman filters [7].", "startOffset": 92, "endOffset": 96}, {"referenceID": 6, "context": "Many standard time-series methods are special cases of DBNs, including Hidden Markov Models [18] and Kalman filters [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 0, "context": "Discrete DBNs in particular are a very popular formalism, but usually suffer from intractability [1] when dense inter-temporal dependencies are present among hidden state variables, leading many to search for approximation algorithms [1, 13, 15, 14].", "startOffset": 97, "endOffset": 100}, {"referenceID": 0, "context": "Discrete DBNs in particular are a very popular formalism, but usually suffer from intractability [1] when dense inter-temporal dependencies are present among hidden state variables, leading many to search for approximation algorithms [1, 13, 15, 14].", "startOffset": 234, "endOffset": 249}, {"referenceID": 11, "context": "Discrete DBNs in particular are a very popular formalism, but usually suffer from intractability [1] when dense inter-temporal dependencies are present among hidden state variables, leading many to search for approximation algorithms [1, 13, 15, 14].", "startOffset": 234, "endOffset": 249}, {"referenceID": 13, "context": "Discrete DBNs in particular are a very popular formalism, but usually suffer from intractability [1] when dense inter-temporal dependencies are present among hidden state variables, leading many to search for approximation algorithms [1, 13, 15, 14].", "startOffset": 234, "endOffset": 249}, {"referenceID": 12, "context": "Discrete DBNs in particular are a very popular formalism, but usually suffer from intractability [1] when dense inter-temporal dependencies are present among hidden state variables, leading many to search for approximation algorithms [1, 13, 15, 14].", "startOffset": 234, "endOffset": 249}, {"referenceID": 19, "context": "We then generalize our results to polytree causal networks, where the algorithm remains exact, and to general networks, where it inherits properties of loopy belief propagation [21].", "startOffset": 177, "endOffset": 181}, {"referenceID": 0, "context": "Nonetheless, we show empirically that, if evidence is observed at every time slice, approximate filtering can be accomplished with fixed window smoothing, producing lower error than approximate Boyen-Koller (BK) filtering [1]", "startOffset": 222, "endOffset": 225}, {"referenceID": 1, "context": "There has been other work that seeks to directly or indirectly exploit general deterministic structure in Bayesian networks using compilation approaches [2], a generalized version belief propagation [10], and variable elimination with algebraic decision diagrams [3, 19].", "startOffset": 153, "endOffset": 156}, {"referenceID": 8, "context": "There has been other work that seeks to directly or indirectly exploit general deterministic structure in Bayesian networks using compilation approaches [2], a generalized version belief propagation [10], and variable elimination with algebraic decision diagrams [3, 19].", "startOffset": 199, "endOffset": 203}, {"referenceID": 2, "context": "There has been other work that seeks to directly or indirectly exploit general deterministic structure in Bayesian networks using compilation approaches [2], a generalized version belief propagation [10], and variable elimination with algebraic decision diagrams [3, 19].", "startOffset": 263, "endOffset": 270}, {"referenceID": 17, "context": "There has been other work that seeks to directly or indirectly exploit general deterministic structure in Bayesian networks using compilation approaches [2], a generalized version belief propagation [10], and variable elimination with algebraic decision diagrams [3, 19].", "startOffset": 263, "endOffset": 270}, {"referenceID": 10, "context": "Sometimes [12] a variable is called persistent if it has an arc to the next-slice copy of itself.", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": "The leaves can be distributed as multinomials, Gaussians etc, as is often done with Hidden Markov models [18] when they are put to their many uses.", "startOffset": 105, "endOffset": 109}, {"referenceID": 7, "context": "It is now useful to recall the types of potentials involved in Pearl\u2019s algorithm [8] and how they relate to the quantities above.", "startOffset": 81, "endOffset": 84}, {"referenceID": 14, "context": "Belief propagation [16, 17] is a powerful framework for exact inference in polytree networks.", "startOffset": 19, "endOffset": 27}, {"referenceID": 15, "context": "Belief propagation [16, 17] is a powerful framework for exact inference in polytree networks.", "startOffset": 19, "endOffset": 27}, {"referenceID": 10, "context": "The 2TBN algorithm [12] suffers a worst-case time complexity O(2M), as all nodes in two slices may be entangled [9] in the clique to connect the two subsequent time-slices, even though the prototype network is a polytree [12, section 3.", "startOffset": 19, "endOffset": 23}, {"referenceID": 2, "context": "by use of ADD compilation [3, 2], could very well work better for networks with only a few persistent variables.", "startOffset": 26, "endOffset": 32}, {"referenceID": 1, "context": "by use of ADD compilation [3, 2], could very well work better for networks with only a few persistent variables.", "startOffset": 26, "endOffset": 32}, {"referenceID": 19, "context": "Pearl\u2019s belief propagation has been generalized to the clique tree propagation algorithm [21].", "startOffset": 89, "endOffset": 93}, {"referenceID": 9, "context": "We implemented our algorithms in Matlab and compare them to the exact and approximate algorithms as implemented in the Bayesian Network Toolbox (BNT) [11].", "startOffset": 150, "endOffset": 154}, {"referenceID": 0, "context": "Namely, we will compare to the Boyen-Koller (BK) algorithm [1] in its 1) exact and 2) fully factored setting.", "startOffset": 59, "endOffset": 62}, {"referenceID": 5, "context": "Such problems are often found in bioinformatics areas such as phylogeny discovery, where time of a mutation is of interest [6].", "startOffset": 123, "endOffset": 126}], "year": 2008, "abstractText": "Numerous temporal inference tasks such as fault monitoring and anomaly detection exhibit a persistence property: for example, if something breaks, it stays broken until an intervention. When modeled as a Dynamic Bayesian Network, persistence adds dependencies between adjacent time slices, often making exact inference over time intractable using standard inference algorithms. However, we show that persistence implies a regular structure that can be exploited for efficient inference. We present three successively more general classes of models: persistent causal chains (PCCs), persistent causal trees (PCTs) and persistent polytrees (PPTs), and the corresponding exact inference algorithms that exploit persistence. We show that analytic asymptotic bounds for our algorithms compare favorably to junction tree inference; and we demonstrate empirically that we can perform exact smoothing on the order of 100 times faster than the approximate Boyen-Koller method on randomly generated instances of persistent tree models. We also show how to handle non-persistent variables and how persistence can be exploited effectively for approximate filtering.", "creator": " TeX output 2008.05.14:2033"}}}