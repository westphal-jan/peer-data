{"id": "1206.3291", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Hierarchical POMDP Controller Optimization by Likelihood Maximization", "abstract": "Planning can often be simpli ed by decomposing the task into smaller tasks arranged hierarchically. Charlin et al. [4] recently showed that the hierarchy discovery problem can be framed as a non-convex optimization problem. However, the inherent computational di culty of solving such an optimization problem makes it hard to scale to realworld problems. In another line of research, Toussaint et al. [18] developed a method to solve planning problems by maximumlikelihood estimation. In this paper, we show how the hierarchy discovery problem in partially observable domains can be tackled using a similar maximum likelihood approach. Our technique rst transforms the problem into a dynamic Bayesian network through which a hierarchical structure can naturally be discovered while optimizing the policy. Experimental results demonstrate that this approach scales better than previous techniques based on non-convex optimization.", "histories": [["v1", "Wed, 13 Jun 2012 15:51:21 GMT  (232kb)", "http://arxiv.org/abs/1206.3291v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["marc toussaint", "laurent charlin", "pascal poupart"], "accepted": false, "id": "1206.3291"}, "pdf": {"name": "1206.3291.pdf", "metadata": {"source": "CRF", "title": "Hierarchical POMDP Controller Optimization by Likelihood Maximization", "authors": ["Marc Toussaint", "Laurent Charlin"], "emails": ["mtoussai@cs.tu-berlin.de", "lcharlin@cs.toronto.edu", "ppoupart@cs.uwaterloo.ca"], "sections": [{"heading": null, "text": "Charlin et al. [4] recently demonstrated that the problem of finding hierarchies can be defined as a non-convex optimization problem. However, the computational difficulty inherent in solving such an optimization problem complicates scaling to real problems. In another line of research, Toussaint et al. [18] developed a method for solving planning problems by maximum probability estimation. In this paper, we show how the problem of finding hierarchies in partially observable areas can be approached with a similar approach of maximum probability. Our technique first transforms the problem into a dynamic Bayesian network through which a hierarchical structure can be discovered naturally while policy is optimized. Experimental results show that this approach is better scaled than previous techniques based on non-convex optimization."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "2 Background", "text": "Throughout the essay, we refer to random variables by uppercase letters (e.g. X), random variable values by their corresponding lowercase letters (e.g. x-dom (X)), and values by uppercase letters with mathematical calligraphy (e.g. X = {x1, x2, x3}). We will now review POMDPs (Section 2.1) on how to represent guidelines as finite state controllers (Section 2.2) and how to optimize limited controllers (Section 2.3)."}, {"heading": "2.1 POMDPs", "text": "Partially observable Markov decision-making processes (POMDPs) provide a natural and principled framework for planning. A POMDP can be formally defined by a series of observations < S, A, O, P, P, S, S, S, S, A, S, A, O, O, O, O, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr, Pr,"}, {"heading": "2.2 Finite State Controllers", "text": "Instead of using beliefs as sufficient statistics of history, the idea is to use a finite internal memory to store relevant information from history. Any configuration of this memory can be considered a node in a finite state controller, in which nodes select actions to be performed, and edges indicate how nodes can be updated based on the observations received. A controller with a finite set of N nodes n can encode a stochastic policy with three distributions: Pr (N0 = n) = pn (initial node distribution), Pr (At = a | Nt = n) = pa | n (action selection distribution), and Pr (Nt + 1 = n) | Nt = n, Ot + 1 = o \u2032) = pn \u00b2 (successor node distribution). Such a policy can be executed by starting from a node n, an action performed from a node to a node."}, {"heading": "2.3 Policy Optimization", "text": "Several techniques have been proposed to optimize controllers of a given size, including gradient ascent [9], stochastic local search [2], limited political iteration [14], non-convex quadratic constrained optimization [1], and probability maximization [18]. We briefly describe the latter technique as we present it in Section 4.Toussaint et al. [18] recently proposed converting POMDPs into equivalent Bayesian Dynamic Networks (DBNs) by normalizing rewards and optimizing a policy by maximizing the probability of normalized rewards. Let R be a binary variable corresponding to the normalized reward values. The reward function ras will then be replaced by a reward distribution, the Pr (R = r = r = r = r = R = R = R = E = E = E = E = E = E = E = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O = O."}, {"heading": "3 Hierarchical Modeling", "text": "While the optimization of a limited controller allows an effective search in the space of limited guidelines, such an approach is clearly suboptimal, since the optimal controller of many problems grows twice exponentially with the planning horizon and can be infinite for infinite horizons. Alternatively, hierarchical representations allow the representation of structured strategies with exponentially fewer parameters. Several approaches have recently been explored to model and learn hierarchical structures in POMDPs. Pineau et al. [13] suggested recursive controllers (which subsume hierarchical controllers) and an approach that detects hierarchy while optimizing a controller."}, {"heading": "3.1 Recursive Controllers", "text": "A recursive controller [4] consists of a recursive hierarchy with concrete nodes n and abstract nodes n. Formally, a recursive controller is parameterized by an action selection distribution for each node (e.g. pa-n and pa-n-n), a successor node distribution for each node (e.g. pn-no and pn-n-o-n).The execution of a recursive controller takes place by executing the action selected by each visited node and by continuing the successor node selected by the observation."}, {"heading": "3.2 Hierarchical HMMs", "text": "Murphy and Paskin [10] proposed to model hierarchical hidden Markov models (HMMs) as dynamic Bayesian networks (DBNs), the idea being to convert a hierarchical L-level HMM into a dynamic Bayesian network of L-state variables, in which each variable encodes abstract states at the corresponding level, whereby abstract states can only call sub-HMMs at the previous level. Fig. 2 illustrates a two-tier hierarchical HMM encoded as DBN. The state variables Slt are determined by the time step t and the level l. The Et variables indicate when a sub-HMM is finished and return its control to the top level HMM. The uppermost abstract state transitions become according to the uppermost HMM, but only if the initial variable Et indicates that the base-level concrete state is an exit state."}, {"heading": "4 Factored Controllers", "text": "We propose to combine the DBN encryption techniques of Murphy et al. [10] and Toussaint et al. [18] to convert a POMDP with a hierarchical controller into a mixture of DBNs. Hierarchy and controller are optimized simultaneously by maximizing the reward probability of the DBN."}, {"heading": "4.1 DBN Encoding", "text": "Fig. 3a illustrates two consecutive distributions of a DBN in the mix (rewards are omitted) for a hierarchical controller on three levels. Consider a POMDP defined by the tuple < S, A, O, ps, ps, ps, ps, ps, as, as, ras, and a hierarchical (non-recursive) controller defined by the tuple. Conditional probability distributions of the initial distributions (non-recursive) are: \u2022 Transitional distributions: p \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2."}, {"heading": "4.2 Maximum Likelihood Estimation", "text": "According to the technique of Toussaint et al. [18] we optimize a factorized controller by maximizing the probability of reward. As the political parameters are conditional probability distributions of the DBN, the EM algorithm can be used to optimize them. The calculation is carried out alternately in the E- and M-steps below. We designate the top and the bottom node according to ntop and nbase within a given time span. We also designate the parents of V and a configuration of the parents of V.E-steps according to ntop and nbase: expected frequency of the hidden variables Entop = Pr (N top 0 = ntop | R Sullivan = 1) Eanbase = \u2211 t Pr (At = a, N base t = nbase | R = 1) En \u2032 lp (n \u2032 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l \u00b2 l l \u00b2 l l \u00b2 l \u00b2 l l \u00b2 l l \u00b2 l l l \u00b2 l = 1) Eanbase = \u2211 t Pr (At = a, N base t t t = nbase | R = 1) En \u2032 lp (n \u2032 l \u00b2 l l \u00b2 l \u00b2 l \u00b2 l \u00b2 l l \u00b2 l l l l \u00b2 l l l l l \u00b2 l \u00b2 l l l l) l (n \u2032 l) l) l) l) l) l (n \u00b2 l) l) l) l) l)"}, {"heading": "4.2.1 Parameter initialization", "text": "W.l.above we initialize the start node N top0 of the topmost layer as the first node (i.e., Pr (N top0 = 1) = 1. The conditional distributions of the node pn \u2032 l | \u03c6 (n \u2032 l) are randomly initialized as a mixture of three distributions: pn \u2032 l | \u03c6 (n \u2032 l) \u0445 c1 + c2Un \u2032 l\u03c6 (n \u2032 l) + c3\u03b4n \u2032 lnlThe mixing components are an even distribution, a random distribution U\u03c6 (n \u2032 l) (an array of uniform random numbers in [0, 1] and a term that forces nl to remain unchanged. For the node distributions at the base level we choose c1 = 1, c2 = 1, c2c2 = Ucde = 1, whereby the forces are associated with 1 = 1, c2 = nci = 1."}, {"heading": "4.2.2 E-step", "text": "To speed up the calculation of the inference queries in the E step, we calculate intermediate terms using a forward-backward method. If we let tmax be the largest value of T, then a simple scheme that answers each query separately, takes O (t2max) time, because there are O (tmax) queries, and each query takes O (tmax) time to execute across the entire network. Since part of the calculation is duplicated in multiple queries, it is possible to calculate intermediate terms \u03b1 and \u03b2 in O (tmax) time, from which any expectation can be calculated in constant time (w.r.t. tmax). To simplify notation, N and n denote all nodes and their common configuration in a given time window. Forward expressed: tns = Pr (Nt = n, St = s) and n (pnps)."}, {"heading": "4.2.3 M-step", "text": "The standard M step adjusts each parameter pv | \u03c6 (v) by normalizing the expectations calculated in the Estep, i.e., pnewv | \u03c6 (v) \u0445 Ev\u03c6 (v). To accelerate convergence, we instead use a variant that performs a smoother greedy M step. In the greedy M step, each parameter pnewv | \u03c6 (v) is greedily set to 1 if v = argmaxv \u0445 fv step and the standard M step, where the partial E step f remains fixed. Combining a standard M step with this specific partial E step updates the pv step (v) proportionally to fv\u03c6 (v) by a multiplicative factor. In the limit value, the largest part step f is fixed."}, {"heading": "4.2.4 Complexity", "text": "For a flat controller, the number of parameters (neglect of normalization) is O (tmax (| N | N | 2 for pn \u2032 | o \u2032 n and | A | | N | for pa | n). The complexity of the forward (backward) method is O (tmax (| N | S | 2 + | N | 2 | S |), where the two terms correspond to the size of the two cliques for the conclusion in the 2-part DBN after O and A. The complexity of calculating the expectations from \u03b1 and \u03b2 is O (| N | S | 2 + | S | | | O |) + | N | 2 | S | | O |), which correspond to the clique sizes of the 2-part DBN including O and A. In comparison, 2-level hierarchical and factorial controllers with | N top | = | N base of the hierarchy in each2The first hierarchy of the E level is not smaller than the space, as it is imposed on the level 1."}, {"heading": "5 Experiments", "text": "In fact, the problems encountered in the first three months of this year are the same as those encountered in the first three months of this year, in which there has been a tightening of controls. (...) In the second half of this decade, there has been a tightening of controls. (...) In the third half of this decade, there has been a tightening of controls. (...) In the second half of this decade, there will be a tightening of controls. (...) In the second half of this decade, there will be a tightening of controls. (...) In the second half of this decade, there will be a tightening of controls. (...) In the second half, there will be a tightening of controls. (...) In the third half, there will be a tightening of controls. (...) In the second half, there will be a tightening of controls. (...) In the second half, there will be a tightening of controls. (...) In the third half, there will be a tightening of controls. (...) In the second half, there will be a tightening of controls. (...) In the second half, there will be a tightening of controls. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S., in the third half of this decade, there will be a tightening of controls. \""}, {"heading": "6 Conclusion", "text": "This facilitates the discovery of hierarchy when the hierarchical structure of the controller is encoded in a corresponding dynamic Bavarian network (DBN). However, our complexity analysis and empirical runtime analysis confirm the favorable scaling. In particular, we have solved problems such as hand washing and the cheese taxi, which could not be solved with the previous approaches in [4]. Compared to flat controllers, factor controllers are faster to optimize and less sensitive to local optimizations when they have many nodes. Our current implementation does not use factor structures in the state, action and observation space, but we imagine that a factor structure would scale to large factor structures POMDPs. For the chain of chains problem, the maximum probability that a valid hierarchy arises."}, {"heading": "Acknowledgments", "text": "Toussaint acknowledges the support of the German Research Foundation (DFG), EmmyNoether Fellowship TO 409 / 1-3. Poupart and Charlin were supported by grants from the Natural Sciences and Engi-neering Research Council of Canada, the Canada Founda for Innovation and the Ontario Innovation Trust."}], "references": [{"title": "Solving POMDPs using quadratically constrained linear programs", "author": ["C. Amato", "D. Bernstein", "S. Zilberstein"], "venue": "In IJCAI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Stochastic local search for POMDP controllers", "author": ["D. Braziunas", "C. Boutilier"], "venue": "In AAAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Exact and approximate algorithms for partially observable Markov decision processes", "author": ["A. Cassandra"], "venue": "PhD thesis, Brown University, Dept. of Computer Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Automated hierarchy discovery for planning in par-  tially observable environments", "author": ["L. Charlin", "P. Poupart", "R. Shioda"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1977}, {"title": "An improved policy iteration algorithm for partially observable MDPs", "author": ["E. Hansen"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Synthesis of hierarchical finite-state controllers for POMDPs", "author": ["E. Hansen", "R. Zhou"], "venue": "In ICAPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Assisting persons with dementia during handwashing using a partially observable Markov decision process", "author": ["J. Hoey", "A. von Bertoldi", "P. Poupart", "A. Mihailidis"], "venue": "ICVS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "Learning finite-state controllers for partially observable environments", "author": ["N. Meuleau", "L. Peshkin", "K.-E. Kim", "L. Kaelbling"], "venue": "In UAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Linear time inference in hierarchical HMMs", "author": ["K. Murphy", "M. Paskin"], "venue": "In NIPS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "author": ["R. Neal", "G. Hinton"], "venue": "Learning in Graphical Models. Kluwer,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Tractable Planning Under Uncertainty: Exploiting Structure", "author": ["J. Pineau"], "venue": "PhD thesis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Policycontingent abstraction for robust robot control", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "In UAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Bounded finite state controllers", "author": ["P. Poupart", "C. Boutilier"], "venue": "In NIPS,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Heuristic search value iteration for POMDPs", "author": ["T. Smith", "R. Simmons"], "venue": "In UAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "A layered approach to learning client behaviors in the RoboCup soccer server", "author": ["P. Stone", "M. Veloso"], "venue": "Applied Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Representing hierarchical POMDPs as DBNs for multi-scale robot localization", "author": ["G. Theocharous", "K. Murphy", "L. Pack Kaelbling"], "venue": "In ICRA,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "Probabilistic inference for solving (PO)MDPs", "author": ["M. Toussaint", "S. Harmeling", "A. Storkey"], "venue": "Technical Report EDI-INF-RR-0934, School of Informatics, University of Edinburgh,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}], "referenceMentions": [{"referenceID": 3, "context": "[4] recently showed that the hierarchy discovery problem can be framed as a non-convex optimization problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] developed a method to solve planning problems by maximumlikelihood estimation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": ", handwashing [8]) can be naturally decomposed into subtasks for each step of an activity.", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "When a decomposition or hierarchy is known a priori, several approaches have demonstrated that planning can be simplified and performed faster [13, 7].", "startOffset": 143, "endOffset": 150}, {"referenceID": 6, "context": "When a decomposition or hierarchy is known a priori, several approaches have demonstrated that planning can be simplified and performed faster [13, 7].", "startOffset": 143, "endOffset": 150}, {"referenceID": 3, "context": "[4] showed how a hierarchy can be discovered automatically by formulating the planning problem as a non-convex quartically constrained optimization problem with variables corresponding to the parameters of the policy, including its hierarchical structure.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "The approach combines Murphy and Paskin\u2019s [10] factored encoding of hierarchical structures (see also [17]) into a dynamic Bayesian network (DBN) with Toussaint et al.", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "The approach combines Murphy and Paskin\u2019s [10] factored encoding of hierarchical structures (see also [17]) into a dynamic Bayesian network (DBN) with Toussaint et al.", "startOffset": 102, "endOffset": 106}, {"referenceID": 17, "context": "\u2019s [18] maximum-likelihood estimation technique for policy optimization.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "The policy and hierarchy parameters are optimized with the expectationmaximization (EM) algorithm [5].", "startOffset": 98, "endOffset": 101}, {"referenceID": 5, "context": "A convenient representation for an important class of policies consists of finite state controllers [6].", "startOffset": 100, "endOffset": 103}, {"referenceID": 8, "context": "Several techniques have been proposed to optimize controllers of a given size, including gradient ascent [9], stochastic local search [2], bounded policy iteration [14], non-convex quadratically constrained optimization [1] and likelihood maximization [18].", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "Several techniques have been proposed to optimize controllers of a given size, including gradient ascent [9], stochastic local search [2], bounded policy iteration [14], non-convex quadratically constrained optimization [1] and likelihood maximization [18].", "startOffset": 134, "endOffset": 137}, {"referenceID": 13, "context": "Several techniques have been proposed to optimize controllers of a given size, including gradient ascent [9], stochastic local search [2], bounded policy iteration [14], non-convex quadratically constrained optimization [1] and likelihood maximization [18].", "startOffset": 164, "endOffset": 168}, {"referenceID": 0, "context": "Several techniques have been proposed to optimize controllers of a given size, including gradient ascent [9], stochastic local search [2], bounded policy iteration [14], non-convex quadratically constrained optimization [1] and likelihood maximization [18].", "startOffset": 220, "endOffset": 223}, {"referenceID": 17, "context": "Several techniques have been proposed to optimize controllers of a given size, including gradient ascent [9], stochastic local search [2], bounded policy iteration [14], non-convex quadratically constrained optimization [1] and likelihood maximization [18].", "startOffset": 252, "endOffset": 256}, {"referenceID": 17, "context": "[18] recently proposed to convert POMDPs into equivalent dynamic Bayesian networks (DBNs) by normalizing the rewards and to optimize a policy by maximizing the likelihood of the normalized rewards.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] sped up planning by exploiting a user specified action hierarchy.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7] proposed hierarchical controllers and an alternative planning technique that also exploits a user specified hierarchy.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] proposed recursive controllers (which subsume hierarchical controllers) and an approach that discovers the hierarchy while optimizing a controller.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "In another line of research, Murphy and Paskin [10] proposed to model hierarchical hidden Markov models (HMMs) with a dynamic Bayesian network (DBN).", "startOffset": 47, "endOffset": 51}, {"referenceID": 16, "context": "[17] also used DBNs to model hierarchical POMDPs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "A recursive controller [4] consists of a recursive automaton with concrete nodes n and abstract nodes n\u0304.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "[4] show that optimizing a recursive controller with a fixed number of concrete and abstract nodes can be framed as a non-convex quartically constrained optimization problem.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "The pa|n and pn\u2032|no\u2032 distributions are combined in one distribution pn\u2032a|no\u2032 in [14]", "startOffset": 80, "endOffset": 84}, {"referenceID": 9, "context": "Murphy and Paskin [10] proposed to model hierarchical hidden Markov models (HMMs) as dynamic Bayesian networks (DBNs).", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "[10] and Toussaint et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] to convert a POMDP with a hierarchical controller into a mixture of DBNs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "\u2019s technique [18], we optimize a factored controller by maximizing the reward likelihood.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "The mixture components are a uniform distribution, a random distribution U\u03c6(n\u2032l) (an array of uniform random numbers in [0, 1]), and a term enforcing n to stay unchanged.", "startOffset": 120, "endOffset": 126}, {"referenceID": 10, "context": "EM variants with certain types of partial E-steps ensure monotonic improvement of the likelihood when the hidden variables are independent [11].", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": "We first compared the performance of the maximum likelihood (ML) approach to previous optimizationbased approaches from [4].", "startOffset": 120, "endOffset": 123}, {"referenceID": 3, "context": "The problems include paint, shuttle and 4x4 maze (previously used in [4]) and three additional problems: chain-of-chains (described below), hand-washing (reduced version from [8]) and cheese-taxi (variant from [12]).", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "The problems include paint, shuttle and 4x4 maze (previously used in [4]) and three additional problems: chain-of-chains (described below), hand-washing (reduced version from [8]) and cheese-taxi (variant from [12]).", "startOffset": 175, "endOffset": 178}, {"referenceID": 11, "context": "The problems include paint, shuttle and 4x4 maze (previously used in [4]) and three additional problems: chain-of-chains (described below), hand-washing (reduced version from [8]) and cheese-taxi (variant from [12]).", "startOffset": 210, "endOffset": 214}, {"referenceID": 2, "context": "Table 2: V \u2217 denotes optimal values (with truncated trajectories) [3] except for handwashing and cheese-taxi where we show the optimal value of the equivalent fully-observable problem.", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "Problem |S|, |A|, |O| V \u2217 HSVI2 Best results from [4] ML approach (avg.", "startOffset": 50, "endOffset": 53}, {"referenceID": 14, "context": "iteration method (HSVI2 [15]).", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": "In particular, we solved problems like handwashing and cheese-taxi that could not be solved with the previous approaches in [4].", "startOffset": 124, "endOffset": 127}], "year": 2008, "abstractText": "Planning can often be simplified by decomposing the task into smaller tasks arranged hierarchically. Charlin et al. [4] recently showed that the hierarchy discovery problem can be framed as a non-convex optimization problem. However, the inherent computational difficulty of solving such an optimization problem makes it hard to scale to realworld problems. In another line of research, Toussaint et al. [18] developed a method to solve planning problems by maximumlikelihood estimation. In this paper, we show how the hierarchy discovery problem in partially observable domains can be tackled using a similar maximum likelihood approach. Our technique first transforms the problem into a dynamic Bayesian network through which a hierarchical structure can naturally be discovered while optimizing the policy. Experimental results demonstrate that this approach scales better than previous techniques based on non-convex optimization.", "creator": "TeX"}}}