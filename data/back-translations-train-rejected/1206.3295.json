{"id": "1206.3295", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Refractor Importance Sampling", "abstract": "In this paper we introduce Refractor Importance Sampling (RIS), an improvement to reduce error variance in Bayesian network importance sampling propagation under evidential reasoning. We prove the existence of a collection of importance functions that are close to the optimal importance function under evidential reasoning. Based on this theoretic result we derive the RIS algorithm. RIS approaches the optimal importance function by applying localized arc changes to minimize the divergence between the evidence-adjusted importance function and the optimal importance function. The validity and performance of RIS is empirically tested with a large setof synthetic Bayesian networks and two real-world networks.", "histories": [["v1", "Wed, 13 Jun 2012 15:53:49 GMT  (295kb)", "http://arxiv.org/abs/1206.3295v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["haohai yu", "robert a van engelen"], "accepted": false, "id": "1206.3295"}, "pdf": {"name": "1206.3295.pdf", "metadata": {"source": "CRF", "title": "Refractor Importance Sampling", "authors": ["Haohai Yu"], "emails": ["hyu@cs.fsu.edu", "engelen@cs.fsu.edu"], "sections": [{"heading": null, "text": "In this paper, we present Refractor Importance Sampling (RIS), an improvement to reduce error variance in Bayesian network meaning sampling propagation from an evident point of view. We demonstrate the existence of a collection of importance functions that come close to the optimal importance function from an evident point of view. Based on this theoretical result, we derive the RIS algorithm. RIS approaches the optimal importance function by applying localized arc changes to minimize the divergence between the evidence-adjusted importance function and the optimal importance function. The validity and performance of the RIS is tested empirically with a large number of synthetic Bayesian networks and two real-world networks."}, {"heading": "1 Introduction", "text": "In fact, it is so that most of them are able to survive themselves by blaming themselves and others. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. \""}, {"heading": "2 Importance Function Divergence", "text": "In this section we first give BN definitions and briefly an overview of the importance sampling. Then we give a CLL divergence below the limit of the importance sample error variance. We prove the existence of a collection of importance functions that approximate the optimal importance sample function by adjusting both the quantitative and the qualitative components of a BN under dynamic updating with evidence."}, {"heading": "2.1 Definitions", "text": "The following definitions and notations are used. Def. 1 A Bayesian network BN = (G, Pr) is a DAG G = (V, A) with vertices V and arcs A, A, V \u00b7 V. Pr is the common probability distribution over the discrete random variables (vertices) V defined by Pr (V) = VP Pr (V). The theorem of the parents of a vertex V is \u03c0 (V). The conditional probability tables (CPT) of the BN assign Pr (V) for all V (V) values. Graph G induces the d separation criterion [Pearl, 1988], denoted by < X, Y | Z >, implying that X and Y are conditionally independent in Pr (V)."}, {"heading": "2.2 Importance Sampling", "text": "The meaning of the sampling is an MC method to improve the convergence rate and reduce the error variance with probability density functions. Let g (X) be a function of the m variable X = {X1,.., Xm} over the domain # IRm, so that the calculation g (X) is feasible for all X. Let us consider the problem of approximating the m variable X = \"BX\" using a sampling method. To achieve a minimum error variance, we must rewrite this problem by f (X) f (X) dX) dX, where f (X) is a probability density that refers to the importance of the function. To achieve a minimum error variance that is equal. (X)"}, {"heading": "2.3 KL-Divergence Bounds", "text": "E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E E"}, {"heading": "3 Refractor Importance Sampling", "text": "The RIS algorithm modifies the BN structure according to the shield \u03b1e (X) for the vertices X-An (E) by extending the parent set of X and adapting the CPT accordingly. Visually in the diagram, RIS breaks arcs from the evidence vertices, which inspired the choice of the name for the method. In this section, the algorithms and the general procedure of the RIS are presented."}, {"heading": "3.1 Computing the Shield", "text": "Alg. 1 calculates \u03b1e (X) in O (| A |) worst-case time, assuming that An (\u00b7) is determined in unit of time (e.g. by means of a reference table). The function topSort\u03b4 sorts the quantity Ah (X) topologically according to the topological order \u03b4 over V of the BN. Note that the shield \u03b1e (X) can be calculated in advance for each X-V given evidence node E."}, {"heading": "3.2 Refractor Procedure", "text": "Alg. 2 modifies the graphical structure of BN. The temporal complexity of this algorithm is O (| V | | A |) if | E | | V |, otherwise O (| V | 2 | A |). The CPT of a vertex X is updated by filling in the extended entries \u03b1e (X)\\ {X} using sample data (described in Section 3.4). Fig. 1 shows an example of a refractory BN using Alg. 2. E is the node of proof. Here \u03b1e (C) = {A} and \u03b1e (B) = {A} are added. Arcs A \u2192 B and A \u2192 C are added. Note that the arc A \u2192 B adapts to the fact that the relationship of influence between A and B has changed due to evidence. Arc E \u2192 D is no longer required and can be removed as in [van der Gaag, 1996]. Input: BN = (G, Pr), evidence for recorder V."}, {"heading": "3.3 General RIS Procedure", "text": "The RIS uses both the qualitative and quantitative properties of a BN to approach the optimal importance function. The general procedure of the RIS is: 1. The structure of the BN is modified by algorithms 1 and 2. CPTs of (a subset of) ancestral citations of evidence points are expanded. 2. Update the CPT values by a specific learning algorithm (see Section 3.4 for details)."}, {"heading": "3.4 Variations of RIS", "text": "Step 1 significantly modifies the BN structure, especially if the evidence object nodes of the ancestors are large, e.g. if evidence object nodes are leaves. This increases the complexity of the BN. However, the effect of evidence on other nodes is diminished if the path length between the evidence object nodes and the vertices is increased [Henrion, 1989]. Thus, instead of modifying all ancestors on (E) of the evidence object nodes in step 1, it is generally sufficient to select a subset of ancestors such as the combined parent set \u03c0 (E). Steps 1 and 2 are independent, since any important functional learning algorithm can be applied in step 2. Steps 2 and 3 can be combined using the same meaning sampling algorithm for learning and inference. In our experiments, we used SIS and AISBN both for learning and for inference (steps 2 and 3), which are referred to as RISSIS and RISAIS respectively."}, {"heading": "4 Results", "text": "This section presents the experimental results of RISSIS and RISAIS in comparison to SIS and AIS for synthetic networks and two real networks."}, {"heading": "4.1 Measurement", "text": "The mean squared error (MSE) metric was used to measure the error of the importance of the sample results compared to the exact solution: MSE = \u221a \u221a 1 \u2211 Xi-X ni \u2211 Xi-X ni \u2211 j = 1 (Pr \u2032 e (xij) \u2212 Pre (xij)) 2, where X = V\\ E. We also measured the CLL divergence of approximate and exact posterior probability distributions: KL divergence = \u2211 Cfg (X) Pre (x) ln Pre (x) Pr \u2032 e (x).Remember that theorem 1 indicates a lower limit for the CLL divergence of the posterior probability distributions of SIS and AIS, which indicates the lower limit of Eq. (1) in the results of the PostKLD. The number of samples is used as a measure of the runtime of the AIS time instead of the CPU time in our experimental implementation."}, {"heading": "4.2 Test Cases", "text": "Since the calculation of MSEs is expensive and the PostKLD is exponential in the number of corners, small format synthetic BNs with random variables with two or three states and | V | = 20 corners and | A | = 30 arcs were evaluated in our experiments. CPT for each variable was randomly generated with a uniform distribution for the probability interval [0.1, 0.9] with bias for the extreme probabilities in intervals (0, 0.1) and (0.9, 1). For the experiments, we generated 100 different synthetic BNs with these properties. We also verified the RIS with two real BNs: Alarm-37 [Beinlich et al., 1989] and HeparII-70 [Onisko, 2003]. The probability distributions of these networks are more extreme compared to synthetic BNs. For each of the two BNs, 20 sets of proofs are randomly selected, each with 10 variables of proof."}, {"heading": "4.3 Results for Synthetic Test Cases", "text": "We compared the MSE of four algorithms, AIS, RISAIS, SIS and RISSIS. For this comparison, a selection of 21 BNs from the generated synthetic test case suite was made, while the other 79 test cases have PostKLD \u2264 0.1, which means that the RIS advantage is limited."}, {"heading": "4.4 Results for Alarm-37 and HeparII-70", "text": "Fig. 5 shows the results for Alarm-37 and HeparII70, where the sample frequency is varied from 1,000 to 19,000 in steps of 1,000. The dark column in the figures represents the ratio of the lowest MSE cases for RISAIS versus AIS and RISSIS versus SIS. A ratio of 50% or higher indicates that the RIS algorithm has a lower error variance than the non-RIS algorithm. In RISAIS, this is the case for all but one of the 19 measurements. Overall, the MSE is the lowest for RISAIS with 56.7% on average of all samples. In RISSIS, this is the case for all 19 measurements. Overall, the MSE is the lowest for RISSIS with 60.3% on average of all measurements. The combined results show that the RIS algorithms have reduced the error variance for the synthetic networks and the two real networks."}, {"heading": "5 Conclusions", "text": "To achieve the optimal importance function for the propagation of meaning tests with a Bayesian network, a modification of the network structure is necessary to eliminate the lower limit of error variance. To this end, the proposed RIS algorithms will refract the network and adjust the conditional probability tables to minimize divergence to the optimal importance function. To this end, the validity and performance of the RIS approach has been empirically tested using a series of synthetic networks and two real networks. Further improvements to the RIS are possible to achieve a better accuracy / cost ratio, the goal being to find an effective subset of the full screen size of an ancestor point of an evidence point, or to select a limited subset of the ancestors of evidence points that are fireproof. Also, some of the additional arcs introduced could be removed using the arc removal algorithm if they exhibit a weak influence of current strategies and higher complexity of the networks."}, {"heading": "A Proof of Theorem 1", "text": "The KL-Divergence = E1 (V) Pr2 (V) Pr2 (V) Pr1 (v) Pr1 (v) Pr1 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr1 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pre (x) Pre (x) Pre (x) Pre (x) Pre (x) Pre (x) Pre (x) Pre (e) (e) (e) (e) (e) (e) (e) (e) (e) (e) Pre (x) Pre (e) (e) (x) Pre (e) (e) (e) (e) (e) (e) (e) Pre (x) Pr (e) Pre (e) (e) (e) (e) Pre (x) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pr2 (v) Pre (v) Pre (x) Pre (x) Pre (x) Pre (x) Pr2 (x) Pre (x) Pre (x) Pre (x) Pre (x) Pre (x) Pre (x) Pre (x) Pre (x) Pre (e) Pre (e) (e) (e) (e) (e) (e) (e) (e (e) (e) (e) (e) (e) Pre (x (e) (e) (e"}, {"heading": "B Proof of Corollary 1", "text": "Proof: Let X = V\\ E, then Cfg (V\\ E) Pre (v\\ e) ln Pre (v\\ e) Pr'e (v\\ e) ln Pr (x | e) ln Pr (x | e) Pr (x | e) Pr (x) x (x) x Cfg (X) Pr (x | e) ln (xj | e) ln (E) x Pr (V\\ E))), Pr (xj | e) Pr (ei (ei)) Pr (e) x Pr (e) e (xj | e).Since \u03c0 (E) x Pr (V\\ E) = Xj (xj | E), Pr (xj | E), e) = Pr (xj | e), e (Pr (xj | e)."}, {"heading": "C Proof of Lemma 2", "text": "Prove that Xk, Ah (Xj), \u03b2e (Xj), exist the following three cases. Case 1: If a path Xk, E exists, then we must show that this path is d-separated by \u03b1e, Xj, d-separated. There are two possibilities: First, Xk, E bypasses Xj so that it must pass one of the parents of Xj. Then we must show that this path (Xj) d-separated the path. Case 2: If paths N, Xk and N, E exist, the path must pass through \u03b1e (Xj), so (Xj), so (Xj) d-separated the path. Case 2: If paths N, Xk and N \u2192 E exist, then N, Ah (Xj) and N d-separated the Xk and E, so (Xj), so (Xj), so (Xj), Xj (Xj) d-separated the path."}], "references": [{"title": "The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks", "author": ["Beinlich et al", "I. 1989] Beinlich", "G. Suermondt", "R. Chavez", "G. Cooper"], "venue": null, "citeRegEx": "al. et al\\.,? \\Q1989\\E", "shortCiteRegEx": "al. et al\\.", "year": 1989}, {"title": "A randomized approximation algorithm for probabilistic inference on Bayesian belief networks. Networks, 20:661\u2013685", "author": ["Chavez", "Cooper", "R.M. 1990] Chavez", "G.F. Cooper"], "venue": null, "citeRegEx": "Chavez et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Chavez et al\\.", "year": 1990}, {"title": "AIS-BN: An adaptive importance sampling algorithm for evidential reasoning in large Bayesian networks", "author": ["Cheng", "Druzdzel", "J. 2000a] Cheng", "M.J. Druzdzel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cheng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2000}, {"title": "Computational investigations of low-discrepancy sequences in simulation algorithms for Bayesian networks", "author": ["Cheng", "Druzdzel", "J. 2000b] Cheng", "M.J. Druzdzel"], "venue": "In Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Cheng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2000}, {"title": "Latin hypercube sampling in Bayesian networks", "author": ["Cheng", "Druzdzel", "J. 2000c] Cheng", "M.J. Druzdzel"], "venue": "In Proceedings of the 13th International Florida Artificial Intelligence Research Symposium Conference", "citeRegEx": "Cheng et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2000}, {"title": "Approximating probabilistic inference in Bayesian belief networks is NP-hard", "author": ["Dagum", "Luby", "P. 1993] Dagum", "M. Luby"], "venue": "Artificial Intelligence,", "citeRegEx": "Dagum et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Dagum et al\\.", "year": 1993}, {"title": "Weighting and integrating evidence for stochastic simulation in Bayesian networks", "author": ["Fung", "Chang", "R. 1989] Fung", "K.C. Chang"], "venue": "In Proceedings of the 5th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Fung et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Fung et al\\.", "year": 1989}, {"title": "A survey of research in deliberative real-time artificial intelligence. Real-Time Systems, 6(3):317\u2013347", "author": ["Garvey", "Lesser", "A.J. 1994] Garvey", "V.R. Lesser"], "venue": null, "citeRegEx": "Garvey et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Garvey et al\\.", "year": 1994}, {"title": "Stochastic relaxation, Gibbs distribution and the Bayesian restoration of images", "author": ["Geman", "S. 1984] Geman", "D. Geman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Geman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Geman et al\\.", "year": 1984}, {"title": "A survey on algorithms for real-time Bayesian network inference. In In the joint AAAI-02/KDD-02/UAI02 workshop on Real-Time Decision Support and Diagnosis", "author": ["Guo", "Hsu", "H. 2002] Guo", "W. Hsu"], "venue": null, "citeRegEx": "Guo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2002}, {"title": "Artificial intelligence: A modern approach", "author": ["Russell", "Norvig", "S. 1995] Russell", "P. Norvig"], "venue": null, "citeRegEx": "Russell et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Russell et al\\.", "year": 1995}, {"title": "On a distributed anytime architecture for probabilistic reasoning", "author": ["Santos et al", "E.J. 1995] Santos", "S.E. Shimony", "E. Solomon", "E. Williams"], "venue": "Technical report,", "citeRegEx": "al. et al\\.,? \\Q1995\\E", "shortCiteRegEx": "al. et al\\.", "year": 1995}, {"title": "Simulation approaches to general probabilistic inference on belief networks", "author": ["Shachter", "Peot", "R.D. 1990] Shachter", "M.A. Peot"], "venue": "In Proceedings of the 5th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Shachter et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Shachter et al\\.", "year": 1990}, {"title": "On evidence absorption for belief networks", "author": ["van der Gaag", "L. 1996] van der Gaag"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "Gaag and Gaag,? \\Q1996\\E", "shortCiteRegEx": "Gaag and Gaag", "year": 1996}, {"title": "Approximating Bayesian belief networks by arc removal", "author": ["van Engelen", "R. 1997] van Engelen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Engelen and Engelen,? \\Q1997\\E", "shortCiteRegEx": "Engelen and Engelen", "year": 1997}], "referenceMentions": [], "year": 2008, "abstractText": "In this paper we introduce Refractor Importance Sampling (RIS), an improvement to reduce error variance in Bayesian network importance sampling propagation under evidential reasoning. We prove the existence of a collection of importance functions that are close to the optimal importance function under evidential reasoning. Based on this theoretic result we derive the RIS algorithm. RIS approaches the optimal importance function by applying localized arc changes to minimize the divergence between the evidence-adjusted importance function and the optimal importance function. The validity and performance of RIS is empirically tested with a large set of synthetic Bayesian networks and two realworld networks.", "creator": "TeX"}}}