{"id": "1206.3296", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Jun-2012", "title": "Inference for Multiplicative Models", "abstract": "The paper introduces a generalization for known probabilistic models such as log-linear and graphical models, called here multiplicative models. These models, that express probabilities via product of parameters are shown to capture multiple forms of contextual independence between variables, including decision graphs and noisy-OR functions. An inference algorithm for multiplicative models is provided and its correctness is proved. The complexity analysis of the inference algorithm uses a more refined parameter than the tree-width of the underlying graph, and shows the computational cost does not exceed that of the variable elimination algorithm in graphical models. The paper ends with examples where using the new models and algorithm is computationally beneficial.", "histories": [["v1", "Wed, 13 Jun 2012 15:55:04 GMT  (407kb)", "http://arxiv.org/abs/1206.3296v1", "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI2008)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["ydo wexler", "christopher meek"], "accepted": false, "id": "1206.3296"}, "pdf": {"name": "1206.3296.pdf", "metadata": {"source": "CRF", "title": "Inference for Multiplicative Models", "authors": ["Ydo Wexler", "Christopher Meek"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, it is not the case that the individual models that we have developed in recent years are just a set of categorical variables without an answer to that question being questioned. [1] It is not as if the way in which we have asked them is an answer to the question. [2] It is as if the way in which we have asked them leads to the way in which they are able to be presented, the way in which they are presented, the way in which they are presented, the way in which they are presented, the way in which they are presented. [3] It is not as if the way in which they are presented, the way in which they are presented, the way in which they are presented. \""}, {"heading": "2 Multiplicative models", "text": "We propose a series of variables D = 1 that adapt to the product. (D) We propose a generalization of variables D = 1. (D) We propose a generalization of variables. (D) We propose a generalization of variables. (D) We propose a generalization of variables. (D) We propose a generalization of variables. (D) We propose a generalization of variables. (D) We propose a generalization of variables. (D) We propose a generalization of variables and random variables. (D) We propose a generalization of variables. (D) We propose a generalization of variables. (D) We propose a generalization of variables. (D) We propose a generalization of variables. (D) We propose a generalization of variables. (D) We propose a generalization of variables. (D) We propose a generalization of variables."}, {"heading": "2.1 Log-linear models", "text": "Log-linear models are usually used to analyze categorical data, and represent a direct generalization of undirected graphical models. These models, which have been widely used for statistical analysis over the past four decades, describe the association patterns between a set of categorical variables without specifying a variable as a response variable (dependent) variable, and treat all variables symmetrically [1]. Formally, a log-linear model defines the natural protocol of the expected frequency of values d for a set of variables D as a linear combination of the main effect \"Vivi\" of each variable \"Vi\" D, \"and if | D | > 1 interaction effects \u03bbSs of each subset of variables\" S \"D,\" in which the instances \"s are consistent with\" d. \"For example, suppose that we want to examine relationships between three categorical variables\" A, B, and C, \"then the full linear model becomes\" S \"(Fsib)."}, {"heading": "2.2 Context-specific independence", "text": "With the introduction of graphical models and in particular Bayesian nets (BNs) = Y = Y = Y = Y = Y = Y = Y = Y = Z = Z = Z = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S = S ="}, {"heading": "2.2.1 Positive models", "text": "The representation of dependence on variables using loglinear models has some desirable properties, such as that they are general and at the same time ensure the existence of a maximum probability without dependencies being strictly positive. However, in the representation discussed in Section 2.1, log-linear models use more parameters than necessary [3, 13]. For example, take the log-linear model for two binary variables A and B. Assuming all possible effects, the corresponding log-linear model uses eight parameters instead of the four parameters in a standard representation as a complete table: \u03bbA0, \u03bb A 1, \u03bb B 1, \u03bb B 1, \u03bb AB 00, \u03bb AB 01, \u03bb AB 10, \u03bb AB 11. Another representation of log-linear models that takes these redundancies into account only uses parameters that include a non-zero instantiation of variables [10]."}, {"heading": "2.2.2 Decision trees and graphs as multiplicative models", "text": "Common structures for the representation of functions with contextual independence are Decision Trees (DTs) and Decision Graphs (DGs) (DGs) [22, 18]. These structures capture contextual dependencies that are the result of repetitive values such as those in Eq. 2. Several studies have used Decision Trees to expand the instance in graphical models [2, 20]. We show how DTs and DGs fall into the category of multiplicative models. For a function (D) over a series of variables D, a Decision Tree T representing the instance (D) is a tree with variables from D on internal nodes and values from D on the sheets. Each edge from a variable V to a child in T corresponds to a different set of values H dom (V), and can be presented as a set of clauses representing a decision."}, {"heading": "3 Inference for multiplicative models", "text": "\"Consider a model that defines the probability distribution P (x) =\" i \"(di), with sets Di =\" Xi1 \"(Ximi), and multiplicative models P =\" i. \"We will first show how to calculate the probability of a set of query variables Q using a multiplicative model. In particular, we will draw conclusions for a multiplicative model using the variable elimination scheme (Zhang & Poole 1996 [24], Dechter 1999 [5], originally proposed for inference in BNs. Then we will check the correctness of the algorithm and analyze its time complexity. We will define an operationM (V, {II}) that gives a variable V (X) and a series of models (i)."}, {"heading": "3.1 Correctness of the inference procedure", "text": "We prove the correctness of the algorithm 1 by showing that the algorithm maintains the property that according to the iteration above the set of variables U, the models 1, 2, 3, 4, 4, 5, 5, 5, 5, 5, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,"}, {"heading": "3.2 Incorporating evidence", "text": "In many practical scenarios we observe the value of some of the variables in the model and want to incorporate these proofs. Multiplicative models allow us to do so in the most natural way. Let's look at a series of evidence nodes E, for which we have observed the values E = e and a multiplicative model \u03c1 = {S\u03c1, \u0435\u043c}. To include the proofs in \u03c1, we adjust the elements in S\u03c1 by s = s, V = v, where v is the projection of e on the variable V-E. Then we remove any element that is not consistent with the proofs, s = E."}, {"heading": "3.3 Complexity of inference", "text": "It is known that the complexity of the conclusion in graphical models is NP-hard and its costs exponentially in the tree width of the underlying graph [4]. We analyze the temporal complexity of the inference method for multiplicative models according to algorithm 1. As a by-product, we refine the standard complexity and provide a new complexity limit based on the representation used. It can then be said that the complexity of the problem is the minimum complexity among all possible representations."}, {"heading": "3.3.1 Diameter of multiplicative models", "text": "The structure of a multiplicative model determines the amount of calculations necessary to obtain the value of a single instances of values in a series of variables. Although, at first glance, it appears that the real number of operations can be drastically lower and we provide it with a function (D), the number of operations necessary to obtain the values of all instances D = d is altogether due to a sum of D = d. The total number of operations can be drastically lower, and we refer to them by hierarchical models in which an element s is not in the structure of the model, then all elements s \"s\" are also not included in the model, Good (1963) offers a method that calculates all these values in time. We refer to the ratio between the number of calculations and the number of elements in S, which is the size of the model, by diam diam. \""}, {"heading": "3.4 Benefits of inference for multiplicative models", "text": "Different multiplicative models capture different contextual independence and therefore specify different number of parameters. For example, take the function of four binary variables A, B, C, D with values according to the table in Figure 1. The structure of the corresponding decision tree model contains six elements, while the structure of the corresponding positive model contains eight elements. In the latter model, the CSI captured in the decision tree affects the runtime of the inference algorithm and returns the value of the proposed inference algorithm independently of B, since A, C, and D are set to one, with no effect. This variation and the structure of the model affect the runtime of the inference algorithm. An example where there are significant computer savings when the proposed inference algorithm is used can be found in a model such as the QMR-DT network [17], which consists of noisy-OR functions mentioned in Section 2.2."}], "references": [{"title": "Discrete multivariate analysis", "author": ["Y. Bishop", "E. Fienberg", "P. Holland"], "venue": "MIT Press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1975}, {"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Log-Linear Models and Logistic Regression", "author": ["R. Christensen"], "venue": "Springer", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "The computational complexity of probabilistic inference using bayesian belief networks", "author": ["G. Cooper"], "venue": "Artificial Intelligence, 42:393\u2013405", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artificial Intelligence, 113:41\u201385", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Statistical Methods for Research Workers", "author": ["R. Fisher"], "venue": "Macmillan Pub Co", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1925}, {"title": "Statistical Methods and Scientific Inference", "author": ["R. Fisher"], "venue": "Oliver and Boyd", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1956}, {"title": "Contingent influence diagrams", "author": ["R. Fung", "R. Shachter"], "venue": "Working Paper, Dept. of Engineering- Economic Systems, Stanford University", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1990}, {"title": "Knowledge representation and inference in similarity networks and bayesian multinets", "author": ["D. Geiger", "D. Heckerman"], "venue": "Artificial Intelligence, 82:45\u201374", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Maximum entropy for hypothesis formulation", "author": ["I. Good"], "venue": "especially for multidimensional contingency tables. The Annals of Math. Stat., 34:911\u2013934", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1963}, {"title": "A tractable inference algorithm for diagnosing multiple diseases", "author": ["D. Heckerman"], "venue": "UAI, 230:362\u2013367", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1989}, {"title": "Probabilistic Similarity Networks", "author": ["D. Heckerman"], "venue": "MIT Press", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1991}, {"title": "Log-Linear Models", "author": ["D. Knoke", "P. Burke"], "venue": "Sage Publications Inc", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1980}, {"title": "Factor graphs and the sum-product algorithm", "author": ["F.R. Kschischang", "B. Frey", "H. Loeliger"], "venue": "IEEE Trans. Inform. Theory, 47(2):498\u2013519", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Graphical Models", "author": ["S. Lauritzen"], "venue": "Oxford University Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1996}, {"title": "Conditional independence and log linear models for multi-dimensional contingency tables", "author": ["J. Lindsey"], "venue": "Quality and Quantity, 8(4):377\u2013390", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1974}, {"title": "Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base: Part II. Evaluation of diagnostic performance", "author": ["B. Middleton"], "venue": "SIAM Journal on Computing,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1991}, {"title": "Decision graphs - an extension of decision trees", "author": ["J.J. Oliver"], "venue": "Proceedings of the Fourth International Workshop on Artificial Intelligence and Statistics, pages 343\u2013350", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1993}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1988}, {"title": "Exploiting contextual independence in probabilistic inference", "author": ["D. Poole", "N. Zhang"], "venue": "JAIR, 18:263\u2013313", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Some generalized order-disorder transformations", "author": ["R. Potts"], "venue": "Proceedings of the Cambridge Philosophical Society, 48:106\u2013109", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1952}, {"title": "A survey of decision tree classifier methodology", "author": ["S. Safavian", "D. Landgrebe"], "venue": "IEEE transactions on systems, man, and cybernetics, 21(3):660\u2013674", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1991}, {"title": "Structuring conditional relationships in influence diagrams", "author": ["J. Smith", "S. Holtzman", "J. Matheson"], "venue": "Oper. Res., 41(2):280\u2013297", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1993}, {"title": "Exploiting causal independence in bayesian network inference", "author": ["N. Zhang", "D. Poole"], "venue": "JAIR, 5:301\u2013328", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 5, "context": "The statistical methods originating with the work of Fisher (1925, 1956) [6, 7] culminated in the log-linear models which describe the association patterns among a set of categorical variables without specifying any variable as a response (dependent) variable [1].", "startOffset": 73, "endOffset": 79}, {"referenceID": 6, "context": "The statistical methods originating with the work of Fisher (1925, 1956) [6, 7] culminated in the log-linear models which describe the association patterns among a set of categorical variables without specifying any variable as a response (dependent) variable [1].", "startOffset": 73, "endOffset": 79}, {"referenceID": 0, "context": "The statistical methods originating with the work of Fisher (1925, 1956) [6, 7] culminated in the log-linear models which describe the association patterns among a set of categorical variables without specifying any variable as a response (dependent) variable [1].", "startOffset": 260, "endOffset": 263}, {"referenceID": 14, "context": "A specific type of probabilistic models, probabilistic graphical models, can be visually described as an interaction graph, and embody independence assumptions in the domain of interest [15].", "startOffset": 186, "endOffset": 190}, {"referenceID": 4, "context": "Algorithms that compute the posterior distribution conditioned on evidence, called inference algorithms, exploit this structure, avoiding a direct computation of the join probabilities [5, 19].", "startOffset": 185, "endOffset": 192}, {"referenceID": 18, "context": "Algorithms that compute the posterior distribution conditioned on evidence, called inference algorithms, exploit this structure, avoiding a direct computation of the join probabilities [5, 19].", "startOffset": 185, "endOffset": 192}, {"referenceID": 14, "context": "The common distinction within graphical models is between undirected graphical models [15], a subset of log-linear models, where there are no restrictions on the functions \u03c8, and Bayesian networks (BNs) [19] in which every function is a conditional distribution \u03c8i(Di) = P (Xi|\u03a0i) where \u03a0i is the set of parent variables of Xi in the model.", "startOffset": 86, "endOffset": 90}, {"referenceID": 18, "context": "The common distinction within graphical models is between undirected graphical models [15], a subset of log-linear models, where there are no restrictions on the functions \u03c8, and Bayesian networks (BNs) [19] in which every function is a conditional distribution \u03c8i(Di) = P (Xi|\u03a0i) where \u03a0i is the set of parent variables of Xi in the model.", "startOffset": 203, "endOffset": 207}, {"referenceID": 13, "context": "Another type of probabilistic models that can be represented visually, called factor graphs, extends undirected graphical models and incorporates many of the desired properties of graphical modes [14].", "startOffset": 196, "endOffset": 200}, {"referenceID": 1, "context": "For such non-structural independences we use the name context-specific independence (CSI), which was suggested in previous studies [2, 20].", "startOffset": 131, "endOffset": 138}, {"referenceID": 19, "context": "For such non-structural independences we use the name context-specific independence (CSI), which was suggested in previous studies [2, 20].", "startOffset": 131, "endOffset": 138}, {"referenceID": 11, "context": "These include similarity networks suggested by Heckerman (1991) [12], multinets (Geiger & Heckerman 1996) [9], asymmetric influence diagrams (Fung and Shachter 1990) [8], and structured representations of the functions \u03c8 based on decision trees (Boutilier et al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 8, "context": "These include similarity networks suggested by Heckerman (1991) [12], multinets (Geiger & Heckerman 1996) [9], asymmetric influence diagrams (Fung and Shachter 1990) [8], and structured representations of the functions \u03c8 based on decision trees (Boutilier et al.", "startOffset": 106, "endOffset": 109}, {"referenceID": 7, "context": "These include similarity networks suggested by Heckerman (1991) [12], multinets (Geiger & Heckerman 1996) [9], asymmetric influence diagrams (Fung and Shachter 1990) [8], and structured representations of the functions \u03c8 based on decision trees (Boutilier et al.", "startOffset": 166, "endOffset": 169}, {"referenceID": 1, "context": "1996 [2], Poole& Zhang 2003 [20]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 19, "context": "1996 [2], Poole& Zhang 2003 [20]).", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "Quickscore algorithm by Heckerman 1989 for noisy-OR functions [11]).", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "The inference algorithm provided herein simplifies over the inference algorithm suggested by Poole & Zhang (2003) [20] when applied to Bayesian networks, by avoiding the use of tables and tables splitting operations.", "startOffset": 114, "endOffset": 118}, {"referenceID": 0, "context": "These models that have been heavily used for statistical analysis for the past four decades describe the association patterns among a set of categorical variables without specifying any variable as a response (dependent) variable, treating all variables symmetrically [1].", "startOffset": 268, "endOffset": 271}, {"referenceID": 3, "context": "With the introduction of graphical models and in particular Bayesian Networks (BNs), and the proof that inference in these models is NP-hard [4], several studies looked for further independences encoded in models that can potentially reduce the amount of work needed for inference [12, 9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 11, "context": "With the introduction of graphical models and in particular Bayesian Networks (BNs), and the proof that inference in these models is NP-hard [4], several studies looked for further independences encoded in models that can potentially reduce the amount of work needed for inference [12, 9].", "startOffset": 281, "endOffset": 288}, {"referenceID": 8, "context": "With the introduction of graphical models and in particular Bayesian Networks (BNs), and the proof that inference in these models is NP-hard [4], several studies looked for further independences encoded in models that can potentially reduce the amount of work needed for inference [12, 9].", "startOffset": 281, "endOffset": 288}, {"referenceID": 22, "context": "(1993) [23] and Boutilier et al.", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "(1996) [2].", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "These repetition which are the basis of compact representations like decision trees and graphs were exploited for inference in BN [2, 20].", "startOffset": 130, "endOffset": 137}, {"referenceID": 19, "context": "These repetition which are the basis of compact representations like decision trees and graphs were exploited for inference in BN [2, 20].", "startOffset": 130, "endOffset": 137}, {"referenceID": 2, "context": "1 the log-linear models use more parameters than necessary [3, 13].", "startOffset": 59, "endOffset": 66}, {"referenceID": 12, "context": "1 the log-linear models use more parameters than necessary [3, 13].", "startOffset": 59, "endOffset": 66}, {"referenceID": 9, "context": "Another representation of the log-linear models that accounts for these redundancies uses only parameters which involve non-zero instantiations of variables [10].", "startOffset": 157, "endOffset": 161}, {"referenceID": 15, "context": "Log-linear models, and thus positive models, are known to capture conditional and contextual independences [16].", "startOffset": 107, "endOffset": 111}, {"referenceID": 21, "context": "Common structures for representing functions with contextual independence are decision trees (DTs) and decision graphs (DGs) [22, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 17, "context": "Common structures for representing functions with contextual independence are decision trees (DTs) and decision graphs (DGs) [22, 18].", "startOffset": 125, "endOffset": 133}, {"referenceID": 1, "context": "Several studies have used decision trees to enhance inference in graphical models [2, 20].", "startOffset": 82, "endOffset": 89}, {"referenceID": 19, "context": "Several studies have used decision trees to enhance inference in graphical models [2, 20].", "startOffset": 82, "endOffset": 89}, {"referenceID": 17, "context": "One can choose to use decision graphs [18] instead of decision trees.", "startOffset": 38, "endOffset": 42}, {"referenceID": 23, "context": "In particular we perform inference for a multiplicative model via the variable elimination scheme (Zhang & Poole 1996 [24], Dechter 1999 [5]) which was originally suggested for inference in BNs.", "startOffset": 118, "endOffset": 122}, {"referenceID": 4, "context": "In particular we perform inference for a multiplicative model via the variable elimination scheme (Zhang & Poole 1996 [24], Dechter 1999 [5]) which was originally suggested for inference in BNs.", "startOffset": 137, "endOffset": 140}, {"referenceID": 4, "context": "The algorithm operates like the bucket-elimination algorithm [5], where given an order on the variables we iterate over them (Line 2), and marginalize out one variable at a time (Line 9).", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Note that for graphical models, in which the elements of Si are a mapping of instances of the functions Di, this algorithm is exactly the known variable elimination algorithm, in its implementation as bucketelimination [5], where the sets Si[j] are the tables in the bucket of the variable Xj .", "startOffset": 219, "endOffset": 222}, {"referenceID": 3, "context": "It is well known that the complexity of inference in graphical models is NP-hard and its cost exponential in the tree-width of the underlying graph [4].", "startOffset": 148, "endOffset": 151}, {"referenceID": 9, "context": "For hierarchical models, in which if an element s is not in the structure of the model then all elements s s\u2032 are also not in the model, Good (1963) provides a method that computes all such values in time |S| log |S| [10].", "startOffset": 217, "endOffset": 221}, {"referenceID": 20, "context": "Example 4 Consider as an example the Potts model [21] in which a function \u03c8(D) over a set D = {Vi}i=1 decomposes according to \u03c8(D = d) = c0 \u220f", "startOffset": 49, "endOffset": 53}, {"referenceID": 16, "context": "An example where there are substantial computational savings when using the inference algorithm proposed can be found in a model such as the QMR-DT network [17], which is comprised of noisy-OR functions, mentioned in Section 2.", "startOffset": 156, "endOffset": 160}, {"referenceID": 10, "context": "Heckerman (1989) has developed an algorithm, called Quickscore, which takes advantage of the independence of the cause variables in the context of a negative finding Ei = 0 and uses it to speed up inference in the QMR-DT network [11].", "startOffset": 229, "endOffset": 233}], "year": 2008, "abstractText": "The paper introduces a generalization for known probabilistic models such as log-linear and graphical models, called here multiplicative models. These models, that express probabilities via product of parameters are shown to capture multiple forms of contextual independence between variables, including decision graphs and noisy-OR functions. An inference algorithm for multiplicative models is provided and its correctness is proved. The complexity analysis of the inference algorithm uses a more refined parameter than the tree-width of the underlying graph, and shows the computational cost does not exceed that of the variable elimination algorithm in graphical models. The paper ends with examples where using the new models and algorithm is computationally beneficial.", "creator": "TeX"}}}