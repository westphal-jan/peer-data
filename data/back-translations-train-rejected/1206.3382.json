{"id": "1206.3382", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jun-2012", "title": "Simple Regret Optimization in Online Planning for Markov Decision Processes", "abstract": "We consider online planning in Markov decision processes. An algorithm for this problem should explore the set of possible policies from the current state, and, when interrupted, recommend an action to follow based on the outcome of the exploration. The performance of such an algorithm is assessed in terms of its simple regret, that is the loss in performance resulting from choosing the recommended action instead of an optimal one, and/or in terms of probability that the recommended action is not an optimal one. The best guarantees provided by the state-of-the-art algorithms for reduction of these measures over time are only polynomial. We introduce a new algorithm, BRUE, that achieves over time exponential reduction of these two measures. The algorithm is based on a simple yet non-standard state-space sampling scheme in which different samples are dedicated to different objectives. Our preliminary empirical evaluation shows that BRUE not only provides superior performance guarantees, but is also very effective in practice and favorably compares to state-of-the-art.", "histories": [["v1", "Fri, 15 Jun 2012 07:23:28 GMT  (141kb,D)", "http://arxiv.org/abs/1206.3382v1", null], ["v2", "Wed, 19 Dec 2012 08:48:44 GMT  (361kb,D)", "http://arxiv.org/abs/1206.3382v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["zohar feldman", "carmel domshlak"], "accepted": false, "id": "1206.3382"}, "pdf": {"name": "1206.3382.pdf", "metadata": {"source": "CRF", "title": "Online Planning in MDPs: Rationality and Optimization", "authors": ["Zohar Feldman"], "emails": [], "sections": [{"heading": "1 INTRODUCTION", "text": "An MDP < S, A, Tr, R > is defined by a set of states S, a set of actions A, a stochastic transition function Tr: S \u00b7 A \u00b7 S \u2192 [0, 1], and a reward functionary Xiv: 120 6.33 82v1 [cs.AR: S \u00b7 A \u00b7 S \u2192 R. The current state of the agent is fully observable, and the objective of the agent is to act in such a way that his accumulated reward is maximized. In the finite horizon that is used for most of the papers, the reward is piled along H levels. The desire to tackle problems of increasing complexity has led to consider online planning in MDPs. In online planning, the decision process of the agent focuses on the next action, rather than on compiling a quality policy for the entire MDP."}, {"heading": "2 MONTE-CARLO PLANNING", "text": "/ / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}, {"heading": "4 EXPERIMENTAL EVALUATION", "text": "We have empirically evaluated BRUE on the MDP sail domain [13], which was used in previous work to evaluate MC planning algorithms [13, 12, 19], as well as on random play poles, which were used in the original empirical evaluation of UCT [12]. In the sail domain, a sailboat is navigated to a destination on an 8-linked grid representing a marine environment, under fluctuating wind conditions. The goal is to reach the destination in the shortest possible time by selecting a neighboring location at each net site to move. The duration of each such step depends on the direction of the step (ceteris paribus, diagonal movements take longer than straight steps), the direction of the wind relative to the sail direction (the sailboat cannot sail against the wind and moves fastest under tailwind), and the direction of wind changes over time, but its strength is assumed to be fixed."}, {"heading": "5 SUMMARY AND DISCUSSION", "text": "We have introduced BRUE, a simple Monte Carlo algorithm for online planning in MDPs, which guarantees an exponential reduction in performance metrics of interest, namely the simple regret and likelihood of erroneous action choices; this improves on previous algorithms such as UCT, which only guaranteed a polynomial reduction in these metrics over time; the algorithm has been formalized for MDPs with a finite horizon and analyzed as such; however, our empirical evaluation shows that it performs well even in target-oriented MDPs and two-person games; and our work leaves a few questions open for future work. Considering that MDPs with an endless horizon are discounted, a direct way to use BRUE in this setting is to fix a horizon H as it is, and to derive BRUC metrics of interest by simply removing the added gap of HRmax / plan \u2212 infinite between this previous MDPs scheme and the DPs scheme."}, {"heading": "Acknowledgements", "text": "Work supported and carried out by the Technion-Microsoft Electronic Commerce Research Center."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "UCT for tactical assault planning in real-time strategy games", "author": ["R. Balla", "A. Fern"], "venue": "In IJCAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Lower bounding Klondike Solitaire with Monte-Carlo planning", "author": ["R. Bjarnason", "A. Fern", "P. Tadepalli"], "venue": "In ICAPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Pure exploration in finitely-armed and continuous-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "Theor. Comput. Sci.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Nested Monte-Carlo search", "author": ["T. Cazenave"], "venue": "In IJCAI, pages 456\u2013461,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Bandit algorithms for tree search", "author": ["P-A. Coquelin", "R. Munos"], "venue": "In Proceedings of the 23rd Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "High-quality policies for the Canadian Traveler\u2019s problem", "author": ["P. Eyerich", "T. Keller", "M. Helmert"], "venue": "In AAAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Monte-Carlo tree search and rapid action value estimation in computer Go", "author": ["S. Gelly", "D. Silver"], "venue": "AIJ, 175(11):1856\u20131875,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Metareasoning for Monte Carlo tree search", "author": ["N. Hay", "S.J. Russell"], "venue": "Technical Report UCB/EECS-2011-119, EECS Department,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "A sparse sampling algorithm for near-optimal planning in large Markov decision processes", "author": ["M.J. Kearns", "Y. Mansour", "A.Y. Ng"], "venue": "In IJCAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Probabilistic planning based on UCT", "author": ["T. Keller", "P. Eyerich"], "venue": "In ICAPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Bandit based Monte-Carlo planning", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": "In ECML, pages 282\u2013293,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "On-line search for solving Markov decision processes via heuristic sampling", "author": ["L. P\u00e9ret", "F. Garcia"], "venue": "In ECAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bull. Amer. Math. Soc.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1952}, {"title": "Nested rollout policy adaptation for Monte Carlo tree search", "author": ["C.D. Rosin"], "venue": "In IJCAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "An analysis of forward pruning", "author": ["S.J. Smith", "D.S. Nau"], "venue": "In AAAI,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1994}, {"title": "An analysis of UCT in multi-player games", "author": ["N. Sturtevant"], "venue": "In CCG, page 3749,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Doing better than UCT: Rational Monte Carlo sampling in trees", "author": ["D. Tolpin", "S.E. Shimony"], "venue": "CoRR, arXiv:1108.3711v1 [cs.AI],", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "An MDP \u3008S,A, Tr,R\u3009 is defined by a set of states S, a set of actions A, a stochastic transition function Tr : S \u00d7 A \u00d7 S \u2192 [0, 1], and a reward function", "startOffset": 122, "endOffset": 128}, {"referenceID": 9, "context": "The sparse sampling algorithm by Kearns, Mansour, and Ng [10] offered near-optimal action selection in time exponential in H but independent of the state space size.", "startOffset": 57, "endOffset": 61}, {"referenceID": 17, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 12, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 11, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 5, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 4, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 14, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 18, "context": "Several other works, closer to our focus here, introduced interruptable algorithms that are designed to provide convergence to the best action if enough time is given, and small performance loss if the algorithm is stopped prematurely [18, 13, 12, 6, 5, 15, 19].", "startOffset": 235, "endOffset": 261}, {"referenceID": 11, "context": "Numerous concrete instances of MCT have been proposed, with UCT [12]", "startOffset": 64, "endOffset": 68}, {"referenceID": 7, "context": "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].", "startOffset": 58, "endOffset": 78}, {"referenceID": 16, "context": "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].", "startOffset": 58, "endOffset": 78}, {"referenceID": 2, "context": "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].", "startOffset": 58, "endOffset": 78}, {"referenceID": 1, "context": "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].", "startOffset": 58, "endOffset": 78}, {"referenceID": 6, "context": "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].", "startOffset": 58, "endOffset": 78}, {"referenceID": 10, "context": "probably being the most popular such algorithm these days [8, 17, 3, 2, 7, 11].", "startOffset": 58, "endOffset": 78}, {"referenceID": 0, "context": "Given si, the next-on-the-probe action ai+1 is determined by the deterministic UCB1 policy [1], originally proposed for optimal cumulative regret minimization in stochastic multiarmed bandit (MAB) problems [14]: If n(si, a) > 0 for all a \u2208 A(si), then", "startOffset": 91, "endOffset": 94}, {"referenceID": 13, "context": "Given si, the next-on-the-probe action ai+1 is determined by the deterministic UCB1 policy [1], originally proposed for optimal cumulative regret minimization in stochastic multiarmed bandit (MAB) problems [14]: If n(si, a) > 0 for all a \u2208 A(si), then", "startOffset": 206, "endOffset": 210}, {"referenceID": 11, "context": "In that respect, according to Theorem 6 of Kocsis and Szepesv\u00e1ri [12], UCT achieves the optimal logarithmic cumulative regret.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "In contrast, the same Theorem 6 of Kocsis and Szepesv\u00e1ri [12] claims only a polynomial-rate reduction of the probability of choosing a non-optimal action, and the recent results by Bubeck, Munos, and Stoltz [4] on simple regret minimization in MABs with stochastic rewards imply that UCT achieves only polynomial reduction of the simple regret over time.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "In contrast, the same Theorem 6 of Kocsis and Szepesv\u00e1ri [12] claims only a polynomial-rate reduction of the probability of choosing a non-optimal action, and the recent results by Bubeck, Munos, and Stoltz [4] on simple regret minimization in MABs with stochastic rewards imply that UCT achieves only polynomial reduction of the simple regret over time.", "startOffset": 207, "endOffset": 210}, {"referenceID": 18, "context": "Some attempts have recently been made to adapt UCT, and MCT-based planning in general, to optimizing decisions in online MDP planning [19, 9].", "startOffset": 134, "endOffset": 141}, {"referenceID": 8, "context": "Some attempts have recently been made to adapt UCT, and MCT-based planning in general, to optimizing decisions in online MDP planning [19, 9].", "startOffset": 134, "endOffset": 141}, {"referenceID": 3, "context": "[4] was probably the first systematic attempt to analyze pure exploration in MABs, showing that the minimal simple regret in MAB can increase as the bound on the cumulative regret is getting smaller.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] for MABs, the BRUE setting of MCTer is as follows: \u2022 probe-S: The probes \u03c1 = \u3008s0, a1, .", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Theorem 1 Let BRUE be called on a state s0 of an MDP \u3008S,A, Tr,R\u3009 with rewards in [0, 1] and finite horizon H.", "startOffset": 81, "endOffset": 87}, {"referenceID": 12, "context": "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 12, "context": "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].", "startOffset": 136, "endOffset": 148}, {"referenceID": 11, "context": "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].", "startOffset": 136, "endOffset": 148}, {"referenceID": 18, "context": "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].", "startOffset": 136, "endOffset": 148}, {"referenceID": 11, "context": "We have evaluated BRUE empirically on the MDP sailing domain [13] that was used in previous works for evaluating MC planning algorithms [13, 12, 19], as well as on random game trees used in the original empirical evaluation of UCT [12].", "startOffset": 231, "endOffset": 235}, {"referenceID": 18, "context": "The comparison was made with two MCT-based algorithms: the UCT algorithm, and a recent modification of UCT, GCT, obtained from the former by replacing the UCB1 policy at the root node with the -greedy policy [19].", "startOffset": 208, "endOffset": 212}, {"referenceID": 18, "context": "The motivation behind the design of GCT was to improve the empirical simple regret of UCT, and the results for GCT reported by [19] (and confirmed by our experiments here) are very impressive.", "startOffset": 127, "endOffset": 131}, {"referenceID": 18, "context": "Consistently with the results reported by Tolpin and Shimony ([19]), GCT outperformed UCT by a very large margin, with the latter exhibiting very poor performance improvement over time even on the smallest, 5\u00d7 5, grids.", "startOffset": 62, "endOffset": 66}, {"referenceID": 11, "context": "This simple game tree model is similar in spirit to many other game tree models used in previous work [12, 16], except for that the success/failure of the players in measured not on a ternary scale of win/lose/draw, but via the actual payoffs received by the players.", "startOffset": 102, "endOffset": 110}, {"referenceID": 15, "context": "This simple game tree model is similar in spirit to many other game tree models used in previous work [12, 16], except for that the success/failure of the players in measured not on a ternary scale of win/lose/draw, but via the actual payoffs received by the players.", "startOffset": 102, "endOffset": 110}], "year": 2017, "abstractText": "We consider online planning in Markov decision processes. An algorithm for this problem should explore the set of possible policies from the current state, and, when interrupted, recommend an action to follow based on the outcome of the exploration. The performance of such an algorithm is assessed in terms of its simple regret, that is the loss in performance resulting from choosing the recommended action instead of an optimal one, and/or in terms of probability that the recommended action is not an optimal one. The best guarantees provided by the state-of-the-art algorithms for reduction of these measures over time are only polynomial. We introduce a new algorithm, BRUE, that achieves over time exponential reduction of these two measures. The algorithm is based on a simple yet non-standard state-space sampling scheme in which different samples are dedicated to different objectives. Our preliminary empirical evaluation shows that BRUE not only provides superior performance guarantees, but is also very effective in practice and favorably compares to state-of-the-art.", "creator": "LaTeX with hyperref package"}}}