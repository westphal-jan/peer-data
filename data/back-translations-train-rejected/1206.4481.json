{"id": "1206.4481", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Jun-2012", "title": "Parsimonious Mahalanobis Kernel for the Classification of High Dimensional Data", "abstract": "The classification of high dimensional data with kernel methods is considered in this article. Exploit- ing the emptiness property of high dimensional spaces, a kernel based on the Mahalanobis distance is proposed. The computation of the Mahalanobis distance requires the inversion of a covariance matrix. In high dimensional spaces, the estimated covariance matrix is ill-conditioned and its inversion is unstable or impossible. Using a parsimonious statistical model, namely the High Dimensional Discriminant Analysis model, the specific signal and noise subspaces are estimated for each considered class making the inverse of the class specific covariance matrix explicit and stable, leading to the definition of a parsimonious Mahalanobis kernel. A SVM based framework is used for selecting the hyperparameters of the parsimonious Mahalanobis kernel by optimizing the so-called radius-margin bound. Experimental results on three high dimensional data sets show that the proposed kernel is suitable for classifying high dimensional data, providing better classification accuracies than the conventional Gaussian kernel.", "histories": [["v1", "Wed, 20 Jun 2012 12:49:48 GMT  (71kb,D)", "https://arxiv.org/abs/1206.4481v1", null], ["v2", "Mon, 10 Sep 2012 13:01:47 GMT  (71kb,D)", "http://arxiv.org/abs/1206.4481v2", null]], "reviews": [], "SUBJECTS": "cs.NA cs.LG", "authors": ["m fauvel", "a villa", "j chanussot", "j a benediktsson"], "accepted": false, "id": "1206.4481"}, "pdf": {"name": "1206.4481.pdf", "metadata": {"source": "CRF", "title": "Parsimonious Mahalanobis Kernel for the Classification of High Dimensional Data", "authors": ["M. Fauvel", "A. Villa", "J. Chanussot", "J.A. Benediktsson"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Regularized Mahalanobis Kernel", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Review of HDDA model", "text": "Most general HDDA models are used in this workspace, i.e., each class has its own specific values. (Here we will check the HDDA model, but it is limited to the problem of covariance matrix inversion.) However, HDDA is originally proposed for classification or clustering with Gaussian mixing model, but it is generally assumed that the data follows a Gaussian distribution. (The covariance matrix of class c can be written by its eigenvalue decomposition: \"QcQ t cwo is the diagonal matrix of eigenvalues qci, i.,\" d \"identical is the matrix of class c, which contains the corresponding eigenvalue decomposition.) The HDDA model assumes that the diagonal matrix of eigenvalues is qci, i.\""}, {"heading": "2.2 Mahalanobis Kernel", "text": "The regulated Mahalanobis kernel for class c is constructed by replacing (3) the Euclidean distance in the Gaussian kernel (1) and switching eigenvalues (1) to hyperparameters (2) that are optimized during the training step: km (x, z | c) = exp (-12 (p, c, i = 1, q).As described in Section 3, these parameters are adjusted during the training step.The hyperparameters were introduced for the following reason: \u03c3ci, i, 1,..., p, c + 1} are the hyperparameters of the core. As described in Section 3, these parameters are adjusted during the training step.The hyperparameters were introduced for the following reason: It is known that the main directions are not optimal for classification as they do not maximize any discrimination criterion."}, {"heading": "2.3 Geometry of the induced feature space", "text": "Working with a core function is equivalent to working with samples mapped to a feature space H in which the point product of the core evaluation in the input space is equivalent [23, 29]: k (x, z) = < p (x), p (z) > H, p is the feature map. Under some weak conditions, the projected samples in the feature space live on a belt manifold [30, 31]. The metric tensor isgij (x) = bits 2k (x, z). This metric section stretches or compresses the euclidean distance between x and z by a factor (2). Each variable is assumed to be equally relevant to the given task, e.g. classification or regression.For the core (4), the euclidean interval between x and z extends or compresses by a factor (2)."}, {"heading": "3 L2-SVM and Radius margin bound optimization", "text": "Supported vector machines (SVM) are a standard kernel classification method [32]. It has been shown to work very well on several datasets ranging from moderate dimensions to high-dimensional data [33, 34]. In the following section, the most important results are presented, but interested readers may see references [32, 35, 36] for further mathematical details on the SVM framework."}, {"heading": "3.1 L2-Support Vector Machines", "text": "In this paper, the L2-SVM is considered instead of the conventional L1-SVM problem (35): With L2-SVM, it is possible to automatically optimize the hyperparameters by optimizing the so-called radius margin limit [37]. The L2-SVM solves the conventional L1-SVM optimization problem with a square penalty of errors [35]. The proposed training parameters (\u03b1i) n = 1 and b of the decision function f, f (z) = n (xi, z) + b are found by solving the convex optimization problem: max \u03b1) = n \u00b2 the parameters (\u03b1i) n = 1 and b of the decision function f, f (z) = n \u00b2 the proposed optimization classes (xi, z) + b."}, {"heading": "3.2 Radius-margin bound Optimization", "text": "x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "4 Estimation of p\u0302c", "text": "s Scree test [41] using the same methodology as in [21]. The test consists in comparing the difference \u0394i between two consecutive eigenvalues \u03bbi and \u03bbi + 1, \u2206 i = \u03bbi \u2212 \u03bbi + 1. If the differences \u2206 i for all i are below a user-defined threshold, i.e. \u2206 j < s, \u0435j \u044b\u043b\u044b {i,..., d \u2212 1}, p \u0445c = i. Generally speaking, the threshold is a percentage of the highest difference. Figure 3 shows an example on a simulated data set (see Section 5.1 for a description of the data). The correct value in this case is p = 10, but its estimate is p \u0441\u0441\u0442\u0435\u0441\u0442\u0435\u0441\u0442\u043e\u0441\u0442\u043e\u0441\u0442\u043e\u0441\u043e\u0441\u0438\u043d\u0438\u043d\u0438\u0439 = 14. The size of the signal undercompartment was estimated by the Scree test of Cattell [41]."}, {"heading": "5 Experimental results", "text": "With regard to the strategy of multi-class fusion, the results must be considered as individual problems of binary classification: no merger rules have been applied. For example, the results of the class \"asphalt\" in Table 4 should be interpreted as \"asphalt against all.\" The reason for applying this approach is the better interpretation of the results obtained because the results are not distorted by the strategy of multi-class fusion."}, {"heading": "5.1 Classification of simulated data following HDDA model", "text": "In this section, the proposed kernel, the HDDA Mahalanobis kernel (HDDA-MK), is used with the SVM for classification and evaluated on simulated data. Classification accuracy performance was compared with an SVM kernel with a conventional Gaussian kernel on the original data and on the data projected on the first main axis of the classes under consideration, the so-called PCA Mahalanobis kernel. The main difference between the HDDA-MK kernel and the PCA Mahalanobis kernel is that the PCAMahalanobis kernel discards the noise subroom, while the HDDA-MK also exploits the noise subroom to improve class discrimination."}, {"heading": "5.1.1 Estimation of p\u0302c", "text": "Simulated data can be used to assess how the size of the intrinsic signal superspace is estimated. Figure 5 shows the nesting graph of the estimates. After several trials, the threshold for the debris test was set at 10%. Setting the threshold at too high a value would cause p to be underestimated, while a low value would lead to a drastic overvaluation. It is clear from the figure that the debris test overestimates the parameter p for each configuration. Moreover, another criterion (the BIC [45]) was used in previous work [44] to estimate the correct dimension of the subspace in which the data live. However, the BIC criterion showed poor results when the number of training samples for individual class nc was close to the more robust dimension of the data."}, {"heading": "5.1.2 Classification accuracies", "text": "The percentages of the correct classification are shown in Figure 6. For the three experiments, the proposed kernel yields the best results in terms of accuracy. Although p-c has been overestimated, the performance of the algorithm in terms of classification accuracy has not been penalized. Nc = 2 yields the second best result from the PCA Mahalanobis kernel, while for Nc = 3 or 4 it is provided by the Gaussian kernel applied to the original data. Thus, the mean value of the correct classification for Nc = 4 is 92.2% for the HDDA Mahalanobis kernel, 91.3% for the conventional Gaussian kernel, and only 76.3% for the PCA Mahalanobis kernel. The results confirm the poor generalization capability of the Mahalanobis kernel in handling high-dimensional spaces. Although the conventional Gaussian kernel is less sensitive to the problem, the proposed kernel shows a significant improvement in classification accuracy."}, {"heading": "5.2 Classification of Madelon data", "text": "Madelon dataset is a simulated dataset used for the NIPS Feature Selection Challenge3. It has 5 useful features, 15 redundant features and 480 random probes for a total of 500 features (d = 500). It is composed of two classes and the number of training samples is 2000 and the number of test samples is 600. The threshold for the scree test has been set at 20%. The proposed kernel has been compared with the same cores as in the previous section. The best results in terms of accuracy are given in Table 2. The conventional Gausian kernel performs poorly on this dataset with a global precision of 69.7% (random classifier would reach 50%). The best accuracy is obtained for the proposed kernel with an average accuracy of 83.9%. The size p \u00b2 c of the signal bandwidth for the two classes was p \u00b2 1 = 4 or p \u00b2 2 = 4, respectively. The results obtained with the signal from the HDnel are confirmed by the PCDA, with the A signal being worse than the analog to the PCDA signal."}, {"heading": "5.3 Classification of Arcene data set", "text": "The Arcene dataset is a dataset used for the NIPS Feature Selection Challenge. It has 7000 real variables, 3000 random probes for a total of 10,000 characteristics (d = 10000). It consists of two classes. The number of training samples available is 100 and the number of test samples is 100. Therefore, the number of training samples is very low compared to the number of variables. Approximately 50% of the data is not zero. Classification accuracies are in Table 3. The threshold selected for the Scree test is set at 0.5%. Results for other values of the threshold are also given for comparison. The Gaussian core achieves a global accuracy of 80%. It performs reasonably well in terms of classification accuracy in terms of the dimension of the data. The PCA Mahalanobis core exhibits the worst behavior, whatever the threshold value is. For the HDDA Mahalanobis core, for the highest value of the s, the results are identical to the Gaussian core."}, {"heading": "5.4 Classification of real hyperspectral data", "text": "The dataset taken into account in this experiment is the University of Pavia, Italy, recorded with the ROSIS03 sensor. The image has 103 spectral variables, i.e., each pixel is represented by a vector with 103 characteristics (d = 103) [46]. Nine classes were defined by photographic interpretation, as in the first column of Table 4. Here, the threshold was set to 0.01%, due to a very high value of the first main component (mainly due to albedo). Classification results are reported in Table 4. However, the proposed kernel results in an increase in accuracy compared to the conventional Gaussian kernel. However, for this dataset, the PCA Mahalanobis and HDDA Mahalanobis kernel perform equally well in terms of accuracy, with the exception of the grassland and bare soil classes. To assess the impact of the p kernel on classification accuracy, the grassy class was classified one area at a time."}, {"heading": "5.5 Analysis of the processing time", "text": "In order to evaluate the calculation load of the proposed method, the processing time for four datasets has been calculated. Results are shown in Table 5. The program runs under Matlab on two cores with 2.67 GHz laptop. For the Arcene dataset, the Gaussian kernel shows the lowest calculation time. However, the calculation of the first eigenvalues / eigenvectors is challenging, as the data has 10,000 features. It takes about 14 seconds. Optimization of the hyperparameters is fast, as a few training samples are available. For the Madelon datasets, the calculation of the first eigenvalues / eigenvectors is fast, as the data has over 10,000 features. Optimization of the hyperparameters is fast, as a number of training patterns are available. For the Madelon datasets, the calculation of the first eigenvalues / eigenvectors is fast, about 1.3 seconds, while the optimization of the kernel hyperparameters is more demanding than that of the PC."}, {"heading": "6 Conclusions", "text": "In this paper, a novel kernel adapted to high-dimensional data was proposed; the economical Mahalanobis kernel is based on the emptiness property of HD spaces; for each class, the original input space is divided into a signal subroom and a noise subroom; however, using this assumption, the inversion of the covariance matrix in the Mahalanobis kernel can be precisely calculated; the proposed kernel was tested in an SVM framework for the purpose of classification; experimental results on four sets of data show the potential of the proposed kernel; in any case, the classification accuracy increased compared to the conventional Gaussian kernel; and in three cases, the proposed kernel showed superior results to simply map the data on the first PCA axes. Consequently, HD data from the HDDA Mahalanobis kernel is preferred to a different compression method; in terms of the compression load, the HDDA Mahalanobis kernel is a higher-level kernel."}, {"heading": "Acknowledgment", "text": "The authors thank IAPR - TC7 for providing the data and Prof. Paolo Gamba and Prof. Fabio Dell'Acqua of the University of Pavia, Italy, for providing the reference data."}], "references": [{"title": "Clustering high-dimensional data: A survey on subspace clustering, pattern-based clustering, and correlation clustering", "author": ["H.-P. Kriegel", "P. Kr\u00f6ger", "A. Zimek"], "venue": "ACM Trans. Knowl. Discov. Data, vol. 3, Mar. 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "High-dimensional data analysis: the curses and blessing of dimensionality", "author": ["D.L. Donoho"], "venue": "AMS Mathematical challenges of the 21st century, 2000.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2000}, {"title": "A new approach to mixed pixel classification of hyperspectral imagery based on extended morphological profiles", "author": ["A. Plaza", "P. Martinez", "R. Perez", "J. Plaza"], "venue": "Pattern Recognition, vol. 37, no. 6, pp. 1097 \u2013 1116, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "A course in the geometry of n-dimensions", "author": ["M.G. Kendall"], "venue": "New York: Dover Publication,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1961}, {"title": "Supervised classification in high dimensional space: geometrical, statistical and asymptotical properties of multivariate data", "author": ["L. Jimenez", "D.A. Landgrebe"], "venue": "IEEE Trans. Syst., Man, Cybern. B, vol. 28, pp. 39\u201354, Feb. 1998.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Searching for the embedded manifolds in high dimensional data, problems and unsolved questions", "author": ["J. Herault", "A. Guerin-Dugue", "P. Villemain"], "venue": "European Symposium on Artificial Neural Networks, pp. 1\u201312, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "On the surprising behavior of distance metrics in high dimensional space", "author": ["C. Aggarwal", "A. Hinneburg", "D. Keim"], "venue": "Database Theory \u2014 ICDT 2001, vol. 1973 of Lecture Notes in Computer Science, pp. 420\u2013434, Springer Berlin / Heidelberg, 2001.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "The concentration of fractional distances", "author": ["D. Francois", "V. Wertz", "M. Verleysen"], "venue": "IEEE Trans. Knowl. Data Eng., vol. 19, pp. 873\u2013886, July 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Distance-preserving projection of high-dimensional data for nonlinear dimensionality reduction", "author": ["L. Yang"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 26, pp. 1243 \u20131246, Sept. 2004.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "On the effects of dimensionality on data analysis with neural networks", "author": ["M. Verleysen", "D. Francois", "G. Simon", "V. Wertz"], "venue": "Artificial Neural Nets Problem Solving Methods, vol. 2687 of Lecture Notes in Computer Science, pp. 1044\u20131044, Springer Berlin / Heidelberg, 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning high dimensional data", "author": ["M. Verleysen"], "venue": "V. Piuri eds, IOS Press, Amsterdam (The Netherlands),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Adaptive control processes - A guided tour", "author": ["R.E. Bellman"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1961}, {"title": "Signal Theory Methods in Multispectral Remote Sensing", "author": ["D.A. Landgrebe"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2003}, {"title": "Dimension reduction: A guided tour", "author": ["C.J.C. Burges"], "venue": "Foundations and Trends in Machine Learning, vol. 2, no. 4, pp. 275\u2013365, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1157\u20131182, Mar. 2003.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2003}, {"title": "Subspace clustering for high dimensional data: a review", "author": ["L. Parsons", "E. Haque", "H. Liu"], "venue": "SIGKDD Explor. Newsl., vol. 6, pp. 90\u2013105, June 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Probabilistic principal component analysis", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology), vol. 61, no. 3, pp. 611\u2013622, 1999.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Mixtures of probabilistic principal component analyzers", "author": ["M.E. Tipping", "C.M. Bishop"], "venue": "Neural Computation, vol. 11, no. 2, pp. 443\u2013482, 1999.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Mixtures of factor analyzers with common factor loadings: Applications to the clustering and visualization of high-dimensional data", "author": ["J. Baek", "G.J. McLachlan", "L.K. Flack"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, pp. 1298 \u20131309, July 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "High-Dimensional Discriminant Analysis", "author": ["C. Bouveyron", "S. Girard", "C. Schmid"], "venue": "Communication in Statistics- Theory and Methods / Communications in Statistics Theory and Methods, vol. 36, p. 2607 \u2013 2623, Jan. 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "High-Dimensional Data Clustering", "author": ["C. Bouveyron", "S. Girard", "C. Schmid"], "venue": "Computational Statistics and Data Analysis, vol. 52, no. 1, pp. 502\u2013519, 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Kernel methods in machine learning", "author": ["T. Hofmann", "B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Annals of Statistics, vol. 36, no. 3, pp. 1171\u20131220, 2008.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "The curse of highly variable functions for local kernel machines", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "Advances in Neural Information Processing Systems 18 (NIPS\u201905) (Y. Weiss, B. Sch\u00f6lkopf, and J. Platt, eds.), pp. 107\u2013114, MIT Press, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Hyperspectral image classification with Mahalanobis relevance vector machines", "author": ["G. Camps-Valls", "A. Rodrigo-Gonzalez", "J. Muoz-Mari", "L. Gomez-Chova", "J. Calpe-Maravilla"], "venue": "Geoscience and Remote Sensing Symposium, 2007. IGARSS 2007. IEEE International, pp. 3802\u20133805, July 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Training of support vector machines with Mahalanobis kernels", "author": ["S. Abe"], "venue": "Artificial Neural Networks: Formal Models and Their Applications - ICANN 2005, Lecture Notes in Computer Science, pp. 571\u2013 576, Springer Berlin / Heidelberg, 2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Support vector regression using mahalanobis kernels", "author": ["Y. Kamada", "S. Abe"], "venue": "Artificial Neural Networks in Pattern Recognition (F. Schwenker and S. Marinai, eds.), vol. 4087 of Lecture Notes in Computer Science, pp. 144\u2013152, Springer Berlin / Heidelberg, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Weighted mahalanobis distance kernels for support vector machines", "author": ["D. Wang", "D. Yeung", "E. Tsang"], "venue": "Neural Networks, IEEE Transactions on, vol. 18, pp. 1453 \u20131462, sept. 2007. 16", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey of kernel and spectral methods for clustering", "author": ["M. Filippone", "F. Camastra", "F. Masulli", "S. Rovetta"], "venue": "Pattern Recognition, vol. 41, no. 1, pp. 176 \u2013 190, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Geometry and Invariance in Kernel Based Methods In Advances in Kernel Methods - Support Vector Learning", "author": ["B. Scholkopf", "C. Burges", "A. Smola"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "A geometrical method to improve performance of the vector machine", "author": ["P. Williams", "S. Li", "J. Feng", "S. Wu"], "venue": "IEEE Trans. Neural Netw., vol. 18, pp. 942\u2013947, May 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "A tutorial on support vector machines for pattern recognition", "author": ["C.J. Burges"], "venue": "Data Mining and Knowledge Discovery, vol. 2, pp. 121\u2013167, 1998.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1998}, {"title": "Svm-based feature extraction for face recognition", "author": ["S.-K. Kim", "Y.J. Park", "K.-A. Toh", "S. Lee"], "venue": "Pattern Recognition, vol. 43, no. 8, pp. 2871 \u2013 2881, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "A spatial-spectral kernel-based approach for the classification of remote-sensing images", "author": ["M. Fauvel", "J. Chanussot", "J.A. Benediktsson"], "venue": "Pattern Recogn., vol. 45, pp. 381\u2013392, Jan. 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "An Introduction to Support Vector Machines and Other Kernelbased Learning Methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2000}, {"title": "The Nature of Statistical Learning Theory, Second Edition", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1999}, {"title": "Choosing multiple parameters for support vector machines", "author": ["O. Chapelle", "V. Vapnik", "O. Bousquet", "S. Mukherjee"], "venue": "Machine Learning, vol. 46, pp. 131\u2013159, 2002.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient tuning of svm hyperparameters using radius/margin bound and iterative algorithms", "author": ["S.S. Keerthi"], "venue": "IEEE Trans. Neural Netw., vol. 13, pp. 1225 \u2013 1229, Sept. 2002.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2002}, {"title": "A comparison of methods for multiclass support vector machines", "author": ["H.C.-W.", "L.C.-J."], "venue": "IEEE Trans. Neural Netw., vol. 13, pp. 415 \u2013425, Mar. 2002.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2002}, {"title": "Selection of kernel parameters.", "author": ["O. Chapelle"], "venue": "http://olivier.chapelle.cc/ams/,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2002}, {"title": "The Scree Test For The Number Of Factors", "author": ["R.B. Cattell"], "venue": "Multivariate Behavioral Research, vol. 1, no. 2, pp. 245\u2013276, 1966.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1966}, {"title": "Spectral unmixing", "author": ["N. Keshava", "J. Mustard"], "venue": "Signal Processing Magazine, IEEE, vol. 19, pp. 44 \u201357, Jan. 2002.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2002}, {"title": "Mahalanobis kernel for the classification of hyperspectral images", "author": ["M. Fauvel", "A. Villa", "J. Chanussot", "J. Benediktsson"], "venue": "Geoscience and Remote Sensing Symposium (IGARSS), 2010 IEEE International, pp. 3724 \u20133727, July 2010.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "The annals of Statistics, vol. 6, no. 2, pp. 461\u2013464, 1978.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1978}, {"title": "Statistical pattern recognition in remote sensing", "author": ["C.H. Chen", "P.-G.P. Ho"], "venue": "Pattern Recognition, vol. 41, no. 9, pp. 2731 \u2013 2741, 2008.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Intrinsic dimension estimation by maximum likelihood in isotropic probabilistic pca", "author": ["C. Bouveyron", "G. Celeux", "S. Girard"], "venue": "Pattern Recognition Letters, vol. 32, no. 14, pp. 1706 \u2013 1713, 2011. 17", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": ") [1, 2, 3].", "startOffset": 2, "endOffset": 11}, {"referenceID": 1, "context": ") [1, 2, 3].", "startOffset": 2, "endOffset": 11}, {"referenceID": 2, "context": ") [1, 2, 3].", "startOffset": 2, "endOffset": 11}, {"referenceID": 1, "context": "Actually, HD data pose critical theoretical and practical problems that need to be addressed specifically [2].", "startOffset": 106, "endOffset": 109}, {"referenceID": 3, "context": "Most of them do not behave in a similar way as in three dimensional Euclidean spaces (Table 1 summarizes the main properties of HD spaces) [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 4, "context": "For instance, samples following a uniform law will have a tendency to have a high concentration in the corners [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "The same property holds for normally distributed data: samples tend to have a high concentration in the tails [6], making density estimation a difficult task.", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "Unfortunately, discriminative methods also suffer if the dimensionality is high, due to the \u201cconcentration of measure phenomenon\u201d [2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 6, "context": "In HD spaces, samples tend to be equally distant from each other [7].", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": ") is affected by this phenomenon [8].", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "Therefore, every method based on the distance between samples [9] (SVM with Gaussian kernel, neural network, Nearest Neighbors, Locally Linear Embedding.", "startOffset": 62, "endOffset": 65}, {"referenceID": 9, "context": ") are potentially affected by this phenomenon [10, 11].", "startOffset": 46, "endOffset": 54}, {"referenceID": 10, "context": ") are potentially affected by this phenomenon [10, 11].", "startOffset": 46, "endOffset": 54}, {"referenceID": 11, "context": "An additional property, for which the consequences are more practical than theoretical, is the \u201cempty space phenomenon\u201d [12]: In HD spaces, the available samples usually fill a very small part of the space.", "startOffset": 120, "endOffset": 124}, {"referenceID": 11, "context": "Bellman [12], refers to the aforementioned problems of HD data and reflects how processing HD data is difficult.", "startOffset": 8, "endOffset": 12}, {"referenceID": 1, "context": "Donoho has noticed [2], there is also a \u201cBlessing of dimensionality\u201d: For instance in classification, the class separability is improved when the dimensionality of the data increases.", "startOffset": 19, "endOffset": 22}, {"referenceID": 12, "context": "Consider for example a comparison between hyperspectral (hundreds of spectral wavelengths) and multispectral (tens of spectral wavelengths) remote sensing images[13].", "startOffset": 161, "endOffset": 165}, {"referenceID": 4, "context": "However, if conventional methods are used, the additional information contained in hyperspectral images will not lead to an increase of the classification accuracy [5].", "startOffset": 164, "endOffset": 167}, {"referenceID": 13, "context": "Recent overviews of DR can be found in [14, 15, 16].", "startOffset": 39, "endOffset": 51}, {"referenceID": 14, "context": "Recent overviews of DR can be found in [14, 15, 16].", "startOffset": 39, "endOffset": 51}, {"referenceID": 15, "context": ", the subspace models [17].", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "For instance, the Probabilistic Principal Component Analysis (PPCA) [18] assumes that the classes are normally distributed in a lower dimensional subspace and are linearly embedded in the original subspace with additive white noise.", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "Such models exploit the empty space property of HD data without discarding any dimension of the data [19, 20].", "startOffset": 101, "endOffset": 109}, {"referenceID": 18, "context": "Such models exploit the empty space property of HD data without discarding any dimension of the data [19, 20].", "startOffset": 101, "endOffset": 109}, {"referenceID": 19, "context": "[21, 22].", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[21, 22].", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "Conversely, kernel based methods do not reduce the dimensionality but rather work with the full HD data [23].", "startOffset": 104, "endOffset": 108}, {"referenceID": 22, "context": "However, local kernel methods are sensitive to the size of the dimensionality [24].", "startOffset": 78, "endOffset": 82}, {"referenceID": 23, "context": "Previous works on the Mahalanobis kernel [25, 26, 27, 28] were limited by the effect of dimensionality on the matrix inversion.", "startOffset": 41, "endOffset": 57}, {"referenceID": 24, "context": "Previous works on the Mahalanobis kernel [25, 26, 27, 28] were limited by the effect of dimensionality on the matrix inversion.", "startOffset": 41, "endOffset": 57}, {"referenceID": 25, "context": "Previous works on the Mahalanobis kernel [25, 26, 27, 28] were limited by the effect of dimensionality on the matrix inversion.", "startOffset": 41, "endOffset": 57}, {"referenceID": 26, "context": "Previous works on the Mahalanobis kernel [25, 26, 27, 28] were limited by the effect of dimensionality on the matrix inversion.", "startOffset": 41, "endOffset": 57}, {"referenceID": 23, "context": "In [25], the covariance matrix was computed on the whole training set.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Diagonal and full covariance matrices were investigated in [26] for the purpose of classification and in [27] for the purpose of regression.", "startOffset": 59, "endOffset": 63}, {"referenceID": 25, "context": "Diagonal and full covariance matrices were investigated in [26] for the purpose of classification and in [27] for the purpose of regression.", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "Interested readers can find a detailed presentation of HDDA in [21, 22].", "startOffset": 63, "endOffset": 71}, {"referenceID": 20, "context": "Interested readers can find a detailed presentation of HDDA in [21, 22].", "startOffset": 63, "endOffset": 71}, {"referenceID": 19, "context": ",pc and bc can be computed from the sample covariance matrix [21]:", "startOffset": 61, "endOffset": 65}, {"referenceID": 19, "context": "Refers to [aijbiQidi] in [21, 22].", "startOffset": 25, "endOffset": 33}, {"referenceID": 20, "context": "Refers to [aijbiQidi] in [21, 22].", "startOffset": 25, "endOffset": 33}, {"referenceID": 21, "context": "3 Geometry of the induced feature space Working with a kernel function is equivalent to work with samples mapped onto a feature space H, where the dot product is equivalent to the kernel evaluation in the input space [23, 29]: k(x, z) = \u3008\u03c6(x), \u03c6(z)\u3009H, \u03c6 being the feature map.", "startOffset": 217, "endOffset": 225}, {"referenceID": 27, "context": "3 Geometry of the induced feature space Working with a kernel function is equivalent to work with samples mapped onto a feature space H, where the dot product is equivalent to the kernel evaluation in the input space [23, 29]: k(x, z) = \u3008\u03c6(x), \u03c6(z)\u3009H, \u03c6 being the feature map.", "startOffset": 217, "endOffset": 225}, {"referenceID": 28, "context": "Under some weak conditions, the projected samples in the feature space live on a Riemannian manifold [30, 31].", "startOffset": 101, "endOffset": 109}, {"referenceID": 29, "context": "Under some weak conditions, the projected samples in the feature space live on a Riemannian manifold [30, 31].", "startOffset": 101, "endOffset": 109}, {"referenceID": 30, "context": "Support vector machines (SVM) is a standard classification kernel methods [32].", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "It has shown to performs very well on several data sets from moderate dimension to high dimensional data [33, 34].", "startOffset": 105, "endOffset": 113}, {"referenceID": 32, "context": "It has shown to performs very well on several data sets from moderate dimension to high dimensional data [33, 34].", "startOffset": 105, "endOffset": 113}, {"referenceID": 30, "context": "In the following section, the main results are presented but interested readers could see references [32, 35, 36] for further mathematical details about the SVM framework.", "startOffset": 101, "endOffset": 113}, {"referenceID": 33, "context": "In the following section, the main results are presented but interested readers could see references [32, 35, 36] for further mathematical details about the SVM framework.", "startOffset": 101, "endOffset": 113}, {"referenceID": 34, "context": "In the following section, the main results are presented but interested readers could see references [32, 35, 36] for further mathematical details about the SVM framework.", "startOffset": 101, "endOffset": 113}, {"referenceID": 33, "context": "1 L2-Support Vector Machines The L2-SVM is considered in this work rather than the conventional L1-SVM [35]: With L2-SVM it is possible to tune the hyperparameters automatically by optimizing the so called radius-margin bound [37].", "startOffset": 103, "endOffset": 107}, {"referenceID": 35, "context": "1 L2-Support Vector Machines The L2-SVM is considered in this work rather than the conventional L1-SVM [35]: With L2-SVM it is possible to tune the hyperparameters automatically by optimizing the so called radius-margin bound [37].", "startOffset": 226, "endOffset": 230}, {"referenceID": 33, "context": "The L2-SVM solves the conventional L1-SVM optimization problem with a quadratic penalization of errors [35].", "startOffset": 103, "endOffset": 107}, {"referenceID": 34, "context": "An estimate of the generalization errors is given by an upper bound on the number of errors of the leave-one-out procedure, the radius-margin bound T [36]: T (p) := RM.", "startOffset": 150, "endOffset": 154}, {"referenceID": 34, "context": "R2 is obtained by the optimal objective function of the following constraint optimization problem [36]:", "startOffset": 98, "endOffset": 102}, {"referenceID": 35, "context": "[37], followed later by S.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "Keerthi [38], have proposed an algorithm based on gradient optimization method.", "startOffset": 8, "endOffset": 12}, {"referenceID": 37, "context": "Indeed, for a multiclass problem, the \u201cone vs one\u201d approach must not be used and the \u201cone vs all\u201d approach should be preferred [39].", "startOffset": 127, "endOffset": 131}, {"referenceID": 35, "context": "have proven that sinceM2 and R2 are computed via an optimization problem, the gradients of \u03b1\u0303i and \u03b2\u0303i do not enter into account in the computation of their gradients [37].", "startOffset": 167, "endOffset": 171}, {"referenceID": 35, "context": "Once the derivatives have been computed, the optimization of T is done through a conventional gradient descent, following the framework in [37].", "startOffset": 139, "endOffset": 143}, {"referenceID": 38, "context": "For implementation details, see [40].", "startOffset": 32, "endOffset": 36}, {"referenceID": 39, "context": "4 Estimation of p\u0302c The size of the signal subspace was estimated by the scree test of Cattell [41] using the same methodology as in [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 19, "context": "4 Estimation of p\u0302c The size of the signal subspace was estimated by the scree test of Cattell [41] using the same methodology as in [21].", "startOffset": 133, "endOffset": 137}, {"referenceID": 40, "context": "Simulated data were constructed using a linear mixture model [42]:", "startOffset": 61, "endOffset": 65}, {"referenceID": 41, "context": "Furthermore, in previous work [44], another criterion (the BIC [45]) was used to estimate the correct dimension of the subspace where the data live.", "startOffset": 30, "endOffset": 34}, {"referenceID": 42, "context": "Furthermore, in previous work [44], another criterion (the BIC [45]) was used to estimate the correct dimension of the subspace where the data live.", "startOffset": 63, "endOffset": 67}, {"referenceID": 43, "context": ", each pixel is represented by a vector with 103 features (d=103) [46].", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": ", regression [27].", "startOffset": 13, "endOffset": 17}, {"referenceID": 44, "context": "For instance, an maximum likelihood estimator for HDDA exits and must be investigated [47].", "startOffset": 86, "endOffset": 90}], "year": 2012, "abstractText": "The classification of high dimensional data with kernel methods is considered in this article. Exploiting the emptiness property of high dimensional spaces, a kernel based on the Mahalanobis distance is proposed. The computation of the Mahalanobis distance requires the inversion of a covariance matrix. In high dimensional spaces, the estimated covariance matrix is ill-conditioned and its inversion is unstable or impossible. Using a parsimonious statistical model, namely the High Dimensional Discriminant Analysis model, the specific signal and noise subspaces are estimated for each considered class making the inverse of the class specific covariance matrix explicit and stable, leading to the definition of a parsimonious Mahalanobis kernel. A SVM based framework is used for selecting the hyperparameters of the parsimonious Mahalanobis kernel by optimizing the so-called radius-margin bound. Experimental results on three high dimensional data sets show that the proposed kernel is suitable for classifying high dimensional data, providing better classification accuracies than the conventional Gaussian kernel.", "creator": "LaTeX with hyperref package"}}}