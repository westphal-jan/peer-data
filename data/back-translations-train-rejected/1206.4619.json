{"id": "1206.4619", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Inductive Kernel Low-rank Decomposition with Priors: A Generalized Nystrom Method", "abstract": "Low-rank matrix decomposition has gained great popularity recently in scaling up kernel methods to large amounts of data. However, some limitations could prevent them from working effectively in certain domains. For example, many existing approaches are intrinsically unsupervised, which does not incorporate side information (e.g., class labels) to produce task specific decompositions; also, they typically work \"transductively\", i.e., the factorization does not generalize to new samples, so the complete factorization needs to be recomputed when new samples become available. To solve these problems, in this paper we propose an\"inductive\"-flavored method for low-rank kernel decomposition with priors. We achieve this by generalizing the Nystr\\\"om method in a novel way. On the one hand, our approach employs a highly flexible, nonparametric structure that allows us to generalize the low-rank factors to arbitrarily new samples; on the other hand, it has linear time and space complexities, which can be orders of magnitudes faster than existing approaches and renders great efficiency in learning a low-rank kernel decomposition. Empirical results demonstrate the efficacy and efficiency of the proposed method.", "histories": [["v1", "Mon, 18 Jun 2012 15:04:39 GMT  (147kb)", "http://arxiv.org/abs/1206.4619v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kai zhang", "liang lan", "jun liu", "reas rauber", "fabian moerchen"], "accepted": false, "id": "1206.4619"}, "pdf": {"name": "1206.4619.pdf", "metadata": {"source": "CRF", "title": "Inductive Kernel Low-rank Decomposition with Priors: A Generalized Nystro\u0308m Method", "authors": ["Kai Zhang", "Liang Lan", "Jun Liu"], "emails": ["kai-zhang@siemens.com", "lanliang@temple.edu", "jun-liu@siemens.com", "rauber@ifs.tuwien.ac.at", "fabian.moerchen@siemens.com"], "sections": [{"heading": null, "text": "Published in Proceedings of the 29th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by author (s) / owner (s).tion. Empirical results show the effectiveness and efficiency of the proposed method."}, {"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2. Nystro\u0308m Method", "text": "The Nystro-m method is a sample-based algorithm for approximating large kernel matrices and their own systems. It arose from the solution of integral equations and was introduced to the machine learning community by (Williams & Seeger, 2001; Fowlkes et al., 2004; Drineas & Mahoney, 2005).With a core function k (\u00b7, \u00b7) and a sample set with underlying distribution p (\u00b7), the Nystro-m method was introduced with respect to p. The idea here is to draw a series of m samples Z (y) p (y) i (y) dy = i (x).Here, Nystro-m eigenfunctions and eigenvalues of the operator k (\u00b7) are to be drawn with respect to p."}, {"heading": "3. Generalized Nystro\u0308m Low-rank Decomposition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Bilateral Extrapolation of Dictionary Kernel", "text": "First, we present an interesting interpretation of the matrix completion, in which the matrix (3) equals q = q = q = q = q = q = q = q = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ="}, {"heading": "3.2. Including Side Information", "text": ", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \",\" \",\" \",\" \",\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \",\" \",\", \"\", \",\" \",\" \",\" \",\" \",\", \"\", \",\" \",\" \",\" \":\", \"\" \",\": \",\" \":\", \":\", \":\", \":\", \":\": \",\": \":\", \":\": \",\": \":\": \",\": \":\": \",\": \":\": \",\": \":\": \",\": \":\": \",\": \":\": \",\": \":\": \",\": \":\", \":\": \":\", \":\": \":\", \":\": \":\", \":\": \":\", \":\": \",\": \":\": \":\"), \":\": \":\": \":\": \""}, {"heading": "3.3. Side Information as Grouping Constraints", "text": "Considering a number of must-link and non-link pairs, which are denoted by I., XI is the subset of samples with such a constraint. Then we define T-R | XI | XI | so that Tij = {1 (xi, xj) and XI 0 are otherwise defined. Then our goal can be conveniently written as in (5)."}, {"heading": "3.4. Optimization", "text": "The target (6) is convex with respect to S, and the psd constraint S 0 is also convex. Therefore, (6) is a smooth convex problem with a global optimum. Note that S is a matrix with only m2 variables, where m-n is a custom value. Therefore, the problem (6) involves only light optimization load. We use the gradient mapping strategy (Nemirovski, 1994), which is composed of iterative gradient descend equipped with a projection step to find the optimal solution. Faced with an initial solution S (t), we update it from S (t) = property-iors (t) + property-scan-t with respect to S (t), (7) where we provide the gradient-S (6) with a projection step with respect to target J (6) to find the optimal solution."}, {"heading": "3.5. Initialization", "text": "In this section we propose a closed initialization, which helps us to quickly find the optimal solution. The basic idea is to drop the PSD constraint in (6) and calculate the vanishing point of the gradient, i.e., it results in P = 1 (E'l El), Q = S0 + 1E'l ElSE l El = E'l K'l El + \u03bbS 0.Then we have S + PSP = Q, (10) where P = 1 (E'l El), Q = S0 + 1E'l K'l El.Equation (10) can be solved as follows. Let's assume that the diagonalization of P = UE is' U 'and define S = US' U ', Q = UQ' U ', then it can be written as US solution."}, {"heading": "3.6. Landmark Selection", "text": "The selection of boundary points Z in the Nystro \ufffd m Method can greatly influence their performance, preferably allowing a faithful reconstruction of the global similarity landscape. We used the k-means-based sampling system (Zhang & Kwok, 2010), which consistently outperforms other popular boundary sampling methods such as random sampling."}, {"heading": "3.7. Complexities", "text": "The spatial complexity of our algorithm is O (mn), where n is the sample size and m is the number of boundary points. Mathematically, it requires only the repeated eigenvalue separation of m \u00b7 m matrices and a single multiplication between the n \u00b7 m extrapolation E and the m \u00b7 m dictionary core S. The total complexity is O (m2n) + O (t log (\u00b5max) m3), where t is the number of gradient imaging siterations and \u00b5max the maximum eigenvalue of the Hessian. This is because A (8) is limited by \u00b5max and a suitable step length can always be found in logical (\u00b5max) steps. Empirically, only a few iterations are required with the initialization in Section 3.5."}, {"heading": "3.8. Selecting \u03bb", "text": "The hyperparameter \u03bb in (6) can be difficult to select when the lateral information (e.g. partially labeled samples) is limited. Here, we propose a heuristic to select \u03bb. Note that the target (6) contains two residuals, S0 \u2212 S and ElSE l, in terms of Euclidean distance, which are additive and require a compromise parameter \u03bb. Here, we use a new criterion with a specific invariance indicator to re-evaluate the quality of fit of the solution. More precisely, we used normalized kernel alignment (NKA) (Cortes et al., 2010) between kernel matrices, \u03c1 [K, K \u2032] = KcK \u00b2 c \u00b2 c, F \u00b2 K \u00b2 c \u00b2 c \u00b2 c \u00b2 c \u00b2 c \u00b2 F, (11), where Kc \u00b2 is the double kernel form of K. The NKA score always has an order of magnitude smaller than 1. it is independent of the S and is the scale."}, {"heading": "4. Related Work", "text": "In fact, it is in such a way that most of us are able to embark on the search for new paths that they are treading in order to travel the world, and that they are able to understand the world, \"he said in an interview with the\" New York Times. \"\" I believe that the world in which they are located is in another world, \"he said.\" I do not believe that they will be able to understand the world. \"He added:\" I do not believe that the world in which we live, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the people, the world, the world, the world, the world, the people, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the people, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the people, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the"}, {"heading": "5. Experiments", "text": "In fact, it is the case that most people in the world are able to recognize themselves and understand how they have behaved. (...) Most people in the world do not know how to behave. (...) Most people in the world do not know how to behave. (...) They do not know what to do. (...) Most people in the world do not know how to behave. (...) People in the world do not know how to behave. (...) They do not know how to behave. (...) They do not know what to do. (...) People in the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world, the world of the world of the world, the world of the world of the world, the world of the world of the world of the world, the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world of the world of the world, the world of the world of the world of the world of the world, the world of the world of the world of the world, the world of the world of the world, the world of the world of the world of the world of the world, the world of the world of the world of the world, the world of the world of the world of the world, the world of the world of the world of the world, the world of the world of the world, the world of the world of the world of the world of the world, the world of the world, the world of the world of the world, the world of the world, the world of the world of the world of the world, the world of the world, the world of the world, the world of the world, the world of the world of the world, the world, the world of the world of the world, the world of the world, the world of the world of the"}, {"heading": "6. Conclusions", "text": "In this paper, we have proposed an efficient low-level kernel decomposition algorithm, equipped with a flexible, non-parametric reconstruction mechanism while being able to handle ancillary information. It shows significant performance gains in benchmark learning tasks. In the future, we will pair dictionary learning with specific classifiers such as an SVM to further improve prediction performance. Another interesting direction is learning a sparse dictionary and applying it to information gathering problems."}], "references": [{"title": "Fast computation of low rank matrix approximations", "author": ["D. Achlioptas", "F. McSherry"], "venue": "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,", "citeRegEx": "Achlioptas and McSherry,? \\Q2001\\E", "shortCiteRegEx": "Achlioptas and McSherry", "year": 2001}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bach and Jordan,? \\Q2002\\E", "shortCiteRegEx": "Bach and Jordan", "year": 2002}, {"title": "Predictive low-rank decomposition for kernel methods", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Bach and Jordan,? \\Q2005\\E", "shortCiteRegEx": "Bach and Jordan", "year": 2005}, {"title": "Twostage learning kernel algorithms", "author": ["C. Cortes", "M. Mohri", "A. Rostamizadeh"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "On kernel-target alignment", "author": ["N. Cristianini", "J. Shawe-taylor", "A. Elissee", "J. Kandola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Cristianini et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Cristianini et al\\.", "year": 2002}, {"title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning", "author": ["P. Drineas", "M.W. Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas and Mahoney,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney", "year": 2005}, {"title": "Liblinear: A library for large linear classification", "author": ["Fan", "R.-E", "Chang", "K.-W", "Hsieh", "C.-J", "X.R. Wang", "Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Efficient svm training using low-rank kernel representations", "author": ["S. Fine", "K. Scheinberg", "N. Cristianini", "J. Shawe-taylor", "B. Williamson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Fine et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Fine et al\\.", "year": 2001}, {"title": "Spectral grouping using the Nystr\u00f6m method", "author": ["C. Fowlkes", "S. Belongie", "F. Chung", "J. Malik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Fowlkes et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fowlkes et al\\.", "year": 2004}, {"title": "Diffusion kernels on graphs and other discrete input spaces", "author": ["R.I. Kondor", "J. Lafferty"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Kondor and Lafferty,? \\Q2007\\E", "shortCiteRegEx": "Kondor and Lafferty", "year": 2007}, {"title": "Low-rank kernel learning with bregman matrix divergences", "author": ["B. Kulis", "M.A. Sustik", "I.S. Dhillon"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Kulis et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kulis et al\\.", "year": 2009}, {"title": "Learning with idealized kenrels", "author": ["J. Kwok", "I. Tsang"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Kwok and Tsang,? \\Q2003\\E", "shortCiteRegEx": "Kwok and Tsang", "year": 2003}, {"title": "Stochastic low-rank kernel learning for regression", "author": ["P. Machart", "T. Peel", "S. Anthoine", "L. Ralaivola", "H. Glotin"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Machart et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Machart et al\\.", "year": 2011}, {"title": "Efficient methods in convex programming", "author": ["A. Nemirovski"], "venue": "Lecture Notes,", "citeRegEx": "Nemirovski,? \\Q1994\\E", "shortCiteRegEx": "Nemirovski", "year": 1994}, {"title": "Online learning in the manifold of low-rank matrices", "author": ["U. Shalit", "D. Weinshall", "G. Chechik"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Shalit et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Shalit et al\\.", "year": 2010}, {"title": "Stochastic low-rank kernel learning for regression", "author": ["A. Talwalkar", "S. Kumar", "H. Rowley"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Talwalkar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Talwalkar et al\\.", "year": 2008}, {"title": "The effect of the input density distribution on kernel-based classifiers", "author": ["C. Williams", "M. Seeger"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Williams and Seeger,? \\Q2000\\E", "shortCiteRegEx": "Williams and Seeger", "year": 2000}, {"title": "Using the Nystr\u00f6m method to speed up kernel machines", "author": ["C. Williams", "M. Seeger"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Williams and Seeger,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger", "year": 2001}, {"title": "Clustered Nystr\u00f6m method for large scale manifold learning and dimension reduction", "author": ["K. Zhang", "J. Kwok"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Zhang and Kwok,? \\Q2010\\E", "shortCiteRegEx": "Zhang and Kwok", "year": 2010}, {"title": "Scaling up kernel svm on limited resources: A low-rank linearization approach", "author": ["K. Zhang", "L. Lan", "Z. Wang", "F. Moerchen"], "venue": "Journal of Machine Learning Research Workshop and Conference Proceedings,", "citeRegEx": "Zhang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2012}, {"title": "Nonparametric transforms of graph kernels for semi-supervised learning", "author": ["X. Zhu", "J. Kandola", "Z. Ghahbramani", "J. Lafferty"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Zhu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": "Therefore the low-rank constraint has been widely applied to kernel learning problems (Kulis et al., 2009; Lanckriet et al., 2004; Shalit et al., 2010; Machart et al., 2011).", "startOffset": 86, "endOffset": 173}, {"referenceID": 14, "context": "Therefore the low-rank constraint has been widely applied to kernel learning problems (Kulis et al., 2009; Lanckriet et al., 2004; Shalit et al., 2010; Machart et al., 2011).", "startOffset": 86, "endOffset": 173}, {"referenceID": 12, "context": "Therefore the low-rank constraint has been widely applied to kernel learning problems (Kulis et al., 2009; Lanckriet et al., 2004; Shalit et al., 2010; Machart et al., 2011).", "startOffset": 86, "endOffset": 173}, {"referenceID": 8, "context": "Such a decomposition produces a compact representation of large matrices, which is the key to scaling up a great variety of kernel learning algorithms, with prominent examples including (Williams & Seeger, 2001; Fowlkes et al., 2004; Drineas & Mahoney, 2005; Fine et al., 2001; Achlioptas & McSherry, 2001).", "startOffset": 186, "endOffset": 306}, {"referenceID": 7, "context": "Such a decomposition produces a compact representation of large matrices, which is the key to scaling up a great variety of kernel learning algorithms, with prominent examples including (Williams & Seeger, 2001; Fowlkes et al., 2004; Drineas & Mahoney, 2005; Fine et al., 2001; Achlioptas & McSherry, 2001).", "startOffset": 186, "endOffset": 306}, {"referenceID": 15, "context": "The Nystr\u00f6m method is a sampling based approach and has gained great popularity in unsupervised kernel low-rank approximation, with both theoretical performance guarantees and empirical successes (Williams & Seeger, 2001; Drineas & Mahoney, 2005; Talwalkar et al., 2008).", "startOffset": 196, "endOffset": 270}, {"referenceID": 8, "context": "It originated from solving integral equations and was introduced to the machine learning community by (Williams & Seeger, 2001; Fowlkes et al., 2004; Drineas & Mahoney, 2005).", "startOffset": 102, "endOffset": 174}, {"referenceID": 15, "context": "It has drawn considerable interest in applications such as clustering and manifold learning (Talwalkar et al., 2008) (Zhang & Kwok, 2010), Gaussian processes (Williams & Seeger, 2001), and kernel methods (Fine et al.", "startOffset": 92, "endOffset": 116}, {"referenceID": 7, "context": ", 2008) (Zhang & Kwok, 2010), Gaussian processes (Williams & Seeger, 2001), and kernel methods (Fine et al., 2001).", "startOffset": 95, "endOffset": 114}, {"referenceID": 4, "context": "To achieve the second goal, we use the concept of kernel target alignment (Cristianini et al., 2002) and require that the reconstructed kernel, ElSE \u22a4 l , is close to the ideal kernel K\u2217 l defined on labeled samples.", "startOffset": 74, "endOffset": 100}, {"referenceID": 4, "context": "Note that in (Cristianini et al., 2002), the closeness between two kernel matrices is measured by their inner product \u27e8K,K \u2032\u27e9 = \u2211 i,j KijK \u2032 ij .", "startOffset": 13, "endOffset": 39}, {"referenceID": 13, "context": "We use the gradient mapping strategy (Nemirovski, 1994) that is composed of iterative gradient descent equipped with a projection step to find the optimal solution.", "startOffset": 37, "endOffset": 55}, {"referenceID": 13, "context": "The step length \u03b7 is determined by the ArmijoGoldstein rule (Nemirovski, 1994).", "startOffset": 60, "endOffset": 78}, {"referenceID": 13, "context": "One can also use more advanced approaches such as the Nesterov\u2019s method (Nemirovski, 1994) to improve the convergence rate.", "startOffset": 72, "endOffset": 90}, {"referenceID": 3, "context": "More specifically, we used normalized kernel alignment (NKA) (Cortes et al., 2010) between kernel matrices,", "startOffset": 61, "endOffset": 82}, {"referenceID": 3, "context": "The criterion (12) has the following properties: (1) it is scale invariant, and does not require any extra trade-off parameter due to its multiplicative form; (2) the first term measures the closeness between S and S0, related to unsupervised structures of kernel matrix; the second term is on the closeness between ElSE \u22a4 l and K \u2217 l , related to side information; therefore the criterion faithfully reflects what (6) optimizes but on the other hand is numerically different; (3) a higher alignment (second term in (12)) indicates existence of a good predictor with higher probability (Cortes et al., 2010); (4) computation of the criterion does not require any extra validation set, which is suited if only limited training samples are available.", "startOffset": 586, "endOffset": 607}, {"referenceID": 10, "context": "Although kernel learning has drawn considerable interest, algorithms on learning low-rank kernel matrices are not very abundant (Kulis et al., 2009), in particular those in a computationally efficient way.", "startOffset": 128, "endOffset": 148}, {"referenceID": 14, "context": "In (Shalit et al., 2010) an online learning algorithm is proposed on the manifold of low-rank matrices, which consists of iterative gradient step and second-order retraction.", "startOffset": 3, "endOffset": 24}, {"referenceID": 12, "context": "In (Machart et al., 2011), a novel low-rank kernel learning approach was proposed for regression via the use of conical combinations of base kernels and a stochastic optimization framework.", "startOffset": 3, "endOffset": 25}, {"referenceID": 4, "context": "In (Cristianini et al., 2002) (Cortes et al.", "startOffset": 3, "endOffset": 29}, {"referenceID": 3, "context": ", 2002) (Cortes et al., 2010), a nonparametric transform is computed by maximizing the alignment with the target.", "startOffset": 8, "endOffset": 29}, {"referenceID": 20, "context": "In (Zhu et al., 2004), an extra order constraint on the weight of eigenvectors was adopted.", "startOffset": 3, "endOffset": 21}, {"referenceID": 20, "context": "Due to the need to compute kernel eigenvalues, spectral kernel learning requires at least quadratic space and time complexities, or even higher if advanced optimization such as QCQP is involved (Zhu et al., 2004).", "startOffset": 194, "endOffset": 212}, {"referenceID": 20, "context": ", 2003); (4) Spectral: non-parametric spectral graph kernel (Zhu et al., 2004); (5) TSK: two stage kernel learning algorithm (Cortes et al.", "startOffset": 60, "endOffset": 78}, {"referenceID": 3, "context": ", 2004); (5) TSK: two stage kernel learning algorithm (Cortes et al., 2010); (6) Breg: low-rank kernel learning with Bregman divergence (Kulis et al.", "startOffset": 54, "endOffset": 75}, {"referenceID": 10, "context": ", 2010); (6) Breg: low-rank kernel learning with Bregman divergence (Kulis et al., 2009); (7) Our method.", "startOffset": 68, "endOffset": 88}, {"referenceID": 19, "context": "The resultant problem will be a linear SVM using G as training/testing samples (Zhang et al., 2012).", "startOffset": 79, "endOffset": 99}, {"referenceID": 3, "context": "For method (5) the base kernel are chosen from a set of RBF kernels whose widths are factors of the averaged pairwise distance as in (Cortes et al., 2010).", "startOffset": 133, "endOffset": 154}, {"referenceID": 6, "context": "For the regularization parameter C in linear SVM, we use the heuristic implemented in liblinear package (Fan et al., 2008).", "startOffset": 104, "endOffset": 122}], "year": 2012, "abstractText": "Low-rank matrix decomposition has gained great popularity recently in scaling up kernel methods to large amounts of data. However, some limitations could prevent them from working effectively in certain domains. For example, many existing approaches are intrinsically unsupervised, which does not incorporate side information (e.g., class labels) to produce task specific decompositions; also, they typically work \u201ctransductively\u201d, i.e., the factorization does not generalize to new samples, so the complete factorization needs to be recomputed when new samples become available. To solve these problems, in this paper we propose an \u201cinductive\u201d-flavored method for low-rank kernel decomposition with priors. We achieve this by generalizing the Nystr\u00f6m method in a novel way. On the one hand, our approach employs a highly flexible, nonparametric structure that allows us to generalize the low-rank factors to arbitrarily new samples; on the other hand, it has linear time and space complexities, which can be orders of magnitudes faster than existing approaches and renders great efficiency in learning a low-rank kernel decomposiAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s). tion. Empirical results demonstrate the efficacy and efficiency of the proposed method.", "creator": " TeX output 2012.05.08:1131"}}}