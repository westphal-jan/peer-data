{"id": "1206.4639", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jun-2012", "title": "Adaptive Regularization for Weight Matrices", "abstract": "Algorithms for learning distributions over weight-vectors, such as AROW were recently shown empirically to achieve state-of-the-art performance at various problems, with strong theoretical guaranties. Extending these algorithms to matrix models pose challenges since the number of free parameters in the covariance of the distribution scales as $n^4$ with the dimension $n$ of the matrix, and $n$ tends to be large in real applications. We describe, analyze and experiment with two new algorithms for learning distribution of matrix models. Our first algorithm maintains a diagonal covariance over the parameters and can handle large covariance matrices. The second algorithm factors the covariance to capture inter-features correlation while keeping the number of parameters linear in the size of the original matrix. We analyze both algorithms in the mistake bound model and show a superior precision performance of our approach over other algorithms in two tasks: retrieving similar images, and ranking similar documents. The factored algorithm is shown to attain faster convergence rate.", "histories": [["v1", "Mon, 18 Jun 2012 15:17:49 GMT  (341kb)", "http://arxiv.org/abs/1206.4639v1", "ICML2012"]], "COMMENTS": "ICML2012", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["koby crammer", "gal chechik"], "accepted": false, "id": "1206.4639"}, "pdf": {"name": "1206.4639.pdf", "metadata": {"source": "META", "title": "Adaptive Regularization for Weight Matrices", "authors": ["Koby Crammer", "Gal Chechik"], "emails": ["KOBY@EE.TECHNION.AC.IL", "GAL.CHECHIK@BIU.AC.IL"], "sections": [{"heading": null, "text": "We describe, analyze and experiment with two new algorithms for learning distribution of matrix models: our first algorithm maintains a diagonal covariance over the parameters and can handle large covariance matrices; the second algorithm takes covariance into account to capture the correlation between the characteristics while keeping the number of parameters linearly in the size of the original matrix; we analyze both algorithms in the fault-bound model and show superior precision of our approach over other algorithms in two tasks: retrieving similar images and ranking similar documents; and the factor algorithm shows that it achieves a faster convergence rate."}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Problem Setting", "text": "We focus on the problem of linear similarity between objects q, q, Rn, in the form of SW (q, p) = q > Wp. This similarity is measured by metric learning models of the form (q, p) > W (q, p) for square matrices W, and is considered equivalent if all vectors p and q have a constant W standard. Interestingly, the similarity measurement variable SW (q, p) does not have to be symmetrical, and can even be defined for objects of different dimensions m 6 = n (non-square W). Generally, it allows to learn a measure of relativization between objects from different domains, such as images and images and text (as in Grangier & Bengio, 2008). Significant if the vectors representing both query and object are economical."}, {"heading": "3. Adaptive Regularization of Weights", "text": "We begin by describing the AROW algorithm designed for the binary classification of vector inputs > > Inputs x > Rd and introduced by Crammer et al. (2009).The key idea of AROW (Dredze et al., 2008, and its predecessors) is that instead of maintaining a single vector w during learning, AROW maintains a distribution over possible models. \u2212 The AROW distribution over vectors designated by N (w, \u03a3), where w-Rd and \u03a3-Rd \u00b7 d \u00b7 d. The mean w encodes the algorithm's knowledge of weight characteristics (linear model) and is used to make predictions. Covariance implication captures the notion of confidence in weights and is used during training to determine an effective learning rate for characteristics with different statistics. AROW was motivated by tasks in natural language processing where many characteristics are very rare and some are predictable."}, {"heading": "4. Modeling Uncertainty over Matrices", "text": "As with online classification learning, online retrieval algorithms work in rounds. In round i, the algorithm receives a triplet consisting of a query Qi-Rm and two possible results p + i, p \u2212 i Rn. The algorithm does not output a single bit indicating which result is better for the given query. It then receives the correct answer and updates its model. For the sake of simplicity, we assume that the first result is always preferable, namely in view of Qi, that the algorithm p + iover p \u2212 i should rank. We now consider the problem of modeling uncertainty about matrices in the context of online learning similarities that (1) obey, and describe algorithms to minimize the loss in (2). A naive approach to modeling uncertainty about matrices would be the linearity of the ranking function SW (q, p) as a variance of matrices in W, c \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 x and write it as a product between W \u00b7 \u00b7 \u00b7 x \u00b7 x (W \u00b7 x) and W \u00b7 x x (W \u00b7 x)."}, {"heading": "4.1. Diagonal Covariance", "text": "Our first algorithm limits the covariance matrices to be described diagonally, using only mn = > q = > q = > qi = > qi elements (the size of the measure of similarity W), denominated by the diagonal elements of the covariance matrix. \u2212 The update (4) is then becomeswi = wi \u2212 1 + max (0, 1 \u2212 yix > i wi \u2212 1) sum (x > i \u2212 1) p (x > i + r yip \u2212 1) and the covariance matrix, which maintains one element per attribute and is thus diagonal like, even though it is rectangular in form. We identify xi = qip > i, pi = p + i \u2212 p \u2212 p \u2212 p to get the update."}, {"heading": "4.2. Factored Covariance", "text": "Our second approach to the distribution of similarity matrices is based on the factorization of the covariance matrix (= > Q = > Q =) in such a way that the correlations in the \"input matrix\" (right \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 P \u2212 \u2212 \u2212 \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212 P \u2212"}, {"heading": "5. Empirical Evaluation", "text": "We evaluated AROMA diagonally and took it into account using two sets of data. First, we learned a semantic similarity between pairs of images in the Caltech 256 dataset (Griffin et al., 2007). Second, we learned a similarity measurement between text documents based on the 20 newsgroups data collected by Lang (1995). In both tasks, we used standard 5-fold cross-validation and reported on the accuracy of the test set."}, {"heading": "5.1. Image similarity in the Caltech256 dataset", "text": "In fact, the majority of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move,"}, {"heading": "5.2. Document similarity, the 20 Newsgroups dataset", "text": "In a second set of experiments, we investigated the problem of learning a similarity measure between pairs of text documents. This task has numerous applications, such as finding content on the web that is related to a particular text document. In this data set, documents are divided into 20 classes, with about 1, 000 documents in each class. Two documents were found to be similar iff they share the same class labels. We used the 20 newsgroups record (Lang, 1995) and removed stubble words, but did not apply stem words. We conveyed high information about the identity of the class (about education) using the infogain criterion (Yang & Pedersen, 1997) and the selected features were standardized with tf-idf and then presented each document as a bag of words. The 20 newsgroups website suggests splitting the data into a turn and test sets."}, {"heading": "6. Summary", "text": "We presented two algorithms that learn distribution across matrices: both exceed state-of-the-art methods in two tasks and model the covariance of the matrix distribution using a linear number of parameters. Diagonal AROMA is likely to be superior when the variance of individual characteristics is large relative to trait dependencies, and factoredAROMA is expected to be superior when the data has strong correlations between characteristics, as in the Caltech256 data. Factored-AROMA converged faster as well."}], "references": [{"title": "A secondorder perceptron algorithm", "author": ["N. Cesa-Bianchi", "A. Conconi", "C. Gentile"], "venue": "Siam Journal of Commutation,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2005}, {"title": "An online algorithm for large scale image similarity learning", "author": ["G. Chechik", "V. Sharma", "U. Shalit", "S. Bengio"], "venue": "In NIPS,", "citeRegEx": "Chechik et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chechik et al\\.", "year": 2009}, {"title": "Adaptive regularization of weighted vectors", "author": ["K. Crammer", "A. Kulesza", "M. Dredze"], "venue": "In NIPS,", "citeRegEx": "Crammer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Crammer et al\\.", "year": 2009}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "In ICML,", "citeRegEx": "Davis et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Davis et al\\.", "year": 2007}, {"title": "Hierarchical semantic indexing for large scale image retrieval", "author": ["J. Deng", "A.C. Berg", "L. Fei-Fei"], "venue": "In cvpr,", "citeRegEx": "Deng et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2011}, {"title": "Confidenceweighted linear classification", "author": ["M. Dredze", "K. Crammer", "F. Pereira"], "venue": "In ICML,", "citeRegEx": "Dredze et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dredze et al\\.", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "In COLT, pp", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Caltech-256 object category dataset", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": "Technical Report 7694, California Institute of Technology,", "citeRegEx": "Griffin et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Griffin et al\\.", "year": 2007}, {"title": "Matrix Variate Distributions", "author": ["A.K. Gupta", "D.K. Nagar"], "venue": "Chapman and Hall/CRC,", "citeRegEx": "Gupta and Nagar,? \\Q1999\\E", "shortCiteRegEx": "Gupta and Nagar", "year": 1999}, {"title": "Online metric learning and fast similarity search", "author": ["P. Jain", "B. Kulis", "I. Dhillon", "K. Grauman"], "venue": "In NIPS22,", "citeRegEx": "Jain et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2008}, {"title": "K.Saenko, and T.Darrell. What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "author": ["B. Kulis"], "venue": "In CVPR,", "citeRegEx": "Kulis,? \\Q2011\\E", "shortCiteRegEx": "Kulis", "year": 2011}, {"title": "Learning to filter netnews", "author": ["K. Lang"], "venue": "In ICML, pp", "citeRegEx": "Lang,? \\Q1995\\E", "shortCiteRegEx": "Lang", "year": 1995}, {"title": "Learning multi-modal similarity", "author": ["McFee", "Brian", "Lanckriet", "Gert"], "venue": "In JMLR,", "citeRegEx": "McFee et al\\.,? \\Q2012\\E", "shortCiteRegEx": "McFee et al\\.", "year": 2012}, {"title": "Multiresolution gray-scale and rotation invariant texture classification with local binary patterns", "author": ["T. Ojala", "M. Pietikainen", "T. Maenpaa"], "venue": "IEEE tran. on pattern analysis and mach. intelligence,", "citeRegEx": "Ojala et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Ojala et al\\.", "year": 2002}, {"title": "New adaptive algorithms for online classification", "author": ["F. Orabona", "K. Crammer"], "venue": "In NIPS,", "citeRegEx": "Orabona and Crammer,? \\Q2010\\E", "shortCiteRegEx": "Orabona and Crammer", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Weinberger", "Kilian Q", "Blitzer", "John", "Saul", "Lawrence K"], "venue": "In NIPS,", "citeRegEx": "Weinberger et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weinberger et al\\.", "year": 2005}, {"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "In IJCAI,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "A comparative study on feature selection in text categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "In Machine learninginternational workshop,", "citeRegEx": "Yang and Pedersen,? \\Q1997\\E", "shortCiteRegEx": "Yang and Pedersen", "year": 1997}], "referenceMentions": [{"referenceID": 2, "context": "Abstract Algorithms for learning distributions over weight-vectors, such as AROW (Crammer et al., 2009) were recently shown empirically to achieve state-of-the-art performance at various problems, with strong theoretical guaranties.", "startOffset": 81, "endOffset": 103}, {"referenceID": 3, "context": "Many algorithms were developed for learning these two tasks, including online algorithms developed recently in the context of classification and ranking costs (Davis et al., 2007; Jain et al., 2008; Chechik et al., 2009).", "startOffset": 159, "endOffset": 220}, {"referenceID": 9, "context": "Many algorithms were developed for learning these two tasks, including online algorithms developed recently in the context of classification and ranking costs (Davis et al., 2007; Jain et al., 2008; Chechik et al., 2009).", "startOffset": 159, "endOffset": 220}, {"referenceID": 1, "context": "Many algorithms were developed for learning these two tasks, including online algorithms developed recently in the context of classification and ranking costs (Davis et al., 2007; Jain et al., 2008; Chechik et al., 2009).", "startOffset": 159, "endOffset": 220}, {"referenceID": 6, "context": "(2009) and the references therein), or using this information during training (Duchi et al., 2010) improves the convergence rate of the learning algorithms as well as the performance of the resulting classifiers.", "startOffset": 78, "endOffset": 98}, {"referenceID": 2, "context": "Methods to generate linear classifiers from data have flourished in the past decade, including SVMImportantly, when learning linear models, it was recently shown that modeling the second order information about the set of models (Crammer et al. (2009) and the references therein), or using this information during training (Duchi et al.", "startOffset": 230, "endOffset": 252}, {"referenceID": 16, "context": "A similar model was recently studied in different contexts (McFee & Lanckriet, 2012; Kulis et al., 2011; Weston et al., 2011).", "startOffset": 59, "endOffset": 125}, {"referenceID": 2, "context": "We first describe the AROW algorithm that was designed for binary classification of vector inputs x \u2208 R and introduced by Crammer et al. (2009).", "startOffset": 122, "endOffset": 144}, {"referenceID": 2, "context": "(3) was shown by Crammer et al. (2009) to be obtained by the update rule:", "startOffset": 17, "endOffset": 39}, {"referenceID": 2, "context": "AROW was shown to attain state-of-the-art performance on many problems (Crammer et al., 2009; Duchi et al., 2010) and its performance is analyzed both for full covariance matrices (Crammer et al.", "startOffset": 71, "endOffset": 113}, {"referenceID": 6, "context": "AROW was shown to attain state-of-the-art performance on many problems (Crammer et al., 2009; Duchi et al., 2010) and its performance is analyzed both for full covariance matrices (Crammer et al.", "startOffset": 71, "endOffset": 113}, {"referenceID": 2, "context": ", 2010) and its performance is analyzed both for full covariance matrices (Crammer et al., 2009) and diagonal covariance matrices (Orabona & Crammer, 2010).", "startOffset": 74, "endOffset": 96}, {"referenceID": 2, "context": "The proof of the theorem relies on the following lemma, which extends Lemma 4 used in the analysis of AROW (Crammer et al., 2009)", "startOffset": 107, "endOffset": 129}, {"referenceID": 0, "context": "1 of Cesa-Bianchi et al. (2005).", "startOffset": 5, "endOffset": 32}, {"referenceID": 7, "context": "First, we learned a semantic similarity between pairs of images in the Caltech-256 dataset (Griffin et al., 2007).", "startOffset": 91, "endOffset": 113}, {"referenceID": 7, "context": "First, we learned a semantic similarity between pairs of images in the Caltech-256 dataset (Griffin et al., 2007). Second, we learned a similarity measure between pairs of text documents using the 20-newsgroups data collected by Lang (1995). In both tasks we used standard 5-fold cross validation and report the precision on the test set.", "startOffset": 92, "endOffset": 241}, {"referenceID": 13, "context": "For edge histograms, we used uniform Local Binary Patterns (uLBPs) (Ojala et al., 2002), which estimate a texture histogram of a patch by considering differences in intensity at circular neighborhoods centered on each pixel.", "startOffset": 67, "endOffset": 87}, {"referenceID": 1, "context": "This approach has been found successful (for a related task) by Grangier & Bengio (2008) and Chechik et al. (2009). We used a 1000-sized codebook, with a median of 27 non-zero values per image and a maximum of 129.", "startOffset": 93, "endOffset": 115}, {"referenceID": 4, "context": "class labels (Deng et al., 2011).", "startOffset": 13, "endOffset": 32}, {"referenceID": 1, "context": "(2) OASIS: An online similarity model based on a ranking cost across triplets, similar to the setup studied here (Chechik et al., 2009).", "startOffset": 113, "endOffset": 135}, {"referenceID": 3, "context": "(3) ITML/LEGO An online approach that succeeds to maintain a proper metric during learning in an efficient way (Davis et al., 2007) (4) LMNN: Large Margin Nearest neighbor, one of the early large margin metric learning methods (Weinberger et al.", "startOffset": 111, "endOffset": 131}, {"referenceID": 15, "context": ", 2007) (4) LMNN: Large Margin Nearest neighbor, one of the early large margin metric learning methods (Weinberger et al., 2005).", "startOffset": 103, "endOffset": 128}, {"referenceID": 11, "context": "We used the 20 newsgroups data set (Lang, 1995) and removed stop words but did not apply stemming.", "startOffset": 35, "endOffset": 47}], "year": 2012, "abstractText": "Algorithms for learning distributions over weight-vectors, such as AROW (Crammer et al., 2009) were recently shown empirically to achieve state-of-the-art performance at various problems, with strong theoretical guaranties. Extending these algorithms to matrix models pose challenges since the number of free parameters in the covariance of the distribution scales as n with the dimension n of the matrix, and n tends to be large in real applications. We describe, analyze and experiment with two new algorithms for learning distribution of matrix models. Our first algorithm maintains a diagonal covariance over the parameters and can handle large covariance matrices. The second algorithm factors the covariance to capture inter-features correlation while keeping the number of parameters linear in the size of the original matrix. We analyze both algorithms in the mistake bound model and show a superior precision performance of our approach over other algorithms in two tasks: retrieving similar images, and ranking similar documents. The factored algorithm is shown to attain faster convergence rate.", "creator": "LaTeX with hyperref package"}}}