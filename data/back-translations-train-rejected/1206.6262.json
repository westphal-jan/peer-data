{"id": "1206.6262", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Scaling Life-long Off-policy Learning", "abstract": "We pursue a life-long learning approach to artificial intelligence that makes extensive use of reinforcement learning algorithms. We build on our prior work with general value functions (GVFs) and the Horde architecture. GVFs have been shown able to represent a wide variety of facts about the world's dynamics that may be useful to a long-lived agent (Sutton et al. 2011). We have also previously shown scaling - that thousands of on-policy GVFs can be learned accurately in real-time on a mobile robot (Modayil, White &amp; Sutton 2011). That work was limited in that it learned about only one policy at a time, whereas the greatest potential benefits of life-long learning come from learning about many policies in parallel, as we explore in this paper. Many new challenges arise in this off-policy learning setting. To deal with convergence and efficiency challenges, we utilize the recently introduced GTD({\\lambda}) algorithm. We show that GTD({\\lambda}) with tile coding can simultaneously learn hundreds of predictions for five simple target policies while following a single random behavior policy, assessing accuracy with interspersed on-policy tests. To escape the need for the tests, which preclude further scaling, we introduce and empirically vali- date two online estimators of the off-policy objective (MSPBE). Finally, we use the more efficient of the two estimators to demonstrate off-policy learning at scale - the learning of value functions for one thousand policies in real time on a physical robot. This ability constitutes a significant step towards scaling life-long off-policy learning.", "histories": [["v1", "Wed, 27 Jun 2012 13:27:56 GMT  (3675kb,D)", "http://arxiv.org/abs/1206.6262v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["adam white", "joseph modayil", "richard s sutton"], "accepted": false, "id": "1206.6262"}, "pdf": {"name": "1206.6262.pdf", "metadata": {"source": "CRF", "title": "Scaling Life-long Off-policy Learning", "authors": ["Adam White", "Joseph Modayil"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "Lifelong learning is an artificial intelligence approach that relies on learning from a long stream of sensor-imotor strategies generated by an agent that interacts with its environment. Lifelong learning emphasizes continuous learning by an autonomous agent over long periods of time, perhaps months or years. This big data problem requires algorithms that efficiently learn a variety of different facts about the large flow of sensor-imotor data. We have a novel approach that uses expressive language to represent sensor-imotor knowledge acquisition and verification based on algorithms that require lifelong interaction of sensor-imotor interactions with a mobile robot. General Value Functions (GVFS) provide an expressive language for the representation of sensor-imotor knowledge about long-lived interaction with the world (Sutton et al. 2011). Knowledge is presented as a pristine value function with a reward function and a pseudo-imotor function."}, {"heading": "2 On-policy and off-policy prediction with value", "text": "The interaction between an agent and its environment is modeled as a discrete time dynamic system with function approximation. At each discrete time step, the agent observes a feature vector, which is calculated from information available to the agent and therefore only implicitly characterizes a function of the current state of the environment. Note: The agent has no access to the underlying environmental space S or the current state st-S; the observed feature vector is calculated from information available to the agent and is therefore only implicitly a function of the current state of the environment. Note: At each time step, the agent takes action in the underlying environmental space S, and the environmental transfers into a new state, which produces a new feature vector. In conventional amplification learning, we try to predict the entire future Discounted Reward, where the Reward Forecast-Prognostic Signal is a predictive signal that we receive from the environment."}, {"heading": "3 An architecture for large-scale, real-time off-", "text": "We imagine an architecture in which many predictive questions are posed and answered in a generalized form of the value function. Each such function, called vi: S \u2192 R, predicts the expected discounted sum of the future measurements of a sensor. The ith value function refers to the sensor measurements r (i) t, the political equation (i) and the time scale \u03b3 (i): vi (s) = E\u03c0 (i). Questions about what will happen to the robot if it follows a behavior that differs from its current behavior can be used non-political methods to learn approximate answers v (i) and the time scale \u03b3 (i) regarding predictive questions in the form of such value functions."}, {"heading": "4 Large-scale off-policy prediction on a robot", "text": "The first question we are considering is whether the Horde architecture produces large-scale off-policy predictions in real time on a physical robot = 457 components. All of our assessments were performed on a customized holonomic mobile robot (see Figure 2).The robot has a variety of 53 sensors for detecting external beings (ambient light, heat, infrared light, magnetic fields and infrared reflection) and also its internal status (battery voltages, acceleration, rotation speed, motor speeds, motor temperatures and motor voltages).The robot can dock to its charging station and can continuously work for its charging station without there being a recharge.The raw sensor vector has been converted into features by tile coding. This produced a binary vector that is equipped with a constant number of 1 features."}, {"heading": "5 An online measure of off-policy learning", "text": "The accuracy of the predictions learned in the previous experiment was assessed using the return error observed during the on-site test excursions. These tests consume considerable wall clock time, as for each sample the target policy system must follow long enough to capture most of the probability mass of the infinite sample yield and multiple samples are required to estimate the NMSRE. Depending on how often the tests are performed, there is a trade-off for how often the NMSRE is updated and on the time scale given by the government. There are other subtle deficits with on-policy tests. The experimenter must choose a test regime and frequency. Depending on how often the tests are performed, there is a trade-off for how often the NMSRE is updated. Changes in the environment and new robot experiences can cause inaccurate NMSRE estimates if the majority of the time steps are used for training."}, {"heading": "6 Large-scale off-policy prediction, with many", "text": "In this section, we will consider scaling the number of target policies and scaling the predictive timescales (order of \u03b3).To expand the space of target policies while maintaining a small number of finite measures, we will consider discrete action as a linear parameterization of Gibbs \"political distributions: \u03c0u (a) = exp (\u2212 u > a). a\" A \"exp\" (\u2212 u > a \") where u is a vector of political parameters. The characteristic vector for each action, a\" Rn \"| A,\" displays a copy of the subvector in an otherwise zero vector; and for each action, the copy is balanced by n, so that a 6 = a. \"= > a\" MSquisition \"= 0.\" Random strategies are generated by randomly selecting 60 components of u and drawing each value independently from the uniform distribution architecture."}, {"heading": "7 Related Work", "text": "Many of the ideas in this paper have precursors in literature; the idea of policy predictions was developed along with the temporal abstraction option framework (Sutton, Precup & Singh, 1999); non-policy learning with alignment of functions was developed through meaning samples (Precup et al., 2006) in an approach that runs online but can exhibit exponentially slow learning progress; learning through many different strategies can also support active exploration, an idea that has been explored in related work on curiosity-based learning (Singh et al., 2005); the idea of creating models from data has been explored, but not in the non-political real-time environment; Thrun and Mitchell (1995) demonstrated the learning of sensor models offline; many Bayesian approaches to online government estimation in robotics (e.g. Thrun et al al., 2005) show that an enormous volume of observations can take place in real-time, but cannot learn system dynamics."}, {"heading": "8 Conclusions and future work", "text": "We have shown that gradient TD methods can be used to learn hundreds of time-extended, policy-driven predictions from out-of-policy samples. To achieve this goal, several challenges had to be solved that are unique to the offpolicy setting. Most significant is the online estimation of non-political learning progress based on the Bellman error, which does not increase the arithmetic complexity of the Horde architecture without interrupting learning and corresponds to the traditional average square prediction error. Adding what-if questions dramatically increases the scope and scale of questions that can be learned by the Horde by providing further evidence of the Horde's role in lifelong learning. Our experiments on a robot are limited, but there are several immediate instructions for future work. The questions learned here are predictive questions about a policy, general value functions can also support learning of predictive ability strategies (using GTGQ), which should be significantly enhanced in 2011."}, {"heading": "9 References", "text": "Atkeson, C. G., Schaal, S. (1997). Robot learning from demonstration. In Proc. 14th Int. Conf. on Machine Learning, pp. 12-20.Baird, L. C. (1995). Resdual algorithms: Reinforcement learning with function approximation. In Proc. 12th Int. Conf. on Machine Learning, pp. 30-37.Boots, B., Siddiqi, S., Gordon, G. (2011). An online spectral learning algorithm for partially observable nonlinear dynamical systems. In Proc. Conf. of the Association for the Advancement of Artificial Intelligence, pp. 30-37.Boots, B., Siddiqi, N., Langford, J. J. (2011). Parallel Online Learning. In The Computing Research Repository.Kalman, R. (1960). A new approach to linear filtering and prediction problems. Trans."}], "references": [{"title": "Robot learning from demonstration", "author": ["C.G. Atkeson", "S. Schaal"], "venue": "Proc. 14th Int. Conf. on Machine Learning, pp. 12\u201320.", "citeRegEx": "Atkeson and Schaal,? 1997", "shortCiteRegEx": "Atkeson and Schaal", "year": 1997}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L.C. Baird"], "venue": "Proc. 12th Int. Conf. on Machine Learning, pp. 30\u201337.", "citeRegEx": "Baird,? 1995", "shortCiteRegEx": "Baird", "year": 1995}, {"title": "An online spectral learning algorithm for partially observable nonlinear dynamical systems", "author": ["B. Boots", "S. Siddiqi", "G. Gordon"], "venue": "Proc. Conf. of the Association for the Advancement of Artificial Intelligence.", "citeRegEx": "Boots et al\\.,? 2011", "shortCiteRegEx": "Boots et al\\.", "year": 2011}, {"title": "Parallel Online Learning", "author": ["D. Hsu", "N. Karampatziakis", "J. Langford", "A.J. Smola"], "venue": "The Computing Research Repository.", "citeRegEx": "Hsu et al\\.,? 2011", "shortCiteRegEx": "Hsu et al\\.", "year": 2011}, {"title": "A new approach to linear filtering and prediction problems", "author": ["R.E. Kalman"], "venue": "Trans. ASME, Journal of Basic Engineering 82:35\u201345.", "citeRegEx": "Kalman,? 1960", "shortCiteRegEx": "Kalman", "year": 1960}, {"title": "Policy search for motor primitives in robotics", "author": ["J. Kober", "J. Peters"], "venue": "Machine Learning 84:171\u2013203.", "citeRegEx": "Kober and Peters,? 2011", "shortCiteRegEx": "Kober and Peters", "year": 2011}, {"title": "The fixed points of off-policy TD", "author": ["J.Z. Kolter"], "venue": "Advances in Neural Information Processing Systems 24.", "citeRegEx": "Kolter,? 2011", "shortCiteRegEx": "Kolter", "year": 2011}, {"title": "Predictive representations of state", "author": ["M.L. Littman", "R.S. Sutton", "S. Singh"], "venue": "Advances in Neural Information Processing Systems 14, pp. 1555\u2013 1561.", "citeRegEx": "Littman et al\\.,? 2002", "shortCiteRegEx": "Littman et al\\.", "year": 2002}, {"title": "Gradient Temporal-Difference Learning Algorithms", "author": ["H.R. Maei"], "venue": "PhD", "citeRegEx": "Maei,? 2011", "shortCiteRegEx": "Maei", "year": 2011}, {"title": "Multi-timescale Nexting in a Reinforcement Learning Robot", "author": ["J. Modayil", "A. White", "R.S. Sutton"], "venue": "Proc. 12th Int. Conf. on Adaptive Behaviour.", "citeRegEx": "Modayil et al\\.,? 2012", "shortCiteRegEx": "Modayil et al\\.", "year": 2012}, {"title": "Off-policy learning with recognizers", "author": ["D. Precup", "R.S. Sutton", "C. Paduraru", "A. Koop", "S. Singh"], "venue": "Advances in Neural Information Processing Systems 18.", "citeRegEx": "Precup et al\\.,? 2006", "shortCiteRegEx": "Precup et al\\.", "year": 2006}, {"title": "Multitask learning without label correspondences", "author": ["N. Quadrianto", "A. Smola", "T. Caetano", "S.V.N. Vishwanathan", "J. Petterson"], "venue": "Advances in Neural Information Processing Systems 23, pp. 1957\u20131965.", "citeRegEx": "Quadrianto et al\\.,? 2010", "shortCiteRegEx": "Quadrianto et al\\.", "year": 2010}, {"title": "Intrinsically motivated reinforcement learning", "author": ["Singh S.", "A.G. Barto", "N. Chentanez"], "venue": "Advances in Neural Information Processing Systems 17, pp. 1281\u20131288.", "citeRegEx": "S. et al\\.,? 2005", "shortCiteRegEx": "S. et al\\.", "year": 2005}, {"title": "Learning to predict by the method of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning 3:9\u201344.", "citeRegEx": "Sutton,? 1988", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press.", "citeRegEx": "Sutton and Barto,? 1998", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "Precup D.", "S. Singh"], "venue": "Artificial Intelligence 112:181\u2013211.", "citeRegEx": "Sutton et al\\.,? 1999", "shortCiteRegEx": "Sutton et al\\.", "year": 1999}, {"title": "Temporal-difference networks", "author": ["R.S. Sutton", "B. Tanner"], "venue": "Advances in Neural Information Processing Systems 17, pp. 1377\u20131384.", "citeRegEx": "Sutton and Tanner,? 2005", "shortCiteRegEx": "Sutton and Tanner", "year": 2005}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "Szepesv\u00e1ri", "Cs.", "E. Wiewiora"], "venue": "Proc. 26th Int. Conf. on Machine Learning.", "citeRegEx": "Sutton et al\\.,? 2009", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction", "author": ["R.S. Sutton", "J. Modayil", "M. Delp", "T. Degris", "P.M. Pilarski", "A. White", "D. Precup"], "venue": "Proc. 10th Int. Conf. on Autonomous Agents and Multiagent Systems.", "citeRegEx": "Sutton et al\\.,? 2011", "shortCiteRegEx": "Sutton et al\\.", "year": 2011}, {"title": "Learning to make predictions in partially observable environments without a generative model", "author": ["E. Talvitie", "S. Singh"], "venue": "Journal of Artificial Intelligence Research 42:353\u2013392.", "citeRegEx": "Talvitie and Singh,? 2011", "shortCiteRegEx": "Talvitie and Singh", "year": 2011}, {"title": "Probabilistic Robotics", "author": ["S. Thrun", "W. Burgard", "D. Fox"], "venue": "MIT Press.", "citeRegEx": "Thrun et al\\.,? 2005", "shortCiteRegEx": "Thrun et al\\.", "year": 2005}, {"title": "Lifelong robot learning", "author": ["S. Thrun", "T. Mitchell"], "venue": "Robotics and Autonomous Systems.", "citeRegEx": "Thrun and Mitchell,? 1995", "shortCiteRegEx": "Thrun and Mitchell", "year": 1995}], "referenceMentions": [{"referenceID": 18, "context": "GVFs have been shown able to represent a wide variety of facts about the world\u2019s dynamics that may be useful to a long-lived agent (Sutton et al. 2011).", "startOffset": 131, "endOffset": 151}, {"referenceID": 18, "context": "General value functions (GVFs) provide an expressive language for representing sensorimotor knowledge about a long-lived agent\u2019s interaction with the world (Sutton et al. 2011).", "startOffset": 156, "endOffset": 176}, {"referenceID": 8, "context": "off-policy sampling with function approximation (Maei, 2011).", "startOffset": 48, "endOffset": 60}, {"referenceID": 13, "context": "GTD(\u03bb) is an incremental prediction algorithm, similar to TD(\u03bb) (Sutton, 1988), except with an additional secondary set of learned weights w, and an additional step size parameter \u03b1w.", "startOffset": 64, "endOffset": 78}, {"referenceID": 7, "context": "This enables the use of predictive state information (Littman et al., 2002) and learning of compositional predictions, similar to a TD network (Sutton and Tanner, 2005).", "startOffset": 53, "endOffset": 75}, {"referenceID": 16, "context": ", 2002) and learning of compositional predictions, similar to a TD network (Sutton and Tanner, 2005).", "startOffset": 75, "endOffset": 100}, {"referenceID": 12, "context": "This architecture, called Horde by Sutton et al. (2011), has several desirable characteristics.", "startOffset": 35, "endOffset": 56}, {"referenceID": 13, "context": "Using the derivation given by Sutton et al. (2009), we can rewrite this error in terms of expectations:", "startOffset": 30, "endOffset": 51}, {"referenceID": 13, "context": "We used the inverted feature representation of Sutton et al (2009). To determine the validity of our new measures we compared the vector and scalar MSPBE estimates with the true MSPBE (Equation 4) and the expensive sample estimate of the MSPBE (Equation 9).", "startOffset": 47, "endOffset": 67}, {"referenceID": 10, "context": "Learning off-policy under function approximation was developed by importance sampling (Precup et al., 2006) in an approach that runs online, but can exhibit exponentially slow learning progress.", "startOffset": 86, "endOffset": 107}, {"referenceID": 18, "context": "The previous work that introduced the Horde architecture (Sutton et al., 2011) also demonstrated parallel off-policy learning on a robot.", "startOffset": 57, "endOffset": 78}, {"referenceID": 13, "context": "Thrun and Mitchell (1995) showed learning of sensor models offline.", "startOffset": 0, "endOffset": 26}, {"referenceID": 0, "context": "Atkeson and Schaal (1997) showed learning of small one-time-step models.", "startOffset": 0, "endOffset": 26}, {"referenceID": 0, "context": "Atkeson and Schaal (1997) showed learning of small one-time-step models. Kober and Peters (2011) showed on-policy episodic learning on a robot.", "startOffset": 0, "endOffset": 97}, {"referenceID": 8, "context": "The questions learned here are predictive questions about a policy, general value functions can also support learning control policies using greedy-gq (Maei 2011) a control variant of GTD(\u03bb) with the same linear complexity.", "startOffset": 151, "endOffset": 162}], "year": 2012, "abstractText": "We pursue a life-long learning approach to artificial intelligence that makes extensive use of reinforcement learning algorithms. We build on our prior work with general value functions (GVFs) and the Horde architecture. GVFs have been shown able to represent a wide variety of facts about the world\u2019s dynamics that may be useful to a long-lived agent (Sutton et al. 2011). We have also previously shown scaling\u2014that thousands of on-policy GVFs can be learned accurately in real-time on a mobile robot (Modayil, White & Sutton 2011). That work was limited in that it learned about only one policy at a time, whereas the greatest potential benefits of life-long learning come from learning about many policies in parallel, as we explore in this paper. Many new challenges arise in this off-policy learning setting. To deal with convergence and efficiency challenges, we utilize the recently introduced GTD(\u03bb) algorithm. We show that GTD(\u03bb) with tile coding can simultaneously learn hundreds of predictions for five simple target policies while following a single random behavior policy, assessing accuracy with interspersed on-policy tests. To escape the need for the tests, which preclude further scaling, we introduce and empirically validate two online estimators of the off-policy objective (MSPBE). Finally, we use the more efficient of the two estimators to demonstrate off-policy learning at scale\u2014the learning of value functions for one thousand policies in real time on a physical robot. This ability constitutes a significant step towards scaling life-long off-policy learning.", "creator": "LaTeX with hyperref package"}}}