{"id": "1206.6403", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Two Step CCA: A new spectral method for estimating vector models of words", "abstract": "Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner. For example, for text applications where the words lie in a very high dimensional space (the size of the vocabulary), one can learn a low rank \"dictionary\" by an eigen-decomposition of the word co-occurrence matrix (e.g. using PCA or CCA). In this paper, we present a new spectral method based on CCA to learn an eigenword dictionary. Our improved procedure computes two set of CCAs, the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our Two Step CCA (TSCCA) procedure on the tasks of POS tagging and sentiment classification.", "histories": [["v1", "Wed, 27 Jun 2012 19:59:59 GMT  (579kb)", "http://arxiv.org/abs/1206.6403v1", "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)"]], "COMMENTS": "Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012)", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["paramveer dhillon", "jordan rodu", "dean foster", "lyle ungar"], "accepted": false, "id": "1206.6403"}, "pdf": {"name": "1206.6403.pdf", "metadata": {"source": "META", "title": "Two Step CCA: A new spectral method for estimating vector models of words", "authors": ["Paramveer S. Dhillon"], "emails": ["dhillon@cis.upenn.edu", "jrodu@wharton.upenn.edu", "foster@wharton.upenn.edu", "ungar@cis.upenn.edu"], "sections": [{"heading": "1. Introduction and Related Work", "text": "It is indeed the case that we will be able to go in search of a solution that will enable us, that will enable us to find a solution that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution that we are able to find a solution. \""}, {"heading": "2. Problem Formulation", "text": "\"Our goal is to estimate a vector for each word that captures the distribution properties of that word in the form of a low-dimensional representation of the correlation between that word and the words in its immediate context. Formally, one considers a document (in practice a concatenation of a large number of documents) consisting of n tokens {w1, w2, wn}, each single word from a vocabulary of v words. Define the left and right contexts of each token wi like the h words on the left or right side of that token. The context is located in a very high-dimensional space, because for a vocabulary of size v, each of the 2h words in the combined context requires an indicator function of dimension. The tokens themselves sit in a v-dimensional space of words that we want to project onto a k-dimensional space. We call the mapping of word types to their latent-vectors that define the self-dictionary space that we define for defining the word space we define. For the series of documents that contain the distributions that we define."}, {"heading": "3. Two Step CCA (TSCCA) Algorithm", "text": "This year, it's about half, but not half, \"he said."}, {"heading": "3.1. Practical Considerations", "text": "As mentioned in Section 2, CCA (either in one step or in two steps) is essentially done by the singular value division of a matrix. For small matrices, this can be done with standard functions in e.g. MATLAB, but for very large matrices (e.g. for vocabulary of tens or hundreds of thousands of words) it is important to take advantage of recent advances in SVD algorithms. For the experiments presented in this paper, we use the method of (Halko et al., 2011), which uses random predictions to calculate SVD of large matrices."}, {"heading": "4. Experimental Results", "text": "This section describes the performance (accuracy and richness of representation) of our vocabulary learned through CCA. We evaluate the quality of the vocabulary by using it in a controlled learning environment to predict a variety of markups that can be assigned to words. For convenience, all the results shown here are mapped from word type to markup3; i.e. any vocabulary is conceivable to learn proper vocabulary books that assign each token to a label, such as how it is done by (Dhillon et al., 2011), but that is not the focus of this paper3. It is assumed that the type will be a single POS tag or some kind of sentiment.First, we compare the One-Step vs. Two Step CCA (TSCCA) procedures on a set of text modules (POS) tagging problems for different languages, looking at how predictive accuracy of the corpus size will be predicted on a prefix vocabulary size."}, {"heading": "4.1. POS Tagging: One step CCA (OSCCA) vs. Two step CCA (TSCCA)", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "4.2. Sentiment Classification Task: Richness of state learned by CCA", "text": "In the above areas, we are dealing with some of the phrases, but the states appreciated by CCA are much richer. We illustrate this by developing predictive models in English for a number of people who do not feel able to identify POS types, so when the same word occurs in education and in the exam, a learning algorithm that focuses only on education. It is often useful to use words in semantic categories such as colors or numbers to compare them with others, such as CW (Collobert & Weston, 2008) and HLBL (HLBL)."}, {"heading": "5. Conclusion", "text": "In this paper, we proposed a new and improved spectral method, a two-step alternative (TSCCA) to the CCA standard (OSCCA), which can be used in areas such as text / NLP that contain word sequences and in which one has three views (the left context, the right context and the words that are of interest themselves). Theoretically, we demonstrated that the proprietary vocabulary books learned by TSCCA provide more accurate state estimates (lower sample complexity) for small businesses than standard OSCCA. This was demonstrated by the superior empirical performance of TSCCA compared to OSCCA and too simple PCA in abandoning the POS tag, especially when less unlabeled data was used to learn the PCA or CCA representations. We also empirically demonstrated that the vector representations learned by CCA are much richer and contain more discriminatory information than the PCA-learnt representations, as well as taking into account other advanced settings of the CCA and the CCA-related word."}], "references": [{"title": "Floresta sinta(c)tica: a treebank for portuguese", "author": ["S. Afonso", "E. Bick", "R. Haber", "D. Santos"], "venue": "Proc. LREC, pp. 1698\u20131703,", "citeRegEx": "Afonso et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Afonso et al\\.", "year": 2002}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R. Ando", "T. Zhang"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ando and Zhang,? \\Q2005\\E", "shortCiteRegEx": "Ando and Zhang", "year": 2005}, {"title": "Class-based n-gram models of natural language", "author": ["P. Brown", "deSouza", "R. Mercer", "Pietra", "V. Della", "J. Lai"], "venue": "Comput. Linguist.,", "citeRegEx": "Brown et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chang", "Chih-Chung", "Lin", "Chih-Jen"], "venue": null, "citeRegEx": "Chang et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2001}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML \u201908,", "citeRegEx": "Collobert and Weston,? \\Q2008\\E", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Multi-view learning of word embeddings via cca", "author": ["Dhillon", "Paramveer S", "Foster", "Dean", "Ungar", "Lyle"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Dhillon et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2011}, {"title": "Using latent semantic analysis to improve access to textual information", "author": ["S. Dumais", "G. Furnas", "T. Landauer", "S. Deerwester", "R. Harshman"], "venue": "In SIGCHI Conference on human factors in computing systems,", "citeRegEx": "Dumais et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Dumais et al\\.", "year": 1988}, {"title": "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions", "author": ["Halko", "Nathan", "Martinsson", "Per-Gunnar", "Tropp", "Joel A"], "venue": "SIAM Rev,", "citeRegEx": "Halko et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Halko et al\\.", "year": 2011}, {"title": "Sparse cca for bilingual word generation", "author": ["Hardoon", "David", "Shawe-Taylor", "John"], "venue": "In EURO Mini Conference, Continuous Optimization and KnowledgeBased Technologies,", "citeRegEx": "Hardoon et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2008}, {"title": "Canonical correlation analysis (cca)", "author": ["H. Hotelling"], "venue": "Journal of Educational Psychology,", "citeRegEx": "Hotelling,? \\Q1935\\E", "shortCiteRegEx": "Hotelling", "year": 1935}, {"title": "A spectral algorithm for learning hidden markov models", "author": ["D. Hsu", "S. Kakade", "T. Zhang"], "venue": "In COLT,", "citeRegEx": "Hsu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsu et al\\.", "year": 2009}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["Kakade", "S M", "Foster", "Dean P"], "venue": "COLT, volume 4539 of Lecture Notes in Computer Science,", "citeRegEx": "Kakade et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2007}, {"title": "The danish dependency treebank and the underlying linguistic theory. in second workshop on treebanks and linguistic theories (tlt)", "author": ["Kromann", "Matthias T"], "venue": "In In Proc. LREC,", "citeRegEx": "Kromann and T.,? \\Q2003\\E", "shortCiteRegEx": "Kromann and T.", "year": 2003}, {"title": "Svd and clustering for unsupervised pos tagging", "author": ["Lamar", "Michael", "Maron", "Yariv", "Johnson", "Mark", "Bienenstock", "Elie"], "venue": "ACL Short \u201910,", "citeRegEx": "Lamar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lamar et al\\.", "year": 2010}, {"title": "Building a large annotated corpus of english: the penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Comput. Linguist.,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Tagging english text with a probabilistic model", "author": ["Merialdo", "Bernard"], "venue": "Comput. Linguist.,", "citeRegEx": "Merialdo and Bernard.,? \\Q1994\\E", "shortCiteRegEx": "Merialdo and Bernard.", "year": 1994}, {"title": "Three new graphical models for statistical language modelling", "author": ["A. Mnih", "G. Hinton"], "venue": "ICML \u201907,", "citeRegEx": "Mnih and Hinton,? \\Q2007\\E", "shortCiteRegEx": "Mnih and Hinton", "year": 2007}, {"title": "Distributional clustering of English words", "author": ["F. Pereira", "N. Tishby", "L. Lee"], "venue": "In 31st Annual Meeting of the ACL,", "citeRegEx": "Pereira et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1993}, {"title": "Flourish: A Visionary New Understanding of Happiness and Well-being", "author": ["Seligman", "Martin"], "venue": null, "citeRegEx": "Seligman and Martin.,? \\Q2011\\E", "shortCiteRegEx": "Seligman and Martin.", "year": 2011}, {"title": "Building a linguistically interpreted corpus of bulgarian: the bultreebank", "author": ["K. Simov", "P. Osenova", "M. Slavcheva", "S. Kolkovska", "E. Balabanova", "D. Doikoff", "K. Ivanova", "A. Simov", "E. Simov", "M. Kouylekov"], "venue": "Proc. LREC,", "citeRegEx": "Simov et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Simov et al\\.", "year": 2002}, {"title": "Contrastive estimation: training log-linear models on unlabeled data", "author": ["Smith", "Noah A", "Eisner", "Jason"], "venue": "ACL \u201905,", "citeRegEx": "Smith et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2005}, {"title": "Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data", "author": ["J. Suzuki", "H. Isozaki"], "venue": null, "citeRegEx": "Suzuki and Isozaki,? \\Q2008\\E", "shortCiteRegEx": "Suzuki and Isozaki", "year": 2008}, {"title": "The structure of scientific articles", "author": ["Teufel", "Simone"], "venue": "CSLI Publications,", "citeRegEx": "Teufel and Simone.,? \\Q2010\\E", "shortCiteRegEx": "Teufel and Simone.", "year": 2010}, {"title": "Word representations: a simple and general method for semisupervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "ACL \u201910,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: vector space models of semantics", "author": ["P.D. Turney", "P. Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "Brown Clustering (Brown et al., 1992; Pereira et al., 1993).", "startOffset": 17, "endOffset": 59}, {"referenceID": 17, "context": "Brown Clustering (Brown et al., 1992; Pereira et al., 1993).", "startOffset": 17, "endOffset": 59}, {"referenceID": 6, "context": "Latent Semantic Analysis/Latent Semantic Indexing (LSA/LSI) (Dumais et al., 1988) and Low Rank Multi-View Learning (LR-MVL) (Dhillon et al.", "startOffset": 60, "endOffset": 81}, {"referenceID": 5, "context": ", 1988) and Low Rank Multi-View Learning (LR-MVL) (Dhillon et al., 2011).", "startOffset": 50, "endOffset": 72}, {"referenceID": 5, "context": "Our main focus in this paper is on eigen-decomposition based methods, as they have been shown to be fast and scalable for learning from large amounts of unlabeled data (Turney & Pantel, 2010; Dhillon et al., 2011), have a strong theoretical grounding, and are guaranteed to converge to globally optimal solutions (Hsu et al.", "startOffset": 168, "endOffset": 213}, {"referenceID": 10, "context": ", 2011), have a strong theoretical grounding, and are guaranteed to converge to globally optimal solutions (Hsu et al., 2009).", "startOffset": 107, "endOffset": 125}, {"referenceID": 9, "context": "We will do this using Canonical Correlation Analysis (CCA) (Hotelling, 1935; Hardoon & Shawe-Taylor, 2008), by taking the CCA between the combined left and right contexts C = [L R] and their associated tokens, W.", "startOffset": 59, "endOffset": 106}, {"referenceID": 9, "context": "CCA (Hotelling, 1935) is the analog to Principal Component Analysis (PCA) for pairs of matrices.", "startOffset": 4, "endOffset": 21}, {"referenceID": 7, "context": "For the experiments presented in this paper we use the method of (Halko et al., 2011), which uses random projections to compute SVD of large matrices.", "startOffset": 65, "endOffset": 85}, {"referenceID": 5, "context": "as done by (Dhillon et al., 2011) but that is not the focus of this paper.", "startOffset": 11, "endOffset": 33}, {"referenceID": 14, "context": "Table 1 provides statistics of all the corpora used, namely: the Wall Street Journal portion of the Penn treebank (Marcus et al., 1993) (we consider both the 17 tags of (PTB 17) (Smith & Eisner, 2005) and the 45 tags version of it (PTB 45)), the Bosque subset of the Portuguese Floresta Sinta(c)tica Treebank (Afonso et al.", "startOffset": 114, "endOffset": 135}, {"referenceID": 0, "context": ", 1993) (we consider both the 17 tags of (PTB 17) (Smith & Eisner, 2005) and the 45 tags version of it (PTB 45)), the Bosque subset of the Portuguese Floresta Sinta(c)tica Treebank (Afonso et al., 2002), the Bulgarian BulTreeBank (Simov et al.", "startOffset": 181, "endOffset": 202}, {"referenceID": 19, "context": ", 2002), the Bulgarian BulTreeBank (Simov et al., 2002) (with only the 12 coarse tags), and the Danish Dependency Treebank (DDT) (Kromann, 2003).", "startOffset": 35, "endOffset": 55}, {"referenceID": 13, "context": "The PCA baseline used is similar to the one that has recently been proposed by (Lamar et al., 2010) except that here we are interested in supervised accuracy and not the unsupervised accuracy as in that paper.", "startOffset": 79, "endOffset": 99}, {"referenceID": 23, "context": "com/projects/wordreprs with k=50 dimensions and scaled as described in the paper (Turian et al., 2010).", "startOffset": 81, "endOffset": 102}], "year": 2012, "abstractText": "Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner. For example, for text applications where the words lie in a very high dimensional space (the size of the vocabulary), one can learn a low rank \u201cdictionary\u201d by an eigendecomposition of the word co-occurrence matrix (e.g. using PCA or CCA). In this paper, we present a new spectral method based on CCA to learn an eigenword dictionary. Our improved procedure computes two set of CCAs, the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself. We prove theoretically that this two-step procedure has lower sample complexity than the simple single step procedure and also illustrate the empirical efficacy of our approach and the richness of representations learned by our Two Step CCA (TSCCA) procedure on the tasks of POS tagging and sentiment classification.", "creator": "LaTeX with hyperref package"}}}