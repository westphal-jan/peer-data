{"id": "1206.6814", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "An Empirical Comparison of Algorithms for Aggregating Expert Predictions", "abstract": "Predicting the outcomes of future events is a challenging problem for which a variety of solution methods have been explored and attempted. We present an empirical comparison of a variety of online and offline adaptive algorithms for aggregating experts' predictions of the outcomes of five years of US National Football League games (1319 games) using expert probability elicitations obtained from an Internet contest called ProbabilitySports. We find that it is difficult to improve over simple averaging of the predictions in terms of prediction accuracy, but that there is room for improvement in quadratic loss. Somewhat surprisingly, a Bayesian estimation algorithm which estimates the variance of each expert's prediction exhibits the most consistent superior performance over simple averaging among our collection of algorithms.", "histories": [["v1", "Wed, 27 Jun 2012 15:37:14 GMT  (289kb)", "http://arxiv.org/abs/1206.6814v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["varsha dani", "omid madani", "david m pennock", "sumit sanghai", "brian galebach"], "accepted": false, "id": "1206.6814"}, "pdf": {"name": "1206.6814.pdf", "metadata": {"source": "CRF", "title": "An Empirical Comparison of Algorithms for Aggregating Expert Predictions", "authors": ["Varsha Dani", "Omid Madani"], "emails": [], "sections": [{"heading": null, "text": "Predicting the outcome of future events is a challenging problem for which a variety of solutions have been researched and tried. We present an empirical comparison of a variety of online and offline adaptive algorithms to aggregate expert predictions of the results of five years of US National Football League games (1319 games) using expert predictions obtained from an Internet competition called ProbabilitySports. We find that it is difficult to improve prediction accuracy over simple averages, but that there is room for improvement in square losses. Somewhat is surprising, a Bajesian estimation algorithm that estimates the variance of each expert's predictions has the most consistent superior performance as a simple averaging of our algorithm collection."}, {"heading": "1 Introduction", "text": "Consider the problem of predicting future events such as the weather, stock markets, political races and sports games. In such prediction problems, we often have access to additional information in the form of predictive probabilities, for the various possible outcomes, from a group of \"experts.\" The task then is to predict such information more effectively. A number of methods have been studied and attempted to fulfill such prediction tasks, including information markets, surveys, prediction methods based on machine learning processes and belief aggregation methods."}, {"heading": "2 Problem Formulation", "text": "Our task is to predict the results of a sequence of events. Events are binary and are presented as y1, y2,.., yt.., yT, where T is the total number of time steps and t is a specific time. Experts \"predictions for the event yt are presented as p 1 t, p 2 t,., p n t, each p j t, pjt and the results y1,., yt \u2212 1 and the previous predictions p 1 1 1,..., pn1,.,., p 2 t,.,. p \u2212 rap and the predicted losses are..., pn1,.,.,........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "2.1 Data Set", "text": "The data we worked with was collected from the website Probability Sports [Pro], a game site that follows the games of the National Football League (NFL) and other leagues every year. In each season, participants make predictions (in the form of probabilities) about the results of the games. In each round, the best participants receive prizes based on the results of the actual game, a score according to a square scoring rule. Their cumulative score for the season is the sum of their scores for each game. As an incentive, the few participants receive prizes at the end of each season. This scoring rule has become quite a popular site; the NFL competition in 2004 had 2231 participants. The website uses a square scoring rule given by 100 \u2212 400 (p \u2212 y) 2, with p being the predicted probability and Y {0, 1} being the result. The scoring rule is a scaled, inverse, square scoring rule of the final scoring function in 2004."}, {"heading": "3 Algorithms", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The Experts Algorithm", "text": "The basic premise of the expert algorithm of CesaBianchi et al. is the prediction \u03b2 = \u03b2 = \u03b2 = \u03b2 = q q (weighted average of expert opinions). The weights associated with the experts are dynamically modified based on their ongoing performance. We briefly give here the algorithm for completeness. 0 Given a parameter \u03b2 1 Start all experts with equal weights 2 For each experiment: 3 r = weighted average of expert estimates. 4 predictor p = F\u03b2 (r) 5 Note the true result y. 6 Update the function of each expert in line 4 is called the prediction function. Any function can be used as a prediction function, provided it meets the limits 1 + ln (1 \u2212 r) \u03b2 + r) \u03b2 2 ln (1 + \u03b2)."}, {"heading": "3.2 The Variance Algorithm", "text": "In the Experiments section, we will see that a simple averaging of the experts \"prediction competes with the experts\" algorithm. If we assume that the experts \"predictions are samples of a Gaussian distribution centered around the true probability of the event (one Gaussian per expert), then the mean of the experts\" predictions will converge with the true probability, as we increase the number of experts. However, not all experts behave in the same way. Each event is associated with a \"true\" probability pt from which the result is derived, some are better informed than others, etc. We will then try to grasp this notion by assuming that the Gaussian method for experts i has the variance \u03c32i (which we assume remains the same across all events)."}, {"heading": "3.3 Other Machine Learning Approaches", "text": "An expert algorithm can be considered an online and hence a single-pass machine learning algorithm: it adjusts expert weights by touching each instance once (hence a single pass over the training data); we also experimented with a variety of other \"batch\" learning algorithms that may not necessarily be single-pass algorithms, such as multipass versions of Perceptron and Winnow algorithms that attempt to minimize an objective function such as 01 errors (maximized prediction accuracy); linear and nonlinear support vector machines (we tuned the regulation constants); and other algorithms such as decision trees and ensemble methods (bagging and boosting via decision trees and decision stumps); and cross-validation (i.e.)."}, {"heading": "3.4 Market Simulation", "text": "We implement a simulated information market in which the agents in the market correspond with the experts. Each agent has a prior belief that corresponds to the expert's prediction, and a logarithmic value for money. For each game, the agents buy and sell a security that pays off if and only if Team A. The agents achieve a competitive equilibrium in which the supply of high-faith agents meets the demand of low-faith agents. In addition, we simulate the agents who learn from the market by fixing their rear belief in an equilibrium that corresponds to the average of their previous run and the equilibrium rate [PW01]. Agents gain or lose money in each round; the agents who are more accurate tend to win money. The extra money will be reflected in their utility function, and the agents with more money tend to risk more money and thus have a higher weight in the future; the agents who are more accurate tend to win money."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Performance on Accuracy", "text": "One approach that also applies in other areas is that the resolution of conflicts is a problem that is not a problem, but a problem that needs to be solved, a problem that cannot be solved."}, {"heading": "4.2 Performances on Quadratic Score", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "4.3 Discussion", "text": "The variance algorithm was motivated by the consideration that the simple averaging of expert predictions competes with other algorithms, 2 and was based on the core assumption that each expert's prediction is a sample of a distribution centered around the \"true probability\" of an event. We have made a number of additional simplistic assumptions in its derivation, such as that the expert distributions are Gaussian in nature, which do not change over time, and that the experts are independent. Rejecting some of the assumptions can (directly) lead to superior performance of the variance algorithm. However, variance is the most successful algorithm we have tested in this area from 2000 to 2003. Perhaps the most peculiar aspect of the variance algorithm is that it does not take into account the results of previous games and expert performances. However, variance may be the most successful algorithm we have tested in this area."}, {"heading": "5 Related Work", "text": "An opinion pool is a mathematical aggregation function for combining expert opinions, for example a weighted algebraic or geometric average. Opinion pools are usually justified on an axiomatic basis [GZ86]. Expert computational algorithms, on the other hand, are typically rated on the basis of the worst performance. Some faith aggregation studies have empirical components. Ng and Abramson [NA92] simulate the distribution of experts \"beliefs as Guassians by the\" true \"probability and test several opinion pools and conclude that a weighted average works best. Three recent papers use the ProbabilitySports data. Servan-Schreiber et al. [SSWPG04] compare the accuracy of real money and play money prediction markets as additional\" participants \"in the ProbabilitySports competition 2003. Wolfers and Zitzewitz [WZ05] draw a number of models from 2003 in which pre-market data can be interpreted as relative to sports-market prices and expert opinions."}, {"heading": "6 Conclusion", "text": "Our experiments with a variety of algorithms and the difficulty of hitting the baseline suggest that the probability prediction problem in probability sports, which goes beyond the baseline of simple averaging, is a difficult task. However, our experiments provide evidence that the Variance algorithm performs the most consistently superior performance compared to the other algorithms. Future directions include enhancements to the Variance algorithm, for example, in assessing expert distortions in addition to deviations, moving away from the assumption of independence and the greater weighting of recent history, and in developing a better understanding of its superior performance and the validity of its core assumption. We would also like to experiment with other algorithmic ideas, especially with algorithms that directly seek to minimize square losses on the training data by using appropriate regulatory constraints. Taking into account other contextual information, such as the division of the game and the identification of the algorithm, some experts may do the same (the algorithm identification)."}], "references": [{"title": "How to use expert advice", "author": ["N. Cesa-Bianchi", "Y. Freund", "D.Helbold", "D. Haussler", "R. Schapire", "M. Warmuth"], "venue": null, "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 1997}, {"title": "Information markets vs. multiple experts: An empirical comparison", "author": ["Y. Chen", "C. Chu", "T. Mullen", "D.M. Pennock"], "venue": "In Electronic Commerce,", "citeRegEx": "Chen et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2005}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": null, "citeRegEx": "Chang and Lin.,? \\Q2001\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2001}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society, Series B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "On-line prediction with kernels and the complexity approximation principle", "author": ["A. Gammerman", "Y. Kalnishkan", "V. Vovk"], "venue": "In UAI\u201904,", "citeRegEx": "Gammerman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gammerman et al\\.", "year": 2004}, {"title": "A generative Bayesian model for aggregating experts", "author": ["J.M. Kahn"], "venue": "probabilities. In UAI\u201904,", "citeRegEx": "Kahn.,? \\Q2004\\E", "shortCiteRegEx": "Kahn.", "year": 2004}, {"title": "Exponentiated gradient versus gradient descent for linear predictors", "author": ["J. Kivinen", "M.K. Warmuth"], "venue": "Information and Computation,", "citeRegEx": "Kivinen and Warmuth.,? \\Q1997\\E", "shortCiteRegEx": "Kivinen and Warmuth.", "year": 1997}, {"title": "Estimating class membership probabilities using classifier learners", "author": ["J. Langford", "B. Zadrozny"], "venue": "In AISTAT,", "citeRegEx": "Langford and Zadrozny.,? \\Q2005\\E", "shortCiteRegEx": "Langford and Zadrozny.", "year": 2005}, {"title": "Consensus diagnosis\u2014A simulation study", "author": ["Keung-Chi Ng", "Bruce Abramson"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Ng and Abramson.,? \\Q1992\\E", "shortCiteRegEx": "Ng and Abramson.", "year": 1992}, {"title": "A market framework for pooling opinions", "author": ["David M. Pennock", "Michael P. Wellman"], "venue": "Technical Report 2001-081, NEC Research Institute,", "citeRegEx": "Pennock and Wellman.,? \\Q2001\\E", "shortCiteRegEx": "Pennock and Wellman.", "year": 2001}, {"title": "Prediction markets: Does money matter", "author": ["E. Servan-Schreiber", "J. Wolfers", "D. Pennock", "B. Galebach"], "venue": "Electronic Markets,", "citeRegEx": "Servan.Schreiber et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Servan.Schreiber et al\\.", "year": 2004}, {"title": "Aggregating strategies", "author": ["V.G. Vovk"], "venue": "In COLT, pages 371\u2013386,", "citeRegEx": "Vovk.,? \\Q1990\\E", "shortCiteRegEx": "Vovk.", "year": 1990}, {"title": "Good probability assessors", "author": ["Robert L. Winkler", "Allan H. Murphy"], "venue": "J. Applied Meteorology,", "citeRegEx": "Winkler and Murphy.,? \\Q1968\\E", "shortCiteRegEx": "Winkler and Murphy.", "year": 1968}, {"title": "Prediction markets", "author": ["Justin Wolfers", "Eric Zitzewitz"], "venue": "Journal of Economic Perspectives,", "citeRegEx": "Wolfers and Zitzewitz.,? \\Q2004\\E", "shortCiteRegEx": "Wolfers and Zitzewitz.", "year": 2004}, {"title": "Interpreting prediction market prices as probabilities", "author": ["Justin Wolfers", "Eric Zitzewitz"], "venue": "Technical report, University of Pennsylvania Wharton School of Business,", "citeRegEx": "Wolfers and Zitzewitz.,? \\Q2005\\E", "shortCiteRegEx": "Wolfers and Zitzewitz.", "year": 2005}], "referenceMentions": [], "year": 0, "abstractText": "Predicting the outcomes of future events is a challenging problem for which a variety of solution methods have been explored and attempted. We present an empirical comparison of a variety of online and offline adaptive algorithms for aggregating experts\u2019 predictions of the outcomes of five years of US National Football League games (1319 games) using expert probability elicitations obtained from an Internet contest called ProbabilitySports. We find that it is difficult to improve over simple averaging of the predictions in terms of prediction accuracy, but that there is room for improvement in quadratic loss. Somewhat surprisingly, a Bayesian estimation algorithm which estimates the variance of each expert\u2019s prediction exhibits the most consistent superior performance over simple averaging among our collection of algorithms.", "creator": null}}}