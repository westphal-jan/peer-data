{"id": "1206.6817", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A Variational Approach for Approximating Bayesian Networks by Edge Deletion", "abstract": "We consider in this paper the formulation of approximate inference in Bayesian networks as a problem of exact inference on an approximate network that results from deleting edges (to reduce treewidth). We have shown in earlier work that deleting edges calls for introducing auxiliary network parameters to compensate for lost dependencies, and proposed intuitive conditions for determining these parameters. We have also shown that our method corresponds to IBP when enough edges are deleted to yield a polytree, and corresponds to some generalizations of IBP when fewer edges are deleted. In this paper, we propose a different criteria for determining auxiliary parameters based on optimizing the KL-divergence between the original and approximate networks. We discuss the relationship between the two methods for selecting parameters, shedding new light on IBP and its generalizations. We also discuss the application of our new method to approximating inference problems which are exponential in constrained treewidth, including MAP and nonmyopic value of information.", "histories": [["v1", "Wed, 27 Jun 2012 15:38:46 GMT  (206kb)", "http://arxiv.org/abs/1206.6817v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["arthur choi", "adnan darwiche"], "accepted": false, "id": "1206.6817"}, "pdf": {"name": "1206.6817.pdf", "metadata": {"source": "CRF", "title": "A Variational Approach for Approximating Bayesian Networks by Edge Deletion", "authors": ["Arthur Choi"], "emails": ["aychoi@cs.ucla.edu", "darwiche@cs.ucla.edu"], "sections": [{"heading": null, "text": "In this paper, we consider the formulation of an approximate inference in Bayesian networks to be a problem of the exact inference of an approximate network resulting from the deletion of edges (to reduce the tree width); we have shown in previous work that the deletion of edges requires the introduction of additional network parameters to compensate for lost dependencies, and have proposed intuitive conditions for determining these parameters; we have also shown that our earlier method corresponds to Iterative Faith Propagation (IBP) if enough edges are deleted to obtain a polytree, and corresponds to some generalizations of IBP if fewer edges are deleted; in this paper, we propose other criteria for determining auxiliary parameters based on optimizing the KL divergence between the original and approximate networks; and we discuss the relationship between the two methods of selecting parameters that shed a new light on IBP and its generalizations."}, {"heading": "1 INTRODUCTION", "text": "In fact, the fact is that most of them will be able to move to another world, in which they are able to integrate, and in which they will be able to move to another world, in which they are able to move to where they are."}, {"heading": "2 DELETING AN EDGE", "text": "Let's say U \u2192 X is an edge on a Bayesian network, and let's say we want to delete that edge to make the network more accessible to exact follow-up algorithms. This deletion will cause two problems. First, the variable X may lose its direct dependence on the edge of the parent U.1Networks elimination jobs with manageable treewidths, but certain queries may limit these commands, resulting in limited treewidths.Second, the variable U may lose its evident information provided by its child X. To solve these problems, we propose to add two auxiliary variables for each deleted edge U \u2192 X, as shown in Figure 1. The first is a variable U \u2032 that is used as the parent of X, thus acting as a clone of the lost parent U. The second is an instantiated variable S \u2032 that a child of U is made to provide evidence for U."}, {"heading": "3 PARAMETRIZING EDGES", "text": "Given a network N and evidence e, our proposal is then to approximate that network with another N \u2032, resulting from the deletion of some edges U \u2192 X as before. Furthermore, in carrying out conclusions on the network N \u2032 on the augmented evidence e \u2032, consisting of the original evidence e and all additional evidence s \u2032 when deleting edges, we will use Pr \u2032 (X | e \u2032) to approximate Pr (X | e) where X is a set of variables in the original network N. To fully define our approximate network N \u2032, we must use the parameters PM (u \u2032) and SE (u) for each edge we delete. We have proposed in (Choi & Darwiche, 2006) an iterative process that uses the following updating equations to edges."}, {"heading": "3.1 A VARIATIONAL APPROACH", "text": "We now propose a variational approach for parameterizing erased edges based on the KL divergence (coincidence): KL (Pr (e), Pr (e), Kr (e), Kr (e), Pr (w), Pr (w), Pr (e), where there is a world that denotes an instance of all variables. Note that the KL divergence is not symmetrical: the divergence KL (Pr), Pr (e), Pr (e), the divergence KL (Pr), KL (e), is not symmetrical. Common practice weighs the KL divergence using the approximate distribution that is typically better computational (e.g. Yedidia, Freeman & Weiss, 2005)."}, {"heading": "3.2 THE APPROXIMATE NETWORK", "text": "We have the KL divergence KL (Pr (. | e), Pr \"(. | e\")) as a function of our edge parameters PM (u \") = \u03b8u\" and SE (u) = \u03b8s \"| u. If we set the partial derivatives of the KL divergence to zero with respect to each edge parameter, we get the following note. Theorem 1 Let N be a Bayesian network and N\" is the network resulting from the deletion of equivalence edges U. \"The edge parameters of N\" are a stationary point of KL \"(Pr.\" | e \"Pr\"), if and only ifPr \"(u\" e \") = Pr\" (u \"e\") = Pr \"(u\" e \"), (4) are a stationary point of KL.\" That is, if we delete the edge U. \"U, then we will look for the marginal parameters on this.\" e network exactly.\""}, {"heading": "3.3 SEARCHING FOR PARAMETERS", "text": "After we have characterized stationary points of the KL divergence, we now proceed to develop an iterative procedure to find a stationary point. Our method is based on the following result.Theorem 2 Let us be a Bavarian network and be the network resulting from the deletion of equivalence boundaries. However, the edge parameters of N are a stationary point of the KL (Pr), Pr (u), if and only if: PM (u) = Pr (u) (e) / margin \"(e) / margin\" (e) / margin \"(e), Pr (u),\" (u), \"(u),\" (Pr), \"(),\" (r), \"(r),\" (), \"(r),\" (), \"(r),\" (), \"(r),\" (), \"(r),\" (), \"(),\" (r), \"(),\" (), \"(),\" (r), \"(),\" (), \"(r),\" ()."}, {"heading": "4 CHOOSING EDGES TO DELETE", "text": "Our method for deciding which edges to delete is based on evaluating each network edge in isolation, resulting in a total order of the network edges, and then deleting edges according to the resulting order. For example, if we want to delete k edges, we simply delete the first k edges in the order. The score for edge U \u2192 U \"is based on the KL divergence between the original network N and an approximate network N,\" which results from the deletion of the individual edge U \u2192 U. \"The KL divergence is calculated using Lemma 1. This problem requires some quantities from the original network N, which can be calculated by assuming that the network has a manageable tree width. The problem also requires that we have for the deleted edge U \u2192 U \u00b2 and the corresponding probability values Pr\" (e \"), which can be calculated relatively easily from Theorem 3, provided that we can calculate the Pr values together with other (pending) edges."}, {"heading": "5 EMPIRICAL ANALYSIS", "text": "This year, it has come to the point where it only takes a few days to get a result."}, {"heading": "6 RELATED WORK", "text": "Many methods of variation present the problem of approximate inference as an exact conclusion in an approximate model, often attempting to minimize the KL divergence but weighting it by approximate distribution (e.g. Jordan, Ghahramani, Jaakkola, & Saul, 1999; Jaakkola, 2000; Wiegerinck, 2000; Geiger & Meek, 2005).An example is the method of the center field, where we attempt to approximate a Network N by a completely decoupled N (Haft, Hofmann, & Tresp, 1999).However, if we remove all edges from the network and attempt to parameterize edges by means of ed-kl, we would solve the same problem that is solved by a center field, except that our KL divergence is weighted by true distribution, leading to more expensive actualization equations that normally proceed from certain structures in their approximate models, such as Khaetten (e.g., 1997, Jordan & Jordan)."}, {"heading": "7 CONCLUSION", "text": "We proposed a method, ed-kl, to approximate Bayesian networks by deleting edges from the original network and then finding stationary points for the KL divergence between the original and approximate network (weighing the divergence by true distribution). We also proposed efficient heuristics to determine which edges should be deleted from a network, with the goal of selecting network substructures that result in high-quality approximations.The updated ed-kl equations require accurate decimal places from the original network. This means that edkl is generally applicable to problems that remain difficult even with manageable tree widths, including MAP, non-myopic information value, and inferences in credential networks. This is in contrast to our previous method ed-bp, which updated parameters differently and coincides with IBP and some of its generalizations. Our empirical results provide good empirical evidence for the quality of our network, which may be returned by both irical and irical, and some of the approximations given by P in particular, irical and irical."}, {"heading": "Acknowledgments", "text": "This work was partially supported by Air Force Promotion No FA9550-05-1-0075-P00002 and JPL / NASA Promotion No 1272258."}, {"heading": "A Proof Sketches", "text": "Note that u \u00b2 w w \u00b2 p \u00b2 p \u00b2 p \u00b2 u \u00b2 u \u00b2 u \u00b2 u \u00b2 u \u00b2 u \u00b2 u \u00b2 u \u00b2 u \u00b2 u \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p p p p \u00b2 p p \u00b2 p p p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p \u00b2 p \u00b2 p p \u00b2 p \u00b2 p \u00b2 p p p p \u00b2 p p p p"}, {"heading": "B Example", "text": "Here is an example where ed-bp fixed points are not necessarily stationary points of KL divergence, and this example also shows that even if we delete a single edge, ed-bp may have an infinite number of parameterizations that meet condition (2), even though there is an ed-bp (and ed-kl) fixed point that minimizes the KL boundary, KL (Pr (. | e), Pr (. | e)))) (as well as a minimization of the exact KL). This example also corresponds to an instance of IBP (with a specific message delivery schedule), since the edge deletion turns the network into a polytree (Choi & Darwiche, 2006). Our example is Figure 9. Variables Ui have parameters suffui = Pacu-Edge i = = = 0.5. The variables Xj are fixed to the states xj and claim the equivalence of U1 and U2: U2 = 1 ff = iff = iff."}], "references": [{"title": "The generalized distributive law and free energy minimization", "author": ["S.M. Aji", "R.J. McEliece"], "venue": "In Proceedings of the 39th Allerton Conference on Communication, Control and Computing,", "citeRegEx": "Aji and McEliece,? \\Q2001\\E", "shortCiteRegEx": "Aji and McEliece", "year": 2001}, {"title": "On the revision of probabilistic beliefs using uncertain evidence", "author": ["H. Chan", "A. Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Chan and Darwiche,? \\Q2005\\E", "shortCiteRegEx": "Chan and Darwiche", "year": 2005}, {"title": "Compiling Bayesian networks with local structure", "author": ["M. Chavira", "A. Darwiche"], "venue": "In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Chavira and Darwiche,? \\Q2005\\E", "shortCiteRegEx": "Chavira and Darwiche", "year": 2005}, {"title": "On Bayesian network approximation by edge deletion", "author": ["A. Choi", "H. Chan", "A. Darwiche"], "venue": "In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Choi et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2005}, {"title": "An edge deletion semantics for belief propagation and its practical impact on approximation quality", "author": ["A. Choi", "A. Darwiche"], "venue": "In Proc. AAAI National Conference. To appear", "citeRegEx": "Choi and Darwiche,? \\Q2006\\E", "shortCiteRegEx": "Choi and Darwiche", "year": 2006}, {"title": "Propositional and relational Bayesian networks associated with imprecise and qualitative probabilistic assessments", "author": ["F.G. Cozman", "C.P. de Campos", "J.S. Ide", "J.C.F. da Rocha"], "venue": "In Proceedings of the Conference on Uncertainty", "citeRegEx": "Cozman et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cozman et al\\.", "year": 2004}, {"title": "Recursive conditioning", "author": ["A. Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "Darwiche,? \\Q2001\\E", "shortCiteRegEx": "Darwiche", "year": 2001}, {"title": "Bucket elimination: A unifying framework for probabilistic inference", "author": ["R. Dechter"], "venue": "In Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Dechter,? \\Q1996\\E", "shortCiteRegEx": "Dechter", "year": 1996}, {"title": "Iterative join-graph propagation", "author": ["R. Dechter", "K. Kask", "R. Mateescu"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Dechter et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Dechter et al\\.", "year": 2002}, {"title": "A revolution: Belief propagation in graphs with cycles", "author": ["B.J. Frey", "D.J.C. MacKay"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Frey and MacKay,? \\Q1997\\E", "shortCiteRegEx": "Frey and MacKay", "year": 1997}, {"title": "Sequentially fitting \u201cinclusive\u201d trees for inference in noisy-or networks", "author": ["B.J. Frey", "R. Patrascu", "T. Jaakkola", "J. Moran"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Frey et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Frey et al\\.", "year": 2000}, {"title": "Structured variational inference procedures and their realizations", "author": ["D. Geiger", "C. Meek"], "venue": "In Proceedings of Tenth International Workshop on Artificial Intelligence and Statistics. The Society for Artificial Intelligence and Statistics", "citeRegEx": "Geiger and Meek,? \\Q2005\\E", "shortCiteRegEx": "Geiger and Meek", "year": 2005}, {"title": "Factorial hidden markov models", "author": ["Z. Ghahramani", "M.I. Jordan"], "venue": "Machine Learning,", "citeRegEx": "Ghahramani and Jordan,? \\Q1997\\E", "shortCiteRegEx": "Ghahramani and Jordan", "year": 1997}, {"title": "Modelindependent mean-field theory as a local method for approximate propagation of information", "author": ["M. Haft", "R. Hofmann", "V. Tresp"], "venue": "Network: Computation in Neural Systems,", "citeRegEx": "Haft et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Haft et al\\.", "year": 1999}, {"title": "Advanced Mean Field methods - Theory and Practice, chap. Tutorial on Variational Approximation Methods", "author": ["T. Jaakkola"], "venue": null, "citeRegEx": "Jaakkola,? \\Q2000\\E", "shortCiteRegEx": "Jaakkola", "year": 2000}, {"title": "Bayesian updating in recursive graphical models by local computation", "author": ["F.V. Jensen", "S. Lauritzen", "K. Olesen"], "venue": "Computational Statistics Quarterly,", "citeRegEx": "Jensen et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Jensen et al\\.", "year": 1990}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T. Jaakkola", "L.K. Saul"], "venue": "Machine Learning,", "citeRegEx": "Jordan et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jordan et al\\.", "year": 1999}, {"title": "Reduction of computational complexity in Bayesian networks through removal of weak dependences", "author": ["U. Kj\u00e6rulff"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Kj\u00e6rulff,? \\Q1994\\E", "shortCiteRegEx": "Kj\u00e6rulff", "year": 1994}, {"title": "Optimal nonmyopic value of information in graphical models - efficient algorithms and theoretical limits", "author": ["A. Krause", "C. Guestrin"], "venue": "In Proc. International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Krause and Guestrin,? \\Q2005\\E", "shortCiteRegEx": "Krause and Guestrin", "year": 2005}, {"title": "Local computations with probabilities on graphical structures and their application to expert systems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "Journal of Royal Statistics Society, Series B,", "citeRegEx": "Lauritzen and Spiegelhalter,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "Tree-structured approximations by expectation propagation", "author": ["T.P. Minka", "Y.A. Qi"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Minka and Qi,? \\Q2003\\E", "shortCiteRegEx": "Minka and Qi", "year": 2003}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Murphy et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 1999}, {"title": "Complexity results and approximation strategies for map explanations", "author": ["J. Park", "A. Darwiche"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Park and Darwiche,? \\Q2004\\E", "shortCiteRegEx": "Park and Darwiche", "year": 2004}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Exploiting tractable substructures in intractable networks", "author": ["L.K. Saul", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Saul and Jordan,? \\Q1995\\E", "shortCiteRegEx": "Saul and Jordan", "year": 1995}, {"title": "Explanation in Bayesian Belief Networks", "author": ["H.J. Suermondt"], "venue": null, "citeRegEx": "Suermondt,? \\Q1992\\E", "shortCiteRegEx": "Suermondt", "year": 1992}, {"title": "Approximating Bayesian belief networks by arc removal", "author": ["R.A. van Engelen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Engelen,? \\Q1997\\E", "shortCiteRegEx": "Engelen", "year": 1997}, {"title": "Variational approximations between mean field theory and the junction tree algorithm", "author": ["W. Wiegerinck"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Wiegerinck,? \\Q2000\\E", "shortCiteRegEx": "Wiegerinck", "year": 2000}, {"title": "A generalized mean field algorithm for variational inference in exponential families", "author": ["E.P. Xing", "M.I. Jordan", "S.J. Russell"], "venue": "In UAI,", "citeRegEx": "Xing et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Xing et al\\.", "year": 2003}, {"title": "Constructing free-energy approximations and generalized belief propagation algorithms", "author": ["J. Yedidia", "W. Freeman", "Y. Weiss"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Yedidia et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yedidia et al\\.", "year": 2005}, {"title": "Exploiting causal independence in bayesian network inference", "author": ["N.L. Zhang", "D. Poole"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Zhang and Poole,? \\Q1996\\E", "shortCiteRegEx": "Zhang and Poole", "year": 1996}], "referenceMentions": [{"referenceID": 7, "context": "The complexity of algorithms for exact inference on Bayesian networks is generally exponential in the network treewidth (Jensen, Lauritzen, & Olesen, 1990; Lauritzen & Spiegelhalter, 1988; Zhang & Poole, 1996; Dechter, 1996; Darwiche, 2001).", "startOffset": 120, "endOffset": 240}, {"referenceID": 6, "context": "The complexity of algorithms for exact inference on Bayesian networks is generally exponential in the network treewidth (Jensen, Lauritzen, & Olesen, 1990; Lauritzen & Spiegelhalter, 1988; Zhang & Poole, 1996; Dechter, 1996; Darwiche, 2001).", "startOffset": 120, "endOffset": 240}, {"referenceID": 23, "context": "Iterative Belief Propagation (IBP), also known as Loopy Belief Propagation (Pearl, 1988; Murphy, Weiss, & Jordan, 1999), is one such algorithm that has been critical for enabling certain classes of applications, which have been intractable for exact algorithms (e.", "startOffset": 75, "endOffset": 119}, {"referenceID": 23, "context": "Note that for queries that are conditioned on evidence s, only the relative ratios of parameters SE (U) matter, not their absolute values (Pearl, 1988; Chan & Darwiche, 2005).", "startOffset": 138, "endOffset": 174}, {"referenceID": 3, "context": "Our proposal for deleting an edge is an extension of the proposal given by (Choi et al., 2005), who proposed the addition of a clone variable U \u2032 but missed the addition of evidence variable S.", "startOffset": 75, "endOffset": 94}, {"referenceID": 5, "context": "In these situations, the goal of deleting edges is to reduce the network constrained treewidth, making it amenable to algorithms that are exponential in constrained treewidth, such as MAP (Park & Darwiche, 2004), inference in credal networks (Cozman et al., 2004), and the computation of nonmyopic value of information (Krause & Guestrin, 2005).", "startOffset": 242, "endOffset": 263}, {"referenceID": 3, "context": "To compute the exact KL\u2013divergence, see, for example, (Choi et al., 2005).", "startOffset": 54, "endOffset": 73}, {"referenceID": 14, "context": "Many variational methods pose the problem of approximate inference as exact inference in some approximate model, often seeking to minimize the KL\u2013 divergence, but weighing it by the approximate distribution (e.g., Jordan, Ghahramani, Jaakkola, & Saul, 1999; Jaakkola, 2000; Wiegerinck, 2000; Geiger & Meek, 2005).", "startOffset": 207, "endOffset": 312}, {"referenceID": 27, "context": "Many variational methods pose the problem of approximate inference as exact inference in some approximate model, often seeking to minimize the KL\u2013 divergence, but weighing it by the approximate distribution (e.g., Jordan, Ghahramani, Jaakkola, & Saul, 1999; Jaakkola, 2000; Wiegerinck, 2000; Geiger & Meek, 2005).", "startOffset": 207, "endOffset": 312}, {"referenceID": 10, "context": "Again, most of these approaches weigh the KL\u2013 divergence by the approximate distribution for computational reasons, with the notable exceptions of (Frey et al., 2000; Minka & Qi, 2003).", "startOffset": 147, "endOffset": 184}, {"referenceID": 25, "context": "Other methods of edge deletion have been proposed for Bayesian networks (Suermondt, 1992; Kj\u00e6rulff, 1994; van Engelen, 1997), some of which can be rephrased using a variational perspective.", "startOffset": 72, "endOffset": 124}, {"referenceID": 17, "context": "Other methods of edge deletion have been proposed for Bayesian networks (Suermondt, 1992; Kj\u00e6rulff, 1994; van Engelen, 1997), some of which can be rephrased using a variational perspective.", "startOffset": 72, "endOffset": 124}], "year": 2006, "abstractText": "We consider in this paper the formulation of approximate inference in Bayesian networks as a problem of exact inference on an approximate network that results from deleting edges (to reduce treewidth). We have shown in earlier work that deleting edges calls for introducing auxiliary network parameters to compensate for lost dependencies, and proposed intuitive conditions for determining these parameters. We have also shown that our earlier method corresponds to Iterative Belief Propagation (IBP) when enough edges are deleted to yield a polytree, and corresponds to some generalizations of IBP when fewer edges are deleted. In this paper, we propose a different criteria for determining auxiliary parameters based on optimizing the KL\u2013 divergence between the original and approximate networks. We discuss the relationship between the two methods for selecting parameters, shedding new light on IBP and its generalizations. We also discuss the application of our new method to approximating inference problems which are exponential in constrained treewidth, including MAP and nonmyopic value of information.", "creator": "dvips(k) 5.94a Copyright 2003 Radical Eye Software"}}}