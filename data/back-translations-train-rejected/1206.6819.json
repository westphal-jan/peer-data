{"id": "1206.6819", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "On the Robustness of Most Probable Explanations", "abstract": "In Bayesian networks, a Most Probable Explanation (MPE) is a complete variable instantiation with a highest probability given the current evidence. In this paper, we discuss the problem of finding robustness conditions of the MPE under single parameter changes. Specifically, we ask the question: How much change in a single network parameter can we afford to apply while keeping the MPE unchanged? We will describe a procedure, which is the first of its kind, that computes this answer for each parameter in the Bayesian network variable in time O(n exp(w)), where n is the number of network variables and w is its treewidth.", "histories": [["v1", "Wed, 27 Jun 2012 15:39:15 GMT  (215kb)", "http://arxiv.org/abs/1206.6819v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hei chan", "adnan darwiche"], "accepted": false, "id": "1206.6819"}, "pdf": {"name": "1206.6819.pdf", "metadata": {"source": "CRF", "title": "On the Robustness of Most Probable Explanations", "authors": ["Hei Chan", "Adnan Darwiche"], "emails": ["chanhe@eecs.oregonstate.edu", "darwiche@cs.ucla.edu"], "sections": [{"heading": "1 Introduction", "text": "An example of this is the fact that the solution to the problem is not a purely formal solution, but a purely formal solution."}, {"heading": "2 Most Probable Explanations", "text": "We will formally define the most likely explanations in this section, but we will specify some of our notational conventions first (1). We will mark variables with uppercase letters (X) and their values with lowercase letters (x). Sentences of variables will be marked with uppercase letters (X) and their instances with lowercase letters (x). For variable X and value x, we will often write x instead of X = x, and therefore Pr (x) instead of Pr (X = x). For a binary variable X with values true and false, we will use x to mark X = true and x to mark X = false. Therefore, Pr (x) will represent the same probability in this case. Likewise, we will represent Pr (X = false) and Pr (x) the same probability."}, {"heading": "3 Relation Between MPE and Network Parameters", "text": "Suppose that we get evidence for it and are able to find its MPE, MPE (e). (In addition, we assume that we can change this parameter without changing any co-varying parameters, such as x parameters, but we will lateralize this assumption. Our solution to this problem is based on some basic observations that we will discuss next. In particular, we will find that complete variable instances x that are consistent with e can be divided into two categories: \u2022 Those that are consistent with xu, the probability of any such instantiation x is a linear function of parameters x. \u2022 Those that are not consistent with xu, the probability of any such instantiation x is."}, {"heading": "4 Dealing with Co-Varying Parameters", "text": "The above analysis assumes that we can change a parameter without having to change any other parameters in our network. (...) This is not realistic, although in the context of Bayesian networks we must always have the same parameters in order to achieve a valid probability distribution. (...) We will limit our attention to binary variables to simplify the discussion, but our results can easily be extended to multivalued variables, as we assume that we will change the parameters. (...) We will limit our attention to binary variables to simplify the discussion, but our results can easily be extended to multivalued variables as we classify them into three groups, depending on whether they are consistent with the respective parameters."}, {"heading": "5 Computing Robustness Conditions", "text": "In this section, we will develop an algorithm for calculating the constants r (e, xu) for all network parameters \u03b8x | u. Specifically, we will show that they can be calculated in time and space, which are O (n exp (w), where n is the number of network variables and w is their tree width."}, {"heading": "5.1 Arithmetic Circuits", "text": "Our algorithm for calculating the r (e, xu) constants is therefore based on an arithmetic circuit representation of the Bayesian network [10]. Figure 4 represents an arithmetic circuit for a small network consisting of two binary nodes, A and B. An arithmetic circuit is a rooted DAG in which each internal node corresponds to multiplication (\u043a) or addition (+) and each leaf node corresponds to either a network parameter succx | u or an evidence indicator \u03bbx; see Figure 4. Operationally, the circuit can be used to calculate the probability of proof e by setting the proof indicator \u03bbx to 0 if x contradicts e and setting it to 1 otherwise. However, from a semantic point of view, the arithmetic circuit is simply a factored representation of an exponential function enabling network distribution."}, {"heading": "5.2 Complete Sub-Circuits and Their", "text": "A complete partial circuit can be constructed recursively from the root, involving all children of each multiplication node and exactly one child of each addition node. Bold lines in Figure 4 represent a complete partial circuit corresponding to the term \u03bba\u03bbb Facility Facility Facility Facility Facility Facility Facilities Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility Facility"}, {"heading": "5.3 Maximizer Circuits", "text": "An arithmetic circuit can easily be modified into a maximizing circuit to calculate the MPE solutions simply by replacing each addition node with a maximization node; see Figure 5. This corresponds to a circuit that calculates the value of the maximum term in a network polynomial rather than adding the values of these terms. Thus, the value of the root calculation will be the MPE probability MPEp (e). All we need to do is construct the MPE sub-circuit from the root by including all the children of each multiplication node, and a child c for each maximization node v so that v and c have the same value; see Figure 5. The MPE sub-circuit will then correspond to an MPE solution."}, {"heading": "6 Example", "text": "We now go back, for example, to network in Figure 1 and calculate the robustness conditions for the current2However, if some of the parameters are equal to 0, one must use a special joint tree solution [11].MPE solution using the inequalities we get in Section 4, and an implementation of algorithm 1. After going through the CPT of each variable, our procedure found nine possible parameter changes that would produce a different MPE solution, as in Figure 7. Of these nine proposed changes, only three make qualitative sense: \u2022 Reducing the probability that the ignition of.9925 will work to maximum.9133 (6th row) \u2022 Reducing the probability that the engine will work, as both the battery and the ignition of.97 will work to maximum.9108. (1st row) \u2022 Reducing the false-negative rate of engine test from.09 to maximum.0285. (9th row) If we apply the first parameter, we will apply both parameters, not the MPE in the third row."}, {"heading": "7 MPE under Evidence Change", "text": "In Section 5.2 we discussed the concept of a complete sub-circuit and its coefficient in relation to a network parameter successx | u contained in the sub-circuit. (In particular, we have shown how each sub-circuit corresponds to a term in the network polynomial, and that if a complete sub-circuit has a coefficient r in relation to parameter successx, then the value of the term will correspond to that sub-circuit coefficient, and that if a complete sub-circuit has a coefficient r in relation to an evidence indicator, this evidence x is contained in relation to an evidence indicator contained in the sub-circuit, then r \u00b7 x will be the value of the term corresponding to that sub-circuit coefficient. Let us suppose that all complete sub-circuits are equal."}, {"heading": "8 Conclusion", "text": "In this paper, we looked at the problem of strength conditions for MPE solutions of a Bayesian network under individual parameter changes. We were able to solve this problem by identifying some interesting relationships between an MPE solution and the network parameters. Specifically, we found that the robustness of an MPE solution under a single parameter change depends on two constants, which are independent of the parameter value. We also proposed a method for calculating such constants and thus the robustness conditions of MPE in O (n exp (w) time and space, where n is the number of network variables and w is the network width. Our algorithm is the first of its kind to ensure the robustness of MPE solutions under parameter changes in a Bayesian network."}, {"heading": "Acknowledgments", "text": "This work was partially supported by Air Force Scholarship # FA9550-05-1-0075-P00002 and JPL / NASA Scholarship # 1272258. We would also like to thank James Park, who reviewed this work and observed how k (e, u) can be calculated in Equation 6."}], "references": [{"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1988}, {"title": "When do numbers really matter", "author": ["Hei Chan", "Adnan Darwiche"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "Sensitivity analysis in Bayesian networks: From single to multiple parameters", "author": ["Hei Chan", "Adnan Darwiche"], "venue": "In Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Sensitivity analysis in discrete Bayesian networks", "author": ["Enrique Castillo", "Jos\u00e9 Manuel Guti\u00e9rrez", "Ali S. Hadi"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part A (Systems and Humans),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Using sensitivity analysis for efficient quantification of a belief network", "author": ["Veerle M.H. Coup\u00e9", "Niels Peek", "Jaap Ottenkamp", "J. Dik F. Habbema"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Making sensitivity analysis computationally efficient", "author": ["Uffe Kj\u00e6rulff", "Linda C. van der Gaag"], "venue": "In Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Sensitivity analysis for probability assessments in Bayesian networks", "author": ["Kathryn B. Laskey"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "The sensitivity of belief networks to imprecise probabilities: An experimental investigation", "author": ["Malcolm Pradhan", "Max Henrion", "Gregory Provan", "Brendan Del Favero", "Kurt Huang"], "venue": "Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Analysing sensitivity data from probabilistic networks", "author": ["Linda C. van der Gaag", "Silja Renooij"], "venue": "In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2001}, {"title": "A differential approach to inference in Bayesian networks", "author": ["Adnan Darwiche"], "venue": "Journal of the ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "A differential semantics for jointree algorithms", "author": ["James D. Park", "Adnan Darwiche"], "venue": "Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Compiling Bayesian networks with local structure", "author": ["Mark Chavira", "Adnan Darwiche"], "venue": "In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Compiling relational Bayesian networks for exact inference", "author": ["Mark Chavira", "Adnan Darwiche", "Manfred Jaeger"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "A Most Probable Explanation (MPE) in a Bayesian network is a complete variable instantiation which has the highest probability given current evidence [1].", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 2, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 3, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 4, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 5, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 6, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 7, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 8, "context": "Previous results on sensitivity analysis have focused mostly on the robustness of probability values, such as the probability of evidence, under single or multiple parameter changes [2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 182, "endOffset": 206}, {"referenceID": 0, "context": "A Bayesian network is specified by its structure, a directed acyclic graph (DAG), and a set of conditional probability tables (CPTs), with one CPT for each network variable [1].", "startOffset": 173, "endOffset": 176}, {"referenceID": 0, "context": "A most probable explanation (MPE) given e is a complete variable instantiation that is consistent with e and has the highest probability [1]:", "startOffset": 137, "endOffset": 140}, {"referenceID": 9, "context": "Our algorithm for computing the r(e, xu) constants is based on an arithmetic circuit representation of the Bayesian network [10].", "startOffset": 124, "endOffset": 128}, {"referenceID": 9, "context": "If not, one can use a technique which gives a linear complexity by simply storing two additional bits with each multiplication node [10].", "startOffset": 132, "endOffset": 136}, {"referenceID": 9, "context": "circuit for any Bayesian network in O(n exp(w)) time and space, where n is the number of network variables and w is its treewidth [10].", "startOffset": 130, "endOffset": 134}, {"referenceID": 11, "context": "\u2022 The arithmetic circuit for a Bayesian network can be much smaller than the corresponding jointree by exploiting the local structures of the Bayesian network [12, 13].", "startOffset": 159, "endOffset": 167}, {"referenceID": 12, "context": "\u2022 The arithmetic circuit for a Bayesian network can be much smaller than the corresponding jointree by exploiting the local structures of the Bayesian network [12, 13].", "startOffset": 159, "endOffset": 167}, {"referenceID": 10, "context": "However, in case some of the parameters are equal to 0, one needs to use a special jointree [11].", "startOffset": 92, "endOffset": 96}], "year": 2006, "abstractText": "In Bayesian networks, a Most Probable Explanation (MPE) is a complete variable instantiation with the highest probability given the current evidence. In this paper, we discuss the problem of finding robustness conditions of the MPE under single parameter changes. Specifically, we ask the question: How much change in a single network parameter can we afford to apply while keeping the MPE unchanged? We will describe a procedure, which is the first of its kind, that computes this answer for all parameters in the Bayesian network in time O(n exp(w)), where n is the number of network variables and w is its treewidth.", "creator": "TeX"}}}