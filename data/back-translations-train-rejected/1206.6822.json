{"id": "1206.6822", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Cutset Sampling with Likelihood Weighting", "abstract": "The paper analyzes theoretically and empirically the performance of likelihood weighting (LW) on a subset of nodes in Bayesian networks. The proposed scheme requires fewer samples to converge due to reduction in sampling variance. The method exploits the structure of the network to bound the complexity of exact inference used to compute sampling distributions, similar to Gibbs cutset sampling. Yet, the extension of the previosly proposed cutset sampling principles to likelihood weighting is non-trivial due to differences in the sampling processes of Gibbs sampler and LW. We demonstrate empirically that likelihood weighting on a cutset (LWLC) is effective time-wise and has a lower rejection rate than LW when applied to networks with many deterministic probabilities. Finally, we show that the performance of likelihood weighting on a cutset can be improved further by caching computed sampling distributions and, consequently, learning 'zeros' of the target distribution.", "histories": [["v1", "Wed, 27 Jun 2012 15:40:35 GMT  (146kb)", "http://arxiv.org/abs/1206.6822v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["bozhena bidyuk", "rina dechter"], "accepted": false, "id": "1206.6822"}, "pdf": {"name": "1206.6822.pdf", "metadata": {"source": "CRF", "title": "Cutset Sampling with Likelihood Weighting", "authors": [], "emails": [], "sections": [{"heading": null, "text": "The paper extends the principle of Bayesian network sampling, previously presented for Gibbs samples, to Likelihood Weight (LW). Cutset sampling is based on the Rao-Blackwell theorem, which implies that sampling over a subset of variables requires fewer samples for convergence due to the reduction in sampling variance. In particular, as we have shown empirically, probability weighting via a loop cutset (abbreviated to LWLC) is cost-effective in terms of network structure when selecting cutsets that allow for efficient calculation of sample distributions."}, {"heading": "1 Introduction", "text": "Based on the samples generated, we can obtain estimates that converge to the exact values as the number of samples increases. However, convergence in large networks can be slow due to the increase in sampling variance, which is the problem we deal with in this paper. However, based on the Rao-Blackwell theorem, we can reduce sampling variance and accelerate convergence by scanning only a subset of variables (a cutset), but the efficiency of sampling from low-dimensional spaces is hampered by the overhead of calculating sampling distributions, the latter equivalent to performing an exact inference exponentially in the induced width of the network, the instantiated variables of which (evidence and sampled) we remove."}, {"heading": "2 Background", "text": "Definition 2.1 (faith networks) X = {X1,..., Xn} is a set of random variables over polyvalent ranges D (X1),..., D (Xn). A faith network is a pair < G, P >, where G is a directed acyclic graph on X and P = {P (Xi | pai) if its underlying undirected graph has no cycles, due to Xi's pai parents. A proof e is an instantiated subset of variables E. A network is connected individually (also called poly tree) if its underlying undirected graph has no cycles. Otherwise, it is connected multiple times. Definition 2.2 (loop cutset) A loop in G is a subgraph of G whose underlying graph is a cycle. A vertex v is a sink in relation to an L, if the edges border v."}, {"heading": "2.1 Likelihood Weighting", "text": "The experimental distribution differs from the target distribution P (X). In general, Q (X) is chosen in such a way that it is easy to calculate. (A typical query in Bayesian networks is to estimate the posterior marginal values P (xi), which are derived from the sample estimates of P (e) and P (xi, e) Q (y) Q (y) Q (y, e) Q (y) Q (y, e) Q (y, e) Q (y) E (e) E (e). Then: EP [P (e)] = [2) yP (y), e) yP (y) Q (y)."}, {"heading": "3 Rao-Blackwellised Likelihood Weighting", "text": "\"It is not as if we exclude the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects, the distribution effects of the distribution effects of the distribution effects of the distribution effects, the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the distribution effects of the"}, {"heading": "3.1 Convergence", "text": "The probability weighting on a loop cutset (LWLC) has a higher overhead in calculating the distributions P (Zi | z1,..., zi \u2212 1) compared to the sampling on a complete variable set. However, as already mentioned, it converges faster. In general, the meaning of the sampling rate is influenced by the sampling rate and the distance between the sampling rate and the target distributions. Estimates obtained by sampling from a low-dimensional space show less variance due to the Rao-Blackwell theorem. That is: V ar {P (Y, C) Q (Y, C) Q (Y)}} ar V ar {P (C) Q (C)} (P)} (P)} (where P (C) = square P (Y, C) = square Q (Y, C) [9, 16] Proof evidence can be found in [9] and [16]."}, {"heading": "3.2 Caching Sampling on a Cutset", "text": "Often we can reduce the calculation time of a sampling scheme by caching the generated samples and their probabilities. LW caching is of limited use because it uses probabilities stored in CPTs. However, in the case of LWLC, caching can partially compensate for the computational effort. A suitable data structure for caching is a search tree above the Cutset C with a root node C1. When new variable values are scanned and the current sample weight and sampling distribution P (Ci | z1,..., zi \u2212 1) are generated, LWLC traverses the search tree along the path c1,..., ci. Whenever a new value of Ci is scanned, the corresponding tree branch is expanded and the current sample weight and sampling distribution P (Ci | z1,..., zi \u2212 1) are stored in the node Ci buffer."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Methodology", "text": "In this section, we will empirically compare the performance of full probability weighting (LWLC-BUF) and buffered probability weighting on a loop cutset (LWLC-BUF). In networks with positive distribution, we will compare probability weighting side by side with Gibbs sampling (Gibbs) and Gibbs-based loop cutset sampling (LCS) [2]. As a reference, we will also compare the estimates obtained by Iterative Belief Propagation (IBP). Faith propagation calculates the exact rear margins in poly trees [18]. In networks with loops, approximate properties are calculated when they converge. IBP is fast and often produces good estimates [17, 20].The quality of approximate posterior margins is measured by the Mean Square Error (MSE)."}, {"heading": "4.2 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Sampling Speed", "text": "In Table 2, we report on the speed of sample generation using LW, LWLC and LWLC-BUF sampling schemes. As expected, LWLC generates significantly fewer samples than LW. It is noteworthy that the relative speed of LW and LWLC remains the same in the two pathfinder networks and in the link network. At the time LW generates 100,000 samples, LWLC generates 1200 samples. Table 2 also shows an order of magnitude of improvement in sample generation by LWLC-BUF in cpcs360b, Pathfinder1 and Pathfinder2, a factor of 2 in cpcs422b, and no change in the link network. The improvement depends on the ratio of the unique samples. The number of unique tubs in pathfinder networks is only about 1% of the total number of samples, and thus 99% of the calculation in the link network is almost unique."}, {"heading": "4.2.2 Rejection Rates", "text": "If the target distribution P (X) has many zeros, where the sampling rate Q (X) remains positive, many samples with a weight of 0 can be generated that do not contribute to the sampling rates. Therefore, we call them \"rejected.\" This is not a problem in cpcs360b and cpcs422b, where all probabilities are positive. However, in deterministic networks many samples can be rejected, contributing to slow convergence. We will use the rejection rate R to mark the percentage of samples weighing 0. If the evidence is rare, we must generate a very large number of samples before we find a single sample weighing not zero. If all samples are rejected, we will say that the rejection rate is 100% and the rejection rate of the network is unresolved. The rejection rates of the three probability weight schemes via Pathfinder1, Pathfinder2 and Link are summarized in Table 3."}, {"heading": "4.2.3 Accuracy of the Estimates", "text": "The MSE results for PathFinder1, Pathfinder2, and Link are shown in Figure 2 depending on time. The comparative behavior of LW, LWLC, and LWLCBUF scanning schemes is comparable across all three networks. LWLC converts consistently faster than LW and outperforms IBP within 2 seconds. LW outperforms IBP within 2 seconds in Pathfinder1 and within 8 seconds in Pathfinder2. However, LW in the Link network is significantly worse than IBP. LWLC-BUF converts faster than LWLC in Pathfinder1 and Pathfinder2 because it generates more samples and has a lower rejection rate. In the Link network, its performance is the same, and therefore we show only the LWLC curve. The PathFinder2 network has also been used as a benchmark for evaluating AIS-BN algorithm. [5], an adaptive meaning for the design scheme."}, {"heading": "5 Related Work and Conclusions", "text": "In this paper, we presented a cutset-based probability weighting. By reducing the dimensionality of the sampling space, we achieve a reduction in the variance of th esampling and also reduce the information gap (KLdistance) between the sampling and the target distributions. Therefore, the cutset sampling scheme requires less sampling convergence. In the past, sampling significance became efficient by exploiting the properties of the conditional probability distributions, e.g. when distributions for the marginalized variables could be analytically calculated using a Kalman filter [9, 8, 1] or when the marginalized variables in a factoralized HMM became conditionally independent (when the sampled variables are observed) due to the numerical structure of the CPTs [8]. In contrast, our method limits the complexity of the sampling distributions by using the structure of the probability distributions."}], "references": [{"title": "Blackwellised particle filtering via data augmentation", "author": ["C. Andrieu", "N. de Freitas", "A. Doucet. Rao"], "venue": "In Advances in Neural Information Processing Systems. MIT Press,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2002}, {"title": "Cycle-cutset sampling for Bayesian networks", "author": ["B. Bidyuk", "R. Dechter"], "venue": "In 16th Canadian Conference on AI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Empirical study of wcutset sampling for Bayesian networks", "author": ["B. Bidyuk", "R. Dechter"], "venue": "In UAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Rao-Blackwellised likelihood weighting", "author": ["B. Bidyuk", "R. Dechter"], "venue": "Technical report, UCI, www.ics.uci.edu/ \u0303bbidyuk/lwlc.html,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "AIS-BN: An adaptive importance sampling algorithm for evidenctial reasoning in large baysian networks", "author": ["J. Cheng", "M. Druzdzel"], "venue": "J. of AI Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Constraint Processing", "author": ["R. Dechter"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Rao-Blackwellised particle filtering for dynamic Bayesian networks", "author": ["A. Doucet", "N. de Freitas", "K. Murphy", "S. Russell"], "venue": "In Uncertainty in AI,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Particle filters for state estimation of jump markov linear systems", "author": ["A. Doucet", "N. Gordon", "V. Krishnamurthy"], "venue": "Technical report,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Monte Carlo: concepts, algorithms, and applications", "author": ["G.S. Fishman"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "Weighing and integrating evidence for stochastic simulation in Bayesian networks", "author": ["R. Fung", "K.-C. Chang"], "venue": "In Uncertainty in AI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1989}, {"title": "Backward simulation in Bayesian networks. In Uncertainty in AI, pages 227\u2013234", "author": ["R. Fung", "B. del Favero"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Towards normative expert systems: Part i. the pathfinder project", "author": ["D. Heckerman", "E. Horvitz", "B. Nathawani"], "venue": "Methods of Information in Medicine,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1992}, {"title": "Blocking Gibbs sampling for linkage analysis in large pedigrees with many loops. Research Report R-96-2048", "author": ["C.S. Jensen", "A. Kong"], "venue": "Aalborg University,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "Information Theory and Statistics", "author": ["S. Kullback"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1959}, {"title": "Sequential importance sampling for nonparametric bayes models: The next generation", "author": ["S. MacEachern", "M. Clyde", "J. Liu"], "venue": "The Canadian Journal of Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Loopy belief propagation for approximate inference: An empirical study", "author": ["K.P. Murphy", "Y. Weiss", "M.I. Jordan"], "venue": "In Uncertainty in AI,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1988}, {"title": "Knowledge engineering for large belief networks", "author": ["M. Pradhan", "G. Provan", "B. Middleton", "M. Henrion"], "venue": "In Uncertainty in AI,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Empirical evaluation of approximation algorithms for probabilistic decoding", "author": ["I. Rish", "K. Kask", "R. Dechter"], "venue": "In Uncertainty in AI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Simulation approaches to general probabilistic inference on belief networks", "author": ["R.D. Shachter", "M.A. Peot"], "venue": "In Uncertainty in AI,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1989}, {"title": "An importance sampling algorithm based on evidence prepropagation", "author": ["C. Yuan", "M. Druzdzel"], "venue": "In Uncertainty in AI,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "We defined previously an efficient parametrized Gibbs cutset sampling scheme, called w-cutset sampling [2, 3], where the complexity of generating a single sample is bounded exponentially by w.", "startOffset": 103, "endOffset": 109}, {"referenceID": 2, "context": "We defined previously an efficient parametrized Gibbs cutset sampling scheme, called w-cutset sampling [2, 3], where the complexity of generating a single sample is bounded exponentially by w.", "startOffset": 103, "endOffset": 109}, {"referenceID": 10, "context": "In this paper, we extend the cutset sampling principle to likelihood weighting (LW) [11, 21], which is a form of importance sampling [21], focusing on sampling from a loop-cutset.", "startOffset": 84, "endOffset": 92}, {"referenceID": 20, "context": "In this paper, we extend the cutset sampling principle to likelihood weighting (LW) [11, 21], which is a form of importance sampling [21], focusing on sampling from a loop-cutset.", "startOffset": 84, "endOffset": 92}, {"referenceID": 20, "context": "In this paper, we extend the cutset sampling principle to likelihood weighting (LW) [11, 21], which is a form of importance sampling [21], focusing on sampling from a loop-cutset.", "startOffset": 133, "endOffset": 137}, {"referenceID": 8, "context": "The advantages of cutset-based importance sampling, also known as Rao-Blackwellised importance sampling, were demonstrated previously in a few special cases [9, 8, 1].", "startOffset": 157, "endOffset": 166}, {"referenceID": 7, "context": "The advantages of cutset-based importance sampling, also known as Rao-Blackwellised importance sampling, were demonstrated previously in a few special cases [9, 8, 1].", "startOffset": 157, "endOffset": 166}, {"referenceID": 0, "context": "The advantages of cutset-based importance sampling, also known as Rao-Blackwellised importance sampling, were demonstrated previously in a few special cases [9, 8, 1].", "startOffset": 157, "endOffset": 166}, {"referenceID": 17, "context": "The queries over a singly-connected network can be processed in time linear in the size of the network [18].", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "Likelihood weighting [11, 21] belongs to a family of importance sampling schemes that draw independent samples from a trial distribution Q(X).", "startOffset": 21, "endOffset": 29}, {"referenceID": 20, "context": "Likelihood weighting [11, 21] belongs to a family of importance sampling schemes that draw independent samples from a trial distribution Q(X).", "startOffset": 21, "endOffset": 29}, {"referenceID": 9, "context": "However, when the sample size is large enough, bias can be ignored [10].", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "Consequently, many importance sampling schemes focus on finding an improved sampling distribution by either changing the variable sampling order [12] or updating the sampling distribution based on previously generated samples [21, 5, 22].", "startOffset": 145, "endOffset": 149}, {"referenceID": 20, "context": "Consequently, many importance sampling schemes focus on finding an improved sampling distribution by either changing the variable sampling order [12] or updating the sampling distribution based on previously generated samples [21, 5, 22].", "startOffset": 226, "endOffset": 237}, {"referenceID": 4, "context": "Consequently, many importance sampling schemes focus on finding an improved sampling distribution by either changing the variable sampling order [12] or updating the sampling distribution based on previously generated samples [21, 5, 22].", "startOffset": 226, "endOffset": 237}, {"referenceID": 21, "context": "Consequently, many importance sampling schemes focus on finding an improved sampling distribution by either changing the variable sampling order [12] or updating the sampling distribution based on previously generated samples [21, 5, 22].", "startOffset": 226, "endOffset": 237}, {"referenceID": 8, "context": "where P (C) = \u2211 y P (Y,C) and Q(C) = \u2211 y Q(Y,C) [9, 16] A proof can be found in [9] and [16].", "startOffset": 48, "endOffset": 55}, {"referenceID": 15, "context": "where P (C) = \u2211 y P (Y,C) and Q(C) = \u2211 y Q(Y,C) [9, 16] A proof can be found in [9] and [16].", "startOffset": 48, "endOffset": 55}, {"referenceID": 8, "context": "where P (C) = \u2211 y P (Y,C) and Q(C) = \u2211 y Q(Y,C) [9, 16] A proof can be found in [9] and [16].", "startOffset": 80, "endOffset": 83}, {"referenceID": 15, "context": "where P (C) = \u2211 y P (Y,C) and Q(C) = \u2211 y Q(Y,C) [9, 16] A proof can be found in [9] and [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "We can show this for the KL-distance [15]:", "startOffset": 37, "endOffset": 41}, {"referenceID": 3, "context": "The details are available in [4].", "startOffset": 29, "endOffset": 32}, {"referenceID": 1, "context": "In networks with positive distributions, we compare likelihood weighting side by side with Gibbs sampling (Gibbs) and Gibbsbased loop-cutset sampling (LCS) [2].", "startOffset": 156, "endOffset": 159}, {"referenceID": 17, "context": "Belief propagation computes the exact posterior marginals in poly-trees [18].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "IBP is fast and often produces good estimates [17, 20].", "startOffset": 46, "endOffset": 54}, {"referenceID": 19, "context": "IBP is fast and often produces good estimates [17, 20].", "startOffset": 46, "endOffset": 54}, {"referenceID": 6, "context": "MSE = \u2211 Xi\u2208X\\E \u2211 D(Xi) [P (xi|e)\u2212 P\u0302 (xi|e)] 2 \u2211 Xi\u2208X\\E |D(Xi)| The exact posterior marginals P (Xi|e) are obtained by bucket-tree elimination [7, 6].", "startOffset": 143, "endOffset": 149}, {"referenceID": 5, "context": "MSE = \u2211 Xi\u2208X\\E \u2211 D(Xi) [P (xi|e)\u2212 P\u0302 (xi|e)] 2 \u2211 Xi\u2208X\\E |D(Xi)| The exact posterior marginals P (Xi|e) are obtained by bucket-tree elimination [7, 6].", "startOffset": 143, "endOffset": 149}, {"referenceID": 12, "context": "Pathfinder is an expert system for identifying disorders from lymph node tissue sections [13].", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "Link is a model for the linkage between two genes [14].", "startOffset": 50, "endOffset": 54}, {"referenceID": 18, "context": "cpcs360b and cpcs422b are derived from the Computer-Based Patient Care Simulation system [19].", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "The PathFinder2 network was also used as a benchmark in the evaluation of AIS-BN algorithm [5], an adaptive importance sampling scheme.", "startOffset": 91, "endOffset": 94}, {"referenceID": 3, "context": "In particular, likelihood weighting outperforms Gibbs sampling in cpcs360b and cpcs422b without evidence [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 8, "context": ", when the distributions for the marginalised variables could be computed analytically using a Kalman filter [9, 8, 1] or when the marginalised variables in a factored HMM became conditionally independent (when sampled variables are observed) due to the numerical structure of the CPTs [8].", "startOffset": 109, "endOffset": 118}, {"referenceID": 7, "context": ", when the distributions for the marginalised variables could be computed analytically using a Kalman filter [9, 8, 1] or when the marginalised variables in a factored HMM became conditionally independent (when sampled variables are observed) due to the numerical structure of the CPTs [8].", "startOffset": 109, "endOffset": 118}, {"referenceID": 0, "context": ", when the distributions for the marginalised variables could be computed analytically using a Kalman filter [9, 8, 1] or when the marginalised variables in a factored HMM became conditionally independent (when sampled variables are observed) due to the numerical structure of the CPTs [8].", "startOffset": 109, "endOffset": 118}, {"referenceID": 7, "context": ", when the distributions for the marginalised variables could be computed analytically using a Kalman filter [9, 8, 1] or when the marginalised variables in a factored HMM became conditionally independent (when sampled variables are observed) due to the numerical structure of the CPTs [8].", "startOffset": 286, "endOffset": 289}], "year": 2006, "abstractText": "The paper extends the principle of cutset sampling over Bayesian networks, presented previously for Gibbs sampling, to likelihood weighting (LW). Cutset sampling is motivated by the Rao-Blackwell theorem which implies that sampling over a subset of variables requires fewer samples for convergence due to the reduction in sampling variance. The scheme exploits the network structure in selecting cutsets that allow efficient computation of the sampling distributions. In particular, as we show empirically, likelihood weighting over a loop-cutset (abbreviated LWLC), is time-wise cost-effective. We also provide an effective way for caching the probabilities of the generated samples which improves the performance of the overall scheme. We compare LWLC against regular liklihood-weighting and against Gibbsbased cutset sampling.", "creator": "dvips(k) 5.94b Copyright 2004 Radical Eye Software"}}}