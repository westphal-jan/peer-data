{"id": "1206.6835", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Dimension Reduction in Singularly Perturbed Continuous-Time Bayesian Networks", "abstract": "Continuous-time Bayesian networks (CTBNs) are graphical representations of multi-component continuous-time Markov processes as directed graphs. The edges in the network represent direct influences among components. The joint rate matrix of the multi-component process is specified by means of conditional rate matrices for each component separately. This paper addresses the situation where some of the components evolve on a time scale that is much shorter compared to the time scale of the other components. In this paper, we prove that in the limit where the separation of scales is infinite, the Markov process converges (in distribution, or weakly) to a reduced, or effective Markov process that only involves the slow components. We also demonstrate that for reasonable separation of scale (an order of magnitude) the reduced process is a good approximation of the marginal process over the slow components. We provide a simple procedure for building a reduced CTBN for this effective process, with conditional rate matrices that can be directly calculated from the original CTBN, and discuss the implications for approximate reasoning in large systems.", "histories": [["v1", "Wed, 27 Jun 2012 16:18:35 GMT  (213kb)", "http://arxiv.org/abs/1206.6835v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nir friedman", "raz kupferman"], "accepted": false, "id": "1206.6835"}, "pdf": {"name": "1206.6835.pdf", "metadata": {"source": "CRF", "title": "Dimension Reduction in Singularly Perturbed Continuous-Time Bayesian Networks", "authors": ["Nir Friedman"], "emails": ["nir@cs.huji.ac.il", "raz@math.huji.ac.il"], "sections": [{"heading": null, "text": "The edges in the network represent direct influences between the components. The common velocity matrix of the multi-component process is specified separately by means of conditional velocity matrices for each component. This paper deals with the situation in which some components develop on a time scale that is much shorter than the time scale of the other components. We also show that in the limit where the separation of scales is infinite, the Markov process (in distribution or weak) converges to a reduced or effective Markov process that includes only the slow components. We also show that for a reasonable separation of scales (an order of magnitude) the reduced process is a good approximation of the boundary process over the slow components. We offer a simple method of building a reduced CTBN for this effective process, with conditional velocity matrices that can be calculated directly from the large systems for approximating the TN, and discuss the effects of the original BN."}, {"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 Continuous-time Bayesian networks", "text": "In this section, we briefly review the CTBN model (Nodelman et al. [2002]). Consider an M component Markov processX (t) = (X1 (t), X2 (t),. XM (t)) with state spacesS = S1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 SM. A notational convention: vectors are denoted by Boldface symbols, e.g. X, a, and matrices are denoted by blackboard style characters, e.g. Q. The states in S are denoted by vectors of indices, a = (a1,., aM). Indexes 1 \u2264 i, j \u2264 M are used to renumber the components. The dynamics of a time-homogeneous continuous Markov process are fully determined by the Markov transition function, pa, b (t) = Pr (Xs = a), where time is homogeneous."}, {"heading": "3 Singularly perturbed CTBNs", "text": "In many situations, it is possible to divide the M components into two groups: \"slow\" components and \"slow\" components into a slow state. A default value for the \"speed\" of a Markov process is the speed at which an equilibrium distribution occurs, which is generally considered to be the absolute value of the second largest eigenvalue of the Q matrix. In the context of CTBNs, each component is assigned a conditional rate, the complementary rate of which matrices are assigned to the complementary components. Intuitively, this means that each fast component has passed through many transitions during a single transition to the slow components. For the sake of mathematical analysis, we consider CTBNs, which are parameterized by a small parameter that represents the ratio of a characteristic residence time in a state of a component, a fast component, and a slow component."}, {"heading": "4 Dimension reduction of CTBNs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Segregated fast components", "text": "We start by looking at CTBNs in which fast components are separated: there are no parent-child relationships between two fast components (Q1). In such a case, the conditional equilibrium distribution of the fast components into a product distribution based on the formal, quantitative and quantitative components belonging to Par (i) is, that is, the marginal equilibrium distribution of each fast component depends only on the state of its parental components, which by assuming are all slow components."}, {"heading": "4.2 Connected fast components", "text": "In situations where parents-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-3-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-1-4-4-4-4-4-4-4-4-4-4-4-4-4-4-1-4-4-4-4-4-4-4-4-4-4-1-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-1-players' 4-4-4-4-4-4-4-players' 4 '4-4-4' ''."}, {"heading": "5 Numerical examples", "text": "Example 5.1 Example 4.1 with the state space S = {0, 1}, j = 1, 2, 3, and conditional rate matrices Q1 = (\u2212 1 2 \u2212 2) Q2 | 1 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 1 = (\u2212 3 3 2 \u2212 2) Q3 | 2 \u00b7 | 0 = (\u2212 3 3 4 \u2212 4) Q3 | 2 \u00b7 | 1 = (\u2212 5 5 5 5 5 6 \u2212 6) The conditional equilibrium distribution \u03c02 | 1 \u00b7 | 0 = 1 5 (3 2) \u03c02 | 1 \u00b7 | 1 = 1 5 (2 3 3).As \u2192 0 the slow components X1, X3 weakly converge to a Markov process with effective rate Q-1 = Q1, and Q-3 | 1 given by Q-3."}, {"heading": "6 Discussion", "text": "In this paper, we have demonstrated a theorem on the dimensional reduction of CTBNs in the limit of an infinite separation of scales between fast and slow components. We demonstrated the implications of this theorem for the construction of a reduced CTBN that captures the dynamics of slow components without applying them directly to the fast components. Our results show that eliminating fast components has a counterproductive property when we started. Typical intuition leads to further simplifications than a variable having dependencies between its children. However, when eliminating fast components, this intuition does not apply directly, and in some cases we end up with a simpler CTBN than we started."}, {"heading": "Acknowledgments", "text": "Nir Friedman was supported in part by scholarships from the Israel Science Foundation (ISF) and the Binational USIsrael Science Foundation (BSF), and Raz Kupferman in part by a scholarship from the Israel Science Foundation (ISF)."}, {"heading": "A Proof sketch of Theorem 3.1", "text": "Using the partition of the Q matrix as the sum of fast and slow components, this equation can be integrated, resulting in an integral equation (tQfast) = e t (Q fast) T p (0) + 0 e (t \u2212 s) (Qfast) T (Qslow) Tp (s) ds.The half-group of operators exp (tQfast) is the solution operator of the master equation, which is derived from the fast dynamics where the slow components are fixed. Assumption 3.1 means that exp (tQfast) Tp (s) ds.The half-group of operators exp (tQfast) exp (tQfast) is the solution operator Equp, which derives the equation where the slow component is fixed."}], "references": [{"title": "Markov chains with stationary transition probabilities", "author": ["K.L. Chung"], "venue": null, "citeRegEx": "Chung.,? \\Q1960\\E", "shortCiteRegEx": "Chung.", "year": 1960}, {"title": "Handbook of stochastic methods", "author": ["C.W. Gardiner"], "venue": "SpringerVerlag, New-York, third edition,", "citeRegEx": "Gardiner.,? \\Q2004\\E", "shortCiteRegEx": "Gardiner.", "year": 2004}, {"title": "Skorokhod. The theory of Stochastic processes II", "author": ["A.V.I.I Gikhman"], "venue": null, "citeRegEx": "Gikhman,? \\Q1975\\E", "shortCiteRegEx": "Gikhman", "year": 1975}, {"title": "Extracting macroscopic dynamics: model problems and algorithms", "author": ["D. Givon", "R. Kupferman", "A.M. Stuart"], "venue": "Nonlinearity, 17:R55\u2013R127,", "citeRegEx": "Givon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Givon et al\\.", "year": 2004}, {"title": "Continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In Eighteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Nodelman et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Nodelman et al\\.", "year": 2002}, {"title": "Learning continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In Nineteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "Nodelman et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Nodelman et al\\.", "year": 2003}, {"title": "Elimination of fast variables", "author": ["N.G. van Kampen"], "venue": "Phys. Rep.,", "citeRegEx": "Kampen.,? \\Q1985\\E", "shortCiteRegEx": "Kampen.", "year": 1985}], "referenceMentions": [{"referenceID": 1, "context": ", chemical kinetics, population dynamics, stock markets, and many more (Gardiner [2004]).", "startOffset": 72, "endOffset": 88}, {"referenceID": 1, "context": ", chemical kinetics, population dynamics, stock markets, and many more (Gardiner [2004]). We consider Markov processes that are homogeneous in time and have a finite state space. Such systems are fully determined by the state space S, the distribution of the process at the initial time, and a description of the dynamics of the process. These dynamics are specified by a rate matrix Q, whose off-diagonal entries qa,b are exponential rate intensities for transitioning between states. Intuitively, we can think of the entry qa,b as the rate parameter of an exponential distribution whose value is the duration of time spent in state a before transitioning to b. When more than one transition is possible, the shortest duration determines the next state. Thus, q\u22121 a,b is the expected duration in state a before transitioning to state b (assuming it were the only possible transition), and ( \u2211 b,a qa,b)\u22121 is the expected duration before transitioning out of state a. In many applications, the state space is of the form of a product space S = S1 \u00d7 S1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SM, where M is the number of components (such processes are called multicomponent). Even if each of the Si is of low dimension, the dimension of the state space is exponential in the number of components, which often poses computational difficulties, e.g., in learning applications. Continuous-time Bayesian networks (Nodelman et al. [2002, 2003]) are a graphical representation for Markov processes that have extra structure, therefore allowing for more a compact representation with fewer parameters. The first assumption is that transitions only occur in one component at a time. Second, the transition rates associated with each component are assumed to only depend on the state of a collection of \u201cparent components\u201d. The CTBN is a directed, possibly cyclic graph whose nodes are the components of the process, and whose edges represent parent-child relations. It is also often the case that certain components are considerably faster than the others. Mathematically, it is assumed that the (conditional) rates associated with the fast components are larger by a factor of 1/ than the (conditional) rates associated with the other, slow components, with 1. Systems having such property are said to have a separation of scales, or to be singularly perturbed. Such situations are ubiquitous, for example, in chemical kinetics, where some reactions may occur much faster than other. In such situations, the fast components tend to reach \u201clocal equilibrium\u201d, relative to the slow components, and under certain conditions, reduced Markovian dynamics can be derived for a lower dimensional system that only involves the slow components (see van Kampen [1985] for a classical", "startOffset": 72, "endOffset": 2720}, {"referenceID": 3, "context": "review on dimension reduction in scale separated systems; see Givon et al. [2004] for a recent review).", "startOffset": 62, "endOffset": 82}, {"referenceID": 3, "context": "In this section we briefly review the CTBN model (Nodelman et al. [2002]).", "startOffset": 50, "endOffset": 73}, {"referenceID": 0, "context": "Provided that the transition function satisfies certain analytical properties (continuity, and regularity; see Chung [1960]) the dynamics are fully captured by a constant matrix Q\u2014the rate, or intensity matrix\u2014whose entries qa,b are defined by", "startOffset": 111, "endOffset": 124}, {"referenceID": 2, "context": "There is a one-to-one correspondence between the description of a Markov process by means of a master equation, and by means of a \u201cpathwise\u201d characterization (up to stochastic equivalence of the latter; see Gikhman and Skorokhod [1975]).", "startOffset": 207, "endOffset": 236}, {"referenceID": 4, "context": "Equation (3) is, using the terminology of Nodelman et al. [2002], the \u201camalgamation\u201d of the M conditional rate matrices.", "startOffset": 42, "endOffset": 65}, {"referenceID": 4, "context": "As stated in Nodelman et al. [2002], the graph structure has two main roles: (i) it provides a data structure to which parameters are associated; (ii) it provides a qualitative description of dependencies among the various components of the system.", "startOffset": 13, "endOffset": 36}], "year": 2006, "abstractText": "Continuous-time Bayesian networks (CTBNs) are graphical representations of multi-component continuous-time Markov processes as directed graphs. The edges in the network represent direct influences among components. The joint rate matrix of the multi-component process is specified by means of conditional rate matrices for each component separately. This paper addresses the situation where some of the components evolve on a time scale that is much shorter compared to the time scale of the other components. We prove that in the limit where the separation of scales is infinite, the Markov process converges (in distribution, or weakly) to a reduced, or effective Markov process that only involves the slow components. We also demonstrate that for a reasonable separation of scales (an order of magnitude) the reduced process is a good approximation of the marginal process over the slow components. We provide a simple procedure for building a reduced CTBN for this effective process, with conditional rate matrices that can be directly calculated from the original CTBN, and discuss the implications for approximate reasoning in large systems.", "creator": "TeX"}}}