{"id": "1206.6836", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Methods for computing state similarity in Markov Decision Processes", "abstract": "A popular approach to solving large probabilistic systems relies on aggregating states based on a measure of similarity. Many approaches in the literature are heuristic. A number of recent methods rely instead on metrics based on the notion of bisimulation, or behavioral equivalence between states (Givan et al, 2001, 2003; Ferns et al, 2004). An integral component of such metrics is the Kantorovich metric between probability distributions. However, while this metric enables many satisfying theoretical properties, it is costly to compute in practice. In this paper, we use techniques from network optimization and statistical sampling to overcome this problem. We obtain in this manner a variety of distance functions for MDP state aggregation, which differ in the tradeoff between time and space complexity, as well as the quality of the aggregation. We provide an empirical evaluation of these trade-offs.", "histories": [["v1", "Wed, 27 Jun 2012 16:18:48 GMT  (182kb)", "http://arxiv.org/abs/1206.6836v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["norman ferns", "pablo samuel castro", "doina precup", "prakash panangaden"], "accepted": false, "id": "1206.6836"}, "pdf": {"name": "1206.6836.pdf", "metadata": {"source": "CRF", "title": "Methods for Computing State Similarity in Markov Decision Processes", "authors": ["Norm Ferns", "Pablo Samuel Castro", "Doina Precup", "Prakash Panangaden"], "emails": ["prakash}@cs.mcgill.ca"], "sections": [{"heading": null, "text": "A popular approach to solving large probable systems is based on aggregation of states based on a measure of similarity. Many approaches in the literature are heuristic in nature. A number of newer methods instead rely on metrics based on the concept of bisimulation or behavioral equivalence between states (Givan et al., 2003; Ferns et al., 2004). An integral part of such metrics is the Kantorovich metric between probability distributions. Although this metric provides many satisfactory theoretical properties, it is costly to calculate in practice. In this paper, we use techniques from network optimization and statistical sampling to overcome this problem. In this way, we obtain a variety of distance functions for MDP state aggregation that differ in the goal conflict between time and space complexity and the quality of aggregation."}, {"heading": "1 Introduction", "text": "In fact it is such that most of them will be able to move to another world, in which they are able to move, in which they are able to change the world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live that they live, in which they live, in which they live that they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live"}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Markov Decision Processes", "text": "A finite Markov decision-making process is a quadruple (S, A, {Pass}, {r a}), where S is a finite series of states, A is a finite series of actions, Pass \"is a (Markovian) probability of transition from DP to s\" under action a, \"and ras is the numerical reward for choosing action a in the state. The discounted, infinite horizon planning task in an MDP is to determine a political \u03c0: S \u2192 A, which maximizes the value of each state, V \u03c0 (s) = E [[[orizon] t = 0 \u03b3trt + 1 | s0 = s, \u03c0], where s0 is the state at time 0, rt + 1 is the reward achieved at time t + 1, \u03b3 is a discount factor in (0.1), and the expectation is taken by following the optimal dynamics induced by GDP. The function V \u03c0 is called the value function of the political line, which is associated with an optimal V, the optimal policy."}, {"heading": "2.2 Bisimulation Metrics", "text": "In Ferns et al. (2004; 2005) we provide the following metric generalization of the bisimulated Q function: Theorem (0.1) and V are the real value functions limited to S \u00b7 S, where R = maxs, s \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a, a, a \u00b2, a, a \u00b2, a, a, c, a, c, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a \u00b2, a, a, a, a, a, a, a, c, a, c, a, a, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c,"}, {"heading": "3 State Similarity Metrics", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Fixed Point", "text": "The metric defined by sentence 2.2 can be rewritten as follows: dcf ix (s, s \u2032) = max a \u0441A (| ras \u2212 r a s \u2032 | + cTK (d c f ix) (P a s, P a s \u2032). Each of these metrics is continuous in the MDP parameters {ras} and {P a ss \u2032} and allows narrow limits to the optimal value function, since the optimal value function with the discounting factor \u03b3 Lipschitz is continuous in relation to any metric that c anymore satisfies. Thus, when applying this metric to aggregate states, it is easy to solve instability problems and restore optimal solutions. Furthermore, the theorem offers a method for calculating dcf ix: starting from the metric that is zero everywhere, F c c is applied iteratively until a prescribed level of accuracy is reached. Unfortunately, the direct calculation of the Kantorovich metric is highly limited to the use of any metric and the fixing point."}, {"heading": "3.2 Fixed Point with Cost Reoptimization", "text": "One way to overcome the costly calculation of Kantorovich LP for each iteration is to optimize the costs.The idea is that if h0 is close to h1 at (uniform standard) distance, then the optimal solutions for TK (h0) (P, Q) and TK (h1) (P, Q) should be close; so instead of starting an algorithm for network optimization for TK (h1) (P, Q) from scratch, we store the optimal solution for TK (h0) (P, Q) and use it as starting solution3. In a sense, we are called sensitivity analysis in LP jargon. The convergence of iterates (Fc) n (h0) to dcf ix in uniform standardization of an MCF-LP with cost function dcf ix. This idea was quite comprehensively and successfully explored in (Frangioni & Manca, 2006)."}, {"heading": "3.3 Fixed Point with Sampling", "text": "A more promising approach is a fast and efficient approach resulting from statistical sampling. Suppose P and Q are approximated using the empirical distributions Pi and Qi. That is, we randomly write i points X1, X2,.., Xi independently after P and define Pi by Pi (x) = 1i \u2211 i = 1 \u03b4Xk (x). Similarly, we write Qi (x) = 1i \u2211 i k = 1 \u03b4Yk (x). ThenTK (h) (Pi, Qi) = min\u03c3 k = 1 h (Xk, Y\u03c3 (k)) (1), where the minimum of all permutations is taken on i-elements (see p. 12 of Villani (2002); this is a sequence of Birkhoff's theorem). Now, the strong law of large numbers (SLLLLN) tells us that both Pi (x) and Qi (x) and Qi (Qi) are safely fixed (x) almost a problem."}, {"heading": "3.4 Total Variation", "text": "The standard metric for measuring the distance between the probability functions is the total variation metric, defined by TV (P, Q) = 12. In graphical theoretical terminology, this is the problem of optimal fit in a weighted chart. \u2212 The norm of P \u2212 Q. It is a strong measure of convergence, in the sense that distributions only have distance zero if they agree precisely on transitions to each state. In contrast, the Kantorovich metric requires agreement only on classes of states. Nevertheless, the total variation is a simple concept and one that is easy to calculate. In Ferns et al. (2004), we suggested that instead of the iterative application of Fc to an initial metric h0 to convergence with dcf ix, the total metric is h0."}, {"heading": "4 Experiments", "text": "It was the only reward in the middle of the room, where the robot has to decide for the user and where he does not have to get wet. Note: the three network worlds have chosen 36, 100 and 196 states, which would allow a thorough investigation of all characteristics of the metrics. For all network worlds, the state includes both the position and the orientation of the agent. So the three network worlds have 36, 100 and 196 states. The actions are deterrent. The coffee domain has 64 states and 4 actions, some with stochastic effects. Five methods were used to calculate distances for these MDPs: dcf ix, d c f ix, c f ix with cost reoptimization, dci sc, d TV, fixed and d 6c."}, {"heading": "5 Conclusions and future work", "text": "In this paper, we discussed four state similarity metrics based on the concept of bisimulation. We compared and compared them both in theory and in practice. On the basis of these results, the measurement obtained by scanning distributions seems to be the clear winner: it clearly outperforms other approaches when it comes to considering the trade-off between the computational demands of time and space on the one hand and the quality of the results obtained when using this method for state aggregation on the other. The next step is to test this measurement in large-scale environments. Various versions of this measurement, which are based on ideas from incremental amplification learning algorithms rather than batch processing, will also be investigated. By using such techniques, the calculation could be done much faster. Versions of these metrics for factored state spaces are also of great interest. The sampling approach is also promising for calculating metrics in continuous state spaces, as we have found in previous work on the existence of MPs."}, {"heading": "Acknowledgments", "text": "This work was partially supported by funding from NSERC and the CGEI."}, {"heading": "Appendix A: Proofs", "text": "So for each P and Q \u2212 \u2212 \u2212 SLN for each x-S, and similarly for each x-S, we can choose for each x-S so large that Pi (x) \u2212 P (x) \u2212 P (x) \u2212 P (x) \u2212 P (x) is so large that we can choose for each x-S so large that Pi (x) \u2212 P (x) \u2212 P (x) \u2212 P (x) \u2212 P (x) \u2212 P (x) is so large that Pi (x) \u2212 P (x) \u2212 P (x) is so large. \u2212 SLN for all x-S, and similarly for each x-S so large that we are ix (x) \u2212 P (x) \u2212 P (x) \u2212 P (x) \u2212 P (x) \u2212 P (x-S)."}], "references": [{"title": "Decisiontheoretic planning: Structural assumptions and computational leverage", "author": ["C. Boutilier", "T. Dean", "S. Hanks"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "Exploiting structure in policy", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "construction. IJCAI,", "citeRegEx": "Boutilier et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1995}, {"title": "Metrics for labeled Markov systems", "author": ["J. Desharnais", "V. Gupta", "R. Jagadeesan", "P. Panangaden"], "venue": "International Conference on Concurrency Theory (pp", "citeRegEx": "Desharnais et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Desharnais et al\\.", "year": 1999}, {"title": "Metrics for finite Markov decision processes Proceedings of the 20th conference on Uncertainty in artificial intelligence", "author": ["N. Ferns", "P. Panangaden", "D. Precup"], "venue": null, "citeRegEx": "Ferns et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ferns et al\\.", "year": 2004}, {"title": "Metrics for Markov decision processes with infinite state spaces Proceedings of the 21st conference on Uncertainty in artificial intelligence", "author": ["N. Ferns", "P. Panangaden", "D. Precup"], "venue": null, "citeRegEx": "Ferns et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ferns et al\\.", "year": 2005}, {"title": "A Computational Study of Cost Reoptimization for Min Cost Flow Problems", "author": ["A. Frangioni", "A. Manca"], "venue": "INFORMS Journal On Computing,", "citeRegEx": "Frangioni and Manca,? \\Q2006\\E", "shortCiteRegEx": "Frangioni and Manca", "year": 2006}, {"title": "Equivalence notions and model minimization in markov decision processes", "author": ["R. Givan", "T. Dean", "M. Greig"], "venue": "Artificial Intelligence,", "citeRegEx": "Givan et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Givan et al\\.", "year": 2003}, {"title": "Bisimulation through probabilistic testing", "author": ["K. Larsen", "A. Skou"], "venue": "Information and Computation,", "citeRegEx": "Larsen and Skou,? \\Q1991\\E", "shortCiteRegEx": "Larsen and Skou", "year": 1991}, {"title": "A calculus of communicating systems", "author": ["R. Milner"], "venue": "Lecture Notes in Computer Science", "citeRegEx": "Milner,? \\Q1980\\E", "shortCiteRegEx": "Milner", "year": 1980}, {"title": "Algorithms for the assignment and transportation problems", "author": ["J. Munkres"], "venue": "J. SIAM (pp", "citeRegEx": "Munkres,? \\Q1957\\E", "shortCiteRegEx": "Munkres", "year": 1957}, {"title": "A faster strongly polynomial minimum cost flow algorithm", "author": ["J. Orlin"], "venue": "Proceedings of the Twentieth annual ACM symposium on Theory of Computing (pp", "citeRegEx": "Orlin,? \\Q1988\\E", "shortCiteRegEx": "Orlin", "year": 1988}, {"title": "Concurrency and automata on infinite sequences", "author": ["D. Park"], "venue": "Proceedings of the 5th GI-Conference on Theoretical Computer Science (pp. 167\u2013183)", "citeRegEx": "Park,? \\Q1981\\E", "shortCiteRegEx": "Park", "year": 1981}, {"title": "Markov decision processes: Discrete stochastic dynamic programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "Puterman,? \\Q1994\\E", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Towards quantitative verification of probabilistic transition systems", "author": ["F. van Breugel", "J. Worrell"], "venue": "Proceedings of the 28th International Colloquium on Automata, Languages, and Programming (ICALP),", "citeRegEx": "Breugel and Worrell,? \\Q2001\\E", "shortCiteRegEx": "Breugel and Worrell", "year": 2001}, {"title": "An algorithm for quantitative verification of probabilistic transition systems", "author": ["F. van Breugel", "J. Worrell"], "venue": "Proceedings of the 12th International Conference on Concurrency Theory (pp. 336\u2013350)", "citeRegEx": "Breugel and Worrell,? \\Q2001\\E", "shortCiteRegEx": "Breugel and Worrell", "year": 2001}, {"title": "Topics in Mass Transportation. [http: //www.math.toronto.edu/hmaroofi", "author": ["C. Villani"], "venue": null, "citeRegEx": "Villani,? \\Q2002\\E", "shortCiteRegEx": "Villani", "year": 2002}], "referenceMentions": [{"referenceID": 6, "context": "A number of recent methods rely instead on metrics based on the notion of bisimulation, or behavioral equivalence between states (Givan et al., 2003; Ferns et al., 2004).", "startOffset": 129, "endOffset": 169}, {"referenceID": 3, "context": "A number of recent methods rely instead on metrics based on the notion of bisimulation, or behavioral equivalence between states (Givan et al., 2003; Ferns et al., 2004).", "startOffset": 129, "endOffset": 169}, {"referenceID": 0, "context": "Markov decision processes (MDPs) are the model of choice for decision making under uncertainty (Boutilier et al., 1999), and provide a standard formalism for describing multi-stage decision making in probabilistic environments.", "startOffset": 95, "endOffset": 119}, {"referenceID": 12, "context": ", value iteration or policy iteration (Puterman, 1994), allow computing the optimal expected return for any state, as well as the way of behaving (policy) that generates this return.", "startOffset": 38, "endOffset": 54}, {"referenceID": 8, "context": "It has been well argued that the notion of \u201cessentially equivalent\u201d in probabilistic systems is perhaps best captured formally by bisimulation (Milner, 1980; Park, 1981; Larsen & Skou, 1991).", "startOffset": 143, "endOffset": 190}, {"referenceID": 11, "context": "It has been well argued that the notion of \u201cessentially equivalent\u201d in probabilistic systems is perhaps best captured formally by bisimulation (Milner, 1980; Park, 1981; Larsen & Skou, 1991).", "startOffset": 143, "endOffset": 190}, {"referenceID": 6, "context": "The bisimulation equivalence classes can even be computed iteratively in polynomial time (Givan et al., 2003).", "startOffset": 89, "endOffset": 109}, {"referenceID": 2, "context": "(2004; 2005), based on similar work in the context of labeled Markov processes (Desharnais et al., 1999; van Breugel and Worrell 2001a; 2001b), we sought to extend bisimulation for MDPs quantitatively in terms of such metrics.", "startOffset": 79, "endOffset": 142}, {"referenceID": 6, "context": "Givan et al. (2003) investigated several notions of MDP state equivalence and determined that the most appropriate is bisimulation.", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "Since there exist strongly polynomial algorithms to compute the MCF problem (Orlin, 1988), the 1Results appear here in slightly modified form.", "startOffset": 76, "endOffset": 89}, {"referenceID": 12, "context": "For our purposes, this amounts to a worst case running time of O(|S|3 log |S|) for each Kantorovich LP (contrast this with the general LP for directly computing the optimal value function: this has |S| variables and |A||S| constraints (Puterman, 1994)).", "startOffset": 235, "endOffset": 251}, {"referenceID": 15, "context": "12 of Villani (2002); this is a consequence of Birkhoff\u2019s theorem).", "startOffset": 6, "endOffset": 21}, {"referenceID": 9, "context": "For example, the Hungarian algorithm (for a description see Munkres (1957)) runs in worst case time O(i3), where i is the number of samples.", "startOffset": 60, "endOffset": 75}, {"referenceID": 3, "context": "In Ferns et al. (2004), we suggested that in place of iteratively applying Fc to an initial metric h0 until convergence to df ix, we start with an appropriately chosen h0 and apply Fc only once.", "startOffset": 3, "endOffset": 23}, {"referenceID": 1, "context": "Experiments were run on four different MDPs: a 3\u00d73 grid world with two actions (move forward and rotate) and a single reward in the center of the room; a 5\u00d75 and a 7\u00d77 grid world each with the same dynamics; and a flattened out version of the coffee robot MDP (Boutilier et al., 1995) where the robot has to get coffee for the user and avoid getting wet.", "startOffset": 260, "endOffset": 284}, {"referenceID": 4, "context": "In prior works, we established the existence of of bisimulation metrics for continuous MDPs (Ferns et al., 2005).", "startOffset": 92, "endOffset": 112}], "year": 0, "abstractText": "A popular approach to solving large probabilistic systems relies on aggregating states based on a measure of similarity. Many approaches in the literature are heuristic. A number of recent methods rely instead on metrics based on the notion of bisimulation, or behavioral equivalence between states (Givan et al., 2003; Ferns et al., 2004). An integral component of such metrics is the Kantorovich metric between probability distributions. However, while this metric enables many satisfying theoretical properties, it is costly to compute in practice. In this paper, we use techniques from network optimization and statistical sampling to overcome this problem. We obtain in this manner a variety of distance functions for MDP state aggregation that differ in the tradeoff between time and space complexity, as well as the quality of the aggregation. We provide an empirical evaluation of these tradeoffs.", "creator": null}}}