{"id": "1206.6838", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Continuous Time Markov Networks", "abstract": "A central task in many applications is reasoning about processes that change in a continuous time. The mathematical framework of Continuous Time Markov Processes provides the basic foundations for modeling such systems. Recently, Nodelman et al introduced continuous time Bayesian networks (CTBNs), which allow a compact representation of continuous-time processes over a factored state space. In this paper, we introduce continuous time Markov networks (CTMNs), an alternative representation language that represents a different type of continuous-time dynamics. In many real life processes, such as biological and chemical systems, the dynamics of the process can be naturally described as an interplay between two forces - the tendency of each entity to change its state, and the overall fitness or energy function of the entire system. In our model, the first force is described by a continuous-time proposal process that suggests possible local changes to the state of the system at different rates. The second force is represented by a Markov network that encodes the fitness, or desirability, of different states; a proposed local change is then accepted with a probability that is a function of the change in the fitness distribution. We show that the fitness distribution is also the stationary distribution of the Markov process, so that this representation provides a characterization of a temporal process whose stationary distribution has a compact graphical representation. This allows us to naturally capture a different type of structure in complex dynamical processes, such as evolving biological sequences. We describe the semantics of the representation, its basic properties, and how it compares to CTBNs. We also provide algorithms for learning such models from data, and discuss its applicability to biological sequence evolution.", "histories": [["v1", "Wed, 27 Jun 2012 16:19:16 GMT  (380kb)", "http://arxiv.org/abs/1206.6838v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["tal el-hay", "nir friedman", "daphne koller", "raz kupferman"], "accepted": false, "id": "1206.6838"}, "pdf": {"name": "1206.6838.pdf", "metadata": {"source": "CRF", "title": "Continuous Time Markov Networks", "authors": ["Tal El-Hay", "Nir Friedman"], "emails": ["tale@cs.huji.ac.il", "nir@cs.huji.ac.il", "koller@cs.stanford.edu", "raz@math.huji.ac.il"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Reversible Continuous Time Markov Processes", "text": "We refer the interested reader to Taylor and Karlin [14] and Chung [2] for more in-depth explanations. Suppose we have a family of random variables {X (t): t \u2265 0} where the continuous index t denotes complete time. A common distribution over these random variables is a homogeneous continuous time history Markov process (CTMP) when it has the Markov property {X (tk + 1) | X (t0)) = Pr (tk + 1) | X (tk + 1) | X (tk))) for all tk > tk >. > t0, and time homogeneity, Pr (s + t) = y | X (s) = Pr (s \u2032 t) = y."}, {"heading": "3 Continuous Time Metropolis Processes", "text": "We start by accepting a reformulation of the reversible CTMPs as a continuous time version of the Metropolis scanning processes. We consider the process to be an interplay between two factors: the first is an unbiased random process that attempts to go through a transition between the states of the system, and the second is the tendency of the system to remain in more likely states, the latter probability being considered as the stationary distribution of the process; the first is the unbiased suggestion of the transitions that are either accepted or rejected; we denote the rate at which proposals for the transition x \u2192 y occur; we assume that we define a CTMP process with rate matrixR. To ensure an unbiased proposal of the transitions, we need R to symmetric rates."}, {"heading": "4 Continuous Time Markov Networks", "text": "We are interested in dealing with structured, multi-component systems whose state description does not depend on the structure of the second system. It is an assignment to a series of state variables X = < X1, X2,.., Xn >, with each Xi assuming a finite set of values. However, the biggest challenge is dealing with the large state space (exponentially in n). We aim to find {{} x {2 brief representations of the dynamics of the system within the continuous time metropolis. We do this in two stages, first dealing with the application rate R and then with the equilibrium distribution. Our first assumption is that proposed transitions are local. Specifically, we require that for x 6 = yrx, y = {rixi, yj = yj) j 6 = i 0 otherwise (4), where Ri = {rixi, yi} are symmetric local transition rates for Xi."}, {"heading": "5 Connection to CTBNs", "text": "The factored form of Eq. (6) allows us to relate CTMNs to CTBNs. A CTBN is defined by a directional (often cyclic) graph whose nodes correspond to the process variables, and whose edges represent direct influences of one variable on the development of another. Specifically, a CTBN is defined by a collection of conditional rate matrices (also referred to as conditional rate matrices) for each Xi, and for every possible value of its direct parents in the CTBN curve, the matrix QXi | ui is a rate matrix across the state space of Xi that combines a conditional rate matrix into a global rate matrix. [9] We call amalgamation if x and y are identical, except for the value of Xi, thenqx, y = qXi."}, {"heading": "6 Parameter Learning", "text": "We therefore assume that we obtain the form of \u03c0, i.e. the characteristic set, and that we must learn the parameters \u03b8, which apply to \u03c0 and the local rate matrices Ri, which determine the supply rates for each variable. We begin by looking at this problem in the context of complete data, where our observations consist of complete trajectories of the system. As we show, we define a gradient ascent procedure to learn the parameters from such data. This result also allows us to learn from incomplete data using the standard EM procedure, which combination is quite standard and follows the lines of similar procedures for CTBNs [10], and therefore we effectively execute the E step from partially observable data in order to calculate expected sufficient statistics. M step is then an application of the learning process for complete data with these expected sufficient statistics."}, {"heading": "6.1 The Likelihood Function", "text": "A key term in solving the learning problem is the probability function, which determines how the probability of the observations depends on the parameters. We assume that the data are complete and that our observations consist of a trajectory of the system, which can be described as a sequence of intervals in which the system is in a state at each interval. (8) One problem with this approach is that the entries in the conditional rate link both parameters from Ri and parameters from the resulting probability function to the estimation of these two sets of parameters; if we had additional information, we could decouple these two sets of parameters."}, {"heading": "6.2 Maximizing the Likelihood Function", "text": "Within the framework of the Maximum Likelihood Principle, our estimated parameters are those that maximize the probability function based on the observations. We will now examine how probability can be maximized.The decoupling of probability into several terms allows us to estimate each set of parameters separately.The estimation of Ri is simple: the introduction of the symmetry condition, the maximum probability estimate isrixi, yi = M [xi, yi] + M [yi] T [xi] + T [yi].The determination of the maximum probability parameters of \u03c0 is somewhat more complicated. Note that the probability parameters of the probability calculation's (hash: hash) are quite different from the probability of a log-linear distribution i.d. data [3], the probability of acceptance or rejection of the probability is that the probability varies."}, {"heading": "6.3 Completing the Data", "text": "Our derivation of probability and the associated optimization process is based on the assumption that rejected q transitional attempts are also observed in the data. As we can see from the form of probability, these failures play an important role in the estimation of the parameters. The question is how we adapt the procedure to the case in which rejected proposals are not complied with. Our solution to this problem is the application of expectation maximization, in which we consider the application attempts as the unobserved variables. In this approach, we start with an initial estimate of the model parameters. We use these to estimate the expected number of rejected proposals; we treat these expected counts as if they were real, and maximize the probability with the procedure described in the previous section. We repeat these iterations until convergence. The question is how to calculate the expected number of rejected attempts. It turns out that this calculation can be performed analytically."}, {"heading": "7 A Numerical Example", "text": "In fact, the fact is that most of them will be able to go to another world, in which they will be able to go to another world, in which they will be able to go to another world, in which they will be able to go to another world, in which they will be able to live."}, {"heading": "8 Discussion and Future Work", "text": "This year it is more than ever before."}, {"heading": "Acknowledgments", "text": "We thank A. Jaimovich, T. Kaplan, M. Ninio, I. Wiener, and the anonymous reviewers for comments on earlier versions of this manuscript. This work was supported by scholarships from the Israel Science Foundation and the USIsrael Binational Science Foundation, as well as the DARPA CALO program under the auspices of SRI International."}, {"heading": "A Gradient for Learning CTMNs", "text": "We now calculate the derivation of the gradient of the log probability as indicated in Proposition 6.1. To find the derivatives, we differentiate these functions in terms of parameters and then apply the chain rule for the derivatives: \"Weight Compensation\" (\"Weight Compensation\") - \"Weight Compensation\" (\"Weight Compensation\") - \"Weight Compensation\" (\"Weight Compensation\") - \"Weight Compensation\" - \"Weight Compensation\" - \"Weight Compensation\" (\"Weight Compensation\") - \"Weight Compensation\" (\"Weight Compensation\") - \"Weight Compensation\" (\"Weight Compensation\") - \"Weight Compensation\" (\"Weight Compensation\") - \"Weight Compensation\" (\"Weight Compensation\") - (\"Weight Compensation\" - \") -\" Weight Compensation \"(\" Weight Compensation \") - (\" Weight Compensation \") - (\" Weight Compensation \"- (\") - (\"Weight Compensation\") - (\"Weight Compensation - (\") - (\"Weight Compensation - (\") - (\"Weight Compensation - (\") - (\"Weight Compensation - (\") - (\"- (\" Weight) - (\"Weight Compensation - (\") - (\"- (\" Weight) - (\"Weight Compensation - (\") - (\"- - (\" Weight)) - (\"- (\" Weight Compensation - (\"- (\")) - (\"- (\" - (\"Weight) - (\" - (\")) - (\" Weight Compensation - (\"- (\")) - (\"- (\" - (\") - (\" Weight)) - (\"- (\" - (\"Weight) - (\" - (\"- (\"))) - (\"Weight Compensation - (\" - (\"- ()) - () - () - (() - () - (())) - () - () - () - () - () - () - () - () - () - () - () - () - () - ("}], "references": [{"title": "On the statistical analysis of dirty pictures", "author": ["J. Besag"], "venue": "J. Roy. Stat. Soc. B Met.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1986}, {"title": "Markov chains with stationary transition probabilities", "author": ["K.L. Chung"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1960}, {"title": "Inducing features of random fields", "author": ["S. Della Pietra", "V. Della Pietra", "J. Lafferty"], "venue": "IEEE Trans. PAMI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Mach. Learn.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Equation of state calculation by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "J. Chem. Phys.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1953}, {"title": "Continuous time particle filtering", "author": ["B. Ng", "A. Pfeffer", "R. Dearden"], "venue": "In IJCAI \u201905", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In UAI", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Learning continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In UAI", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "Expectation maximization and complex duration distributions for continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In UAI", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Expectation propagation for continuous time Bayesian networks", "author": ["U. Nodelman", "C.R. Shelton", "D. Koller"], "venue": "In UAI", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1988}, {"title": "Evolutionary information for specifying a protein", "author": ["M. Socolich"], "venue": "fold. Nature,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "An Introduction to Stochastic Modeling", "author": ["H.M. Taylor", "S. Karlin"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}], "referenceMentions": [{"referenceID": 6, "context": "[8, 9, 10, 11] introduced the representation language of continuous time Bayesian networks (CTBNs), which provides a factorized, componentbased representation of CTMP: each component is characterized by a conditional CTMP dynamics, which describes its local evolution as a function of the current state of its parents in the network.", "startOffset": 0, "endOffset": 14}, {"referenceID": 7, "context": "[8, 9, 10, 11] introduced the representation language of continuous time Bayesian networks (CTBNs), which provides a factorized, componentbased representation of CTMP: each component is characterized by a conditional CTMP dynamics, which describes its local evolution as a function of the current state of its parents in the network.", "startOffset": 0, "endOffset": 14}, {"referenceID": 8, "context": "[8, 9, 10, 11] introduced the representation language of continuous time Bayesian networks (CTBNs), which provides a factorized, componentbased representation of CTMP: each component is characterized by a conditional CTMP dynamics, which describes its local evolution as a function of the current state of its parents in the network.", "startOffset": 0, "endOffset": 14}, {"referenceID": 9, "context": "[8, 9, 10, 11] introduced the representation language of continuous time Bayesian networks (CTBNs), which provides a factorized, componentbased representation of CTMP: each component is characterized by a conditional CTMP dynamics, which describes its local evolution as a function of the current state of its parents in the network.", "startOffset": 0, "endOffset": 14}, {"referenceID": 11, "context": "[13] suggest that pairwise Markov networks can fairly accurately capture the fitness of protein sequences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "We provide a reduction from CTMNs to CTBNs, allowing us to use CTBN algorithms [7, 11] to perform effective approximate inference in CTMNs.", "startOffset": 79, "endOffset": 86}, {"referenceID": 9, "context": "We provide a reduction from CTMNs to CTBNs, allowing us to use CTBN algorithms [7, 11] to perform effective approximate inference in CTMNs.", "startOffset": 79, "endOffset": 86}, {"referenceID": 12, "context": "We refer the interested reader to Taylor and Karlin [14] and Chung [2] for more thorough expositions.", "startOffset": 52, "endOffset": 56}, {"referenceID": 1, "context": "We refer the interested reader to Taylor and Karlin [14] and Chung [2] for more thorough expositions.", "startOffset": 67, "endOffset": 70}, {"referenceID": 1, "context": "Provided that the transition function satisfies certain analytical properties (see [2]) the dynamics are fully captured by a constant matrix Q \u2014 the rate, or intensity matrix \u2014whose entries qx,y are defined by", "startOffset": 83, "endOffset": 86}, {"referenceID": 4, "context": "The structure of the process can be thought of as going through iterations of proposed transitions that are either accepted or rejected, similar to the Metropolis sampler [6].", "startOffset": 171, "endOffset": 174}, {"referenceID": 10, "context": "We define theMarkov Blanket, NG(i), of the variableXi as the set of neighbors of Xi in the graph G [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 6, "context": "However, since we are examining a continuous process, we need to consider independencies between full trajectories (see also [8]).", "startOffset": 125, "endOffset": 128}, {"referenceID": 10, "context": "Proof: (sketch) Using the global independence properties of a Markov network (see for example, [12]), we have that \u03c0 can be written as a product of two function each with its own domain X1 and X2 such that X1 \u2229 X2 = C and A \u2286 X1 and B \u2286 X2.", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "That is, the usual conditional separation criterion in Markov networks [12] applies in a trajectory-wise fashion to CTMNs.", "startOffset": 71, "endOffset": 75}, {"referenceID": 7, "context": "[9] call amalgamation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "This parametrization violates both local and global parameter independence [5] in the resulting CTBN.", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "ference methods for CTBNs [11, 7], including for the Estep used when learning CTMNs for partially observable data.", "startOffset": 26, "endOffset": 33}, {"referenceID": 5, "context": "ference methods for CTBNs [11, 7], including for the Estep used when learning CTMNs for partially observable data.", "startOffset": 26, "endOffset": 33}, {"referenceID": 8, "context": "This combination is quite standard and follows the lines of similar procedure for CTBNs [10], and therefore we do not expand on it here.", "startOffset": 88, "endOffset": 92}, {"referenceID": 7, "context": "[9] to write the probability of the data as a function of sufficient statistics and entries in the conditional rate matrices of Eq.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The first vector, \u03c4 = \u3008\u03c4 [1], .", "startOffset": 25, "endOffset": 28}, {"referenceID": 6, "context": "1 2 3 4 5 6 7 8 9 10 11 12 14 13 15 \u03c4[8]", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "y[8] x[8]", "startOffset": 1, "endOffset": 4}, {"referenceID": 6, "context": "y[8] x[8]", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "posal took place at time \u03c4 [1], the second at time \u03c4 [1]+\u03c4 [2], and so on.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "posal took place at time \u03c4 [1], the second at time \u03c4 [1]+\u03c4 [2], and so on.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "posal took place at time \u03c4 [1], the second at time \u03c4 [1]+\u03c4 [2], and so on.", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "The second vector, \u039e = \u3008x[0],x[1], .", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "Thus, x[0] is the initial state of the system, x[1] is the state after the first proposal, and so on.", "startOffset": 48, "endOffset": 51}, {"referenceID": 0, "context": "Finally, \u03a5 = \u3008y[1], .", "startOffset": 15, "endOffset": 18}, {"referenceID": 2, "context": "data [3].", "startOffset": 5, "endOffset": 8}, {"referenceID": 0, "context": "In a sense, our likelihood is closely related to the pseudo-likelihood for log-linear models [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "This procedure uses gradient ascent to maximize the likelihood [3].", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "Here we used the methods designed for parameter learning of CTBNs in [9].", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "These results can be combined for learning from partial observations, by plugging in the learning procedure as the M-step in the EM procedure for CTBNs [10].", "startOffset": 152, "endOffset": 156}, {"referenceID": 8, "context": "[10] show how one can expand the framework of CTBNs to allow a richer set of duration distributions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The latter function is of course very complex, but there is empirical evidence that modeling pairwise interactions can provide a good approximation [13].", "startOffset": 148, "endOffset": 152}], "year": 2006, "abstractText": "A central task in many applications is reasoning about processes that change over continuous time. Recently, Nodelman et al. introduced continuous time Bayesian networks (CTBNs), a structured representation for representing Continuous Time Markov Processes over a structured state space. In this paper, we introduce continuous time Markov networks (CTMNs), an alternative representation language that represents a different type of continuous-time dynamics, particularly appropriate for modeling biological and chemical systems. In this language, the dynamics of the process is described as an interplay between two forces: the tendency of each entity to change its state, which we model using a continuous-time proposal process that suggests possible local changes to the state of the system at different rates; and a global fitness or energy function of the entire system, governing the probability that a proposed change is accepted, which we capture by a Markov network that encodes the fitness of different states. We show that the fitness distribution is also the stationary distribution of the Markov process, so that this representation provides a characterization of a temporal process whose stationary distribution has a compact graphical representation. We describe the semantics of the representation, its basic properties, and how it compares to CTBNs. We also provide an algorithm for learning such models from data, and demonstrate its potential benefit over other learning approaches.", "creator": "TeX"}}}