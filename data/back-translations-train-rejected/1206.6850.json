{"id": "1206.6850", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Visualization of Collaborative Data", "abstract": "Collaborative data consist of ratings relating two distinct sets of objects: users and items. Much of the work with such data focuses on filtering: predicting unknown ratings for pairs of users and items. In this paper we focus on the problem of visualizing the information. Given all of the ratings, our task is to embed all of the users and items as points in the same Euclidean space. We would like to place users near items that they have rated (or would rate) high, and far away from those they would give a low rating. We pose this problem as a real-valued non-linear Bayesian network and employ Markov chain Monte Carlo and expectation maximization to find an embedding. We present a metric by which to judge the quality of a visualization and compare our results to local linear embedding and Eigentaste on three real-world datasets.", "histories": [["v1", "Wed, 27 Jun 2012 16:24:29 GMT  (447kb)", "http://arxiv.org/abs/1206.6850v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.GR cs.AI cs.HC", "authors": ["guobiao mei", "christian r shelton"], "accepted": false, "id": "1206.6850"}, "pdf": {"name": "1206.6850.pdf", "metadata": {"source": "CRF", "title": "Visualization of Collaborative Data", "authors": ["Guobiao Mei", "Christian R. Shelton"], "emails": ["gmei@cs.ucr.edu", "cshelton@cs.ucr.edu"], "sections": [{"heading": null, "text": "Common data consists of ratings that relate to two different types of objects: users and objects. Much of the work with such data focuses on filtering: predicting unknown ratings for users and objects. In this essay, we focus on the problem of visualizing the information. In light of all these ratings, our task is to embed all users and objects as points in the same Euclidean space. We want to place users near objects that they have highly rated (or would rate) and far from those that they would give low ratings. We present this problem as a real valuable non-linear Bayesian network and deal with the Markov chain Monte Carlo and the expectation maximization to find an embedding. We present a metric by which we can assess the quality of a visualization and compare our results with property keys, locally linear embedding and coincidence data embedded in three real data sets."}, {"heading": "1 Introduction", "text": "Common data composed of correlated users and items is abundant: movie recommendations, music rankings and book reviews, for example. Most of the work on such data has been done in the area of collaborative filtering: making predictions or recommendations based on previous user ratings. For this task, we introduce a simple evaluation criterion that is natural and allows numerical comparisons of possible visualizations. With all ratings, the visualization problem is \"soft\" in nature and it is difficult to compare alternative methods. For this task, we introduce a simple evaluation criterion that allows numerical comparisons of possible visualizations. With all ratings, the visualization problem is to extract intrinsic similarities or differences between all users and items involved, and visualize them graphically. This has a wide range of applications, for example, that are conducted on online shopping. Visualize traditional stores, allowing them to go up and down the items, and move easily."}, {"heading": "1.1 Prior Work", "text": "There are many existing collaborative distribution mechanisms that focus on the task of predicting; we cannot check them all and apply them directly to the task of visualization. Breese et al. (1998) classifies the approaches into two main categories: memory based on the learned algorithms. Our work is mainly based on the existing preferences, while the model-based algorithms first try to learn the parameters of a particular model for the existing user preferences, and then on the learned approaches."}, {"heading": "2 The Visualization Problem", "text": "In 2.2 we formulate the visualization problem of collaborative data. In 2.3 we specify your distribution assumptions."}, {"heading": "2.1 Notation", "text": "Let U = {u1, u2,.., to} and G = {g1, g2,.., gn} be the sentences of all users or items. Without ambiguity, we use ui to refer to both the i-th user and the corresponding point in the embedded space for that user. We use the same notation for gj. We define \u03b4ij as 1 if ui is rated gj and 0 otherwise. Let rij be the rating of ui by gj if \u03b4ij = 1. Here, we normalize all ratings to the range [0, 1] (i.e. 1 is the highest rating, and 0 is the lowest). Let R = {rij | \u00e7ij = 1}. Further, let us call Gi = {gj | inspj = 1} and U j = {ui | inspj = 1}. Gi is the set of all items rated ui, and U j is the set of all users rated j."}, {"heading": "2.2 Formulation of the Problem", "text": "The visualization problem is to find an embedding of all points U and G in a Euclidean space, which we call the embedded space. Embedding should be one in which the distance between a user and an object is related to the corresponding evaluation. In the embedded space, we assume that each object is a random variable drawn independently of previous distributions Pu (ui) and Pg (gj). We introduce an evaluation function f: < + 0 7 \u2192 [0, 1] which sets the distance between two points (a user and an object) in the embedded space to a real value at [0, 1]: the expected evaluation for the two points.f (x) is a monotonically non-increasing function, f (0) = 1, and f (a user and an object) in the embedded space to a real value at [0, 1]: the expected evaluation for the two points."}, {"heading": "2.3 Gaussian Assumptions", "text": "We assume that all distributions originate from the Gaussian family. More precisely: Pu = N (0, \u0394u) Pg = N (0, \u0394g) Pf (rij | ui, gj) = N (f (\u0441ui \u2212 gj), \u03c3r). Note that these distributions are all normal, but the function f is nonlinear and therefore the resulting common distribution is not of Gaussian origin."}, {"heading": "3 Approach", "text": "It is difficult to calculate the buttocks directly in Equation 1. We use the Monte Carlo sample of the Markov chain."}, {"heading": "3.1 Metropolis-Hastings Algorithm", "text": "In particular, we use the Metropolis-Hastings (MH) algorithm (Metropolis et al., 1953), which has been extended to graphical models. In view of a graphical model using the random variables X = {x1, x2,..., xN}, we assume a target distribution \u03c0 over X. For each variable xi, there is a corresponding supply distribution Qxi, the distribution of new samples for this variable. In view of a current allocation to X, MH randomly selects a variable xi and tries to replace its value with a new sample x \u2032 i, which is drawn from the proposal Qxi. Let us leave Y = X \u2212 {xi}.The transition gain ratio for changing the sample xi to x \u2032 i is defined as T Q (xi x \u2032 i) = Qx \u2032 i (xi, x \u2032 i) Qxi (x \u2032 i) Qxi (Y, xi). (2) The probability of accepting this new sample is A (xi x \u2032 min = 1 \u2032 i, dependencies {x) to be set in Qxi (i)."}, {"heading": "3.2 Sampling Approach", "text": "In our visualization problem \u03c0 is the posterior distribution of U and G givenR (equation 1). First, we take Pu for each node and Pg for each node. Together, this forms a single initial sample (a common assignment to U and G) for our MCMC method. We use suggestion distributions Qui for the node ui and Qgj for the node gj. When we select the node ui, we extract u \u2032 i from Qui and then calculate the acceptance ratio for this change according to Equation 3. Based on the local independence properties, the transition gain ratio in relation to the rating function f (ui \u2032 g) is obtained."}, {"heading": "3.3 Simulated Annealing", "text": "The Metropolis-Hastings algorithm will provide us with common samples of U and G drawn from this rear part. To obtain the samples that maximize the rear end, we modify the standard MH algorithm along the lines of the simulated annealing algorithm (Kirkpatrick et al., 1983). In particular, we modify Equation 2 to add an annealing temperature, \u03b2: T Q (xi x \u2032 i) = Qx \u2032 i (xi) \u03c0 (Y, x \u2032 i) \u03b2Qxi (x \u2032 i) \u03c0 (Y, xi) \u03b2The transition gains of equations 4 and 5 are thenT Qf (ui u \u2032 i) = Qu \u2032 i (ui) = Qu \u2032 i (u \u2032 i) [Pu \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i \"i\" i. \"i\""}, {"heading": "3.4 Learn the Rating Function", "text": "Until now, we have assumed that the rating function f = 1 \u2212 K \u2212 1 is a problem if we want to adapt this function to the collaborative data. (However, it would be possible that the actual rating function could take a very different form.) Instead, we find that all collaborative datasets, of which we know a finite number of values for the rating values. (Many are binary (\"like\" or \"not like\") and others are based on a five- or ten-point scale. (Continuous, real rating values are rarely used.) We will let f be a step function with discrete quantization functions. We will discredit f f in K quantizations. (Let us) be in the series of K-splitting points in sorted order. (Given the amount of splitting points that the rating function isf (x) = 1 \u2212 K \u2212 K = 1,."}, {"heading": "3.5 Overview of the Full Algorithm", "text": "To sum up, the total algorithm of our approach is listed in algorithm 1. Parameters of this algorithm are ls (the number of samples used to estimate the expectation), lb (the number of samples required for the convergence of the MCMC process), (the amount by which \u03b2 can be increased), and the variances of the Gaussian distributions. In Section 4.2, we specify these sets for the experiments we have performed."}, {"heading": "4 Experiments and Results", "text": "We discuss our three datasets, our comparison method, and then compare our algorithm with three others."}, {"heading": "4.1 Experiment Datasets", "text": "The SAT dataset contains SAT II exam results for 40 questions selected from a study guide for historical questions and 296 users. SAT II is a standard exam taken by high school seniors applying to colleges in the United States. All results are either 0 or 1 (indicating whether the student answered the question correctly), and there are no missing scores. The 40 questions are from French, math, history, and biology. The exam was held online over a one-week period. The BGG dataset comes from www.boardgamegeek.com, which contains ratings from thousands of gamers and thousands of board games. Ratings range in semi-finalist increments from 0 to 10. We selected the 400 users and 80 games with the highest rating density. The rating matrix density, which we define as P i, j ijP i, j 1, is 63.4% for this subset. Our snapshot of the dataset is from the variety of the 4x datasets from January."}, {"heading": "4.2 Algorithm Initialization", "text": "Since we learn the evaluation function f, the absolute positions of the embedded points do not have a direct influence on our approach. Rather, the relative positions of each point play a role. Therefore, the entire scale of \u0442u and \u0442g does not affect the result. In particular, we specify the values \u043fu, \u0445g, \u03a3 \u2032 u and \u03a3 \u2032 g for all three sets of data for each identity matrix. Our informal tests show that the algorithm is not sensitive to these particular numbers and we have made no effort to do them.For the evaluation function f, we select an M step directly from samples taken from their priors. For our experiments, we specify ls = 2000, lb = 1000 and = 0.02 for all three sets of data."}, {"heading": "4.3 Implementation Issues", "text": "eiD rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc eid rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc if\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the r"}, {"heading": "4.4 Sample Results", "text": "The SAT data were chosen because of our ability to extract a \"basic truth.\" In particular, we expect the questions of the same topics to be grouped together when embedded. Figure 1 shows the embedding for dimension 2 using the simulated incandescent approach (along with the other three approaches).There are ten questions in each category. We can clearly see that our method ranks all French questions closely together. The same thing happens with the math questions. (There are eight math questions that overlap in a small range.) The other methods do not provide such narrow clues. Also, the history and biology questions are not grouped together. Further data analysis has shown that there is very little predictability in the history and biology questions, so this result is perhaps not surprise.11The French and mathematics questions tended to test a body of knowledge that is often retained as a coherent block, with the questions of history and biology leading up to this examination tending to mixing the points of the IDE with the points of the IDE and the IDE."}, {"heading": "4.5 Evaluation Criteria", "text": "In this thesis we present Kendall's tau (Kendall, 1955) as a suitable evaluation criterion. Kendall's tau is used to calculate the correlation in the order between two sequences X and Y. It is particularly useful to evaluate the correlation between two sequences, which can have many hints. In the face of two sequences X and Y of the same length, a pair (i, j), i 6 = j is called concordant if the order of Xi and Xj is the same as the order of Yi and Yj. In contrast, if the relative order X and Y of the same order is different, this pair (i, j) is called discordant. If Xi = Xj or Yj = Yj, then (i, j) is neither concordant nor discordant, and it is called an additional x-pair or additional y-pair. Kendall's tau is defined as cordant = C + D = 1. Couples are always X + C + Ey = EY pairs."}, {"heading": "4.6 Experimental Results", "text": "We have run our MCMC algorithm both with and without simulated annealing, with the exception of LLE, ET and CODE. We randomly selected a quarter of the users and a quarter of the test elements (U and G from above). We randomly selected other users and test elements to form a training set (U and G). All evaluations between members of G and U, G and U, G and U, G and U, and G and U, as well as G and U, are used for training. As mentioned above, the evaluations between members of G and U are used for testing. It is necessary to include the evaluations between G and U (and also between G and U) to link test users and test elements to the training elements and test elements. Since the existence of the test set is always missing in the assessment matrices used. We use linear regression to fill out these evaluations."}, {"heading": "4.7 Analysis of the Results", "text": "From the experiments above, we can see that when the evaluation matrix is denser, the embedding algorithm achieves better results. Our scanning method with (MCMC-SA) simulated annealing and without (MCMC) simulated annealing exceeds LLE, ET and CODE. None of them was designed for this type of data, so we do not present these results to reduce these methods, but no other methods were available to test them. Note that our MCMC linear regression-filled data algorithm (MCMC-REG) performs similarly to the direct application of our MCMC method to data with missing evaluations. This implies that it is not our regression that causes the bad results from the other algorithms, but rather their unsuitability for this problem. We would also note that our method appears more stable (smaller variance) than the other algorithms that have the ideal value for the SAT dataset."}, {"heading": "5 Conclusions", "text": "We formulated a new problem with the visualization of collaborative data. This is a potentially very useful problem. Not only are online databases with user ratings growing, but personal databases are also becoming more common. We expect the problem of collaborative visualization to be useful for organizing personal music or photography collections as well as for online shopping. We have not addressed the arithmetic problems or the stability of the resulting embedding in this work. Both are important problems for online use in changing databases. Due to the readily available nature of sampling methods and the ease of introducing restrictions, we hope that the solution presented here can be adapted to provide stable and adaptable solutions."}, {"heading": "Acknowledgments", "text": "We thank Titus Winters for collecting and distributing the SAT dataset. This work was partially supported by a grant from Intel Research and the UC MICRO program."}], "references": [{"title": "Empirical analysis of predictive algorithms for collaborative filtering", "author": ["J. Breese", "D. Heckerman", "C. Kadie"], "venue": "UAI \u201998", "citeRegEx": "Breese et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Breese et al\\.", "year": 1998}, {"title": "An emperical study of memory and information retrieval with a spatial user interface", "author": ["C. Chennawasin", "J. Cole", "C. Chen"], "venue": "21st Annual BCS-IRSG Colloquium on IR", "citeRegEx": "Chennawasin et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Chennawasin et al\\.", "year": 1999}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J. of the Royal Stat. Society B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Euclidean embedding of co-occurrence data", "author": ["A. Globerson", "G. Chechik", "F. Pereira", "N. Tishby"], "venue": null, "citeRegEx": "Globerson et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Globerson et al\\.", "year": 2005}, {"title": "Eigentaste: A constant time collaborative filtering algorithm", "author": ["K. Goldberg", "T. Roeder", "D. Gupta", "C. Perkins"], "venue": "Information Retrieval,", "citeRegEx": "Goldberg et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2001}, {"title": "Rank correlation methods", "author": ["M.G. Kendall"], "venue": null, "citeRegEx": "Kendall,? \\Q1955\\E", "shortCiteRegEx": "Kendall", "year": 1955}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "C.D. Gelatt", "M.P. Vecchi"], "venue": "Science,", "citeRegEx": "Kirkpatrick et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Kirkpatrick et al\\.", "year": 1983}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "Journal of Chemical Physics,", "citeRegEx": "Metropolis et al\\.,? \\Q1953\\E", "shortCiteRegEx": "Metropolis et al\\.", "year": 1953}, {"title": "Collaborative filtering by personality diagnosis: A hybrid memory- and model-based approach", "author": ["D. Pennock", "E. Horvitz", "S. Lawrence", "C.L. Giles"], "venue": "UAI \u201900", "citeRegEx": "Pennock et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Pennock et al\\.", "year": 2000}, {"title": "Think globally, fit locally: Unsupervised learning of low dimensional manifolds", "author": ["L.K. Saul", "S.T. Roweis"], "venue": null, "citeRegEx": "Saul and Roweis,? \\Q2003\\E", "shortCiteRegEx": "Saul and Roweis", "year": 2003}], "referenceMentions": [{"referenceID": 1, "context": "Spatial layouts have been shown in the past to increase interest in exploration and to aid in finding information (Chennawasin et al., 1999).", "startOffset": 114, "endOffset": 140}, {"referenceID": 0, "context": "Breese et al. (1998) classifies the approaches into two major categories: memory based and model based.", "startOffset": 0, "endOffset": 21}, {"referenceID": 8, "context": "Pennock et al. (2000) propose a collaborative filter based on personality diagnosis.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "CODE (Globerson et al., 2005) is one such recent example.", "startOffset": 5, "endOffset": 29}, {"referenceID": 7, "context": "In particular, we use the Metropolis-Hastings (MH) algorithm (Metropolis et al., 1953), which was extended to graphical models.", "startOffset": 61, "endOffset": 86}, {"referenceID": 6, "context": "To get the samples that maximize the posterior, we modify the standard MH algorithm along the lines of the simulated annealing (SA) algorithm (Kirkpatrick et al., 1983).", "startOffset": 142, "endOffset": 168}, {"referenceID": 2, "context": "We use expectation maximization (EM) algorithm (Dempster et al., 1977) to learn the rating function.", "startOffset": 47, "endOffset": 70}, {"referenceID": 4, "context": "We compare our results with Locally Linear Embedding (LLE) (Saul & Roweis, 2003), Eigentaste (ET) (Goldberg et al., 2001) and co-occurrence data embedding (CODE) (Globerson et al.", "startOffset": 98, "endOffset": 121}, {"referenceID": 3, "context": ", 2001) and co-occurrence data embedding (CODE) (Globerson et al., 2005).", "startOffset": 48, "endOffset": 72}, {"referenceID": 5, "context": "In this work, we introduce Kendall\u2019s tau (Kendall, 1955) as a suitable evaluation criterion.", "startOffset": 41, "endOffset": 56}], "year": 2006, "abstractText": "Collaborative data consist of ratings relating two distinct sets of objects: users and items. Much of the work with such data focuses on filtering: predicting unknown ratings for pairs of users and items. In this paper we focus on the problem of visualizing the information. Given all of the ratings, our task is to embed all of the users and items as points in the same Euclidean space. We would like to place users near items that they have rated (or would rate) high, and far away from those they would give low ratings. We pose this problem as a real-valued non-linear Bayesian network and employ Markov chain Monte Carlo and expectation maximization to find an embedding. We present a metric by which to judge the quality of a visualization and compare our results to Eigentaste, locally linear embedding and cooccurrence data embedding on three real-world datasets.", "creator": "TeX"}}}