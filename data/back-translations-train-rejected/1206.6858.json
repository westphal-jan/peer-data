{"id": "1206.6858", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Sequential Document Representations and Simplicial Curves", "abstract": "The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efficient, such a representation is unable to maintain any sequential information. We present a continuous and differentiable sequential document representation that goes beyond the bag of words assumption, and yet is efficient and effective. This representation employs smooth curves in the multinomial simplex to account for sequential information. We discuss the representation and its geometric properties and demonstrate its applicability for the task of text classification.", "histories": [["v1", "Wed, 27 Jun 2012 16:26:46 GMT  (1442kb)", "http://arxiv.org/abs/1206.6858v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["guy lebanon"], "accepted": false, "id": "1206.6858"}, "pdf": {"name": "1206.6858.pdf", "metadata": {"source": "CRF", "title": "Sequential Document Representations and Simplicial Curves", "authors": ["Guy Lebanon"], "emails": [], "sections": [{"heading": null, "text": "Popular word acceptance represents a document as a histogram of word occurrences. Although mathematically efficient, such representation is unable to maintain sequential information. We present a continuous and differentiated sequential document representation that goes beyond word acceptance, but is efficient and effective nonetheless. This representation uses smooth curves in the multinomic simplex to consider sequential information. Unlike n-Grammys, the new representation is capable of robustly modelling long-term sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability to the task of text classification."}, {"heading": "1 Introduction", "text": "The modeling of text documents is an essential part in a variety of applications such as classification, segmentation, visualization, and restoration of text. < 1 | A critical part of the modeling process is selecting an appropriate representation for the documents. The original representation of documents as a sequence of words is extremely high-dimensional, discrete, and brittle, making it an inconvenient representation for most models to edit. Instead, the word sequences are typically pre-processed by mapping to a low-dimensional lossy representation such as the n-gram on which the modeling is performed. The n-gram representation tracks the number of n consecutive words of different types, called n-grams. In the case n = 1, this amounts to ignoring the order of words in the document, preserving only the histogram of word events. This specific case, also known as the bag (arc)."}, {"heading": "2 The Multinomial Simplex and its Geometry", "text": "In this section we present a brief description of the multinomial simplex and its geometry. As the simplex is the space of the arc representations, its geometry is crucial for the representation of the lowbow. The short description uses some concepts from the Riemannian geometry. However, more information about the geometry of the simplex refer to (Amari & Nagaoka, 2000; Lafferty & Lebanon, 2005) A standard textbook on Riemannian geometry is (Spivak, 1975).The multinomial Simplex Pm for m > 0 is the dimensional subset of the Rm + 1 of all positive probabilities vectors or histograms over m + 1 objectPm = Rm + 1: i > 0, m + 1."}, {"heading": "3 Sequential Document Representations", "text": "Normally we consider documents above a vocabulary V = {1,.., | V |} as a finite sequence < y1,.., yN > of words represented in V as integers. However, we use the following broader definition for documents that will be more convenient later. Definition 1. A document x of length N is a function x: {1,..., N} \u00b7 V \u2192 [0, 1] such that | V | \u2211 j = 1x (i, j) = 1,."}, {"heading": "The set of documents (of all lengths) is denoted by X.", "text": "The default way to display a word sequence as a document in X is that each place contains the appropriate single word with constant weight, which corresponds to the \u03b4c representation defined below with c = 0.Definition 2. The default representation \u03b4c (y) \u0445 X, where c \u2265 0 of a word sequence y = < y1,.., yN > is\u03b4c (y, j) = {c 1 + c | V | yi 6 = j1 + c 1 + c | V | yi = j.The above is a legitimate representation, since \u2211 j \u03b4c (y) (i, j) = 1 + c | 1 + c | V | = 1. The parameter c in the above definition is useful to avoid zero counts in the obtained local histograms."}, {"heading": "The set of length-normalized documents is denoted X\u2217.", "text": "Pre-conversion procedure for the x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}, {"heading": "Proof.", "text": "Theorem 3 is expressed in relation to the Euclidean distance on the simplex, but it is easy to extend it to the case of the geodesic distance d (\u03b3\u00b5 (y), the geoclidean distance gen (y), since this is a smooth function of the Euclidean distance."}, {"heading": "4 Modeling of Simplicial Curves", "text": "A lowbow representation is equivalent to a parametrized curve in the simplex. As such, it is a point in an infinite product of Simplices P [0,1] m, which is of course equipped with the product topology and geometry of each simplex. In practice, it is difficult to maintain a continuous representation, and the simple solution is to find the path at representative points t1,. The discussion focuses on continuous lowbow curves, but the discrete case resulting from a discrete lowbow representation corresponding to a point in the finite dimensional product space can be Plm. Below, we describe geometric properties of the lowbow and their use in modeling. The discussion focuses on continuous lowbow curve cases, but the discrete case resulting from a finite lowbow representation can be achieved by substituting integrals with sums. The distance between lowbow representations of two word representations (1), (\u03b3) and (1) coordinate the corresponding time sequence is (1)."}, {"heading": "5 Text Classification Experiments", "text": "In this section, we examine lowbow and its properties in the context of text classification \u2192 using a next neighbor classifier. We report on experimental results for the WebKB faculty vs. course assignment and the Reuters top ten categories using the standard mod-apte split. In the WebKB assignment, we have repeatedly sampled subsets for training and testing with equal positive and negative examples. In the Reuters assignment, we show randomly sampled subsets of the mod-apte split for training (to investigate the influence of the train set size), resulting in unbalanced tensile and test sets with more negative than positive examples. Continuous quantities in the lowbow calculation were approximated by a discrete sample of 5 equally graded points in the interval. [0, 1] Rotating the integrals into efficiently calculated Riemann sums. The core used was the limited Gaussian core (5). During the experiments, we calculated several alternatives for the kernel and chose the best parameters."}, {"heading": "6 Related Work and Conclusion", "text": "The use of n-grams and arcs in document modeling has a long history; a geometric point of view that takes into account the representation of the arc as a point in the multinomial simplex (Amari & Nagaoka, 2000) is expressed in (Lafferty & Lebanon, 2005), and an up-to-date overview of the geometric properties of probability spaces (Amari & Nagaoka, 2000); the use of simplified curves in text modeling is a relatively new approach promoted by Gous (1998) and Hall and Hofmann ().? (?) describes some related ideas that lead to a non-smooth multi-scale view of images; the representation of the low arc is a promising new direction in text modeling; by varying between the standard word sequence representation < y1,.., yN > and the arc. It corresponds to a smooth curve that makes techniques of real analysis and geometry accessible; by varying between the standard word sequence representation < y1,.., yN > and the arc."}], "references": [{"title": "Methods of information geometry. American Mathematical Society", "author": ["S. Amari", "H. Nagaoka"], "venue": null, "citeRegEx": "Amari and Nagaoka,? \\Q2000\\E", "shortCiteRegEx": "Amari and Nagaoka", "year": 2000}, {"title": "Statistical decision rules and optimal inference. American Mathematical Society", "author": ["N.N. \u010cencov"], "venue": null, "citeRegEx": "\u010cencov,? \\Q1982\\E", "shortCiteRegEx": "\u010cencov", "year": 1982}, {"title": "Exponential and spherical subfamily models", "author": ["A. Gous"], "venue": "Doctoral dissertation,", "citeRegEx": "Gous,? \\Q1998\\E", "shortCiteRegEx": "Gous", "year": 1998}, {"title": "Diffusion kernels on statistical manifolds", "author": ["J. Lafferty", "G. Lebanon"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Lafferty and Lebanon,? \\Q2005\\E", "shortCiteRegEx": "Lafferty and Lebanon", "year": 2005}, {"title": "A comprehensive introduction to differential geometry, vol. 1-5", "author": ["M. Spivak"], "venue": "Publish or Perish", "citeRegEx": "Spivak,? \\Q1975\\E", "shortCiteRegEx": "Spivak", "year": 1975}], "referenceMentions": [{"referenceID": 4, "context": "A standard textbook on Riemannian geometry is (Spivak, 1975).", "startOffset": 46, "endOffset": 60}, {"referenceID": 1, "context": "Fortunately, a particular choice of a Riemannian metric is motivated by \u010cencov (1982) who proved that the Figure 1: The 2-simplex P2 may be visualized as a surface in R (left) or as a triangle in R (right).", "startOffset": 72, "endOffset": 86}, {"referenceID": 3, "context": "The distance (7) may also be used to construct the approximated heat kernel for use in SVM classification and regression as described by Lafferty and Lebanon (2005). The lowbow distance may also be used to construct generative models for text that generalizes the naive Bayes or multinomial model.", "startOffset": 137, "endOffset": 165}], "year": 2006, "abstractText": "The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efficient, such a representation is unable to maintain any sequential information. We present a continuous and differentiable sequential document representation that goes beyond the bag of words assumption, and yet is efficient and effective. This representation employs smooth curves in the multinomial simplex to account for sequential information. In contrast to n-grams the new representation is able to robustly model long rage sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for the task of text classification.", "creator": "dvips(k) 5.94a Copyright 2003 Radical Eye Software"}}}