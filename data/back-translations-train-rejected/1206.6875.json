{"id": "1206.6875", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "A simple approach for finding the globally optimal Bayesian network structure", "abstract": "We study the problem of learning the best Bayesian network structure with respect to a decomposable score such as BDe, BIC or AIC. This problem is known to be NP-hard, which means that solving it becomes quickly infeasible as the number of variables increases. Nevertheless, in this paper we show that it is possible to learn the best Bayesian network structure with over 30 variables, which covers many practically interesting cases. Our algorithm is less complicated and more efficient than the techniques presented earlier. It can be easily parallelized, and offers a possibility for efficient exploration of the best networks consistent with different variable orderings. In the experimental part of the paper we compare the performance of the algorithm to the previous state-of-the-art algorithm. Free source-code and an online-demo can be found at", "histories": [["v1", "Wed, 27 Jun 2012 16:30:42 GMT  (220kb)", "http://arxiv.org/abs/1206.6875v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["tomi silander", "petri myllymaki"], "accepted": false, "id": "1206.6875"}, "pdf": {"name": "1206.6875.pdf", "metadata": {"source": "CRF", "title": "A Simple Approach for Finding the Globally Optimal Bayesian Network Structure", "authors": ["Tomi Silander"], "emails": [], "sections": [{"heading": null, "text": "We study the problem of learning the best Bayesian network structure in terms of a dismantable score such as BDe, BIC, or AIC. This problem is known as NP-hard, which means that its solution quickly becomes unfeasible as the number of variables increases. Nevertheless, in this paper we show that it is possible to learn the best Bayesian network structure with over 30 variables, covering many cases of practical interest. Our algorithm is less complicated and more efficient than the previously presented techniques. It is easy to parallelise and provides an opportunity to efficiently explore the best networks consistent with different variable orders. In the experimental part of the paper, we compare the performance of the algorithm with the previous state-of-the-art algorithm. Free source code and an online demo can be found at http: / / b-course.hiit.fi / bene."}, {"heading": "1 INTRODUCTION", "text": "Inspired by Koivisto & Sood (2004), we strive for an accurate understanding of network structures. In their beautiful generality, the original work focuses on the calculation of the probability of any modular feature in Bayesian networks and on indications of the actual discovery of general theory. Much of the theory and experimentation in Koivisto & Sood (2004) also deals with cases that are subject to a certain upper limit on the number of parents in Bayesian networks. In this work, we provide a much simpler algorithm for searching for the globally optimal network structure without structural constraints."}, {"heading": "2 DEFINITIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 BAYESIAN NETWORKS", "text": "Bayean network structures (see e.g. (Heckerman, 1996) or simply networks, because a variable set V = {V1,..., Vn} are directional acyclic graphs (DAGs) with exactly one node per variable. Often, we set V by the indexes of its variables V = {1,.., n}. A network G can be described as a vector G = (G1,..., Gn) of superior sentences: Gi is the subset of V from which there are arcs to Vi. For example, the network G = ({4}, {1, 3}, {1}, corresponds to the DAG with arcs {4 \u2192 1, 2, 3 \u2192 2, 1 \u2192 3}. We will also use a concept of the variable order to Vi. The order of the variable V is simply the order of V in any order, e.g. an order AG."}, {"heading": "2.2 DATA", "text": "In this paper, we focus on multivariate discrete data, where we have many variables V = {V1,..., Vn}, and each variable Vi has a (usually very) finite number of values (vi1,..., vini); the variables are measured on a nominal scale, i.e. the arrangement of values is irrelevant; we often refer to the values on the basis of their indices, so that the values of Vi are considered as (1,.., ni); the complete discrete data D = (D1,.,., DN) is a collection of N data vectors Dj = (Dj1,., Djn) in which the component Dji has one of the values of Vi. It is customary to view a data vector as a series in anN \u00d7 n data matrix in which the columns correspond to the variables W."}, {"heading": "2.3 SCORES", "text": "Our goal is to find the best Bayesian network structure for the given complete (no missing values), discrete data. We usually formalize the definition of the \"best\" by defining a scoring function that, given the data, binds a real number to a particular network. We consider the problem to be a maximization problem: the better the network, the greater the score. The nature of many popular scoring functions is such that several different networks can have the same scores (Chickering, 1995). Therefore, we often write \"one best\" instead of \"the best.\" During the process of finding the best network structure, it does not matter which one is selected, the end result will still be a best network. It is worth noting that on many popular scores we can find a best network, other best networks easily find. In order to use the method to be presented, the Vick scoring functions must be, i.e. the scoring functions must be yedular, i.e. the data must be Bayesian = G."}, {"heading": "3 SIMPLE METHOD FOR FINDING BEST NETWORK STRUCTURES", "text": "Our method consists of five logical steps: 1. Calculate the local values for all n2n \u2212 1 different (variable, variable set) pairs; 2. Use the local values to find the best parents for all n2n \u2212 1 (variable, parent candidate) pairs; 3. Find the best sink for all 2n variable sectors; 4. Use the results from step 3 to find the best order of variables; 5. Find the best network based on the results computed in steps 2 and 4 of the algorithm. (Step 3 of the algorithm is based on the following observation: The best network G (W) must have a sink, and that sink must have incoming arcs from its best possible parents. (W\\ {s) The rest of the nodes and the arcs must form the best possible network."}, {"heading": "3.1 CALCULATING LOCAL SCORES", "text": "It is also the most time-consuming phase of the algorithm. We start by calculating the contingent table for all variables V and calculating incrementally (degressively) contingent cell tables for smaller variables (Figure 1). From each contingent table we calculate contingent frequency tables for the variables that appear in the contingent table (Figure 2). These contingent frequency tables can then be used to calculate the local scores. The main method, GetLocalScores, is called with a contingent table and the variables are evars marginalized."}, {"heading": "3.2 FINDING BEST PARENTS", "text": "The best parents in C for v are either the entire candidate himself or the best parents for v from one of the smaller candidate sets {C\\ {c} | c \u00b2 C}. More formally, this means that Scorei (g * i (C) = max (scorei (C), score1 (C))), (4) where core1 (C) = max c \u00b2 Cscorei (g \u00b2 i (C))), translates directly into an algorithm that runs through all candidate sets in lexicographic order and evaluates the above formula for each of them. Below, you will find the pseudo code of the algorithm 2, which runs the variable v and the previously calculated local scores cs [s] [s] [bss] bps [bps] for all possible parent candidate scores of v.Algorithm 2 GetBestParents (V, v > cs] if the variable v and the previously calculated local scores of cs [s] s [s], i.e. cs = 1 cores ss, and the best scores of all possible parent candidates, v.m]."}, {"heading": "3.3 FINDING BEST SINKS", "text": "As already mentioned, a best network for a W variable set can always be constructed by first finding the best sinks in W, then constructing the best network for W\\ {s} and finally selecting the best parents for s from the variables in W\\ {s}. Algorithm 3 GetBestSinks (V, bps, LS) for all W'V in lexicographic order doscores [W] \u2190 0.0 sinks [W] \u2190 \u2212 1 for all sinks, W doupvars \u2190 W\\ {sink} skore \u2190 Score \u2190 Skore + LS [sink] [bps [sink] [upvars] [upvars]] if sinks [W] = \u2212 1 or skore > Score > Score [W] thenscores [W] skore sinks [W] \u2190 sinks if end forend for return sinksAgain, this idea translates directly into an algorithm (algorithm 3)."}, {"heading": "3.4 FINDING BEST ORDERING", "text": "The method to find the best order is simply a non-recursive implementation of Equation (2). Algorithm 4 Sinks2ord (V, Sink) ord = array 1 to | V | of variables left = V for i = | V | to 1 Doord [i] \u2190 Sink [left] left \u2190 left\\ {ord [i]} end for return ord"}, {"heading": "3.5 FINDING BEST NETWORK", "text": "After calculating the best parents, bps, for all (variable, parental candidate pairs) pairs, as explained in Section 3.2, we can use Equation (3) to quickly find the best network consistent with a given order of variables: Algorithm 5 Ord2net (V, ord, bps) Parents = Array 1 to | V | of variable sets Previous Previous Previous Previous Previous Previous"}, {"heading": "4 EXPERIMENTS", "text": "To demonstrate the effectiveness of our method, we selected 30 publicly available data sets and conducted experiments that would have been difficult to perform without our software. Since the main purpose of these experiments was merely to illustrate what kind of problems can be investigated with this software, more elaborate empirical studies were left than future work."}, {"heading": "4.1 STUDYING MAXIMAL IN-DEGREES", "text": "Searching for the best network would be feasible for a much larger number of variables if we could set an upper limit on the maximum number of parents (in degrees) that each node can have. To investigate the problem, we used our 30 data sets. For each data set, we first constructed a best net using the BDe score (with equivalent sample size 1.0) and a best net using the BIC score. The maximum in degrees (D1 for BDe and D2 for BIC) are listed in Table 1. We also list the number of variables n, the number of data vectors N and runtimes T1 and T2 that are required to find the best networks using BDe and BIC scores, respectively. All experiments were run on a 2.20 Mhz Compaq Evo N800w laptop with 1 GB of memory and Linux 2.6. We note that while the maximum degree is normally low, this is not always the case."}, {"heading": "4.2 SPEED COMPARISON", "text": "We also compared the runtime (Tus) of our algorithm with that of the modern SM algorithm (TSM). We were able to use four identical datasets as Singh & Moore (2005).The results are shown in Table 2.Although some of the differences are due to the different computers used and possibly some other implementation details, the results clearly show that our method is competitive.The comparison also confirmed that our algorithm and its implementation are likely correct, as the BDe values obtained for these datasets were exactly the same.The faster time for adult data in this table compared to Table 1 is due to the fact that in previous runs we used double floating-point numbers for values just in case BIC needed them while we used single precise floating-point numbers in this run."}, {"heading": "4.3 STUDYING THE ROLE OF ESS", "text": "As a foretaste of a more meaningful study, we learned the best network for the yeast data (9 variables, 1484 data vectors) with different values of the corresponding sample size (ESS).The results in Figure 3 show that it is possible to get any number of arcs to your \"globally optimal\" network structure by going from very small values of ESS (2e-20) to very large values of ESS (34000).For this dataset, the network learned with BIC has 6 arcs, which corresponds to the ESS value of about 0.02.The results clearly suggest further studies on the robustness of the MAP structure."}, {"heading": "4.4 STUDYING PREDICTION", "text": "As an example of the ability to study the predictive behavior of the score to maximize the network structures, we first learned a best network for the shuttle data (10 variables, 58000 data vectors) based on the BDe score with ESS of 1.0. Then we used this network with its expected parameter values to generate 100 training samples with sample sizes between 10 and 50000, and tested the predictive power of the best network structures predicted by the best network structures with integrated parameters (again using the ESS of 1.0) and the BIC Score. Predictive power was also used as the reference value of the network structure of the data generation model. Unsurprisingly, the results (Figure 4) show that the sample structure of the model generating the small IC is too conservative for the sample size."}, {"heading": "4.5 BUILDING A BIG NET", "text": "In order to demonstrate the feasibility of our approach beyond the n = 25 limit, the maximum feasibility specified for the SM algorithm, we have built a network of 29 variables (Figure 5).The data used in this experiment contains information about 194 nations and their flags. These data were selected based on their free availability and the appropriate number of variables.The flag data can be distributed at http: / / www.ailab.si / orange / datasets.asp.We started by splitting the calculation of local values into 1025 smaller jobs and distributing these jobs on 4 dual-core computers (dual-3GHz Intel Xeon, 512KB L2 cache, 4GB RAM).The calculation of local values took 6 hours and 16 minutes. Step 2, the search for the best parent, took 3 hours, but this task could be distributed on 4 dual-core computers (dual-3GHz Intel Xeon, 512KB L2 cache, 4GB RAM), so the search for the best parent took only 4 minutes, and the search for the best parent took 4 minutes each, but this task could be completed on 4 dual-core computers (dual-3GHz Intel Xeon, 512KB L2 cache, 4GB cache)."}, {"heading": "4.6 STUDY OF ORDERINGS", "text": "The question of the plausibility of different orders often arises when we want to speculate on the causal relationships between the variables. Therefore, in the last experiment, we used the ability to quickly find the best network for each arrangement of variables. We selected an optimal arrangement of 29 variables constructed in the previous experiment and evaluated the results of the best networks that match the arrangement close to the optimal arrangement. First, we looked at all 28 possible rotations, 14 left and 14 right, of the optimal arrangement. The resulting values of the best networks are in Figure 6. We can clearly see the tendency that the more we rotate the arrangement, the worse the networks get. The very first right rotation lowers the value by 68, according to the probability ratio of 1030. We also examined 29 x 28 / 2 = 392 possible swaps of two variables in the optimal arrangement. The results are presented in Figure 7. Dark colors indicate bad swaps. It is easy to see the optimal proximity of the swaps as often results from each other in the optimal arrangement of the variables."}, {"heading": "5 DISCUSSION AND FUTURE WORK", "text": "We have presented a simple way to find the best Bayesian networks and demonstrate their feasibility. We believe that our method is state-of-the-art. Due to the nature of the problem, but also the method, this approach will not increase indefinitely. In the current implementation, the memory requirement (2n + 2 bytes) may be the first factor limiting the size of the networks that can be constructed. Currently, n = 32 is the practical upper limit, as it requires over 16GB of memory. Also, the memory requirement (12n2n \u2212 1 bytes) can be restrictive. Distributing the calculation makes the process faster, but in this case one should make sure that disk access is efficient in a distributed environment. However, this is hardly the end of the road. Memory-intensive steps 2 and 3 have a very regular memory access pattern, and it is actually possible to get rid of the storage requirements and turn them into a (larger) disk requirement."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the Finnish Academy of Sciences under the Prose and Civi projects, the Finnish Promotion Agency for Technology and Innovation under the PMMA and SIB projects, and the European Community IST Programme under the PASCAL Network of Excellence, IST-2002-506778."}], "references": [], "referenceMentions": [], "year": 2006, "abstractText": "We study the problem of learning the best Bayesian network structure with respect to a decomposable score such as BDe, BIC or AIC. This problem is known to be NP-hard, which means that solving it becomes quickly infeasible as the number of variables increases. Nevertheless, in this paper we show that it is possible to learn the best Bayesian network structure with over 30 variables, which covers many practically interesting cases. Our algorithm is less complicated and more efficient than the techniques presented earlier. It can be easily parallelized, and offers a possibility for efficient exploration of the best networks consistent with different variable orderings. In the experimental part of the paper we compare the performance of the algorithm to the previous state-of-the-art algorithm. Free source-code and an online-demo can be found at http://b-course.hiit.fi/bene.", "creator": "TeX"}}}