{"id": "1206.6879", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2012", "title": "Practical Linear Value-approximation Techniques for First-order MDPs", "abstract": "Recent work on approximate linear programming (ALP) techniques for first-order Markov Decision Processes (FOMDPs) represents the value function linearly w.r.t. a set of first-order basis functions and uses linear programming techniques to determine suitable weights. This approach offers the advantage that it does not require simplification of the first-order value function, and allows one to solve FOMDPs independent of a specific domain instantiation. In this paper, we address several questions to enhance the applicability of this work: (1) Can we extend the first-order ALP framework to approximate policy iteration to address performance deficiencies of previous approaches? (2) Can we automatically generate basis functions and evaluate their impact on value function quality? (3) How can we decompose intractable problems with universally quantified rewards into tractable subproblems? We propose answers to these questions along with a number of novel optimizations and provide a comparative empirical evaluation on logistics problems from the ICAPS 2004 Probabilistic Planning Competition.", "histories": [["v1", "Wed, 27 Jun 2012 16:31:33 GMT  (157kb)", "http://arxiv.org/abs/1206.6879v1", "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)"]], "COMMENTS": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence (UAI2006)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["scott sanner", "craig boutilier"], "accepted": false, "id": "1206.6879"}, "pdf": {"name": "1206.6879.pdf", "metadata": {"source": "CRF", "title": "Practical Linear Value-approximation Techniques for First-order MDPs", "authors": ["Scott Sanner", "Craig Boutilier"], "emails": ["ssanner@cs.toronto.edu", "cebly@cs.toronto.edu"], "sections": [{"heading": null, "text": "Recent work on Approximate Linear Programming Techniques (ALP) for Markov First Order Decision Processes (FOMDPs) presents the value function linear and uses linear programming techniques to determine appropriate weights. This approach has the advantage that it does not require a simplification of the first order value function and allows FOMDPs to be solved independently of a specific domain instantiation. In this paper, we address several questions to improve the applicability of this work: (1) Can we expand the first order ALP framework to approximate political iteration, and if so, how can these two algorithms be compared? (2) Can we automatically generate base functions and assess their impact on value functionality quality? (3) How can we break down insoluble problems with universally quantified rewards into tractable sub-problems? We propose answers to these questions along with a new set of optimizations and provide an empirical evaluation of the ICPS in 2004."}, {"heading": "1 Introduction", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2 Markov Decision Processes", "text": "First, we check the linear alignment of the MDPs."}, {"heading": "2.1 MDP Representation", "text": "An MDP consists of: a finite statespace S; a finite set of actions A; a transitional function T, where T (s, a, \u00b7) has a distribution over S for all s, a) A; and a reward function R: S \u00b7 A \u2192 R. Our goal is to find a policy that maximizes the value function, defined using the infinite horizons, discounted reward criterion: V \u03c0 (s) = E\u03c0 [p] (s) = 0 \u03b3 t \u00b7 rt | s], where rt is a reward that is at the given time t and 0 \u2264 \u03b3 < 1 the discount factor is p). For each function V over S and politics \u03c0 we define the backup operator B\u03c0 (s) (s) (s) (s): (s) T (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p (s), p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p"}, {"heading": "2.2 Linear Value Approximation for MDPs", "text": "In a linear value function, we represent V as a linear combination of the k base functions bj: V (s) = 1 wjbj (s).Our goal is to find weights that come as close as possible to the optimal value function. Since both backup operators are B\u03c0 and Ba linear operators, securing a linear combination of basic functions is only the linear combination of thebackups of the individual basic functions. Adequate linear programming: One way to find a good linear approximation is to throw the optimization problem (s) as a linear program that directly solves the weights of an L1 minimizing value function. [6]: Variables: w1,.., wkMinimize: a good linear approximation is to throw the optimization problem (s) as a linear program that directly solves the weights of an L1 minimizing value function."}, {"heading": "3 First-Order MDPs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The Situation Calculus", "text": "The situation Calculus [19] is a language of the first order for the axiomatization of dynamic worlds. Its basic components consist of actions, situations and fluids. Actions are terms of the first order, which contain action function symbols. For example, the action of a truck t from the city c1 to the city c2 mightbe is designated by the action term drive (t, c1, c2). A situation is a term of the first order, which denotes the occurrence of an action sequence. These are represented by a binary function symbol do: do (\u03b1, s) denotes the situation resulting from the action \u03b1 in the situation. In a logistics area, the situation term do (drive (t, c2, c3), do (drive (c1, c2), S0)) denotes the situation resulting from the execution of the sequence [drive (t, c1, c2, c3)."}, {"heading": "3.2 Case Representation and Operators", "text": "Before generalizing the situation to allow for a primary representation of MDPs, we introduce a case notation to enable primary specifications of rewards, probabilities and values required for FOMDPs (see [5, 21] for formal details): t = \u03c61: t1::: \u03c6n: tn \u2261 i \u2264 n {\u03c6i \u0445 t = ti} Here are \u03c6i state formulas (whose situation does not use term) and ti are concepts. Frequently the ti will be constants and the \u03c6i will divide the state space. For example, by using Dst (t, c) to specify the destination of the truck t is the city c, we can use our reward function rCase (s, a) as: rCase (s, a) = a = noop."}, {"heading": "3.3 Stochastic Actions and the Situation Calculus", "text": "To generalize the classical situation, we calculate stochastic actions required by FOMDPs and divide stochastic \"agent\" actions into a collection of deterministic actions, each corresponding to a possible outcome of the stochastic action. We then determine a distribution according to which \"nature\" can select a deterministic action from this sentence whenever this stochastic action is performed. As a consequence, we only have to formulate SSAs using deterministic nature decisions [1, 5] and thus the need for special treatment of stochastic actions in SSAs.Letting A (~ x) is a stochastic act with the decisions of nature (i.e., deterministic actions) n1 (~ x), \u00b7 nk (~ x), we represent the distribution over ni (~ x) given A (~ x) with the notation pase (~ x), A (cdriase), A (~ x), cdriase (cdriase), cdriase ()."}, {"heading": "3.4 Symbolic Dynamic Programming", "text": "Saving a value function vCase (s) by an action A (~ x) yields a case statement (examples) that contains the logical description of states that would lead to vCase (s) after the execution of action A (~ x), as well as the values obtained (i.e., a Q (s, a) function in classic MDPs. In fact, there are three types of backups we can perform. The first, BA (~ x), regresses a value function by an action and creates a case statement with free variables for the action parameters. The second, BA, existentially quantifies via the free variables ~ x in BA (~ x). The third, BAmax applies the maximum operator to BA, which results in a case description of the regressed value function that indicates the best value that could be achieved by executing an instantiation of A (~ x) in the state before the action. To define the backup operators, we first define a FOJ vase (FOJ) FOJ (FOJ) FOR (FOJ) FOJ = FOX (FOJ) decision theory."}, {"heading": "4 Linear Value Approximation for FOMDPs", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Value Function Representation", "text": "Following [21], we present a value function as a weighted sum k Basic functions of the first order, referred to as bCase i (s), each containing a small number of formulas that provide an abstraction of the first order state space: vCase (s) = 1 wi \u00b7 bCasei (s) (7). With this format, we can often achieve a reasonable approximation of the exact value function by often compiling the 3SSAs from \"effect axioms,\" specifying action effects [19] and effect axioms that can be compiled from the PPDDL probability planning domain specifications [25]. Additive structure inherent in many real-world problems (e.g. additive reward functions or problems with independent partial targets), as opposed to exact solution methods, where value functions can grow exponentially in size during the solution process and effect compounds (PDDL probability specification) must be calculated."}, {"heading": "4.2 First-order Approximate Linear Programming", "text": "Now we have all the building blocks needed to define approximate first-order linear programming (FOALP) and approximate iteration of first-order policy (FOAPI), for now we are simply focusing on the definitions of the algorithm; we will deal with efficient implementation in a subsequent paragraph. FOALP was introduced by Sanner and Boutilier [21]. Here we present a linear program (LP) with the first-order limitations that generalize Eq. 1 from MDPs to FOMDPs: Variables: wi; i \u2264 kMinimize: k = 1wi \u0445 < \u03c6j, tj > bCaseitj conject: 0 = 1 wi \u00b7 bCasei (s)."}, {"heading": "4.3 First-order Approximate Policy Iteration", "text": "We now turn to the first post of this paper. < p > p > p > p > p > p > p (Eq. 1) to FOMDPs.p & p; p > p > p > p > p > p > p (Eq. 1) to FOMDP.p & p (Eq. 1) to FOMDP.p & p > p > p > p > p > p > p > p > p > p (Eq. 1) to FOMDP.p (Eq. 1) to FOMDP.p & p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p & p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p & p > p > p > p > p > p > p > p > p & p > p > p > p > p > p > p > p > p > p > p > p > p > p & p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p & p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p > p"}, {"heading": "5 Greedy Basis Function Generation", "text": "The use of linear approximate values requires a good set of basic functions that encompass a space that contains a good approximation of the value function. While some work addresses the problem of generating basic functions [18, 16], none has been applied to RMDPs or FOMDPs. We are considering a method for generating basic functions based on the work of Gretton and Thiebaux [9] who use inductive logic (ILP) techniques to construct a value function from accumulated experience. Specifically, they use regressions of reward as building blocks for the ILP-based construction of the value function. This technique has enabled them to generate complete or k-stage-to-go optimal guidelines for a number of block-world problems."}, {"heading": "6 Handling Universal Rewards", "text": "In the first episode of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of the series \"The World in Change of Time\" of Time \"The World in Change of Time\" of the series \"The World in Change of Time\" The World in Change of Time \"of the series\" The World in Change of Time \"The World in Change of Time\" of the series \"The World in Change of Time\" The World in Change of Time \"of the series\" The World in Change of Time \"The World in Change of Time\" of Time \"of the series\" The World in Change of the series \"The World in Change of the series\" The World in Change of the time \"The World in Change of Time\" of the series \"The World in Change of the series\" The World in Time \"The World in Change of the series\" The World in Change of the series \"The World in Time\" The World in Change of the time \"The World in Change of the series\" The World in Change of the series \"The World in Time\" The World in Change of the series \"The World in Change of the series\" The World in Change of the series \"The World in Change of the series\" of the series \"The World in Change of Time\" of the series \"The World in the series\" The World in Change of the series \"The World in Change of the series\" The World"}, {"heading": "7 Optimizations", "text": "Below are some new additional optimizations that provided significant performance improvements to our FOALP and FOAPI implementations. Firstly, and most importantly, one just has to consider taking a real partition (each base function has some very nice computing properties that we can take advantage of) and setting the other n-1 base functions to their wrong partition. Obviously, any other setting would lead to an inconsistent state due to the disjointness of the n base functions. Consequently, searching for a consistent state reduces the exponential complexity of the 2n combinations to a polynomial complexity of the n combinations (attempting any real partition of a base function)."}, {"heading": "8 Empirical Results", "text": "This year it is more than ever before."}, {"heading": "9 Concluding Remarks", "text": "We have introduced a novel algorithm for performing firststorder approximate policy iteration, as well as new basic function generation techniques that enable FOALP and FOAPI to use their structure efficiently, resulting in a significant increase in the number of basic functions that these algorithms can handle. In addition, we have addressed the intractability of solving problems with universal rewards by automatically splitting the task into independent sub-targets that can be solved and then recombined to determine a policy that facilitates \"coordination\" between the sub-targets. Taken together, these techniques have enabled us to evaluate FOALP and FOAPI solutions to logistics problems from the ICAPS 2004 Probabilistic Planning Competition. Empirically, we have shown that FOALP performs better than other autonomous stochastic planners on these issues and surpasses the FOAPI solution when the POL hypothesis is currently being optimized due to additional memory limitations we are working on this problem."}], "references": [{"title": "Reasoning about noisy sensors in the situation calculus", "author": ["Fahiem Bacchus", "Joseph Y. Halpern", "Hector J. Levesque"], "venue": "In IJCAI-95,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1995}, {"title": "mGPT: A probabilistic planner based on heuristic search", "author": ["Blai Bonet", "Hector Geffner"], "venue": "In Online Proceedings for The Probablistic Planning Track of IPC-04: http://www.cs.rutgers.edu/ \u0303mlittman/topics/ipc04pt/proceedings/,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Prioritized goal decomposition of Markov decision processes: Toward a synthesis of classical and decision theoretic planning", "author": ["Craig Boutilier", "Ronen I. Brafman", "Christopher Geib"], "venue": "In IJCAI-97,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Decisiontheoretic planning: Structural assumptions and computational leverage", "author": ["Craig Boutilier", "Thomas Dean", "Steve Hanks"], "venue": "JAIR, 11:1\u201394,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Symbolic dynamic programming for first-order MDPs", "author": ["Craig Boutilier", "Ray Reiter", "Bob Price"], "venue": "In IJCAI-01,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "The linear programming approach to approximate dynamic programming", "author": ["DP de Farias", "Ben Van Roy"], "venue": "Operations Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Approximate policy iteration with a policy language bias", "author": ["Alan Fern", "SungWook Yoon", "Robert Givan"], "venue": "In NIPS-", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Thiebaux. NMRDPP: Decision-theoretic planning with control knowledge", "author": ["Charles Gretton", "David Price", "Sylvie"], "venue": "In Online Proceedings for The Probablistic Planning Track of IPC-04: http://www.cs.rutgers.edu/ \u0303mlittman/topics/ipc04pt/proceedings/,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Exploiting first-order regression in inductive policy selection", "author": ["Charles Gretton", "Sylvie Thiebaux"], "venue": "In UAI-04,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Generalizing plans to new environments in relational MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Chris Gearhart", "Neal Kanodia"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Efficient solution methods for factored MDPs", "author": ["Carlos Guestrin", "Daphne Koller", "Ronald Parr", "Shobha Venktaraman"], "venue": "JAIR, 19:399\u2013468,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "A logic-based approach to dynamic programming", "author": ["Steffen H\u00f6lldobler", "Olga Skvortsova"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "A heuristic search algorithm for solving first-order MDPs", "author": ["Eldar Karabaev", "Olga Skvortsova"], "venue": "In UAI-2005,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Bellman goes relational", "author": ["Kristian Kersting", "Martijn van Otterlo", "Luc de Raedt"], "venue": "In ICML-04. ACM Press,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Introduction to the probabilistic planning track", "author": ["Michael L. Littman", "Hakan L.S. Younes"], "venue": "In Online Proceedings for The Probablistic Planning Track of IPC-04: http://www.cs.rutgers.edu/ \u0303mlittman/topics/ipc04pt/proceedings/,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Samuel meets Amarel: Automating value function approximation using global state space analysis", "author": ["Sridhar Mahadevan"], "venue": "In AAAI-05,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Solving very large weakly coupled Markov decision processes", "author": ["Nicolas Meuleau", "Milos Hauskrecht", "Kee-Eung Kim", "Leonid Peshkin", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "venue": "In AAAI-98,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Piecewise linear value function approximation for factored MDPs", "author": ["Pascal Poupart", "Craig Boutilier", "Relu Patrascu", "Dale Schuurmans"], "venue": "In AAAI-02,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2002}, {"title": "Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems", "author": ["Ray Reiter"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}, {"title": "The design and implementation of vampire", "author": ["Alexandre Riazanov", "Andrei Voronkov"], "venue": "AI Communications,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}, {"title": "Approximate linear programming for first-order MDPs", "author": ["Scott Sanner", "Craig Boutilier"], "venue": "In UAI-2005,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2005}, {"title": "Direct value approximation for factored MDPs", "author": ["Dale Schuurmans", "Relu Patrascu"], "venue": "In NIPS-2001,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "How to dynamically merge Markov decision processes", "author": ["Satinder P. Singh", "David Cohn"], "venue": "In NIPS-98,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}, {"title": "Learning reactive policies for probabilistic planning domains", "author": ["Alan Fern SungWook Yoon", "Robert Givan"], "venue": "In Online Proceedings for The Probablistic Planning Track of IPC-04: http://www.cs.rutgers.edu/ \u0303mlittman/topics/ipc04pt/proceedings/,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "PPDDL: The probabilistic planning domain definition language: http://www.cs.cmu.edu/ \u0303lorens/papers/ppddl.pdf", "author": ["Hakan Younes", "Michael Littman"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}], "referenceMentions": [{"referenceID": 3, "context": "While classic dynamic programming algorithms for MDPs require explicit state and action enumeration, recent techniques for exploiting propositional structure in factored MDPs [4] avoid explicit state and action enumeration.", "startOffset": 175, "endOffset": 178}, {"referenceID": 4, "context": "1 Symbolic dynamic programming (SDP) [5], first-order value iteration (FOVIA) [12, 13], and the relational Bellman algorithm (ReBel) [14] are model-based algorithms for solving FOMDPs and RMDPs, using appropriate generalizations of value iteration.", "startOffset": 37, "endOffset": 40}, {"referenceID": 11, "context": "1 Symbolic dynamic programming (SDP) [5], first-order value iteration (FOVIA) [12, 13], and the relational Bellman algorithm (ReBel) [14] are model-based algorithms for solving FOMDPs and RMDPs, using appropriate generalizations of value iteration.", "startOffset": 78, "endOffset": 86}, {"referenceID": 12, "context": "1 Symbolic dynamic programming (SDP) [5], first-order value iteration (FOVIA) [12, 13], and the relational Bellman algorithm (ReBel) [14] are model-based algorithms for solving FOMDPs and RMDPs, using appropriate generalizations of value iteration.", "startOffset": 78, "endOffset": 86}, {"referenceID": 13, "context": "1 Symbolic dynamic programming (SDP) [5], first-order value iteration (FOVIA) [12, 13], and the relational Bellman algorithm (ReBel) [14] are model-based algorithms for solving FOMDPs and RMDPs, using appropriate generalizations of value iteration.", "startOffset": 133, "endOffset": 137}, {"referenceID": 6, "context": "Approximate policy iteration [7] induces rule-based policies from sampled experience in small-domain instantiations of RMDPs and generalizes these policies to larger domains.", "startOffset": 29, "endOffset": 32}, {"referenceID": 8, "context": "In a similar vein, inductive policy selection using first-order regression [9] uses regression to provide the hypothesis space over which a policy is induced.", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "Approximate linear programming (for RMDPs) [10] is an approximation technique using linear program optimization to find a best-fit value function over a number of sampled RMDP domain instantiations.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "A recent technique for first-order approximate linear programming (FOALP) [21] in FOMDPs approximates a value function by a linear combination of first-order basis functions.", "startOffset": 74, "endOffset": 78}, {"referenceID": 4, "context": "While FOALP incorporates elements of symbolic dynamic programming (SDP) [5], it uses a more compact approximation framework and avoids the need for logical simplification.", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "This stands in contrast with exact value iteration frameworks [5, 12, 13, 14] that prove intractable in many cases due to blowup of the value function representation and the need to perform complex simplifications.", "startOffset": 62, "endOffset": 77}, {"referenceID": 11, "context": "This stands in contrast with exact value iteration frameworks [5, 12, 13, 14] that prove intractable in many cases due to blowup of the value function representation and the need to perform complex simplifications.", "startOffset": 62, "endOffset": 77}, {"referenceID": 12, "context": "This stands in contrast with exact value iteration frameworks [5, 12, 13, 14] that prove intractable in many cases due to blowup of the value function representation and the need to perform complex simplifications.", "startOffset": 62, "endOffset": 77}, {"referenceID": 13, "context": "This stands in contrast with exact value iteration frameworks [5, 12, 13, 14] that prove intractable in many cases due to blowup of the value function representation and the need to perform complex simplifications.", "startOffset": 62, "endOffset": 77}, {"referenceID": 6, "context": "And in contrast to approaches that require sampling of ground domains [7, 9, 10], FOALP solves a FOMDP at the first-order level, thus obviating the need for domain instantiation.", "startOffset": 70, "endOffset": 80}, {"referenceID": 8, "context": "And in contrast to approaches that require sampling of ground domains [7, 9, 10], FOALP solves a FOMDP at the first-order level, thus obviating the need for domain instantiation.", "startOffset": 70, "endOffset": 80}, {"referenceID": 9, "context": "And in contrast to approaches that require sampling of ground domains [7, 9, 10], FOALP solves a FOMDP at the first-order level, thus obviating the need for domain instantiation.", "startOffset": 70, "endOffset": 80}, {"referenceID": 8, "context": "We address the important issue of automatic basis function generation by extending regression-based techniques originally proposed by Gretton and Thiebaux [9].", "startOffset": 155, "endOffset": 158}, {"referenceID": 5, "context": "Approximate Linear Programming: One way of finding a good linear approximation is to cast the optimization problem as a linear program that directly solves for the weights of an L1-minimizing approximation of the optimal value function [6]:", "startOffset": 236, "endOffset": 239}, {"referenceID": 10, "context": "While the size of the objective and the number of constraints in this LP is proportional to the number of states (and therefore exponential), recent solution techniques use compact, factored basis functions and exploit the reward and transition structure of factored MDPs [11, 22], making it possible to avoid generating an exponential number of constraints (and rendering the entire LP compact).", "startOffset": 272, "endOffset": 280}, {"referenceID": 21, "context": "While the size of the objective and the number of constraints in this LP is proportional to the number of states (and therefore exponential), recent solution techniques use compact, factored basis functions and exploit the reward and transition structure of factored MDPs [11, 22], making it possible to avoid generating an exponential number of constraints (and rendering the entire LP compact).", "startOffset": 272, "endOffset": 280}, {"referenceID": 10, "context": "[11] provide the following bound on the loss of V (i+1) w.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The situation calculus [19] is a first-order language for axiomatizing dynamic worlds.", "startOffset": 23, "endOffset": 27}, {"referenceID": 18, "context": "A domain theory is axiomatized in the situation calculus with four classes of axioms [19].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "These characterize the truth values of the fluent F in the next situation do(a, s) in terms of the current situation s, and embody a solution to the frame problem for deterministic actions [19].", "startOffset": 189, "endOffset": 193}, {"referenceID": 4, "context": "We refer the reader to [5, 21] for a formal definition and discussion of the Regr(\u00b7) operator.", "startOffset": 23, "endOffset": 30}, {"referenceID": 20, "context": "We refer the reader to [5, 21] for a formal definition and discussion of the Regr(\u00b7) operator.", "startOffset": 23, "endOffset": 30}, {"referenceID": 4, "context": "Prior to generalizing the situation calculus to permit a firstorder representation of MDPs, we introduce a case notation to allow first-order specifications of the rewards, probabilities, and values required for FOMDPs (see [5, 21] for formal details):", "startOffset": 224, "endOffset": 231}, {"referenceID": 20, "context": "Prior to generalizing the situation calculus to permit a firstorder representation of MDPs, we introduce a case notation to allow first-order specifications of the rewards, probabilities, and values required for FOMDPs (see [5, 21] for formal details):", "startOffset": 224, "endOffset": 231}, {"referenceID": 4, "context": "We define four additional operators on cases [5, 21]: Regr(\u00b7), \u2203~x, max, and \u222a.", "startOffset": 45, "endOffset": 52}, {"referenceID": 20, "context": "We define four additional operators on cases [5, 21]: Regr(\u00b7), \u2203~x, max, and \u222a.", "startOffset": 45, "endOffset": 52}, {"referenceID": 0, "context": "As a consequence we need only formulate SSAs using the deterministic nature\u2019s choices [1, 5], thus obviating the need for a special treatment of stochastic actions in SSAs.", "startOffset": 86, "endOffset": 92}, {"referenceID": 4, "context": "As a consequence we need only formulate SSAs using the deterministic nature\u2019s choices [1, 5], thus obviating the need for a special treatment of stochastic actions in SSAs.", "startOffset": 86, "endOffset": 92}, {"referenceID": 4, "context": "To define the backup operators, we first define a slightly modified version of the first-order decision theoretic regression (FODTR) operator [5]:", "startOffset": 142, "endOffset": 145}, {"referenceID": 20, "context": "Previous work [21] provides examples ofB andB max.", "startOffset": 14, "endOffset": 18}, {"referenceID": 20, "context": "Following [21], we represent a value function as a weighted sum of k first-order basis functions, denoted bCase i(s), each containing a small number of formulae that provide a first-order abstraction of state space:", "startOffset": 10, "endOffset": 14}, {"referenceID": 18, "context": "SSAs can often be compiled from \u201ceffect\u201d axioms that specify action effects [19] and effect axioms can be compiled from PPDDL probabilistic planning domain specifications [25].", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "SSAs can often be compiled from \u201ceffect\u201d axioms that specify action effects [19] and effect axioms can be compiled from PPDDL probabilistic planning domain specifications [25].", "startOffset": 171, "endOffset": 175}, {"referenceID": 4, "context": "Unlike exact solution methods where value functions can grow exponentially in size during the solution process and must be logically simplified [5], here we maintain the value function in a compact form that requires no simplification, just discovery of good weights.", "startOffset": 144, "endOffset": 147}, {"referenceID": 20, "context": "FOALP was introduced by Sanner and Boutilier [21].", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Fortunately, we can generalize constraint generation techniques [22] to avoid generating all constraints.", "startOffset": 64, "endOffset": 68}, {"referenceID": 20, "context": "We refer to [21] for further details.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "[11], we can generalize approximate policy iteration to the first-order case by calculating successive iterations of weights w j that represent the best approximation of the fixed point value function for policy \u03c0Case(s) at iteration i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "While some work has addressed the issue of basis function generation [18, 16], none has been applied to RMDPs or FOMDPs.", "startOffset": 69, "endOffset": 77}, {"referenceID": 15, "context": "While some work has addressed the issue of basis function generation [18, 16], none has been applied to RMDPs or FOMDPs.", "startOffset": 69, "endOffset": 77}, {"referenceID": 8, "context": "We consider a basis function generation method that draws on the work of Gretton and Thiebaux [9], who use inductive logic programming (ILP) techniques to construct a value function from sampled experience.", "startOffset": 94, "endOffset": 97}, {"referenceID": 8, "context": "As noted by Gretton and Thiebaux [9], effectively handling universally quantified rewards is one of the most pressing issues in the practical solution of FOMDPs.", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "To address this problem we adopt a decompositional approach, motivated in part by techniques for additive rewards in MDPs [3, 23, 17, 18].", "startOffset": 122, "endOffset": 137}, {"referenceID": 22, "context": "To address this problem we adopt a decompositional approach, motivated in part by techniques for additive rewards in MDPs [3, 23, 17, 18].", "startOffset": 122, "endOffset": 137}, {"referenceID": 16, "context": "To address this problem we adopt a decompositional approach, motivated in part by techniques for additive rewards in MDPs [3, 23, 17, 18].", "startOffset": 122, "endOffset": 137}, {"referenceID": 17, "context": "To address this problem we adopt a decompositional approach, motivated in part by techniques for additive rewards in MDPs [3, 23, 17, 18].", "startOffset": 122, "endOffset": 137}, {"referenceID": 20, "context": "Since the B operator can often retain much of the additive structure in the linear approximation of vCase(s) [21], representation and computation with this Q-function is very efficient.", "startOffset": 109, "endOffset": 113}, {"referenceID": 2, "context": "Following [3, 17], we do this by treating the sum of the Q-values of any action in the subgoal MDPs as a measure of its Q-value in the joint (original) MDP.", "startOffset": 10, "endOffset": 17}, {"referenceID": 16, "context": "Following [3, 17], we do this by treating the sum of the Q-values of any action in the subgoal MDPs as a measure of its Q-value in the joint (original) MDP.", "startOffset": 10, "endOffset": 17}, {"referenceID": 20, "context": ", during constraint generation [21]), the max over B will implicitly enforce the max constraint of B max.", "startOffset": 31, "endOffset": 35}, {"referenceID": 14, "context": "We applied FOALP and FOAPI to the Box World logistics and Blocks World probabilistic planning problems from the ICAPS 2004 IPPC [15].", "startOffset": 128, "endOffset": 132}, {"referenceID": 19, "context": "We used the Vampire [20] theorem prover and the CPLEX 9.", "startOffset": 20, "endOffset": 24}, {"referenceID": 20, "context": "There appears to be exponential growth in the running time as the number of basis functions increases; this reflects the results of previous work [21].", "startOffset": 146, "endOffset": 150}, {"referenceID": 7, "context": "We compared our planning system to the three other top-performing planners on these domains: G2 is a temporal logic planner with human-coded control knowledge [8]; P is an RTDP-based planner [2]; J1 is a human-coded planner, J2 is an inductive policy iteration planner, and J3 is a deterministic replanner [24].", "startOffset": 159, "endOffset": 162}, {"referenceID": 1, "context": "We compared our planning system to the three other top-performing planners on these domains: G2 is a temporal logic planner with human-coded control knowledge [8]; P is an RTDP-based planner [2]; J1 is a human-coded planner, J2 is an inductive policy iteration planner, and J3 is a deterministic replanner [24].", "startOffset": 191, "endOffset": 194}, {"referenceID": 23, "context": "We compared our planning system to the three other top-performing planners on these domains: G2 is a temporal logic planner with human-coded control knowledge [8]; P is an RTDP-based planner [2]; J1 is a human-coded planner, J2 is an inductive policy iteration planner, and J3 is a deterministic replanner [24].", "startOffset": 306, "endOffset": 310}, {"referenceID": 14, "context": ") on the Box World and Blocks World probabilistic planning problems from the ICAPS 2004 IPPC [15] (\u2013 indicates no data).", "startOffset": 93, "endOffset": 97}], "year": 0, "abstractText": "Recent work on approximate linear programming (ALP) techniques for first-order Markov Decision Processes (FOMDPs) represents the value function linearly w.r.t. a set of first-order basis functions and uses linear programming techniques to determine suitable weights. This approach offers the advantage that it does not require simplification of the first-order value function, and allows one to solve FOMDPs independent of a specific domain instantiation. In this paper, we address several questions to enhance the applicability of this work: (1) Can we extend the first-order ALP framework to approximate policy iteration and if so, how do these two algorithms compare? (2) Can we automatically generate basis functions and evaluate their impact on value function quality? (3) How can we decompose intractable problems with universally quantified rewards into tractable subproblems? We propose answers to these questions along with a number of novel optimizations and provide a comparative empirical evaluation on problems from the ICAPS 2004 Probabilistic Planning Competition.", "creator": null}}}