{"id": "1207.0052", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2012", "title": "The Complexity of Learning Principles and Parameters Grammars", "abstract": "We investigate models for learning the class of context-free and context-sensitive languages (CFLs and CSLs). We begin with a brief discussion of some early hardness results which show that unrestricted language learning is impossible, and unrestricted CFL learning is computationally infeasible; we then briefly survey the literature on algorithms for learning restricted subclasses of the CFLs. Finally, we introduce a new family of subclasses, the principled parametric context-free grammars (and a corresponding family of principled parametric context-sensitive grammars), which roughly model the \"Principles and Parameters\" framework in psycholinguistics. We present three hardness results: first, that the PPCFGs are not efficiently learnable given equivalence and membership oracles, second, that the PPCFGs are not efficiently learnable from positive presentations unless P = NP, and third, that the PPCSGs are not efficiently learnable from positive presentations unless integer factorization is in P.", "histories": [["v1", "Sat, 30 Jun 2012 06:36:04 GMT  (9kb)", "https://arxiv.org/abs/1207.0052v1", null], ["v2", "Tue, 3 Jul 2012 03:49:57 GMT  (9kb)", "http://arxiv.org/abs/1207.0052v2", null], ["v3", "Fri, 6 Jul 2012 15:20:17 GMT  (9kb)", "http://arxiv.org/abs/1207.0052v3", null]], "reviews": [], "SUBJECTS": "cs.FL cs.CL", "authors": ["jacob", "reas"], "accepted": false, "id": "1207.0052"}, "pdf": {"name": "1207.0052.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 120 7,00 52v3 [cs.FL] 6 Y"}, {"heading": "1 Introduction", "text": "Much of modern psycholinguistics has been concerned with solving the problem of the so-called \"poverty of stimulus\" - the assertion that natural languages cannot be learned, since only the data available to infants is available, and consequently that part of the syntax must be \"original\" (i.e. predetermined) and not learned. Gold's theorem (which is described below), which states that there is a superficial class of languages that cannot be learned because of positive representations, is often offered as proof of this fact (although the extent to which the theorem is psycholinguistically informative remains disputed). [gor90] But how is innate linguistic knowledge represented? A mechanism that is usually offered is the framework of Chomsky's \"principles and parameters\" [cho93], which suggests that there are a number of universal principles of grammar that are more easily defined in the structure of the brain than the number of language dwelling within this framework of the appropriate learning process."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Definitions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1.1 Learnability in the limit", "text": "Gold defines the problem of language acquisition as follows: [gol67] Definition 1. With a language class L and an algorithm A, we say that A identifies L in the boundary of positive representations if there is a time t, so that for all u > t, hu = ht = A (i1, i2, \u00b7 \u00b7 \u00b7, it)."}, {"heading": "2.1.2 Exact identification using queries", "text": "Modelling the language learning process as entirely dependent on positive examples seems quite extreme; it is useful to consider environments in which the learner has access to a richer representation of the language. Angluin [ang90] describes a language learning model from oracle queries as follows: Definition 2. An equivalence oracle for a language L takes the representation of a language r (L) as input and outputs \"true\" if L = L, or something w = L (the symmetrical difference between languages) otherwise. There is an obvious equivalence between the equivalence query model and the online error-bound model.Definition 3. A belonging oracle for a language L with the starting symbol S takes a string w and outputs true if S \u21d2 w and a false other gives an equivalence."}, {"heading": "2.2 Hardness of language learning", "text": "Theorem (Gold): There is a class of languages that cannot be learned within the framework of positive representations. Sketch of evidence. Construct an infinite sequence of languages L1, L2, \u00b7 \u00b7, all finite, and leave L \u00b2 = i Li. Suppose there is an algorithm A that could identify each Li from positive representations. Then, there is a positive representation of L \u00b2 that causes A to make an infinite number of errors. First, imagine a series of examples, all in L1 that force A to identify L1. Then, present a series of examples that force it to identify L2, then L3, etc. An infinite number of errors can be forced in this way, so that L \u221e is not learnable within the constraintion.While space does not allow us to discuss the evidence here, we also note the following important result for CFL learning: Theorem (Angluang80)."}, {"heading": "2.3 Learnable subclasses of the CFLs", "text": "While this last finding excludes the possibility of a general algorithm for learning CFLs, it has been shown that subsets of CFLs can be learned if given slightly more powerful oracles, including simple deterministic languages [ish90], enumeration languages [ber87], and so-called very simple languages [yok91]. Particularly encouraging is Anglin's finding that k-limited CFGs can be learned in polynomial time if non-terminal member requests are admitted [ang87]."}, {"heading": "3 Principled Parametric Grammars", "text": "We are now introducing a formal model of the Principles and Parameters framework, which is described in the introduction."}, {"heading": "3.1 Motivation", "text": "Before going into the details of the construction, it is useful to look at a few examples of \"principles\" and \"parameters\" proposed by the advocates of the model. \u2022 The Pro-Drop parameter: Does this language drop pronouns? If PNP is a non-terminal symbol that denotes a pronoun, this parameter determines whether or not a PNP \u2192 \u03b5 rule exists in the language. \u2022 The Ergative / Nominative parameter: Ergative languages distinguish between transitive and intransitive verbs by marking the subject, while nominative languages (such as English) mark the object. Let NP and VP be non-terminal symbols for noun and verb phrases, and let NPtrans be distinguishable versions of these symbols for ergative / nominative marking. Now, each language with verb-subject-object-object-order must be free, there will be a rule \u2192 S-form for each rule, an ergative rule for NP, and a rule for each language with an inmar rule."}, {"heading": "3.2 Construction", "text": "Definition 6. A n-principle-led, k-parametric, context-free grammar ((n, k) -PPCFG) is a quadruple (V, \u03a3, \u044b, S), where: 1. V is a finite alphabet of nonterminal symbols. 2), \u00b7, (Ai, j, j), \u00b7, (Ai, k \u2192 \u03b1i, k), where each n production group of the form (Ai, 1 \u2192 \u03b1i, 1), i.e. a finite sequence of terminals and non-terminals is. Leave i, j the production (Ai, j \u2192 \u03b1i, j).4. S \u0432V is the initial symbol. Definition 7. A parameter setting p = (p1, p2, \u00b7 pn) is a sequence of production (Ai, j \u2192 \u03b1i, j).4."}, {"heading": "3.3 Equivalence", "text": "Some useful facts about the PPCFG: Observation. A \"heterogeneous PPCFG\" with a variable number of right-hand pages can be converted into a \"homogeneous PPCFG\" of the kind described above by \"filling in\" the shorter principles with duplicate rules. Observation: A (n, k) -PPCFG can be converted into a (n, 2) -PPCFG by replacing each PolypleA \u2192 (A \u2192 \u03b1), (A \u2192 \u03b1), by a series of principles. A (n, k) -PPCFG can be converted into a (n (k \u2212 1), 2) -PPCFG as follows: Replace each PolypleA \u2192 (\u03b11, \u03b12,..) by a set of principles A1 \u2192 (\u03b11, A2) -PPCFG."}, {"heading": "4 Generic hardness results for PPCFGs", "text": "We will construct a minimal adequate teacher T, which consists of two oracles EQ (an equivalent oracle) and M (a membership oracle), so that each algorithm A requires an exponential number of queries to identify the correct parameter setting p from a PPCFG.Theorem 1. Without condition, there is no algorithm A, which is able to learn the PPCFGs from equivalence queries and membership queries in polynomic time.Proof. Attach a number n. Construct the PPCFG with V = Xi: i 1.. N S = START \u03a3 = {0, 1} and defined as follows: (START \u2192 X1X2 \u00b7 \u00b7 XN) (Xk \u2192 0, Xk \u2192 1).NEvery parameter setting p in this grammar allows to derive exactly 1 string: each production is deterministic."}, {"heading": "5 Complexity-theoretic hardness results for", "text": "PPCFGsWe will construct a reduction from 3SAT to PPCFG. Let X = {xi} be a set of variables and C = {ci} be a set of clauses. Let's write xj \u00b2 ci if the jth \u00b2 clause is met by the jth \u00b2 variable, and x \u00b2 j \u00b2 if the ith \u00b2 clause is met by the negation of the jth \u00b2 variable. Let's then construct the PPCFG \u00b2 clause with V = X \u00b2 - {START}, and then we will include the following production groups: (START} x1x2 \u00b7 xn) (xi \u00b2 xi \u00b2 xi \u00b2 xi \u00b2 clause, F), (xi \u00b2 xi \u00b2 clause, F) if the x \u00b2 clause exists."}, {"heading": "6 Cryptographic hardness results for PPCSGs", "text": "We will consider a further reduction from integer factorization to PPCSG learning sensation (n \u2212 1) whether the PPCSG learning sensation will be able to terminate the PPCSG learning sensation if the PPCSG learning sensation is able to terminate the PPCSG learning sensation (A0 \u2192 B0), (Aj \u2212 1), (BjAj \u2212 1), (Bj \u2212 1), (Bj \u2212 1), (Bj \u2212 1), (Bj \u2212 1), (Bj \u2212 1), (Bj \u2212 1), (Bj \u2212 1)."}, {"heading": "7 Conclusion", "text": "We have introduced a new model, the principally parametric context-free (including context-sensitive) grammars as a model of the \"Principles and Parameters\" model in psycholinguistics, and presented three learning outcomes for the class of PPCFGs and PPCSGs. Although these results certainly do not clearly show that learning within the framework of principles and parameters is completely impossible (all that is required for learning human language is that a PPCFG is efficiently learnable), we have shown that there is probably no generic algorithm for learning a class of PPCFGs, neither in oracle and member queries, nor in a positive presentation. In general, these results prove that even a radical restriction of the class of candidate grammars does not guarantee a successful outcome in trying to learn CFGs and CSGs."}], "references": [{"title": "Inductive inference of formal languages from positive data.", "author": ["Angluin", "Dana"], "venue": "Info. Contr. 1980,", "citeRegEx": "Angluin and Dana.,? \\Q1980\\E", "shortCiteRegEx": "Angluin and Dana.", "year": 1980}, {"title": "Learning k-bounded context-free grammars.", "author": ["Angluin", "Dana"], "venue": "Yale Technical Report RR-557,", "citeRegEx": "Angluin and Dana.,? \\Q1987\\E", "shortCiteRegEx": "Angluin and Dana.", "year": 1987}, {"title": "Negative Results for Equivalence Queries.", "author": ["Angluin", "Dana"], "venue": "InMachine Learning 1990,", "citeRegEx": "Angluin and Dana.,? \\Q1990\\E", "shortCiteRegEx": "Angluin and Dana.", "year": 1990}, {"title": "Learning one-counter languages in polynomial time.", "author": ["Berman", "Piotr", "Roos", "Robert"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Berman et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Berman et al\\.", "year": 1987}, {"title": "Language Identification in the Limit.", "author": ["Gold", "E. Mark"], "venue": "In Information and Control 1967,", "citeRegEx": "Gold and Mark.,? \\Q1967\\E", "shortCiteRegEx": "Gold and Mark.", "year": 1967}, {"title": "Learnability and Feedback.", "author": ["Gordon", "Peter"], "venue": "In Developmental Psychology 1990,", "citeRegEx": "Gordon and Peter.,? \\Q1990\\E", "shortCiteRegEx": "Gordon and Peter.", "year": 1990}, {"title": "Polynomial Time Learnability of Simple Deterministic Languages.", "author": ["Ishizawa", "Hiroki"], "venue": "In Machine Learning 1990,", "citeRegEx": "Ishizawa and Hiroki.,? \\Q1990\\E", "shortCiteRegEx": "Ishizawa and Hiroki.", "year": 1990}, {"title": "Learning Quickly When Irrelevant Attributes Abound.", "author": ["Littlestone", "Nick"], "venue": "In Machine Learning 1988,", "citeRegEx": "Littlestone and Nick.,? \\Q1988\\E", "shortCiteRegEx": "Littlestone and Nick.", "year": 1988}, {"title": "Polynomial Time Learning of Very Simple Grammars from Positive Data.", "author": ["Yokomori", "Takashi"], "venue": "In proceedings of COLT,", "citeRegEx": "Yokomori and Takashi.,? \\Q1991\\E", "shortCiteRegEx": "Yokomori and Takashi.", "year": 1991}], "referenceMentions": [], "year": 2012, "abstractText": "We investigate models for learning the class of context-free and context-sensitive languages (CFLs and CSLs). We begin with a brief discussion of some early hardness results which show that unrestricted language learning is impossible, and unrestricted CFL learning is computationally infeasible; we then briefly survey the literature on algorithms for learning restricted subclasses of the CFLs. Finally, we introduce a new family of subclasses, the principled parametric context-free grammars (and a corresponding family of principled parametric context-sensitive grammars), which roughly model the \u201cPrinciples and Parameters\u201d framework in psycholinguistics. We present three hardness results: first, that the PPCFGs are not efficiently learnable given equivalence and membership oracles, second, that the PPCFGs are not efficiently learnable from positive presentations unless P = NP, and third, that the PPCSGs are not efficiently learnable from positive presentations unless integer factorization is in P.", "creator": "LaTeX with hyperref package"}}}