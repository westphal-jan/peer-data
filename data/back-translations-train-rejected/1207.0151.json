{"id": "1207.0151", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2012", "title": "Differentiable Pooling for Hierarchical Feature Learning", "abstract": "We introduce a parametric form of pooling, based on a Gaussian, which can be optimized alongside the features in a single global objective function. By contrast, existing pooling schemes are based on heuristics (e.g. local maximum) and have no clear link to the cost function of the model. Furthermore, the variables of the Gaussian explicitly store location information, distinct from the appearance captured by the features, thus providing a what/where decomposition of the input signal. Although the differentiable pooling scheme can be incorporated in a wide range of hierarchical models, we demonstrate it in the context of a Deconvolutional Network model (Zeiler et al. ICCV 2011). We also explore a number of secondary issues within this model and present detailed experiments on MNIST digits.", "histories": [["v1", "Sat, 30 Jun 2012 21:04:13 GMT  (1997kb,D)", "http://arxiv.org/abs/1207.0151v1", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["matthew d zeiler", "rob fergus"], "accepted": false, "id": "1207.0151"}, "pdf": {"name": "1207.0151.pdf", "metadata": {"source": "CRF", "title": "Differentiable Pooling for Hierarchical Feature Learning", "authors": ["Matthew D. Zeiler"], "emails": ["zeiler@cs.nyu.edu", "fergus@cs.nyu.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to move to a different world in which they are able than to another world in which they are able to, in which they are able to, in which they live."}, {"heading": "2. Model Overview", "text": "We explain our contributions in the context of a deconvolutional network introduced by Zeiler et al. [22] This model is a hierarchical form of Convolutional Spare Coding that can learn invariant image features in an uncontrolled context. Xiv: 120 7,01 51v1 [cs.CV] 3 0Ju n20 12 * \u03a3 Input Image yConv Layer 1 Conv Layer 2 U npool Layer 1, 1 Unpooled Map z1,1 Feature Map p1,11st Layer Filters2nd Layer Filters |. | 0.5 z1,2 p1,2U npool Layer 2 \u03b81,2zB, 2 f 1 1,1 f C 1,1 f 1 B, 1 Layer 1 features p1Reconstructed input yLayer 2 unpooled features z2L0.5 SparsityLayer 1 unpooled Features * z1Unpool Vars 2 Pact1 Pact1 F1 F F2 F T 2 ^ R2 Unpooling Vpoars *."}, {"heading": "2.1. Unpooling", "text": "In the unpooling sub-stage, each 2D feature map pb is subjected to an unpooling operation to create a larger 2D unpooled feature map zb1. Each element j in pb affects a small neighborhood Nj (typically 2 \u00b7 2 or 3 \u00b7 3) in the unpooling map zb, via a set of weightsw (i) within the neighborhood: zb (i) = w (i) pk (j). The inverse pooling operation calculates each element j in pb as the sum of weights w (i) in neighborhood Nj of the unpooled map Nj (i) as a unit 2 norm, as this makes the unpooling operation invertible2."}, {"heading": "2.2. Convolution", "text": "In the lower stage of folding, the reconstruction v \u0435c is formed by entangling and adding 2D unpooled feature maps e.g. with filters f cb. Filters f are the parameters of the model common to all images. 2Combining Eqs. 2 and 3 yields pb (j) = \u2211 i w 2 (i) pb (j), i.e. \u2211 i w 2 (i) = 1.maps z are latent variables specific to each image. For the notorious brevity, we combine the folding and summing operations to a single folding matrix F and convert the multiple 2D maps zb into a single vector z: v = Fz."}, {"heading": "2.3. Discussion of Single Layer", "text": "The combination of unpooling and folding operations results in the reconstruction v \u0442 of p: \u03bb 2-FUwp \u2212 v-22 + | p | \u03b1 (5). This integrated formulation enables the simple optimization of filters f, features p and (in) pooling weights w to minimize a single objective function. While most other models also learn filters and features, the pooling process is typically fixed. The direct optimization of Equation 5 with respect to w is one of the most important contributions of this work and will be discussed in Section 2.5.Note that the reconstruction is linear in p with respect to fixed weights w, i.e. Equation 5 describes a trilinear model, where the coding position (where) information about the (what) features p.Eqn. 5 differs from the original deformation network formulation. < the sparseness is imposed directly on the pooling, with the position of the hyperposition (where) the perposition is most perposition, but not only (where) the perposition is negative (and where) the hypoferation is."}, {"heading": "2.4. Multiple Layers", "text": "Multi-layer models are constructed by stacking the single-layer model described above in the same way as Zeiler et al. [22]. Characteristic cards p from one layer become input cards for the top layer (which now has B \"color channels\"). An important feature of the model is that characteristic cards exist exclusively at the top of the model (there are no explicit features in interlayers), so the only variables in the interlayers are filters F and unbundling weights w. For an l-layer model, the reconstruction v-V-V = F1Uw1F2Uw2.... FlUwlpl = Rlpl (6), where Fk and Uwk are the revolving and unbundling operations from each layer. We condense the sequence of unbundling and folding operations into a single reconstruction operation operator Rl, with which we can write the entire layer for one (but one) model per image."}, {"heading": "2.5. Differentiable Pooling", "text": "We impose a parametric form on the (in) pooling weights w to ensure that the characteristics are invariable for small changes in the input. The parametric model we use is a 2D-axis aligned Gaussian with mean (\u00b5x, \u00b5y) and precision (\u03b3x, \u03b3y) relative to the pooling neighborhood Nj introduced in Section 2.1. The Gaussian model is standardized within the extent of the pooling region to give weights w whose square sum is 1 (and thus unit '2 norm): w (i) = a (i)."}, {"heading": "2.6. Non-Negativity", "text": "This is in contrast to our model in which we force non-negativity, which is motivated by several factors: firstly, there is no idea of negative intensities or objects in the visual world; secondly, there is some biological evidence of non-negative representations within the brain [10]; and finally, we find experimentally that non-negativity reduces the flexibility of the model and encourages it to learn good representations; and the characteristics calculated at the time of testing have improved classification performance compared to models without this limitation (see Section 6.4)."}, {"heading": "2.7. Hyper-Laplacian Sparsity", "text": "Most sparse coding models use the \"1 standard\" to impose a limitation on the sparseness of features [16], as a proxy for optimizing the \"0 sparseness.\" [21] One disadvantage of this form of regularization, however, is that two elements cost 0.5 compared to a single element at 1 and the other at 0, even though the latter has lower \"0 costs.\" To promote features with lower \"0 costs, we use a pseudo-standard\" 0.5 \"(i.e. \u03b1 = 0.5 for Eq. 5), inspired by Krishnan and Fergus [11], which aggressively pushes small elements toward zero. To optimize this, we experimented with techniques in [11], but chose parentage for simplicity."}, {"heading": "3. Inference", "text": "In the conclusion, the filters f are fixed at all levels and the goal is to find the characteristics p and (in) pooling variables \u03b8 for all neighborhoods and all levels that minimize Equation 7. We do this by alternately updating the characteristics p and the Gaussian variables \u03b8 while holding the others firmly."}, {"heading": "3.1. Feature Updates", "text": "For a given layer l = Rlpl e, which we can then update: l = pl pl = pl (v \u2212 l), and then we will change the top layer pl e, and then we will update it: l = pl (v \u2212 v), and then we will change the error signals e = (v \u2212 v), and then we will change the error signals e = (v \u2212 v), and then we will change the error signals e = (v \u2212 v), and then we will change the error signals e = (v \u2212 v), and then we will change the top layer pl = Rlpl (n \u2212 v), and then we will change the error signals e = (v \u2212 v), and then we will calculate the error signals e = (v \u2212 v \u2212 v), and then we will change the top layer pl = Rlpl e."}, {"heading": "3.2. (Un)pooling Variable Updates", "text": "In view of a model with l-planes, we would like to fix the (un) pooling variables at each intermediate layer k (to optimize the objective Cl (v). We assume that the filters f1,., fl and functions pl (n) are fixed. Formally: The pooling variable gradients imply the combination, at level k (R), the forward propagated error signal with the top-down reconstruction signal. (n) Formally: The pooling variable gradients imply the combination, at level k (R), the forward propagated error signal with the top-down reconstruction signal. \u2212 This combined signal then drives the updating of the pooling variables. Formally: The error propagation at level k and RTk is the error propagation up to zk.With the selected Gaussian parameterization of the pooling regions, the chain rule can be used to calculate the individual parameters."}, {"heading": "4. Learning", "text": "After the characteristic maps for the top layer and the (un) pooling variables for all layers are completed, the filters in each layer are updated, using the gradient with respect to the filters of each layer: \u2202 Cl \u2202 (f bc) k = \u03bbl [R T k \u2212 1 (Rlpl \u2212 v)] c * [(U\u03b8kR (l \u2192 k) pl)] b (26), where the left term is the error signal propagated up to the characteristic maps under the predefined filters, pk \u2212 1 and the right term the reconstruction from top to bottom to the unpooled characteristic maps zk. The gradient is therefore the folding between all combinations of entered error maps on the layer (indexed by c) and the unpooled characteristic maps reconstructed from the top (reconstructed by b), which leads to updates of each filter layer f bc. In the practice we use the second layer of the learning digitization for adjustments in the juggling layer."}, {"heading": "4.1. Joint Inference", "text": "The objective function explicitly restricts the reconstruction of the characteristics of the top layer to be close to the input image. From this, we can calculate gradients for the filters and pooling variables of each layer and at the same time optimize the characteristic maps of the top layer. Therefore, for each image, we can derive the local shifts and scales of lower characteristics than the concepts of the higher layer. We have found that the pre-training of the first layer in a training phase and then the use of the pooling variables and learned layer-1 filters work best to initialize a second training phase. The second training phase optimizes the goal of the second layer, from which we can jointly update p2, Uw2, Uw1, F2 and F1. If in this joint update, the characteristics of the first layer do not exercise care, the imaging power can exchange with the filters of the second layer. This may result in us to keep the points of the second layer collectively detecting the filters during the first layer, so that the first filter details remain."}, {"heading": "5. Initialization of Parameters", "text": "After this random initialization, the filters are projected so as not to be negative and normalize to the length of the unit before the inference, either at the beginning of the training or at the test time, we initialize the characteristic cards to 0. This results in a reconstruction of 0 in pixel space, so the initial gradient propagated in the network is \u2212 y. This is similar to an upstream network for the first iteration of the inference. As we propagate this signal forward in the network, we can use Gaussian parameterization of the pooling regions to adjust these pooling parameters by moment matching. That is, at each level we extract the optimal pooling parameter that matches this bottom-up signal. This provides a natural initialization of both the pooling variables at each level and the top level of the activation of the characteristics based on the input image and the filter initialization."}, {"heading": "6. Experiments", "text": "Evaluation on MNIST We opt for our model on the MNIST handwritten number classification task. This dataset provides a relatively large number of training instances per class, has many other results to compare, and allows a simple interpretation of how a trained model disassembles each image. Pre-processing: The inputs were the unprocessed MNIST digits at 28x28 resolution. Since no pre-processing was performed, the elements remained nonnegative. Model Architecture: We trained a 2-layer model with 5x5 filters in each layer and 2x2 non-overlapping regions. The first layer contained 16 feature maps and the second layer contained 48 feature maps. Each of these 48 features are randomly associated with 8 different layer 1 feature filters in the second layer filter. These sizes were chosen that are comparable to [22] favored during GPU processing. The receptive fields of the second layer features are 14x14 pixels with this configuration."}, {"heading": "6.1. Model visualization", "text": "In fact, it is the case that one sees oneself as being able to go to a place where one can go to a place where one is able to move."}, {"heading": "6.2. Max Pooling vs Gaussian Pooling", "text": "Figure 5 (below) shows that a significant alias effect is present in the visualizations of the model when Max Pooling is used. As the complex interactions between positive and negative elements are removed, the model cannot form smooth transitions between non-overlapping pooling regions, although the filters used in the subsequent Convolution sublayer overlap between regions. By using Gaussian pooling, the model can derive the desired precision and means to optimize the reconstruction quality from high layers of the model. This fine-tuning of the reconstruction allows improvements without significantly varying the activation of the features (i.e. the thinness maintains or decreases while adjusting the pooling parameters). This is confirmed in Figure 6, where we subdivide the cost function into the reconstruction and regulation of the pool. In this figure, we also show the \"0-spareness of each pool,\" this goal can be significantly improved."}, {"heading": "6.3. Joint Inference", "text": "One of the main criticisms of the austerity measures is that they themselves are able to take the measures mentioned in order to be effective."}, {"heading": "6.4. Effects of Non-Negativity", "text": "If negative elements are present in the system, many possible solutions can be found during optimization, because subtractions allow the removal of parts of high-level features, which makes them less discriminatory, since the model can change parameters between high-level activations and the input image in order to better reconstruct, while assigning less importance to the activations themselves. To show that this is not an artifact of the Gaussian association that is better suited for non-negative systems (due to the summation over the pool region, which may lead to cancellations if negatives are present), we include the comparison in Table 3 on Max Pooling. In both cases, asserting positivity over projected gradient departure improves the discriminatory information preserved in the features."}, {"heading": "6.5. Effects of Feature Reset", "text": "When training the model on MNIST, some less optimal filters are learned if the feature maps are not reset. For example, in Fig. 8 (c), many of these layer-1 filters are block-like, like the 3rd row, 2nd column. However, the same feature in (a) improves when the feature maps are reset to 0 midway through the training. This single reset is sufficient to encourage the filters to specialize and improve. Similarly, the layer-2-pixel visualizations in (b) are significantly more discrepancies due to the reset than in (d), where there was no reset. In particular, note many blob-like features learned in (d) without being reset, such as the 2nd and 5th rows of the 1st column, which improve in (b). These larger, more diverse features learned with the reset help improve the classification performance, as shown in Table 4."}, {"heading": "6.6. Effects of Hyper-Laplacian Sparsity", "text": "In this comparison, we trained two models, one with a '1 before the function board and the other with a' 0.5 before the function board. Once trained, we took each model and drew conclusions with both '1 and' 0.5 priors. For reference, the '0 spareness for training runs was 4.2 for' 0.5-regulated training and 20.2 for '1-regulated training with the same setting \u03bb2 = 0.5. Since the amount of sparseness can also be controlled during inference by the Khal2 parameter, in Fig. 9 we record the classification performance for different Khal2 settings in these four model combinations. Interestingly, the use of the additional sparseness during training, which is forced by the' 0.5 factor before inference, is the optimal combination for all of these four model combinations."}, {"heading": "6.7. Comparison to Other Methods", "text": "We have chosen the MNIST dataset because of the large number of results we can compare it with. Of these, deep learning methods typically fall into one of two categories: 1) those that are completely unattended and have a simple classifier, or 2) those that are discriminatively refined with labels. Our method falls into the first category because it is completely opaque during training, and only the linear SVM used above has access to the label information of the training set. We do not disseminate this information over the network, but this would be an interesting future direction. Table 5 shows that our method competes with other deep generative models, and even outperforms several that use discriminatory fine tuning."}, {"heading": "7. Discussion", "text": "In this thesis, we introduced the concept of differentiated pooling for deep learning methods. We also demonstrated that joint training improves the performance of the model, positivity encourages the model to learn better representations, and that there is an optimal degree of sparseness that can be used during training and conclusion. Finally, we introduced a simple reset scheme to avoid local minimum values and learn better characteristics. We believe that many of the approaches and insights in this thesis are applicable not only to Deconvolutional Networks, but also to frugal coding and other deep learning methods in general."}], "references": [{"title": "A fast iterative shrinkagethresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Learning midlevel features for recognition", "author": ["Y. Boureau", "F. Bach", "Y. LeCun", "J. Ponce"], "venue": "In CVPR. IEEE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "A theoretical analysis of feature pooling in vision algorithms", "author": ["Y. Boureau", "J. Ponce", "Y. LeCun"], "venue": "In ICML,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Rapid inference on a novel and/or graph for object detection, segmentation and parsing", "author": ["Y. Chen", "L. Zhu", "C. Lin", "A. Yuille", "Z. H"], "venue": "In NIPS,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "The shape boltzmann machine: a strong model of object shape", "author": ["S. Eslami", "N. Heess", "J. Winn"], "venue": "In CVPR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Liblinear: A library for large linear classification", "author": ["R.E. Fan", "K.W. Chang", "C.J. Hsieh", "X.R. Wang", "C.J. Lin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Transforming auto-encoders", "author": ["G.E. Hinton", "A. Krizhevsky", "S. Wang"], "venue": "In ICANN-11,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. The"], "venue": "Neuro Computation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Modeling receptive fields with non-negative sparse coding", "author": ["P.O. Hoyer"], "venue": "Neurocomputing, 52-54:547\u2013552,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Analytic Hyper-Laplacian Priors for Fast Image Deconvolution", "author": ["D. Krishnan", "R. Fergus"], "venue": "In NIPS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "K. Chen", "G. Corrado", "J. Dean", "A. Ng"], "venue": "In ICML,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural Comput.,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H. Lee", "R. Grosse", "R. Ranganath", "A.Y. Ng"], "venue": "In ICML,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Learning deep energy models", "author": ["J. Ngiam", "Z. Chen", "P. Koh", "A. Ng"], "venue": "In ICML,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Sparse coding with an overcomplete basis set: A strategy employed by V1", "author": ["B.A. Olshausen", "D.J. Field"], "venue": "Vision Research,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Unsupervised learning of invariant feature hierarchies with applications to object reocgnition", "author": ["M. Ranzato", "F. Huang", "Y. Boureau", "Y. LeCun"], "venue": "In CVPR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Are sparse representations really relevant for image classification", "author": ["R. Rigamonti", "M. Brown", "V. Lepetit"], "venue": "In CVPR,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In AISTATS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "An introduction to the conjugate gradient method without the agonizing pain", "author": ["J.R. Shewchuk"], "venue": "Neural Comput.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1996}, {"title": "Adaptive deconvolutional networks for mid and high level feature learning", "author": ["M. Zeiler", "G. Taylor", "R. Fergus"], "venue": "In ICCV,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Latent hierarchical structural learning for object detection", "author": ["L. Zhu", "Y. Chen", "A. Yuille", "W. Freeman"], "venue": "In CVPR,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}], "referenceMentions": [{"referenceID": 21, "context": "[22]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "One class of methods, for example Convolutional Neural Networks [13] or the recent RICA model of Le et al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 11, "context": "[12], use a purely feed-forward hierarchy that maps the input image to a set of features which are presented to a simple classifier.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "These include Deep Belief Networks [9], Deep Boltzmann Machines [19] and the Compositional Models of Zhu et al.", "startOffset": 35, "endOffset": 38}, {"referenceID": 18, "context": "These include Deep Belief Networks [9], Deep Boltzmann Machines [19] and the Compositional Models of Zhu et al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "[23, 4].", "startOffset": 0, "endOffset": 7}, {"referenceID": 3, "context": "[23, 4].", "startOffset": 0, "endOffset": 7}, {"referenceID": 2, "context": "[3] for an analysis).", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "A few approaches do perform full joint training of the layers, notably the Deep Boltzmann Machine [19], and Eslami et al.", "startOffset": 98, "endOffset": 102}, {"referenceID": 4, "context": "[5], as applied to images, and the Deep Energy Models of Ngiam et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17], the transforming auto-encoders of Hinton et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[7], and Zeiler et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "13D (un)pooling is also possible, as explored in [22].", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "[22], max (un)pooling was used, equivalent to w(i) being all zero, except for a single element set to 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "5 differs from the original Deconvolutional Network formulation [22] in several important ways.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "Second, [22] considers only \u03b1 = 1, rather than the hyper-Laplacian (\u03b1 < 1) sparsity we employ.", "startOffset": 8, "endOffset": 12}, {"referenceID": 21, "context": "Third, p is non-negative, as opposed to [22] where there was no such constraint.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "Third, there is some biological evidence for non-negative representations within the brain [10].", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": "Most sparse coding models utilize the `1-norm to enforce a sparsity constraint on the features [16], as a proxy for optimizing `0 sparsity [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "Most sparse coding models utilize the `1-norm to enforce a sparsity constraint on the features [16], as a proxy for optimizing `0 sparsity [21].", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "5) inspired by Krishnan and Fergus [11], which aggressively pushes small elements toward zero.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "To optimize this, we experimented with techniques in [11], but settled on gradient descent for simplicity.", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "This is a large convolutional sparse coding problem and we adapt the ISTA scheme of Beck and Teboulle [1].", "startOffset": 102, "endOffset": 105}, {"referenceID": 19, "context": "Step size calculation: In order to set a learning rate for the feature map optimization, we employ an estimation technique for steepest descent problems [20] which uses the gradients\u2207pl = \u2202C \u2202pl : \u03b2pl = \u2207pl \u2207pl \u2207pl RT l Rl\u2207pl (15)", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "These sizes were chosen comparable to [22] while being more amenable to GPU processing.", "startOffset": 38, "endOffset": 42}, {"referenceID": 21, "context": "[22] could be simplified by making the top level features of the network more informative.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Therefore, in this paper we simply treat the top level activations inferred for each image as input to a linear SVM [6].", "startOffset": 116, "endOffset": 119}, {"referenceID": 1, "context": "The only post processing done to these high level activations is that overlapping patches are extracted and pooled, analogous to the dense SIFT processing which is shown by many computer vision researchers to improve results [2].", "startOffset": 225, "endOffset": 228}, {"referenceID": 21, "context": "Searching through the dataset of inferred feature map activations and selecting the maximum element per feature map to project downward into the pixel space as in [22] is one way of visualizing these invariances.", "startOffset": 163, "endOffset": 167}, {"referenceID": 17, "context": "It has previously been shown that sparsity encourages learning of distinctive features, however it is not necessarily useful for classification [18] [22].", "startOffset": 144, "endOffset": 148}, {"referenceID": 21, "context": "It has previously been shown that sparsity encourages learning of distinctive features, however it is not necessarily useful for classification [18] [22].", "startOffset": 149, "endOffset": 153}, {"referenceID": 13, "context": "84% \u2013 CDBN (1+2 layers) [14] 0.", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "82% \u2013 DBN (3 layers) [8] [9] 2.", "startOffset": 21, "endOffset": 24}, {"referenceID": 8, "context": "82% \u2013 DBN (3 layers) [8] [9] 2.", "startOffset": 25, "endOffset": 28}, {"referenceID": 18, "context": "DBM (2 layers) [19] \u2013 0.", "startOffset": 15, "endOffset": 19}], "year": 2012, "abstractText": "We introduce a parametric form of pooling, based on a Gaussian, which can be optimized alongside the features in a single global objective function. By contrast, existing pooling schemes are based on heuristics (e.g. local maximum) and have no clear link to the cost function of the model. Furthermore, the variables of the Gaussian explicitly store location information, distinct from the appearance captured by the features, thus providing a what/where decomposition of the input signal. Although the differentiable pooling scheme can be incorporated in a wide range of hierarchical models, we demonstrate it in the context of a Deconvolutional Network model (Zeiler et al. [22]). We also explore a number of secondary issues within this model and present detailed experiments on MNIST digits.", "creator": "LaTeX with hyperref package"}}}