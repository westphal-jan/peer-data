{"id": "1211.3046", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Nov-2012", "title": "Recovering the Optimal Solution by Dual Random Projection", "abstract": "In this work, we address the problem of how to recover the optimal solution to the optimization problem related to high dimensional data classification using random projection, to which we refer as Recovery of Optimal Solution. This is in contrast to the previous studies that were focused on analyzing the classification performance using random projection. We reveal the relationship between compressive sensing and the problem of recovering optimal solution using random projection. We also present a simple algorithm, termed as Dual Random Projection, that recovers the optimal solution with a small error by computing dual solution provided that the data matrix is of low rank.", "histories": [["v1", "Tue, 13 Nov 2012 16:39:45 GMT  (26kb)", "http://arxiv.org/abs/1211.3046v1", null], ["v2", "Thu, 15 Nov 2012 22:09:06 GMT  (26kb)", "http://arxiv.org/abs/1211.3046v2", null], ["v3", "Tue, 2 Apr 2013 19:05:01 GMT  (25kb)", "http://arxiv.org/abs/1211.3046v3", null], ["v4", "Fri, 21 Feb 2014 20:57:42 GMT  (28kb)", "http://arxiv.org/abs/1211.3046v4", "The 26th Annual Conference on Learning Theory (COLT 2013)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["lijun zhang", "mehrdad mahdavi", "rong jin", "tianbao yang", "shenghuo zhu"], "accepted": false, "id": "1211.3046"}, "pdf": {"name": "1211.3046.pdf", "metadata": {"source": "CRF", "title": "Recovering Optimal Solution by Dual Random Projection", "authors": ["Lijun Zhang", "Rong Jin", "Tianbao Yang", "Zhang Jin Yang"], "emails": ["zhanglij@msu.edu", "rongjin@cse.msu.edu", "tyang@ge.com"], "sections": [{"heading": null, "text": "ar Xiv: 121 1.30 46v1 [cs.LG] 1 3Keywords: random projection, primary solution, dual solution, low rank"}, {"heading": "1. Introduction", "text": "Random projection has been widely used in many tasks of machine learning, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al., 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al., 2010), dimensional reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al., 2008) and information retrieval (Goel et al., 2005). In this work we focus on random projection for classification. Many studies have focused on the analysis of classification performance by random projection."}, {"heading": "2. The Problem of Recovering Optimal Solutions from Random Projection", "text": "Lass (xi, yi), i = 1,., n be a set of training examples, where xi-Rd is a vector of d dimension and yi (yi). (xi, yi) is the binary class assignment for xi. let X = (x1,.., xn) and y = (y1,., yn) is include input patterns and the class assignments of all training examples. A classifier w's learned from the examples training by solving the following optimization problem: min w's 2 + n \u00b2). (yix) is a), where s a convex loss function that is differentiable 1. By writing (z) in its convex conjugate form, i.e.s (z) = min."}, {"heading": "3. Algorithm", "text": "To motivate our algorithms, let's restore the optimal primary solution (1) that will be restored in Proposition 1. (2) It is the simplest method that will be restored in Proposition 1. (3) It is the simplest method that will be restored in Proposition 1. (3) It is the simplest method that will be restored in Proposition 1. (4) It is the simplest method that will be used in Proposition 3. (4) It is the simplest method that will be used in Proposition 3. (4) It is the best method that will be used in Proposition 3. (5) The recovered solution that will be used in Proposition 3. (4) The recovered solution that will be used in Proposition 3. (4) The recovered solution that will be used in Proposition 3. (y) The recovered solution where the optimal solution to the dual problem is. (2) Given the projected data x."}, {"heading": "4. Main Results", "text": "In this section, we will present a sufficient number of random projections. (1) We will then extend the result to the iterative algorithm basis. (2) In our case, we assume that the data matrix X is of low rank. (2) We point out that the low rank assumption closely relates to the savings-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-assumptions-X."}, {"heading": "5. Analysis", "text": "Before we present the analysis, we first ascertain some notations and facts. Let's get the SVD of X beX = U\u0442V = r \u2211 i = 1\u03bbiuiv iwo \u03bbi is the ith singular value of X, ui and vi the corresponding left and right singular vectors of X. Let's get U = (u1,..., ur), V = (v1,.., vr). Using the singular value degradation of X, we define that U is an othorgonal matrix, i.e. w = V D (y)."}, {"heading": "5.1. Proof of Theorem 2", "text": "The key to our analysis is to show that G in (7) is close to G in (2), if the number of random projections is sufficiently large. To this end, we need the following concentration inequality for Gaussian random matrix.Korollary 6 Let M + Rr \u00b7 m be a standard Gaussian random matrix. Then, with a probability of at least 1 \u2212 3, we have the following concentration inequality for Gaussian random matrix.Korollary 6 Less M + r) + log (1 / 3) + log (1 / 3)) c\u03b52where M + 2 is the spectral norm of matrix M and c is a constant whose value is at least 1 / 32. Remark 8 \u00b0 C is the key to our analysis that enables us to bind G \u2212 G \u2212 G and further bind it. We have the following theorem that limits the difference between G and G.theorem."}, {"heading": "5.2. Proof of Theorem 3", "text": "As already indicated, the main reason for the big difference between w \u00b2 and w \u00b2 is that they are not in the same subspace: w \u00b2 is in the subspace spanned by the columns in U, while w \u00b2 is in the subspace spanned by the column vectors in a random matrix. Before presenting our analysis, we first present a version of John Linderstrauss that is useful for our analysis. Theorem 8 (Theorem 2 (Blum, 2006) Let us leave x x \u00b2 x \u00b2 x \u00b2 m, where S \u00b2 Rd \u00b2 m is a random matrix whose entries are chosen independently of N (0, 1)."}, {"heading": "5.3. Proof of Theorem 5", "text": "Considering a solution we obtained with iteration t, we consider the following optimization to be a problem = problem w = problem w = problem w = error w (w; X, y) = solution w (w) is the optimal solution for (23). Then we can use the dual random approach to not solve the problem + 1 x x x x x x. Similarly, if we can show that the solution t + 1 x x x x (w) is the optimal solution for (23), then we define the updated recovered solution by w (t) t + 1 x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x (t) x."}, {"heading": "6. Conclusion", "text": "In this article, we will discuss the problem of restoring optimal solutions by random projection. Our goal is to first efficiently obtain an approximate solution z \u043a for a given optimization problem by using random projections, and then to reconstruct the truly optimal solution from the random projection solution z \u043a. We developed a dual approach to random projection and show that the proposed approach, assuming that the data matrix X is of low rank, is capable of accurately restoring the true optimal solution w \u0445 with a small error. There are several open questions that need to be answered in the future. The first open question is to analyze the behavior of the proposed algorithm if X can be well approximated by a low-graded matrix, an assumption that is significantly weaker than the assumption of low rank. The second open question is the development of a parallel version of the proposed algorithm by running it independently over multiple machines. The challenge is to design an effective solution to significantly smaller projection sets of a combination of random solutions."}, {"heading": "Acknowledgments", "text": "We thank a whole range of people."}, {"heading": "Appendix A. Proof of Proposition 1 and Proposition 3", "text": "Since the two propositions can be proven in a similar way, we present only the proof for proposition 1. First, if \u03b1 \u0445 is the optimal dual solution, the optimal primary solution can be obtained by w \u043a = arg min w-Rd\u03bb 2-w-22 + n-i = 1 [\u03b1] iyix iBy setting the gradient to zero in relation to w, we obtain w \u0445 = \u2212 \u2211 ni = 1 [\u03b1] iyixi / \u03bb = \u2212 XD (y) \u03b1 / \u03bbSecond, in order to prove the dual solution \u03b1 \u0445 in view of the primary solution w \u0445, we find that we (yix i w-1) = [\u03b1) i (yix i w-1) \u2212 ([\u03b1] iThrough the fennel conjugate theory (e.g. Theorem 11.4 in (Cesa-Bianchi and Lugosi, 2006)) have a satisfactory solution."}, {"heading": "Appendix B. Proof of Corollary 6", "text": "s random matrix. Theorem 9 (Korollar7.2 (Gittens and Tropp, 2011) Let C-Rp \u00b7 p be a positive definitive matrix. Let the eigenvalues of the Gaussian random matrix, j = 1,.., n be i.i.d. samples from an N (0, C) distribution. DefineC-Rp \u00b7 p = 1nn, j = 1n. Write the eigenvalues for the kth eigenvalue of C, and then write the results for the kth eigenvalue of C-n. Then for k = 1,.., p, Pr-k \u00b2 k \u00b2 (1 + \u03b5). (p \u2212 k + 1) exp (\u2212 k + 1) exp (\u2212 k) exp (\u2212 k) exp (\u2212 k) exp (\u2212 k + 2) exp (\u2212 m) exp (\u2212 m) exp (\u2212 cn \u00b2) exp (exp \u00b2) (\u2212 exp), \u2212 cn \u00b2 (\u2212 cp) exp (\u2212 m), \u2212 m (\u2212 m) exp \u00b2 (\u2212 m) exp (\u2212 cn)."}, {"heading": "Appendix C. Proof of Theorem 4", "text": "We write w \"in terms such as w\" = SS \"w\" / m and then \"x\" x \"2\" \u2264 1, \"x\" span \"(X) x\" w \"(W\" W \"W\") \u2264 \"W\" 2 + max \"X\" 2 \"1,\" x \"span\" (X) x \"(W\" S \"U\") = \"W\" 2 + max \"a\" 2 \"1a\" (I \"U\" SS \"U\"). / \"W\" 2 + \"max\" (I \"U\" SS \"U\"). / \"W\" max \"(I\" U \"SS\" U \")."}, {"heading": "Appendix D. Proof of Theorem 1", "text": "(5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5)) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5) (5"}], "references": [{"title": "Database-friendly random projections: Johnson-lindenstrauss with binary coins", "author": ["Dimitris Achlioptas"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Achlioptas.,? \\Q2003\\E", "shortCiteRegEx": "Achlioptas.", "year": 2003}, {"title": "An algorithmic theory of learning: robust concepts and random projection", "author": ["Rosa I. Arriaga", "Santosh Vempala"], "venue": "In Proceedings of the 40th Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Arriaga and Vempala.,? \\Q1999\\E", "shortCiteRegEx": "Arriaga and Vempala.", "year": 1999}, {"title": "A pac-style model for learning from labeled and unlabeled data", "author": ["Maria-Florina Balcan", "Avrim Blum"], "venue": "In Proceedings of the 18th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Balcan and Blum.,? \\Q2005\\E", "shortCiteRegEx": "Balcan and Blum.", "year": 2005}, {"title": "Kernels as features: On kernels, margins, and low-dimensional mappings", "author": ["Maria-Florina Balcan", "Avrim Blum", "Santosh Vempala"], "venue": "Machine Learning,", "citeRegEx": "Balcan et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Balcan et al\\.", "year": 2006}, {"title": "Random projection in dimensionality reduction: applications to image and text data", "author": ["Ella Bingham", "Heikki Mannila"], "venue": "In Proceedings of the 7th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Bingham and Mannila.,? \\Q2001\\E", "shortCiteRegEx": "Bingham and Mannila.", "year": 2001}, {"title": "Random projection, margins, kernels, and feature-selection", "author": ["Avrim Blum"], "venue": "In Proceedings of the 2005 international conference on Subspace, Latent Structure and Feature Selection,", "citeRegEx": "Blum.,? \\Q2006\\E", "shortCiteRegEx": "Blum.", "year": 2006}, {"title": "Random projections for kmeans clustering", "author": ["Christos Boutsidis", "Anastasios Zouzias", "Petros Drineas"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Boutsidis et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2010}, {"title": "Convex Optimization", "author": ["Stephen Boyd", "Lieven Vandenberghe"], "venue": null, "citeRegEx": "Boyd and Vandenberghe.,? \\Q2004\\E", "shortCiteRegEx": "Boyd and Vandenberghe.", "year": 2004}, {"title": "Rademacher chaos, random eulerian graphs and the sparse johnson-lindenstrauss transform", "author": ["V. Braverman", "R. Ostrovsky", "Y. Rabani"], "venue": "ArXiv e-prints,", "citeRegEx": "Braverman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Braverman et al\\.", "year": 2010}, {"title": "An introduction to compressive sampling", "author": ["Emmanuel J. Cand\u00e8s", "Michael B. Wakin"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Cand\u00e8s and Wakin.,? \\Q2008\\E", "shortCiteRegEx": "Cand\u00e8s and Wakin.", "year": 2008}, {"title": "Prediction, Learning, and Games", "author": ["Nicolo Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "Random projection trees and low dimensional manifolds", "author": ["Sanjoy Dasgupta", "Yoav Freund"], "venue": "In Proceedings of the 40th annual ACM symposium on Theory of computing,", "citeRegEx": "Dasgupta and Freund.,? \\Q2008\\E", "shortCiteRegEx": "Dasgupta and Freund.", "year": 2008}, {"title": "Compressed sensing", "author": ["David L. Donoho"], "venue": "IEEE Transaction on Information Theory,", "citeRegEx": "Donoho.,? \\Q2006\\E", "shortCiteRegEx": "Donoho.", "year": 2006}, {"title": "Relative-error cur matrix decompositions", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Drineas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2008}, {"title": "Random projection for high dimensional data clustering: a cluster ensemble approach", "author": ["Xiaoli Zhang Fern", "Carla E. Brodley"], "venue": "In Proceedings of the 20th International Conference on Machine Learning,", "citeRegEx": "Fern and Brodley.,? \\Q2003\\E", "shortCiteRegEx": "Fern and Brodley.", "year": 2003}, {"title": "Experiments with random projections for machine learning", "author": ["Dmitriy Fradkin", "David Madigan"], "venue": "In Proceedings of the 9th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Fradkin and Madigan.,? \\Q2003\\E", "shortCiteRegEx": "Fradkin and Madigan.", "year": 2003}, {"title": "Learning the structure of manifolds using random projections", "author": ["Yoav Freund", "Sanjoy Dasgupta", "Mayank Kabra", "Nakul Verma"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Freund et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Freund et al\\.", "year": 2008}, {"title": "Tail bounds for all eigenvalues of a sum of random matrices", "author": ["Alex Gittens", "Joel A. Tropp"], "venue": "CoRR, abs/1104.4513v2,", "citeRegEx": "Gittens and Tropp.,? \\Q2011\\E", "shortCiteRegEx": "Gittens and Tropp.", "year": 2011}, {"title": "Face recognition experiments with random projection", "author": ["Navin Goel", "George Bebis", "Ara Nefian"], "venue": "In Proceedings of SPIE,", "citeRegEx": "Goel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Goel et al\\.", "year": 2005}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Guyon and Elisseeff.,? \\Q2003\\E", "shortCiteRegEx": "Guyon and Elisseeff.", "year": 2003}, {"title": "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization", "author": ["Elad Hazan", "Satyen Kale"], "venue": "In Proceedings of the 24th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Hazan and Kale.,? \\Q2011\\E", "shortCiteRegEx": "Hazan and Kale.", "year": 2011}, {"title": "Beating sgd: Learning svms in sublinear time", "author": ["Elad Hazan", "Tomer Koren", "Nati Srebro"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Hazan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2011}, {"title": "Dimensionality reduction by random mapping: fast similarity computation for clustering", "author": ["Samuel Kaski"], "venue": "In Proceedings of the 1998 IEEE International Joint Conference on Neural Networks,", "citeRegEx": "Kaski.,? \\Q1998\\E", "shortCiteRegEx": "Kaski.", "year": 1998}, {"title": "Dense fast random projections and lean walsh transforms", "author": ["Edo Liberty", "Nir Ailon", "Amit Singer"], "venue": "Proceedings of the 12th International Workshop on Randomization and Computation (RANDOM),", "citeRegEx": "Liberty et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Liberty et al\\.", "year": 2008}, {"title": "Linear regression with random projections", "author": ["Oldalric-Ambrym Maillard", "Remi Munos"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maillard and Munos.,? \\Q2012\\E", "shortCiteRegEx": "Maillard and Munos.", "year": 2012}, {"title": "Randomized Algorithms", "author": ["Rajeev Mowani", "Prabhakar Raghavan"], "venue": null, "citeRegEx": "Mowani and Raghavan.,? \\Q1995\\E", "shortCiteRegEx": "Mowani and Raghavan.", "year": 1995}, {"title": "Smooth minimization of non-smooth functions", "author": ["Yu. Nesterov"], "venue": "Mathematical Programming,", "citeRegEx": "Nesterov.,? \\Q2005\\E", "shortCiteRegEx": "Nesterov.", "year": 2005}, {"title": "Random features for large-scale kernel machines", "author": ["Ali Rahimi", "Benjamin Recht"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Rahimi and Recht.,? \\Q2008\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2008}, {"title": "Online learning meets optimization in the dual", "author": ["Shai Shalev-Shwartz", "Yoram Singer"], "venue": "In Proceedings of 19th Annual Conference on Learning Theory (COLT),", "citeRegEx": "Shalev.Shwartz and Singer.,? \\Q2006\\E", "shortCiteRegEx": "Shalev.Shwartz and Singer.", "year": 2006}, {"title": "Is margin preserved after random projection", "author": ["Qinfeng Shi", "Chunhua Shen", "Rhys Hill", "Anton van den Hengel"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Shi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shi et al\\.", "year": 2012}, {"title": "The Random Projection Method", "author": ["Santosh S. Vempala"], "venue": "American Mathematical Society,", "citeRegEx": "Vempala.,? \\Q2004\\E", "shortCiteRegEx": "Vempala.", "year": 2004}], "referenceMentions": [{"referenceID": 1, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.", "startOffset": 109, "endOffset": 237}, {"referenceID": 30, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.", "startOffset": 109, "endOffset": 237}, {"referenceID": 15, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.", "startOffset": 109, "endOffset": 237}, {"referenceID": 2, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.", "startOffset": 109, "endOffset": 237}, {"referenceID": 5, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.", "startOffset": 109, "endOffset": 237}, {"referenceID": 27, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al.", "startOffset": 109, "endOffset": 237}, {"referenceID": 24, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al., 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al.", "startOffset": 250, "endOffset": 298}, {"referenceID": 13, "context": "Introduction Random projection has been widely used in many machine learning tasks, including classification (Arriaga and Vempala, 1999; Vempala, 2004; Fradkin and Madigan, 2003; Balcan and Blum, 2005; Blum, 2006; Rahimi and Recht, 2008), regression (Maillard and Munos, 2012; Drineas et al., 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al.", "startOffset": 250, "endOffset": 298}, {"referenceID": 22, "context": ", 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al., 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.", "startOffset": 20, "endOffset": 81}, {"referenceID": 14, "context": ", 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al., 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.", "startOffset": 20, "endOffset": 81}, {"referenceID": 6, "context": ", 2008), clustering (Kaski, 1998; Fern and Brodley, 2003; Boutsidis et al., 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.", "startOffset": 20, "endOffset": 81}, {"referenceID": 22, "context": ", 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.", "startOffset": 34, "endOffset": 74}, {"referenceID": 4, "context": ", 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al.", "startOffset": 34, "endOffset": 74}, {"referenceID": 11, "context": ", 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al., 2008), and information retrieval (Goel et al.", "startOffset": 94, "endOffset": 142}, {"referenceID": 16, "context": ", 2010), dimensionality reduction (Kaski, 1998; Bingham and Mannila, 2001), manifold learning (Dasgupta and Freund, 2008; Freund et al., 2008), and information retrieval (Goel et al.", "startOffset": 94, "endOffset": 142}, {"referenceID": 18, "context": ", 2008), and information retrieval (Goel et al., 2005).", "startOffset": 35, "endOffset": 54}, {"referenceID": 19, "context": "This is particularly useful for feature selection (Guyon and Elisseeff, 2003), where important features are often selected based on their weights in the linear prediction model learned from the training data.", "startOffset": 50, "endOffset": 77}, {"referenceID": 26, "context": "For non differentiable loss functions such as hinge loss, we could apply the smoothing technique (Nesterov, 2005) to make it differentiable.", "startOffset": 97, "endOffset": 113}, {"referenceID": 1, "context": "(Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)).", "startOffset": 0, "endOffset": 92}, {"referenceID": 29, "context": "(Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)).", "startOffset": 0, "endOffset": 92}, {"referenceID": 3, "context": "(Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)).", "startOffset": 0, "endOffset": 92}, {"referenceID": 24, "context": "(Arriaga and Vempala, 1999; Shi et al., 2012; Balcan et al., 2006; Maillard and Munos, 2012)).", "startOffset": 0, "endOffset": 92}, {"referenceID": 9, "context": "Relationship to Compression Sensing The proposed problem is closely related to compressive sensing (Cand\u00e8s and Wakin, 2008; Donoho, 2006) where the goal is to recover a high dimensional but sparse vector using a small number of random projections.", "startOffset": 99, "endOffset": 137}, {"referenceID": 12, "context": "Relationship to Compression Sensing The proposed problem is closely related to compressive sensing (Cand\u00e8s and Wakin, 2008; Donoho, 2006) where the goal is to recover a high dimensional but sparse vector using a small number of random projections.", "startOffset": 99, "endOffset": 137}, {"referenceID": 7, "context": "We note that although dual variables have been widely used in the analysis of convex optimization (Boyd and Vandenberghe, 2004; Hazan et al., 2011) and online learning (Shalev-Shwartz and Singer, 2006), to the best of our knowledge, this is the first time that dual variables have been used in conjunction with random projection for recovering optimal solutions.", "startOffset": 98, "endOffset": 147}, {"referenceID": 21, "context": "We note that although dual variables have been widely used in the analysis of convex optimization (Boyd and Vandenberghe, 2004; Hazan et al., 2011) and online learning (Shalev-Shwartz and Singer, 2006), to the best of our knowledge, this is the first time that dual variables have been used in conjunction with random projection for recovering optimal solutions.", "startOffset": 98, "endOffset": 147}, {"referenceID": 28, "context": ", 2011) and online learning (Shalev-Shwartz and Singer, 2006), to the best of our knowledge, this is the first time that dual variables have been used in conjunction with random projection for recovering optimal solutions.", "startOffset": 28, "endOffset": 61}, {"referenceID": 0, "context": ", (Achlioptas, 2003; Liberty et al., 2008; Braverman et al., 2010).", "startOffset": 2, "endOffset": 66}, {"referenceID": 23, "context": ", (Achlioptas, 2003; Liberty et al., 2008; Braverman et al., 2010).", "startOffset": 2, "endOffset": 66}, {"referenceID": 8, "context": ", (Achlioptas, 2003; Liberty et al., 2008; Braverman et al., 2010).", "startOffset": 2, "endOffset": 66}, {"referenceID": 20, "context": "We also note that Algorithm 2 is related to the Epoch gradient descent algorithm (Hazan and Kale, 2011) for stochastic optimization in that the solution obtained in the previous iteration is served as the center to the optimization problem of the current iteration.", "startOffset": 81, "endOffset": 103}, {"referenceID": 20, "context": "Unlike the algorithm in (Hazan and Kale, 2011), we do not shrink the domain size over the iterations in Algorithm 2.", "startOffset": 24, "endOffset": 46}, {"referenceID": 25, "context": "Following the same arguments as compressive sensing, it may be possible to argue that \u03a9(r log r) is optimal due to the result of coupon collector\u2019s problem (Mowani and Raghavan, 1995), although the rigorous analysis remains to be developed.", "startOffset": 156, "endOffset": 183}, {"referenceID": 5, "context": "We note that Theorem 4 directly implies the result of margin classification error for random projection (Blum, 2006).", "startOffset": 104, "endOffset": 116}, {"referenceID": 5, "context": "Theorem 8 (Theorem 2 (Blum, 2006)) Let x \u2208 R, and x\u0302 = S\u22a4x/\u221am, where S \u2208 Rd\u00d7m is a random matrix whose entries are chosen independently from N (0, 1).", "startOffset": 21, "endOffset": 33}], "year": 2017, "abstractText": "In this work, we address the problem of how to recover the optimal solution to the optimization problem related to high dimensional data classification using random projection, to which we refer as Recovery of Optimal Solution. This is in contrast to the previous studies that were focused on analyzing the classification performance using random projection. We reveal the relationship between compressive sensing and the problem of recovering optimal solution using random projection. We also present a simple algorithm, termed as Dual Random Projection, that recovers the optimal solution with a small error by computing dual solution provided that the data matrix is of low rank.", "creator": "LaTeX with hyperref package"}}}