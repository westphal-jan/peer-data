{"id": "1211.3955", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Nov-2012", "title": "On Calibrated Predictions for Auction Selection Mechanisms", "abstract": "Calibration is a basic property for prediction systems, and algorithms for achieving it are well-studied in both statistics and machine learning. In many applications, however, the predictions are used to make decisions that select which observations are made. This makes calibration difficult, as adjusting predictions to achieve calibration changes future data. We focus on click-through-rate (CTR) prediction for search ad auctions. Here, CTR predictions are used by an auction that determines which ads are shown, and we want to maximize the value generated by the auction.", "histories": [["v1", "Fri, 16 Nov 2012 17:07:33 GMT  (18kb)", "http://arxiv.org/abs/1211.3955v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["h brendan mcmahan", "omkar muralidharan"], "accepted": false, "id": "1211.3955"}, "pdf": {"name": "1211.3955.pdf", "metadata": {"source": "CRF", "title": "On Calibrated Predictions for Auction Selection Mechanisms", "authors": ["H. Brendan McMahan", "Omkar Muralidharan"], "emails": ["mcmahan@google.com", "omuralidharan@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 121 1.39 55v1 [cs.GT] 1 6We show that certain natural notions of calibration are impossible to achieve depending on the details of the auction. We also show that it can be impossible to maximize auction efficiency while using calibrated predictions. Finally, we specify conditions under which calibration is possible while maximizing auction efficiency: Broadly speaking, bids and queries must not contain information about CTRs that are not already covered by the predictions."}, {"heading": "1 Introduction", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 Problem Formalization", "text": "The interaction of calibration and selection has received little direct attention in the literature, so the construction of a suitable model requires some care: we need a formula that is theoretically comprehensible but nevertheless captures the most important characteristics of the real-world problems of interest. We start by defining our prediction units (queries and displays) and applying the mechanism used to select them (auctions). We assume that we can use a fixed, existing prediction system for each ad; our study will then include prediction systems, functions that attempt to map these raw predictions for calibrated probabilities. Once this framework is established, we can formally specify the questions we study.We model the interaction between the users of a search engine and the advertising system. There is a fixed finite set of predictions such as \"flowers\" or \"car insurance\" that are entered into the search engine."}, {"heading": "5 Sufficient Conditions", "text": "In this section, we present other assumptions that are sufficient to ensure the distribution of these expectations."}, {"heading": "5.1 Properties that Imply Nice Maps Exist", "text": "First, we show that we are equal under the mechanism ALL, Prop E2 and Prop SI; then we assume that Prop E2 (and therefore Prop SI) implies a nice problem. (Suppose Prop E2 has such a problem. (Suppose Prop E2 has such a problem.) (Suppose Prop E2 has such a problem. (4) Then we must show all candidates with a given (z, b) combination, or none of them. (So, for each f in which Prf (z, b) > 0, we must haveEf [p, b] = EC, b]. (4) Then, assuming Ef [p, z] is defined, Ef [p, z] = Ef [p, b] = Ef [p, b] = Ef [z] = Ef [p, z]."}, {"heading": "5.2 Negative Results", "text": "We show several negative results in relation to the assumptions taken into account in the previous paragraph.ONE and ALL: Prop SI = > E1 does not mean Prop E1 Consider an example with two queries, each of which is equally probable. Each query has two candidates, since the following (p, b) tuples (they all have a common z): q1 q2 A (0,1, 1) C (0,1, 2) B (0,2, 2) D (0,2, 1) Due to the symmetry between these queries, under each f (and each selection mechanism), ad A must show with the same probability as ad D, as ads B and C. Therefore, for each f, Ef [p | b = 1, z] = 0,15 and similar Ef [p | b = 2, z] = 0,15 we must show that selection invariance is as probable as Prop E2, as Prop E2."}, {"heading": "6 Discussion and Future Work", "text": "Our sufficient conditions are quite strong, but not unrealistic; they require that the bid and query do not add information about the CTR, depending on the raw prediction. CTR estimation systems typically use queries as characteristics (e.g. [13]), so it is reasonable to hope that the query does not add additional information. Bids are set by advertisers for query and ad pairs that are already used by CTR estimation systems, so systematic patterns are likely to be considered in tenders. Since advertisers have much less information than the auctioneer, it seems unlikely that they will be able to add additional information about CTRs through fine-grained bid manipulation. We can test whether our sufficient conditions are met through randomized experiments that alter the mix of ads shown. Since randomized predictions generally cannot lead to maximum efficiency, it is natural to consider deterministic prediction cards first."}], "references": [{"title": "Verification of forecasts expressed in terms of probability", "author": ["Glenn W. Brier"], "venue": "Monthly Weather Review,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1950}, {"title": "An empirical comparison of supervised learning algorithms", "author": ["Rich Caruana", "Alexandru Niculescu-Mizil"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "An empirical comparison of supervised learning algorithms", "author": ["Rich Caruana", "Alexandru Niculescu-Mizil"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Properties and benefits of calibrated classifiers", "author": ["Ira Cohen", "Moises Goldszmidt"], "venue": "European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD). Springer,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "The well-calibrated Bayesian", "author": ["A. Dawid"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1982}, {"title": "The comparison and evaluation of forecasters", "author": ["Morris H. DeGroot", "Stephen E. Fienberg"], "venue": "The Statistician,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1983}, {"title": "Internet advertising and the generalized second-price auction: Selling billions of dollars worth of keywords", "author": ["Benjamin Edelman", "Michael Ostrovsky", "Michael Schwarz"], "venue": "American Economic Review,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2007}, {"title": "A proof of calibration via blackwell\u2019s approachability theorem", "author": ["Dean P. Foster"], "venue": "Games and Economic Behavior,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Strictly proper scoring rules, prediction, and estimation", "author": ["Tilmann Gneiting", "Adrian Es Raftery"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2007}, {"title": "Probabilistic forecasts, calibration and sharpness", "author": ["Tilmann Gneiting", "Fadoua Balabdaoui", "Adrian E. Raftery"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft\u2019s bing search engine", "author": ["Thore Graepel", "Joaquin Qui\u00f1onero Candela", "Thomas Borchert", "Ralf Herbrich"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Predicting good probabilities with supervised learning", "author": ["Niculescu-Mizil", "Alexandru", "Rich Caruana"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Predicting good probabilities with supervised learning", "author": ["Alexandru Niculescu-Mizil", "Rich Caruana"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers", "author": ["John C. Platt"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Combining probability forecasts", "author": ["Roopesh Ranjan", "Tilmann Gneiting"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["Bianca Zadrozny", "Charles Elkan"], "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}], "referenceMentions": [{"referenceID": 7, "context": "This auction selection mechanism has been extensively studied, and has many nice properties [19, 8].", "startOffset": 92, "endOffset": 99}, {"referenceID": 11, "context": ", [13]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "Much of the earliest work is in the probabilistic forecasting literature [1, 6, 18].", "startOffset": 73, "endOffset": 83}, {"referenceID": 5, "context": "Much of the earliest work is in the probabilistic forecasting literature [1, 6, 18].", "startOffset": 73, "endOffset": 83}, {"referenceID": 15, "context": "Much of the earliest work is in the probabilistic forecasting literature [1, 6, 18].", "startOffset": 73, "endOffset": 83}, {"referenceID": 6, "context": "Calibration is particularly important when comparing predictors, since two sets of calibrated predictions can be fairly evaluated by how concentrated they are on observed outcomes [7, 12, 11].", "startOffset": 180, "endOffset": 191}, {"referenceID": 10, "context": "Calibration is particularly important when comparing predictors, since two sets of calibrated predictions can be fairly evaluated by how concentrated they are on observed outcomes [7, 12, 11].", "startOffset": 180, "endOffset": 191}, {"referenceID": 9, "context": "Calibration is particularly important when comparing predictors, since two sets of calibrated predictions can be fairly evaluated by how concentrated they are on observed outcomes [7, 12, 11].", "startOffset": 180, "endOffset": 191}, {"referenceID": 4, "context": "For example, it is easier to threshold the output of a calibrated classifier to minimize weighted classification error [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 13, "context": "For example, boosted trees are uncalibrated, but become excellent probability estimators after calibration [16, 2].", "startOffset": 107, "endOffset": 114}, {"referenceID": 1, "context": "For example, boosted trees are uncalibrated, but become excellent probability estimators after calibration [16, 2].", "startOffset": 107, "endOffset": 114}, {"referenceID": 14, "context": "The two most common methods for calibration are Platt scaling, which is equivalent to logistic regression, and isotonic regression [17, 20, 15, 3].", "startOffset": 131, "endOffset": 146}, {"referenceID": 16, "context": "The two most common methods for calibration are Platt scaling, which is equivalent to logistic regression, and isotonic regression [17, 20, 15, 3].", "startOffset": 131, "endOffset": 146}, {"referenceID": 12, "context": "The two most common methods for calibration are Platt scaling, which is equivalent to logistic regression, and isotonic regression [17, 20, 15, 3].", "startOffset": 131, "endOffset": 146}, {"referenceID": 2, "context": "The two most common methods for calibration are Platt scaling, which is equivalent to logistic regression, and isotonic regression [17, 20, 15, 3].", "startOffset": 131, "endOffset": 146}, {"referenceID": 8, "context": "However, if the system is allowed to use randomness (that is, predict a distribution), then calibration can be achieved ([9, 10] and [4, Sec 4.", "startOffset": 121, "endOffset": 128}, {"referenceID": 0, "context": ",K} \u2192 [0, 1].", "startOffset": 6, "endOffset": 12}, {"referenceID": 0, "context": "Formally, define T : [0, 1] \u2192 [0, 1] (a function from prediction maps to prediction maps) by T (f) = f \u2032 where", "startOffset": 21, "endOffset": 27}, {"referenceID": 0, "context": "Formally, define T : [0, 1] \u2192 [0, 1] (a function from prediction maps to prediction maps) by T (f) = f \u2032 where", "startOffset": 30, "endOffset": 36}, {"referenceID": 0, "context": "Thus, choosing prediction map reduces to choosing a single value p\u0302 \u2208 [0, 1].", "startOffset": 70, "endOffset": 76}, {"referenceID": 11, "context": ", [13]), so it is reasonable to hope that the query does not add extra information.", "startOffset": 2, "endOffset": 6}], "year": 2012, "abstractText": "Calibration is a basic property for prediction systems, and algorithms for achieving it are well-studied in both statistics and machine learning. In many applications, however, the predictions are used to make decisions that select which observations are made. This makes calibration difficult, as adjusting predictions to achieve calibration changes future data. We focus on click-through-rate (CTR) prediction for search ad auctions. Here, CTR predictions are used by an auction that determines which ads are shown, and we want to maximize the value generated by the auction. We show that certain natural notions of calibration can be impossible to achieve, depending on the details of the auction. We also show that it can be impossible to maximize auction efficiency while using calibrated predictions. Finally, we give conditions under which calibration is achievable and simultaneously maximizes auction efficiency: roughly speaking, bids and queries must not contain information about CTRs that is not already captured by the predictions.", "creator": "LaTeX with hyperref package"}}}