{"id": "1211.4488", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Nov-2012", "title": "A Rule-Based Approach For Aligning Japanese-Spanish Sentences From A Comparable Corpora", "abstract": "The performance of a Statistical Machine Translation System (SMT) system is proportionally directed to the quality and length of the parallel corpus it uses. However for some pair of languages there is a considerable lack of them. The long term goal is to construct a Japanese-Spanish parallel corpus to be used for SMT, whereas, there are a lack of useful Japanese-Spanish parallel Corpus. To address this problem, In this study we proposed a method for extracting Japanese-Spanish Parallel Sentences from Wikipedia using POS tagging and Rule-Based approach. The main focus of this approach is the syntactic features of both languages. Human evaluation was performed over a sample and shows promising results, in comparison with the baseline.", "histories": [["v1", "Mon, 19 Nov 2012 16:38:32 GMT  (135kb)", "http://arxiv.org/abs/1211.4488v1", "International Journal on Natural Language Computing (IJNLC) Vol.1, No.3, October 2012"]], "COMMENTS": "International Journal on Natural Language Computing (IJNLC) Vol.1, No.3, October 2012", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["jessica c ram\\'irez", "yuji matsumoto"], "accepted": false, "id": "1211.4488"}, "pdf": {"name": "1211.4488.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["COMPARABLE CORPORA", "Jessica C. Ram\u00edrez", "Yuji Matsumoto"], "emails": ["Jessicrv1@yahoo.com.mx", "matsu@is.naist.jp"], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijnlc.2012.1301 1The performance of a statistical machine translation (SMT) system depends proportionally on the quality and length of the parallel corpus used, but some language pairs are significantly lacking. The long-term goal is to construct a Japanese-Spanish parallel corpus that can be used for SMT, while there is a lack of useful Japanese-Spanish parallel corpus. To solve this problem, in this study we proposed a method for extracting Japanese-Spanish parallel sentences from Wikipedia using POS tagging and a rules-based approach. The focus of this approach is on the syntactic characteristics of both languages. Human evaluation was conducted via a random sample and shows promising results compared to the principles. KEYWORDSComparable Corpora, POS tagging, Sentences Aligment, Machine Translation"}, {"heading": "1. INTRODUCTION", "text": "Much research in recent years has focused on the construction of semi-automatic and automatically aligned data resources, which are essential for many natural language processing tasks, but some language pairs still lack annotated data. Manually constructing parallel corpus requires high-quality translators, and is time-consuming and costly. Given the prevalence of the Internet and the vast amount of data, a number of researchers have proposed using the World Wide Web as a large-scale corpus [5]. However, due to redundancy and ambiguous information on the Web, we need to find methods to extract only the information that is useful for a given assignment.4] In this study, we propose an approach to extract parallel sentences from a comparable corpus, because despite two documents referencing the same topic, it may be possible that both documents do not have a single sentence in common. In this study, we propose an approach for extracting parallel sentences from a Japanese Wikipedia lexicon, using a Japanese Wikipedia lexicon."}, {"heading": "2. RELATED WORKS", "text": "The use of Wikipedia as a data resource in NLP is relatively new, and therefore research is relatively limited, but there is work showing promising results. [2] Attempts to extract the aforementioned entities from Wikipedia and present two disambiguation methods using cosine similarity and SVM. First, the identification of a named entity from Wikipedia using IE technology and the disambiguization between multiple entities using cosine similarity of context articles and the \"Babel-MT-System of Altavista1\" to align sentences. The second approach, the link-based bilingual lexicon, uses two approaches to sentence similarity in Wikipedia. First, they introduced an MT-based approach using Jaccard similarity and the \"Babel-MT-System of Altavista1\" to align sentences."}, {"heading": "3. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Comparable Corpora2", "text": "A comparable corpus is a collection of texts on a specific topic in two or more languages. For example, The \"Yomiuri Shimbun3 corpora,\" a corpus extracted from their daily news in both English and Japanese. Although the messages in both languages are the same, they do not make a correct translation of the content. Comparable corpus are used for many NLP tasks such as: information retrieval, machine translation, bilingual lexicon extraction, etc. In languages with scarce resources, comparable corpus are a first-order alternative in NLP research."}, {"heading": "3.2. Wikipedia", "text": "Wikipedia4 is a multilingual web-based encyclopedia with articles on a wide range of topics, in which texts are aligned in different languages.Wikipedia is the successor to Nupedia- an online encyclopedia written by experts in different fields, which no longer exist.Wikipedia was created as a monolingual project (English) in January 2001 to support Wikipedia, differs from Nupedia mainly in that it can be written by anyone, and authors do not need to be experts in the field that is being writed.1 Babel, is a multilingual online translation system 2 Corpora is a plural of Corpus.3) Wikipedia4 is a Japanese newspaper with an English version. http: / / www.yomiuri.co.jp / 4 http: / en.wikipedia.org / wiki / Main _ PageWikipedia is a multilingual translation system 2 Corpora is a plurality of Corpus.3) Wikipedia4 is a Japanese newspaper with an English version."}, {"heading": "3.2.1. Redirect pages", "text": "The redirect page is a very suitable resource for eliminating redundant articles. This means avoiding the existence of two articles that relate to the same subject, in the case of: \u2022 synonyms such as \"altruism,\" which redirects to \"selflessness\" \u2022 abbreviations such as \"USA,\" which are redirected to \"United States of America\" \u2022 spelling variations such as \"color,\" which are redirected to \"color\" \u2022 nicknames and pseudonyms such as \"Einstein,\" which are redirected to \"Albert Einstein.\""}, {"heading": "3.2.2. Disambiguation pages", "text": "Ambiguities are pages that contain the list of different meanings of a word."}, {"heading": "3.2.3. Hyperlinks", "text": "Articles contain words or entities that contain an article about them, so when a user clicks on the link, they will be redirected to an article about that word."}, {"heading": "3.2.4. Category pages", "text": "Category pages are pages without articles that list members of a particular category and their subcategories. These pages have titles that start with \"category:\" and are followed by the name of the respective category. Categorization is a Wikipedia project that tries to assign a category to each article. Wikipedians assign the category manually and therefore not all pages have a category article.Some articles belong to several categories. For example, the article \"Dominican Republic\" belongs to three categories such as: \"Dominican Republic,\" \"Island Countries\" and \"Spanish-speaking Countries.\" Thus, the article \"Dominican Republic\" appears on three different category pages."}, {"heading": "4. Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. General Description", "text": "Figure 1 shows a general overview of the methodology. First, we extract all aligned links from Wikipedia, i.e. Wikipedia article titles. We extract the Japanese and Spanish on the same topic. Then, we eliminate the unnecessary data (pre-processing). Divided into sentences. After extracting these articles, use a POS tagger to add the lexical category to each word in a particular sentence. Select the sentences that match according to their lexical category. Use the dictionaries to make a word for word translation."}, {"heading": "4.2. Dictionary Extraction from Wikipedia", "text": "The aim of this phase is to acquire Japanese-Spanish-English tuples of Wikipedia article titles in order to acquire translations. Wikipedia provides links to corresponding articles in different languages in each article. Each Wikipedia article page has several boxes on the left with the caption: \"Navigation,\" \"Search,\" \"Toolbox\" and finally \"Other Languages.\" This contains a list of all languages available for this article, although the article does not have exactly the same content in each language. In most cases, English articles are longer or have more information than the same article in other languages, since most Wikipedia staff are native English speakers."}, {"heading": "4.2.1. Methodology", "text": "Let's take all articles that are nouns or named entities, and look for the field \"In other languages\" in the contents of the articles. Check if it has at least one link. If the box exists, it redirects to the same article in other languages. Extract the words in these other languages and match it with the original title of the article. For example, the Spanish article entitled \"econom\u00eda\" (Economy) is translated into Japanese as \"keizaigaku.\" If we click on Spanish or Japanese in the other language, we will get an article on the same topic in the other language that the translation gives us."}, {"heading": "4.3. Extract Japanese and Spanish articles", "text": "We used the Japanese-Spanish dictionary (4.2.) to select articles with links in Japanese and Spanish."}, {"heading": "4.4. Pre-processing", "text": "We remove the irrelevant information from Wikipedia articles to make processing easier and faster. Steps are as follows: 1. Remove all irrelevant information from pages, such as images, menus, characters such as \"(),\" \"& quot,\" \"*,\" etc... 2. Check if a link is a redirected article and extract the original article 3. Remove all stopwords - generic words that do not provide information about a specific topic, such as \"that,\" \"between,\" \"an,\" etc."}, {"heading": "4.5. Spliting into Sentences and POS tagging", "text": "For the splitting of sentences in the Spanish articles, we used NLTK toolkit5, a well-known platform for creating Python scripts. For the splitting of Spanish sentences, we used FreeLing6, an open source suit for language analysts specializing in the Spanish language.5 http: / / nltk.org / 6 http: / / nlp.lsi.upc.edu / freeling / For splitting sentences to add words and a word category, we used MeCab7, a part-of-speech and morphological analyzer for Japanese."}, {"heading": "4.6. Constructing the Rules", "text": "Japanese is a subject-object-verb language, while Spanish is a subject-verb-object language.Figure 2 shows the basic sequence of sentences in both Japanese and Spanish, using the phrase \"The dog drinks water\" as an example.Table 1 shows some of the rules used for this thesis. These rules are created taking into account the morphological and syntactical characteristics of each language. In Japanese, for example, there are no genders for the adjectives, while in Spanish it is indispensable.7 http: / / cl.naist.jp / ~ eric-n / ubuntu-nlp / dists / hardy / Japanese /"}, {"heading": "5. Experimental Evaluation", "text": "To evaluate the proposed method, we took a sample of 20 random Japanese and Spanish articles. These experiments were based on two approaches: the hyperlink approach [1] as the starting point and the rules-based approach. In order to match other grammatical forms such as verbs, adjectives, etc., we need a different dictionary. Table 2 shows the result achieved with both the baseline [1] and our approach. In column 1, the \"correct identification\" shows that the sentences with the high values and alignment were correct. \"Partial matching\" refers to the sentences in both the source and target languages with a noun phrase in Komun. Finally, \"incorrect identification\" refers to sentences with the higher score. However, there was not even one word that matched the sentences."}, {"heading": "5.2. Discussion", "text": "We noticed that in the sentences the first part of the article. It is usually the definition of the title of the article, have a more correct identification, both approaches. Overall rule-based approach is better than the baseline. This is because when in a particular sentence, when the hyperlink word is or the title of the article is repeated, automatically the best score, even if it is redundant. Some identification is not as good as expected, because we need to add more rules, It can be manual or by bootstrapping methods, which is very interesting for future work. We have noticed that with this method it is possible to construct new sentences, even if they are not in both articles."}, {"heading": "6. CONCLUSIONS AND FUTURE WORKS", "text": "This study focuses on the alignment of Japanese-Spanish sentences using a rules-based approach. We have demonstrated the feasibility of Wikipedia's multi-language alignment functions. We have used POS and rules for aligning sentences in both the source and target articles. 8 Wikipedia data is constantly increasing. 9 http: / / aulex.org / ja-es / The same method can be applied to any language pair in Wikipedia and any other type of comparable corporation. In future work, we will examine the use of English as a pivot and fishing language and the automatic construction on a corpus through translation."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We would like to thank Yuya R. for her contribution and helpful comments."}], "references": [{"title": "Finding Similar Sentences across Multiple Languages in Wikipedia", "author": ["Adafre", "Sisay F", "De Rijke", "Maarten"], "venue": "In Proceeding of EACL-06,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Using Encyclopedic Knowledge for Named Entity Disambiguation", "author": ["Bunescu", "Razvan", "Pasca", "Marius"], "venue": "In Proceeding of EACL-06,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Japanese-Spanish Thesaurus Construction Using English as a Pivot", "author": ["Ram\u00edrez", "Jessica", "Asahara", "Masayuki", "Matsumoto", "Yuji"], "venue": "In Proceeding of The Third International Joint Conference on Natural Language Processing (IJCNLP),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "A Bean-Search Extraction Algorithm for Comparable Data", "author": ["Tillman", "Christoph"], "venue": "In Proceeding of ACL,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "A Simple Sentence-Level Extraction Algorithm for Comparable Data", "author": ["Tillman", "Christoph", "Xu", "Jian-Ming"], "venue": "Proceeding of HLT/NAACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] attempts to extract the named entities from Wikipedia and presents two disambiguation methods using cosine similarity and SVM.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Such work that is directly related to this research is [1] .", "startOffset": 55, "endOffset": 58}, {"referenceID": 0, "context": "This experiments were based on two approaches: the hyperlink approach [1] as a baseline and the Rule-Based approach.", "startOffset": 70, "endOffset": 73}, {"referenceID": 0, "context": "Table 2 shows the result obtained both with the baseline [1] and our approach.", "startOffset": 57, "endOffset": 60}], "year": 2012, "abstractText": "The performance of a Statistical Machine Translation System (SMT) system is proportionally directed to the quality and length of the parallel corpus it uses. However for some pair of languages there is a considerable lack of them. The long term goal is to construct a Japanese-Spanish parallel corpus to be used for SMT, whereas, there are a lack of useful Japanese-Spanish parallel Corpus. To address this problem, In this study we proposed a method for extracting Japanese-Spanish Parallel Sentences from Wikipedia using POS tagging and Rule-Based approach. The main focus of this approach is the syntactic features of both languages. Human evaluation was performed over a sample and shows promising results, in comparison with the baseline.", "creator": "Microsoft Office Word"}}}