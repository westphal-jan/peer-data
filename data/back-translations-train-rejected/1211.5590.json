{"id": "1211.5590", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Nov-2012", "title": "Theano: new features and speed improvements", "abstract": "Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.", "histories": [["v1", "Fri, 23 Nov 2012 20:42:41 GMT  (44kb,D)", "http://arxiv.org/abs/1211.5590v1", "Presented at the Deep Learning Workshop, NIPS 2012"]], "COMMENTS": "Presented at the Deep Learning Workshop, NIPS 2012", "reviews": [], "SUBJECTS": "cs.SC cs.LG", "authors": ["fr\\'ed\\'eric bastien", "pascal lamblin", "razvan pascanu", "james bergstra", "ian goodfellow", "arnaud bergeron", "nicolas bouchard", "david warde-farley", "yoshua bengio"], "accepted": false, "id": "1211.5590"}, "pdf": {"name": "1211.5590.pdf", "metadata": {"source": "CRF", "title": "Theano: new features and speed improvements", "authors": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio"], "emails": ["nouiz@nouiz.org", "lamblinp@iro.umontreal.ca", "r.pascanu@gmail.com", "james.bergstra@gmail.com", "goodfeli@iro.umontreal.ca", "bergearn@iro.umontreal.ca", "nicolas.bouchard.1@gmail.com", "wardefar@iro.umontreal.ca", "yoshua.bengio@umontreal.ca"], "sections": [{"heading": "1 Introduction", "text": "Theano was introduced to the machine learning community by Bergstra et al. (2010) as a mathematical compiler for the CPU and GPU, demonstrating how it can be used to symbolically define mathematical functions, automatically derive gradient expressions, and compile these expressions into executable functions that exceed implementations with other existing tools. Bergstra et al. (2011) then demonstrated how Theano could be used to implement deep learning models. In Section 2, we will briefly outline the main goals and properties of Theano. Section 3 will present some of the new features and measures available to speed up Theano's implementations. Section 4 compares Theano's performance with that of Torch7 (Collobert et al., 2011) on neural network benchmarks and RNLM (Mikolov et al., 2011) on recurring neural network benchmarks."}, {"heading": "2 Main features of Theano", "text": "Here we briefly summarize Theanos main features and advantages for machine learning tasks. Bergstra et al. (2010, 2011) and Theanos Website1 have more detailed descriptions and examples."}, {"heading": "2.1 Symbolic Mathematical Expressions", "text": "Theano includes powerful tools to manipulate and optimize graphs that represent symbolic mathematical expressions. In particular, Theano's optimization constructs can eliminate duplicate or unnecessary calculations (e.g. by replacing x \u2212 x with 0, eliminating the need to calculate x at all), increase numerical stability (e.g. by replacing stable implementations of log (1 + x), while 1http: / / deeplearning.net / software / theano / ar Xiv: 121 1.55 90v1 [cs.SC] 2 is 3x tiny or log (sigmoid (x))), or increase speed (e.g. by using loop fusion to apply a sequence of scalar operations to all elements of an array in a single pass over the data).This graph representation also allows symbolic differentiation of mathematical expressions, allowing users to quickly prototype complex machine learning models using codients, without having to manually specify the set of R and 1."}, {"heading": "2.2 Fast to write and to execute", "text": "Theano's dependence on NumPy and SciPy (Jones et al., 2001) makes it easy to add an implementation for a mathematical operation, making effective use of the effort of its developers, and it is always possible to add a more optimized version, which may then be transparently replaced. Theano, for example, defines operations on sparse matrices using the sparse matrix types of SciPy to hold values. Some of these operations simply call SciPy's functions, others are reimplemented in C + + and use BLAS routines to increase speed."}, {"heading": "2.3 Parallelism on the GPU", "text": "Theano uses CUDA to define a class of n-dimensional (dense) arrays in GPU memory with Python binding. Theano also includes CUDA code generators for fast implementation of mathematical operations. Most of these operations are currently limited to dense arrays of floating-point numbers of single precision."}, {"heading": "2.4 Stability and community support", "text": "Theano's development team has increased its commitment to code quality and correctness as the use of Theano in university and industrial laboratories begins to spread: a full test suite runs every evening, with a shorter version running for each pull request, and the project regularly releases stable releases. There is also a growing community of users who ask and answer questions on the project's mailing lists every day."}, {"heading": "3 New features in Theano", "text": "This section presents features of Theano that have recently been developed or improved, some of which are completely new and extend the scenarios in which Theano can be used (in particular scan and the R operator); others aim to improve performance, especially the time not spent on the actual computation (such as the overhead of the Python interpreter), and the parallelism between the CPU and the GPU."}, {"heading": "3.1 Scan: Symbolic Loop in Theano", "text": "This year is the highest in the history of the country."}, {"heading": "3.2 R-operator for Hessian-Free optimization", "text": "The pipeline is based on the \"R-Operator,\" introduced by Pearlmutter (1994), which is a mathematical operator that efficiently calculates the \"Jacobian vector\" product using a function f (\u03b8), f: RM \u2192 RN, the current parameter configuration f: RM and a Vec operator \u03b3 RM, but the \"Jacobian vector\" product is efficient (axy f: RN), where the current configuration f: RM and a Vec vector efficiently calculates the \"Jacobian vector\" product. For the sake of completeness, we would like to mention that the \"R-Operator\" evaluates the direction derivative of f: \"Barcelona's lovector,\" and in the automatic differentiation community f: \"The Jacobian vector,\" only the direction derivative of f: \"Barcelona's lovector vector is efficient,\" and can be considered in the forward and backward modes. \""}, {"heading": "3.3 Lazy Evaluation, CVM", "text": "When a compiled theano function is called, a runtime machine orchestrates which operations to execute on which data and calls the corresponding functions in the correct order, which was previously implemented as a Python loop, calling either native Python functions or C functions provided via a Python module interface in a predetermined order (i.e., moving the graph forward from inputs to outputs).The main drawback of this approach is that it was impossible to implement a lazy evaluation in the computational graph. For example, the \"if-then-otherwise\" construct would always calculate the result of both the \"then\" and the \"otherwise\" branches, as well as the condition before updating its output value. A new runtime called \"VM\" (for \"virtual machine\" because it drives the execution of small code units) allows a lazy evaluation of such operations, which means that we only evaluate the two operations necessary for the computation."}, {"heading": "3.4 More operations implemented in C", "text": "To take full advantage of the existence of the CVM, we added new C implementations of existing operations (even if Python implementations were almost as efficient) to avoid context changes. For example, matrix vector point products on the CPU had previously led to a call to a SciPy function that enveloped the GEMV routine of BLAS."}, {"heading": "3.5 Better support for sparse matrices", "text": "In addition to the dense tensors, Theano supports sparse matrices based on SciPy's implementations of compressed sparse series (CSR) and compressed sparse columns (CSC). Support for efficient sparse operations, especially for calculating derivatives of sparse operations, has been greatly improved.The online documentation lists currently supported operations. Theano supports two types of gradient calculation using sparse matrices. \"Regular\" differentiation does not require that the sparse structure of a matrix is maintained at a given time, and therefore a sparse variable can have a dense gradient. \"Structured\" differentiation considers the sparse structure of a matrix to be permanent, and the gradient in relation to this matrix of the same sparse structure. 2http: / / deeplearning.net / software / theano / library / sparse /"}, {"heading": "3.6 Parallelism on CPU", "text": "In the past, no major effort was made to enable Theano to use multi-core CPU architectures for parallel execution; development efforts instead focused on GPU implementations and new automatic optimizations. Multi-core parallelism was therefore only available to companies that called for parallel BLAS implementation. Collobert et al. (2011) showed that using OpenMP to parallelise C implementation of CPU operations can bring significant speed improvements with relatively little development effort."}, {"heading": "3.7 Asynchronous function calls on GPU", "text": "When running the CUDA kernel on the GPU, the calling function call does not wait for the kernel to execute, but only for the kernel to run at some point in the future, allowing the main program to perform other tasks, including scheduling other GPU kernels. If the result of the GPU calculation is needed, the program can wait for the end of the kernel and return its result.Prior to the 0.6 release, Theano always waited for the result of the kernel calculation as soon as it was started, effectively preventing the execution of other operations on the CPU during this time.This approach facilitates profiling and debugging, as it is clear at all times which GPU kernel is running and retrieves error messages as quickly as possible; however, such an approach forbids the simultaneous use of CPU-based calculation and leaves an opportunity for further speed gains.Theano's new default behavior is that the result is that the GPU does not necessarily wait for the GPU to execute."}, {"heading": "4 Benchmarks", "text": "Bergstra et al. (2010) showed that Theano was faster than many other tools available at the time, including Torch5. The following year, Collobert et al. (2011) showed that Torch7 was faster than Theano for the same benchmarks. At this point, we briefly introduce Torch7 and evaluate the performance of their latest neural network versions against the above benchmarks. Subsequently, Section 4.3 will compare the performance of Theano with another package, RNNLM, in the formation of recurrent neural networks."}, {"heading": "4.1 Torch7", "text": "Torch7 (Collobert et al., 2011) is promoted as a Matlab-like machine learning environment that aims to simplify the development of numerical algorithms and allow them to be executed quickly while being easy to extended.Table 1 provides a summary comparison of the features and differences provided by Torch7 (including those inherited from Lua) and Theano (including those provided by Python and NumPy / SciPy). In this section, the common features and differences between Torch7 and Theano are outlined."}, {"heading": "4.1.1 Common features shared by Torch7 and Theano", "text": "Theano and Torch7 are two computational frameworks designed for the machine learning community to facilitate the rapid implementation and testing of new mathematical models and algorithms, without sacrificing the execution speed that a manually optimized implementation would offer. Both form the basis for machine learning of specific packages or projects, especially for neural networks and unattended learning. 3https: / / github.com / jaberg / DeepLearningBenchmarksLike Theano, Torch7 is based on a scripting language (Lua), uses highly optimized scientific computing libraries (such as BLAS and LAPACK for linear algebra computations) and internal modules written in C / C + +, for the sections where execution speed is critical. In addition, Torch7 has the ability to perform parallel computations on multi-core CPUs (via OpenMP) and on GPUs via 2007 CDA.Both have access to a match7 environment similar to the Match7 module from 2001."}, {"heading": "4.1.2 Main differences", "text": "Some of the strengths of Torch7 derive from the advantages of Lua over Python: less interpreter effort, easier integration with C code, easy embedding into a C application. Since the effort required to call a function and execute C code is less, higher performance results in simple functions (which do not execute large amounts of computing) and functions that process only a small amount of data at once. Parallelism on multi-core CPUs is another important feature of Torch7, as it is designed to use parallel Open Multi-Processing (OpenMP) policies, especially in tensor and neural network modules. The potential for CPU parallelization (outside of BLAS calls) in Theano has yet to be explored. Theano's characteristic feature is its powerful graph optimization engine and symbolic differentiation, which is mentioned in Section 2.1. The disadvantage is that the additional workflow must be confronted first with an abstract one, and then a more complex one."}, {"heading": "4.2 Benchmarking on Deep Learning Tasks", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Experimental Setup", "text": "The experiments reported here were conducted on a machine with an Intel Core i7 CPU 930 @ 2.80GHz and an nVidia GTX480 GPU. Theano's commit ID was 254894fac, Torch7 was 41d3b8b93.Because the Multi-Layer Perceptron (MLP) examples in the benchmarks rely on function calls to a BLAS library, we made sure that the same BLAS library was used for both Torch7 and Theano to ensure a fair comparison. We benchmarked the GEMM routine (Matrix Matrix Dot Product, Scaling and Accumulation) with matrix sizes large enough for each overhead to be negligible for a number of OpenMP threads limited to 1 and 4, and confirmed that both tools are associated with the same BLAS library, which are large enough to make each overhead negligible, for a number of OpenMP threads limited to 1 and 4, and that both are matrix matrix and matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix"}, {"heading": "4.2.2 How to boost Theano\u2019s performance", "text": "In fact, most of them are able to survive on their own if they are unable to play by the rules they have imposed on themselves."}, {"heading": "4.2.3 Results", "text": "Figure 1 shows speed results (e.g. per second, higher is better) for three neural network learning tasks, which consist of a 10-class classification of a 784-dimensional input. Figure 1a shows simple logistic regression, Figure 1b shows a neural network with a layer of 500 hidden units, and Figure 1c shows a deep neural network with 3 layers of 1000 hidden units each. Torch7 was tested with the standard Lua interpreter (pale red bar) and LuaJIT4, a Lua just-in-time compressor (darker red bar); Theano was tested with various optimizations (blues), described in Section 4.2.2. If Theano does not use mini-stacks, it beats Torch7 on models with at least one hidden layer and even benefits from the parallel implementation of BLAS. On the logistic regression benchmark, the Torch7 has the advantage of being executed quickly on each of the 7, but due to the low number of prizes, the number of LAS is insufficient."}, {"heading": "4.3 Benchmarking on Recurrent Neural Networks", "text": "In Figure 2, we present a benchmark for a simple recursive network, based on Theano and RNNLM5, a C + + implementation of recursive networks for voice modeling. They were performed with a batch size of 1, common in recursive neural networks.4http: / / luajit.org / 5http: / / www.fit.vutbr.cz / \u02dc imikolov / rnnlm / While RNNLM is faster in smaller models than Theano, Theano is catching up quickly in larger models, showing that Theano is an interesting option for forming recursive neural networks in a realistic scenario. This is mainly due to the overhead in Theano, which is a disadvantage to the flexibility provided for recursive models."}, {"heading": "5 Conclusion", "text": "We have presented Theano's recent additions and demonstrated how they make it a more powerful tool for machine learning software development, and in most cases make it faster than competing software on different benchmarks. These benchmarks aim to highlight the relative strengths of existing software so that users can choose what best suits their needs. We also hope that such benchmarks will help improve the tools available, which can only have a positive impact on the research community."}, {"heading": "Acknowledgments", "text": "We would like to thank the community of Theano users and developers for their support, NSERC and Canada Research Chairs for funding and Compute Canada and Calcul Que \ufffd bec for computing resources."}], "references": [{"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. Warde-Farley", "Y. Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Theano: Deep learning on gpus with python", "author": ["J. Bergstra", "F. Bastien", "O. Breuleux", "P. Lamblin", "R. Pascanu", "O. Delalleau", "G. Desjardins", "D. Warde-Farley", "I. Goodfellow", "A. Bergeron", "Y. Bengio"], "venue": "In Big Learn workshop,", "citeRegEx": "Bergstra et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2011}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In BigLearn,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Matplotlib: A 2d graphics environment", "author": ["J.D. Hunter"], "venue": "Computing in Science and Engineering,", "citeRegEx": "Hunter,? \\Q2007\\E", "shortCiteRegEx": "Hunter", "year": 2007}, {"title": "SciPy: Open source scientific tools for Python", "author": ["E. Jones", "T. Oliphant", "P Peterson"], "venue": null, "citeRegEx": "Jones et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Jones et al\\.", "year": 2001}, {"title": "Learning recurrent neural networks with hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11),", "citeRegEx": "Martens and Sutskever,? \\Q2011\\E", "shortCiteRegEx": "Martens and Sutskever", "year": 2011}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. Cernocky"], "venue": "In Proc. 12th annual conference of the international speech communication association (INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Python for scientific computing", "author": ["T.E. Oliphant"], "venue": "Computing in Science and Engineering,", "citeRegEx": "Oliphant,? \\Q2007\\E", "shortCiteRegEx": "Oliphant", "year": 2007}, {"title": "IPython: A system for interactive scientific computing", "author": ["F. P\u00e9rez", "B.E. Granger"], "venue": "Computing in Science and Engineering,", "citeRegEx": "P\u00e9rez and Granger,? \\Q2007\\E", "shortCiteRegEx": "P\u00e9rez and Granger", "year": 2007}, {"title": "Learning internal representations by error propagation. volume 1, chapter 8, pages 318\u2013362", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}], "referenceMentions": [{"referenceID": 2, "context": "Section 4 compares Theano\u2019s performance with that of Torch7 (Collobert et al., 2011) on neural network benchmarks, and RNNLM (Mikolov et al.", "startOffset": 60, "endOffset": 84}, {"referenceID": 6, "context": ", 2011) on neural network benchmarks, and RNNLM (Mikolov et al., 2011) on recurrent neural network benchmarks.", "startOffset": 48, "endOffset": 70}, {"referenceID": 0, "context": "Theano was introduced to the machine learning community by Bergstra et al. (2010) as a CPU and GPU mathematical compiler, demonstrating how it can be used to symbolically define mathematical functions, automatically derive gradient expressions, and compile these expressions into executable functions that outperform implementations using other existing tools.", "startOffset": 59, "endOffset": 82}, {"referenceID": 0, "context": "Theano was introduced to the machine learning community by Bergstra et al. (2010) as a CPU and GPU mathematical compiler, demonstrating how it can be used to symbolically define mathematical functions, automatically derive gradient expressions, and compile these expressions into executable functions that outperform implementations using other existing tools. Bergstra et al. (2011) then demonstrated how Theano could be used to implement Deep Learning models.", "startOffset": 59, "endOffset": 384}, {"referenceID": 4, "context": "Theano\u2019s dependency on NumPy and SciPy (Jones et al., 2001) makes it easy to add an implementation for a mathematical operation, leveraging the effort of their developers, and it is always possible to add a more optimized version that will then be transparently substituted where applicable.", "startOffset": 39, "endOffset": 59}, {"referenceID": 9, "context": "The specific algorithm used is backpropagation through time Rumelhart et al. (1986), which optimizes for speed but not memory consumption.", "startOffset": 60, "endOffset": 84}, {"referenceID": 9, "context": "The specific algorithm used is backpropagation through time Rumelhart et al. (1986), which optimizes for speed but not memory consumption. 2. Scan allows for efficient evaluation of the R-operator (see Pearlmutter (1994)), required for computing quantities such as the Gauss-Newton approximation of Hessian-vector products.", "startOffset": 60, "endOffset": 221}, {"referenceID": 5, "context": "2 R-operator for Hessian-Free optimization Recent results (Martens and Sutskever, 2011) proposed a specific pipeline for efficiently implementing truncated Newton-like second-order methods such as Hessian-Free optimization.", "startOffset": 58, "endOffset": 87}, {"referenceID": 5, "context": "2 R-operator for Hessian-Free optimization Recent results (Martens and Sutskever, 2011) proposed a specific pipeline for efficiently implementing truncated Newton-like second-order methods such as Hessian-Free optimization. The pipeline relies on the \u201cR-operator\u201d, introduced by Pearlmutter (1994), which is a mathematical operator that given a function f(\u03b8), f : R \u2192 R , the current parameter configuration \u03b8t \u2208 R and a vector \u03b3 \u2208 R , efficiently computes the \u201cJacobian-vector\u201d product ( \u2202f \u2202\u03b8 \u2223\u2223\u2223 \u03b8=\u03b8t ) \u03b3, where ( \u2202f \u2202\u03b8 \u2223\u2223\u2223 \u03b8=\u03b8t )", "startOffset": 59, "endOffset": 298}, {"referenceID": 5, "context": "A more careful implementation however reveals that two passes should be sufficient (see Martens and Sutskever (2011)).", "startOffset": 88, "endOffset": 117}, {"referenceID": 2, "context": "Collobert et al. (2011) showed that using OpenMP to parallelize the C implementation of CPU operations can bring substantial speed improvements with relatively little development effort.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Torch7 (Collobert et al., 2011) is advertised as a Matlab-like environment for machine learning.", "startOffset": 7, "endOffset": 31}, {"referenceID": 4, "context": "Both have access to a Matlab-like environment: Torch7 includes modules for tensor manipulations and plotting, while Theano benefits from various external Python libraries to perform those tasks (notably SciPy (Jones et al., 2001), NumPy (Oliphant, 2007), matplotlib (Hunter, 2007), IPython (P\u00e9rez and Granger, 2007)).", "startOffset": 209, "endOffset": 229}, {"referenceID": 7, "context": ", 2001), NumPy (Oliphant, 2007), matplotlib (Hunter, 2007), IPython (P\u00e9rez and Granger, 2007)).", "startOffset": 15, "endOffset": 31}, {"referenceID": 3, "context": ", 2001), NumPy (Oliphant, 2007), matplotlib (Hunter, 2007), IPython (P\u00e9rez and Granger, 2007)).", "startOffset": 44, "endOffset": 58}, {"referenceID": 8, "context": ", 2001), NumPy (Oliphant, 2007), matplotlib (Hunter, 2007), IPython (P\u00e9rez and Granger, 2007)).", "startOffset": 68, "endOffset": 93}], "year": 2012, "abstractText": "Theano is a linear algebra compiler that optimizes a user\u2019s symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano\u2019s performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.", "creator": "TeX"}}}