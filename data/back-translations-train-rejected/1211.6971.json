{"id": "1211.6971", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2012", "title": "A New Automatic Method to Adjust Parameters for Object Recognition", "abstract": "To recognize an object in an image, the user must apply a combination of operators, where each operator has a set of parameters. These parameters must be well adjusted in order to reach good results. Usually, this adjustment is made manually by the user. In this paper we propose a new method to automate the process of parameter adjustment for an object recognition task. Our method is based on reinforcement learning, we use two types of agents: User Agent that gives the necessary information and Parameter Agent that adjusts the parameters of each operator. Due to the nature of reinforcement learning the results do not depend only on the system characteristics but also on the user favorite choices.", "histories": [["v1", "Thu, 29 Nov 2012 16:35:07 GMT  (474kb)", "http://arxiv.org/abs/1211.6971v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["issam qaffou", "mohamed sadgal", "aziz elfazziki"], "accepted": false, "id": "1211.6971"}, "pdf": {"name": "1211.6971.pdf", "metadata": {"source": "META", "title": "A New Automatic Method to Adjust Parameters for Object Recognition", "authors": ["Issam Qaffou", "Mohamed Sadgal", "Aziz Elfazziki"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is the case that most of them will be able to move into another world, in which they are able to put themselves into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they, in which they are able to put themselves into another world, in which they are able to put themselves into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in"}, {"heading": "A. A. Defining actions", "text": "Generally, all possible combinations of parameter values are defined as an action for the RL agent. Then, the set of actions is the set of all possible value combinations, see Fig. 3. Each operator OPk has a series of parameters: 1 2 (,,...,) k k k k nP P P PEach parameter kjP has a value range: 1 2 {,,,...,} k k k k k j j j jmV V VAn elementary action of the operator OPk is: 1 (,...,) k k k k j jra u where 1 k j ju VAn action of the agent PA is defined by the combinations of elementary actions of the operator as defined above: 1 2 (,,...,) na a aActions for object identification task are given in experience."}, {"heading": "B. Defining states", "text": "A state is defined by a set of attributes extracted from the resulting image: 1 2,,..., ns i is a characteristic that reflects the state of the image after processing. The nature of the extracted attributes depends on the respective task. Here, we give a general definition and explicitly define it in the experience for a detection task."}, {"heading": "C. Defining the reward", "text": "The return is a reward if the agent chooses the right action, otherwise it is a penalty. The reward is defined according to the quality of the processing result, this quality is evaluated by using soil-truth models. To define the return, we calculate the similarity between the resulting image and its basic truth, the similarity is calculated based on some characteristics extracted from the two images. The nature of these characteristics depends on the task. For example, if we want to detect an object in an image, we extract the number of objects, their ranges, their sizes, etc. We express the difference between these scalars by: i iiD w DThe weights iw are selected according to the importance of each characteristic. A general form of the reward definition in the proposed approach is presented by: Reward: r = -10, 0 or 10; if (D <) r = + 10; f = true; elseif = true; elseif (seif > & D) (+ D) < + D (+)."}, {"heading": "A. Actions Definition", "text": "The UA proposes two operators: GLCM and k mean. GLCM has n in the size of the slider window as a parameter for adjustment. n can take seven odd values: 9, 11, 13,..., 21, so an elementary action for GLCM is one of these values. k mean has k the number of possible clusters, its possible values are {1,2,3,4,5}. An elementary action for k mean is one of these values. then an action for the agent PA consists of a few values of \"n\" and a value of \"k.\" All actions are all possible combinations of the values of \"n\" and \"k.\""}, {"heading": "B. States Definition", "text": "For object detection we extract four characteristics to define the state space: 216 | P a g ewww.ijacsa.thesai.orgWhere is the selected characteristic. Is the number of objects in the resulting image after segmentation. Is the ratio between the area of the extracted object and the area of the entire image. Is the ratio between the area of the resulting object and the object reference. is the mean of the structural characteristics used: angular moment (energy), contrast, correlation and entropy."}, {"heading": "C. Reward Definition", "text": "The rewards and punishments are defined according to the quality criterion, which represents how well the image is segmented. A simple method is the comparison of the resulting image with its basic truth. This comparison is made between the scalar properties of the obtained regions and those of the desired ones. In this work, we define the reward based on a difference between the components of the image. We define this difference as: D = weighted sum of the four following differences in the two images (the resulting image and its basic truth): D1 = difference in the number of objects; D2 = difference in the size of the objects; D3 = difference in the surfaces of the objects; D4 = difference in the characteristic textures. Fig. 4 shows the three images that were randomly taken from the process of recognition. The three images contain the same object of interest, the textured discourses. The agent PA assumes that the learning is strengthened by affirmation and finds that the optimal action is the best result."}], "references": [{"title": "S.Jacob, \u201cParameter Optimization of an Image Processing System using Evolutionary Algorithms", "author": ["B. Nickolay", "B. Schneider"], "venue": "CAIP", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "A Reinforcement Learning Framework for Parameter Control in Computer Vision Applications", "author": ["G.W.Taylor"], "venue": "Proceedings of the First Canadian Conference on Computer and Robot Vision (CRV\u201904),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "AG: \u201cReinforcement Learning", "author": ["Sutton RS", "Barto"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Q-Learning\u201d. Machine Learning", "author": ["Watkins CJCH", "Dayan P"], "venue": "(IJACSA) International Journal of Advanced Computer Science and Applications,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Artificial intelligence: a modern approach", "author": ["SJ Russell", "P Norvig"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Introduction to Reinforcement Learning", "author": ["S Singh", "P Norving", "D Cohn"], "venue": "Harlequin Inc;", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1996}, {"title": "Textural Features for Image Classification", "author": ["R.M. Haralick", "K. Shanmugan", "I. Dinstein"], "venue": "IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1973}, {"title": "Application of reinforcement learning for segmentation of transrectal ultrasound images", "author": ["F. Sahba", "H.R. Tizhoosh", "M. Salama"], "venue": "BMC Medical Imaging", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "A few years later, Taylor proposed a reinforcement learning framework which uses connectionist systems as function approximators to handle the problem of determining the optimal parameters for a computer vision application even in the case of a highly dimensional, continuous parameter space [3].", "startOffset": 292, "endOffset": 295}, {"referenceID": 6, "context": "[9] introduced a new method for the segmentation of the prostate in transrectal ultrasound images, using a reinforcement learning (RL) scheme.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Watkins has developed Q-learning, a wellestablished on-line learning algorithm, as a practical RL method [6].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "Table 1 represents an iterative policy evaluation for updating the state-action values where r is the reward value received for taking action in states, s' is the next state, \u03b1 is the learning rate, and \u03b3 is the discount factor [7].", "startOffset": 228, "endOffset": 231}, {"referenceID": 4, "context": "In the greedy policy, all actions may not be explored, whereas the \u03b5-greedy selects the action with the highest Q-value in the given state with a probability of 1 \u2013 \u03b5, and other ones with a probability of \u03b5 [7,8].", "startOffset": 207, "endOffset": 212}, {"referenceID": 5, "context": "In the greedy policy, all actions may not be explored, whereas the \u03b5-greedy selects the action with the highest Q-value in the given state with a probability of 1 \u2013 \u03b5, and other ones with a probability of \u03b5 [7,8].", "startOffset": 207, "endOffset": 212}, {"referenceID": 2, "context": "The online nature of RL distinguishes it from other techniques that approximately solve Markov decision processes (MDP) [5,7].", "startOffset": 120, "endOffset": 125}, {"referenceID": 4, "context": "The online nature of RL distinguishes it from other techniques that approximately solve Markov decision processes (MDP) [5,7].", "startOffset": 120, "endOffset": 125}, {"referenceID": 5, "context": "GLCM extracts fourteen texture features [8].", "startOffset": 40, "endOffset": 43}], "year": 2012, "abstractText": "To recognize an object in an image, the user must apply a combination of operators, where each operator has a set of parameters. These parameters must be \u201cwell\u201d adjusted in order to reach good results. Usually, this adjustment is made manually by the user. In this paper we propose a new method to automate the process of parameter adjustment for an object recognition task. Our method is based on reinforcement learning, we use two types of agents: User Agent that gives the necessary information and Parameter Agent that adjusts the parameters of each operator. Due to the nature of reinforcement learning the results do not depend only on the system characteristics but also the user\u2019s favorite choices. Keywordscomponent; Parameters adjustment; image segmentation; Q-learning; reinforcement learning.", "creator": "Microsoft Word 2010"}}}