{"id": "1212.0763", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Dec-2012", "title": "Dynamic recommender system : using cluster-based biases to improve the accuracy of the predictions", "abstract": "It is today accepted that matrix factorization models allow a high quality of rating prediction in recommender systems. However, a major drawback of matrix factorization is its static nature that results in a progressive declining of the accuracy of the predictions after each factorization. This is due to the fact that the new obtained ratings are not taken into account until a new factorization is computed, which can not be done very often because of the high cost of matrix factorization.", "histories": [["v1", "Mon, 3 Dec 2012 13:00:27 GMT  (111kb)", "http://arxiv.org/abs/1212.0763v1", "31 pages, 7 figures"]], "COMMENTS": "31 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.LG cs.DB cs.IR", "authors": ["modou gueye", "talel abdessalem", "hubert naacke"], "accepted": false, "id": "1212.0763"}, "pdf": {"name": "1212.0763.pdf", "metadata": {"source": "CRF", "title": "Dynamic recommender system : using cluster-based biases to improve the accuracy of the predictions", "authors": ["Modou Gueye", "Talel Abdessalem", "Hubert Naacke"], "emails": ["firstname.lastname@telecom-paristech.fr", "gmodou@ucad.sn", "hubert.naacke@lip6.fr"], "sections": [{"heading": null, "text": "ar Xiv: 1In this paper, which aims to improve the accuracy of recommendation systems, we propose a cluster-based matrix factorization technique that enables the online integration of new ratings. In this way, we greatly improve the predictions obtained between two matrix factorizations. We use finer-grained user presets by grouping similar items into groups and assigning a bias to each user in these groups. Experiments we conducted with large data sets showed the efficiency of our approach."}, {"heading": "1 Introduction", "text": "The reason for this is that most of the people who live in the city are not in a position to move around the city in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Preliminaries", "text": "This section defines the prediction problem and describes the matrix factorization technique on which our work is based. It also outlines the most important requirements to be considered for the design of our system."}, {"heading": "2.1 Prediction issue", "text": "The purpose of recommendation systems is to predict a user's interest in a particular element, i.e. how much the user wants the element. Most of the time, this interest is represented by numerical values from a specified range. A number of interfaces, such as widgets, are commonly used to allow users to rate the element. User ratings can be considered tuples (u, i, rui, tui), where u denotes a user, i denotes an element, rui denotes the user's rating of u for the element i, and tui is a timestamp. We assume that a user will rate an element at most once. The problem is to predict future ratings in such a way that the difference between an estimated rating r-ui and its actual predictability is the lowest possible. To form the estimator, the part of the system is divided for the first step of the rating."}, {"heading": "2.2 Matrix factorization", "text": "The columns of R represent the users in which the items represent the items. The value of each non-empty cell of R corresponding to the user u and the items i is a pair of values (rui, tui). rui is the value given by the items i in time. The table below represents such a matrix.u1 u2 u2 u2 u2... uni1 i2 2. i3 1. i4. iIn its basic form (Basic MF), matrix factorization techniques we try to capture the different rating values."}, {"heading": "2.3 Biased MF", "text": "Several improvements to the above matrix factoring techniques are proposed in the literature, one of which assumes that much of the observed variation in valuation values is due to some effects related to either the users or the items, regardless of interactions [33, 24, 26]. In fact, there are always some users who tend to give higher (or lower) ratings than others, and some items may be rated higher (or lower) than others because they are widely perceived to be better (or worse) than the others. Basic MF cannot capture these tendencies, so some distortions are introduced to highlight these rating variants. We refer to such techniques as distorted MF. The distortions reflect the tendencies of the users or items.An approximation of the first order of rui rating is as follows: Bui = \u00b5 + bu + bi (10) bui is the global effect of the distortions considered, taking into account tendencies and sensitivities."}, {"heading": "2.4 Dynamicity and performance requirements", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "3 Dynamic recommendations", "text": "As mentioned above, we focus on dynamic contexts in which new ratings are constantly being created. In such a case, it is not possible to have an up-to-date model, as the number of unprocessed ratings can increase very quickly, and then an increasing loss of quality in recommendations can be observed as long as the static model is used. To address this problem, our model relies on distortions, which are among the most overlooked components of recommendation models [18]. Distortions allow the capture of a significant part of the observed rating behavior. We combine global user distortions with local distortions. The local user distortions allow users to refine the tendency to small articles whether the global distortions capture the overall behavior of users. To be precise (i.e., to allow good predictions of user ratings), we need to compile local distortions based on a similar approach, which we initially use as a guarantee for the stability of our proposed steps."}, {"heading": "3.1 Why clustering ?", "text": "We argued that the accuracy of local user bias depends on the degree of similarity between items in each set (i.e. cluster). This section formalizes the relationship between the similarity of a set of items and the variance of items. We show that the more similar the items in each cluster are, the stronger the variance of local user bias is. A smaller variance means a lower predictive error, i.e. a more accurate recommendation. Let U be a set of items, I a set of items, rui a rating of a user-u-U for an item i-i, and the overall average rating. If we look at Iu-I, the set of items rated by a user-u, then the bias group of the user-u is defined as follows: bu = 1card (Iu)."}, {"heading": "3.2 The CBMF model", "text": "Our cluster-based matrix factorization model (CBMF) is based on the observation that many users tend to underestimate (or overestimate) the items they rate. However, a user may find that users tend to be inconsistent: It can switch from one item to another. For some types of items, a user may tend to keep the rating close to average. While for some other items (e.g., those that they really like / dislike), the user objectively distinguishes by either using extreme ratings or maintaining moderated ratings. To account for this discrepancy, we define several biases per user instead of one. We assign each user a bias to bCu and each group of similar elements."}, {"heading": "3.3 Integration of incoming ratings", "text": "After generating the recommendation model, the incoming reviews will continue to be added to the rating matrix R. Their integration into the model is simply by adjusting the local user biases, hence the importance of local biases. In fact, the recommendation of the Top-K point generally consists of items from different clusters (in our experiments for three clusters). If we adjust the local user biases with the new ratings, the recommendations can be influenced in the composition of the recommended list of items or in the ranking (Top-K) of these items. Let us use V to denote the set of known reviews in R, including the newly added Ite.V = {rui-R / u-U, i-c-Cluster, i-I > 0} (30), where V and I denote the groups of referenced users and Ite.u-Ite."}, {"heading": "3.4 Complexity analysis", "text": "The costs of our cluster-based matrix factorization solution (algorithm 1) can be divided into two parts: the cost of matrix factorization and the cost of the clustering step. The time complexity of training the entire model (matrix factorization) is O (| V | \u00b7 k \u00b7 t), where V denotes the number of known evaluations, k the number of factors and t the maximum number of iterations. The time complexity of the clustering step depends on the selected clustering algorithm. Algorithm 2: Inratings coming integration algorithmData: P, Q, V (u, c (i)))), bi, bu, \u03b4 c (i) u, \u03b2, and \u03b31 repeat 2 foreach ruj-V (u, c (i)) do 3 Compute euj; 4 Update \u03b4c (i) u; 5 end 6 Calculate the global error \u0432ruj > 0 e 2 uj; 7 terminal condition."}, {"heading": "4 Experimental evaluation", "text": "In Section 2.4, we proposed improving the widely used MF model and coupling it with two techniques that tend to improve the quality of the predictions: preliminary clustering of the assessments before factorization and final adjustment of the predicted assessments using distortions. This section presents the experiments that we have agreed to validate our approach. We recall that our approach consists of generating a high-quality recommendation model based on in-depth assessments, then using this model to recommend items as long as possible (provided the quality remains sufficient) until the next model is ready, and so on. Thus, the quality of our approach depends on two factors (i) to verify the initial quality of the model generated, and (ii) the loss of quality over time. Accordingly, we validate each factor independently by proceeding in two separate steps. Step 1 focuses on the initial quality of the model generated, the loss of our model."}, {"heading": "4.1 Implementation and Experimental setup", "text": "We implemented our proposal in C + + and conducted our experiments on a 64-bit Linux computer (Intel / Xeon x 8 threads, 2.66 Ghz, 16 GB RAM). We used a LIL matrix structure to store the data set of the evaluations. To cluster the items, we performed a basic factorization with some iterations and a K-Means algorithm on the iterators. We performed preliminary tests to calibrate the parameters of the model and the number of clusters: \u03bb = 0.001, \u03b2 = 0.02, \u03b3 = 0.05, Nc = 3. The \u03bb, \u03b2 and \u03b3 values are close to the values proposed in [26]. We limit the training to a maximum of 120 iterations and use 40 factors for both matrices P and Q."}, {"heading": "4.2 Datasets", "text": "We perform experiments with the Netflix dataset and the largest MovieLens datasets [1,2]. These datasets are very often used by the recommendation system community [30]. Table 1 shows their characteristics. Ratings are represented by integers from 1 to 5 for the Netflix dataset and real numbers for the Movielens dataset. Each dataset is ordered by ascending date."}, {"heading": "4.3 Initial quality", "text": "The aim of this experiment is to compare the original qualities of the three models. We split the data sets into two parts: a training set, which represents 98% of the ratings, and a test set, which maintains the rest (the 2% most up-to-date ratings to predict), so the test set contains 1.88 million ratings, which is higher than the Netflix price, which has 1.4 million ratings [1]. Table 2 reports on the various RMSE errors achieved for the three models named Basic MF, Biased MF and CBMF. We note that the CBMF outperforms other models, achieving a 1.12% improvement over the distorted MF with the Netflix dataset. Let's remember that even an improvement of just 1% accuracy leads to a significant difference in the ranking of the \"top K\" most recommended items for a user [21, 12]."}, {"heading": "4.4 Large training sets improve quality", "text": "We check the intuitive rule that the more reviews we take as input, the better quality we get. To realize this experiment, we first sort each user's ratings according to their timestamps. Then we divide the training set (98% of the original record) into 10 parts (c1 to c10) of the same size: 10%. More specifically, the number of ratings a user gets in each part is almost the same. From this, we generate 10 training sets (T1 to T10) of increasing size by compiling the chunks so that we always use the most recent ratings to generate the model. More precisely: T1 = {c10}, T2 = {c9}, T3 = i i [ci}, T10] {ci}, T10 = i i i."}, {"heading": "4.5 Quantifying the need for online integration", "text": "We wonder to what extent the most up-to-date reviews affect the recommendation. In the face of a training set that includes a set number of reviews, we examine the quality fluctuations as the reviews become less up-to-date. In addition, we target the \"input-intensive\" scenarios in which many new reviews are created in a short space of time, potentially missing millions of reviews. For example, the company Netflix receives 4 million reviews per day [5]. To reflect this, we need to consider several million missing reviews in our experiments. Therefore, we are experimenting only with the Netflix dataset, which is the largest, the MovieLens dataset does not have enough reviews to set up a sufficient number of missing reviews per day."}, {"heading": "4.6 Robustness to time of our online integration model", "text": "The goal is to show that our model is time-stable, i.e. that it provides good quality forecasts even when many ratings have been generated since the last factorization, using the same training and testing sets, we now take into account the missing ratings to immediately adjust the distortions of local users (cf. Algorithm 2). Specifically, we scan the test set sequentially and look at the ratings individually. For each rating, we calculate the prediction error, then immediately integrate the rating to improve the next predictions. The average time to integrate a rating is 0.4 milliseconds. It is fast and adds little overhead in the online recommendation task. As a comparison, it enables the integration of more than 216 million reviews per day, which is 27 times greater than the Netflix need reported in [5]. In Figure 4, we report on the new development of the CBMF prediction quality when we analyze the incoming reviews from the test setup."}, {"heading": "4.7 Quality vs. Performance tradeoff for online inte-", "text": "We examined three possible methods for integrating a new assessment: (i) updating only the user factors, (ii) updating only the local biases of the user, and (iii) updating both the user factors and the local biases. Of course, processing further updates comes at a cost. We wondered whether the computing time spent on greater integration would ultimately pay for itself by postponing the next model recalculation. Figure 5 shows the quality improvements that these three integration methods entail. In Table 4, we reported on the update time and the respective average quality gain (in RMSE) for each of the above three integration methods. We concluded that integrating both the local biases and the factors has a relative benefit of 7% over integrating the local biases. On the other hand, it adds up to 151% relative additional costs in terms of calculation costs."}, {"heading": "4.8 Benefit of refactorization", "text": "The aim of this experiment is to quantify the benefits of recalculating the CBMF model. Intuitively, you want to recalculate the model if its quality exceeds the expected level of quality. On the other hand, to save computational resources, we do not want to recalculate the model unless it is necessary. Therefore, we set up an experiment consisting of five consecutive factorizations, starting with the same test set and training set as in the previous experiment: the 10% most recent ratings are in the test set, the remaining 90% are in the training set. We generate five models consisting of five consecutive factorizations scattered over time as described below. Let M0 denote the initial model resulting from the training set factorization, and then sequentially scan the test set by integrating the incoming ratings into M0, on the fly, until we reach 20% of the test set."}, {"heading": "5 Related work", "text": "This year, it has come to the point where it will only take a few days to get to that point, to get to that point where an agreement can be reached."}, {"heading": "6 Conclusion", "text": "We have addressed the collaborative filtering problem by recommending accurate articles to users when incoming reviews are continuously produced and when the only information available has several million user / item reviews. Through years of experimentation, the recommendation community has shown that model-based solutions achieve the best quality, but such solutions suffer from a major drawback: they are offline. They take as input a snapshot of the reviews at the time of model calculation. They simply ignore the newer reviews and skip potentially useful information for better recommendations. Our challenging goal then was to find a way to integrate incoming reviews for a well-known model-based solution that requires a heavy compilation of billions of reviews. To this end, we implemented the matrix factoring model, which provides very good offline quality by introducing personalized biases that capture the user's subjectivity for different groups of articles."}], "references": [{"title": "Impact of data characteristics on recommender systems performance", "author": ["G. Adomavicius", "J. Zhang"], "venue": "ACM Trans. Manage. Inf. Syst.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Fast online learning through offline initialization for time-sensitive recommendation", "author": ["D. Agarwal", "B.-C. Chen", "P. Elango"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Netflix recommendations: Beyond the 5 stars", "author": ["X. Amatriain", "J. Basilico"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Modeling relationships at multiple scales to improve accuracy of large recommender systems", "author": ["R. Bell", "Y. Koren", "C. Volinsky"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "The million dollar programming prize", "author": ["R.M. Bell", "J. Bennett", "Y. Koren", "C. Volinsky"], "venue": "IEEE Spectr.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Large scale matrix factorization - yahoo! kdd cup, 2011. Large Scale Machine Learning and Other Animals", "author": ["D. Bickson"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Detect and track latent factors with online nonnegative matrix factorization", "author": ["B. Cao", "D. Shen", "J.-T. Sun", "X. Wang", "Q. Yang", "Z. Chen"], "venue": "In Proceedings of the 20th international joint conference on Artifical 28  intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "A scalable collaborative filtering based recommender system using incremental clustering", "author": ["P. Chakraborty"], "venue": "In Advance Computing Conference,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "The value of personalised recommender systems to e-business: a case study", "author": ["M.B. Dias", "D. Locher", "M. Li", "W. El-Deredy", "P.J. Lisboa"], "venue": "In Proceedings of the 2008 ACM conference on Recommender systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "The yahoo! music dataset and kdd-cup\u201911", "author": ["G. Dror", "N. Koenigstein", "Y. Koren", "M. Weimer"], "venue": "In Proceedings of KDDCup 2011,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Recommender systems and their impact on sales diversity", "author": ["D.M. Fleder", "K. Hosanagar"], "venue": "In Proceedings of the 8th ACM conference on Electronic commerce, EC", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Large-scale matrix factorization with distributed stochastic gradient descent", "author": ["R. Gemulla", "E. Nijkamp", "P.J. Haas", "Y. Sismanis"], "venue": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Evaluating collaborative filtering recommender systems", "author": ["J.L. Herlocker", "J.A. Konstan", "L.G. Terveen", "J.T. Riedl"], "venue": "ACM Trans. Inf. Syst.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "Using control theory for stable and efficient recommender systems", "author": ["T. Jambor", "J. Wang", "N. Lathia"], "venue": "In Proceedings of the 21st international conference on World Wide Web,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "A case study on the effectiveness of recommendations in the mobile internet", "author": ["D. Jannach", "K. Hegelich"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Yahoo! music recommendations: modeling music ratings with temporal dynamics and item taxonomy", "author": ["N. Koenigstein", "G. Dror", "Y. Koren"], "venue": "In Proceedings of the 5th ACM conference on Recommender systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Introduction to Clustering Large and High-Dimensional Data", "author": ["J. Kogan"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Grouping Multidimensional Data: Recent Advances in Clustering", "author": ["J. Kogan", "C. Nicholas", "M. Teboulle"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "How useful is a lower rmse", "author": ["Y. Koren"], "venue": "Netflix Prize Forum", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "The bellkor solution to the netflix grand prize", "author": ["Y. Koren"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Collaborative filtering with temporal dynamics", "author": ["Y. Koren"], "venue": "Commun. ACM,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2009}, {"title": "Industry report: Amazon.com recommendations: Item-to-item collaborative filtering", "author": ["G. Linden", "B. Smith", "J. York"], "venue": "IEEE Distributed Systems Online,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "Improving regularized singular value decomposition for collaborative filtering", "author": ["A. Paterek"], "venue": "In Proc. KDD Cup Workshop at SIGKDD\u201907,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Online-updating regularized kernel matrix factorization models for large-scale recommender systems", "author": ["S. Rendle", "L. Schmidt-Thieme"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Incremental singular value decomposition algorithms for highly scalable recommender systems", "author": ["B. Sarwar", "G. Karypis", "J. Konstan", "J. Riedl"], "venue": "In Proceedings of the 5th International Conference in Computers and Information Technology,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "Recommender systems in ecommerce", "author": ["J.B. Schafer", "J. Konstan", "J. Riedi"], "venue": "In Proceedings of the 1st ACM conference on Electronic commerce, EC", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1999}, {"title": "A survey of collaborative filtering techniques", "author": ["X. Su", "T.M. Khoshgoftaar"], "venue": "Adv. in Artif. Intell.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2009}, {"title": "A k-means-based projected clustering algorithm", "author": ["Y. Sun", "G. Liu", "K. Xu"], "venue": "In Proceedings of the 2010 Third International Joint Conference on Computational Science and Optimization - Volume 01,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Investigation of various matrix factorization methods for large recommender systems", "author": ["G. Tak\u00e1cs", "I. Pil\u00e1szy", "B. N\u00e9meth", "D. Tikk"], "venue": "In Proceedings of the 2nd KDD Workshop on Large-Scale Recommender Systems and the Netflix Prize Competition,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Scalable collaborative filtering approaches for large recommender systems", "author": ["G. Tak\u00e1cs", "I. Pil\u00e1szy", "B. N\u00e9meth", "D. Tikk"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "On exploiting classification taxonomies in recommender systems", "author": ["C.-N. Ziegler", "G. Lausen", "J.A. Konstan"], "venue": "AI Commun.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}], "referenceMentions": [{"referenceID": 26, "context": "Because the user is often overwhelmed for facing the considerable amount of items provided by electronic retailers, the predictions are a salient function of all types of e-commerce [29, 7].", "startOffset": 182, "endOffset": 189}, {"referenceID": 4, "context": "Because the user is often overwhelmed for facing the considerable amount of items provided by electronic retailers, the predictions are a salient function of all types of e-commerce [29, 7].", "startOffset": 182, "endOffset": 189}, {"referenceID": 8, "context": "That is why recommender systems attract a lot of attention due to their great commercial value [11, 17, 25, 13].", "startOffset": 95, "endOffset": 111}, {"referenceID": 14, "context": "That is why recommender systems attract a lot of attention due to their great commercial value [11, 17, 25, 13].", "startOffset": 95, "endOffset": 111}, {"referenceID": 22, "context": "That is why recommender systems attract a lot of attention due to their great commercial value [11, 17, 25, 13].", "startOffset": 95, "endOffset": 111}, {"referenceID": 10, "context": "That is why recommender systems attract a lot of attention due to their great commercial value [11, 17, 25, 13].", "startOffset": 95, "endOffset": 111}, {"referenceID": 27, "context": "It consists in analyzing relationships between users and interdependencies among items to identify new user-item associations [30, 24, 26].", "startOffset": 126, "endOffset": 138}, {"referenceID": 21, "context": "It consists in analyzing relationships between users and interdependencies among items to identify new user-item associations [30, 24, 26].", "startOffset": 126, "endOffset": 138}, {"referenceID": 23, "context": "It consists in analyzing relationships between users and interdependencies among items to identify new user-item associations [30, 24, 26].", "startOffset": 126, "endOffset": 138}, {"referenceID": 30, "context": "It gives good scalability and predictive accuracy [33, 22].", "startOffset": 50, "endOffset": 58}, {"referenceID": 19, "context": "It gives good scalability and predictive accuracy [33, 22].", "startOffset": 50, "endOffset": 58}, {"referenceID": 18, "context": "Indeed it has been claimed that even an improvement as small as 1% of the accuracy leads to a significant difference in the ranking of the \"Top-K\" most recommended items for a user [21, 12].", "startOffset": 181, "endOffset": 189}, {"referenceID": 9, "context": "Indeed it has been claimed that even an improvement as small as 1% of the accuracy leads to a significant difference in the ranking of the \"Top-K\" most recommended items for a user [21, 12].", "startOffset": 181, "endOffset": 189}, {"referenceID": 23, "context": "It combines clustering, matrix factorization and bias adjustment [26, 32], in order to startup with a high quality model.", "startOffset": 65, "endOffset": 73}, {"referenceID": 29, "context": "It combines clustering, matrix factorization and bias adjustment [26, 32], in order to startup with a high quality model.", "startOffset": 65, "endOffset": 73}, {"referenceID": 20, "context": "It can be used in fully-fledged models with weights, temporal dynamics and so on [23, 24, 32, 6].", "startOffset": 81, "endOffset": 96}, {"referenceID": 21, "context": "It can be used in fully-fledged models with weights, temporal dynamics and so on [23, 24, 32, 6].", "startOffset": 81, "endOffset": 96}, {"referenceID": 29, "context": "It can be used in fully-fledged models with weights, temporal dynamics and so on [23, 24, 32, 6].", "startOffset": 81, "endOffset": 96}, {"referenceID": 3, "context": "It can be used in fully-fledged models with weights, temporal dynamics and so on [23, 24, 32, 6].", "startOffset": 81, "endOffset": 96}, {"referenceID": 12, "context": "The Root Mean Square Error (RMSE), which computes the root of the mean of the squarred difference between the predictions and true ratings, is one of the most widely used metric for the evaluation of recommender systems since the Netflix Prize [15, 30, 1].", "startOffset": 244, "endOffset": 255}, {"referenceID": 27, "context": "The Root Mean Square Error (RMSE), which computes the root of the mean of the squarred difference between the predictions and true ratings, is one of the most widely used metric for the evaluation of recommender systems since the Netflix Prize [15, 30, 1].", "startOffset": 244, "endOffset": 255}, {"referenceID": 30, "context": "One of these assumes that much of the observed variations in the rating values is due to some effects associated with either the users or the items, independently of any interactions [33, 24, 26].", "startOffset": 183, "endOffset": 195}, {"referenceID": 21, "context": "One of these assumes that much of the observed variations in the rating values is due to some effects associated with either the users or the items, independently of any interactions [33, 24, 26].", "startOffset": 183, "endOffset": 195}, {"referenceID": 23, "context": "One of these assumes that much of the observed variations in the rating values is due to some effects associated with either the users or the items, independently of any interactions [33, 24, 26].", "startOffset": 183, "endOffset": 195}, {"referenceID": 23, "context": "Since biases tend to capture much of the observed variations and can bring significant improvements, we consider that their accurate modeling is crucial [26, 22].", "startOffset": 153, "endOffset": 161}, {"referenceID": 19, "context": "Since biases tend to capture much of the observed variations and can bring significant improvements, we consider that their accurate modeling is crucial [26, 22].", "startOffset": 153, "endOffset": 161}, {"referenceID": 13, "context": "Some recent work [16], shows interesting directions that we plan to study in the future work.", "startOffset": 17, "endOffset": 21}, {"referenceID": 15, "context": "To tackle this problem, our model relies on biases which are among the most overlooked components of recommender models [18].", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "Thus, we come down to observe local ratings variation in place of a single global ratings variation as used in previous approaches [26, 24, 32].", "startOffset": 131, "endOffset": 143}, {"referenceID": 21, "context": "Thus, we come down to observe local ratings variation in place of a single global ratings variation as used in previous approaches [26, 24, 32].", "startOffset": 131, "endOffset": 143}, {"referenceID": 29, "context": "Thus, we come down to observe local ratings variation in place of a single global ratings variation as used in previous approaches [26, 24, 32].", "startOffset": 131, "endOffset": 143}, {"referenceID": 15, "context": "When additional information on the items is available (metadata on the items), it may be used for clustering [18, 35].", "startOffset": 109, "endOffset": 117}, {"referenceID": 31, "context": "When additional information on the items is available (metadata on the items), it may be used for clustering [18, 35].", "startOffset": 109, "endOffset": 117}, {"referenceID": 17, "context": "If no metadata is available, there are still many possible clustering techniques, only based on item ratings, each one having its own cost: projected K-means, PDDP and so on [20, 19, 31].", "startOffset": 174, "endOffset": 186}, {"referenceID": 16, "context": "If no metadata is available, there are still many possible clustering techniques, only based on item ratings, each one having its own cost: projected K-means, PDDP and so on [20, 19, 31].", "startOffset": 174, "endOffset": 186}, {"referenceID": 28, "context": "If no metadata is available, there are still many possible clustering techniques, only based on item ratings, each one having its own cost: projected K-means, PDDP and so on [20, 19, 31].", "startOffset": 174, "endOffset": 186}, {"referenceID": 23, "context": "The \u03bb, \u03b2, and \u03b3 values are close to the ones suggested in [26].", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "These datasets are very often used by the recommendation system community [30].", "startOffset": 74, "endOffset": 78}, {"referenceID": 18, "context": "Let us remind there, even an improvement as small as 1% of the accuracy leads to a significant difference in the ranking of the \"Top-K\" most recommended items for a user [21, 12].", "startOffset": 170, "endOffset": 178}, {"referenceID": 9, "context": "Let us remind there, even an improvement as small as 1% of the accuracy leads to a significant difference in the ranking of the \"Top-K\" most recommended items for a user [21, 12].", "startOffset": 170, "endOffset": 178}, {"referenceID": 0, "context": "Adomavicius and Zhang mention this phenomenon in [3].", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "For instance Netflix company receives 4 million ratings per day [5].", "startOffset": 64, "endOffset": 67}, {"referenceID": 2, "context": "As a comparison, it allows for integrating more than 216 million ratings per day, which is 27 times bigger than the Netflix need reported in [5].", "startOffset": 141, "endOffset": 144}, {"referenceID": 2, "context": "33 million ratings (according to the Netflix rate [5]).", "startOffset": 50, "endOffset": 53}, {"referenceID": 7, "context": "Especially, previous work on MF-based recommender systems did not consider biases for the integration of incoming ratings, and focuses on the techniques of factorization [10, 28, 27, 9].", "startOffset": 170, "endOffset": 185}, {"referenceID": 25, "context": "Especially, previous work on MF-based recommender systems did not consider biases for the integration of incoming ratings, and focuses on the techniques of factorization [10, 28, 27, 9].", "startOffset": 170, "endOffset": 185}, {"referenceID": 24, "context": "Especially, previous work on MF-based recommender systems did not consider biases for the integration of incoming ratings, and focuses on the techniques of factorization [10, 28, 27, 9].", "startOffset": 170, "endOffset": 185}, {"referenceID": 6, "context": "Especially, previous work on MF-based recommender systems did not consider biases for the integration of incoming ratings, and focuses on the techniques of factorization [10, 28, 27, 9].", "startOffset": 170, "endOffset": 185}, {"referenceID": 7, "context": "In [10] and [28], the authors deal with \"new user/item\" problem, which aims at integrating newly registered users and items (and their ratings).", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "In [10] and [28], the authors deal with \"new user/item\" problem, which aims at integrating newly registered users and items (and their ratings).", "startOffset": 12, "endOffset": 16}, {"referenceID": 24, "context": "focus on users (and items) which have small rating profiles [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 1, "context": "propose in [4] a fast online bilinear factor model (called FOBFM).", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "[9] point the problem of data dynamicity in latent factors detection approaches.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "So far, we only know a few number of works close to ours [23, 18].", "startOffset": 57, "endOffset": 65}, {"referenceID": 15, "context": "So far, we only know a few number of works close to ours [23, 18].", "startOffset": 57, "endOffset": 65}, {"referenceID": 20, "context": "In [23] the author models the drift of user behaviours and item popularity.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "The work described in [18] considers the type of the items in addition to the users and items temporal dynamics.", "startOffset": 22, "endOffset": 26}, {"referenceID": 11, "context": "Of course, it is obvious that using parallel implementations leads to better computation time, as shown in [14, 8].", "startOffset": 107, "endOffset": 114}, {"referenceID": 5, "context": "Of course, it is obvious that using parallel implementations leads to better computation time, as shown in [14, 8].", "startOffset": 107, "endOffset": 114}, {"referenceID": 2, "context": "However, the need of online integration remains necessary for large scale applications with billions of ratings and many millions of incoming ratings each day (Netflix has more than 5 billion user ratings and receives daily 4 milion new ratings from 23 million subscribers [5]).", "startOffset": 273, "endOffset": 276}, {"referenceID": 2, "context": "Our challenging goal was then to find a way to enable the integration of the incoming ratings for a well-know model-based recommendation solution requiring heavy computation with billions of ratings [5].", "startOffset": 199, "endOffset": 202}], "year": 2012, "abstractText": "It is today accepted that matrix factorization models allow a high quality of rating prediction in recommender systems. However, a major drawback of matrix factorization is its static nature that results in a progressive declining of the accuracy of the predictions after each factorization. This is due to the fact that the new obtained ratings are not taken into account until a new factorization is computed, which can not be done very often because of the high cost of matrix factorization. In this paper, aiming at improving the accuracy of recommender systems, we propose a cluster-based matrix factorization technique that enables online integration of new ratings. Thus, we significantly enhance the obtained predictions between two matrix factorizations.", "creator": "gnuplot 4.2 patchlevel 6 "}}}