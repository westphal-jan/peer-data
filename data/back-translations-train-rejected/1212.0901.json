{"id": "1212.0901", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Dec-2012", "title": "Advances in Optimizing Recurrent Networks", "abstract": "After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modelling sequences,their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.", "histories": [["v1", "Tue, 4 Dec 2012 23:25:34 GMT  (19kb)", "http://arxiv.org/abs/1212.0901v1", null], ["v2", "Fri, 14 Dec 2012 01:44:53 GMT  (19kb)", "http://arxiv.org/abs/1212.0901v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio", "nicolas boulanger-lewandowski", "razvan pascanu"], "accepted": false, "id": "1212.0901"}, "pdf": {"name": "1212.0901.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 121 2.09 01v1 [cs.LG] 4 Dec 201 2Index Terms - Recurring Networks, Deep Learning, Visual Learning, Long-Term Dependencies"}, {"heading": "1. INTRODUCTION", "text": "In fact, it is not as if it is a pure conspiracy theory, but a conspiracy theory, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, which is about the conspiracy theories, and which is about the conspiracy theories, and which is about the conspiracy theories, and which is about the conspiracy theories, and which is about the conspiracy theories, and which is about the conspiracy theories, and which is about the conspiracy theories, and which is about the conspiracy theories, and which is about the conspiracy theories and which is about the conspiracy theories, and which is about the conspiracy theories, and which is about the conspiracy theories and which is about the conspiracy theories, and which is about the conspiracy theories, and which is about the conspiracy theories and which is about the conspiracy theories, and which is about the conspiracy theories and which is about the conspiracy theories and which is about the conspiracy theories, and which is about the conspiracy theories and which is the conspiracy theories and which are the conspiracy theories, and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories, and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories and which are the conspiracy theories"}, {"heading": "2. LEARNING LONG-TERM DEPENDENCIES AND THE OPTIMIZATION DIFFICULTY WITH DEEP LEARNING", "text": "There have been several breakthroughs in recent years in algorithms and results achieved with so-called deep learning algorithms (see [7] and [8] for reviews). Deep learning algorithms detect multiple levels of representation, typically as deep neural networks or graphical models organized with many levels of representation of latent variables. Very little work on deep architectures occurred before the great advances of 2006 [9, 10, 11], probably due to optimization difficulties due to the high level of nonlinearity in deeper networks (the output of which is the composition of non-linearity at each level). Some experiments showed the presence of an extremely large number of apparent local minima of training criteria, with no two different initializations going to the same function (i.e.) eliminating the effect of permutations and other symmetries of parameterization leading to qualitatively different initialization."}, {"heading": "3. ADVANCES IN TRAINING RECURRENT NETWORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Clipped Gradient", "text": "Under the hypothesis that the explosion takes place in very small regions (the cliffs in the above-mentioned cost function), this will have no effect most of the time, but will avoid deviating parameter changes in these cliff regions and at the same time guarantee that the resulting updates are still in descending direction.The specific form of the cut-out used here was proposed in [15] and is discussed in much more detail: If the norm of the gradient vector g for a particular sequence is above a threshold, the update takes place towards the threshold g | | g | |. As argued in [15], this very simple method implements a very simple form of second-order optimization in the sense that the second derivative is proportionally large even in these exploding gradient regions."}, {"heading": "3.2. Spanning Longer Time Ranges with Leaky Integration", "text": "An old idea to reduce the effect of disappearing gradients is to introduce shorter distances between t1 and t2, either through connections with longer time delays [17] or through inertia (slowly changing units) in some of the hidden units [18, 19] or both [20]. Long-short-term memory (LSTM) networks [21], which have been shown to be able to handle much longer range dependencies, also benefit from a linear self-contained storage unit with almost 1 eigenweight, which allows signals (and gradients) to be spread over long periods of time. Another interpretation of these slowly changing units is that they behave like low-pass filters and can therefore be used to focus certain units on different frequency ranges of the data. Analogy can be taken a step further by introducing band-pass filter units [22], using domain-specific knowledge to decide on which frequency bands to focus different units of the network as an improvement [1]."}, {"heading": "3.3. Combining Recurrent Nets with a Powerful Output Probability Model", "text": "One way to reduce the underequipment of RNNs is to introduce multiplicative interactions into the parameterization of F, as has been successfully done in [4]. If the output predictions are multivariate, another approach is to capture the dependencies of high order between the output variables using a powerful output probability model such as a Restricted Boltzmann Machine (RBM) [23, 24] or a deterministic variant of it called NADE [25, 24]. In the experiments conducted here, we have experimented with a NADE output model for the music data."}, {"heading": "3.4. Sparser Gradients via Sparse Output Regularization and Rectified Outputs", "text": "[7] suggests that one reason for the difficulty of optimizing deep networks is that in ordinary neural networks, gradients diffuse through the layers and credits and blame are distributed across many units, potentially making it harder for hidden units to specialize. If the gradient is thinner in hidden units, one might imagine that symmetries would be broken more easily and credits or blame assignments less uniformly, as advocated in [26], taking advantage of the idea of non-linearity of correctors previously introduced in [27], i.e. the non-linearity of neurons is out = max (0, in) instead of out = tanh (in) or out = sigmoid (in). This approach has been very successful in recent work on deep learning on object recognition [28], far surpassing the state of the art on ImageNet (1000 classes)."}, {"heading": "3.5. Simplified Nesterov Momentum", "text": "Nesterov's accelerated gradient (NAG \u2212 \u2212 1) is a first-order optimization method to improve stability and convergence of the regular gradient descent (1) has recently shown that the NAG \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 f (1 + 1 + 0 \u2212 1) could be calculated using the following update rules: \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 1 \u2212 f (0, 1] of the dynamic (decay) coefficient and 1 \u2212 0 of the learning rate in the iteration t, f (0, f) is the objective function and the objective performance in the iteration t, f \u2212 1 \u2212 1 \u2212 4 \u2212 0 \u2212 4 \u2212 4 is the objective performance in the iteration."}, {"heading": "4. EXPERIMENTS", "text": "In the experimental section, we compare vanilla SGD with SGD plus some of the improvements discussed above. Specifically, we use the letter \"C\" to indicate that gradient section is used, \"L\" for leaky integration units, \"R\" for rectifier units with L1 penalty and \"M\" for Nesterov impulse."}, {"heading": "4.1. Music Data", "text": "We evaluate our models on the basis of the four multi-part music datasets of varying complexity used in [24]: classical piano music (Pianomidi.de), folk tunes with chords from ABC notation (Nottingham), orchestral music (MuseData), and the four-part chorals of J.S. Bach (JSB-Chorales).The symbolic sequences contain high-grade pitch and time information in the form of a binary matrix or piano roll, which precisely specify which tones occur in each time step. They form interesting benchmarks for RNNs due to their high dimensionality and the complex temporal dependencies that occur in different timescales. Each dataset contains at least 7 hours of polyphonic music with an average polyphony (number of simultaneous notes) of 3.9 piano rolls prepared by matching each time step (88 pitch markings covering the entire range of the piano) on an integer fraction of the quarter note."}, {"heading": "4.1.1. Setup and Results", "text": "We select hyperparameters such as the number of hidden units nh, regulation coefficients L1, choice of nonlinearity function or dynamic plan \u00b5t, learning rate, number of leaky units nh or leaky factors \u03b1 according to the log probability on a validation set and we report the final performance on the test set for the best choice in each category. We do this by random search [30] at the following intervals: nh [100, 400], [10 \u2212 4, 10 \u2212 1], [10 \u2212 3, 0.95], [10 \u2212 6, 10 \u2212 3], leaky [0%, 25%, 50%}, \u03b1 [0.02], the limit threshold for cutting gradients is determined on the basis of the average gradient norm over a passage on the data, and in this case we used 15 for all music data sets. The data is divided into sequences, 2wtw 2-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5-5."}, {"heading": "4.2. Text Data", "text": "We use the Penn Treebank Corpus to explore both word and character prediction tasks, dividing the data by using sections 0-20 as training data (5017k characters), sections 21-22 as validation (393k characters), and sections 23-24 as test data (442k characters).For word level prediction, we set the dictionary to 10,000 words, which we divide into 30 classes according to their frequency in the text (each class holds about 3.3% of the total number of tokens in the training set).Such factoring allows for faster implementation since we do not have to evaluate the entire output layer (10,000 units), which represents the computational bottleneck, but only the output of the corresponding class [31]."}, {"heading": "4.2.1. Setup and Results", "text": "In the case of the next word prediction, we calculate gradients over sequences of 40 steps, in which we carry the hidden state from one sequence to the next. We use a small grid search around the parameters used to obtain state-of-the-art results for this number of classes [31], i.e. with a network of 200 hidden units giving a perplexity of 134. We study learning rates of 0.1, 0.01, 0.001, 0.001, 0.001, rectifier units versus sigmoid units, limits for gradients of 30, 50 or none, and no leaky units versus 50 of the units resulting from 0.2 and 02. For the character level model, we calculate gradients over sequences of 150 steps, assuming that longer dependencies are more decisive in this case. We use 500 hidden units and examine learning rates of 0.5, 0.1, and 01. In Table 2 we have entropy (bits per character) or perplexity for different Ns, which we prefer both Ns and Ns."}, {"heading": "5. CONCLUSIONS", "text": "Through our experiments, we provide evidence that part of the problem of training RNNs is due to the coarse defect surface, which is not so easy for SGD to handle. We follow a series of step-by-step improvements of SGD and show that in most cases they improve both the training and the test defect, enabling this advanced SGD to compete or even improve with a second-order method that has proven to be particularly good for RNNs, i.e. the Hessen-free optimization."}, {"heading": "6. REFERENCES", "text": "[1] S. Hochreiter, \"Investigations on Dynamic Neural Networks 2011, 2011.\" [6] Sutolov, S. Burget, J. Cernocky, and S. Khudanpur, \"Extensions of recurrent neural network language model.\" [7] S. Hochreiter, S. Hochreiter, T.U. Jaever works, \"1991. [2] Y. Bengio, P. Simard, and P. Frasconi,\" Learning representations by back-propagating errors, \"Nature, vol. 323, pp. 533-536, 1986. [4] J. Martens and I. Sutskever,\" Learning recurrent neural networks with Hessian-free optimization, \"in ICML '2011, 2011. [5] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, and S. Khudanpur,\" Extensions of recurrent neural network language, \"in ICASSP 2011, 2011."}], "references": [{"title": "Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, T.U. M\u00fcnich", "author": ["S. Hochreiter"], "venue": "1991.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE T. Neural Nets, 1994.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, vol. 323, pp. 533\u2013536, 1986.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning recurrent neural networks with Hessian-free optimization", "author": ["J. Martens", "I. Sutskever"], "venue": "ICML\u20192011, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Extensions of recurrent neural network language model", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. Cernocky", "S. Khudanpur"], "venue": "ICASSP 2011, 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Training Recurrent Neural Networks, Ph.D", "author": ["I. Sutskever"], "venue": "thesis, CS Dept., U. Toronto,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Learning deep architectures for AI", "author": ["Yoshua Bengio"], "venue": "Now Publishers,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "Tech. Rep., arXiv:1206.5538, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, vol. 18, pp. 1527\u20131554, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Greedy layer-wise training of deep networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "NIPS\u20192006, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient learning of sparse representations with an energy-based model", "author": ["M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun"], "venue": "NIPS\u20192006, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Manzagol", "P. Vincent", "S. Bengio"], "venue": "J. Machine Learning Res., (11) 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "ICML\u20192010, 2010, pp. 735\u2013742.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Building high-level features using large scale unsupervised learning", "author": ["Q. Le", "M. Ranzato", "R. Monga", "M. Devin", "G. Corrado", "K. Chen", "J. Dean", "A. Ng"], "venue": "ICML\u20192012, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding the exploding gradient problem", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "Tech. Rep., Universit\u00e9 De Montr\u00e9al, 2012, arXiv:arXiv:1211.5063.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical Language Models based on", "author": ["Tomas Mikolov"], "venue": "Neural Networks, Ph.D. thesis, Brno University of Technology,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Learning longterm dependencies is not as difficult with NARX recurrent neural networks", "author": ["T. Lin", "B.G. Horne", "P. Tino", "C.L. Giles"], "venue": "Tech. Rep. UMICAS-TR-95-78, U. Mariland, 1995.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["S. ElHihi", "Y. Bengio"], "venue": "NIPS\u20191995, 1996.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "Optimization and applications of echo state networks with leaky- integrator neurons", "author": ["Herbert Jaeger", "Mantas Lukosevicius", "Dan Popovici", "Udo Siewert"], "venue": "Neural Networks, vol. 20, no. 3, pp. 335\u2013352, 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Temporal kernel recurrent neural networks", "author": ["I. Sutskever", "G. Hinton"], "venue": "Neural Networks, vol. 23, no. 2, (23) 2, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1997}, {"title": "Echo-state networks with band-pass neurons: Towards generic time-scale-independent reservoir structures", "author": ["Udo Siewert", "Welf Wustlich"], "venue": "\u00a1p\u00bfPreliminary Report\u00a1/p\u00bf, October 2007.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "The recurrent temporal restricted Boltzmann machine", "author": ["I. Sutskever", "G. Hinton", "G. Taylor"], "venue": "NIPS\u20192008. 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "author": ["N. Boulanger-Lewandowski", "Y. Bengio", "P. Vincent"], "venue": "ICML\u20192012, 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "The Neural Autoregressive Distribution Estimator", "author": ["H. Larochelle", "I. Murray"], "venue": "AISTATS\u20192011, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "AISTATS\u20192011, 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML\u20192010, 2010.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS\u20192012. 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence o(1/k)", "author": ["Yu Nesterov"], "venue": "Doklady AN SSSR (translated as Soviet. Math. Docl.), vol. 269, pp. 543\u2013 547, 1983.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1983}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": "J. Machine Learning Res., vol. 13, pp. 281\u2013305, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Extensions of recurrent neural network language model", "author": ["Tomas Mikolov", "Stefan Kombrink", "Lukas Burget", "Jan Cernocky", "Sanjeev Khudanpur"], "venue": "Proc. 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP 2011), 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Machine learning algorithms for capturing statistical structure in sequential data face a fundamental problem [1, 2], called the difficulty of learning long-term dependencies.", "startOffset": 110, "endOffset": 116}, {"referenceID": 1, "context": "Machine learning algorithms for capturing statistical structure in sequential data face a fundamental problem [1, 2], called the difficulty of learning long-term dependencies.", "startOffset": 110, "endOffset": 116}, {"referenceID": 2, "context": "Recurrent neural networks [3] can represent such non-linear maps (F , below) that iteratively build a relevant summary of past observations.", "startOffset": 26, "endOffset": 29}, {"referenceID": 2, "context": "In principle, RNNs can be trained by gradient-based optimization procedures (using the back-propagation algorithm [3] to compute a gradient), but it was observed early on [1, 2] that capturing dependencies that span a long interval was difficult, making the task of optimizing \u03b8 to minimize the average of Ct\u2019s almost impossible for some tasks when the span of the dependencies of interest increases sufficiently.", "startOffset": 114, "endOffset": 117}, {"referenceID": 0, "context": "In principle, RNNs can be trained by gradient-based optimization procedures (using the back-propagation algorithm [3] to compute a gradient), but it was observed early on [1, 2] that capturing dependencies that span a long interval was difficult, making the task of optimizing \u03b8 to minimize the average of Ct\u2019s almost impossible for some tasks when the span of the dependencies of interest increases sufficiently.", "startOffset": 171, "endOffset": 177}, {"referenceID": 1, "context": "In principle, RNNs can be trained by gradient-based optimization procedures (using the back-propagation algorithm [3] to compute a gradient), but it was observed early on [1, 2] that capturing dependencies that span a long interval was difficult, making the task of optimizing \u03b8 to minimize the average of Ct\u2019s almost impossible for some tasks when the span of the dependencies of interest increases sufficiently.", "startOffset": 171, "endOffset": 177}, {"referenceID": 3, "context": "However, a revival of interest in these learning algorithms is taking place, in particular thanks to [4] and [5].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "However, a revival of interest in these learning algorithms is taking place, in particular thanks to [4] and [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "This paper studies the issues giving rise to these difficulties and discusses, reviews, and combines several techniques that have been proposed in order to improve training of RNNs, following up on a recent thesis devoted to the subject [6].", "startOffset": 237, "endOffset": 240}, {"referenceID": 3, "context": ", as add-ons to stochastic gradient descent (SGD), they allow to compete with batch (or large minibatch) second-order methods such as Hessian-Free optimization, recently found to greatly help training of RNNs [4].", "startOffset": 209, "endOffset": 212}, {"referenceID": 6, "context": "There has been several breakthroughs in recent years in the algorithms and results obtained with so-called deep learning algorithms (see [7] and [8] for reviews).", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "There has been several breakthroughs in recent years in the algorithms and results obtained with so-called deep learning algorithms (see [7] and [8] for reviews).", "startOffset": 145, "endOffset": 148}, {"referenceID": 8, "context": "Very little work on deep architectures occurred before the major advances of 2006 [9, 10, 11], probably because of optimization difficulties due to the high level of nonlinearity in deeper networks (whose output is the composition of the non-linearity at each layer).", "startOffset": 82, "endOffset": 93}, {"referenceID": 9, "context": "Very little work on deep architectures occurred before the major advances of 2006 [9, 10, 11], probably because of optimization difficulties due to the high level of nonlinearity in deeper networks (whose output is the composition of the non-linearity at each layer).", "startOffset": 82, "endOffset": 93}, {"referenceID": 10, "context": "Very little work on deep architectures occurred before the major advances of 2006 [9, 10, 11], probably because of optimization difficulties due to the high level of nonlinearity in deeper networks (whose output is the composition of the non-linearity at each layer).", "startOffset": 82, "endOffset": 93}, {"referenceID": 11, "context": "Some experiments [12] showed the presence of an extremely large number of apparent local minima of the training criterion, with no two different initializations going to the same function (i.", "startOffset": 17, "endOffset": 21}, {"referenceID": 12, "context": "Some ill-conditioning has clearly been shown to be involved, especially for the difficult problem of training deep auto-encoders, through comparisons [13] of stochastic gradient descent and Hessian-free optimization (a second order optimiza-", "startOffset": 150, "endOffset": 154}, {"referenceID": 13, "context": "These optimization questions become particularly important when trying to train very large networks on very large datasets [14], where one realizes that a major challenge for deep learning is the underfitting issue.", "startOffset": 123, "endOffset": 127}, {"referenceID": 3, "context": "Hessian-free optimization has been successfully used to considerably extend the span of temporal dependencies that a RNN can learn [4], suggesting that ill-conditioning effects are also at play in the difficulties of training RNN.", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "An important aspect of these difficulties is that the gradient can be decomposed [2, 15] into terms that involve products of Jacobians \u2202ht \u2202ht\u22121 over subsequences linking an event at time t1 and one at time", "startOffset": 81, "endOffset": 88}, {"referenceID": 14, "context": "An important aspect of these difficulties is that the gradient can be decomposed [2, 15] into terms that involve products of Jacobians \u2202ht \u2202ht\u22121 over subsequences linking an event at time t1 and one at time", "startOffset": 81, "endOffset": 88}, {"referenceID": 14, "context": "A much deeper discussion of this issue can be found in [15], along with a point of view inspired by dynamical systems theory and by the geometrical aspect of the problem, having to do with the shape of the training criterion as a function of \u03b8 near those regions of exploding gradient.", "startOffset": 55, "endOffset": 59}, {"referenceID": 3, "context": ", pre-multiplying by the inverse of some proxy for the Hessian matrix) could in principle reduce the exploding and vanishing gradient effects, as argued in [4].", "startOffset": 156, "endOffset": 159}, {"referenceID": 15, "context": "To address the exploding gradient effect, [16, 15] recently proposed to clip gradients above a given threshold.", "startOffset": 42, "endOffset": 50}, {"referenceID": 14, "context": "To address the exploding gradient effect, [16, 15] recently proposed to clip gradients above a given threshold.", "startOffset": 42, "endOffset": 50}, {"referenceID": 14, "context": "The specific form of clipping used here was proposed in [15] and is discussed there at much greater length: when the norm of the gradient vector g for a given sequence is above a threshold, the update is done in the direction threshold g ||g|| .", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "As argued in [15], this very", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "An old idea to reduce the effect of vanishing gradients is to introduce shorter paths between t1 and t2, either via connections with longer time delays [17] or inertia (slow-changing units) in some of the hidden units [18, 19], or both [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 17, "context": "An old idea to reduce the effect of vanishing gradients is to introduce shorter paths between t1 and t2, either via connections with longer time delays [17] or inertia (slow-changing units) in some of the hidden units [18, 19], or both [20].", "startOffset": 218, "endOffset": 226}, {"referenceID": 18, "context": "An old idea to reduce the effect of vanishing gradients is to introduce shorter paths between t1 and t2, either via connections with longer time delays [17] or inertia (slow-changing units) in some of the hidden units [18, 19], or both [20].", "startOffset": 218, "endOffset": 226}, {"referenceID": 19, "context": "An old idea to reduce the effect of vanishing gradients is to introduce shorter paths between t1 and t2, either via connections with longer time delays [17] or inertia (slow-changing units) in some of the hidden units [18, 19], or both [20].", "startOffset": 236, "endOffset": 240}, {"referenceID": 20, "context": "Long-Short-Term Memory (LSTM) networks [21], which were shown to be able to handle much longer range dependencies, also benefit from a linearly self-connected memory unit with a near 1 self-weight which allows signals (and gradients) to propagate over long time spans.", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "The analogy can be brought one step further by introducing band-pass filter units [22] or by using domain specific knowledge to decide on what frequency bands different units should focus.", "startOffset": 82, "endOffset": 86}, {"referenceID": 3, "context": "One way to reduce the underfitting of RNNs is to introduce multiplicative interactions in the parametrization of F , as was done successfully in [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 22, "context": "When the output predictions are multivariate, another approach is to capture the high-order dependencies between the output variables using a powerful output probability model such as a Restricted Boltzmann Machine (RBM) [23, 24] or a deterministic variant of it called NADE [25, 24].", "startOffset": 221, "endOffset": 229}, {"referenceID": 23, "context": "When the output predictions are multivariate, another approach is to capture the high-order dependencies between the output variables using a powerful output probability model such as a Restricted Boltzmann Machine (RBM) [23, 24] or a deterministic variant of it called NADE [25, 24].", "startOffset": 221, "endOffset": 229}, {"referenceID": 24, "context": "When the output predictions are multivariate, another approach is to capture the high-order dependencies between the output variables using a powerful output probability model such as a Restricted Boltzmann Machine (RBM) [23, 24] or a deterministic variant of it called NADE [25, 24].", "startOffset": 275, "endOffset": 283}, {"referenceID": 23, "context": "When the output predictions are multivariate, another approach is to capture the high-order dependencies between the output variables using a powerful output probability model such as a Restricted Boltzmann Machine (RBM) [23, 24] or a deterministic variant of it called NADE [25, 24].", "startOffset": 275, "endOffset": 283}, {"referenceID": 6, "context": "[7] hypothesized that one reason for the difficulty in optimizing deep networks is that in ordinary neural networks gradients diffuse through the layers, diffusing credit and blame through many units, maybe making it difficult for hidden units to specialize.", "startOffset": 0, "endOffset": 3}, {"referenceID": 25, "context": "This is what was advocated in [26], exploiting the idea of rectifier non-linearities introduced earlier in [27], i.", "startOffset": 30, "endOffset": 34}, {"referenceID": 26, "context": "This is what was advocated in [26], exploiting the idea of rectifier non-linearities introduced earlier in [27], i.", "startOffset": 107, "endOffset": 111}, {"referenceID": 27, "context": "This approach was very successful in recent work on deep learning for object recognition [28], beating by far the state-of-the-art on ImageNet (1000 classes).", "startOffset": 89, "endOffset": 93}, {"referenceID": 28, "context": "Nesterov accelerated gradient (NAG) [29] is a first-order optimization method to improve stability and convergence of regular gradient descent.", "startOffset": 36, "endOffset": 40}, {"referenceID": 5, "context": "Recently, [6] showed that NAG could be computed by the following update rules:", "startOffset": 10, "endOffset": 13}, {"referenceID": 0, "context": "where \u03b8t are the model parameters, vt the velocity, \u03bct \u2208 [0, 1] the momentum (decay) coefficient and \u01ebt > 0 the learning rate at iteration t, f(\u03b8) is the objective function and \u2207f(\u03b8) is a shorthand notation for the gradient \u2202f(\u03b8) \u2202\u03b8 |\u03b8=\u03b8\u2032 .", "startOffset": 57, "endOffset": 63}, {"referenceID": 23, "context": "We evaluate our models on the four polyphonic music datasets of varying complexity used in [24]: classical piano music (Pianomidi.", "startOffset": 91, "endOffset": 95}, {"referenceID": 29, "context": "We do so by using random search [30] on the following intervals:", "startOffset": 32, "endOffset": 36}, {"referenceID": 9, "context": "nh \u2208 [100, 400] \u01ebt \u2208 [10 , 10]", "startOffset": 21, "endOffset": 30}, {"referenceID": 9, "context": "nh \u2208 [100, 400] \u01ebt \u2208 [10 , 10]", "startOffset": 21, "endOffset": 30}, {"referenceID": 9, "context": "95] \u03bbL1 \u2208 [10 , 10] nleaky \u2208 {0%, 25%, 50%} \u03b1 \u2208 [0.", "startOffset": 10, "endOffset": 19}, {"referenceID": 9, "context": "95] \u03bbL1 \u2208 [10 , 10] nleaky \u2208 {0%, 25%, 50%} \u03b1 \u2208 [0.", "startOffset": 10, "endOffset": 19}, {"referenceID": 30, "context": "Such a factorization allows for faster implementation, as we are not required to evaluate the whole output layer (10000 units) which is the computational bottleneck, but only the output of the corresponding class [31].", "startOffset": 213, "endOffset": 217}, {"referenceID": 30, "context": "We use a small grid-search around the parameters used to get state of the art results for this number of classes [31], i.", "startOffset": 113, "endOffset": 117}], "year": 2017, "abstractText": "After a more than decade-long period of relatively little research activity in the area of recurrent neural networks, several new developments will be reviewed here that have allowed substantial progress both in understanding and in technical solutions towards more efficient training of recurrent networks. These advances have been motivated by and related to the optimization issues surrounding deep learning. Although recurrent networks are extremely powerful in what they can in principle represent in terms of modeling sequences, their training is plagued by two aspects of the same issue regarding the learning of long-term dependencies. Experiments reported here evaluate the use of clipping gradients, spanning longer time ranges with leaky integration, advanced momentum techniques, using more powerful output probability models, and encouraging sparser gradients to help symmetry breaking and credit assignment. The experiments are performed on text and music data and show off the combined effects of these techniques in generally improving both training and test error.", "creator": "LaTeX with hyperref package"}}}