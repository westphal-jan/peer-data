{"id": "1212.1527", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Dec-2012", "title": "Learning Mixtures of Arbitrary Distributions over Large Discrete Domains", "abstract": "We give an algorithm for learning a mixture of unstructured distributions. This problem arises in various unsupervised learning scenarios, for example in learning topic models from a corpus of documents spanning several topics. We show how to learn the constituents (the topic distributions and the mixture weights) of a mixture of $k$ (constant) arbitrary distributions over a large discrete domain $[n]={1,2,...,n}$, using $O(n\\polylog n)$ samples.", "histories": [["v1", "Fri, 7 Dec 2012 04:03:06 GMT  (34kb)", "https://arxiv.org/abs/1212.1527v1", null], ["v2", "Tue, 9 Apr 2013 18:41:14 GMT  (39kb)", "http://arxiv.org/abs/1212.1527v2", "Update of previous version, which includes aperture and sample-size lower bounds"], ["v3", "Wed, 18 Sep 2013 04:18:49 GMT  (40kb)", "http://arxiv.org/abs/1212.1527v3", "Update of previous version with improved aperture and sample-size lower bounds"]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["yuval rabani", "leonard schulman", "chaitanya swamy"], "accepted": false, "id": "1212.1527"}, "pdf": {"name": "1212.1527.pdf", "metadata": {"source": "CRF", "title": "Learning Mixtures of Arbitrary Distributions over Large Discrete Domains", "authors": ["Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy"], "emails": ["yrabani@cs.huji.ac.il.", "schulman@caltech.edu."], "sections": [{"heading": null, "text": "ar Xiv: 121 2.15 27v3 [cs.LG] 1 8Se p20 13This task is theoretically impossible for k > 1 in the normal scanning procedure of a mixture distribution. However, there are situations (such as the above-mentioned topic model case) in which each sample point consists of several observations of the same mixture component. This number of observations, which we call \"sample stacking,\" is a crucial parameter of the problem. We obtain the first limits of this mixing learning problem without imposing any assumptions on the mixture components. We show that efficient learning is possible precisely at the information theory least possible aperture of 2k \u2212 1. Thus, we achieve an almost optimal dependence on n and optimal aperture. While the sample size required by our algorithm depends exponentially on k, we prove that such dependence is inevitable when general mixes are taken into account."}, {"heading": "1 Introduction", "text": "In fact, it is a question of a way in which one sees oneself in a position to surpass oneself. (...) It is a question of whether it is a way in which people are able to survive themselves. (...) It is a question of whether people are able to survive themselves. (...) It is a question of the extent to which people are able to survive themselves. (...) It is a question of whether they are able to put themselves in a position. \"(...) It is a question of the extent to which they are able to involve themselves in the situation.\" (...)"}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Mixture sources, snapshots, and projections", "text": "Suppose [n] denotes {1, 2,., n}, and \u2206 n \u2212 1 denotes (n \u2212 1) simplicity {x,.., pk), where pt has support [n] for all t [k], along with the corresponding mixture weights w = (w1,..., wk) components of the k mixture P = (p1,..., a m snapshot of (w, P) is obtained by selecting t [k] according to the distribution w and then i [n] times independently of the distribution pt. Thus, the probability distribution on m snapshots is a mixture of the k power distribution on the product space [n] m. We also consider mixture sources whose components are distributions on R. Thus, a probability distribution on m snapshots is a mixture of the k distribution on the product space [n] s. We consider mixture sources whose components are distributions on R. k distributions."}, {"heading": "2.2 Transportation distance for mixtures", "text": "Leave (w, (p1,.., pk)) and (w, (p, 1,.., p)) k or p mixture sources on [n]. The transport distance (in relation to the total variable distance of 12 x \u2212 y \u2212 1 to measured variables on.n \u2212 1) between these two sources, which is indicated by Tran (w, P; w, P), is the optimal value of the following linear program (LP).min k = 1j = 1j \u00b7 1xij \u00b7 12 x pi \u2212 p, 1 s.t."}, {"heading": "2.3 Perturbation results and operator norm of random matrices", "text": "Definition 2.1. The operator standard of A = Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy. \u2212 Economy."}, {"heading": "3 Our algorithm", "text": "We describe our algorithm which 1-, 2-, 2- and (2k \u2212 1) -1) -2) -3) -3 (n) -3 (n) -3 (n) -3 (n) -3 (n) -3 (n) -3 (n) -3 (n) -3 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -4 (n) -5 (n) -5 (n) -5 (n) -6 (n), 6 (n), 6 (n), 7 (n), 7 (n), 8 (n), 8 (n), 8 (n), 8 (n), 8 (n)."}, {"heading": "4 Analysis", "text": "lp. (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.).). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.).). (lp. (lp. (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp.). (lp..). (lp. (lp.). (lp.).). (lp.). (lp.). (lp.).). (lp. (lp.).). (lp.)."}, {"heading": "5 The one-dimensional problem: learning mixture sources on [0,1]", "text": "In this section we provide the key subbroutine called in step L2 of the algorithm. (Remember that learning completes the procedure for the mixture (w, \u03c0a / 2H (P)), where we cannot reconstruct the k-tip distribution (w, p) in general. (Remember that learning completes the procedure for the mixture (w, \u03c0a / 2H (P))), where the k-tip distribution (w, p) in general cannot be reconstructed. (Note that the procedure for the mixture 2k \u2212 1. However, our goal is somewhat different and more modest. We are trying to reconstruct the k-tip distribution (w, p). (P), and we show that this can be achieved with the smallest procedure possible. (this is the smallest procedure).It is easy to get a (2k, 1) snapshot."}, {"heading": "5.2 Proofs of Lemma 5.5 and Lemma 5.6", "text": "There are two simple cases that need to be dismissed before we get to the more subtle part of the problem: the first simple case is = 1. In this case, it is a single Lagrange interpolant: - \u2212 \u2212 \u2212 \u2212 \u2212"}, {"heading": "6 Lower bounds", "text": "In this section, we prove that we have exactly the same 2k moments that are also applicable to the setting in which we set the k mixture sources to {0, 1} (so n = 2). We must remember that a k mixture source to {0, 1} can be considered a k mixture source that has a minimal separation between its spikes. Theorem 6.2 proves that 2k \u2212 1 is the smallest aperture at which it becomes possible to reconstruct a k-spike distribution. We emphasize that this is a lower limit of information theory. We show (Theorem 6.2) that there are two k-spike distributions supported on [0, 1] with separation (1 k)."}, {"heading": "A Probability background", "text": "In our analysis, we use the following large deviation limits: Lemma A.1 (Chernoff limit; see Theorem 1.1 in [23]). Let X1,..., XN be independent random variables with Xi [0, 1] for all i and \u00b5 = (\u2211 i EXi) / n. Let Pr [1 N * iXi \u2212 \u00b5 * *] \u2264 2e * 2N * Lemma A.2 (Amber's inequality; see Theorem 1.2 in [23]). Let X1,..., XN be independent random variables with | Xi | \u2264 b, E [Xi] = 0 for all i and let us allow \u03c32 = \u0445 i Var [Xi]. Then Pr [| ichiXi | > t] \u2264 2 Exp (\u2212 t2 (\u04412 + bt / 3))."}, {"heading": "B Sample-size dependence of [2, 3, 4] on n for \u21131-reconstruction", "text": "We look at P = (p1, pk) as a n \u00b7 k matrix. Let's remember that r = 1 wtptp \u2020 t, A = 2 wt = 1 wt (p \u2212 r), and M = rr \u2020 + A. Let's wmax: = 1 wtptp \u2020 t, A = 1 wt (p \u2212 r), A = 1 wt (p \u2212 r), A = 2 wt (p \u2212 r), B = 1 wtp), A = 2 wtp (p), A = 1 wtp (p), B = 2 wtp (p), B = 2 wtp), B = 2 wtp (p), B = 2 wtp (p), B = 2 wtp (p), B = 2 wtp (p)."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>We give an algorithm for learning a mixture of unstructured distributions. This problem<lb>arises in various unsupervised learning scenarios, for example in learning topic models from<lb>a corpus of documents spanning several topics. We show how to learn the constituents of<lb>a mixture of k arbitrary distributions over a large discrete domain [n] = {1, 2, . . . , n} and the<lb>mixture weights, using O(n polylogn) samples. (In the topic-model learning setting, the mixture<lb>constituents correspond to the topic distributions.)<lb>This task is information-theoretically impossible for k > 1 under the usual sampling process<lb>from a mixture distribution. However, there are situations (such as the above-mentioned topic<lb>model case) in which each sample point consists of several observations from the same mixture<lb>constituent. This number of observations, which we call the \u201csampling aperture\u201d, is a crucial<lb>parameter of the problem.<lb>We obtain the first bounds for this mixture-learning problem without imposing any assump-<lb>tions on the mixture constituents. We show that efficient learning is possible exactly at the<lb>information-theoretically least-possible aperture of 2k \u2212 1. Thus, we achieve near-optimal de-<lb>pendence on n and optimal aperture. While the sample-size required by our algorithm depends<lb>exponentially on k, we prove that such a dependence is unavoidable when one considers general<lb>mixtures.<lb>A sequence of tools contribute to the algorithm, such as concentration results for random<lb>matrices, dimension reduction, moment estimations, and sensitivity analysis.", "creator": "LaTeX with hyperref package"}}}