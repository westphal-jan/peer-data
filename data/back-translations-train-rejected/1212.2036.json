{"id": "1212.2036", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Dec-2012", "title": "Query-focused Multi-document Summarization: Combining a Novel Topic Model with Graph-based Semi-supervised Learning", "abstract": "Graph-based semi-supervised learning has proven to be an effective approach for query-focused multi-document summarization. The problem of previous semi-supervised learning is that sentences are ranked without considering the higher level information beyond sentence level. Researches on general summarization illustrated that the addition of topic level can effectively improve the summary quality. Inspired by previous researches, we propose a two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised learning approach. At the same time, we propose a novel topic model which makes full use of the dependence between sentences and words. Experimental results on DUC and TAC data sets demonstrate the effectiveness of our proposed approach.", "histories": [["v1", "Mon, 10 Dec 2012 11:35:29 GMT  (615kb)", "http://arxiv.org/abs/1212.2036v1", null], ["v2", "Fri, 27 Dec 2013 17:24:00 GMT  (0kb,I)", "http://arxiv.org/abs/1212.2036v2", "This paper has been withdrawn by the author due to a crucial sign error in equation"], ["v3", "Tue, 31 Dec 2013 17:13:33 GMT  (0kb,I)", "http://arxiv.org/abs/1212.2036v3", "This paper has been withdrawn by the author due to a crucial sign error in equation"]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["jiwei li", "sujian li"], "accepted": false, "id": "1212.2036"}, "pdf": {"name": "1212.2036.pdf", "metadata": {"source": "CRF", "title": "Query-focused Multi-document Summarization: Combining a Novel Topic Model with Graph-based Semi-supervised Learning", "authors": ["Jiwei Li", "Sujian Li"], "emails": ["jl3226@cornell.edu", "lisujian@pku.edu.cn"], "sections": [{"heading": null, "text": "Quality. Inspired by previous research, we propose a two-tiered (i.e. sentence and topic layers) graph-based semi-supervised learning approach. At the same time, we propose a novel topic model that fully exploits the dependence between sentences and words. Experimental results on DUC- and TAC-datasets show the effectiveness of our proposed approach."}, {"heading": "1 Introduction", "text": "This year it has come to the point where we will be able to retaliate, \"he said.\" We must be able to retaliate, \"he said.\" We must be able to retaliate, \"he said.\" We must be able to retaliate, \"he said.\" We must be able to retaliate, \"he said.\" We must be able to hide, \"he said.\" We are able to hide. \""}, {"heading": "2 Related works", "text": "In recent years, graph-based summaries have attracted a lot of attention, both for generic and query-focused summaries (Zhouet al., 2003; Zhou et al., 2004; Erkan and Radev, 2004; Wan and Yang, 2007; Wei et al., 2008). Frequently used graph-based summaries are mainly inspired by linkage analysis approaches such as PageRank (Page et al., 1998) and HITS (Kleinberg 1999), which have achieved much success in web page ranking. LexRank (Erkan et al., 2004) is a typical PageRank-like algorithm for generic summaries and is further extended to query-focused summaries by formulating query-focused summaries, which have the effect of sentences (Wei et al., 2008)."}, {"heading": "3 Two-Layer Graph-Based Semisupervised Learning", "text": "In fact, it is the case that it is a matter of a way in which people move in the most different areas of life, in which people move in the most different life worlds and living worlds of the world in which they live. (...) It is the case that people live in the most different life worlds of the world in which they live. (...) It is as if people live in the most different life worlds and living worlds of the world in which they live, live. (...) It is as if people live in the most different life worlds of the world in which they live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, life, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live, live"}, {"heading": "4 Topic Modeling", "text": "In this section we start with two basic methods of modeling documents, LDA and a simply revised LDA with sentence level added (called IS-LDA). Then we clarify why and how we will use our new theme model called DS-LDA.4.1 LDA and IS-LDAThe hierarchical LDA models the likelihood of a document collection on hidden topics as in Fig. 3 (a) Let K be the number of topics; V be the vocabulary size; M be the number of documents. The topic distribution of each document 1 {} k Km k is drawn from a prioritized Dir distribution (\u03b1), and each word, m nw issampled from a topic word distribution 1 {k Vw w w w w w by a random drawn word allocation from the topic-topic document 1 {} k m."}, {"heading": "4.2 DS-LDA", "text": "4.2.1 Model OverviewWe can see that IS-LDA still does not just describe the generation process of one document collection. Firstly, it is observed that although most words in a sentence are associated with the topic as the sentence is assigned, there is still the possibility that some words can be associated with other topics, especially if the length of a sentence is long. Secondly, we find that the topic assignments are not independent of sentences and words: neighboring sentences (or words) are likely to talk about the same topic, and the longer the distance between two sentences (words) are, the less the influence is between their topic assignments. Based on our observation, we propose our topic model DS-LDA, which means that the topic assignments depend on sentences and words. The DS-LDA model is illustrated in Fig. 4. We assume that the mth document Nm sentences and the sth sentences in document m have Nm, s-words are fixed."}, {"heading": "4.2.2 Inference", "text": "We use Gibbs sampling i () K i i (6), where a sentence is derived from the height of the sentence from the height of the sentence. (D) We use Gibbs sampling i (K) to perform a conclusion (K), where we use a value for, m sz, we use the following formula:,,,,,,, 2 [1] (1) (1) () (), 1 (), 1 (), 1 (), 1 (), 1 (), 1 (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (),"}, {"heading": "5 Experiments", "text": "Our experimental data consists of DUC data (2005-2007) and TAC data (2008-2009) 2. Table 1 illustrates the number of document collections, the average number of documents per collection, 2 http: / / www.nist.gov / the average number of records per collection and the average number of words per sentence for each year. In DUC data, the length of a system-generated summary is limited to 250 words. In TAC, we use their docset A3 data sets, while in Recall-Oriented Understudy for Gisting Evaluation (Lin, 2004), we use ROUGE-1, ROUGE-2 and ROUGE-SU4 and the corresponding 95% confidential intervals to evaluate the performance of the system-generated summaries."}, {"heading": "5.1 Determination of Topic Number", "text": "Let n be the total number of sentences for each collection, and a simplerule of thumb sets the number of subjects to n. Here, we assume that there is a linear relationship between the optimal number of subjects and n. For each collection of documents in the training set, we extract a summary using our DS-LDA-based semi-monitored ranking method with the number of subjects ranging from 1 to 50. The value with which the summary receives the highest ROUGE-2 theme is assumed to be the optimal number of subjects of the corresponding document collection. Fig. 6 (a) shows the relationship between the optimal number of subjects and the square root of the sentence number. From this figure, we get the appropriate curve with the algebra formula as: 1.11 5.05K n (11), based on the ceiling function that maps a real number to the smallest following integer. Next, we test the DS-LDA model with the formula of the theme on DUC3In."}, {"heading": "5.2 Parameter Tuning in DS-LDA", "text": "In the experiments, the hyperparameters \u03b1, 1 and \u03b3 in DS-LDA are set respectively to 1, 0.1 and 0.5. Now, we have to set four parameters: the parameter a in Eq. (1), which denotes the relative influence of homogeneous nodes, and the sentence influence parameter c in Eq. (6), the word influence parameter d in Eq. (7), and the ratio (between the two hyperparameters 2 1 and). Here, we apply a consensual strategy to search for the optimal values of the three parameters. That is, we coordinate each parameter and get its optimal value when the other three parameters are fixed. First, a sentence is set to 0.5, and both c and d are set to 1.0. Looking at Eq. (7), we can see that the topic assignment of a word is independent of a word to a sentence."}, {"heading": "5.3 Comparison with Other Approaches", "text": "With the exception of the hierarchical Bayesian topic models LDA (D), LDA (S) and IS-LDA introduced above, clustering is also seen as topic-specific modeling technology. Thus, we introduce other three baselines: K-means clustering, agglomerative clustering, divisive clustering, and use clusters as topics for the construction of the two-layer graph. Divisive clustering and agglomerative clustering are hierarchical algorithms which do not need the specification of the cluster number (Kmeans clustering) (1386 clustering algorithm, the cluster number is specified respectively as the squared root of thesentence number) (named Kmeans (n) method), and the value computed by Formula (11) 0.37GE-37C 2006 and TAC 2008 data, the two-layer graph-based semi-supervised learning algorithm is combined with each topic ranking, and the summed GE in UDA 390.36DA-35ds."}, {"heading": "6 Conclusions and Future Work", "text": "This paper examines a two-layer graph-based semi-supervised learning approach combined with the methodology of topic modeling for query-focused multi-layered document summaries. To model the topic level in the two-layered diagram, we propose a novel method of topic modeling called DS-LDA that can efficiently and naturally model the entire document collection and depict the relationships between sentences and topics. Experiments with DUC- and TAC-datasets have shown that the addition of appropriate topic information can enhance the performance of the summary. In our future work, we plan to further investigate the topic modeling techniques for the influence of hyperparameters."}], "references": [{"title": "Latent dirichlet allocation", "author": ["David Blei", "Andrew Ng", "Micheal Jordan."], "venue": "The Journal of Machine Learning Research, page: 993-1022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Lexrank: graph-based lexical centrality as salience in text summarization", "author": ["Gune Erkan", "Dragomir Radev."], "venue": "J. Artif. Intell. Res. (JAIR), page 457-479.", "citeRegEx": "Erkan and Radev.,? 2004", "shortCiteRegEx": "Erkan and Radev.", "year": 2004}, {"title": "Hidden topic markov models", "author": ["Amit Gruber", "Michal Rosen-zvi", "Yair"], "venue": "In Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "Gruber et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gruber et al\\.", "year": 2007}, {"title": "Exploring content models for multi-document summarization", "author": ["Aria Haghighi", "Lucy Vanderwende."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational", "citeRegEx": "Haghighi and Vanderwende.,? 2009", "shortCiteRegEx": "Haghighi and Vanderwende.", "year": 2009}, {"title": "Cross-document summarization by concept classification", "author": ["Hilda Hardy", "Nobuyuki Shimizu", "Tomek Strzakowski", "Liu Ting", "Xinyang Zhang", "Bowden Wize."], "venue": "Proceedings of the 25th annual international ACM SIGIR conference on Research", "citeRegEx": "Hardy et al\\.,? 2002", "shortCiteRegEx": "Hardy et al\\.", "year": 2002}, {"title": "Authoritative Sources in a Hyperlinked Environment", "author": ["Jon M. Kleinberg."], "venue": "Journal of the ACM, page 604-632.", "citeRegEx": "Kleinberg.,? 1999", "shortCiteRegEx": "Kleinberg.", "year": 1999}, {"title": "Generating templates of entity summaries with an entity-aspect model and pattern mining", "author": ["Peng Li", "Jing Jiang", "Yingli Wang."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics page:640-649", "citeRegEx": "Li et al\\.,? 2010", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["Chen-Yew Lin."], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop Page:71-84", "citeRegEx": "Lin.,? 2004", "shortCiteRegEx": "Lin.", "year": 2004}, {"title": "Using random walks for question-focused sentence retrieval", "author": ["Jahna Otterbacher", "Gunes Erkan", "Dragomir Radev."], "venue": "Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, page 915-", "citeRegEx": "Otterbacher et al\\.,? 2005", "shortCiteRegEx": "Otterbacher et al\\.", "year": 2005}, {"title": "Developing learning strategies for topic-based summarization", "author": ["You Ouyang", "Sujian. Li", "Wenjie. Li"], "venue": "In Proceedings of the sixteenth ACM conference on Conference on information and knowledge management,", "citeRegEx": "Ouyang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ouyang et al\\.", "year": 2007}, {"title": "Multi-document Summarization using cluster-based link analysis", "author": ["Xiaojun Wan", "Jianwu Yang."], "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, page: 299-306.", "citeRegEx": "Wan and Yang.,? 2008", "shortCiteRegEx": "Wan and Yang.", "year": 2008}, {"title": "Manifold-ranking based topic-focused multidocument summarization", "author": ["Xiaojun Wan", "Jianwu Yang", "Jianguo Xiao."], "venue": "Proceedings of International Joint Conference on Artificial Intelligence, page 2903-2908.", "citeRegEx": "Wan et al\\.,? 2007", "shortCiteRegEx": "Wan et al\\.", "year": 2007}, {"title": "Query-sensitive mutual reinforcement chain and its application in query-oriented multi-document summarization", "author": ["Furu Wei", "Wenjie Li", "Qin Lu", "Yanxiang He."], "venue": "Proceedings of the 31st annual international ACM SIGIR conference on Research", "citeRegEx": "Wei et al\\.,? 2008", "shortCiteRegEx": "Wei et al\\.", "year": 2008}, {"title": "Generic summarization and keyphrase extraction using mutual reinforcement principle and sentence clustering", "author": ["Hongyuan Zha."], "venue": "Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information", "citeRegEx": "Zha.,? 2002", "shortCiteRegEx": "Zha.", "year": 2002}, {"title": "Ranking on Data Manifolds", "author": ["Dengzhong Zhou", "Jason Weston", "Arthur Gretton", "Olivier Bousquet", "Bernhard Sch\u00f6lkopf."], "venue": "Proceedings of the Conference on Advances in Neural Information Processing Systems, page 169-176.", "citeRegEx": "Zhou et al\\.,? 2003", "shortCiteRegEx": "Zhou et al\\.", "year": 2003}, {"title": "Learning with Local and Global Consistency", "author": ["Dengyou Zhou", "Olivier Bousquet", "Thomas Navin", "JasonWeston."], "venue": "Advances in neural information processing systems, page 321-328.", "citeRegEx": "Zhou et al\\.,? 2004", "shortCiteRegEx": "Zhou et al\\.", "year": 2004}, {"title": "Semisupervised Learning using Gaussian Fields and Harmonic Functions", "author": ["X. Zhu", "Z. Ghahramani", "J. Lafferty."], "venue": "ICML, 03\u2019, 2003, page. 912919.", "citeRegEx": "Zhu et al\\.,? 2003", "shortCiteRegEx": "Zhu et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 10, "context": "The problem of such semi-supervised learning methods is that sentences are ranked without considering the higher-level information beyond the sentence level and the sentences expressing the main topic or theme cannot be treated with priority (Wan and Yang, 2008).", "startOffset": 242, "endOffset": 262}, {"referenceID": 13, "context": "Previous studies usually saw one topic as a cluster of sentences which can be obtained by the traditional clustering techniques such as k-means clustering (Zha, 2002).", "startOffset": 155, "endOffset": 166}, {"referenceID": 6, "context": "To solve this problem, various topic modeling techniques have been explored, and Latent Dirichlet Allocation (LDA) model and its variants have recently shown their advantages in topic modeling (Blei, et al., 2003; Titov and McDonld, 2008; Li et al., 2010), due to their clear and rigorous probabilistic interpretation of topics.", "startOffset": 193, "endOffset": 255}, {"referenceID": 14, "context": "In recent years, graph-based summarization approaches have attracted much attention for both generic and query-focused summarizations (Zhou et al., 2003; Zhou et al., 2004; Erkan and Radev, 2004; Wan and Yang, 2007; Wei et al., 2008).", "startOffset": 134, "endOffset": 233}, {"referenceID": 15, "context": "In recent years, graph-based summarization approaches have attracted much attention for both generic and query-focused summarizations (Zhou et al., 2003; Zhou et al., 2004; Erkan and Radev, 2004; Wan and Yang, 2007; Wei et al., 2008).", "startOffset": 134, "endOffset": 233}, {"referenceID": 1, "context": "In recent years, graph-based summarization approaches have attracted much attention for both generic and query-focused summarizations (Zhou et al., 2003; Zhou et al., 2004; Erkan and Radev, 2004; Wan and Yang, 2007; Wei et al., 2008).", "startOffset": 134, "endOffset": 233}, {"referenceID": 12, "context": "In recent years, graph-based summarization approaches have attracted much attention for both generic and query-focused summarizations (Zhou et al., 2003; Zhou et al., 2004; Erkan and Radev, 2004; Wan and Yang, 2007; Wei et al., 2008).", "startOffset": 134, "endOffset": 233}, {"referenceID": 1, "context": "LexRank (Erkan and Radev, 2004) is a typical PageRank-like algorithm for generic summarization and further is extended to queryfocused summarization by formulating query\u201fs effect on links of sentences (Wei et al.", "startOffset": 8, "endOffset": 31}, {"referenceID": 12, "context": "LexRank (Erkan and Radev, 2004) is a typical PageRank-like algorithm for generic summarization and further is extended to queryfocused summarization by formulating query\u201fs effect on links of sentences (Wei et al., 2008).", "startOffset": 201, "endOffset": 219}, {"referenceID": 10, "context": "It is worthy of noting that many researches on generic summarization have introduced the topic level and effectively improved the summarization performance (Wan and Yang, 2008; Hardy et al., 2002; Harabagiu and Lacatusu, 2005).", "startOffset": 156, "endOffset": 226}, {"referenceID": 4, "context": "It is worthy of noting that many researches on generic summarization have introduced the topic level and effectively improved the summarization performance (Wan and Yang, 2008; Hardy et al., 2002; Harabagiu and Lacatusu, 2005).", "startOffset": 156, "endOffset": 226}, {"referenceID": 10, "context": "K-means or agglomerate clustering) to acquire topics (Wan and Yang, 2008; Hardy et al., 2002).", "startOffset": 53, "endOffset": 93}, {"referenceID": 4, "context": "K-means or agglomerate clustering) to acquire topics (Wan and Yang, 2008; Hardy et al., 2002).", "startOffset": 53, "endOffset": 93}, {"referenceID": 0, "context": "Recently the generative probabilistic models such as LDA (Latent Dirichlet Allocation) and its variants have been proposed with the goal of reducing dimensionality and acquiring semantic topics (Blei et al., 2003; Li et al., 2010; Titov and McDonald, 2008; Gruber et al., 2007).", "startOffset": 194, "endOffset": 277}, {"referenceID": 6, "context": "Recently the generative probabilistic models such as LDA (Latent Dirichlet Allocation) and its variants have been proposed with the goal of reducing dimensionality and acquiring semantic topics (Blei et al., 2003; Li et al., 2010; Titov and McDonald, 2008; Gruber et al., 2007).", "startOffset": 194, "endOffset": 277}, {"referenceID": 2, "context": "Recently the generative probabilistic models such as LDA (Latent Dirichlet Allocation) and its variants have been proposed with the goal of reducing dimensionality and acquiring semantic topics (Blei et al., 2003; Li et al., 2010; Titov and McDonald, 2008; Gruber et al., 2007).", "startOffset": 194, "endOffset": 277}, {"referenceID": 0, "context": ", 2004; Erkan and Radev, 2004; Wan and Yang, 2007; Wei et al., 2008). Commonly used graph-based ranking are mainly inspired by the link analysis approaches like PageRank (Page et al., 1998) and HITS (Kleinberg 1999), which have achieved much success in Web page ranking. LexRank (Erkan and Radev, 2004) is a typical PageRank-like algorithm for generic summarization and further is extended to queryfocused summarization by formulating query\u201fs effect on links of sentences (Wei et al., 2008). Wan et al. (2007) first proposed to treat query-focused summarization as a semi-supervised learning task, in which the query is deemed as a labeled node, and the sentences as unlabeled nodes.", "startOffset": 8, "endOffset": 510}, {"referenceID": 0, "context": ", 2004; Erkan and Radev, 2004; Wan and Yang, 2007; Wei et al., 2008). Commonly used graph-based ranking are mainly inspired by the link analysis approaches like PageRank (Page et al., 1998) and HITS (Kleinberg 1999), which have achieved much success in Web page ranking. LexRank (Erkan and Radev, 2004) is a typical PageRank-like algorithm for generic summarization and further is extended to queryfocused summarization by formulating query\u201fs effect on links of sentences (Wei et al., 2008). Wan et al. (2007) first proposed to treat query-focused summarization as a semi-supervised learning task, in which the query is deemed as a labeled node, and the sentences as unlabeled nodes. Then the importance scores of the query and the sentences are tuned by applying the manifold learning algorithm proposed by Zhou et al. (2003) or the harmonic approach proposed by Zhu et al.", "startOffset": 8, "endOffset": 827}, {"referenceID": 0, "context": ", 2004; Erkan and Radev, 2004; Wan and Yang, 2007; Wei et al., 2008). Commonly used graph-based ranking are mainly inspired by the link analysis approaches like PageRank (Page et al., 1998) and HITS (Kleinberg 1999), which have achieved much success in Web page ranking. LexRank (Erkan and Radev, 2004) is a typical PageRank-like algorithm for generic summarization and further is extended to queryfocused summarization by formulating query\u201fs effect on links of sentences (Wei et al., 2008). Wan et al. (2007) first proposed to treat query-focused summarization as a semi-supervised learning task, in which the query is deemed as a labeled node, and the sentences as unlabeled nodes. Then the importance scores of the query and the sentences are tuned by applying the manifold learning algorithm proposed by Zhou et al. (2003) or the harmonic approach proposed by Zhu et al. (2003). The difference between manifold algorithm and harmonic approach is whether the given labeling of the labeled nodes is clamped.", "startOffset": 8, "endOffset": 882}, {"referenceID": 2, "context": "3(b), based on the assumption made by Gruber et al. (2007) that words in the same sentence belong to one same topic.", "startOffset": 38, "endOffset": 59}, {"referenceID": 7, "context": "As for evaluation metrics, we use ROUGE (Recall-Oriented Understudy for Gisting Evaluation) (Lin, 2004) measures, including ROUGE-1, ROUGE-2, and ROUGE-SU4 4 and their corresponding 95% confidential intervals, to evaluate the performance of the system-generated summaries.", "startOffset": 92, "endOffset": 103}], "year": 2012, "abstractText": "Graph-based semi-supervised learning has proven to be an effective approach for query-focused multi-document summarization. The problem of previous semi-supervised learning is that sentences are ranked without considering the higher level information beyond sentence level. Researches on general summarization illustrated that the addition of topic level can effectively improve the summary quality. Inspired by previous researches, we propose a two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised learning approach. At the same time, we propose a novel topic model which makes full use of the dependence between sentences and words. Experimental results on DUC and TAC data sets demonstrate the effectiveness of our proposed approach.", "creator": "Microsoft Office Word 2007"}}}