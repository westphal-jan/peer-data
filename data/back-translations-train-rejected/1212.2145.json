{"id": "1212.2145", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Dec-2012", "title": "A Scale-Space Theory for Text", "abstract": "Scale-space theory has been established primarily by the computer vision and signal processing communities as a well-founded and promising framework for multi-scale processing of signals (e.g., images). By embedding an original signal into a family of gradually coarsen signals parameterized with a continuous scale parameter, it provides a formal framework to capture the structure of a signal at different scales in a consistent way. In this paper, we present a scale space theory for text by integrating semantic and spatial filters, and demonstrate how natural language documents can be understood, processed and analyzed at multiple resolutions, and how this scale-space representation can be used to facilitate a variety of NLP and text analysis tasks.", "histories": [["v1", "Mon, 10 Dec 2012 17:39:44 GMT  (186kb)", "http://arxiv.org/abs/1212.2145v1", "9 pages, 6 figures; Nature language processing"]], "COMMENTS": "9 pages, 6 figures; Nature language processing", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["shuang-hong yang"], "accepted": false, "id": "1212.2145"}, "pdf": {"name": "1212.2145.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Shuang Hong Yang"], "emails": ["shy@gatech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 121 2.21 45v1 [cs.IR] 1 0D ec2 012"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2 Scale Space Representation", "text": "Consider the most common case in which it is applied to two-dimensional signals such as images. Faced with a picture f (x1, x2), its scale representation \u03b3 (x1, x2, s) is defined by the following scales: \u03b3 (x1, x2, s) = f (x1, x2) \u0445 (x1, x2) \u043c (x1, x2, s) (1) = \"R2f\" (x1 \u2212 u1 \u2212 u1, x2 \u2212 u2) (u1, u2, s) \"du1du2,\" which denotes the convolution operator, and: R 2 \u00b7 R \u2212 R is a smoothing core (i.e. a low-pass filter) with a number of desired properties (i.e., the scale-space plane axioms (Lindeberg, 1994) x. \"The bandwidth parameter s is called a scale parameter, as s increases, the derived image gradually becomes smoother."}, {"heading": "3 Scale Space Model for Text", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Word-level 2D Image Analogy of Text", "text": "A simple step towards a textual salesroom would be to display texts in the manner of an image signal. In this section we will show how this can be done. Other alternative signal formulas will be discussed in the following section. Let V = {v1, v2,.., vM} be our vocabulary consisting of M-words, since a document d consists of a finite N-word sequence d = w1w2... wN, without loss of information, we can characterize d as a 2D N-M binary matrix f, with the (x, y) th entry f (x, y) th entry f (x, y) indicating whether the umpteenth vocabulary word vy is observed at the umpteenth position or not."}, {"heading": "3.2 Textual Signals", "text": "The 2D binary matrix described above is not the only option we can work with on a 1: 1 scale. In general, any vector, matrix or even tensor representation of a document can be used as a signal to which scale spatial filtering can be applied. Specifically, in the current paper we use the following: \u2022 Word-level 2D signal, f (x, y), is the binary matrix we have described in \u00a7 3.1. It captures the spatial position of each word and is defined in the common spatial-semantic area. \u2022 Bag-of-word 1D signal is the BOW representation f (y) = \u2211 x f (x, y), i.e. the 2D matrix has collapsed to a 1D vector. As the spatial axis is wiped out, this signal is defined on the semantic level alone. \u2022 Sentence-level 2D signal is a compromise between word-level 2D and the BOW signal."}, {"heading": "3.3 Spatial Filtering", "text": "Spatial filtering has long been popular in signal processing (Witkin, 1983; Lindeberg, 1994) and has recently been researched in NLP by (Lebanon et al., 2007; Yang and Zha, 2010). It can be achieved by merging the signal with a low-pass spatial filter, i.e., \u03b3 (x, s) = f (x) \u0445 (x, s). For texts, this amounts to borrowing the occurrence of words at a position from adjacent positions, similar to what was done by a cache-based language model (Jelinek et al., 1991; Beeferman et al., 1999). In order to avoid introducing false information, the filter must satisfy a series of scalspace axioms (Lindeberg, 1994). If we consider the positions in a text as a spatial domain, the Gaussian nucleus (x, s) is a causal temporal order (\u2212 x2 / 2s) or the temporal options that vary (1) one at a time."}, {"heading": "3.4 Semantic Filtering", "text": "Unlike my spatial domain, the semantic domain has some unique properties. The first thing we notice is that semantic coordinates are nothing more than dictionary indexes, we can permutate them without changing the semantic meaning of representation. We refer to this property as permutation invariance. Semantic smoothing has been extensively researched in natural language processing (Manning and Schuetze, 1999; Zhai and Lafferty, 2004). Classic smoothing methods, such as Laplacian and Dirichlet, tend to smooth the original distributions to a predefined reference distribution. Recent advances have explored local smoothing, where correlated words are smoothed according to their meaning defined by a semantic network."}, {"heading": "3.5 Text Scale Space", "text": "However, it is not trivial to determine which scale is appropriate for a particular task (D = 1.1). In fact, the best choice usually varies from task to task and from document to document. Even within a document, it might be heterogeneous, varying from paragraph to paragraph and sentence to sentence. For the purpose of automatic modelling, there is no way to decide which scale fits the best. More importantly, it might be impossible to capture all the correct meanings on a single scale. Therefore, the only reasonable way is to represent the document simultaneously on multiple scales, which is exactly the concept of scale space representation, which embeds a textual signal in a continuous scale space, i.e., by a family of progressively smoothed signals parameterized by continuous scale parameters. In particular, for a 2D textual signal f (x, y) we have: \"x, y, x.\""}, {"heading": "4 Scale Space Applications", "text": "Scale space representation creates a new dimension of text analysis. In addition to providing a multi-scale representation that enables text to be analyzed in a scale-invariant manner, it also enables the adaptation and application of established computer vision tools for text analysis. The scale space model can be used in a variety of ways in NLP and text mining. To stimulate further research in this direction, we are initiating some instances."}, {"heading": "4.1 Scale-Invariant Text Classification", "text": "In this section we show how to make text classification an appropriate procedure by exploring the concept of scaleninvariant text structure (SITK). (SITK) Given a pair of documents, d \"s\" and d \"s, at any given scale, the representation induces a single scale core (d\" s). (D \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s"}, {"heading": "4.2 Scale-Invariant Document Retrieval", "text": "It is about the question to what extent people are able to understand themselves and their environment. (...) It is about the question to what extent they are able to understand themselves and the world. (...) It is about the question to what extent they are able to change the world. (...) It is about the question to what extent they want to change the world. (...) It is about the question to what extent they want to change the world. (...) It is about the question to what extent they want to change the world. (...) It is about the question to what extent they want to change the world. (...) It is about the question to what extent they want to change the world of the world of the world of the world of the world of the world of the world of the world of the world of the world of the world, the world of the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world, the world, the world around the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world."}, {"heading": "4.3 Hierarchical Document Keywording", "text": "The extreme (i.e., maxima and minima) of a signal and its first few derivatives contain important information to describe the structure of the signal, such as significance fields, boundaries, corners, ridges, and blobs in an image. The scale space model provides a handy framework for obtaining the extreme of a signal on different scales. In particular, the extreme in the (k \u2212 1) -th derivative of a signal is given by the zero junction in the k-the derivative, which is conveniently achieved in each scale in the scale space by merging the original signal with the derivative material of the Gaussian core, i.e.: the k. k. k. k. k. k."}, {"heading": "4.4 Hierarchical Text Segmentation", "text": "In the previous section, we show how semantic keywords can be extracted from a text in a hierarchical way by following the extreme of its scale model. In the same sense, here, we show how subject boundaries in a text can be identified by following the extreme of the first derivative system. Text segmentation is an important issue in the NLP and has already been extensively investigated (Beeferman et al., 1999). However, many existing approaches are only able to identify a flat structure, i.e., all boundaries are identified on a flat level. A more difficult task is to automatically identify a hierarchical content style structure for a text, i.e. to organize the boundaries of different text units in a tree structure according to their thematic granularities, i.e., chapter boundaries at the top level, followed by the boundaries of the sections, subsections, paragraphs x and sentences as the level of depth increases."}, {"heading": "5 Summary", "text": "This paper presented scalability theory for texts, adapting concepts, formulations and algorithms originally developed for images to address the unique properties of texts in natural language. We also show how scalability models can be used to facilitate a variety of NLP tasks. Along this line, there are a number of promising topics, such as algorithms that expand scalability implementations towards massive corpus, structures of semantic networks that enable efficient or even closed scale core / relevance models, and effective scale invariant descriptors (e.g. named units, themes, semantic text trends) for texts similar to the SIFT feature for images (Lowe, 2004)."}], "references": [{"title": "Statistical models for text segmentation", "author": ["A. Berger", "J. Lafferty"], "venue": "Machine Learning,", "citeRegEx": "Beeferman et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Beeferman et al\\.", "year": 1999}, {"title": "Modeling long distance dependence in language: Topic mixtures vs. dynamic cache models", "author": ["Iyer", "Ostendorf1996] R. Iyer", "M. Ostendorf"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "Iyer et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Iyer et al\\.", "year": 1996}, {"title": "A dynamic language model for speech recognition", "author": ["Jelinek et al.1991] F. Jelinek", "B. Merialdo", "S. Roukos", "M. Strauss"], "venue": null, "citeRegEx": "Jelinek et al\\.,? \\Q1991\\E", "shortCiteRegEx": "Jelinek et al\\.", "year": 1991}, {"title": "The locally weighted bag of words framework for document representation", "author": ["Lebanon et al.2007] G. Lebanon", "Y. Mao", "J. Dillon"], "venue": null, "citeRegEx": "Lebanon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Lebanon et al\\.", "year": 2007}, {"title": "Scale-space theory: A basic tool for analysing structures at different scales", "author": ["T. Lindeberg"], "venue": "Journal of Applied Statistics,", "citeRegEx": "Lindeberg.,? \\Q1994\\E", "shortCiteRegEx": "Lindeberg.", "year": 1994}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": null, "citeRegEx": "Lowe.,? \\Q2004\\E", "shortCiteRegEx": "Lowe.", "year": 2004}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["Manning", "Schuetze1999] C. Manning", "H. Schuetze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Manning et al\\.", "year": 1999}, {"title": "A general optimization framework for smoothing language models on graph structures", "author": ["Mei et al.2008] Q. Mei", "D. Zhang", "C. Zhai"], "venue": "SIGIR", "citeRegEx": "Mei et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2008}, {"title": "A markov random field model for term dependencies", "author": ["Metzler", "Croft2005] D. Metzler", "W. Croft"], "venue": "SIGIR", "citeRegEx": "Metzler et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Metzler et al\\.", "year": 2005}, {"title": "Quantitative evaluation of passage retrieval algorithms for question answering", "author": ["Tellex et al.2003] S. Tellex", "B. Katz", "J. Lin", "A. Fernandes", "G. Marton"], "venue": "SIGIR", "citeRegEx": "Tellex et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Tellex et al\\.", "year": 2003}, {"title": "Scale-space filtering", "author": ["A. Witkin"], "venue": "IJCAI", "citeRegEx": "Witkin.,? \\Q1983\\E", "shortCiteRegEx": "Witkin.", "year": 1983}, {"title": "Feature selection by nonparametric bayes error minimization", "author": ["Yang", "Hu2008] S. Yang", "B. Hu"], "venue": "PAKDD", "citeRegEx": "Yang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2008}, {"title": "Language pyramid and multi-scale text analysis", "author": ["Yang", "Zha2010] S. Yang", "H. Zha"], "venue": "In CIKM \u20192010,", "citeRegEx": "Yang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2010}, {"title": "A study of smoothing methods for language models applied to information retrieval", "author": ["Zhai", "Lafferty2004] C. Zhai", "J. Lafferty"], "venue": "ACM TOIS,", "citeRegEx": "Zhai et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Zhai et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 10, "context": ", a low pass filter with certain properties) (Witkin, 1983; Lindeberg, 1994).", "startOffset": 45, "endOffset": 76}, {"referenceID": 4, "context": ", a low pass filter with certain properties) (Witkin, 1983; Lindeberg, 1994).", "startOffset": 45, "endOffset": 76}, {"referenceID": 4, "context": ", the scalespace axioms (Lindeberg, 1994)).", "startOffset": 24, "endOffset": 41}, {"referenceID": 7, "context": "This contradicts the common knowledge since neighboring words in text are highly correlated both semantically (Mei et al., 2008) and spatially (Lebanon et al.", "startOffset": 110, "endOffset": 128}, {"referenceID": 3, "context": ", 2008) and spatially (Lebanon et al., 2007).", "startOffset": 22, "endOffset": 44}, {"referenceID": 10, "context": "As a result, it motivates more reliable estimate of f by using smooth kernels such as Gaussian (Witkin, 1983; Lindeberg, 1994), which, as we will see, leads exactly to the Gaussian filtering used in the linear scale-space theory.", "startOffset": 95, "endOffset": 126}, {"referenceID": 4, "context": "As a result, it motivates more reliable estimate of f by using smooth kernels such as Gaussian (Witkin, 1983; Lindeberg, 1994), which, as we will see, leads exactly to the Gaussian filtering used in the linear scale-space theory.", "startOffset": 95, "endOffset": 126}, {"referenceID": 10, "context": "Spatial filtering has long been popularized in signal processing (Witkin, 1983; Lindeberg, 1994), and was recently explored in NLP by (Lebanon et al.", "startOffset": 65, "endOffset": 96}, {"referenceID": 4, "context": "Spatial filtering has long been popularized in signal processing (Witkin, 1983; Lindeberg, 1994), and was recently explored in NLP by (Lebanon et al.", "startOffset": 65, "endOffset": 96}, {"referenceID": 3, "context": "Spatial filtering has long been popularized in signal processing (Witkin, 1983; Lindeberg, 1994), and was recently explored in NLP by (Lebanon et al., 2007; Yang and Zha, 2010).", "startOffset": 134, "endOffset": 176}, {"referenceID": 4, "context": "In order not to introduce spurious information, the filter l need to satisfy a set of scale-space axioms (Lindeberg, 1994).", "startOffset": 105, "endOffset": 122}, {"referenceID": 7, "context": "Recent advances explored local smoothing where correlated words are smoothed according to their interrelations defined by a semantic network (Mei et al., 2008).", "startOffset": 141, "endOffset": 159}, {"referenceID": 4, "context": "; see (Lindeberg, 1994) for details and proofs.", "startOffset": 6, "endOffset": 23}, {"referenceID": 5, "context": "More interestingly, scale-space model can also be used, together with techniques for interest point detection (Lowe, 2004), to address passage retrieval (PR) in a scale-invariant manner, i.", "startOffset": 110, "endOffset": 122}, {"referenceID": 9, "context": ", scan through every possible passage, compute relevance scores and rank all of them (Tellex et al., 2003).", "startOffset": 85, "endOffset": 106}, {"referenceID": 10, "context": ", the so-called \u201cinterval tree\u201d in (Witkin, 1983).", "startOffset": 35, "endOffset": 49}, {"referenceID": 0, "context": "Text segmentation is an important topic in NLP and has been extensively investigated previously (Beeferman et al., 1999).", "startOffset": 96, "endOffset": 120}, {"referenceID": 10, "context": "conveniently by the interval tree and coarse-to-fine tracking idea presented in (Witkin, 1983).", "startOffset": 80, "endOffset": 94}, {"referenceID": 5, "context": ", named entities, topics, semantic trends in text) for texts similar to the SIFT feature for images (Lowe, 2004).", "startOffset": 100, "endOffset": 112}], "year": 2012, "abstractText": "Scale-space theory has been established primarily by the computer vision and signal processing communities as a well-founded and promising framework for multi-scale processing of signals (e.g., images). By embedding an original signal into a family of gradually coarsen signals parameterized with a continuous scale parameter, it provides a formal framework to capture the structure of a signal at different scales in a consistent way. In this paper, we present a scale space theory for text by integrating semantic and spatial filters, and demonstrate how natural language documents can be understood, processed and analyzed at multiple resolutions, and how this scale-space representation can be used to facilitate a variety of NLP and text analysis tasks.", "creator": "LaTeX with hyperref package"}}}