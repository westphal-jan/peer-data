{"id": "1212.2262", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2012", "title": "Bag-of-Words Representation for Biomedical Time Series Classification", "abstract": "Automatic analysis of biomedical time series such as electroencephalogram (EEG) and electrocardiographic (ECG) signals has attracted great interest in the community of biomedical engineering due to its important applications in medicine. In this work, a simple yet effective bag-of-words representation that is able to capture both local and global structure similarity information is proposed for biomedical time series representation. In particular, similar to the bag-of-words model used in text document domain, the proposed method treats a time series as a text document and extracts local segments from the time series as words. The biomedical time series is then represented as a histogram of codewords, each entry of which is the count of a codeword appeared in the time series. Although the temporal order of the local segments is ignored, the bag-of-words representation is able to capture high-level structural information because both local and global structural information are well utilized. The performance of the bag-of-words model is validated on three datasets extracted from real EEG and ECG signals. The experimental results demonstrate that the proposed method is not only insensitive to parameters of the bag-of-words model such as local segment length and codebook size, but also robust to noise.", "histories": [["v1", "Tue, 11 Dec 2012 00:49:27 GMT  (681kb,D)", "http://arxiv.org/abs/1212.2262v1", "10 pages, 7 figures. Submitted to IEEE Transaction on Biomedical Engineering"]], "COMMENTS": "10 pages, 7 figures. Submitted to IEEE Transaction on Biomedical Engineering", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jin wang", "ping liu", "mary f h she", "saeid nahavandi", "and abbas kouzani"], "accepted": false, "id": "1212.2262"}, "pdf": {"name": "1212.2262.pdf", "metadata": {"source": "CRF", "title": "Bag-of-words Representation for Biomedical Time Series Classification", "authors": ["Jin Wang", "Ping Liu", "Mary F.H.She", "Saeid Nahavandi", "Abbas Kouzani"], "emails": ["jay.wangjin@gmail."], "sections": [{"heading": null, "text": "However, these biomedical signals are very useful for monitoring the physical condition of humans. However, it is a challenging task to analyze these patterns efficiently and effectively. Traditionally, however, these signals are analyzed manually by professional experts. However, there are several disadvantages of manual analysis. First, it is difficult to compare the large amount of biomedical signals that professional experts, especially those with extensive experience, are very limited. Second, the inspection and monitoring of long-term biomedical signals such as EEG and ECG signals are always very time consuming. It is difficult to maintain a high level of concentration during a lengthy test, which leads to an increase in the false hit rate."}, {"heading": "A. Overview of the Proposed Approach", "text": "In the sack-of-words representation, a time series is treated as a text document and local segments from the time series as words. The general flowchart of the proposed method is demonstrated in Fig. 1. First, we continuously move a window with a predefined length along the time series to extract a group of local segments. Then, we extract a feature vector from each of the local segments using DWT. Next, similar to the sack-of-visual-words model in image and video analysis [17] [18], all local segments from the training time series are grouped by k-means clustering to create a codebook, i.e. the cluster centers are treated as codewords. Then, a local segment is assigned the codeword that has the minimum distance from the local segment, and the time series is represented as a histogram of the codewords. Finally, the sack-of-words representation is used as input for the classification."}, {"heading": "B. Contribution and Organization", "text": "The main contribution of the paper consists of two parts: (i) a simple but effective sack-of-words representation is proposed for the analysis of biomedical time series; (ii) a series of experiments has been conducted to investigate the effectiveness and robustness of the sack-of-words representation for the classification of biomedical time series.The structure of the essay is as follows: Section II describes the proposed method including sack-of-words representation, distance measurements and classification methods.Section III describes the biomedical time series data used in the experiments. Experimental results are reported and analysed in Section IV. Discussion and conclusion are presented in Section V and Section VI respectively."}, {"heading": "II. PROPOSED METHOD", "text": "In this section we describe the sack-of-words representation for classifying biomedical time series. The sack-of-words representation ignores the temporal order of local segments within a time series and displays the time series as a histogram of code words, i.e. local segments."}, {"heading": "A. Bag-of-words Representation", "text": "The process of generating local segments may take place at different levels, all local segments are derived from the local segments that have identified with the local segments at the local level."}, {"heading": "B. Classifier", "text": "Some discriminatory classifiers such as Artificial Neural Networks (ANN) [2], Support Vector Machine (SVM) [21] and Probabilistic Neural Networks (PNN) [13] are widely used for classifying biomedical signals. As our goal in this paper is to investigate the effectiveness of sack-of-words representation, we are using the simplest classifier classifier, the 1-nearest neighbor (1-NN) classifier. Let us not be a test time series and Ri represents the time series from the ith category. Test data is determined as class C of the training sample, which has a minimal distance from the test data, i.e. C \u0445 = argminiD (t, Ri), with D (\u00b7, \u00b7) being the measure of similarity defined below."}, {"heading": "C. Similarity Measure", "text": "In the following, we describe four commonly used similarity measures for measuring the distance between two pouch representations. (D) The Euclidean distance: The Euclidean distance between histogram h and histogram k is defined as: DL2 (h, k) = (h, k) = (h, h) = (h, i) \u2212 k (i) \u2212 k (i) -histogram K2, (2) where DL2 (h, k) is the Euclidean distance commonly used in pattern recognition. 2) Chi-squared Distance: The Euclidean distance subtracts the two histograms bin-by-bin and contributes each bin pair equally to the distance. The problem is that some words like \"the,\" but \"and\" however \"appear more frequently in documents; therefore, they contribute more to the distance in the Euclidean distance measurement."}, {"heading": "D. Practical Implementation", "text": "In practice, this strategy is also used in image and video analysis to reduce the calculation of the code book structure [17]. We continuously move a window along a time series to extract local segments. However, if the time series contains too many data points, a large number of local segments are extracted from the time series, which requires expensive calculations. For example, for a time series consisting of 2000 data points, about 1900 local segments are extracted using a window of 100 data points. In practice, we can move the window with a step of n data points (n = 2, 4 or 8).In this time series consisting of 2000 data points, about 1900 local segments are extracted using a window of 100 lengths."}, {"heading": "III. EXPERIMENTAL DATASETS", "text": "This study uses three sets of data constructed from ECG and ECG signals to evaluate the performance of the ECG imaging; the first set is collected from EEG signals and is widely used for automatic seizure detection; the other two sets are extracted from long ECG signals (more than 1000,000 points) collected from different subjects with random starting points; each of the long ECG signals corresponds to a class, i.e. the identity of the subjects; to show that the ECG imaging can be of different length for time series, the third set of data is extracted at different lengths between 2048 and 4096, while the first two sets of data have the same length of 4096 and 2048. It is worth noting that the extracted ECG time series in the same class may vary in length, but are extracted from the same long ECG signal, but there are significant variations between classes."}, {"heading": "A. EEG Dataset", "text": "The complete EEG data set consists of five classes (i.e. A, B, C, D and E), of which each contains 100 single-channel EEG sequences of the same length 4096. All signals were recorded with the same 128-channel amplifier system and visually examined for artefacts. Set A and set B are from surface EEG recordings of five healthy subjects with eyes open and closed, respectively. The other three sets (C, D and E) are from intracranial EEG recordings of five epileptic patients. Set C and set D are from the epileptic zone or hippocampal formation of the opposite hemisphere of the brain. Set C and set D were recorded at seizure-free intervals, while set E contains only seizure activity. Fig. 4 shows sample time series from each of the five classes."}, {"heading": "B. ECG-40 Dataset", "text": "The ECG-40 dataset was obtained from the Fantasia ECG database [27], which consists of twenty adolescents and twenty healthy old subjects. 40 long ECG signals are collected from each of the forty subjects who were monitored for about two hours at a sampling rate of 250 Hz. All signals contain more than 1000,000 very long data points. We extracted fifty time series of length 2048 with random starting points from each of the forty long signals. In total, the ECG-40 dataset contains 2000 time series of length 2048, evenly distributed among the forty classes."}, {"heading": "C. ECG-15 Dataset", "text": "The ECG-15 dataset consists of 1500 time series extracted from fifteen long ECG signals in the BIDMC Congestive Heart Failure Database. [27] The fifteen long ECG signals were recorded from fifteen patients with severe heart failure. A hundred time series between 2048 and 4096 are extracted from each of the fifteen long random start ECG signals. In total, the ECG-15 dataset consists of fifteen classes, each of which has 100 time series between 2048 and 4096. Table I summarizes the three data sets used in the experiments. It should be noted that the lengths of the 1500 time series in the ECG-15 dataset are not the same, which vary between 2048 and 4096."}, {"heading": "IV. RESULTS", "text": "In this section, we report on experimental results for the three datasets. First, we investigated the effects of parameters by varying the length of local segments and the size of codebook K. We then compared the proposed method with the discrete wavelet transformation (DWT) [13] representation, the discrete Fourier transformation (DFT) [28] representation, the NN classifier based on dynamic time shift (DTW) [29] distance, and the sack-of-patterns representation (BoP) [14]. In addition, we compared the classification accuracies obtained with the proposed method with those obtained with other state-of-the-art methods on the EEG dataset. Finally, we investigated the robustness of the sack-of-words representation vis-\u00e0-vis noise. To ensure an unbiased evaluation, a dataset is randomly divided into ten subsets."}, {"heading": "A. Length of Local Segments", "text": "We varied the length of local segments in the experiments between 8 and 256. Determination of such parameter ranges is based on the fact that the biomedical time series such as ECG and EEG signals are relatively flat. Classification accuracies of the ECG, ECG 40 and ECG 15 datasets with a code book size of 1000 based on the chi-square distance are shown in Fig. 5 (a), Fig. 5 (b) and Fig. 5 (c), respectively. Experimental results show that the performance with respect to the length of local segments is relatively stable when it is between 64 and 192. Classification accuracies increase with the length of less than 16. This is mainly due to the fact that a local segment with too short or too long length cannot capture local structural information within time series. In the following experiments, we empirically set the length of local segments to 128."}, {"heading": "B. Codebook Size", "text": "In order to show the performance of the sack-of-words representation in terms of the size of the codebook, we report on the classification accuracy of the three datasets in Fig. 6, increasing the size of the codebook from 10 to 3500. We see that the results become very stable if the size of the codebook is greater than 500. Classification accuracy quickly decreases if the size of the codebook is smaller than 100, confirming that a compact codebook with too few entries has limited discriminatory capability. Roughly, the optimal size of the codebook can be identified as 1000 x 3500."}, {"heading": "C. Distance Measurement", "text": "We compared the classification performance of the three datasets with the four similarity scales described in Section II-C. Figure 7 shows the classification accuracy based on the four distance scales with codebook sizes 10, 100, 1000 and 2000. We see that the results are slightly different at different distance scales, suggesting that the distance scales have limited impact on the performance of the sack-of-words representation. Overall, the chi square distance measure performs slightly better than the other scales for all four scales in the codebook."}, {"heading": "D. Comparison with Other Methods", "text": "In fact, most people who see themselves in a position to put themselves in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, to put themselves in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live."}, {"heading": "E. Robustness to Noise", "text": "The standard deviation of white Gaussian noise varies, so that the SNRs are between 10 dB and 0 dB. Training data and test data are exactly as separate as in the previous experiments. Table IV summarizes the classification accuracies of the three data sets contaminated by white Gaussian noise with different SNRs. It turns out that the SNRs approach is relatively robust against noise. Accuracies decreased by less than 2 percent when the SNR is 10 dB. Even with considerable noise exposure with the SNR 0 dB, the accuracies decreased by less than 10 percent for the EEG and ECG-40 data sets and only by less than 2 percent for the SNR-15 data sets."}, {"heading": "V. DISCUSSION", "text": "Although the sack-of-words representation ignores the temporal order of local segments, it is able to capture structural information effectively at a high level because the frequency of code words (local segments) is well used in a time series. However, because the local segments are extracted by moving a window along time series, a time series that is not sufficiently long cannot provide enough local segments to capture local structures in a time series. Therefore, the sack-of-words representation may be ineffective in representing short time series, mainly due to the limitation that the sack-of-words representation cannot extract enough meaningful and discriminatory local segments from short sequences. The size of code book N is predefined and empirically determined in the method. A compact, small-size code book has limited discriminatory ability, while a large-size code book is likely to cause noise."}, {"heading": "VI. CONCLUSION", "text": "The proposed method treats a time series like a document and local segments extracted from the time series as words; the time series is represented as a histogram of code words. Although the temporal order information of the local segments is ignored, both local structural and global structural information of the time series is captured. Experimental results from three publicly available datasets show that the sack-of-words representation is effective for characterizing biomedical time series such as EEG and ECG signals. Furthermore, the bag-of-words representation is not only insensitive to the length of local segments and the size of the codebook, but also robust to noise. Distance measurements for comparing histograms are also investigated in the experiments, which shows that the chi-squared distance measure is better suited for comparing histograms than the other distance measures. We compared the performance of the bag-of-words representation of the classical state-of-1 with-of-1."}], "references": [{"title": "Entropies for detection of epilepsy in EEG,", "author": ["N. Kannathal", "M.L. Choo", "U.R. Acharya", "P. Sadasivan"], "venue": "Compu. Meth. Prog. Bio.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Automatic epileptic seizure detection in EEGs based on line length feature and artificial neural networks,", "author": ["L. Guo", "D. Rivero", "J. Dorado", "J.R. Rabual", "A. Pazos"], "venue": "J. Neurosci Meth., vol. 191,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Epileptic EEG classification based on extreme learning machine and nonlinear features,", "author": ["Q. Yuan", "W. Zhou", "S. Li", "D. Cai"], "venue": "Epilepsy Res.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Brain computer interfaces for communication and control,", "author": ["J.R. Wolpaw", "N. Birbaumer", "D.J. McFarland", "G. Pfurtscheller", "T.M. Vaughan"], "venue": "Clin. Neurophysiol.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Multiclass filters by a weighted pairwise criterion for EEG single-trial classification,", "author": ["H. Wang"], "venue": "IEEE Trans. Biomed. Eng.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "A feature selection method for multilevel mental fatigue EEG classification,", "author": ["K.-Q. Shen", "C.-J. Ong", "X.-P. Li", "Z. Hui", "E. Wilder-Smith"], "venue": "IEEE Trans. Biomed. Eng., vol. 54,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "A generic and robust system for automated patient-specific classification of ECG signals,", "author": ["T. Ince", "S. Kiranyaz", "M. Gabbouj"], "venue": "IEEE Trans. Biomed. Eng.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Heartbeat time series classification with support vector machines,", "author": ["A. Kampouraki", "G. Manis", "C. Nikou"], "venue": "IEEE Trans. Inf. Technol. Biomed.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Recurrent neural networks for time series", "author": ["M. Huken", "P. Stagge"], "venue": "classification,\u201d Neurocomputing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Classification of the electrocardiogram signals using supervised classifiers and efficient features,", "author": ["A.E. Zadeh", "A. Khazaee", "V. Ranaee"], "venue": "Compu. Meth. Prog. Bio.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Rotation-invariant similarity in time series using bag-of-patterns representation,", "author": ["J. Lin", "R. Khade", "Y. Li"], "venue": "J. Intell. Inf. Syst,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "The locally weighted bag of words framework for document representation,", "author": ["G. Lebanon", "Y. Mao", "J. Dillon"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Latent dirichlet allocation,", "author": ["D. Blei", "A. Ng", "M. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "A bayesian hierarchical model for learning natural scene categories,", "author": ["L. Fei-Fei", "P. Perona"], "venue": "Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Unsupervised learning of human action categories using spatial-temporal words,", "author": ["J.C. Niebles", "H. Wang", "L. Fei-Fei"], "venue": "Int. J. Comput. Vision.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Creating efficient codebooks for visual recognition,", "author": ["F. Jurie", "B. Triggs"], "venue": "Proc. IEEE Int\u2019l Conf. Computer Vision,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Supervised learning of gaussian mixture models for visual vocabulary generation,", "author": ["B. Fernando", "E. Fromont", "D. Muselet", "M. Sebban"], "venue": "Pattern Recogn.,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "A new metric for probability distributions,", "author": ["D. Endres", "J. Schindelin"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "The pyramid match kernel: Efficient learning with sets of features,", "author": ["K. Grauman", "T. Darrell"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "ECG analysis: a new approach in human identification,", "author": ["L. Biel", "O. Pettersson", "L. Philipson", "P. Wide"], "venue": "IEEE Trans. Instrum. Meas., vol. 50,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Human identification by quantifying similarity and dissimilarity in electrocardiogram phase space,", "author": ["S.-C. Fang", "H.-L. Chan"], "venue": "Pattern Recogn.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: dependence on recording region and brain state.", "author": ["R.G. Andrzejak", "K. Lehnertz", "F. Mormann", "C. Rieke", "P. David", "C.E. Elger"], "venue": "Phys. rev. E,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2001}, {"title": "A review on time series data mining,", "author": ["T. chung Fu"], "venue": "Eng. Appl. Artif. Intel.,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Classification of epileptiform EEG using a hybrid system based on decision tree classifier and fast fourier transform,", "author": ["K. Polat", "S. G\u00fcnes"], "venue": "Appl. Math. Comput.,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Automatic detection of epileptic seizures in EEG using discrete wavelet transform and approximate entropy,", "author": ["H. Ocak"], "venue": "Expert Syst. Appl.,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "For instance, EEG signals are automatically analyzed for epileptic seizure detection [1] [2] [3], brain computer interaction [4] [5] [6], human mental fatigue detection [7] and emotion recognition [8].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "For instance, EEG signals are automatically analyzed for epileptic seizure detection [1] [2] [3], brain computer interaction [4] [5] [6], human mental fatigue detection [7] and emotion recognition [8].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "For instance, EEG signals are automatically analyzed for epileptic seizure detection [1] [2] [3], brain computer interaction [4] [5] [6], human mental fatigue detection [7] and emotion recognition [8].", "startOffset": 93, "endOffset": 96}, {"referenceID": 3, "context": "For instance, EEG signals are automatically analyzed for epileptic seizure detection [1] [2] [3], brain computer interaction [4] [5] [6], human mental fatigue detection [7] and emotion recognition [8].", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "For instance, EEG signals are automatically analyzed for epileptic seizure detection [1] [2] [3], brain computer interaction [4] [5] [6], human mental fatigue detection [7] and emotion recognition [8].", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "For instance, EEG signals are automatically analyzed for epileptic seizure detection [1] [2] [3], brain computer interaction [4] [5] [6], human mental fatigue detection [7] and emotion recognition [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 6, "context": "ECG signals that provide useful information about heart rhythm are used to study heart arrhythmias [9] [10].", "startOffset": 99, "endOffset": 102}, {"referenceID": 7, "context": "ECG signals that provide useful information about heart rhythm are used to study heart arrhythmias [9] [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 8, "context": "Some methods [11] [12] directly describe time series in time domain while some others extract features from transformed domain [2] [13] [10].", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "Some methods [11] [12] directly describe time series in time domain while some others extract features from transformed domain [2] [13] [10].", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "Some methods [11] [12] directly describe time series in time domain while some others extract features from transformed domain [2] [13] [10].", "startOffset": 127, "endOffset": 130}, {"referenceID": 7, "context": "Some methods [11] [12] directly describe time series in time domain while some others extract features from transformed domain [2] [13] [10].", "startOffset": 136, "endOffset": 140}, {"referenceID": 9, "context": "[12] extracted morphological and timing-interval features from ECG segments to classify heartbeats.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] extracted line length features based on Discrete Wavelet Transform (DWT) to detect epileptic EEG segments.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "In order to capture the high-level structural information of time series, Lin [14] proposed a bag-of-patterns (BoP) representation by converting a time series to a words string using the Symbolic", "startOffset": 78, "endOffset": 82}, {"referenceID": 11, "context": "In this work, motivated by the success of the bag-ofwords model in text document analysis [15] [16] and image analysis [17] [18], we propose a simple yet effective bag-ofwords representation whose dimension is much lower than the bag-of-patterns representation to characterize biomedical time series.", "startOffset": 90, "endOffset": 94}, {"referenceID": 12, "context": "In this work, motivated by the success of the bag-ofwords model in text document analysis [15] [16] and image analysis [17] [18], we propose a simple yet effective bag-ofwords representation whose dimension is much lower than the bag-of-patterns representation to characterize biomedical time series.", "startOffset": 95, "endOffset": 99}, {"referenceID": 13, "context": "In this work, motivated by the success of the bag-ofwords model in text document analysis [15] [16] and image analysis [17] [18], we propose a simple yet effective bag-ofwords representation whose dimension is much lower than the bag-of-patterns representation to characterize biomedical time series.", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "In this work, motivated by the success of the bag-ofwords model in text document analysis [15] [16] and image analysis [17] [18], we propose a simple yet effective bag-ofwords representation whose dimension is much lower than the bag-of-patterns representation to characterize biomedical time series.", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "Recently, the bag-of-words model is extended to analyze images and videos in computer vision [17] [18].", "startOffset": 93, "endOffset": 97}, {"referenceID": 14, "context": "Recently, the bag-of-words model is extended to analyze images and videos in computer vision [17] [18].", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "Next, similar to the bag-of-visual-words model in images and videos analysis [17] [18], all local segments from the training time series are clustered by k-means clustering to create a codebook, i.", "startOffset": 77, "endOffset": 81}, {"referenceID": 14, "context": "Next, similar to the bag-of-visual-words model in images and videos analysis [17] [18], all local segments from the training time series are clustered by k-means clustering to create a codebook, i.", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "The classical k-means clustering algorithm [17] [18] is commonly used to construct the codebook, although some other unsupervised and supervised methods are also developed such as mean-sift [19] and supervised Gaussian mixture models [20].", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "The classical k-means clustering algorithm [17] [18] is commonly used to construct the codebook, although some other unsupervised and supervised methods are also developed such as mean-sift [19] and supervised Gaussian mixture models [20].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "The classical k-means clustering algorithm [17] [18] is commonly used to construct the codebook, although some other unsupervised and supervised methods are also developed such as mean-sift [19] and supervised Gaussian mixture models [20].", "startOffset": 190, "endOffset": 194}, {"referenceID": 16, "context": "The classical k-means clustering algorithm [17] [18] is commonly used to construct the codebook, although some other unsupervised and supervised methods are also developed such as mean-sift [19] and supervised Gaussian mixture models [20].", "startOffset": 234, "endOffset": 238}, {"referenceID": 1, "context": "Some discriminative classifiers such as Artificial Neural Networks (ANN) [2], Support Vector Machine (SVM) [21], and Probabilistic Neural Networks (PNN) [13] are widely used for biomedical signal classification.", "startOffset": 73, "endOffset": 76}, {"referenceID": 17, "context": "In order to keep the distance symmetric, the Jensen-Shannon distance [22] is introduced as a symmetric extension:", "startOffset": 69, "endOffset": 73}, {"referenceID": 18, "context": "The distance based on the histogram intersection is defined as [23]:", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "This strategy is also employed in image and video analysis to reduce the computation of codebook construction [17], [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 14, "context": "This strategy is also employed in image and video analysis to reduce the computation of codebook construction [17], [18].", "startOffset": 116, "endOffset": 120}, {"referenceID": 19, "context": ", extracted ECG time series, to their subjects\u2019 identity, which can be used for human identification from ECG signals in real application [24], [25].", "startOffset": 138, "endOffset": 142}, {"referenceID": 20, "context": ", extracted ECG time series, to their subjects\u2019 identity, which can be used for human identification from ECG signals in real application [24], [25].", "startOffset": 144, "endOffset": 148}, {"referenceID": 19, "context": "However, the bagof-words representation does not need to detect or localize any heartbeat waveforms or fiducial points, which is always required in previous works [24], [25].", "startOffset": 163, "endOffset": 167}, {"referenceID": 20, "context": "However, the bagof-words representation does not need to detect or localize any heartbeat waveforms or fiducial points, which is always required in previous works [24], [25].", "startOffset": 169, "endOffset": 173}, {"referenceID": 21, "context": "The EEG dataset described in [26] was used in our experiments.", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "Then, we compared the proposed method with the Discrete Wavelet Transform (DWT) [13] representation, the Discrete Fourier Transform (DFT) [28] representation, the NN classifier based on Dynamic Time Warping (DTW) [29] distance and the bag-of-patterns representation (BoP) [14].", "startOffset": 213, "endOffset": 217}, {"referenceID": 10, "context": "Then, we compared the proposed method with the Discrete Wavelet Transform (DWT) [13] representation, the Discrete Fourier Transform (DFT) [28] representation, the NN classifier based on Dynamic Time Warping (DTW) [29] distance and the bag-of-patterns representation (BoP) [14].", "startOffset": 272, "endOffset": 276}, {"referenceID": 22, "context": "We compared the performance of the proposed bag-ofwords representation with that of the DWT representation [13], the DFT representation [28], and the NN classifier based on the DTW distance [29].", "startOffset": 190, "endOffset": 194}, {"referenceID": 10, "context": "proposed bag-of-words representation with the bag-of-patterns representation [14], which is very similar to the proposed approach.", "startOffset": 77, "endOffset": 81}, {"referenceID": 10, "context": "BoP [14] 87.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "[1] A, E 2 Entropy+adaptive neuro-fuzzy inference system 95", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Polat and G\u00fcnes [30] A, E 2 FFT+decision tree 98.", "startOffset": 16, "endOffset": 20}, {"referenceID": 2, "context": "[3] D, E 2 Nonlinear features+extreme learning machine 96.", "startOffset": 0, "endOffset": 3}, {"referenceID": 24, "context": "Ocak [31] (A, C, D), E 2 DWT+approximate entropy 96.", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "[2] A, E 2 Line length feature based on DWT+artificial neural networks 99.", "startOffset": 0, "endOffset": 3}], "year": 2012, "abstractText": "Automatic analysis of biomedical time series such as electroencephalogram (EEG) and electrocardiographic (ECG) signals has attracted great interest in the community of biomedical engineering due to its important applications in medicine. In this work, a simple yet effective bag-of-words representation that is able to capture both local and global structure similarity information is proposed for biomedical time series representation. In particular, similar to the bag-of-words model used in text document domain, the proposed method treats a time series as a text document and extracts local segments from the time series as words. The biomedical time series is then represented as a histogram of codewords, each entry of which is the count of a codeword appeared in the time series. Although the temporal order of the local segments is ignored, the bag-of-words representation is able to capture high-level structural information because both local and global structural information are well utilized. The performance of the bag-of-words model is validated on three datasets extracted from real EEG and ECG signals. The experimental results demonstrate that the proposed method is not only insensitive to parameters of the bag-of-words model such as local segment length and codebook size, but also robust to noise.", "creator": "LaTeX with hyperref package"}}}