{"id": "1212.2287", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Dec-2012", "title": "Runtime Optimizations for Prediction with Tree-Based Models", "abstract": "Tree-based models have proven to be an effective solution for web ranking as well as other problems in diverse domains. This paper focuses on optimizing the runtime performance of applying such models to make predictions, given an already-trained model. Although exceedingly simple conceptually, most implementations of tree-based models do not efficiently utilize modern superscalar processor architectures. By laying out data structures in memory in a more cache-conscious fashion, removing branches from the execution flow using a technique called predication, and micro-batching predictions using a technique called vectorization, we are able to better exploit modern processor architectures and significantly improve the speed of tree-based models over hard-coded if-else blocks. Our work represents the first instance of an architecture-conscious runtime implementation of tree-based models that we are aware of.", "histories": [["v1", "Tue, 11 Dec 2012 03:20:46 GMT  (61kb,D)", "https://arxiv.org/abs/1212.2287v1", null], ["v2", "Fri, 26 Apr 2013 16:33:08 GMT  (60kb,D)", "http://arxiv.org/abs/1212.2287v2", null]], "reviews": [], "SUBJECTS": "cs.DB cs.IR cs.LG", "authors": ["nima asadi", "jimmy lin", "arjen p de vries"], "accepted": false, "id": "1212.2287"}, "pdf": {"name": "1212.2287.pdf", "metadata": {"source": "CRF", "title": "Runtime Optimizations for Prediction with Tree-Based Models", "authors": ["Nima Asadi", "Jimmy Lin", "Arjen P. de Vries"], "emails": ["nima@cs.umd.edu,", "jimmylin@umd.edu,", "arjen@acm.org"], "sections": [{"heading": "1. INTRODUCTION", "text": "Recent years have shown that the number of people able to remain in the EU has increased sharply in recent years, while the number of people living in the EU has fallen by more than half in the last ten years."}, {"heading": "2. BACKGROUND AND RELATED WORK", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "3. TREE IMPLEMENTATIONS", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "4. EXPERIMENTAL SETUP", "text": "Given that the focus of our work is on efficiency, our primary evaluation metric is prediction speed. We define this as the elapsed time between the moment a feature vector (i.e., a test instance) is submitted to the tree-based model until a prediction (in our case a regression value) is made for the instance. To increase the reliability of our results, we conducted several tests and reported mean and variance. All experiments were conducted on a Red Hat Linux server running Intel Xeon Westmere quad-core processors (E5620 2.4GHz), which architecture has a 64KB L1 cache per core split between data and instructions."}, {"heading": "4.1 Synthetic Data", "text": "The synthetic data consisted of randomly generated trees and randomly generated feature vectors. Each internode in a tree has two fields: a feature ID and a threshold at which the decision is made. Each leaf is associated with a regression value. Constructing a random tree of depth d begins with the root node. We randomly select a feature ID and generate a random threshold to split the tree into left and right subtrees. This process is performed recursively to build each subtree until we reach the desired tree depth. When we reach a leaf node, we generate a random regression value. Note that our randomly generated trees are fully balanced, i.e., a tree of depth d has 2d leaf nodes. Once a tree is constructed, the next step is to generate random feature vectors that we are able to generate a random feature vector."}, {"heading": "4.2 Learning-to-Rank Experiments", "text": "In addition to randomly generated trees, we have conducted experiments with standard Learning-to-Rank datasets, in which training, validation, and test data are provided; with the training and validation sets, we have learned a complete Treeensemble ranking model, and the evaluation is then run on test instances to determine the speed of the various implementations; these experiments evaluate performance in a real-world application; and we have used Gradient-Increased Regression Trees (GBRTs) to train a Learning-to-Rank model. GBRTs are ensembles of regression trees that reach the state of the art in terms of learning tasks; and the learning algorithm adds new trees one after another that best take into account the remaining regression error (i.e., the residuals are used by us)."}, {"heading": "5. RESULTS", "text": "In this section, we present experimental results, starting with the evaluation of synthetic data and then using Learning Torank datasets."}, {"heading": "5.1 Synthetic Data: Base Results", "text": "This year it is more than ever before."}, {"heading": "5.2 Tuning Vectorization Parameter", "text": "In Section 3, we proposed vectorizing the prediction technique to mask memory latences.The idea is to work simultaneously on V instances (feature vectors) so that the processor waits for memory access for one instance, useful calculation can happen on another. This uses pipelining and multiple shipments in modern superscalar processors. The effectiveness of vectorization depends on the relationship between the time spent in the actual calculation and memory latences.For example, if memory takes only one cycle, vectorization cannot possibly help. The longer the memory latencies, the more we would expect vectorization (larger batch sizes) to help. But beyond a certain point, memory latencies are effectively masked by vectorization, then vectorization cannot help."}, {"heading": "5.3 Learning-to-Rank Experiments", "text": "After evaluating various implementations of synthetic data, we now arrive at learning-to-rank data sets based on tree groups. As already described, we have used the implementation of LambdaMART by Ganjisaffar et al. [8]."}, {"heading": "3 17.4 22.6 23% 26.0 33%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 21.3 31.9 33% 41.3 48%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 25.1 44.4 44% 52.4 52%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 28.9 58.9 51% 63.9 55%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "11 39.2 75.8 57% 85.8 54%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 42.2 61.0 31% 50.0 16%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 55.8 85.9 35% 64.6 14%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 69.3 96.3 28% 76.2 9%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 77.9 102.0 24% 85.8 9%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "11 89.6 118.7 25% 116.0 23%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 49.7 110.6 55% 82.8 40%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 72.3 200.3 64% 123.9 42%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 97.8 302.5 68% 164.2 40%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9 120.4 395.5 70% 187.8 36%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "11 149.5 476.1 69% 250.7 40%", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "6. DISCUSSION AND FUTURE WORK", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "7. CONCLUSION", "text": "Modern processor architectures are incredibly complex because technological improvements are uneven. In this paper, we focus on one particular issue: Not all memory references are equally fast, and in fact, latency can vary by an order of magnitude. There are a number of mechanisms for obscuring these latencies, although it largely depends on developers knowing how to exploit these mechanisms. The database community has been studying these problems for some time, and in this regard, the communities of information gathering, machine learning, and data mining.In this paper, we show that two relatively simple techniques, prediction and vectorization, along with more efficient memory layouts, can significantly accelerate prediction performance for tree-based models, both on synthetic data and on data sets that are learning to rank in the real world. Our work deals with architectural-conscious implementations of a learning machine-like learning model, but we also believe that there are many other areas where there are."}, {"heading": "8. ACKNOWLEDGMENTS", "text": "This work has been supported by NSF under the awards IIS0916043, IIS-1144034 and IIS-1218043. Any opinions, findings, conclusions or recommendations expressed are those of the authors and do not necessarily reflect those of the sponsor. Deepest gratitude from the first author goes to Katherine for her invaluable encouragement and sincere support. The second author is grateful to Esther and Kiri for their loving support and dedicates this work to Joshua and Jacob."}, {"heading": "9. REFERENCES", "text": "[1] A. Ailamaki, D. DeWitt, M. Hill, and D. Wood.DBMSs on a modern processor: Where does time go? Proceedings of the 25th International Conference on Very Large Data Bases (VLDB1999), pp. 266-277, Lyon, Scotland, 1999. [2] D. August, W. Hwu, and S. Mahlke. [3] P. Boncz, M. Kersten, and S. Manegold. Breaking the memory wall in MonetDB. Communications of the ACM, 51 (12): 77-85, 2008. [4] P. Boncz, M. Zukowski, and N. Nes."}], "references": [{"title": "DBMSs on a modern processor: Where does time go", "author": ["A. Ailamaki", "D. DeWitt", "M. Hill", "D. Wood"], "venue": "Proceedings of the 25th International Conference on Very Large Data Bases (VLDB1999),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "A framework for balancing control flow and predication", "author": ["D. August", "W. Hwu", "S. Mahlke"], "venue": "Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Breaking the memory wall in MonetDB", "author": ["P. Boncz", "M. Kersten", "S. Manegold"], "venue": "Communications of the ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "MonetDB/X100: Hyper-pipelining query execution", "author": ["P. Boncz", "M. Zukowski", "N. Nes"], "venue": "Proceedings of the 2nd Biennial Conference on Innovative Data Systems Research (CIDR2005),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "From RankNet to LambdaRank to LambdaMART: An overview", "author": ["C. Burges"], "venue": "Technical Report MSR-TR-2010-82, Microsoft Research,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Early exit optimizations for additive machine learned ranking systems", "author": ["B. Cambazoglu", "H. Zaragoza", "O. Chapelle", "J. Chen", "C. Liao", "Z. Zheng", "J. Degenhardt"], "venue": "Proceedings of the 3rd ACM International Conference on Web Search and Data Mining (WSDM", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning", "author": ["A. Criminisi", "J. Shotton", "E. Konukoglu"], "venue": "Foundations and Trends in Computer Graphics and Vision,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Bagging gradient-boosted trees for high precision, low variance ranking models", "author": ["Y. Ganjisaffar", "R. Caruana", "C. Lopes"], "venue": "Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Classification of premalignant pancreatic cancer mass-spectrometry data using decision tree ensembles", "author": ["G. Ge", "G. Wong"], "venue": "BMC Bioinformatics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2008}, {"title": "The Memory System: You Can\u2019t Avoid It, You Can\u2019t Ignore It, You Can\u2019t Fake It", "author": ["B. Jacob"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Cumulative gain-based evaluation of IR techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "Financial black swans driven by ultrafast machine ecology", "author": ["N. Johnson", "G. Zhao", "E. Hunsader", "J. Meng", "A. Ravindar", "S. Carran", "B. Tivnan"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Wish branches: Enabling adaptive and aggressive predicated execution", "author": ["H. Kim", "O. Mutlu", "Y. Patt", "J. Stark"], "venue": "IEEE Micro,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Learning to Rank for Information Retrieval and Natural Language Processing", "author": ["H. Li"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "The future of microprocessors", "author": ["K. Olukotun", "L. Hammond"], "venue": "ACM Queue,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "PLANET: Massively parallel learning of tree ensembles with MapReduce", "author": ["B. Panda", "J. Herbach", "S. Basu", "R. Bayardo"], "venue": "Proceedings of the 35th International Conference on Very Large Data Bases (VLDB2009),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Cache conscious indexing for decision-support in main memory", "author": ["J. Rao", "K. Ross"], "venue": "Proceedings of the 25th International Conference on Very Large Data Bases (VLDB1999),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Architecture sensitive database design: Examples from the Columbia group", "author": ["K. Ross", "J. Cieslewicz", "J. Rao", "J. Zhou"], "venue": "Bulletin of the Technical Committee on Data Engineering,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Predicting gene function using hierarchical multi-label decision tree ensembles", "author": ["L. Schietgat", "C. Vens", "J. Struyf", "H. Blockeel", "D. Kocev", "S. Dz\u0306eroski"], "venue": "BMC Bioinformatics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Large-scale learning to rank using boosted decision trees. Scaling Up Machine Learning", "author": ["K. Svore", "C. Burges"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Parallel boosted regression trees for web search ranking", "author": ["S. Tyree", "K. Weinberger", "K. Agrawal"], "venue": "Proceedings of the 20th International Conference on World Wide Web (WWW2011),", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "A cascade ranking model for efficient ranked retrieval", "author": ["L. Wang", "J. Lin", "D. Metzler"], "venue": "Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "The greedy miser: Learning under test-time budgets", "author": ["Z. Xu", "K. Weinberger", "O. Chapelle"], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "MonetDB/X100\u2014a DBMS in the CPU cache", "author": ["M. Zukowski", "P. Boncz", "N. Nes", "S. H\u00e9man"], "venue": "Bulletin of the Technical Committee on Data Engineering,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "Recent studies have shown that machine-learned tree-based models, combined with ensemble techniques, are highly effective for building web ranking algorithms [5, 8, 21] within the \u201clearning to rank\u201d framework [14].", "startOffset": 158, "endOffset": 168}, {"referenceID": 7, "context": "Recent studies have shown that machine-learned tree-based models, combined with ensemble techniques, are highly effective for building web ranking algorithms [5, 8, 21] within the \u201clearning to rank\u201d framework [14].", "startOffset": 158, "endOffset": 168}, {"referenceID": 20, "context": "Recent studies have shown that machine-learned tree-based models, combined with ensemble techniques, are highly effective for building web ranking algorithms [5, 8, 21] within the \u201clearning to rank\u201d framework [14].", "startOffset": 158, "endOffset": 168}, {"referenceID": 13, "context": "Recent studies have shown that machine-learned tree-based models, combined with ensemble techniques, are highly effective for building web ranking algorithms [5, 8, 21] within the \u201clearning to rank\u201d framework [14].", "startOffset": 209, "endOffset": 213}, {"referenceID": 15, "context": "Beyond document retrieval, tree-based models have also been proven effective for tackling problems in diverse domains such as online advertising [16], medical diagnosis [9], genomic analysis [19], and computer vision [7].", "startOffset": 145, "endOffset": 149}, {"referenceID": 8, "context": "Beyond document retrieval, tree-based models have also been proven effective for tackling problems in diverse domains such as online advertising [16], medical diagnosis [9], genomic analysis [19], and computer vision [7].", "startOffset": 169, "endOffset": 172}, {"referenceID": 18, "context": "Beyond document retrieval, tree-based models have also been proven effective for tackling problems in diverse domains such as online advertising [16], medical diagnosis [9], genomic analysis [19], and computer vision [7].", "startOffset": 191, "endOffset": 195}, {"referenceID": 6, "context": "Beyond document retrieval, tree-based models have also been proven effective for tackling problems in diverse domains such as online advertising [16], medical diagnosis [9], genomic analysis [19], and computer vision [7].", "startOffset": 217, "endOffset": 220}, {"referenceID": 11, "context": "Firms fight over the length of cables due to speed-of-light propagation delays, both within an individual datacenter and across oceans [12].", "startOffset": 135, "endOffset": 139}, {"referenceID": 0, "context": "Although similar work has long existed in the database community [1, 17, 18, 24, 3], there is little research on the application of architecture-conscious optimizations for information retrieval and machine learning problems.", "startOffset": 65, "endOffset": 83}, {"referenceID": 16, "context": "Although similar work has long existed in the database community [1, 17, 18, 24, 3], there is little research on the application of architecture-conscious optimizations for information retrieval and machine learning problems.", "startOffset": 65, "endOffset": 83}, {"referenceID": 17, "context": "Although similar work has long existed in the database community [1, 17, 18, 24, 3], there is little research on the application of architecture-conscious optimizations for information retrieval and machine learning problems.", "startOffset": 65, "endOffset": 83}, {"referenceID": 23, "context": "Although similar work has long existed in the database community [1, 17, 18, 24, 3], there is little research on the application of architecture-conscious optimizations for information retrieval and machine learning problems.", "startOffset": 65, "endOffset": 83}, {"referenceID": 2, "context": "Although similar work has long existed in the database community [1, 17, 18, 24, 3], there is little research on the application of architecture-conscious optimizations for information retrieval and machine learning problems.", "startOffset": 65, "endOffset": 83}, {"referenceID": 14, "context": "The broadest trend is perhaps the multi-core revolution [15]: the relentless march of Moore\u2019s Law continues to increase the number of transistors on a chip exponentially, but experts widely agree that we are long past the point of diminishing returns in extracting instruction-level parallelism in hardware.", "startOffset": 56, "endOffset": 60}, {"referenceID": 2, "context": "A less-discussed, but just as important trend over the past two decades is the so-called \u201cmemory wall\u201d [3], where increases in processor speed have far outpaced improvements in memory latency.", "startOffset": 103, "endOffset": 106}, {"referenceID": 9, "context": "There is, of course, much more complexity beyond this short description; see [10] for an overview.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "The database community has explored in depth the consequences of modern processor architectures for relational query processing [1, 17, 18, 24, 3].", "startOffset": 128, "endOffset": 146}, {"referenceID": 16, "context": "The database community has explored in depth the consequences of modern processor architectures for relational query processing [1, 17, 18, 24, 3].", "startOffset": 128, "endOffset": 146}, {"referenceID": 17, "context": "The database community has explored in depth the consequences of modern processor architectures for relational query processing [1, 17, 18, 24, 3].", "startOffset": 128, "endOffset": 146}, {"referenceID": 23, "context": "The database community has explored in depth the consequences of modern processor architectures for relational query processing [1, 17, 18, 24, 3].", "startOffset": 128, "endOffset": 146}, {"referenceID": 2, "context": "The database community has explored in depth the consequences of modern processor architectures for relational query processing [1, 17, 18, 24, 3].", "startOffset": 128, "endOffset": 146}, {"referenceID": 15, "context": "Researchers have explored scaling the training of tree-based models to massive datasets [16, 20], which is of course an important problem, but orthogonal to the issue we tackle here: given a trained model, how do we make predictions quickly? Another salient property of modern CPUs is pipelining, where instruction execution is split between several stages (modern processors have between one to two dozen stages).", "startOffset": 88, "endOffset": 96}, {"referenceID": 19, "context": "Researchers have explored scaling the training of tree-based models to massive datasets [16, 20], which is of course an important problem, but orthogonal to the issue we tackle here: given a trained model, how do we make predictions quickly? Another salient property of modern CPUs is pipelining, where instruction execution is split between several stages (modern processors have between one to two dozen stages).", "startOffset": 88, "endOffset": 96}, {"referenceID": 0, "context": "The impact of data and control hazards can be substantial: an influential paper in 1999 concluded that in commercial RDBMSes at the time, almost half of the execution time is spent on stalls [1].", "startOffset": 191, "endOffset": 194}, {"referenceID": 1, "context": "However, with a technique called predication [2, 13], which we explore in our work, it is possible to convert control dependencies into data dependencies (see Section 3).", "startOffset": 45, "endOffset": 52}, {"referenceID": 12, "context": "However, with a technique called predication [2, 13], which we explore in our work, it is possible to convert control dependencies into data dependencies (see Section 3).", "startOffset": 45, "endOffset": 52}, {"referenceID": 3, "context": "Another optimization that we adopt, called vectorization, was pioneered by database researchers [4, 24]: the basic idea is that instead of processing a tuple at a time, a relational query engine should process a \u201cvector\u201d (i.", "startOffset": 96, "endOffset": 103}, {"referenceID": 23, "context": "Another optimization that we adopt, called vectorization, was pioneered by database researchers [4, 24]: the basic idea is that instead of processing a tuple at a time, a relational query engine should process a \u201cvector\u201d (i.", "startOffset": 96, "endOffset": 103}, {"referenceID": 13, "context": "Beyond processor architectures, the other area of relevant work is the vast literature on learning to rank [14], application of machine learning techniques to document ranking in search.", "startOffset": 107, "endOffset": 111}, {"referenceID": 4, "context": "Our work uses gradient-boosted regression trees (GBRTs) [5, 21, 8], a state-of-the-art ensemble method.", "startOffset": 56, "endOffset": 66}, {"referenceID": 20, "context": "Our work uses gradient-boosted regression trees (GBRTs) [5, 21, 8], a state-of-the-art ensemble method.", "startOffset": 56, "endOffset": 66}, {"referenceID": 7, "context": "Our work uses gradient-boosted regression trees (GBRTs) [5, 21, 8], a state-of-the-art ensemble method.", "startOffset": 56, "endOffset": 66}, {"referenceID": 21, "context": "The focus of most learning-to-rank research is on learning effective models, without considering efficiency, although there is an emerging thread of work that attempts to better balance both factors [22, 23].", "startOffset": 199, "endOffset": 207}, {"referenceID": 22, "context": "The focus of most learning-to-rank research is on learning effective models, without considering efficiency, although there is an emerging thread of work that attempts to better balance both factors [22, 23].", "startOffset": 199, "endOffset": 207}, {"referenceID": 16, "context": "This is similar to the idea behind CSS-Trees [17] used in the database community.", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "A well-known trick in the compiler community for overcoming these issues is known as predication [2, 13].", "startOffset": 97, "endOffset": 104}, {"referenceID": 12, "context": "A well-known trick in the compiler community for overcoming these issues is known as predication [2, 13].", "startOffset": 97, "endOffset": 104}, {"referenceID": 3, "context": "A common technique adopted in the database literature to mask these memory latencies is called vectorization [4, 24].", "startOffset": 109, "endOffset": 116}, {"referenceID": 23, "context": "A common technique adopted in the database literature to mask these memory latencies is called vectorization [4, 24].", "startOffset": 109, "endOffset": 116}, {"referenceID": 4, "context": "We used gradient-boosted regression trees (GBRTs) [5, 21, 8] to train a learning-to-rank model.", "startOffset": 50, "endOffset": 60}, {"referenceID": 20, "context": "We used gradient-boosted regression trees (GBRTs) [5, 21, 8] to train a learning-to-rank model.", "startOffset": 50, "endOffset": 60}, {"referenceID": 7, "context": "We used gradient-boosted regression trees (GBRTs) [5, 21, 8] to train a learning-to-rank model.", "startOffset": 50, "endOffset": 60}, {"referenceID": 10, "context": "We used the open-source jforests implementation of LambdaMART to optimize NDCG [11].", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Table 3 shows the average NDCG values (at different ranks) measured across five folds on the LETOR and MSLR datasets with different values of this parameter, similar to the range of values explored in [8].", "startOffset": 201, "endOffset": 204}, {"referenceID": 5, "context": "[6] (authors from Yahoo!) experimented with reranking 200 candidate documents to produce the final ranked list of 20 results (the first two pages of search results).", "startOffset": 0, "endOffset": 3}, {"referenceID": 21, "context": "Thus, it is perhaps possible to jointly learn models that are both fast and good, as in the recently-proposed \u201clearning to efficiently rank\u201d framework [22, 23].", "startOffset": 151, "endOffset": 159}, {"referenceID": 22, "context": "Thus, it is perhaps possible to jointly learn models that are both fast and good, as in the recently-proposed \u201clearning to efficiently rank\u201d framework [22, 23].", "startOffset": 151, "endOffset": 159}], "year": 2013, "abstractText": "Tree-based models have proven to be an effective solution for web ranking as well as other problems in diverse domains. This paper focuses on optimizing the runtime performance of applying such models to make predictions, given an alreadytrained model. Although exceedingly simple conceptually, most implementations of tree-based models do not efficiently utilize modern superscalar processor architectures. By laying out data structures in memory in a more cache-conscious fashion, removing branches from the execution flow using a technique called predication, and micro-batching predictions using a technique called vectorization, we are able to better exploit modern processor architectures and significantly improve the speed of tree-based models over hard-coded if-else blocks. Our work contributes to the exploration of architecture-conscious runtime implementations of machine learning algorithms.", "creator": "TeX"}}}