{"id": "1212.3493", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2012", "title": "Sentence Compression in Spanish driven by Discourse Segmentation and Language Models", "abstract": "Previous works demonstrated that Automatic Text Summarization (ATS) by sentences extraction may be improved using sentence compression. In this work we present a sentence compressions approach guided by level-sentence discourse segmentation and probabilistic language models (LM). The results presented here show that the proposed solution is able to generate coherent summaries with grammatical compressed sentences. The approach is simple enough to be transposed into other languages.", "histories": [["v1", "Fri, 14 Dec 2012 14:51:22 GMT  (19kb)", "https://arxiv.org/abs/1212.3493v1", "7 pages, 3 tables"], ["v2", "Mon, 17 Dec 2012 13:10:47 GMT  (19kb)", "http://arxiv.org/abs/1212.3493v2", "7 pages, 3 tables"]], "COMMENTS": "7 pages, 3 tables", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["alejandro molina", "juan-manuel torres-moreno", "iria da cunha", "eric sanjuan", "gerardo sierra"], "accepted": false, "id": "1212.3493"}, "pdf": {"name": "1212.3493.pdf", "metadata": {"source": "CRF", "title": "Sentence Compression in Spanish driven by Discourse Segmentation and Language Models", "authors": ["Alejandro Molina", "Juan-Manuel Torres-Moreno", "Iria da Cunha", "Eric SanJuan", "Gerardo Sierra"], "emails": ["alejandro.molina@univ-avignon.fr", "juan-manuel.torres@univ-avignon.fr", "eric.sanjuan@univ-avignon.fr", "iria.dacunha@upf.edu", "gerardo.sierra@iingen.unam.mx"], "sections": [{"heading": null, "text": "ar Xiv: 121 2,34 93v2 [cs.CL]"}, {"heading": "1 Introduction", "text": "Automatic text compression (ATS) is indispensable for coping with ever-increasing amounts of valuable information. An abstraction is by far the most concrete and recognised form of text compression [1]. Sentence compression enables the generation of summaries by extraction sentences [10, 5, 24]. Sentence compression can be used to improve extraction compression [6, 13]. Previous work suggests that sentence segmentation could be helpful in sentence compression [18]. In this paper, we present a new approach to automatic sentence compression. First sentences are segmented with a discourse segmenter and then compression candidates are generated. Finally, the best candidate, i.e. the most grammatical candidate, is selected as a sequence in a language model based on its probability."}, {"heading": "2 Sentence compression", "text": "The task of sentence compression is defined as follows: \"Consider an input sentence as a sequence of n words W = (w1, w2,..., wn). An algorithm can drop any subset of these words. The words that remain (in unchanged order) form a compression.\" [7] There are interesting algorithms to determine the distance of words in a sentence, but people also tend to delete long phrases in an abstraction [15]. However, recent studies have produced good results by focusing on clauses rather than isolated words. In [19] an algorithm, sentences are first divided into clauses before each elimination, and then compression candidates are evaluated based on the latent semantic analysis proposed in [4]."}, {"heading": "3 Compression Candidates Generation", "text": "In this paper, we use a sentence-level discourse segmentation approach. Formally, \"discourse segmentation is the process of breaking down the discourse into elementary discourse units (EDUs), which can be simple sentences or clauses in a complex sentence, and from which discourse trees are constructed.\" [22] Discourse segmentation is only the first stage for discourse analysis (the others are the recognition of rhetorical relationships and the construction of discourse trees). However, we can look at sentence-level segmentation to identify segments that are to be eliminated in the sentence compression task. This fragmentation of a sentence in EDUs that use only local information is referred to as flat discourse segmentation. In [14], the authors use a discourse segmentator to segment sentences in Spanish that we are eliminated in the sentence compression task."}, {"heading": "4 Compression Candidates Scoring with Language Model", "text": "A language model (LM) estimates the probability distribution of natural language. Statistical language modeling [2, 12] is a widely used technique for assigning probability to a word sequence. We assume that good compression candidates must have a high probability than sequences in an LM. Generally, the probability for a sentence is W = (w1, w2,..., wn): P (wn1) = P (w1) P (w2 | w1) P (w3 | w 2 1)... P (wn | w n \u2212 1) (1) (1) Where wba = (wa,..., wb). The probabilities in an LM are estimated sequences from a corpus. Although we will never get enough data to calculate the statistics for all possible sentences, we can base our estimates on large corpus and interpolation methods of p \u2212 \u2212 scoreds2."}, {"heading": "5 Experimental Results", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "6 Conclusions and future work", "text": "We have found that the use of Probabilistic Language models can be helpful in evaluating compression candidates, and the results presented in Spanish in this paper are very encouraging. We believe that this approach is independent enough of the language to be translated into other languages such as English or French. In future work, we aim to improve the score (4) by adding content constraints. Evaluating compressed sentences and summaries with compression is still a challenge in languages other than English that do not have a reference corpora. We believe that further studies are needed to assess whether ROUGE or FRESA are good methods for compressed text evaluations."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the Consejo Nacional de Ciencia y Tecnolog\u00eda (CONACYT) M\u00e9xico, grant number 211963."}], "references": [{"title": "An empirical study of smoothing techniques for language modeling", "author": ["S.F. Chen", "J. Goodman"], "venue": "Computer Speech & Language,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Discourse segmentation for spanish based on shallow parsing", "author": ["Iria da Cunha", "Eric SanJuan", "Juan-Manuel Torres-Moreno", "Marina Lloberes", "Irene Castell\u00f3n"], "venue": "Advances in Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Indexing by Latent Semantic Analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "New Methods in Automatic Extraction", "author": ["H.P. Edmundson"], "venue": "Journal of the Association for Computing Machinery,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1969}, {"title": "Statistics-based summarization \u2013 step one: Sentence compression", "author": ["Kevin Knight", "Daniel Marcu"], "venue": "In Proceedings of the 17th National Conference on Artificial Intelligence and 12th Conference on Innovative Applications of Artificial Intelligence,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Summarization beyond sentence extraction: a probabilistic approach to sentence compression", "author": ["Kevin Knight", "Daniel Marcu"], "venue": "Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author": ["Chin-Yew Lin"], "venue": "Proceedings of the Workshop Text Summarization Branches Out (ACL\u201904),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2004}, {"title": "Automatic Evaluation of Summaries using N-gram Co-occurrence Statistics", "author": ["Chin-Yew Lin", "Eduard Hovy"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (NAACL\u201903),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2003}, {"title": "The Automatic Creation of Literature Abstracts", "author": ["H.P. Luhn"], "venue": "IBM Journal of Research and Development,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1958}, {"title": "Rhetorical Structure Theory: A Theory of Text Organization", "author": ["W.C. Mann", "S.A. Thompson"], "venue": "Information Sciences Institute, Marina del Rey,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1987}, {"title": "Foundations of Statistical Natural Language Processing", "author": ["Christopher D. Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Discourse Segmentation for Sentence Compression", "author": ["Alejandro Molina", "Torres-Moreno Juan-Manuel", "Eric SanJuan Eric", "Iria da Cunha", "Gerardo Sierra", "Patricia Vel\u00e1zquez-Morales"], "venue": "In Proceedings of the Mexican International Conference on Artificial Intelligence (MICAI\u201911),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Automatic evaluation of linguistic quality in multidocument summarization", "author": ["Emily Pitler", "Annie Louis", "Ani Nenkova"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Multilingual summarization evaluation without human models", "author": ["Horacio Saggion", "Juan-Manuel Torres-Moreno", "Iria da Cunha", "Eric SanJuan"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics: Posters (COLING\u201910),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Sentence level discourse parsing using syntactic and lexical information", "author": ["Radu Soricut", "Daniel Marcu"], "venue": "In HLT-NAACL,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Discourse chunking and its application to sentence compression", "author": ["Caroline Sporleder", "Mireille Lapata"], "venue": "In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Sentence Compression for the LSA-based Summarizer, pages 141\u2014148", "author": ["Josef Steinberger", "Karel Jezek"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Knowledge-poor multilingual sentence compression", "author": ["Josef Steinberger", "Roman Tesar"], "venue": "In 7th Conference on Language Engineering", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Srilm \u2013 an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "In Intl. Conf. on Spoken Language Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "A syntactic and lexical-based discourse segmenter", "author": ["Milan Tofiloski", "Julian Brooke", "Maite Taboada"], "venue": "In ACL-IJCNLP,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Fresa: a framework for evaluating summaries", "author": ["Juan-Manuel Torres-Moreno"], "venue": "automatically. rapport technique, Universite\u0301 d\u2019Avignon et des Pays de Vaucluse, Avignon, France,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "R\u00e9sum\u00e9 automatique de documents: une approche statistique", "author": ["Juan-Manuel Torres-Moreno"], "venue": "Herme\u0300s-Lavoisier, Paris,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Summary Evaluation With and Without References", "author": ["Juan-Manuel Torres-Moreno", "Horacio Saggion", "Iria da Cunha", "Eric SanJuan"], "venue": "Polibits: Research journal on Computer science and computer engineering with applications,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "Sentences extraction allows to generate summaries by extraction sentences [10, 5, 24].", "startOffset": 74, "endOffset": 85}, {"referenceID": 3, "context": "Sentences extraction allows to generate summaries by extraction sentences [10, 5, 24].", "startOffset": 74, "endOffset": 85}, {"referenceID": 21, "context": "Sentences extraction allows to generate summaries by extraction sentences [10, 5, 24].", "startOffset": 74, "endOffset": 85}, {"referenceID": 4, "context": "Sentence compression can be used to improve extract summarization [6, 13].", "startOffset": 66, "endOffset": 73}, {"referenceID": 15, "context": "Previous works suggest that sentence segmentation could be helpful in sentence compression generation [18].", "startOffset": 102, "endOffset": 106}, {"referenceID": 5, "context": "The words that remain (order unchanged) form a compression\u201d [7].", "startOffset": 60, "endOffset": 63}, {"referenceID": 12, "context": "There are interesting algorithms to determine the removal of words in a sentence but humans tend also to delete long phrases in an abstract [15].", "startOffset": 140, "endOffset": 144}, {"referenceID": 16, "context": "In [19] an algorithm first divides sentences into clauses prior to any elimination and then, compression candidates are scored based on Latent Semantic Analysis proposed in [4].", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [19] an algorithm first divides sentences into clauses prior to any elimination and then, compression candidates are scored based on Latent Semantic Analysis proposed in [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 16, "context": "However, no component to mitigate grammaticality issues is included in this algorithm [19].", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "The authors attempted to solve this issue by including features in a machine learning approach [20].", "startOffset": 95, "endOffset": 99}, {"referenceID": 15, "context": "Discourse chunking [18] is an alternative to discourse parsing, thereby, showing a direct application to sentence compression.", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "In addition, some sentence-level discourse models have shown accuracies comparable to human performance [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 19, "context": "Formally, \u201cDiscourse segmentation is the process of decomposing discourse into Elementary Discourse Units (EDUs), which may be simple sentences or clauses in a complex sentence, and from which discourse trees are constructed\u201d [22].", "startOffset": 226, "endOffset": 230}, {"referenceID": 11, "context": "In [14], the authors use a discourse segmenter in order to segment sentences in spanish.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "The discourse segmenter is described in [3] and is based in the Rhetorical Structure framework [11].", "startOffset": 40, "endOffset": 43}, {"referenceID": 9, "context": "The discourse segmenter is described in [3] and is based in the Rhetorical Structure framework [11].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "Statistical language modeling [2, 12] is a technique widely used to assign a probability to a sequence of words.", "startOffset": 30, "endOffset": 37}, {"referenceID": 10, "context": "Statistical language modeling [2, 12] is a technique widely used to assign a probability to a sequence of words.", "startOffset": 30, "endOffset": 37}, {"referenceID": 0, "context": "In our experiments we use a big corpus with 1T words to get the sequences counts and a LM interpolation based on Jelinek-Mercer smoothing [2]:", "startOffset": 138, "endOffset": 141}, {"referenceID": 18, "context": "For our experiments we use the Language Modeling Toolkit SRILM [21].", "startOffset": 63, "endOffset": 67}, {"referenceID": 11, "context": "For the experiments, two annotators were required to compress each sentence following the instructions in [14].", "startOffset": 106, "endOffset": 110}, {"referenceID": 7, "context": "We wanted to evaluate the content quality of summaries with the ROUGE package [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "ROUGE is used to evaluate summaries because some results show that it correlates well with human judgements [8].", "startOffset": 108, "endOffset": 111}, {"referenceID": 20, "context": "As an alternative, we compare the divergence of texts with respect to the original uncompressed text using the FRESA package [23, 16, 25].", "startOffset": 125, "endOffset": 137}, {"referenceID": 13, "context": "As an alternative, we compare the divergence of texts with respect to the original uncompressed text using the FRESA package [23, 16, 25].", "startOffset": 125, "endOffset": 137}, {"referenceID": 22, "context": "As an alternative, we compare the divergence of texts with respect to the original uncompressed text using the FRESA package [23, 16, 25].", "startOffset": 125, "endOffset": 137}], "year": 2012, "abstractText": "Previous works demonstrated that Automatic Text Summarization (ATS) by sentences extraction may be improved using sentence compression. In this work we present a sentence compressions approach guided by level-sentence discourse segmentation and probabilistic language models (LM). The results presented here show that the proposed solution is able to generate coherent summaries with grammatical compressed sentences. The approach is simple enough to be transposed into other languages.", "creator": "easychair.cls-3.0"}}}