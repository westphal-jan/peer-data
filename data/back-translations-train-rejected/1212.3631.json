{"id": "1212.3631", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Dec-2012", "title": "Learning efficient sparse and low rank models", "abstract": "Parsimony, including sparsity and low rank, has been shown to successfully model data in numerous machine learning and signal processing tasks. Traditionally, such modeling approaches rely on an iterative algorithm that minimizes an objective function with parsimony-promoting terms. The inherently sequential structure and data-dependent complexity and latency of iterative optimization constitute a major limitation in many applications requiring real-time performance or involving large-scale data. Another limitation encountered by these modeling techniques is the difficulty of their inclusion in discriminative learning scenarios. In this work, we propose to move the emphasis from the model to the pursuit algorithm, and develop a process-centric view of parsimonious modeling, in which a learned deterministic fixed-complexity pursuit process is used in lieu of iterative optimization. We show a principled way to construct learnable pursuit process architectures for structured sparse and robust low rank models, derived from the iteration of proximal descent algorithms. These architectures learn to approximate the exact parsimonious representation at a fraction of the complexity of the standard optimization methods. We also show that appropriate training regimes allow to naturally extend parsimonious models to discriminative settings. State-of-the-art results are demonstrated on several challenging problems in image and audio processing with several orders of magnitude speedup compared to the exact optimization algorithms.", "histories": [["v1", "Fri, 14 Dec 2012 22:50:44 GMT  (2274kb,D)", "http://arxiv.org/abs/1212.3631v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["pablo sprechmann", "alex m bronstein", "guillermo sapiro"], "accepted": false, "id": "1212.3631"}, "pdf": {"name": "1212.3631.pdf", "metadata": {"source": "CRF", "title": "Learning Efficient Sparse and Low Rank Models", "authors": ["P. Sprechmann", "\u2217A", "M. Bronstein", "G. Sapiro"], "emails": ["pablo.sprechmann@duke.edu,", "guillermo.sapiro@duke.edu.", "bron@eng.tau.ac.il."], "sections": [{"heading": null, "text": "Traditionally, such modeling approaches are based on an iterative algorithm that minimizes an objective function with parsimonious terms. Another constraint these modeling techniques encounter is the difficulty of incorporating them into discriminatory learning scenarios. In this paper, we propose to shift the focus from the model to the tracking algorithm and develop a process-centric view of parsimonious modeling, using a learned deterministic process to track fixed complexity instead of iterative learning scenarios. In this paper, we demonstrate a principle-driven method of constructing learnable tracking architectures for structured, sparse, and robust models."}, {"heading": "1 Introduction", "text": "Parsimony, which prefers a simple explanation to a more complex one, is probably one of the most intuitive principles widely used in modeling nature. Recent two decades of research have shown that the signal has many coefficients close to or equal to zero when present in a domain normally referred to as a dictionary. Striving for sparse representations proved possible by using tools from convex optimization, particularly through \"1 norm minimization,\" Works [3, 4] followed by efficient composition techniques for dictionaries. Sparse modeling is at the heart of modern approaches to image enhancement, such as denocialization, demosazialization, and superresolution."}, {"heading": "1.1 From model-centric to process-centric parsimonious", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "1.2 Contributions", "text": "The obtained encoders can be used to achieve fast approximations or predictors of optimization based on parsimonial models. We start with a principled way to construct encoders that are able to create a significant family of parsimonial models."}, {"heading": "2 Parsimonious models", "text": "In this thesis, we focus our attention on the general parsimonious modeling problem that can be posed as a solution to the minimization problem. \u2212 In this context, Z-Rq \u00b7 n is the representation (parsimonious code) of the data in the dictionary, and the punitive terms \u0441 (Z) and \u03c6 (D) each induce a specific structure of the code and the dictionary. If the minimization is performed both in the dictionary and in the representation, it is not convex. We will explicitly distinguish between parsimonious coding or representation tracking problems (representation of the data with a given model), and the harder parsimonious modeling problem (construction of a model that describes given data)."}, {"heading": "2.1 Structured sparsity", "text": "The underlying assumption of economical models is that the input vectors can be reconstructed exactly as a linear combination of dictionary atoms with a small number of non-zero coefficients. Sparse models are forced by the use of economical-stimulating regulatory mechanisms (Z). The simplest choice of such a regulatory mechanism (Z) is that the classically unstructured, sparse coding problems can be divided into n independent problems on the columns of Z. (Z) The simplest choice of such a regulatory mechanism (Z) is that the classical, unstructured, sparse coding problems, often referred to as lasso [2]. Structured sparse models further assume that the pattern of the non-zero coefficients of Z has a specific structure."}, {"heading": "2.2 Low-rank models and robust PCA", "text": "Another significant manifestation of parsimony, which is typical for many classes of data, is the low rank level. The classic low rank model is the main component analysis (PCA), in which the data matrix X + 2 is a perturbation matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix data matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix matrix"}, {"heading": "2.3 Non-negative matrix factorization", "text": "Another popular low-ranking model is the non-negative matrix factorization (NMF). Given a non-negative data matrix X-Rm \u00b7 n, NMF aims to find a factorization X-DZ in non-negative matrices. (10) Problem (10) can be specified as a special instance of (1) by defining the sum of the non-convex indicator functions of formi + (t) = {0-Rank 0-X \u2212 DZ-2F. (10) Problem (1) by defining the sum of the element-wise indicator functions of formi + (t) = {0-Rank 0-X-DZ-2F. (11) The non-negativity problem has been proven to be crucial for learning a partial representation of the data, making it particularly attractive to the problem of source separation."}, {"heading": "3 Proximal methods", "text": "They have been adopted by the machine learning and signal processing communities for their simplicity, convergence guarantees, and the fact that they are well suited to address economical and structured coding problems that can be written as (1). In Section 4, we will use these algorithms to construct efficient learning processes. Proximal splitting methods are designed to solve optimization problems where the cost function can be divided into the sum of two terms, a convex and differentiable with an alexact Lipschitz threshold."}, {"heading": "4 Learnable pursuit processes", "text": "The general parsimonious modeling problem (1) may alternatively be considered a minimization problem: Rm \u2192 Rq x: Rq \u2192 Rm1 n + 1 L (xi, z, x), (18) with L (x, z, x) = 12 (id \u2212 x), (x), (z), (x), (x), (x), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), (c), c, c, c, (c), (c), (c), (c), (c), c (c), (c), c (c), \"(c), c (c,\" (c), c (c), c (c), c (c), \"(c), c (c), c (c),\" (c), c (c, \"(c), c (c), c (c),\" (c, \"(c), c (c),\" (c (c), \"(c), c (c (c),\" (c, \"(c), c (c),\" (c (c), \"(c,\" (c), \"(c), c,\" (c, \"(c), c (c,\" (c), c (c), \"(c),\" (c (c), \"(c (c),\" (c, \"(c), c,\" (c), \"(c,\" (c), \"(c),\" (c), \"(c,\" (c), (c), (c, \"(c,\" (c), (c), (c), (c), (c), \"(c,\" (c), \"(c, c),\" (c), (c), \"(), (c), (),"}, {"heading": "4.1 Process learning", "text": "With the process-centric perspective in mind, we can specify problem (18) as problem (21), which imposes some desired properties on the encoder, such as continuity and almost everywhere differentiability and certain computational complexity. As in (1), this problem can of course be solved by using an alternative minimization scheme that sequentially minimizes the number of target values while the other is fixed. Note that when the encoder is zoned, the problem in D remains essentially the same problem of dictionary updating and can be solved exactly as before. In what follows, we focus on solving the process learning problem in zonal or D while the other is fixed. Note that if the encoder is zoned, the problem in D remains essentially the same dictionary updating problem and can be solved exactly as before."}, {"heading": "4.2 Approximation accuracy", "text": "Following [44], we divide the approximation error into three terms: = app + est + opt. The approximation error app = E {L (z *) \u2212 L (z *)} measures how well the optimal unrestricted tracking process given by (19) is approximated by the optimal tracking process limited to F, z * * * = arg minz\u044b = F L (z *). The estimation error est = E {L (z *) \u2212 L (z *)} with z * (z *) measures the costs of optimizing empirical risk instead of the expected risk. Finally, the optimization error opt = E {L (z *) \u2212 L (z *)} measures the effect that z * only approximately minimizes empirical risk."}, {"heading": "4.3 Process architecture", "text": "s examine a generic proximal descendant algorithm described in Section 3. Each iteration can be described as a function that receives the current state (bin, zin) and produces the next state (bout, zout) by using the nonlinear transformation zout = \u03c0t (bin, zin) (representing the proximal operator) and the linear transformation bout = bin + H (zout \u2212 zin) (representing the linear part of the gradient), which can be described by the function (bout, zout) = fH (bin, zin) parameterized by the matrix H, which describes the linear transformation, and the vector t, which describes the parameters of the proximal operator."}, {"heading": "4.4 Approximation error vs. complexity trade-off", "text": "Since the objective target is minimized, it is guaranteed that there is a selection of parameters that do not exist. (This is the best method ever.) In addition, the reformulation of the non-asymptotic convergence analysis of [18] in our language is as follows: Theorem 1 (Beck & Teboulle). For each D there is such a gap that for each Z and T \u2265 1, L (x, zT,). \u2212 L (x, z) \u2264 C2T (x) Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z"}, {"heading": "5 Training regimes", "text": "It has been described in the previous section, parsimonial modeling can be used as training of an encoder / decoder pair (z, x) that minimizes empirical loss in problem (18). We point to this setting as unsupervised since no information is provided besides the data itself. An important feature of this school system is that it performs a specific task that involves online adaptation to an online dictionary as described in Section 2.0."}, {"heading": "5.1 Supervised learning", "text": "In many applications, parsimonious models are used as an approximation of the first order to different classes of natural signals. An example is the music separation problem discussed in Section 4.4. While we achieve excellent separation results, both the RPCA and the RNMF models are merely a rough representation of reality: the music is never exactly at a low level, and the voice does not allow the objective function L of (18) to include domain-specific information to be performed. Figure 2 (below) shows that encoders trained in this way perform not only their uncontrolled trained counterparts, but also the exact RPCA / RNMF algorithm.Sticking, for example, of audio source separation using robust PCA or NMF, which represent exemplarily trained counterparts, but also the exact RPCA / RNMF algorithms."}, {"heading": "5.2 Discriminative learning", "text": "The tested training is also useful for extending parsimonious models beyond the conventional generative scenario, in which the data can be almost recovered from representation, to discriminatory scenarios such as classification problems, where representation is typically not reversible and must capture various invariant properties of the data. As an illustration, we use the simultaneous speech and speaker recognition model from [45], in which the spectrogram of the input signal is dissected into X-D0S + DO, in which D0S must capture the noise, while the activation O representing the speech must be spared. A collection of speaker-specific words, D1, will be trained offline, and the models will match previously unobserved data."}, {"heading": "5.3 Data transformations", "text": "Parsimonious models are based on the assumption that the input data vectors are \"aligned\" with our data, and this assumption could be violated in many applications. A representative example is face modeling via RPCA, where the low-dimensional model holds only if the facial images are aligned pixel-by-pixel [33]. Even small misalignments can break the structure in the data; the representation then deteriorates rapidly as the rank of the low-dimensional component increases and the outlier matrix loses its thriftness. In [33] it was proposed to align the input vectors simultaneously and solve RPCA by including the transformation parameters in the optimization variables. This problem is highly non-convex, but if a good initialization of the transformation parameters is available, a solution can be found by solving a sequence of convex optimization problems, each of which is comparable to (7)."}, {"heading": "6 Experimental results", "text": "In the following, we describe experiments to evaluate the efficiency of the proposed framework. Fast encoders were implemented in Matlab with built-in GPU acceleration and run on Intel Xeon E5620 CPU and NVIDIA Tesla C2070 GPU. When we refer to a particular encoder, we define its architecture and training regime; e.g. \"CoD (Unsupervised)\" stands for the CoD network trained in the unsupervised regime. We refer to Approximation, Supervised, Unsupervised and Discriminative as the corresponding training regime specified in Section 5. Untrained means an untrained encoder, that is, with parameters defined in fixed steps as in the corresponding Proximal Descent algorithm; the performance of such encoder coincides with that of a shortened Proximal Descent."}, {"heading": "6.1 Online sparse encoders", "text": "To evaluate the performance of the unstructured CoD (Unsupervised) encoders in the online learning regime, we used 30 x 104 randomly localized 8 x 8 patches from three images of the Brodatz texture dataset [46]. Patches were arranged in three consecutive blocks of 104 patches each from each image; the dictionary size was set to q = 64 atoms, and T = 4 layers were used in all CoD encoders. Unsupervised online learning was performed using the lasso lens with \u03bb = 1 overlapping windows of 1,000 vectors with a step of 100 vectors; online trained encoders were compared to an online version of Lasso, where the dictionary was adapted to the same data. As a reference, we traced offline a CoD (Approximation) encoder and another CoD (Unsupervised) encoder (Unsupervised) encoder from the same set of online encoders, the unique set of 6.000 encoders were extracted from a single encoder."}, {"heading": "6.2 Structured sparse encoders", "text": "The performance of the BCoD structured sparse architecture derived from algorithm 1 combined with various training systems is reproduced from [47] on a speakeridentifying task. In this application, the authors use hierarchical sparse coding to automatically detect the speakers in a particular mixing signal. The data set consists of recordings of five different radio speakers, two women and three men. Signals were used for testing in a series of overlapping timeframes of 512%. Within the test data, two sets of breakers were created containing all possible combinations of two speakers. Signals were broken down into a series of overlapping timeframes of 512 samples so that the characteristics of the signal within each frame remain stable."}, {"heading": "6.3 Robust PCA encoders with geometric optimization", "text": "To evaluate the performance of the rugged PCA encoders, we used a face dataset consisting of 800 66 x 48 images of a female face photographed over a 4.5-year period, roughly normalized and aligned to pose and scale, revealing a significant variety of hairstyles and clothing styles while maintaining similar facial features. [33] We then used RPCA to dissect a collection of faces represented as columns of data matrix X in L + O, with the low term L = D0S approximating facial identity (atoms of D0 can be considered \"natural surfaces\"), while outlier O captured appearance variability. We trained a five-layered RPCA encoder on 600 images from the Faces datasets, initializing the dictionary with standard SVD and setting the parameters to q = 50."}, {"heading": "6.4 Robust NMF encoders", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.4.1 Singing voice separation", "text": "We now evaluate the problem of source division (voice / background accompaniment) described in Section 4.4. Separation performance was evaluated using the MIR-1K dataset [48], which contained 1,000 Chinese karaoke clips of amateur singers. Experimental settings followed those of [48], which the reader is referred to for further details. As evaluation criteria, we used the normalized source-to-distortion ratio (NSDR) from the BSS-EVAL metrics [49], averaged over the test kit. RNMF encoders with an RNMF architecture, composed of T = 20 layers and q = 25, were trained using different training programs. We used Same = \u221a 2n\u03c3 and Same = 2 \u03c3 with a distance = 0.3 following [13]. Table 2 summarizes the separation performance achieved. During (unattended) training regimen, fast RNMF encoding details were applied on par with the RNMF accuracy achieved with the RMF and RMF accuracy of a fraction component."}, {"heading": "6.4.2 Robust speaker identification", "text": "The purpose of this experiment is to demonstrate the advantages of training economical models in a discriminatory manner when used for classification tasks. As an example, we use the loudspeaker identification described in Section 5.2 in environments heavily contaminated by unstructured noise. We evaluated the classification capabilities of different low-level NMF architectures in combination with two supervised training programs discussed in Section 5.1, one aimed at producing a good reconstruction of the speech signal and another optimized to achieve the best classification. The reconstructive approach is similar to that used in Section 6.4.1 for separating music and singing, but using a low NMF for both noise and human speech. In all of our examples, we used T = 10 layers and q = 50 parameters, just as in Section 6.4.1. As a speech dataset, we used a subset of the GRID dataset consisting of 50 speakers, each of which consists of 1,000 (10) short speakers."}, {"heading": "7 Conclusions and future work", "text": "In this work, we have developed a comprehensive framework for process-centric low-cost modeling. By combining ideas from convex optimization with multi-layered neural networks, we have shown how deterministic functions can be generated that allow encoders to accurately approximate the optimization-based solution of economical models at a fraction of the computing time. In addition, the framework includes various objective functions that allow encoders to train discriminatively or solve demanding alignment problems at almost the same computing cost. We have conducted empirical experiments in different environments and real-world applications such as image modeling, robust face modeling, audio source separation, and robust speaker recognition. Simple, non-optimized implementation often achieves multiple orders of magnitude compared to exact solutions. While we have limited our attention to synthesis models, the proposed framework can of course be expanded to analytical parameter models, which is limited to the signal specific in a 53 part of the signal."}], "references": [{"title": "Atomic decomposition by basis pursuit", "author": ["S. Chen", "D. Donoho", "M. Saunders"], "venue": "SIAM J. Scientific Computing, vol. 20, no. 1, pp. 33\u201361, 1999.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "Regression shrinkage and selection via the LASSO", "author": ["R. Tibshirani"], "venue": "J. Royal Stat. Society: Series B, vol. 58, no. 1, pp. 267\u2013288, 1996.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B. Olshausen", "D.J. Field"], "venue": "Nature, vol. 381, no. 6583, pp. 607\u2013609, 1996.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1996}, {"title": "k-SVD: an algorithm for designing overcomplete dictionaries for sparse representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Trans. Sig. Proc., vol. 54, no. 11, pp. 4311\u20134322, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Model selection and estimation in regression with grouped variables", "author": ["M. Yuan", "Y. Lin"], "venue": "J. Royal Stat. Society, Series B, vol. 68, pp. 49\u201367, 2006.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Group lasso with overlap and graph lasso", "author": ["L. Jacob", "G. Obozinski", "J. Vert"], "venue": "ICML, 2009, pp. 433\u2013440.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "The composite absolute penalties family for grouped and hierarchical variable selection", "author": ["P. Zhao", "G. Rocha", "B. Yu"], "venue": "Annals of Statistics, vol. 37, no. 6A, p. 3468, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Proximal methods for hierarchical sparse coding", "author": ["R. Jenatton", "J. Mairal", "G. Obozinski", "F. Bach"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2297\u20132334, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "C-hilasso: A collaborative hierarchical sparse modeling framework", "author": ["P. Sprechmann", "I. Ram\u0131\u0301rez", "G. Sapiro", "Y.C. Eldar"], "venue": "IEEE Trans. Signal Process., vol. 59, no. 9, pp. 4183\u20134198, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting statistical dependencies in sparse representations for signal recovery", "author": ["T. Peleg", "Y. Eldar", "M. Elad"], "venue": "IEEE Trans. Sig. Proc., vol. 60, no. 5, pp. 2286\u20132303, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Rank, trace-norm and max-norm", "author": ["N. Srebro", "A. Shraibman"], "venue": "Proc. COLT, pp. 599\u2013764, 2005. 31", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Exact matrix completion via convex optimization", "author": ["E. Cand\u00e8s", "B. Recht"], "venue": "Foundations of Computational mathematics, vol. 9, no. 6, pp. 717\u2013 772, 2009.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust principal component analysis?", "author": ["E. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Robust PCA via outlier pursuit", "author": ["H. Xu", "C. Caramanis", "S. Sanghavi"], "venue": "IEEE Trans. Inf. Theory, vol. 58, no. 5, pp. 3047\u20133064, 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust non-negative matrix factorization", "author": ["L. Zhang", "Z. Chen", "M. Zheng", "X. He"], "venue": "Frontiers of Electrical and Electronic Engineering in China, vol. 6, no. 2, pp. 192\u2013200, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning parts of objects by non-negative matrix factorization", "author": ["D. Lee", "H. Seung"], "venue": "Nature, vol. 401, no. 6755, pp. 788\u2013791, 1999.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1999}, {"title": "An iterative thresholding algorithm for linear inverse problems with a sparsity constraint", "author": ["I. Daubechies", "M. Defrise", "C. De Mol"], "venue": "Communications on Pure and Applied Mathematics, vol. 57, no. 11, pp. 1413\u20131457, 2004.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM J. Img. Sci., vol. 2, pp. 183\u2013202, March 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Coordinate descent optimization for `1 minimization with application to compressed sensing; a greedy algorithm", "author": ["Y. Li", "S. Osher"], "venue": "Inverse Problems and Imaging, vol. 3, pp. 487\u2013503, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Gradient methods for minimizing composite objective function", "author": ["Y. Nesterov"], "venue": "CORE. Catholic University of Louvain, Louvain-la-Neuve, Belgium, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Nonlinear programming", "author": ["D. Bertsekas"], "venue": "1999.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1999}, {"title": "Convex optimization with sparsity-inducing norms", "author": ["F. Bach", "R. Jenatton", "J. Mairal", "G. Obozinski"], "venue": "Optimization for Machine Learning. MIT Press, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "A singular value thresholding algorithm for matrix completion", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM J. on Opt., vol. 20, no. 4, pp. 1956\u2013 1982, 2010. 32", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1956}, {"title": "Accelerated low-rank visual recovery by random projection", "author": ["Y. Mu", "J. Dong", "X. Yuan", "S. Yan"], "venue": "CVPR, 2011, pp. 2609\u20132616.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallel stochastic gradient algorithms for large-scale matrix completion", "author": ["B. Recht", "C. R\u00e9"], "venue": "Optimization Online, 2011.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Projected gradient methods for non-negative matrix factorization", "author": ["C.-J. Lin"], "venue": "Neural Computation, vol. 19, pp. 2756\u20132779, 2007.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "An overview of bilevel optimization", "author": ["B. Colson", "P. Marcotte", "G. Savard"], "venue": "Annals of Operations Research, vol. 153, no. 1, pp. 235\u2013256, 2007.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "What is the best multi-stage architecture for object recognition?", "author": ["K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "in CVPR,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Fast inference in sparse coding algorithms with applications to object recognition", "author": ["K. Kavukcuoglu", "M. Ranzato", "Y. LeCun"], "venue": "arXiv:1010.3467, 2010.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning fast approximations of sparse coding", "author": ["K. Gregor", "Y. LeCun"], "venue": "ICML, 2010, pp. 399\u2013406.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2006}, {"title": "Measuring invariances in deep networks", "author": ["I. Goodfellow", "Q. Le", "A. Saxe", "H. Lee", "A.Y. Ng"], "venue": "In NIPS, 2009, pp. 646\u2013654.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "RASL: Robust alignment by sparse and low-rank decomposition for linearly correlated images", "author": ["Y. Peng", "A. Ganesh", "J. Wright", "W. Xu", "Y. Ma"], "venue": "CVPR, 2010, pp. 763\u2013770.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning efficient structured sparse models", "author": ["P. Sprechmann", "A.M. Bronstein", "G. Sapiro"], "venue": "ICML, 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Real-time online singing voice separation from monaural recordings using robust low-rank modeling", "author": ["\u2014\u2014"], "venue": "ISMIR, 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Online dictionary learning for sparse coding", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "ICML, 2009, pp. 689\u2013696.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2009}, {"title": "Average case analysis of multichannel sparse recovery using convex relaxation", "author": ["Y.C. Eldar", "H. Rauhut"], "venue": "IEEE Trans. on Inf. Theory, vol. 56, no. 1, pp. 505\u2013519, 2010. 33", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust PCA as bilinear decomposition with outlier-sparsity regularization", "author": ["G. Mateos", "G.B. Giannakis"], "venue": "arXiv.org:1111.1788, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Unveiling network anomalies in large-scale networks via sparsity and low rank", "author": ["M. Mardani", "G. Mateos", "G.B. Giannakis"], "venue": "Proc. of 44th Asilomar Conf. on Signals, Systems, and Computers, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "A note on the group lasso and a sparse group lasso", "author": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "venue": "2010, preprint.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "author": ["P. Tseng"], "venue": "J. Optim. Theory Appl., vol. 109, no. 3, pp. 475\u2013494, June 2001.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2001}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["V. Vapnik", "A. Chervonenkis"], "venue": "Theory of Probability & Its Applications, vol. 16, no. 2, pp. 264\u2013280, 1971.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1971}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "COMPSTAT, August 2010, pp. 177\u2013187.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Learnable low rank sparse models for speech denoising", "author": ["P. Sprechmann", "A.M. Bronstein", "M.M. Bronstein", "G. Sapiro"], "venue": "arXiv.org:1221.1288, 2012.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Filtering for texture classification: a comparative study", "author": ["T. Randen", "J.H. Husoy"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 21, no. 4, pp. 291\u2013310, 1999.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1999}, {"title": "Collaborative sources identification in mixed signals via hierarchical sparse modeling", "author": ["P. Sprechmann", "I. Ramirez", "P. Cancela", "G. Sapiro"], "venue": "Proc. ICASSP, May 2011.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "On the improvement of singing voice separation for monaural recordings using the MIR-1K dataset", "author": ["C. Hsu", "J. Jang"], "venue": "IEEE Trans. on Audio, Speech, and Lang. Proc., vol. 18, no. 2, pp. 310\u2013319, 2010.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "R. Gribonval", "C. F\u00e9votte"], "venue": "IEEE Trans. on Audio, Speech, and Lang. Proc., vol. 14, no. 4, pp. 1462\u20131469, 2006. 34", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2006}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "J. of the Acoust. Society of America, vol. 120, p. 2421, 2006.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2006}, {"title": "The AURORA experimental framework for the performance evaluation of speech recognition systems under noisy conditions", "author": ["D. Pearce", "H.-G. Hirsch"], "venue": "INTERSPEECH, 2000, pp. 29\u201332.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2000}, {"title": "The cosparse analysis model and algorithms", "author": ["S. Nam", "M. Davies", "M. Elad", "R. Gribonval"], "venue": "Applied and Computational Harmonic Analysis, 2012.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "Robust sparse analysis regularization", "author": ["S. Vaiter", "G. Peyr\u00e9", "C. Dossal", "J. Fadili"], "venue": "arXiv preprint arXiv:1109.6222, 2011. 35", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The pursuit of sparse representations was shown to be possible using tools from convex optimization, in particular, via `1 norm minimization [1, 2].", "startOffset": 141, "endOffset": 147}, {"referenceID": 1, "context": "The pursuit of sparse representations was shown to be possible using tools from convex optimization, in particular, via `1 norm minimization [1, 2].", "startOffset": 141, "endOffset": 147}, {"referenceID": 2, "context": "Works [3, 4], followed by many others, introduced efficient computational techniques for dictionary learning and adaptation.", "startOffset": 6, "endOffset": 12}, {"referenceID": 3, "context": "Works [3, 4], followed by many others, introduced efficient computational techniques for dictionary learning and adaptation.", "startOffset": 6, "endOffset": 12}, {"referenceID": 4, "context": "As many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10].", "startOffset": 268, "endOffset": 287}, {"referenceID": 5, "context": "As many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10].", "startOffset": 268, "endOffset": 287}, {"referenceID": 6, "context": "As many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10].", "startOffset": 268, "endOffset": 287}, {"referenceID": 7, "context": "As many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10].", "startOffset": 268, "endOffset": 287}, {"referenceID": 8, "context": "As many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10].", "startOffset": 268, "endOffset": 287}, {"referenceID": 9, "context": "As many classes of data are not described well by the element-wise sparsity model and the `1 norm inducing it, more elaborate structured sparse models have been developed, in which non-zero elements are no more unrelated, but appear in groups or hierarchies of groups [5, 6, 7, 8, 9, 10].", "startOffset": 268, "endOffset": 287}, {"referenceID": 10, "context": "A recent series of works have elucidated the beautiful relationship between sparsity and low rank representations, showing that rank minimization can be achieved through convex optimization [11, 12].", "startOffset": 190, "endOffset": 198}, {"referenceID": 11, "context": "A recent series of works have elucidated the beautiful relationship between sparsity and low rank representations, showing that rank minimization can be achieved through convex optimization [11, 12].", "startOffset": 190, "endOffset": 198}, {"referenceID": 12, "context": "The combination of low-rank and sparse models paved the path to new robust alternatives of principal component analysis (RPCA) [13, 14] and nonnegative matrix factorization (RNMF) [15], and addressing challenging matrix completion problems [12].", "startOffset": 127, "endOffset": 135}, {"referenceID": 13, "context": "The combination of low-rank and sparse models paved the path to new robust alternatives of principal component analysis (RPCA) [13, 14] and nonnegative matrix factorization (RNMF) [15], and addressing challenging matrix completion problems [12].", "startOffset": 127, "endOffset": 135}, {"referenceID": 14, "context": "The combination of low-rank and sparse models paved the path to new robust alternatives of principal component analysis (RPCA) [13, 14] and nonnegative matrix factorization (RNMF) [15], and addressing challenging matrix completion problems [12].", "startOffset": 180, "endOffset": 184}, {"referenceID": 11, "context": "The combination of low-rank and sparse models paved the path to new robust alternatives of principal component analysis (RPCA) [13, 14] and nonnegative matrix factorization (RNMF) [15], and addressing challenging matrix completion problems [12].", "startOffset": 240, "endOffset": 244}, {"referenceID": 15, "context": "Another relevant low rank modeling scheme is non-negative matrix factorization (NMF)[16], where the input vectors are represented as nonnegative linear combination of a non-negative under-complete dictionary.", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 134, "endOffset": 158}, {"referenceID": 17, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 134, "endOffset": 158}, {"referenceID": 18, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 134, "endOffset": 158}, {"referenceID": 19, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 134, "endOffset": 158}, {"referenceID": 20, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 134, "endOffset": 158}, {"referenceID": 21, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 134, "endOffset": 158}, {"referenceID": 12, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 166, "endOffset": 182}, {"referenceID": 22, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 166, "endOffset": 182}, {"referenceID": 23, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 166, "endOffset": 182}, {"referenceID": 24, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 166, "endOffset": 182}, {"referenceID": 15, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 191, "endOffset": 199}, {"referenceID": 25, "context": "The quest for efficiently solving sparse representation pursuit has given rise to a rich family of algorithms, both for sparse coding [17, 18, 19, 20, 21, 22] , RPCA [13, 23, 24, 25] and NMF [16, 26] problems.", "startOffset": 191, "endOffset": 199}, {"referenceID": 26, "context": "The resulting bilevel optimization problems are notoriously difficult to solve in general; the non-differentiability of the lowerlevel parsimony-inducing objective makes the solution practically impossible [27].", "startOffset": 206, "endOffset": 210}, {"referenceID": 27, "context": "Recently, [28, 29] have proposed to trade off precision in the sparse representation for computational speed-up by learning non-linear regressors capable", "startOffset": 10, "endOffset": 18}, {"referenceID": 28, "context": "Recently, [28, 29] have proposed to trade off precision in the sparse representation for computational speed-up by learning non-linear regressors capable", "startOffset": 10, "endOffset": 18}, {"referenceID": 29, "context": "In their inspiring recent paper, [30] showed that a particular network architecture can be derived from the iterative shrinkage-thresholding (ISTA) [17] and proximal coordinate descent (CoD) algorithms [19].", "startOffset": 33, "endOffset": 37}, {"referenceID": 16, "context": "In their inspiring recent paper, [30] showed that a particular network architecture can be derived from the iterative shrinkage-thresholding (ISTA) [17] and proximal coordinate descent (CoD) algorithms [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 18, "context": "In their inspiring recent paper, [30] showed that a particular network architecture can be derived from the iterative shrinkage-thresholding (ISTA) [17] and proximal coordinate descent (CoD) algorithms [19].", "startOffset": 202, "endOffset": 206}, {"referenceID": 30, "context": "These works were among the first to bridge between the optimization based sparse models and the inherently process-centric neural networks, and in particular auto-encoder networks [31, 32], extensively explored by the deep learning community.", "startOffset": 180, "endOffset": 188}, {"referenceID": 31, "context": "These works were among the first to bridge between the optimization based sparse models and the inherently process-centric neural networks, and in particular auto-encoder networks [31, 32], extensively explored by the deep learning community.", "startOffset": 180, "endOffset": 188}, {"referenceID": 29, "context": "By extending the original ideas in [30], we propose tailored pursuit architectures derived from first-order proximal descent algorithms, which are briefly presented in Section 3.", "startOffset": 35, "endOffset": 39}, {"referenceID": 24, "context": "As a remedy, we propose to use an algorithm inspired by the non-convex optimization techniques in [25].", "startOffset": 98, "endOffset": 102}, {"referenceID": 29, "context": "Second, this new approach allows the encoders to be trained in an online manner, which makes the fast encoders no more restricted to work with a fixed distribution of input vectors known a priori (limitation existing, for example, in [30]), and removes the need to run the exact algorithms at training.", "startOffset": 234, "endOffset": 238}, {"referenceID": 31, "context": "While differently motivated, in this setting, our framework is related to the sparse autoencoders [32].", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "In particular, in Section 5 we show a very simple and efficient extension of the proposed RPCA framework to cases where the data undergo an unknown transformation that is sought for during the pursuit [33].", "startOffset": 201, "endOffset": 205}, {"referenceID": 33, "context": "The present paper generalizes and gives a more rigorous treatment to results previously published by the authors in [34, 35].", "startOffset": 116, "endOffset": 124}, {"referenceID": 34, "context": "The present paper generalizes and gives a more rigorous treatment to results previously published by the authors in [34, 35].", "startOffset": 116, "endOffset": 124}, {"referenceID": 35, "context": "Online parsimonious modeling aims at estimating and refining the model as the data come in [36].", "startOffset": 91, "endOffset": 95}, {"referenceID": 0, "context": "where \u03b2j \u2208 [0, 1] is a forgetting factor that can be added to rescale older information so that newer estimates have more weight.", "startOffset": 11, "endOffset": 17}, {"referenceID": 35, "context": "This can be efficiently solved without remembering all the past codes [36].", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "This is the classical unstructured sparse coding problem, often referred to as Lasso [2] or basis pursuit [1].", "startOffset": 85, "endOffset": 88}, {"referenceID": 0, "context": "This is the classical unstructured sparse coding problem, often referred to as Lasso [2] or basis pursuit [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "Several important structured sparsity settings can be cast as particular cases of (6): Group sparse coding, a generalization of the standard sparse coding to the cases in which the dictionary is sub-divided into groups [5], in this case G is a partition of {1, .", "startOffset": 219, "endOffset": 222}, {"referenceID": 8, "context": ", q}; Hierarchical sparse coding, assuming a hierarchical structure of the non-zero coefficients [9, 7, 8].", "startOffset": 97, "endOffset": 106}, {"referenceID": 6, "context": ", q}; Hierarchical sparse coding, assuming a hierarchical structure of the non-zero coefficients [9, 7, 8].", "startOffset": 97, "endOffset": 106}, {"referenceID": 7, "context": ", q}; Hierarchical sparse coding, assuming a hierarchical structure of the non-zero coefficients [9, 7, 8].", "startOffset": 97, "endOffset": 106}, {"referenceID": 8, "context": "Collaborative sparse coding generalizes the concept of structured sparse coding to collections of input vectors by promoting given patterns of non-zero elements in the coefficient matrix [9, 37].", "startOffset": 187, "endOffset": 194}, {"referenceID": 36, "context": "Collaborative sparse coding generalizes the concept of structured sparse coding to collections of input vectors by promoting given patterns of non-zero elements in the coefficient matrix [9, 37].", "startOffset": 187, "endOffset": 194}, {"referenceID": 12, "context": "[13, 14] proposed to robustify the model by adding a new term to the decomposition to account for the presence of outliers, X = L + O + E, where O is an outlier matrix with a sparse number of non-zero coefficients of arbitrarily large magnitude.", "startOffset": 0, "endOffset": 8}, {"referenceID": 13, "context": "[13, 14] proposed to robustify the model by adding a new term to the decomposition to account for the presence of outliers, X = L + O + E, where O is an outlier matrix with a sparse number of non-zero coefficients of arbitrarily large magnitude.", "startOffset": 0, "endOffset": 8}, {"referenceID": 10, "context": "In [11] it was shown that the nuclear norm of a matrix of L can be reformulated as a penalty over all possible factorizations", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "This factorization has been recently exploited in parallel processing across multiple processors to produce state-ofthe-art algorithms for matrix completion problems [25], as well as an alternative approach to robustifying PCA in [39].", "startOffset": 166, "endOffset": 170}, {"referenceID": 37, "context": "This factorization has been recently exploited in parallel processing across multiple processors to produce state-ofthe-art algorithms for matrix completion problems [25], as well as an alternative approach to robustifying PCA in [39].", "startOffset": 230, "endOffset": 234}, {"referenceID": 37, "context": "Combining this with (8), it was proposed in [39] to reformulate (7) as", "startOffset": 44, "endOffset": 48}, {"referenceID": 38, "context": "Fortunately, it can be shown that any stationary point of (9), {D0,S,O}, satisfying ||X \u2212 D0S \u2212 O||2 \u2264 \u03bb\u2217 is an globally optimal solution of (9) [40].", "startOffset": 145, "endOffset": 149}, {"referenceID": 14, "context": "A robust variant can be obtained by adding a sparse outlier term to the decomposition, X \u2248 D0S+O, as done in the RPCA model [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 34, "context": "In [35], we proposed a cure to this phenomenon by incorporating a rank-reducing term into (10), establishing in this way a link between NMF and the RPCA problem (9).", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "They have been adopted by the machine learning and signal processing communities for their simplicity, convergence guarantees, and the fact that they are well suited for tackling sparse and structured sparse coding problems that can be written as (1) (refer to [22] for recent reviews).", "startOffset": 261, "endOffset": 265}, {"referenceID": 17, "context": "Fixed-step algorithms have been shown to have relatively slow sub-linear convergence, and many alternatives have been studied in the literature to improve the convergence rate [18, 20].", "startOffset": 176, "endOffset": 184}, {"referenceID": 19, "context": "Fixed-step algorithms have been shown to have relatively slow sub-linear convergence, and many alternatives have been studied in the literature to improve the convergence rate [18, 20].", "startOffset": 176, "endOffset": 184}, {"referenceID": 16, "context": "In this case, the fixed step proximal splitting algorithm corresponds to the popular iterative shrinkage-thresholding algorithm (ISTA) [17, 18] summarized in Algorithm 1.", "startOffset": 135, "endOffset": 143}, {"referenceID": 17, "context": "In this case, the fixed step proximal splitting algorithm corresponds to the popular iterative shrinkage-thresholding algorithm (ISTA) [17, 18] summarized in Algorithm 1.", "startOffset": 135, "endOffset": 143}, {"referenceID": 7, "context": "Then, the proximal operator of \u03c8G can be shown to be given by the composition of the proximal operators of \u03c8Gl in ascending order from the leaves to the root [8, 9], \u03c0 G = \u03c0G1 \u03bb1 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c0 GL \u03bbL .", "startOffset": 158, "endOffset": 164}, {"referenceID": 8, "context": "Then, the proximal operator of \u03c8G can be shown to be given by the composition of the proximal operators of \u03c8Gl in ascending order from the leaves to the root [8, 9], \u03c0 G = \u03c0G1 \u03bb1 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c0 GL \u03bbL .", "startOffset": 158, "endOffset": 164}, {"referenceID": 8, "context": "A particular case of the tree-structured hierarchical sparse model is the two-level HiLasso model introduced to simultaneously promote sparsity at both group and coefficient level [9, 41].", "startOffset": 180, "endOffset": 187}, {"referenceID": 39, "context": "A particular case of the tree-structured hierarchical sparse model is the two-level HiLasso model introduced to simultaneously promote sparsity at both group and coefficient level [9, 41].", "startOffset": 180, "endOffset": 187}, {"referenceID": 40, "context": "Several variants of coordinate descent (CoD) and blockcoordinate descent (BCoD) proximal methods have been proposed [42, 19].", "startOffset": 116, "endOffset": 124}, {"referenceID": 18, "context": "Several variants of coordinate descent (CoD) and blockcoordinate descent (BCoD) proximal methods have been proposed [42, 19].", "startOffset": 116, "endOffset": 124}, {"referenceID": 41, "context": "When the family F is sufficiently restrictive, the statistical learning theory justifies minimizing the empirical risk instead of the expected risk[43].", "startOffset": 147, "endOffset": 151}, {"referenceID": 42, "context": "When the functions belonging to F are almost everywhere differentiable with respect to the parameters \u0398, stochastic gradient descent (SGD) can be used to optimize (21), with almost sure convergence to a stationary point [44].", "startOffset": 220, "endOffset": 224}, {"referenceID": 42, "context": "Following [44], we split the process training approximation error into three terms, = app + est + opt.", "startOffset": 10, "endOffset": 14}, {"referenceID": 29, "context": "We extend the ideas introduced in [30] to derive families of trainable pursuit processes from proximal methods.", "startOffset": 34, "endOffset": 38}, {"referenceID": 29, "context": "Following [30], we consider the family of pursuit processes derived from truncated proximal descent algorithms with T iterations, FT = {zT,\u0398(x) = fH,t \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 fH,t(Wx,0)}.", "startOffset": 10, "endOffset": 14}, {"referenceID": 17, "context": "Furthermore, reformulating the non-asymptotic convergence analysis from [18] in our language, the following holds:", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "The idea of [30] to train pursuit processes to approximate the output of iterative pursuit algorithms falls into this category.", "startOffset": 12, "endOffset": 16}, {"referenceID": 43, "context": "As an illustration, we use the simultaneous speech denoising and speaker identification model from [45], in which the spectrogram of the input signal is decomposed into X \u2248 D0S + DO, where D0S capturing the noise is required to be low-rank, while the activation O representing the speech is required to be sparse.", "startOffset": 99, "endOffset": 103}, {"referenceID": 32, "context": "A representative example is face modeling via RPCA, where the low dimensional model only holds if the facial images are pixel-wise aligned [33].", "startOffset": 139, "endOffset": 143}, {"referenceID": 32, "context": "In [33], it was proposed to simultaneously align the input vectors and solve RPCA by including the transformation parameters into the optimization variables.", "startOffset": 3, "endOffset": 7}, {"referenceID": 32, "context": "Following [33], we propose to incorporate the optimization over geometric transformations of the input data into our modeling framework.", "startOffset": 10, "endOffset": 14}, {"referenceID": 44, "context": "To evaluate the performance of the unstructured CoD (Unsupervised) encoders in the online learning regime, we used 30\u00d7 10 randomly located 8\u00d7 8 patches from three images from the Brodatz texture dataset [46].", "startOffset": 203, "endOffset": 207}, {"referenceID": 45, "context": "identification task reproduced from [47].", "startOffset": 36, "endOffset": 40}, {"referenceID": 45, "context": "An 80-dimensional feature vector is obtained for each audio frame as its short-time power spectrum envelope (refer to [47] for details).", "startOffset": 118, "endOffset": 122}, {"referenceID": 33, "context": "Other comparative experiments substantiating this observation can be found in [34].", "startOffset": 78, "endOffset": 82}, {"referenceID": 32, "context": "Following [33], we use RPCA to decompose a collection of faces represented as columns of the data matrix X into L+O, with the low-rank term L = D0S approximating the face identity (atoms of D0 can be thought of as \u201ceigenfaces\u201d), while the outlier term O capturing the appearance variability.", "startOffset": 10, "endOffset": 14}, {"referenceID": 46, "context": "The separation performance was evaluated on the MIR-1K dataset [48], containing 1,000 Chinese karaoke clips performed by amateur singers.", "startOffset": 63, "endOffset": 67}, {"referenceID": 46, "context": "The experimental settings closely followed that of [48], to which the reader is referred for further details.", "startOffset": 51, "endOffset": 55}, {"referenceID": 47, "context": "As the evaluation criteria, we used the normalized source-to-distortion ratio (NSDR) from the BSS-EVAL metrics [49], averaged over the test set.", "startOffset": 111, "endOffset": 115}, {"referenceID": 12, "context": "3 set following [13].", "startOffset": 16, "endOffset": 20}, {"referenceID": 34, "context": "For further details refer to [35].", "startOffset": 29, "endOffset": 33}, {"referenceID": 48, "context": "As speech dataset we used a subset of the GRID dataset [50] containing 10 distinct speakers; each speaker comprising 1,000 short clips.", "startOffset": 55, "endOffset": 59}, {"referenceID": 49, "context": "The GRID clips were artificially contaminated by six categories of noise recorded from different real environments (street, restaurant, car, exhibition, train, and airport) taken from the AURORA corpus [51].", "startOffset": 202, "endOffset": 206}, {"referenceID": 43, "context": "For further details please refer to [45].", "startOffset": 36, "endOffset": 40}, {"referenceID": 50, "context": "While we limited our attention to synthesis models, the proposed framework can be naturally extended to analysis cosparse models [52, 53], in which the signal is known to be sparse in a transformed domain.", "startOffset": 129, "endOffset": 137}, {"referenceID": 51, "context": "While we limited our attention to synthesis models, the proposed framework can be naturally extended to analysis cosparse models [52, 53], in which the signal is known to be sparse in a transformed domain.", "startOffset": 129, "endOffset": 137}, {"referenceID": 20, "context": "The space F can be set by truncating suitable iterative optimization algorithms such as the augmented Lagrangian methods of multipliers (ADMM) [21].", "startOffset": 143, "endOffset": 147}], "year": 2012, "abstractText": "Parsimony, including sparsity and low rank, has been shown to successfully model data in numerous machine learning and signal processing tasks. Traditionally, such modeling approaches rely on an iterative algorithm that minimizes an objective function with parsimony-promoting terms. The inherently sequential structure and data-dependent complexity and latency of iterative optimization constitute a major limitation in many applications requiring real-time performance or involving large-scale data. Another limitation encountered by these modeling techniques is the difficulty of their inclusion in discriminative learning scenarios. In this work, we propose to move the emphasis from the model to the pursuit algorithm, and develop a process-centric view of parsimonious modeling, in which a learned deterministic fixed-complexity pursuit process is used in lieu of iterative optimization. We show a principled way to construct learnable pursuit process architectures for structured sparse and robust low rank models, derived from the iteration of proximal descent algorithms. These architectures learn to approximate the exact parsimonious representation at a fraction of the complexity of the standard optimization methods. We also show that appropriate training regimes allow to naturally extend parsimonious models to discriminative settings. State-ofthe-art results are demonstrated on several challenging problems in image and audio processing with several orders of magnitude speedup compared to the exact optimization algorithms. \u2217P. Sprechmann and G. Sapiro are with the Department of Electrical and Computer Engineering, Duke University, Durham 27708, USA. Email: pablo.sprechmann@duke.edu, guillermo.sapiro@duke.edu. \u2020A. M. Bronsteind is with School of Electrical Engineering, Tel Aviv University, Tel Aviv 69978, Israel.Email: bron@eng.tau.ac.il. \u2021Work partially supported by NSF, ONR, NGA, DARPA, AFOSR, ARO, and BSF. 1 ar X iv :1 21 2. 36 31 v1 [ cs .L G ] 1 4 D ec 2 01 2", "creator": "LaTeX with hyperref package"}}}