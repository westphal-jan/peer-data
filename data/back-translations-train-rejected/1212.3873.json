{"id": "1212.3873", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2012", "title": "Learning Markov Decision Processes for Model Checking", "abstract": "Constructing an accurate system model for formal model verification can be both resource demanding and time-consuming. To alleviate this shortcoming, algorithms have been proposed for automatically learning system models based on observed system behaviors. In this paper we extend the algorithm on learning probabilistic automata to reactive systems, where the observed system behavior is in the form of alternating sequences of inputs and outputs. We propose an algorithm for automatically learning a deterministic labeled Markov decision process model from the observed behavior of a reactive system. The proposed learning algorithm is adapted from algorithms for learning deterministic probabilistic finite automata, and extended to include both probabilistic and nondeterministic transitions. The algorithm is empirically analyzed and evaluated by learning system models of slot machines. The evaluation is performed by analyzing the probabilistic linear temporal logic properties of the system as well as by analyzing the schedulers, in particular the optimal schedulers, induced by the learned models.", "histories": [["v1", "Mon, 17 Dec 2012 03:40:47 GMT  (150kb)", "http://arxiv.org/abs/1212.3873v1", "In Proceedings QFM 2012,arXiv:1212.3454"]], "COMMENTS": "In Proceedings QFM 2012,arXiv:1212.3454", "reviews": [], "SUBJECTS": "cs.LG cs.LO cs.SE", "authors": ["hua mao", "yingke chen", "manfred jaeger", "thomas d nielsen", "kim g larsen", "brian nielsen"], "accepted": false, "id": "1212.3873"}, "pdf": {"name": "1212.3873.pdf", "metadata": {"source": "CRF", "title": "Learning Markov Decision Processes for Model Checking", "authors": ["\u00a9 H. Mao", "Y. Chen", "M. Jaeger", "T. D. Nielsen", "K. G. Larsen", "Hua Mao", "Yingke Chen", "Manfred Jaeger", "Thomas D. Nielsen", "Kim G. Larsen", "Brian Nielsen"], "emails": ["@cs.aau.dk"], "sections": [{"heading": null, "text": "U. Fahrenberg, A. Legay and C. Thrane: Quantities in Formal Methods (QFM 2012) EPTCS 103, 2012, pp. 49-63, doi: 10.4204 / EPTCS.103.6c \u00a9 H. Mao, Y. Chen, M. Jaeger, T. D. Nielsen, K. G. Larsen, & B. Nielsen This work is published under the Creative Commons Attribution License.Learning Markov Decision Processes for Model CheckingHua Mao, Yingke Chen, Manfred Jaeger, Thomas D. Nielsen, Kim G. Larsen, and Brian Nielsen Department of Computer ScienceAalborg University Denmark [huamao, ykchen, jaeger, tdn, kgl, bnielsen] @ cs.aau.dkConstructing an ministered labyrinthine labyrinthine models can be both resource consuming and time consuming."}, {"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far, until it is so far."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Labeled Markov Decision Processes", "text": "Definition 1 (LMDP) A process designated as a Markov decision-making process (LMDP) is a tuple M = (Q, 1, 2, 3, 4, 5) \u2022 Q is a finite set of states, \u2022 I is a finite input alphabet, and I is a finite output alphabet, \u2022 \u03c0: Q \u2192 [0,1] is an initial probability distribution, so that Q-Q (q, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,"}, {"heading": "2.2 Strings", "text": "For a finite string, the set of all its prefixes is defined as follows: Prefix (s) = {\u03c30\u03b11\u03c31..... \u03b1n\u03c3n, \u03b1i-I and \u03c3i-N} For a string S, prefix (S) stands for the set of all prefixes of strings s-S. We assume a lexicographic sequence of strings. In a DLMDP, there is a close connection between strings and states: in the face of an observed string s, there is a unique state q in which the LMDP must be. Conversely, each state q is associated with the set strings (q). In a DLMDP, there is a close link between strings and states: In a observed string s, there is a unique state q in which the strings must be."}, {"heading": "2.3 Scheduler", "text": "A scheduler [3] for an MDP M is a function S: Q + \u2192 \u0421I such that S (q0q1... qn) \"Act (qn) for all q0, q1,..., qn\" Q +. The scheduler selects an action \u03b1 in any state q, and induces a Markov chain, i.e. the behavior of an MDP M under the decisions of the scheduler S can be formalized by a Markov chain MS [3, Section 10.6]. A scheme called a Markov chain (LMC) MS of an LMDP M, induced by a scheduler S, defines a PMS probability measure that is the basis for associating probabilities with events in the LMC MS. The probability of a string s = \u03c30\u04451."}, {"heading": "2.4 Probabilistic LTL", "text": "Linear temporal logic (LTL) over \"MS\" is defined as usual by the syntax \"MS\": \"A,\" \"1,\" \"2,\" \"1,\" \"2\" and \"2.\" For better readability we also use the derived time operators \"(always) and\" 2. \"Let's be an LTL formula. For\" s, \"\" 1, \"\" s, \"\" s, \"\" s, \"\" s, \"\" \"s,\" \"1\" and \"2.\" is the suffix of \"s,\" \"\" \"s,\" \"\" s, \"\" and \".\" Then the LTL semantics for infinite words over \"O\" is as follows: \"s,\" \"s,\" \"s,\" \"\" \"s,\" \"\" \"s,\" \"\" s \"and\". \""}, {"heading": "3 Data Generation", "text": "The data we learn from is generated by observing the running reactive system. From the system we can observe input actions that determine probability distributions over succession states, and outputs that are labels of succession states. The learning algorithm requires that all non-deterministic decisions be resolved by a fair scheduler S. This means that each input action is selected infinitely often. We assume that the input and output sequence is observed alternately, and each observation sequence starts from the labeling of the initial state and ends in a state, i.e. that each input action is selected infinitely often. We assume that the input and output sequence for states in a black box system is unsafe. Therefore, we allow all actions to be selected in each state of the system, i.e. that the system is idle, independent."}, {"heading": "4 Learning", "text": "IOALERGIA for learning DLMDP consists of two phases. First, present the data as an I / O frequency prefix acceptor (IOFPTA), where common prefixes are combined with each other. Then, do compatibility testing on the tree following lexicographic order. If two states are compatible that require that the next state distributions that have given the same input are compatible, they and their subsequent states are merged accordingly. Since in DLMDP the same sequences lead to the same state, common prefixes are then merged in IOFPTA and lead to a tree-shaped automaton. Each node in the tree is identified by an output symbol."}, {"heading": "4.3 Compatibility Test", "text": "If two nodes qr and qb are compatible with each other, i.e. the distance of the distributions for each action is within the Hoeffding boundary [19], algorithm 3, parameterized by \u03b5. Formally, two nodes qr and qb \u03b5-compatible (1 \u2265 \u03b5 > 0) are valid if: 1. L (qr) = L (qb) 2. Hoeffding (f (qr, \u03b1, \u03c3), f (qb, \u03b1), f (qb, \u03c3), \u03b5 (qb, \u03b1), \u03b5) is TRUE, for all."}, {"heading": "4.4 Merge states", "text": "If two states qr and qb are compatible, qb is merged to qr. The merge procedure (line 9 of algorithm 1) follows the same path as described in [8]: First, the (unique) transition that leads to qb from its predecessor node q \u2032 (f A (q \u2032, \u03b1, qb) > 0) is performed by setting f A (q \u2032, \u03b1, qr) \u2190 f A (q \u2032, \u03b1, qb) and f A (q \u2032, q, qb) = 0. Then, successor nodes of qb are recursively merged to the corresponding successor nodes of qr. Example 2 Merge States Fig. 2 shows the procedure that the node qb (shaded) is merged to the node qr (q \u2032, qb, qb) to the node qr: (shaded, double circle)."}, {"heading": "4.5 Discussion", "text": "Here \u03b5 is used to bind the type I error, except for the probability that a correct compatibility hypothesis is discarded incorrectly. Smaller values of \u03b5 lead to loose Hoeffding limits and make IOALERGIA a smaller model. For each specific finite sample size, we try to adjust the choice of \u03b5 in such a way that we get the best approximation to the real model. To do this, we perform IOALERGIA with different \u03b5 values and evaluate the learned model using the Bavarian Information Criterion (BIC) score. This score combines the probability of a model with a term that punishes model complexity. Specifically, the BIC score of a data S given by DLMDP-A is defined as an asBIC (A | S) score."}, {"heading": "5 Experiments", "text": "In this section, we will demonstrate the applicability of the IOALERGIA algorithm based on a case study based on the slot machine. [9] The slot machine we looked at has 3 reels referred to as Reel-1, Reel-2 and Reel-3, and each reel contains 5 different symbols: Lemon, Grape, Cherry, Bar and Apple. The slot machine will return a prize based on the combination of symbols on those 3 reels. Prizes for different configurations are shown in Table 1 (a). We expand the basic slot machines as follows: on each round, the player can choose one of the reels to spin and other reels are held. Player starts by paying one coin for the first 3 reels, and after that, each additional reel costs 1 additional coin. Each reel must be spun at least once, and the player can only complete the game when all reels have been spun."}, {"heading": "5.1 Learning models from Deterministic systems", "text": "We implemented the slot machines in PRISM. The distribution for 3 reels that show different symbols are (0,2,0,0,0,0,0,0,0,0,2), (0,2,0,1,0,0,0,0,0,2) and (0,2,0,3,0,0,0,0,0,2), respectively. In this model there are 4 actions: Spin Reel-1 (sp1), Spin Reel-2 (sp2), Spin Reel-2 (sp3), Spin Reel-3 (sp3), and get the prize (pay), so I = {sp1, sp2, sp3, pay}. Each state is characterized by the combination of states on the 3 reels and the number of times in which the reels were spun. We also have reward variables on the states that are labeled by prize. Table 2 shows statistics for models with different number of spins. Here is N (N \u2265 3) the number of spins, | Q | is the number of states, and the number of transitions."}, {"heading": "5.2 Learning models from Nondeterministic systems", "text": "To make the slot machine more interesting, we increase the prize for three bars, but decrease the probability of getting it. This is done by adding another bar on reel 2, two bars called b1 and b2, which are indistinguishable but have different mechanical properties. Probability for these two bars depends on the symbols on the other two reels, but the distributions for all reels are in Table 4 (a) and Table 4 (b). Since reels are no longer independent, we refer to the machine as a attached slot machine. In this machine, the probability of getting three reels is reduced, but the reward for getting three reels is the same as the previous game."}, {"heading": "6 Conclusion", "text": "In this thesis, we have proposed the IOALERGIA algorithm for learning deterministic Markov processes (DLMDPs). Given the sequences of alternating input and output symbols, the algorithm can automatically construct a model for the observed reactive system, and we have a similar convergence result of the IOALERGIA algorithm as in [13] for deterministic Markov chain models. The algorithm is analyzed empirically on the basis of a case study on slot machines. Comparing the learning results with respect to PLTL properties and maximum expected rewards of both the learned model with the known generating models and the accuracy of the optimal actions derived from the learned models, further research is needed to design the learning algorithm in such a way that it is suitable for routine use. In addition to the empirical proof of the individual learning model, the learning part of the learning model could be extended by the use of the component acquisition."}], "references": [{"title": "Learning I/O Automata", "author": ["Fides Aarts", "Frits W. Vaandrager"], "venue": "In: CONCUR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Learning regular sets from queries and counterexamples", "author": ["D. Angluin"], "venue": "Information and Computation", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1987}, {"title": "Principles of model checking", "author": ["Christel Baier", "Joost-Pieter Katoen"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Partial Order Methods for", "author": ["J. Bogdoll", "L.M.F. Fioriti", "A. Hartmanns", "H. Hermanns"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Learning Stochastic Regular Grammars by Means of a State Merging Method", "author": ["R.C. Carrasco", "J. Oncina"], "venue": "In: ICGI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "Learning Markov Models for Stationary System Behaviors", "author": ["Y. Chen", "H. Mao", "M. Jaeger", "T.D. Nielsen", "K.G. Larsen", "B. Nielsen"], "venue": "In: NFM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Identification in the Limit with Probability One of Stochastic Deterministic Finite Automata", "author": ["Colin de la Higuera", "Franck Thollard"], "venue": "ICGI, pp. 141\u2013156", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Grammatical Inference \u2014 Learning Automata and Grammars", "author": ["Colin de la Higuera"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Probabilistic UML Statecharts for Specification and Verification a Case Study", "author": ["D.N. Jansen"], "venue": "Critical Systems Development with UML \u2013 Proc. of the UML\u201902 workshop,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Statistical Model Checking: An Overview", "author": ["A. Legay", "B. Delahaye", "S. Bensalem"], "venue": "RV, pp. 122\u2013135", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Learning Meets Verification", "author": ["Martin Leucker"], "venue": "FMCO, pp. 127\u2013151", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Learning Probabilistic Automata for Model Checking", "author": ["H. Mao", "Y. Chen", "M. Jaeger", "T.D. Nielsen", "K.G. Larsen", "B. Nielsen"], "venue": "http://doi", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "LearnLib: A Library for Automata Learning and Experimentation", "author": ["H. Raffelt", "B. Steffen"], "venue": "In: FASE,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length", "author": ["D. Ron", "Y. Singer", "N. Tishby"], "venue": "Machine Learning 25(2-3),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Learning Continuous Time Markov Chains from Sample Executions. In:  QEST, pp. 146\u2013155", "author": ["K. Sen", "M. Viswanathan", "G. Agha"], "venue": "Available at http://doi.ieeecomputersociety", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "An Introduction to Probabilistic Automata", "author": ["Mari\u00eblle Stoelinga"], "venue": "Bulletin of the EATCS", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Introduction to Data Mining. Addison Wesley", "author": ["P.-N. Tan", "M. Steinbach", "V. Kumar"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Probability Inequalities for Sums of Bounded Random Variables", "author": ["H. Wassily"], "venue": "Journal of the American Statistical Association", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1963}, {"title": "Probabilistic Verification of Discrete Event Systems Using Acceptance Sampling", "author": ["H.L.S. Younes", "R.G. Simmons"], "venue": "CAV, pp. 223\u2013235. Available at http://dx.doi.org/10", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2002}], "referenceMentions": [{"referenceID": 10, "context": "Therefore, we consider system model learning techniques [12\u201314, 16], which can automatically construct or learn an accurate high-level model from observations of a given black-box embedded system component.", "startOffset": 56, "endOffset": 67}, {"referenceID": 11, "context": "Therefore, we consider system model learning techniques [12\u201314, 16], which can automatically construct or learn an accurate high-level model from observations of a given black-box embedded system component.", "startOffset": 56, "endOffset": 67}, {"referenceID": 12, "context": "Therefore, we consider system model learning techniques [12\u201314, 16], which can automatically construct or learn an accurate high-level model from observations of a given black-box embedded system component.", "startOffset": 56, "endOffset": 67}, {"referenceID": 14, "context": "Therefore, we consider system model learning techniques [12\u201314, 16], which can automatically construct or learn an accurate high-level model from observations of a given black-box embedded system component.", "startOffset": 56, "endOffset": 67}, {"referenceID": 1, "context": "For learning non-probabilistic system models, Angluin\u2019s approaches [2] has been well developed and implemented [1, 12, 14].", "startOffset": 67, "endOffset": 70}, {"referenceID": 0, "context": "For learning non-probabilistic system models, Angluin\u2019s approaches [2] has been well developed and implemented [1, 12, 14].", "startOffset": 111, "endOffset": 122}, {"referenceID": 10, "context": "For learning non-probabilistic system models, Angluin\u2019s approaches [2] has been well developed and implemented [1, 12, 14].", "startOffset": 111, "endOffset": 122}, {"referenceID": 12, "context": "For learning non-probabilistic system models, Angluin\u2019s approaches [2] has been well developed and implemented [1, 12, 14].", "startOffset": 111, "endOffset": 122}, {"referenceID": 14, "context": "[16] adapted the algorithm from [5] for learning Markov chain models in purpose of verification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[16] adapted the algorithm from [5] for learning Markov chain models in purpose of verification.", "startOffset": 32, "endOffset": 35}, {"referenceID": 11, "context": "In [13], a learning approach related to [16] is developed, and strong theoretical and experimental consistency results are established.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "In [13], a learning approach related to [16] is developed, and strong theoretical and experimental consistency results are established.", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "Considering a limited situation that the target system is not fully under control and only a single observation sequence is available, the algorithm for learning variable order Markov chains [15] is developed to verify stationary system properties on the learned models [6].", "startOffset": 191, "endOffset": 195}, {"referenceID": 5, "context": "Considering a limited situation that the target system is not fully under control and only a single observation sequence is available, the algorithm for learning variable order Markov chains [15] is developed to verify stationary system properties on the learned models [6].", "startOffset": 270, "endOffset": 273}, {"referenceID": 15, "context": "It is a natural choice to model by nondeterminism a system which is open for interaction from environment, system properties then need to be guaranteed for all potential environments [17].", "startOffset": 183, "endOffset": 187}, {"referenceID": 2, "context": "Therefore, Markov decision processes (MDPs), which exhibit both nondeterministic and probabilistic behavior, are widely used for modeling reactive systems [3].", "startOffset": 155, "endOffset": 158}, {"referenceID": 9, "context": "Besides model learning, statistical model checking (SMC) [11, 20] techniques can also be used to analyze black-box systems.", "startOffset": 57, "endOffset": 65}, {"referenceID": 18, "context": "Besides model learning, statistical model checking (SMC) [11, 20] techniques can also be used to analyze black-box systems.", "startOffset": 57, "endOffset": 65}, {"referenceID": 3, "context": "Unfortunately, this technique is not well suited to MDPs since the presence of nondeterminism making running for sample paths is not well defined [4] without an extra scheduler.", "startOffset": 146, "endOffset": 149}, {"referenceID": 4, "context": "The main contribution of this paper is the development of IOALERGIA algorithm for learning DLMDP, which is obtained as an adaptation of the previous ALERGIA [5] algorithm.", "startOffset": 157, "endOffset": 160}, {"referenceID": 0, "context": "\u2022 \u03a3I is a finite input alphabet, and \u03a3O is a finite output alphabet, \u2022 \u03c0 : Q \u2192 [0,1] is an initial probability distribution such that \u2211q\u2208Q \u03c0(q) = 1, \u2022 \u03c4 : Q\u00d7\u03a3I \u00d7Q \u2192 [0,1] is the transition probability function such that for all q \u2208 Q and all \u03b1 \u2208 \u03a3I , \u2211q\u2032\u2208Q \u03c4(q,\u03b1 ,q\u2032) = 1, or \u2211q\u2032\u2208Q \u03c4(q,\u03b1 ,q\u2032) = 0, \u2022 L : Q \u2192 \u03a3O is a labeling function.", "startOffset": 79, "endOffset": 84}, {"referenceID": 0, "context": "\u2022 \u03a3I is a finite input alphabet, and \u03a3O is a finite output alphabet, \u2022 \u03c0 : Q \u2192 [0,1] is an initial probability distribution such that \u2211q\u2208Q \u03c0(q) = 1, \u2022 \u03c4 : Q\u00d7\u03a3I \u00d7Q \u2192 [0,1] is the transition probability function such that for all q \u2208 Q and all \u03b1 \u2208 \u03a3I , \u2211q\u2032\u2208Q \u03c4(q,\u03b1 ,q\u2032) = 1, or \u2211q\u2032\u2208Q \u03c4(q,\u03b1 ,q\u2032) = 0, \u2022 L : Q \u2192 \u03a3O is a labeling function.", "startOffset": 165, "endOffset": 170}, {"referenceID": 2, "context": "3 Scheduler A scheduler [3] for a MDP M is a function S : Q+ \u2192 \u03a3I such that S(q0q1 .", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "\u03c6 ::= P\u22b2\u22b3r(\u03c6) (\u22b2\u22b3 \u2208\u2265, \u2264, =; r \u2208 [0,1]; \u03c6 \u2208 LTL)", "startOffset": 32, "endOffset": 37}, {"referenceID": 4, "context": "IOALERGIA algorithm, is an adapted version of the ALERGIA algorithm [5, 8].", "startOffset": 68, "endOffset": 74}, {"referenceID": 7, "context": "IOALERGIA algorithm, is an adapted version of the ALERGIA algorithm [5, 8].", "startOffset": 68, "endOffset": 74}, {"referenceID": 7, "context": "Following the terminology from [8], Algorithm 1 maintains two sets of states: RED states, which have already been determined as representative states of partitions and will be included in the final output DLMDP, and BLUE states which are going to be tested.", "startOffset": 31, "endOffset": 34}, {"referenceID": 17, "context": ", the distance of distributions for every action is within the Hoeffding bound [19], Algorithm 3, parameterized by \u03b5 .", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "The Merge procedure (line 9 of the Algorithm 1) follows the same way as described in [8]: firstly, the (unique) transition leading to qb from its predecessor node q\u2032 ( f A(q\u2032,\u03b1 ,qb)> 0) is re-directed to qr by setting f A(q\u2032,\u03b1 ,qr)\u2190 f A(q\u2032,\u03b1 ,qb) and f A(q\u2032,\u03b1 ,qb) = 0.", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "A convergence analysis, similar to the analysis in [7, 13] for deterministic Markov chain models, can be obtained for IOALERGIA: first, one can show that in the large sample limit, IOALERGIA will identify up to bisimulation equivalence the structure of the true model from which the data was sampled; the structure of a model refers to all of its components, except the probability values of transitions.", "startOffset": 51, "endOffset": 58}, {"referenceID": 11, "context": "A convergence analysis, similar to the analysis in [7, 13] for deterministic Markov chain models, can be obtained for IOALERGIA: first, one can show that in the large sample limit, IOALERGIA will identify up to bisimulation equivalence the structure of the true model from which the data was sampled; the structure of a model refers to all of its components, except the probability values of transitions.", "startOffset": 51, "endOffset": 58}, {"referenceID": 11, "context": "As a slight refinement of Theorem 2 in [13], one then obtains that for any LTL formula \u03c6 :", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "As also observed in [13], similar results do not carry over to PCTL formulas.", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "In this section, we are going to show the applicability of the IOALERGIA algorithm using a case study based on the slot machine [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 11, "context": "Given sequences of alternating input and output symbols, the algorithm can automatically construct a model, for the reactive system under observation, and we have similar convergence result of the IOALERGIA algorithm as given in [13] for deterministic Markov chain models.", "startOffset": 229, "endOffset": 233}, {"referenceID": 12, "context": "Compared to the learning algorithm for deterministic automata [14], further research is required to make the learning algorithm that suitable for routine use.", "startOffset": 62, "endOffset": 66}], "year": 2012, "abstractText": "Constructing an accurate system model for formal model verification can be both resource demanding and time-consuming. To alleviate this shortcoming, algorithms have been proposed for automatically learning system models based on observed system behaviors. In this paper we extend the algorithm on learning probabilistic automata to reactive systems, where the observed system behavior is in the form of alternating sequences of inputs and outputs. We propose an algorithm for automatically learning a deterministic labeled Markov decision process model from the observed behavior of a reactive system. The proposed learning algorithm is adapted from algorithms for learning deterministic probabilistic finite automata, and extended to include both probabilistic and nondeterministic transitions. The algorithm is empirically analyzed and evaluated by learning system models of slot machines. The evaluation is performed by analyzing the probabilistic linear temporal logic properties of the system as well as by analyzing the schedulers, in particular the optimal schedulers, induced by the learned models.", "creator": "LaTeX with hyperref package"}}}