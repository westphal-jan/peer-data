{"id": "1212.3913", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Dec-2012", "title": "Group Component Analysis for Multiblock Data: Common and Individual Feature Extraction", "abstract": "Very often the data we encounter in practice is a collection of matrices measured from a same subject or under quite similar conditions, rather than a single one matrix. These matrices are linked naturally and should share some common features and at the same time they have their own individual features, due to the background in which they are measured and collected. In this study we proposed a new scheme for common and individual feature analysis (CIFA) on a given set of matrices, which can be viewed as a population based feature analysis and thus differs from most existing data analysis tools. According to whether the number of common features is known or not, two algorithms are proposed to extract the common basis shared by all the data. Then feature extraction can be performed on the common and individual space separately by incorporating the techniques such as dimensionality reduction and blind source separation. We also discussed how the proposed CIFA can be applied to classification and clustering tasks to significantly improve the performance. Our experimental results show some encouraging features of the proposed methods in comparison to the state-of-the-art methods on synthetic and real data.", "histories": [["v1", "Mon, 17 Dec 2012 07:56:15 GMT  (2272kb)", "https://arxiv.org/abs/1212.3913v1", "13 pages,11 figures"], ["v2", "Wed, 27 Feb 2013 02:24:36 GMT  (735kb,D)", "http://arxiv.org/abs/1212.3913v2", "13 pages,11 figures"], ["v3", "Tue, 1 Sep 2015 02:20:23 GMT  (825kb,D)", "http://arxiv.org/abs/1212.3913v3", "13 pages,11 figures"], ["v4", "Sun, 12 Mar 2017 08:36:27 GMT  (825kb,D)", "http://arxiv.org/abs/1212.3913v4", "13 pages,11 figures"]], "COMMENTS": "13 pages,11 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["guoxu zhou", "andrzej cichocki", "yu zhang", "danilo mandic"], "accepted": false, "id": "1212.3913"}, "pdf": {"name": "1212.3913.pdf", "metadata": {"source": "CRF", "title": "Group Component Analysis for Multi-block Data: Common and Individual Feature Extraction", "authors": ["Guoxu Zhou", "Andrzej Cichocki", "Danilo Mandic"], "emails": ["zhouguoxu@brain.riken.jp.", "cia@brain.riken.jp.", "yuzhang@ecust.edu.cn.", "d.mandic@imperial.ac.uk."], "sections": [{"heading": null, "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "II. COMMON ORTHOGONAL BASIS EXTRACTION", "text": "It is not that we are looking for a solution, but it is that we are managing to find a solution that would enable us to find a solution. (...) It is not that we are managing to find a solution. (...) It is that we are managing to find a solution. (...) It is that we are not managing to find a solution. (...) It is that we are not managing to find a solution. (...) It is that we are trying to find a solution. (...) It is that we are not managing to find a solution. (...) It is that we are trying to create it. (...) It is that we want to create it. (...) It is that we want to do it. (...) It is that we want to do it. (...) It is that we want to do it. (...) It is that we want to do it. (...) It is that we want to do it. (...) It is that we want to do it."}, {"heading": "A. The COBE Algorithm: The Number of Common Components C is Unknown", "text": "The estimation of the common components A and B is a central part of the solution (7)."}, {"heading": "B. The COBE Algorithm When The Number of Common Components C is Given", "text": "For a given C, after analysis in section II-A, the common components can be found by solving: min Zn, A-N-N-N = 1-QnZn-A-2F, s.t. A-T A-A-I, (22) by alternating optimization with respect to Zn and A-A. In this way, with the A-track fixed, the optimal Zn of Zn-QTn A-N, (23) is calculated, while with the Zn-n-N-track fixed, (22) Tomax A-track (PT A-A-A), s.t. A-T-A-A-I, (24) where track (\u00b7) is the track of a matrix and P-Z is N-N-1 QnZn. To solve (24), let P-VT-track (PT-RD-C) be the truncated SVD track (tSVD) of P, where track (\u00b7) and P-T-track (n-ZN-1)."}, {"heading": "C. Pre-processing: Dimensionality Reduction", "text": "Like CCA, COBE is meaningless if Rn = D applies to all n, because for each invertible matrix A \u00b7 D there are always matrices B \u00b2 n, so Yn = A \u00b2 B \u00b2 Tn, i.e. each invertible matrix D \u00b2 D forms a common basis. Therefore, in the model (7) the condition Rn < D is required for all Yn. This requirement is not too restrictive and has physical justification, since for real data of high dimensionality the latent rank is often significantly lower than dimensionality3The main difference here is that A \u00b2 is not necessarily square of the observed data. If Rn < D is not met, we must perform a dimensionality reduction (such as PCA), justifying, for example, (1) the need for a two-step method before applying COBE (1)."}, {"heading": "D. Relation to Other Methods", "text": "In order to illustrate the relationship between COBE and CCA, we must bear in mind that the data Y1 and Y2, CCA are the vectors w1 and w2, which are the correlation between the two variables (Y1w1, Y2w2), while COBE is only the components for which the correlation is higher than a specific threshold (1). (1) We must continue to focus on this relationship in order to understand the correlations of the variables. (2) The correlations of the variables are then for the correlations 1: 1, 2, 2, 3, 5, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,"}, {"heading": "E. Scalability For Large-Scale Problems", "text": "For large-scale data, the D and Jn (n-N) indices in (1) are quite large. First, we should remember that in (15), (20) and (22) we use dimensionally reduced matrices Q-RD-Rn with Rn < D, and therefore the value of Jn is generally not a problem. On the other hand, for a very large D, the time and memory requirements of COBE can be reduced in the following way. Let P-RDP-D create a random matrix with maxn-N (Rn) < DP D. Then, starting with (12), we can first solve the model: min-n-N-YPnWn \u2212 A-P-2F (30), where YPn = PYn-RDP-Jn is much smaller than Yn and A-P = PA-D. After estimating the matrices Wn using COBE or COBEC, the corresponding common base of YPn-RDP-Jn components can be calculated as more common component-Yn-Yn."}, {"heading": "III. COMMON AND INDIVIDUAL FEATURE EXTRACTION", "text": "(CIFE) We will now examine the versatility of the COBE approach using several established paradigms for trait extraction."}, {"heading": "A. Linked BSS with Pre-whitening", "text": "This year it is more than ever before."}, {"heading": "B. Common Nonnegative Features Extraction (CNFE)", "text": "For non-negative latent sources, we cannot directly apply the NMF methods. Instead, we must first extract the common subspace with (11), i.e., Y-n = A-A-T Yn, and then the following low approximation based on (semi-) non-negative matrix factorization (NMF) model. (35) The subsequent use of low-level NMF methods (even if M-n is not negative) or low-level semiNMF methods (where M-n is real) allows us to extract the common non-negative components F. (35) The subsequent iterative multiplicative updating of the rules results in non-negative processing (M-N)."}, {"heading": "IV. EXAMPLE APPLICATIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Classification Using Common Features", "text": "In classification and pattern recognition, the training data contain training samples and their designations, while the objects of the same category naturally share some common characteristics. Specifically, for a number of common characteristics extracted from the kth category, k-k = {1, 2,.., K}, which are denoted by F-k, after the arrival of a new test sample yt-RD can calculate their matching value rt (k) with each F-k as follows: rt (k) = matching (yt, F-k), k-K. (37) Since the samples within a given class should have some characteristics in common, the marking of yt is estimated as aslt = arg max k-K rt (k). (38) The matching value rt (k) can be defined in many ways, such as the Euclidean distance or correlation (angle) between yt and the space spanned by F-k, which can be resolved by leastquares or CCA. See Fig.6 for the diagram for the same category."}, {"heading": "B. Clustering Using Individual Features", "text": "Cluster analysis assigns a set of objects to clusters in such a way that the objects belonging to the same cluster are the most similar. In contrast to classification, clusters use an uncontrolled learning approach. In cluster analysis, it is common for all samples to share some common characteristics, although they may come from different clusters. For example, it is logical that each face has common facial characteristics, such as cheek, nose, eyes and mouth, whose shapes and locations are similar. Their common characteristics are not significant for clustering because they do not provide discriminatory information. Therefore, it is logical to first remove these common / similar characteristics in all samples and then use their individual characteristics to cluster the objects. Fig.7 shows that COBE is able to extract common faces (characteristics) in the PIE database using CNFE (details are given in the next section)."}, {"heading": "V. SIMULATIONS AND VALIDATION", "text": "In fact, the fact is that most of us are able to go in search of a solution that is able, in which they are able to move, \"he said in an interview with the\" Welt. \""}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "A new scheme for extracting common and individual characteristics (CIFE) for naturally linked multi-block data has been proposed, along with two new efficient algorithms for extracting common orthogonal bases (COBE), depending on whether the number of common components is known or not. We have also introduced the concept of combined blind source separation (BSS) of multi-block data to perform effective task-dependent feature extraction in shared and individual sub-spaces, rather than in a global high-dimensional space. The proposed CIFE scheme has been validated on classification and cluster tasks, taking advantage of the separate common and individual characteristics. Comprehensive simulations have demonstrated the ability of the proposed methods to efficiently and accurately extract common characteristics that exist in multi-block data.In this study, we have focused on developing a uniform and versatile scheme of common and individual feature extraction."}], "references": [{"title": "Joint and individual variation explained (JIVE) for integrated analysis of multiple data types", "author": ["E. Lock", "K. Hoadley", "J.S. Marron", "A. Nobel"], "venue": "The Annals of Applied Statistics, vol. 7, no. 1, pp. 523\u2013542, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayesian joint analysis of heterogeneous genomics data", "author": ["P. Ray", "L. Zheng", "J. Lucas", "L. Carin"], "venue": "Bioinformatics, vol. 30, no. 10, pp. 1370\u2013 1376, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Performing disco-sca to search for distinctive and common information in linked data", "author": ["M. Schouteden", "K. Van Deun", "T. Wilderjans", "I. Van Mechelen"], "venue": "Behavior Research Methods, vol. 46, no. 2, pp. 576\u2013587, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Common and cluster-specific simultaneous component analysis", "author": ["K.D. Roover", "M.E. Timmerman", "B. Mesquita", "E. Ceulemans"], "venue": "PLoS ONE, vol. 9, no. 4, p. e93796, April 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Proc. Advances in Neural Information Processing Systems 19, B. Sch\u00f6lkopf, J. Platt, and T. Hoffman, Eds. MIT Press, 2007, pp. 41\u201348.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "J. Mach. Learn. Res., vol. 6, pp. 1817\u20131853, Dec. 2005.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1817}, {"title": "Relations between two sets of variants", "author": ["H. Hotelling"], "venue": "Biometrika, vol. 28, no. 3-4, pp. 321\u2013377, 1936.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1936}, {"title": "A least-squares framework for component analysis", "author": ["F. De la Torre"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 6, pp. 1041 \u20131055, June 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Analysis and online realization of the cca approach for blind source separation", "author": ["W. Liu", "D. Mandic", "A. Cichocki"], "venue": "IEEE Transactions on Neural Networks, vol. 18, no. 5, pp. 1505\u20131510, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Partial least squares", "author": ["W. Herman"], "venue": "Proc. Encyclopedia of statistical sciences, S. Kotz and N. L. Johnson, Eds. New York: Wiley, 1995, vol. 6.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Global, local and unique decompositions in onpls for multiblock data analysis", "author": ["T. L\u00f6fstedt", "D. Hoffman", "J. Trygg"], "venue": "Analytica Chimica Acta, vol. 791, no. 0, pp. 13 \u2013 24, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "O2-PLS, a two-block (X-Y) latent variable regression (LVR) method with an integral OSC filter", "author": ["J. Trygg", "S. Wold"], "venue": "Journal of Chemometrics, vol. 17, no. 1, pp. 53\u201364, 2003.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Canonical analysis of several sets of variables", "author": ["J.R. Kettenring"], "venue": "Biometrika, vol. 58, no. 3, pp. 433\u2013451, 1971.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1971}, {"title": "Joint blind source separation by multiset canonical correlation analysis", "author": ["Y.-O. Li", "T. Adali", "W. Wang", "V. Calhoun"], "venue": "IEEE Transactions on Signal Processing, vol. 57, no. 10, pp. 3918 \u20133929, oct. 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Canonical correlation analysis for multilabel classification: A least-squares formulation, extensions, and analysis", "author": ["L. Sun", "S. Ji", "J. Ye"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 1, pp. 194 \u2013200, jan. 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Extensions of sparse canonical correlation analysis with applications to genomic data", "author": ["D.M. Witten", "R.J. Tibshirani"], "venue": "Statistical Applications in Genetics and Molecular Biology, vol. 8, no. 1, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Frequency recognition in SSVEP-based BCI using multiset canonical correlation analysis", "author": ["Y. Zhang", "G. Zhou", "J. Jin", "X. Wang", "A. Cichocki"], "venue": "International Journal of Neural Systems, vol. 4, no. 24, pp. 1 450 013(1\u201314), 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Population value decomposition, a framework for the analysis of image populations", "author": ["C. Crainiceanu", "B.S. Caffo", "S. Luo", "V.M. Zipunnikov", "N.M. Punjabi"], "venue": "Journal of the American Statistical Association, vol. 106, no. 495, pp. 775\u2013790, 2011.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonnegative Matrix and Tensor Factorizations: Applications to Exploratory Multi-way Data Analysis and Blind Source Separationpplications to Exploratory Multi-Way Data Analysis and Blind Source Separation", "author": ["A. Cichocki", "R. Zdunek", "A.H. Phan", "S.-I. Amari"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Diversity in independent component and vector analyses: Identifiability, algorithms, and applications in medical imaging", "author": ["T. Adali", "M. Anderson", "G.-S. Fu"], "venue": "IEEE Signal Processing Magazine, vol. 31, no. 3, pp. 18\u201333, May 2014.  IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS  14", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Group information guided ICA for fMRI data analysis", "author": ["Y. Du", "Y. Fan"], "venue": "NeuroImage, vol. 69, no. 0, pp. 157 \u2013 197, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Capturing group variability using IVA: A simulation study and graph-theoretical analysis", "author": ["S. Ma", "R. Phlypo", "V. Calhoun", "T. Adali"], "venue": "Proc. IEEE 38th International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Vancouver, BC, May 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive Blind Signal and Image Processing: Learning Algorithms and Applications", "author": ["A. Cichocki", "S. Amari"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Common components analysis via linked blind source separation", "author": ["G. Zhou", "A. Cichocki", "D. Mandic"], "venue": "Proc. IEEE 40th International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, April 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Detecting the number of clusters in N -way probabilistic clustering", "author": ["Z. He", "A. Cichocki.", "S. Xie", "K. Choi"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 11, pp. 2006\u2013 2021, 2010.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Matrix Computations, 3rd ed", "author": ["G. Golub", "C. Van Loan"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1996}, {"title": "Robust principal component analysis?", "author": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}, {"title": "Time-frequency approach to underdetermined blind source separation", "author": ["S. Xie", "L. Yang", "J.-M. Yang", "Y. Xiang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 2, pp. 306 \u2013316, Feb. 2012.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "A unified framework for group independent component analysis for multi-subject fMRI data", "author": ["Y. Guo", "G. Pagnoni"], "venue": "NeuroImage, vol. 42, no. 3, pp. 1078 \u2013 1093, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast nonnegative matrix/tensor factorization based on low-rank approximation", "author": ["G. Zhou", "A. Cichocki", "S. Xie"], "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 6, pp. 2928\u20132940, june 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Minimum-volumeconstrained nonnegative matrix factorization: Enhanced ability of learning parts", "author": ["G. Zhou", "S. Xie", "Z. Yang", "J.-M. Yang", "Z. He"], "venue": "IEEE Transactions on Neural Networks, vol. 22, no. 10, pp. 1626 \u20131637, Oct. 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "Projection-pursuit-based method for blind separation of nonnegative sources", "author": ["Z. Yang", "Y. Xiang", "Y. Rong", "S. Xie"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 24, no. 1, pp. 47\u201357, Jan 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonnegative blind source separation by sparse component analysis based on determinant measure", "author": ["Z. Yang", "Y. Xiang", "S. Xie", "S. Ding", "Y. Rong"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 10, pp. 1601\u20131610, Oct 2012.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Nonnegative matrix factorization applied to nonlinear speech and image cryptosystems", "author": ["S. Xie", "Z. Yang", "Y. Fu"], "venue": "IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 55, no. 8, pp. 2356\u20132367, Sept 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "A general framework for dimensionality-reducing data visualization mapping", "author": ["K. Bunte", "M. Biehl", "B. Hammer"], "venue": "Neural Computation, vol. 24, no. 3, pp. 771\u2013804, Dec. 2011.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "ICALAB toolbox", "author": ["A. Cichocki", "S. Amari", "K. Siwek", "T. Tanaka"], "venue": "2007. [Online]. Available: http://www.bsp.brain.riken.jp/ICALAB.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2007}, {"title": "A blind source separation technique using second-order statistics", "author": ["A. Belouchrani", "K. AbedMeraim", "J.F. Cardoso", "E. Moulines"], "venue": "IEEE Transactions on Signal Processing, vol. 45, no. 2, pp. 434\u2013444, Feb 1997.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1997}, {"title": "Nonnegative leastcorrelated component analysis for separation of dependent sources by volume maximization", "author": ["F.Y. Wang", "C.Y. Chi", "T.H. Chan", "Y. Wang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 5, pp. 875\u2013888, May 2010.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "From few to many: illumination cone models for face recognition under variable lighting and pose", "author": ["A. Georghiades", "P. Belhumeur", "D. Kriegman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 6, pp. 643 \u2013660, Jun 2001.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2001}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 8, pp. 1548 \u20131560, aug. 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing data using t-SNE", "author": ["L. Van Der Maaten", "C. Detection"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 2579\u20132605, 2008.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2008}, {"title": "Improved minmax cut graph clustering with nonnegative relaxation", "author": ["F. Nie", "C. Ding", "D. Luo", "H. Huang"], "venue": "Proc. Machine Learning and Knowledge Discovery in Databases, ser. Lecture Notes in Computer Science. Springer Berlin / Heidelberg, 2010, vol. 6322, pp. 451\u2013466.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2010}, {"title": "Analyzing appearance and contour based methods for object categorization", "author": ["B. Leibe", "B. Schiele"], "venue": "Proc. International Conference on Computer Vision and Pattern Recognition, vol. 2, Los Alamitos, CA, USA, 2003, p. 409. [Online]. Available: http://www.d2.mpi-inf.mpg.de/Datasets/ETH80", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2003}, {"title": "LIBSVM: A library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology, vol. 2, pp. 27:1\u201327:27, 2011, software available at http://www.csie.ntu. edu.tw/\u223ccjlin/libsvm.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2011}, {"title": "Eigenfeature regularization and extraction in face recognition", "author": ["X. Jiang", "B. Mandal", "A. Kot"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 3, pp. 383\u2013394, March 2008.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2008}, {"title": "Singletrial analysis and classification of ERP components \u2014 a tutorial", "author": ["B. Blankertz", "S. Lemm", "M. Treder", "S. Haufe", "K.-R. M\u00fcller"], "venue": "NeuroImage, vol. 56, no. 2, pp. 814 \u2013 825, 2011.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "The identification and separation of such common and individual information in order to employ the features highly relevant to the data analysis task at hand promises to significantly improve data analysis [1], [2], [3], [4].", "startOffset": 206, "endOffset": 209}, {"referenceID": 1, "context": "The identification and separation of such common and individual information in order to employ the features highly relevant to the data analysis task at hand promises to significantly improve data analysis [1], [2], [3], [4].", "startOffset": 211, "endOffset": 214}, {"referenceID": 2, "context": "The identification and separation of such common and individual information in order to employ the features highly relevant to the data analysis task at hand promises to significantly improve data analysis [1], [2], [3], [4].", "startOffset": 216, "endOffset": 219}, {"referenceID": 3, "context": "The identification and separation of such common and individual information in order to employ the features highly relevant to the data analysis task at hand promises to significantly improve data analysis [1], [2], [3], [4].", "startOffset": 221, "endOffset": 224}, {"referenceID": 4, "context": "For example, shared features among tasks have been exploited to improve the performance of supervised and semi-supervised learning [5], [6].", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "For example, shared features among tasks have been exploited to improve the performance of supervised and semi-supervised learning [5], [6].", "startOffset": 136, "endOffset": 139}, {"referenceID": 6, "context": "Common data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12].", "startOffset": 164, "endOffset": 167}, {"referenceID": 7, "context": "Common data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12].", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "Common data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12].", "startOffset": 174, "endOffset": 177}, {"referenceID": 9, "context": "Common data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12].", "startOffset": 266, "endOffset": 270}, {"referenceID": 10, "context": "Common data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12].", "startOffset": 272, "endOffset": 276}, {"referenceID": 11, "context": "Common data analysis techniques that relate two data sets include canonical correlation analysis (CCA) which aims to maximize the correlation between two data sets [7], [8], [9] and the class of partial least squares (PLS) methods that maximize the cross-covariance [10], [11], [12].", "startOffset": 278, "endOffset": 282}, {"referenceID": 12, "context": "CCA has also been generalized to multiple data sets in the context of blind source separation (BSS) and feature extraction [13], [14], [15], [16], [17].", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "CCA has also been generalized to multiple data sets in the context of blind source separation (BSS) and feature extraction [13], [14], [15], [16], [17].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "CCA has also been generalized to multiple data sets in the context of blind source separation (BSS) and feature extraction [13], [14], [15], [16], [17].", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "CCA has also been generalized to multiple data sets in the context of blind source separation (BSS) and feature extraction [13], [14], [15], [16], [17].", "startOffset": 141, "endOffset": 145}, {"referenceID": 16, "context": "CCA has also been generalized to multiple data sets in the context of blind source separation (BSS) and feature extraction [13], [14], [15], [16], [17].", "startOffset": 147, "endOffset": 151}, {"referenceID": 17, "context": "For images, the population value decomposition (PVD) jointly analyzes same-size images [18], and can be considered as a special case of tensor (Tucker) decompositions, an active research topic in high-order data analysis and exploration [19].", "startOffset": 87, "endOffset": 91}, {"referenceID": 18, "context": "For images, the population value decomposition (PVD) jointly analyzes same-size images [18], and can be considered as a special case of tensor (Tucker) decompositions, an active research topic in high-order data analysis and exploration [19].", "startOffset": 237, "endOffset": 241}, {"referenceID": 19, "context": "Very recently, group independent component analysis (ICA) and independent vector analysis (IVA) were proposed to capture group variables from multiblock data [20], [21], [22].", "startOffset": 158, "endOffset": 162}, {"referenceID": 20, "context": "Very recently, group independent component analysis (ICA) and independent vector analysis (IVA) were proposed to capture group variables from multiblock data [20], [21], [22].", "startOffset": 164, "endOffset": 168}, {"referenceID": 21, "context": "Very recently, group independent component analysis (ICA) and independent vector analysis (IVA) were proposed to capture group variables from multiblock data [20], [21], [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": "The joint and individual variation explained (JIVE) [1] is a step in the right direction which simultaneously extracts both joint and individual variations across the members of a heterogeneous data ensemble.", "startOffset": 52, "endOffset": 55}, {"referenceID": 22, "context": "algorithms guarantee the identification of true common components and can be applied to separate common and individual subspaces of multi-block data; 2) A unifying framework for multi-block data analysis, which deals with heterogeneous data structures by respectively applying suitable well-established matrix factorization methods (such as blind source separation (BSS) [23] and nonnegative matrix factorization (NMF) [19]) to the common and individual subspaces separately rather than to the global space of a data ensemble.", "startOffset": 371, "endOffset": 375}, {"referenceID": 18, "context": "algorithms guarantee the identification of true common components and can be applied to separate common and individual subspaces of multi-block data; 2) A unifying framework for multi-block data analysis, which deals with heterogeneous data structures by respectively applying suitable well-established matrix factorization methods (such as blind source separation (BSS) [23] and nonnegative matrix factorization (NMF) [19]) to the common and individual subspaces separately rather than to the global space of a data ensemble.", "startOffset": 419, "endOffset": 423}, {"referenceID": 23, "context": "A conference summary of part of our results has been accepted for publication at ICASSP 2015 [24].", "startOffset": 93, "endOffset": 97}, {"referenceID": 23, "context": "Relative to [24], this journal version includes detailed derivations, new algorithms, potential applications in machine learning, and further experiments.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "A number of matrix factorization techniques exist to solve (1), including PCA, BSS [23], however, these methods consider each matrix Yn separately.", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "The JIVE method also solves the model (7), although this is not stated explicitly [1].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "The JIVE approach performs these two functions simultaneously by applying the alternating least squares (ALS) to (7) and quantifies the amount of joint variation between data types [1].", "startOffset": 181, "endOffset": 184}, {"referenceID": 24, "context": "Then the Second ORder sTatistic of the Eigenvalues (SORTE) method [25], which was proposed to detect the gap between the eigenvalues corresponding to the signal space and those belonging to the noise space, can be applied here to estimate the number of common components.", "startOffset": 66, "endOffset": 70}, {"referenceID": 25, "context": "Motivated by the work3 in [26] (page 601), the optimal solution of (24) becomes \u0100 = EV , (25)", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": "Gaussian noise; for sparse distributions we may use robust PCA (RPCA) [27], while the number of latent components Rn can be estimated using, e.", "startOffset": 70, "endOffset": 74}, {"referenceID": 24, "context": "SORTE [25], before applying PCA.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "If our aim is to project the common components onto a feature space with some desired properties (uniqueness), this can be achieved by BSS [23], which finds latent variables S from their linear mixtures Y = SM such that", "startOffset": 139, "endOffset": 143}, {"referenceID": 27, "context": "To this end, for example, we can apply the novel tensor based approach proposed in [28].", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "Note that the JBSS method in [14] also performs BSS involving multi-block data; it extracts a group of signals with the highest correlations each time, and requires that the extracted groups have distinct correlations.", "startOffset": 29, "endOffset": 33}, {"referenceID": 28, "context": "Recently in neural science, group ICA and independent vector analysis (IVA) have been widely applied to capture common (group) variables or components from a group (set) of data matrices [29], [21], [22].", "startOffset": 187, "endOffset": 191}, {"referenceID": 20, "context": "Recently in neural science, group ICA and independent vector analysis (IVA) have been widely applied to capture common (group) variables or components from a group (set) of data matrices [29], [21], [22].", "startOffset": 193, "endOffset": 197}, {"referenceID": 21, "context": "Recently in neural science, group ICA and independent vector analysis (IVA) have been widely applied to capture common (group) variables or components from a group (set) of data matrices [29], [21], [22].", "startOffset": 199, "endOffset": 203}, {"referenceID": 29, "context": ", \u0232n = \u0100\u0100 T Yn, and subsequently use the following low-rank approximation based (semi-) nonnegative matrix factorization (NMF) model [30]:", "startOffset": 133, "endOffset": 137}, {"referenceID": 29, "context": "where ~ and are element-wise product and division of matrices, see [30] for detailed convergence analysis.", "startOffset": 67, "endOffset": 71}, {"referenceID": 30, "context": "Similar to BSS, we may impose additional constraints to extract unique nonnegative components, see [31], [32], [33], [34].", "startOffset": 99, "endOffset": 103}, {"referenceID": 31, "context": "Similar to BSS, we may impose additional constraints to extract unique nonnegative components, see [31], [32], [33], [34].", "startOffset": 105, "endOffset": 109}, {"referenceID": 32, "context": "Similar to BSS, we may impose additional constraints to extract unique nonnegative components, see [31], [32], [33], [34].", "startOffset": 111, "endOffset": 115}, {"referenceID": 33, "context": "Similar to BSS, we may impose additional constraints to extract unique nonnegative components, see [31], [32], [33], [34].", "startOffset": 117, "endOffset": 121}, {"referenceID": 34, "context": "For example, we may wish to visualize data in a lowdimensional space [35], or to extract discriminative information, or to establish neighbor relationship.", "startOffset": 69, "endOffset": 73}, {"referenceID": 35, "context": "mat) [36], and the other six components were drawn from independent standard normal distributions.", "startOffset": 5, "endOffset": 9}, {"referenceID": 0, "context": "We used the model Yn = SnMn + En, where En contains white Gaussian noise (SNR=20dB), and first employed the COBE, JIVE [1], JBSS [14], and PCA methods5 to extract the common bases \u0100, followed by the SOBI method [37] to extract the latent common speech signals S from \u0100, see Section III-A.", "startOffset": 119, "endOffset": 122}, {"referenceID": 13, "context": "We used the model Yn = SnMn + En, where En contains white Gaussian noise (SNR=20dB), and first employed the COBE, JIVE [1], JBSS [14], and PCA methods5 to extract the common bases \u0100, followed by the SOBI method [37] to extract the latent common speech signals S from \u0100, see Section III-A.", "startOffset": 129, "endOffset": 133}, {"referenceID": 36, "context": "We used the model Yn = SnMn + En, where En contains white Gaussian noise (SNR=20dB), and first employed the COBE, JIVE [1], JBSS [14], and PCA methods5 to extract the common bases \u0100, followed by the SOBI method [37] to extract the latent common speech signals S from \u0100, see Section III-A.", "startOffset": 211, "endOffset": 215}, {"referenceID": 0, "context": "Since the estimation of the number of components in [1] is quite time consuming and the performance is sensitive to selection of its parameters (for this instance, JIVE took more than two hours to estimate the rank); this limits the practical applications of JIVE for large-scale problems.", "startOffset": 52, "endOffset": 55}, {"referenceID": 37, "context": "Dual-energy chest X-ray imaging is a diagnostic tool for early signs of lung cancer [38], however, the ribs, clavicles overlapped with soft tissues, and environmental noise make it challenging to detect affected lung nodules.", "startOffset": 84, "endOffset": 88}, {"referenceID": 37, "context": "10(d) displays four samples of nonnegative components extracted by nLCA-IVM [38].", "startOffset": 76, "endOffset": 80}, {"referenceID": 38, "context": "The database contains 16,128 images of 28 human subjects under 9 poses and 64 illumination conditions [39].", "startOffset": 102, "endOffset": 106}, {"referenceID": 39, "context": "We used a pre-processed version from [40] which consists of 2,856 full frontal face gray scale images taken at the pose c27.", "startOffset": 37, "endOffset": 41}, {"referenceID": 40, "context": "The number of common components was specified as 2 in all experiments, and the two t-SNE components of their individual parts Y\u0306n were used to cluster the data by using K-means (see [41] for the t-SNE method).", "startOffset": 182, "endOffset": 186}, {"referenceID": 39, "context": "Two widely used performance indices: Accuracy (%) and Normalized Mutual Information (NMI) were adopted to evaluate the clustering results, see [40] for detailed definitions of these two metrics.", "startOffset": 143, "endOffset": 147}, {"referenceID": 39, "context": "The proposed method was compared with PCA with K principal components, GNMF [40] (using their recommended settings), and the improved MinMax Cut (MMCut) method [42].", "startOffset": 76, "endOffset": 80}, {"referenceID": 41, "context": "The proposed method was compared with PCA with K principal components, GNMF [40] (using their recommended settings), and the improved MinMax Cut (MMCut) method [42].", "startOffset": 160, "endOffset": 164}, {"referenceID": 42, "context": "hemisphere [43].", "startOffset": 11, "endOffset": 15}, {"referenceID": 43, "context": "6) with the KNN classifier included in MATLAB 2010b, the SVM classifier [44], the Eigenfeature Regularization and Extraction method [45] (ERE), and the shrinkage LDA [46] (SLDA).", "startOffset": 72, "endOffset": 76}, {"referenceID": 44, "context": "6) with the KNN classifier included in MATLAB 2010b, the SVM classifier [44], the Eigenfeature Regularization and Extraction method [45] (ERE), and the shrinkage LDA [46] (SLDA).", "startOffset": 132, "endOffset": 136}, {"referenceID": 45, "context": "6) with the KNN classifier included in MATLAB 2010b, the SVM classifier [44], the Eigenfeature Regularization and Extraction method [45] (ERE), and the shrinkage LDA [46] (SLDA).", "startOffset": 166, "endOffset": 170}], "year": 2017, "abstractText": "Real world data are often acquired as a collection of matrices rather than as a single matrix. Such multi-block data are naturally linked and typically share some common features and at the same time exhibit their own individual features, reflecting the background in which they are measured and collected. To exploit the linked nature of data, we propose a new framework for common and individual feature extraction (CIFE) which identifies and separates the common and individual features from multi-block data. Two efficient algorithms termed common orthogonal basis extraction (COBE) are proposed to extract the common basis which is shared by all data, independent on whether the number of common components is given or not. Feature extraction is then performed on the common and the individual subspaces separately, by incorporating dimensionality reduction and blind source separation techniques. Extensive experimental results on both synthetic and real-world data show significant advantages of the proposed CIFE method in comparison to the state-of-the-art.", "creator": "LaTeX with hyperref package"}}}