{"id": "1212.5524", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Dec-2012", "title": "Reinforcement learning for port-Hamiltonian systems", "abstract": "Passivity-based control for port-Hamiltonian systems provides an intuitive way of achieving stabilization by rendering a system passive with respect to a desired storage function. However, in most instances the control law has to be calculated by solving a complex partial differential equation (PDE). This paper considers energy-balancing passivity-based control (EB-PBC), which is a form of PBC in which the closed-loop energy is equal to the difference between the stored and supplied energies. We propose a method to parameterize EB-PBC that preserves the systems's PDE matching conditions, does not require the specification of a global desired Hamiltonian, includes performance criteria, and is robust to extra non-linearities such as control input saturation. The parameters of the control law are found using actor-critic reinforcement learning, enabling learning near-optimal control policies satisfying a desired closed-loop energy landscape. The advantages are that near-optimal controllers can be generated using standard energy shaping techniques and that the solutions learned can be interpreted in terms of energy shaping and damping injection, which makes it possible to numerically assess stability using passivity theory. From the reinforcement learning perspective, our proposal allows for the class of port-Hamiltonian systems to be incorporated in the actor-critic framework, speeding up the learning thanks to the resulting parameterization of the policy. The method has been successfully applied to the pendulum swing-up problem in simulations and real-life experiments.", "histories": [["v1", "Fri, 21 Dec 2012 16:57:28 GMT  (213kb,D)", "https://arxiv.org/abs/1212.5524v1", "submitted"], ["v2", "Thu, 22 Aug 2013 16:16:31 GMT  (263kb,D)", "http://arxiv.org/abs/1212.5524v2", "submitted"]], "COMMENTS": "submitted", "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["olivier sprangers", "gabriel a d lopes", "robert babuska"], "accepted": false, "id": "1212.5524"}, "pdf": {"name": "1212.5524.pdf", "metadata": {"source": "CRF", "title": "Reinforcement learning for port-Hamiltonian systems", "authors": ["Olivier Sprangers", "Gabriel A. D. Lopes"], "emails": ["osprangers@gmail.com;", "r.babuska}@tudelft.nl"], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "II. PORT-HAMILTONIAN SYSTEMS", "text": "Port Hamiltonian (PH) systems are a natural method of representing a physical system in terms of its energy exchange with the environment through ports (4).The general framework of PH systems was introduced in [19] and formalized in [20].In this essay, we will consider the input-state-output representation of the PH system used by Form1: \u03a3: {x \u0432 = [J (x) \u2212 R (x)], [xH) + g (x) u y = gT (x), (x) (1), where x-Rn is the state vector, u-Rm, m-R: Rn \u2192 Rn \u00b7 n with J (x) = \u2212 J (x) T and R (x) = R (x), where x-Rn is the state vector, u-Rp, m-Rm \u2264 d is the control input, J \u2212 Rn \u2192 Rn \u00b7 n with J (x) = \u2212 J (x) T (x), R \u2212 xH (x) and T (x): x-T (x), T (x) and T (x)."}, {"heading": "A. Energy Shaping", "text": "Define the added energy function: Ha (x): = Hd (x) \u2212 H (x) (7) A law for feedback of the state (x) is supposed to satisfy the energy balancing property if it satisfies: H (x) = \u2212 uTes (x) y (8) If (8) applies, the desired energy Hd (x) is the difference between the stored and supplied energy. If we take g (x) Rn \u00b7 m, m < n, rank {g (x)} = m, all (gradients) vectors are column vectors. with g \u2020 (x) = (gT (x) \u2020 (x) g (x) \u2212 1gT (x), the EB-PBC problem with Ha (x) solves the following vector. With g (x) (G (2) (T) = (full) x (x)."}, {"heading": "B. Damping Injection", "text": "Attenuation occurs by feeding back the (new) passive output gT (x) \u0394xHd (x), udi (x) = \u2212 K (x) gT (x) \u0394xHd (x) (11) with K (x) \u0435Rm \u00b7 m, K (x) = KT (x) \u2265 0, so that: Rd (x) = R (x) + g (x) K (x) g T (x) (12) The complete control law thus consists of an energy-forming part and a damping injection part: u (x) = ues (x) + udi (x) = g \u2020 (x) F (x) \u0445xHa (x) \u2212 K (x) gT (x) \u0445xHd (x) (13)"}, {"heading": "III. ACTOR-CRITIC REINFORCEMENT LEARNING", "text": "In a deterministic setting, this MDP is defined by the tuple M (X, U, f, \u03c1), where X is the state space, U is the action space, and f: X \u00b7 U \u2192 X describes the state transition function, which describes the process to be controlled that returns the state xk + 1 after applying measures uk in state xk. The vector xk is achieved by applying a zero order, discretizing xk = x (kTs) with Ts describing the sampling time. The reward function is defined by: X \u00d7 U \u2192 R and gives a scalable reward rk + 1 = 1, uk) after each transition. The goal of RL is to find an optimal control policy: X \u2192 U by maximizing an expected cumulative or total reward, which is described as a function of the immediate expected rewards."}, {"heading": "IV. ENERGY-BALANCING ACTOR-CRITIC", "text": "In this section we present our main findings: We will use the PDE (10) in relation to the desired closed-loop energy Hd (x) and Damping K (x) in relation to the desired closed-loop energy Hd (x) and Damping K (x) in relation to the desired closed-loop energy Hd (x) and Damping K (x) in relation to the desired closed-loop energy Hd (x): [g) FT (x) gT (x)."}, {"heading": "V. MECHANICAL SYSTEMS", "text": "To illustrate an application of the method, let us consider a fully activated mechanical system of form: \u0435m: [q-p] = [0 I-I-R] [0 qH (q, p) = pH (q, p)] + [0 I] u y = [0 I] [0 qH (q, p) = pH (q, p)] (37) with q-Rn, p-Rn: H (q, p) = 12 pTM \u2212 1 (q) p + P (q) (38) with M (q) = MT (q) > 0 of the damping matrix. The system allows (1) the inertial matrix and the hazard: H (q, p) = 12 pTM \u2212 1 (q) p + P (q) (38) with M (q) = T (q) > 0 the inertial matrix and P (q) the potential energy."}, {"heading": "VI. EXAMPLE: PENDULUM SWING-UP", "text": "In order to validate our method, the problem of the upswing of an inverted pendulum subject to the control is investigated in the simulation and examined on the basis of the actual physical structure shown in Figure 1. \u2212 The pendulum oscillation problem is a low-dimensional but highly nonlinear control problem that is frequently used as a yardstick in the literature of the RL [9] and has also been investigated in the PBC [11]. \u2212 The equations of motion allow (37) and read: [q] p: [q] p: [0 1 \u2212 1 \u2212 R: (q)] [q \u00b2 (q \u00b2)] p: p: [0 Kp Rp] u y = [0Kp Rp] p: [1 \u2212 p) p: p: p: p \u00b2 pH (40) with the angle of the pendulum and p: p \u00b2 p: p \u00b2 Jp: p \u00b2 p \u00b2 s, which is the complete measurable state x."}, {"heading": "A. Function Approximation", "text": "In order to bring the critic and the two actors closer together, approximate quantities of the function are necessary. In this paper, we use the Fourier base (25) for its ease of use, the ability to integrate symmetry information into the system, and the ability to determine properties useful for the stability analysis of this specific problem. We define a multivariate N th-order2 Fourier base for n dimensions as: \u03c6i (x) = cos T i x-x value, i-value {1,. (N + 1) n-value, which means that all possible N + 1 integer values or frequencies in a vector in Zn are combined to create2 \"order\" refers to the order of approximation."}, {"heading": "B. Simulation", "text": "The task is to define and stabilize the functioning of the system. (...) The task is to learn and to stabilize the basis. (...) The task is to learn and to stabilize. \"(...) The task is to define the basis for the functioning of the system. (...) The task is to learn.\" (...) The task is to learn. \"(...) The task is to learn.\" (...) The task is to learn. \"(...) The task is to learn.\" (...) The task is to learn. \"(...) The task is to learn.\" (...) The task is to learn. \"(...) The task is to learn.\" (...) The task is to learn. \"(...) The task is to learn.\" (...) (...) (...) The task is to learn. \"(...). (...) The task is to learn.\""}, {"heading": "C. Stability of the Learned Controller", "text": "Since the control saturation is present, the target dynamics are not sufficient (5). Thus, to infer the local stability of x on the basis of (6), we calculate the sign of the difference (Fig. 5b) for the unsaturated case (Fig. 5a) 3 and the saturated case (Fig. 5b) and calculate the sign of the difference (Fig. 5b). If we look at Fig. 5b, we can see that such a case exists, i.e. a small grey region around the equilibrium x. Therefore, we can use the definition of x Hd (x, Hd) and judge the stability on the basis of (6)."}, {"heading": "D. Real-time Experiments", "text": "The result is shown in Figure 6. the algorithm shows a slightly slower convergence - approximately 3 minutes of learning time (60 attempts) to achieve an approximately optimal policy instead of 40 - and a3Figure 5a stands in contrast to Figure 4b, which is logical because the negative (positive) regions of K-Hd (x,) correspond to a negative (positive) damping that corresponds to a positive (negative) value of Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd-Hd"}, {"heading": "VII. CONCLUSIONS", "text": "In this paper, we have presented a methodology for systematically parameterizing the EB-PBC regulatory laws, which is robust to additional non-linearities such as the saturation of the input control, and the parameters are then found through the use of amplification techniques between actors and critics. In this way, we are able to learn a closed-loop energy landscape for PH systems. The advantage is that optimal regulators can be generated using energy-based control techniques, there is no need to specify a global Hamiltonian system, and the solutions learned through amplification learning can be interpreted in terms of energy formation and attenuation, which makes it possible to numerically assess stability by means of passivity theory. By using model knowledge, the actor-critic method is able to quickly learn near-optimal strategies. A disadvantage is that for multiple input systems, many actuator updates can be generated for the desired attenuation matrix."}], "references": [{"title": "Adaptive motion control of rigid robots: a tutorial", "author": ["R. Ortega", "M.W. Spong"], "venue": "Automatica, vol. 25, pp. 877\u2013888, 1989.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Control by interconnection and standard passivity-based control of port-Hamiltonian systems", "author": ["R. Ortega", "A. van der Schaft", "F. Castanos", "A. Astolfi"], "venue": "IEEE Transactions on Automatic Control, vol. 53, no. 11, pp. 2527\u20132542, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Putting energy back in control", "author": ["R. Ortega", "A. van der Schaft", "I. Mareels", "B. Maschke"], "venue": "IEEE Control Systems Magazine, vol. 21, no. 2, pp. 18\u201333, 2001.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2001}, {"title": "Modeling and Control for Efficient Bipedal Walking Robots: A Port-Based", "author": ["V. Duindam", "S. Stramigioli"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Control of Interactive Robotic Interfaces: a port-Hamiltonian approach", "author": ["C. Secchi", "S. Stramigioli", "C. Fantuzzi"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Reinforcement Learning: An Introduction", "author": ["R. Sutton", "A. Barto"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "On actor-critic algorithms", "author": ["V. Konda", "J. Tsitsiklis"], "venue": "SIAM Journal on Control and Optimization, vol. 42, no. 4, pp. 1143\u20131166, 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient model learning methods for actor-critic control", "author": ["I. Grondman", "M. Vaandrager", "L. Bu\u015foniu", "R. Babu\u0161ka", "E. Schuitema"], "venue": "IEEE Transactions on Systems, Man and Cybernetics, Part B: Cybernetics, In press.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 0}, {"title": "Stabilization and H\u221e control of nonlinear portcontrolled Hamiltonian systems subject to actuator saturation", "author": ["Y.W.A. Wei"], "venue": "Automatica, vol. 46, no. 12, pp. 2008\u20132013, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "A family of smooth controllers for swinging up a pendulum", "author": ["K. \u00c5strom", "J. Aracil", "F. Gordillo"], "venue": "Automatica, vol. 44, pp. 1841\u20131848, 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1841}, {"title": "Output-feedback global stabilization of a nonlinear benchmark system using a saturated passivity-based controller", "author": ["G. Escobar", "R. Ortega", "H. Sira-Ramirez"], "venue": "IEEE Transactions on Control Systems Technology, vol. 7, no. 2, pp. 289\u2013293, 1999.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1999}, {"title": "Iterative learning control of Hamiltonian systems: I/O based optimal control approach", "author": ["K. Fujimoto", "T. Sugie"], "venue": "IEEE Transactions on Automatic Control, vol. 48, no. 10, pp. 1756\u20131761, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Port Hamiltonian systems: A unified approach for modeling and control finite and infinite dimensional physical systems", "author": ["A. Macchelli"], "venue": "Ph.D. dissertation, University of Bologna, 2002.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "A variable structure approach to energy shaping", "author": ["A. Macchelli", "C. Melchiorri", "C. Secchi", "C. Fantuzzi"], "venue": "Proceedings of the European Control Conference, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Global asymptotic and finite-gain L2 stabilization of port-controlled Hamiltonian systems subject to actuator saturation", "author": ["W. Sun", "Z. Lin", "Y. Wang"], "venue": "Proceedings of the American Control Conference, St. Louis, 2009.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Iterative feedback tuning: an overview", "author": ["H. Hjalmarsson"], "venue": "International Journal Adaptive Control and Signal Processing, vol. 16, pp. 373\u2013395, 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Iterative feedback tuning for Hamiltonian systems", "author": ["K. Fujimoto", "I. Koyama"], "venue": "Proceedings of the 17th IFAC World Congress, Seoul, Korea, 2008, pp. 15 678\u201315 683.  XXXXXXXXX, VOL. XX, NO. XX, XXXXX 2013  10", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Port-controlled Hamiltonian systems: modelling origins and system theoretic properties", "author": ["B. Maschke", "A. van der Schaft"], "venue": "Proceedings of the third Conference on nonlinear control systems (NOLCOS), 1992, pp. 282\u2013288.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1992}, {"title": "On the Hamiltonian formulation of nonholonomic mechanical systems", "author": ["A. van der Schaft", "B. Maschke"], "venue": "Reports on Mathematical Physics, vol. 34, pp. 225\u2013233, 1994.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1994}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A. Barto", "R. Sutton", "C. Anderson"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, vol. 13, pp. 835\u2013846, 1983.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1983}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["R. Sutton", "D. McAllester", "S. Singh", "Y. Mansour"], "venue": "Advances in Neural Information Processing Systems, vol. 12, pp. 1057\u2013 1063, 2000.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "A Bayesian sampling approach to exploration in reinforcement learning", "author": ["J. Asmuth", "L. Li", "M. Littman", "A. Nouri", "D. Wingate"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2009, pp. 19\u201326.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Casimir-Based Control Beyond the Dissipation Obstacle", "author": ["J. Koopman", "D. Jeltsema"], "venue": "arXiv.org, Jun. 2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Value function approximation in reinforcement learning using the Fourier basis", "author": ["G. Konidaris", "S. Osentoski"], "venue": "Autonomous Learning Laboratory, Computer Science Department, University of Massachusetts Amherst, Tech. Rep. 101, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "PASSIVITY-based control (PBC) [1] is a methodology that achieves the control objective by rendering a system passive with respect to a desired storage function [2].", "startOffset": 30, "endOffset": 33}, {"referenceID": 1, "context": "PASSIVITY-based control (PBC) [1] is a methodology that achieves the control objective by rendering a system passive with respect to a desired storage function [2].", "startOffset": 160, "endOffset": 163}, {"referenceID": 1, "context": "Different forms of PBC have been successfully applied to design robust controllers [3] for mechanical systems and electrical circuits [2], [4].", "startOffset": 134, "endOffset": 137}, {"referenceID": 2, "context": "Different forms of PBC have been successfully applied to design robust controllers [3] for mechanical systems and electrical circuits [2], [4].", "startOffset": 139, "endOffset": 142}, {"referenceID": 3, "context": "PH systems have been widely used in PBC applications [5], [6].", "startOffset": 53, "endOffset": 56}, {"referenceID": 4, "context": "PH systems have been widely used in PBC applications [5], [6].", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "Much research in the literature concerns solving or simplifying such generally complex PDE\u2019s [2].", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "One such example is reinforcement learning (RL) [7].", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "The goal is to find an optimal control policy that maximizes the cumulative long-term rewards, which corresponds to maximizing a value function [7].", "startOffset": 144, "endOffset": 147}, {"referenceID": 6, "context": "In this paper, we use actor-critic techniques [8], which are a class of RL methods in which a separate actor and critic are learned.", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "However, by incorporating (partial) model knowledge, learning can be sped up [9].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "The simplest example to illustrate this idea is by considering a reward function to be 1 when the system is in a small neighborhood of the desired goal and 0 everywhere else [7].", "startOffset": 174, "endOffset": 177}, {"referenceID": 8, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 101, "endOffset": 105}, {"referenceID": 10, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 107, "endOffset": 111}, {"referenceID": 11, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 113, "endOffset": 117}, {"referenceID": 12, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 119, "endOffset": 123}, {"referenceID": 13, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "Control input saturation in PBC for PH systems has been addressed explicitly in the literature [10], [11], [12], [13], [14], [15], [16].", "startOffset": 131, "endOffset": 135}, {"referenceID": 15, "context": "The work presented in this paper draws an interesting parallel with the application of iterative feedback tuning (IFT) [17] in the PH framework [18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 16, "context": "The work presented in this paper draws an interesting parallel with the application of iterative feedback tuning (IFT) [17] in the PH framework [18].", "startOffset": 144, "endOffset": 148}, {"referenceID": 2, "context": "Port-Hamiltonian (PH) systems are a natural way of representing a physical system in terms of its energy exchange with the environment through ports [4].", "startOffset": 149, "endOffset": 152}, {"referenceID": 17, "context": "The general framework of PH systems was introduced in [19] and was formalized in [20], [3].", "startOffset": 54, "endOffset": 58}, {"referenceID": 18, "context": "The general framework of PH systems was introduced in [19] and was formalized in [20], [3].", "startOffset": 81, "endOffset": 85}, {"referenceID": 1, "context": "System (1) satisfies the power-balance equation [2]:", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "which is called the passivity inequality, if H(x) is positive semi-definite, and cyclo-passivity inequality, if H(x) is not positive semi-definite nor bounded from below [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 1, "context": "through energy shaping using EB-PBC [2] and damping injection, such that Hd(x) is the desired closed-loop energy which has a minimum at the desired equilibrium x\u2217 and satisfies: \u1e22d(x) = \u2212 (\u2207xHd(x)) Rd(x)\u2207xHd(x) (6)", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "with g\u2020(x) = (g (x)g(x))\u22121gT (x) solves the EB-PBC problem with Ha(x) a solution of the following set of PDE\u2019s [2]: [ g\u22a5(x)FT (x) g (x) ] \u2207xHa(x) = 0 (10)", "startOffset": 111, "endOffset": 114}, {"referenceID": 19, "context": "Actor-critic (AC) algorithms [21], [8] learn a separate actor (policy \u03c0) and critic (value function V ).", "startOffset": 29, "endOffset": 33}, {"referenceID": 6, "context": "Actor-critic (AC) algorithms [21], [8] learn a separate actor (policy \u03c0) and critic (value function V ).", "startOffset": 35, "endOffset": 38}, {"referenceID": 20, "context": "This is beneficial when dealing with continuous action spaces [22].", "startOffset": 62, "endOffset": 66}, {"referenceID": 7, "context": "In this paper, the temporaldifference based Standard Actor-Critic (S-AC) algorithm from [9] is used.", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "The temporal difference [7]:", "startOffset": 24, "endOffset": 27}, {"referenceID": 5, "context": "Eligibility traces ek \u2208 R [7] can be used to speed up learning by including reward information about previously visited states.", "startOffset": 26, "endOffset": 29}, {"referenceID": 21, "context": "[23]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "This idea has been used in [24] to overcame the dissipation obstacle when synthesizing controllers by interconnection.", "startOffset": 27, "endOffset": 31}, {"referenceID": 5, "context": "In the RL community it is generally accepted that during learning no stability and convergence guarantees can be given [7], as exploration is a necessary component of the framework.", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "The last point to consider is that since the Energy Balancing PBC suffers from the dissipation obstacle [2], limiting its applicability to special classes of systems such as mechanical systems, the algorithm we present contains the same limitation.", "startOffset": 104, "endOffset": 107}, {"referenceID": 22, "context": "Eliminating such limitation is ongoing work (see also the results presented in [24])", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "The pendulum swing-up is a low-dimensional, but highly nonlinear control problem commonly used as a benchmark in the RL literature [9] and it has also been studied in PBC [11].", "startOffset": 131, "endOffset": 134}, {"referenceID": 9, "context": "The pendulum swing-up is a low-dimensional, but highly nonlinear control problem commonly used as a benchmark in the RL literature [9] and it has also been studied in PBC [11].", "startOffset": 171, "endOffset": 175}, {"referenceID": 23, "context": "In this paper we use the Fourier basis [25] because of its ease of use, the possibility to incorporate information about the symmetry in the system and the ability to ascertain properties useful for stability analysis of this specific problem.", "startOffset": 39, "endOffset": 43}, {"referenceID": 0, "context": "c1 = [0 0] T , c2 = [1 0] T , .", "startOffset": 20, "endOffset": 25}, {"referenceID": 2, "context": ", c(3+1)2 = [4 4] T (51)", "startOffset": 12, "endOffset": 17}, {"referenceID": 2, "context": ", c(3+1)2 = [4 4] T (51)", "startOffset": 12, "endOffset": 17}, {"referenceID": 23, "context": "We adopt the adjusted learning rate from [25] such that:", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "[9].", "startOffset": 0, "endOffset": 3}], "year": 2013, "abstractText": "Passivity-based control (PBC) for port-Hamiltonian systems provides an intuitive way of achieving stabilization by rendering a system passive with respect to a desired storage function. However, in most instances the control law is obtained without any performance considerations and it has to be calculated by solving a complex partial differential equation (PDE). In order to address these issues we introduce a reinforcement learning approach into the energy-balancing passivity-based control (EBPBC) method, which is a form of PBC in which the closed-loop energy is equal to the difference between the stored and supplied energies. We propose a technique to parameterize EB-PBC that preserves the systems\u2019s PDE matching conditions, does not require the specification of a global desired Hamiltonian, includes performance criteria, and is robust to extra non-linearities such as control input saturation. The parameters of the control law are found using actor-critic reinforcement learning, enabling learning near-optimal control policies satisfying a desired closedloop energy landscape. The advantages are that near-optimal controllers can be generated using standard energy shaping techniques and that the solutions learned can be interpreted in terms of energy shaping and damping injection, which makes it possible to numerically assess stability using passivity theory. From the reinforcement learning perspective, our proposal allows for the class of port-Hamiltonian systems to be incorporated in the actor-critic framework, speeding up the learning thanks to the resulting parameterization of the policy. The method has been successfully applied to the pendulum swing-up problem in simulations and real-life experiments.", "creator": "LaTeX with hyperref package"}}}