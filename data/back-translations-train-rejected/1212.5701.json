{"id": "1212.5701", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Dec-2012", "title": "ADADELTA: An Adaptive Learning Rate Method", "abstract": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.", "histories": [["v1", "Sat, 22 Dec 2012 15:46:49 GMT  (266kb,D)", "http://arxiv.org/abs/1212.5701v1", "6 pages"]], "COMMENTS": "6 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["matthew d zeiler"], "accepted": false, "id": "1212.5701"}, "pdf": {"name": "1212.5701.pdf", "metadata": {"source": "CRF", "title": "ADADELTA: AN ADAPTIVE LEARNING RATE METHOD", "authors": ["Matthew D. Zeiler"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "2.1. Learning Rate Annealing", "text": "These approaches either attempt to accelerate learning when appropriate, or to slow learning near local minimas.ar Xiv: 121 2,57 01v1 [cs.LG] 2 2D ec2 012When the drop in the gradient approaches a minimum in the cost surface, parameter values can fluctuate back and forth around the minima. One way to prevent this is to slow down parameter updates by reducing the learning rate. This can be done manually when validation accuracy seems to be approaching the plateau. Alternatively, learning rate plans have been proposed [1] to automatically nullify the learning rate based on the number of epochs through the data. These approaches typically add additional hyperparameters to control how fast the learning rate declines."}, {"heading": "2.2. Per-Dimension First Order Methods", "text": "The heuristic annealing method discussed above modifies a single global learning rate that applies to all dimensions of the parameters. Since each dimension of the parameter vector can relate to the total cost in completely different ways, a pro-dimensional learning rate that can compensate for these differences is often beneficial."}, {"heading": "2.2.1. Momentum", "text": "One method of accelerating training per dimension is the Dynamics Method [2], which is perhaps the simplest extension of the SGD that has been successfully applied for decades. The basic idea behind the dynamics is to accelerate progress in dimensions where the gradient consistently points in the same direction, and to slow progress in dimensions where the sign of the gradient continues to change, by keeping track of past parameter updates with exponential decay: the gradients along the valley, although much smaller than the gradients across the valley, are typically in the same direction, and therefore the dynamics concept gathers to accelerate progress. In the SGD, progress along the valley would be slow because the gradient size is small and the global learning rate shared by all dimensions goes in the same direction."}, {"heading": "2.2.2. ADAGRAD", "text": "A current first-order method called ADAGRAD [3] has shown remarkably good results on large-scale learning tasks in a distributed environment [4]. This method is based only on first-order information, but has some properties of second-order methods and annealing. The update rule for ADAGRAD is as follows: \u2206 xt = \u2212 \u03b7 \u221a t \u03c4 = 1 g 2 \u03c4 gt (5) Here, the denominator calculates the \"2 norm of all previous gradients on a pro-dimensional basis and is a global learning rate shared by all dimensions. While there is a hand-tuned global learning rate, each dimension has its own dynamic rate. As this dynamic rate grows with the inversion of the gradient magnitudes, large gradients have smaller learning rates and small gradients have high learning rates. This has the nice property that progress is balanced along each dimension."}, {"heading": "2.3. Methods Using Second Order Information", "text": "While the above methods only use gradients and function evaluations to optimize the target, the second order methods such as Newton's method or quasi-Newton's methods use the Hessian matrix or approximations to it. While this provides additional curvature information for optimization, the calculation of accurate second order is often complex. As the calculation of the entire Hessian matrix of the second derivatives is too computationally expensive for large models, Becker and LecCun [5] proposed a diagonal approximation to the Hessian. This diagonal approximation can be calculated with an additional forward and backward trajectory through the model, effectively doubling the calculation via the SGD. Once the diagonal of the Hessian is calculated, diag (H) becomes the updating rule: \"xt\" D \"(Ht)."}, {"heading": "3.1. Idea 1: Accumulate Over Window", "text": "Instead of accumulating the sum of the square gradients over time, we limited the window of past gradients accumulated to a fixed quantity w (instead of the quantity t, where t is the current iteration as in ADAGRAD).With this window-shaped accumulation, the denominator of ADAGRAD cannot be accumulated to infinity, and instead becomes a local estimate using current gradients. This ensures that learning continues even after many updates. As the storage of the previous square gradients is inefficient, we implement this accumulation as an exponentially declining average of the square gradients. Suppose that this current average is E [g2] t, then we calculate: E [g2] t = square E [g 2] t \u2212 1 + (1 \u2212 g) t: [MS + (1 \u2212 b) g2t (8), with constraints similar to those in the gradient."}, {"heading": "3.2. Idea 2: Correct Units with Hessian Approximation", "text": "When updating the parameters, we will add this discrepancy of units to the units we take into account, that is, if the parameters have any hypothetical units, the changes to the parameters will also be changed in these units. If we look at the SGD, Momentum, or ADAGRAD, we can see that this is not the case. Units in SGD and Momentum refer to the gradients, not to the parameters of the gradient quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities quantities."}, {"heading": "4.1. Handwritten Digit Classification", "text": "In our first set of experiments, we train a neural network on the MNIST handwritten number classification task. For comparison with Schaul et al. \"s method, we trained with tanh nonlinearity and 500 hidden units in the first layer followed by 300 hidden units in the second layer, with the final Softmax output layer on top. Our method was trained on mini-stacks of 100 frames per stack for 6 epochs through the training set. If we set the hyperparameters to = 1e \u2212 6 and \u03c1 = 0.95, we achieve a test rate of 2.00% compared to the 2.10% of Schaul et al. Although this is not nearly convergence, there is a sense of how quickly the algorithms can objectively optimize the classification. To further analyze various methods of convergence, we train the same neural network with 500 hidden units in the first layer, 300 hidden units in the second layer and reactive activation radio in both layers."}, {"heading": "4.2. Sensitivity to Hyperparameters", "text": "While the dynamics converged after many training periods to a better final solution than ADADADELTA, they were very sensitive to the selection of the learning rate, as were SGD and ADAGRAD. In Table 1, we vary the learning rates for each method and show the errors of the test sets after 6 training periods using corrected linear units as an activation function. The optimal settings from each column were used to illustrate Fig. 1. For SGD, Momentum or ADAGRAD, the learning rate must be set to the correct order of magnitude above which the solutions typically diverge and below which the optimization proceeds slowly. We can see that these results are highly variable for each method compared to ADADELTA in Table 2, where the two hyperparameters do not significantly alter performance."}, {"heading": "4.3. Effective Learning Rates", "text": "To examine some of the characteristics of ADELTA, in Fig. 2 we plot the step sizes and parameter updates of 10 randomly selected dimensions in each of the three weight matrices throughout the training. There are several interesting things that can be seen in this figure. Firstly, the step sizes or effective learning rates (all terms except Eqn. 14) shown in the left part of the figure are larger for the lower layers of the network and much smaller for the top layer at the beginning of the training. This feature of ADADELTA helps compensate for the fact that lower layers should have smaller gradients due to the decreasing gradient problem in neural networks and therefore have higher learning rates. Secondly, near the end of the training, these step sizes converge to 1. This is typically a high learning rate that would lead to divergence in most methods, but this convergence in the direction of 1 only occurs towards the end of the training, when the gradients are small."}, {"heading": "4.4. Speech Data", "text": "In the next series of experiments, we trained a large-scale neural network with 4 hidden layers on several hundred hours of U.S. English data collected using Voice Search, Voice IME, and read data. [4] The network was trained using either 100 or 200 such replica networks to test the performance of ADADADELTA in a highly distributed environment; the neural network is constructed so that the inputs are 26 frames of audio, each consisting of 40 log energy filter outputs; the outputs of the network were 8,000 senon labels produced by a GMM-HMM system with forced alignment to the input frame; each hidden layer of the neural network had 2560 hidden units and was formed using either logistic or fictitious methods."}], "references": [{"title": "A stochastic approximation method", "author": ["H. Robinds", "S. Monro"], "venue": "Annals of Mathematical Statistics, vol. 22, pp. 400\u2013407, 1951.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1951}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, vol. 323, pp. 533\u2013536, 1986.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1986}, {"title": "Adaptive subgradient methods for online leaning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "COLT, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q. Le", "M. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A. Ng"], "venue": "NIPS, 2012.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Improving the convergence of back-propagation learning with second order methods", "author": ["S. Becker", "Y. LeCun"], "venue": "Tech. Rep., Department of Computer Science, University of Toronto, Toronto, ON, Canada, 1988.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1988}, {"title": "No more pesky learning rates", "author": ["T. Schaul", "S. Zhang", "Y. LeCun"], "venue": "arXiv:1206.1106, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Application of pretrained deep neural networks to large vocabulary speech recognition", "author": ["N. Jaitly", "P. Nguyen", "A. Senior", "V. Vanhoucke"], "venue": "Interspeech, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Following this negative gradient for each new sample or batch of samples chosen from the dataset gives a local estimate of which direction minimizes the cost and is referred to as stochastic gradient descent (SGD) [1].", "startOffset": 214, "endOffset": 217}, {"referenceID": 0, "context": "Alternatively, learning rate schedules have been proposed [1] to automatically anneal the learning rate based on how many epochs through the data have been done.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "One method of speeding up training per-dimension is the momentum method [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "A recent first order method called ADAGRAD [3] has shown remarkably good results on large scale learning tasks in a distributed environment [4].", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "A recent first order method called ADAGRAD [3] has shown remarkably good results on large scale learning tasks in a distributed environment [4].", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "Since computing the entire Hessian matrix of second derivatives is too computationally expensive for large models, Becker and LecCun [5] proposed a diagonal approximation to the Hessian.", "startOffset": 133, "endOffset": 136}, {"referenceID": 5, "context": "[6] incorporating the diagonal Hessian with ADAGRAD-like terms has been introduced to alleviate the need for hand specified learning rates.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "also introduce a heuristic for this window size w (see [6] for more details).", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "The idea presented in this paper was derived from ADAGRAD [3] in order to improve upon the two main drawbacks of the method: 1) the continual decay of learning rates throughout training, and 2) the need for a manually selected global learning rate.", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "[6], which will be compared to below.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "where a constant is added to better condition the denominator as in [5].", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "This approximation is always positive as in Becker and LeCun [5], ensuring the update direction follows the negative gradient at each step.", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "The network was trained using the distributed system of [4] in which a centralized parameter server accumulates the gradient information reported back from several replicas of the neural network.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "The neural network is setup as in [7] where the inputs are 26 frames of audio, each consisting of 40 log-energy filter bank outputs.", "startOffset": 34, "endOffset": 37}], "year": 2012, "abstractText": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.", "creator": "LaTeX with hyperref package"}}}