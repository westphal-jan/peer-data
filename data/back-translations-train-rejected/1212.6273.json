{"id": "1212.6273", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Dec-2012", "title": "Human-Recognizable Robotic Gestures", "abstract": "For robots to be accommodated in human spaces and in humans daily activities, robots should be able to understand messages from the human conversation partner. In the same light, humans must also understand the messages that are being communicated by robots, including the non-verbal ones. We conducted a web-based video study wherein participants gave interpretations on the iconic gestures and emblems that were produced by an anthropomorphic robot. Out of the 15 gestures presented, we found 6 robotic gestures that can be accurately recognized by the human observer. These were nodding, clapping, hugging, expressing anger, walking, and flying. We reviewed these gestures for their meaning from literatures in human and animal behavior. We conclude by discussing the possible implications of these gestures for the design of social robots that are aimed to have engaging interactions with humans.", "histories": [["v1", "Wed, 26 Dec 2012 22:10:14 GMT  (5293kb)", "http://arxiv.org/abs/1212.6273v1", "21 pages, 5 figures"]], "COMMENTS": "21 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.HC", "authors": ["john-john cabibihan", "wing-chee so", "soumo pramanik"], "accepted": false, "id": "1212.6273"}, "pdf": {"name": "1212.6273.pdf", "metadata": {"source": "CRF", "title": "Human-Recognizable Robotic Gestures", "authors": ["John-John Cabibihan", "Wing-Chee So", "Soumo Pramanik"], "emails": ["(elecjj@nus.edu.sg)"], "sections": [{"heading": null, "text": "In fact, it is so that most people are able to understand themselves, both in relation to themselves and in relation to themselves. (...) We have it in our hand to understand the world, as well as in relation to ourselves. (...) We have it in our hand to change the world. (...) We have it in our hand to change the world. (...) We have it in our hand to change the world. (...) \"We have it in our hand to change the world.\" (...) \"We have it in our hand to change the world.\" (...) \"(...)\" We have not succeeded in changing the world. \"(...)\" We have succeeded in changing the world. \"(...)\" (... \"We have succeeded in changing the world.\" (...) \"(...)\" (...) \"We have it.\" (...) \"We have done.\" (...). \"We have.\" We have done. \"(...\" We have done. \"(...)\" We have done. \"We have done.\" (... \"We have done.\" We have. \"We have.\" We have. \"We have.\" (...) We have done. \"We have.\" We have. \"We have.\" (... \"We have.\" We have. \"We have.\" We have. \"We have.\" We have. \"We have.\" We have. \"We have.\""}, {"heading": "V. RESULTS", "text": "In fact, most people who are able to help themselves, to help themselves, find themselves in a difficult situation in which they are no longer able to live. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) () (...) (...) () (...) ()"}, {"heading": "Nodding", "text": "In human-human interaction, head movements can indicate agreement or affirmation, disapproval or denial, and many other semantic messages [37, 53-56]. It has been observed that the nodding of the listener during a conversation encouraged utterances by the speaker that led to a lively conversation between the speaker and the listener [57]. Recognition of the nod may have been simple, as it is a primitive form of communication observed in the interaction between mother and child [58, 59]. When applied to a sociable robot penguin, it was discovered that the human interaction partner nodded more often when the robot intentionally responded to the PLEASE CITE IN PRESS AS: John-John Cabibihan, Wing Chee So, \"Human-Recognizable Robotic Gestures,\" \"Autonomous Mental Development, IEEE Transactions, 2012, 4 (4), 305-314, Gestures, Human Recognition Practices.\""}, {"heading": "Clapping", "text": "People clap their hands as a sign of appreciation or approval. Normally, an audience expresses appreciation for a good performance by, among other things, the strength and length of their applause [62]. Clapping is one of the human activities studied in the field of video streaming analysis for sports, music and human-machine interaction [63-67]. Researchers have found that the monkeys clap in captivity to attract human attention [68-70]; for monkeys in the wild, monkeys use clapping as a form of long-distance communication to maintain cohesion in the group in alarm situations [71, 72]. From these representations of human and animal behavior, clapping appears to be a useful act for human-robot interaction. Hanahara and Tada [73] proposed a new communication language based on clapping so that humans can better communicate with robots. They described how the sound of clapping can be used to facilitate human-robot communication, and expanded the coding of communication with a robot in this way."}, {"heading": "Hugging", "text": "Among non-verbal behaviors, social touch (e.g. hugging, petting, stroking, shaking hands) appears to be an important modality [76]. Hugging or hugging is a form of affective touch in which children clasp their arms or cling to another person (e.g. [79]). Recent medical evidence shows that hugging can have health benefits to lower blood pressure and increase oxytocin levels. Oxytocin is the hormone involved in social bonding, building trust, and increasing generosity in humans [80, 81]. Hugging has been recognized as an important component of parent-child interaction."}, {"heading": "Expressing Anger", "text": "The majority of participants expressed this movement as a gesture to express anger. Anger is an emotional response when situations or circumstances are unfair or unjust, personality rights are not expected or realistic expectations are not fulfilled. [88] Although anger is considered a normal human emotion, the fields of robotics and virtual reality have been very active in trying to replicate this emotion and other emotional behaviors on robotic faces and body movements. [90] Programming emotions in robots has been identified as an important step in creating lifelike social robots to improve human-robot interaction [89]. For example, the Waseda Eye No.4 Refined II (WE4-RII) is a humanoid robot that displays human emotions [90]. Its robotic face has a total of 26 DOFs that can control the faceless action. (cf] Comparing the higher intensity of human eyes and eyes of the authors means that the disgruntled faces of the participants were bent outwards. [This gesture is also referred to as an acimbo]."}, {"heading": "Walking and Flying", "text": "The fact is that we will be able to find a solution that will enable us to find a solution, that will enable us to find a solution that will enable us to find a solution, that will enable us to find a solution, that will enable us to find a solution that will enable us to find a solution, that will enable us to find a solution that will enable us to find a solution, that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that we are able to find a solution. \""}, {"heading": "VII. CONCLUSION", "text": "Communication is a one-way street. In other words, robots should understand not only the messages of the human interlocutor, but also the messages that are communicated by robots, including the non-verbal ones. In this study, we found six robot gestures that can be accurately recognized by the human observer. Nodding and clapping are gestures that are common for recognition or approval; hugging and angry gestures are gestures that express emotions while walking and flying, action gestures that can be used to tell stories or teach languages. The development cycle time for robot programming and testing can be shortened if roboticists know at the outset what basic robot gestures humans can understand. When these gestures are programmed into robots, they can lead to human-robot interactions that are natural, appropriate and engaged."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank Wendy Yusson for preparing the robot videos and for the pilot experiments."}], "references": [{"title": "A survey of socially interactive robots", "author": ["T. Fong", "I. Nourbakhsh", "K. Dautenhahn"], "venue": "Robotics and Autonomous Systems, vol. 42, pp. 143-166, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Toward sociable robots", "author": ["C. Breazeal"], "venue": "Robotics and Autonomous Systems, vol. 42, pp. 167-175, 2003.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Socialization between toddlers and robots at an early childhood education center", "author": ["F. Tanaka", "A. Cicourel", "J.R. Movellan"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 104, pp. 17954-17958, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Social and collaborative aspects of interaction with a service robot", "author": ["K. Severinson-Eklundh", "A. Green", "H. H\u00fcttenrauch"], "venue": "Robotics and Autonomous Systems, vol. 42, pp. 223-234, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "A social robot that stands in line", "author": ["Y. Nakauchi", "R. Simmons"], "venue": "Autonomous Robots, vol. 12, pp. 313-324, 2002.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "A biologically inspired architecture for an autonomous and social robot", "author": ["M. Malfaz", "A. Castro-Gonz\u00e1lez", "R. Barber", "M.A. Salichs"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 3, pp. 232-246, 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Robot therapy: A new approach for mental healthcare of the elderly - A mini-review", "author": ["T. Shibata", "K. Wada"], "venue": "Gerontology, vol. 57, pp. 378-386, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Socially intelligent robots: Dimensions of human-robot interaction", "author": ["K. Dautenhahn"], "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences, vol. 362, pp. 679-704, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Children-robot interaction: a pilot study in autism therapy", "author": ["H. Kozima", "C. Nakagawa", "Y. Yasuda"], "venue": "Progress in Brain Research. vol. 164, 2007, pp. 385-400.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Social robotics: Integrating advances in engineering and computer science (keynote speech)", "author": ["S.S. Ge"], "venue": "Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI) Mae Fah Luang University, Chang Rai, Thailand, 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards an effective design of social robots ", "author": ["H. Li", "J.J. Cabibihan", "Y.K. Tan"], "venue": "International Journal of Social Robotics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Human-Inspired Robots", "author": ["S. Coradeschi", "H. Ishiguro", "M. Asada", "S.C. Shapiro", "M. Thielscher", "C. Breazeal", "M.J. Mataric", "H. Ishida"], "venue": "Intelligent Systems, IEEE, vol. 21, pp. 74-85, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Android science: Conscious and subconscious recognition", "author": ["H. Ishiguro"], "venue": "Connection Science, vol. 18, pp. 319-332, 2006.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Integration of action and language knowledge: A roadmap for developmental robotics", "author": ["A. Cangelosi", "G. Metta", "G. Sagerer", "S. Nolfi", "C. Nehaniv", "K. Fischer", "J. Tani", "T. Belpaeme", "G. Sandini", "F. Nori", "L. Fadiga", "B. Wrede", "K. Rohlfing", "E. Tuci", "K. Dautenhahn", "J. Saunders", "A. Zeschel"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 2, pp. 167-195, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "The intelligent ASIMO: System overview and integration", "author": ["Y. Sakagami", "R. Watanabe", "C. Aoyama", "S. Matsunaga", "N. Higaki", "K. Fujimura"], "venue": "IEEE International Conference on Intelligent Robots and Systems, 2002, pp. 2478-2483.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "Natural interface using pointing behavior for human-robot gestural interaction", "author": ["E. Sato", "T. Yamaguchi", "F. Harashima"], "venue": "IEEE Transactions on Industrial Electronics, vol. 54, pp. 1105-1112, 2007.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Explorations in engagement for humans and robots", "author": ["C.L. Sidner", "C. Lee", "C.D. Kidd", "N. Lesh", "C. Rich"], "venue": "Artificial Intelligence, vol. 166, pp. 140-164, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Gesture based interface for human-robot interaction", "author": ["S. Waldherr", "R. Romero", "S. Thrun"], "venue": "Autonomous Robots, vol. 9, pp. 151-173, 2000.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Using the rhythm of nonverbal human-robot interaction as a signal for learning", "author": ["P. Andry", "A. Blanchard", "P. Gaussier"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 3, pp. 30-42, 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Cognitive development in partner robots for information support to elderly people", "author": ["A. Yorita", "N. Kubota"], "venue": "IEEE Transactions on Autonomous Mental Development, vol. 3, pp. 64-73, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Human-Recognizable Robotic Gestures", "author": ["C. Breazeal", "B. Scassellati", "\"Robots that imitate humans", "Trends in Cognitive Sciences", "vol. 6", "pp. 481-487", "2002. 15 PLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan", "Wing Chee So", "Soumo Pramanik"], "venue": "Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards gesture-based programming: Shape from motion primordial learning of sensorimotor primitives", "author": ["R.M. Voyles", "J.D. Morrow", "P.K. Khosla"], "venue": "Robotics and Autonomous Systems, vol. 22, pp. 361-375, 1997.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1997}, {"title": "Teaching and learning of robot tasks via observation of human performance", "author": ["R. Dillmann"], "venue": "Robotics and Autonomous Systems. vol. 47, 2004, pp. 109-116.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2004}, {"title": "Mimetic communication model with compliant physical contact in human-humanoid interaction", "author": ["D. Lee", "C. Ott", "Y. Nakamura"], "venue": "International Journal of Robotics Research, vol. 29, pp. 1684-1704, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Dynamical system modulation for robot learning via kinesthetic demonstrations", "author": ["M. Hersch", "F. Guenter", "S. Calinon", "A. Billard"], "venue": "IEEE Transactions on Robotics, vol. 24, pp. 1463-1467, 2008.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Incremental learning of gestures by imitation in a humanoid robot", "author": ["S. Calinon", "A. Billard"], "venue": "HRI 2007 - Proceedings of the 2007 ACM/IEEE Conference on Human-Robot Interaction - Robot as Team Member, 2007, pp. 255-262.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning and reproduction of gestures by imitation", "author": ["S. Calinon", "F. D'Halluin", "E.L. Sauser", "D.G. Caldwell", "A.G. Billard"], "venue": "IEEE Robotics and Automation Magazine. vol. 17, June 2010, pp. 44-54.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "A humanoid robot that pretends to listen to route guidance from a human", "author": ["T. Kanda", "M. Kamasima", "M. Imai", "T. Ono", "D. Sakamoto", "H. Ishiguro", "Y. Anzai"], "venue": "Autonomous Robots, vol. 22, pp. 87-100, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "EEG evidence for mirror neuron activity during the observation of human and robot actions: Toward an analysis of the human qualities of interactive robots", "author": ["L.M. Oberman", "J.P. McCleery", "V.S. Ramachandran", "J.A. Pineda"], "venue": "Neurocomputing, vol. 70, pp. 2194-2203, 2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Iconic Gestures of Children and Adults", "author": ["D. McNeill"], "venue": "Semiotica, vol. 62, pp. 107-128, 1986.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1986}, {"title": "Strategy signals in face-to-face interaction", "author": ["S. Duncan", "L. Brunner", "D. Fiske"], "venue": "Journal of Personality and Social Psychology. vol. 37, 1979, pp. 301-313.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1979}, {"title": "On signalling that it's your turn to speak", "author": ["S. Duncan", "G. Niederehe"], "venue": "Journal of Experimental Social Psychology. vol. 10, 1974, pp. 234-247.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1974}, {"title": "Some signals and rules for taking speaking turns in conversations", "author": ["S. Duncan"], "venue": "Journal of Personality and Social Psychology. vol. 23, 1972, pp. 283-292.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1972}, {"title": "Some functions of gaze-direction in social interaction", "author": ["A. Kendon"], "venue": "Acta psychologica. vol. 26, 1967, pp. 22-63.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1967}, {"title": "Ellesworth, Person Perception, 2nd ed", "author": ["D.J. Schneider", "A.H. Hastorf", "P. C"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1979}, {"title": "The nature of rapport and its nonverbal correlates", "author": ["L. Tickle-Degnen", "R. Rosenthal"], "venue": "Psychological Inquiry, vol. 1, pp. 285-293, 1990.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1990}, {"title": "Nonverbal Communication in Human Interaction, 4th ed", "author": ["M. Knapp", "J. Hall"], "venue": "Fort Worth, TX: Harcourt Brace College,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1997}, {"title": "Nonverbal Signals", "author": ["J.K. Burgoon"], "venue": "Handbook of Interpersonal Communication, 2nd ed, M. L. Knapp and G. R. Miller, Eds. Thousand Oaks, CA: Sage, 1994, pp. 229-285.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1994}, {"title": "Interpersonal perception in Japanese and British observers", "author": ["T. Kito", "B. Lee"], "venue": "Perception, vol. 33, pp. 957-974, 2004.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2004}, {"title": "Mind, Self and Society from the Standpoint of a Social Behaviorist", "author": ["G.H. Mead"], "venue": null, "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1934}, {"title": "Why do we gesture when we speak", "author": ["R.M. Krauss"], "venue": "Current Directions in Psychological Science, vol. 7, pp. 54-59, 1998.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1998}, {"title": "Gesture and speech: How they interact", "author": ["A. Kendon"], "venue": "Nonverbal interaction, J. M. Weimann and R. P. Harrison, Eds. Beverly Hills, CA: Sage, 1983.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1983}, {"title": "The communicative functions of hand illustrators", "author": ["A.A. Cohen"], "venue": "Journal of Communication, vol. 27, pp. 54-63, 1977.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1977}, {"title": "Human-Recognizable Robotic Gestures", "author": ["J.P. de Ruiter", "\"Can gesticulation help aphasic people speak", "or rather", "communicate?", "Advances Speech-Lang. Pathol.", "vol. 8", "pp. 124-127", "2006. 16 PLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan", "Wing Chee So", "Soumo Pramanik"], "venue": "Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Minimal Set of Recognizable Gestures for a 10 DOF Anthropomorphic Robot", "author": ["J.J. Cabibihan", "W. Yusson", "S. Salehi", "S.S. Ge"], "venue": "Social Robotics. vol. 6414, S. S. Ge, H. Li, J. J. Cabibihan, and Y. K. Tan, Eds.: Springer Berlin / Heidelberg, 2010, pp. 63-70.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}, {"title": "Evidence for distinct contributions of form and motion information to the recognition of emotions from body gestures", "author": ["A.P. Atkinson", "M.L. Tunstall", "W.H. Dittrich"], "venue": "Cognition, vol. 104, pp. 59-72, 2007.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2007}, {"title": "Perceiving affect from arm movement", "author": ["F.E. Pollick", "H.M. Paterson", "A. Bruderlin", "A.J. Sanford"], "venue": "Cognition, vol. 82, pp. B51-B61, 2001.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2001}, {"title": "Distributional analyses in auditory lexical decision: Neighborhood density and word-frequency effects", "author": ["W.D. Goh", "S. Lidia", "M.J. Yap", "S. Hui Tan"], "venue": "Psychonomic Bulletin and Review, vol. 16, pp. 882-887, 2009.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "Iconic gestures prime words", "author": ["D.F. Yap", "W.C. So", "J.M. Yap", "Y.Q. Tan", "R.L. Teoh"], "venue": "Cognitive Science, vol. 35, pp. 171-183, 2011.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "Gaze and task performance in shared virtual environments", "author": ["J.N. Bailenson", "A.C. Beall", "J. Blascovich"], "venue": "Journal of Visualization and Computer Animation, vol. 13, pp. 313-320, 2002/// 2002.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2002}, {"title": "Head gestures, gaze and the principles of conversational structure", "author": ["D. Heylen"], "venue": "International Journal of Humanoid Robotics, vol. 3, pp. 241-267, 2006.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2006}, {"title": "Some Uses of the Head Shake", "author": ["A. Kendon"], "venue": "Gesture, vol. 2, pp. 147-182, 2002.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2002}, {"title": "Linguistic functions of head movements in the context of speech", "author": ["E.Z. McClave"], "venue": "Journal of Pragmatics. vol. 32, 2000, pp. 855-878.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2000}, {"title": "Interviewer Head Nodding and Interviewee Speech Durations", "author": ["J.D. Matarazzo", "G. Saslow", "A.N. Wiens", "M. Weitman", "B.V. Allen"], "venue": "Psychotherapy: Theory, Research and Practice, vol. 1, pp. 54-63, 1964.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1964}, {"title": "Neonate movement is synchronized with adult speech: interactional participation and language acquisition", "author": ["W.S. Condon", "L.W. Sander"], "venue": "Science, vol. 183, pp. 99-101, 1974.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1974}, {"title": "Quantitative evaluation of infant behaviour and mother\u2013infant interaction", "author": ["N. Kobayashi", "T. Ishii", "T. Watanabe"], "venue": "Early Development and Parenting, vol. 1, pp. 23-31, 1992.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1992}, {"title": "The effect of head-nod recognition in human-robot conversation", "author": ["C.L. Sidner", "C. Lee", "L.P. Morency", "C. Forlines"], "venue": "HRI 2006: Proceedings of the 2006 ACM Conference on Human-Robot Interaction, 2006, pp. 290-296.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2006}, {"title": "When my robot smiles at me: Enabling human-robot rapport via real-time head gesture mimicry", "author": ["L.D. Riek", "P.C. Paul", "P. Robinson"], "venue": "Journal on Multimodal User Interfaces, vol. 3, pp. 99-108, 2010.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2010}, {"title": "The sound of many hands clapping", "author": ["Z. Neda", "E. Ravasz", "Y. Brechet", "T. Vicsek", "A.L. Barabasi"], "venue": "Nature, vol. 403, pp. 849-850, 2000.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2000}, {"title": "A novel eigenspace-based method for human action recognition", "author": ["A. Diaf", "R. Benlamri", "B. Boufama", "R. Ksantini"], "venue": "2010 5th International Conference on Digital Information Management, ICDIM 2010, 2010, pp. 182-187.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2010}, {"title": "Detection of goal event in soccer videos", "author": ["H.G. Kim", "S. Roeber", "A. Samour", "T. Sikora"], "venue": "Proceedings of SPIE - The International Society for Optical Engineering, 2005, pp. 317-325.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2005}, {"title": "Action recognition for surveillance applications using optic flow and SVM", "author": ["Part", "S. Danafar", "N. Gheissari"], "venue": "Lecture Notes in Computer Science. vol. 4844 LNCS, 2007, pp. 457-466.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2007}, {"title": "Human action recognition using Dynamic Time Warping", "author": ["S. Sempena", "N.U. Maulidevi", "P.R. Aryan"], "venue": "Proceedings of the 2011 International Conference on Electrical Engineering and Informatics, ICEEI 2011, 2011.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2011}, {"title": "Human activity recognition in videos: A systematic approach", "author": ["S. Singh", "J. Wang"], "venue": "Lecture Notes in Computer Science vol. 4224 LNCS, 2006, pp. 257-264.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2006}, {"title": "Differential use of attentional and visual communicative signaling by orangutans (Pongo pygmaeus) and gorillas (Gorilla gorilla) in response to the attentional status of a human", "author": ["S.R. Poss", "C. Kuhar", "T.S. Stoinski", "W.D. Hopkins"], "venue": "American Journal of Primatology, vol. 68, pp. 978-992, 2006.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2006}, {"title": "Clapping in chimpanzees: Evidence of exclusive hand preference in a spontaneous, bimanual gesture", "author": ["A.W. Fletcher"], "venue": "American Journal of Primatology, vol. 68, pp. 1081-1088, 2006.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2006}, {"title": "Ape gestures and language evolution", "author": ["A.S. Pollick", "F.B.M. De Waal"], "venue": "Proceedings of the National Academy of Sciences of the United States of America, vol. 104, pp. 8184-8189, 2007.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2007}, {"title": "Hand-clapping as a communicative gesture by wild female swamp gorillas", "author": ["A.K. Kalan", "H.J. Rainey"], "venue": "Primates, vol. 50, pp. 273-275, 2009.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2009}, {"title": "Human-Recognizable Robotic Gestures", "author": ["J.M. Fay", "\"Hand-clapping in western low land gorillas", "Mammalia", "vol. 53", "pp. 457-458", "1989. 17 PLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan", "Wing Chee So", "Soumo Pramanik"], "venue": "Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2012}, {"title": "Human-robot communication with hand-clapping language", "author": ["K. Hanahara", "Y. Tada"], "venue": "Journal of Computers, vol. 3, pp. 58-66, 2008.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2008}, {"title": "Lying and nonverbal behavior: Theoretical issues and new findings", "author": ["P. Ekman"], "venue": "Journal of Nonverbal Behavior, vol. 12, pp. 163-175, 1988.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 1988}, {"title": "Nonverbal behavior and communication", "author": ["A.W. Siegman", "S. Feldstein"], "venue": "Hillsdale, NJ: Erlbaum,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 1987}, {"title": "Towards Humanlike Social Touch for Sociable Robotics and Prosthetics: Comparisons on the Compliance, Conformance and Hysteresis of Synthetic and Human Fingertip Skins", "author": ["J.J. Cabibihan", "S. Pattofatto", "M. Jomaa", "A. Benallal", "M.C. Carrozza"], "venue": "International Journal of Social Robotics vol. 1, pp. 29-40, 2009.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonverbal maternal warmth and children's locus of control of reinforcement", "author": ["J.S. Carton", "E.E.R. Carton"], "venue": "Journal of Nonverbal Behavior, vol. 22, pp. 77-86, 1998.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 1998}, {"title": "Antecedents of individual differences in locus of control of reinforcement: A critical review", "author": ["J.S. Carton", "S. Nowicki", "Jr."], "venue": "Genetic, Social, and General Psychology Monographs, vol. 120, pp. 31-81, 1994.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 1994}, {"title": "Origins of Generalized Control Expectancies: Reported Child Stress and Observed Maternal Control and Warmth", "author": ["J.S. Carton", "S. Nowicki Jr"], "venue": "Journal of Social Psychology, vol. 136, pp. 753-760, 1996.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 1996}, {"title": "Oxytocin increases trust in humans", "author": ["M. Kosfeld", "M. Heinrichs", "P.J. Zak", "U. Fischbacher", "E. Fehr"], "venue": "Nature, vol. 435, pp. 673-676, 2005.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2005}, {"title": "Oxytocin increases generosity in humans", "author": ["P.J. Zak", "A.A. Stanton", "S. Ahmadi"], "venue": "PLoS ONE, vol. 2, 2007.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2007}, {"title": "Design and Development of Nancy, a Social Robot", "author": ["S.S. Ge", "J.J. Cabibihan", "Z. Zhang", "Y. Li", "C. Meng", "H. He", "M.R. Safizadeh", "Y.B. Li", "J. Yang"], "venue": "Proc of the 8th Intl Conf on Ubiquitous Robots and Ambient Intelligence, Incheon, Korea, 2011.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2011}, {"title": "Model-free impedance control for safe human-robot interaction", "author": ["Y. Li", "S.S. Ge", "C. Yang", "K.P. Tee"], "venue": "Proc of IEEE Intl Conf on Robotics and Automation (ICRA), Shanghai, China, 2011, pp. 6021-6026.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2011}, {"title": "Impedance control for multi-point human-robot interaction", "author": ["Y. Li", "S.S. Ge", "C. Yang"], "venue": "Proc of ASCC 2011 - 8th Asian Control Conference, 2011, pp. 1187-1192.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2011}, {"title": "Prosthetic finger phalanges with lifelike skin compliance for low-force social touching interactions", "author": ["J.J. Cabibihan", "R. Pradipta", "S.S. Ge"], "venue": "Journal of NeuroEngineering and Rehabilitation, vol. 8, p. 16, 2011.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards humanlike social touch for prosthetics and sociable robotics: Handshake experiments and finger phalange indentations", "author": ["J.J. Cabibihan", "R. Pradipta", "Y.Z. Chew", "S.S. Ge"], "venue": "Lecture Notes in Computer Science vol. 5744 LNCS, 2009, pp. 73-79.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2009}, {"title": "Force and motion analyses of the human patting gesture for robotic social touching", "author": ["J.J. Cabibihan", "I. Ahmed", "S.S. Ge"], "venue": "Proc of IEEE Cybernetics and Intelligent Systems, Robotics, Automation and Mechatronics (CIS-RAM), Qingdao, China, 2011.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2011}, {"title": "Towards social robots: Designing a emotion-based architecture", "author": ["J. Hirth", "N. Schmitz", "K. Berns"], "venue": "International Journal of Social Robotics, vol. 3, pp. 273-290, 2011.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2011}, {"title": "Umilta \"Brain response to a humanoid robot in areas implicated in the perception of human emotional gestures,", "author": ["T. Chaminade", "M. Zecca", "S.J. Blakemore", "A. Takanishi", "C.D. Frith", "S. Micera", "P. Dario", "G. Rizzolatti", "V. Gallese", "M. A"], "venue": "PLoS ONE,", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2010}, {"title": "Facial expression and emotion", "author": ["P. Ekman"], "venue": "American Psychologist, vol. 48, pp. 384-392, 1993.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 1993}, {"title": "Scientific Issues Concerning Androids", "author": ["H. Ishiguro"], "venue": "International Journal of Robotics Research, vol. 26, pp. 105-117, 2007.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2007}, {"title": "Efficient bipedal robots based on passive-dynamic walkers", "author": ["S. Collins", "A. Ruina", "R. Tedrake", "M. Wisse"], "venue": "Science, vol. 307, pp. 1082-1085, 2005.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2005}, {"title": "Effort-Shape and kinematic assessment of bodily expression of emotion during gait", "author": ["M.M. Gross", "E.A. Crane", "B.L. Fredrickson"], "venue": "Human Movement Science, 2011.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2011}, {"title": "Measurement of lower extremity kinematics during level walking", "author": ["M.P. Kadaba", "H.K. Ramakrishnan", "M.E. Wootten"], "venue": "Journal of Orthopaedic Research, vol. 8, pp. 383-392, 1990.", "citeRegEx": "95", "shortCiteRegEx": null, "year": 1990}, {"title": "Human-Recognizable Robotic Gestures", "author": ["A. Koenig", "X. Omlin", "L. Zimmerli", "M. Sapa", "C. Krewer", "M. Bolliger", "F. Muller", "R. Riener", "\"Psychological state estimation from physiological recordings during robot-assisted gait rehabilitation", "Journal of rehabilitation research", "development", "vol. 48", "pp. 367-385", "2011. 18 PLEASE CITE THIS ARTICLE IN PRESS AS: John-John Cabibihan", "Wing Chee So", "Soumo Pramanik"], "venue": "Autonomous Mental Development, IEEE Transactions, 2012, 4(4), 305-314, doi 10.1109/TAMD.2012.2208962", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2012}, {"title": "Emotional influences on locomotor behavior", "author": ["K.M. Naugle", "J. Joyner", "C.J. Hass", "C.M. Janelle"], "venue": "Journal of Biomechanics, vol. 43, pp. 3099-3103, 2011.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2011}, {"title": "Full-body gesture recognition using inertial sensors for playful interaction with small humanoid robot", "author": ["M.D. Cooney", "C. Becker-Asano", "T. Kanda", "A. Alissandrakis", "H. Ishiguro"], "venue": "IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings, 2010, pp. 2276-2282.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2010}, {"title": "A storytelling robot: Modeling and evaluation of human-like gaze behavior", "author": ["B. Mutlu", "J. Forlizzi", "J. Hodgins"], "venue": "Proceedings of the 2006 6th IEEE-RAS International Conference on Humanoid Robots, HUMANOIDS, 2006, pp. 518-523.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2006}, {"title": "Culture in the elementary foreign language classroom", "author": ["C.A. Pesola"], "venue": "Foreign Language Annals, vol. 24, pp. 331-346, 1991.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 1991}, {"title": "The educational use of Home Robots for children", "author": ["J. Han", "M. Jo", "S. Park", "S. Kim"], "venue": "Proceedings - IEEE International Workshop on Robot and Human Interactive Communication, 2005, pp. 378-383.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2005}, {"title": "Interactive robots as social partners and peer tutors for children : A field trial", "author": ["T. Kanda", "T. Hirano", "D. Eaton", "H. Ishiguro"], "venue": "Human-Computer Interaction, vol. 19, pp. 61-84, 2004.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "[1-6]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 1, "context": "[1-6]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "[1-6]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 3, "context": "[1-6]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 4, "context": "[1-6]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 5, "context": "[1-6]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 6, "context": "[7-9]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 7, "context": "[7-9]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 8, "context": "[7-9]).", "startOffset": 0, "endOffset": 5}, {"referenceID": 9, "context": "Social robots are autonomous robots that are able to interact and communicate among themselves, with humans, and with the environment and are designed to operate according to the established social and cultural norms [10, 11].", "startOffset": 217, "endOffset": 225}, {"referenceID": 10, "context": "Social robots are autonomous robots that are able to interact and communicate among themselves, with humans, and with the environment and are designed to operate according to the established social and cultural norms [10, 11].", "startOffset": 217, "endOffset": 225}, {"referenceID": 11, "context": "For robots to achieve socially engaging interactions with humans, researchers have argued that robots are expected to learn and produce human-like attributes such as body movements [12-14].", "startOffset": 181, "endOffset": 188}, {"referenceID": 12, "context": "For robots to achieve socially engaging interactions with humans, researchers have argued that robots are expected to learn and produce human-like attributes such as body movements [12-14].", "startOffset": 181, "endOffset": 188}, {"referenceID": 13, "context": "For robots to achieve socially engaging interactions with humans, researchers have argued that robots are expected to learn and produce human-like attributes such as body movements [12-14].", "startOffset": 181, "endOffset": 188}, {"referenceID": 0, "context": "Thus, robotic gestures have become one of the key design features for engaging human-robot interaction [1].", "startOffset": 103, "endOffset": 106}, {"referenceID": 14, "context": "Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 15, "context": "Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 16, "context": "Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 17, "context": "Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 18, "context": "Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 19, "context": "Using the captured data, several methods were proposed for the robot to classify human emotions or to follow the human teacher\u2019s instructions [15-20].", "startOffset": 142, "endOffset": 149}, {"referenceID": 20, "context": "In this approach, the robot captures the human demonstrator\u2019s motions through its on-board vision system [21].", "startOffset": 105, "endOffset": 109}, {"referenceID": 21, "context": "can also learn from the demonstrator\u2019s actions through the motion sensors that the demonstrator wears and the robot repeats the movements [22, 23].", "startOffset": 138, "endOffset": 146}, {"referenceID": 22, "context": "can also learn from the demonstrator\u2019s actions through the motion sensors that the demonstrator wears and the robot repeats the movements [22, 23].", "startOffset": 138, "endOffset": 146}, {"referenceID": 23, "context": "kinesthetic teaching [24-27].", "startOffset": 21, "endOffset": 28}, {"referenceID": 24, "context": "kinesthetic teaching [24-27].", "startOffset": 21, "endOffset": 28}, {"referenceID": 25, "context": "kinesthetic teaching [24-27].", "startOffset": 21, "endOffset": 28}, {"referenceID": 26, "context": "kinesthetic teaching [24-27].", "startOffset": 21, "endOffset": 28}, {"referenceID": 27, "context": "[28] found that human beings responded to body movements and utterances by a route guidance robot, while Oberman et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[29] further reported that comprehending robotic actions might activate the mirror neuron system that was previously thought to be specifically", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[30]), whereby the gestures carry the semantic meaning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "Even simple body movements, like eye gaze and head nods, allow the fluid exchange in the roles of speaker and listener [32-35].", "startOffset": 119, "endOffset": 126}, {"referenceID": 31, "context": "Even simple body movements, like eye gaze and head nods, allow the fluid exchange in the roles of speaker and listener [32-35].", "startOffset": 119, "endOffset": 126}, {"referenceID": 32, "context": "Even simple body movements, like eye gaze and head nods, allow the fluid exchange in the roles of speaker and listener [32-35].", "startOffset": 119, "endOffset": 126}, {"referenceID": 33, "context": "Even simple body movements, like eye gaze and head nods, allow the fluid exchange in the roles of speaker and listener [32-35].", "startOffset": 119, "endOffset": 126}, {"referenceID": 34, "context": "Interestingly, humans have been found to be very sensitive to these nonverbal cues [36].", "startOffset": 83, "endOffset": 87}, {"referenceID": 35, "context": "Previous research has shown that among conversation participants, the appropriate nonverbal gestures play a key role in helping communicate intent, instruct, lead, and build rapport [37-39].", "startOffset": 182, "endOffset": 189}, {"referenceID": 36, "context": "Previous research has shown that among conversation participants, the appropriate nonverbal gestures play a key role in helping communicate intent, instruct, lead, and build rapport [37-39].", "startOffset": 182, "endOffset": 189}, {"referenceID": 37, "context": "Previous research has shown that among conversation participants, the appropriate nonverbal gestures play a key role in helping communicate intent, instruct, lead, and build rapport [37-39].", "startOffset": 182, "endOffset": 189}, {"referenceID": 38, "context": "posture, gesture, interpersonal distance, and positioning have physiological effects on the other person, which have been found to be distinct from the effects of linguistic information [40].", "startOffset": 186, "endOffset": 190}, {"referenceID": 39, "context": "Gestures are the spontaneous movements exhibited by speakers from all cultural and linguistic backgrounds as they are engaged in conversations [41-43].", "startOffset": 143, "endOffset": 150}, {"referenceID": 40, "context": "Iconic gestures and emblems are often used to complement speech since they are meaningful hand configurations, and thus, are clearly communicative [44].", "startOffset": 147, "endOffset": 151}, {"referenceID": 41, "context": "Gesture not only facilitates speech production, but also speech comprehension [45, 46].", "startOffset": 78, "endOffset": 86}, {"referenceID": 42, "context": "Gesture not only facilitates speech production, but also speech comprehension [45, 46].", "startOffset": 78, "endOffset": 86}, {"referenceID": 43, "context": "It was suggested that speakers gesture to maximize the information conveyed to listeners [47].", "startOffset": 89, "endOffset": 93}, {"referenceID": 44, "context": "Inspired by typical human gestures, we evaluated an initial set of 25 gestures in a pilot study [48].", "startOffset": 96, "endOffset": 100}, {"referenceID": 45, "context": "emotional perception [49, 50], i.", "startOffset": 21, "endOffset": 29}, {"referenceID": 46, "context": "emotional perception [49, 50], i.", "startOffset": 21, "endOffset": 29}, {"referenceID": 47, "context": "This cut-off threshold was similar to the human gesture experiments in [51, 52].", "startOffset": 71, "endOffset": 79}, {"referenceID": 48, "context": "This cut-off threshold was similar to the human gesture experiments in [51, 52].", "startOffset": 71, "endOffset": 79}, {"referenceID": 48, "context": "In an earlier paper [52], experimental subjects were presented with 80 videotaped gestures and only 40 gestures (i.", "startOffset": 20, "endOffset": 24}, {"referenceID": 35, "context": "disapproval or negation, and many other semantic messages [37, 53-56].", "startOffset": 58, "endOffset": 69}, {"referenceID": 49, "context": "disapproval or negation, and many other semantic messages [37, 53-56].", "startOffset": 58, "endOffset": 69}, {"referenceID": 50, "context": "disapproval or negation, and many other semantic messages [37, 53-56].", "startOffset": 58, "endOffset": 69}, {"referenceID": 51, "context": "disapproval or negation, and many other semantic messages [37, 53-56].", "startOffset": 58, "endOffset": 69}, {"referenceID": 52, "context": "disapproval or negation, and many other semantic messages [37, 53-56].", "startOffset": 58, "endOffset": 69}, {"referenceID": 53, "context": "It was observed that the nods by the listener in a conversation encouraged utterances of the speaker, which achieved an animated conversation between the speaker and the listener [57].", "startOffset": 179, "endOffset": 183}, {"referenceID": 54, "context": "of nodding may have come easy as it is a primitive form of communication that has been observed in mother-infant interaction [58, 59].", "startOffset": 125, "endOffset": 133}, {"referenceID": 55, "context": "of nodding may have come easy as it is a primitive form of communication that has been observed in mother-infant interaction [58, 59].", "startOffset": 125, "endOffset": 133}, {"referenceID": 56, "context": "2208962 human\u2019s nods, as compared to when the robot did not nod [60].", "startOffset": 64, "endOffset": 68}, {"referenceID": 57, "context": "[61] confirmed this co-nodding phenomenon through a robotic monkey\u2019s head that mimicked the nodding", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] observed that people have generally rated their interactions with a robot highly when robots know when to nod and to gaze appropriately as compared with robots that have not learned these behaviors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "An audience normally expresses their appreciation for a good performance by the strength and length of their applause [62].", "startOffset": 118, "endOffset": 122}, {"referenceID": 59, "context": "streaming analysis for sports, music and human-machine interaction, among many others [63-67].", "startOffset": 86, "endOffset": 93}, {"referenceID": 60, "context": "streaming analysis for sports, music and human-machine interaction, among many others [63-67].", "startOffset": 86, "endOffset": 93}, {"referenceID": 61, "context": "streaming analysis for sports, music and human-machine interaction, among many others [63-67].", "startOffset": 86, "endOffset": 93}, {"referenceID": 62, "context": "streaming analysis for sports, music and human-machine interaction, among many others [63-67].", "startOffset": 86, "endOffset": 93}, {"referenceID": 63, "context": "streaming analysis for sports, music and human-machine interaction, among many others [63-67].", "startOffset": 86, "endOffset": 93}, {"referenceID": 64, "context": "Researchers have discovered that for apes in captivity, the apes clap in order to attract the attention of humans [68-70]; for apes in the wild, apes use clapping as a form of", "startOffset": 114, "endOffset": 121}, {"referenceID": 65, "context": "Researchers have discovered that for apes in captivity, the apes clap in order to attract the attention of humans [68-70]; for apes in the wild, apes use clapping as a form of", "startOffset": 114, "endOffset": 121}, {"referenceID": 66, "context": "Researchers have discovered that for apes in captivity, the apes clap in order to attract the attention of humans [68-70]; for apes in the wild, apes use clapping as a form of", "startOffset": 114, "endOffset": 121}, {"referenceID": 67, "context": "long distance communication to maintain group cohesiveness during instances of alarm [71, 72].", "startOffset": 85, "endOffset": 93}, {"referenceID": 68, "context": "long distance communication to maintain group cohesiveness during instances of alarm [71, 72].", "startOffset": 85, "endOffset": 93}, {"referenceID": 69, "context": "Hanahara and Tada [73] proposed a new communication language based on clapping for humans to communicate better with robots.", "startOffset": 18, "endOffset": 22}, {"referenceID": 70, "context": ", [74, 75]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 71, "context": ", [74, 75]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 72, "context": "hugs, patting, caress, handshake) appears to be an important modality [76].", "startOffset": 70, "endOffset": 74}, {"referenceID": 73, "context": "It has been suggested that the warmth from a parent\u2019s touch may help children feel secure in their exploration of their environments [77, 78].", "startOffset": 133, "endOffset": 141}, {"referenceID": 74, "context": "It has been suggested that the warmth from a parent\u2019s touch may help children feel secure in their exploration of their environments [77, 78].", "startOffset": 133, "endOffset": 141}, {"referenceID": 75, "context": ", [79]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 76, "context": "[80, 81].", "startOffset": 0, "endOffset": 8}, {"referenceID": 77, "context": "[80, 81].", "startOffset": 0, "endOffset": 8}, {"referenceID": 78, "context": "A robotic nurse named Nancy was developed as a research platform to explore the effects of robotic gestures and social touching on humans [82].", "startOffset": 138, "endOffset": 142}, {"referenceID": 79, "context": "Among the touching behaviors that have been investigated are hugging [83, 84], handshaking [85, 86], and patting [87].", "startOffset": 69, "endOffset": 77}, {"referenceID": 80, "context": "Among the touching behaviors that have been investigated are hugging [83, 84], handshaking [85, 86], and patting [87].", "startOffset": 69, "endOffset": 77}, {"referenceID": 81, "context": "Among the touching behaviors that have been investigated are hugging [83, 84], handshaking [85, 86], and patting [87].", "startOffset": 91, "endOffset": 99}, {"referenceID": 82, "context": "Among the touching behaviors that have been investigated are hugging [83, 84], handshaking [85, 86], and patting [87].", "startOffset": 91, "endOffset": 99}, {"referenceID": 83, "context": "Among the touching behaviors that have been investigated are hugging [83, 84], handshaking [85, 86], and patting [87].", "startOffset": 113, "endOffset": 117}, {"referenceID": 84, "context": "Programming emotions into robots has been identified as an important step to create lifelike social robots for improving human-robot interaction [89].", "startOffset": 145, "endOffset": 149}, {"referenceID": 85, "context": "4 Refined II (WE4-RII) humanoid robot was designed to show human-like emotions [90].", "startOffset": 79, "endOffset": 83}, {"referenceID": 86, "context": "[91]) of the eyebrows, eyelids, eyes, mouth, and lips.", "startOffset": 0, "endOffset": 4}, {"referenceID": 85, "context": "[90]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 87, "context": "Due to the tight coupling of the robot's appearance and behavior, there are still many issues that have to be addressed in replicating human emotions through the facial expressions and full body gestures by robots [92].", "startOffset": 214, "endOffset": 218}, {"referenceID": 88, "context": "67] and gait analysis [93-97] while the flapping gesture has not been often described in the literatures.", "startOffset": 22, "endOffset": 29}, {"referenceID": 89, "context": "67] and gait analysis [93-97] while the flapping gesture has not been often described in the literatures.", "startOffset": 22, "endOffset": 29}, {"referenceID": 90, "context": "67] and gait analysis [93-97] while the flapping gesture has not been often described in the literatures.", "startOffset": 22, "endOffset": 29}, {"referenceID": 91, "context": "67] and gait analysis [93-97] while the flapping gesture has not been often described in the literatures.", "startOffset": 22, "endOffset": 29}, {"referenceID": 92, "context": "67] and gait analysis [93-97] while the flapping gesture has not been often described in the literatures.", "startOffset": 22, "endOffset": 29}, {"referenceID": 93, "context": "Considering that robot companions will be expected to engage in playful interactions, one study investigated the possible full-body motions that a human playmate will do to a small humanoid robot [98].", "startOffset": 196, "endOffset": 200}, {"referenceID": 94, "context": "If robots were to be used for education and entertainment applications, storytelling was suggested to be a necessary skill [99].", "startOffset": 123, "endOffset": 127}, {"referenceID": 95, "context": "Story telling has been found to be one of the most powerful tools to teach a new language to a child [100].", "startOffset": 101, "endOffset": 106}, {"referenceID": 96, "context": "2208962 the English language [101] while Robovie teaches English vocabulary to Japanese children [102].", "startOffset": 29, "endOffset": 34}, {"referenceID": 97, "context": "2208962 the English language [101] while Robovie teaches English vocabulary to Japanese children [102].", "startOffset": 97, "endOffset": 102}], "year": 2012, "abstractText": null, "creator": "Word"}}}