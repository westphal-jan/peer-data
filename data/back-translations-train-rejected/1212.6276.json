{"id": "1212.6276", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Dec-2012", "title": "Echo State Queueing Network: a new reservoir computing learning tool", "abstract": "In the last decade, a new computational paradigm was introduced in the field of Machine Learning, under the name of Reservoir Computing (RC). RC models are neural networks which a recurrent part (the reservoir) that does not participate in the learning process, and the rest of the system where no recurrence (no neural circuit) occurs. This approach has grown rapidly due to its success in solving learning tasks and other computational applications. Some success was also observed with another recently proposed neural network designed using Queueing Theory, the Random Neural Network (RandNN). Both approaches have good properties and identified drawbacks. In this paper, we propose a new RC model called Echo State Queueing Network (ESQN), where we use ideas coming from RandNNs for the design of the reservoir. ESQNs consist in ESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. The paper positions ESQNs in the global Machine Learning area, and provides examples of their use and performances. We show on largely used benchmarks that ESQNs are very accurate tools, and we illustrate how they compare with standard ESNs.", "histories": [["v1", "Wed, 26 Dec 2012 22:31:13 GMT  (125kb,D)", "http://arxiv.org/abs/1212.6276v1", "Proceedings of the 10th IEEE Consumer Communications and Networking Conference (CCNC), Las Vegas, USA, 2013"]], "COMMENTS": "Proceedings of the 10th IEEE Consumer Communications and Networking Conference (CCNC), Las Vegas, USA, 2013", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["sebasti\\'an basterrech", "gerardo rubino"], "accepted": false, "id": "1212.6276"}, "pdf": {"name": "1212.6276.pdf", "metadata": {"source": "CRF", "title": "Echo State Queueing Network: a new reservoir computing learning tool", "authors": ["Sebasti\u00e1n Basterrech", "Gerardo Rubino"], "emails": ["Sebastian.Basterrech@inria.fr", "Gerardo.Rubino@inria.fr"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. DESCRIPTION OF THE RANDOM NEURAL NETWORK MODEL", "text": "A neuronal network (RandNN) is a specific queue network proposed in [1], which brings together concepts of spying on neural networks and the queue theory. Depending on the context, its nodes are considered as queues or spiking neurons. Each of these neurons receives spikes (impulses) from outside, which emanate from one of two different types of neurons, which are called trigatory (or positive) and inhibiting (or negative). Associated with a neuron, there is an integer variable called the neuron potential. Each time a neuron receives a precitatory spike, its potential is increased by one. If a neuron receives an inhibitory spike and its potential, it decreases by one; if it was equal to 0, it remains at this value. As far as the potential of the neuron is strictly positive, the neuron sends excitatory spike outwards. If the potential of the neuron is active, we say that the neuron is strict, or that is positive."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a region and in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country, in which it is a country and in which it is a country."}, {"heading": "VI. CONCLUSIONS", "text": "In this paper, we presented a novel reservoir computing model, which we call the Echo State Queuing Network (ESQN). It combines ideas from queues and neural networks. It is based on two computing models: the Echo State Network (ESN) and the Random Neural Network. Both methods have been used successfully in prediction and machine learning. ESNs have been used in particular in many time learning tasks. Our model was used to predict three time series data that are widely used in machine learning literature. In all the cases tested, the performance results were very good. We empirically investigated the relationship between the size of the reservoir and ESQN performance. We found that the size of the reservoir has a significant impact on accuracy. Another positive feature of ESQNs is its simplicity, as reservoir units are only counterfunctions. Finally, our tool is very easy to implement, both in software and hardware."}], "references": [{"title": "Random Neural Networks with Negative and Positive Signals and Product Form Solution", "author": ["E. Gelenbe"], "venue": "Neural Computation, vol. 1, no. 4, pp. 502\u2013510, 1989.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Learning in the recurrent random neural network", "author": ["\u2014\u2014"], "venue": "Neural Computation, vol. 5, pp. 154\u2013164, 1993.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1993}, {"title": "Training the Random Neural Network using Quasi-Newton Methods", "author": ["A. Likas", "A. Stafylopatis"], "venue": "Eur.J.Oper.Res, vol. 126, pp. 331\u2013339, 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Levenberg-Marquardt Training Algorithms for Random Neural Networks", "author": ["S. Basterrech", "S. Mohammed", "G. Rubino", "M. Soliman"], "venue": "Computer Journal, vol. 54, no. 1, pp. 125\u2013135, January 2011. [Online]. Available: http://dx.doi.org/10.1093/comjnl/bxp101", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "The Spiked Random Neural Network: Nonlinearity, Learning and Approximation", "author": ["E. Gelenbe"], "venue": "Proc. Fifth IEEE International Workshop on Cellular Neural Networks and Their Applications, London, England, april 1998, pp. 14\u201319.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Function Approximation by Random Neural Networks with a Bounded Number of Layers", "author": ["E. Gelenbe", "Z. Mao", "Y. Da-Li"], "venue": "Journal of Differential Equations and Dynamical Systems, vol. 12, pp. 143\u2013170, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning in the feedforward random neural network: A critical review", "author": ["M. Georgiopoulos", "C. Li", "T. Ko\u00e7ak"], "venue": "Performance Evaluation, vol. 68, no. 4, pp. 361 \u2013 384, 2011. [Online]. Available: http://www.sciencedirect.com/science/article/pii/S0166531610000970", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Reservoir computing approaches to recurrent neural network training", "author": ["M. Lukos\u0306evic\u0306ius", "H. Jaeger"], "venue": "Computer Science Review, pp. 127\u2013149, 2009. [Online]. Available: http://dx.doi.org/10.1016/j. cosrev2009.03.005", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Bifurcations in the learning of Recurrent Neural Networks", "author": ["K. Doya"], "venue": "IEEE International Symposium on Circuits and Systems, 1992, pp. 2777\u20132780.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "The \u201cecho state\u201d approach to analysing and training recurrent neural networks", "author": ["H. Jaeger"], "venue": "German National Research Center for Information Technology, Tech. Rep. 148, 2001.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Real-time computing without stable states: a new framework for a neural computation based on perturbations", "author": ["W. Maass", "T. Natschl\u00e4ger", "H. Markram"], "venue": "Neural Computation, pp. 2531\u20132560, november 2002.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "An experimental unification of reservoir computing methods", "author": ["D. Verstraeten", "B. Schrauwen", "M. D\u2019Haene", "D. Stroobandt"], "venue": "Neural Networks, no. 3, pp. 287\u2013289, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Product-Form Queueing Networks with Negative and Positive Customers", "author": ["E. Gelenbe"], "venue": "Journal of Applied Probability, vol. 28, no. 3, pp. 656\u2013663, September 1991.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1991}, {"title": "Cognitive Packet Networks", "author": ["E. Gelenbe", "Z. Xu", "E. Seref"], "venue": "11th IEEE International Conference on Tools with Artificial Intelligence (ICTAI\u201999), 1999, pp. 47\u201354.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1999}, {"title": "The Cognitive Packet Network: A Survey", "author": ["G. Sakellari"], "venue": "The Computer Journal, vol. 53, no. 3, pp. 268\u2013279, 2010. [Online]. Available: http://comjnl.oxfordjournals.org/cgi/content/abstract/bxp053v1", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Quantifying the Quality of Audio and Video Transmissions over the Internet: the PSQA Approach", "author": ["G. Rubino"], "venue": "Design and Operations of Communication Networks: A Review of Wired and Wireless Modelling and Management Challenges, ser. Edited by J. Barria. Imperial College Press, 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Minimum cost graph covering with the random neural network", "author": ["E. Gelenbe", "F. Batty"], "venue": "Computer Science and Operations Research. New York: Pergamon, 1992, pp. 139\u2013147.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1992}, {"title": "A GRASP algorithm with RNN based local search for designing a WAN access network", "author": ["H. Cancela", "F. Robledo", "G. Rubino"], "venue": "Electronic Notes in Discrete Mathematics, vol. 18, pp. 59\u201365, 2004. [Online]. Available: http://dx.doi.org/10.1016/j.endm.2004.06.010", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2004}, {"title": "Support-Vector Networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Mach. Learn., vol. 20, no. 3, pp. 273\u2013297, Sep. 1995. [Online]. Available: http://dx.doi.org/10.1023/A:1022627411411", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1995}, {"title": "Training Recurrent Networks by Evolino", "author": ["J. Schmidhuber", "D. Wierstra", "M. Gagliolo", "F. Gomez"], "venue": "Neural Computation, vol. 19, no. 3, pp. 757\u2013779, Mar. 2007. [Online]. Available: http://dx.doi.org/10.1162/ neco.2007.19.3.757", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication", "author": ["H. Jaeger", "H. Haas"], "venue": "Science, vol. 304, no. 5667, pp. 78\u201380, 2004. [Online]. Available: http://www.sciencemag.org/content/304/5667/78.abstract", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Improving reservoirs using Intrinsic Plasticity", "author": ["B. Schrauwen", "M. Wardermann", "D. Verstraeten", "J.J. Steil", "D. Stroobandt"], "venue": "Neurocomputing, vol. 71, pp. 1159\u20131171, March 2007.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Backpropagation-Decorrelation: online recurrent learning with O(n) complexity", "author": ["J.J. Steil"], "venue": "Proceedings of IJCNN\u201904, vol. 1, 2004.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Decoupled Echo State Networks with lateral inhibition", "author": ["Y. Xue", "L. Yang", "S. Haykin"], "venue": "Neural Networks, no. 3, pp. 365\u2013376, 2007.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimization and applications of Echo State Networks with leaky-integrator neurons", "author": ["H. Jaeger", "M. Lukos\u0306evic\u0306ius", "D. Popovici", "U. Siewert"], "venue": "Neural Networks, no. 3, pp. 335\u2013352, 2007.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2007}, {"title": "Numerical Recipes in C, 2nd ed", "author": ["W. Press", "S. Teukolsky", "W. Vetterling", "B. Flannery"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1992}, {"title": "Minimum Complexity Echo State Network", "author": ["A. Rodan", "P. Tin\u0306o"], "venue": "IEEE Transactions on Neural Networks, pp. 131\u2013144, 2011. [Online]. Available: http://dx.doi.org/10.1109/TNN.2010.2089641", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiscale Internet traffic forecasting using neural networks and time series methods", "author": ["P. Cortez", "M. Rio", "M. Rocha", "P. Sousa"], "venue": "Expert Systems, 2012. [Online]. Available: http://dx.doi.org/10.1111/j. 1468-0394.2010.00568.x", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Time Series Data Library", "author": ["R. Hyndman"], "venue": "Accessed on: August 31, 2012. [Online]. Available: http://robjhyndman.com/TSDL/ miscellaneous/", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Reservoir Computing Trends", "author": ["M. Lukos\u0306evic\u0306ius", "H. Jaeger", "B. Schrauwen"], "venue": "KI - K\u00fcnstliche Intelligenz, pp. 1\u20137, 2012. [Online]. Available: http://dx.doi.org/10.1007/s13218-012-0204-5", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "New results on recurrent network training: unifying the algorithms and accelerating convergence", "author": ["A.F. Atiya", "A.G. Parlos"], "venue": "IEEE Trans. Neural Networks, vol. 11, pp. 697\u2013709, 2000.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "Self-Organizing Maps and Scale-Invariant Maps in Echo State Networks", "author": ["S. Basterrech", "C. Fyfe", "G. Rubino"], "venue": "Intelligent Systems Design and Applications (ISDA), 2011 11th International Conference on, nov. 2011, pp. 94 \u201399.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "On self-organizing reservoirs and their hierarchies", "author": ["M. Lukos\u0306evic\u0306ius"], "venue": "Jacobs University, Bremen, Tech. Rep. 25, 2010.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Gelenbe in 1989 [1], is a mathematical object inspired by biological neuronal behavior which", "startOffset": 16, "endOffset": 19}, {"referenceID": 1, "context": "In order to use RandNNs in supervised learning problems, a gradient descent algorithm has been described in [2], and Quasi-Newton methods have been proposed in [3], [4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 2, "context": "In order to use RandNNs in supervised learning problems, a gradient descent algorithm has been described in [2], and Quasi-Newton methods have been proposed in [3], [4].", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "In order to use RandNNs in supervised learning problems, a gradient descent algorithm has been described in [2], and Quasi-Newton methods have been proposed in [3], [4].", "startOffset": 165, "endOffset": 168}, {"referenceID": 4, "context": "Additionally, the function approximation properties of the model were studied in [5], [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "Additionally, the function approximation properties of the model were studied in [5], [6].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "Some of them are related to the use of a feedforward topology (see [7]).", "startOffset": 67, "endOffset": 70}, {"referenceID": 7, "context": "The main drawbacks related to learning algorithms are the following: convergence is not always guaranteed, many algorithmic parameters are involved, sometimes long training times are required [8], [9].", "startOffset": 192, "endOffset": 195}, {"referenceID": 8, "context": "The main drawbacks related to learning algorithms are the following: convergence is not always guaranteed, many algorithmic parameters are involved, sometimes long training times are required [8], [9].", "startOffset": 197, "endOffset": 200}, {"referenceID": 9, "context": "About ten years ago two main RC models were proposed: Echo State Networks (ESNs) [10] and Liquid State Machines (LSMs) [11].", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "About ten years ago two main RC models were proposed: Echo State Networks (ESNs) [10] and Liquid State Machines (LSMs) [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 7, "context": "The RC approach have been successfully applied in many machine learning tasks achieving goods results, specially in temporal learning tasks [8], [11], [12].", "startOffset": 140, "endOffset": 143}, {"referenceID": 10, "context": "The RC approach have been successfully applied in many machine learning tasks achieving goods results, specially in temporal learning tasks [8], [11], [12].", "startOffset": 145, "endOffset": 149}, {"referenceID": 11, "context": "The RC approach have been successfully applied in many machine learning tasks achieving goods results, specially in temporal learning tasks [8], [11], [12].", "startOffset": 151, "endOffset": 155}, {"referenceID": 0, "context": "A Random Neural Network (RandNN) is a specific queuing network proposed in [1] which merges concepts from spiking neural networks and queuing theory.", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": "Gelenbe in [1], [13] shows that in an equilibrium situation the %us satisfy the following non-linear system of equations:", "startOffset": 11, "endOffset": 14}, {"referenceID": 12, "context": "Gelenbe in [1], [13] shows that in an equilibrium situation the %us satisfy the following non-linear system of equations:", "startOffset": 16, "endOffset": 20}, {"referenceID": 0, "context": "For more details and proofs, see [1], [5].", "startOffset": 33, "endOffset": 36}, {"referenceID": 4, "context": "For more details and proofs, see [1], [5].", "startOffset": 38, "endOffset": 41}, {"referenceID": 13, "context": "The model has been widely used in fields such as: combinatorial optimization, machine learning problems, communication networks and computer systems [14]\u2013[18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 17, "context": "The model has been widely used in fields such as: combinatorial optimization, machine learning problems, communication networks and computer systems [14]\u2013[18].", "startOffset": 154, "endOffset": 158}, {"referenceID": 7, "context": "They are also very useful for building associative memories, in data compression and for static pattern classification [8].", "startOffset": 119, "endOffset": 122}, {"referenceID": 18, "context": "From this point of view, the reservoir idea is similar to the expansion function used in Kernel Methods, for example in the Support Vector Machine [19].", "startOffset": 147, "endOffset": 151}, {"referenceID": 11, "context": "The projection can enhance the linear separability of the data [12].", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "The RC approach is based on the empirical observation that under certain assumptions, training only a linear readout is often sufficient to achieve good performance in many learning tasks [8].", "startOffset": 188, "endOffset": 191}, {"referenceID": 19, "context": "For instance, the ESN model has the best known learning performance on the Mackey\u2013Glass times series prediction task [20], [21].", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "For instance, the ESN model has the best known learning performance on the Mackey\u2013Glass times series prediction task [20], [21].", "startOffset": 123, "endOffset": 127}, {"referenceID": 9, "context": "To ensure good properties in the reservoir, the w matrix is usually scaled to control its spectral radius (to have \u03c1(w) < 1) [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 11, "context": "The role of the spectral radius is more complex when the reservoir is built with spiking neurons (in the LSM model) [12], [22].", "startOffset": 116, "endOffset": 120}, {"referenceID": 21, "context": "Several extensions of the two pioneering RC models have been suggested in the literature, such as: intrinsic plasticity [23], backpropagation-decorrelation [24], decoupled ESN [25], leaky integrator [26], Evolino [20], etc.", "startOffset": 120, "endOffset": 124}, {"referenceID": 22, "context": "Several extensions of the two pioneering RC models have been suggested in the literature, such as: intrinsic plasticity [23], backpropagation-decorrelation [24], decoupled ESN [25], leaky integrator [26], Evolino [20], etc.", "startOffset": 156, "endOffset": 160}, {"referenceID": 23, "context": "Several extensions of the two pioneering RC models have been suggested in the literature, such as: intrinsic plasticity [23], backpropagation-decorrelation [24], decoupled ESN [25], leaky integrator [26], Evolino [20], etc.", "startOffset": 176, "endOffset": 180}, {"referenceID": 24, "context": "Several extensions of the two pioneering RC models have been suggested in the literature, such as: intrinsic plasticity [23], backpropagation-decorrelation [24], decoupled ESN [25], leaky integrator [26], Evolino [20], etc.", "startOffset": 199, "endOffset": 203}, {"referenceID": 19, "context": "Several extensions of the two pioneering RC models have been suggested in the literature, such as: intrinsic plasticity [23], backpropagation-decorrelation [24], decoupled ESN [25], leaky integrator [26], Evolino [20], etc.", "startOffset": 213, "endOffset": 217}, {"referenceID": 25, "context": "The output weights w can be computed using some of the traditional algorithms to solve regressions such as the \u201cridge regression\u201d or the least mean square algorithms [27].", "startOffset": 166, "endOffset": 170}, {"referenceID": 9, "context": "In our numerical experiences, we consider a simulated time series data widely used in the ESN literature [10], [28] and two real world data sets about Internet traffic, used in research work about forecasting techniques [29], [30].", "startOffset": 105, "endOffset": 109}, {"referenceID": 26, "context": "In our numerical experiences, we consider a simulated time series data widely used in the ESN literature [10], [28] and two real world data sets about Internet traffic, used in research work about forecasting techniques [29], [30].", "startOffset": 111, "endOffset": 115}, {"referenceID": 27, "context": "In our numerical experiences, we consider a simulated time series data widely used in the ESN literature [10], [28] and two real world data sets about Internet traffic, used in research work about forecasting techniques [29], [30].", "startOffset": 220, "endOffset": 224}, {"referenceID": 28, "context": "In our numerical experiences, we consider a simulated time series data widely used in the ESN literature [10], [28] and two real world data sets about Internet traffic, used in research work about forecasting techniques [29], [30].", "startOffset": 226, "endOffset": 230}, {"referenceID": 0, "context": "2] and [0, 1],", "startOffset": 7, "endOffset": 13}, {"referenceID": 0, "context": "The preprocessing data step consisted in rescaling the data in the interval [0, 1].", "startOffset": 76, "endOffset": 82}, {"referenceID": 29, "context": "The learning method used was offline ridge regression [31].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "We rescaled it in [0, 1].", "startOffset": 18, "endOffset": 24}, {"referenceID": 27, "context": "This configuration was suggested in [29] where the authors discuss different neural network topologies taking into account seasonal traits of the data.", "startOffset": 36, "endOffset": 40}, {"referenceID": 27, "context": "The network input at any time t is the triple composed of the traffic at times t, t \u2212 6 and t \u2212 7, as studied in [29].", "startOffset": 113, "endOffset": 117}, {"referenceID": 11, "context": "The NARMA series data was studied in deep in [12], [21], [28], [32], [33].", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "The NARMA series data was studied in deep in [12], [21], [28], [32], [33].", "startOffset": 51, "endOffset": 55}, {"referenceID": 26, "context": "The NARMA series data was studied in deep in [12], [21], [28], [32], [33].", "startOffset": 57, "endOffset": 61}, {"referenceID": 30, "context": "The NARMA series data was studied in deep in [12], [21], [28], [32], [33].", "startOffset": 63, "endOffset": 67}, {"referenceID": 31, "context": "The NARMA series data was studied in deep in [12], [21], [28], [32], [33].", "startOffset": 69, "endOffset": 73}, {"referenceID": 27, "context": "For the last two data sets the performance using NN, ARIMA and Holt-Winters methods can be seen in [29].", "startOffset": 99, "endOffset": 103}, {"referenceID": 7, "context": "A typical ESN model consists in a reservoir with the following characteristics: random topology, Nx large enough, sparsely connected (roughly between 15% and 20% of their weights are non-zeros) [8].", "startOffset": 194, "endOffset": 197}, {"referenceID": 11, "context": "In [12], the authors obtained the best performance in the NARMA data problem when the spectral radius was close to 1.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "The ESN performance can be improved using leakyintegrator neurons [26], feedback connections [8] or initializing the reservoir weights using another initializing criteria [33], [34].", "startOffset": 66, "endOffset": 70}, {"referenceID": 7, "context": "The ESN performance can be improved using leakyintegrator neurons [26], feedback connections [8] or initializing the reservoir weights using another initializing criteria [33], [34].", "startOffset": 93, "endOffset": 96}, {"referenceID": 31, "context": "The ESN performance can be improved using leakyintegrator neurons [26], feedback connections [8] or initializing the reservoir weights using another initializing criteria [33], [34].", "startOffset": 171, "endOffset": 175}, {"referenceID": 32, "context": "The ESN performance can be improved using leakyintegrator neurons [26], feedback connections [8] or initializing the reservoir weights using another initializing criteria [33], [34].", "startOffset": 177, "endOffset": 181}], "year": 2012, "abstractText": "In the last decade, a new computational paradigm was introduced in the field of Machine Learning, under the name of Reservoir Computing (RC). RC models are neural networks which a recurrent part (the reservoir) that does not participate in the learning process, and the rest of the system where no recurrence (no neural circuit) occurs. This approach has grown rapidly due to its success in solving learning tasks and other computational applications. Some success was also observed with another recently proposed neural network designed using Queueing Theory, the Random Neural Network (RandNN). Both approaches have good properties and identified drawbacks. In this paper, we propose a new RC model called Echo State Queueing Network (ESQN), where we use ideas coming from RandNNs for the design of the reservoir. ESQNs consist in ESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. The paper positions ESQNs in the global Machine Learning area, and provides examples of their use and performances. We show on largely used benchmarks that ESQNs are very accurate tools, and we illustrate how they compare with standard ESNs. Index Terms Reservoir Computing, Echo State Network, Random Neural Network, Queueing Network, Machine Learning", "creator": "LaTeX with hyperref package"}}}