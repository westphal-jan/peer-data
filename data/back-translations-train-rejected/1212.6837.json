{"id": "1212.6837", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2012", "title": "Autonomously Learning to Visually Detect Where Manipulation Will Succeed", "abstract": "Visual features can help predict if a manipulation behavior will succeed at a given location. For example, the success of a behavior that flips light switches depends on the location of the switch. Within this paper, we present methods that enable a mobile manipulator to autonomously learn a function that takes an RGB image and a registered 3D point cloud as input and returns a 3D location at which a manipulation behavior is likely to succeed. Given a pair of manipulation behaviors that can change the state of the world between two sets (e.g., light switch up and light switch down), classifiers that detect when each behavior has been successful, and an initial hint as to where one of the behaviors will be successful, the robot autonomously trains a pair of support vector machine (SVM) classifiers by trying out the behaviors at locations in the world and observing the results. When an image feature vector associated with a 3D location is provided as input to one of the SVMs, the SVM predicts if the associated manipulation behavior will be successful at the 3D location. To evaluate our approach, we performed experiments with a PR2 robot from Willow Garage in a simulated home using behaviors that flip a light switch, push a rocker-type light switch, and operate a drawer. By using active learning, the robot efficiently learned SVMs that enabled it to consistently succeed at these tasks. After training, the robot also continued to learn in order to adapt in the event of failure.", "histories": [["v1", "Mon, 31 Dec 2012 08:39:14 GMT  (2661kb,D)", "http://arxiv.org/abs/1212.6837v1", "15 pages, 10 figures. Submitted to the Autonomous Robots Journal Special Issue \"Beyond Grasping - Modern Approaches for Dexterous Manipulation\""]], "COMMENTS": "15 pages, 10 figures. Submitted to the Autonomous Robots Journal Special Issue \"Beyond Grasping - Modern Approaches for Dexterous Manipulation\"", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV", "authors": ["hai nguyen", "charles c kemp"], "accepted": false, "id": "1212.6837"}, "pdf": {"name": "1212.6837.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Will Succeed", "Hai Nguyen", "Charles C. Kemp"], "emails": ["haidai@gmail.com", "charlie.kemp@bme.gatech.edu"], "sections": [{"heading": null, "text": "In this paper, we present methods that allow a mobile manipulator to autonomously learn a function that uses an RGB image and a registered 3D point cloud as input and returns a 3D location where a manipulation behavior is likely to be successful. Faced with two manipulation behaviors that can change the state of the world between two sets (e.g. turning light on and off), classifiers that recognize when each behavior has been successful, and an initial indication of where one of the behaviors will be successful, the robot autonomously trains a pair of support vector machine classifiers (SVM) by testing the behavior in locations around the world and observing the results. If an image trait associated with a 3D location is provided as input to one of the SVMs, the SVM predicts that the associated manipulation behavior at the 3D location will be successful. To evaluate our approach, we conduct experiments with a PR2 robot that works through an e-mail light switch in the e-mail to effectively switch the behavior of the garage."}, {"heading": "1 Introduction", "text": "It is a question of whether and how the people who are able to understand and understand the world, understand and understand the world in which they want to live, understand the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which, in which they, in which they, in which they, in which, in which they, in which, in which they, in which, in which they, in which, in which they, in which, in which they, in which, in which they, in which, in which, in which, in which they, in which they, in which, in which they, in which, in which they, in which they, in which, in which they, in which, in which, in which, live, in which, in which, in which they, in which they, in which, in which, in which they, in which, in which,"}, {"heading": "2 Related Work", "text": "In this section, we will discuss the relationship between our work and current learning methods, as well as work that has shown the effectiveness of using task-relevant clues for perception in human environment.The research presented in this paper builds on our previous workshop publication [47].2.1 Robot Learning Although the use of learning-based methods can produce powerful detectors, labeled training examples are often time-consuming and expensive to procure. Different robot learning methods such as imitation learning, interactive learning, and developmental learning [37,51] can be grouped according to their approach to data collection. We will now discuss various forms of robot learning, and in particular work where the robot learns with little or no human input after an initiation or training phase."}, {"heading": "2.1.1 Autonomously Learning to Act", "text": "In this context, it has to be said that most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2.1.2 Autonomously Learning to Perceive", "text": "In fact, it is the case that most people who are able to survive themselves are able to survive themselves, and this also applies to the way in which they are able to act in the way in which they are able to act in the way in which they are able, and in the way in which they are able, in the way in which they are able to survive themselves."}, {"heading": "3 Approach", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "3.3.1 Initialization", "text": "Our initialization process is motivated by the scenario in which a user would take the robot on a journey home and use a green laser pointer to point out 3D locations [46] and determine behaviors applicable to those locations. After this tour, the robot would later navigate back independently and learn to perform the behaviors robustly. [46] For this paper, we have implemented an initialization procedure that begins with the user navigating the robot in front of the device to be operated via a gamepad interface. Subsequently, the user names a first 3D location with the help of a green laser pointer [46] where exploration can begin. The robot samples 3D around that particular location (using a spherical mouse with a variance of 4 cm) and executes the behavioral pair in relation to it. After each execution of a behavior at a 3D location, the behavior verification function returns a label for success or failure. The stitch process continues until at least one failed attempt and the process continues to collect data."}, {"heading": "3.3.2 Training Procedure", "text": "Our training procedure is designed to emulate conditions that the robot would encounter when performing the task. After receiving a command, the robot navigates to the device so that it can execute the commanded behavior. Navigation and localization errors result in variations that can significantly reduce the performance of a behavior, such as the variation in the view of the robot. We illustrate the variation of the task by navigating in Figure 6. Our procedure begins with an active learning phase by instructing the robot to navigate in space and then navigate back to the device (see Figure 5).After navigation, our procedure begins with an active learning phase (see Figure 3).We capture this phase in Algorithm 1. The process begins with the capture of an RGB image and a registered cloud."}, {"heading": "3.3.3 Behavior Execution Procedure", "text": "To use this classifier, our robot navigates to the device using the 2D card that is stored during initialization, classifies 3D points in the view that it sees, finds the mode of positive classified points using the core density estimation [1], selects the 3D point in the point cloud that comes closest to this mode, and executes the associated behavior using the resulting 3D localization. If the behavior is not performed using this 3D localization, our procedure adds the associated image vector as a negative example to the dataset and retrains the classifier. This new example changes the decision boundary of the classifier with the resulting 3D localization. The robot then selects a new 3D localization with the originally calculated image attribute vectors. This continues until the behavior is successful."}, {"heading": "3.4.1 Active Learning Heuristic", "text": "Our training process iteratively builds a dataset that it uses to train the classifier. Before each attempt, the system selects the image attribute vector to be designated. To select the attribute vector, the system uses heuristics developed in [58] that select the attribute vector that comes closest to the decision limit of the existing SVM, on condition that it is closer to the limit than the support vectors of the SVM. The procedure converges when there are no more feature vectors that are closer to the decision limit than the support vectors. For each iteration i of our procedure, we define the dataset of the previous iteration as Di \u2212 1, the current set of support vectors as Xsvi = {xsv1,., xsvP}, the unlabeled image attribute vectors as Xqi = {x q 1,.,. vv., the SVM function that measures the distance (xi) of the system (q) to the decision limit (xi) of the q (vq)."}, {"heading": "3.4.2 Features", "text": "This year, it has come to the point where there is only one person who is able to move around the world."}, {"heading": "5 Evaluation", "text": "In the past, we have always put ourselves at the forefront when it came to getting the situation under control, \"he told the German Press Agency.\" We have it in our hands, \"he said.\" But we have it in our hands. \"\" We have it in our hands, \"he said.\" We don't have it lightly. \"\" We have it in our hands. \"\" We have it in our hands, \"he said.\" \"We have it in our hands.\" \"We have it in our hands.\" \"We have it in our hands,\" he said. \"We have it in our hands.\" \"We have it in our hands,\" he said. \"We have it in our hands.\" \"We have it in our hands.\" \"We have it.\" We have it. \"We have it.\" We have it. \"We have it.\" We have it. \"We have it.\" We have it. \"We have it.\" We have it. \"We have it.\" We have it. \"We have it.\""}, {"heading": "6 Future Work", "text": "Although we have selected a specific active learning frame, other frameworks may perform better. Furthermore, we currently assume that each new device is entirely new to the robot, but that many devices in a particular class share visual similarities. Data from other devices could provide lead-time and reduce the training required. Likewise, the structure of successful locations could be used across devices, even if they are visually different. For example, drawer fronts are often push-by, drawer centers are often pull-out, and light switch centers are often switchable, which could be useful information even if aspects of their appearance change dramatically."}, {"heading": "7 Discussion and Conclusions", "text": "In general, there are risks for a robot learning in a human environment, and an uninhibited learning system can get into situations that are dangerous for itself, the environment, or humans. We address this problem by limiting the robot to using some behavioral classes in parts of the home that users have called the safety of robot learning. In addition, the behaviors move the robot's arm flexibly and use haptic sensors to decide when to stop moving. On-site learning does not require a robot's data collection activities to stop after its training phase and can potentially continue as long as the robot remains on duty. Autonomous learning in a human environment is a promising area of research that gives robots methods to cope with devices they have not previously encountered, and many forms of real variation. We have presented methods that allow a mobile manipulator to learn autonomously, where manipulation attempts will succeed."}, {"heading": "Acknowledgments", "text": "We thank Aaron Bobick, Jim Rehg and Tucker Hermans for their contribution. We thank Willow Garage for using a PR2 robot, financial support and other support. This work was partially funded by NSF awards CBET-0932592, CNS-0958545 and IIS-1150157."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "Visual features can help predict if a manipulation behavior will succeed at a given location. For example, the success of a behavior that flips light switches depends on the location of the switch. Within this paper, we present methods that enable a mobile manipulator to autonomously learn a function that takes an RGB image and a registered 3D point cloud as input and returns a 3D location at which a manipulation behavior is likely to succeed. Given a pair of manipulation behaviors that can change the state of the world between two sets (e.g., light switch up and light switch down), classifiers that detect when each behavior has been successful, and an initial hint as to where one of the behaviors will be successful, the robot autonomously trains a pair of support vector machine (SVM) classifiers by trying out the behaviors at locations in the world and observing the results. When an image feature vector associated with a 3D location is provided as input to one of the SVMs, the SVM predicts if the associated manipulation behavior will be successful at the 3D location. To evaluate our approach, we performed experiments with a PR2 robot from Willow Garage in a simulated home using behaviors that flip a light switch, push a rocker-type light switch, and operate a drawer. By using active learning, the robot efficiently learned SVMs that enabled it to consistently succeed at these tasks. After training, the robot also continued to learn in order to adapt in the event of failure. Healthcare Robotics Lab, Georgia Institute of Technology Atlanta, GA, USA E-mail: haidai@gmail.com E-mail: charlie.kemp@bme.gatech.edu Fig. 1 Left: Willow Garage PR2 operating a drawer, light switch and rocker switch using learned detector that detects regions where manipulation will succeed. Right: Results from learned detectors during execution.", "creator": "LaTeX with hyperref package"}}}