{"id": "1212.6922", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Dec-2012", "title": "Training a Functional Link Neural Network Using an Artificial Bee Colony for Solving a Classification Problems", "abstract": "Artificial Neural Networks have emerged as an important tool for classification and have been widely used to classify a non-linear separable pattern. The most popular artificial neural networks model is a Multilayer Perceptron (MLP) as it is able to perform classification task with significant success. However due to the complexity of MLP structure and also problems such as local minima trapping, over fitting and weight interference have made neural network training difficult. Thus, the easy way to avoid these problems is to remove the hidden layers. This paper presents the ability of Functional Link Neural Network (FLNN) to overcome the complexity structure of MLP by using single layer architecture and propose an Artificial Bee Colony (ABC) optimization for training the FLNN. The proposed technique is expected to provide better learning scheme for a classifier in order to get more accurate classification result", "histories": [["v1", "Mon, 31 Dec 2012 16:40:50 GMT  (233kb)", "http://arxiv.org/abs/1212.6922v1", "6 pages, 3 figures, 4 tables"]], "COMMENTS": "6 pages, 3 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["yana mazwin mohmad hassim", "rozaida ghazali"], "accepted": false, "id": "1212.6922"}, "pdf": {"name": "1212.6922.pdf", "metadata": {"source": "CRF", "title": "Training a Functional Link Neural Network Using an Artificial Bee Colony for Solving a Classification Problems", "authors": ["Yana Mazwin Mohmad Hassim", "Rozaida Ghazali"], "emails": [], "sections": [{"heading": null, "text": "The most popular model of artificial neural networks is a Multilayer Perceptron (MLP), which is capable of performing classification tasks with considerable success. However, due to the complexity of the MLP structure and also problems such as local minima-trapping, overfitting and weight interference, it is difficult to avoid these problems by removing the hidden layers. In this paper, the ability of Functional Link Neural Network (FLNN) to overcome the complexity structure of MLP by using a single-layer architecture and to propose an artificial bee colony (ABC) to form the FLNN is presented. The proposed technique is intended to provide a better learning scheme for a classifier to achieve more accurate classification outcomes. Index terms - Neural networks, functional link Neural Network, Learning, Artificial Bee Colony - - - - - - - - - - - -"}, {"heading": "1 INTRODUCTION", "text": "Artificial Neural Networks (ANNs), especially the Multilayer Perceptron (MLP), are known to successfully perform a variety of real-world classification tasks, particularly in industry, business and academia. Artificial Neural Networks (ANNs), in particular the Multilayer Perceptron (MLP), are capable of generating complex mapping between the input and output space when performing arbitrarily complex nonlinear decision boundaries. To perform a classification task, ANNs must be \"trained\" so that the network can generate the desired input-output mapping. In the training phase, sample data are transmitted to the network and the connecting weights of the network are adjusted by using a learning algorithm. The purpose of weight adjustment is for the network to \"learn\" so that the network adapts to the given training data. The most common architecture of ANNs is the multi-layered feedback network (MLP)."}, {"heading": "2 RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Artificial Neural Network", "text": "Artificial Neural Networks (ANNs) are information processing models inspired by the way the human brain processes information. ANNs require knowledge through a learning process, while the strength of the interneuron connection, known as synaptic weights, is used to store knowledge [7]. Therefore, Neural Networks with these capabilities provides a suitable solution to pattern recognition or data classification problems. One of the most common types of Neural Networks is the Multilayer Perceptron (MLP). It has one or more hidden layers between the input and output layers. Figure 1 illustrates the layout of the MLP with a single hidden layer. The function of hidden neurons is to give the ANNs the ability to process nonlinear input-output mapping. By adding another hidden layer, the network is able to extract higher order statistics, which is especially valuable when the input layer is large."}, {"heading": "2.2 Higher Order Neural Network", "text": "Higher order neural networks (HONNs) are another type of neural networks with extended input space in their single-layer feed architecture. HONNs contain summing and product units that multiply their inputs. These high order terms or product units can increase the information capacity for the input characteristics and provide nonlinear decision boundaries to provide a better classification capability than the linear neuron [8]. A major advantage of HONNs is that only one layer of bearing weight is required to be non-linear separable, as opposed to the typical MLP or push neural network [9]. Although most models of neural networks have a common goal in performing functional mappings, different network architectures can vary considerably in their ability to handle different types of problems. For some tasks, the architecture of higher order of some inputs or activations may be appropriate to provide a good representation of the learning network problem, especially if they cannot contain HONs, as they are highly complex."}, {"heading": "2.2.1 Functional Link Neural Network", "text": "Functional Link Neural Network (FLNN) is a class of higher-order neural networks (HONNs) that use a higher combination of their inputs [5, 6]. It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel balancing [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24]. In this paper we would discuss the FLNN for the classification task. FLNN, however, is much more modest than MLP because it has a single-layer network compared to MLP, but is still capable of performing a non-linear separable classification task [23, 24]. Basically, the FLNN architecture is a flat network without any hidden layer, making the learning algorithm in the network less complicated [9]. In FLNN, the input vector is extended with a successively improved representation of the nodes."}, {"heading": "2.3 Artificial Bee Colony", "text": "The artificial bee algorithm is an optimization tool that provides a population-based search method [30]. The ABC algorithm simulates the intelligent feeding behavior of a swarm of honeybees to solve the multidimensional and multimodal optimization problem [31]. In this population-based search method, each population called ABC positions consists of the artificial bees, while the goal of the bee is to discover the places of the food sources with the highest nectar content and finally the places with the highest nectar content. In this model, the colony of artificial bees consists of three groups: the employed bees, observers and scouts [32]. For each food source, there is only one artificially employed bee and finally the ones with the highest nectar content. The number of employed bees in the colony is equal to the number of food sources around the hive. Employed bees return to their food source and come back with three information about the food sources: 1) the direction of the problem and then dance around it."}, {"heading": "3 PROPOSED TRAINING SCHEME", "text": "Inspired by the robustness and flexibility offered by the population-based optimization algorithm, we proposed the implementation of the ABC algorithm as a learning scheme to overcome the disadvantages caused by back propagation in FLNN training. The proposed flow chart is shown in Figure 4. In the initial process, the FLNN architecture (weight and preload) is converted into objective function along with the training data set. This objective function is then fed to the ABC algorithm to search for the optimal weight parameters. Weight changes are then adjusted by the ABC algorithm based on the error calculation (difference between actual and expected results). Based on the ABC algorithm, each bee presents the solutions with a specific set of weight vectors. The ABC algorithm [35] used for training the FLNN is summarized as follows: 1. Cycle 0: 2. Cycle Nxive."}, {"heading": "4 SIMULATION RESULT", "text": "To evaluate the performance of the proposed learning scheme FLNN-ABC for the classification problem, the simulation experiments were performed on a 2.30 GHz Core i5-2410M Intel CPU with 8.0 GB RAM in a 64-bit operating system. Comparison of standard BP education and ABC algorithms is based on the simulation results implemented in Matlab 2010b. In this thesis we consider 4 benchmark classification problems, Breast Cancer Wisconsin, PIMA Indian Diabetes and BUPA-Liver Disorder.During the experiment, simulations were performed on the training of MLP architecture with backpropagation algorithms."}, {"heading": "5 CONCLUSION", "text": "The experiment showed that FLNN-ABC fulfills the classification task quite well. In the case of Krebs, PIMA and BUPA, the simulation results show that the proposed ABC algorithm can successfully train the FLNN to solve classification problems with better accuracy on invisible data. The importance of this research is to provide an alternative training program for the formation of the Functional Link Neural Network instead of the traditional BP learning algorithm."}], "references": [{"title": "Neural networks for classification: a survey", "author": ["G.P. Zhang"], "venue": "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, vol. 30, pp. 451-462, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Artificial neural networks classification and clustering of methodologies and applications \u2013 literature analysis from 1995 to 2005", "author": ["S.-H. Liao", "C.-H. Wen"], "venue": "Expert Systems with Applications, vol. 32, pp. 1-11, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "A functional link artificial neural network for adaptive channel equalization", "author": ["J.C. Patra", "R.N. Pal"], "venue": "Signal Processing, vol. 43, pp. 181- 195, 1995.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "A comprehensive survey on functional link neural networks and an adaptive PSO\u2013BP learning for CFLNN", "author": ["S. Dehuri", "S.-B. Cho"], "venue": "Neural Computing & Applications vol. 19, pp. 187-205, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Functional-link net computing: theory, system architecture, and functionalities", "author": ["Y.H. Pao", "Y. Takefuji"], "venue": "Computer, vol. 25, pp. 76-79, 1992.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Adaptive pattern recognition and neural networks", "author": ["Y.H. Pao"], "venue": "1989.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "Neural Networks: A Comprehensive Foundation", "author": ["S. Haykin"], "venue": "The Knowledge Engineering Review vol. 13, pp. 409-412, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "A New Higher-order Binary-input Neural Unit: Learning and Generalizing Effectively via Using Minimal Number of Monomials ", "author": ["E. Sahin"], "venue": "Master, Middle East Technical University of Ankara,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Functional Link Artificial Neural Network for Classification Task in Data Mining", "author": ["B.B. Misra", "S. Dehuri"], "venue": "Journal of Computer Science, vol. 3, pp. 948-955, 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Regression neural network for error correction in foreign exchange forecasting and trading", "author": ["A.-S. Chen", "M.T. Leung"], "venue": "Computers &amp; Operations Research, vol. 31, pp. 1049-1068, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Nonlinear dynamic system identification using Legendre neural network", "author": ["J.C. Patra", "C. Bornand"], "venue": "Neural Networks (IJCNN), The 2010 International Joint Conference on, 2010, pp. 1-7.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonlinear dynamic system identification using Chebyshev functional link artificial neural networks", "author": ["J.C. Patra", "A.C. Kot"], "venue": "Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, vol. 32, pp. 505-511, 2002.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "System Identification Using Optimally Designed  Functional Link Networks via a Fast Orthogonal Search Technique", "author": ["H.M. Abbas"], "venue": "JOURNAL OF COMPUTERS, vol. 4, FEBRUARY 2009 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Individual particle optimized functional link neural network for real time identification of nonlinear dynamic systems", "author": ["S. Emrani"], "venue": "Industrial Electronics and Applications (ICIEA), 2010 the 5th IEEE Conference on, 2010, pp. 35-40.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Improved Identification of Nonlinear MIMO Plants using New Hybrid FLANN-AIS Model", "author": ["S.J. Nanda"], "venue": "Advance Computing Conference, 2009. IACC 2009. IEEE International, 2009, pp. 141-146.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Application of functional link neural network to HVAC thermal dynamic system identification", "author": ["J. Teeter", "C. Mo-Yuen"], "venue": "Industrial Electronics, IEEE Transactions on, vol. 45, pp. 170-176, 1998.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "A combined neural network approach for texture classification", "author": ["P.P. Raghu"], "venue": "Neural Networks, vol. 8, pp. 975-987, 1995.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1995}, {"title": "A comparative study of three artificial neural networks for the detection and classification of gear faults ", "author": ["I.-A. Abu-Mahfouz"], "venue": "International Journal of General Systems", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Image classification in remote sensing using functional link neural networks", "author": ["L.M. Liu"], "venue": "Image Analysis and Interpretation, 1994., Proceedings of the IEEE Southwest Symposium on, 1994, pp. 54-58.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "Evolutionarily optimized features in functional link neural network for classification", "author": ["S. Dehuri", "S.-B. Cho"], "venue": "Expert Systems with Applications, vol. 37, pp. 4379-4391, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "The functional link net in structural pattern recognition", "author": ["M. Klaseen", "Y.H. Pao"], "venue": "TENCON 90. 1990 IEEE Region 10 Conference on Computer and Communication Systems, 1990, pp. 567-571 vol.2.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1990}, {"title": "Unconstrained word-based approach for off-line script recognition using density-based random-vector functional-link net", "author": ["G.H. Park", "Y.H. Pao"], "venue": "Neurocomputing, vol. 31, pp. 45-65, 2000.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Development and performance evaluation of FLANN based model for forecasting of stock markets", "author": ["R. Majhi"], "venue": "Expert Systems with Applications, vol. 36, pp. 6800-6808, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Dynamic Ridge Polynomial Neural Network: Forecasting the univariate non-stationary and stationary trading signals", "author": ["R. Ghazali"], "venue": "Expert Systems with Applications, vol. 38, pp. 3765-3776, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Pattern classification with Chebyshev neural network", "author": ["A. Namatame", "N. Veda"], "venue": "International Jounal of Neural Network, vol. 3, pp. 23\u2013 31, 1992.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1992}, {"title": "Finding functional links for neural networks by evolutionary computation", "author": ["S. Haring", "J. Kok"], "venue": "In: Van de Merckt Tet al (eds) BENELEARN1995, proceedings of the fifth Belgian\u2013Dutch conference on machine learning, Brussels, Belgium, 1995.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1995}, {"title": "Feature selection for neural networks through functional links found by evolutionary computation", "author": ["S. Haring"], "venue": "In: ILiu X et al (eds) Adavnces in intelligent data analysis (IDA-97). LNCS 1280, pp. 199\u2013210, 1997.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1997}, {"title": "Evolution of functional link networks", "author": ["A. Sierra"], "venue": "Evolutionary Computation, IEEE Transactions on, vol. 5, pp. 54-65, 2001.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2001}, {"title": "Genetic Feature Selection for Optimal Functional Link Artificial Neural Network in Classification", "author": ["S. Dehuri"], "venue": "presented at the Proceedings of the 9th International Conference on Intelligent Data Engineering and Automated Learning, Daejeon, South Korea, 2008.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "The Bees Algorithm", "author": ["D. Pham"], "venue": "Manufacturing Engineering Centre, Cardiff University, UK,2005.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "An Idea Based on Honey Bee Swarm for Numerical Optimization", "author": ["D. Karaboga"], "venue": "Erciyes University, Engineering Faculty, Computer Science Department, Kayseri/Turkiye2005.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "On the performance of artificial bee colony (ABC) algorithm", "author": ["D. Karaboga", "B. Basturk"], "venue": "Elsevier Applied Soft Computing, vol. 8, pp. 687-697, 2007.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2007}, {"title": "A modified Artificial Bee Colony algorithm for real-parameter optimization", "author": ["B. Akay", "D. Karaboga"], "venue": "Information Sciences, vol. In Press, Corrected Proof, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "A novel clustering approach: Artificial Bee Colony (ABC) algorithm", "author": ["D. Karaboga", "C. Ozturk"], "venue": "Applied Soft Computing, vol. 11, pp. 652-657, 2011.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "RTIFICIAL Neural Networks have been known to be successfully applied to a variety of real world classification tasks especially in industry, business and science [1, 2].", "startOffset": 162, "endOffset": 168}, {"referenceID": 1, "context": "RTIFICIAL Neural Networks have been known to be successfully applied to a variety of real world classification tasks especially in industry, business and science [1, 2].", "startOffset": 162, "endOffset": 168}, {"referenceID": 2, "context": "However, due to its multi-layered structure, the training speeds are typically much slower as compared to other single layer feedforward networks [3].", "startOffset": 146, "endOffset": 149}, {"referenceID": 3, "context": "Problems such as local minima trapping, overfitting and weight interference also make the network training in MLP become challenging [4].", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "Hence, Pao [5] has introduce an alternative approach named Functional Link Neural Network (FLNN) in avoiding these problems.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "This approach removes the hidden layer from the ANN architecture to help in reducing the neural architectural complexity and provides them with an enhancement representation of input nodes for the network to be able to perfom a non-linear separable classification task[5, 6].", "startOffset": 268, "endOffset": 274}, {"referenceID": 5, "context": "This approach removes the hidden layer from the ANN architecture to help in reducing the neural architectural complexity and provides them with an enhancement representation of input nodes for the network to be able to perfom a non-linear separable classification task[5, 6].", "startOffset": 268, "endOffset": 274}, {"referenceID": 6, "context": "ANNs required knowledge through a learning process while the interneuron connection strength known as synaptic weights are used to store knowledge [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 6, "context": "By adding one more hidden layer, the network is able to extract higher order statistics, which is particularly valuable when the size of the input layer is large [7].", "startOffset": 162, "endOffset": 165}, {"referenceID": 7, "context": "These high order terms or product units can increase the information capacity for the input features and provides nonlinear decision boundaries to give a better classification capability than the linear neuron [8].", "startOffset": 210, "endOffset": 213}, {"referenceID": 8, "context": "A major advantage of HONNs is that only one layer of trainable weight is needed to achieve nonlinear separable, unlike the typical MLP or feed-forward neural network [9].", "startOffset": 166, "endOffset": 169}, {"referenceID": 9, "context": "HONNs are needed because ordinary feed-forward network like MLP cannot avoid the problem of slow learning, especially when involving highly complex nonlinear problems [10].", "startOffset": 167, "endOffset": 171}, {"referenceID": 4, "context": "1 Functional Link Neural Network Functional Link Neural Network (FLNN) is a class of Higher Order Neural Networks (HONNs) that utilize higher combination of its inputs [5, 6].", "startOffset": 168, "endOffset": 174}, {"referenceID": 5, "context": "1 Functional Link Neural Network Functional Link Neural Network (FLNN) is a class of Higher Order Neural Networks (HONNs) that utilize higher combination of its inputs [5, 6].", "startOffset": 168, "endOffset": 174}, {"referenceID": 5, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 22, "endOffset": 25}, {"referenceID": 10, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 108, "endOffset": 115}, {"referenceID": 11, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 108, "endOffset": 115}, {"referenceID": 12, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 108, "endOffset": 115}, {"referenceID": 13, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 108, "endOffset": 115}, {"referenceID": 14, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 108, "endOffset": 115}, {"referenceID": 15, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 108, "endOffset": 115}, {"referenceID": 2, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 138, "endOffset": 141}, {"referenceID": 16, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 158, "endOffset": 165}, {"referenceID": 17, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 158, "endOffset": 165}, {"referenceID": 18, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 158, "endOffset": 165}, {"referenceID": 19, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 158, "endOffset": 165}, {"referenceID": 20, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 187, "endOffset": 195}, {"referenceID": 21, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 187, "endOffset": 195}, {"referenceID": 22, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 211, "endOffset": 219}, {"referenceID": 23, "context": "It was created by Pao [6] and has been successfully used in many applications such as system identification [11-16], channel equalization [3], classification [17-20], pattern recognition [21, 22] and prediction [23, 24].", "startOffset": 211, "endOffset": 219}, {"referenceID": 8, "context": "The FLNN architecture is basically a flat network without any hidden layer which has make the learning algorithm used in the network less complicated [9].", "startOffset": 150, "endOffset": 153}, {"referenceID": 4, "context": "In FLNN, the input vector is extended with a suitably enhanced representation of the input nodes, thereby artificially increasing the dimension of the input space [5, 6].", "startOffset": 163, "endOffset": 169}, {"referenceID": 5, "context": "In FLNN, the input vector is extended with a suitably enhanced representation of the input nodes, thereby artificially increasing the dimension of the input space [5, 6].", "startOffset": 163, "endOffset": 169}, {"referenceID": 5, "context": "Pao [6], Patra [12], Namatamee [25] has demonstrated that this architecture is very effective for classification task.", "startOffset": 4, "endOffset": 7}, {"referenceID": 11, "context": "Pao [6], Patra [12], Namatamee [25] has demonstrated that this architecture is very effective for classification task.", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": "Pao [6], Patra [12], Namatamee [25] has demonstrated that this architecture is very effective for classification task.", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "In most previous researches, the learning algorithm used for training the FLNN is the Backpropagation (BP) [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "TABLE 1 previous research on FLNN Training[4]", "startOffset": 42, "endOffset": 45}, {"referenceID": 25, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 48, "endOffset": 52}, {"referenceID": 17, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 96, "endOffset": 100}, {"referenceID": 8, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 125, "endOffset": 128}, {"referenceID": 28, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 147, "endOffset": 151}, {"referenceID": 23, "context": "[26] ClFLNN Pseudoinverse [27] EFLN BP-learning [28] ClaFLNN Genetic Algorithm with BP-learning [18] GFLNN Adaptive learning [9] FLANN BP-learning [29] ClasFLNN BP-learning [24] FLNN BP-learning", "startOffset": 173, "endOffset": 177}, {"referenceID": 3, "context": "Third, the convergence behavior of the BP algorithm depends on the choices of initial values of the network connection weights as well as the parameters in the algorithm such as the learning rate and momentum [4].", "startOffset": 209, "endOffset": 212}, {"referenceID": 29, "context": "3 Artificial Bee Colony The Artificial Bees Algorithm is an optimization tool, which provides a population-based search procedure [30].", "startOffset": 130, "endOffset": 134}, {"referenceID": 30, "context": "The ABC algorithm simulates the intelligent foraging behavior of a honey bee swarm for solving multidimensional and multimodal optimization problem [31].", "startOffset": 148, "endOffset": 152}, {"referenceID": 31, "context": "In this model, the colony of artificial bees consists of three groups: which are employed bees, onlookers and scouts [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 30, "context": "This foraging process is called local search method as the method of choosing the food source is depend on the on the experience of the employed bees and their nest mates [31].", "startOffset": 171, "endOffset": 175}, {"referenceID": 30, "context": "If the nectar amount of a new source is higher than that of the previous one in their memory, they memorize the new position and forget the previous one [31].", "startOffset": 153, "endOffset": 157}, {"referenceID": 30, "context": "Several studies done by [31-33] has described that the Artificial Bee Colony algorithm is very simple, flexible and robust as compared to the existing population-based optimization algorithms: Genetic Algorithm (GA) , Differential Evolution (DE) and Particle Swarm Optimization (PSO) in solving numerical optimization problem.", "startOffset": 24, "endOffset": 31}, {"referenceID": 31, "context": "Several studies done by [31-33] has described that the Artificial Bee Colony algorithm is very simple, flexible and robust as compared to the existing population-based optimization algorithms: Genetic Algorithm (GA) , Differential Evolution (DE) and Particle Swarm Optimization (PSO) in solving numerical optimization problem.", "startOffset": 24, "endOffset": 31}, {"referenceID": 32, "context": "Several studies done by [31-33] has described that the Artificial Bee Colony algorithm is very simple, flexible and robust as compared to the existing population-based optimization algorithms: Genetic Algorithm (GA) , Differential Evolution (DE) and Particle Swarm Optimization (PSO) in solving numerical optimization problem.", "startOffset": 24, "endOffset": 31}, {"referenceID": 33, "context": "As in classification task in data mining, ABC algorithm also provide a good performance in gathering data into classes [34].", "startOffset": 119, "endOffset": 123}, {"referenceID": 0, "context": "where k is a solution in the neighbourhood of i, \u03a6 is a random number in the range [-1,1] and evaluate them.", "startOffset": 83, "endOffset": 89}, {"referenceID": 0, "context": "Parameters range [-1,1] [-1,1] [-10,10]", "startOffset": 17, "endOffset": 23}, {"referenceID": 0, "context": "Parameters range [-1,1] [-1,1] [-10,10]", "startOffset": 24, "endOffset": 30}, {"referenceID": 9, "context": "Parameters range [-1,1] [-1,1] [-10,10]", "startOffset": 31, "endOffset": 39}], "year": 2012, "abstractText": "Artificial Neural Networks have emerged as an important tool for classification and have been widely used to classify a non-linear separable pattern. The most popular artificial neural networks model is a Multilayer Perceptron (MLP) as is able to perform classification task with significant success. However due to the complexity of MLP structure and also problems such as local minima trapping, over fitting and weight interference have made neural network training difficult. Thus, the easy way to avoid these problems is to remove the hidden layers. This paper presents the ability of Functional Link Neural Network (FLNN) to overcome the complexity structure of MLP by using single layer architecture and propose an Artificial Bee Colony (ABC) optimization for training the FLNN. The proposed technique is expected to provide better learning scheme for a classifier in order to get more accurate classification result.", "creator": "PScript5.dll Version 5.2.2"}}}