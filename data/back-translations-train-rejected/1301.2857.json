{"id": "1301.2857", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Jan-2013", "title": "SpeedRead: A Fast Named Entity Recognition Pipeline", "abstract": "Online content analysis employs algorithmic methods to identify entities in unstructured text. Both machine learning and knowledge-base approaches lie at the foundation of contemporary named entities extraction systems. However, the progress in deploying these approaches on web-scale has been been hampered by the computational cost of NLP over massive text corpora. We present SpeedRead (SR), a named entity recognition pipeline that runs at least 10 times faster than Stanford NLP pipeline. This pipeline consists of a high performance Penn Treebank- compliant tokenizer, close to state-of-art part-of-speech (POS) tagger and knowledge-based named entity recognizer.", "histories": [["v1", "Mon, 14 Jan 2013 04:01:25 GMT  (75kb,D)", "http://arxiv.org/abs/1301.2857v1", "Long paper at COLING 2012"]], "COMMENTS": "Long paper at COLING 2012", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rami al-rfou'", "steven skiena"], "accepted": false, "id": "1301.2857"}, "pdf": {"name": "1301.2857.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Steven Skiena"], "emails": ["skiena}@cs.stonybrook.edu"], "sections": [{"heading": null, "text": "Key concepts: tokenization, part of the language, recognition of designated units, NLP pipelines.ar Xiv: 130 1.28 57v1 [cs.C"}, {"heading": "1 Introduction", "text": "It is indeed the case that we are able to go in search of a solution."}, {"heading": "1.1 Experimental Setup", "text": "All the experiments presented in this article were carried out on a single computer running the i7 Intel 920 processor at 2.67 GHz, the operating system used is Ubuntu 11.10. Execution time is the sum of the time periods calculated by the Linux command time. Speeds are calculated by averaging the execution time of five runs without taking into account initialization times."}, {"heading": "2 Related Work", "text": "In fact, it is the case that most of them are able to move into another world, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they live."}, {"heading": "3 Tokenizer", "text": "The first task an NLP pipeline has to perform is tokenization and sentence segmentation of TB texts (Webster and Kit, 1992); the second task is to identify tokens in the text. Tokens are the basic units that do not need to be processed in subsequent stages; however, part of the complexity of tokenization stems from the fact that the definition of what a token is depends on the application being developed. Punctuation brings a different level of ambiguity that can play different roles in the text. For example, we do not need to divide a number like 1,000.54 into more units, while we need a comma-separated list of words. On the other hand, tokenization is important as it reduces the size of the word and improves the accuracy of the tagger by producing similar vocabularies."}, {"heading": "3.1 Sentence Segmentation", "text": "While PTB provides a set of tokenization rules, its tokenizer assumes that the sentences are already segmented, which is done manually. SpeedRead's sentence segmentation uses the same rules as Stanford's tokenizer. A period is the end of a sentence, unless it is part of an acronym or abbreviation. The list of rules for recognizing these acronyms and abbreviations comes from Stanford's tokenizer. Any quotations or brackets that follow the end of the sentence will be part of that sentence. Running SpeedRead's sentence segmentation on Reuters RCV1 generated 7.8 million sentences, while Stanford Tokenizer generated 8.2 million sentences. Tokenizer Accuracy PTB Sed Script 100.0% Stanford Tokenizer 99.7% SpeedRead 99.0% White Space 0.0% Table 4: Accuracy of the Tokenizers in the Treebank Stanford Script 100.0% Stanford Tokenizer 100.0% Tokenizer 100.0%"}, {"heading": "4 Part of Speech Tagger (POS)", "text": "Previous work to solve the problem of POS tagging relied on lexical and local characteristics using maximum entropy models (Toutanova and Manning, 2000). Later, more advanced models used context words and their predicted tags (Toutanova et al., 2003) to achieve greater accuracy. As the marking of POS tags is a sequence tagging problem, modelling the sequence into a Maximum Entropy Markov Model (MEMM) or a Conditional Random Fields Model (CRF) seems to be the preferred option. The probability of each tag is then calculated using a loglinear model with characteristics containing sufficiently large context words and their already calculated tags, transforming each instance of the problem into a large vector of characteristics whose calculation is expensive. Then, the sequence of the vectors is passed on to a graphical model to calculate the probability of each class using inference rules."}, {"heading": "4.1 Algorithm", "text": "To understand such influences, we used a Stanford POS tag (left 3 words model (L3W); trained at the Wall Street Journal (WSJ), sections 1-18) to calculate the following dictionaries: \u2022 The most common POS tag of each token (Uni). \u2022 The most common POS tag of each token results in different precision / callback values compared to the previous POS tag (Bi). (Lee et al., 2011) shows that the use of sieves is the solution to combine multiple rules / dictionaries, as the previous and next POS tags (Tri) are used. In a sieve algorithm, there are a number of rules that follow each other."}, {"heading": "4.2 Results", "text": "Table 5 shows the performance of different algorithms that run on different sections of the PTB. Stanford and SENNA models use the sections 1-18, 19-21, 22-24 for the training, development and testing of data sets, respectively. Despite the simplicity of our algorithm, it achieves relatively high accuracy on the different data sets that are available. Applying context-sensitive rules, SpeedRead implemented with Seven 1-5 (SR [Tri / Bi / Uni]) shows an improvement in accuracy of around 2.85% compared to using only unigrams, SpeedRead with Sieves 1.4-5 (SR [Uni]). To be sure that our algorithm is robust enough and does not exceed the dataset, we re-calculated the dictionaries by running SENNA POS Tagger (Collobert et al., 2011) over Reuters RCV1 Corpus and the results were comparable.Tables 5 and 6 show the trade between accuracy and speed."}, {"heading": "4.3 Error Analysis", "text": "The most common errors are functional words, like that, more that have multiple roles in the language. This confirms some of the conclusions reported by (Manning, 2011). Figure 3 shows that less than 10% of misunderstood words are responsible for slightly more than 50% of the errors. In terms of unknown words, the only part of the tagger that generalizes about invisible tokens is. Regular expressions are not comprehensive enough to achieve high accuracy. Therefore, we plan to implement another backoff phase for the frequent invisible words, in which we accumulate the sentences that contain those words after sufficient amount of text is processed and then run Stanford / SENNA taggers over those sentences to calculate the confusion matrix of most ambiguous tags."}, {"heading": "5 Named Entity Recognition (NER)", "text": "CONLL 2000 / 2003 (Tjong Kim Sang and De Meulder, 2003) are some of the common tasks dealing with the task of designated entity recognition. We use CONLL 2003's definition of designated entity recognition and classification task. CONLL 2003 defines the piece boundaries of an entity by using IOB tags, where I-TYPE means that the word is within an entity, B-TYPE means the beginning of a new entity when the previous token is part of an entity of the same type, and O for anything that is not part of an entity. For classification, the task defines four different types: person (PER), organization (ORG), location (LOC) and miscellaneous (see Figure 4)."}, {"heading": "5.1 Chunking", "text": "We rely on the POS tags of phrase words to identify the phrase that constitutes an entity. A word is considered part of an entity: (1) if it is a demonym (our compiled list contains 320 nationalities), (2) if one of the following conjunctions {&, de, of} appears in the middle of an entity phrase, or (3) if its POS tag is NNP (S), unless it belongs to one of these sentences: \u2022 Days of the week and months and their abbreviations. \u2022 Sports (our compiled list contains 182 names). \u2022 Job and profession titles (our compiled list contains 314 titles). \u2022 Single Capital Letters. These sentences are associated with freebase.CONLL Dataset showing a strong correlation between POS tags NNP (S) and the words that are part of the entities that are phrases of the entities."}, {"heading": "5.1.1 Results", "text": "Table 8 shows the F1 score of the chunking phase using different taggers to generate the POS tags. This score is calculated based on the chunking tags of the words. I and B tags are considered a class, while O is left as it is. Table 8 shows that using better POS taggers does not necessarily lead to better results. I and B tags are considered to be of sufficient quality for the chunking phase. SENNA and SpeedRead POS taggers work better for the detection phase because they are more aggressive and assign the NNP tag to each capitalized word."}, {"heading": "5.1.2 Error Analysis", "text": "The most common error class in the chunking phase are titles such as \"RESULTS, DIVISION, CONFERENCE, PTS, PCT.\" These words seem to confuse the POS tagger. Another source of confusion for the POS tagger are the words \"Women, Men\"; such words appear in the name of sports, so they are assigned an NNP tag. As expected, all numbers that are part of units are not recognized. Conjunction words are the second major error class. (Pawel and Robert, 2007) show that conjunctions that occur in the middle of unit phrases are difficult to recognize and require a special classification task. As most occurrences are part of units and the reverse holds true for the former and we decided to exclude the latter."}, {"heading": "5.2 Classification", "text": "This year, as never before, it will be able to retaliate, to retaliate."}], "references": [{"title": "Natural Language Processing with Python", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": "O\u2019Reilly Media.", "citeRegEx": "Bird et al\\.,? 2009", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "Deep learning for efficient discriminative parsing", "author": ["R. Collobert"], "venue": "International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Collobert,? 2011", "shortCiteRegEx": "Collobert", "year": 2011}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, ICML \u201908, pages 160\u2013167, New York, NY, USA. ACM.", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Tagme: on-the-fly annotation of short text fragments (by wikipedia entities)", "author": ["P. Ferragina", "U. Scaiella"], "venue": "Proceedings of the 19th ACM international conference on Information and knowledge management, CIKM \u201910, pages 1625\u20131628, New York, NY, USA. ACM.", "citeRegEx": "Ferragina and Scaiella,? 2010", "shortCiteRegEx": "Ferragina and Scaiella", "year": 2010}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["J.R. Finkel", "T. Grenager", "C. Manning"], "venue": "In ACL, pages 363\u2013370.", "citeRegEx": "Finkel et al\\.,? 2005", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Sentence boundary detection and the problem with the u.s. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, NAACL-Short", "author": ["D. Gillick"], "venue": "Association for Computational Linguistics", "citeRegEx": "Gillick,? \\Q2009\\E", "shortCiteRegEx": "Gillick", "year": 2009}, {"title": "Named entity recognition with character-level models", "author": ["D. Klein", "J. Smarr", "H. Nguyen", "C.D. Manning"], "venue": "Daelemans, W. and Osborne, M., editors, Proceedings of CoNLL-2003, pages 180\u2013183. Edmonton, Canada.", "citeRegEx": "Klein et al\\.,? 2003", "shortCiteRegEx": "Klein et al\\.", "year": 2003}, {"title": "Stanford\u2019s multi-pass sieve coreference resolution system at the conll-2011 shared task", "author": ["H. Lee", "Y. Peirsman", "A. Chang", "N. Chambers", "M. Surdeanu", "D. Jurafsky"], "venue": "Proceedings of the CoNLL-2011 Shared Task.", "citeRegEx": "Lee et al\\.,? 2011", "shortCiteRegEx": "Lee et al\\.", "year": 2011}, {"title": "Part-of-speech tagging from 97% to 100%: Is it time for some linguistics? In Gelbukh, A", "author": ["C.D. Manning"], "venue": "F., editor, CICLing (1), volume 6608 of Lecture Notes in Computer Science, pages 171\u2013189. Springer.", "citeRegEx": "Manning,? 2011", "shortCiteRegEx": "Manning", "year": 2011}, {"title": "An effective, low-cost measure of semantic relatedness obtained from wikipedia links", "author": ["D. Milne", "I.H. Witten"], "venue": "In Proceedings of AAAI 2008.", "citeRegEx": "Milne and Witten,? 2008", "shortCiteRegEx": "Milne and Witten", "year": 2008}, {"title": "A survey of named entity recognition and classification", "author": ["D. Nadeau", "S. Sekine"], "venue": "Lingvisticae Investigationes, 30(1):3\u201326.", "citeRegEx": "Nadeau and Sekine,? 2007", "shortCiteRegEx": "Nadeau and Sekine", "year": 2007}, {"title": "Faster and smaller n-gram language models", "author": ["A. Pauls", "D. Klein"], "venue": "Lin, D., Matsumoto, Y., and Mihalcea, R., editors, ACL, pages 258\u2013267. The Association for Computer Linguistics.", "citeRegEx": "Pauls and Klein,? 2011", "shortCiteRegEx": "Pauls and Klein", "year": 2011}, {"title": "Handling conjunctions in named entities", "author": ["M. Pawel", "D. Robert"], "venue": "Lingvisticae Investigationes, 30(1):49\u201368. Schafer, F.-R. (2012). Quex - fast universal lexical analyzer generator. http://quex. sourceforge.net.", "citeRegEx": "Pawel and Robert,? 2007", "shortCiteRegEx": "Pawel and Robert", "year": 2007}, {"title": "Scikit-learn: Machine learning in python", "author": ["Scikit", "S.-l. D."], "venue": "Journal of Machine Learning Research, 12:2825\u20132830.", "citeRegEx": "Scikit and D.,? 2011", "shortCiteRegEx": "Scikit and D.", "year": 2011}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["E.F. Tjong Kim Sang", "F. De Meulder"], "venue": "Daelemans, W. and Osborne, M., editors, Proceedings of CoNLL-2003, pages 142\u2013147. Edmonton, Canada.", "citeRegEx": "Sang and Meulder,? 2003", "shortCiteRegEx": "Sang and Meulder", "year": 2003}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL \u201903, pages 173\u2013180, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Toutanova et al\\.,? 2003", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger", "author": ["K. Toutanova", "C.D. Manning"], "venue": "Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics - Volume 13, EMNLP \u201900, pages 63\u201370, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Toutanova and Manning,? 2000", "shortCiteRegEx": "Toutanova and Manning", "year": 2000}, {"title": "Tokenization as the initial phase in nlp", "author": ["J.J. Webster", "C. Kit"], "venue": "Proceedings of the 14th conference on Computational linguistics - Volume 4, COLING \u201992, pages 1106\u20131110, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Webster and Kit,? 1992", "shortCiteRegEx": "Webster and Kit", "year": 1992}], "referenceMentions": [{"referenceID": 12, "context": "Since NLP algorithms require computationally expensive operations, the NLP stages of an IR system become the bottleneck with regards to scalability (Pauls and Klein, 2011).", "startOffset": 148, "endOffset": 171}, {"referenceID": 11, "context": "However, this section is not meant to review the literature of named entity recognition research as this is already available in (Nadeau and Sekine, 2007).", "startOffset": 129, "endOffset": 154}, {"referenceID": 17, "context": "Stanford NLP pipeline (Toutanova and Manning, 2000; Toutanova et al., 2003; Klein et al., 2003; Finkel et al., 2005; Lee et al., 2011) is one of the most popular and used NLP packages.", "startOffset": 22, "endOffset": 134}, {"referenceID": 16, "context": "Stanford NLP pipeline (Toutanova and Manning, 2000; Toutanova et al., 2003; Klein et al., 2003; Finkel et al., 2005; Lee et al., 2011) is one of the most popular and used NLP packages.", "startOffset": 22, "endOffset": 134}, {"referenceID": 7, "context": "Stanford NLP pipeline (Toutanova and Manning, 2000; Toutanova et al., 2003; Klein et al., 2003; Finkel et al., 2005; Lee et al., 2011) is one of the most popular and used NLP packages.", "startOffset": 22, "endOffset": 134}, {"referenceID": 5, "context": "Stanford NLP pipeline (Toutanova and Manning, 2000; Toutanova et al., 2003; Klein et al., 2003; Finkel et al., 2005; Lee et al., 2011) is one of the most popular and used NLP packages.", "startOffset": 22, "endOffset": 134}, {"referenceID": 8, "context": "Stanford NLP pipeline (Toutanova and Manning, 2000; Toutanova et al., 2003; Klein et al., 2003; Finkel et al., 2005; Lee et al., 2011) is one of the most popular and used NLP packages.", "startOffset": 22, "endOffset": 134}, {"referenceID": 2, "context": "Even though it lacks a proper tokenizer, it offers POS tagging, named entity recognition, chunking, semantic role labeling(Collobert and Weston, 2008) and parsing (Collobert, 2011).", "startOffset": 122, "endOffset": 150}, {"referenceID": 1, "context": "Even though it lacks a proper tokenizer, it offers POS tagging, named entity recognition, chunking, semantic role labeling(Collobert and Weston, 2008) and parsing (Collobert, 2011).", "startOffset": 163, "endOffset": 180}, {"referenceID": 0, "context": "NLTK (Bird et al., 2009) is a set of tools and interfaces to other NLP packages.", "startOffset": 5, "endOffset": 24}, {"referenceID": 10, "context": "WikipediaMiner (Milne and Witten, 2008) detects conceptual words and named entities; it also disambiguates the word senses.", "startOffset": 15, "endOffset": 39}, {"referenceID": 4, "context": "TAGME (Ferragina and Scaiella, 2010) is extending the work of WikipediaMiner to annotate short snippets of text.", "startOffset": 6, "endOffset": 36}, {"referenceID": 18, "context": "The first task that an NLP pipeline has to deal with is tokenization and sentence segmentation (Webster and Kit, 1992).", "startOffset": 95, "endOffset": 118}, {"referenceID": 6, "context": ", prove to be complex (Gillick, 2009).", "startOffset": 22, "endOffset": 37}, {"referenceID": 17, "context": "Earlier work to solve the POS tagging problem relied on lexical and local features using maximum entropy models (Toutanova and Manning, 2000).", "startOffset": 112, "endOffset": 141}, {"referenceID": 16, "context": "Later, more advanced models took advantage of the context words and their predicted tags (Toutanova et al., 2003) to achieve higher accuracy.", "startOffset": 89, "endOffset": 113}, {"referenceID": 8, "context": "(Lee et al., 2011) shows that using sieves is the solution to combine several rules/dictionaries.", "startOffset": 0, "endOffset": 18}, {"referenceID": 3, "context": "To be sure that our algorithm is robust enough and not overfitting the dataset, we calculated the dictionaries again by running SENNA POS tagger(Collobert et al., 2011) over Reuters RCV1 corpus and the results were similar.", "startOffset": 144, "endOffset": 168}, {"referenceID": 9, "context": "This confirms some of the conclusions reported by (Manning, 2011).", "startOffset": 50, "endOffset": 65}, {"referenceID": 7, "context": "For example, in the original Stanford MEMM implementation, the classifier (Klein et al., 2003) generates IOB chunking tags while in the later CRF models (Finkel et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 5, "context": ", 2003) generates IOB chunking tags while in the later CRF models (Finkel et al., 2005) only IO chunking tags are generated.", "startOffset": 66, "endOffset": 87}, {"referenceID": 13, "context": "(Pawel and Robert, 2007) shows that conjunction words that appear in middle of entities\u2019 phrases are hard to detect and need special classification task.", "startOffset": 0, "endOffset": 24}], "year": 2013, "abstractText": "Online content analysis employs algorithmic methods to identify entities in unstructured text. Both machine learning and knowledge-base approaches lie at the foundation of contemporary named entities extraction systems. However, the progress in deploying these approaches on web-scale has been been hampered by the computational cost of NLP over massive text corpora. We present SpeedRead (SR), a named entity recognition pipeline that runs at least 10 times faster than Stanford NLP pipeline. This pipeline consists of a high performance Penn Treebankcompliant tokenizer, close to state-of-art part-of-speech (POS) tagger and knowledge-based named entity recognizer.", "creator": "LaTeX with hyperref package"}}}