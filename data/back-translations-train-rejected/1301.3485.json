{"id": "1301.3485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "A Semantic Matching Energy Function for Learning with Multi-relational Data", "abstract": "Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature.", "histories": [["v1", "Tue, 15 Jan 2013 20:52:50 GMT  (12kb)", "https://arxiv.org/abs/1301.3485v1", null], ["v2", "Thu, 21 Mar 2013 17:02:48 GMT  (12kb)", "http://arxiv.org/abs/1301.3485v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["xavier glorot", "antoine bordes", "jason weston", "yoshua bengio"], "accepted": false, "id": "1301.3485"}, "pdf": {"name": "1301.3485.pdf", "metadata": {"source": "CRF", "title": "A Semantic Matching Energy Function for Learning with Multi-relational Data", "authors": ["Xavier Glorot", "Antoine Bordes", "Jason Weston", "Yoshua Bengio"], "emails": ["glorotxa@iro.umontreal.ca", "bengioy@iro.umontreal.ca", "bordesan@hds.utc.fr", "jweston@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 130 1.34 85v2 [cs.LG] 2 1M ar2 013A Semantic Matching Energy Function for Learning with Multi-Relationtional DataXavier Glorot (1), Antoine Bordes (2), Jason Weston (3), Yoshua Bengio (1) (1) DIRO, Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, QC, Canada - {glorotxa, bengioy} @ iro.umontreal.ca (2) CNRS - Heudiasyc, Universit\u00e9 de Technologie de Compi\u00e8gne, France - bordesan @ hds.utc.fr (3) Google, New York, NY, USA - jweston @ google.com"}, {"heading": "1 Introduction", "text": "Multirelational data, which relate to graphs whose nodes, entities and edges correspond to relationships linking these entities, play a central role in many areas such as recommendation systems, the semantic web, or computer biology. Relationships are modelled as triplets of form (subject, relationship, object), with a relationship modelling either the relationship between two entities or between an entity and an attribute value; relationships are therefore of several types. Despite their attractive ability to represent complex data, multirelational graphs remain complicated to manipulate for several reasons (noise, heterogeneity, large dimensions, etc.) and conveniently represent, summarize, or emit this type of data as a central challenge in statistical relational learning [2]."}, {"heading": "2 Semantic Matching Energy Function", "text": "This work considers multirelational databases as graph models. Each node of the graph corresponds to an element of the database that we call an entity, and each linkage defines a relationship between entities. Relationships are directed, and there are typically several different types of relationships. Let C denote the dictionary that includes all entities and relationship types, and let R-C be the subset of entities that are relationship types. A relationship is denoted by a triplet (lhs, rel, rhs), where lhs is the left entity, rhs is the right entity, and rel is the type of relationship between them."}, {"heading": "2.1 Main ideas", "text": "The main ideas behind our semantic matching energy function are the following. \u2022 Named symbolic units (units and relationship types) are associated with a d-dimensional vector space called \"embedding space.\" \u2022 The ith unit is assigned a vector egg-Rd. Note that more general mappings of a unit to its embedding are possible. \u2022 The semantic matching energy value associated with a particular triplet (lhs, rel, rhs) is calculated by a parameterized function E, which starts by mapping all symbols to their embedding and then combines them in a structured way. \u2022 Our model is called \"semantic matching\" because E is based on a matching criterion calculated between both sides of the triplet. \u2022 The energy function E is optimized to be lower for training examples than for other possible configurations of symbols."}, {"heading": "2.2 Neural network parametrization", "text": "The green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "3 Empirical Evaluation", "text": "In order to combat existing methods, we conducted link prediction experiments on benchmarks from the literature, the statistics of which are shown in Table 1. The link prediction task consists in predicting whether two entities should be connected by a certain type of relation, useful for completing missing values of a graph, predicting the behavior of a network, etc., but also for assessing the quality of a representation. We evaluate our model on UMLS, nations and kinships, following the setting introduced in [4]. The standard evaluation metric is the area below the precision recall curve (AUC). Table 2 presents the results of SME together with those of RESCAL, MRC, IRM, CP (CANDECOMP-PARAFAC) and LFM, which were extracted from [5, 3]. The linear formulation of SME is performed by SME (bilinear) across all three tasks."}, {"heading": "Acknowledgements", "text": "This work was supported by the French ANR (EVEREST-12-JS02-005-01), the Pascal2 European NoE, the DARPA DL Program, NSERC, CIFAR, the Canada Research Chairs and Compute Canada."}], "references": [{"title": "Signature verification using a siamese time delay neural network", "author": ["Jame Bromley", "Jim W. Bentz", "L\u00e9on Bottou", "Isabelle Guyon", "Yann Le Cun", "C. Moore", "Eduard S\u00e4ckinger", "Roopak Shah"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1993}, {"title": "Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning)", "author": ["Lise Getoor", "Ben Taskar"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "A latent factor model for highly multi-relational data", "author": ["Rodolphe Jenatton", "Nicolas Le Roux", "Antoine Bordes", "Guillaume Obozinski"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["Charles Kemp", "Joshua B. Tenenbaum", "Thomas L. Griffiths", "Takeshi Yamada", "Naonori Ueda"], "venue": "In Proceedings of the 21st national conference on Artificial intelligence - Volume 1,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "A three-way model for collective learning on multirelational data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Modelling relational data using bayesian clustered tensor factorization", "author": ["Ilya Sutskever", "Ruslan Salakhutdinov", "Josh Tenenbaum"], "venue": "In Adv. in Neur. Inf. Proc. Syst", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "), and conveniently represent, summarize or de-noise this kind of data is now a central challenge in statistical relational learning [2].", "startOffset": 133, "endOffset": 136}, {"referenceID": 3, "context": "Unlike in previous work [4, 6, 5, 3], in this model, relation types are modeled similarly as entities.", "startOffset": 24, "endOffset": 36}, {"referenceID": 5, "context": "Unlike in previous work [4, 6, 5, 3], in this model, relation types are modeled similarly as entities.", "startOffset": 24, "endOffset": 36}, {"referenceID": 4, "context": "Unlike in previous work [4, 6, 5, 3], in this model, relation types are modeled similarly as entities.", "startOffset": 24, "endOffset": 36}, {"referenceID": 2, "context": "Unlike in previous work [4, 6, 5, 3], in this model, relation types are modeled similarly as entities.", "startOffset": 24, "endOffset": 36}, {"referenceID": 0, "context": "2 Neural network parametrization The energy function E (denoted SME) is encoded using a neural network, whose architecture first processes each entity in parallel, like in siamese networks [1].", "startOffset": 189, "endOffset": 192}, {"referenceID": 6, "context": "To train the parameters of the energy function E we loop over all of the training data resources and use stochastic gradient descent with a ranking objective inspired by [7].", "startOffset": 170, "endOffset": 173}, {"referenceID": 3, "context": "We evaluate our model on UMLS, Nations and Kinships, following the setting introduced in [4].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "Table 2 presents results of SME along with those of RESCAL, MRC, IRM, CP (CANDECOMP-PARAFAC) and LFM, which have been extracted from [5, 3].", "startOffset": 133, "endOffset": 139}, {"referenceID": 2, "context": "Table 2 presents results of SME along with those of RESCAL, MRC, IRM, CP (CANDECOMP-PARAFAC) and LFM, which have been extracted from [5, 3].", "startOffset": 133, "endOffset": 139}], "year": 2013, "abstractText": null, "creator": "LaTeX with hyperref package"}}}