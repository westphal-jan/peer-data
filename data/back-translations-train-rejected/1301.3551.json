{"id": "1301.3551", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Information Theoretic Learning with Infinitely Divisible Kernels", "abstract": "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's entropy definition and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. We show how analogues to quantities such as conditional entropy can be defined, enabling solutions to learning problems. In particular, we derive a supervised metric learning algorithm with very competitive results.", "histories": [["v1", "Wed, 16 Jan 2013 01:49:52 GMT  (159kb)", "https://arxiv.org/abs/1301.3551v1", "Initial submission for International Conference on Learning Representations 2013"], ["v2", "Wed, 20 Mar 2013 06:40:01 GMT  (190kb)", "http://arxiv.org/abs/1301.3551v2", "Modified submission for International Conference on Learning Representations 2013"], ["v3", "Fri, 22 Mar 2013 14:53:42 GMT  (190kb)", "http://arxiv.org/abs/1301.3551v3", "Modified submission for International Conference on Learning Representations 2013"], ["v4", "Tue, 16 Apr 2013 00:12:21 GMT  (190kb)", "http://arxiv.org/abs/1301.3551v4", "Modified submission for International Conference on Learning Representations 2013"], ["v5", "Wed, 1 May 2013 06:18:31 GMT  (192kb)", "http://arxiv.org/abs/1301.3551v5", "Modified submission for International Conference on Learning Representations 2013"], ["v6", "Tue, 4 Jun 2013 04:42:39 GMT  (190kb)", "http://arxiv.org/abs/1301.3551v6", "Modified submission for International Conference on Learning Representations 2013"]], "COMMENTS": "Initial submission for International Conference on Learning Representations 2013", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["luis g sanchez giraldo", "jose c principe"], "accepted": false, "id": "1301.3551"}, "pdf": {"name": "1301.3551.pdf", "metadata": {"source": "CRF", "title": "Information Theoretic Learning with Infinitely Divisible Kernels", "authors": ["Luis G. Sanchez Giraldo", "Jose C. Principe"], "emails": ["sanchez@cnel.ufl.edu", "principe@cnel.ufl.edu"], "sections": [{"heading": null, "text": "ar Xiv: 130 1.35 51v6 [cs.LG] 4 Jun 2In this paper, we develop a framework for information theory learning based on infinitely divisible matrices. We formulate an entropy-like function based on positively defined matrices based on Renyi's axiomatic definition of entropy and examine some key characteristics of this function leading to the concept of infinite divisibility. The proposed formulation avoids clogged density estimation and brings with it the representational power of the reproduction of Hilbert kernel spaces. As an example of application, we derive a supervised metric learning algorithm that achieves results comparable to state-of-the-art on a matrix-based analog of conditional entropy."}, {"heading": "1 Introduction", "text": "The expressive wealth of quantities such as entropy or mutual information has proven to be very useful for machine learning problems where the only available information comes from the sample {zi} ni = 1. Therefore, the use of information theoretical quantities as descriptors of data requires the development of suitable estimators. In [1], the use of Renyi's definition of entropy along with Parzen density estimation is proposed as the main tool for information theoretical learning (ITL). The optimization criteria are expressed in terms of quantities such as Renyi's entropy, divergences based on Cauchy-Schwarz inequality, among others."}, {"heading": "2 Positive Definite Matrices, and Renyi\u2019s Entropy Axioms", "text": "Let's start with an informal observation that motivated our matrix on the basis of entropy. In [1], the use of Renyi's entropy is proposed as an alternative to the generally applied definition of entropy given by Shannon. Specifically, it was noted that Renyi's second-order entropy provides an affordable quantity for practical purposes. An empirical plug-in estimate of Renyi's second-order entropy, based on the score density estimator f (x) = 1n window i (xi, x), can be obtained as follows: \u2212 log 1 n2n value of KK value i, j = 1 h (xi, x j), where h (x, y) = x value of f-value (y, z) dz value is of i-value. Note: Since h is a positive definite core, there is a mapping of xi-value based on a value of f-value (x, z) that is x-KHS value."}, {"heading": "2.1 Renyi\u2019s Axioms for Gram matrices", "text": "It is possible to define a partial order on this basis by using positive definitive matrices that represent a generalization of positive real numbers. Let Mn be the set of all real matrices. The following spectral decomposition theorem [7] refers to the functional calculation on matrices and provides a reasonable way to perform the continuous scalar-weighted functions on Hermitian matrices.Theorem 2.1 Let D be a predetermined set and let Nn (D): = {A = Mn is normal and sober (A), where the spectrum of A. If f f (t) is a continuous scalar-weighted function on the primary function, then we will use the function x."}, {"heading": "2.2 Hadamard Products and the Notion of Joint Entropy", "text": "s look at a sequence of example pairs {(xi, yi)} Ni = 1, where xi-X and yi-Y are defined. Suppose we have a positive definitive nucleus \u03ba1 defined on X \u00b7 X and \u03ba2 defined on X \u00b7 X. As we can see, the Hadamard product arises as a common representation in a matrix-based entropy. Consider two matrices A and B in a positive definitive kernel on (X \u00b7 Y) \u00d7 (X \u00b7 Y), for which there is a certain relationship between the elements Ai j and Bi."}, {"heading": "2.3 Conditional Entropy as a Difference Between Entropies", "text": "s definition of conditional entropy, H (X | Y) can be expressed as H (X | Y) = H (X, Y) \u2212 H (Y). The properties of this definition have recently been studied in the case of Renyi's entropy [8] and in the case of the matrix, so that Aii = 1n for all i = 1,..., n. The quantity given above is not negative and limited by S\u03b1 (A) \u2212 with positive semidefinitive matrices A and B with non-negative entries and unit properties, so that Aii = 1n for all i = 1,.., n. The quantity given above is not negative and limited by S\u03b1 (A). Naturally, normalization is an important property of the matrices involved in the above results."}, {"heading": "3 Infinitely Divisible Functions", "text": "The theory of infinitely divisible data developed below is not new, but it is included because it provides a fundamental understanding of the role of infinitely divisible nuclei in the calculation of the above-mentioned information-theoretical quantities from data. To avoid confusion, we will describe the most important points that we should keep in mind before proceeding to the mathematical description. Infinitely divisible nuclei and negative definitive functions are linked together by the exponential logarithm functions. Both functions provide Hilbert spatial representations of the data. We can imagine the RKHS of the infinitely divisible core as a representation for the calculation of the descriptors of higher order of the data. On the other hand, Hilberian metrics can be the representation space for which we want to calculate the statistics of high order. Normalization, as we show below, is not only important to fulfill the conditions for the already defined information-theoretical quantities, but it also shows that many possible representation options are equivalent."}, {"heading": "3.1 Negative Definite Functions and Hilbertian Metrics", "text": "Let M = (X, d) be a divisible metric space. A necessary and sufficient condition for M to be embedded in a Hilbert space H is that for each set {xi} \u0445 X of n + 1 points, \u2211 ni, j = 1 \u03b1i\u03b1 j (d2 (x0, xi) + d2 (x0, x j) \u2212 d2 (xi, x j))) \u2265 0, for each \u03b1-Rn. This condition is equivalent to \u2211 ni, j = 0 \u03b1i\u03b1 jd2 (xi, x j) \u2264 0, for each \u03b1-Rn + 1, so that \u2211 ni = 0 \u03b1i = 0. This condition is known as a negative definition. Interestingly, the above condition implies that exp (\u2212 rd2 (xi, x j)) is positively defined for all r > 0 [9]. In fact, matrices derived from functions fulfilling the above property form a special class known as infinite matrices."}, {"heading": "3.2 Infinitely Divisible Matrices", "text": "According to the Schur product theorem A < 0, A and n = A \u0445 A \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 A < 0 implies n for each positive integer. Does the above hold if we take broken powers from A? In other words, is the matrix A \u0445 1 m < 0 for each positive integer m? This question leads to the concept of infinitely divisible matrices [10, 11]. A non-negative matrix A is considered infinitely divisible if the matrix B = \u2212 logAi j is for each non-negative. Infinitely divisible matrices are closely related to negative definitions, as we can see from the following proposition. 3.1 If the matrix A is infinitely divisible, then the matrix Bi j = \u2212 logAi j is a negative definition. For this reason it is possible to refer infinitely divisible matrices with isometric embedding in Hilbert spaces."}, {"heading": "4 Application to Metric Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Adaptation Using the Matrix-Based Entropy", "text": "By definition, the matrix entropy function (5) belongs to the family of matrix functions known as spectral functions. These functions depend only on the eigenvalues of the matrix and therefore its name [12]. With theorem (1.1) from [13], it is easy to obtain the derivative of (5) at A in such a way that it can be used to our advantage: Instead of calculating the complete set of eigenvectors and eigenvalues of A, we can approximate the gradient of S\u03b1 with only a few leading eigenvalues. It is easy to see that this approximation will be optimal in the Frobenius norm of X-Fro = \u221a tr (X-X)."}, {"heading": "4.2 Metric Learning Using Conditional Entropy", "text": "Here we apply the proposed matrix framework to the problem of supervised metric learning. This problem can be formulated as follows: In view of a number of points {(xi, li) ni = 1, we are looking for a more positive semidefinitive matrix AAT that minimizes a certain distance between the samples x, x \"and\" Rd \"such as d (x, x\") = (x, x \"). Our goal is to find such a parameterization matrix that the conditional entropy of the li labels in view of the projected samples yi = ATxi with yi, Rp\" and p \"d,\" is posed as the following optimization problem: minimizes A \"Rd\" pS\u03b1 \"we are subject to ATxi = yi, for i = 1,., n; tr\" (ATA) where the trace constraint prevents the solution of the growing problem."}, {"heading": "5 Conclusions", "text": "In this paper, we have presented a data-based framework for information theory learning based on infinitely divisible matrices. We define estimators of entropy-like quantities that can be calculated from the gram matrices by evaluating infinitely divisible nuclei based on sample pairs. The proposed quantities do not assume that the density of the data has been estimated, this can be beneficial in many scenarios where even the definition of a density is impracticable. We will discuss some key properties of the proposed quantities and show how they can be applied to define useful analogies to quantities such as conditional entropy. On the basis of the proposed framework, we will introduce a supervised metric learning algorithm with results that compete with the state of the art. Nevertheless, we believe that many interesting formulations for learning problems have yet to be found based on the proposed framework. It is also important to emphasize that the link between the RHS and the infinite functionality associated with the KHS and the infinite functionality of the metallic functions."}, {"heading": "A Additional results and proofs", "text": "Definition A.1 (Majorization): Let p and q be two nonnegative vectors in Rn so that the property ni = 1 Pi = 1 Qi = 1 Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi Qi."}], "references": [{"title": "Information Theoretic Learning: Renyi\u2019s Entropy and Kernel Perspectives, ser", "author": ["J.C. Principe"], "venue": "Series in Information Science and Statistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "A reproducing kernel hilbert space framework for information theoretic learning", "author": ["J.-W. Xu", "A.R.C. Paiva", "I. Park", "J.C. Principe"], "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 12, pp. 5891\u20135902, December 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1\u201348, July 2002.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Measuring statistical dependence with hilbertschmidt norms", "author": ["A. Gretton", "O. Bousquet", "A. Smola", "B. Sch\u00f6lkopf"], "venue": "Proceedings of Algorithmic Learning Theory, S. Jain, H. Simon, and E. Tomita, Eds., 2005, pp. 63\u201377.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "A unified framework for quadratic measures of independence", "author": ["S. Seth", "M. Rao", "I. Park", "J.C. Pr\u0131\u0301ncipe"], "venue": "IEEE Transactions on Signal Processing, vol. 59, no. 8, pp. 3624\u20133635, August 2011.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Topics in Matrix Analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1991}, {"title": "Conditional r\u00e9nyi entropies", "author": ["A. Teixeira", "A. Matos", "L. Antunes"], "venue": "IEEE Transactions on Information Theory, vol. 58, no. 7, pp. 4273\u20134277, July 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Metric spaces and positive definite functions", "author": ["I.J. Schoenberg"], "venue": "Transactions of the American Mathematical Society, vol. 44, no. 3, pp. 522\u2013536, November 1938.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1938}, {"title": "Infinite divisible matrices", "author": ["R. Bhatia"], "venue": "The American Mathematical Monthly, vol. 113, no. 3, pp. 221\u2013 235, March 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "The theory of infinitely divisible matrices and kernels", "author": ["R.A. Horn"], "venue": "Transactions of the American Mathematical Society, vol. 136, pp. 269\u2013286, February 1969.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1969}, {"title": "Convex spectral functions", "author": ["S. Friedland"], "venue": "Linear and Multilinear Algebra, vol. 9, pp. 299\u2013316, 1981.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1981}, {"title": "Derivatives of spectral functions", "author": ["A.S. Lewis"], "venue": "Mathematics of Operations Research, vol. 21, no. 3, pp. 576\u2013588, August 1996.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1996}, {"title": "Information-theoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "ICML, Corvalis, Oregon, USA, June 2007, pp. 209\u2013216.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Neighborhood component analysis", "author": ["J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov"], "venue": "NIPS, 2004.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2004}, {"title": "Metric learning by collapsing classes", "author": ["A. Globerson", "S. Roweis"], "venue": "NIPS, 2005.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "NIPS, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "In [1], the use of Renyi\u2019s definition of entropy along with Parzen density estimation is proposed as the main tool for information theoretic learning (ITL).", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "Part of the research effort in this context has pointed out connections to reproducing kernel Hilbert spaces [2].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "The generality of this representation has been exploited in many practical applications, even for data that do not come in standard vector representation Rd [3].", "startOffset": 157, "endOffset": 160}, {"referenceID": 3, "context": "Some examples exploring this idea are: kernel independent component analysis [4], the work on measures of dependence and independence using Hilbert-Schmidt norms [5], and the quadratic measures of independence proposed in [6].", "startOffset": 77, "endOffset": 80}, {"referenceID": 4, "context": "Some examples exploring this idea are: kernel independent component analysis [4], the work on measures of dependence and independence using Hilbert-Schmidt norms [5], and the quadratic measures of independence proposed in [6].", "startOffset": 162, "endOffset": 165}, {"referenceID": 5, "context": "Some examples exploring this idea are: kernel independent component analysis [4], the work on measures of dependence and independence using Hilbert-Schmidt norms [5], and the quadratic measures of independence proposed in [6].", "startOffset": 222, "endOffset": 225}, {"referenceID": 0, "context": "In [1], the use of Renyi\u2019s entropy is proposed as an alternative to the more commonly adopted definition of entropy given by Shannon.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "The following spectral decomposition theorem [7] relates to the functional calculus on matrices and provides a reasonable way to extend continuous scalar-valued functions to Hermitian matrices.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "The properties of this definition has been recently studied in the case of Renyi\u2019s entropies [8] and in the matrix case, this definition yields:", "startOffset": 93, "endOffset": 96}, {"referenceID": 0, "context": "If A and B are normalized to have unit trace, then for r \u2208 [0,1] it is true that the Hadamard product of A\u25e6r \u25e6B\u25e6(1\u2212r), (11) is also normalized.", "startOffset": 59, "endOffset": 64}, {"referenceID": 8, "context": "Interestingly, the above condition implies that exp(\u2212rd(xi,x j)) is positive definite in X for all r > 0 [9].", "startOffset": 105, "endOffset": 108}, {"referenceID": 9, "context": "Does the above hold if we to take fractional powers of A? In other words,is the matrix A\u25e6 1 m < 0 for any positive integer m? This question leads to the concept of infinitely divisible matrices [10, 11].", "startOffset": 194, "endOffset": 202}, {"referenceID": 10, "context": "Does the above hold if we to take fractional powers of A? In other words,is the matrix A\u25e6 1 m < 0 for any positive integer m? This question leads to the concept of infinitely divisible matrices [10, 11].", "startOffset": 194, "endOffset": 202}, {"referenceID": 11, "context": "These functions only depend on the eigenvalues of matrix and therefore their name [12].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "1) from [13] it is straightforward to obtain the derivative of (5) at A as", "startOffset": 8, "endOffset": 12}, {"referenceID": 13, "context": "UCI Data: To evaluate the results we use the same experimental setup proposed in [14], we compares 5 different approaches to supervised metric learning based on the classification error obtained from two-fold cross-validation using a 4-nearest neighbor classifier.", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "Figure 2(a) shows the results of the proposed approach conditional entropy metric learning (CEML), information theoretic metric learning (ITML) proposed in [14], neighborhood component analysis (NCA) from [15], the maximally collapsing metric learning (MCML) method from [16], the large margin nearest neighbor (LMNN) method found in [17], and, as a baseline, the the inverse covariance and Euclidean distances.", "startOffset": 156, "endOffset": 160}, {"referenceID": 14, "context": "Figure 2(a) shows the results of the proposed approach conditional entropy metric learning (CEML), information theoretic metric learning (ITML) proposed in [14], neighborhood component analysis (NCA) from [15], the maximally collapsing metric learning (MCML) method from [16], the large margin nearest neighbor (LMNN) method found in [17], and, as a baseline, the the inverse covariance and Euclidean distances.", "startOffset": 205, "endOffset": 209}, {"referenceID": 15, "context": "Figure 2(a) shows the results of the proposed approach conditional entropy metric learning (CEML), information theoretic metric learning (ITML) proposed in [14], neighborhood component analysis (NCA) from [15], the maximally collapsing metric learning (MCML) method from [16], the large margin nearest neighbor (LMNN) method found in [17], and, as a baseline, the the inverse covariance and Euclidean distances.", "startOffset": 271, "endOffset": 275}, {"referenceID": 16, "context": "Figure 2(a) shows the results of the proposed approach conditional entropy metric learning (CEML), information theoretic metric learning (ITML) proposed in [14], neighborhood component analysis (NCA) from [15], the maximally collapsing metric learning (MCML) method from [16], the large margin nearest neighbor (LMNN) method found in [17], and, as a baseline, the the inverse covariance and Euclidean distances.", "startOffset": 334, "endOffset": 338}, {"referenceID": 0, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 1, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 2, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 3, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 4, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 5, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 6, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 7, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 8, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 9, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 10, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 11, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 12, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 13, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 14, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 15, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}, {"referenceID": 16, "context": "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.", "startOffset": 110, "endOffset": 117}], "year": 2013, "abstractText": "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi\u2019s axiomatic definition of entropy and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. As an application example, we derive a supervised metric learning algorithm using a matrix based analogue to conditional entropy achieving results comparable with the state of the art.", "creator": "LaTeX with hyperref package"}}}