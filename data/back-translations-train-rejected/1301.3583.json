{"id": "1301.3583", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Big Neural Networks Waste Capacity", "abstract": "This article exposes the failure of some big neural networks to leverage added capacity to reduce underfitting. Past research suggest diminishing returns when increasing the size of neural networks. Our experiments on ImageNet LSVRC-2010 show that this may be due to the fact that bigger networks underfit the training objective, sometimes performing worse on the training set than smaller networks. This suggests that the optimization method - first order gradient descent - fails at this regime. Directly attacking this problem, either through the optimization method or the choices of parametrization, may allow to improve the generalization error on large datasets, for which a large capacity is required.", "histories": [["v1", "Wed, 16 Jan 2013 04:45:29 GMT  (38kb,D)", "http://arxiv.org/abs/1301.3583v1", null], ["v2", "Thu, 17 Jan 2013 18:11:34 GMT  (38kb,D)", "http://arxiv.org/abs/1301.3583v2", null], ["v3", "Wed, 27 Feb 2013 23:07:05 GMT  (38kb,D)", "http://arxiv.org/abs/1301.3583v3", null], ["v4", "Thu, 14 Mar 2013 20:49:20 GMT  (40kb,D)", "http://arxiv.org/abs/1301.3583v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yann n dauphin", "yoshua bengio"], "accepted": false, "id": "1301.3583"}, "pdf": {"name": "1301.3583.pdf", "metadata": {"source": "CRF", "title": "Big Neural Networks Waste Capacity", "authors": ["Yann N. Dauphin"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In all of these cases, relatively large data sets were involved, but in all of these cases even larger networks could be used. One of the biggest challenges remains expanding neural networks on a much larger scale, and with this goal in mind, this paper poses a simple question: Is there an optimization problem that prevents the efficient formation of larger networks? Previous evidence of the failure of large networks in literature can be found, for example, in Coates et al. (2011), which shows that increasing the capacity of certain neural network methods quickly reaches a point where the yield of the test error is reduced. These results have since been extended to other types of auto-encoders and RBMs (Rifai et al., 2011; Courville et al., 2011). Furthermore, Coates et al. (2011) shows that neural network methods cannot effectively utilize K-Means, which enabled K-Means to turn this result into K-dumb methods in comparison to auto-AR methods in the first place."}, {"heading": "2 Experimental Setup", "text": "In fact, it is so that it is a way in which most people are able to survive themselves. (...) In fact, it is so that most people are able to survive themselves. (...) In fact, it is so that most people are able to survive themselves. (...) It is so that most people are able to survive themselves. (...) It is so that people are able to survive themselves. (...) It is so that people are able to survive themselves. (...) It is as if people are able to survive themselves. (...) It is as if people are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "3 Experimental Results", "text": "Figure 1 shows the evolution of the training error as capacity increases. Common intuition is that this increased capacity will help adjust the training set - possibly to the detriment of generalization errors. For this reason, practitioners focus mainly on the problem of overadjusting the data set when dealing with large networks - not on underadjustment. In fact, much research focuses on the proper regulation of such large networks (Hinton et al., 2012, 2006).However, our results show that adding capacity quickly cannot reduce underadjustment. It is becoming increasingly difficult for networks to use additional units to adjust the training set. As shown in Figure 2, the return for additional units pales to zero without achieving 0 errors in the training set. This means that there is an optimization problem that prevents us from effectively training large networks.For reference, we also refer to the learning curves of the networks used for Figure 2 and Figure 3."}, {"heading": "4 Future directions", "text": "In fact, we know that the approximation of the first order fails when there are many interactions between hidden units. It may be that the addition of units increases the interactions between units and results in the Hessians being poorly conditioned. This type of method can be implemented efficiently. Examples of this approach are thriftiness and orthognality penalties. \u2022 Methods that model interactions between hidden units. For example, second order methods (Martens, 2010) and methods of natural gradients (Le Roux et al., 2008). Typically, these are expensive approaches and the challenge is to scale them to large data sets where structurally weak gradient approaches can dominate. The ideal goal is a stochastic natural gradient or stochastic problem of the second order."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Now Publishers", "citeRegEx": "Bengio,? \\Q2009\\E", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Y. Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "Coates et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2011}, {"title": "Unsupervised models of images by spike-andslab RBMs", "author": ["A. Courville", "J. Bergstra", "Y. Bengio"], "venue": "In Proceedings of the Twenty-eight International Conference on Machine Learning (ICML\u201911)", "citeRegEx": "Courville et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Courville et al\\.", "year": 2011}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "Li", "L.-J", "K. Li", "L. Fei-Fei"], "venue": null, "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Why does unsupervised pre-training help deep learning", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Manzagol", "P. Vincent", "S. Bengio"], "venue": "J. Machine Learning Res", "citeRegEx": "Erhan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2010}, {"title": "An overview of the hdf5 technology suite and its applications", "author": ["M. Folk", "G. Heber", "Q. Koziol", "E. Pourmal", "D. Robinson"], "venue": "In Proceedings of the EDBT/ICDT 2011 Workshop on Array Databases,", "citeRegEx": "Folk et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Folk et al\\.", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "In JMLR W&CP: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010),", "citeRegEx": "Glorot and Bengio,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y. Teh"], "venue": "Neural Computation,", "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Technical report,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["N. Le Roux", "Manzagol", "P.-A", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Roux et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2008}, {"title": "Deep learning via Hessian-free optimization", "author": ["J. Martens"], "venue": "Proceedings of the Twenty-seventh International Conference on Machine Learning", "citeRegEx": "Martens,? \\Q2010\\E", "shortCiteRegEx": "Martens", "year": 2010}, {"title": "Empirical evaluation and combination of advanced language modeling techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. Cernocky"], "venue": "In Proc. 12th annual conference of the international speech communication association (INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Higher order contractive auto-encoder. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)", "author": ["S. Rifai", "G. Mesnil", "P. Vincent", "X. Muller", "Y. Bengio", "Y. Dauphin", "X. Glorot"], "venue": null, "citeRegEx": "Rifai et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Rifai et al\\.", "year": 2011}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "In Interspeech", "citeRegEx": "Seide et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Seide et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "These results have since been extended to other types of auto-encoders and RBMs (Rifai et al., 2011; Courville et al., 2011).", "startOffset": 80, "endOffset": 124}, {"referenceID": 3, "context": "These results have since been extended to other types of auto-encoders and RBMs (Rifai et al., 2011; Courville et al., 2011).", "startOffset": 80, "endOffset": 124}, {"referenceID": 2, "context": "One of the major challenges remains to extend neural networks on a much larger scale, and with this objective in mind, this paper asks a simple question: is there an optimization issue that prevents efficiently training larger networks? Prior evidence of the failure of big networks in the litterature can be found for example in Coates et al. (2011), which shows that increasing the capacity of certain neural net methods quickly reaches a point of diminishing returns on the test error.", "startOffset": 330, "endOffset": 351}, {"referenceID": 2, "context": "One of the major challenges remains to extend neural networks on a much larger scale, and with this objective in mind, this paper asks a simple question: is there an optimization issue that prevents efficiently training larger networks? Prior evidence of the failure of big networks in the litterature can be found for example in Coates et al. (2011), which shows that increasing the capacity of certain neural net methods quickly reaches a point of diminishing returns on the test error. These results have since been extended to other types of auto-encoders and RBMs (Rifai et al., 2011; Courville et al., 2011). Furthermore, Coates et al. (2011) shows that while neural net methods fail to leverage added capacity K-Means can.", "startOffset": 330, "endOffset": 649}, {"referenceID": 2, "context": "One of the major challenges remains to extend neural networks on a much larger scale, and with this objective in mind, this paper asks a simple question: is there an optimization issue that prevents efficiently training larger networks? Prior evidence of the failure of big networks in the litterature can be found for example in Coates et al. (2011), which shows that increasing the capacity of certain neural net methods quickly reaches a point of diminishing returns on the test error. These results have since been extended to other types of auto-encoders and RBMs (Rifai et al., 2011; Courville et al., 2011). Furthermore, Coates et al. (2011) shows that while neural net methods fail to leverage added capacity K-Means can. This has allowed K-Means to reach state-of-the-art performance on CIFAR-10 for methods that do not use artificial transformations. This is an unexpected result because K-Means is a much dumber unsupervised learning algorithm when compared with RBMs and regularized auto-encoders. Coates et al. (2011) argues that this is mainly due to K-Means making better use of added capacity, but it does not explain why the neural net methods failed to do this.", "startOffset": 330, "endOffset": 1031}, {"referenceID": 10, "context": "For instance, Krizhevsky et al. (2012) was able to reduce the error by half recently.", "startOffset": 14, "endOffset": 39}, {"referenceID": 4, "context": "Improvements on ImageNet are thought to be a good proxy for progress in object recognition (Deng et al., 2009).", "startOffset": 91, "endOffset": 110}, {"referenceID": 0, "context": "We initialize the weights of the hidden layer according to the formula proposed by Glorot and Bengio (2010). The parameters of the classification layer are initialized to 0, along with all the bias (offset) parameters of the MLP.", "startOffset": 94, "endOffset": 108}, {"referenceID": 1, "context": "The experiments are run on a cluster of Nvidia Geforce GTX 580 GPUs with the help of the Theano library (Bergstra et al., 2010).", "startOffset": 104, "endOffset": 127}, {"referenceID": 6, "context": "We make use of HDF5 (Folk et al., 2011) to load the dataset in a lazy fashion because of its large size.", "startOffset": 20, "endOffset": 39}, {"referenceID": 12, "context": "For example, second order methods (Martens, 2010) and natural gradient methods (Le Roux et al.", "startOffset": 34, "endOffset": 49}, {"referenceID": 0, "context": "Based on past observations (Bengio, 2009; Erhan et al., 2010), we expect this optimization problem to worsen for deeper networks, and our experimental setup should be extended to measure the effect of depth.", "startOffset": 27, "endOffset": 61}, {"referenceID": 5, "context": "Based on past observations (Bengio, 2009; Erhan et al., 2010), we expect this optimization problem to worsen for deeper networks, and our experimental setup should be extended to measure the effect of depth.", "startOffset": 27, "endOffset": 61}], "year": 2017, "abstractText": "This article exposes the failure of some big neural networks to leverage added capacity to reduce underfitting. Past research suggest diminishing returns when increasing the size of neural networks. Our experiments on ImageNet LSVRC-2010 show that this may be due to the fact that bigger networks underfit the training objective, sometimes performing worse on the training set than smaller networks. This suggests that the optimization method first order gradient descent fails at this regime. Directly attacking this problem, either through the optimization method or the choices of parametrization, may allow to improve the generalization error on large datasets, for which a large capacity is required.", "creator": "LaTeX with hyperref package"}}}