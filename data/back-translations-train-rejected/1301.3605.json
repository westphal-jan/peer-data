{"id": "1301.3605", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks", "abstract": "Recent studies have shown that deep neural networks (DNNs) perform significantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper we argue that the difficulty in speech recognition is primarily caused by the high variability in speech signals. DNNs, which can be considered a joint model of a nonlinear feature transform and a log-linear classifier, achieve improved recognition accuracy by extracting discriminative internal representations that are less sensitive to small perturbations in the input features. However, if test samples are very dissimilar to training samples, DNNs perform poorly. We demonstrate these properties empirically using a series of recognition experiments on mixed narrowband and wideband speech and speech distorted by environmental noise.", "histories": [["v1", "Wed, 16 Jan 2013 07:23:19 GMT  (141kb,D)", "http://arxiv.org/abs/1301.3605v1", "ICLR 2013, 9 pages, 4 figures"], ["v2", "Mon, 21 Jan 2013 07:42:07 GMT  (144kb,D)", "http://arxiv.org/abs/1301.3605v2", "ICLR 2013, 9 pages, 4 figures"], ["v3", "Fri, 8 Mar 2013 19:42:37 GMT  (145kb,D)", "http://arxiv.org/abs/1301.3605v3", "ICLR 2013, 9 pages, 4 figures"]], "COMMENTS": "ICLR 2013, 9 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["dong yu", "michael l seltzer", "jinyu li", "jui-ting huang", "frank seide"], "accepted": false, "id": "1301.3605"}, "pdf": {"name": "1301.3605.pdf", "metadata": {"source": "CRF", "title": "Feature Learning in Deep Neural Networks \u2013 Studies on Speech Recognition Tasks", "authors": ["Dong Yu", "Michael L. Seltzer", "Jinyu Li", "Jui-Ting Huang"], "emails": ["dongyu@microsoft.com", "mseltzer@microsoft.com", "jinyli@microsoft.com", "jthuang@microsoft.com", "fseide@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Automatic Speech Recognition (ASR) has been an active field of research for more than five decades. However, the performance of ASR systems is far from satisfactory, and the gap between ASR and human speech recognition is still wide for most tasks. This gap becomes more apparent as we move from speech in controlled conditions to real-world applications that require a degree of mismatch between training and test conditions. For example, speakers can set different accents, pronounce different dialects, speak in different styles, speak in different emotional states, and spontaneously deal with coarticulation, reduction, and hesitation."}, {"heading": "2 Deep Neural Networks", "text": "A deep neural network (DNN) is conventionally multi-layer perceptron (MLP) with many hidden layers. If the input and output of the DNN are called x and y, a DNN can be interpreted as a directional graphical model that approximates the posterior probabilities of hidden binary vectors hl (y = s | x) of a class s with an observation vector x, as a stack of (L + 1) layers of log-linear models. The first L layers model the posterior probabilities of hidden binary vectors hl given input vectors vectors vl. If hl consists of hidden units, each designated as hlj, the posterior probabilities can be expressed (hl | vl) = N l, j = 1ez l j (v l) hljez l j (v l) j (v l) 0wo zl."}, {"heading": "3 Invariant and discriminative features", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Deeper is better", "text": "The use of DNNs instead of flat MLPs is a key component for the success of CD-DNN HMMs. Table 1, which is extracted from [3], summarizes the word error rates (WHO) on the PBX [12] Eval 2000 test set using the 309-hour training set whose senone alignment label is generated from a maximum probability (ML) trained GMM system. PBX is a corpus of telephone conversations. From this table, we can conclude that deeper networks outperform shallow ones. WHO decreases as the number of hidden layers increases, using a fixed layer size of 2000 hidden units. In other words, deeper models have stronger discriminatory capabilities than flat models. This is also reflected in the improvement of the training criterion (not shown). More interesting is that architectures with an equal number of parameters are compared by ER, the depth layers are increased by the continuous and the 17%."}, {"heading": "3.2 DNN learns more invariant features", "text": "This is because many layers of simple nonlinear processing can produce a complicated nonlinear transformation. To show that this nonlinear transformation is robust to small variations in the input characteristics, we assume that the output of the layer l \u2212 1 or equivalent will change the input into the layer l from vl to vl + \u03b4l, which is a small change. This change will cause the output of the layer l, or equivalent the input into the layer l + 1 to change the layer l + 1 = Perspective capability (vl + \u03b4l) \u2212 n (vl) \u00b2 diag (zl (vl)))) T-layer l, or equivalent the input into the layer l + 1 to change the layer l + 1 = Perspectivity (vl) \u2212 n-layer (vl) is greater than the layer (wl) T-layer in the layer (vl)."}, {"heading": "4 Learning by seeing", "text": "In this section, we point out that the above result is applicable only to minor disturbances in the training samples. However, if the test samples differ significantly from the training samples, DNNs cannot accurately classify them. In other words, DNNs must see examples of representative variations in the data during training in order to generalize similar variations in the test data. We demonstrate this point with a mixed-bandwidth ASR study. Typical speech recognition systems are trained either on narrow-band speech signals recorded at 8 kHz or broad-band speech signals recorded at 16 kHz. It would be beneficial if a single system could detect both narrow-band and broad-band speech."}, {"heading": "5 Robustness to environmental distortions", "text": "In fact, it is the case that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they are able to"}, {"heading": "6 Conclusion", "text": "In this paper, we demonstrated through speech recognition experiments that higher-level DNNs can extract more invariable and discriminatory features. In other words, the features learned from DNNs are less sensitive to small input interference. This feature allows DNNs to generalize better than flat networks and allows CD-DNN HMMs to perform speech recognition in a way that is more robust to discrepancies in loudspeaker, environment, or bandwidth. On the other hand, DNNs cannot learn from scratch. They require representative samples to perform well. By using a multi-level training strategy and generalizing DNNs to similar patterns, we have achieved the best result ever reported on the Aurora 4 Noise Robustness Benchmark task without the need for multiple recognition credentials and model adjustments."}], "references": [{"title": "Roles of pretraining and fine-tuning in context-dependent DBN- HMMs for real-world speech recognition", "author": ["D. Yu", "L. Deng", "G. Dahl"], "venue": "Proc. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Context-dependent pretrained deep neural networks for large vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "IEEE Trans. Audio, Speech, and Lang. Proc., vol. 20, no. 1, pp. 33\u201342, Jan. 2012.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["F. Seide", "G. Li", "D. Yu"], "venue": "Proc. Interspeech, 2011.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Feature engineering in context-dependent deep neural networks for conversational speech transcription", "author": ["F. Seide", "G.Li", "X. Chen", "D. Yu"], "venue": "Proc. ASRU, 2011, pp. 24\u201329.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting sparseness in deep neural networks for large vocabulary speech recognition", "author": ["D. Yu", "F. Seide", "G.Li", "L. Deng"], "venue": "Proc. ICASSP, 2012, pp. 4409\u20134412.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "An application of pretrained deep neural networks to large vocabulary conversational speech recognition", "author": ["N. Jaitly", "P. Nguyen", "A. Senior", "V. Vanhoucke"], "venue": "Tech. Rep. Tech. Rep. 001, Department of Computer Science, University of Toronto, 2012. 8", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Making deep belief networks effective for large vocabulary continuous speech recognition", "author": ["T.N. Sainath", "B. Kingsbury", "B. Ramabhadran", "P. Fousek", "P. Novak", "A. r. Mohamed"], "venue": "Proc. ASRU, 2011, pp. 30\u201335.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Large vocabulary continuous speech recognition with context-dependent dbn-hmms", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "Proc. ICASSP, 2011, pp. 4688\u20134691.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Mean field theory for sigmoid belief networks", "author": ["Lawrence Saul", "Tommi Jaakkola", "Michael I. Jordan"], "venue": "Journal of Artificial Intelligence Research, vol. 4, pp. 61\u201376, 1996.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "Using continuous features in the maximum entropy model", "author": ["D. Yu", "L. Deng", "A. Acero"], "venue": "Pattern Recognition Letters, vol. 30, no. 14, pp. 1295\u20131300, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep-structured hidden conditional random fields for phonetic recognition", "author": ["D. Yu", "L. Deng"], "venue": "Proc. Interspeech, 2010, pp. 2986\u20132989.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Switchboard-1 Release 2, Linguistic Data", "author": ["J. Godfrey", "E. Holliman"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Improving wideband speech recognition using mixedbandwidth training data in CD-DNN-HMM", "author": ["J. Li", "D. Yu", "J.-T. Huang", "Y. Gong"], "venue": "Proc. SLT, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "HMM Adaptation Using Vector Taylor Series for Noisy Speech Recognition", "author": ["A. Acero", "L. Deng", "T. Kristjansson", "J. Zhang"], "venue": "Proc. of ICSLP, 2000.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Maximum likelihood linear transformations for HMM-based speech recognition", "author": ["M.J.F. Gales"], "venue": "Computer Speech and Language, vol. 12, pp. 75\u201398, 1998.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Aurora working group: DSR front end LVCSR evaluation AU/384/02", "author": ["N. Parihar", "J. Picone"], "venue": "Tech. Rep., Inst. for Signal and Information Process, Mississippi State University.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 0}, {"title": "Minimum phone error and i-smoothing for improved discriminative training", "author": ["D. Povey", "P.C. Woodland"], "venue": "Proc. ICASSP, 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Noise adaptive training for robust automatic speech recognition", "author": ["O. Kalinli", "M.L. Seltzer", "J. Droppo", "A. Acero"], "venue": "IEEE Trans. on Audio, Speech, and Language Proc., vol. 18, no. 8, pp. 1889 \u20131901, Nov. 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1889}, {"title": "Discriminative adaptive training with VTS and JUD", "author": ["F. Flego", "M.J.F. Gales"], "venue": "Proc. ASRU, 2009, pp. 170\u2013175.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Derivative kernels for noise robust ASR", "author": ["A. Ragni", "M.J.F. Gales"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Speaker and noise factorisation for robust speech recognition", "author": ["Y.-Q. Wang", "M.J.F. Gales"], "venue": "IEEE Trans. on Audio Speech and Language Proc., vol. 20, no. 7, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive training with joint uncertainty decoding for robust recognition of noisy data", "author": ["H. Liao", "M.J.F. Gales"], "venue": "Proc. of ICASSP, Honolulu, Hawaii, 2007. 9", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 1, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 2, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 3, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 4, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 5, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 6, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 7, "context": "By using the context-dependent deep neural network hidden Markov models (CD-DNN-HMMs), a number of research groups have obtained strong recognition results on a variety of large scale speech tasks [1\u20138].", "startOffset": 197, "endOffset": 202}, {"referenceID": 8, "context": "An effective practical trick is to replace the marginalization with the mean-field approximation [9] at each hidden layer.", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "In the conventional MaxEnt model, features are manually designed [10].", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "For example in [11] the first and second-order statistics of the observations are used as features.", "startOffset": 15, "endOffset": 19}, {"referenceID": 0, "context": "In all the following discussions, we use DNNs in the framework of the CD-DNN-HMM [1\u20135] and use speech recognition as our classification task.", "startOffset": 81, "endOffset": 86}, {"referenceID": 1, "context": "In all the following discussions, we use DNNs in the framework of the CD-DNN-HMM [1\u20135] and use speech recognition as our classification task.", "startOffset": 81, "endOffset": 86}, {"referenceID": 2, "context": "In all the following discussions, we use DNNs in the framework of the CD-DNN-HMM [1\u20135] and use speech recognition as our classification task.", "startOffset": 81, "endOffset": 86}, {"referenceID": 3, "context": "In all the following discussions, we use DNNs in the framework of the CD-DNN-HMM [1\u20135] and use speech recognition as our classification task.", "startOffset": 81, "endOffset": 86}, {"referenceID": 4, "context": "In all the following discussions, we use DNNs in the framework of the CD-DNN-HMM [1\u20135] and use speech recognition as our classification task.", "startOffset": 81, "endOffset": 86}, {"referenceID": 0, "context": "The detailed training procedure and decoding technique for CD-DNN-HMMs can be found in [1\u20133].", "startOffset": 87, "endOffset": 92}, {"referenceID": 1, "context": "The detailed training procedure and decoding technique for CD-DNN-HMMs can be found in [1\u20133].", "startOffset": 87, "endOffset": 92}, {"referenceID": 2, "context": "The detailed training procedure and decoding technique for CD-DNN-HMMs can be found in [1\u20133].", "startOffset": 87, "endOffset": 92}, {"referenceID": 2, "context": "Table 1, which is extracted from [3], summarizes the word error rates (WER) on the Switchboard (SWB) [12] Eval 2000 test set using the 309-hour training set whose senone alignment label was generated from a maximum likelihood (ML) trained GMM system.", "startOffset": 33, "endOffset": 36}, {"referenceID": 11, "context": "Table 1, which is extracted from [3], summarizes the word error rates (WER) on the Switchboard (SWB) [12] Eval 2000 test set using the 309-hour training set whose senone alignment label was generated from a maximum likelihood (ML) trained GMM system.", "startOffset": 101, "endOffset": 105}, {"referenceID": 12, "context": "One such system was recently proposed using a CD-DNN-HMM [13].", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "GMM-based acoustic models are highly sensitive to environmental mismatch and as a result, several techniques to normalize of the input features or adapt the model parameters have been developed, such as Vector Taylor Series (VTS) adaption [14] and Maximum Likelihood", "startOffset": 239, "endOffset": 243}, {"referenceID": 14, "context": "Linear Regression (MLLR) [15].", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "We performed a series of experiments on the Aurora 4 corpus [16], a 5000-word vocabulary task based on the Wall Street Journal (WSJ0) corpus.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "The second system combines Minimum Phone Error (MPE) discriminative training [17] and noise adaptive training (NAT) [18] using VTS adaptation to compensate for noise and channel mismatch [19].", "startOffset": 77, "endOffset": 81}, {"referenceID": 17, "context": "The second system combines Minimum Phone Error (MPE) discriminative training [17] and noise adaptive training (NAT) [18] using VTS adaptation to compensate for noise and channel mismatch [19].", "startOffset": 116, "endOffset": 120}, {"referenceID": 18, "context": "The second system combines Minimum Phone Error (MPE) discriminative training [17] and noise adaptive training (NAT) [18] using VTS adaptation to compensate for noise and channel mismatch [19].", "startOffset": 187, "endOffset": 191}, {"referenceID": 19, "context": "The third system uses a hybrid generative/discriminative classifier [20] as follows .", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "The fourth system uses an HMM trained with NAT and combines VTS adaptation for environment compensation and MLLR for speaker adaptation [21].", "startOffset": 136, "endOffset": 140}, {"referenceID": 18, "context": "6 MPE-NAT + VTS [19] 7.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": "3 NAT + Derivative Kernels [20] 7.", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "8 NAT + Joint MLLR/VTS [21] 5.", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "It is noteworthy that to obtain good performance, the GMM-based systems required complicated adaptive training procedures [18, 22] and multiple iterations of recognition in order to perform explicit environment and/or speaker adaptation.", "startOffset": 122, "endOffset": 130}, {"referenceID": 21, "context": "It is noteworthy that to obtain good performance, the GMM-based systems required complicated adaptive training procedures [18, 22] and multiple iterations of recognition in order to perform explicit environment and/or speaker adaptation.", "startOffset": 122, "endOffset": 130}], "year": 2017, "abstractText": "Recent studies have shown that deep neural networks (DNNs) perform significantly better than shallow networks and Gaussian mixture models (GMMs) on large vocabulary speech recognition tasks. In this paper we argue that the difficulty in speech recognition is primarily caused by the high variability in speech signals. DNNs, which can be considered a joint model of a nonlinear feature transform and a log-linear classifier, achieve improved recognition accuracy by extracting discriminative internal representations that are less sensitive to small perturbations in the input features. However, if test samples are very dissimilar to training samples, DNNs perform poorly. We demonstrate these properties empirically using a series of recognition experiments on mixed narrowband and wideband speech and speech distorted by environmental noise.", "creator": "LaTeX with hyperref package"}}}