{"id": "1301.3614", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Joint Space Neural Probabilistic Language Model for Statistical Machine Translation", "abstract": "A neural probabilistic language model (NPLM) provide an idea to achieve the better perplexity than n-gram language model and their smoothed language models. This paper investigates application area in bilingual NLP, specifically Statistical Machine Translation (SMT). We focus on the perspectives that NPLM has potential to open the possibility to complement potentially `huge' monolingual resources into the `resource-constraint' bilingual resources. In order to facilitate the application to various tasks, we propose the joint space model of ngram-HMM language model. We show two experiments in SMT: system combination and word alignment.", "histories": [["v1", "Wed, 16 Jan 2013 07:56:20 GMT  (280kb)", "https://arxiv.org/abs/1301.3614v1", null], ["v2", "Sun, 20 Jan 2013 22:58:41 GMT  (56kb)", "http://arxiv.org/abs/1301.3614v2", null], ["v3", "Fri, 21 Apr 2017 02:42:35 GMT  (56kb)", "http://arxiv.org/abs/1301.3614v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tsuyoshi okita"], "accepted": false, "id": "1301.3614"}, "pdf": {"name": "1301.3614.pdf", "metadata": {"source": "CRF", "title": "Joint Space Neural Probabilistic Language Model for Statistical Machine Translation", "authors": ["Tsuyoshi Okita"], "emails": ["tokita@computing.dcu.ie"], "sections": [{"heading": null, "text": "ar Xiv: 130 1.36 14v3 [cs.CL] 2 1"}, {"heading": "1 Introduction", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2 Ngram-HMM Language Model", "text": "We will consider an example of ngram-HMM language model, i.e. hi-gramHMM language model in this case, in blue (in the middle).We will consider a Hidden Markov model (HMM) [40, 21, 2] of size K, the n-gram word sequence wi,., wi-K + 1, in which hi-ney smoothing [26], High-Turing smoothing [24], and hierarchical Pitman-Yor LM smoothing [48].In the left hand side of Figure 1, we will introduce a Dirichlet Process DP (\u03b1, H), with concentration parameters and base measure H, for the transition probabilities emanating from each hidden state."}, {"heading": "3 Joint Space Model", "text": "In this essay, we mechanically introduce a common spatial model. Unlike the ngram-HMM language model obtained in the previous section, we will often encounter the situation where we have another hidden variable h1 that is irrelevant to h0 that is shown in Figure 2. Suppose we have the ngram-HMM language model that provides the hidden variables that suggest a semantic and syntactical role of words. In addition, we may also have another hidden variable that suggests, for example, a genre ID. This genre ID can be considered the second context that is often not closely related to the first context. This also has an advantage in this mechanical construction, that the resulting language model is often smaller in the confusion than the original ngram-HMM language model. Note that we do not intend to learn this model jointly based on the universal criteria, but we just want to link the descriptions to this same language model by using different sequences."}, {"heading": "4 Intrinsic Evaluation", "text": "We compared the perplexity of ngram-HMMLM (1 feature), ngram-HMMLM (2 features, same as in this work and genre ID is 4 class), modified Kneser-Ney smoothing (irstlm) [18] and hierarchical Pitman Yor LM [48]."}, {"heading": "5 Extrinsic Evaluation: Task of System Combination", "text": "We present the second feature in this paragraph, which is the motivation for the study."}], "references": [{"title": "Computing consensus translation from multiple machine translation systems", "author": ["S. BANGALORE", "G. BORDEL", "G. RICCARDI"], "venue": "In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Variational algorithms for approximate bayesian inference", "author": ["M.J. BEAL"], "venue": "PhD Thesis at Gatsby Computational Neuroscience Unit, University College London", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "A neural probabilistic language model", "author": ["Y. BENGIO", "R. DUCHARME", "P. VINCENT"], "venue": "In Proceedings of Neural Information Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Neural probabilistic language models", "author": ["Y. BENGIO", "H. SCHWENK", "SEN\u00c9CAL", "J.-S", "F. MORIN", "GAUVAIN", "J.-L"], "venue": "Innovations in Machine Learning: Theory and Applications Edited", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Latent dirichlet allocation", "author": ["D. BLEI", "A.Y. NG", "M.I. JORDAN"], "venue": "Journal of Machine Learning Research", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Introduction to probabilistic topic models", "author": ["D.M. BLEI"], "venue": "Communications of the ACM", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Towards open-text semantic parsing via multi-task learning of structured embeddings", "author": ["A. BORDES", "X. GLOROT", "J. WESTON", "Y. BENGIO"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["S. CHEN", "J. GOODMAN"], "venue": "Technical report TR-10-98 Harvard University", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Deep learning for efficient discriminative parsing", "author": ["R. COLLOBERT"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics (AISTATS)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. COLLOBERT", "J. WESTON"], "venue": "In International Conference on Machine Learning", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. COLLOBERT", "J. WESTON", "L. BOTTOU", "M. KARLEN", "K. KAVUKCUOGLU", "P. KUKSA"], "venue": "Journal of Machine Learning Research", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Fast consensus decoding over translation forests", "author": ["J. DENERO", "D. CHIANG", "K. KNIGHT"], "venue": "In proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "The latent words language model", "author": ["K. DESCHACHT", "J.D. BELDER", "MOENS", "M.-F"], "venue": "Computer Speech and Language", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "MaTrEx: the DCU MT System for WMT", "author": ["DU J", "HE Y", "PENKALE S", "WAY"], "venue": "In Proceedings of the Third EACL Workshop on Statistical Machine Translation", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "An incremental three-pass system combination framework by combining multiple hypothesis alignment methods", "author": ["DU J", "WAY"], "venue": "International Journal of Asian Language Processing 20,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Using terp to augment the system combination for smt", "author": ["DU J", "WAY"], "venue": "In Proceedings of the Ninth Conference of the Association for Machine Translation", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Irstlm: an open source toolkit for handling large scale language models", "author": ["M. FEDERICO", "N. BERTOLDI", "M. CETTOLO"], "venue": "Proceedings of Interspeech", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "The infinite hmm for unsupervised pos tagging", "author": ["J.V. GAEL", "A. VLACHOS", "Z. GHAHRAMANI"], "venue": "The 2009 Conference on Empirical Methods on Natural Language Processing (EMNLP", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Lossless compression based on the sequence memoizer", "author": ["J. GASTHAUS", "F. WOOD", "TEH", "Y. W"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "An introduction to hidden markov models and bayesian networks", "author": ["Z. GHAHRAMANI"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence 15,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Factorial hidden markov models", "author": ["Z. GHAHRAMANI", "M.I. JORDAN", "P. SMYTH"], "venue": "Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Contextual dependencies in unsupervised word segmentation", "author": ["S. GOLDWATER", "T.L. GRIFFITHS", "M. JOHNSON"], "venue": "In Proceedings of Conference on Computational Linguistics / Association for Computational Linguistics", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2006}, {"title": "The population frequencies of species and the estimation of population paramters", "author": ["I.J. GOOD"], "venue": "Biometrika 40,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1953}, {"title": "Distributed representations. Parallel Distributed Processing: Explorations in the Microstructure of Cognition(Edited by D.E", "author": ["G.E. HINTON", "J.L. MCCLELLAND", "D. RUMELHART"], "venue": "Rumelhart and J.L. McClelland) MIT Press", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1986}, {"title": "Improved backing-off for n-gram language modeling", "author": ["KNESER R", "NEY"], "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1995}, {"title": "Probabilistic graphical models: Principles and techniques", "author": ["D. KOLLER", "N. FRIEDMAN"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Minimum Bayes-Risk word alignment of bilingual texts", "author": ["S. KUMAR", "W. BYRNE"], "venue": "In Proceedings of the Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2002}, {"title": "Computing consensus translation frommultiple machine translation systems using enhanced hypotheses alignment", "author": ["E. MATUSOV", "N. UEFFING", "NEY"], "venue": "In Proceedings of the 11st Conference of the European Chapter of the Association for Computational Linguistics (EACL)", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2006}, {"title": "Wordnet: A lexical database for english", "author": ["G.A. MILLER"], "venue": "Communications of the ACM 38,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1995}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["MNIH A", "TEH", "W. Y"], "venue": "In Proceedings of the International Conference on Machine Learning", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Bayesian unsupervised word segmentation with nested pitman-yor language modeling", "author": ["D. MOCHIHASHI", "T. YAMADA", "N. UEDA"], "venue": "In Proceedings of Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACL-IJCNLP", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2009}, {"title": "Machine learning: A probabilistic perspective", "author": ["K.P. MURPHY"], "venue": "The MIT Press", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "A systematic comparison of various statistical alignment models", "author": ["OCH F", "NEY"], "venue": "Computational Linguistics 29,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Minimum bayes risk decoding with enlarged hypothesis space in system combination", "author": ["T. OKITA", "J. VAN GENABITH"], "venue": "In Proceedings of the 13th International Conference on Intelligent Text Processing and Computational Linguistics (CICLING", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Hierarchical pitman-yor language model in machine translation", "author": ["OKITA T", "WAY"], "venue": "In Proceedings of the International Conference on Asian Language Processing (IALP", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "AND WAY, A. Pitman-Yor process-based language model for Machine Translation", "author": ["T. OKITA"], "venue": "International Journal on Asian Language Processing 21,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2010}, {"title": "AND WAY, A. Given bilingual terminology in statistical machine translation: Mwe-sensitve word alignment and hierarchical pitman-yor processbased translation model smoothing", "author": ["T. OKITA"], "venue": "In Proceedings of the 24th International Florida Artificial Intelligence Research Society Conference", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}, {"title": "Evaluating machine translation with LFG dependencies", "author": ["K. OWCZARZAK", "J. VAN GENABITH", "WAY"], "venue": "Machine Translation 21,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. RABINER"], "venue": "Proceedings of the IEEE 77,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1989}, {"title": "Continuous space language models", "author": ["H. SCHWENK"], "venue": "Computer Speech and Language", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2007}, {"title": "Continuous space language models for statistical machine translation", "author": ["H. SCHWENK"], "venue": "The Prague Bulletin of Mathematical Linguistics", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2010}, {"title": "Large, pruned or continuous space language models on a gpu for statistical machine translation", "author": ["H. SCHWENK", "A. ROUSSEAU", "M. ATTIK"], "venue": "In Proceeding of the NAACL workshop on the Future of Language Modeling", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Approximate inference in graphical models using LP relaxations", "author": ["D. SONTAG"], "venue": "Massachusetts Institute of Technology (Ph.D. thesis)", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "The complexity of inference in latent dirichlet allocation", "author": ["SONTAG D", "ROY", "M. D"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2011}, {"title": "Probabilistic topic models", "author": ["M. STEYVERS", "T. GRIFFITHS"], "venue": "Handbook of Latent Semantic Analysis. Psychology", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2007}, {"title": "SRILM \u2013 An extensible language modeling toolkit", "author": ["A. STOLCKE"], "venue": "In Proceedings of the International Conference on Spoken Language Processing", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2002}, {"title": "A hierarchical bayesian language model based on pitman-yor processes", "author": ["TEH Y. W"], "venue": "In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL-06),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2006}, {"title": "Lattice minimum bayes-risk decoding for statistical machine translation", "author": ["R. TROMBLE", "S. KUMAR", "F. OCH", "W. MACHEREY"], "venue": "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2008}, {"title": "Exploding the creativity myth: The computational foundations of linguistic creativity. London: Bloomsbury", "author": ["T. VEALE"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2012}, {"title": "A stochastic memoizer for sequence data", "author": ["F. WOOD", "C. ARCHAMBEAU", "J. GASTHAUS", "L. JAMES", "TEH", "Y. W"], "venue": "In Proceedings of the 26th International Conference on Machine Learning", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2009}], "referenceMentions": [{"referenceID": 2, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 45, "endOffset": 51}, {"referenceID": 3, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 45, "endOffset": 51}, {"referenceID": 23, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 88, "endOffset": 92}, {"referenceID": 45, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 169, "endOffset": 173}, {"referenceID": 24, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 209, "endOffset": 220}, {"referenceID": 7, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 209, "endOffset": 220}, {"referenceID": 46, "context": "A neural probabilistic language model (NPLM) [3, 4] and the distributed representations [25] provide an idea to achieve the better perplexity than n-gram language model [47] and their smoothed language models [26, 9, 48].", "startOffset": 209, "endOffset": 220}, {"referenceID": 46, "context": "smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38].", "startOffset": 156, "endOffset": 160}, {"referenceID": 49, "context": "smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38].", "startOffset": 188, "endOffset": 196}, {"referenceID": 18, "context": "smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38].", "startOffset": 188, "endOffset": 196}, {"referenceID": 34, "context": "smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38].", "startOffset": 230, "endOffset": 242}, {"referenceID": 35, "context": "smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38].", "startOffset": 230, "endOffset": 242}, {"referenceID": 36, "context": "smoothed language model, has had a lot of developments in the line of nonparametric Bayesian methods such as hierarchical Pitman-Yor language model (HPYLM) [48] and Sequence Memoizer (SM) [51, 20], including an application to SMT [36, 37, 38].", "startOffset": 230, "endOffset": 242}, {"referenceID": 2, "context": "A NPLM of [3] implemented this using the multi-layer neural network and yielded 20% to 35% better perplexity than the language model with the modified Kneser-Ney methods [9].", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "A NPLM of [3] implemented this using the multi-layer neural network and yielded 20% to 35% better perplexity than the language model with the modified Kneser-Ney methods [9].", "startOffset": 170, "endOffset": 173}, {"referenceID": 39, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 9, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 40, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 8, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 10, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 12, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 41, "context": "There are several successful applications of NPLM [41, 11, 42, 10, 12, 14, 43].", "startOffset": 50, "endOffset": 78}, {"referenceID": 10, "context": "First, one category of applications include POS tagging, NER tagging, and parsing [12, 7].", "startOffset": 82, "endOffset": 89}, {"referenceID": 6, "context": "First, one category of applications include POS tagging, NER tagging, and parsing [12, 7].", "startOffset": 82, "endOffset": 89}, {"referenceID": 10, "context": "Second, the other category of applications include Semantic Role Labeling (SRL) task [12, 14].", "startOffset": 85, "endOffset": 93}, {"referenceID": 12, "context": "Second, the other category of applications include Semantic Role Labeling (SRL) task [12, 14].", "startOffset": 85, "endOffset": 93}, {"referenceID": 40, "context": "Third, the final category includes MERT process [42]", "startOffset": 48, "endOffset": 52}, {"referenceID": 9, "context": "Although most of the applications described in [11, 10, 12, 14] are monolingual tasks, the application of this approach to a bilingual task introduces really astonishing aspects, which we can call \u201ccreative words\u201d [50], automatically into the traditional resource constrained SMT components.", "startOffset": 47, "endOffset": 63}, {"referenceID": 8, "context": "Although most of the applications described in [11, 10, 12, 14] are monolingual tasks, the application of this approach to a bilingual task introduces really astonishing aspects, which we can call \u201ccreative words\u201d [50], automatically into the traditional resource constrained SMT components.", "startOffset": 47, "endOffset": 63}, {"referenceID": 10, "context": "Although most of the applications described in [11, 10, 12, 14] are monolingual tasks, the application of this approach to a bilingual task introduces really astonishing aspects, which we can call \u201ccreative words\u201d [50], automatically into the traditional resource constrained SMT components.", "startOffset": 47, "endOffset": 63}, {"referenceID": 12, "context": "Although most of the applications described in [11, 10, 12, 14] are monolingual tasks, the application of this approach to a bilingual task introduces really astonishing aspects, which we can call \u201ccreative words\u201d [50], automatically into the traditional resource constrained SMT components.", "startOffset": 47, "endOffset": 63}, {"referenceID": 48, "context": "Although most of the applications described in [11, 10, 12, 14] are monolingual tasks, the application of this approach to a bilingual task introduces really astonishing aspects, which we can call \u201ccreative words\u201d [50], automatically into the traditional resource constrained SMT components.", "startOffset": 214, "endOffset": 218}, {"referenceID": 41, "context": "Although most of this line has not been even tested mostly due to the problem of computational complexity of training NPLM, [43] applied this to MERT process which reranks the n-best lists using NPLM.", "startOffset": 124, "endOffset": 128}, {"referenceID": 0, "context": "This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35].", "startOffset": 64, "endOffset": 87}, {"referenceID": 27, "context": "This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35].", "startOffset": 64, "endOffset": 87}, {"referenceID": 47, "context": "This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35].", "startOffset": 64, "endOffset": 87}, {"referenceID": 13, "context": "This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35].", "startOffset": 64, "endOffset": 87}, {"referenceID": 11, "context": "This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35].", "startOffset": 64, "endOffset": 87}, {"referenceID": 33, "context": "This paper aims at different task, a task of system combination [1, 29, 49, 15, 13, 35].", "startOffset": 64, "endOffset": 87}, {"referenceID": 25, "context": "This category of tasks employs the sequential method such as Maximum A Posteriori (MAP) inference (Viterbi decoding) [27, 44, 33] on Conditional Random Fields (CRFs) / Markov Random Fields (MRFs).", "startOffset": 117, "endOffset": 129}, {"referenceID": 42, "context": "This category of tasks employs the sequential method such as Maximum A Posteriori (MAP) inference (Viterbi decoding) [27, 44, 33] on Conditional Random Fields (CRFs) / Markov Random Fields (MRFs).", "startOffset": 117, "endOffset": 129}, {"referenceID": 31, "context": "This category of tasks employs the sequential method such as Maximum A Posteriori (MAP) inference (Viterbi decoding) [27, 44, 33] on Conditional Random Fields (CRFs) / Markov Random Fields (MRFs).", "startOffset": 117, "endOffset": 129}, {"referenceID": 17, "context": "Although this paper discusses an ngram-HMM language model which we introduce as one model of NPLM where we borrow many of the mechanism from infinite HMM [19] and hierarchical PitmanYor LM [48], one main contribution would be to show one new application area of NPLM in SMT.", "startOffset": 154, "endOffset": 158}, {"referenceID": 46, "context": "Although this paper discusses an ngram-HMM language model which we introduce as one model of NPLM where we borrow many of the mechanism from infinite HMM [19] and hierarchical PitmanYor LM [48], one main contribution would be to show one new application area of NPLM in SMT.", "startOffset": 189, "endOffset": 193}, {"referenceID": 38, "context": "We consider a Hidden Markov Model (HMM) [40, 21, 2] of size K which emits n-gram word sequence wi, .", "startOffset": 40, "endOffset": 51}, {"referenceID": 19, "context": "We consider a Hidden Markov Model (HMM) [40, 21, 2] of size K which emits n-gram word sequence wi, .", "startOffset": 40, "endOffset": 51}, {"referenceID": 1, "context": "We consider a Hidden Markov Model (HMM) [40, 21, 2] of size K which emits n-gram word sequence wi, .", "startOffset": 40, "endOffset": 51}, {"referenceID": 24, "context": "The arcs from wi\u22123 to wi, \u00b7 \u00b7 \u00b7 , wi\u22121 to wi show the back-off relations appeared in language model smoothing, such as Kneser-Ney smoothing [26], Good-Turing smoothing [24], and hierarchical Pitman-Yor LM smoothing [48].", "startOffset": 140, "endOffset": 144}, {"referenceID": 22, "context": "The arcs from wi\u22123 to wi, \u00b7 \u00b7 \u00b7 , wi\u22121 to wi show the back-off relations appeared in language model smoothing, such as Kneser-Ney smoothing [26], Good-Turing smoothing [24], and hierarchical Pitman-Yor LM smoothing [48].", "startOffset": 168, "endOffset": 172}, {"referenceID": 46, "context": "The arcs from wi\u22123 to wi, \u00b7 \u00b7 \u00b7 , wi\u22121 to wi show the back-off relations appeared in language model smoothing, such as Kneser-Ney smoothing [26], Good-Turing smoothing [24], and hierarchical Pitman-Yor LM smoothing [48].", "startOffset": 215, "endOffset": 219}, {"referenceID": 1, "context": "This construction is borrowed from the infinite HMM [2, 19].", "startOffset": 52, "endOffset": 59}, {"referenceID": 17, "context": "This construction is borrowed from the infinite HMM [2, 19].", "startOffset": 52, "endOffset": 59}, {"referenceID": 17, "context": "This is since the observations can be regarded as being generated from a dynamic mixture model [19] as in (1), the Dirichlet priors", "startOffset": 95, "endOffset": 99}, {"referenceID": 46, "context": "This construction is borrowed from hierarchical Pitman-Yor language model [48].", "startOffset": 74, "endOffset": 78}, {"referenceID": 17, "context": "Inference We compute the expected value of the posterior distribution of the hidden variables with a beam search [19].", "startOffset": 113, "endOffset": 117}, {"referenceID": 17, "context": "As is mentioned in [19], this sampler has characteristic in that it adaptively truncates the state space and run dynamic programming as in (3):", "startOffset": 19, "endOffset": 23}, {"referenceID": 46, "context": "Initialization First, we obtain the parameters for hierarchical Pitman-Yor process-based language model [48, 23], which can be obtained using a block Gibbs sampling [32].", "startOffset": 104, "endOffset": 112}, {"referenceID": 21, "context": "Initialization First, we obtain the parameters for hierarchical Pitman-Yor process-based language model [48, 23], which can be obtained using a block Gibbs sampling [32].", "startOffset": 104, "endOffset": 112}, {"referenceID": 30, "context": "Initialization First, we obtain the parameters for hierarchical Pitman-Yor process-based language model [48, 23], which can be obtained using a block Gibbs sampling [32].", "startOffset": 165, "endOffset": 169}, {"referenceID": 17, "context": "This EM algorithm incorporates the above mentioned truncation [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 20, "context": "This step aims at obtaining the expected value of the posterior distribution (Similar construction to use expectation can be seen in factored HMM [22]).", "startOffset": 146, "endOffset": 150}, {"referenceID": 16, "context": "We compared the perplexity of ngram-HMMLM (1 feature), ngram-HMMLM (2 features, the same as in this paper and genre ID is 4 class), modified Kneser-Ney smoothing (irstlm) [18], and hierarchical Pitman Yor LM [48].", "startOffset": 171, "endOffset": 175}, {"referenceID": 46, "context": "We compared the perplexity of ngram-HMMLM (1 feature), ngram-HMMLM (2 features, the same as in this paper and genre ID is 4 class), modified Kneser-Ney smoothing (irstlm) [18], and hierarchical Pitman Yor LM [48].", "startOffset": 208, "endOffset": 212}, {"referenceID": 4, "context": "(LDA) [5, 46, 6, 45, 33] to obtain the genre ID via (unsupervised) document classification since our interest here is on the genre of sentences in testset.", "startOffset": 6, "endOffset": 24}, {"referenceID": 44, "context": "(LDA) [5, 46, 6, 45, 33] to obtain the genre ID via (unsupervised) document classification since our interest here is on the genre of sentences in testset.", "startOffset": 6, "endOffset": 24}, {"referenceID": 5, "context": "(LDA) [5, 46, 6, 45, 33] to obtain the genre ID via (unsupervised) document classification since our interest here is on the genre of sentences in testset.", "startOffset": 6, "endOffset": 24}, {"referenceID": 43, "context": "(LDA) [5, 46, 6, 45, 33] to obtain the genre ID via (unsupervised) document classification since our interest here is on the genre of sentences in testset.", "startOffset": 6, "endOffset": 24}, {"referenceID": 31, "context": "(LDA) [5, 46, 6, 45, 33] to obtain the genre ID via (unsupervised) document classification since our interest here is on the genre of sentences in testset.", "startOffset": 6, "endOffset": 24}, {"referenceID": 26, "context": "\u2022 Minimum Bayes Risk decoding [28] (with Minimum Error Rate Training (MERT) process [34])", "startOffset": 30, "endOffset": 34}, {"referenceID": 32, "context": "\u2022 Minimum Bayes Risk decoding [28] (with Minimum Error Rate Training (MERT) process [34])", "startOffset": 84, "endOffset": 88}, {"referenceID": 41, "context": "Similar to the task of n-best reranking in MERT process [43], we consider the reranking of nbest lists in the third step of above, i.", "startOffset": 56, "endOffset": 60}, {"referenceID": 41, "context": "The n-best reranking in MERT process [43] alternate the", "startOffset": 37, "endOffset": 41}, {"referenceID": 12, "context": "The task of WSD [14] can be written as in (6):", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "Note that we use the term \u201csynset\u201d as an analogy of the WordNet [30]: this is equivalent to \u201csense\u201d or \u201cmeaning\u201d.", "startOffset": 64, "endOffset": 68}, {"referenceID": 48, "context": "On the one hand, the paraphrases obtained in this way have attractive aspects that can be called \u201ca creative word\u201d [50].", "startOffset": 115, "endOffset": 119}, {"referenceID": 29, "context": "On the other hand, unfortunately in practice, the notorious training time of NPLM only allows us to use fairly small monolingual corpus although many papers made an effort to reduce it [31].", "startOffset": 185, "endOffset": 189}, {"referenceID": 37, "context": "If we add paraphrases and the resulted sentence has a higher score in terms of the modified dependency score [39] (See Figure 3), this means that the addition of paraphrases is a good choice.", "startOffset": 109, "endOffset": 113}, {"referenceID": 37, "context": "Figure 3: By the modified dependency score [39], the score of these two sentences, \u201cJohn resigned yesterday\u201d and \u201cYesterday John resigned\u201d, are the same.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "We use our system combination module [16, 17, 35], which has its own language modeling tool, MERT process, and MBR decoding.", "startOffset": 37, "endOffset": 49}, {"referenceID": 15, "context": "We use our system combination module [16, 17, 35], which has its own language modeling tool, MERT process, and MBR decoding.", "startOffset": 37, "endOffset": 49}, {"referenceID": 33, "context": "We use our system combination module [16, 17, 35], which has its own language modeling tool, MERT process, and MBR decoding.", "startOffset": 37, "endOffset": 49}], "year": 2017, "abstractText": "A neural probabilistic language model (NPLM) provides an idea to achieve the better perplexity than n-gram language model and their smoothed language models. This paper investigates application area in bilingual NLP, specifically Statistical Machine Translation (SMT). We focus on the perspectives that NPLM has potential to open the possibility to complement potentially \u2018huge\u2019 monolingual resources into the \u2018resource-constraint\u2019 bilingual resources. We introduce an ngram-HMM language model as NPLM using the non-parametric Bayesian construction. In order to facilitate the application to various tasks, we propose the joint space model of ngram-HMM language model. We show an experiment of system combination in the area of SMT. One discovery was that our treatment of noise improved the results 0.20 BLEU points if NPLM is trained in relatively small corpus, in our case 500,000 sentence pairs, which is often the case due to the long training time of NPLM.", "creator": "LaTeX with hyperref package"}}}