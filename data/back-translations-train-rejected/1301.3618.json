{"id": "1301.3618", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Learning New Facts From Knowledge Bases With Neural Tensor Networks and Semantic Word Vectors", "abstract": "Knowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 82.8%.", "histories": [["v1", "Wed, 16 Jan 2013 08:05:35 GMT  (14kb)", "https://arxiv.org/abs/1301.3618v1", null], ["v2", "Sat, 16 Mar 2013 03:23:26 GMT  (11kb)", "http://arxiv.org/abs/1301.3618v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["danqi chen", "richard socher", "christopher d manning", "rew y ng"], "accepted": false, "id": "1301.3618"}, "pdf": {"name": "1301.3618.pdf", "metadata": {"source": "CRF", "title": "Learning New Facts From Knowledge Bases With Neural Tensor Networks and Semantic Word Vectors", "authors": ["Danqi Chen", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng"], "emails": ["danqi@stanford.edu,", "manning@stanford.edu,", "ang@stanford.edu,", "richard@socher.org"], "sections": [{"heading": null, "text": "ar Xiv: 130 1.36 18v2 [cs.CL] 1 6M arKnowledge Databases offer applications the advantage of easily accessible, systematic relationship knowledge, but in practice they often suffer from their incompleteness and lack of knowledge about new units and relationships. Much work has been devoted to building or expanding them by finding patterns in large, uncommented text corpora. In contrast, our main goal here is to complete a knowledge database by predicting additional true relationships between units, based on generalizations that are recognizable in the given knowledge database. We are introducing a neural tensor network (NTN) that predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors that are learned from text in an unsupervised manner, querying existing relationships even for units that were not present in the database."}, {"heading": "1 Introduction", "text": "Ontologies and knowledge bases such as WordNet [1] or Yago [2] are extremely useful resources for expanding queries [3], resolving correferences [4], answering questions (Siri), retrieving information (Google Knowledge Graph), or generally providing structured knowledge inferences to users. Much work focuses on expanding existing knowledge bases [5, 6, 2] using patterns or classifiers applied to large corporations. We are introducing a model that can precisely learn to add additional facts to a database that only uses that database. This is achieved by representing each unit (i.e. each object or individual) in the database by a vector that can capture facts and their certainty about that unit. Each relationship is defined by the parameters of a novel neural tensor network that explicitly relates two vectors to each other, and is more powerful than a standard network model that surrounds the unit."}, {"heading": "2 Related Work", "text": "The work closest to us is that of Bordes et al. [9]. We implement their approach and compare it directly, and our model clearly surpasses it in both accuracy and ranking, both of which can benefit from initialization with uncontrolled word vectors. Another related approach is that of Sutskever et al. [10], which uses tensor factorization and Bayesian clustering to learn relational structures. Instead of clustering the units in a non-parametric Bayesian framework, we rely on scholarly entity vectors. Their calculation of the truth of a relationship can be considered a special case of our proposed model. Instead of using MCMC for conclusions, we use standard backpropagation that is modified for the Neural Tensor Network."}, {"heading": "3 Neural Tensor Networks", "text": "In this section we describe the complete neural tensor network. We begin by describing the representation of entities and continue with the model, the entity relation ship.We compare two randomly initialized word vectors and pre-formed 100-dimensional word vectors from the unattended model of Collobert and Weston [13, 7]. Using free Wikipedia text, this model learns word vectors by predicting how likely it is for each word to occur in its context. The model uses both the local context around each word and the global document context. Similar to other local occurrences based on vector space models, the resulting word is vectors capture syntactical and semantic information. For more details and evaluations of these embeddings see [14, 13, 15].For cases where the entity name has multiple words, we will simply replace the word vector.The neural tensor network (NT) with a standard layer of an N."}, {"heading": "4 Experiments", "text": "In fact, it is that we are able to hold our own, that we are able to put ourselves in the lead."}, {"heading": "5 Conclusion", "text": "Unlike previous models for predicting relationships based solely on entity representations in knowledge databases, our model enables direct interaction of entity vectors via a tensor. This architecture enables significantly better performance both in ranking correct responses from tens of thousands of possible responses and in predicting invisible relationships between entities. It enables database expansion without external text resources, but can also benefit from unattended large corporations, even without manually designed extraction rules."}], "references": [{"title": "WordNet: A Lexical Database for English", "author": ["G.A. Miller"], "venue": "Communications of the ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1995}, {"title": "Yago: a core of semantic knowledge", "author": ["F.M. Suchanek", "G. Kasneci", "G. Weikum"], "venue": "Proceedings of the 16th international conference on World Wide Web", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "The SphereSearch engine for unified ranked retrieval of heterogeneous XML and web documents", "author": ["J. Graupmann", "R. Schenkel", "G. Weikum"], "venue": "Proceedings of the 31st international conference on Very large data bases, VLDB", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Improving machine learning approaches to coreference resolution", "author": ["V. Ng", "C. Cardie"], "venue": "ACL", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["R. Snow", "D. Jurafsky", "A.Y. Ng"], "venue": "NIPS", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Identifying relations for open information extraction", "author": ["A. Fader", "S. Soderland", "O. Etzioni"], "venue": "EMNLP", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Word representations: a simple and general method for semisupervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of ACL, pages 384\u2013394", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing", "author": ["A. Bordes", "X. Glorot", "J. Weston", "Y. Bengio"], "venue": "AISTATS", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning structured embeddings of knowledge bases", "author": ["A. Bordes", "J. Weston", "R. Collobert", "Y. Bengio"], "venue": "AAAI", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Modelling relational data using Bayesian clustered tensor factorization", "author": ["I. Sutskever", "R. Salakhutdinov", "J.B. Tenenbaum"], "venue": "NIPS", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Semantic Compositionality Through Recursive Matrix-Vector Spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng"], "venue": "EMNLP", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Factored 3-Way Restricted Boltzmann Machines", "author": ["M. Ranzato", "A. Krizhevsky G.E. Hinton"], "venue": "For Modeling Natural Images. AISTATS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "ICML", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "ACL", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Ontologies and knowledge bases such as WordNet [1] or Yago [2] are extremely useful resources for query expansion [3], coreference resolution [4], question answering (Siri), information retrieval (Google Knowledge Graph), or generally providing inference over structured knowledge to users.", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "Ontologies and knowledge bases such as WordNet [1] or Yago [2] are extremely useful resources for query expansion [3], coreference resolution [4], question answering (Siri), information retrieval (Google Knowledge Graph), or generally providing inference over structured knowledge to users.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "Ontologies and knowledge bases such as WordNet [1] or Yago [2] are extremely useful resources for query expansion [3], coreference resolution [4], question answering (Siri), information retrieval (Google Knowledge Graph), or generally providing inference over structured knowledge to users.", "startOffset": 114, "endOffset": 117}, {"referenceID": 3, "context": "Ontologies and knowledge bases such as WordNet [1] or Yago [2] are extremely useful resources for query expansion [3], coreference resolution [4], question answering (Siri), information retrieval (Google Knowledge Graph), or generally providing inference over structured knowledge to users.", "startOffset": 142, "endOffset": 145}, {"referenceID": 4, "context": "Much work has focused on extending existing knowledge bases [5, 6, 2] using patterns or classifiers applied to large corpora.", "startOffset": 60, "endOffset": 69}, {"referenceID": 5, "context": "Much work has focused on extending existing knowledge bases [5, 6, 2] using patterns or classifiers applied to large corpora.", "startOffset": 60, "endOffset": 69}, {"referenceID": 1, "context": "Much work has focused on extending existing knowledge bases [5, 6, 2] using patterns or classifiers applied to large corpora.", "startOffset": 60, "endOffset": 69}, {"referenceID": 6, "context": "These vectors are learned by a neural network model [7] using unsupervised text corpora such as Wikipedia.", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "There is a vast amount of work extending knowledge bases using external corpora [5, 6, 2], among many others.", "startOffset": 80, "endOffset": 89}, {"referenceID": 5, "context": "There is a vast amount of work extending knowledge bases using external corpora [5, 6, 2], among many others.", "startOffset": 80, "endOffset": 89}, {"referenceID": 1, "context": "There is a vast amount of work extending knowledge bases using external corpora [5, 6, 2], among many others.", "startOffset": 80, "endOffset": 89}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] who use tensor factorization and Bayesian clustering for learning relational structures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Many methods that use knowledge bases as features such as [3, 4] could benefit from a method that maps the provided information into vector representations.", "startOffset": 58, "endOffset": 64}, {"referenceID": 3, "context": "Many methods that use knowledge bases as features such as [3, 4] could benefit from a method that maps the provided information into vector representations.", "startOffset": 58, "endOffset": 64}, {"referenceID": 6, "context": "Furthermore, the resulting vectors could be used in other tasks such as NER [7] or relation classification in natural language [11].", "startOffset": 76, "endOffset": 79}, {"referenceID": 10, "context": "Furthermore, the resulting vectors could be used in other tasks such as NER [7] or relation classification in natural language [11].", "startOffset": 127, "endOffset": 131}, {"referenceID": 11, "context": "[12] introduced a factored 3-way Restricted Boltzmann Machine which is also parameterized by a tensor.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "We compare using both randomly initialized word vectors and pre-trained 100-dimensional word vectors from the unsupervised model of Collobert and Weston [13, 7].", "startOffset": 153, "endOffset": 160}, {"referenceID": 6, "context": "We compare using both randomly initialized word vectors and pre-trained 100-dimensional word vectors from the unsupervised model of Collobert and Weston [13, 7].", "startOffset": 153, "endOffset": 160}, {"referenceID": 13, "context": "For further details and evaluations of these embeddings, see [14, 13, 15].", "startOffset": 61, "endOffset": 73}, {"referenceID": 12, "context": "For further details and evaluations of these embeddings, see [14, 13, 15].", "startOffset": 61, "endOffset": 73}, {"referenceID": 14, "context": "For further details and evaluations of these embeddings, see [14, 13, 15].", "startOffset": 61, "endOffset": 73}, {"referenceID": 9, "context": "The bilinear model for truth values in [10] becomes a special case of this model with VR = 0, bR = 0, k = 1, f = identity.", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "In our experiments, we follow the data settings of WordNet in [9].", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "[9, 8], which have the same goal as ours.", "startOffset": 0, "endOffset": 6}, {"referenceID": 7, "context": "[9, 8], which have the same goal as ours.", "startOffset": 0, "endOffset": 6}, {"referenceID": 8, "context": "The model of [9] has the following scoring function:", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "The model of [8] also maps each relation type to an embedding eR \u2208 R d and scores the relationships by: g(e1, R, e2) = \u2212(W1e1 \u2297Wrel,1eR + b1) \u00b7 (W2e2 \u2297Wrel,2eR + b2), (5) where W1,Wrel,1,W2,Wrel,2 \u2208 R, b1, b2 \u2208 R.", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "[9] because it has became apparent to us and them that there were issues of overlap between their training and testing sets which impacted the quality and interpretability of their evaluation.", "startOffset": 0, "endOffset": 3}], "year": 2013, "abstractText": "Knowledge bases provide applications with the benefit of easily accessible, systematic relational knowledge but often suffer in practice from their incompleteness and lack of knowledge of new entities and relations. Much work has focused on building or extending them by finding patterns in large unannotated text corpora. In contrast, here we mainly aim to complete a knowledge base by predicting additional true relationships between entities, based on generalizations that can be discerned in the given knowledgebase. We introduce a neural tensor network (NTN) model which predicts new relationship entries that can be added to the database. This model can be improved by initializing entity representations with word vectors learned in an unsupervised fashion from text, and when doing this, existing relations can even be queried for entities that were not present in the database. Our model generalizes and outperforms existing models for this problem, and can classify unseen relationships in WordNet with an accuracy of 75.8%.", "creator": "LaTeX with hyperref package"}}}