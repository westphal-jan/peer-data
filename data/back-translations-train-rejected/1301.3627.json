{"id": "1301.3627", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Two SVDs produce more focal deep learning representations", "abstract": "A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality -- a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by Anandkumar (2012).", "histories": [["v1", "Wed, 16 Jan 2013 08:37:39 GMT  (84kb)", "https://arxiv.org/abs/1301.3627v1", null], ["v2", "Sat, 11 May 2013 12:17:44 GMT  (41kb)", "http://arxiv.org/abs/1301.3627v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["hinrich schuetze", "christian scheible"], "accepted": false, "id": "1301.3627"}, "pdf": {"name": "1301.3627.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["hs999@ifnlp.org", "scheibcn@ims.uni-stuttgart.de"], "sections": [{"heading": null, "text": "In one setup inspired by (Anandkumar et al., 2012), the first SVD is intended for denocialization; the second SVD representation is intended to increase the representation we invoke; what we seek is to increase the representation we aspire to, what we aspire to; and in one setup, we are inspired by (Anandkumar et al., 2012), the first SVD representation is intended for denocialization; and the second SVD representation is intended to increase the representation we aspire to."}, {"heading": "2 1LAYER vs. 2LAYER", "text": "We compare two representations of a word trigram: (i) the 1LAYER representation from section 1 and (ii) a 2LAYER representation that goes through two rounds of autocoding, which is a profound learning representation in the sense that layer 2 represents more general and superordinate properties of the input than layer 1. The architecture of 2LAYER is represented in Figure 3.To create 2LAYER representations, we first create a vector for each of the 20701 word types occurring in the corpus. This vector is the concatenation of its left vector and its right vector. The resulting 20701 \u00d7 500 matrix is the input representation to SVD1. We set k = 100 again. A trigraph is then presented as a concatenation of three of these 100-dimensional vectors. We apply the SVD2 construction algorithm to the resulting 100000 \u00d7 300 matrix and truncate to k = 100 LAER - we now have two worse representations for each of these 100-dimensional vectors."}, {"heading": "3 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Focality", "text": "One advantage of focal representations is that many classifiers cannot handle conjunctions of multiple traits unless they are explicitly defined as separate traits. Compare two representations in which ~ x \u2032 is a rotation of ~ x (as could be achieved by an SVD), but since one vector is a rotation of the other, they contain exactly the same information. However, if (i) an individual \"hidden unit\" of the rotated vector ~ x \u2032 can be interpreted directly as a \"verb\" (or a property similar to \"adjective\" or \"takes NP argument\") the same trait requires a conjunction of multiple hidden units for ~ x, then rotated representation is superior to many upstream statistical classifications. Focal representations can be argued to be closer to biological reality than broadly distributed representations (Thorpe, 2010); and they have the nice property of becoming categorical representations in the boundary."}, {"heading": "3.2 mSVD method", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4 Conclusion", "text": "As a next step, a direct comparison of SVD2 with traditional deep learning should be made (Hinton et al., 2006). As we have argued, SVD2 would be an interesting alternative to the deep learning initialization methods currently used, since SVD is efficient and a simple and well understood formalism. However, this argument is only valid if the resulting representations are of comparable quality. Data sets and tasks for this comparative evaluation could be those of Turian et al. (2010), Maas et al. (2011) and Socher et al. (2011), for example."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Bengio", "Yoshua."], "venue": "Foundations and Trends in Machine", "citeRegEx": "Bengio and Yoshua.,? 2009", "shortCiteRegEx": "Bengio and Yoshua.", "year": 2009}, {"title": "Latent Dirichlet allocation", "author": ["Blei", "David M.", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "JMLR,", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "A unified architecture for natural language processing", "author": ["Collobert", "Ronan", "Jason Weston"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Domain adaptation for large-scale", "author": ["Glorot", "Xavier", "Antoine Bordes", "Yoshua Bengio"], "venue": null, "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "A fast learning algorithm for deep", "author": ["Hinton", "Geoffrey E", "Simon Osindero", "Yee-Whye Teh"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Probabilistic latent semantic indexing", "author": ["Hofmann", "Thomas."], "venue": "SIGIR, pages 50\u201357.", "citeRegEx": "Hofmann and Thomas.,? 1999", "shortCiteRegEx": "Hofmann and Thomas.", "year": 1999}, {"title": "Learning the parts of objects by non-negative matrix", "author": ["Lee", "David D", "H. Sebastian Seung"], "venue": null, "citeRegEx": "Lee et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lee et al\\.", "year": 1999}, {"title": "Learning word vectors for sentiment analysis", "author": ["Potts."], "venue": "ACL, pages 142\u2013150.", "citeRegEx": "Potts.,? 2011", "shortCiteRegEx": "Potts.", "year": 2011}, {"title": "A sentimental education: Sentiment analysis using subjectivity", "author": ["Pang", "Bo", "Lillian Lee"], "venue": null, "citeRegEx": "Pang et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2004}, {"title": "Distributional part-of-speech tagging", "author": ["Sch\u00fctze", "Hinrich."], "venue": "Conference of the European", "citeRegEx": "Sch\u00fctze and Hinrich.,? 1995", "shortCiteRegEx": "Sch\u00fctze and Hinrich.", "year": 1995}, {"title": "Grandmother cells and distributed representations", "author": ["Thorpe", "Simon."], "venue": "Kriegeskorte, Nikolaus", "citeRegEx": "Thorpe and Simon.,? 2010", "shortCiteRegEx": "Thorpe and Simon.", "year": 2010}, {"title": "Word representations: A simple and", "author": ["Turian", "Joseph", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "From frequency to meaning: Vector space models", "author": ["Turney", "Peter D", "Patrick Pantel"], "venue": null, "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 3, "context": "The notion of focality is similar to disentanglement (Glorot et al., 2011) \u2013 in fact, the two notions may be identical.", "startOffset": 53, "endOffset": 74}, {"referenceID": 3, "context": "The notion of focality is similar to disentanglement (Glorot et al., 2011) \u2013 in fact, the two notions may be identical. However, Glorot et al. (2011) introduce disentanglement in the context of domain adaptation, focusing on the idea that \u201cdisentangled\u201d hidden units capture general cross-domain properties and for that reason are a good basis for domain adaptation.", "startOffset": 54, "endOffset": 150}, {"referenceID": 1, "context": ", 1998), matrix factorization techniques that obey additional constraints \u2013 such as non-negativity in the case of non-negative matrix factorization (Lee and Seung, 1999) \u2013 , latent dirichlet allocation (Blei et al., 2003) and different forms of autoencoding (Bengio, 2009; Chen et al.", "startOffset": 202, "endOffset": 221}, {"referenceID": 4, "context": "Finally, there is one big difference between mSVD and deep learning representations such as those proposed by Hinton et al. (2006), Collobert and Weston (2008) and Socher et al.", "startOffset": 110, "endOffset": 131}, {"referenceID": 4, "context": "Finally, there is one big difference between mSVD and deep learning representations such as those proposed by Hinton et al. (2006), Collobert and Weston (2008) and Socher et al.", "startOffset": 110, "endOffset": 160}, {"referenceID": 4, "context": "Finally, there is one big difference between mSVD and deep learning representations such as those proposed by Hinton et al. (2006), Collobert and Weston (2008) and Socher et al. (2012). Most deep learning representations are induced in a setting that also includes elements of supervised learning as is the case in contrastive divergence or when labeled data are available for adjusting initial representations produced by a process like autoencoding or dimensionality reduction.", "startOffset": 110, "endOffset": 185}, {"referenceID": 4, "context": "As a next step a direct comparison should be performed of SVD with traditional deep learning (Hinton et al., 2006).", "startOffset": 93, "endOffset": 114}, {"referenceID": 4, "context": "As a next step a direct comparison should be performed of SVD with traditional deep learning (Hinton et al., 2006). As we have argued, SVD would be an interesting alternative to deep learning initialization methods currently used since SVD is efficient and a simple and well understood formalism. But this argument is only valid if the resulting representations are of comparable quality. Datasets and tasks for this comparative evaluation could e.g. be those of Turian et al. (2010), Maas et al.", "startOffset": 94, "endOffset": 484}, {"referenceID": 4, "context": "As a next step a direct comparison should be performed of SVD with traditional deep learning (Hinton et al., 2006). As we have argued, SVD would be an interesting alternative to deep learning initialization methods currently used since SVD is efficient and a simple and well understood formalism. But this argument is only valid if the resulting representations are of comparable quality. Datasets and tasks for this comparative evaluation could e.g. be those of Turian et al. (2010), Maas et al. (2011), and Socher et al.", "startOffset": 94, "endOffset": 504}, {"referenceID": 4, "context": "As a next step a direct comparison should be performed of SVD with traditional deep learning (Hinton et al., 2006). As we have argued, SVD would be an interesting alternative to deep learning initialization methods currently used since SVD is efficient and a simple and well understood formalism. But this argument is only valid if the resulting representations are of comparable quality. Datasets and tasks for this comparative evaluation could e.g. be those of Turian et al. (2010), Maas et al. (2011), and Socher et al. (2011).", "startOffset": 94, "endOffset": 530}], "year": 2013, "abstractText": "A key characteristic of work on deep learning and neural networks in general is that it relies on representations of the input that support generalization, robust inference, domain adaptation and other desirable functionalities. Much recent progress in the field has focused on efficient and effective methods for computing representations. In this paper, we propose an alternative method that is more efficient than prior work and produces representations that have a property we call focality \u2013 a property we hypothesize to be important for neural network representations. The method consists of a simple application of two consecutive SVDs and is inspired by (Anandkumar et al., 2012). In this paper, we propose to generate representations for deep learning by two consecutive applications of singular value decomposition (SVD). In a setup inspired by (Anandkumar et al., 2012), the first SVD is intended for denoising. The second SVD rotates the representation to increase what we call focality. In this initial study, we do not evaluate the representations in an application. Instead we employ diagnostic measures that may be useful in their own right to evaluate the quality of representations independent of an application. We use the following terminology. SVD (resp. SVD) refers to the method using one (resp. two) applications of SVD; 1LAYER (resp. 2LAYER) corresponds to a single-hidden-layer (resp. twohidden-layer) architecture. In Section 1, we introduce the two methods SVD and SVD and show that SVD generates better (in a sense to be defined below) representations than SVD. In Section 2, we compare 1LAYER and 2LAYER SVD representations and show that 2LAYER representations are better. Section 3 discusses the results. We present our conclusions in Section 4.", "creator": "LaTeX with hyperref package"}}}