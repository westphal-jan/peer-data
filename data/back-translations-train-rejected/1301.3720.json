{"id": "1301.3720", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "The IBMAP approach for Markov networks structure learning", "abstract": "In this work we consider the problem of learning the structure of Markov networks from data. We present an approach for tackling this problem called IBMAP, together with an efficient instantiation of the approach: the IBMAP-HC algorithm, designed for avoiding important limitations of existing independence-based algorithms. These algorithms proceed by performing statistical tests of independence on data, trusting completely the outcome of each test. In practice tests may be incorrect, resulting in potential cascading errors and the consequent reduction in the quality of the structures learned. IBMAP contemplates this uncertainty in the outcome of the tests through a probabilistic maximum-a-posteriori approach. The approach is instantiated in the IBMAP-HC algorithm, a structure selection strategy that performs a polynomial heuristic local search in the space of possible structures. We present an extensive empirical evaluation on synthetic and real data, showing that our algorithm outperforms significantly the existent independence-based algorithms, in terms of data efficiency and quality of learned structures, with equivalent computational complexities. We also show the performance of IBMAP-HC in a real-world application of knowledge discovery: EDAs, which are evolutive algorithms that use structure learning on each generation for modeling the distribution of populations. The experiments show that when IBMAP-HC is used to learn the structure, EDAs improve the convergence to the optimum.", "histories": [["v1", "Wed, 16 Jan 2013 15:21:19 GMT  (458kb)", "http://arxiv.org/abs/1301.3720v1", null], ["v2", "Tue, 25 Feb 2014 17:32:50 GMT  (346kb)", "http://arxiv.org/abs/1301.3720v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["federico schl\\\"uter", "facundo bromberg", "alejandro edera"], "accepted": false, "id": "1301.3720"}, "pdf": {"name": "1301.3720.pdf", "metadata": {"source": "CRF", "title": "The IBMAP approach for Markov networks structure learning", "authors": ["Federico Schl\u00fcter"], "emails": ["federico.schluter@frm.utn.edu.ar", "fbromberg@frm.utn.edu.ar", "aedera@frm.utn.edu.ar"], "sections": [{"heading": null, "text": "ar Xiv: 130 1.37 20v1 [cs.AI] 16 Jan 20Keywords Markov Networks \u00b7 Structure Learning \u00b7 Independence Tests \u00b7 knowledge discovery \u00b7 EDAsF. Key, F. Bromberg, A. Edera Lab. DHARMa of Artificial Intelligence, Departamento de Sistemas de informacio \u0301 n, Facultad Regional Mendoza, Universidad Tecnolo \u0301 gica Nacional, Argentina. Tel.: + 54-261-5244566 Email: {federico.schluter, fbromberg, aedera} @ frm.utn.edu.ar"}, {"heading": "1 Introduction", "text": "It is not only a matter of time before there is such a process, but also whether there will be such a process. (...) It is a matter of time before there will be such a process. (...) It is a matter of time before there will be such a process. (...) It is a matter of time before there will be such a process. (...) It is a matter of time before there will be such a process. (...) It is a matter of time before there will be such a process. (...) It is a matter of time before there will be such a process. \"(...) It is a matter of time before there will be such a process.\" (...) It is a matter of time before there will be such a process. \"(...) It is a matter of time before there will be such a process.\""}, {"heading": "2 Background", "text": "This section provides some background information on Markov networks, defines the problem of structure learning and the motives of our independence-based approach. A Markov network represents an underlying distribution P (V) over the set of n = | V random variables V consists of an undirected graph G and a set of potential functions defined by a set of numerical parameters. Diagram G is a mapping of dependencies in P (V), and such dependencies can be read from the graph by vertex separation, each variable being conditionally independent of all its non-adjacent variables in the graph, since the set of its adjacent variables [26].The structureG of P (V) can be converted into a product of potential functions (Vc) via the fully connected sub-graphs (a.k.a., cliques) Vc of G, which is, P (V) = 1Z-cliques."}, {"heading": "3 The independence-based MAP approach", "text": "We will now describe the main contribution of this paper: the IBMAP approach to Markov networks structure learning (G). The central idea is to aggregate the result of many statistical tests of conditional independence into the rear probability P (G | D) of the independence structure G and, taking the maximum a-posteriori (MAP) model selection approach, select the structure that maximizes this rear probability. Formally, we will begin by replacing G in Pr (G | D) with Closing C (G), an equivalent representation of G consisting of a series of declarations of independence that determine it. FormallyDefinition 1 (Closure) The closure of an undirected independence structure G is a conditional assertion of independence."}, {"heading": "3.1.1 Markov Blanket Closure", "text": "The Markov shield is the closure of IBMAP-HC, consisting of a polynomial number of declarations of independence (in the number of nodes of the diagram). This shield is defined via the concept of the Markov shield BX-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X-X X-X-X-X X-X-X-X X X X-X-X-X-X X-X X X X X"}, {"heading": "3.1.2 Our structure selection technique", "text": "To complete the presentation of IBMAP-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-"}, {"heading": "4 Experimental results", "text": "This section describes several experiments on synthetic and real data sets for testing empirically the robustness of our IBMAP approach and the efficiency of our IBMAP-HC algorithm. We report on a detailed and systematic experimental comparison between IBMAP-HC and state-of-the-art structure learning algorithms. To compare all algorithms on the same ground, we have gone through all of these algorithms using the Bayesian test as statistical independence tests. We compare the quality of the structures we have learned through our solution against the quality of the structures we have learned through GSMN [9], a state-of-the-art based algorithm in terms of quality. We also present a competitor called HHC-MN, as an adaptation for learning the structure of the Markov networks of the HHC algorithm [5], a state-of-the-art based algorithm for learning Bayesian networks."}, {"heading": "25 50 100", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "25 50 100", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "25 50 100", "text": "In fact, the number of persons in employment, who are able to hide themselves in the position, in the middle of society to stay, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle of society, in the middle, in the middle, in the middle, in the middle, in the middle, in the middle, in the middle, in the middle, in the middle, in the middle, in the middle, in the middle of the middle, in the middle, in the middle of the middle, in the middle, in the middle, in the middle of the middle, in the middle, in the middle of the middle, in the middle, in the middle of the middle, in the middle, in the middle of the middle, in the middle of the middle, in the middle, in the middle of the middle, in the middle, in the middle of the middle, in the middle, in the middle, in the middle, in the middle of the middle, in the middle, in the middle, in the middle, in the middle of the middle, in the middle, in the middle, in the middle, in the middle of the middle, in the middle of the middle, in the middle, in the middle, in the middle of the middle, in the middle of the middle, in the middle of the middle, in the middle of the middle of the middle, in the middle, in the middle of the middle, in the middle of the middle, in the middle of the middle, in the middle of the middle, in the middle of the middle, in the middle, in the middle of the middle, in the middle, in the middle of the middle of the middle of the middle, in the middle, in the middle, in the middle of the middle, in the middle, in the middle of the middle, in the middle, in the middle, in the middle of the middle, in the middle of the middle, in the middle of the"}, {"heading": "5 IBMAP-HC for Estimation of Distribution Algorithms", "text": "These are variations of known evolutionary algorithms that perform the same selection and stages of variation, but replace the crossover and mutation stages by estimating and sampling in the task of generating a new population.The earlier stage estimates a probability distribution from the current population, generating the next population by sampling from that population (i.e. its name).In the estimation phase, EDAs estimate the probability distribution from the datasets according to the current population.This is because they assign each gene to a random variable, each individual to a common mapping of these variables, and the selected population to a sample of the distribution."}, {"heading": "6 Conclusions and future work", "text": "This work proposes a novel independence-based, maximally a posteriori approach to learning the structure of Markov networks, and IBMAP-HC, an efficient instantiation of IBMAP. Our method follows an independence-based strategy to obtain the MAP independence structure from data suggesting an independence-based score. Experiments comparing IBMAP-HC with state-of-the-art independence-based algorithms suggest that in most cases our method improves over independence-based competitors with equivalent computational complexities. IBMAP-HC has also been tested in a practical, challenging environment: estimation of distribution algorithms, resulting in faster convergence with optimum results than a state-of-the-art Markov network-EDA algorithm for the benchmark functions selected. According to our experimental results and the conclusions of Appendix B, the effectiveness of our structural selection strategy, therefore, is confirmed by our work on IPG, and we believe that it is worthwhile to evaluate the improvement in actual IPG."}, {"heading": "7 Acknowledgements", "text": "This work was funded by the grant PICT-241 of the National Agency for Scientific and Technological Promotion, FONCyT, Argentina; the grant PID-1205 of the National Technological University, Argentina; and the scholarship program for teachers of the National Technological University and the Ministry of Science, Technology and Productive Innovation, Argentina. Special thanks go to Roberto Santana and Siddartha Shakya for their help and support in the implementation of our experiments with EDAs."}, {"heading": "A Completeness of Markov blanket closure", "text": "This appendix represents Theorem 1, a formal proof that the Markov Definition 2 of Section 3.1.1 is actually a closure, i.e. that its independence can fully determine the structure it generates. < W = > Definition 2 of [16,18,26]: the pair-wise Markov Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Defini@-@ Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-"}], "references": [{"title": "UCI machine learning repository", "author": ["D.N.A. Asuncion"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Categorical Data Analysis, 2nd edn", "author": ["A. Agresti"], "venue": "Wiley", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "MARLEDA: Effective Distribution Estimation Through Markov Random Fields", "author": ["M. Alden"], "venue": "Ph.D. thesis, Dept of CS, University of Texas Austin", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part I: Algorithms and Empirical Evaluation", "author": ["C. Aliferis", "A. Statnikov", "I. Tsamardinos", "S. Mani", "X. Koutsoukos"], "venue": "JMLR 11, 171\u2013234", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Local Causal and Markov Blanket Induction for Causal Discovery and Feature Selection for Classification Part II: Analysis and Extensions", "author": ["C. Aliferis", "A. Statnikov", "I. Tsamardinos", "S. Mani", "X. Koutsoukos"], "venue": "JMLR 11, 235\u2013284", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "HITON, a novel Markov blanket algorithm for optimal variable selection", "author": ["C. Aliferis", "I. Tsamardinos", "A. Statnikov"], "venue": "AMIA Fall", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Improving the Reliability of Causal Discovery from Small Data Sets using Argumentation", "author": ["F. Bromberg", "D. Margaritis"], "venue": "JMLR 10, 301\u2013340", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient markov network structure discovery using independence tests", "author": ["F. Bromberg", "D. Margaritis", "V. Honavar"], "venue": "In Proc SIAM Data Mining, p. 06", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Efficient Markov Network Structure Discovery Using Independence Tests", "author": ["F. Bromberg", "D. Margaritis", "H.V."], "venue": "JAIR 35, 449\u2013485", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning Bayesian networks is NP-Complete", "author": ["D.M. Chickering"], "venue": "D. Fisher, H. Lenz (eds.) Learning from Data: Artificial Intelligence and Statistics V, pp. 121\u2013130. SpringerVerlag", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1996}, {"title": "Elements of information theory", "author": ["T.M. Cover", "J.A. Thomas"], "venue": "Wiley-Interscience, New York, NY, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1991}, {"title": "Bottom-Up Learning of Markov Network Structure", "author": ["J. Davis", "P. Domingos"], "venue": "ICML, pp. 271\u2013278", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Inducing Features of Random Fields", "author": ["S. Della Pietra", "V.J. Della Pietra", "J.D. Lafferty"], "venue": "IEEE Trans. PAMI. 19(4), 380\u2013393", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1997}, {"title": "Constrained Approximate Maximum Entropy Learning of Markov Random Fields", "author": ["V. Ganapathi", "D. Vickrey", "J. Duchi", "D. Koller"], "venue": "Uncertainty in Artificial Intelligence, pp. 196\u2013203", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "The UCI KDD archive", "author": ["S. Hettich", "S.D. Bay"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT Press", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimation of Distribution Algorithms", "author": ["P. Larra\u00f1aga", "J.A. Lozano"], "venue": "A New Tool for Evolutionary Computation. Kluwer Pubs", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Graphical Models", "author": ["S.L. Lauritzen"], "venue": "Oxford University Press", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient structure learning of Markov networks using L1-regularization", "author": ["S.I. Lee", "V. Ganapathi", "D. Koller"], "venue": "NIPS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Distribution-Free Learning of Bayesian Network Structure in Continuous Domains", "author": ["D. Margaritis"], "venue": "Proceedings of AAAI", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient Markov Network Discovery Using Particle Filter", "author": ["D. Margaritis", "F. Bromberg"], "venue": "Comp. Intel. 25(4), 367\u2013394", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficiently inducing features of conditional random fields", "author": ["A. McCallum"], "venue": "Proceedings of Uncertainty in Artificial Intelligence (UAI)", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Divergence measures and message passing", "author": ["T. Minka"], "venue": "Tech. rep., Microsoft Research", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "An Introduction to Genetic Algorithms", "author": ["M. Mitchell"], "venue": "MIT Press, Cambridge, MA, USA", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1998}, {"title": "From recombination of genes to the estimation of distributions I", "author": ["H. M\u00fchlenbein", "G. Paa\u00df"], "venue": "binary parameters. In: H.M. Voigt, W. Ebeling, I. Rechenberg, H.P. Schwefel (eds.) Parallel Problem Solving from Nature PPSN IV, Lecture Notes in Computer Science, vol. 1141, pp. 178\u2013187. Springer Berlin / Heidelberg", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1996}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann Publishers, Inc.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1988}, {"title": "High-dimensional Ising model selection using L1-regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty"], "venue": "Annals of Statistics 38, 1287\u20131319", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Estimation of distribution algorithms with kikuchi approximations", "author": ["R. Santana"], "venue": "Evol. Comput. 13(1), 67\u201397", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2005}, {"title": "A survey on independence-based markov networks learning", "author": ["F. Schl\u00fcter"], "venue": "Artificial Intelligence Review pp. 1\u201325", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Optimization by estimation of distribution with deum framework based on markov random fields", "author": ["S. Shakya", "J. McCall"], "venue": "International Journal of Automation and Computing 4(3), 262\u2013272", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "A markovianity based optimisation algorithm", "author": ["S. Shakya", "R. Santana", "J.A. Lozano"], "venue": "Genetic Programming and Evolvable Machines 13(2), 159\u2013195", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Causation, Prediction, and Search", "author": ["P. Spirtes", "C. Glymour", "R. Scheines"], "venue": "Adaptive Computation and Machine Learning Series. MIT Press", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2000}, {"title": "Markov network structure learning: A randomized feature generation approach", "author": ["J. Van Haaren", "J. Davis"], "venue": "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Complexity: knots, colourings and counting", "author": ["D.J.A. Welsh"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1993}], "referenceMentions": [{"referenceID": 15, "context": "Markov networks, together with Bayesian networks, belong to the family of probabilistic graphical models [16], a computational framework for compact representation of joint probability distributions.", "startOffset": 105, "endOffset": 109}, {"referenceID": 31, "context": "The importance of these independences is that they factorize the joint distribution over the domain variables into factors over subsets of variables, resulting in important reductions in the space complexity required to store a distribution [32].", "startOffset": 241, "endOffset": 245}, {"referenceID": 9, "context": "In this work we focus in the problem of learning G, that has shown to be an NP-hard problem [10] since the number of structures grows superexponentially with the number of variables of the domain.", "startOffset": 92, "endOffset": 96}, {"referenceID": 12, "context": "The literature considers two broad approaches for learning G: score-based [13, 22,19,14], and independence-based (also known as constraint-based) algorithms [32].", "startOffset": 74, "endOffset": 88}, {"referenceID": 21, "context": "The literature considers two broad approaches for learning G: score-based [13, 22,19,14], and independence-based (also known as constraint-based) algorithms [32].", "startOffset": 74, "endOffset": 88}, {"referenceID": 18, "context": "The literature considers two broad approaches for learning G: score-based [13, 22,19,14], and independence-based (also known as constraint-based) algorithms [32].", "startOffset": 74, "endOffset": 88}, {"referenceID": 13, "context": "The literature considers two broad approaches for learning G: score-based [13, 22,19,14], and independence-based (also known as constraint-based) algorithms [32].", "startOffset": 74, "endOffset": 88}, {"referenceID": 31, "context": "The literature considers two broad approaches for learning G: score-based [13, 22,19,14], and independence-based (also known as constraint-based) algorithms [32].", "startOffset": 157, "endOffset": 161}, {"referenceID": 33, "context": "The score-based approach is intractable in practice for large domains for two reasons: (i) it approaches the problem as an optimization on the space of G, and (ii) for each G during the search it requires a learning step of the model parameters \u03b8, which involves an expensive inference step [34].", "startOffset": 291, "endOffset": 295}, {"referenceID": 28, "context": "These problems are described in detail in a recently published survey [29].", "startOffset": 70, "endOffset": 74}, {"referenceID": 22, "context": "Generally, score-based approaches are better suited for the density estimation goal, that is, tasks where inferences or predictions are required [23].", "startOffset": 145, "endOffset": 149}, {"referenceID": 31, "context": "In contrast, independence-based methods are better suited for other learning goals, such as feature selection for classification, or knowledge discovery [32,4,5].", "startOffset": 153, "endOffset": 161}, {"referenceID": 3, "context": "In contrast, independence-based methods are better suited for other learning goals, such as feature selection for classification, or knowledge discovery [32,4,5].", "startOffset": 153, "endOffset": 161}, {"referenceID": 4, "context": "In contrast, independence-based methods are better suited for other learning goals, such as feature selection for classification, or knowledge discovery [32,4,5].", "startOffset": 153, "endOffset": 161}, {"referenceID": 7, "context": "In all those cases we compared the structural errors of the structures learned by IBMAP-HC and against those learned by representative, state-of-the-art competitors: GSMN [8,9], and HHC-MN, a simple adaptation for Markov networks of an independence-based structure learning algorithm for Bayesian networks, called HHC [5].", "startOffset": 171, "endOffset": 176}, {"referenceID": 8, "context": "In all those cases we compared the structural errors of the structures learned by IBMAP-HC and against those learned by representative, state-of-the-art competitors: GSMN [8,9], and HHC-MN, a simple adaptation for Markov networks of an independence-based structure learning algorithm for Bayesian networks, called HHC [5].", "startOffset": 171, "endOffset": 176}, {"referenceID": 4, "context": "In all those cases we compared the structural errors of the structures learned by IBMAP-HC and against those learned by representative, state-of-the-art competitors: GSMN [8,9], and HHC-MN, a simple adaptation for Markov networks of an independence-based structure learning algorithm for Bayesian networks, called HHC [5].", "startOffset": 318, "endOffset": 321}, {"referenceID": 24, "context": "Additionally we tested the performance of IBMAP-HC in a real world application: Estimation of Distribution algorithms (EDAs) [25,17].", "startOffset": 125, "endOffset": 132}, {"referenceID": 16, "context": "Additionally we tested the performance of IBMAP-HC in a real world application: Estimation of Distribution algorithms (EDAs) [25,17].", "startOffset": 125, "endOffset": 132}, {"referenceID": 30, "context": "We tested IBMAP-HC in a state-of-the-art EDA algorithm based on Markov networks structure learning, called the MOA algorithm [31].", "startOffset": 125, "endOffset": 129}, {"referenceID": 25, "context": "The graph G is a map of the independences in P (V), and such independences can be read from the graph through vertex separation, considering that each variable is conditionally independent of all its non-neighbor variables in the graph, given the set of its neighbor variables [26].", "startOffset": 277, "endOffset": 281}, {"referenceID": 25, "context": "The optimal solution of the problem is a perfect-map of P (V) [26], that is, a structure that encodes all the dependences and the independences present in P (V).", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "Examples of independence tests used in practice are Mutual Information [11], Pearson\u2019s \u03c7 and G [2], the Bayesian test [20], and for continuous Gaussian data the partial correlation test [32].", "startOffset": 71, "endOffset": 75}, {"referenceID": 1, "context": "Examples of independence tests used in practice are Mutual Information [11], Pearson\u2019s \u03c7 and G [2], the Bayesian test [20], and for continuous Gaussian data the partial correlation test [32].", "startOffset": 95, "endOffset": 98}, {"referenceID": 19, "context": "Examples of independence tests used in practice are Mutual Information [11], Pearson\u2019s \u03c7 and G [2], the Bayesian test [20], and for continuous Gaussian data the partial correlation test [32].", "startOffset": 118, "endOffset": 122}, {"referenceID": 31, "context": "Examples of independence tests used in practice are Mutual Information [11], Pearson\u2019s \u03c7 and G [2], the Bayesian test [20], and for continuous Gaussian data the partial correlation test [32].", "startOffset": 186, "endOffset": 190}, {"referenceID": 31, "context": "When tests outcome incorrect independences, independence-based algorithms produce what is commonly called cascade errors [32], that not only discard the true underlying structure, but further confuse the algorithm in the test to perform next.", "startOffset": 121, "endOffset": 125}, {"referenceID": 28, "context": "This assumption is made implicitly by all the Markov networks independence-based algorithms [29], because the statistical tests are used as a black box, only using data for deciding independence for each assertion ci.", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "i logPr(ci | D), (3) where each term logPr(ci | D) can be computed using the Bayesian test of conditional independence [20].", "startOffset": 119, "endOffset": 123}, {"referenceID": 15, "context": "This closure is defined on the concept of Markov blanket BX \u2286 V \\ {X} of a variable X \u2208 V [16], defined as the set of all the nodes connected to X by an edge [26], i.", "startOffset": 90, "endOffset": 94}, {"referenceID": 25, "context": "This closure is defined on the concept of Markov blanket BX \u2286 V \\ {X} of a variable X \u2208 V [16], defined as the set of all the nodes connected to X by an edge [26], i.", "startOffset": 158, "endOffset": 162}, {"referenceID": 8, "context": "We compare the quality of structures learned by our solution, against the quality of structures learned by GSMN [9], a state-of-the-art independence-based algorithm in terms of quality.", "startOffset": 112, "endOffset": 115}, {"referenceID": 4, "context": "We introduce also a competitor called HHC-MN, as an adaptation for learning the structure of Markov networks of the HHC algorithm [5], a state-of-the-art independence-based algorithm for learning Bayesian networks.", "startOffset": 130, "endOffset": 133}, {"referenceID": 5, "context": "The HHC algorithm learns the structure by learning the set of parents and children (PC) of each variable through the interleaved HITON-PC with symmetry correction algorithm [6,4].", "startOffset": 173, "endOffset": 178}, {"referenceID": 3, "context": "The HHC algorithm learns the structure by learning the set of parents and children (PC) of each variable through the interleaved HITON-PC with symmetry correction algorithm [6,4].", "startOffset": 173, "endOffset": 178}, {"referenceID": 1, "context": "For that, we forced the parameters to result in a log-odds ratio of each pairwise factor \u03b5X,Y = log ( \u03c6(X=0,Y =0)\u03c6(X=1,Y=1) \u03c6(X=0,Y =1)\u03c6(X=1,Y=0) ) to be equal to 1 for all edges (see [2]).", "startOffset": 184, "endOffset": 187}, {"referenceID": 0, "context": "We therefore chose 3 parameters randomly in the range [0, 1], and solved for the remaining one.", "startOffset": 54, "endOffset": 60}, {"referenceID": 0, "context": "We used the publicly available benchmark datasets obtained from the UCI Repositories of machine learning [1] and KDD datasets [15].", "startOffset": 105, "endOffset": 108}, {"referenceID": 14, "context": "We used the publicly available benchmark datasets obtained from the UCI Repositories of machine learning [1] and KDD datasets [15].", "startOffset": 126, "endOffset": 130}, {"referenceID": 8, "context": "Therefore, to measure the structure\u2019s quality we used a quantity called here accuracy, used for the same purpose in other related works [9,21,7].", "startOffset": 136, "endOffset": 144}, {"referenceID": 20, "context": "Therefore, to measure the structure\u2019s quality we used a quantity called here accuracy, used for the same purpose in other related works [9,21,7].", "startOffset": 136, "endOffset": 144}, {"referenceID": 6, "context": "Therefore, to measure the structure\u2019s quality we used a quantity called here accuracy, used for the same purpose in other related works [9,21,7].", "startOffset": 136, "endOffset": 144}, {"referenceID": 24, "context": "5 IBMAP-HC for Estimation of Distribution Algorithms In contrast to benchmark datasets that comes from arbitrary applications, we present now results of evaluating IBMAP-HC in a real world application of knowledgediscovery: the Estimation of Distribution algorithms (EDAs) [25,17].", "startOffset": 273, "endOffset": 280}, {"referenceID": 16, "context": "5 IBMAP-HC for Estimation of Distribution Algorithms In contrast to benchmark datasets that comes from arbitrary applications, we present now results of evaluating IBMAP-HC in a real world application of knowledgediscovery: the Estimation of Distribution algorithms (EDAs) [25,17].", "startOffset": 273, "endOffset": 280}, {"referenceID": 27, "context": "Several Markov networks based EDAs has been proposed recently that uses Markov networks for modeling the distribution [28,3,30,31].", "startOffset": 118, "endOffset": 130}, {"referenceID": 2, "context": "Several Markov networks based EDAs has been proposed recently that uses Markov networks for modeling the distribution [28,3,30,31].", "startOffset": 118, "endOffset": 130}, {"referenceID": 29, "context": "Several Markov networks based EDAs has been proposed recently that uses Markov networks for modeling the distribution [28,3,30,31].", "startOffset": 118, "endOffset": 130}, {"referenceID": 30, "context": "Several Markov networks based EDAs has been proposed recently that uses Markov networks for modeling the distribution [28,3,30,31].", "startOffset": 118, "endOffset": 130}, {"referenceID": 30, "context": "As a test-bed we considered the Markovianity Optimization Algorithm (MOA) [31].", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "Both versions were tested on two benchmark functions widely used in the EDA\u2019s literature: Royal Road and OneMax, both bit-string optimization tasks, detailed in [24].", "startOffset": 161, "endOffset": 165}, {"referenceID": 13, "context": "Also, it is clearly worthwhile considering testing our approach in more practical real world testbeds, potentially comparing its performance against state-of-the-art score-based algorithms, such as [14,27,12, 33].", "startOffset": 198, "endOffset": 212}, {"referenceID": 26, "context": "Also, it is clearly worthwhile considering testing our approach in more practical real world testbeds, potentially comparing its performance against state-of-the-art score-based algorithms, such as [14,27,12, 33].", "startOffset": 198, "endOffset": 212}, {"referenceID": 11, "context": "Also, it is clearly worthwhile considering testing our approach in more practical real world testbeds, potentially comparing its performance against state-of-the-art score-based algorithms, such as [14,27,12, 33].", "startOffset": 198, "endOffset": 212}, {"referenceID": 32, "context": "Also, it is clearly worthwhile considering testing our approach in more practical real world testbeds, potentially comparing its performance against state-of-the-art score-based algorithms, such as [14,27,12, 33].", "startOffset": 198, "endOffset": 212}, {"referenceID": 15, "context": "Let us start by reproducing some necessary theoretical results extracted from [16,18,26]: the pairwise Markov property, the Intersection property of conditional independence, and the Strong Union property of conditional independence, all satisfied by any Markov network G of a positive graph-isomorph distribution P : Definition 3 (Pairwise Markov property) Let G be a Markov network of some graphisomorph distribution P , then (X, Y ) / \u2208 E(G) \u21d4 \u3008X\u22a5\u22a5Y |V \\{X, Y }\u3009 in P .", "startOffset": 78, "endOffset": 88}, {"referenceID": 17, "context": "Let us start by reproducing some necessary theoretical results extracted from [16,18,26]: the pairwise Markov property, the Intersection property of conditional independence, and the Strong Union property of conditional independence, all satisfied by any Markov network G of a positive graph-isomorph distribution P : Definition 3 (Pairwise Markov property) Let G be a Markov network of some graphisomorph distribution P , then (X, Y ) / \u2208 E(G) \u21d4 \u3008X\u22a5\u22a5Y |V \\{X, Y }\u3009 in P .", "startOffset": 78, "endOffset": 88}, {"referenceID": 25, "context": "Let us start by reproducing some necessary theoretical results extracted from [16,18,26]: the pairwise Markov property, the Intersection property of conditional independence, and the Strong Union property of conditional independence, all satisfied by any Markov network G of a positive graph-isomorph distribution P : Definition 3 (Pairwise Markov property) Let G be a Markov network of some graphisomorph distribution P , then (X, Y ) / \u2208 E(G) \u21d4 \u3008X\u22a5\u22a5Y |V \\{X, Y }\u3009 in P .", "startOffset": 78, "endOffset": 88}], "year": 2010, "abstractText": "In this work we consider the problem of learning the structure of Markov networks from data. We present an approach for tackling this problem called IBMAP, together with an efficient instantiation of the approach: the IBMAP-HC algorithm, designed for avoiding important limitations of existing independence-based algorithms. These algorithms proceed by performing statistical tests of independence on data, trusting completely the outcome of each test. In practice tests may be incorrect, resulting in potential cascading errors and the consequent reduction in the quality of the structures learned. IBMAP contemplates this uncertainty in the outcome of the tests through a probabilistic maximum-aposteriori approach. The approach is instantiated in the IBMAP-HC algorithm, a structure selection strategy that performs a polynomial heuristic local search in the space of possible structures. We present an extensive empirical evaluation on synthetic and real data, showing that our algorithm outperforms significantly the existent independence-based algorithms, in terms of data efficiency and quality of learned structures, with equivalent computational complexities. We also show the performance of IBMAP-HC in a real-world application of knowledge discovery: EDAs, which are evolutive algorithms that use structure learning on each generation for modeling the distribution of populations. The experiments show that when IBMAP-HC is used to learn the structure, EDAs improve the convergence to the optimum.", "creator": "gnuplot 4.2 patchlevel 5 "}}}