{"id": "1301.3753", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Switched linear encoding with rectified linear autoencoders", "abstract": "Several recent results in machine learning have established formal connections between autoencoders---artificial neural network models that attempt to reproduce their inputs---and other coding models like sparse coding and K-means. This paper explores in depth an autoencoder model that is constructed using rectified linear activations on its hidden units. Our analysis builds on recent results to further unify the world of sparse linear coding models. We provide an intuitive interpretation of the behavior of these coding models and demonstrate this intuition using small, artificial datasets with known distributions.", "histories": [["v1", "Wed, 16 Jan 2013 17:04:10 GMT  (8405kb,D)", "https://arxiv.org/abs/1301.3753v1", null], ["v2", "Sat, 19 Jan 2013 19:38:36 GMT  (8405kb,D)", "http://arxiv.org/abs/1301.3753v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["leif johnson", "craig corcoran"], "accepted": false, "id": "1301.3753"}, "pdf": {"name": "1301.3753.pdf", "metadata": {"source": "CRF", "title": "Switched linear coding with rectified linear autoencoders", "authors": ["Leif Johnson", "Craig Corcoran"], "emails": ["leif@cs.utexas.edu", "ccor@cs.utexas.edu"], "sections": [{"heading": null, "text": "Several recent results of machine learning have established formal links between autoencoders - artificial neural network models that attempt to reproduce their input - and other coding models such as sparse coding and K-means. This paper extensively examines an autoencoder model constructed by rectified linear activations on its hidden units. Our analysis builds on current results to further unify the world of sparse linear coding models. We provide an intuitive interpretation of the behavior of these coding models and demonstrate this intuition using small, artificial datasets with known distributions."}, {"heading": "1 Introduction", "text": "Large amounts of natural data are commonplace in the digital world today - images, videos, and sounds are all relatively cheap and easy to obtain. By comparison, labels for these data sets (for example, answers to questions like, \"Is this a picture of a cow or a horse?\") remain relatively expensive and difficult to assemble. Moreover, most data collections from the natural world also seem to be unevenly distributed across all possible inputs, suggesting that there is an underlying structure in a particular data set, regardless of labels or tasks. Moreover, many unattended learning approaches assume that natural data is even on a relatively consistent, low-size database."}, {"heading": "2 Background", "text": "One way to take advantage of an easily available, unlabeled DatasetO is to learn features that encode the data well, and such features should probably be useful for effectively describing the data, regardless of the task at hand. Formally, we would like to find a \"dictionary\" or \"code book\" matrix D-Rn \u00b7 k, whose columns can be combined or \"decrypted\" to represent elements from the dataset. There are many ways to do this: for example, you could use random vectors or examples from the O, or define and optimize a cost function over the space of dictionaries to find a good one. Once D is known, we would like to calculate for each input x the coefficients h-Rk, which provide the best linear approximation x = Dh an x. This problem is often referred to as optimizing a square error lossh = argmin u-x-R (u), where R is a regulation problem."}, {"heading": "2.1 Autoencoders", "text": "Autoencoders [8, 15, 10] are a versatile family of neural network models that can be used to learn a dictionary from a set of unlabeled data and to calculate an encoding h for an input x. This learning occurs simultaneously as the dictionary change changes the optimal encodings. An autoencoder attempts to reconstruct its input forward after activation by one or more nonlinear hidden layers. The general loss function for a single-layer autoencoder can be written as \"(O) = 1 2 | O | 0\" x \"(Wx + b). \u2212 x \u2022 22 + R (O, D, W, b), which provides activation functions for the output (\u00b7) and hidden neurons. b \u2022 Rk is a vector of hidden unit activation distortions, and W \u2022 Rk \u00b7 n is a matrix of weights for the hidden units."}, {"heading": "2.2 Rectified linear autoencoders", "text": "The rectified linear activation function [z] + = max (0, z) has been shown to improve training and performance in multi-layer networks by addressing several shortcomings of sigmoid activation functions [11, 5]. In particular, the rectified linear activation function [z] + = max (0, z] generates a true 0 (not just a small value) for negative inputs and then increases linearly. In contrast to sigmoids, which have a derivative pattern that disappears for large inputs, the derivative pattern of the reflected linear activation is either 0 for negative inputs or 1 for positive inputs. This \"switching behavior\" is inspired by the pattern of firing rate observed in some biological neurons, which is inactive for subthreshold inputs, and then increases the firing rate as an input magnitude. The true 0 output of the linear activation function effectively disables neurons that are not matched to a specific input."}, {"heading": "2.3 Whitening", "text": "In fact, Le et al. [9] showed that sparse autoencoders, sparse encodings, and ICA all compute the same family of loss functions, but only if the input data is approximately white. Empirically, Coates et al. [1, 2] observed that linear encoding schemes connected to several single-layer encoding models work best when combined with upstream data. Whitening also seems to be important in biological systems: Although the human eye does not receive white input, neurons in the retina and LGN appear to brighten local patches of natural images [13, 3] before they are processed further in the visual cortex."}, {"heading": "3 Switched linear codes", "text": "In fact, we are able to put ourselves at the top of society in the way we have seen it in recent years, \"he said.\" We have to put ourselves at the top of society, \"he said.\" We have to put ourselves at the top of society, \"he said.\" We have to put ourselves at the top of society, \"he said.\" We have to put ourselves at the top of society. \""}, {"heading": "3.1 Effective coding region", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "4 Behavior on complex data", "text": "So far, the visualization tools we use are based on simple Gaussian data distributions. However, many interesting data sets are explicitly non-Gaussian - in fact, one of the main reasons why ICA is so effective is that it explicitly looks for a non-Gaussian data model! In this section, we present several visualizations for these models using more complex data sets: First, we examine mixtures of Gaussian data sets, and then we use the MNIST digit data set as an excursion into a more natural dataset. We first tested the behavior of rectified linear auto-encoders on a small, 2D mixture of Gaussian data sets (see Figure 5.). After training, even massively overcomplete dictionaries tended to display the pairing behavior described above, especially when combined with a sparse regularizer, indicating that these networks are able to determine the number of features required to effectively encode the data."}, {"heading": "4.1 MNIST digits", "text": "The MNIST dataset consists of 60000 28 x 28 grayscale images of the handwritten digits 0 to 9. Figure 7 shows the \"eigendigits\" of the PCA calculated with this dataset; these digits point in the direction of the highest variance for this dataset as a whole and are often interpreted as encoding the digit on a Fourier basis. Figure 8 shows the characteristics with the highest bias values calculated by a single-layer linear autoencoder. For visualization purposes, each characteristic w on the left side of each column is paired with its maximum negative characteristic v = argminu wTu from the dictionary. Many of the primary characteristics of the dictionary are negative images of each other, suggesting that the \"merging\" of these characteristic layers observed for low-dimensional datasets is merely a two-dimensional mathematical scheme. Figure 9 finally shows features trained by a two-layer, unlinear second autocoded layer of a first 64."}, {"heading": "5 Conclusion", "text": "This paper has synthesized many recent developments in coding and neural networks, focusing on corrected linear activation functions and whitening. It also presented an intuitive interpretation of the behavior of these codes through simple visualizations. Exemplary features from artificial and natural datasets provided examples of learning behavior when these algorithms are applied to different types of data. There are many other avenues to explore in this area of machine learning research. Whitening has appeared in many places in this paper and appears to be a very important component of linear autoencoders and coding systems in general. However, this connection seems to be poorly understood and would benefit from further research, especially with regard to the idea of brightening up local regions of the input space."}], "references": [{"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A. Coates", "H. Lee", "A.Y. Ng"], "venue": "In AISTATS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A. Coates", "A.Y. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Efficient coding of natural scenes in the lateral geniculate nucleus: experimental test of a computational theory", "author": ["Y. Dan", "J.J. Atick", "R.C. Reid"], "venue": "The Journal of Neuroscience,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "Recklessly approximate sparse coding", "author": ["M. Denil", "N. de Freitas"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Learning fast approximations of sparse coding", "author": ["K Gregor", "Y LeCun"], "venue": "In Proc. International Conference on Machine learning (ICML\u201910),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Permitted and forbidden sets in symmetric threshold-linear networks", "author": ["R.H.R. Hahnloser", "H.S. Seung", "J.J. Slotine"], "venue": "Neural Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2003}, {"title": "Autoencoders, minimum description length, and Helmholtz free energy", "author": ["G.E. Hinton", "R.S. Zemel"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "ICA with reconstruction cost for efficient overcomplete feature learning", "author": ["Q.V. Le", "A. Karpenko", "J. Ngiam", "A.Y. Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Efficient online learning of a non-negative sparse autoencoder", "author": ["A. Lemme", "R.F. Reinhart", "J.J. Steil"], "venue": "In Proceedings of the ESANN,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "In 27th International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["B.A. Olshausen", "D.J. Field"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Natural image statistics and neural representation", "author": ["E.P. Simoncelli", "B.A. Olshausen"], "venue": "Annual review of neuroscience,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}], "referenceMentions": [{"referenceID": 3, "context": "Once such a set has been learned, this representation is often useful in other tasks like classification or recognition (but see [4] for interesting analysis).", "startOffset": 129, "endOffset": 132}, {"referenceID": 11, "context": ", [12]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "If R = 0, for example, then this is linear regression; when R = \u2016 \u00b7 \u20161, the encoding problem is known as sparse coding or lasso regression [14].", "startOffset": 139, "endOffset": 143}, {"referenceID": 1, "context": "Sparse coding yields stateof-the art results on many tasks for a wide variety of dictionary learning methods [2], but it requires a full optimization procedure to compute h, which might require more or less computation depending on the input.", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "To address this complexity, Gregor and LeCun [6] tried approximating this coding procedure by training a neural network (which has a bounded complexity) on a dataset labeled with its sparse codes.", "startOffset": 45, "endOffset": 48}, {"referenceID": 7, "context": "Autoencoders [8, 15, 10] are a versatile family of neural network models that can be used to learn a dictionary from a set of unlabeled data, and to compute an encoding h for an input x.", "startOffset": 13, "endOffset": 24}, {"referenceID": 14, "context": "Autoencoders [8, 15, 10] are a versatile family of neural network models that can be used to learn a dictionary from a set of unlabeled data, and to compute an encoding h for an input x.", "startOffset": 13, "endOffset": 24}, {"referenceID": 9, "context": "Autoencoders [8, 15, 10] are a versatile family of neural network models that can be used to learn a dictionary from a set of unlabeled data, and to compute an encoding h for an input x.", "startOffset": 13, "endOffset": 24}, {"referenceID": 8, "context": "Seen another way, tied weights provide an implicit orthonormality constraint on the dictionary [9].", "startOffset": 95, "endOffset": 98}, {"referenceID": 10, "context": "The rectified linear activation function [z]+ = max(0, z) has been shown to improve training and performance in multilayer networks by addressing several shortcomings of sigmoid activation functions [11, 5].", "startOffset": 199, "endOffset": 206}, {"referenceID": 4, "context": "The rectified linear activation function [z]+ = max(0, z) has been shown to improve training and performance in multilayer networks by addressing several shortcomings of sigmoid activation functions [11, 5].", "startOffset": 199, "endOffset": 206}, {"referenceID": 4, "context": "The true 0 output of the rectified linear activation function effectively deactivates neurons that are not tuned for a specific input, isolating a smaller active model for that input [5].", "startOffset": 183, "endOffset": 186}, {"referenceID": 10, "context": "This also has the effect of combining exponentially many linear coding schemes into one codebook [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 4, "context": "[5] proposed the single-layer rectified linear autoencoder with tied weights by setting W = D , \u03c2(z) = z and \u03c3(z) = [z]+ in the general autoencoder loss, giving", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1], which produces a coefficient vector h for input x such that hi = [ \u03bc\u2212 \u2016x\u2212 ci\u2016 ]+, where \u03bc = 1 k \u2211k j=1 \u2016x\u2212 cj\u2016 is the mean distance to all cluster centroids.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "An equivalent [4] variation is the \u201csoft thresholding\u201d scheme [2], where hi = [ di x\u2212 \u03bb ]", "startOffset": 14, "endOffset": 17}, {"referenceID": 1, "context": "An equivalent [4] variation is the \u201csoft thresholding\u201d scheme [2], where hi = [ di x\u2212 \u03bb ]", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "[9] showed that sparse autoencoders, sparse coding, and ICA all compute the same family of loss functions, but only if the input data are approximately white.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1, 2] observed that for several different single-layer coding models, switched-linear coding schemes perform best when combined with pre-whitened data.", "startOffset": 0, "endOffset": 6}, {"referenceID": 1, "context": "[1, 2] observed that for several different single-layer coding models, switched-linear coding schemes perform best when combined with pre-whitened data.", "startOffset": 0, "endOffset": 6}, {"referenceID": 12, "context": "Whitening seems to be important in biological systems as well: Although the human eye does not receive white input, neurons in retina and LGN appear to whiten local patches of natural images [13, 3] before further processing in the visual cortex.", "startOffset": 191, "endOffset": 198}, {"referenceID": 2, "context": "Whitening seems to be important in biological systems as well: Although the human eye does not receive white input, neurons in retina and LGN appear to whiten local patches of natural images [13, 3] before further processing in the visual cortex.", "startOffset": 191, "endOffset": 198}, {"referenceID": 6, "context": ", [7, 11, 5]) have described how the rectified linear activation function separates its inputs into two natural groups of outputs: those with 0 values, and those with positive values.", "startOffset": 2, "endOffset": 12}, {"referenceID": 10, "context": ", [7, 11, 5]) have described how the rectified linear activation function separates its inputs into two natural groups of outputs: those with 0 values, and those with positive values.", "startOffset": 2, "endOffset": 12}, {"referenceID": 4, "context": ", [7, 11, 5]) have described how the rectified linear activation function separates its inputs into two natural groups of outputs: those with 0 values, and those with positive values.", "startOffset": 2, "endOffset": 12}, {"referenceID": 8, "context": "[9] for ICA with a reconstruction cost.", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "For whitened input, then, the rectified linear autoencoder behaves not only like a mixture of exponentially many linear models [11], but rather like a mixture of exponentially many ICA models\u2014one ICA model is selected for each input x, but all features are selected from, and must coexist in, one large feature set.", "startOffset": 127, "endOffset": 131}, {"referenceID": 0, "context": "Regardless of the activation function, this network requires just one hidden unit to represent points in L: we can set the bias to 0 and weights to [1, 0] to align the feature of the hidden unit with the linear function that describes the data.", "startOffset": 148, "endOffset": 154}], "year": 2013, "abstractText": null, "creator": "LaTeX with hyperref package"}}}