{"id": "1301.3781", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Efficient Estimation of Word Representations in Vector Space", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "histories": [["v1", "Wed, 16 Jan 2013 18:24:43 GMT  (16kb)", "http://arxiv.org/abs/1301.3781v1", "submitted to ICLR 2013"], ["v2", "Thu, 7 Mar 2013 21:40:37 GMT  (48kb,D)", "http://arxiv.org/abs/1301.3781v2", "submitted to ICLR 2013"], ["v3", "Sat, 7 Sep 2013 00:30:40 GMT  (48kb,D)", "http://arxiv.org/abs/1301.3781v3", null]], "COMMENTS": "submitted to ICLR 2013", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tomas mikolov", "kai chen", "greg corrado", "jeffrey dean"], "accepted": false, "id": "1301.3781"}, "pdf": {"name": "1301.3781.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 130 1.37 81v1 [cs.CL]"}, {"heading": "1 Introduction", "text": "Many current NLP systems and techniques treat words as atomic units. There are several good reasons for this choice - simplicity, robustness, and the observation that simple models trained on huge amounts of data exceed complex systems trained on less data. However, with recent advances in machine learning and the increase in computing power, simple techniques have reached their limits in many tasks. An example is the popular N-gram model used for statistical speech modeling - today, it is possible to train these models on virtually all available data (trillions of words [3]).In recent years, it has become possible to train more complex models on much larger data sets, and they typically exceed the simple models. For example, neural network language models trained on a few hundred million words can perform significantly better than N-gram models trained on the same data [13]. For many tasks, the amount of relevant domain data in fact is limited - the linguistic biometric performance of linguistic parameters is evidently limited by the size of linguistic biases."}, {"heading": "1.1 Previous work", "text": "The representation of words as continuous vectors has a long history [17, 6]. A very popular model architecture for estimating word vectors was proposed in [1], where a forward-facing neural network with a linear projection layer and a nonlinear hidden layer was used to jointly learn word vector representation and a statistical language model. This work was followed by many more - later it was shown that the word vectors can be used to significantly improve and simplify many NLP applications [5]. The estimation of the word vector itself was carried out using various model architectures and trained on various corpora [4, 18, 15, 13, 7], and some of the resulting word vectors were made available for future research and comparisons."}, {"heading": "1.2 Goals of the Paper", "text": "We propose two new architectures for estimating continuous word vectors. We are using recently proposed techniques to measure the quality of the resulting vector representations, expecting that not only similar words are close to each other, but that words can have several degrees of similarity [14]. This has been observed before in the context of inflection languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words with similar endings [8]. Somewhat surprisingly, it has been found that the similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique, in which simple algebraic operations are performed on the word vectors, it has been shown, for example, that vector (\"king\") - vector (\"man\") + vector (\"woman\") lead to a vector that is closest to the vector representation of the word queen [14].In this paper, we try to synximize the modalities of these two words by using a high precision."}, {"heading": "2 Model Architectures", "text": "In this paper, we focus on neural network architectures, as it has previously been shown that they perform significantly better than LSA in maintaining linear regularities between words [14]; LDA also becomes very expensive in terms of computing for large amounts of data. Similar to Mikolov et al. [12], in order to compare different model architectures, we first define the computational complexity of a model as the number of parameters that need to be accessed to fully train the model. Next, we will try to maximize accuracy while minimizing computational complexity. In all of the following models, the training complexity is proportional to O = E \u00d7 T \u00d7 Q, (1) where E is the number of training periods."}, {"heading": "2.1 Feedforward Neural Net Language Model (NNLM)", "text": "It consists of input, projection, hidden and output levels. At the input level, N previous words are encoded with 1-by-V encoding, where V is the size of the vocabulary. The input layer is then projected onto a projection layer P, which has dimensionality N. Since only N inputs are active at all times, the composition of the projection layer is a relatively cheap operation.The NNLM architecture becomes complex for calculating between the projection layer and the hidden layer, because the values in the projection layer are dense."}, {"heading": "2.2 Recurrent Neural Net Language Model (RNNLM)", "text": "Recurring neural network-based language model has been proposed to overcome certain limitations of the upstream NNLM, such as the need to determine the context length (the order of model N), and because RNs can theoretically display more complex patterns efficiently than the flat neural networks [9, 2]. The RNN model does not have a projection layer, but only input, hide, and output layers. What is special about this model is the recursive matrix that connects the hidden layer to itself through time-delayed connections, allowing the recursive model to form a kind of short-term memory, since information from the past can be represented by the hidden layer state, which is updated based on the current input and state of the hidden layer in the previous time step. The complexity per training example of the RNN model is Q = H + H + H + H \u00b7 V, with word representations showing the same dimensions as D"}, {"heading": "3 New Log-linear Models", "text": "In this section, we propose two new model architectures that attempt to minimize computational complexity, thus enabling high-dimensional word vectors to be efficiently estimated from large amounts of data. The most important observation from the previous section was that most of the complexity is caused by the non-linear, hidden layer in the model. Although this makes neural networks so attractive, we decided to explore simpler models that may not be able to represent the data as accurately as neural networks, but may be able to be trained more efficiently on much more data."}, {"heading": "3.1 Continuous Bag-of-Words Model", "text": "The first proposed architecture is similar to the feedback NNLM, where the non-linear hidden layer is removed and the projection layer is split for all words (not just the projection matrix); therefore, all words are projected into the same position (their vectors are averaged). We call this architecture a sack-of-words model, because the order of words in history does not affect the projection. In addition, we also use words from the future; the best performance in the task presented in the next section was achieved by building a log-linear classifier with four future and four history words at the entrance, where the training criterion is to correctly classify the current (middle) word."}, {"heading": "3.2 Continuous Skip-gram Model", "text": "The second architecture is similar to the CBOW, but instead of predicting the current word from the context, it tries to maximize the classification of a word based on another word in the same sentence. Specifically, we use each current word as an input for a loglinear classifier with a continuous projection layer, predicting words within a certain range before and after the current word. We found that increasing the range improves the quality of the resulting word vectors, but also increases the arithmetic complexity. Since the more distant words are usually less related to the current word than those closest to it, we give less weight to the distant words by selecting fewer of these words in our training examples. The training complexity of this architecture is proportional to Q = C \u00d7 (D + D \u00b7 log2 (V), (5) where C is the maximum distance of the words. So, if we choose C = 5, we choose a number R in the range < C > and then R > we use the C of the current words from the current R and the actual words from the history of the R and the current words from the context of the current words."}, {"heading": "4 Results", "text": "To compare the quality of different versions of word vectors, previous work typically uses a table of sample words and their most similar words and understands them intuitively. Although it is easy to show that word France is similar to Italy and perhaps some other countries, it is much more difficult to subject these vectors to a more complex similarity task, as follows. We follow the earlier observation that there may be many different types of similarity between words, for example, word large is similar to larger in the same sense, that small is similar to smaller. For example, another type of relationship, word pairs can be large - large and small - small [14]. We also refer to two pairs of words with the same relationship as a question, as we might ask: \"What is the word that is small in the same sense as the largest word, similar to large?\" Somewhat is surprising is the answer to these questions is that we perform simple algebraic operations with the vector representation of words."}, {"heading": "4.1 Task Description", "text": "To measure the quality of the word vectors, we define a comprehensive test set that includes five types of semantic questions and nine types of syntactic questions. Table 1 shows two examples from each category. In total, there are 8869 semantic questions and 10675 syntactic questions. Questions in each category were created in two steps: First, a list of similar word pairs was created manually; then a large list of questions is formed by combining two pairs of words. For example, we plan to publish the test set soon so that other researchers can use it in their experiments. We evaluate the general accuracy for all question types and for each question type individually (semantic, syntactic)."}, {"heading": "4.2 Maximization of Accuracy", "text": "In order to estimate the best choice of hyperparameters to achieve the best possible results quickly, we first evaluated models trained on subsets of training data, with vocabulary limited to the most common 30k words. Results using the CBOW architecture with varying choice of word vector dimensions and increasing amount of training data are shown in Table 2.It can be seen that after some time, decreasing improvements are achieved by adding more dimensions or adding more training data. Thus, we need to collectively increase both the vector dimensionality and the amount of training data. While this observation may seem trivial, it should be noted that it is currently popular to train word vectors or add more training data to achieve improvements."}, {"heading": "4.3 Comparison of Model Architectures", "text": "To evaluate our best models and compare them with publicly available word vectors, we used a complete questionnaire, i.e. unlimited to the 30k vocabulary. We also included word vectors from a feedback NNLM similar [1] that were trained on a larger Google News corpus using about a hundred machine cores in parallel over a period of about two weeks. Comparison is in Table 3. The CBOW model was trained on a subset of Google News data within about one day, while the training time for the Loglinear Skip-gram model was about three days. For further experiments, we use only one training epoch (again, we reduce the learning rate linearly so that it approaches zero at the end of the training), because we have more training data than we can process efficiently."}, {"heading": "5 Examples of the Learned Relationships", "text": "Table 5 shows words that follow different relationships. We follow the approach described above: the relationship is defined by subtracting two word vectors, and the result is added to another word. So, for example, France - Paris + Italy = Rome. As you can see, the accuracy is quite good, although there is clearly a lot of room for further improvement. We believe that word vectors trained on even larger data sets with larger dimensions will work much better and allow the development of new innovative applications. Another way to improve accuracy is to provide more than one example of the relationship. Using ten examples instead of one, we have observed an improvement in the accuracy of our best models by about 10% absolutely in the semantic-syntactic test. It is also possible to apply the vector operations to solve different tasks."}, {"heading": "6 Conclusion", "text": "In this paper, we examined the quality of vector representations of words derived from different models. We found that it is possible to learn high-quality word vectors using very simple model architectures compared to popular neural network models (both forward and retrograde). Due to the much lower computational complexity, we hope that it will be possible to calculate very precise high-dimensional word vectors from a much larger dataset. We intend to publish a number of high-quality word vectors in the future. In the future, it would be interesting to compare our techniques with Latent Relational Analysis [19] and others. Application to certain previously proposed problems such as sentence completion [20] also appears attractive. We believe that our comprehensive test set will help the research community improve existing techniques for estimating word vectors. We also hope that high-quality word vectors will become an important building block for future NLP applications."}], "references": [{"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "Journal of Machine Learning Research, 3:1137-1155", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Scaling learning algorithms towards AI", "author": ["Y. Bengio", "Y. LeCun"], "venue": "In: Large-Scale Kernel Machines, MIT Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Large language models in machine translation", "author": ["T. Brants", "A.C. Popat", "P. Xu", "F.J. Och", "J. Dean"], "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Language Learning", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["R. Collobert", "J. Weston"], "venue": "International Conference on Machine Learning, ICML", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Natural Language Processing (Almost) from Scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, 12:2493- 2537", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding Structure in Time", "author": ["J. Elman"], "venue": "Cognitive Science, 14, 179-211", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Improving Word Representations via Global Context and Multiple Word Prototypes", "author": ["Eric H. Huang", "R. Socher", "C.D. Manning", "Andrew Y. Ng"], "venue": "Proc. Association for Computational Linguistics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "\u010cernock\u00fd: Neural network based language models for higly inflective languages", "author": ["T. Mikolov", "J. Kopeck\u00fd", "L. Burget", "J.O. Glembek"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "S", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. \u010cernock\u00fd"], "venue": "Khudanpur: Recurrent neural network based language model, In: Proceedings of Interspeech", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Extensions of recurrent neural network language", "author": ["T. Mikolov", "S. Kombrink", "L. Burget", "J. \u010cernock\u00fd", "S. Khudanpur"], "venue": "Proceedings of ICASSP", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Empirical Evaluation and Combination of Advanced Language Modeling Techniques", "author": ["T. Mikolov", "A. Deoras", "S. Kombrink", "L. Burget", "J. \u010cernock\u00fd"], "venue": "In: Proceedings of Interspeech", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Strategies for Training Large Scale Neural Network Language Models", "author": ["T. Mikolov", "A. Deoras", "D. Povey", "L. Burget", "J. \u010cernock\u00fd"], "venue": "In: Proc. Automatic Speech Recognition and Understanding", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Statistical Language Models based on Neural Networks", "author": ["T. Mikolov"], "venue": "PhD thesis, Brno University of Technology", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["T. Mikolov", "G. Zweig"], "venue": "In: submitted to NAACL HLT", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "A Scalable Hierarchical Distributed Language Model", "author": ["A. Mnih", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 21, MIT Press", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Y", "author": ["F. Morin"], "venue": "Bengio: Hierarchical Probabilistic Neural Network Language Model. AISTATS", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning internal representations by backpropagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323:533.536", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1986}, {"title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "author": ["Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "Proc. Association for Computational Linguistics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "Measuring Semantic Similarity by Latent Relational Analysis", "author": ["P.D. Turney"], "venue": "In: Proc. International Joint Conference on Artificial Intelligence", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "The Microsoft Research Sentence Completion Challenge", "author": ["G. Zweig", "C.J.C. Burges"], "venue": "Microsoft Research Technical Report MSR-TR-2011-129", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 2, "context": "An example is the popular N-gram model used for statistical language modeling - today, it is possible to train these models on virtually all available data (trillions of words [3]).", "startOffset": 176, "endOffset": 179}, {"referenceID": 12, "context": "For example, neural network language models trained on a few hundred million words significantly outperform N-gram models trained on the same data [13].", "startOffset": 147, "endOffset": 151}, {"referenceID": 0, "context": "word as an atomic unit, this approach gives significantly better results [1].", "startOffset": 73, "endOffset": 76}, {"referenceID": 10, "context": "Language models based on these techniques are currently the state of the art [11].", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "Representation of words as continuous vectors has a long history [17, 6].", "startOffset": 65, "endOffset": 72}, {"referenceID": 5, "context": "Representation of words as continuous vectors has a long history [17, 6].", "startOffset": 65, "endOffset": 72}, {"referenceID": 0, "context": "A very popular model architecture for estimating word vectors was proposed in [1], where a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model.", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "This work has been followed by many others it was shown later that the word vectors can be used to significantly improve and simplify many NLP applications [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 3, "context": "Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 18, 15, 13, 7], and some of the resulting word vectors were made available for future research and comparison1.", "startOffset": 119, "endOffset": 137}, {"referenceID": 17, "context": "Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 18, 15, 13, 7], and some of the resulting word vectors were made available for future research and comparison1.", "startOffset": 119, "endOffset": 137}, {"referenceID": 14, "context": "Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 18, 15, 13, 7], and some of the resulting word vectors were made available for future research and comparison1.", "startOffset": 119, "endOffset": 137}, {"referenceID": 12, "context": "Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 18, 15, 13, 7], and some of the resulting word vectors were made available for future research and comparison1.", "startOffset": 119, "endOffset": 137}, {"referenceID": 6, "context": "Estimation of the word vectors itself was performed using different model architectures and trained on various corpora [4, 18, 15, 13, 7], and some of the resulting word vectors were made available for future research and comparison1.", "startOffset": 119, "endOffset": 137}, {"referenceID": 13, "context": "We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [14].", "startOffset": 244, "endOffset": 248}, {"referenceID": 7, "context": "This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [8].", "startOffset": 259, "endOffset": 262}, {"referenceID": 13, "context": "Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(\u201dKing\u201d) - vector(\u201dMan\u201d) + vector(\u201dWoman\u201d) results in a vector that is closest to the vector representation of the word Queen [14].", "startOffset": 261, "endOffset": 265}, {"referenceID": 13, "context": "In this paper, we focus on neural network architectures, as it was previously shown that they perform significantly better than LSA for preserving linear regularities among words [14]; LDA moreover becomes computationally very expensive on large data sets.", "startOffset": 179, "endOffset": 183}, {"referenceID": 11, "context": "[12], to compare different model architectures we define first the computational complexity of a model as the number of parameters that need to be accessed to fully train the model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "All models are trained using stochastic gradient descent and backpropagation [17].", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "The probabilistic feedforward neural network language model has been proposed in [1].", "startOffset": 81, "endOffset": 84}, {"referenceID": 15, "context": "However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [16, 15, 12], or avoiding normalized models completely by using models that are not normalized during training [4, 7].", "startOffset": 118, "endOffset": 130}, {"referenceID": 14, "context": "However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [16, 15, 12], or avoiding normalized models completely by using models that are not normalized during training [4, 7].", "startOffset": 118, "endOffset": 130}, {"referenceID": 11, "context": "However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [16, 15, 12], or avoiding normalized models completely by using models that are not normalized during training [4, 7].", "startOffset": 118, "endOffset": 130}, {"referenceID": 3, "context": "However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [16, 15, 12], or avoiding normalized models completely by using models that are not normalized during training [4, 7].", "startOffset": 229, "endOffset": 235}, {"referenceID": 6, "context": "However, several practical solutions were proposed for avoiding it; either using hierarchical versions of the softmax [16, 15, 12], or avoiding normalized models completely by using models that are not normalized during training [4, 7].", "startOffset": 229, "endOffset": 235}, {"referenceID": 9, "context": "This follows previous observations that the frequency of words works well for obtaining classes in neural net language models [10].", "startOffset": 126, "endOffset": 130}, {"referenceID": 8, "context": "Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N ), and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks [9, 2].", "startOffset": 309, "endOffset": 315}, {"referenceID": 1, "context": "Recurrent neural network based language model has been proposed to overcome certain limitations of the feedforward NNLM, such as the need to specify the context length (the order of the model N ), and because theoretically RNNs can efficiently represent more complex patterns than the shallow neural networks [9, 2].", "startOffset": 309, "endOffset": 315}, {"referenceID": 13, "context": "Example of another type of relationship can be word pairs big - biggest and small - smallest [14].", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "We also included word vectors derived from a feedforward NNLM similar to [1], trained on a larger Google News corpus using about one hundred machine cores in parallel over a period of about two weeks.", "startOffset": 73, "endOffset": 76}, {"referenceID": 18, "context": "In the future, it would be interesting to compare our techniques to Latent Relational Analysis [19] and others.", "startOffset": 95, "endOffset": 99}, {"referenceID": 19, "context": "Also, application to certain previously proposed problems such as sentence completion [20] seems appealing.", "startOffset": 86, "endOffset": 90}], "year": 2013, "abstractText": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "creator": "LaTeX with hyperref package"}}}