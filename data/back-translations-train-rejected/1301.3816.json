{"id": "1301.3816", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Learning Output Kernels for Multi-Task Problems", "abstract": "Simultaneously solving multiple related learning tasks is beneficial under a variety of circumstances, but the prior knowledge necessary to correctly model task relationships is rarely available in practice. In this paper, we develop a novel kernel-based multi-task learning technique that automatically reveals structural inter-task relationships. Building over the framework of output kernel learning (OKL), we introduce a method that jointly learns multiple functions and a low-rank multi-task kernel by solving a non-convex regularization problem. Optimization is carried out via a block coordinate descent strategy, where each subproblem is solved using suitable conjugate gradient (CG) type iterative methods for linear operator equations. The effectiveness of the proposed approach is demonstrated on pharmacological and collaborative filtering data.", "histories": [["v1", "Wed, 16 Jan 2013 20:16:02 GMT  (31kb,D)", "http://arxiv.org/abs/1301.3816v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["francesco dinuzzo"], "accepted": false, "id": "1301.3816"}, "pdf": {"name": "1301.3816.pdf", "metadata": {"source": "CRF", "title": "Learning Output Kernels for Multi-Task Problems", "authors": ["Francesco Dinuzzo"], "emails": ["fdinuzzo@tuebingen.mpg.de"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point that it will be able to use the mentionlcihsrteeSe rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rf\u00fc the rf\u00fc the rfu"}, {"heading": "2 Weighted output kernel learning", "text": "Let me denote the identity matrix and e the vector of all ones (with suitable dimensions): For each matrix A, AT should denote the transposition, tr (A) the track, rank (A) the rank, vec (A) the vectorization operator, A \u2020 the pseudo-inverse, and A \u00b2 F = (tr (ATA) 1 / 2 the Frobenius norm, and vice versa A \u00b2: = tr (ATA) 1 / 2 the core norm. For each pair of matrices of the same size A, B < A, B > F: = tr (ATB) the Frobenius inner product. The symbols and denote the Kronecker product or the Hadamard product (element-wise). Finally, Sm + should denote the closed cone of positively defined matrices of the order m, and Sm + = {A \u00b2 Srix + (element-wise) or the Hadamard product (element-wise) as less."}, {"heading": "2.1 Output kernel learning for multi-task problems", "text": "The problem with missing output components is that the number of output components for each example is missing, more precisely, the i-th set of output components is a binary vector that can contain missing components. Since the weight matrix is always available, we can assume that all missing output components do without loss of generality. In the following, we will describe a regulation approach in which the function is always available in relation to the missing components. Since the weight matrix is always available, we can assume that all missing output components are zeros.We will describe a regulation method in which the function is sought in relation to the space."}, {"heading": "2.2 Equivalent formulations", "text": "Before going into the details of the optimization process proposed to solve (2), it is useful to introduce some equivalent reformulations of the optimization problem. (The proofs of the two following terms are given in the appendix. (A, B) If (A, B) is an optimal solution to the following problem: min A, R, p, p, p, p, p, p, p, p, (2). (2) Then each pair (C, L) is an optimal solution to the problem (2). (A, B, A = BBT, A = CB, an optimal solution to the problem (2). (Lemma 2.1 offers a new formulation of the low-grade production kernel learning problem (2), which includes only \"thin\" matrices A and B, the size of which can be controlled by selecting the parameter p. (BBT, A = CB, A = CB, is an optimal solution to the problem (2)."}, {"heading": "3 A block coordinate descent strategy", "text": "In the following, we will focus on the solution of (4). If p is much smaller than m, the handling of the low-dimensional factor B is much more convenient than the direct handling of the full output core matrix. Let J (A, B) denote the objective functioning of (4). Note that J, although it is not convex, is unlimited and separately convex with respect to both factors. Therefore, it is natural to use a block descending technique that alternates iteratively between the optimization with respect to the two blocks. Of course, other optimization strategies are also possible, but the block descending approach is memory efficient, robust, and easy to implement. Furthermore, it proves to be good in practice, since very few iterations are typically sufficient to obtain a good solution, especially in combination with a warm-start regularization method (see subsection 3.3)."}, {"heading": "3.1 Sub-problem w.r.t. A", "text": "In fact, it is such that we are in a position to enter into another world, in which we are in a position, in which we are located, and in which we are in a position, in which we are located, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we, in which we live, in which we live, in which we live, in which we, in which we, in which we live, in which we, in which we, in which we live, in which we live, in which we, in which we live, in which we, in which we live, in which we, in which we, in which we, in which we live, in which we, in which we, in which we, in which we, in which we, in which we, in which we live, in which we, in which we, in which we, in which we, in which we, in which we live, in which we, in which we, in which we, in which we live, in which we, in which we, in which we, in which we, in which we, in which we live, in which we, in which we, in which we, in which we, we, in which we, in which we, in which we, in which we, in which we, we, in which we, in which we, we, in which we, in which we, in which we, in which we, in which we, we, in which we, in which we, in which we, in which we, we, in which we, in which we, in which we, we, in which we, in which we, in which we, in which we, we, in which we, in which we, we, in which we, in which we, in which we, in which we, we, in which we, we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, we, we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, in which we, we, in which we, in which"}, {"heading": "3.2 Sub-problem w.r.t. B", "text": "The partial problem w.r.t B is essentially a multiple burr regression problem. First, it should be noted that the objective functionality of A depends only on the product E = KA. Thus, if A is fixed, a necessary and sufficient condition for B to be optimal is \u2202 J \u2202 B (A, B) = ETW (EBT \u2212 Y) \u03bb + BT = 0.In this case, it turns out that an expression for the lines of B can even be written in a closed form. Let bj (j = 1,.., m) the lines of B, y j and wj the columns of Y and W. Then we have havebj = (ET diag (wj) E + \u03bbI) \u2212 1 Ediag (wj) yj. Note that the updates for the different lines can be calculated in parallel. Finally, by using LBB = [(ET I) diag (vec (WT)))) (E) + actualization I [wj], we can also make sure that the variations for the different lines can be calculated in parallel (Y)."}, {"heading": "3.3 Implementation details", "text": "Several important details need to be taken into account to ensure correct and efficient convergence behaviour."}, {"heading": "3.3.1 Warm-start path procedure", "text": "Typically, it is necessary to train the core machine for several different values of the regularization parameter \u03bb. In general, the regularization problem is better conditioned to large values of \u03bb. At the same time, the solution is expected to continuously depend on the regularization parameter. Therefore, it is convenient to initialize the optimization of each problem with the solution achieved with the previous value of \u03bb, after the different values of the regularization parameter have been sorted in descending order. Such a warm-start regularization path strategy is very effective in practice: If the value grid of \u03bb is sufficiently fine or two iterations of block coordinate reduction for each value of \u03bb are sufficient to align with good accuracy."}, {"heading": "3.3.2 Initialization", "text": "It is possible to show that the rank of B cannot increase between an iteration of block coordinate descent and the next one. Therefore, B is initialized to a full-fledged matrix right at the beginning of the warm-start procedure. Otherwise, an optimal solution of a high rank can be missed by the algorithm. Another problem arises from the fact that the origin is a stationary point of the objective function value for any value of \u03bb. Although the origin of very large values of the regulation parameter is actually a global minimizer, this is no longer the case for small values. Now, in view of the warm-start procedure, the algorithm can never get away from the origin if the initial value is very large. To protect this behavior, B is initialized back to a full-fledged matrix each time it becomes too small (e.g. the Frobenius standard)."}, {"heading": "4 Experiments", "text": "In this section, we analyze the performance of weighted OKL on a variety of multi-task problems, including pharmacokinetic-pharmacodynamic (PK-PD) problems and popular collaborative filter benchmarks (MovieLens data sets). In all real-world experiments, the performance of weighted OKL is compared with the following methods: \u2022 Independent learning with a single task: equivalent to defining the source core as L = I, rather than optimizing it, allowing each task to be learned independently of the others. \u2022 Pooled Single-Task Learning: equivalent to defining the source core as L = eeT, assuming that all tasks are the same. \u2022 Regulated Matrix Factorization (RMF): equivalent to defining the input kernel as a Kronecker delta kernel, so that K = I. All experiments were conducted in a MATLAB environment with an Intel i5 PU 2.4 GHz RAM."}, {"heading": "4.1 Reconstruction and denoising of multiple signals", "text": "In order to analyze the dependence of the computing time on the ranking parameters p of the proposed OKL method, we performed some controlled experiments on synthetic data. Experiments show both the computational and the potential predictive advantage induced by the hard ranking constraint. First, we generated 50 independent findings Zk, (k = 1,., 50) of a Gaussian process on the interval [\u2212 1, 1] of the real line with the zero mean and the covariance functioncov [(Zk) x1, (Zk) x2] = exp (\u2212 10 | x1 \u2212 x2 |).Then we created m = 200 new processes Uj asUj k = 1 BjkZk, whereby the mixing coefficients Bjk are plotted independently of a uniform distribution on the interval."}, {"heading": "4.2 Pharmacological data", "text": "rE \"s tis, so rf\u00fc ide rf\u00fc ide rf\u00fc the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf for the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the r the rf the rf the rf the rf the rf the rf the ru the rf the rf the rf the rf the ru the rf the rf the rf the rf the ru the rf the rf the rf the ru the rf the rf the rf the rf the rf the rf the rf the ru the rf the rf the rf the f the rf the f the f the f the rf the rf the"}, {"heading": "4.3 Collaborative filtering (MovieLens) data", "text": "The MovieLens datasets 1 are popular collaborative filter benchmarks, collections of ratings in the range {1,.., 5} assigned by multiple users to a series of movies. The goal is to learn the preferences of each user for all movies. This can be interpreted as a multi-task learning problem, where each task is the preference function of one of the users. Currently, three datasets of different sizes are available, see Table 3. In addition to the ratings, the records contain additional metrics associated with the movies (e.g. genre and title), the users (e.g. gender, age, cast) or the ratings themselves (timestamps, tags).The OKL method described in this paper has been applied to the three MovieLens datasets. The input kernel uses the metadata of the movie, while the similarity between users is automatically learned from the data."}, {"heading": "5 Conclusions and future developments", "text": "Learning multiple tasks and simultaneously concluding the relationships between them is possible by applying a nuclear-based method that learns from multiple sets of data a dismantled multi-task nucleus. By using a strategy for parentage of block coordinates and iterative solvers for linear operator equations, it is possible to efficiently obtain a minimizer that provides good predictive results. It avoids the problem of manually specifying the similarities between the tasks. Furthermore, systematic improvement of learning performance with respect to single-task baseline and standard regulated matrix approximations can be observed on multiple sets of data. In the future, it would be worthwhile to expand the proposed method by using loss functions that differ from the least square loss. Furthermore, it would be interesting to expand the framework so that other types of structural knowledge about the task relationships can be used, for example along the lines of [4, 35]."}, {"heading": "A Proofs", "text": "The detection of Lemma 2.1Any optimal A-R '\u00b7 p for problem (4) allows a unique decomposition of FormA = CB + U, UBT = 0.By allowing L = BBT, it follows that L-Sm, p +, and we KABT = KCBBT = KCL.In addition, we have < A, KA > F 2 = < U, KU > F 2 + < CTKC, L > F 2 \u2265 < C TKC, L > F2.It follows that we can set U = 0 and A = CB without any loss of generality. The detection of Lemma 2.2letter combination = CL, Problem (2) can be rewritten asmin-Sm, L-Sm, p-Sp (L), subject to rg (KL)."}, {"heading": "B Matrix identities", "text": "We remember some identities concerning the vectorization operator, the Kronecker and Hadamard products, see e.g. [21]: vec (AXB) = (BT A) vec (X), (8) (A B) (C D) = (AC) (BD), (9) vec (A B) = diag (vec (A) vec (B). (10) These identities are always valid when the sizes of the matrices involved are compatible."}], "references": [{"title": "A new approach to collaborative filtering: Operator estimation with spectral regularization", "author": ["J. Abernethy", "F. Bach", "T. Evgeniou", "J.-P. Vert"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Kernels for vector-valued functions: a review", "author": ["M.A. Alvarez", "L. Rosasco", "N.D. Lawrence"], "venue": "Foundations and Trends in Machine Learning, (3):195\u2013 266", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["R.K. Ando", "T. Zhang"], "venue": "Journal of Machine Learning Research, 6:1817\u20131853", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Convex multi-task feature learning", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning, 73(3):243\u2013272", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "A spectral regularization framework for multi-task structure learning", "author": ["A. Argyriou", "C.A. Micchelli", "M. Pontil", "Y. Ying"], "venue": "J. C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 25\u201332. MIT Press, Cambridge, MA, USA", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Task clustering and gating for Bayesian multitask learning", "author": ["B. Bakker", "T. Heskes"], "venue": "Journal of Machine Learning Research, 4:83\u201399", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Multi-task preference learning with an application to hearing aid personalization", "author": ["A. Birlutiu", "P. Groot", "T. Heskes"], "venue": "Neurocomputing, 73:1177\u20131185", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-task Gaussian process prediction", "author": ["E. Bonilla", "K.M. Chai", "C. Williams"], "venue": "J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems, pages 153\u2013160. MIT Press, Cambridge, MA", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Multitask learning", "author": ["R. Caruana"], "venue": "Machine Learning, 28:41\u201375", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Collaborative prediction using ensembles of maximum margin matrix factorizations", "author": ["D. DeCoste"], "venue": "Proceedings of the 23rd international conference on Machine learning, ICML \u201906, pages 249\u2013256, New York, NY, USA", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning low-rank output kernels", "author": ["F. Dinuzzo", "K. Fukumizu"], "venue": "Journal of Machine Learning Research - Proceedings Track, 20:181\u2013196", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning output kernels with block coordinate descent", "author": ["F. Dinuzzo", "C.S. Ong", "P. Gehler", "G. Pillonetto"], "venue": "Proceedings of the 28th Annual International Conference on Machine Learning, Bellevue, WA, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Client-server multi-task learning from distributed datasets", "author": ["F. Dinuzzo", "G. Pillonetto", "G. De Nicolao"], "venue": "IEEE Transactions on Neural Networks, 22(2):290\u2013303", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "The representer theorem for Hilbert spaces: a necessary and sufficient condition", "author": ["F. Dinuzzo", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Learning multiple tasks with kernel methods", "author": ["T. Evgeniou", "C.A. Micchelli", "M. Pontil"], "venue": "Journal of Machine Learning Research, 6:615\u2013637", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "A rank minimization heuristic with application to minimum order system approximation", "author": ["M. Fazel", "H. Hindi", "S. Boyd"], "venue": "In Proceedings of the American Control Conference,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Eigentaste: A constant time collaborative filtering algorithm", "author": ["K.Y. Goldberg", "T. Roeder", "D. Gupta", "C. Perkins"], "venue": "Information Retrieval, 4(2):133\u2013151", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}, {"title": "Modelling placebo response in depression trials using a longitudinal model with informative dropout", "author": ["R. Gomeni", "A. Lavergne", "E. Merlo-Pich"], "venue": "European Journal of Pharmaceutical Sciences, 36(1):4\u201310", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Empirical Bayes for learning to learn", "author": ["T. Heskes"], "venue": "Proceedings of ICML, pages 367\u2013374. Morgan Kaufmann", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "Methods of conjugate gradients for solving linear systems", "author": ["M.R. Hestenes", "E. Stiefel"], "venue": "Journal Of Research Of The National Bureau Of Standards, 49(6):409\u2013436", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1952}, {"title": "Topics in Matrix Analysis", "author": ["R.A. Horn", "C.R. Johnson"], "venue": "Cambridge University Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1991}, {"title": "Sulovsk\u00fd. A simple algorithm for nuclear norm regularized problems", "author": ["M.M. Jaggi"], "venue": "Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Conic programming for multitask learning", "author": ["T. Kato", "H. Kashima", "M. Sugiyama", "K. Asai"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 22(7):957 \u2013968", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Some results on Tchebycheffian spline functions", "author": ["G. Kimeldorf", "G. Wahba"], "venue": "Journal of Mathematical Analysis and Applications, 33(1):82\u201395", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1971}, {"title": "Non-linear matrix factorization with Gaussian processes", "author": ["N.D. Lawrence", "R. Urtasun"], "venue": "Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Model-based approach and signal detection theory to evaluate the performance of recruitment centers in clinical trials with antidepressant drugs", "author": ["E. Merlo-Pich", "R. Gomeni"], "venue": "Clinical Pharmacology and Therapeutics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "On learning vector-valued functions", "author": ["C.A. Micchelli", "M. Pontil"], "venue": "Neural Computation, 17:177\u2013204", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2005}, {"title": "Invexity and optimization", "author": ["S.K. Mishra", "G. Giorgi"], "venue": "Nonconvex Optimization and Its Applications. Springer, Dordrecht", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Nonparametric identification of population models via Gaussian processes", "author": ["M. Neve", "G. De Nicolao", "L. Marchesi"], "venue": "Automatica, 43(7):1134\u20131144", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 22(10):1345 \u20131359", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian online multitask learning of Gaussian processes", "author": ["G. Pillonetto", "F. Dinuzzo", "G. De Nicolao"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(2):193\u2013205", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-task compressive sensing with Dirichlet process priors", "author": ["Y. Qi", "D. Liu", "D. Dunson", "L. Carin"], "venue": "Andrew McCallum and Sam Roweis, editors, Proceedings of the 25th Annual International Conference on Machine Learning ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "author": ["J.D.M. Rennie", "N. Srebro"], "venue": "In Proceedings of the 22nd International Conference on Machine Learning, pages 713\u2013719. ACM", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2005}, {"title": "Comparison of the Bailer and Yeh methods using real data", "author": ["M. Rocchetti", "I. Poggesi"], "venue": "L. Aarons et al., editor, The population approach: Measuring and managing variability in response, concentration and dose, pages 385\u2013390, Brussels, Belgium", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1997}, {"title": "Exploiting unrelated tasks in multi-task learning", "author": ["B. Romera-Paredes", "A. Argyriou", "N. Berthouze", "M. Pontil"], "venue": "Journal of Machine Learning Research - Proceedings Track, 22:951\u2013959", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Iterative Methods for Sparse Linear Systems", "author": ["Y. Saad"], "venue": "Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2nd edition", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2003}, {"title": "GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems", "author": ["Y. Saad", "M.H. Schultz"], "venue": "SIAM Journal on Scientific and Statistical Computing,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1986}, {"title": "Online learning of multiple tasks and their relationships", "author": ["A. Saha", "P. Rai", "H. Daum\u00e9 III", "S. Venkatasubramanian"], "venue": "AISTATS, Ft. Lauderdale, Florida", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning with Kernels: Support Vector Machines", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": "Regularization, Optimization, and Beyond. (Adaptive Computation and Machine Learning). MIT Press", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning Gaussian process kernels via hierarchical Bayes", "author": ["A. Schwaighofer", "V. Tresp", "K. Yu"], "venue": "Advances in Neural Information Processing Systems, volume 17, pages 1209\u20131216", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2005}, {"title": "Algorithms for Linear-quadratic Optimization", "author": ["V. Sima"], "venue": "Marcel Dekker, New York", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1996}, {"title": "Weighted low-rank approximations", "author": ["N. Srebro", "T. Jaakkola"], "venue": "In 20th International Conference on Machine Learning, pages 720\u2013727. AAAI Press", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2003}, {"title": "Maximum-margin matrix factorization", "author": ["N. Srebro", "J.D.M. Rennie", "T.S. Jaakkola"], "venue": "L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 1329\u20131336. MIT Press, Cambridge, MA", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "editors", "author": ["S. Thrun", "L.Y. Pratt"], "venue": "Learning To Learn. Kluwer Academic Publishers, Boston, MA", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1998}, {"title": "An accelerated proximal gradient algorithm for nuclear norm regularized least squares problems", "author": ["K Toh", "S. Yun"], "venue": "Optimization Online", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Shift-invariant grouped multitask learning for gaussian processes", "author": ["Y. Wang", "R. Khardon", "P. Protopapas"], "venue": "ECML/PKDD (3), pages 418\u2013434", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2010}, {"title": "Inferring latent task structure for multitask learning by multiple kernel learning", "author": ["C. Widmer", "N. Toussaint", "Y. Altun", "G. R\u00e4tsch"], "venue": "BMC bioinformatics, 11(Suppl 8):S5", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-task learning for classification with Dirichlet process priors", "author": ["Y. Xue", "X. Liao", "L. Carin", "B. Krishnapuram"], "venue": "Journal of Machine Learning Research, 8:35\u201363", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning Gaussian processes from multiple tasks", "author": ["K. Yu", "V. Tresp", "A. Schwaighofer"], "venue": "Proceedings of the 22th Annual international conference on Machine learning ", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2005}, {"title": "Robust multi-task learning with t-processes", "author": ["S. Yu", "V. Tresp", "K. Yu"], "venue": "Proceedings of the 24th Annual international conference on Machine learning ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "Flexible latent variable models for multi-task learning", "author": ["J. Zhang", "Z. Ghahramani", "Y. Yang"], "venue": "Machine Learning, 73(3):221\u2013242", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2008}, {"title": "A convex formulation for learning task relationships in multi-task learning", "author": ["Y. Zhang", "D.-Y. Yeung"], "venue": "Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence (UAI), pages 733\u2013442, Catalina Island, CA, USA", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "The themes of transfer learning, multi-task learning or \u201clearning to learn\u201d [9, 19, 44] have attracted considerable attention in the literature, see [30] for a recent survey.", "startOffset": 76, "endOffset": 87}, {"referenceID": 18, "context": "The themes of transfer learning, multi-task learning or \u201clearning to learn\u201d [9, 19, 44] have attracted considerable attention in the literature, see [30] for a recent survey.", "startOffset": 76, "endOffset": 87}, {"referenceID": 43, "context": "The themes of transfer learning, multi-task learning or \u201clearning to learn\u201d [9, 19, 44] have attracted considerable attention in the literature, see [30] for a recent survey.", "startOffset": 76, "endOffset": 87}, {"referenceID": 29, "context": "The themes of transfer learning, multi-task learning or \u201clearning to learn\u201d [9, 19, 44] have attracted considerable attention in the literature, see [30] for a recent survey.", "startOffset": 149, "endOffset": 153}, {"referenceID": 8, "context": "This can be done by employing multi-layer neural networks where the hidden layer is shared among the tasks [9], or also within a convex regularization framework [4].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "This can be done by employing multi-layer neural networks where the hidden layer is shared among the tasks [9], or also within a convex regularization framework [4].", "startOffset": 161, "endOffset": 164}, {"referenceID": 6, "context": "Task relationships can be also modeled within the framework of Gaussian Processes estimation [7, 8, 31, 40, 46, 49] by designing a joint covariance function.", "startOffset": 93, "endOffset": 115}, {"referenceID": 7, "context": "Task relationships can be also modeled within the framework of Gaussian Processes estimation [7, 8, 31, 40, 46, 49] by designing a joint covariance function.", "startOffset": 93, "endOffset": 115}, {"referenceID": 30, "context": "Task relationships can be also modeled within the framework of Gaussian Processes estimation [7, 8, 31, 40, 46, 49] by designing a joint covariance function.", "startOffset": 93, "endOffset": 115}, {"referenceID": 39, "context": "Task relationships can be also modeled within the framework of Gaussian Processes estimation [7, 8, 31, 40, 46, 49] by designing a joint covariance function.", "startOffset": 93, "endOffset": 115}, {"referenceID": 45, "context": "Task relationships can be also modeled within the framework of Gaussian Processes estimation [7, 8, 31, 40, 46, 49] by designing a joint covariance function.", "startOffset": 93, "endOffset": 115}, {"referenceID": 48, "context": "Task relationships can be also modeled within the framework of Gaussian Processes estimation [7, 8, 31, 40, 46, 49] by designing a joint covariance function.", "startOffset": 93, "endOffset": 115}, {"referenceID": 2, "context": "A variety of other models have been proposed to represent and exploit inter-task relationships [3, 23,32,48,50,51].", "startOffset": 95, "endOffset": 114}, {"referenceID": 22, "context": "A variety of other models have been proposed to represent and exploit inter-task relationships [3, 23,32,48,50,51].", "startOffset": 95, "endOffset": 114}, {"referenceID": 31, "context": "A variety of other models have been proposed to represent and exploit inter-task relationships [3, 23,32,48,50,51].", "startOffset": 95, "endOffset": 114}, {"referenceID": 47, "context": "A variety of other models have been proposed to represent and exploit inter-task relationships [3, 23,32,48,50,51].", "startOffset": 95, "endOffset": 114}, {"referenceID": 49, "context": "A variety of other models have been proposed to represent and exploit inter-task relationships [3, 23,32,48,50,51].", "startOffset": 95, "endOffset": 114}, {"referenceID": 50, "context": "A variety of other models have been proposed to represent and exploit inter-task relationships [3, 23,32,48,50,51].", "startOffset": 95, "endOffset": 114}, {"referenceID": 5, "context": "One possibility is to assume that the tasks can be clustered into homogeneous groups and try to learn such clustering from the data [6].", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "Optimization-based approaches that jointly infer the task parameters and the inter-task relationships in the form of a similarity matrix include the spectral regularization approach of [5] and the method of [52] based on convex optimization.", "startOffset": 185, "endOffset": 188}, {"referenceID": 51, "context": "Optimization-based approaches that jointly infer the task parameters and the inter-task relationships in the form of a similarity matrix include the spectral regularization approach of [5] and the method of [52] based on convex optimization.", "startOffset": 207, "endOffset": 211}, {"referenceID": 37, "context": "An online method has been also proposed recently to learn multiple linear classifiers as well as a task relationship matrix [38].", "startOffset": 124, "endOffset": 128}, {"referenceID": 38, "context": "Regularized kernel methods [39] have been employed successfully in a variety of single-task learning problems.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "Their extension to the multi-task setting [15] calls for the design of suitable operator-valued kernels that model similarities of both inputs and tasks.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "Some classes of operator-valued kernels have been recently reviewed in [2].", "startOffset": 71, "endOffset": 74}, {"referenceID": 46, "context": "A possible way to address this problem consists in learning a linear combination of task-specific kernels [47], within the framework of multiple kernel learning (MKL).", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "Recently, a class of output kernel learning (OKL) methods [12, 13] has been introduced in the context of multi-output learning problems (such as vectorial regression, multi-class and multi-label classification) to automatically synthesize a decomposable matrixvalued kernel that encodes the relationships between the output components.", "startOffset": 58, "endOffset": 66}, {"referenceID": 12, "context": "Recently, a class of output kernel learning (OKL) methods [12, 13] has been introduced in the context of multi-output learning problems (such as vectorial regression, multi-class and multi-label classification) to automatically synthesize a decomposable matrixvalued kernel that encodes the relationships between the output components.", "startOffset": 58, "endOffset": 66}, {"referenceID": 10, "context": "To this end, we extend a technique for learning low-rank output kernels proposed in [11] to the multi-task setting.", "startOffset": 84, "endOffset": 88}, {"referenceID": 41, "context": "[42], since existing optimization techniques based on eigendecompositions do not carry over to the weighted case.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "Similarly to [52], we learn multiple tasks as well as their relationships by solving a joint optimization problem.", "startOffset": 13, "endOffset": 17}, {"referenceID": 51, "context": "However, differently from [52] and [38], that learn multiple linear functions by convex optimization, we learn multiple non-linear functions by solving a non-convex optimization problem.", "startOffset": 26, "endOffset": 30}, {"referenceID": 37, "context": "However, differently from [52] and [38], that learn multiple linear functions by convex optimization, we learn multiple non-linear functions by solving a non-convex optimization problem.", "startOffset": 35, "endOffset": 39}, {"referenceID": 7, "context": "Our method is also related to the technique derived in [8] in the context of Bayesian estimation of Gaussian Processes, that allows to learn a similarity (covariance) matrix between the tasks.", "startOffset": 55, "endOffset": 58}, {"referenceID": 7, "context": "While [8] aims at optimizing a marginal likelihood type functional, our method is based on the minimization of a functional with trace norm regularization plus rank constraint.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "In [8], the authors adopt a general purpose gradient-descent solver to optimize their objective functional, whereas in this paper we develop a novel optimization strategy that is specifically designed to solve the proposed OKL optimization problem.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "Differently from [8], our method is able to deal with the case of incomplete sampling, and automatically encourages low-rank solutions without introducing relaxations of the original problem.", "startOffset": 17, "endOffset": 20}, {"referenceID": 26, "context": "For more details about RKHS of vector-valued functions, see [27].", "startOffset": 60, "endOffset": 64}, {"referenceID": 13, "context": "In view of the representer theorem [14, 24], the minimization problem with respect to g admits a solution of the form", "startOffset": 35, "endOffset": 43}, {"referenceID": 23, "context": "In view of the representer theorem [14, 24], the minimization problem with respect to g admits a solution of the form", "startOffset": 35, "endOffset": 43}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "In addition, for p = m it is an invex function [28] in the interior of the feasible set, meaning that every stationary point is a global minimizer.", "startOffset": 47, "endOffset": 51}, {"referenceID": 32, "context": "The related MMMF (maximum-margin matrix factorization) technique [33,43] would also correspond to the case K = I, but with hinge-type (SVM) losses instead of the square loss.", "startOffset": 65, "endOffset": 72}, {"referenceID": 42, "context": "The related MMMF (maximum-margin matrix factorization) technique [33,43] would also correspond to the case K = I, but with hinge-type (SVM) losses instead of the square loss.", "startOffset": 65, "endOffset": 72}, {"referenceID": 40, "context": "[41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "A first possible way to attack the problem is trying to directly obtain a solution of the non-symmetric operator equation (5) by means of iterative methods that can handle non-symmetric equations, such as the generalized minimal residual method (GMRES) [37].", "startOffset": 253, "endOffset": 257}, {"referenceID": 19, "context": "The alternative is to introduce a change of variable to make the problem symmetric, and then apply a preconditioned conjugate gradient (CG) algorithm [20] on the new linear equation.", "startOffset": 150, "endOffset": 154}, {"referenceID": 35, "context": "[36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "where the mixing coefficients Bjk are independently drawn from a uniform distribution on the interval [0, 1].", "startOffset": 102, "endOffset": 108}, {"referenceID": 17, "context": "The Study 810 dataset [18, 26] has been obtained from a multicentric clinical trial (Study 810) for testing the efficacy of paroxetine (an antidepressant drug).", "startOffset": 22, "endOffset": 30}, {"referenceID": 25, "context": "The Study 810 dataset [18, 26] has been obtained from a multicentric clinical trial (Study 810) for testing the efficacy of paroxetine (an antidepressant drug).", "startOffset": 22, "endOffset": 30}, {"referenceID": 12, "context": "Following the setup of [13], we extracted a test set containing 1012 scores, including all the scores taken after the third week for a subset of 450 randomly chosen patients.", "startOffset": 23, "endOffset": 27}, {"referenceID": 28, "context": "The PK-PD 27 dataset [29, 34] contains xenobiotics concentration time profiles for 27 human subjects, with samples taken at {0.", "startOffset": 21, "endOffset": 29}, {"referenceID": 33, "context": "The PK-PD 27 dataset [29, 34] contains xenobiotics concentration time profiles for 27 human subjects, with samples taken at {0.", "startOffset": 21, "endOffset": 29}, {"referenceID": 30, "context": "In order to simulate a realistic sparse sampling scenario, we follow the approach of [31], where only 3 measurements per subject (out of the 8 available) are randomly selected for training.", "startOffset": 85, "endOffset": 89}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The dataset MovieLens1M does not come with predefined test sets, therefore we also extracted a random test set containing about the 50% of the ratings for each user, a setup adopted in [22,45].", "startOffset": 185, "endOffset": 192}, {"referenceID": 44, "context": "The dataset MovieLens1M does not come with predefined test sets, therefore we also extracted a random test set containing about the 50% of the ratings for each user, a setup adopted in [22,45].", "startOffset": 185, "endOffset": 192}, {"referenceID": 0, "context": "Other recent results on these datasets under various experimental settings can be found, for example, in [1, 10,22,25,33,45].", "startOffset": 105, "endOffset": 124}, {"referenceID": 9, "context": "Other recent results on these datasets under various experimental settings can be found, for example, in [1, 10,22,25,33,45].", "startOffset": 105, "endOffset": 124}, {"referenceID": 21, "context": "Other recent results on these datasets under various experimental settings can be found, for example, in [1, 10,22,25,33,45].", "startOffset": 105, "endOffset": 124}, {"referenceID": 24, "context": "Other recent results on these datasets under various experimental settings can be found, for example, in [1, 10,22,25,33,45].", "startOffset": 105, "endOffset": 124}, {"referenceID": 32, "context": "Other recent results on these datasets under various experimental settings can be found, for example, in [1, 10,22,25,33,45].", "startOffset": 105, "endOffset": 124}, {"referenceID": 44, "context": "Other recent results on these datasets under various experimental settings can be found, for example, in [1, 10,22,25,33,45].", "startOffset": 105, "endOffset": 124}, {"referenceID": 3, "context": "Also, it would be interesting to extend the framework so as to exploit other types of structural knowledge about the task relationships, for instance along the lines of [4, 35].", "startOffset": 169, "endOffset": 176}, {"referenceID": 34, "context": "Also, it would be interesting to extend the framework so as to exploit other types of structural knowledge about the task relationships, for instance along the lines of [4, 35].", "startOffset": 169, "endOffset": 176}, {"referenceID": 20, "context": "[21]:", "startOffset": 0, "endOffset": 4}], "year": 2013, "abstractText": "Simultaneously solving multiple related learning tasks is beneficial under a variety of circumstances, but the prior knowledge necessary to correctly model task relationships is rarely available in practice. In this paper, we develop a novel kernel-based multi-task learning technique that automatically reveals structural inter-task relationships. Building over the framework of output kernel learning (OKL), we introduce a method that jointly learns multiple functions and a low-rank multi-task kernel by solving a non-convex regularization problem. Optimization is carried out via a block coordinate descent strategy, where each subproblem is solved using suitable conjugate gradient (CG) type iterative methods for linear operator equations. The effectiveness of the proposed approach is demonstrated on pharmacological and collaborative filtering data.", "creator": "LaTeX with hyperref package"}}}