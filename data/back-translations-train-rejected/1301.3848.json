{"id": "1301.3848", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Any-Space Probabilistic Inference", "abstract": "We have recently introduced an any-space algorithm for exact inference in Bayesian networks, called Recursive Conditioning, RC, which allows one to trade space with time at increments of X-bytes, where X is the number of bytes needed to cache a floating point number. In this paper, we present three key extensions of RC. First, we modify the algorithm so it applies to more general factorization of probability distributions, including (but not limited to) Bayesian network factorizations. Second, we present a forgetting mechanism which reduces the space requirements of RC considerably and then compare such requirmenets with those of variable elimination on a number of realistic networks, showing orders of magnitude improvements in certain cases. Third, we present a version of RC for computing maximum a posteriori hypotheses (MAP), which turns out to be the first MAP algorithm allowing a smooth time-space tradeoff. A key advantage of presented MAP algorithm is that it does not have to start from scratch each time a new query is presented, but can reuse some of its computations across multiple queries, leading to significant savings in ceratain cases.", "histories": [["v1", "Wed, 16 Jan 2013 15:49:42 GMT  (311kb)", "http://arxiv.org/abs/1301.3848v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["adnan darwiche"], "accepted": false, "id": "1301.3848"}, "pdf": {"name": "1301.3848.pdf", "metadata": {"source": "CRF", "title": "Any-Space Probabilistic Inference", "authors": ["Adnan Darwiche"], "emails": ["darwiche@cs."], "sections": [{"heading": null, "text": "This year, it has come to the point where it is only a matter of time before a solution is found, in which a solution is found."}, {"heading": "2 Recursive Conditioning", "text": "The intuition behind the RC is simple: We have a number of variables that will split a network into two separate parts; < / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /"}], "references": [{"title": "Bayesian belief-network in\u00ad ference using recursive decomposition", "author": ["Gregory F. Cooper"], "venue": "Tech\u00ad nical Report KSL-90-05,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1990}, {"title": "Probabilistic Networks and Expert Systems", "author": ["R. Cowell", "A. Dawid", "S. Lauritzen", "D. Spiegelhalter"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Recursive conditioning: Any\u00ad space conditioning algorithm with treewidth\u00ad bounded complexity", "author": ["Adnan Darwiche"], "venue": "Artificial Intelligence Journal,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Bucket elimination: A unifying framework for probabilistic inference", "author": ["Rina Dechter"], "venue": "In Pro\u00ad ceedings of the 12th Conference on Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Topological parameters for time\u00ad space tradeoff", "author": ["Rina Dechter"], "venue": "In Proceedings of the 12th Con-", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Inference in belief networks: A procedural guide", "author": ["Cecil Huang", "Adnan Darwiche"], "venue": "In\u00ad ternational Journal of Approximate Reasoning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "Bayesian updating in recursive graphical mod\u00ad els by local computation", "author": ["F.V. Jensen", "S.L. Lauritzen", "K.G. Olesen"], "venue": "Computational Statis\u00ad tics Quarterly,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1990}, {"title": "Probabilistic Reasoning in Intelli\u00ad gent Systems: Networks of Plausible Inference", "author": ["Judea Pearl"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1988}, {"title": "Fusion and propagation with multiple observations in belief networks", "author": ["Mark A. Peot", "Ross D. Shachter"], "venue": "Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1991}, {"title": "Symbolic Probabilistic Inference in Be\u00ad lief Networks", "author": ["R. Shachter", "B.D. D'Ambrosio", "B. del Favero"], "venue": "In Proc. Conf. on Uncertainty in AI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "Ex\u00ad ploiting causal independence in bayesian net\u00ad work inference", "author": ["Nevin Lianwen Zhang", "David Poole"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1996}], "referenceMentions": [{"referenceID": 2, "context": "We have recently introduced an any-space algorithm for exact inference in Bayesian networks, called Re\u00ad cursive Conditioning, RC, which allows one to trade space with time at increments of X-bytes, where X is the number of bytes needed to cache a floating point number [3].", "startOffset": 269, "endOffset": 272}, {"referenceID": 6, "context": "Given a network of size n (has n variables ) and an elimination order of width w, RC takes O(nexp(wlogn)) time under O(n) spp,ce, which is a new complexity result for linear-space Bayesian network inference, and takes O(nexp(w)) time under O(nexp(w)) space, therefore, matching the complexity of clustering [7, 6] and elimination [10, 4, 11] algorithms.", "startOffset": 307, "endOffset": 313}, {"referenceID": 5, "context": "Given a network of size n (has n variables ) and an elimination order of width w, RC takes O(nexp(wlogn)) time under O(n) spp,ce, which is a new complexity result for linear-space Bayesian network inference, and takes O(nexp(w)) time under O(nexp(w)) space, therefore, matching the complexity of clustering [7, 6] and elimination [10, 4, 11] algorithms.", "startOffset": 307, "endOffset": 313}, {"referenceID": 9, "context": "Given a network of size n (has n variables ) and an elimination order of width w, RC takes O(nexp(wlogn)) time under O(n) spp,ce, which is a new complexity result for linear-space Bayesian network inference, and takes O(nexp(w)) time under O(nexp(w)) space, therefore, matching the complexity of clustering [7, 6] and elimination [10, 4, 11] algorithms.", "startOffset": 330, "endOffset": 341}, {"referenceID": 3, "context": "Given a network of size n (has n variables ) and an elimination order of width w, RC takes O(nexp(wlogn)) time under O(n) spp,ce, which is a new complexity result for linear-space Bayesian network inference, and takes O(nexp(w)) time under O(nexp(w)) space, therefore, matching the complexity of clustering [7, 6] and elimination [10, 4, 11] algorithms.", "startOffset": 330, "endOffset": 341}, {"referenceID": 10, "context": "Given a network of size n (has n variables ) and an elimination order of width w, RC takes O(nexp(wlogn)) time under O(n) spp,ce, which is a new complexity result for linear-space Bayesian network inference, and takes O(nexp(w)) time under O(nexp(w)) space, therefore, matching the complexity of clustering [7, 6] and elimination [10, 4, 11] algorithms.", "startOffset": 330, "endOffset": 341}, {"referenceID": 7, "context": "In cutset conditioning, this power is exploited to singly-connect a network so it can be solved using the polytree algorithm [8, 9].", "startOffset": 125, "endOffset": 131}, {"referenceID": 8, "context": "In cutset conditioning, this power is exploited to singly-connect a network so it can be solved using the polytree algorithm [8, 9].", "startOffset": 125, "endOffset": 131}, {"referenceID": 5, "context": "1 One way to define the width w of a variable elimina\u00ad tion order 1r is as follows: If a jointree for the Bayesian network is constructed based on the ordering 1r [6], then the size of its maximal clique would be w + 1.", "startOffset": 163, "endOffset": 166}, {"referenceID": 0, "context": "2 A similar algorithm was developed independently by Gregory Cooper in [1], under the name recursive decom\u00ad position.", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "We compare the two algorithms in [3].", "startOffset": 33, "endOffset": 36}, {"referenceID": 2, "context": "Definition 2 [3] The cutset of internal node T in a dtree is cutset(T) d.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "We showed in [3] that given an elimination order of length n and width w, we can generate a dtree in which the size of every a-cutset is O(w log n) .", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "Moreover, if cf(T) = 1 for each node T, we obtain another extreme which has the same time and space complexity of clustering and elimination algorithms [3].", "startOffset": 152, "endOffset": 155}, {"referenceID": 2, "context": "rem, one can plot time-space tradeoff curves for com\u00ad putationally demanding networks-we show such curves in [3].", "startOffset": 109, "endOffset": 112}, {"referenceID": 2, "context": "5 Given an elimi\u00ad nation order of width w, we can always generate a dtree such that acutset(T)# is O(exp(w log n)), or such that cutset(T)#context(T)# is O(exp(w) ) [3].", "startOffset": 165, "endOffset": 168}, {"referenceID": 2, "context": "Under full caching, the time and space complexity of recursive conditioning is O(n exp(w)), where n is the number of factors and w is the width of used dtree [3].", "startOffset": 158, "endOffset": 161}, {"referenceID": 4, "context": "of separators, which are typically smaller than clus\u00ad ters [5].", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "space by using a jointree with smaller separators, at the expense of introducing larger clusters [5].", "startOffset": 97, "endOffset": 100}, {"referenceID": 4, "context": "To address this problem, a hybrid algorithm is proposed which uses cutset conditioning to solve each enlarged cluster, where the complexity of this hybrid method can be less than exponential in the size of enlarged clusters [5].", "startOffset": 224, "endOffset": 227}, {"referenceID": 4, "context": "The second key difference between the proposal of [5] and ours is that when the hybrid algorithm of [5] is run in linear space, it will reduce to cutset conditioning since the whole jointree will be combined into a single cluster.", "startOffset": 50, "endOffset": 53}, {"referenceID": 4, "context": "The second key difference between the proposal of [5] and ours is that when the hybrid algorithm of [5] is run in linear space, it will reduce to cutset conditioning since the whole jointree will be combined into a single cluster.", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "8This phenomenon is also true in variable elimination algorithms for computing MAP, so it appears to be a property of MAP rather than a problem of our approach for computing it [4].", "startOffset": 177, "endOffset": 180}, {"referenceID": 2, "context": "We have presented in [3] a linear-time algorithm, EL2DT, for converting an elimination order 1r into a dtree T, with the guarantee that the width ofT is no greater than the width of 1r.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "EL2SDT is important not only for computing MAP, but for inference in conditional Gaussian networks which contain both discrete and continuous variables [2] .", "startOffset": 152, "endOffset": 155}], "year": 2011, "abstractText": "We have recently introduced an any-space algorithm for exact inference in Bayesian networks, called Recursive Conditioning, RC, which allows one to trade space with time at increments of X-bytes, where X is the number of bytes needed to cache a floating point number. In this paper, we present three key extensions of RC. First, we modify the algorithm so it applies to more general factorizations of probability distributions, including (but not limited to ) Bayesian network factorizations. Sec\u00ad ond, we present a forgetting mechanism which reduces the space requirements of RC considerably and then compare such requirements with those of variable elim\u00ad ination on a number of realistic networks, showing orders of magnitude improvements in certain cases. Third, we present a ver\u00ad sion of RC for computing maximum a pos\u00ad teriori hypotheses (MAP}, which turns out to be the first MAP algorithm allowing a smooth time-space tradeoff. A key advan\u00ad tage of the presented MAP algorithm is that it does not have to start from scratch each time a new query is presented, but can reuse some of its computations across mul\u00ad tiple queries, leading to significant savings in certain cases.", "creator": "pdftk 1.41 - www.pdftk.com"}}}