{"id": "1301.3867", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Fast Planning in Stochastic Games", "abstract": "Stochastic games generalize Markov decision processes (MDPs) to a multiagent setting by allowing the state transitions to depend jointly on all player actions, and having rewards determined by multiplayer matrix games at each state. We consider the problem of computing Nash equilibria in stochastic games, the analogue of planning in MDPs. We begin by providing a generalization of finite-horizon value iteration that computes a Nash strategy for each player in generalsum stochastic games. The algorithm takes an arbitrary Nash selection function as input, which allows the translation of local choices between multiple Nash equilibria into the selection of a single global Nash equilibrium.", "histories": [["v1", "Wed, 16 Jan 2013 15:50:58 GMT  (278kb)", "http://arxiv.org/abs/1301.3867v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.GT cs.AI", "authors": ["michael kearns", "yishay mansour", "satinder singh"], "accepted": false, "id": "1301.3867"}, "pdf": {"name": "1301.3867.pdf", "metadata": {"source": "CRF", "title": "Fast Planning in Stochastic Games", "authors": ["Michael Kearns", "Yishay Mansour", "Satinder Singh"], "emails": ["mkearns@research.att.com", "mansour@math.", "baveja@research.att.com"], "sections": [{"heading": null, "text": "This year it is more than ever before in the history of the city."}, {"heading": "Algorithm SparseGame(s, T):", "text": "andE [max {IVk] [8, T] (i, j) - Qk = > k [s, T] (i, j) l]:; f. (\u2022, J) Proof: By induction on T we show that the probability that IQk [s, T] - Vk [s, T] (i, j) l: 2: >.T is limited by any quantity s; and that for any fixed (i, j), the probability that 1Qk [s, T] (i, j) - Vk [s, T] (i, j) l: 2: >.T is limited by any quantity s;. These limits are then used to bind the expectations of interest.In the case T = 0 we have 6r = J! r = 0. Let's take the inductive hypothesis for the horizon times 1,..., T - 1. We define the random variable Uk [8, T] (i, j)."}], "references": [{"title": "Contin\u00ad uous Value Function Approximation for Sequential Bidding Policies", "author": ["C. Boutilier", "M. Goldszmidt", "B. Sabata"], "venue": "In Proceedings of the 15th Conference on Uncer\u00ad tainty in Artificial Intelligence", "citeRegEx": "Boutilier et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1999}, {"title": "A Near\u00ad Optimal Polynomial Time Algorithm for Learning in Cer\u00ad tain Classes of Stochastic Games", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "Proceedings of the 16th International Joint Conference on Artificial Intelli\u00ad gence.", "citeRegEx": "Brafman and Tennenholtz,? 1999", "shortCiteRegEx": "Brafman and Tennenholtz", "year": 1999}, {"title": "Multiagent reinforce\u00ad ment learning: Theoretical framework and an algorithm", "author": ["J. Hu", "M.P. Wellman"], "venue": "Proceedings of the Fifteenth International Conference on Machine Learning, pages 242-250. Kearns M., Mansour Y., and Ng A. (1999). A Sparse", "citeRegEx": "Hu and Wellman,? 1998", "shortCiteRegEx": "Hu and Wellman", "year": 1998}, {"title": "Approximate Planning for Factored POMDPs using Belief State Simplification", "author": ["D. McAJJester", "S. Singh"], "venue": "Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence.", "citeRegEx": "McAJJester and Singh,? 1999", "shortCiteRegEx": "McAJJester and Singh", "year": 1999}, {"title": "Game Theory", "author": ["G. Owen"], "venue": "Academic Press, UK. Shapley L.S. (1953). Stochastic Games. Proceedings of the National Academy of Sciences of the United States of America, 39:1095-1100.", "citeRegEx": "Owen,? 1995", "shortCiteRegEx": "Owen", "year": 1995}], "referenceMentions": [{"referenceID": 1, "context": "Given the detailed theoretical and practical under\u00ad standing of single-agent planning and learning in Markov decision processes (MDPs) that has been built over the last decade, one natural line of research is the extension of these algorithms and analyses to a multi\u00ad agent setting (Boutilier, Goldszmidt, and Sabata 1999; Brafman and Tennenholtz 1999; Hu and Wellman 1998).", "startOffset": 282, "endOffset": 373}, {"referenceID": 2, "context": "Given the detailed theoretical and practical under\u00ad standing of single-agent planning and learning in Markov decision processes (MDPs) that has been built over the last decade, one natural line of research is the extension of these algorithms and analyses to a multi\u00ad agent setting (Boutilier, Goldszmidt, and Sabata 1999; Brafman and Tennenholtz 1999; Hu and Wellman 1998).", "startOffset": 282, "endOffset": 373}, {"referenceID": 4, "context": "It is well-known (Owen 1995) that every game has at least one Nash pair in the space of mixed (but not necessarily pure) strategies, and many games have multiple Nash pairs.", "startOffset": 17, "endOffset": 28}, {"referenceID": 4, "context": "It is known (Owen 1995) that we can restrict attention to policies (in the infinite-horizon discounted case) or time-dependent policies (in the finite-horizon case) without loss of generality- that is, no advantage can be gained by a player by con\u00ad sidering the history of play.", "startOffset": 12, "endOffset": 23}, {"referenceID": 4, "context": "Again, it is known (Owen 1995) that for the infinite discounted case, a Nash pair always exists in the space of policies, and for the average return case, a Nash pair always exists in the space of time-dependent policies.", "startOffset": 19, "endOffset": 30}], "year": 2011, "abstractText": "Stochastic games generalize Markov decision processes (MDPs) to a multiagent setting by allowing the state transitions to depend jointly on all player actions, and having rewards determined by multiplayer matrix games at each state. We consider the prob\u00ad lem of computing Nash equilibria in stochas\u00ad tic games, the analogue of planning in MDPs. We begin by providing a simple general\u00ad ization of finite-horizon value iteration that computes a Nash strategy for each player in general-sum stochastic games. The algo\u00ad rithm takes an arbitrary Nash selection func\u00ad tion as input, which allows the translation of local choices between multiple Nash equilib\u00ad ria into the selection of a single global Nash equilibrium. Our main technical result is an algorithm for computing near-Nash equilibria in large or in\u00ad finite state spaces. This algorithm builds on our finite-horizon value iteration algorithm, and adapts the sparse sampling methods of Kearns, Mansour and Ng (1999) to stochas\u00ad tic games. We conclude by describing a coun\u00ad terexample showing that infinite-horizon dis\u00ad counted value iteration, which was shown by Shapley to converge in the zero-sum case (a result we give extend slightly here), does not converge in the general-sum case.", "creator": "pdftk 1.41 - www.pdftk.com"}}}