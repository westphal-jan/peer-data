{"id": "1301.3869", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Policy Iteration for Factored MDPs", "abstract": "Many large MDPs can be represented compactly using a dynamic Bayesian network. Although the structure of the value function does not retain the structure of the process, recent work has shown that value functions in factored MDPs can often be approximated well using a decomposed value function: a linear combination of &lt;I&gt;restricted&lt;/I&gt; basis functions, each of which refers only to a small subset of variables. An approximate value function for a particular policy can be computed using approximate dynamic programming, but this approach (and others) can only produce an approximation relative to a distance metric which is weighted by the stationary distribution of the current policy. This type of weighted projection is ill-suited to policy improvement. We present a new approach to value determination, that uses a simple closed-form computation to directly compute a least-squares decomposed approximation to the value function &lt;I&gt;for any weights&lt;/I&gt;. We then use this value determination algorithm as a subroutine in a policy iteration process. We show that, under reasonable restrictions, the policies induced by a factored value function are compactly represented, and can be manipulated efficiently in a policy iteration process. We also present a method for computing error bounds for decomposed value functions using a variable-elimination algorithm for function optimization. The complexity of all of our algorithms depends on the factorization of system dynamics and of the approximate value function.", "histories": [["v1", "Wed, 16 Jan 2013 15:51:06 GMT  (306kb)", "http://arxiv.org/abs/1301.3869v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daphne koller", "ron parr"], "accepted": false, "id": "1301.3869"}, "pdf": {"name": "1301.3869.pdf", "metadata": {"source": "CRF", "title": "Policy Iteration for Factored MDPs", "authors": ["Daphne Koller", "Ronald Parr"], "emails": ["koller@cs.", "parr@cs."], "sections": [{"heading": null, "text": "This year it is more than ever before in the history of the city."}], "references": [{"title": "Ex\u00ad ploiting structure in policy construction", "author": ["C. Boutilier", "R. Dearden", "M. Goldszmidt"], "venue": "In Proc. IJCAI-95", "citeRegEx": "Boutilier et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Boutilier et al\\.", "year": 1995}, {"title": "The frame problem and bayesian network action representations", "author": ["C. Boutilier", "M. Goldszmidt"], "venue": "In Proc. 11th Biennial Canadian Conference on Artificial In\u00ad telligence,", "citeRegEx": "Boutilier and Goldszmidt.,? \\Q1996\\E", "shortCiteRegEx": "Boutilier and Goldszmidt.", "year": 1996}, {"title": "Linear least-squares al\u00ad gorithms for temporal difference learning", "author": ["S. Bradtke", "A. Barto"], "venue": "Machine Learning,", "citeRegEx": "Bradtke and Barto.,? \\Q1996\\E", "shortCiteRegEx": "Bradtke and Barto.", "year": 1996}, {"title": "Model minimization in Markov decision processes", "author": ["T. Dean", "R. Givan"], "venue": "In Proc", "citeRegEx": "Dean and Givan.,? \\Q1997\\E", "shortCiteRegEx": "Dean and Givan.", "year": 1997}, {"title": "A model for reasoning about persistence and causation", "author": ["T. Dean", "K. Kanazawa"], "venue": "Computational In\u00ad telligence,", "citeRegEx": "Dean and Kanazawa.,? \\Q1989\\E", "shortCiteRegEx": "Dean and Kanazawa.", "year": 1989}, {"title": "Bucket elimination: A unifying framework for reasoning", "author": ["R. Dechter"], "venue": "Artificial Intelligence,", "citeRegEx": "Dechter.,? \\Q1999\\E", "shortCiteRegEx": "Dechter.", "year": 1999}, {"title": "Learning and Value Function Approxi\u00ad mation in Complex Decision Problems", "author": ["B. Van Roy"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "Roy.,? \\Q1998\\E", "shortCiteRegEx": "Roy.", "year": 1998}, {"title": "Tight performance bounds on greedy policies based on imperfect value functions", "author": ["R. Williams", "L. Baird"], "venue": "Technical Report NU-CCS-93-14, Northeastern Uni\u00ad versity,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1993}], "referenceMentions": [], "year": 2011, "abstractText": "Many large MDPs can be represented compactly using a dynamic Bayesian network. Although the structure of the value function does not re\u00ad tain the structure of the process, recent work has suggested that value functions in factored MDPs can often be approximated well using a factored value function: a linear combination of restr icted basis functions, each of which refers only to a small subset of variables. An approximate fac\u00ad tored value function for a particular policy can be computed using approximate dynamic pro\u00ad gramming, but this approach (and others) can only produce an approximation relative to a dis\u00ad tance metric which is weighted by the station\u00ad ary distribution of the current policy. This type of weighted projection is ill-suited to policy im\u00ad provement. We present a new approach to value determination, that uses a simple closed-form computation to compute a least-squares decom\u00ad posed approximation to the value function for any weights directly. We then use this value de\u00ad termination algorithm as a subroutine in a pol\u00ad icy iteration process. We show that, under rea\u00ad sonable restrictions, the policies induced by a factored value function can be compactly repre\u00ad sented as a decision list, and can be manipulated efficiently in a policy iteration process. We also present a method for computing error bounds for decomposed value functions using a variable\u00ad elimination algorithm for function optimization. The complexity of all of our algorithms depends on the factorization of the system dynamics and of the approximate value function.", "creator": "pdftk 1.41 - www.pdftk.com"}}}