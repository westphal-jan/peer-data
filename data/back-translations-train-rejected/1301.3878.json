{"id": "1301.3878", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "PEGASUS: A Policy Search Method for Large MDPs and POMDPs", "abstract": "We propose a new approach to the problem of searching a space of policies for a Markov decision process (MDP) or a partially observable Markov decision process (POMDP), given a model. Our approach is based on the following observation: Any (PO)MDP can be transformed into an \"equivalent\" POMDP in which all state transitions (given the current state and action) are deterministic. This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions. We give a natural way of estimating the value of all policies in these transformed POMDPs. Policy search is then simply performed by searching for a policy with high estimated value. We also establish conditions under which our value estimates will be good, recovering theoretical results similar to those of Kearns, Mansour and Ng (1999), but with \"sample complexity\" bounds that have only a polynomial rather than exponential dependence on the horizon time. Our method applies to arbitrary POMDPs, including ones with infinite state and action spaces. We also present empirical results for our approach on a small discrete problem, and on a complex continuous state/continuous action problem involving learning to ride a bicycle.", "histories": [["v1", "Wed, 16 Jan 2013 15:51:42 GMT  (472kb)", "http://arxiv.org/abs/1301.3878v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["andrew y ng", "michael i jordan"], "accepted": false, "id": "1301.3878"}, "pdf": {"name": "1301.3878.pdf", "metadata": {"source": "CRF", "title": "PEGASUS: A policy search method for large MDPs and POMDPs", "authors": [], "emails": [], "sections": [{"heading": null, "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a region and in which it is a country."}], "references": [{"title": "Gradient descent for gen\u00ad eral Reinforcement Learning", "author": ["L. Baird", "A.W. Moore"], "venue": "In NIPS II,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1999}, {"title": "Dynamic Programming and Opti\u00ad mal Control, Vol. I", "author": ["Dimitri Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": ", [16, 8, 15, 1, 7]), which Michael Jordan", "startOffset": 2, "endOffset": 19}, {"referenceID": 0, "context": "We assume we have a deterministic function g : S x A x [0, 1]dp 1-t S, so that for any fixed (s, a)-pair, if pis distributed Uniform[O, 1]dp, then g(s, a,jf) is distributed according to the transition dis\u00ad tribution Psa ( \u00b7 ) .", "startOffset": 55, "endOffset": 61}, {"referenceID": 0, "context": "In other words, to draw a sample from Psa ( \u00b7) for some fixed s and a, we need only draw p uni\u00ad formly in [0, 1]dp, and then take g(s, a,jf) to be our sample.", "startOffset": 106, "endOffset": 112}, {"referenceID": 1, "context": "[2]) routinely define POMDPs using essentially deterministic simulative mod\u00ad els.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "S x [0, 1]dp into successor states S.", "startOffset": 4, "endOffset": 10}, {"referenceID": 0, "context": "\" For each f, also let fi be the i-th coordinate function (so that fi(s,if) is the i-th coordinate of f(s,P)) and let :F; be the corresponding families of coordinate functions map\u00ad ping from S x [0, 1]dp into [0, 1].", "startOffset": 195, "endOffset": 201}, {"referenceID": 0, "context": "\" For each f, also let fi be the i-th coordinate function (so that fi(s,if) is the i-th coordinate of f(s,P)) and let :F; be the corresponding families of coordinate functions map\u00ad ping from S x [0, 1]dp into [0, 1].", "startOffset": 209, "endOffset": 215}, {"referenceID": 0, "context": "The state space for M1 is S x [0, 1]00\u2022 In other words, a typical state in M1 can be written as a vector ( s, PI, P2, .", "startOffset": 30, "endOffset": 36}, {"referenceID": 0, "context": ") -this consists of a state s from the original state space S, followed by an infinite sequence of real numbers in [0, 1].", "startOffset": 115, "endOffset": 121}, {"referenceID": 0, "context": "S1 = S x [0, 1]00, so that (s, PI, P2, .", "startOffset": 9, "endOffset": 15}, {"referenceID": 0, "context": "The final implementational detail is that, since the states s\ufffdi) E S x [0, 1]00 are infinite-dimensional vectors, we have no way of representing them (and their successor states) explicitly.", "startOffset": 71, "endOffset": 77}, {"referenceID": 0, "context": "2More precisely, if the agent enters the goal region on some time step, then rather than giving it a reward of 1, we figure out what fraction r E [0, 1] of that time step (measured in continuous time) the agent had taken to enter the goal region, and then give it reward'\"( instead.", "startOffset": 146, "endOffset": 152}, {"referenceID": 0, "context": "For the remainder of this section, assume S = [0, 1]ds.", "startOffset": 46, "endOffset": 52}, {"referenceID": 0, "context": "Then :F is a class of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1]ds, and so a simple way to capture its \"complexity\" is to capture the complexity of its families of coordinate functions, :Fi, i = 1, .", "startOffset": 45, "endOffset": 51}, {"referenceID": 0, "context": "Then :F is a class of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1]ds, and so a simple way to capture its \"complexity\" is to capture the complexity of its families of coordinate functions, :Fi, i = 1, .", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "Then :F is a class of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1]ds, and so a simple way to capture its \"complexity\" is to capture the complexity of its families of coordinate functions, :Fi, i = 1, .", "startOffset": 70, "endOffset": 76}, {"referenceID": 0, "context": "Each :Fi is a family of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1], the i-th coordinate of the state vector.", "startOffset": 47, "endOffset": 53}, {"referenceID": 0, "context": "Each :Fi is a family of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1], the i-th coordinate of the state vector.", "startOffset": 58, "endOffset": 64}, {"referenceID": 0, "context": "Each :Fi is a family of functions mapping from [0, 1]ds x [0, 1]dp into [0, 1], the i-th coordinate of the state vector.", "startOffset": 72, "endOffset": 78}, {"referenceID": 0, "context": "Theorem 3 Let a POMDP with state spaceS = [0, 1]ds, and a possibly infinite action space be given.", "startOffset": 42, "endOffset": 48}, {"referenceID": 0, "context": "For each (s, a)-pair, let hs,a : [0, 1] I-t [0, 1] be a hash function that maps any Uniform(O, 1] random variable into another Uniform[O, 1] random variable.", "startOffset": 33, "endOffset": 39}, {"referenceID": 0, "context": "For each (s, a)-pair, let hs,a : [0, 1] I-t [0, 1] be a hash function that maps any Uniform(O, 1] random variable into another Uniform[O, 1] random variable.", "startOffset": 44, "endOffset": 50}], "year": 2011, "abstractText": "We propose a new approach to the problem of searching a space of policies for a Markov decision process (MDP) or a partially observ\u00ad able Markov decision process (POMDP), given a model. Our approach is based on the following observation: Any (PO)MDP can be transformed into an \"equivalent\" POMDP in which all state transitions (given the current state and action) are deterministic. This reduces the general problem of policy search to one in which we need only consider POMDPs with deterministic transitions. We give a natural way of estimating the value of all policies in these transformed POMDPs. Pol\u00ad icy search is then simply performed by searching for a policy with high estimated value. We also establish conditions under which our value esti\u00ad mates will be good, recovering theoretical results similar to those of Kearns, Mansour and Ng [7], but with \"sample complexity\" bounds that have only a polynomial rather than exponential depen\u00ad dence on the horizon time. Our method applies to arbitrary POMDPs, including ones with infi\u00ad nite state and action spaces. We also present empirical results for our approach on a small discrete problem, and on a complex continuous state/continuous action problem involving learn\u00ad ing to ride a bicycle.", "creator": "pdftk 1.41 - www.pdftk.com"}}}