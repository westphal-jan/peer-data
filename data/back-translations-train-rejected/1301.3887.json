{"id": "1301.3887", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Jan-2013", "title": "Value-Directed Belief State Approximation for POMDPs", "abstract": "We consider the problem belief-state monitoring for the purposes of implementing a policy for a partially-observable Markov decision process (POMDP), specifically how one might approximate the belief state. Other schemes for belief-state approximation (e.g., based on minimixing a measures such as KL-diveregence between the true and estimated state) are not necessarily appropriate for POMDPs. Instead we propose a framework for analyzing value-directed approximation schemes, where approximation quality is determined by the expected error in utility rather than by the error in the belief state itself. We propose heuristic methods for finding good projection schemes for belief state estimation - exhibiting anytime characteristics - given a POMDP value fucntion. We also describe several algorithms for constructing bounds on the error in decision quality (expected utility) associated with acting in accordance with a given belief state approximation.", "histories": [["v1", "Wed, 16 Jan 2013 15:52:18 GMT  (428kb)", "http://arxiv.org/abs/1301.3887v1", "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)"]], "COMMENTS": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI2000)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["pascal poupart", "craig boutilier"], "accepted": false, "id": "1301.3887"}, "pdf": {"name": "1301.3887.pdf", "metadata": {"source": "CRF", "title": "Value-Directed Belief State Approximation for POMDPs", "authors": ["Pascal Poupart"], "emails": ["ppoupart@cs.toronto.edu", "cebly@cs.toronto.edu"], "sections": [{"heading": null, "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "3 Error Bounds on Approximation Schemes", "text": "This year, it will be able to put itself at the top of the group."}], "references": [{"title": "Tractable inference for complex stochastic processes", "author": ["Xavier Boyen", "Daphne Koller"], "venue": "In Proceedings of the Four\u00ad teenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Acting optimally in partially ob\u00ad servable stochastic domains", "author": ["Anthony R. Cassandra", "Leslie Pack Kaelbling", "Michael L. Littman"], "venue": "In Proceedings of the Twelfth National Conference on Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Algorithms for Partially Observable Markov Decision Processes", "author": ["Hsien-Te Cheng"], "venue": "PhD thesis, University of British Columbia,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1988}, {"title": "A model for reason\u00ad ing about persistence and causation", "author": ["Thomas Dean", "Keiji Kanazawa"], "venue": "Computational Intel\u00ad ligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}, {"title": "Dynamic programming for POMDPs using a factored state representation", "author": ["Eric A. Hansen", "Zhengzhu Feng"], "venue": "In Pro\u00ad ceedings of the Fifth International Conference on AI Plan\u00ad ning Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "SPUDD: Stochastic planning using decision diagrams", "author": ["Jesse Hoey", "Robert St-Aubin", "Alan Hu", "Craig Boutilier"], "venue": "In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Inference in belief net\u00ad works: A procedural guide", "author": ["Cecil Huang", "Adnan Darwiche"], "venue": "Approximate Reasoning,", "citeRegEx": "Huang and Darwiche.,? \\Q1994\\E", "shortCiteRegEx": "Huang and Darwiche.", "year": 1994}, {"title": "CONDENSATION\u00ad conditional density propagation for visual tracking", "author": ["Michael Isard", "Andrew Blake"], "venue": "Inter\u00ad national Journal of Computer Vision,", "citeRegEx": "Isard and Blake.,? \\Q1998\\E", "shortCiteRegEx": "Isard and Blake.", "year": 1998}, {"title": "A computational scheme for reasoning in dy\u00ad namic probabilistic networks", "author": ["Uffe Kjaerulff"], "venue": "In Proceedings of the Eighth Conference on Uncertainty in AI,", "citeRegEx": "Kjaerulff.,? \\Q1992\\E", "shortCiteRegEx": "Kjaerulff.", "year": 1992}, {"title": "Using learning for ap\u00ad proximation in stochastic processes", "author": ["Daphne Koller", "Raya Fratkina"], "venue": "In Proceedings of the 15th International Conference on Machine Learning,", "citeRegEx": "Koller and Fratkina.,? \\Q1998\\E", "shortCiteRegEx": "Koller and Fratkina.", "year": 1998}, {"title": "Approximate plan\u00ad ning for factored POMDPs using belief state simplification", "author": ["David McAllester", "Satinder Singh"], "venue": "In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "McAllester and Singh.,? \\Q1999\\E", "shortCiteRegEx": "McAllester and Singh.", "year": 1999}, {"title": "A survey of partially observable Markov decision processes: Theory, models and algorithms", "author": ["George E. Monahan"], "venue": "Management Science,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1982}, {"title": "A vector-space analy\u00ad sis of value-directed belief-state approximation", "author": ["Pascal Poupart", "Craig Boutilier"], "venue": "(in prepa\u00ad ration),", "citeRegEx": "Poupart and Boutilier.,? \\Q2000\\E", "shortCiteRegEx": "Poupart and Boutilier.", "year": 2000}, {"title": "The optimal control of partially observable Markov processes over a fi\u00ad nite horizon", "author": ["Richard D. Smallwood", "Edward J. Sondik"], "venue": "Operations Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1973}, {"title": "The optimal control of partially observ\u00ad able Markov processes over the infinite horizon: Discounted costs", "author": ["Edward J. Sondik"], "venue": "Operations Research,", "citeRegEx": "Sondik.,? \\Q1978\\E", "shortCiteRegEx": "Sondik.", "year": 1978}], "referenceMentions": [], "year": 2011, "abstractText": "We consider the problem belief-state monitoring for the purposes of implementing a policy for a partially-observable Markov decision process (POMDP), specifically how one might approxi\u00ad mate the belief state. Other schemes for belief\u00ad state approximation (e.g., based on minimizing a measure such as KL-divergence between the true and estimated state) are not necessarily appropri\u00ad ate for POMDPs. Instead we propose a frame\u00ad work for analyzing value-directed approximation schemes, where approximation quality is deter\u00ad mined by the expected error in utility rather than by the error in the belief state itself. We propose heuristic methods for finding good projection schemes for belief state estimation-exhibiting anytime characteristics-given a POMDP value function. We also describe several algorithms for constructing bounds on the error in decision qual\u00ad ity (expected utility) associated with acting in ac\u00ad cordance with a given belief state approximation.", "creator": "pdftk 1.41 - www.pdftk.com"}}}