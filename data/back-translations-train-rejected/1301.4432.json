{"id": "1301.4432", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Jan-2013", "title": "Language learning from positive evidence, reconsidered: A simplicity-based approach", "abstract": "Children learn their native language by exposure to their linguistic and communicative environment, but apparently without requiring that their mistakes are corrected. Such learning from positive evidence has been viewed as raising logical problems for language acquisition. In particular, without correction, how is the child to recover from conjecturing an over-general grammar, which will be consistent with any sentence that the child hears? There have been many proposals concerning how this logical problem can be dissolved. Here, we review recent formal results showing that the learner has sufficient data to learn successfully from positive evidence, if it favours the simplest encoding of the linguistic input. Results include the ability to learn a linguistic prediction, grammaticality judgements, language production, and form-meaning mappings. The simplicity approach can also be scaled-down to analyse the ability to learn a specific linguistic constructions, and is amenable to empirical test as a framework for describing human language acquisition.", "histories": [["v1", "Fri, 18 Jan 2013 16:53:13 GMT  (464kb)", "http://arxiv.org/abs/1301.4432v1", "39 pages, pdf, 1 figure"]], "COMMENTS": "39 pages, pdf, 1 figure", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["anne s hsu", "nick chater", "paul m b vit\\'anyi"], "accepted": false, "id": "1301.4432"}, "pdf": {"name": "1301.4432.pdf", "metadata": {"source": "CRF", "title": "Running Head: Simplicity-based approach to language learning Language learning from positive evidence, reconsidered: A simplicity-based approach", "authors": ["Anne S. Hsu", "Nick Chater", "Paul Vit\u00e1nyi"], "emails": ["Nick.Chater@wbs.ac.uk", "paul.vitanyi@cwi.nl"], "sections": [{"heading": null, "text": "Children learn their mother tongue by exposing themselves to their linguistic and communicative environment, but apparently without having to correct their mistakes. Such learning of \"positive evidence\" was seen as raising the \"logical\" problems for language acquisition. In particular, if the child recovers from the presumption of an exaggerated grammar that is consistent with every sentence the child hears, there will be many suggestions on how to solve this \"logical problem.\" Here, we will review current formal results showing that the learner has enough data to successfully learn from positive evidence if he prefers the simplest coding of linguistic input. Results include the learning ability of the linguistic expression, grammar judgments, language production and the formal meaning of applications. The simplicity approach can also be \"scaled\" to analyze the learning ability of specific linguistic constructions, and is learnable."}, {"heading": "4.1 Grammaticality errors: overgeneralization", "text": "When considering grammaticality, as we have noted, it is convenient to consider speech input as a sequence of words rather than encoding it as a binary form. So, instead of dealing with distributions, binary sequences can be used to look at distributions P and P. about sequences of a finite vocabulary. Suppose the student has seen a corpus x of j-1 words and has a chance j (x) of incorrectly guessing a jten word that is randomly ungrammatic, i.e., the string cannot be completed to produce a grammatical sentence. You can write: j (x) P (x) x: xk is un grammatical, l (x) j1) j1 (9) As before, focus on the expected value j P (x) x: l (x) j1 j (x) j."}, {"heading": "4.2 Grammaticality errors: undergeneralization", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country and in which it is a country and in which it is a country and in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country,"}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "Children learn their native language by exposure to their linguistic and communicative environment, but apparently without requiring that their mistakes are corrected. Such learning from \u201cpositive evidence\u201d has been viewed as raising \u201clogical\u201d problems for language acquisition. In particular, without correction, how is the child to recover from conjecturing an over-general grammar, which will be consistent with any sentence that the child hears? There have been many proposals concerning how this \u201clogical problem\u201d can be dissolved. Here, we review recent formal results showing that the learner has sufficient data to learn successfully from positive evidence, if it favours the simplest encoding of the linguistic input. Results include the learnability of linguistic prediction, grammaticality judgments, language production, and form-meaning mappings. The simplicity approach can also be \u201cscaled-down\u201d to analyse the learnability of specific linguistic constructions, and is amenable to empirical test as a framework for describing human language acquisition. Simplicity-based approach to language learning 3 Children appear to learn language primarily by exposure to the language of others. But how is this possible? The computational challenges of inferring the structure of language from mere exposure are formidable. In light of this, many theorists have conjectured that language acquisition is only possible because the child possesses cognitive machinery that fits especially closely with the structure of natural language. This could be because the brain has adapted to language (Pinker & Bloom, 1990), or because language has been shaped by the brain Christiansen & Chater, 2007). A number of informal arguments concerning the challenge of language learning from experience have been influential. Chomsky (1980) argued that the \u201cpoverty of the stimulus\u201d available to the child was sufficiently great that the acquisition of language should be viewed as analogous to the growth of an organ, such as the lung or the heart, unfolding along channels pre-specified in the genome. Here, we focus on a specific facet of poverty of the stimulus: that children do not appear to receive or attend to \u201cnegative evidence:\" explicit feedback that certain utterances are ungrammatical (Bowerman, 1988; Brown & Hanlon, 1970; Marcus, 1993). 1 The ability to learn language in the absence of negative evidence is especially puzzling, given that linguistic rules are riddled with apparently capricious restrictions. For example, a child might naturally conclude from experience that there is a general rule that is can be contracted, as in He\u2019s taller than she is. But contractions are not always allowed, for example: *He is taller than she\u2019s. The puzzle is that, once the learner has entertained the possibility that the overgeneral rule is correct, it appears to have no way to \u201crecover\u201d from overgeneralization and recognise that restrictions should be added. This is because each contraction that it hears Simplicity-based approach to language learning 4 conforms to the overgeneral rule. Now, if the learner uses the overgeneral rule to generate language, then it will from time to time produce utterances such as *John isn't coming but Mary\u2019s. A listener\u2019s startled reaction or look of incomprehension might provide a crucial clue that the rule is overgeneral. However, such feedback is the very negative evidence that appears to be inessential to child language acquisition. Thus, if children do not require such negative evidence, how can they recover from such overgeneralisations? Various scholars argue that they cannot: Restrictions on overgeneral grammatical rules must, instead, be innately specified (e.g., Crain & Lillo-Martin, 1999). Other theorists argue that avoiding overgeneral rules poses a fundamental \u201clogical problem\u201d for language acquisition (Baker & McCarthy, 1981; Dresher & Hornstein, 1976). One way to defuse the puzzle is to challenge its premise. One possibility is that, despite appearances, children can access and use negative evidence in a subtle form. In this paper, we set aside these contentious issues (e.g., Demetras, Post & Snow, 1986; Marcus, 1993) and argue that, whether or not negative evidence is available to, or used by, the child, language can successfully be learned without it (following, for example, MacWhinney, 1993; 2004; Rohde & Plaut, 1999; Tomasello, 2004). The arguments for learnability from positive evidence presented here are part of a broader tradition of research on learnability (e.g., Angluin, 1980, 1988; Clark, & Eyraud, 2007; Feldman, 1972; Gold, 1967; Horning, 1969; Jain, Osherson, Royer & Kumar Sharma, 1999; Niyogi, 2006; Wharton, 1974). And formal learnability arguments are complementary to recent developments of language engineering systems, which have shown that it is possible to learn automatically non-trivial aspects of phonology, morphology, syntax and semantics from positive language input (Goldsmith, 2001; Klein & Manning, 2005; Steyvers, Griffiths & Simplicity-based approach to language learning 5 Tenenbaum, 2006). While such systems are still very far from being able to acquire language from mere exposure, the pace of progress suggests that a priori barriers to learning may not necessarily be insurmountable. Rather than surveying these developments, and indicating how they may be extended, here we will take a more direct approach: we focus on one major line of positive learnability results based on the \u2018simplicity principle\u2019. We begin by introducing the simplicity principle (Section 1) and considering how it can be embodied in an \u201cideal learner\u201d (Section 2). We then outline some recent formal results on how the simplicity principle can be used to learn aspects of language such as utterance prediction, grammaticality judgments, language production, and mapping between form and meaning (Sections 3-6). We then briefly describe a practical method for assessing learnability of linguistic patterns using the simplicity approach, and how this assessment can be linked with experimental data (Section 7). Overall, the contribution of the work reviewed here is to show that, under fairly mild conditions, language acquisition from sufficient amounts of positive evidence is possible; and to indicate how the simplicity-based approach can potentially provide a framework for understanding child language acquisition. 1. Ideal learning using a simplicity principle The simplicity principle has a long history in the philosophy of science and the study of perception (e.g., Mach, 1959/1886), and has been proposed as a general cognitive principle (Chater & Vit\u00e1nyi, 2002). A formal analysis of simplicity learning starts with supposing a learner (human or artificial) that is faced with a set of positive data. For language, this data is a set of observed grammatical sentences. 2 Any set of observed sentences will be consistent with an infinite number of grammars. That is, any set of sentences could have been generated by any Simplicity-based approach to language learning 6 of an infinite number of grammars. How can the learner choose among these infinite possibilities? The simplicity principle recommends that the learner prefers hypotheses which allows for the simplest encoding of the data. For language, the data will be the observed sentences, and hypotheses are grammars (or other linguistic representations), which can be viewed as a set of probabilistic rules which captures the patterns in the linguistic input to the learner. Simplicity can be measured by viewing hypotheses (here, grammars) as computer programs which encode the data (the data is generated as the output of the program). The simplicity principle thus favors the grammar that provides the shortest encoding of the data. 3 How can a grammar be viewed as a computer program for encoding linguistic input? One concrete approach involve two steps. The first step is to specifying the grammatical rules (and, crucially, probabilities of their use). This defines a probabilistic process for generating sentences; and thus defines a probability distribution over possible strings. The second step is to encode the specific sentences in the input. It is intuitively clear that the most efficient way to do this is to reserve shorter codes for probable strings; and longer codes for less probable strings. A basic result from information theory (e.g., Cover & Thomas, 2006) is that the optimal way to do this is to assign a binary code of length log21/p to a string with probability p. 4 So, intuitively, a (probabilistic) grammar provides a short encoding of the linguistic input if it can itself be specified briefly; and if it makes the sentences that are actually observed as probable as possible. There is a tension between these objectives. An \u201cover-precise\u201d grammar, which encodes exactly those sentences that have been encountered and no others will make those particular sentences highly probably; but the code for such a grammar will be long (roughly, it will consist just of a list of the \u201callowed\u201d sentences). Conversely, a very simple but Simplicity-based approach to language learning 7 overgeneral grammar (e.g., stating, roughly, that words can occur in any order with equal probability) will have a short code, but, because the space of possible allowed sentences is vast, the code for the specific sentences observed by the learner will be very long. The simplicity principle recommends finding an balance between these extremes: postulating restrictions in the grammar just when these \u201cpay off\u201d by sufficiently reducing the code length of the sentences, encoded by the grammar. As we have indicated, in general, the better the grammar captures the structure of the language, the shorter the encoded representation of the linguistic input will be. For a concrete example, let us first consider hypotheses (i.e. grammars) describing artificially simplistic language data. Suppose the observed language was the following repeating infinite string of sentences: Hi! Bye! Hi! Bye!... One hypothesis could be \u201cThe language is a sequence of \u2018Hi!\u2019 and \u2018Bye!\u2019 occurring independently, and each with .5 probability.\u201d Under this hypothesized grammar, the encoded specification of the language input will be \u201c0101...\u201d, where 0 and 1 correspond to \u2018Hi!\u2019 and \u2018Bye!\u2019 respectively. Now if the hypothesis was a more powerfully descriptive grammar such as \u201cThe language contains a single sentence \u2018Hi! Bye!\u2019, then no further code at all is required to specify the linguistic input. Now, an infinite language input is fully specified in a simple finite description\u2014and, more generally, the more precisely the grammar captures the structure of the linguistic input, the shorter the encoding of that linguistic input will be. Initially, the learner may not have sufficient data to favour the latter hypothesis; but eventually the latter \u201cgrammar\u201d will provide the simpler encoding, because it correctly captures regularities in the input. Hence, as linguistic input accumulates, the grammar which provides the simplest encoding will be updated. An ideal simplicity learner (as in the mathematical Simplicity-based approach to language learning 8 results below) will have access to all (infinite) possibly hypothetical grammars that describe its current language data input, and choose the \u201csimplest;\u2019 any real, and hence computationally limited, learner can of course only approximate this calculation to some degree. Crucially, note that the simplicity-based learner has a mechanism for avoiding overgeneral grammars, when learning from positive evidence. Although our artificial data is compatible with a random sequence of \u2018Hi!\u2019 and \u2018Bye!,\u2019 the corresponding grammar is eliminated without the need for negative evidence, but because another grammar provides a shorter encoding of the input. 5 This point applies equally to learning natural languages. Consider the case of is contraction mentioned above. Consider two possible grammars, one that allows is contraction everywhere, and one that is more restricted (allowing He\u2019s taller than she is but not *He is taller than she\u2019s). The latter \u201cgrammar\u201d will be more complex (because it involves specifying more precisely when contraction can occur); but it will encode the linguistic input more briefly, because it more accurately captures the structure of the language. Given sufficient linguistic input, the benefit of the more accurate encoding of the linguistic input will overwhelm any additional costs in encoding the grammatical rule, and the more precise rule will be favoured. Thus, it appears that an overgeneral grammar can be eliminated by applying the simplicity principle to positive data only. This intuition is encouraging but hardly definitive. Knowing that a learner can potentially eliminate a single over-general grammar does not, of course, indicate that it can successfully choose between an infinity of possible grammars, and home in on the \u201ctrue\u201d grammar, or some approximation to it. We shall see, however, that positive mathematical results along these lines are possible. Moreover, in Section 7, we shall apply the style of Simplicity-based approach to language learning 9 argument sketched above to the learnability of some specific, and much-discussed, linguistic regularities (see Hsu, Chater & Vit\u00e1nyi, 2011). 2 An\u201cideal\u201d learner Below, we consider some formal theoretical results describing what an \u201cideal\u201d learner can learn purely from exposure to an (indefinitely long) sequence of linguistic input (i.e., positive evidence) by using the simplicity principle. What is the structure of the linguistic material to be learned? Fortunately, it turns out that we need assume only that this input is generated probabilistically by some computable process. 6 This restriction is mild because cognitive science takes computability constraints on mental processes, including the generation of language, as founding assumptions (Pylyshyn, 1984) and, indeed, specific models of language structure and generation all adhere to this assumption. Finally, for mathematical convenience, and without loss of generality, we assume that the linguistic input is coded in binary form. Importantly, note that these assumptions allow that there can be any (computable) relationship between different parts of the input---we do not, for example, assume that sentences are independently sampled from a specific probability distribution. Our very mild assumption allows sentences to be highly interdependent (this is one generalization with respect to earlier results, e.g., Angluin, 1980; Jerome Feldman, 1972; Wharton, 1974), and includes the possibility that the language may be modified or switched during the input or indeed that sentences from many different languages might be interleaved. Specifically, suppose that the linguistic input, coded as a binary sequence, x, is generated by a computable probability distribution, \uf06dC(x). 7 Intuitively, we can view this as Simplicity-based approach to language learning 10 meaning that there is a computer program, C, (which might, for example, encode a grammar, as above) which receives random input y, from a stream of coin flips. When fed to C, this random input generates x as output, i.e., C(y) = x. The probability of this y is 2 -l(y) (the probability of generating any specific binary sequence of length l(y) from unbiased coin flips). Many y may generate the same x, so the probability of an output with initial segment x, \uf06dC(x), is the sum of the probabilities of such y: \uf0e5 \uf03d \uf02d \uf03d ... ) ( : ) ( 2 ) ( x y C y y l C x \uf06d (1) The distribution \uf06dC(x) is built on a simplicity principle: outputs which correspond to short programs for the computer program, C, are overwhelmingly more probable than outputs for which there are no short programs. The learner\u2019s task, then, can be viewed as approximating \uf06dC(x), given a sample x, generated from the computer program, C. So, for example, if C generated independent samples from a specific stochastic phrase structure grammar, then the learner\u2019s aim is to find a probability distribution which matches the probabilities generated by that stochastic grammar as accurately as possible. To the extent that this is possible, we might conjecture that the learner should (i) be able to predict how the corpus will continue; (ii) decide which strings are allowed by \uf06dC(x); and (iii) generate output similar to that generated by \uf06dC(x). Framing these points in terms of language acquisition, this means that, by approximating \uf06dC(x), the learner can, to some approximation, (i) predict what phoneme, word, or sentence will come next (insofar as this is predictable at all); (ii) learn to judge grammaticality; and (iii) learn to produce language, indistinguishable from that to which it has been exposed. We explore these issues in turn in Sections 3-5. Simplicity-based approach to language learning 11 How, then, can the learner approximate \uf06dC(x), given that it has exposure to just one (admittedly arbitrarily long) corpus x, and no prior knowledge of the specific computational process, C, which has generated this corpus? It turns out that we can make progress by assuming only that the learner can, in principle, entertain all and only computable hypotheses\u2014i.e., that the learner\u2019s representational resources are universal: i.e., sufficient to encode any possible computation. Elementary results in computability theory (e.g., Odifreddi, 1988) have shown that this assumption of universality is surprisingly mild, and is satisfied by very simple abstract languages (such as the lambda calculus, Barendregt, 1984) and familiar practical languages from Fortran, to C++, to Java and Scheme. We assume, then, that the brain (and our ideal learner) has at least these representational resources. We have stated that a simplicity-based learner favor simple \u201cexplanations,\u201d measured in terms of code length in some programming language. But surely the length of a program depends on the programming language used? What may be easy to write in Matlab may be difficult to write in Prolog; and vice versa. It turns out, though, that the choice of programming language affects program lengths only to a limited degree. An important result, known as the invariance theorem (Li & Vit\u00e1nyi, 1997), states that, for any two universal programming languages, the length of the shortest program for any computable object in each language is bounded by a fixed constant. A caveat is appropriate, however: \u201cinvariance\u201d up to an additive constant is sufficient for establishing mathematical results, such as those below; but choice of representation language is crucial for making learning practically feasible, as we shall note in Section 7. 8 Nonetheless, so long as we assume that the learner\u2019s coding language is universal, we can avoid having to provide a specific account of the program that the learner uses. 9 Simplicity-based approach to language learning 12 Now suppose the learner assumes only that the corpus, x, is generated by a computable process (and hence makes no assumptions that it is generated by a specific type of grammar, or indeed, any grammar at all; makes no assumption that \u201csentences\u201d are sampled independently, etc.). Then the probability of each possible x is given by the probability that this sequence will be generating from the output of a random input, y, of length l(y) (as before, by random coin flips) fed to a universal computer, U. 10 Analogous to (1), we can define this \u201cuniversal monotone distribution\u201d (Solomonoff, 1978) \uf06c(x): \uf0e5 \uf03d \uf02d \uf03d x y U y y l x ...) ( : ) ( 2 ) ( \uf06c (2) where U(y) are programs y written in the universal programming language. Thus, an ideal learner draws on its universal programming language and the simplicity principle to formulate \uf06c(x). Remarkably, it turns out that \uf06c(x) serves as a good enough approximation to \uf06dC(x) to allow the ideal learner to predict future linguistic input; and we show below that this allows the ideal learner to make grammaticality judgments, produce grammatical utterances, and map sound to meaning. What is the mysterious \uf06c(x) in more concrete terms? Roughly, it is what would result from randomly typing into a computer; feeding the resulting \u201cprograms\u201d (most of which will, of course, not even be syntactically valid, or will loop indefinitely) to the interpreter for some universal programming language (say, C++); and considering the outputs of the (small number of) valid and terminating programs. Thus, we can alter the familiar image of monkeys randomly hitting the keys on a typewriter and, supposedly, eventually generating the works of Shakespeare, to the image of monkeys typing computer programs, and generating outputs x according to \uf06c(x). The probability \uf06c(x) will depend, of course, on the length of the shortest Simplicity-based approach to language learning 13 program generating x, as short programs are overwhelmingly more likely to be chanced upon by the monkey. We shall explore the remarkable properties of \uf06c(x) shortly. But it is worth noting at the outset that \uf06c(x) is known to be uncomputable (Li & Vit\u00e1nyi, 1997), and hence must be approximated. It remains an open question how closely \uf06c(x) can be approximated and how this affects learnability results. Promisingly, computable approximations to the universal distribution can be developed into practical tools in statistics and machine learning (e.g., Rissanen, 1987; Wallace & Freeman, 1987). Related approximations will be considered briefly in Section 7 in relation to developing a methodology for assessing the learnability of specific linguistic patterns. 3. Prediction One indication of the degree to which a learner understands the patterns in the data in any domain, is its ability to predict. Thus, if the linguistic input is governed by grammatical or other principles of whatever complexity, any learner that can predict how the linguistic material will continue, arbitrarily well, must, in some sense, have learned such regularities. Prediction has been used as a measure of how far the structure of a language has been learned since Shannon (1951); and is widely used as a measure of learning in connectionist models of language processing (Christiansen & Chater, 1994, 1999; Elman, 1990) . And, as we have noted, this result for prediction will be a foundation for results concerning grammaticality judgments, language production, and form-meaning mapping, as we discuss in subsequent sections. Simplicity-based approach to language learning 14 We formulate the task of prediction as follows. At each point in a binary sequence x (encoding our linguistic input), generated by computer C, the probabilities that, given input x, that the next symbol is 0 or 1 can be written: ) ( ) 0 ( ) | 0 ( x x x C C C \uf06d \uf06d \uf06d \uf03d ; ) ( ) 1 ( ) | 1 ( x x x C C C \uf06d \uf06d \uf06d \uf03d (3) where ) | 0 ( x C \uf06d and ) | 1 ( x C \uf06d represent the probabilities that the subsequence x is followed by a 0 and 1 respectively; and ) 0 (x C \uf06d and ) 1 (x C \uf06d are the probabilities the specific sequence of x followed by 0 or 1, respectively. But the ideal learner does not have access to \uf06dC(x), but instead uses \uf06c(x) for prediction. Thus, the learner\u2019s predictions for the next item of a binary sequence that has started with x is: ) ( ) 0 ( ) | 0 ( x x x \uf06c \uf06c \uf06c \uf03d ; ) ( ) 1 ( ) | 1 ( x x x \uf06c \uf06c \uf06c \uf03d (4) A key result by Solomonoff (1978), which we call the Prediction Theorem, shows that, in a specific rigorous sense, the universal monotone distribution \uf06c, described above, is reliable for predicting any computable monotone distribution, \uf06d, with very little expected error. More specifically, the difference in these predictions is measured by the square of difference in the probabilities that \uf06d and \uf06c assign to 0 being the next symbol: \uf028 \uf029 ) | 0 ( ) | 0 ( ) ( Error x x x \uf06d \uf06c \uf02d \uf03d (5) Simplicity-based approach to language learning 15 And the expected sum-squared error for the nth item in the sequence is: sn \uf03d \uf06d(x)Error x:l (x )\uf03dn \uf02d1 \uf0e5 (x) (6) The better \uf06c predicts \uf06d, the smaller sn will be. Given this, the overall expected predictive success of the method across the entire sequence is obtained by summing the sn across all n:", "creator": "Microsoft\u00ae Word 2010"}}}