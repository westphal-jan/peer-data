{"id": "1301.4753", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2013", "title": "Pattern Matching for Self- Tuning of MapReduce Jobs", "abstract": "In this paper, we study CPU utilization time patterns of several MapReduce applications. After extracting running patterns of several applications, they are saved in a reference database to be later used to tweak system parameters to efficiently execute unknown applications in future. To achieve this goal, CPU utilization patterns of new applications are compared with the already known ones in the reference database to find/predict their most probable execution patterns. Because of different patterns lengths, the Dynamic Time Warping (DTW) is utilized for such comparison; a correlation analysis is then applied to DTWs outcomes to produce feasible similarity patterns. Three real applications (WordCount, Exim Mainlog parsing and Terasort) are used to evaluate our hypothesis in tweaking system parameters in executing similar applications. Results were very promising and showed effectiveness of our approach on pseudo-distributed MapReduce platforms.", "histories": [["v1", "Mon, 21 Jan 2013 04:57:05 GMT  (701kb)", "http://arxiv.org/abs/1301.4753v1", "7 pages, previously published as \"On Using Pattern Matching Algorithms in MapReduce Applications\" at ISPA 2011. arXiv admin note: substantial text overlap witharXiv:1112.5505"]], "COMMENTS": "7 pages, previously published as \"On Using Pattern Matching Algorithms in MapReduce Applications\" at ISPA 2011. arXiv admin note: substantial text overlap witharXiv:1112.5505", "reviews": [], "SUBJECTS": "cs.DC cs.AI cs.LG", "authors": ["nikzad babaii rizvandi", "javid taheri", "albert y zomaya"], "accepted": false, "id": "1301.4753"}, "pdf": {"name": "1301.4753.pdf", "metadata": {"source": "CRF", "title": "Pattern Matching for Self- Tuning of MapReduce Jobs", "authors": ["Nikzad Babaii Rizvandi"], "emails": [], "sections": [{"heading": null, "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "2. RELATED WORKS", "text": "Early work on analyzing / improving MapReduce performance began almost since 2009, as an approach by Zaharia et al. [4], who addressed the problem of improving Hadoop's performance for heterogeneous environments, and based on the critical assumption in Hadoop, which is based on homogeneous cluster nodes where the tasks are linear. Hadoop uses these assumptions to efficiently plan tasks and (re) execute the stragglers. This paper outlines a new shift policy to overcome these assumptions. Besides their work, there are many other approaches to improve or analyze the performance of MapReduce frameworks, particularly in shift planning [5], energy efficiency and workplace optimization."}, {"heading": "3. THEORITICAL BACKGROUND", "text": "Such a transformation is essential to determine the most suitable running properties of an application before comparing it to a reference application in a database in order to find its similar pairs. Such approaches consist of two general phases: (1) profile phase and (2) matching phase. In the profile phase, the time series patterns of multiple applications are extracted. After applying some mathematical operations to these patterns, they are stored as references in a database during the matching phase. In the matching phase, the same procedure is repeated first for an unknown / new application; and then, the time series of this application are compared with the time series of applications stored in the database, using a pattern matching algorithm to find the most similar ones."}, {"heading": "3.1 Pattern matching", "text": "One of the most important problems in data mining is the measurement of similarity between two data series. Similarity measurement algorithms have often been used in pattern matching, classification and sequence alignment in bioinformatics. Measuring similarity between two time series means finding a function:), (YXSIM, where X and Y are two time series of the same length. This function is typically called 1), (0 YXSIM, where greater values mean greater similarities. In this case 1), (YXSIM should only be used for identical rows, whereas 0), (YXSIM should not reflect any similarity at all. Towards this end, the similarity between two rows is represented by the definition of a specific distance between them, which is called \"similarity distance.\" (YXSIM should only be achieved for identical rows, while 0), (YXSIM should not reflect any similarity at all."}, {"heading": "3.1.1 Filtering and Magnitude Normalization", "text": "Therefore, in order to increase accuracy, it is almost necessary to reduce the noise of patterns before applying DTW. Among the filtering methods in the literature, we chose the low-pass Chebyshev filter because it is also widely used when matching speakers to the pre-process CPU load time series of MapReduce applications. In addition, we have normalized our time series so that their values are always between 0 and 1."}, {"heading": "3.1.2 Dynamic Time Warping (DTW)", "text": "A simple way to overcome the unevenness of the series is to re-select one row before comparing it to compare the others, but this method usually yields unacceptable results as the time series are usually not logically correctly aligned. DTW uses a nonlinear search to overcome this problem and map appropriate samples to each other. As a result,) (1tX may be aligned with) (1tY, while) 8 (2 tX to) 3 (2 tY. DTW uses the following mathematical recursive formulas to achieve similarities between two CPU usage periods],..., [21 NxxX and], [21 NxxX and], [21 MyyY, where MN:) 1 (), () xxxxxix, (), (min), (jiDjiDjiDjiD, X and], with YY elements (YY) each reflecting the (UxX distance)."}, {"heading": "3.1.3 Similarity measurement", "text": "After DTW finds the minimum distance path between two time series, which leads to the formation of a new series Y, the similarity between the series X and Y is measured by calculating the correlation coefficient between these time series as follows:) 3 () (1), (1 NiYiXi Yx N YXCORR This coefficient shows how much two series correlate or are similar. 0), (YXCORR does not indicate similarity, whereas 1), (YXCORR reflects a perfect match; for other values, the greater correlation value, the greater similarity, is considered. In our approach, 9.0), (YXCORR is assumed to be an acceptable match - this value is determined empirically. In distributed computing systems, MapReduce is known as large-area data processing or CPU-intensive database values."}, {"heading": "4. PATTERN MATCHING IN MAPREDUCE APPLICATIONS", "text": "The goal in this paper is to develop an approach for predicting the CPU usage patterns of an unknown / new application based on their similarity to already known ones in a reference database. Our approach consists of two phases: profiling and comparison.At the profiling stage, CPU usage time series of multiple MapReduce applications in the database is extracted. For each application, we generate a series of experiments with different values of four MapReduce configuration parameters on a particular platform. These parameters are: number of mappers, number of reducers, size of shared file systems and size of input file. Within the system, we try the CPU usage of each experiment from the start of the map phase to the completion of the reduction phase with time interval of one second; SysStat monitoring package in Linux is used to collect such CPU usage time series (or patterns)."}, {"heading": "5. EXPERIMENTAL RESULTS", "text": "The first two are used to build the reference database, and only the last one is used for evaluation, and our method has been implemented and evaluated, in the form of a single laptop, in which case all the data is processed in the same way as it is used on the other side of the Atlantic, and in other cases the data is processed in the same way as it is processed on the other side of the Atlantic on the other side of the Atlantic on the other side of the Atlantic on the other side of the Atlantic on the other side of the Atlantic on the other side of the Atlantic on the other side of the Atlantic."}], "references": [{"title": "Towards Understanding Cloud Performance Tradeoffs Using Statistical Workload Analysis and Replay", "author": ["Y. Chen"], "venue": "University of California at  TABLE 1. the result of similarity between Exim_manlog application as new application and WordCount and TeraSort as Reference applications for different sets of configuration parameters values  Berkeley,Technical Report No. UCB/EECS-2010-81, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards automatic optimization of MapReduce programs", "author": ["S. Babu"], "venue": "presented at the 1st ACM symposium on Cloud computing, Indianapolis, Indiana, USA, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Improving MapReduce Performance in Heterogeneous Environments", "author": ["M. Zaharia"], "venue": "8th USENIX Symposium on Operating Systems Design and Implementation (OSDI 2008), pp. 29-42, 18 December 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Job Scheduling for Multi-User MapReduce Clusters", "author": ["M. Zaharia"], "venue": "University of California at Berkeley,Technical Report No. UCB/EECS-2009-55, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "On the Energy (In)efficiency of Hadoop Clusters", "author": ["J. Leverich", "C. Kozyrakis"], "venue": "SIGOPS Oper. Syst. Rev., vol. 44, pp. 61-65, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical Workloads for Energy Efficient MapReduce", "author": ["Y. Chen"], "venue": "University of California at Berkeley,Technical Report No. UCB/EECS-2010-6, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "MapReduce optimization using regulated dynamic prioritization", "author": ["T. Sandholm", "K. Lai"], "venue": "presented at the the eleventh international joint conference on  Measurement and modeling of computer systems, Seattle, WA, USA, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Brief Announcement: Modelling MapReduce for Optimal Execution in the Cloud", "author": ["A. Wieder"], "venue": "presented at the Proceeding of the 29th ACM SIGACT- SIGOPS symposium on Principles of distributed computing, Zurich, Switzerland, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Conductor: orchestrating the clouds", "author": ["A. Wieder"], "venue": "presented at the 4th International Workshop on Large Scale Distributed Systems and Middleware, Zurich, Switzerland, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Speaker identification using dynamic time warping with stress compensation technique \" presented at the IEEE Southeastcon", "author": ["I.I. Shahin", "N. Botros"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "MapReduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Commun. ACM, vol. 51, pp. 107-113, 2008.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Jumbo: Beyond MapReduce for Workload Balancing", "author": ["S. Groot", "M. Kitsuregawa"], "venue": "presented at the 36th International Conference on Very Large Data Bases, Singapore 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Toward accurate dynamic time warping in linear time and space", "author": ["Stan Salvador", "P. Chan"], "venue": "Intell. Data Anal., vol. 11, pp. 561-580, 2007. (a) (b) Figure 4. the detailed algorithms of profiling and matching phases. (a) (b) (C) Figure 6. Samples of similarity measurement between original Exaim-Manlog and WordCount and TeraSort time series in Reference database. As can be seen for the same configuration parameters values, Exim_mainlog and WordCount are more similar than Exim_mainlog and TeraSort for the same configuration parameters.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Therefore, there is a significant benefit to application developers in understanding performance trade-offs in MapReduce-style computations in order to better utilize their computational resources [1].", "startOffset": 197, "endOffset": 200}, {"referenceID": 1, "context": "One of the major problems with direct influence on MapReduce performance is tweaking/tuning the effective configuration parameters [3] (e.", "startOffset": 131, "endOffset": 134}, {"referenceID": 2, "context": "Early works on analysing/improving MapReduce performance started almost since 2009; such as an approach by Zaharia et al [4] that addressed problem of improving the performance of Hadoop for heterogeneous environments.", "startOffset": 121, "endOffset": 124}, {"referenceID": 3, "context": "different parts of MapReduce frameworks, particularly in scheduling [5], energy efficiency [1, 6-7] and workload optimization [8].", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "different parts of MapReduce frameworks, particularly in scheduling [5], energy efficiency [1, 6-7] and workload optimization [8].", "startOffset": 91, "endOffset": 99}, {"referenceID": 4, "context": "different parts of MapReduce frameworks, particularly in scheduling [5], energy efficiency [1, 6-7] and workload optimization [8].", "startOffset": 91, "endOffset": 99}, {"referenceID": 5, "context": "different parts of MapReduce frameworks, particularly in scheduling [5], energy efficiency [1, 6-7] and workload optimization [8].", "startOffset": 91, "endOffset": 99}, {"referenceID": 6, "context": "different parts of MapReduce frameworks, particularly in scheduling [5], energy efficiency [1, 6-7] and workload optimization [8].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "A statistics-driven workload modeling was introduced in [7] to effectively evaluate design decisions in scaling, configuration and scheduling.", "startOffset": 56, "endOffset": 59}, {"referenceID": 7, "context": "A modeling method was proposed in [9] for finding the total execution time of a MapReduce application.", "startOffset": 34, "endOffset": 37}, {"referenceID": 7, "context": "Recent works in [9-10] reported a basic model for MapReduce computation utilizations.", "startOffset": 16, "endOffset": 22}, {"referenceID": 8, "context": "Recent works in [9-10] reported a basic model for MapReduce computation utilizations.", "startOffset": 16, "endOffset": 22}, {"referenceID": 9, "context": "In speaker recognition (or signature verification) applications, it has been already validated that if two voices (or signatures) are significantly similar \u2013 based on a same set of parameters as well as their combinations \u2013; then, they are most probably produced by a unique person [11].", "startOffset": 282, "endOffset": 286}, {"referenceID": 1, "context": "In distributed computing systems, MapReduce has been known as a large-scale data processing or CPU intensive job [3, 14-15].", "startOffset": 113, "endOffset": 123}, {"referenceID": 11, "context": "In distributed computing systems, MapReduce has been known as a large-scale data processing or CPU intensive job [3, 14-15].", "startOffset": 113, "endOffset": 123}, {"referenceID": 12, "context": "DTW [20], working on N 3 dimensions is computationally very expensive.", "startOffset": 4, "endOffset": 8}], "year": 2013, "abstractText": "In this paper, we study CPU utilization time patterns of several MapReduce applications. After extracting running patterns of several applications, they are saved in a reference database to be later used to tweak system parameters to efficiently execute unknown applications in future. To achieve this goal, CPU utilization patterns of new applications are compared with the already known ones in the reference database to find/predict their most probable execution patterns. Because of different patterns lengths, the Dynamic Time Warping (DTW) is utilized for such comparison; a correlation analysis is then applied to DTWs\u2019 outcomes to produce feasible similarity patterns. Three real applications (WordCount, Exim Mainlog parsing and Terasort) are used to evaluate our hypothesis in tweaking system parameters in executing similar applications. Results were very promising and showed effectiveness of our approach on pseudo-distributed MapReduce platforms", "creator": "Microsoft\u00ae Word 2010"}}}