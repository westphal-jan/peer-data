{"id": "1301.4862", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Jan-2013", "title": "Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots", "abstract": "We introduce the Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsi- cally motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy pa- rameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters. We present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: 1) learning the inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped robot, 3) an arm learning to control a fishing rod with a flexible wire. We show that 1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; 2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; 3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot.", "histories": [["v1", "Mon, 21 Jan 2013 13:26:07 GMT  (6765kb,D)", "http://arxiv.org/abs/1301.4862v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CV cs.NE cs.RO", "authors": ["adrien baranes", "pierre-yves oudeyer"], "accepted": false, "id": "1301.4862"}, "pdf": {"name": "1301.4862.pdf", "metadata": {"source": "CRF", "title": "Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots", "authors": ["Adrien Baranes", "Pierre-Yves Oudeyer"], "emails": [], "sections": [{"heading": null, "text": "We present the Self-Adaptive Goal Generation - Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsically motivated exploration mechanism that enables active learning of inverse models in high-dimensional redundant robots, enabling a robot to efficiently and actively learn distributions of parameterized motor skills / strategies that solve a corresponding distribution of parameterized tasks / objectives. The architecture makes the robot actively perform novel parameterized tasks in the task space, based on a metric of competence progressions, each triggering a low-targeted learning of motor parameters that enable the use of both learning and generalization techniques that allow infecting motor policy parameters."}, {"heading": "1 Motor Learning and Exploration of Forward", "text": "In fact, most of them are able to survive on their own by setting out in search of new ways to travel the world."}, {"heading": "1.1 Constraining the Exploration", "text": "A common way to carry out the exploration is to use a series of constraints on the guiding mechanisms and to minimize the size and / or dimensions of the spaces explored. An important source of such constraints is widespread robot building through demonstration / imitation, in which an external human demonstrator assists the robot in its learning process. This strategy prevents the robot from conducting an autonomous exploration of its space and requires an attentive demonstrator by showing it a fewbehaviors corresponding to a desired movement or goal, which it then has to reproduce. This strategy prevents the robot from carrying out an autonomous exploration of its space and requires an attentive demonstrator."}, {"heading": "1.2 Driving Autonomous Exploration", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "1.3 Driving the Exploration at a Higher Level", "text": "This year, it is more than ever before in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a, a place, a place, a place, a place, a place, a place, a, a place, a place, a place, a place, a place, a place, a, a place, a, a, a place, a, a, a, a place, a place, a, a, a place, a, a, a, a, a place, a, a, a, a, a, a place, a, a, a, a, a, a place, a place, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a"}, {"heading": "2 Competence Based Intrinsic Motivation: The", "text": "Self-Adaptive Goal Generation RIAC Architecture"}, {"heading": "2.1 Global Architecture", "text": "Consider the definition of competence-based models outlined in [74] and extract from it two different levels of active learning defined on different time scales (Fig. 1): 1. The higher level of active learning (higher time scale) takes care of the active self-generation and selfselection of goals / tasks in a parameterized task area, depending on a level of interest based on the level of competences to achieve previously generated goals (e.g. competence progress); 2. The lower level of active learning (lower time scale) considers the targeted active selection and active exploration of actions at a lower level that need to be taken in order to achieve the goals selected at a higher level, and depending on local measures of interest that are related to the development of the quality of learned reverse and / or forward models;"}, {"heading": "2.2 Model Formalization", "text": "Let us consider a robotic system that is described both in a state / context space S and in a task space Y, which is a field of parameterized tasks / goals that can be considered a field of parameterized affirmation problems."}, {"heading": "2.3 Lower Time Scale: Active Goal Directed Exploration and Learning", "text": "In SAGG-RIAC, once a high-level goal has been chosen, the goal is directed exploration and learning mechanism at the lower level can be accomplished in many ways: the architecture makes little assumptions about it, and is therefore compatible with many methods like those described below (this is why SAGG-RIAC defines an architecture that has previously learned that the system is heading toward the goal by taking low-level measures that allow such development, while advancing local practices and using inverse models that were previously learned)."}, {"heading": "2.4 Higher Time Scale: Goal Self-Generation and Self-Selection", "text": "The process of self-generation and self-selection of objectives is based on feedback defined by the concept of competence, or more precisely, the improvement of competence in specific regions (or sub-areas) of the task area in which the objectives are selected. The level of competence can be calculated at different stages of the learning process. Firstly, it can be assessed as soon as an achievement attempt towards a goal has been declared completed. Secondly, for robot constructions that are compatible with this option, competence can be calculated in low-level experiments. In the following sections, we describe these two different cases:"}, {"heading": "2.4.1 Measure of Competence for a Terminated Reaching Attempt", "text": "An attempt to achieve a goal is considered completed according to two conditions: \u2022 A timeout associated with a maximum number of iterations permitted by the low level of active learning has been exceeded. \u2022 The goal has been effectively achieved. \u2022 We introduce a competence measure for a particular attempt to achieve a goal as dependent on two measures: the similarity between the point in the task area reached by yf when the achievement attempt is completed and the actual goal yg; and respect for constraints. These conditions are represented by costs or competences, function C, defined in [\u2212 \u221e; 0], so that a higher C (yg, yf, \u03c1) the more an achievement attempt is considered efficient. From this definition, we derive a measure of competence that is directly linked to the value of C (yg, yf, \u03c1)."}, {"heading": "2.4.2 Measure of Competence During a Reaching Attempt or During Goal-Directed Optimization", "text": "If the system uses its previously learned models to achieve a target yg by using a calculated \u03c0\u03b8 by adequate local regression, or if it uses low-level targeted optimisation to optimise the best current to achieve a self-generated target yg, it will not only collect data that will enable it to measure its competence to achieve yg, but since the calculated \u03c0\u03b8 could lead to a different effect ye 6 = yg, it will also allow to collect new data to improve the inverse model and competence measure in order to achieve other goals in the locality of yours. This will allow it to use all the experiments of the robot to update the competence model over the space of parameterised targets."}, {"heading": "2.4.3 Definition of Local Competence Progress", "text": "The active goal of self-generation and self-selection is based on feedback related to the concept of competence introduced above, and more precisely on monitoring the progress of local competences."}, {"heading": "2.4.4 Goal Self-Generation Using the Measure of Interest", "text": "Using the previous description of interest, the self-generation and self-selection mechanism performs two different processes: 1. Division of the area Y in which the targets are selected into sub-spaces, according to the heuristics, which allows maximum differentiation of areas according to their level of interest. 2. Selection of the next target to be executed. Here, we use the same kind of methods as a recursive partition of space, triggering any partition once a predefined maximum number of targets gmax has been applied within the actuator space S and not to the target / task space Y, as is done in SAGG-RIAC. (This allows the simple partition of space, triggering any partition once a predefined maximum number of targets gmax has been attempted within the area. Each partition is carried out in such a way that it maximizes the difference of the rate of interest described above in the two resulting sub-areas.) This allows easy partitioning of areas of different degree of interest and, therefore, of different interest and distinction of interest."}, {"heading": "2.4.5 Reduction of the Initiation Set", "text": "In order to improve the quality of the learned inverse model, we add a heuristics inspired by two observations of infant motor research and learning; the first, proposed by Berthier et al. [17], is that infants \"attempts at attainment are often preceded by movements that either raise their hand or move their hand back to the side; and the second, noted in [85], is that infants do not try to reach for objects forever, but sometimes relax and rest their muscles. Practically, these two characteristics allow to reduce the number of attainment positions they use to reach an object, which simplifies the attainment problem by allowing them to learn a smaller number of attainment motions.Such mechanisms can be applied in robotics to motor learning of tasks to reach the arms, as well as to other skills such as locomotion or fishing, as those shown in experiments below, in order to enable the system to achieve a highly redundant space directly with the help of the attainment system."}, {"heading": "2.5 New Challenges of Unknown Limits of the Task Space", "text": "In traditional active learning methods, and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is usually designed to perform actions within a set of values within an already known interval (for example, the range of angles that can be hit by a motor, or the phases and amplitudes of CPGs that can be easily identified).In these cases, the challenge is to select which areas potentially pass the most information to the system to improve its knowledge, within this fixed range of possibilities. As previously argued, one limit to these approaches is that they become less and less efficient as the dimensionality of the control room increases. Competence-based approaches allow to address this problem when a low-dimensional task space can be identified. Nevertheless, in this case, a new problem arises when considering unlimited learning that can be extremely large, and it is generally very difficult to ask the engineer to identify it."}, {"heading": "2.6 PseudoCode", "text": "Algorithms 3 and 4 are simple alternative examples of low-threshold targeted optimization algorithms used in the experimental section, but they could be replaced by other algorithms such as PI2 \u2212 CMA [112], CMA [45], or those presented in [79].The Inefficient function can also be built in many ways and will not be described in detail in the pseudo-code (examples are then described for each experiment).Their function is to assess whether the current model was efficient enough to achieve or get closer to the agreed goal, or whether the model needs to be improved to achieve it. In the following sections, we will present two different types of experiments, the first being an achievement experiment in which a robot arm must learn its inverse kinematics to achieve the self-generated end effect positions."}, {"heading": "3 Experimental Setup 1: Learning Inverse Kine-", "text": "In this section, we propose an experiment to be performed with a robotic arm that needs to explore and learn its forward and reverse kinematics. Before discussing the details of our active exploration approach in this first experiment case, we first define the representations of the models and control paradigms involved in this experiment. Here, we focus on robotic systems whose actuators can be adjusted by positions and speeds, and limit our analysis to discrete time models. Allowing robots to adapt themselves to environmental conditions and changes in their own geometry is an important challenge of machine learning. Changes in robot geometry have a direct impact on their IK reverse kinematics, work area coordinates (where tasks are usually specified), and actuators coordinates (such as articulated position, velocity or torque used to command the robot)."}, {"heading": "3.1 Control Paradigms for Learning Inverse Kinematics", "text": "Let us formulate mathematically forward and inverse kinematic relations. We define the intrinsic coordinates (joint / actuator positions) of a manipulator as the n-dimensional vector S = \u03b1-Rn, and the position and orientation of the manipulator as the m-dimensional vector y-Rm. In relation to this formalization, the action of the robot corresponds to velocity commands parameterized by a vector \u03b8 = \u03b1-1 (y). If a redundant manipulator is considered as the momentary velocity of each of the n joints of the arm (n > m), or if m = n, solutions for the inverse relationship are generally written as y = f (\u03b1), and inverse kinematic relationship is defined as \u03b1 = f-1 (y)."}, {"heading": "3.2 Representation of Forward and Inverse Models to be Learnt", "text": "We use non-parametric models, which typically determine local models close to a current data point. By calculating a model using parametrized functions at locally limited data points, they have been suggested as useful for real-time queries and for incremental learning. Learning inverse kinematics typically deals with such limitations, and these local methods have therefore been proposed as an efficient approach for IK learning [125, 107]. In the following study, we use an incremental version of the approximately closest algorithm (ANN) [70], based on tree division in the k-mean method, to determine the environment of the current \u03b1. Even in the environments we use to introduce our contribution, we do not need an extremely robust and computerically very complex regression method (ANN). Using the pseudo-inverse model of Moore-Penrose [2] to compress the pseudo-inverse environment of the current \u03b1. Even in the environments we use to introduce our contribution, J-J-J, we do not need an extremely robust J-J-J-J J-J J-J J-J J J-J-J J-J-J J-J-J-J J-J J-J-J-J-J J-J-J-J-J J-J-J-J-J-J-J J-J-J-J J-J-J-J-J J-J-J-J-J-J J-J-J-J-J-J J-J-J-J J-J J-J-J-J-J-J-J J-J-J-J-J-J-J-J-J J-J-J-J J-J-J-J-J J-J-J-J J-J-J-J-J-J-J-J-J-J-J-J-J J-J-J-J-J J-J-J-J problems (J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J"}, {"heading": "3.3 Robotic Setup", "text": "In the following experiments, we consider a n-dimensional manipulator controlled in position and velocity (like many modern robots), updated to discrete time values. The vector \u03b1-Rn, which represents the angle of articulation, corresponds to the context / state space S and the vector y-Rm, which represents the position of the end effector of the manipulator in m-dimensions in Euclidean space Rm, corresponds to the scope Y (see Fig. 3, where n = 7 and m = 2). We evaluate how the SAGG-RIAC architecture can be used by a robot to learn how to achieve all achievable points in the environment Y with the end effector of this arm. Learning the inverse kinematics is an online process here, where n the J-J position, J-J J-Q, J-J J J-J J J-J J-J J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J J-J-J-J-J J-J-J-J J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J-J J J-J-J-J-J J J-J J-J-J-J J-J-J-J J-J-J J J J J-J-J-J-J J J-J J-J-J J J J-J-J J J J-J J J-J-J J J J-J J J-J-J J J J-J J-J J J J-J-J J-J J J-J J-J J J J J-J J-J-J J-J J J-J J J J J J J-J J J J J-J-J-J J-J-J J J J-J J J J J J J-J-J J J J J J-J-J J J-J J J J J"}, {"heading": "3.4 Evaluation of Competence", "text": "In order to clarify the main contribution of our algorithm, we do not consider any limitations \u03c1 in this experiment and focus only on achieving the target positions yg. However, it is important to note that a limitation \u03c1 has a direct impact on the low level of active learning of SAGG-RIAC and thus can have an indirect impact on the higher level. Since the use of a limitation may require a more complex, low level exploration process, a more important number of iterations at this level may be required to achieve a goal that will have an impact on the global development of the performance of the higher level of SAGG-RIAC. Here, we define the competence function C with the euclidean interval D (yg, yf) between the target position and the final reached position yf, which may be normalized by the starting distance D (ystart, yg), yf being the starting position of the end effector. This allows, for example, to give an equal competence when yf is reached the target position yf = yannyf (yf = yf), where yf = 5cm is reached."}, {"heading": "3.5 Addition of subgoals", "text": "The calculation of local competence advances in sub-regions typically requires the achievement of numerous goals. As the achievement of a goal may require several micro-actions, the attainment of competence benchmarks can take a long time. Furthermore, without affecting the learning process, we improve this mechanism by exploiting the Euclidean nature of Y: we artificially increase the number of goals by adding sub-goals along the path between the starting position and the goal on which competences are calculated. Therefore, if we consider a starting state ystart in Y and a self-generated goal yg, we define the set of l targets {y1, y2,..., yl} where yi = (i / l) \u00d7 (yg \u2212 ystart) must be achieved before we attempt to achieve the final goal yg. We also consider another way to increase the number of competence benchmarks that take into account each experimental position of the end effector as a goal with a competence value. This will typically contribute to the discovery of regions that are potentially of greater interest."}, {"heading": "3.6 Active Goal Directed Exploration and Learning", "text": "At this point, we propose a method inspired by the SSA algorithm to guide the system toward the chosen target position yg. This instance of the SAGG-RIAC architecture uses algorithm 3 and takes into account evolving contexts as explained below."}, {"heading": "3.6.1 Reaching Phase", "text": "The achievement phase deals with the creation of a path to the current target position yg. This phase consists in determining from the current position yc an optimal micro-action that would lead the end effector towards yg. For this purpose, the system calculates the shift of the required end effector towards yc \u2212 yg = v. yc \u2212 yg, where v is the speed limited by vmax and yc \u2212 yg, yc \u2212 yg, a normalized vector towards the target, and performs the action. Next year, with J +, we calculate the pseudo-inverse of the Jacobin estimate close to \u03b1 and based on the data collected so far by the robot. After each action next year, we calculate the error rate."}, {"heading": "3.6.2 Exploration Phase", "text": "This phase consists in the execution of small random exploration actions \u2206 \u03b1i around the current position \u03b1, where the variations can be derandomized as in [45]. This enables the learning system to improve its regression model of the relationship (\u03b1, \u2206 \u03b1) \u2192 \u2206 y close to \u03b1, which is required for the calculation of the inverse kinematic model around \u03b1. In both phases, an equivalent value is incremented for each micro-action and reset for each new target. The time span in which a target is defined as unachieved and an achievement attempt is stopped uses this counter. A maximum number of micro-actions is specified for each target, which is proportional to the number of micro-actions it must achieve. In the next experiments, the system is allowed to perform up to 1.5 times the distance between ystart and yg before the achievement attempt is stopped."}, {"heading": "3.7 Qualitative Results for a 15 DOF Simulated Arm", "text": "In the simulated experiment presented in this section, we consider the robot arm to be Figure 3 with 15 DOF, with each arm of the robot having the same length (taking into account a 15 DOF arm corresponds to a problem of 32 continuous dimensions, with 30 dimensions in the actuator / state space and 2 dimensions in the target / task space).We define the dimensions of task space Y as limited in intervals yg [0; 150] \u00d7 [\u2212 150; 150], with 50 units being the total length of the arm, which means that the arm comprises less than 1 / 9 of room Y in which targets can be selected (i.e. the majority of the areas in the work / task space are unreachable that must be discovered by the robot itself).We set the number of sub-targets per target to 5, and the maximum number of elements within a region before dividing to gmax = 50. We also specify the desired velocity v = 2 units / micro-action, and the number of plots per target not specified on the number of exploration data)."}, {"heading": "3.7.1 Evolution of Competences over Time", "text": "Fig 4 represents the total distribution of the self-generated objectives and sub-objectives selected by the higher level of the active learning module, and their corresponding competencies after the execution of 30,000 micro-actions. The global form of the point distribution allows us to observe the great values of the competence levels within the achievable space and its immediate surroundings, as well as the global low competencies within the remaining space.The progressive increase in competence is shown in Figure 5, where over time (here indicated by the number of self-generated objectives) we observe the global competence of the system in order to reach positions that are located on a grid covering the entire scope of assignment.From these competencies estimates, we can extract two interesting phenomena: firstly, the first two sub-figures, estimated after the self-generation of 42 and 83 objectives, show that the system is capable of reaching the positions that are close to the limits of the achievable space at the beginning of the exploration and learning process."}, {"heading": "3.7.2 Global Exploration over Time", "text": "Fig. 6 shows histograms of target positions generated during the execution of the 30,000 micro-actions themselves (targets only, not subtargets for easy readability of the figure), each subfigure corresponding to a specific time window indexed by the number of targets generated: the first (upper left) shows that the system is already concentrated in a small area around the resting position of the end effector at the beginning of the learning process, thus distinguishing between a part of the achievable area and the remaining space (the entire achievable zone is represented by the black semicircle on each subfigure of Fig. 6), while the second subfigure concentrates almost exclusively on regions of the space that cannot be reached by the arm, due to the inaccurate division of the space at this level of exploration, which undermines very small achievable areas (which have already been reached with high competence), at the boundary within each large unachievable region."}, {"heading": "3.7.3 Exploration over Time inside Reachable Areas", "text": "A more precise observation of the sub-figures 3 to 6 is presented in Fig.8, where we can observe specifically the self-created goals within the attainable range. First, we can perceive that the system initially focuses on an area around the resting of the end effector (represented by grey points in Fig.8).Then it increases the radius of its exploration by the rest and focuses on areas further away from the resting of the end effector. Subfigures 2 and 3 show 0.5 1 00.20.40.20.40.610 Goals 1 00.20.40.610 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5"}, {"heading": "3.7.4 Emergent Process", "text": "The addition of sub-targets and the consideration of the position of each end effector as a goal achieved with the highest level of competence have important influences on the learning process. If we look at traditional active learning algorithms that cannot deal with unlimited learning [30, 43, 32], as well as RIAC-like algorithms that differ from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can find that even if these techniques deal with the avoidance of excessive exploration in unlearnable or extremely complex areas, the learning process still needs to begin with a period of random exploration of the entire space in order to distinguish and identify which sub-areas are most interesting according to the definition of interest used. Thanks to the addition of sub-targets and / or the consideration of the position of each end effector in SAGG-RIAC, in addition to exploring the entire space, we reduce the number of required random global explorations that are of interest."}, {"heading": "3.7.5 Robustness in High-Volume Task Spaces", "text": "In the previous experiment, the timeout, which describes a target as not being reached and stops an attempt to reach it, was defined as being directly proportional to the number of micro-actions required to achieve each target. Practically, as illustrated in Section 3.6.2, we allowed the system to perform 1.5 times the distance between ystart and yg before declaring a target as not being reached (including exploratory movements) This timeout is efficient enough to learn efficiently by considering regions of varying complexity in the medium size S \u2032 = = [0; 150] in this experiment. Nevertheless, it can have an important impact on the SAGG learning process when considering extremely large tasks."}, {"heading": "3.7.6 Conclusion of Qualitative Results", "text": "The SAGG-RIAC algorithm is able to distinguish very efficiently attainable areas in such high-volume spaces, and is then also able to drive progressive self-generation of targets through attainable sub-spaces with increasing complexity of accessibility. In this experiment, the attainable region was convex and without obstacles in the task area. However, as we will see in the subsequent fishing experiment, SAGGRIAC is able to correctly identify its attainable areas because there is an optimization algorithm at a low level, even if there are \"holes\" or obstacles: objectives originally created in unattainable positions or in positions where obstacles prevent their attainment provide a low level of competence, and therefore the system no longer attempts to achieve them. It is also possible to imagine that some given self-generated targets can only be achieved through an action policy that goes around an obstacle. Such an ability is not represented by the SAGG-level of ownership, but rather by an increased architecture of the SAGG-RIAC itself."}, {"heading": "3.8 Quantitative Results for Experiments with Task Spaces of Different Sizes", "text": "In the following evaluation, we consider the same robotic system as previously described (15DOF arm with 50 units) and design various experiments, for each of which we estimate the efficiency of the inverse model we learned by testing how the robot, on average, achieves positions selected within a test database with 100 attainable positions (evenly distributed within the attainable range and independent of the exploration of the robot) and comparing SAGG-RIAC with three other types of exploration techniques: 1. SAGG-RANDOM, where targets are randomly selected (higher level of active learning (RIAC) hinders) 2. AKTUATOR-RANDOM, where small random micro-actions are performed. This method corresponds to classical random motorized chatter. 3. AKTUATOR-RIAC, which corresponds to the original RIAC algorithm, which uses the reduction of prediction errors (\u03b1, UK) \u2022 x to calculate a value that is equal to the respective space and actions (equilibrium, equilibrium, \u03b1)."}, {"heading": "3.8.1 Exploration in the Reachable Space", "text": "The first quantitative experiment is designed to compare the quality of the inverse models learned with babbling in the task / operating room (i.e. with targets) rather than with more traditional motor heuristics performed in the configuration / actuator room. We also consider an n = 15 DOF arm of 50 units, which is also suitable for the first study, dimensions of Y are limited at intervals yg [0; 50] \u00d7 [\u2212 50], which means that the arm can reach almost the entire space Y in which targets can be chosen (the limits of accessibility are thus almost given to the robot). In this experiment, we fix q = 20 for the SAGG methods and use a time span that is only relative to the current target (an end-effector motion of 1.5 times what is needed). Figure 10 shows the evolution of the ability of the system to achieve the 100 test targets with the inverse model that is learned by each technique."}, {"heading": "3.8.2 Robustness in Large Task Spaces", "text": "In the following experiment, we would like to test the ability of SAGG-RIAC to focus on attainable areas when faced with high-volume task areas (we will call this phenomenon discriminatory).Therefore, we consider here a task area Y = [0; 500] \u00b7 [\u2212 500; 500].Fig. 11 shows the learning efficiency of SAGG-RIAC based on the timeout with blocking criteria as described in Section 3.7.5. This makes it possible to test the quantitative aspect of SAGG-RIAC's discriminatory capacity and its comparison with the three other techniques when faced with high-volume tasks in which only small subranges are attainable. As Fig. 11 shows, SAGG-RIAC is the only method capable of advancing efficient learning in such a space. SAGG-RANDOM actually spends most of the time comparing unattainable positions. Also, the size of the task area we perform has an influence on the two algorithms ATOM with each other."}, {"heading": "3.8.3 Robustness in Very Large Task Spaces", "text": "Finally, we test the robustness of SAGG-RIAC in tasks larger than in the previous section. Fig. 12 shows the behavior of SAGG-RIAC when used with tasks of different sizes, ranging from 1 to 900 times larger than the available space, and compares these results to random research in the actuator space when the value of max is set as Y = [0; 500] \u00d7 [\u2212 500; 500]. We can note here that although the high discriminatory capacity of SAGG-RIAC in large rooms such as Y = [0; 500] \u00d7 [\u2212 500; 500], as previously shown, the performance of this technique decreases as the size of the task under consideration increases. Therefore, we can observe that SAGG-RIAC achieves better results than ACTUATOR-RANDOM since 5000 micro-actions, when rooms are smaller than Y = [0; 500] \u00d7 [\u2212 100; 500], as rooms that are clearly considered as weather as TUATOR spaces."}, {"heading": "3.9 Quantitative Results for Experiments with Arm of Different Number of DOF and Geometries", "text": "In each experiment, we define the dimensions of Y as limited by the intervals yg. [0; 150] \u00b7 [\u2212 150; 150], with 50 units being the total length of the arm, which means that the arm covers less than 1 / 9 of the Y space in which targets can be selected (i.e. the majority of the areas in the work / task space are unreachable, which must be discovered by the robot).For each experiment, we set the desired velocity v = 0.5 units / micro-action, and the number of exploration actions q = 20. In addition, we reset the arm to the resting position (\u03b1rest, yrest) all r = 2 attainment attempts, which increases the complexity of the attainment process. We present a series of experiments aimed at testing the robustness of SAGGRIAC in arm setups with different shapes and number of degrees of freedom. Tests performed use 7, 15, and 30 DOF arms whose limbs give either the same length or a relative length of the space structure, each of which we assign according to the maximum distance of the load structure."}, {"heading": "3.9.1 Quantitative Results", "text": "Fig. 13 illustrates the performance of the learned inverse models when used to achieve objectives from an independent test database and to develop together with the number of experimental microactions. Firstly, we can observe globally the slower decreasing velocity (in terms of the number of microactions) of SAGG-RANDOM and SAGG-RIAC, compared to the previous experiment, which is due to the higher value of q and the remote consideration of each end effector position. Fig. 13 show the achievement errors of 7, 15 and 30 DOF arms of decreasing length. The first figure shows that when considering 7 DOF, which is a relatively low number of degrees of freedom, SAGGRANDOM is not the second more efficient algorithm. In fact, the ACTUATORRANDOM method is more efficient here than SAGG-RANDOM experiment, the SAGG-RANDOM experiment, which is a relatively low number of degrees of freedom, reaches the second more efficient algorithm after micro-actions, and then stabilizes the SANDOM-RANDG during SAGG-RANDOM-RANDG."}, {"heading": "3.9.2 Conclusion of Quantitative Results", "text": "Global quantitative results presented here underscore the high efficiency and robustness of SAGG-RIAC when performed with highly redundant robotic assemblies of different morphologies, compared to more traditional approaches exploring in the actuator (input) space, and show that random exploration in the target (output) space can be very efficient when used in high performance areas."}, {"heading": "1 to 29 Goals", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "57 to 85 Goals", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "113 to 141 Goals", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 to 29 Goals", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "57 to 85 Goals", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "113 to 14 G als", "text": "These results therefore point to the high potential that competence-based motor learning offers for ICT learning in highly redundant robots."}, {"heading": "3.10 Qualitative Results for a Real 8 DOF Arm", "text": "In this section, we test the robustness of the algorithm in qualitative terms when considering (not simulating) a real robotic device that corresponds to the simulation shown above: We use an 8 DOF arm that is controlled in position. To test the robustness of our method, we use low-quality motors whose average noise is 20% for each movement. The specified task space corresponds to the total surface that can be observed by a camera on the top of the robot, which is more than three times larger than the available space (see the left part of Fig. 14). To allow the camera to distinguish the final effect of the arm and to create a visual reference frame on the 2D surface, we used visual tags and the ARToolKit Tracker software [81]. Fig. 14 (right part) shows histograms of self-generated targets indexed over shiftable time windows (without taking into account partial targets), we reach a limit of the space for the experiment, whereby we continue to focus on 10,000 actions in the system."}, {"heading": "4 Experimental Setup 2: Learning Omnidirec-", "text": "Motor synergies, sometimes derived from pre-wired neural structures (e.g. central pattern generators [44, 73, 49]), are defined as coherent activations (spatial or temporal) of a group of muscles, which are proposed as building blocks for the simplification of the framework of motor behaviors because they enable the reduction of the number of parameters required to represent complex movements [34, 58, 15, 120]. They are described as crucial for the development of motor skills and can be regarded as encoding an unconscious continuous control of muscles that simplifies the complexity of the learning process: learning complex tasks by means of parameterized motor synergies (such as walking or swimming) actually corresponds to the coordination of relatively low dimensions (which can, however, have several dozen dimensions), compared to the important number of degrees of freedom that need to be controlled (see [16])."}, {"heading": "4.1 Formalization", "text": "In the following two experiments, we simplify the learning process by using such parameterized motor synergies that control the amplitude, phase and speed of the central pattern generators (CPGs). Mathematically, the use of motor synergies simplifies the description of the robot system under consideration. In the framework presented above (Section 2.2), we define our system as being represented by the relationship (s, a) \u2192 y, whereby for a given configuration s-S, a sequence of actions a = {a1, a2,..., a} A allows a transition towards y-Y. In the current framework, we consider the sequence of actions to be generated directly by parameterized motor synergies, which means that the sequence of actions is directly encoded and controlled (using internal feedback to the synergy) by specifying parameters specified at the beginning of an action. In the experiment described in this section, for example, we define a synergy setting by placing a supposition (a supposition of supposition to a risk) at the beginning of an action."}, {"heading": "4.2 Robotic Setup", "text": "In the following experiment we consider a four-legged robot simulated with the Breve simulator [54] (the physics simulation is based on ODE).Each leg consists of 2 joints, the first (closest to the robot body) being controlled by two rotating DOF, and the second, a rotation (1 DOF).Each leg therefore consists of 3 DOF, where the robot as a whole has 12 DOF (see fig. 15).This robot is controlled by motor synergies, whose parameters directly indicate the phase and amplitude of each sinusoid, which controls the precise rotation value of each DOF over time. These synergies are parameterized using a set of 24 continuous values, where 12 represent the phases of each joint, and the 12 others, the amplitude at; V = {ph1,2,..,.., 12} which can control the dimensions of each DOF over the course of each other phase of each other = 12 phases of each American."}, {"heading": "4.3 Measure of competence", "text": "In this experiment, we do not consider any limitations \u03c1 and concentrate only on reaching the target positions yg = (ug, vg, \u03c6g). In each iteration, the robot is reset to the same configuration after an attempt to reach it, which is designated as the original position (see Fig. 17). We define the competence function C using the Euclidean distance target / robot position D (yg, yf) after an attempt to reach it, which is normalized by the original distance between the original position yorigin and the target D (yorigin, yg) (see Fig. 17). On this competence scale, we calculate the euclidean distance using (u, v, \u03c6), with the dimensions in [0; 1] being re-scaled. Therefore, each dimension has the same weight in estimating the competence (an angular error of \u03c6 = 12\u03c0 is just as important as an error u = 190 or v = 1 90).C (yg, yf (yf, yf (), yf (yf), yf (D \u2212 start), yf (), yf ()."}, {"heading": "4.4 Active Goal Directed Exploration and Learning", "text": "Achieving a target yg requires the estimation of a motor synergy \u03c0\u03b8i that leads to this selected state yg. Taking into account a single starting configuration for each experiment and motor synergies \u03c0\u03b8, the forward model that defines this system can be described as follows: \u03b8 \u2192 (u, v, \u03c6) (9) Here we have a direct relationship that takes into account only the 24-dimensional parameter vector \u03b8 = {ph1,2,.., 12; am1,2,.., 12} of the synergy as system inputs and a position in (u, v, \u03c6) as output. Thus, we have a fixed context and here use an instantiation of the SAGG-RIAC architecture with the local optimization algorithm Alg. 4, which is detailed below."}, {"heading": "4.4.1 Reaching Phase", "text": "The achievement phase is about the reuse of the already collected data and the use of local regression to calculate an inverse model ((u, v, \u03c6). To create such a local inverse model (there are numerous other solutions, such as [20, 8, 55, 108]), we extract the potentially more reliable data using the following method: We first extract from the learned data the quantity L of the l nearest neighbors of (ug, vg, \u03c6g) and then retrieve their corresponding motor synergies using an ANN method [70]: L = {u, v, \u03c6, \u03b8} 1, {u, v, \u03b8} 2,..., {u, v, v, Celsius, l} l} (10). Then we consider the quantity M, which contains l sets of m elements: M = {u, v, vi, \u03b8} 1,."}, {"heading": "4.4.2 Exploration Phase", "text": "If the achievement phase fails to bring the system closer to yg, i.e. D (yg, yt) > D (yg, yc), with yt as the last effective point reached in an attempt towards yg, the exploration phase is triggered. In this phase, the system first takes into account the closest neighbor yc = (uc, vc, \u03c6c) of the target (ug, vg, \u03c6g) and receives the corresponding known synergy phenomenon. Then, it adds a random noise limit (24) to the 24 parameters {ph1,2,.., 12, am1,2,.., 12} c of this synergy phenomenon, which is proportional to the euclidean distance D (yg, yc). The next synergy phenomenon (t \u2212 1 = {ph1,2,..,.., 12}.t + 1 for experimentation can thus be described using the following equation D (yg, yc) (yg, = 1,2.. 12)."}, {"heading": "4.5 Qualitative Results", "text": "Fig. 16 represents the positions studied by the quadrupeds within the task area u, v, \u03c6 after 10,000 experiments (execution of motor synergies during the same fixed period of time) using the previously introduced exploration mechanisms. However, ACTUATOR-RANDOM and ACTUATOR-RIAC select parameters of motor synergies in this experiment, whereas we do not see any significant difference - the explored space appears similar and the extent of the explored space on the (u, v) axis is composed at intervals [\u2212 5; 5] for u and [\u2212 2.5; 2.5] for v on both diagrams. Furthermore, we note that the difference between u \u2212 and v \u2212 scales is due to the inherent space of the robot, which simplifies space and simplifies space forward rather than \u2212 G."}, {"heading": "4.6 Quantitative Results", "text": "In this section, we will try to test the efficiency of the learned forward / reverse models to get the quadruped to reach a series of target positions from an independently created test database. Here, we will look at a test database with 100 objectives, which was independently created and covers the achievable part of the task almost uniformly, and calculate the distance between each target being attempted and the achieved position. Figure 18 shows the performance of the 4 previously introduced methods. First, we can observe the higher efficiency of SAGG-RIAC compared to the other three methods, which can be observed after only 1000 iterations. The high speed of reaching errors (in the number of experiments) is due to the consideration of regions limited to a limited number of elements (30 in this experiment). It allows the creation of a very high number of regions within a small time interval, which helps the system to discover and focus the achievable regions and their surroundings."}, {"heading": "4.7 Conclusion of Results for the Quadruped Experiment", "text": "These experiments first underscore the high efficiency of the methods that advance the study of motor synergies in terms of their effects in the task area. Qualitative results show that SAGG methods, especially SAGG-RIAC, allow the study of large areas that are hardly discovered by chance when the limits of accessibility are very difficult to predict. Subsequently, quantitative results demonstrated the ability of the SAGG-RANDOM and SAGG-RIAC methods to efficiently learn inverse models taking into account highly redundant robotic systems controlled by motor synergies."}, {"heading": "5 Experimental Setup 3: Learning to Control a", "text": "Fishing rod with motor synergies"}, {"heading": "5.1 Robotic Setup", "text": "This experiment consists in having a robot that learns to control a fishing rod (with a flexible wire) in order to reach certain positions of the float when it touches the water. This setup is simulated with the Breve simulator, as in the previous experiment. The rod is attached to a 4 DOF arm controlled by motor synergies that affect the speed of each joint, and is parameterized by the values \u03b8 = (v1, v2, v3, v4), vi-0-1. Specifically, for each robot experiment we use a low-programmed PID controller that tracks the desired speed of each joint vi for a fixed short time (2 seconds), starting from a fixed resting position, until the sudden stop of the movement. During the movement, as well as a few seconds thereafter, we monitor the 3D position of the float to detect a potential contact with the water (a flat level corresponding to the water level)."}, {"heading": "5.2 Qualitative Results", "text": "Fig. 20 shows histograms of the redistribution of the positions reached by the float on the surface of the water, calculated after 10,000 \"water touched\" attempts (a \"water touched\" attempt corresponds to an attempt to reach where the float effectively touches the surface), after performing ACTUATOR-RANDOM and SAGGRIAC exploration processes. The point located in the center corresponds to the base on which the arm that moves the fishing rod is located (see Fig. 19). While observing the two numbers, we can detect a redistribution of positions within a disc whose radius limits the position that will be reached if the line is maximally flat. However, the distribution of the reached (and achievable) positions within this disc is both asymetric between and between the two exploration processes. The asymmetries on each figure reflect the asymetric position of the robot (see Fig. 19)."}, {"heading": "5.3 Quantitative Results", "text": "Fig. 21 shows the mean of the achievement errors achieved with ACTUATOR-RANDOM and SAGG-RIAC, statistically calculated after 10 experiments with different random seeds. Here, the comparison of these two methods shows that SAGGRIAC yielded significantly more efficient results after 1000 successful attempts. Furthermore, after 6000 attempts, we can observe a small increase in the achievement errors of SAGGRIAC. This phenomenon is due to the discovery of new motor synergies that have led to already mastered target positions. This discovered redundancy reduces the generalization ability of the inverse model for a short period of time until these new parameters of motor synergy have been sufficiently researched to refute the inverted model (i.e. two different local inverse models are well coded and do not interfere)."}, {"heading": "6 Conclusion and Future Work", "text": "In fact, most of them will be able to survive on their own."}, {"heading": "Acknowledgment", "text": "We would like to thank everyone who gave us their feedback on this work, which was partly funded by ERC grant EXPLORERS 240007."}, {"heading": "7 Biographies", "text": "Adrien Baranes received an M.S. degree in Artificial Intelligence and Robotics from the University of Paris VI, France, in 2008 and a Ph.D. in Artificial Intelligence from the National Institute of Computer Sciences and Control (INRIA) / University of Bordeaux 1, France, in 2011. During his doctoral thesis, he studied mechanisms of development that allow the research process of robots to be limited and advanced in order to enable them to progressively acquire high quantities of knowledge and know-how in unprepared open spaces. Since January 2012, he has been studying intrinsic motivations with a biological / neurological perspective as a doctoral student at Columbia University Medical Center, New York, thanks to a Fulbright fellowship, and will continue his research thanks to an HFSP Cross-Disciplinary Fellowship.Dr. Pierre-Yves Oudeyer is a permanent researcher at the University of Inria and responsible for the FLOWERS team."}], "references": [{"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A. Ng"], "venue": "Proceedings of the 21st International Conference on Machine Learning (2004), pages 1\u20138. ACM Press, New York,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Regression and the moore-penrose pseudo inverse", "author": ["A. Albert"], "venue": "Mathematics in science and engineering. Academic Press, Inc.,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1972}, {"title": "Optimal level theories", "author": ["H.R. Arkes", "J.P. Garske"], "venue": "Psychological theories of motivation, volume 2, pages 172\u2013195.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1982}, {"title": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions", "author": ["S. Arya", "D.M. Mount", "N.S. Netanyahu", "R. Silverman", "A.Y. Wu"], "venue": "Journal of the ACM (JACM), 45(6):891\u2013923, November", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Cognitive developmental robotics: A survey", "author": ["M. Asada", "K. Hosoda", "Y. Kuniyoshi", "H. Ishiguro", "T. Inui", "Y. Yoshikawa", "M. Ogino", "C. Yoshida"], "venue": "IEEE Trans. Autonomous Mental Development, 1(1),", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Hierarchical reinforcement learning based on subgoal discovery and subpolicy specialization", "author": ["B. Bakker", "J. Schmidhuber"], "venue": "Proc. 8th Conf. on Intelligent Autonomous Systems (2004),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "What are intrinsic motivations? a biological perspective", "author": ["G. Baldassare"], "venue": "Proceeding of the IEEE ICDL-EpiRob Joint Conference,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Riac: Robust intrinsically motivated exploration and active learning", "author": ["A. Baranes", "P-Y. Oudeyer"], "venue": "IEEE Trans. on Auto. Ment. Dev., 1(3):155\u2013 169,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Intrinsically motivated goal exploration for active motor learning in robots: A case study", "author": ["A. Baranes", "P.Y. Oudeyer"], "venue": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Taipei, Taiwan,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "The interaction of maturational constraints and intrinsic motivations in active motor development", "author": ["A. Baranes", "P-Y. Oudeyer"], "venue": "Proceedings of ICDL-EpiRob 2011,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Intrinsic motivation for reinforcement learning systems", "author": ["A. Barto", "O. Simsek"], "venue": "CT New Haven, editor, Proceedings of the Thirteenth Yale Workshop on Adaptive and Learning Systems (2005),", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Intrinsically motivated learning of hierarchical collections of skills", "author": ["A Barto", "S Singh", "N. Chenatez"], "venue": "Proc. 3rd Int. Conf. Dvp. Learn., pages 112\u2013119, San Diego, CA,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Conflict, Arousal and Curiosity", "author": ["D. Berlyne"], "venue": "McGraw-Hill,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1960}, {"title": "Curiosity and exploration", "author": ["D. Berlyne"], "venue": "Science, 153(3731):25\u201333,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1966}, {"title": "Simplified and effective motor control based on muscle synergies to exploit musculoskeletal dynamics", "author": ["M. Berniker", "A. Jarc", "E. Bizzi", "M.C. Tresch"], "venue": "Proceedings of the National Academy of Sciences of the United States of America (PNAS), 106(18):7601\u20137606, May", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "The Coordination and Regulation of Movements", "author": ["N Bernstein"], "venue": "Pergamon,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1967}, {"title": "Proximodistal structure of early reaching in human infants", "author": ["N.E. Berthier", "R.K. Clifton", "D.D. McCall", "D.J. Robin"], "venue": "Exp Brain Res,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Robot programming by demonstration", "author": ["Aude Billard", "Sylvain Calinon", "Rudiger Dillmann", "Stefan Schaal"], "venue": "Handbook of Robotics,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "Information Science and Statistics. Springer,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Latent spaces for dynamic movement primitives", "author": ["S. Bitzer", "S. Vijayakumar"], "venue": "Proceedings of IEEE/RAS International Conference on Humanoid Robots,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Bringing up robot: Fundamental mechanisms for creating a self-motivated, self-organizing architecture", "author": ["D. Blank", "D. Kumar", "L. Meeden", "J. Marshall"], "venue": "Cybernetics and Systems, 36(2),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2005}, {"title": "Morphological change in machines accelerates the evolution of robust behavior", "author": ["Josh C. Bongard"], "venue": "Proceedigns of the National Academy of Sciences of the United States of America (PNAS),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "A self-organizing neural model of motor equivalent reaching and tool use by a multijoint arm", "author": ["D. Bullock", "S. Grossberg", "F.H. Guenther"], "venue": "Journal of Cognitive Neuroscience, 5(4):408\u2013435,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1993}, {"title": "Statistial learning by imitation of competing constraints in joint space and task space", "author": ["S. Calinon", "A. Billard"], "venue": "Advanced Robotics, 23(15):2059\u2013 2076,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "Body schema acquisition through active learning", "author": ["R. Cantin-Martinez", "M. Lopes", "L. Montesano"], "venue": "EEE - International Conference on Robotics and Automation (ICRA), Anchorage, Alaska, USA,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Incremental local inline gaussian mixture regression for imitation learning of multiple tasks", "author": ["T. Cederborg", "M. Li", "A. Baranes", "P-Y. Oudeyer"], "venue": "Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (2010), Taipei, Taiwan,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Interactive policy learning through confidencebased autonomy", "author": ["S. Chernova", "M. Veloso"], "venue": "J. Artificial Intelligence Research, 34:1\u201325,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Singularity-robust task-priority redundancy resolution for real-time kinematic control of robot manipulators", "author": ["S Chiaverini"], "venue": "IEEE Transactions on Robotics and Automation, 13(3):398\u2013410,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1997}, {"title": "Improving generalization with active learning", "author": ["D. Cohn", "L. Atlas", "R. Ladner"], "venue": "Mach. Learn., 15(2):201\u2013221,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1994}, {"title": "Active learning with statistical models", "author": ["David A. Cohn", "Zoubin Ghahramani", "Michael I. Jordan"], "venue": "J Artificial Intelligence Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1996}, {"title": "Creativity-Flow and the Psychology of Discovery and Invention", "author": ["M. Csikszentmihalyi"], "venue": "Harper Perennial, New York,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1996}, {"title": "Analysis of a greedy active learning strategy", "author": ["S. Dasgupta"], "venue": "Adv. Neural Inform. Process. Systems, 17,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2004}, {"title": "Convex Optimization and Euclidean Distance Geometry", "author": ["J. Dattorro"], "venue": "Meboo Publishing USA,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Combinations of muscle synergies in the construction of a natural motor behavior", "author": ["A. D\u2019Avella", "P. Saltiel", "E. Bizzi"], "venue": "Nature neuroscience,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Intrinsic Motivation and self-determination in human behavior", "author": ["E.L. Deci", "M. Ryan"], "venue": "Plenum Press, New York,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1985}, {"title": "Neural basis for rhythmic behaviour in animals", "author": ["F. Delcomyn"], "venue": "Science, 210:492\u2013498,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1980}, {"title": "Mastery motivation: Appropriate tasks for toddlers", "author": ["T.B. Dichter", "N.A. Busch", "D.E. Knauf"], "venue": "Infant Behavior and Development, 20(4):545\u2013 548,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1997}, {"title": "Made-Up Minds: A Constructivist Approach to Artificial Intelligence", "author": ["G.L. Drescher"], "venue": "MIT Press,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning and development in neural networks: The importance of starting small", "author": ["J. Elman"], "venue": "Cognition, 48:71\u201399,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1993}, {"title": "Intrinsically motivated information foraging", "author": ["I. Fasel", "A. Wilt", "N. Mafi", "C.T. Morrison"], "venue": "Proceedings of the IEEE 9th International Conference on Development and Learning (2010),", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Theory of Optimal Experiment", "author": ["V. Fedorov"], "venue": "Academic Press, Inc., New York, NY,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1972}, {"title": "The importance of starting blurry: Simulating improved basic-level category learning in infants due to weak visual acuity", "author": ["R.M. French", "M. Mermillod", "P.C. Quinn", "A. Chauvin", "D. Mareschal"], "venue": "LEA, editor, Proceedings of the 24th Annual Conference of the Cognitive Science Society (2002), pages 322\u2013327, New Jersey,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2002}, {"title": "Selective sampling using the query by committee algorithm", "author": ["Y. Freund", "H.S. Seung", "E. Shamir", "N. Tishby"], "venue": "Machine Learning, 28(2-3):133\u2013168,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1997}, {"title": "Central pattern generators for locomotion, with special reference to vertebrates", "author": ["S. Grillner", "P. Wallen"], "venue": "Annual Review of Neuroscience, 8:233\u2013 261, march", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1985}, {"title": "Completely derandomized self- adaptation in evolution strategies", "author": ["N. Hansen", "A. Ostermeier"], "venue": "Evolutionary Computation, 9(2):159\u2013195,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2001}, {"title": "Intrinsically motivated hierarchical manipulation", "author": ["S. Hart", "R. Grupen"], "venue": "Proceedings of the IEEE Conference on Robots and Automation (2008),", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2008}, {"title": "Generalization and transfer in robot control", "author": ["S. Hart", "S. Sen", "R. Grupen"], "venue": "Lund Univeristy Cognitive Studies, editor, Proc. Of the 8th International Conference On Epigenetic Robotics (2008), University of Sussex,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2008}, {"title": "Novelty and reinforcement learning in the value system of developmental robots", "author": ["X. Huang", "J. Weng"], "venue": "C. Prince, Y. Demiris, Y. Marom, H. Kozima, and C. Balkenius, editors, Proc 2nd Int. Workshop Epigenetic Robotics: Modeling Cognitive Development in Robotic Systems, volume 94, pages 47\u201355. Lund University Cognitive Studies,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2002}, {"title": "Central pattern generators for locomotion control in animals and robots: A review", "author": ["A.J. Ijspeert"], "venue": "Neural Networks, 21(4):642\u2013653,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2008}, {"title": "Task space retrieval using inverse feedback control", "author": ["N. Jetchev", "M. Toussaint"], "venue": "L. Getoor and T. Scheffer, editors, International Conference on Machine Learning (ICML-11), volume 28, pages 449\u2013456, New York, NY, USA,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["D. Jones", "M. Schonlau", "W. Welch"], "venue": "Global Optimization, 13(4):455\u2013492,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1998}, {"title": "Dopamine: Generalization and bonuses", "author": ["S. Kakade", "P. Dayan"], "venue": "Neural Networks, 15(4-6):549\u201359,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2002}, {"title": "Active learning with gaussian processes for object categorization", "author": ["A. Kapoor", "K. Grauman", "R. Urtasun", "T. Darrell"], "venue": "Proceeding of the IEEE 11th Int. Conf. Comput. Vis. (2007), Crete, Greece,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2007}, {"title": "Breve: a 3d environment for the simulation of decentralized systems and artificial life", "author": ["J. Klein"], "venue": "MIT Press, editor, Proceeding of the eighth international conference on artificial life (2003),", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2003}, {"title": "Reinforcement learning to adjust robot movements to new situations", "author": ["J. Kober", "E. Oztop", "J. Peters"], "venue": "Proceedings of Robotics: Science and Systems,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonmyopic active learning of gaussian processes: an exploration-exploitation approach", "author": ["A. Krause", "C. Guestrin"], "venue": "24th international conference on Machine learning,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2007}, {"title": "Near-optimal sensor placements in gaussian processes: Theory, efficientalgorithms and empirical studies", "author": ["A. Krause", "A. Singh", "C. Guestrin"], "venue": "Journal of Machine Learning Research, 9:235\u2013284,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2008}, {"title": "Neuromotor synergies as a basis for coordinated intentional action", "author": ["W.A. Lee"], "venue": "J. Mot. Behav., 16:135\u2013170,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1984}, {"title": "A computational model of social-learning mechanisms", "author": ["M. Lopes", "F. Melo", "B. Kenward", "J. Santos-Victor"], "venue": "Adaptive Behavior, 467(17),", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2009}, {"title": "Abstraction levels for robotic imitation: Overview and computational approaches", "author": ["M. Lopes", "f. Melo", "J. Santos-Victor"], "venue": "In From Motor Learning to Interaction Learning in Robots. SpringerLink,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2010}, {"title": "Active learning for reward estimation in inverse reinforcement learning", "author": ["M. Lopes", "F.S. Melo", "L. Montesano"], "venue": "European Conference on Machine Learning (ECML/PKDD), Bled, Slovenia,", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2009}, {"title": "Active learning and intrinsically motivated exploration in robots: Advances and challenges (guest editorial)", "author": ["M. Lopes", "P-Y. Oudeyer"], "venue": "IEEE Transactions on Autonomous Mental Development, 2(2):65\u201369,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "A developmental roadmap for learning by imitation in robots", "author": ["M. Lopes", "J. Santos-Victor"], "venue": "IEEE Transactions in Systems Man and Cybernetic, Part B: Cybernetics, 37(2),", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2007}, {"title": "Artificial curiosity with planning for autonomous perceptual and cognitive development", "author": ["M. Luciw", "V. Graziano", "M. Ring", "J. Schmidhuber"], "venue": "Proceeding of the First IEEE ICDL-EpiRob Joint Conference (2011),", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2011}, {"title": "An emergent framework for selfmotivation in developmental robotics", "author": ["J. Marshall", "D. Blank", "L. Meeden"], "venue": "Proc. 3rd Int. Conf. Development Learn. (2004), pages 104\u2013111, San Diego, CA,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2004}, {"title": "On the influence of sensor morphology on eye motion coordination", "author": ["Harold Martinez", "Max Lungarella", "Rolf Pfeifer"], "venue": "In Proc. of the IEEE 9th International Conference on Development and Learning", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2010}, {"title": "A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot. Autonomous Robots ", "author": ["R. Martinez-Cantin", "N. de Freitas", "E. Brochu", "J. Castellanos", "A. Doucet"], "venue": "Special Issue on Robot Learning, Part B,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2009}, {"title": "Motivated learning from interesting events: Adaptive, multitask learning agents for complex environments. Adaptive Behavior - Animals, Animats, Software Agents, Robots", "author": ["Kathryn Merrick", "Mary Lou Maher"], "venue": "Adaptive Systems,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2009}, {"title": "Autonomously learning an action hierarchy using a learned qualitative state representation", "author": ["J. Mugan", "B. Kuipers"], "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (2009),", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast approximate nearest neighbors with automatic algorithm", "author": ["M. Muja", "D.G. Lowe"], "venue": "International Conference on Computer Vision Theory and Applications (2009),", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2009}, {"title": "Bootstrapping intrinsically motivated learning with human demonstrations", "author": ["M. Nguyen", "A. Baranes", "P-Y. Oudeyer"], "venue": "proceedings of the IEEE International Conference on Development and Learning, Frankfurt, Germany,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2011}, {"title": "Model learning for robot control: A survey", "author": ["D. Nguyen-Tuong", "J. Peters"], "venue": "Cognitive Processing, 12(4):319\u2013340,", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2011}, {"title": "Mathematical models for the swimming pattern of a lamprey", "author": ["J. Nishii", "Y. Uno", "R. Suzuki"], "venue": "Biological Cybernetics, volume 72. Springer,", "citeRegEx": "73", "shortCiteRegEx": null, "year": 1994}, {"title": "How can we define intrinsic motivations ? In Proc", "author": ["P. Oudeyer", "F. Kaplan"], "venue": "Of the 8th Conf. On Epigenetic Robotics (2008),", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2008}, {"title": "The playground experiment: Task-independent development of a curious robot", "author": ["P. Oudeyer", "F. Kaplan", "V. Hafner", "A. Whyte"], "venue": "Proceedings of the AAAI Spring Symposium on Developmental Robotics, pages 42\u201347,", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2005}, {"title": "What is intrinsic motivation? a typology of computational approaches", "author": ["P.-Y. Oudeyer", "F. Kaplan"], "venue": "Frontiers of Neurorobotics, page 1:6,", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2007}, {"title": "Intrinsic motivation systems for autonomous mental development", "author": ["P-Y. Oudeyer", "F. Kaplan", "V. Hafner"], "venue": "IEEE Transactions on Evolutionary Computation, 11(2):pp. 265\u2013286,", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2007}, {"title": "Natural actor critic", "author": ["J. Peters", "S. Schaal"], "venue": "Neurocomputing, (7-9):1180\u2013 1190,", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2008}, {"title": "reinforcement learning of motor skills with policy gradients", "author": ["J. Peters", "S. Schaal"], "venue": "(4):682\u201397,", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2008}, {"title": "Reinforcement learning for humanoid robotics", "author": ["J. Peters", "S. Vijayakumar", "S. Schaal"], "venue": "Third IEEE-RAS International Conference on Humanoid Robots (2003),", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2003}, {"title": "Artoolkit user manual, version 2.33", "author": ["I. Poupyrev", "H. Kato", "M. Billinghurst"], "venue": "Technical report, University of Washington,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2000}, {"title": "Mastery motivation in infants and toddlers: Is it greatest when tasks are moderately challenging", "author": ["R.E. Redding", "G.A. Morgan", "R.J. Harmon"], "venue": "Infant Behavior and Development,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 1988}, {"title": "The short-latency dopamine signal: A role in discovering novel actions? Nat", "author": ["P. Redgrave", "K. Gurney"], "venue": "Rev. Neurosci., 7(12):967\u201375, Nov", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2006}, {"title": "Reinforcement learning for robot soccer", "author": ["M. Riedmiller", "T. Gabel", "R. Hafner", "S. Lange"], "venue": "Autonomous Robot, 27:55\u201373,", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2009}, {"title": "Goal babbling permits direct learning of inverse kinematics", "author": ["M. Rolf", "J. Steil", "M. Gienger"], "venue": "IEEE Trans. Autonomous Mental Development, 2(3):216\u2013229, 09/2010", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2010}, {"title": "Online goal babbling for rapid bootstrapping of inverse models in high dimensions", "author": ["M. Rolf", "J. Steil", "M. Gienger"], "venue": "Proceeding of the IEEE ICDL-EpiRob Joint Conference (2011),", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2011}, {"title": "Neonatal finger and arm movements as determined by a social and an object context", "author": ["L. Ronnquist", "C. von Hofsten"], "venue": "Early Develop. Parent.,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 1994}, {"title": "An integrated system for teaching new visually grounded words to a robot for non-expert users using a mobile device", "author": ["P. Rouanet", "P-Y. Oudeyer", "D. Filliat"], "venue": "Proceedings of IEEE-RAS International Conference on Humanoid Robots (HUMANOIDS 2010), Paris, France,", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards optimal active learning through sampling estimation of error reduction", "author": ["N. Roy", "A. McCallum"], "venue": "Proc. 18th Int. Conf. Mach. Learn. (2001), volume 1, pages 143\u2013160,", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2001}, {"title": "Intrinsic and extrinsic motivations: Classic definitions and new directions", "author": ["Richard M. Ryan", "Edward L. Deci"], "venue": "Contemporary Educational Psychology,", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2000}, {"title": "Learning forward models for the operational space control of redundant robots", "author": ["C. Salaun", "V. Padois", "O. Sigaud"], "venue": "From Motor Learning to Interaction Learning in Robots, volume 264, pages 169\u2013192. Springer,", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2010}, {"title": "robot juggling: an implementation of memory-based learning", "author": ["S. Schaal", "C.G. Atkeson"], "venue": "Control systems magazine, pages 57\u201371,", "citeRegEx": "92", "shortCiteRegEx": null, "year": 1994}, {"title": "Active learning for logistic regression: An evaluation", "author": ["A. Schein", "L.H. Ungar"], "venue": "Machine Learning, 68:235\u2013265,", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2007}, {"title": "Evolution and learning in an intrinsically motivated reinforcement learning robot", "author": ["M. Schembri", "M. Mirolli", "Baldassarre G"], "venue": "In Springer, editor, Proceedings of the 9th European Conference on Artificial Life", "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2007}, {"title": "Evolving childhood\u2019s length and learning parameters in an intrinsically motivated reinforcement learning robot", "author": ["M. Schembri", "M. Mirolli", "Baldassarre G"], "venue": "In Proceedings of the Seventh International Conference on Epigenetic Robotics. Lund University Cognitive Studies,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2007}, {"title": "Curious model-building control systems", "author": ["J. Schmidhuber"], "venue": "Proc. Int. Joint Conf. Neural Netw. (1991), volume 2, pages 1458\u20131463,", "citeRegEx": "96", "shortCiteRegEx": null, "year": 1991}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "author": ["J. Schmidhuber"], "venue": "J. A. Meyer and S. W. Wilson, editors, Proc. SAB\u201991, pages 222\u2013227,", "citeRegEx": "97", "shortCiteRegEx": null, "year": 1991}, {"title": "Artificial curiosity based on discovering novel algorithmic predictability through coevolution", "author": ["J. Schmidhuber"], "venue": "P. Angeline, Z. Michalewicz, M. Schoenauer, X. Yao, and Z. Zalzala, editors, Congress on Evolutionary Computation, pages 1612\u20131618, Piscataway, NJ,", "citeRegEx": "98", "shortCiteRegEx": null, "year": 1999}, {"title": "Exploring the Predictable, pages 579\u2013612", "author": ["J. Schmidhuber"], "venue": "Springer,", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2002}, {"title": "Optimal artificial curiosity, developmental robotics, creativity, music, and the fine arts", "author": ["J. Schmidhuber"], "venue": "Connection Science, 18(2),", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2006}, {"title": "Formal theory of creativity, fun, and intrinsic motivation", "author": ["J. Schmidhuber"], "venue": "IEEE Transaction on Autonomous Mental Development, 2(3):230\u2013 247,", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2010}, {"title": "Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem", "author": ["J. Schmidhuber"], "venue": "Report arXiv:1112.5309,", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2011}, {"title": "Less is more: Active learning with support vector machines", "author": ["G. Schohn", "D. Cohn"], "venue": "Proceedings of the Seventeenth International Conference on Machine Learning,", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2000}, {"title": "A neural substrate of prediction and reward", "author": ["W. Schultz", "P. Dayan", "P. Montague"], "venue": "Science, 275:1593\u20131599,", "citeRegEx": "104", "shortCiteRegEx": null, "year": 1997}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "CS Tech. Rep. 1648, Univ. Wisconsin-Madison, Madison, WI,", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2009}, {"title": "On-line regression algorithms for learning mechanical models of robots: a survey", "author": ["O. Sigaud", "C. Salaun", "V. Padois"], "venue": "Robotics and Autonomous System, 59(12):1115\u20131129, July", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning parameterized skills", "author": ["B. Castro Da Silva", "G. Konidaris", "A. Barto"], "venue": "In Proceedings of International Conference of Machine Learning,", "citeRegEx": "108", "shortCiteRegEx": "108", "year": 2012}, {"title": "An intrinsic reward mechanism for efficient exploration", "author": ["\u00d6. \u015eim\u015fek", "A.G. Barto"], "venue": "Proceedings of the Twenty-Third International Conference on Machine Learning (2006),", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2006}, {"title": "Instrinsically motivated reinforcement learning: An evolutionary perspective", "author": ["S. Singh", "R.L. Lewis", "A.G. Barto", "J. Sorg"], "venue": "IEEE Transactions on Autonomous Mental Development (IEEE TAMD), 2(2):70\u201382,", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2010}, {"title": "Competence progress intrinsic motivation", "author": ["A. Stout", "A Barto"], "venue": "Proceedings of the International Conference on Development and Learning (2010),", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2010}, {"title": "Path integral policy improvement with covariance matrix adaptation", "author": ["F. Stulp", "O. Sigaud"], "venue": "Proceedings of International Conference of Machine Learning,", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning motion primitive goals for robust manipulation", "author": ["F. Stulp", "E. Theodorou", "M. Kalakrishnan", "P. Pastor", "L. Righetti", "S. Schaal"], "venue": "Int. Conference on Intelligent Robots and Systems (IROS),,", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2011}, {"title": "Integrated architectures for learning, planning, and reacting based on approximating integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "author": ["R.S. Sutton"], "venue": "Proceedings of the International Machine Learning Conference, pages 212\u2013218,", "citeRegEx": "115", "shortCiteRegEx": null, "year": 1990}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial Intelligence, 1123(181-211),", "citeRegEx": "116", "shortCiteRegEx": null, "year": 1999}, {"title": "Reinforcement learning for optimal control of arm movements", "author": ["E Theodorou", "J Peters", "S. Schaal"], "venue": "Abstracts of the 37st meeting of the society of neuroscience (2007),", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2007}, {"title": "Exploration in active learning", "author": ["S. Thrun"], "venue": "M Arbib, editor, Handbook of Brain Science and Neural Networks, Cambridge, MA,", "citeRegEx": "118", "shortCiteRegEx": null, "year": 1995}, {"title": "Active exploration in dynamic environments", "author": ["S. Thrun", "K. Moller"], "venue": "Proceedings of Advances of Neural Information Processing Systems,", "citeRegEx": "119", "shortCiteRegEx": null, "year": 1992}, {"title": "Neuromechanics of muscle synergies for posture and movement", "author": ["L. Ting", "J. McKay"], "venue": "Curr. Opin. Neubiol., 7:622\u2013628,", "citeRegEx": "120", "shortCiteRegEx": null, "year": 2007}, {"title": "Probabilistic inference for solving discrete and continuous state markov decision processes", "author": ["Marc Toussaint", "Amos Storkey"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "121", "shortCiteRegEx": "121", "year": 2006}, {"title": "The role of developmental limitations of sensory input on sensory/perceptual organization", "author": ["G. Turkewitz", "P.A. Kenny"], "venue": "J Dev Behav. Pediatr., 6(5):302\u20136,", "citeRegEx": "122", "shortCiteRegEx": null, "year": 1985}, {"title": "Keeping the arm in the limelight: Advanced visual control of arm movements in neonates", "author": ["A. van der Meer"], "venue": "Eur. J. Paediatric Neurol,", "citeRegEx": "123", "shortCiteRegEx": "123", "year": 1997}, {"title": "The functional significance of arm movements in neonates", "author": ["A. van der Meer", "F. van der Weel", "D. Lee"], "venue": null, "citeRegEx": "124", "shortCiteRegEx": "124", "year": 1995}, {"title": "Incremental online learning in high dimensions", "author": ["S. Vijayakumar", "A. D\u2019Souza", "S. Schaal"], "venue": "Neural Computation,", "citeRegEx": "125", "shortCiteRegEx": "125", "year": 2005}, {"title": "An action perspective on motor an action perspective on motor development", "author": ["C. von Hofsen"], "venue": "TRENDS in Cognitive Science,", "citeRegEx": "126", "shortCiteRegEx": "126", "year": 2004}, {"title": "Mind and society: The development of higher mental processes", "author": ["L.S. Vygotsky"], "venue": "Cambridge, MA: Harvard University Press,", "citeRegEx": "127", "shortCiteRegEx": null, "year": 1978}, {"title": "Developmental robotics: Theory and experiments", "author": ["J. Weng"], "venue": "Int. J. Humanoid Robotics, 1(2):199\u2013236,", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2004}, {"title": "Autonomous mental development by robots and animals", "author": ["J. Weng", "J. McClelland", "A. Pentland", "O. Sporns", "I. Stockman", "M. Sur", "E. Thelen"], "venue": "Science, 291(599-600),", "citeRegEx": "129", "shortCiteRegEx": null, "year": 2001}, {"title": "Motivation reconsidered: The concept of competence", "author": ["R. White"], "venue": "Psychol. Rev., 66:297\u2013333,", "citeRegEx": "130", "shortCiteRegEx": null, "year": 1959}, {"title": "A study of cooperative mechanisms for faster reinfocement learning", "author": ["S. Whitehead"], "venue": "Technical Report 365, Univ. Rochester, Rochester, NY, 1991. 7 Biographies Adrien Baranes received the M.S. degree in artificial intelligence and robotics from the University Paris VI, France, in 2008 and the a Ph.D. degree in artificial intelligence from the French National Institute of Computer Sciences and Control (INRIA)/University Bordeaux 1, France, in", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 105, "context": "Advanced statistical learning techniques dedicated to incremental high-dimensional regression have been elaborated recently, such as [107, 72].", "startOffset": 133, "endOffset": 142}, {"referenceID": 71, "context": "Advanced statistical learning techniques dedicated to incremental high-dimensional regression have been elaborated recently, such as [107, 72].", "startOffset": 133, "endOffset": 142}, {"referenceID": 0, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 62, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 17, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 23, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 58, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 25, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 59, "context": "Social guidance is an important source of such constraints, widely studied in robot learning by demonstration/imitation where an external human demonstrator assists the robot in its learning process [1, 63, 18, 24, 59, 26, 60].", "startOffset": 199, "endOffset": 226}, {"referenceID": 83, "context": "This is typically what happens in the reinforcement learning (RL) framework where no demonstration is originally required and only a goal has to be fixed (as a reward) by the engineer who conceives the system [114, 84, 117].", "startOffset": 209, "endOffset": 223}, {"referenceID": 114, "context": "This is typically what happens in the reinforcement learning (RL) framework where no demonstration is originally required and only a goal has to be fixed (as a reward) by the engineer who conceives the system [114, 84, 117].", "startOffset": 209, "endOffset": 223}, {"referenceID": 79, "context": "For instance, studies presented in [80] combine RL with the framework of learning by demonstration.", "startOffset": 35, "endOffset": 39}, {"referenceID": 91, "context": "The Shifting Setpoint Algorithm (SSA) introduced by Schaal and Atkeson [92] proposes another way to constrain the exploration process.", "startOffset": 71, "endOffset": 75}, {"referenceID": 126, "context": "Nevertheless, in a framework where one would like a robot to learn a variety of tasks inside unprepared spaces like in developmental robotics [129, 77, 128, 5], or more simply full inverse models (i.", "startOffset": 142, "endOffset": 159}, {"referenceID": 76, "context": "Nevertheless, in a framework where one would like a robot to learn a variety of tasks inside unprepared spaces like in developmental robotics [129, 77, 128, 5], or more simply full inverse models (i.", "startOffset": 142, "endOffset": 159}, {"referenceID": 125, "context": "Nevertheless, in a framework where one would like a robot to learn a variety of tasks inside unprepared spaces like in developmental robotics [129, 77, 128, 5], or more simply full inverse models (i.", "startOffset": 142, "endOffset": 159}, {"referenceID": 4, "context": "Nevertheless, in a framework where one would like a robot to learn a variety of tasks inside unprepared spaces like in developmental robotics [129, 77, 128, 5], or more simply full inverse models (i.", "startOffset": 142, "endOffset": 159}, {"referenceID": 40, "context": "2 Driving Autonomous Exploration Active learning algorithms can be considered as organized and constrained selfexploration processes [41, 30, 89, 105, 60].", "startOffset": 133, "endOffset": 154}, {"referenceID": 29, "context": "2 Driving Autonomous Exploration Active learning algorithms can be considered as organized and constrained selfexploration processes [41, 30, 89, 105, 60].", "startOffset": 133, "endOffset": 154}, {"referenceID": 88, "context": "2 Driving Autonomous Exploration Active learning algorithms can be considered as organized and constrained selfexploration processes [41, 30, 89, 105, 60].", "startOffset": 133, "endOffset": 154}, {"referenceID": 104, "context": "2 Driving Autonomous Exploration Active learning algorithms can be considered as organized and constrained selfexploration processes [41, 30, 89, 105, 60].", "startOffset": 133, "endOffset": 154}, {"referenceID": 59, "context": "2 Driving Autonomous Exploration Active learning algorithms can be considered as organized and constrained selfexploration processes [41, 30, 89, 105, 60].", "startOffset": 133, "endOffset": 154}, {"referenceID": 115, "context": "A large diversity of criteria can be used to evaluate the utility of given sampling candidates, such as the maximization of prediction errors [118], the local density of already queried points [131], the maximization of the decrease of global model variance [30], expected improve-", "startOffset": 142, "endOffset": 147}, {"referenceID": 128, "context": "A large diversity of criteria can be used to evaluate the utility of given sampling candidates, such as the maximization of prediction errors [118], the local density of already queried points [131], the maximization of the decrease of global model variance [30], expected improve-", "startOffset": 193, "endOffset": 198}, {"referenceID": 29, "context": "A large diversity of criteria can be used to evaluate the utility of given sampling candidates, such as the maximization of prediction errors [118], the local density of already queried points [131], the maximization of the decrease of global model variance [30], expected improve-", "startOffset": 258, "endOffset": 262}, {"referenceID": 50, "context": "ment [51], or maximal uncertainty of the model [119] among others.", "startOffset": 5, "endOffset": 9}, {"referenceID": 116, "context": "ment [51], or maximal uncertainty of the model [119] among others.", "startOffset": 47, "endOffset": 52}, {"referenceID": 92, "context": "logistic regression [93], support vector machines [103], gaussian processes [53, 56, 57].", "startOffset": 20, "endOffset": 24}, {"referenceID": 102, "context": "logistic regression [93], support vector machines [103], gaussian processes [53, 56, 57].", "startOffset": 50, "endOffset": 55}, {"referenceID": 52, "context": "logistic regression [93], support vector machines [103], gaussian processes [53, 56, 57].", "startOffset": 76, "endOffset": 88}, {"referenceID": 55, "context": "logistic regression [93], support vector machines [103], gaussian processes [53, 56, 57].", "startOffset": 76, "endOffset": 88}, {"referenceID": 56, "context": "logistic regression [93], support vector machines [103], gaussian processes [53, 56, 57].", "startOffset": 76, "endOffset": 88}, {"referenceID": 66, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 126, "endOffset": 135}, {"referenceID": 115, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 126, "endOffset": 135}, {"referenceID": 11, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 160, "endOffset": 164}, {"referenceID": 24, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 187, "endOffset": 191}, {"referenceID": 60, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 203, "endOffset": 211}, {"referenceID": 26, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 203, "endOffset": 211}, {"referenceID": 76, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 256, "endOffset": 260}, {"referenceID": 45, "context": "Nevertheless examples that consider robotic problems already exist for a large variety of problems: building environment maps [67, 118], reinforcement learning [12], body schema learning [25], imitation [61, 27], exploration of objects and body properties [77], manipulation [46], among many others.", "startOffset": 275, "endOffset": 279}, {"referenceID": 112, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 72, "endOffset": 90}, {"referenceID": 95, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 72, "endOffset": 90}, {"referenceID": 11, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 72, "endOffset": 90}, {"referenceID": 99, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 72, "endOffset": 90}, {"referenceID": 47, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 133, "endOffset": 153}, {"referenceID": 20, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 133, "endOffset": 153}, {"referenceID": 74, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 133, "endOffset": 153}, {"referenceID": 76, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 133, "endOffset": 153}, {"referenceID": 94, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 133, "endOffset": 153}, {"referenceID": 127, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 404, "endOffset": 417}, {"referenceID": 34, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 404, "endOffset": 417}, {"referenceID": 12, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 404, "endOffset": 417}, {"referenceID": 103, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 482, "endOffset": 495}, {"referenceID": 51, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 482, "endOffset": 495}, {"referenceID": 82, "context": "Two communities of researchers, the first one in reinforcement learning [115, 96, 12, 100], the second one in developmental robotics [48, 21, 75, 77, 95], formalized, implemented and experimented several mechanisms based on the concept of intrinsic motivation (sometimes called curiosity-driven learning) grounded into theories of motivation, spontaneous exploration, free play and development in humans [130, 35, 13] as well as in recent findings in the neuroscience of motivation [104, 52, 83].", "startOffset": 482, "endOffset": 495}, {"referenceID": 95, "context": "As argumented in [96, 12, 8, 62], architectures based on intrinsically motivated learning can be conceptualized as active learning mechanisms which, in addition to allowing for the self-organized formation of behavioral and developmental complexity, can also also allow an agent to efficiently learn a model of the world by parsimoniously designing its own experiments/queries.", "startOffset": 17, "endOffset": 32}, {"referenceID": 11, "context": "As argumented in [96, 12, 8, 62], architectures based on intrinsically motivated learning can be conceptualized as active learning mechanisms which, in addition to allowing for the self-organized formation of behavioral and developmental complexity, can also also allow an agent to efficiently learn a model of the world by parsimoniously designing its own experiments/queries.", "startOffset": 17, "endOffset": 32}, {"referenceID": 7, "context": "As argumented in [96, 12, 8, 62], architectures based on intrinsically motivated learning can be conceptualized as active learning mechanisms which, in addition to allowing for the self-organized formation of behavioral and developmental complexity, can also also allow an agent to efficiently learn a model of the world by parsimoniously designing its own experiments/queries.", "startOffset": 17, "endOffset": 32}, {"referenceID": 61, "context": "As argumented in [96, 12, 8, 62], architectures based on intrinsically motivated learning can be conceptualized as active learning mechanisms which, in addition to allowing for the self-organized formation of behavioral and developmental complexity, can also also allow an agent to efficiently learn a model of the world by parsimoniously designing its own experiments/queries.", "startOffset": 17, "endOffset": 32}, {"referenceID": 99, "context": "Thus, different authors claimed that typical criteria used in traditional active learning approaches, such as the search for maximal uncertainty or prediction errors, might get trapped or become inefficient in situations that are common in open-ended robotic environments [100, 77, 8, 101].", "startOffset": 272, "endOffset": 289}, {"referenceID": 76, "context": "Thus, different authors claimed that typical criteria used in traditional active learning approaches, such as the search for maximal uncertainty or prediction errors, might get trapped or become inefficient in situations that are common in open-ended robotic environments [100, 77, 8, 101].", "startOffset": 272, "endOffset": 289}, {"referenceID": 7, "context": "Thus, different authors claimed that typical criteria used in traditional active learning approaches, such as the search for maximal uncertainty or prediction errors, might get trapped or become inefficient in situations that are common in open-ended robotic environments [100, 77, 8, 101].", "startOffset": 272, "endOffset": 289}, {"referenceID": 100, "context": "Thus, different authors claimed that typical criteria used in traditional active learning approaches, such as the search for maximal uncertainty or prediction errors, might get trapped or become inefficient in situations that are common in open-ended robotic environments [100, 77, 8, 101].", "startOffset": 272, "endOffset": 289}, {"referenceID": 89, "context": "This is the reason why new active learning heuristics have been proposed in developmental robotics, such as those based on the psychological concept of intrinsic motivations [90, 35, 14] which relate to mechanisms that drive a learning agent to perform different activities for their own sake, without requiring any", "startOffset": 174, "endOffset": 186}, {"referenceID": 34, "context": "This is the reason why new active learning heuristics have been proposed in developmental robotics, such as those based on the psychological concept of intrinsic motivations [90, 35, 14] which relate to mechanisms that drive a learning agent to perform different activities for their own sake, without requiring any", "startOffset": 174, "endOffset": 186}, {"referenceID": 13, "context": "This is the reason why new active learning heuristics have been proposed in developmental robotics, such as those based on the psychological concept of intrinsic motivations [90, 35, 14] which relate to mechanisms that drive a learning agent to perform different activities for their own sake, without requiring any", "startOffset": 174, "endOffset": 186}, {"referenceID": 95, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 11, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 107, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 108, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 64, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 67, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 93, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 96, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 47, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 6, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 75, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 63, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 39, "context": "external reward [96, 12, 109, 110, 65, 68, 94, 97, 48, 7, 76, 64, 40].", "startOffset": 16, "endOffset": 69}, {"referenceID": 95, "context": "Different criteria were elaborated, such as the search for maximal reduction in empirically evaluated prediction error, maximal compression progress, or maximal competence progress [96, 100, 77].", "startOffset": 181, "endOffset": 194}, {"referenceID": 99, "context": "Different criteria were elaborated, such as the search for maximal reduction in empirically evaluated prediction error, maximal compression progress, or maximal competence progress [96, 100, 77].", "startOffset": 181, "endOffset": 194}, {"referenceID": 76, "context": "Different criteria were elaborated, such as the search for maximal reduction in empirically evaluated prediction error, maximal compression progress, or maximal competence progress [96, 100, 77].", "startOffset": 181, "endOffset": 194}, {"referenceID": 7, "context": "For instance, the architecture called RobustIntelligent Adaptive Curiosity (RIAC) [8], which is a refinement of the IAC architecture which was elaborated for open-ended learning of affordances and skills in real robots [77], defines the interestingness of a sensorimotor subspace by the velocity of the decrease of the errors made by the robot when predicting the consequences of its actions, given a context, within this subspace.", "startOffset": 82, "endOffset": 85}, {"referenceID": 76, "context": "For instance, the architecture called RobustIntelligent Adaptive Curiosity (RIAC) [8], which is a refinement of the IAC architecture which was elaborated for open-ended learning of affordances and skills in real robots [77], defines the interestingness of a sensorimotor subspace by the velocity of the decrease of the errors made by the robot when predicting the consequences of its actions, given a context, within this subspace.", "startOffset": 219, "endOffset": 223}, {"referenceID": 76, "context": "As shown in [77, 8], it biases the system to explore subspaces of progressively increasing complexity.", "startOffset": 12, "endOffset": 19}, {"referenceID": 7, "context": "As shown in [77, 8], it biases the system to explore subspaces of progressively increasing complexity.", "startOffset": 12, "endOffset": 19}, {"referenceID": 73, "context": "Nevertheless, RIAC and similar \u201dknowledge based\u201d approaches (see [74]) have some limitations: first, while they can deal with the spatial or temporal non-stationarity of the model to be learned, they face the curse-of-dimensionality and can only be efficient when considering a moderate number of control dimensions (e.", "startOffset": 65, "endOffset": 69}, {"referenceID": 18, "context": "Also, because performing these measure costs time, this approach becomes more and more inefficient as the dimensionality of the control space grows [19].", "startOffset": 148, "endOffset": 152}, {"referenceID": 19, "context": "In fact, exploring the task space will be used to learn a subpart of the forward model that is enough for reaching most of reachable parts in the task space through local inversion and regression, leveraging techniques for generalizing policy parameters corresponding to novel task parameters based on previously learnt correspondences, such as in [20, 8, 55, 108].", "startOffset": 348, "endOffset": 364}, {"referenceID": 7, "context": "In fact, exploring the task space will be used to learn a subpart of the forward model that is enough for reaching most of reachable parts in the task space through local inversion and regression, leveraging techniques for generalizing policy parameters corresponding to novel task parameters based on previously learnt correspondences, such as in [20, 8, 55, 108].", "startOffset": 348, "endOffset": 364}, {"referenceID": 54, "context": "In fact, exploring the task space will be used to learn a subpart of the forward model that is enough for reaching most of reachable parts in the task space through local inversion and regression, leveraging techniques for generalizing policy parameters corresponding to novel task parameters based on previously learnt correspondences, such as in [20, 8, 55, 108].", "startOffset": 348, "endOffset": 364}, {"referenceID": 106, "context": "In fact, exploring the task space will be used to learn a subpart of the forward model that is enough for reaching most of reachable parts in the task space through local inversion and regression, leveraging techniques for generalizing policy parameters corresponding to novel task parameters based on previously learnt correspondences, such as in [20, 8, 55, 108].", "startOffset": 348, "endOffset": 364}, {"referenceID": 123, "context": "One way to address this issue is to take inspiration infant\u2019s motor exploration/babbling behavior, which has been argued to be teleological via introducing goals explicitly inside a task space and driving exploration at the level of these goals [126, 87, 124, 123].", "startOffset": 245, "endOffset": 264}, {"referenceID": 86, "context": "One way to address this issue is to take inspiration infant\u2019s motor exploration/babbling behavior, which has been argued to be teleological via introducing goals explicitly inside a task space and driving exploration at the level of these goals [126, 87, 124, 123].", "startOffset": 245, "endOffset": 264}, {"referenceID": 121, "context": "One way to address this issue is to take inspiration infant\u2019s motor exploration/babbling behavior, which has been argued to be teleological via introducing goals explicitly inside a task space and driving exploration at the level of these goals [126, 87, 124, 123].", "startOffset": 245, "endOffset": 264}, {"referenceID": 120, "context": "One way to address this issue is to take inspiration infant\u2019s motor exploration/babbling behavior, which has been argued to be teleological via introducing goals explicitly inside a task space and driving exploration at the level of these goals [126, 87, 124, 123].", "startOffset": 245, "endOffset": 264}, {"referenceID": 48, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 79, "endOffset": 99}, {"referenceID": 35, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 79, "endOffset": 99}, {"referenceID": 33, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 79, "endOffset": 99}, {"referenceID": 57, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 79, "endOffset": 99}, {"referenceID": 14, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 79, "endOffset": 99}, {"referenceID": 76, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 269, "endOffset": 277}, {"referenceID": 46, "context": "These motor synergies are often encoded using Central Pattern Generators (CPG) [49, 36, 34, 58, 15] or as more traditional innate low-level control loops which are part of the innate structure allowing a robot to bootstrap the learning of new skills, as for example in [77, 47] where it is combined with intrinsically motivated learning.", "startOffset": 269, "endOffset": 277}, {"referenceID": 16, "context": "Second, we will use a heuristic inspired by observations of infants who sometimes prepare their reaching movements by starting from a same rest position [17], by resetting the robot to such a rest position, which allows reducing the set of starting states used to perform a task.", "startOffset": 153, "endOffset": 157}, {"referenceID": 8, "context": "In this paper, we propose an approach which allows us to transpose some of the basic ideas of IAC and RIAC architectures, combined with ideas from the SSA algorithm, into a multi-level active learning architecture called SelfAdaptive Goal Generation RIAC algorithm (SAGG-RIAC) (an outline and initial evaluation of this architecture was presented in [9]).", "startOffset": 350, "endOffset": 353}, {"referenceID": 78, "context": "[79, 112]), as well as regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters (e.", "startOffset": 0, "endOffset": 9}, {"referenceID": 110, "context": "[79, 112]), as well as regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters (e.", "startOffset": 0, "endOffset": 9}, {"referenceID": 19, "context": "[20, 8, 55, 108]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 7, "context": "[20, 8, 55, 108]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 54, "context": "[20, 8, 55, 108]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 106, "context": "[20, 8, 55, 108]).", "startOffset": 0, "endOffset": 16}, {"referenceID": 78, "context": "approaches such as [79, 112] or [20, 55, 108] do not consider the problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work as they could be used as the low-level techniques for low-level learning of action parameter policies for self-generated tasks in the SAGG-RIAC architecture.", "startOffset": 19, "endOffset": 28}, {"referenceID": 110, "context": "approaches such as [79, 112] or [20, 55, 108] do not consider the problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work as they could be used as the low-level techniques for low-level learning of action parameter policies for self-generated tasks in the SAGG-RIAC architecture.", "startOffset": 19, "endOffset": 28}, {"referenceID": 19, "context": "approaches such as [79, 112] or [20, 55, 108] do not consider the problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work as they could be used as the low-level techniques for low-level learning of action parameter policies for self-generated tasks in the SAGG-RIAC architecture.", "startOffset": 32, "endOffset": 45}, {"referenceID": 54, "context": "approaches such as [79, 112] or [20, 55, 108] do not consider the problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work as they could be used as the low-level techniques for low-level learning of action parameter policies for self-generated tasks in the SAGG-RIAC architecture.", "startOffset": 32, "endOffset": 45}, {"referenceID": 106, "context": "approaches such as [79, 112] or [20, 55, 108] do not consider the problem of autonomous life-long exploration of novel parameterized tasks, they are very complemetary to the present work as they could be used as the low-level techniques for low-level learning of action parameter policies for self-generated tasks in the SAGG-RIAC architecture.", "startOffset": 32, "endOffset": 45}, {"referenceID": 73, "context": "SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127].", "startOffset": 138, "endOffset": 142}, {"referenceID": 36, "context": "SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127].", "startOffset": 282, "endOffset": 302}, {"referenceID": 30, "context": "SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127].", "startOffset": 282, "endOffset": 302}, {"referenceID": 81, "context": "SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127].", "startOffset": 282, "endOffset": 302}, {"referenceID": 2, "context": "SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127].", "startOffset": 282, "endOffset": 302}, {"referenceID": 124, "context": "SAGG-RIAC can be considered as an active learning algorithm carrying out the concept of competence based intrinsically motivated learning [74] and is in line with concepts of mastery motivation, Flow, Optimal Level theories and zone of Proximal Development introduced in psychology [37, 31, 82, 3, 127].", "startOffset": 282, "endOffset": 302}, {"referenceID": 73, "context": "In a competence based active exploration mechanism, according to the definition [74], the robot is pushed to perform an active exploration in the goal/operational space as opposed to motor babbling in the actuator space.", "startOffset": 80, "endOffset": 84}, {"referenceID": 97, "context": "First, algorithms achieving competence based exploration and allowing general computer programs to actively and adaptively self-generate abstract computational problems, or goals, of increasing complexity were studied in a theoretical computer science perspective [98, 99, 102].", "startOffset": 264, "endOffset": 277}, {"referenceID": 98, "context": "First, algorithms achieving competence based exploration and allowing general computer programs to actively and adaptively self-generate abstract computational problems, or goals, of increasing complexity were studied in a theoretical computer science perspective [98, 99, 102].", "startOffset": 264, "endOffset": 277}, {"referenceID": 101, "context": "First, algorithms achieving competence based exploration and allowing general computer programs to actively and adaptively self-generate abstract computational problems, or goals, of increasing complexity were studied in a theoretical computer science perspective [98, 99, 102].", "startOffset": 264, "endOffset": 277}, {"referenceID": 5, "context": "Measures of interestingness based on a measure of competence to perform a skill were studied in [6], as well as in [94] where a selector chooses to perform different skills depending on the temporal difference error to reach each skill.", "startOffset": 96, "endOffset": 99}, {"referenceID": 93, "context": "Measures of interestingness based on a measure of competence to perform a skill were studied in [6], as well as in [94] where a selector chooses to perform different skills depending on the temporal difference error to reach each skill.", "startOffset": 115, "endOffset": 119}, {"referenceID": 109, "context": "The study proposed in [111] is based on the competence progress, which they use to select goals in a pre-specified set of skills considered in a discrete world.", "startOffset": 22, "endOffset": 27}, {"referenceID": 84, "context": "A mechanism for passive exploration in the task space for learning inverse models in high-dimensional continuous robotics spaces was presented in [85, 86], where a robot has to learn its arm inverse kinematics while trying to reach in a preset order goals put on a pre-specified grid informing the robot about the limits of its reachable space.", "startOffset": 146, "endOffset": 154}, {"referenceID": 85, "context": "A mechanism for passive exploration in the task space for learning inverse models in high-dimensional continuous robotics spaces was presented in [85, 86], where a robot has to learn its arm inverse kinematics while trying to reach in a preset order goals put on a pre-specified grid informing the robot about the limits of its reachable space.", "startOffset": 146, "endOffset": 154}, {"referenceID": 73, "context": "1 Global Architecture Let us consider the definition of competence based models outlined in [74], and extract from it two different levels for active learning defined at different time scales (Fig.", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "Also, it is important to notice that \u03c0\u03b8(Data)(sstart, yg, \u03c1) can be computed on the fly, as in the experiments below, with regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters, such as in [20, 8, 55, 108].", "startOffset": 340, "endOffset": 356}, {"referenceID": 7, "context": "Also, it is important to notice that \u03c0\u03b8(Data)(sstart, yg, \u03c1) can be computed on the fly, as in the experiments below, with regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters, such as in [20, 8, 55, 108].", "startOffset": 340, "endOffset": 356}, {"referenceID": 54, "context": "Also, it is important to notice that \u03c0\u03b8(Data)(sstart, yg, \u03c1) can be computed on the fly, as in the experiments below, with regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters, such as in [20, 8, 55, 108].", "startOffset": 340, "endOffset": 356}, {"referenceID": 106, "context": "Also, it is important to notice that \u03c0\u03b8(Data)(sstart, yg, \u03c1) can be computed on the fly, as in the experiments below, with regression techniques allowing to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters, such as in [20, 8, 55, 108].", "startOffset": 340, "endOffset": 356}, {"referenceID": 113, "context": "We can make an analogy of this formalization with the Semi-Markov Option framework introduced by Sutton [116].", "startOffset": 104, "endOffset": 109}, {"referenceID": 19, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 379, "endOffset": 395}, {"referenceID": 7, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 379, "endOffset": 395}, {"referenceID": 54, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 379, "endOffset": 395}, {"referenceID": 106, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 379, "endOffset": 395}, {"referenceID": 77, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 572, "endOffset": 581}, {"referenceID": 110, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 572, "endOffset": 581}, {"referenceID": 44, "context": "The main assumptions about the methods that can be used for this lower level are: \u2022 Incremental learning and generalization: based on the data collected incrementally, the method must be able to build incrementally local forward and inverse models that can be reused later on, in particular when considering other goals, such as the task-space regression techniques presented in [20, 8, 55, 108]; \u2022 Goal-directed optimization: when a goal is set, an optimization procedure can improve the parameters of the action policy to reach the goal, such as policy gradient methods [78, 112] or stochastic optimization [45]; A optional feature, which is a variant of the second assumption above, is: \u2022 Active optimization: goal-directed optimization of the parameters of the action policy for reaching a self-generated goal.", "startOffset": 609, "endOffset": 613}, {"referenceID": 91, "context": "In the following experiments that will be introduced, we will use two different methods: one mechanism where optimization is inspired by the SSA algorithm [92], coupled with memory-based local forward and inverse regression models using local Moore-Penrose pseudo-inverses, and a more generic optimization algorithm mixing stochastic optimization with memory-based regression models using pseudo-inverse.", "startOffset": 155, "endOffset": 159}, {"referenceID": 77, "context": "For the optimization part, algorithms such as natural actor-critic architectures in model based reinforcement learning [78], algorithms of convex optimization [33], algorithms of stochastic optimization like CMA (e.", "startOffset": 119, "endOffset": 123}, {"referenceID": 32, "context": "For the optimization part, algorithms such as natural actor-critic architectures in model based reinforcement learning [78], algorithms of convex optimization [33], algorithms of stochastic optimization like CMA (e.", "startOffset": 159, "endOffset": 163}, {"referenceID": 44, "context": "[45]), or path-integral methods (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 111, "context": "[113, 112]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 110, "context": "[113, 112]).", "startOffset": 0, "endOffset": 10}, {"referenceID": 3, "context": "For the regression part, we are here using a memory-based approach, which if combined with efficient data storage and access structures [4, 70], scales well from a computational point of view.", "startOffset": 136, "endOffset": 143}, {"referenceID": 69, "context": "For the regression part, we are here using a memory-based approach, which if combined with efficient data storage and access structures [4, 70], scales well from a computational point of view.", "startOffset": 136, "endOffset": 143}, {"referenceID": 105, "context": "Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered [107], such as in [20, 8, 55, 108].", "startOffset": 327, "endOffset": 332}, {"referenceID": 19, "context": "Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered [107], such as in [20, 8, 55, 108].", "startOffset": 345, "endOffset": 361}, {"referenceID": 7, "context": "Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered [107], such as in [20, 8, 55, 108].", "startOffset": 345, "endOffset": 361}, {"referenceID": 54, "context": "Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered [107], such as in [20, 8, 55, 108].", "startOffset": 345, "endOffset": 361}, {"referenceID": 106, "context": "Yet, if memory limits would be a limited resource, and as little assumption about the low-level regression algorithms are made in the SAGG-RIAC architecture, parameterized models allowing to control memory requirements such as Neural networks, Support Vector Regression, Gaussian Process Regression could instead be considered [107], such as in [20, 8, 55, 108].", "startOffset": 345, "endOffset": 361}, {"referenceID": 76, "context": "A typical instantiation of C, without constraints \u03c1, is defined as C(yg, yf , \u2205) = \u2212\u2016yg\u2212yf\u2016, and is the direct transposition of prediction error in RIAC [77, 8] to the task space in SAGG-RIAC.", "startOffset": 153, "endOffset": 160}, {"referenceID": 7, "context": "A typical instantiation of C, without constraints \u03c1, is defined as C(yg, yf , \u2205) = \u2212\u2016yg\u2212yf\u2016, and is the direct transposition of prediction error in RIAC [77, 8] to the task space in SAGG-RIAC.", "startOffset": 153, "endOffset": 160}, {"referenceID": 7, "context": "Such a mechanism has been described in the RIAC algorithm introduced in [8], but was previously applied to the actuator space S rather than to the goal/task space Y as is done in SAGG-RIAC.", "startOffset": 72, "endOffset": 75}, {"referenceID": 16, "context": "[17] is that infant\u2019s reaching attempts are often preceded by movements that either elevate their hand or move their hand back to their side.", "startOffset": 0, "endOffset": 4}, {"referenceID": 84, "context": "And the second one, noticed in [85], is that infants do not try to reach for objects forever but sometimes relax their muscles and rest.", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "5 New Challenges of Unknown Limits of the Task Space In traditional active learning methods and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is typically designed to select actions to perform inside a set of values inside an already known interval", "startOffset": 159, "endOffset": 179}, {"referenceID": 76, "context": "5 New Challenges of Unknown Limits of the Task Space In traditional active learning methods and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is typically designed to select actions to perform inside a set of values inside an already known interval", "startOffset": 159, "endOffset": 179}, {"referenceID": 64, "context": "5 New Challenges of Unknown Limits of the Task Space In traditional active learning methods and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is typically designed to select actions to perform inside a set of values inside an already known interval", "startOffset": 159, "endOffset": 179}, {"referenceID": 95, "context": "5 New Challenges of Unknown Limits of the Task Space In traditional active learning methods and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is typically designed to select actions to perform inside a set of values inside an already known interval", "startOffset": 159, "endOffset": 179}, {"referenceID": 20, "context": "5 New Challenges of Unknown Limits of the Task Space In traditional active learning methods and especially knowledge-based intrinsically motivated exploration [11, 77, 65, 96, 21], the system is typically designed to select actions to perform inside a set of values inside an already known interval", "startOffset": 159, "endOffset": 179}, {"referenceID": 110, "context": "Algorithms 3 and 4 are simple alternative examples of lowlevel goal-directed optimization algorithms that are used in the experimental section, but they could be replaced by other algorithms like PI \u2212CMA [112], CMA [45], or those presented in [79].", "startOffset": 204, "endOffset": 209}, {"referenceID": 44, "context": "Algorithms 3 and 4 are simple alternative examples of lowlevel goal-directed optimization algorithms that are used in the experimental section, but they could be replaced by other algorithms like PI \u2212CMA [112], CMA [45], or those presented in [79].", "startOffset": 215, "endOffset": 219}, {"referenceID": 78, "context": "Algorithms 3 and 4 are simple alternative examples of lowlevel goal-directed optimization algorithms that are used in the experimental section, but they could be replaced by other algorithms like PI \u2212CMA [112], CMA [45], or those presented in [79].", "startOffset": 243, "endOffset": 247}, {"referenceID": 22, "context": "A solution to this non-convex problem has then been proposed by Bullock in [23] who converted it into a convex problem, by only considering the learning task within the spatial vicinity \u0302\u0307 \u03b1 of a particular \u03b1 : \u1e8f = J(\u03b1)\u0302\u0307 \u03b1 (5)", "startOffset": 75, "endOffset": 79}, {"referenceID": 122, "context": "Learning inverse kinematics typically deals with these kind of constraints, and these local methods have thus been proposed as an efficient approach to IK learning [125, 107].", "startOffset": 164, "endOffset": 174}, {"referenceID": 105, "context": "Learning inverse kinematics typically deals with these kind of constraints, and these local methods have thus been proposed as an efficient approach to IK learning [125, 107].", "startOffset": 164, "endOffset": 174}, {"referenceID": 69, "context": "In the following study, we use an incremental version of the Approximate Nearest Neighbors algorithm (ANN) [70], based on a tree split using the k-means process, to determine the vicinity of the current \u03b1.", "startOffset": 107, "endOffset": 111}, {"referenceID": 1, "context": "Using the pseudo-inverse of Moore-Penrose [2] to compute the pseudo-inverse J(\u03b1) of the Jacobian J(\u03b1) in a vicinity \u0302\u0307 \u03b1 is thus sufficient.", "startOffset": 42, "endOffset": 45}, {"referenceID": 27, "context": "Possible problems happening due to singularities [106, 28, 91] being bypassed by adding noise in the joint configurations (see [86] for a study about this problem).", "startOffset": 49, "endOffset": 62}, {"referenceID": 90, "context": "Possible problems happening due to singularities [106, 28, 91] being bypassed by adding noise in the joint configurations (see [86] for a study about this problem).", "startOffset": 49, "endOffset": 62}, {"referenceID": 85, "context": "Possible problems happening due to singularities [106, 28, 91] being bypassed by adding noise in the joint configurations (see [86] for a study about this problem).", "startOffset": 127, "endOffset": 131}, {"referenceID": 37, "context": "Learning the inverse kinematics is here an online process that arises each time a micro-action \u03b8 = \u2206\u03b1 \u2208 A is executed by the manipulator: by doing each micro-action, the robot stores measures (\u03b1,\u2206\u03b1,\u2206x) in its memory and creates a database Data which contains elements (\u03b1i,\u2206\u03b1i,\u2206yi) representing the discovered change \u2206yi corresponding to a given \u2206\u03b1i in the configuration \u03b1i (this learning entity can be called a schema according to the terminology of Drescher [38]).", "startOffset": 459, "endOffset": 463}, {"referenceID": 44, "context": "2 Exploration Phase This phase consists in performing q \u2208 N small random explorative actions \u2206\u03b1i, around the current position \u03b1, where the variations can be derandomized such as in [45].", "startOffset": 181, "endOffset": 185}, {"referenceID": 29, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 96, "endOffset": 108}, {"referenceID": 42, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 96, "endOffset": 108}, {"referenceID": 31, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 96, "endOffset": 108}, {"referenceID": 76, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 167, "endOffset": 190}, {"referenceID": 64, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 167, "endOffset": 190}, {"referenceID": 93, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 167, "endOffset": 190}, {"referenceID": 68, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 167, "endOffset": 190}, {"referenceID": 95, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 167, "endOffset": 190}, {"referenceID": 7, "context": "If we look at traditional active learning algorithms which cannot deal with open-ended learning [30, 43, 32], as well as RIAC-like algorithms different from SAGG-RIAC [77, 65, 94, 69, 96, 8], we can notice that even if these techniques deal with avoiding excessive exploration in unlearnable or extremely complex areas, the learning process still has to begin by a period of random exploration of the whole space, to distinguish and extract which subparts are the most interesting according to the used definition of interest.", "startOffset": 167, "endOffset": 190}, {"referenceID": 118, "context": "[121]), the zones of reachability would be increased if obstacles are introduced since the low-level system could learn to go around them.", "startOffset": 0, "endOffset": 5}, {"referenceID": 28, "context": "More precisely, we observe both increase in learning speed and final generalization performances (this results resonates with results from more classic active learning, see [29]).", "startOffset": 173, "endOffset": 177}, {"referenceID": 80, "context": "In order to allow the camera to distinguish the end-effector of the arm and to create a visual referent framework on the 2D surface, we used visual tags and the software ARToolKit Tracker [81].", "startOffset": 188, "endOffset": 192}, {"referenceID": 43, "context": "central pattern generators [44, 73, 49]), motor synergies are defined as the coherent activations (in space or time) of a group of muscles.", "startOffset": 27, "endOffset": 39}, {"referenceID": 72, "context": "central pattern generators [44, 73, 49]), motor synergies are defined as the coherent activations (in space or time) of a group of muscles.", "startOffset": 27, "endOffset": 39}, {"referenceID": 48, "context": "central pattern generators [44, 73, 49]), motor synergies are defined as the coherent activations (in space or time) of a group of muscles.", "startOffset": 27, "endOffset": 39}, {"referenceID": 33, "context": "They have been proposed as building blocks simplifying the scaffolding of motor behaviors because allowing the reduction of the number of parameters needed to represent complex movements [34, 58, 15, 120].", "startOffset": 187, "endOffset": 204}, {"referenceID": 57, "context": "They have been proposed as building blocks simplifying the scaffolding of motor behaviors because allowing the reduction of the number of parameters needed to represent complex movements [34, 58, 15, 120].", "startOffset": 187, "endOffset": 204}, {"referenceID": 14, "context": "They have been proposed as building blocks simplifying the scaffolding of motor behaviors because allowing the reduction of the number of parameters needed to represent complex movements [34, 58, 15, 120].", "startOffset": 187, "endOffset": 204}, {"referenceID": 117, "context": "They have been proposed as building blocks simplifying the scaffolding of motor behaviors because allowing the reduction of the number of parameters needed to represent complex movements [34, 58, 15, 120].", "startOffset": 187, "endOffset": 204}, {"referenceID": 15, "context": "Described as crucial for the development of motor abilities, they can be seen as encoding an unconscious continuous control of muscles which simplifies the complexity of the learning process: learning complex tasks using parameterized motor synergies (such as walking, or swimming) indeed corresponds to the tuning of relatively low-dimensional (but yet which can have a few dozen dimensions) high-level control parameters, compared to the important number of degrees of freedom which have to be controlled (thousand in the human body, see [16]).", "startOffset": 540, "endOffset": 544}, {"referenceID": 53, "context": "2 Robotic Setup In the following experiment, we consider a quadruped robot simulated using the Breve simulator [54] (physics simulation is based on ODE).", "startOffset": 111, "endOffset": 115}, {"referenceID": 19, "context": "In order to create such a local inverse model (numerous other solutions exist, such as [20, 8, 55, 108]), we extract the potentially more reliable data using the following method: We first extract from the learned data the set L of the l nearest neighbors of (ug, vg, \u03c6g) and then retrieve their corresponding motor synergies using an ANN method [70]: L = {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, .", "startOffset": 87, "endOffset": 103}, {"referenceID": 7, "context": "In order to create such a local inverse model (numerous other solutions exist, such as [20, 8, 55, 108]), we extract the potentially more reliable data using the following method: We first extract from the learned data the set L of the l nearest neighbors of (ug, vg, \u03c6g) and then retrieve their corresponding motor synergies using an ANN method [70]: L = {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, .", "startOffset": 87, "endOffset": 103}, {"referenceID": 54, "context": "In order to create such a local inverse model (numerous other solutions exist, such as [20, 8, 55, 108]), we extract the potentially more reliable data using the following method: We first extract from the learned data the set L of the l nearest neighbors of (ug, vg, \u03c6g) and then retrieve their corresponding motor synergies using an ANN method [70]: L = {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, .", "startOffset": 87, "endOffset": 103}, {"referenceID": 106, "context": "In order to create such a local inverse model (numerous other solutions exist, such as [20, 8, 55, 108]), we extract the potentially more reliable data using the following method: We first extract from the learned data the set L of the l nearest neighbors of (ug, vg, \u03c6g) and then retrieve their corresponding motor synergies using an ANN method [70]: L = {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, .", "startOffset": 87, "endOffset": 103}, {"referenceID": 69, "context": "In order to create such a local inverse model (numerous other solutions exist, such as [20, 8, 55, 108]), we extract the potentially more reliable data using the following method: We first extract from the learned data the set L of the l nearest neighbors of (ug, vg, \u03c6g) and then retrieve their corresponding motor synergies using an ANN method [70]: L = {{u, v, \u03c6, \u03b8}1, {u, v, \u03c6, \u03b8}2, .", "startOffset": 346, "endOffset": 350}, {"referenceID": 29, "context": "Again, as in the previous experiment, we can also observe that SAGG-RIAC does not only allow to learn faster how to master the sensorimotor space, but that the asymptotic performances also seem to be better [30].", "startOffset": 207, "endOffset": 211}, {"referenceID": 23, "context": "\u2022 Social guidance and learning by interaction: social guidance mechanisms allowing a non-engineer human to drive the attention of a robot toward particular task spaces, through physical guidance [24, 71] or human-robot interfaces allowing the robot to be attracted toward particular dimensions of the environment [88], may be introduced.", "startOffset": 195, "endOffset": 203}, {"referenceID": 70, "context": "\u2022 Social guidance and learning by interaction: social guidance mechanisms allowing a non-engineer human to drive the attention of a robot toward particular task spaces, through physical guidance [24, 71] or human-robot interfaces allowing the robot to be attracted toward particular dimensions of the environment [88], may be introduced.", "startOffset": 195, "endOffset": 203}, {"referenceID": 87, "context": "\u2022 Social guidance and learning by interaction: social guidance mechanisms allowing a non-engineer human to drive the attention of a robot toward particular task spaces, through physical guidance [24, 71] or human-robot interfaces allowing the robot to be attracted toward particular dimensions of the environment [88], may be introduced.", "startOffset": 313, "endOffset": 317}, {"referenceID": 49, "context": "to examples of action policies could also be seen as a mean to infer interesting task spaces from human demonstrations [50].", "startOffset": 119, "endOffset": 123}, {"referenceID": 70, "context": "Social guidance may also be used as a mechanism to bootstrap the evaluation of competence progress, and the identification of zones of reachability, in very large or high-dimensional spaces such as shown in [71], which presents an approach to combine intrinsically motivated learning like SAGG-RIAC with techniques for learning by demonstration.", "startOffset": 207, "endOffset": 211}, {"referenceID": 15, "context": "An important direction for future work is to take inspiration from the maturational processes of infants which are constrained in their learning and development by numerous physiological and cognitive mechanisms such as the limitation of their sensorimotor apparatus, as well as the evolving capabilities of their brain [16, 122, 22, 66].", "startOffset": 320, "endOffset": 337}, {"referenceID": 119, "context": "An important direction for future work is to take inspiration from the maturational processes of infants which are constrained in their learning and development by numerous physiological and cognitive mechanisms such as the limitation of their sensorimotor apparatus, as well as the evolving capabilities of their brain [16, 122, 22, 66].", "startOffset": 320, "endOffset": 337}, {"referenceID": 21, "context": "An important direction for future work is to take inspiration from the maturational processes of infants which are constrained in their learning and development by numerous physiological and cognitive mechanisms such as the limitation of their sensorimotor apparatus, as well as the evolving capabilities of their brain [16, 122, 22, 66].", "startOffset": 320, "endOffset": 337}, {"referenceID": 65, "context": "An important direction for future work is to take inspiration from the maturational processes of infants which are constrained in their learning and development by numerous physiological and cognitive mechanisms such as the limitation of their sensorimotor apparatus, as well as the evolving capabilities of their brain [16, 122, 22, 66].", "startOffset": 320, "endOffset": 337}, {"referenceID": 9, "context": "Using such constraints in synergy with goal babbling and intrinsic motivation, such as explored in [10], would potentially allow to constrain and simplify further learning since the first actions of the robot [39, 42], and could be crucial when considering lifelong learning in unbounded task spaces.", "startOffset": 99, "endOffset": 103}, {"referenceID": 38, "context": "Using such constraints in synergy with goal babbling and intrinsic motivation, such as explored in [10], would potentially allow to constrain and simplify further learning since the first actions of the robot [39, 42], and could be crucial when considering lifelong learning in unbounded task spaces.", "startOffset": 209, "endOffset": 217}, {"referenceID": 41, "context": "Using such constraints in synergy with goal babbling and intrinsic motivation, such as explored in [10], would potentially allow to constrain and simplify further learning since the first actions of the robot [39, 42], and could be crucial when considering lifelong learning in unbounded task spaces.", "startOffset": 209, "endOffset": 217}], "year": 2013, "abstractText": "We introduce the Self-Adaptive Goal Generation Robust Intelligent Adaptive Curiosity (SAGG-RIAC) architecture as an intrinsically motivated goal exploration mechanism which allows active learning of inverse models in high-dimensional redundant robots. This allows a robot to efficiently and actively learn distributions of parameterized motor skills/policies that solve a corresponding distribution of parameterized tasks/goals. The architecture makes the robot sample actively novel parameterized tasks in the task space, based on a measure of competence progress, each of which triggers low-level goal-directed learning of the motor policy parameters that allow to solve it. For both learning and generalization, the system leverages regression techniques which allow to infer the motor policy parameters corresponding to a given novel parameterized task, and based on the previously learnt correspondences between policy and task parameters. \u2217Baranes, A., Oudeyer, P-Y. (2012) Active Learning of Inverse Models with Intrinsically Motivated Goal Exploration in Robots, Robotics and Autonomous Systems, 61(1), pp. 49-73. http://dx.doi.org/10.1016/j.robot.2012.05.008 1 ar X iv :1 30 1. 48 62 v1 [ cs .L G ] 2 1 Ja n 20 13 We present experiments with high-dimensional continuous sensorimotor spaces in three different robotic setups: 1) learning the inverse kinematics in a highly-redundant robotic arm, 2) learning omnidirectional locomotion with motor primitives in a quadruped robot, 3) an arm learning to control a fishing rod with a flexible wire. We show that 1) exploration in the task space can be a lot faster than exploration in the actuator space for learning inverse models in redundant robots; 2) selecting goals maximizing competence progress creates developmental trajectories driving the robot to progressively focus on tasks of increasing complexity and is statistically significantly more efficient than selecting tasks randomly, as well as more efficient than different standard active motor babbling methods; 3) this architecture allows the robot to actively discover which parts of its task space it can learn to reach and which part it cannot.", "creator": "LaTeX with hyperref package"}}}