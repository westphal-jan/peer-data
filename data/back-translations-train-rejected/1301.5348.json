{"id": "1301.5348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "Why Size Matters: Feature Coding as Nystrom Sampling", "abstract": "Recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this paper we propose a novel view of this pipeline based on kernel methods and Nystrom sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nystrom sampling step to select a subset of all data points.", "histories": [["v1", "Tue, 15 Jan 2013 21:36:06 GMT  (25kb,D)", "https://arxiv.org/abs/1301.5348v1", null], ["v2", "Tue, 16 Apr 2013 00:17:54 GMT  (25kb,D)", "http://arxiv.org/abs/1301.5348v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["oriol vinyals", "yangqing jia", "trevor darrell"], "accepted": false, "id": "1301.5348"}, "pdf": {"name": "1301.5348.pdf", "metadata": {"source": "CRF", "title": "Why Size Matters: Feature Coding as Nystro\u0308m Sampling", "authors": ["Oriol Vinyals", "Yangqing Jia"], "emails": [], "sections": [{"heading": null, "text": "Why Size Matters: Feature Coding as Nystro \ufffd m SamplingOriol Vinyals UC Berkeley Berkeley, CAYangqing Jia UC Berkeley Berkeley, CATrevor Darrell UC Berkeley Berkeley, CA"}, {"heading": "1 Introduction", "text": "Lately, due to its general simplicity, well-understood properties of linear classifiers, and computing power, the community has advocated feature extraction pipelines based on a single coding step followed by a linear classifier. In this paper, we propose a novel view of this pipeline based on core methods and nystro-m scanning. In particular, we focus on encoding a data point with a local representation based on a dictionary with fewer elements than the number of data points and consider it an approximation to the actual function that would calculate pairwise similarity to all data points (often too many to calculate in practice), followed by a nystro-m scanning step to select a subset of all data points. Furthermore, since boundaries beyond the approximation power of nystro-m scanning are known as a function that serves as a function for calculating the computational size (i.e., we can approximate the size of the word book, but nevertheless we can approximate the size of the word book)."}, {"heading": "2 The Nystro\u0308m View", "text": "To encode a new sample x > > Rd, we use a (generally nonlinear) encoding function c, so that c (x) \u00b2 Rc \u2032. Note that d is the dimensionality of the original attribute space, while c is the size of the dictionary. The standard classification pipeline considers c (x) as the new attribute space and typically uses a linear classifier in that space. Note that our discussion of encoding applies to many different forward-facing encoding schemases.Ideally (infinite computation and memory), we encode each sample x = max (0, x > D \u2212 \u03b1), where D \u2212 \u03b1 x is the dictionary."}, {"heading": "3 Main Results on Approximation Bounds", "text": "Interestingly, there are many limits to the error made in the estimation of C by C, and finding better sampling schemes that improve these limits is an active topic in the machine learning community (see, for example, [4]). The limit we begin with is [4]: | | C \u2212 C \u2032 | F \u2264 | | C \u2212 Ck | | F + max (nCii) (1) is valid if c \u2265 64k / 4 (c is the number of columns we scan scan from C to form E, i.e. the code book size), where k is the sufficient rank to estimate the structure of C, and Ck is the optimal rank approximation (indicated by Singular Value Decomposition (SVD), which we cannot calculate in practice). Note that if we assume that our training set can be explained by a multiple of dimension k (i.e. the first term in the right side of eq disappears from a constant time, which is proportional to a constant)."}, {"heading": "4 Experiments", "text": "We empirically evaluate the boundary to the core matrix, which is used as a proxy for the classification of models, which is the measure of interest. To estimate the constants in the boundary, we interpolate the observed accuracy in the first two samples of accuracy versus the codebook size, which is of practical interest: you can quickly run a new dataset through the pipeline of small codebook sizes, and then quickly estimate what the accuracy would be when running a full experiment with a much larger dictionary size. Figure 1 shows the results on the CIFAR-10 image classification and TIMIT speech recognition datasets. It is observed that the derived model is closely linked to our own empirical observations, which serve as the lower boundary of actual accuracy."}], "references": [{"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A Coates", "H Lee", "AY Ng"], "venue": "In AISTATS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A Coates", "AY Ng"], "venue": "In ICML,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Pooling-invariant image feature learning", "author": ["Y Jia", "O Vinyals", "T Darrell"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Sampling methods for the nystr\u00f6m method", "author": ["S Kumar", "M Mohri", "A Talwalkar"], "venue": "JMLR, 13(4):981\u20131006,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "On random weights and unsupervised feature learning", "author": ["A Saxe", "PW Koh", "Z Chen", "M Bhand", "B Suresh", "AY Ng"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Locality-constrained linear coding for image classification", "author": ["J Wang", "J Yang", "K Yu", "F Lv", "T Huang", "Y Gong"], "venue": "In CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Efficient highly over-complete sparse coding using a mixture model", "author": ["J Yang", "K Yu", "T Huang"], "venue": "In ECCV,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}], "referenceMentions": [{"referenceID": 1, "context": "This model may help explaining the positive effect of the codebook size [2, 7] and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity.", "startOffset": 72, "endOffset": 78}, {"referenceID": 6, "context": "This model may help explaining the positive effect of the codebook size [2, 7] and justifying the need to stack more layers (often referred to as deep learning), as flat models empirically saturate as we add more complexity.", "startOffset": 72, "endOffset": 78}, {"referenceID": 1, "context": "For example, one may use the threshold encoding function [2] as an example: c(x) = max(0,x>D \u2212 \u03b1) where D \u2208 Rd\u00d7c is the dictionary.", "startOffset": 57, "endOffset": 60}, {"referenceID": 5, "context": "In general, larger dictionary sizes yield better performance assuming the linear classifier is well regularized, as it can be seen as a way to do manifold learning [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 3, "context": "[4]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "The bound we start with is [4]:", "startOffset": 27, "endOffset": 30}, {"referenceID": 2, "context": "We refer to our tech report [3] for more details.", "startOffset": 28, "endOffset": 31}, {"referenceID": 1, "context": "The Nystr\u00f6m view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nystr\u00f6m sampling context [4].", "startOffset": 284, "endOffset": 287}, {"referenceID": 6, "context": "The Nystr\u00f6m view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nystr\u00f6m sampling context [4].", "startOffset": 315, "endOffset": 318}, {"referenceID": 0, "context": "The Nystr\u00f6m view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nystr\u00f6m sampling context [4].", "startOffset": 425, "endOffset": 431}, {"referenceID": 4, "context": "The Nystr\u00f6m view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nystr\u00f6m sampling context [4].", "startOffset": 425, "endOffset": 431}, {"referenceID": 3, "context": "The Nystr\u00f6m view of feature encoding and the approximation bounds we proposed helps understanding several key observations in the recent literature: (1) the linear classifier performance is always bounded when using a fixed codebook, and performance increases when the codebook grows [2], even with a huge codebook [7], and (2) simple dictionary learning techniques have been found efficient in some classification pipelines [1, 5], and K-means works particularly well as a dictionary learning algorithm albeit its simplicity, a phenomenon that is common in the Nystr\u00f6m sampling context [4].", "startOffset": 587, "endOffset": 590}, {"referenceID": 2, "context": "In one of our related work [3], we form a dictionary by first \u201covershooting\u201d the coding stage with a larger dictionary, and then pruning it using K-centers with pooled features.", "startOffset": 27, "endOffset": 30}], "year": 2013, "abstractText": "Recently, the computer vision and machine learning community has been in favor of feature extraction pipelines that rely on a coding step followed by a linear classifier, due to their overall simplicity, well understood properties of linear classifiers, and their computational efficiency. In this paper we propose a novel view of this pipeline based on kernel methods and Nystr\u00f6m sampling. In particular, we focus on the coding of a data point with a local representation based on a dictionary with fewer elements than the number of data points, and view it as an approximation to the actual function that would compute pair-wise similarity to all data points (often too many to compute in practice), followed by a Nystr\u00f6m sampling step to select a subset of all data points.", "creator": "TeX"}}}