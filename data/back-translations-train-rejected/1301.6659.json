{"id": "1301.6659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jan-2013", "title": "Clustering-Based Matrix Factorization", "abstract": "Recommender systems are emerging technologies that nowadays can be found in many applications such as Amazon, Netflix, and so on. These systems help users find relevant information, recommendations, and their preferred items. Matrix Factorization is a popular method in Recommendation Systems showing promising results in accuracy and complexity. In this paper we propose an extension of matrix factorization that uses the clustering paradigm to cluster similar users and items in several communities. We then establish their effects on the prediction model then. To the best of our knowledge, our proposed model outperforms all other published recommender methods in accuracy and complexity. For instance, our proposed method's accuracy is 0.8122 on the Netflix dataset which is better than the Netflix prize winner's accuracy of 0.8567.", "histories": [["v1", "Mon, 28 Jan 2013 20:01:57 GMT  (202kb,D)", "http://arxiv.org/abs/1301.6659v1", null], ["v2", "Fri, 8 Feb 2013 22:16:44 GMT  (0kb,I)", "http://arxiv.org/abs/1301.6659v2", "This paper has been withdrawn by the authors. As the problem in the source code the results are nor valid anymore"], ["v3", "Wed, 27 Feb 2013 01:04:55 GMT  (344kb,D)", "http://arxiv.org/abs/1301.6659v3", null], ["v4", "Thu, 1 Aug 2013 22:06:49 GMT  (0kb,I)", "http://arxiv.org/abs/1301.6659v4", "This paper has been withdrawn by the author due to crucial typo and the poor grammatical text"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["nima mirbakhsh", "charles x ling"], "accepted": false, "id": "1301.6659"}, "pdf": {"name": "1301.6659.pdf", "metadata": {"source": "META", "title": "Clustering-Based Matrix Factorization (under review)", "authors": ["Nima Mirbakhsh", "Charles X. Ling"], "emails": ["smirbakh@uwo.ca", "cling@csd.uwo.ca"], "sections": [{"heading": "1. Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far, until it is so far."}, {"heading": "2. Problem Definition", "text": "In a general recommendation problem we have a number of users U = {u1, u2,., un} and a number of items I = {i1, i2,.. that they are rated by a rating matrix R = [rui] n \u00b7 m, in which rui the rating of the user u on item i. These rating values are limited to a range [a, b] depending on the scope, but a range [0, 5] or a Boolean value {0, 1} are usually quite common forms of rating in the real world. The aim of a recommendation system is to find an unknown rating for user u and item, which I use information including the rating matrix R.R.2 consists of predicting unknown rijs based on the known ri-j's within the matrix R: R = i1 i2..... in the u1 r1m u2 r21 r22."}, {"heading": "3. Clustering-Based Matrix Factorization", "text": "In our proposed extension, we add neighborhood information about the basic matrix factorization model and assume that there is a novel improvement. There are two common approaches in the neighborhood of conscious matrix factorization models: 1) Item - Item models that take into account whether the user is interested in item i of his similar items. 2) User models that take into account whether user u and his similar items are interested and then apply the effect of the latent factors of these homogeneous communities in the basic model. As discussed in Section 1, it has two advantages over the common neighborhood models: 1) We generalize the interests and items of the communities to which they belong. Thus, we look at whether the general interests of users coincide with the general characteristics of the items. 2) We look at deeper similarities."}, {"heading": "3.1. Locality Sensitive Hashing", "text": "As already mentioned, the calculation costs are important when dealing with large sets of referral system data. Therefore, we use Locality Sensitive Hashing (LSH) methods to establish a new technique for estimating clusters, rather than using common, similarity-based cluster methods. In the literature, LSH is commonly used to reduce dimension, and then cluster algorithms such as k means are used for cluster purposes on the reduced dimension. Even in this way, similarity verification between all users and items is required, which is still costly. LSH is a method for performing a probable dimension reduction in order to hack the input elements in such a way that similar items are highly likely to be mapped to the same bucket (Broder et al., 1998). We use at least-wise independent permutations (MinHash) as a hash function in our method. It consists of \u03b2-hash functions and hash functions, as < Itex = 2 in mutations."}, {"heading": "3.2. Repeating clustering and using hierarchical clusters in CBMF", "text": "LSH algorithms are based on the probability logic. Thus, it is assumed that the repeated use of the LSH buckets will reduce the cluster error. This can be modelled by re-bundling fixed \u03b2-buckets for h-times. We then add the effect of new clusters in the predictor function (Repeated CBMF) or by applying all h-precise clusters with different bucket sizes in the predictor function (Hierarchical CBMF). However, the use of large h linear increases the learning time. Thus, the use of re-clusters or fewer and fewer general clusters leads to a reduction in the effect of LSH cluster errors and adds more information to the model. As already noted, the change \u03b2 influences the number of clusters and the generality of the clusters. Starting from \u03b2 = 1, this technique reaches the most general clusters and by increasing the number of clusters, the number of clusters would be increased and thus the number of clusters would be less general."}, {"heading": "4. Experiment Results", "text": "These data sets are previously used in related articles such as (To \u00bc scher & Jahrer, 2009), (Jamali & Ester, 2010), and (Herlocker et al., 1999).Table 1 shows a general statistic of these data sets.We apply a repeated random partial sample, which divides the known ratings into two groups in each fold; 80% for the turn and the rest for the test.We use three folds and obtain the mean of these folds to verify the accuracy of the final accuracy of the model. We first collect all articles rated above 3 for Netflix, and do the same for the articles.Then we establish the LSH method for these folds and obtain the mean of these folds accuracy as the final accuracy of the model."}, {"heading": "4.1. Number of Clusters", "text": "Figure 5 shows the change in similarity between the elements in the same clusters for different \u03b2 values. To determine \u03b2, the similarity is obviously increased, but also the number of clusters is increased. Figure 4 shows how the change \u03b2 affects the RMSE results for the Netflix dataset. One question is: How should \u03b2 be selected? To determine \u03b2, we apply a cross validation on the train set and then select the length of the buckets with the lowest RMSE. Figure 6 illustrates the application of our proposed method to the Movielens datasets that change \u03b2 and in a triple cross validation. We use the same \u03b2 for users and items. As Figure 6 shows \u03b2 = 2 the best RMSE result for the verification sets."}, {"heading": "4.2. Complexity", "text": "As an advantage of using LSH methods, the proposed model of basic matrix factorization does not add a costly extension, it increases the learning time of basic matrix factorization by less than twice in each epoch. Table 2 shows the complexity of our proposed model, the Basic Matrix Factorization, and two Neighborhood Aware Matrix Factorization models according to (To \ufffd scher et al., 2008) and (Koren & Bell, 2011).The second statements show the complexity of pre-processing for each model. Basic matrix factorization and Neighborhood Aware Matrix Factorization by (Koren & Bell, 2011) have no pre-processing, but the (To \ufffd scher et al., 2008) proposed model adds a pre-processing complexity of O (n2 + m2) to find the similar users and items. R (u) is the set of items for which their evaluations are available by the 2 m2, and also designated by the O (i)."}, {"heading": "4.3. Selecting the Alpha Paremeter (\u03b1)", "text": "Two approaches can be applied to the selection of \u03b1: 1. To train the evaluation matrix for user items and the evaluation matrix for the clusters separately. Figure 8 shows the RMSE result of the change of \u03b1 for the first case and Figure 7 shows the results for the second case of the selection of \u03b1. At \u03b1 = 1, the effect of the predictor function of clusters is not taken into account in both cases. Thus, the RMSE result is identical to the basic matrix factoring for user item evaluation matrix."}, {"heading": "4.4. Using Contents", "text": "Since the data sets already used do not provide profiles for articles and users, we use a smaller version of the data set provided in the \"KDD Cup 2012 - Track1\" to experiment with this effect. The data set consists of a Chinese social network, Tencent Weibo, user profiles and items, and a list of recommended items for some users who have accepted or rejected them (Boolean values). Our experiment shows that using keywords of users and items in the model improves accuracy by 1%."}, {"heading": "4.5. Using RCBMF and HCBMF", "text": "In the end, we use the validation results of the CBMF to apply the effect of using several buckets to the model.As already mentioned, we do this in two cases: 1) Repeat the most accurate CBMF model that changes the \u03b2 (RCBMF), and 2) Apply the most accurate models that change the \u03b2 (HCBMF).We use h = 3 in our experiment in both algorithms. As Figure 9 shows, we experience different improvements in the accuracy of the models in different datasets. For example, HCBMF shows a significant improvement in the RMSE for Epinions datasets, but on the other hand, its result for Flixter is no better than simple CBMF. RCBMF has better results for Movielens100k, Netflix, and Flixter datasets. We cross-validate the parameters (including W \u03b2j and \u03b1) in our experiment, but we still use the same learning rate (GP) and regulation parameters as BMF."}, {"heading": "5. Related Works", "text": "(To \u00bc scher et al., 2008) presents a neighbor-conscious matrix factorization, which, however, includes neighborhood information in the basic matrix factorization. Your proposed algorithm calculates three predictions for each user-item pair: a prediction rMFui based on basic matrix factorization; a prediction ruserui based on a user-item model; and finally a prediction ritemui based on an item-neighborhood-model. A combination of these three predictions is the final prediction of this algorithm. The evaluation prognosis ruserui based on a user-item-item-item-item-item-item-item-item-item is calculated as follows: ruserui-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item-item:"}, {"heading": "6. Conclusion", "text": "Matrix factorization is a popular method in recommendation systems that shows promising results in terms of accuracy and complexity. In this paper, we propose an extension of matrix factorization, which uses cluster paradigms to cluster similar users and items in multiple communities. We then determine the impact of these communities on the predictive model. Specifically, our proposed model is an improvement on item / user merger models, where we apply deeper similarities and broader interests. As the datasets in recommendation systems are usually huge, we use a new technique for clustering that keeps our model practically efficient. Our experiments show promising accuracy that to the best of our knowledge exceeds all other suggested recommendation methods. Thus, the accuracy of our proposed method is 0.8122 on the Netflix dataset, which is better than the accuracy of the Netflix award winner of 0.8567."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "Recommender systems are emerging technologies that nowadays can be found in many applications such as Amazon, Netflix, and so on. These systems help users find relevant information, recommendations, and their preferred items. Matrix Factorization is a popular method in Recommendation Systems showing promising results in accuracy and complexity. In this paper we propose an extension of matrix factorization that uses the clustering paradigm to cluster similar users and items in several communities. We then establish their effects on the prediction model then. To the best of our knowledge, our proposed model outperforms all other published recommender methods in accuracy and complexity. For instance, our proposed method\u2019s accuracy is 0.8122 on the Netflix dataset which is better than the Netflix prize winner\u2019s accuracy of 0.8567.", "creator": "LaTeX with hyperref package"}}}