{"id": "1301.6686", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Causal Discovery from a Mixture of Experimental and Observational Data", "abstract": "This paper describes a Bayesian method for combining an arbitrary mixture of observational and experimental data in order to learn causal Bayesian networks. Observational data are passively observed. Experimental data, such as that produced by randomized controlled trials, result from the experimenter manipulating one or more variables (typically randomly) and observing the states of other variables. The paper presents a Bayesian method for learning the causal structure and parameters of the underlying causal process that is generating the data, given that (1) the data contains a mixture of observational and experimental case records, and (2) the causal process is modeled as a causal Bayesian network. This learning method was applied using as input various mixtures of experimental and observational data that were generated from the ALARM causal Bayesian network. In these experiments, the absolute and relative quantities of experimental and observational data were varied systematically. For each of these training datasets, the learning method was applied to predict the causal structure and to estimate the causal parameters that exist among randomly selected pairs of nodes in ALARM that are not confounded. The paper reports how these structure predictions and parameter estimates compare with the true causal structures and parameters as given by the ALARM network.", "histories": [["v1", "Wed, 23 Jan 2013 15:57:22 GMT  (355kb)", "http://arxiv.org/abs/1301.6686v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gregory f cooper", "changwon yoo"], "accepted": false, "id": "1301.6686"}, "pdf": {"name": "1301.6686.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Gregory F. Cooper"], "emails": ["gfc@cbmi.upmc.edu", "cwyoo@cbmi.upmc.edu"], "sections": [{"heading": null, "text": "In fact, it is so that most of them are able to survive themselves, and that they see themselves as being able to survive themselves if they are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves."}], "references": [{"title": "The ALARM monitoring system: A case study with two probabilistic inference techniques for belief networks", "author": ["I.A. Beinlich", "H.J. Suermondt", "R.M. Chavez", "G.F. Cooper"], "venue": "Proceedings of the Second European Conference on Artificial Intelligence in Medicine", "citeRegEx": "Beinlich et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Beinlich et al\\.", "year": 1989}, {"title": "A method for learning belief networks that contain hidden variables, In: Proceedings of the Workshop on Knowledge Discovery in Databases 112-124", "author": ["G.F. Cooper"], "venue": null, "citeRegEx": "Cooper,? \\Q1993\\E", "shortCiteRegEx": "Cooper", "year": 1993}, {"title": "An overview of the representation and discovery of causal relationships using Bayesian networks", "author": ["G.F. Cooper"], "venue": null, "citeRegEx": "Cooper,? \\Q1999\\E", "shortCiteRegEx": "Cooper", "year": 1999}, {"title": "A Bayesian method for the induction of probabilistic networks from data, Machine Learning", "author": ["G.F. Cooper", "E. Herskovits"], "venue": null, "citeRegEx": "Cooper and Herskovits,? \\Q1992\\E", "shortCiteRegEx": "Cooper and Herskovits", "year": 1992}, {"title": "A characterization of the Dirichlet distribution with application to learning", "author": ["D. Geiger", "D. Heckerman"], "venue": "Bayesian networks,", "citeRegEx": "Geiger and Heckerman,? \\Q1995\\E", "shortCiteRegEx": "Geiger and Heckerman", "year": 1995}, {"title": "A Bayesian approach to learning causal networks", "author": ["D. Heckerman"], "venue": "Proceedings of the Conference on Uncertainty in Artificial Intelligence", "citeRegEx": "Heckerman,? \\Q1995\\E", "shortCiteRegEx": "Heckerman", "year": 1995}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D. Chickering"], "venue": "Machine Learning", "citeRegEx": "Heckerman et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Heckerman et al\\.", "year": 1995}, {"title": "Propagating uncertainty in Bayesian networks by logic sampling", "author": ["M. Henrion"], "venue": "Uncertainty in Artificial Intelligence 2 (North-Holland,", "citeRegEx": "Henrion,? \\Q1988\\E", "shortCiteRegEx": "Henrion", "year": 1988}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["J. Pearl"], "venue": null, "citeRegEx": "Pearl,? \\Q1988\\E", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "Causation, Prediction, and Search (Available at http://hss.cmu.edu/htrnlldepartrnents/philosophy!TETRA D.BOOK/book.htrnl)", "author": ["P. Spirtes", "C. Glymour", "R. Scheines"], "venue": null, "citeRegEx": "Spirtes et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Spirtes et al\\.", "year": 1993}], "referenceMentions": [{"referenceID": 2, "context": "Bayesian discovery of causal networks is an active field of research in which numerous advances have been-and are continuing to be-made in areas that include causal representation, model assessment and scoring, and model search (Cooper 1999).", "startOffset": 228, "endOffset": 241}, {"referenceID": 5, "context": "A notable exception is a paper by Heckerman on learning influence diagrams as causal models, which contains the essential ideas for learning causal Bayesian networks from a combination of experimental data under deterministic manipulation and observational data (Heckerman 1995).", "startOffset": 262, "endOffset": 278}, {"referenceID": 8, "context": "A causal Bayesian network (or causal network for short) is a Bayesian network in which each arc is interpreted as a direct causal influence between a parent node (variable) and a child node, relative to the other nodes in the network (Pearl 1988).", "startOffset": 234, "endOffset": 246}, {"referenceID": 8, "context": "The causal Markov condition permits the joint distribution of the n variables in a causal Bayesian network to be factored as follows (Pearl 1988):", "startOffset": 133, "endOffset": 145}, {"referenceID": 1, "context": "When there are hidden variables or missing data, the Bayesian approach can model them explicitly and normatively (Cooper 1993); however, exact computation of the integral in Equation 5 with current methods is usually intractable, even when a causal network contains only one hidden variable.", "startOffset": 113, "endOffset": 126}, {"referenceID": 4, "context": "Under the assumptions that follow in this section, as expressed in (Geiger and Heckerman 1995), the integral in Equation 6 can be computed efficiently in closed form.", "startOffset": 67, "endOffset": 94}, {"referenceID": 5, "context": "4 Heckerman uses the term mechanism independence for the causal version of global parameter independence, and the term component independence for the causal version of local parameter independence (Heckerman 1995).", "startOffset": 197, "endOffset": 213}, {"referenceID": 8, "context": "to Y (H1), (2) there is one or more causal paths from Y to X (H2), or (3) X and Y have no d-connecting paths (Pearl 1988) between each other (H3).", "startOffset": 109, "endOffset": 121}, {"referenceID": 7, "context": "For each pair (X, Y) in U, we used stochastic simulation (Henrion 1988) to generate three types of data from ALARM.", "startOffset": 57, "endOffset": 71}], "year": 2011, "abstractText": "This paper describes a Bayesian method for combining an arbitrary mixture of observational and experimental data in order to learn causal Bayesian networks. Observational data are passively observed. Experimental data, such as that produced by randomized controlled trials, result from the experimenter manipulating one or more variables (typically randomly) and observing the states of other variables. The paper presents a Bayesian method for learning the causal structure and parameters of the underlying causal process that is generating the data, given that (I) the data contains a mixture of observational and experimental case records, and (2) the causal process is modeled as a causal Bayesian network. This learning method was applied using as input various mixtures of experimental and observational data that were generated from the ALARM causal Bayesian network. In these experiments, the absolute and relative quantities of experimental and observational data were varied systematically. For each of these training datasets, the learning method was applied to predict the causal structure and to estimate the causal parameters that exist among randomly selected pairs of nodes in ALARM that are not confounded. The paper reports how these structure predictions and parameter estimates compare with the true causal structures and parameters as given by the ALARM network.", "creator": "pdftk 1.41 - www.pdftk.com"}}}