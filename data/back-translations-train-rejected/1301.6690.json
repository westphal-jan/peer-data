{"id": "1301.6690", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Model-Based Bayesian Exploration", "abstract": "Reinforcement learning systems are often concerned with balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Information - the expected improvement in future decision quality arising from the information acquired by exploration. Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states. In this paper we investigate ways of representing and reasoning about this uncertainty in algorithms where the system attempts to learn a model of its environment. We explicitly represent uncertainty about the parameters of the model and build probability distributions over Q-values based on these. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation.", "histories": [["v1", "Wed, 23 Jan 2013 15:57:38 GMT  (575kb)", "http://arxiv.org/abs/1301.6690v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["richard dearden", "nir friedman", "david", "re"], "accepted": false, "id": "1301.6690"}, "pdf": {"name": "1301.6690.pdf", "metadata": {"source": "CRF", "title": "Model based Bayesian Exploration", "authors": ["Richard Dearden", "Nir Friedman", "David Andre"], "emails": ["dearden@cs.ubc.ca", "nir@cs.huji.ac.il", "dandre@cs.berkeley.edu"], "sections": [{"heading": null, "text": "In fact, it is as if most people are able to survive themselves by going in search of their own identity and identity. (...) In fact, it is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...)"}, {"heading": "O ur exploration strategy also keeps a record of how con", "text": "The main difference is that we cannot engage in a binary classification of states, but in a way that takes into account the possible value of exploitation. (This leads us to engage in a way that takes into account the possible value of exploitation and exploitation.) Firstly, we are currently able to show how our method works. We are also interested in applying it to more compact models (e.g., us dynamic Bayesian networks), and to problems with consistent state spaces. Finally, the most challenging future orientation is towards the actual value of an action rather than myopic assessments."}], "references": [{"title": "Generalized priori\u00ad tized sweeping, in 'Advances in Neural information", "author": ["D. Andre", "N. Friedman", "R. Parr"], "venue": "Process\u00ad ing Systems\u00b7,", "citeRegEx": "Andre et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Andre et al\\.", "year": 1997}, {"title": "Neural Networks for Pattern Recognition", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop,? \\Q1995\\E", "shortCiteRegEx": "Bishop", "year": 1995}, {"title": "Bayesian Q\u00ad leaming, in 'Proceedings of the Fifteenth National Confer\u00ad ence on Artificial intelligence (AAAI-98)", "author": ["R. Dearden", "N. Friedman", "S. Russell"], "venue": null, "citeRegEx": "Dearden et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1998}, {"title": "Proability and Statistics, 2nd edn, Addison-Wesley", "author": ["M.H. Degroot"], "venue": null, "citeRegEx": "Degroot,? \\Q1986\\E", "shortCiteRegEx": "Degroot", "year": 1986}, {"title": "A tutorial on learning with Bayesian net\u00ad", "author": ["D. Heckerman"], "venue": "ed., 'Learning in Graphical Models',", "citeRegEx": "Heckerman,? \\Q1998\\E", "shortCiteRegEx": "Heckerman", "year": 1998}, {"title": "Information value theory", "author": ["R.A. Howard"], "venue": "IEEE Transac\u00ad tions on Systems Science and Cybernetics", "citeRegEx": "Howard,? \\Q1966\\E", "shortCiteRegEx": "Howard", "year": 1966}, {"title": "Stochastic simula\u00ad tion algorithms for dynamic probabilistic networks, in 'Pro\u00ad ceedings of the Eleventh Conference on Uncertainty in Arti\u00ad ficial Intelligence (UAI-95)", "author": ["K. Kanazawa", "D. Koller", "S. Russell"], "venue": null, "citeRegEx": "Kanazawa et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kanazawa et al\\.", "year": 1995}, {"title": "Near-optimal performance for re\u00ad inforcement learning in polynomial time, in 'Proceedings of the Fifteenth", "author": ["M. Keams", "S. Singh"], "venue": "Int. Conf. on Machine Learning',", "citeRegEx": "Keams and Singh,? \\Q1998\\E", "shortCiteRegEx": "Keams and Singh", "year": 1998}, {"title": "Using learning for approxima\u00ad tion in stochastic processes, in 'Proceedings of the Fifteenth International Conference on Machine Learning", "author": ["D. Koller", "R. Fratkina"], "venue": null, "citeRegEx": "Koller and Fratkina,? \\Q1998\\E", "shortCiteRegEx": "Koller and Fratkina", "year": 1998}, {"title": "Prioritized sweeping\u00ad reinforcement learning with less data and less time", "author": ["A.W. Moore", "C.G. Atkeson"], "venue": "Ma\u00ad chine Learning", "citeRegEx": "Moore and Atkeson,? \\Q1993\\E", "shortCiteRegEx": "Moore and Atkeson", "year": 1993}, {"title": "Stochastic Simulation, Wiley, NY", "author": ["B.D. Ripley"], "venue": null, "citeRegEx": "Ripley,? \\Q1987\\E", "shortCiteRegEx": "Ripley", "year": 1987}, {"title": "Do the Right Thing: Studies in Limited Rationality", "author": ["S.J. Russell", "E.H. Wefald"], "venue": null, "citeRegEx": "Russell and Wefald,? \\Q1991\\E", "shortCiteRegEx": "Russell and Wefald", "year": 1991}, {"title": "Integrated architectures for learning, plan\u00ad ning, and reacting based on approximating dynamic pro\u00ad gramming", "author": ["R.S. Sutton"], "venue": "'Proceedings of the Seventh Int. Conf. on Ma\u00ad chine Learning',", "citeRegEx": "Sutton,? \\Q1990\\E", "shortCiteRegEx": "Sutton", "year": 1990}], "referenceMentions": [{"referenceID": 12, "context": "steps actually executed by the learner, since it can \"learn\" from simulated steps in the model (Sutton 1990).", "startOffset": 95, "endOffset": 108}, {"referenceID": 5, "context": "tion: the agent should choose actions based on the value of the information it can expect to learn by performing them (Howard 1966).", "startOffset": 118, "endOffset": 131}, {"referenceID": 12, "context": "This ap\u00ad proach was pursued in the DYNA (Sutton 1990) frame\u00ad work, where after the execution of an action, the agent updates its model of the environment, and then performs some bounded number of value propagation steps to up\u00ad date its approximation of the value function.", "startOffset": 40, "endOffset": 53}, {"referenceID": 4, "context": "We show that this is possible by adopting re\u00ad sults from Bayesian learning of probabilistic models, such as Bayesian networks (Heckerman 1998).", "startOffset": 126, "endOffset": 142}, {"referenceID": 3, "context": "We can use well-known Bayesian methods for learning standard distributions such as multinomials or Gaussian distributions (Degroot 1986).", "startOffset": 122, "endOffset": 136}, {"referenceID": 4, "context": "Nonetheless, much of the above discussion and conclusions about parameter independence and Dirichlet priors apply to these models (Heckerman 1998).", "startOffset": 130, "endOffset": 146}, {"referenceID": 2, "context": "In a recent paper, Dearden et al. (1998) examined model\u00ad free Bayesian reinforcement learning.", "startOffset": 19, "endOffset": 41}, {"referenceID": 1, "context": "One of the simplest ones is K erne/ estimation (see for example (Bishop 1995)).", "startOffset": 64, "endOffset": 77}, {"referenceID": 10, "context": "Procedures for sampling from these distributions can be found in (Ripley 1987).", "startOffset": 65, "endOffset": 78}, {"referenceID": 10, "context": "Procedures for sampling from these distributions can be found in (Ripley 1987). Friedman and Singer (1999) introduce a structured prior that captures our uncertainty about the set of\"feasible\" val\u00ad ues of X.", "startOffset": 66, "endOffset": 107}], "year": 2011, "abstractText": "Reinforcement learning systems are often concerned with balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classi\u00ad cal notion of Value of Informationthe expected im\u00ad provement in future decision quality arising from the tnformation acquired by exploration. Estimating this quantity requires an assessment of the agent's uncer\u00ad tainty about its current value estimates for states. In this paper we investigate ways to represent and rea\u00ad son about this uncertainty in algorithms where the sys\u00ad tem attempts to learn a model of its environment. We explicitly represent uncertainty about the parameters of the model and build probability distributions over Q\u00ad values based on these. These distributions are used to compute a myopic approximation to the value of infor\u00ad mation for each action and hence to select the action that best balances exploration and exploitation.", "creator": "pdftk 1.41 - www.pdftk.com"}}}