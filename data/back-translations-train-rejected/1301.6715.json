{"id": "1301.6715", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "My Brain is Full: When More Memory Helps", "abstract": "We consider the problem of finding good finite-horizon policies for POMDPs under the expected reward metric. The policies considered are {em free finite-memory policies with limited memory}; a policy is a mapping from the space of observation-memory pairs to the space of action-memeory pairs (the policy updates the memory as it goes), and the number of possible memory states is a parameter of the input to the policy-finding algorithms. The algorithms considered here are preliminary implementations of three search heuristics: local search, simulated annealing, and genetic algorithms. We compare their outcomes to each other and to the optimal policies for each instance. We compare run times of each policy and of a dynamic programming algorithm for POMDPs developed by Hansen that iteratively improves a finite-state controller --- the previous state of the art for finite memory policies. The value of the best policy can only improve as the amount of memory increases, up to the amount needed for an optimal finite-memory policy. Our most surprising finding is that more memory helps in another way: given more memory than is needed for an optimal policy, the algorithms are more likely to converge to optimal-valued policies.", "histories": [["v1", "Wed, 23 Jan 2013 15:59:22 GMT  (272kb)", "http://arxiv.org/abs/1301.6715v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["christopher lusena", "tong li", "shelia sittinger", "chris wells", "judy goldsmith"], "accepted": false, "id": "1301.6715"}, "pdf": {"name": "1301.6715.pdf", "metadata": {"source": "CRF", "title": "My Brain is Full: When More Memory Helps", "authors": ["Christopher Lusena", "Tong Li", "Shelia Sittinger", "Chris Wells", "Judy Goldsmith"], "emails": ["@cs."], "sections": [{"heading": null, "text": "We consider the problem of finding good strategies for the infinite horizon, up to the amount needed for an optimal reward, as another surprising finding. The policy we are looking at is a policy of free infinite memory politics with limited memory; a policy is an illustration from the space of the observation-memory pairs into the space of the plot-memory pairs (the policy upwards dates memory as it goes), and the number of possible memory states is a parameter of input to the policy-finding algorithms. Fortunately, the algorithms that are considered here are three types of search heuris: local search, simulated approximation, and genetic algorithms. We compare their results with each other and to the optimal poli species for each instance. We compare the times of each policy and the dynamic programming of POMDPs that iteratively improves a finite state processing - the former state of art for finite storage strategies."}], "references": [{"title": "Anthony Cassandra's POMDP Page. http://www.cs.brown.edu/people/arc /research/pomdp.html", "author": ["A. Cassandra"], "venue": null, "citeRegEx": "Cassandra,? \\Q1997\\E", "shortCiteRegEx": "Cassandra", "year": 1997}, {"title": "Finite-Memory Control of Par\u00ad", "author": ["E. Hansen"], "venue": null, "citeRegEx": "Hansen,? \\Q1998\\E", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Solving POMDPs by searching", "author": ["E. Hansen"], "venue": null, "citeRegEx": "Hansen,? \\Q1998\\E", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Adaptation in Natural and Arti\u00ad", "author": ["J.H. Holland"], "venue": null, "citeRegEx": "Holland,? \\Q1975\\E", "shortCiteRegEx": "Holland", "year": 1975}, {"title": "1998a (October). Adding Memory", "author": ["Lanzi", "Pier Luca"], "venue": null, "citeRegEx": "Lanzi and Luca.,? \\Q1998\\E", "shortCiteRegEx": "Lanzi and Luca.", "year": 1998}, {"title": "Adding Memory to XCS", "author": ["Lanzi", "Pier Luca."], "venue": "In:", "citeRegEx": "Lanzi and Luca.,? 1998b", "shortCiteRegEx": "Lanzi and Luca.", "year": 1998}, {"title": "Memoryless policies: Theoretical", "author": ["M.L. Littman"], "venue": null, "citeRegEx": "Littman,? \\Q1994\\E", "shortCiteRegEx": "Littman", "year": 1994}, {"title": "A Survey of Algorithmic Meth\u00ad ods for Partially Observed Markov Decision Pro\u00ad", "author": ["W.S. Lovejoy"], "venue": null, "citeRegEx": "Lovejoy,? \\Q1991\\E", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "Nonapproximability Results for Markov Decision Processes", "author": ["C. Lusena", "J. Goldsmith", "M. Mundhenk"], "venue": "Tech. rept. 274-98", "citeRegEx": "Lusena et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lusena et al\\.", "year": 1998}, {"title": "Finite\u00ad Memory Policies for POMDPs", "author": ["Lusena", "Christopher", "Wells", "Chris", "Goldsmit", "Judy", "Sittinger", "Shelia", "Li", "Tong"], "venue": null, "citeRegEx": "Lusena et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Lusena et al\\.", "year": 1999}, {"title": "On the Computability of Infinite\u00ad Horizon Partially Observable Markov Decision Processes", "author": ["Madani", "Omid."], "venue": "Appeared in AAAI fall symposium on Planning with POMDPs.", "citeRegEx": "Madani and Omid.,? 1998", "shortCiteRegEx": "Madani and Omid.", "year": 1998}, {"title": "Solving POMDPs by Searching the Space of Finite Policies", "author": ["Meuleau", "Nicolas", "Kim", "Kee-Eung", "Hauskrecht", "Milos", "Cassandra", "Anthony R", "Kaelbling", "Leslie P"], "venue": null, "citeRegEx": "Meuleau et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Meuleau et al\\.", "year": 1999}, {"title": "The Complexity of Markov Decision Processes", "author": ["C.H. Papadimitriou", "J.N. Tsitsiklis"], "venue": "Math\u00ad ematics of Operations Research,", "citeRegEx": "Papadimitriou and Tsitsiklis,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "The Optimal Control of Partially Ob\u00ad servable Markov Processes", "author": ["E. Sondik"], "venue": "Ph.D. thesis, Stanford University.", "citeRegEx": "Sondik,? 1971", "shortCiteRegEx": "Sondik", "year": 1971}, {"title": "Genetic Algorithms for Approximating So\u00ad lutions to POMDPs. (In Preparation)", "author": ["Wells", "Chris", "Goldsmith", "Judy", "Lusena", "Christopher"], "venue": null, "citeRegEx": "Wells et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Wells et al\\.", "year": 1999}, {"title": "Classifier Fitness Based on Accuracy", "author": ["Wilson", "Stewart W."], "venue": "Evolutionary Computation, 3(2), 148175.", "citeRegEx": "Wilson and W.,? 1995", "shortCiteRegEx": "Wilson and W.", "year": 1995}, {"title": "Region-Based Ap\u00ad proximations for Planning in Stochastic Domains", "author": ["N.L. Zhang", "W. Liu"], "venue": "Proceedings of the 13th Conference on Uncertainty in Artificial Intelli\u00ad", "citeRegEx": "Zhang and Liu,? \\Q1997\\E", "shortCiteRegEx": "Zhang and Liu", "year": 1997}], "referenceMentions": [{"referenceID": 8, "context": "Furthermore, there can only be a provably good polynomial-time approxi\u00ad mation algorithm (a so-called \"\ufffd;-approximation\") if P=PSPACE (Lusena et al., 1998).", "startOffset": 134, "endOffset": 155}, {"referenceID": 8, "context": "NP=P (Lusena et al., 1998).", "startOffset": 5, "endOffset": 26}, {"referenceID": 13, "context": "These policies were introduced in Sondik's thesis (Sondik, 1971), but the most general form of finite memory policies has received little attention until recently.", "startOffset": 50, "endOffset": 64}, {"referenceID": 7, "context": "Finite memory can be used to record the last k states seen; this restriction, finite-history policies, was exten\u00ad sively explored in the '70's and '80's (Lovejoy, 1991).", "startOffset": 153, "endOffset": 168}, {"referenceID": 6, "context": "Our idea of applying search heuristics to the space of free finite memory policies for a POMDP has been ap\u00ad plied by Littman (Littman, 1994) and others to the problem of finding good stationary policies.", "startOffset": 125, "endOffset": 140}, {"referenceID": 11, "context": "Based on the data in (Meuleau et al., 1999), it ap\u00ad pears that this is not as efficient as our local search algorithms, for several reasons.", "startOffset": 21, "endOffset": 43}, {"referenceID": 3, "context": "This formalism was introduced in (Holland, 1975).", "startOffset": 33, "endOffset": 48}, {"referenceID": 11, "context": "present another approach for solving POMDPs by searching in the policy space (Meuleau et al., 1999).", "startOffset": 77, "endOffset": 99}, {"referenceID": 6, "context": "30; Sutton's gridworld and McCallum's maze (Littman, 1994), Maze7 and MazelO (Lanzi, 1998a), and test1), for 1, 2, 3, and 4 memory states.", "startOffset": 43, "endOffset": 58}, {"referenceID": 14, "context": "In (Wells et al., 1999) the positions of the cells in the policies are made maleable, enabling the use of the inversion genetic operator.", "startOffset": 3, "endOffset": 23}, {"referenceID": 9, "context": "In a submitted paper, (Lusena et al., 1999), we describe an extension to this work in which we evaluate policies with infinite horizons and discounted rewards.", "startOffset": 22, "endOffset": 43}], "year": 2011, "abstractText": "We consider the problem of finding good finite-horizon policies for POMDPs under the expected reward metric. The policies con\u00ad sidered are free finite-memory policies with limited memory; a policy is a mapping from the space of observation-memory pairs to the space of action-memory pairs (the policy up\u00ad dates the memory as it goes), and the num\u00ad ber of possible memory states is a parameter of the input to the policy-finding algorithms. The algorithms considered here are prelimi\u00ad nary implementations of three search heuris\u00ad tics: local search, simulated annealing, and genetic algorithms. We compare their out\u00ad comes to each other and to the optimal poli\u00ad cies for each instance. We compare run times of each policy and of a dynamic programming algorithm for POMDPs developed by Hansen that iteratively improves a finite-state con\u00ad troller the previous state of the art for finite memory policies. The value of the best policy can only improve as the amount of memory increases, up to the amount needed for an optimal finite-memory policy. Our most surprising finding is that more memory helps in another way: given more memory than is needed for an optimal policy, the algo\u00ad rithms are more likely to converge to optimal\u00ad valued policies.", "creator": "pdftk 1.41 - www.pdftk.com"}}}