{"id": "1301.6718", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "On the Complexity of Policy Iteration", "abstract": "Decision-making problems in uncertain or stochastic domains are often formulated as Markov decision processes (MDPs). Policy iteration (PI) is a popular algorithm for searching over policy-space, the size of which is exponential in the number of states. We are interested in bounds on the complexity of PI that do not depend on the value of the discount factor. In this paper we prove the first such non-trivial, worst-case, upper bounds on the number of iterations required by PI to converge to the optimal policy. Our analysis also sheds new light on the manner in which PI progresses through the space of policies.", "histories": [["v1", "Wed, 23 Jan 2013 15:59:34 GMT  (294kb)", "http://arxiv.org/abs/1301.6718v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["yishay mansour", "satinder singh"], "accepted": false, "id": "1301.6718"}, "pdf": {"name": "1301.6718.pdf", "metadata": {"source": "CRF", "title": "On the Complexity of Policy Iteration", "authors": ["Satinder Singh"], "emails": ["}@research.att.com"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}], "references": [{"title": "Dynamic Programming: Deter\u00ad ministic and Stochastic Models. Prentice-Hall, En\u00ad", "author": ["D.P. Bertsekas"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1987}, {"title": "Dynamic Programming and Op\u00ad timal Control", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Dynamic Programming and Markov Processes", "author": ["R. Howard"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1960}, {"title": "Algorithms for Sequential Decision Making", "author": ["M.L. Littman"], "venue": "PhD thesis, Brown University,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "On the complex\u00ad ity of policy iteration for stochastic games", "author": ["M. Melekopoglou", "A. Condon"], "venue": "Techni\u00ad cal Report CS-TR-90-941,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Artificial Intelligence: A Modern Approach", "author": ["S.J. Russell", "P. Norvig"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}], "referenceMentions": [{"referenceID": 6, "context": "The problem of decision-making in uncertain or stochastic environments is central to artificial intel\u00ad ligence (AI) [7, 6].", "startOffset": 116, "endOffset": 122}, {"referenceID": 5, "context": "The problem of decision-making in uncertain or stochastic environments is central to artificial intel\u00ad ligence (AI) [7, 6].", "startOffset": 116, "endOffset": 122}, {"referenceID": 0, "context": "The framework of Markov deci\u00ad sion processes (MDPs) developed in the operations re\u00ad search community [1] is increasingly used within AI to formulate such problems.", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "One reason for the popularity of the MDP framework within AI is the availability of a number of well-studied classes of algorithms for planning in MDPs: linear\u00ad programming [2], value iteration [1], and policy itera\u00ad tion [3].", "startOffset": 173, "endOffset": 176}, {"referenceID": 0, "context": "One reason for the popularity of the MDP framework within AI is the availability of a number of well-studied classes of algorithms for planning in MDPs: linear\u00ad programming [2], value iteration [1], and policy itera\u00ad tion [3].", "startOffset": 194, "endOffset": 197}, {"referenceID": 2, "context": "One reason for the popularity of the MDP framework within AI is the availability of a number of well-studied classes of algorithms for planning in MDPs: linear\u00ad programming [2], value iteration [1], and policy itera\u00ad tion [3].", "startOffset": 222, "endOffset": 225}, {"referenceID": 3, "context": "Linear programming and value iteration are known to compute the optimal policy in time poly\u00ad nomial in the size of the representation of the MDP and the discount factor [4, 2].", "startOffset": 169, "endOffset": 175}, {"referenceID": 1, "context": "Linear programming and value iteration are known to compute the optimal policy in time poly\u00ad nomial in the size of the representation of the MDP and the discount factor [4, 2].", "startOffset": 169, "endOffset": 175}, {"referenceID": 2, "context": "This implies that policy iteration also runs in time polynomial in the size of the representation and the discount factor [3, 2].", "startOffset": 122, "endOffset": 128}, {"referenceID": 1, "context": "This implies that policy iteration also runs in time polynomial in the size of the representation and the discount factor [3, 2].", "startOffset": 122, "endOffset": 128}, {"referenceID": 3, "context": "This is an important issue be\u00ad cause there is strong empirical evidence in favor of PI over value iteration and linear programming in solving MDPs [4].", "startOffset": 147, "endOffset": 150}, {"referenceID": 4, "context": "A lower\u00ad bound is known for a particular form of PI, called se\u00ad quential PI (which at each step accepts only one of the single-state action changes that are improvements) in the worst-case, sequential PI can take 11(2\") steps on a two-action MDP, when the adversary controls which improvements are selected [5, 4].", "startOffset": 307, "endOffset": 313}, {"referenceID": 3, "context": "A lower\u00ad bound is known for a particular form of PI, called se\u00ad quential PI (which at each step accepts only one of the single-state action changes that are improvements) in the worst-case, sequential PI can take 11(2\") steps on a two-action MDP, when the adversary controls which improvements are selected [5, 4].", "startOffset": 307, "endOffset": 313}, {"referenceID": 1, "context": ", [2].", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": "This should be contrasted with the lower-bound of !1(2\") for sequential policy iteration [5, 4].", "startOffset": 89, "endOffset": 95}, {"referenceID": 3, "context": "This should be contrasted with the lower-bound of !1(2\") for sequential policy iteration [5, 4].", "startOffset": 89, "endOffset": 95}], "year": 2011, "abstractText": "Decision-making problems in uncertain or stochastic domains are often formulated as Markov decision processes (MD Ps). Pol\u00ad icy iteration (PI) is a popular algorithm for searching over policy-space, the size of which is exponential in the number of states. We are interested in bounds on the complexity of PI that do not depend on the value of the discount factor. In this paper we prove the first such non-trivial, worst-case, upper bounds on the number of iterations required by PI to converge to the optimal policy. Our analysis also sheds new light on the manner in which PI progresses through the space of policies.", "creator": "pdftk 1.41 - www.pdftk.com"}}}