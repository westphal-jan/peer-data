{"id": "1301.6720", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Solving POMDPs by Searching the Space of Finite Policies", "abstract": "Solving partially observable Markov decision processes (POMDPs) is highly intractable in general, at least in part because the optimal policy may be infinitely large. In this paper, we explore the problem of finding the optimal policy from a restricted set of policies, represented as finite state automata of a given size. This problem is also intractable, but we show that the complexity can be greatly reduced when the POMDP and/or policy are further constrained. We demonstrate good empirical results with a branch-and-bound method for finding globally optimal deterministic policies, and a gradient-ascent method for finding locally optimal stochastic policies.", "histories": [["v1", "Wed, 23 Jan 2013 15:59:42 GMT  (387kb)", "http://arxiv.org/abs/1301.6720v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nicolas meuleau", "kee-eung kim", "leslie pack kaelbling", "anthony r cassandra"], "accepted": false, "id": "1301.6720"}, "pdf": {"name": "1301.6720.pdf", "metadata": {"source": "CRF", "title": "Solving POMDPs by Searching the Space of Finite Policies", "authors": ["Nicolas Meuleau", "Kee-Eung Kim", "Leslie Pack Kaelbling"], "emails": ["@cs.brown.edu"], "sections": [{"heading": null, "text": "The solution to the problems that arise from this is highly insoluble, at least in part, because the optimal policy can be infinitely large. In this paper, we show that complexity can be greatly reduced if POMDP and / or policy are further restricted. We have good empirical results with a branch and a bound method for finding globally optimal policies, and a linear method for finding locally optimal policies. [1, 22, 5, 9, 4, 13] is a much more realistic model than its fully observable counterpart, the classical MDP [11, 20]."}, {"heading": "Ordering of free parameters. The tree of all possible", "text": "This year is the highest in the history of the country."}], "references": [{"title": "Optimal control of Markov decision pro\u00ad cesses with incomplete state estimation", "author": ["K.J. Astrom"], "venue": "J. Math. Ani. Appl.,", "citeRegEx": "Astrom.,? \\Q1965\\E", "shortCiteRegEx": "Astrom.", "year": 1965}, {"title": "Gradient descent for general reinforcement learning", "author": ["L.C. Baird", "A.W. Moore"], "venue": "In Advances in Neu\u00ad ral Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Exact and Approximate Algorithms for Partially Observable Markov Decision Processes", "author": ["A.R. Cassandra"], "venue": "PhD thesis, Brown University,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Acting optimally in partially observable stochastic domains", "author": ["A.R. Cassandra", "L.P. Kaelbling", "M.L. Littman"], "venue": "In Proceedings of the Twelfth National Con\u00ad ference on Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1994}, {"title": "An improved policy iteration algorithm for partially observable MDPs. In Advances in Neu\u00ad ral Information Processing Systems, IO", "author": ["E.A. Hansen"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Finite-Memory Control of Partially Observable Systems", "author": ["E.A. Hansen"], "venue": "PhD thesis, Department of Computer Science, University of Massachusetts at Amherst,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "Solving POMDPs by searching in pol\u00ad icy space", "author": ["E.A. Hansen"], "venue": "In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Planning and Control in Stochas\u00ad tic Domains with Imperfect Information", "author": ["M. Hauskrecht"], "venue": "PhD thesis,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Optimal Control of Complex Structured Processes", "author": ["0. Higelin"], "venue": "PhD thesis, University of Caen, France,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1999}, {"title": "Dynamic Programming and Markov Processes", "author": ["R.A. Howard"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1960}, {"title": "Rein\u00ad forcement learning algorithm for partially observable Markov problems", "author": ["T. Jaakkola", "S. Singh", "M.R. Jordan"], "venue": "In Advances in Neural Informa\u00ad tion Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Overcoming incomplete perception with utile distinction memory", "author": ["R.A. McCallum"], "venue": "In The Proceedings of the Tenth International Machine Learning Confer\u00ad ence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1993}, {"title": "Reinforcement Learning with Selec\u00ad tive Perception and Hidden State", "author": ["R.A. McCallum"], "venue": "PhD thesis, Univer\u00ad sity of Rochester,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1995}, {"title": "Kael\u00ad bling. Learning finite-state controllers for partially observable environments", "author": ["N. Meuleau", "L. Peshkin", "K.E. Kim", "L.P"], "venue": "Proceedings of the Fif\u00ad teenth Conference on Uncertainty in Artificial Intel\u00ad ligence, To appear,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1999}, {"title": "Reinforcement learning with hierarchies of machines. In Advances in Neural In\u00ad formation Processing Systems I I", "author": ["R. Parr", "S. Russell"], "venue": null, "citeRegEx": "Parr and Russell.,? \\Q1998\\E", "shortCiteRegEx": "Parr and Russell.", "year": 1998}, {"title": "Learn\u00ad ing policies with external memory", "author": ["L. Peshkin", "N. Meuleau", "L.P. Kaelbling"], "venue": "Proceedings of the Sixteenth International Conference on Machine Learning, To appear,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1999}, {"title": "Lave. Markov decision processes with probabilistic observation of states", "author": ["R.E.J.K. Satia"], "venue": "Management Science,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1973}, {"title": "Sondik. The optimal con\u00ad trol of partially observable Markov decision processes over a finite horizon", "author": ["E.J.R.D. Smallwood"], "venue": "Operations Research,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1973}, {"title": "The optimal control of partially observ\u00ad able Markov decision processes over the infinite hori\u00ad zon: Discounted costs", "author": ["E.J. Sondik"], "venue": "Operations Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1978}, {"title": "Learning from Delayed Rewards", "author": ["C. Watkins"], "venue": "PhD thesis, King's College,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1989}], "referenceMentions": [], "year": 2011, "abstractText": "Solving partially observable Markov decision processes (POMDPs) is highly intractable in gen\u00ad eral, at least in part because the optimal policy may be infinitely large. In this paper, we ex\u00ad plore the problem of finding the optimal policy from a restricted set of policies, represented as finite state automata of a given size. This prob\u00ad lem is also intractable, but we show that the com\u00ad plexity can be greatly reduced when the POMDP and/or policy are further constrained. We demon\u00ad strate good empirical results with a branch-and\u00ad bound method for finding globally optimal deter\u00ad ministic policies, and a gradient-ascent method for finding locally optimal stochastic policies.", "creator": "pdftk 1.41 - www.pdftk.com"}}}