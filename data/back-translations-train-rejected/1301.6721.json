{"id": "1301.6721", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Learning Finite-State Controllers for Partially Observable Environments", "abstract": "Reactive (memoryless) policies are sufficient in completely observable Markov decision processes (MDPs), but some kind of memory is usually necessary for optimal control of a partially observable MDP. Policies with finite memory can be represented as finite-state automata. In this paper, we extend Baird and Moore's VAPS algorithm to the problem of learning general finite-state automata. Because it performs stochastic gradient descent, this algorithm can be shown to converge to a locally optimal finite-state controller. We provide the details of the algorithm and then consider the question of under what conditions stochastic gradient descent will outperform exact gradient descent. We conclude with empirical results comparing the performance of stochastic and exact gradient descent, and showing the ability of our algorithm to extract the useful information contained in the sequence of past observations to compensate for the lack of observability at each time-step.", "histories": [["v1", "Wed, 23 Jan 2013 15:59:46 GMT  (381kb)", "http://arxiv.org/abs/1301.6721v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI cs.SY", "authors": ["nicolas meuleau", "leonid peshkin", "kee-eung kim", "leslie pack kaelbling"], "accepted": false, "id": "1301.6721"}, "pdf": {"name": "1301.6721.pdf", "metadata": {"source": "CRF", "title": "Learning Finite-State Controllers for Partially Observable Environments", "authors": ["Nicolas Meuleau", "Leonid Peshkin", "Leslie Pack Kaelbling"], "emails": ["@cs.brown.edu"], "sections": [{"heading": null, "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}], "references": [{"title": "Optimal control of Markov decision pro\u00ad cesses with incomplete state estimation", "author": ["K.J. Astrom"], "venue": "J. Math. An/. Appl.,", "citeRegEx": "Astrom.,? \\Q1965\\E", "shortCiteRegEx": "Astrom.", "year": 1965}, {"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L.C. Baird"], "venue": "In Machine Learning: Proceedings of the Twelfth International Conference,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Gradient descent for general reinforcement learning", "author": ["L.C. Baird", "A.W. Moore"], "venue": "In Advances in Neu\u00ad ral information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Neuro-dynamic Programming. Athena", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Decision the\u00ad oretic planning: structural assumptions and computa\u00ad tional leverage", "author": ["C. Boutillier", "T.L. Dean", "S. Hanks"], "venue": "Journal of AI Research, To appear,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Exact and Approximate Algorithms for Partially Observable Markov Decision Processes", "author": ["A.R. Cassandra"], "venue": "PhD thesis, Brown University,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "Acting optimally in partially observable stochastic domains", "author": ["A.R. Cassandra", "L.P. Kaelbling", "M.L. Littman"], "venue": "In Proceedings of the Twelfth National Con\u00ad ference on Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1994}, {"title": "An improved policy iteration algorithm for partially observable MDPs", "author": ["E.A. Hansen"], "venue": "In Advances in Neu\u00ad ral Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Finite-Memory Control of Partially Observable Systems", "author": ["E.A. Hansen"], "venue": "PhD thesis, Department of Computer Science, University of Massachusetts at Amherst,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Solving POMDPs by searching in pol\u00ad icy space", "author": ["E.A. Hansen"], "venue": "In Proceedings of the Eighth Conference on Uncertainty in Arti.ficia/Jntelligence,", "citeRegEx": "Hansen.,? \\Q1998\\E", "shortCiteRegEx": "Hansen.", "year": 1998}, {"title": "Planning and Control in Stochas\u00ad tic Domains with Imperfect Information", "author": ["M. Hauskrecht"], "venue": "PhD thesis,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "Optimal Control of Complex Structured Processes", "author": ["0. Higelin"], "venue": "PhD thesis, University of Caen, France,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Dynamic Programming and Markov Processes", "author": ["R.A. Howard"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1960}, {"title": "Rein\u00ad forcement learning algorithm for partially observable Markov problems", "author": ["T. Jaakkola", "S. Singh", "M.R. Jordan"], "venue": "In Advances in Neural Informa\u00ad tion Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Searching for finite-state POMDP controllers", "author": ["L.P. Kaelbling", "K.E. Kim", "N. Meuleau", "L. Peshkin"], "venue": "Technical Report CS-99-06,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Artificial intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1998}, {"title": "Re\u00ad inforcement learning: a survey", "author": ["L.P. Kaelbling", "M.L. Littman", "A.W. Moore"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}, {"title": "Memory less policies: Theoretica1 lim\u00ad itations and practical results", "author": ["M.L. Littman"], "venue": "In From Animals to Animats 3: Proceedings of the T hird International Conference on Simulation of Adaptive Behavior", "citeRegEx": "Littman.,? \\Q1994\\E", "shortCiteRegEx": "Littman.", "year": 1994}, {"title": "Overcoming incomplete perception with utile distinction memory", "author": ["R.A. McCallum"], "venue": "In The Proceedings of the Tenth International Machine Learning Confer\u00ad ence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1993}, {"title": "Reinforcement Learning with Selec\u00ad tive Perception and Hidden State", "author": ["R.A. McCallum"], "venue": "PhD thesis, Univer\u00ad sity of Rochester,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1995}, {"title": "The importance of impossible trajecto\u00ad ries in the VAPS algorithm", "author": ["N. Meuleau"], "venue": "In preparation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1999}, {"title": "Cas\u00ad sandra. Solving POMDPs by searching the space of finite policies", "author": ["N. Meuleau", "K.E. Kim", "L.P. Kaelbling", "A.R"], "venue": "Proceedings of the Fifteenth Confer\u00ad ence on Uncertainty in Artificial Intelligence, To ap\u00ad pear,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1999}, {"title": "Learn\u00ad ing policies with external memory", "author": ["L. Peshkin", "N. Meuleau", "L.P. Kaelbling"], "venue": "Proceedings of the Sixteenth International Conference on Machine Learning, To appear,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1999}, {"title": "Markov Decision Processes: Dis\u00ad crete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1994}, {"title": "Lave. Markov decision processes with probabilistic observation of states", "author": ["R.E.J.K. Satia"], "venue": "Management Science,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1973}, {"title": "Learn\u00ad ing without state-estimation in partially observable Markovian decision processes", "author": ["S. Singh", "T. Jaakkola", "M.R. Jordan"], "venue": "In Machine Learn\u00ad ing: Proceedings of the Eleventh International Con\u00ad ference", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}, {"title": "Sondik. The optimal con\u00ad trol of partially observable Markov decision processes over a finite horizon", "author": ["E.J.R.D. Smallwood"], "venue": "Operations Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1973}, {"title": "The optimal control of partially observ\u00ad able Markov decision processes over the infinite hori\u00ad zon: Discounted costs", "author": ["E.J. Sondik"], "venue": "Operations Research,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1978}, {"title": "Learning to predict by the method of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1989}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 1998}, {"title": "Towards a theory of reinforcement\u00ad learning connectionist systems", "author": ["R.J. Williams"], "venue": "Technical Re\u00ad port NU-CCS-88-3,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1988}], "referenceMentions": [], "year": 2011, "abstractText": "Reactive (memoryless) policies are sufficient in completely observable Markov decision pro\u00ad cesses (MDPs), but some kind of memory is usually necessary for optimal control of a par\u00ad tially observable MDP. Policies with finite mem\u00ad ory can be represented as finite-state automata. In this paper, we extend Baird and Moore's YAPS algorithm to the problem of learning gen\u00ad eral finite-state automata. Because it performs stochastic gradient descent, this algorithm can be shown to converge to a locally optimal finite\u00ad state controller. We provide the details of the algorithm and then consider the question of un\u00ad der what conditions stochastic gradient descent will outperform exact gradient descent. We con\u00ad clude with empirical results comparing the per\u00ad formance of stochastic and exact gradient de\u00ad scent, and showing the ability of our algorithm to extract the useful information contained in the sequence of past observations to compensate for the lack of observability at each time-step.", "creator": "pdftk 1.41 - www.pdftk.com"}}}