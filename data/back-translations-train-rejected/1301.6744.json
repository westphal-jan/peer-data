{"id": "1301.6744", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "Mixture Approximations to Bayesian Networks", "abstract": "Structure and parameters in a Bayesian network uniquely specify the probability distribution of the modeled domain. The locality of both structure and probabilistic information are the great benefits of Bayesian networks and require the modeler to only specify local information. On the other hand this locality of information might prevent the modeler - and even more any other person - from obtaining a general overview of the important relationships within the domain. The goal of the work presented in this paper is to provide an \"alternative\" view on the knowledge encoded in a Bayesian network which might sometimes be very helpful for providing insights into the underlying domain. The basic idea is to calculate a mixture approximation to the probability distribution represented by the Bayesian network. The mixture component densities can be thought of as representing typical scenarios implied by the Bayesian model, providing intuition about the basic relationships. As an additional benefit, performing inference in the approximate model is very simple and intuitive and can provide additional insights. The computational complexity for the calculation of the mixture approximations criticaly depends on the measure which defines the distance between the probability distribution represented by the Bayesian network and the approximate distribution. Both the KL-divergence and the backward KL-divergence lead to inefficient algorithms. Incidentally, the latter is used in recent work on mixtures of mean field solutions to which the work presented here is closely related. We show, however, that using a mean squared error cost function leads to update equations which can be solved using the junction tree algorithm. We conclude that the mean squared error cost function can be used for Bayesian networks in which inference based on the junction tree is tractable. For large networks, however, one may have to rely on mean field approximations.", "histories": [["v1", "Wed, 23 Jan 2013 16:01:18 GMT  (280kb)", "http://arxiv.org/abs/1301.6744v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["volker tresp", "michael haft", "reimar hofmann"], "accepted": false, "id": "1301.6744"}, "pdf": {"name": "1301.6744.pdf", "metadata": {"source": "CRF", "title": "Mixture Approximations to Bayesian Networks", "authors": ["Volker Tresp", "Michael Haft", "Reimar Hofmann"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them are able to play by the rules."}], "references": [{"title": "Approximating posterior distribu\u00ad tions in belief networks using mixtures", "author": ["C.M. Bishop", "N.D. Lawrence", "T. Jaakkola", "M.I. Jordan"], "venue": "Advances in Neural Information Processing Sys\u00ad", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Using scenarios to explain probabilistic inference", "author": ["M.J. Druzdzel", "M. Henrion"], "venue": "American Asso\u00ad ciation for Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1990}, {"title": "Model\u00ad independent mean-field theory as a local method for approximate propagation of information", "author": ["H. Haft", "R. Hofmann", "V. Tresp"], "venue": "Net\u00ad work,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1999}, {"title": "Model-independent mean field theory as a local method for approx\u00ad imate propagation of information", "author": ["M. Haft", "R. Hofmann", "V. Tresp"], "venue": "Technical Re\u00ad port,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Bayesian updating in causal probabilistic net\u00ad works by local computations", "author": ["F.V. Jensen", "S.L. Lauritzen", "K.G. Olsen"], "venue": "Computational Statistics Quaterly,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1990}, {"title": "Mean field ap\u00ad proach to learning in boltzmann machines", "author": ["H.J. Kappen", "F.B. Rodriguez"], "venue": "Pat\u00ad tern Recogntion Letters,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Lo\u00ad cal computations with probabilities on graphical structures and their application to expert sys\u00ad tems", "author": ["S.L. Lauritzen", "D.J. Spiegelhalter"], "venue": "J. Roy. Statist. Soc. B,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1988}, {"title": "Mixture representations for inference and learning in boltzmann machines", "author": ["N.D. Lawrence", "C.M. Bishop", "M.I. Jor\u00ad dan"], "venue": "Uncertainty in Artificial Intelligence: Proceedings of the Fourteenth Con\u00ad ference,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "A mean field the\u00ad ory learning algorithm for neural networks", "author": ["C. Peterson", "J.R. Anderson"], "venue": "Com\u00ad plex Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1987}, {"title": "Mean field theory for sigmoid belief networks", "author": ["K.L. Saul", "T. Jaakkola", "M.I. Jordan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1996}], "referenceMentions": [], "year": 2011, "abstractText": "Structure and parameters in a Bayesian net\u00ad work uniquely specify the probability distri\u00ad bution of the modeled domain. The locality of both structure and probabilistic informa\u00ad tion are the great benefits of Bayesian net\u00ad works and require the modeler to only spec\u00ad ify local information. On the other hand this locality of information might prevent the modeler -and even more any other person\u00ad from obtaining a general overview of the im\u00ad portant relationships within the domain. The goal of the work presented in this paper is to provide an \"alternative\" view on the knowl\u00ad edge encoded in a Bayesian network which might sometimes be very helpful for pro\u00ad viding insights into the underlying domain. The basic idea is to calculate a mixture ap\u00ad proximation to the probability distribution represented by the Bayesian network. The mixture component densities can be thought of as representing typical scenarios implied by the Bayesian model, providing intuition about the basic relationships. As an addi\u00ad tional benefit, performing inference in the approximate model is very simple and intu\u00ad itive and can provide additional insights. The computational complexity for the calculation of the mixture approximations critically de\u00ad pends on the measure which defines the dis\u00ad tance between the probability distribution represented by the Bayesian network and the approximate distribution. Both the KL\u00ad divergence and the backward KL-divergence lead to inefficient algorithms. Incidentally, the latter is used in recent work on mixtures of mean field solutions to which the work pre\u00ad sented here is closely related. We show, how\u00ad ever, that using a mean squared error cost function leads to update equations which can be solved using the junction tree algorithm. We conclude that the mean squared error cost function can be used for Bayesian net\u00ad works in which inference based on the junc\u00ad tion tree is tractable. For large networks, however, one may have to rely on mean field approximations.", "creator": "pdftk 1.41 - www.pdftk.com"}}}