{"id": "1301.6751", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jan-2013", "title": "A Method for Speeding Up Value Iteration in Partially Observable Markov Decision Processes", "abstract": "We present a technique for speeding up the convergence of value iteration for partially observable Markov decisions processes (POMDPs). The underlying idea is similar to that behind modified policy iteration for fully observable Markov decision processes (MDPs). The technique can be easily incorporated into any existing POMDP value iteration algorithms. Experiments have been conducted on several test problems with one POMDP value iteration algorithm called incremental pruning. We find that the technique can make incremental pruning run several orders of magnitude faster.", "histories": [["v1", "Wed, 23 Jan 2013 16:01:45 GMT  (292kb)", "http://arxiv.org/abs/1301.6751v1", "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)"]], "COMMENTS": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI1999)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["nevin lianwen zhang", "stephen s lee", "weihong zhang"], "accepted": false, "id": "1301.6751"}, "pdf": {"name": "1301.6751.pdf", "metadata": {"source": "CRF", "title": "A Method for Speeding Up Value Iteration in Partially Observable Markov Decision Processes", "authors": ["Nevin L. Zhang", "Stephen S. Lee", "Weihong Zhang"], "emails": [], "sections": [{"heading": null, "text": "The underlying idea is similar to what lies behind the modified political iteration called the cremental pruning method. We find that the technology can incorporate incremental pruning into all existing POMDP values, which exist in all possible orders of magnitude faster.1 in the order of magnitude faster1. Experiments have been conducted on several test problems with a POMDP value iteration algorithm called in cremental pruning processes. We find that the tech niche incremental pruning effects are orders of magnitude faster1 in the order faster.1, which is a model for sequential decisions in which the effects of actions are not deterministic. They have attracted many researchers in operations research and AI because of their potential applications in a number of areas (Monahan 1982, Cas 1998b). However, there is still a significant gap between this potential and the actual applications that are effective."}], "references": [{"title": "Exact and approximate al\u00ad gorithms for partially observable Markov decision processes", "author": ["A.R. Cassandra"], "venue": "PhD thesis,", "citeRegEx": "Cassandra,? \\Q1998\\E", "shortCiteRegEx": "Cassandra", "year": 1998}, {"title": "A survey of POMDP ap\u00ad plications, in Working Notes of AAAI", "author": ["A.R. Cassandra"], "venue": "Jggs Fall Symposium on Planning with Partially Obseroable Markov Decision Processes,", "citeRegEx": "Cassandra,? \\Q1998\\E", "shortCiteRegEx": "Cassandra", "year": 1998}, {"title": "Finite-memory control of par\u00ad tially obseroable systems, Ph.D", "author": ["E.A. Hansen"], "venue": null, "citeRegEx": "Hansen,? \\Q1998\\E", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "A survey of partially observ\u00ad", "author": ["G.E. Monahan"], "venue": null, "citeRegEx": "Monahan,? \\Q1982\\E", "shortCiteRegEx": "Monahan", "year": 1982}, {"title": "Action elim\u00ad", "author": ["M.L. Puterman", "M.C. Shin"], "venue": null, "citeRegEx": "Puterman and Shin,? \\Q1982\\E", "shortCiteRegEx": "Puterman and Shin", "year": 1982}, {"title": "The optimal control of partially", "author": ["E.J. Sondik"], "venue": null, "citeRegEx": "Sondik,? \\Q1971\\E", "shortCiteRegEx": "Sondik", "year": 1971}, {"title": "The optimal control of partially", "author": ["E.J. Sondik"], "venue": null, "citeRegEx": "Sondik,? \\Q1978\\E", "shortCiteRegEx": "Sondik", "year": 1978}, {"title": "A set of successive ap\u00ad", "author": ["van Nunen", "J.A.E. E"], "venue": null, "citeRegEx": "Nunen and E.,? \\Q1976\\E", "shortCiteRegEx": "Nunen and E.", "year": 1976}, {"title": "Partially observed Markov decision processes: A survey", "author": ["III C.C. White"], "venue": "Annals of Opera\u00ad tions Research,", "citeRegEx": "White,? \\Q1991\\E", "shortCiteRegEx": "White", "year": 1991}, {"title": "A model approxi\u00ad", "author": ["N.L. Zhang", "W. Liu"], "venue": null, "citeRegEx": "Zhang and Liu,? \\Q1997\\E", "shortCiteRegEx": "Zhang and Liu", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "See Cassandra (1998a) for excellent descriptions, analyses, and empirical compar\u00ad isons of those algorithms.", "startOffset": 4, "endOffset": 22}, {"referenceID": 4, "context": "The first one is proposed by Sondik (1978). A simpler one is recently developed by Hansen (1998).", "startOffset": 29, "endOffset": 43}, {"referenceID": 2, "context": "A simpler one is recently developed by Hansen (1998).", "startOffset": 39, "endOffset": 53}, {"referenceID": 4, "context": "Numerical results reported in Puterman and Shin (1978) suggest that modified pol\u00ad icy iteration is more efficient than either value iteration or policy iteration in practice.", "startOffset": 30, "endOffset": 55}, {"referenceID": 8, "context": "One way to doing so is to simply apply Lark's algorithm (White 1991).", "startOffset": 56, "endOffset": 68}], "year": 2012, "abstractText": "We present a technique for speeding up the convergence of value iteration for par\u00ad tially observable Markov decisions processes (POMDPs). The underlying idea is similar to that behind modified policy iteration for fully observable Markov decision processes (MDPs). The technique can be easily incor\u00ad porated into any existing POMDP value it\u00ad eration algorithms. Experiments have been conducted on several test problems with one POMDP value iteration algorithm called in\u00ad cremental pruning. We find that the tech\u00ad nique can make incremental pruning run sev\u00ad eral orders of magnitude faster.", "creator": "Adobe Acrobat 7.0"}}}