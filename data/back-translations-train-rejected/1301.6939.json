{"id": "1301.6939", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jan-2013", "title": "Multi-Step Regression Learning for Compositional Distributional Semantics", "abstract": "We present a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it on two benchmark data sets, and find it to outperform existing leading methods. We argue in our analysis that the nature of this learning method renders it suitable also for solving more subtle problems compositional distributional models might face.", "histories": [["v1", "Tue, 29 Jan 2013 14:59:34 GMT  (58kb,D)", "https://arxiv.org/abs/1301.6939v1", "10 pages + 1 page references, to be presented at the 10th International Conference on Computational Semantics (IWCS 2013)"], ["v2", "Wed, 30 Jan 2013 12:01:23 GMT  (58kb,D)", "http://arxiv.org/abs/1301.6939v2", "10 pages + 1 page references, to be presented at the 10th International Conference on Computational Semantics (IWCS 2013)"]], "COMMENTS": "10 pages + 1 page references, to be presented at the 10th International Conference on Computational Semantics (IWCS 2013)", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["edward grefenstette", "georgiana dinu", "yao-zhong zhang", "mehrnoosh sadrzadeh", "marco baroni"], "accepted": false, "id": "1301.6939"}, "pdf": {"name": "1301.6939.pdf", "metadata": {"source": "CRF", "title": "Multi-Step Regression Learning for Compositional Distributional Semantics", "authors": ["E. Grefenstette", "G. Dinu", "Y. Zhang", "M. Sadrzadeh", "M. Baroni"], "emails": [], "sections": [{"heading": null, "text": "We are introducing a new learning method for tensors by generalizing the approach of Baroni and Zamparelli (2010). We evaluate it using two benchmark data sets and find that it outperforms existing best practices. In our analysis, we argue that this learning method is also capable of solving more subtle problems that compositional distribution models might face."}, {"heading": "1 Introduction", "text": "The staggering amount of texts that can be found on the Internet today has fueled the search for more subtle and complex representations of language and the methods for learning such models. Two well-researched but imperfect approaches to this problem are the logical models, each of which stands alone. This allows us to realize Frege's views (1892) - that the semantic content is of a logical form - by defining a systematic passage in the composition of parts of logic."}, {"heading": "2 Related work", "text": "This year it has come to the point that it is a purely reactionary, reactionary, reactionary and reactionary project."}, {"heading": "3 A general framework for distributional function application", "text": "A popular approach to compositivity in formal semantics is to derive a formal representation of a phrase from its grammatical structure by presenting the semantics of words as functions and arguments, and to use the grammatical structure to dictate the order and scope of functional application. For example, formal semantic models in the style of Montague (1970) associate a semantic rule with each syntactic rule in a context-free grammar space. An example of formal semantic model is shown here: Syntax Semantics S \u21d2 VP \u21d2 [V P]] ([NP]) NP \u21d2 n \u21d2 \u21d2 P \u21d2) \u21d2 VP \u21d2 VP \u21d2 Vi [V] \u21d2."}, {"heading": "4 Learning functions by multi-step regression", "text": "This year it is more than ever before."}, {"heading": "5 Experimental procedure", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Construction of distributional semantic vectors", "text": "We collect the most frequent nouns and 4K verbs in the corpus, as well as for the subject verb (320K) and subject objects (363K). We collect the most frequent nouns and 4K verbs in the corpus. We collect the most frequent nouns and 4K verbs in the corpus, as well as the subject verb (320K) and subject generalizations of the subject generalizations of the subject generalizations of the subject generalizations of the subject generalizations. We collect the most frequent nouns and 4K verbs in the corpus, as well as the subject generalizations of the subject generalizations of the subject generalizations of the subject generalizations of the subject generalizations."}, {"heading": "5.2 Composition methods", "text": "It is a baseline that measures the cosine between verbs in two sentences as a proxy for sentence similarity (e.g. similarity of moms and youth dances). (For example, similarity of moms and youth dances is approached here by the cosinery of singing and dancing). We take the widely used and generally successful multiplicative and additive models of Mitchell and Lapata (2010) and others. The composition of the multiple and additive methods is multiplied and added by the respective component ways when the vectors of the sentence we wish to represent are standardized, as has been consistently demonstrated to improve performance in our earlier experiments.Grefenstette and Sadrzadeh (2011b) have a specific implementation of the general DisCoCat approach to compositional distribution semantics (Coecke et al, 2010), which we call Kronecker. 1We have the experiments that were possible to multiply below in full space for that."}, {"heading": "6 Experiment 1: Predicting similarity judgments on intransitive sentences", "text": "We use the Mitchell and Lapata test theorem (2008), consisting of 180 pairs of simple sentences consisting of a subject and an intransitive verb; the stimuli were constructed so that there would be pairs in which the sentences were very similar (the fire was burning against the fire) and cases in which the sentences were unequal while exhibiting a similar degree of lexical overlap (the face was glowing against the burned face); the sentence pairs were evaluated by 49 subjects on a scale of 1-7. Following Mitchell and Lapata, we evaluate each composition method based on the spearman correlation of the cosines of the sentence pair vectors, as predicted by the method, with the individual evaluations of the subjects for the corresponding sentence pairs; the results in Table 1 (a) show that the regression model achieves the best correlation when applied to the SVD space, and confirm that the Baroni and Zadan approach to constructive can be successful."}, {"heading": "7 Experiment 2: Predicting similarity judgments on transitive sentences", "text": "We use the test theorem of Grefenstette and Sadrzadeh (2011a), which was constructed according to the same criteria used by Mitchell and Lapata, but in this case the theorem has a simple transitive structure. An example of a pair with a high similarity is table shows result against table expresses result; while map shows location against map expresses a pair with little similarity. Grefenstette and Sadrzadeh had 25 subjects evaluating each theorem. Model evaluation proceeds as in the intransitive case.5As the results in Table 1 (b) show, the regression model again performs very well, better than any other method in the NMF space, and with a further improvement in SVD, similar to the first experiment. The Kronecker model is also competitive and confirms the results of the experiments of Grefenstette and Sadrzadeh. Neither add nor verb achieve very good results, although even for them the correlation with human evaluations is important."}, {"heading": "8 General discussion of the results", "text": "It is only a matter of time before that happens, that it happens."}, {"heading": "9 Conclusion", "text": "The most important advances presented in this paper are the following: First, we discussed a tensor-based compositional semantic framework in the style of Coecke et al. (2010), which presents the compositional mechanism of Baroni and Zamparelli (2010) as a specific case, bringing the two lines of research together in a common framework; second, we presented a generalization of Baroni and Zamparelli's matrix learning method to higher-level tensors, which allows us to induce the semantic representation of functions modelled within this framework; and finally, we evaluated this new semantic tensor learning model using existing benchmark data sets provided by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh, showing that it surpasses other models; and we maintain that the universality of our advanced regression method allows us to gather more information than the multiplicative and crown-sectional plans, and that we want to improve directly."}, {"heading": "Acknowledgments", "text": "Edward Grefenstette is supported by the EPSRC Project A Unified Model of Compositional and Distributional Semantics: Theory and Applications (EP / I03808X / 1), Georgiana Dinu and Marco Baroni are partially supported by the ERC 2011 Starting Independent Research Grant to the COMPOSES project (No. 283554), Mehrnoosh Sadrzadeh is supported by an EPSRC Career Acceleration Fellowship (EP / J002607 / 1)."}], "references": [{"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli"], "venue": "Proceedings of EMNLP, Boston, MA, pp. 1183\u20131193.", "citeRegEx": "Baroni and Zamparelli,? 2010", "shortCiteRegEx": "Baroni and Zamparelli", "year": 2010}, {"title": "Commutative Algebra: Chapters 1-7", "author": ["N. Bourbaki"], "venue": "Springer-Verlag (Berlin and New York).", "citeRegEx": "Bourbaki,? 1989", "shortCiteRegEx": "Bourbaki", "year": 1989}, {"title": "Extracting semantic representations from word co-occurrence statistics: Stop-lists, stemming and SVD", "author": ["J. Bullinaria", "J. Levy"], "venue": "Behavior Research Methods 44, 890\u2013907.", "citeRegEx": "Bullinaria and Levy,? 2012", "shortCiteRegEx": "Bullinaria and Levy", "year": 2012}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["B. Coecke", "M. Sadrzadeh", "S. Clark"], "venue": "Linguistic Analysis 36, 345\u2013384.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "Measuring distributional similarity in context", "author": ["G. Dinu", "M. Lapata"], "venue": "Proceedings of EMNLP, Cambridge, MA, pp. 1162\u20131172.", "citeRegEx": "Dinu and Lapata,? 2010", "shortCiteRegEx": "Dinu and Lapata", "year": 2010}, {"title": "Data-driven approaches to information access", "author": ["S. Dumais"], "venue": "Cognitive Science 27, 491\u2013524.", "citeRegEx": "Dumais,? 2003", "shortCiteRegEx": "Dumais", "year": 2003}, {"title": "A structured vector space model for word meaning in context", "author": ["K. Erk", "S. Pad\u00f3"], "venue": "Proceedings of EMNLP, Honolulu, HI, USA, pp. 897\u2013906.", "citeRegEx": "Erk and Pad\u00f3,? 2008", "shortCiteRegEx": "Erk and Pad\u00f3", "year": 2008}, {"title": "Papers in linguistics, 1934-1951", "author": ["J. Firth"], "venue": "Oxford University Press.", "citeRegEx": "Firth,? 1957", "shortCiteRegEx": "Firth", "year": 1957}, {"title": "\u00dcber Sinn und Bedeutung", "author": ["G. Frege"], "venue": "Zeitschrift fuer Philosophie un philosophische Kritik 100, 25\u201350.", "citeRegEx": "Frege,? 1892", "shortCiteRegEx": "Frege", "year": 1892}, {"title": "Generalized cross-validation as a method for choosing a good Ridge parameter", "author": ["G. Golub", "M. Heath", "G. Wahba"], "venue": "Technometrics 21, 215\u2013223.", "citeRegEx": "Golub et al\\.,? 1979", "shortCiteRegEx": "Golub et al\\.", "year": 1979}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["E. Grefenstette", "M. Sadrzadeh"], "venue": "Proceedings of EMNLP, Edinburgh, UK, pp. 1394\u20131404.", "citeRegEx": "Grefenstette and Sadrzadeh,? 2011a", "shortCiteRegEx": "Grefenstette and Sadrzadeh", "year": 2011}, {"title": "Experimenting with transitive verbs in a DisCoCat", "author": ["E. Grefenstette", "M. Sadrzadeh"], "venue": "Proceedings of GEMS, Edinburgh, UK, pp. 62\u201366.", "citeRegEx": "Grefenstette and Sadrzadeh,? 2011b", "shortCiteRegEx": "Grefenstette and Sadrzadeh", "year": 2011}, {"title": "Concrete sentence spaces for compositional distributional models of meaning", "author": ["E. Grefenstette", "M. Sadrzadeh", "S. Clark", "B. Coecke", "S. Pulman"], "venue": "Proceedings of IWCS, pp. 125\u2013134.", "citeRegEx": "Grefenstette et al\\.,? 2011", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "Explorations in Automatic Thesaurus Discovery", "author": ["G. Grefenstette"], "venue": "Boston, MA: Kluwer.", "citeRegEx": "Grefenstette,? 1994", "shortCiteRegEx": "Grefenstette", "year": 1994}, {"title": "A regression model of adjective-noun compositionality in distributional semantics", "author": ["E. Guevara"], "venue": "Proceedings of the ACL GEMS Workshop, Uppsala, Sweden, pp. 33\u201337.", "citeRegEx": "Guevara,? 2010", "shortCiteRegEx": "Guevara", "year": 2010}, {"title": "The Elements of Statistical Learning, 2nd ed", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": "New York: Springer.", "citeRegEx": "Hastie et al\\.,? 2009", "shortCiteRegEx": "Hastie et al\\.", "year": 2009}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T. Landauer", "S. Dumais"], "venue": "Psychological Review 104(2), 211\u2013240.", "citeRegEx": "Landauer and Dumais,? 1997", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "Riemannian manifolds: An introduction to curvature, Volume 176", "author": ["J. Lee"], "venue": "Springer Verlag.", "citeRegEx": "Lee,? 1997", "shortCiteRegEx": "Lee", "year": 1997}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["Lin", "C.-J."], "venue": "Neural Computation 19(10), 2756\u20132779.", "citeRegEx": "Lin and C..J.,? 2007", "shortCiteRegEx": "Lin and C..J.", "year": 2007}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata"], "venue": "Proceedings of ACL, Columbus, OH, pp. 236\u2013244.", "citeRegEx": "Mitchell and Lapata,? 2008", "shortCiteRegEx": "Mitchell and Lapata", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["J. Mitchell", "M. Lapata"], "venue": "Cognitive Science 34(8), 1388\u20131429.", "citeRegEx": "Mitchell and Lapata,? 2010", "shortCiteRegEx": "Mitchell and Lapata", "year": 2010}, {"title": "English as a formal language", "author": ["R. Montague"], "venue": "Linguaggi nella societ\u00e0 e nella tecnica, 189\u2013224.", "citeRegEx": "Montague,? 1970", "shortCiteRegEx": "Montague", "year": 1970}, {"title": "Compositionality in Formal Semantics", "author": ["B. Partee"], "venue": "Malden, MA: Blackwell.", "citeRegEx": "Partee,? 2004", "shortCiteRegEx": "Partee", "year": 2004}, {"title": "The Word-Space Model", "author": ["M. Sahlgren"], "venue": "Dissertation, Stockholm University.", "citeRegEx": "Sahlgren,? 2006", "shortCiteRegEx": "Sahlgren", "year": 2006}, {"title": "Ambiguity Resolution in Natural Language Learning", "author": ["H. Sch\u00fctze"], "venue": "Stanford, CA: CSLI.", "citeRegEx": "Sch\u00fctze,? 1997", "shortCiteRegEx": "Sch\u00fctze", "year": 1997}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C. Manning", "A. Ng"], "venue": "Proceedings of EMNLP, Jeju Island, Korea, pp. 1201\u20131211.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Word meaning in context: A simple and effective vector model", "author": ["S. Thater", "H. F\u00fcrstenau", "M. Pinkal"], "venue": "Proceedings of IJCNLP, Chiang Mai, Thailand, pp. 1134\u20131143.", "citeRegEx": "Thater et al\\.,? 2011", "shortCiteRegEx": "Thater et al\\.", "year": 2011}, {"title": "Philosophical Investigations", "author": ["L. Wittgenstein"], "venue": "Oxford: Blackwell. Translated by G.E.M. Anscombe.", "citeRegEx": "Wittgenstein,? 1953", "shortCiteRegEx": "Wittgenstein", "year": 1953}], "referenceMentions": [{"referenceID": 2, "context": "We present a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors.", "startOffset": 90, "endOffset": 111}, {"referenceID": 0, "context": "We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it on two benchmark data sets, and find it to outperform existing leading methods.", "startOffset": 77, "endOffset": 106}, {"referenceID": 21, "context": "This allows us to derive the logical form a of sentence from its syntactic structure (Montague, 1970).", "startOffset": 85, "endOffset": 101}, {"referenceID": 8, "context": "Formal semantic models generally implement the view of Frege (1892)\u2014that the semantic content of an expression is its logical form\u2014by defining a systematic passage from syntactic rules to the composition of parts of logical expressions.", "startOffset": 55, "endOffset": 68}, {"referenceID": 13, "context": "Such models have been successfully applied to various tasks such as thesaurus extraction (Grefenstette, 1994) and essay grading (Landauer and Dumais, 1997; Dumais, 2003).", "startOffset": 89, "endOffset": 109}, {"referenceID": 16, "context": "Such models have been successfully applied to various tasks such as thesaurus extraction (Grefenstette, 1994) and essay grading (Landauer and Dumais, 1997; Dumais, 2003).", "startOffset": 128, "endOffset": 169}, {"referenceID": 5, "context": "Such models have been successfully applied to various tasks such as thesaurus extraction (Grefenstette, 1994) and essay grading (Landauer and Dumais, 1997; Dumais, 2003).", "startOffset": 128, "endOffset": 169}, {"referenceID": 4, "context": "In contrast, distributional semantic models, suggested by Firth (1957), implement the linguistic philosophy of Wittgenstein (1953) stating that meaning is associated with use, and therefore meaning can be learned through the observation of linguistic practises.", "startOffset": 58, "endOffset": 71}, {"referenceID": 4, "context": "In contrast, distributional semantic models, suggested by Firth (1957), implement the linguistic philosophy of Wittgenstein (1953) stating that meaning is associated with use, and therefore meaning can be learned through the observation of linguistic practises.", "startOffset": 58, "endOffset": 131}, {"referenceID": 0, "context": "In this paper, we present a new approach to the development of compositional distributional semantic models, based on earlier work by Baroni and Zamparelli (2010), Coecke et al.", "startOffset": 134, "endOffset": 163}, {"referenceID": 0, "context": "In this paper, we present a new approach to the development of compositional distributional semantic models, based on earlier work by Baroni and Zamparelli (2010), Coecke et al. (2010) and Grefenstette et al.", "startOffset": 134, "endOffset": 185}, {"referenceID": 0, "context": "In this paper, we present a new approach to the development of compositional distributional semantic models, based on earlier work by Baroni and Zamparelli (2010), Coecke et al. (2010) and Grefenstette et al. (2011), combining features from the compositional distributional framework of the latter two with the learning methods of the former.", "startOffset": 134, "endOffset": 216}, {"referenceID": 8, "context": "In formal semantics, composition has always been modeled in terms of function application, treating certain words as functions that operate on other words to construct meaning incrementally according to a calculus of composition that reflects the syntactic structure of sentences (Frege, 1892; Montague, 1970; Partee, 2004).", "startOffset": 280, "endOffset": 323}, {"referenceID": 21, "context": "In formal semantics, composition has always been modeled in terms of function application, treating certain words as functions that operate on other words to construct meaning incrementally according to a calculus of composition that reflects the syntactic structure of sentences (Frege, 1892; Montague, 1970; Partee, 2004).", "startOffset": 280, "endOffset": 323}, {"referenceID": 22, "context": "In formal semantics, composition has always been modeled in terms of function application, treating certain words as functions that operate on other words to construct meaning incrementally according to a calculus of composition that reflects the syntactic structure of sentences (Frege, 1892; Montague, 1970; Partee, 2004).", "startOffset": 280, "endOffset": 323}, {"referenceID": 3, "context": "Coecke et al. (2010) have proposed a general formalism for composition in distributional semantics that captures the same notion of function application.", "startOffset": 0, "endOffset": 21}, {"referenceID": 3, "context": "Coecke et al. (2010) have proposed a general formalism for composition in distributional semantics that captures the same notion of function application. Empirical implementations of Coecke\u2019s et al.\u2019s formalism have been developed by Grefenstette et al. (2011) and tested by Grefenstette and Sadrzadeh (2011a,b).", "startOffset": 0, "endOffset": 261}, {"referenceID": 0, "context": "Baroni and Zamparelli (2010) propose a different approach to function application in distributional space, that they apply to adjective-noun composition (see also Guevara (2010) for similar ideas).", "startOffset": 0, "endOffset": 29}, {"referenceID": 0, "context": "Baroni and Zamparelli (2010) propose a different approach to function application in distributional space, that they apply to adjective-noun composition (see also Guevara (2010) for similar ideas).", "startOffset": 0, "endOffset": 178}, {"referenceID": 6, "context": "Several studies tackle word meaning in context, that is, how to adapt the distributional representation of a word to the specific context in which it appears (e.g., Dinu and Lapata, 2010; Erk and Pad\u00f3, 2008; Thater et al., 2011).", "startOffset": 158, "endOffset": 228}, {"referenceID": 26, "context": "Several studies tackle word meaning in context, that is, how to adapt the distributional representation of a word to the specific context in which it appears (e.g., Dinu and Lapata, 2010; Erk and Pad\u00f3, 2008; Thater et al., 2011).", "startOffset": 158, "endOffset": 228}, {"referenceID": 21, "context": "For example, formal semantic models in the style of Montague (1970) will associate a semantic rule to each syntactic rule in a context-free grammar.", "startOffset": 52, "endOffset": 68}, {"referenceID": 1, "context": "In the case of multilinear maps, this correspondence generalises to a correlation between n-ary maps and rank n + 1 tensors (Bourbaki, 1989; Lee, 1997).", "startOffset": 124, "endOffset": 151}, {"referenceID": 17, "context": "In the case of multilinear maps, this correspondence generalises to a correlation between n-ary maps and rank n + 1 tensors (Bourbaki, 1989; Lee, 1997).", "startOffset": 124, "endOffset": 151}, {"referenceID": 1, "context": "\u2297 A encoding it (Bourbaki, 1989; Lee, 1997).", "startOffset": 16, "endOffset": 43}, {"referenceID": 17, "context": "\u2297 A encoding it (Bourbaki, 1989; Lee, 1997).", "startOffset": 16, "endOffset": 43}, {"referenceID": 0, "context": "Multi-step regression learning is a generalisation of linear regression learning for tensors of rank 3 or higher, as procedures already exist for tensors of rank 1 (lexical semantic vectors) and rank 2 (Baroni and Zamparelli, 2010).", "startOffset": 202, "endOffset": 231}, {"referenceID": 2, "context": "This, very much like in the case of the DisCoCat framework of Coecke et al. (2010) from which it originated, is intentional: There may be more than one suitable semantic representation for arguments, functions, and sentences, and it is a desirable feature that we may alternate between such representations or combine them while leaving the mechanics of function composition intact.", "startOffset": 62, "endOffset": 83}, {"referenceID": 0, "context": "Previous work on learning tensors has been described independently by Grefenstette and Sadrzadeh (2011a,b) for transitive verbs, and by Baroni and Zamparelli (2010) for adjective-noun constructions.", "startOffset": 136, "endOffset": 165}, {"referenceID": 0, "context": "The idea is to progressively learn the functions of arity two or higher encoded by such tensors by recursively learning the partial application of these functions, thereby reducing the problem to the same matrix-learning problem as addressed by Baroni and Zamparelli. To start with an example: the matrix-by-vector operation of Baroni and Zamparelli (2010) is a special case of the general tensor-based function application model we are proposing, where a \u2018mono-argumental\u2019 function (intransitive verbs) corresponds to a rank 2 tensor (a matrix).", "startOffset": 245, "endOffset": 357}, {"referenceID": 2, "context": "Extensive evidence suggests that dimensionality reduction does not affect, and might even improve the quality of lexical semantic vectors (Bullinaria and Levy, 2012; Landauer and Dumais, 1997; Sahlgren, 2006; Sch\u00fctze, 1997).", "startOffset": 138, "endOffset": 223}, {"referenceID": 16, "context": "Extensive evidence suggests that dimensionality reduction does not affect, and might even improve the quality of lexical semantic vectors (Bullinaria and Levy, 2012; Landauer and Dumais, 1997; Sahlgren, 2006; Sch\u00fctze, 1997).", "startOffset": 138, "endOffset": 223}, {"referenceID": 23, "context": "Extensive evidence suggests that dimensionality reduction does not affect, and might even improve the quality of lexical semantic vectors (Bullinaria and Levy, 2012; Landauer and Dumais, 1997; Sahlgren, 2006; Sch\u00fctze, 1997).", "startOffset": 138, "endOffset": 223}, {"referenceID": 24, "context": "Extensive evidence suggests that dimensionality reduction does not affect, and might even improve the quality of lexical semantic vectors (Bullinaria and Levy, 2012; Landauer and Dumais, 1997; Sahlgren, 2006; Sch\u00fctze, 1997).", "startOffset": 138, "endOffset": 223}, {"referenceID": 4, "context": "NMF is a less commonly adopted method, but it has also been shown to be an effective dimensionality reduction technique for distributional semantics (Dinu and Lapata, 2010).", "startOffset": 149, "endOffset": 172}, {"referenceID": 0, "context": "SVD is the most common technique in distributional semantics, and it was used by Baroni and Zamparelli (2010). NMF is a less commonly adopted method, but it has also been shown to be an effective dimensionality reduction technique for distributional semantics (Dinu and Lapata, 2010).", "startOffset": 81, "endOffset": 110}, {"referenceID": 0, "context": "SVD is the most common technique in distributional semantics, and it was used by Baroni and Zamparelli (2010). NMF is a less commonly adopted method, but it has also been shown to be an effective dimensionality reduction technique for distributional semantics (Dinu and Lapata, 2010). It has a fundamental advantage from our point of view: The Multiply and Kronecker composition approaches (see Section 5.2 below), because of their multiplicative nature, cannot be meaningfully applied to vectors containing negative values. NMF, unlike SVD, produces non-negative vectors, and thus allows a fair comparison of all composition methods in the same reduced space.1 We perform the Singular Value Decomposition of the input matrix X: X = U\u03a3V T and, like Baroni and Zamparelli and many others, pick the first k = 300 columns ofU\u03a3 to obtain reduced representations. Non-negative Matrix Factorization factorizes a (m \u00d7 n) non-negative matrix X into two (m \u00d7 k) and (k \u00d7 n) non-negative matrices: X \u2248 WH (we normalize the input matrix to \u2211 i,j Xij = 1 before applying NMF). We use the Matlab implementation2 of the projected gradient algorithm proposed in Lin (2007), which minimizes the squared error of Frobenius norm F (W,H) = \u2016X \u2212WH\u2016F .", "startOffset": 81, "endOffset": 1158}, {"referenceID": 3, "context": "Grefenstette and Sadrzadeh (2011b) proposed a specific implementation of the general DisCoCat approach to compositional distributional semantics (Coecke et al., 2010) that we call Kronecker here.", "startOffset": 145, "endOffset": 166}, {"referenceID": 15, "context": "We adopt the widely used and generally successful multiplicative and additive models of Mitchell and Lapata (2010) and others.", "startOffset": 88, "endOffset": 115}, {"referenceID": 9, "context": "Grefenstette and Sadrzadeh (2011b) proposed a specific implementation of the general DisCoCat approach to compositional distributional semantics (Coecke et al.", "startOffset": 0, "endOffset": 35}, {"referenceID": 15, "context": "Composition of nouns and verbs under the proposed (multi-step) Regression model is implemented using Ridge Regression (RR) (Hastie et al., 2009).", "startOffset": 123, "endOffset": 144}, {"referenceID": 0, "context": "RR, also known as L2 regularized regression, is a different approach from the Partial Least Square Regression (PLSR) method that was used in previous related work (Baroni and Zamparelli, 2010; Guevara, 2010) to deal with the multicollinearity problem.", "startOffset": 163, "endOffset": 207}, {"referenceID": 14, "context": "RR, also known as L2 regularized regression, is a different approach from the Partial Least Square Regression (PLSR) method that was used in previous related work (Baroni and Zamparelli, 2010; Guevara, 2010) to deal with the multicollinearity problem.", "startOffset": 163, "endOffset": 207}, {"referenceID": 9, "context": "For each verb matrix or tensor to be learned, we tuned the parameter \u03bb by generalized cross-validation (Golub et al., 1979).", "startOffset": 103, "endOffset": 123}, {"referenceID": 8, "context": "Grefenstette and Sadrzadeh show that this method outperforms other implementations of the same formalism and is the current state of the art on the transitive sentence task of Grefenstette and Sadrzadeh (2011a) we also tackle below.", "startOffset": 0, "endOffset": 211}, {"referenceID": 19, "context": "We use the test set of Mitchell and Lapata (2008), consisting of 180 pairs of simple sentences made", "startOffset": 23, "endOffset": 50}, {"referenceID": 10, "context": "We use the test set of Grefenstette and Sadrzadeh (2011a), which was constructed with the same criteria that Mitchell and Lapata applied, but here the sentences have a simple transitive structure.", "startOffset": 23, "endOffset": 58}, {"referenceID": 10, "context": "Confirming what Grefenstette and Sadrzadeh found, we saw that Kronecker performs very well also in our experimental setup (although not as well as Regression). The main advantage of Kronecker over Regression lies in its simplicity: there is no training involved, all it takes is two outer vector products and a component-wise multiplication. However, as pointed out by Grefenstette and Sadrzadeh (2011b), this method is ad hoc compared to the linguistically motivated Categorical method they initially presented in Grefenstette and Sadrzadeh (2011a).", "startOffset": 16, "endOffset": 404}, {"referenceID": 10, "context": "Confirming what Grefenstette and Sadrzadeh found, we saw that Kronecker performs very well also in our experimental setup (although not as well as Regression). The main advantage of Kronecker over Regression lies in its simplicity: there is no training involved, all it takes is two outer vector products and a component-wise multiplication. However, as pointed out by Grefenstette and Sadrzadeh (2011b), this method is ad hoc compared to the linguistically motivated Categorical method they initially presented in Grefenstette and Sadrzadeh (2011a). It is conceivable that the Kronecker model\u2019s good performance is primarily tied to the nature of the evaluation data-set, where only verbs change while subject and object stay the same in sentence pairs.", "startOffset": 16, "endOffset": 550}, {"referenceID": 2, "context": "First, we discussed a tensor-based compositional distributional semantic framework in the vein of that of Coecke et al. (2010) which has the compositional mechanism of Baroni and Zamparelli (2010) as a specific case, thereby uniting both lines of research in a common framework.", "startOffset": 106, "endOffset": 127}, {"referenceID": 0, "context": "(2010) which has the compositional mechanism of Baroni and Zamparelli (2010) as a specific case, thereby uniting both lines of research in a common framework.", "startOffset": 48, "endOffset": 77}, {"referenceID": 0, "context": "(2010) which has the compositional mechanism of Baroni and Zamparelli (2010) as a specific case, thereby uniting both lines of research in a common framework. Second, we presented a generalisation of Baroni and Zamparelli\u2019s matrix learning method to higher rank tensors, allowing us to induce the semantic representation of functions modelled in this framework. Finally, we evaluated this new semantic tensor learning model against existing benchmark data-sets provided by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011a), and showed it to outperform other models.", "startOffset": 48, "endOffset": 500}, {"referenceID": 0, "context": "(2010) which has the compositional mechanism of Baroni and Zamparelli (2010) as a specific case, thereby uniting both lines of research in a common framework. Second, we presented a generalisation of Baroni and Zamparelli\u2019s matrix learning method to higher rank tensors, allowing us to induce the semantic representation of functions modelled in this framework. Finally, we evaluated this new semantic tensor learning model against existing benchmark data-sets provided by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011a), and showed it to outperform other models.", "startOffset": 48, "endOffset": 539}, {"referenceID": 10, "context": "We want moreover to test the Regression model against the Categorical model of Grefenstette and Sadrzadeh (2011a) and to design", "startOffset": 79, "endOffset": 114}, {"referenceID": 25, "context": "evaluation scenarios allowing a direct comparison with the MV-RNN model of Socher et al. (2012).", "startOffset": 75, "endOffset": 96}], "year": 2013, "abstractText": "We present a model for compositional distributional semantics related to the framework of Coecke et al. (2010), and emulating formal semantics by representing functions as tensors and arguments as vectors. We introduce a new learning method for tensors, generalising the approach of Baroni and Zamparelli (2010). We evaluate it on two benchmark data sets, and find it to outperform existing leading methods. We argue in our analysis that the nature of this learning method also renders it suitable for solving more subtle problems compositional distributional models might face.", "creator": "LaTeX with hyperref package"}}}