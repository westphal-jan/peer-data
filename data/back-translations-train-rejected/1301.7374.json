{"id": "1301.7374", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "Learning the Structure of Dynamic Probabilistic Networks", "abstract": "Dynamic probabilistic networks are a compact representation of complex stochastic processes. In this paper we examine how to learn the structure of a DPN from data. We extend structure scoring rules for standard probabilistic networks to the dynamic case, and show how to search for structure when some of the variables are hidden. Finally, we examine two applications where such a technology might be useful: predicting and classifying dynamic behaviors, and learning causal orderings in biological processes. We provide empirical results that demonstrate the applicability of our methods in both domains.", "histories": [["v1", "Wed, 30 Jan 2013 15:03:42 GMT  (457kb)", "http://arxiv.org/abs/1301.7374v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["nir friedman", "kevin murphy", "stuart russell"], "accepted": false, "id": "1301.7374"}, "pdf": {"name": "1301.7374.pdf", "metadata": {"source": "CRF", "title": "Learning the Structure of Dynamic Probabilistic Networks", "authors": ["Nir Friedman", "Kevin Murphy", "Stuart Russell"], "emails": ["nir@cs.berkeley.edu", "rnurphyk@cs.berkeley.edu", "russell@cs.berkeley.edu"], "sections": [{"heading": null, "text": "In fact, most of them are able to abide by the rules that they have imposed on themselves, and they are able to abide by the rules that they have imposed on themselves. (...) In fact, it is as if they are able to understand the rules that they have given themselves. (...) In fact, it is as if they are able to determine for themselves what they want. (...) \"It is as if they want to.\" (...)"}], "references": [{"title": "Adaptive probabilistic networks with hidden variables", "author": ["J. Binder", "D. Koller", "S. Russell", "K. Kanazawa"], "venue": "Mach. Learn\u00ad ing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Tractable Inference for Complex Stochastic Processes", "author": ["X. Boyen", "D. Koller"], "venue": "In UAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Theory refinement on Bayesian networks", "author": ["W. Buntine"], "venue": "In UA/-91,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G. Cooper", "E. Herskovits"], "venue": "Mach. Learn\u00ad ing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Probabilistic temporal reasoning", "author": ["T. Dean", "K. Kanazawa"], "venue": "InAAAI-88, pp. 524-528,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Optimal Statistical Decisions", "author": ["M. DeGroot"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1970}, {"title": "Maxi\u00ad mum likelihood from incomplete data via the EM algorithm", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "J. Royal Stat. Soc., B", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1977}, {"title": "The BATmobile: Towards a Bayesian automated taxi", "author": ["J. Forbes", "T. Huang", "K. Kanazawa", "S. Russell"], "venue": "In /JCAI-", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1995}], "referenceMentions": [{"referenceID": 4, "context": "Somewhat less well\u00ad established, but perhaps of equal importance, are dynamic probabilistic networks (DPNs), which model the stochastic evolution of a set of random variables over time [5].", "startOffset": 185, "endOffset": 188}, {"referenceID": 0, "context": "Algorithms for learning the parameters of PNs [1, 21] and DPNs [1, 14] are becoming widely used.", "startOffset": 46, "endOffset": 53}, {"referenceID": 0, "context": "Algorithms for learning the parameters of PNs [1, 21] and DPNs [1, 14] are becoming widely used.", "startOffset": 63, "endOffset": 70}, {"referenceID": 3, "context": ", where the value\ufffd of all variables are specified in each training case [ 4, 15].", "startOffset": 72, "endOffset": 80}, {"referenceID": 1, "context": "To represent beliefs about the possible trajectories of the process, we need a probability distribution over the random variables X[O] U X[l] U X[2] U.", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "\u2022 a prior network B0 that specifies a distribution over initial states X[O]; and \u2022 a transition network B--+ over the variables X[O] U X[1] that is taken to specify the transition probability P(X[t + 1] 1 X[t]) for all t.", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "2 An alternative approach is to evaluate (2) in closed form given a restricted family of priors [3, 4, 15].", "startOffset": 96, "endOffset": 106}, {"referenceID": 3, "context": "2 An alternative approach is to evaluate (2) in closed form given a restricted family of priors [3, 4, 15].", "startOffset": 96, "endOffset": 106}, {"referenceID": 5, "context": "In order to obtain a closed-form solution, we assume Dirichlet priors [6].", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "These two properties can be exploited by hill-climbing search procedures [3, 15] that gradually improve a candidate structure by applying the best arc addition, deletion, or reversal.", "startOffset": 73, "endOffset": 80}, {"referenceID": 6, "context": "The most commonly used method to alleviate this problem is the Expectation-Maximization (EM) algorithm [7, 21].", "startOffset": 103, "endOffset": 110}, {"referenceID": 7, "context": "A prime example arises in the BATmobile autonomous car project [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 0, "context": "We also tried using gradient descent, following [1], but en\u00ad countered difficulties with convergence in cases where the optimal parameter values were close to the boundaries (0", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "There are various approxi\u00ad mations that we could use to speed up inference: (1) The method proposed by Boyen and Koller [2], which approxi\u00ad mates posterior probabilities in the DPN in a factored form; this should be particularly appropriate for the biological models we are investigating.", "startOffset": 120, "endOffset": 123}], "year": 2011, "abstractText": "Dynamic probabilistic networks are a compact repre\u00ad sentation of complex stochastic processes. In this pa\u00ad per we examine how to learn the structure of a DPN from data. We extend structure scoring rules for stan\u00ad dard probabilistic networks to the dynamic case, and show how to search for structure when some of the vari\u00ad ables are hidden. Finally, we examine two applications where such a technology might be useful: predicting and classifying dynamic behaviors, and learning causal orderings in biological processes. We provide empir\u00ad ical results that demonstrate the applicability of our methods in both domains.", "creator": "pdftk 1.41 - www.pdftk.com"}}}