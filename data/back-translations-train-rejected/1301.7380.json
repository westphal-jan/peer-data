{"id": "1301.7380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "Solving POMDPs by Searching in Policy Space", "abstract": "Most algorithms for solving POMDPs iteratively improve a value function that implicitly represents a policy and are said to search in value function space. This paper presents an approach to solving POMDPs that represents a policy explicitly as a finite-state controller and iteratively improves the controller by search in policy space. Two related algorithms illustrate this approach. The first is a policy iteration algorithm that can outperform value iteration in solving infinitehorizon POMDPs. It provides the foundation for a new heuristic search algorithm that promises further speedup by focusing computational effort on regions of the problem space that are reachable, or likely to be reached, from a start state.", "histories": [["v1", "Wed, 30 Jan 2013 15:04:11 GMT  (266kb)", "http://arxiv.org/abs/1301.7380v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["eric a hansen"], "accepted": false, "id": "1301.7380"}, "pdf": {"name": "1301.7380.pdf", "metadata": {"source": "CRF", "title": "Solving POMDPs by Searching in Policy Space", "authors": ["Eric A. Hansen"], "emails": ["hansen@cs.umass.edu"], "sections": [{"heading": null, "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he said the role of the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"the\" New York Times."}, {"heading": "In Proceedings of the Fourth European Conference on", "text": "Planning."}], "references": [{"title": "A heuristic variable grid solu\u00ad tion method for POMDPs", "author": ["R.I. Brafman"], "venue": "In Proceedings of the Fif\u00ad teenth National Conference on Artificial Intelligence,", "citeRegEx": "Brafman,? \\Q1997\\E", "shortCiteRegEx": "Brafman", "year": 1997}, {"title": "Acting Optimally in Partially Observable Stochastic Domains", "author": ["A Cassandra", "L.P. Kaelbling", "M.L. Littman"], "venue": "In Proceedings of the Tweltth National Conference on Artificial Intelligence,", "citeRegEx": "Cassandra et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Cassandra et al\\.", "year": 1994}, {"title": "Incremental pruning: A simple, fast, exact algorithm for partially observable Markov decision processes", "author": ["A. Cassandra", "M.L. Littman", "N.L. Zhang"], "venue": "Proceedings of the T hirteenth Annual Conference on Uncertainty in Artificial Intelligence, 54-61. Morgan", "citeRegEx": "Cassandra et al\\.,? 1997", "shortCiteRegEx": "Cassandra et al\\.", "year": 1997}, {"title": "Heuristic search in restricted mem\u00ad ory", "author": ["P.P Chakrabarti", "S. Ghose", "A. Acharya", "S.C. de\u00ad Sarkar"], "venue": "Artificial Intelligence", "citeRegEx": "Chakrabarti et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Chakrabarti et al\\.", "year": 1990}, {"title": "An improved policy iteration al\u00ad gorithm for partially observable MDPs", "author": ["E.A. Hansen"], "venue": "Advances in Neural Information Processing Systems 10. In press.", "citeRegEx": "Hansen,? 1998a", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Finite-Memory Control of Par\u00ad tially Observable Systems", "author": ["E.A. Hansen"], "venue": "Ph.D. Diss., Department of Computer Science, University of Massachusetts at Amherst.", "citeRegEx": "Hansen,? 1998b", "shortCiteRegEx": "Hansen", "year": 1998}, {"title": "Incremental methods for com\u00ad puting bounds in partially observable Markov decision processes", "author": ["M. Hauskrecht"], "venue": "Proceedings of the Fifteenth National Conference on Artificial Intelligence, 734-739. AAAI Press/The MIT Press.", "citeRegEx": "Hauskrecht,? 1997", "shortCiteRegEx": "Hauskrecht", "year": 1997}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra"], "venue": "Computer Science Technical Re\u00ad port CS-96-08, Brown University.", "citeRegEx": "Kaelbling et al\\.,? 1996", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1996}, {"title": "A Decision Tree Approach to Main\u00ad taining a Deteriorating Physical System", "author": ["J.B. Larsen"], "venue": "PhD thesis, University of Texas at Austin.", "citeRegEx": "Larsen,? 1989", "shortCiteRegEx": "Larsen", "year": 1989}, {"title": "Efficient dynamic-programming updates in par\u00ad tially observable Markov decision processes", "author": ["M.L. Littman", "A.R. Cassandra", "L.P. Kaebling"], "venue": "Com\u00ad puter Science Technical Report CS-95-19, Brown Uni\u00ad versity.", "citeRegEx": "Littman et al\\.,? 1995", "shortCiteRegEx": "Littman et al\\.", "year": 1995}, {"title": "Markovian Decision Processes with Probabilistic Observation of States", "author": ["J.K. Satia", "R.E. Lave"], "venue": "Management Science 20(1):1-13.", "citeRegEx": "Satia and Lave,? 1973", "shortCiteRegEx": "Satia and Lave", "year": 1973}, {"title": "The optimal control of partially observable Markov processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research 21:1071-1088.", "citeRegEx": "Smallwood and Sondik,? 1973", "shortCiteRegEx": "Smallwood and Sondik", "year": 1973}, {"title": "The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs", "author": ["E.J. Sondik"], "venue": "Operations Research 26:282-304.", "citeRegEx": "Sondik,? 1978", "shortCiteRegEx": "Sondik", "year": 1978}, {"title": "Incremental Markov-model planning", "author": ["R. Washington"], "venue": "Proceedings of TAI-96, Eighth IEEE In\u00ad ternational Conference on Tools with Artificial Intelli\u00ad gence, 41-47.", "citeRegEx": "Washington,? 1996", "shortCiteRegEx": "Washington", "year": 1996}, {"title": "BI-POMDP: Bounded, incre\u00ad mental partially-observable Markov-model planning", "author": ["R. Washington"], "venue": "Proceedings of the Fourth European Conference on Planning.", "citeRegEx": "Washington,? 1997", "shortCiteRegEx": "Washington", "year": 1997}], "referenceMentions": [{"referenceID": 12, "context": "Sondik (1978) describes a policy iteration algorithm that rep\u00ad resents a policy in this way.", "startOffset": 0, "endOffset": 14}, {"referenceID": 4, "context": "The first is a policy iteration algorithm, first described by Hansen (1998a), that sim\u00ad plifies policy iteration for POMDPs by representing a policy as a finite-state controller.", "startOffset": 62, "endOffset": 77}, {"referenceID": 11, "context": "The key to computing the dynamic-programming up\u00ad date is Smallwood and Sondik's (1973) proof that it preserves the piecewise linearity and convexity of the value function.", "startOffset": 57, "endOffset": 87}, {"referenceID": 5, "context": "We do not describe here how to com\u00ad pute the dynamic-programming update and instead re\u00ad fer to this paper, Kaelbling et al. (1996), Cassandra et al.", "startOffset": 107, "endOffset": 131}, {"referenceID": 7, "context": "Given this correspondence between vectors and one-step policy choices, Kaelbling et al. (1996) point out that an optimal policy for a finite-horizon POMDP can be represented by an acyclic finite-state controller in which each machine state corresponds to a vector in a nonstationary value function.", "startOffset": 71, "endOffset": 95}, {"referenceID": 10, "context": "Moreover Sondik (1978) and Cassandra et al.", "startOffset": 9, "endOffset": 23}, {"referenceID": 1, "context": "Moreover Sondik (1978) and Cassandra et al. (1994) point out that sometimes, al\u00ad though not reliably, value iteration converges to an optimal piecewise linear and convex value function that is equivalent to a cyclic finite-state controller.", "startOffset": 27, "endOffset": 51}, {"referenceID": 12, "context": "Sondik (1978) describes a policy itera\u00ad tion algorithm for POMDPs that represents a policy as a mapping from a finite number of polyhedral regions of belief space to actions.", "startOffset": 0, "endOffset": 14}, {"referenceID": 5, "context": "We can prove the following generalization of Howards's policy improvement theorem (Hansen 1998b).", "startOffset": 82, "endOffset": 96}, {"referenceID": 1, "context": "Table 1: Comparison of value iteration and policy itera\u00ad tion on nine test problems from Cassandra et al. (1997).", "startOffset": 89, "endOffset": 113}, {"referenceID": 5, "context": "We use the same stopping condition Sondik uses to de\u00ad tect \u20ac-optimality: a finite-state controller is \u20ac-optimal when the Bellman residual is less than or equal to t:(l- (3)/(3, where (3 is the discount factor, and we can prove the following convergence result (Hansen 1998b).", "startOffset": 260, "endOffset": 274}, {"referenceID": 9, "context": "For POMDPs, policy evaluation has low-order polynomial complexity com\u00ad pared to the worst-case exponential complexity of the dynamic-programming update (Littman et al. 1995).", "startOffset": 152, "endOffset": 173}, {"referenceID": 9, "context": "For POMDPs, policy evaluation has low-order polynomial complexity com\u00ad pared to the worst-case exponential complexity of the dynamic-programming update (Littman et al. 1995). Therefore, policy iteration appears to have a clearer advantage over value iteration for POMDPs. Table 1 compares the performance of value iteration and policy iteration on nine test problems from Cas\u00ad sandra et al. (1997). (For these problems, the average number of states is 9.", "startOffset": 153, "endOffset": 398}, {"referenceID": 9, "context": "Satia and Lave (1973) describe a branch-and-bound algorithm for solving infinite-horizon POMDPs, given an initial belief state, and Larsen (1989) and Washington (1996,1997) use the best-first heuristic search algorithm AO* in a similar way.", "startOffset": 0, "endOffset": 22}, {"referenceID": 8, "context": "Satia and Lave (1973) describe a branch-and-bound algorithm for solving infinite-horizon POMDPs, given an initial belief state, and Larsen (1989) and Washington (1996,1997) use the best-first heuristic search algorithm AO* in a similar way.", "startOffset": 132, "endOffset": 146}, {"referenceID": 10, "context": "The error bound (the dif\u00ad ference between the upper and lower bounds on the value of the starting belief state) can be made arbi\u00ad trarily small by expanding the search tree far enough and, for discounted POMDPs, an E-optimal policy for the belief state at the root of the tree can be found after a finite search (Satia and Lave 1973).", "startOffset": 312, "endOffset": 333}, {"referenceID": 5, "context": "The theoretical properties of the algorithm are similar to those for policy iteration, but are specialized to a starting belief state (Hansen 1998b) .", "startOffset": 134, "endOffset": 148}, {"referenceID": 0, "context": "Sophisticated methods for computing upper bound functions have been developed that we have not yet implemented (e.g., Hauskrecht 1997; Brafman 1997) and we expect these will improve performance of the heuristic search algorithm and accelerate conver\u00ad gence of the error bound.", "startOffset": 111, "endOffset": 148}, {"referenceID": 3, "context": "We also plan to implement a memory-bounded version of AO* that can search more deeply in the tree (Chakrabarti et al. 1990; Washing\u00ad ton 1997).", "startOffset": 98, "endOffset": 142}, {"referenceID": 6, "context": "Consider a simple maze problem de\u00ad scribed by Hauskrecht (1997) that has 20 states, 6 ac\u00ad tions, and 8 observations.", "startOffset": 46, "endOffset": 64}, {"referenceID": 9, "context": "The bottleneck of both value iteration and policy iter\u00ad ation for POMDPs is the dynamic-programming up\u00ad date; Littman et al. (1995) prove that its worst-case complexity is exponential in the number of actions, ob\u00ad servations, and vectors in the current value function.", "startOffset": 110, "endOffset": 132}, {"referenceID": 12, "context": "work on exact algorithms for POMDPs that use dy\u00ad namic programming and a piecewise linear and convex representation of the value function (e.g., Smallwood & Sondik 1973; Sondik 1978; Cassandra et al. 1994; Cassandra et al. 1997).", "startOffset": 138, "endOffset": 228}, {"referenceID": 1, "context": "work on exact algorithms for POMDPs that use dy\u00ad namic programming and a piecewise linear and convex representation of the value function (e.g., Smallwood & Sondik 1973; Sondik 1978; Cassandra et al. 1994; Cassandra et al. 1997).", "startOffset": 138, "endOffset": 228}, {"referenceID": 2, "context": "work on exact algorithms for POMDPs that use dy\u00ad namic programming and a piecewise linear and convex representation of the value function (e.g., Smallwood & Sondik 1973; Sondik 1978; Cassandra et al. 1994; Cassandra et al. 1997).", "startOffset": 138, "endOffset": 228}, {"referenceID": 8, "context": "On the other, it draws from work on approximation algorithms for POMDPs that perform forward search from a starting belief state, including work on computing bounds for the fringe nodes of a search tree (e.g., Satia & Lave 1973; Larsen 1989; Washington 1996, 1997; Hauskrecht 1997).", "startOffset": 203, "endOffset": 281}, {"referenceID": 6, "context": "On the other, it draws from work on approximation algorithms for POMDPs that perform forward search from a starting belief state, including work on computing bounds for the fringe nodes of a search tree (e.g., Satia & Lave 1973; Larsen 1989; Washington 1996, 1997; Hauskrecht 1997).", "startOffset": 203, "endOffset": 281}], "year": 2011, "abstractText": "Most algorithms for solving POMDPs itera\u00ad tively improve a value function that implic\u00ad itly represents a policy and are said to search in value function space. This paper presents an approach to solving POMDPs that repre\u00ad sents a policy explicitly as a finite-state con\u00ad troller and iteratively improves the controller by search in policy space. Two related al\u00ad gorithms illustrate this approach. The first is a policy iteration algorithm that can out\u00ad perform value iteration in solving infinite\u00ad horizon POMDPs. It provides the founda\u00ad tion for a new heuristic search algorithm that promises further speedup by focusing compu\u00ad tational effort on regions of the problem space that are reachable, or likely to be reached, from a start state.", "creator": "pdftk 1.41 - www.pdftk.com"}}}