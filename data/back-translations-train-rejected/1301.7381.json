{"id": "1301.7381", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "Hierarchical Solution of Markov Decision Processes using Macro-actions", "abstract": "We investigate the use of temporally abstract actions, or macro-actions, in the solution of Markov decision processes. Unlike current models that combine both primitive actions and macro-actions and leave the state space unchanged, we propose a hierarchical model (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the state space. This is achieved by treating macroactions as local policies that act in certain regions of state space, and by restricting states in the abstract MDP to those at the boundaries of regions. The abstract MDP approximates the original and can be solved more efficiently. We discuss several ways in which macro-actions can be generated to ensure good solution quality. Finally, we consider ways in which macro-actions can be reused to solve multiple, related MDPs; and we show that this can justify the computational overhead of macro-action generation.", "histories": [["v1", "Wed, 30 Jan 2013 15:04:16 GMT  (413kb)", "http://arxiv.org/abs/1301.7381v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["milos hauskrecht", "nicolas meuleau", "leslie pack kaelbling", "thomas l dean", "craig boutilier"], "accepted": false, "id": "1301.7381"}, "pdf": {"name": "1301.7381.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Solution of Markov Decision Processes using Macro-actions", "authors": ["Milos Hauskrecht", "Nicolas Meuleau", "Leslie Pack Kaelbling", "Thomas Dean", "Craig Boutilier"], "emails": ["tld}@cs.brown.edu", "cebly@cs.ubc.ca"], "sections": [{"heading": null, "text": "In fact, it is the case that most of them are able to abide by the rules which they have imposed on themselves, and that they are able to abide by the rules which they have imposed on themselves. (...) In fact, it is the case that they are able to abide by the rules which they have imposed on themselves. (...) \"In fact, it is the case that they are able to determine what they want. (...)\" \"It is\" (...) \"(...)\" (...) \"(...)\" (...) \"(...)\" (... \"()\" () (() \"() () (()\" () () \"() () () (()\" () () () () () \"() () () () () () () () () (() () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () ()) () () () () () () ()) () () () () () () () () ()) () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () (() () () () (() () () (() () () (() ((() (() (() () ((() () ((() () () (() () (() (() () (((() () () (((() () () ((() () ((((() () () ((() (() ("}, {"heading": "Acknowledgements", "text": "We would like to thank Ronald Parr for a motivating discussion on macro actions and for pointing out further references. This work was supported in part by DARPA / Rome Labs Planning Initiative grant F30602-95-l-0020 and in part by NSF grants IRI-9453383 and IRI-9312395. Craig Boutilier was supported by NSERC Research Grant OGP0121843 and IRIS-II Project IC-7, and this work was carried out while the author visited BrowUniversity. Thanks also to the generous support of KillamFoundation.References (1) R. E. Bellman. Dynamic Programming. Princeton Univer Press, Princeton, 1957. D. P. Bertsekas and J.. N. Tsitsiklis. Neuro-dynamic Pro-Programming. Athena, 1996. [3] C. Boutilier, R. Dearden. Exploiting structure in policy."}], "references": [{"title": "Dynamic Programming", "author": ["R.E. Bellman"], "venue": "Princeton Univer\u00ad sity Press, Princeton,", "citeRegEx": "Bellman.,? \\Q1957\\E", "shortCiteRegEx": "Bellman.", "year": 1957}, {"title": "Plan\u00ad ning under time constraints in stochastic domains", "author": ["T. Dean", "L.P. Kaelbling", "J. Kirman", "A. Nicholson"], "venue": "Artif. In\u00ad tell.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}, {"title": "Abstraction and approximate decision theoretic planning", "author": ["R. Dearden", "C. Boutilier"], "venue": "Artif. lntell.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Learning and executing generalized robot plans, Artif. /ntell", "author": ["R. Fikes", "P. Hart", "N. Nilsson"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1972}, {"title": "Multilayer control of large Markov chains", "author": ["J.P. Forestier", "P. Varaiya"], "venue": "IEEE Trans. on Aut. Control,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1978}, {"title": "Planning with temporally abstract actions", "author": ["M. Hauskrecht"], "venue": "Technical report,", "citeRegEx": "Hauskrecht.,? \\Q1998\\E", "shortCiteRegEx": "Hauskrecht.", "year": 1998}, {"title": "Dynamic Programming and Markov Pro\u00ad cesses", "author": ["R.A. Howard"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1960}, {"title": "Hierarchical reinforcement learning: Preliminary results", "author": ["L. Pack Kaelbling"], "venue": "ICML-93, pp.167-173,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1993}, {"title": "Macro-operators: A weak method for learning", "author": ["R. Korf"], "venue": "Artif. Intel/. ,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1985}, {"title": "Decomposition of systems governed by Markov chains", "author": ["H.J. Kushner", "C.-H. Chen"], "venue": "IEEE Trans. Automatic Con\u00ad trol,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1974}, {"title": "SOAR: An archi\u00ad tecture for general intelligence", "author": ["J.E. Laird", "A. Newell", "P.S. Rosenbloom"], "venue": "Art. Intel/.,", "citeRegEx": "Laird et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Laird et al\\.", "year": 1987}, {"title": "Selectively generalizing plans for problem solv\u00ad", "author": ["S. Minton"], "venue": "ing. IJCA/-85,", "citeRegEx": "Minton.,? \\Q1985\\E", "shortCiteRegEx": "Minton.", "year": 1985}, {"title": "Reinforcement learning with hier\u00ad archies of machines", "author": ["R. Parr", "S. Russell"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Flexible Decomposition Algorithms for Weakly Coupled Markov Decision Processes", "author": ["R. Parr"], "venue": "In this proceedings,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Hierarchical control and learning with hierarchies of machines", "author": ["R. Parr"], "venue": "Chapters 1-3, under preparation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1998}, {"title": "Multi-time models for tem\u00ad porally abstract planning", "author": ["D. Precup", "R.S. Sutton"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "Theoretical results on reinforcement learning with temporally abstract behaviors", "author": ["D. Precup", "R.S. Sutton", "S. Singh"], "venue": "lOth Eur. Con f. Mach. Learn.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "author": ["M.L. Puterman"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1994}, {"title": "TD models: Modeling the world at a mixture of time scales", "author": ["R.S. Sutton"], "venue": "Lake Tahoe,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1995}, {"title": "Finding structure in reinforce\u00ad ment learning", "author": ["S. Thrun", "A. Schwartz"], "venue": "eds., N/PS-7,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1995}], "referenceMentions": [], "year": 2011, "abstractText": "We investigate the use of temporally abstract actions, or macro-actions, in the solution of Markov decision processes. Unlike current mod\u00ad els that combine both primitive actions and macro-actions and leave the state space un\u00ad changed, we propose a hierarchical model (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the state space. This is achieved by treating macro\u00ad actions as local policies that act in certain regions of state space, and by restricting states in the ab\u00ad stract MDP to those at the boundaries of regions. The abstract MDP approximates the original and can be solved more efficiently. We discuss sev\u00ad eral ways in which macro-actions can be gen\u00ad erated to ensure good solution quality. Finally, we consider ways in which macro-actions can be reused to solve multiple, related MDPs; and we show that this can justify the computational over\u00ad head of macro-action generation.", "creator": "pdftk 1.41 - www.pdftk.com"}}}