{"id": "1301.7403", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jan-2013", "title": "A Multivariate Discretization Method for Learning Bayesian Networks from Mixed Data", "abstract": "In this paper we address the problem of discretization in the context of learning Bayesian networks (BNs) from data containing both continuous and discrete variables. We describe a new technique for &lt;EM&gt;multivariate&lt;/EM&gt; discretization, whereby each continuous variable is discretized while taking into account its interaction with the other variables. The technique is based on the use of a Bayesian scoring metric that scores the discretization policy for a continuous variable given a BN structure and the observed data. Since the metric is relative to the BN structure currently being evaluated, the discretization of a variable needs to be dynamically adjusted as the BN structure changes.", "histories": [["v1", "Wed, 30 Jan 2013 15:06:05 GMT  (381kb)", "http://arxiv.org/abs/1301.7403v1", "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)"]], "COMMENTS": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI1998)", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["stefano monti", "gregory f cooper"], "accepted": false, "id": "1301.7403"}, "pdf": {"name": "1301.7403.pdf", "metadata": {"source": "CRF", "title": "A Multivariate Discretization Method for Learning Bayesian Networks from Mixed Data", "authors": ["Stefano Montit"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is the case that one is able to find a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution and that is able to find a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution that is able to find a solution that is capable of finding a solution that is capable of finding a solution that is capable of finding a solution."}], "references": [{"title": "Efficient ap\u00ad proximation for the marginal likelihood of incom\u00ad plete data given a Bayesian network", "author": ["D.M. Chickering", "D. Heckerman"], "venue": "In Proceedings of the 12-th Conference of Uncertainty in AI,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Global discretization of continuous attributes as preprocessing for machine learning", "author": ["M.R. Chmielewski", "J.W. Grzymala-Busse"], "venue": "International Journal of Approximate Reasoning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E. Herskovits"], "venue": "Machine Learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "Super\u00ad vised and unsupervized discretization of continuous features", "author": ["J. Dougherty", "R. Kohavi", "M. Sahami"], "venue": "Ma\u00ad chine Learning: Proocedings of the 12th Interna\u00ad tional Conference,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1995}, {"title": "Discretization of continuous attributes while learning Bayesian net-  Bayesian Multivariate Discretization 413 works", "author": ["N. Friedman", "M. Goldszmidt"], "venue": "Proceedings of 13-th In\u00ad ternational Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Learning Gaussian networks", "author": ["D. Geiger", "D. Heckerman"], "venue": "Prooceedings of the 1Oth Conference of Uncer\u00ad tainty in AI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1994}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Bayesian updating in causal probabilistic networks by local computation", "author": ["F. Jensen", "S. Lauritzen", "K. Olesen"], "venue": "Computational Statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1990}, {"title": "Learning Bayesian belief networks- an approach based on the MDL princi\u00ad ple", "author": ["W. Lam", "F. Bacchus"], "venue": "Computational Intelligence,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Local computa\u00ad tions with probabilities on graphical structures and their application to expert systems", "author": ["S. Lauritzen", "D. Spiegelhalter"], "venue": "Journal of the Royal Statistical Society,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "Mixed graphical association models (with discussion)", "author": ["S.L. Lauritzen"], "venue": "Scandinavian Journal of Statis\u00ad tics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1989}, {"title": "Learning hybrid Bayesian networks from data", "author": ["S. Monti", "G.F. Cooper"], "venue": "ed\u00ad itor, Learning and Inference in Graphical Models,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Probabilistic Reasoning in Intelligent Sys\u00ad tems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1988}, {"title": "Multivariate Density Estimation, Theory, Practice, and Visualization", "author": ["D. Scott"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1992}], "referenceMentions": [{"referenceID": 2, "context": "One common assumption is that all variables are discrete [3, 7], or that all variables are continuous and normally distributed [6].", "startOffset": 57, "endOffset": 63}, {"referenceID": 6, "context": "One common assumption is that all variables are discrete [3, 7], or that all variables are continuous and normally distributed [6].", "startOffset": 57, "endOffset": 63}, {"referenceID": 5, "context": "One common assumption is that all variables are discrete [3, 7], or that all variables are continuous and normally distributed [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "In these cases, discretization can actually be preferable, and some empirical evidence supports this conclu\u00ad sion [4, 12].", "startOffset": 114, "endOffset": 121}, {"referenceID": 11, "context": "In these cases, discretization can actually be preferable, and some empirical evidence supports this conclu\u00ad sion [4, 12].", "startOffset": 114, "endOffset": 121}, {"referenceID": 3, "context": "discretization of a given feature variable is carried out by at most considering its interaction with the class variable of interest, while ignoring possible in\u00ad teractions with other feature variables [4].", "startOffset": 202, "endOffset": 205}, {"referenceID": 1, "context": "Very few multivariate discretization strategies have been proposed in machine learning [2], and to our knowledge there is only one example of multivariate discretization strategy tailored to the needs of BN learning [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "Very few multivariate discretization strategies have been proposed in machine learning [2], and to our knowledge there is only one example of multivariate discretization strategy tailored to the needs of BN learning [5].", "startOffset": 216, "endOffset": 219}, {"referenceID": 4, "context": "Similar to the approach proposed in [5], we describe a technique for multivariate discretization where each continuous variable is discretized by taking into consideration its interaction with the other variables.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "The derived Bayesian scoring metric shares many of the properties of the scoring metric described in [5], which is based on the MDL principle.", "startOffset": 101, "endOffset": 104}, {"referenceID": 12, "context": "The essential property of BNs is summarized by the Markov property, which asserts that each variable is independent of its non-descendants given its parents [13].", "startOffset": 157, "endOffset": 161}, {"referenceID": 2, "context": "high-scoring network structures based on the defined scoring metric [3, 7, 9].", "startOffset": 68, "endOffset": 77}, {"referenceID": 6, "context": "high-scoring network structures based on the defined scoring metric [3, 7, 9].", "startOffset": 68, "endOffset": 77}, {"referenceID": 8, "context": "high-scoring network structures based on the defined scoring metric [3, 7, 9].", "startOffset": 68, "endOffset": 77}, {"referenceID": 2, "context": "analytical evaluation of this integral has been de\u00ad veloped for the cases when all variables in X are discrete, [3, 7] or all the variables are continuous and normally distributed [6].", "startOffset": 112, "endOffset": 118}, {"referenceID": 6, "context": "analytical evaluation of this integral has been de\u00ad veloped for the cases when all variables in X are discrete, [3, 7] or all the variables are continuous and normally distributed [6].", "startOffset": 112, "endOffset": 118}, {"referenceID": 5, "context": "analytical evaluation of this integral has been de\u00ad veloped for the cases when all variables in X are discrete, [3, 7] or all the variables are continuous and normally distributed [6].", "startOffset": 180, "endOffset": 183}, {"referenceID": 2, "context": "Also, N;jk is the number of cases in D where the variable Xi = k, and the parent set Pai = j, and N;j is the number of cases in D where Xi's parent set Pai takes its j-th value, irrespective of the value of Xi [3, 7].", "startOffset": 210, "endOffset": 216}, {"referenceID": 6, "context": "Also, N;jk is the number of cases in D where the variable Xi = k, and the parent set Pai = j, and N;j is the number of cases in D where Xi's parent set Pai takes its j-th value, irrespective of the value of Xi [3, 7].", "startOffset": 210, "endOffset": 216}, {"referenceID": 4, "context": "ditional distribution implied by the MDL-metric de\u00ad scribed in [5].", "startOffset": 63, "endOffset": 66}], "year": 2011, "abstractText": "In this paper we address the problem of discretization in the context of learning Bayesian networks (BNs) from data con\u00ad taining both continuous and discrete vari\u00ad ables. We describe a new technique for multivariate discretization, whereby each continuous variable is discretized while tak\u00ad ing into account its interaction with the other variables. The technique is based on the use of a Bayesian scoring metric that scores the discretization policy for a con\u00ad tinuous variable given a BN structure and the observed data. Since the metric is rel\u00ad ative to the BN structure currently being evaluated, the discretization of a variable needs to be dynamically adjusted as the BN structure changes.", "creator": "pdftk 1.41 - www.pdftk.com"}}}