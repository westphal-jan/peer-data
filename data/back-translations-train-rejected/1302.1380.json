{"id": "1302.1380", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2013", "title": "Towards the Rapid Development of a Natural Language Understanding Module", "abstract": "When developing a conversational agent, there is often an urgent need to have a prototype available in order to test the application with real users. A Wizard of Oz is a possibility, but sometimes the agent should be simply deployed in the environment where it will be used. Here, the agent should be able to capture as many interactions as possible and to understand how people react to failure. In this paper, we focus on the rapid development of a natural language understanding module by non experts. Our approach follows the learning paradigm and sees the process of understanding natural language as a classification problem. We test our module with a conversational agent that answers questions in the art domain. Moreover, we show how our approach can be used by a natural language interface to a cinema database.", "histories": [["v1", "Wed, 6 Feb 2013 14:17:55 GMT  (171kb,D)", "http://arxiv.org/abs/1302.1380v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["catarina moreira", "ana cristina mendes", "lu\\'isa coheur", "bruno martins"], "accepted": false, "id": "1302.1380"}, "pdf": {"name": "1302.1380.pdf", "metadata": {"source": "CRF", "title": "Towards the Rapid Development of a Natural Language Understanding Module", "authors": ["Catarina Moreira", "Ana Cristina Mendes", "L\u00fa\u0131sa Coheur", "Bruno Martins"], "emails": ["catarina.p.moreira@ist.utl.pt", "ana.mendes@l2f.inesc-id.pt", "luisa.coheur@l2f.inesc-id.pt", "bruno.g.martins@ist.utl.pt"], "sections": [{"heading": "1 Introduction", "text": "In order to have a clear idea of how people interact with a conversation agent, the agent should ideally be deployed at its final location so that it can be used by people who share the characteristics of end users. This scenario allows the agent's developers to collect corpora of real interactions. Although the Oz Technical Wizard [7] can also provide this corpora, sometimes testing the system with many different real users over a long period of time is not a solution and / or it is not predictable when users will be available. The Natural Language Understanding Module (NLU) is one of the most important components in a conversation agent responsible for interpreting user requirements. Thear Xiv: 130 2.13 80v1 [cs.Csymbolic approach to NLU usually involves a certain level of natural language processing that includes handcrafted grammars and requires some competence in a conversation agent responsible for interpreting the user's requirements."}, {"heading": "2 Related Work", "text": "The semantics of an enunciation can be a logical form, frame or set of natural language already understood by the machine. NLU techniques can be roughly divided into two categories: symbolic and sub-symbolic. There are also hybrid techniques that use characteristics of both categories. In terms of symbolic NLU, it includes keyword recognition, pattern comparison and rule-based techniques. For example, the virtual therapist ELIZA [11] is a classic example of a system based on pattern comparison. Many early systems were based on an elaborate syntax / semantics interface in which each syntactic rule is generated with a semantic rule and logical forms in a bottom-up composition process. Deviations of this approach are described in [2, 6]."}, {"heading": "3 The natural language understanding module", "text": "The NLU receives as input a file of possible interactions (the training results) from which several characteristics are extracted, and these characteristics are in turn used as input for a classifier. In our implementation we have support for Vector Machines (SVM) as classifiers and the characteristics are unigrammable. However, to refine the results, other characteristics can easily be included. Figure 2 describes the training phase of the NLU model. Each interaction cited in the training results is a pair in which the first element represents a series of expressions that paraphrase each other and trigger the same reaction. The second element is a series of responses that represent possible responses to the previous expressions. That is, each expression in an interaction represents different manners of expression and each answer represents a possible response."}, {"heading": "4 Experiments", "text": "In this section, the validation methodology and the results obtained are presented."}, {"heading": "4.1 Experimental setup", "text": "To test our approach to the rapid development of an NLU module, we first gathered a corpus containing art interactions: the Art corpus. It was built to train Edgar, an interlocutor whose job it is to engage in exploratory conversations with users and teach about the Monserrate Palace. Edgar answers questions about his field of knowledge, although it also answers questions about himself. Art corpus includes 283 expressions with 1471 words, 279 of which are unique; the expressions represent 52 different interactions (i.e. an average of 5.4 paraphrases); and for our experiments in the field of cinema, we used the Cinema corpus, which maps 229 questions into 28 different logical forms, each representing different SQL queries; and a dictionary containing actors \"names and movie titles was created."}, {"heading": "4.2 Results", "text": "The focus of the first experiment was on the selection of a correct answer to a given enunciation. This scenario implies the correct association of the enunciation with the quantity of its paraphrases. For example, taking into account the previous example sentence As obras va o acabar quando?, it should be assigned to the category Agent 7 (the category of its paraphrases). The focus of the second experiment was to map a question into an intermediate language (a logical form) [3]. Thus, for example, sentence Que actriz contracena com Viggo Mortensen no Senhor dos Ane \"is? (Which actress plays with Viggo Mortensen in The Lord of the Rings?) should be mapped into the form Who acts WITH (Viggo Mortensen, The Lord of the Rings). The results are shown in Table 1."}, {"heading": "4.3 Discussion", "text": "From the analysis of Table 1, we conclude that a simple technique can lead to very interesting results, especially if we compare the accuracy obtained for the Cinema Corpus with previous results of 75% achieved by relying on a linguistically rich framework that required several months of skilled work to build. In fact, the previous implementation of JaTeDigo was based on a natural language processing chain that was responsible for morphosyntactic analysis, so-called entity detection, and rules-based semantic interpretation. Another conclusion is that it is easy to develop an NLU module. In less than an hour we can have the interactions required for the training, and from there, the creation of the NLU module for this area is easy. In addition, new information can easily be added, allowing us to develop the model.Nevertheless, we are aware of the weaknesses of our approach. The NLU module is highly dependent on the words used during the training and the recognition of paraphrases QS."}, {"heading": "5 Conclusions and Future Work", "text": "We have presented an approach for the rapid development of an NLU module based on a number of possible interactions. This approach treats the problem of understanding natural language as a classification process where utterances that paraphrase each other get the same category. It receives two files as input, the only condition being to write them in a predefined XML format, making it very easy to use, even for non-experts. In addition, it achieves very promising results. As a future work, although we are moving away from linguistic independence, we would like to try additional features, and we would also like to try to automatically enrich the dictionary and training files with relationships extracted from WordNet."}, {"heading": "Acknowledgments", "text": "This work was supported by FCT (INESC-ID multiannual grant) through the funds of the PIDDAC programme and also by the project FALACOMIGO (Projecto em co-promoc, QREN n 13449). Ana Cristina Mendes is supported by a doctoral scholarship from Fundac and a scholarship from Cie Ncia e a Tecnologia (SFRH / BD / 43487 / 2008)."}], "references": [{"title": "Efficient string matching: an aid to bibliographic search", "author": ["Alfred V. Aho", "Margaret J. Corasick"], "venue": "Communications of the ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1975}, {"title": "Natural language understanding (2nd ed.)", "author": ["James Allen"], "venue": "Benjamin-Cummings Publishing Co., Inc.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "Natural language interfaces to databases\u2013an introduction", "author": ["I. Androutsopoulos", "G.D. Ritchie", "P. Thanisch"], "venue": "Journal of Language Engineering,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1995}, {"title": "Domain-Oriented Conversation with H.C", "author": ["Niels Ole Bernsen", "Laila Dybkj\u00e6r"], "venue": "Andersen. In Proc. of the Workshop on Affective Dialogue Systems, Kloster Irsee,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Shallow semantic parsing despite little training data", "author": ["Rahul Bhagat", "A. Leuski", "Eduard Hovy"], "venue": "In Proc. ACL/SIGPARSE 9th Int. Workshop on Parsing Technologies,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Speech and Language Processing (2nd Edition)", "author": ["Daniel Jurafsky", "James H. Martin"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "An iterative design methodology for user-friendly natural language office information applications", "author": ["J.F. Kelley"], "venue": "In ACM Transactions on Office Information Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1984}, {"title": "A conversational agent as museum guide: design and evaluation of a real-world application, pages 329\u2013343", "author": ["Stefan Kopp", "Lars Gesellensetter", "Nicole C. Kr\u00e4mer", "Ipke Wachsmuth"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "A statistical segment-based approach for spoken language understanding", "author": ["Lu\u0107\u0131a Ortega", "Isabel Galiano", "Ll\u00fa\u0131s F. Hurtado", "Emilio Sanchis", "Encarna Segarra"], "venue": "In Proceedings of the 11th Annual Conference of the International Speech Communication Association,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Doing research on a deployed spoken dialogue system: One year of let\u2019s go! experience", "author": ["Antoine Raux", "Dan Bohus", "Brian Langner", "Alan W Black", "Maxine Eskenazi"], "venue": "In Proceedings of the 7th Annual Conference of the International Speech Communication Association,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Eliza - a computer program for the study of natural language communication between man and machine", "author": ["Joseph Weizenbaum"], "venue": "Communications of the ACM,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1966}], "referenceMentions": [{"referenceID": 6, "context": "Although the Wizard of Oz technique [7] can also provide these corpora, sometimes it is not a solution if one needs to test the system with many different real users during a long period and/or it is not predictable when the users will be available.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "Here, we follow the approach described in [5], although their focus is on frame-based dialogue systems.", "startOffset": 42, "endOffset": 45}, {"referenceID": 10, "context": "For instance, the virtual therapist ELIZA [11] is a classical example of a system based on pattern matching.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "Variations of this approach are described in [2, 6].", "startOffset": 45, "endOffset": 51}, {"referenceID": 5, "context": "Variations of this approach are described in [2, 6].", "startOffset": 45, "endOffset": 51}, {"referenceID": 3, "context": "Recently, many systems follow the symbolic approach, by using in-house rule-based NLU modules [4, 8].", "startOffset": 94, "endOffset": 100}, {"referenceID": 7, "context": "Recently, many systems follow the symbolic approach, by using in-house rule-based NLU modules [4, 8].", "startOffset": 94, "endOffset": 100}, {"referenceID": 9, "context": "However, some systems use the NLU modules of available dialogue frameworks, like the Let\u2019s Go system [10], which uses Olympus.", "startOffset": 101, "endOffset": 105}, {"referenceID": 4, "context": "In what concerns sub-symbolic NLU, some systems receive text as input [5] and many are dealing with transcriptions from an Automatic Speech Recognizer [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "In what concerns sub-symbolic NLU, some systems receive text as input [5] and many are dealing with transcriptions from an Automatic Speech Recognizer [9].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "This process uses the LingPipe implementation of the Aho-Corasick algorithm [1], that searches for matches against a dictionary in linear time in terms of the length of the text, independently of the size of the dictionary.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "The focus of the second experiment was to map a question into an intermediate representation language (a logical form) [3].", "startOffset": 119, "endOffset": 122}], "year": 2013, "abstractText": "When developing a conversational agent, there is often an urgent need to have a prototype available in order to test the application with real users. A Wizard of Oz is a possibility, but sometimes the agent should be simply deployed in the environment where it will be used. Here, the agent should be able to capture as many interactions as possible and to understand how people react to failure. In this paper, we focus on the rapid development of a natural language understanding module by non experts. Our approach follows the learning paradigm and sees the process of understanding natural language as a classification problem. We test our module with a conversational agent that answers questions in the art domain. Moreover, we show how our approach can be used by a natural language interface to a cinema database.", "creator": "LaTeX with hyperref package"}}}