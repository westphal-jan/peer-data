{"id": "1302.1612", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Feb-2013", "title": "Arabic text summarization based on latent semantic analysis to enhance arabic documents clustering", "abstract": "Arabic Documents Clustering is an important task for obtaining good results with the traditional Information Retrieval (IR) systems especially with the rapid growth of the number of online documents present in Arabic language. Documents clustering aim to automatically group similar documents in one cluster using different similarity/distance measures. This task is often affected by the documents length, useful information on the documents is often accompanied by a large amount of noise, and therefore it is necessary to eliminate this noise while keeping useful information to boost the performance of Documents clustering. In this paper, we propose to evaluate the impact of text summarization using the Latent Semantic Analysis Model on Arabic Documents Clustering in order to solve problems cited above, using five similarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard Coefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler Divergence, for two times: without and with stemming. Our experimental results indicate that our proposed approach effectively solves the problems of noisy information and documents length, and thus significantly improve the clustering performance.", "histories": [["v1", "Wed, 6 Feb 2013 23:24:37 GMT  (260kb)", "http://arxiv.org/abs/1302.1612v1", null]], "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["hanane froud", "abdelmonaime lachkar", "said alaoui ouatik"], "accepted": false, "id": "1302.1612"}, "pdf": {"name": "1302.1612.pdf", "metadata": {"source": "CRF", "title": "SEMANTIC ANALYSIS TO ENHANCE ARABIC DOCUMENTS CLUSTERING", "authors": ["Hanane Froud", "Abdelmonaime Lachkar", "Said Alaoui Ouatik", "Sidi Mohamed", "Ben Abdellah (USMBA", "Fez", "Sameh H. Ghwanmeh"], "emails": ["hanane_froud@yahoo.fr,", "abdelmonaime_lachkar@yahoo.fr", "s_ouatik@yahoo.com"], "sections": [{"heading": null, "text": "KEYWORDS Information retrieval systems, Arabic language, Arabic text clusters, Arabic text summary, similarity measurements, latent semantic analysis, root and light stems."}, {"heading": "1. INTRODUCTION", "text": "This year it has come to the point that it will be able to put itself at the top, \"he said in an interview with the German Press Agency.\" We have never lost as much time as this year, \"he said.\" But it is too early to say what we need to do, \"he said."}, {"heading": "2. ARABIC TEXT SUMMARIZATION BASED ON LATENT SEMANTIC ANALYSIS MODEL", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. LSA Summarization", "text": "In this thesis, we propose to apply the latent semantic analysis model to generate the general Arabic text summary [13] [17] [18] [19], starting with the generation of terms by sentence matrix A = [A1 A2... An], each column vector representing Ai whose weighted term frequency vector is i in the document under consideration. The weighted term frequency vector Ai = [a1i a2i... ani] T of the sentence i is defined as: (). () ij ija L t G t = where: 1. L (tji) is the local weighting of the term j in the sentence i: L (tji) = tf (tji), where tf (tji) is the number of times in which the term j occurs in the sentence. () ij ija L t = where: 1. G (tji) is the global weighting of the term in the entire document: jj / iG order (iG)."}, {"heading": "1 2 1... ... 0r r n\u03c3 \u03c3 \u03c3 \u03c3 \u03c3+\u2265 \u2265 = = =\u227b", "text": "The interpretation of the SVD to the terms of the sentence matrix A can be made from two different angles. From the transformation perspective, the SVD derives a mapping between the m-dimensional space generated by the weighted term frequency vectors and the rdimensional singular vector space. From a semantic perspective, the SVD derives the latent semantic structure from the document represented by the matrix A. This operation reflects a breakdown of the original document into r linear-independent base vectors or concepts. Each term and sentence from the document is collectively indexed by these base vectors / concepts. A unique SVD property is able to capture and model interactions between terms so that it can semantically display cluster terms and sentences from the document. Furthermore, it is more than shown that a word combination pattern from the document stands out and is reflected in the document."}, {"heading": "2.2. Arabic Summarization", "text": "In this paper, we propose to use the above method to identify semantically important sentences for Arabic summaries (Figure 1) in order to expand the Arabic document cluster task. Figure 1. Arabic text summary based on the latent semantic analysis model After creating the test corpus, we split each document into individual sentences; this decomposition is a source of ambiguity because, on the one hand, punctuation is rarely used in Arabic texts and, on the other, punctuation that, if it exists, is not always critical for the composition examples. For example: Input DataDocumentDecomposition using Table.1: Sentences Words The weighted term frequency vector Ai = [a1i a2i... ani] T of Sentences iSentences 1. In this document, iSentences 1. and 2. Sentence Sample: Input DataDocumentDecomposition using Table.1: SentsordsThe weighted Term-A2Sentences 1."}, {"heading": "3. ARABIC TEXT PREPROCESSING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Arabic Language Structure", "text": "The Arabic language is the language of the Holy Koran. It is one of the six official languages of the United Nations and the mother tongue of approximately 300 million people. It is a semitic language with 28 letters of the alphabet. Its written orientation is from right to left. It can be divided into three types: classical Arabic (- @ A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-"}, {"heading": "3.2. Stemming", "text": "Arabic word stemming is a technique that aims to find the lexical root or stem (Figure 2) of words in the natural language by removing attachments to the root, since an Arabic word with these attachments can have a more complicated form. An Arabic word can represent a phrase in English, for example the word 23-4 5555555556: \"speak to them\" is broken down as follows (Table 2): Table 2. Arabic word decompositionAntefix prefix root suffix postfix, meaning that \"to\" means a letter indicating the tense and the person of the conjugationspeakBeenTermination of the conjugationA pronoun means \"them\" Figure 2. An example of root / stem preprocessing."}, {"heading": "3.3. Root-based versus Stem-based approaches", "text": "In this section, a brief review of the two derivative approaches to deriving the word with all possible patterns is used to extract it. [25] The algorithms were developed for this approach. Al-Fedaghi and Al-Anzi algorithms attempt to compare the word with all possible patterns it has appended. The algorithms do not remove prefixes or suffixes. Al-Fedaghi and Al-Anzi algorithms attempt to fill the word with all possible patterns."}, {"heading": "3.4. Document Representation", "text": "There are several ways to model a text document: For example, it can be presented as a bag of words in which words appear independently of each other and the sequence is intangible. This model is commonly used in information gathering and word processing [6]. Each word corresponds to a dimension in the resulting data space, and each document then becomes a vector consisting of non-negative values on each dimension. Allow it to be..., 1D d dn = a set of documents and} {, 1T t tf d tf d d d d d the frequency of the documents, () tf d t the frequency of the term t in document t D. Then the vector representation of a document is d: ((,), 1t tf d t tf d tf d d the frequency of the documents, () tf tf d the frequency of the documents, df the meaning."}, {"heading": "4. SIMILARITY MEASURES", "text": "In this section, we discuss the five similarity measures tested in [3], and we include these five measures in our work to effect clustering of Arabic text documents."}, {"heading": "4.1. Metric", "text": "Not every distance measure is a measurement quantity. To count as a measurement value, a measure d must meet the following four conditions: Let x and y be any two objects in a set and (,) d x y be the distance between x and y.1. The distance between any two points must not be negative, i.e. (,) 0d x y \u2265.2. The distance between two objects must be zero if and only if the two objects are identical, i.e. (,) 0d x y = whether and only if x y =.3. The distance must be symmetrical, i.e. the distance from x to y is equal to the distance from y to x, i.e. (,) (,) d x y x =.4. The measure must meet the triangular inequality that is (,) (,) (,) d x z d x y y z \u2264 +."}, {"heading": "4.2. Euclidean Distance", "text": "Euclidean distance is often used in clustering problems, including clustering text. It fulfils all of the above four conditions and is therefore a true measure. It is also the standard measure of distance between text documents, since two documents are represented here and db by their term vectors ta and tbrespectiv. Euclidean distance between the two documents is defined as 2 1 2 (,,,, 1 m D t t w wa t aE b bt = \u2212 \u2211 =, where the term phrase} {,..., 1T t tm = is. As already mentioned, we use the tfidf value as term weights, i.e. (,), w tfidf d tat a =."}, {"heading": "4.3. Cosine Similarity", "text": "Cosine similarity is one of the most popular measures of similarity applied to text documents, such as in numerous information retrieval applications [6] and also in clusters [7]. For two documents, their cosine similarity is:. (,), t ta bSIM t taC b t ta b = \u00d7 where ta and tb are m-dimensional vectors via the term set} {,..., 1T t tm =. Each dimension represents a term with its weight in the document that is not negative. Consequently, the cosine similarity is not negative and limits between [] 0.1. An important property of cosine similarity is its independence from the document length. For example, the combination of two identical copies of a document d represents a new pseudo-document 0 d, the cosine similarity between d and 0 dis 1, which means that these two documents are considered identical."}, {"heading": "4.4. Jaccard Coefficient", "text": "The Jaccard coefficient, sometimes referred to as the Tanimoto coefficient, measures similarity as a point of intersection divided by the union of the objects. In text documents, the Jaccard coefficient compares the total weight of divided terms with the total weight of terms present in one of the two documents but not the common terms. (,) 2 2. t ta bSIM t taJ b t ta ab b = + \u2212 The Jaccard coefficient is a measure of similarity and lies between 0 and 1. It is 1 if ta b = and 0 if ta and table disjoints. The corresponding distance measure is 1D S IMJ J = \u2212 and we will use D J instead in subsequent experiments."}, {"heading": "5.5. Pearson Correlation Coefficient", "text": "Pearson's correlation coefficient is another measure of the extent to which two vectors are related to each other. There are different forms of Pearson's correlation coefficient formula. Taking into account the term set} {,..., 1T t tm = is a commonly used form, 1, (,) 2 2 2, 1, mm w TF TFat at at at at at at t b b b b SIM t taP b m mm w TF m w TFat at at at t b b \u00b7 \u2212 \u00b7 = = = where, 1 mTF wa t at = \u2211 = and 1, mTF wtb t b = \u2211 = This is also a measure of similarity. In contrast to the other measures, however, it ranges from -1 to + 1 and it is 1 if t ta b =. In subsequent experiments we use the corresponding distance measure, which is 1D SIMP P = \u2212 if 0SIM P \u2265 and D SIMP P = if 0SIM \u00b2."}, {"heading": "4.6. Averaged Kullback-Leibler Divergence", "text": "The Kullback-Leibler divergence (KL divergence), also known as relative entropy, is a widely used measure for evaluating the differences between two probability distributions. In two distributions P and Q, the KL divergence from the distribution P to the distribution Q is called (| |) Log () PD P Q PKL Q = In the document scenario, the divergence between two word distributions is as follows: (| |) Log (), 1, wm t a D t wa t aKL b = x x x x x x x x b = In contrast to the previous measurements, however, the KL divergence is not symmetrical, i.e. (|) the divergence between two word distributions w (|) Log ()."}, {"heading": "5. EXPERIMENTS AND RESULTS", "text": "In our experiments (Figure 4), we used the K-mean algorithm as the document clustering method, working with distance measurements that essentially aim to minimize the distances within a cluster. Therefore, similarity measurements do not fit directly into the algorithm, because smaller values indicate dissimilarity. Euclidean distance and averaged KL divergence are distance measurements, while cosmic similarity, the Jaccard coefficient and the Pearson coefficient are similarity measurements. [3] applies a simple transformation to convert the similarity measurement into distance values. As both cosmic similarity and the Jaccard coefficient are limited in [] 0, 1 and monotonically, we take 1D SIM = \u2212 as the corresponding distance value. For Pearson coefficient, which ranges from \u2212 1 to + 1, we take 1D SIM = \u2212 if the cosmic similarity and the Jaccard coefficient are limited, we use [1] and the initial coefficient [1]."}, {"heading": "5.1. Dataset", "text": "The test data set [9] (Corpus of Contemporary Arabic (CCA)) consists of 12 different categories, each of which contains documents from websites and Radio Qatar. A summary of the test data set is given in Table 3.As already mentioned, the basic method is full-text presentation, for each document we have removed stopwords and replaced the remaining words with the use of Khoja strains and Larkey strains. To illustrate the advantages of our proposed approach, we then use document summaries to cluster our data set."}, {"heading": "5.2. Results", "text": "The quality of the cluster result was evaluated using two evaluation measures: purity and entropy, which are widely used to evaluate the performance of unattended learning algorithms [10] [11].The purity measures evaluate the coherence of a cluster, that is, the degree to which a cluster contains documents from a single category. In view of a particular cluster Ci of size ni, the purity of Ci is formally defined as: 1 () max () hi ihiP C n = where max () hi h n is the number of documents that are assigned from the dominant category in the cluster Ci and h in the number of documents from the cluster Ci to category h. Generally speaking, the higher the purity value, the better the quality of the cluster is is.The entropy measurement evaluates the distribution of categories in a given cluster. The entropy of cluster Ci with size ni is assigned to be11 () log () logh hropi ic total number i = the number of categories in which n is."}, {"heading": "5.2. 1. Results Using Full-Text Representation", "text": "5.2. 1.a. Results with StemmingIn Table 4, with Khoja's Stemmer, the total purity values for Euclidean distance, cosmic similarity and average KL divergence are quite similar and perform poorly relative to the other measurements. Meanwhile, the Jaccard measurement is better when it comes to generating more coherent clusters with a considerable purity scale. In this context, Table 5 shows the higher purity values (0.77) than those in Table 4 for Euclidean distance, cosmic similarity and Jaccard measurements. On the other hand, the Pearson correlation and average KL divergence are quite similar, but still better than the purity values for these measurements in Table 4.The total purity value for each measurement is presented in the two tables."}, {"heading": "5.2. 2. Results Using Document Summaries", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "6. CONCLUSION", "text": "In this paper, we have proposed to illustrate the advantages of the summary using the latent semantic analysis model by comparing cluster results based on the summary with the full-text basis of Arabic document clustering for five similarity / distance measurements three times: without descent and with descent using Khoja's descent and the descent of the larkey. We found that descent for full-text representation of Euclidean distance, cosmic similarity and Jaccard measurements exhibit comparable effectiveness for the partitional Arabic document cluster task if we did not use the descent for full-text representation. On the other hand, the Pearson correlation and average KL divergence are quite similar to their results, but they are no better than the other measurements in the same case. Instead of using the full text as the representation for the summary, we use the LSA model as a summary documentation to eliminate the overlapping of the documents once we have selected."}], "references": [{"title": "Ghwanmeh, \u201cApplying Clustering of Hierarchical K-means-like Algorithm on Arabic Language", "author": ["H. Sameh"], "venue": "International Journal of Information Technology IJIT Volume 3 Number", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Stemming Arabic Text", "author": ["S. Khoja", "R. Garside"], "venue": "http://www.comp.lancs.ac.uk/computing/users/khoja/stemmer.ps", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1999}, {"title": "Improving Stemming for Arabic Information Retrieval: Light Stemming and Co-occurrence Analysis", "author": ["Larkey", "Leah S", "Ballesteros", "Lisa", "Connell", "Margaret"], "venue": "In Proceedings of the 25th Annual International Conference on Research and Development in Information Retrieval (SIGIR", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Neto.\u201dModern Information Retrieval", "author": ["B.R.R.B. Yates"], "venue": "ADDISON-WESLEY, New York,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Fast and Effective Text Mining using Linear-time Document Clustering", "author": ["B. Larsen", "C. Aone"], "venue": "In Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "The Information Bottleneck Method", "author": ["N.Z. Tishby", "F. Pereira", "W. Bialek"], "venue": "In Proceedings of the 37th Allerton Conference on Communication, Control and Computing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1999}, {"title": "Karypis.\u201dEvaluation of Hierarchical Clustering Algorithms for Document Datasets", "author": ["G.Y. Zhao"], "venue": "In Proceedings of the International Conference on Information and Knowledge Management,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2002}, {"title": "Karypis.\u201dEmpirical and Theoretical Comparisons of Selected Criterion Functions for Document Clustering", "author": ["G.Y. Zhao"], "venue": "Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Web page clustering enhanced by summarization", "author": ["Xuanhui Wang", "Dou Shen", "Hua-Jun Zeng", "Zheng Chen", "Wei-Ying Ma"], "venue": "Proceedings of the 2004 ACM CIKM International Conference on Information and Knowledge Management,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2004}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S. Dumais", "G. Furnas", "T. Landauer", "R. Harshman"], "venue": "Journal of the American Society for Information Science, vol. 41, pp.391-407, 1990.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1990}, {"title": "Stemming Versus Light Stemming for Measuring the Simitilarity between Arabic Words with Latent Semantic Analysis Model\u201d, 2 International Colloquium in Information Science and Technology (CIST", "author": ["H. Froud", "A. Lachkar", "S. Alaoui Ouatik"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Generic text summarization using relevance measure and latent semantic analysis", "author": ["Y.H. Gong", "X. Liu"], "venue": "Proc. The 24th annual international ACM SIGIR, pp. 19 - 25, 2001.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Using Latent Semantic Analysis in text summarization and summary evaluation", "author": ["J. Steinberger", "K. Jezek"], "venue": "Proceedings of ISIM", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Numerical Recipes in C: The Art of Scientific Computing", "author": ["W. Press"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1992}, {"title": "Using Linear Algebra for Intelligent Information Retrieval", "author": ["M.W. Berry", "S.T. Dumais", "G. W O\u2019Brien"], "venue": "SIAM Review", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "A major offshoot of the DIINAR-MBC project: AraParse, a morphosyntactic analyzer for unvowelled Arabic texts, ACL/EACL", "author": ["R. Ouersighni"], "venue": "Workshop on Arabic Language Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "A new algorithm to generate Arabic root-pattern forms", "author": ["Al-Fedaghi S", "F. Al-Anzi"], "venue": "In proceedings of the 11th national Computer Conference and Exhibition", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1989}, {"title": "A computational morphology system for Arabic", "author": ["Al-Shalabi R", "M. Evens"], "venue": "In Workshop on Computational Approaches to Semitic Languages,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1998}, {"title": "On Arabic search: improving the retrieval effectiveness via a light temming approach", "author": ["Aljlayl M", "O. Frieder"], "venue": "In ACM CIKM 2002 International Conference on Information and Knowledge Management,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2002}, {"title": "Arabic information retrieval at UMass in TREC-10", "author": ["Larkey L", "M.E. Connell"], "venue": "Proceedings of TREC", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "Building an Arabic Stemmer for Information Retrieval", "author": ["Chen A", "F. Gey"], "venue": "In Proceedings of the 11th Text Retrieval Conference (TREC", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2002}], "referenceMentions": [{"referenceID": 0, "context": "Ghwanmeh in [2] presented a comparison study between the traditional Information Retrieval system and the clustered one.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "In this paper, we propose to investigate the use of summarization techniques to tackle these issues when clustering documents [13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 8, "context": "Latent Semantics Analysis (LSA) has been successfully applied to information retrieval [13] [15][16][17] as well as many other related domains.", "startOffset": 87, "endOffset": 91}, {"referenceID": 9, "context": "Latent Semantics Analysis (LSA) has been successfully applied to information retrieval [13] [15][16][17] as well as many other related domains.", "startOffset": 92, "endOffset": 96}, {"referenceID": 10, "context": "Latent Semantics Analysis (LSA) has been successfully applied to information retrieval [13] [15][16][17] as well as many other related domains.", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Recently, LSA has been introduced into generic text summarization by [18].", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "LSA Summarization In this work, we propose to apply the Latent Semantic Analysis Model in order to generic Arabic Text Summarization [13] [17][18][19].", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "LSA Summarization In this work, we propose to apply the Latent Semantic Analysis Model in order to generic Arabic Text Summarization [13] [17][18][19].", "startOffset": 142, "endOffset": 146}, {"referenceID": 12, "context": "LSA Summarization In this work, we propose to apply the Latent Semantic Analysis Model in order to generic Arabic Text Summarization [13] [17][18][19].", "startOffset": 146, "endOffset": 150}, {"referenceID": 13, "context": "Given an m x n matrix A (such as m\u2265n) the SVD of A is defined as [20]:", "startOffset": 65, "endOffset": 69}, {"referenceID": 14, "context": "If rank(A) = r, then [21] \u03a3 satisfies:", "startOffset": 21, "endOffset": 25}, {"referenceID": 14, "context": "Further-more, as demonstrated in [21], if a word combination pattern is salient and recurring in document, this pattern will be captured and represented by one of the singular vectors.", "startOffset": 33, "endOffset": 37}, {"referenceID": 11, "context": "Based on the above discussion, authors [18] proposed a summarization method which uses the matrix V.", "startOffset": 39, "endOffset": 43}, {"referenceID": 11, "context": "The proposed method in [18] is as follows:", "startOffset": 23, "endOffset": 27}, {"referenceID": 15, "context": "For text decomposition [22] uses:", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "Root-based versus Stem-based approaches Arabic stemming algorithms can be classified, according to the desired level of analysis, as root-based approach (Khoja [4]); and stem-based approach (Larkey [5]).", "startOffset": 160, "endOffset": 163}, {"referenceID": 2, "context": "Root-based versus Stem-based approaches Arabic stemming algorithms can be classified, according to the desired level of analysis, as root-based approach (Khoja [4]); and stem-based approach (Larkey [5]).", "startOffset": 198, "endOffset": 201}, {"referenceID": 16, "context": "Al-Fedaghi and Al-Anzi algorithms try to find the root of the word by matching the word with all possible patterns with all possible affixes attached to it [25].", "startOffset": 156, "endOffset": 160}, {"referenceID": 17, "context": "Al-Shalabi morphology system uses different algorithms to find the roots and patterns [26].", "startOffset": 86, "endOffset": 90}, {"referenceID": 1, "context": "Khoja has developed an algorithm that removes prefixes and suffixes, all the time checking that it\u2019s not removing part of the root and then matches the remaining word against the patterns of the same length to extract the root [4].", "startOffset": 227, "endOffset": 230}, {"referenceID": 18, "context": "Light stemmer is mentioned by some authors [27,28,5,29], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.", "startOffset": 43, "endOffset": 55}, {"referenceID": 19, "context": "Light stemmer is mentioned by some authors [27,28,5,29], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.", "startOffset": 43, "endOffset": 55}, {"referenceID": 2, "context": "Light stemmer is mentioned by some authors [27,28,5,29], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.", "startOffset": 43, "endOffset": 55}, {"referenceID": 20, "context": "Light stemmer is mentioned by some authors [27,28,5,29], but till now there is almost no standard algorithm for Arabic light stemming, all trials in this field were a set of rules to strip off a small set of suffixes and prefixes, also there is no definite list of these strippable affixes.", "startOffset": 43, "endOffset": 55}, {"referenceID": 1, "context": "Prior to applying document clustering techniques to an Arabic document, the latter is typically preprocessed: it is parsed, in order to remove stop words, and then words are stemmed using tow famous Stemming algorithms: the Morphological Analyzer from Khoja and Garside [4], and the Light Stemmer developed by Larkey [5].", "startOffset": 270, "endOffset": 273}, {"referenceID": 2, "context": "Prior to applying document clustering techniques to an Arabic document, the latter is typically preprocessed: it is parsed, in order to remove stop words, and then words are stemmed using tow famous Stemming algorithms: the Morphological Analyzer from Khoja and Garside [4], and the Light Stemmer developed by Larkey [5].", "startOffset": 317, "endOffset": 320}, {"referenceID": 3, "context": "This model is widely used in information retrieval and text mining [6].", "startOffset": 67, "endOffset": 70}, {"referenceID": 3, "context": "Cosine Similarity Cosine similarity is one of the most popular similarity measure applied to text documents, such as in numerous information retrieval applications [6] and clustering too [7].", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "Cosine Similarity Cosine similarity is one of the most popular similarity measure applied to text documents, such as in numerous information retrieval applications [6] and clustering too [7].", "startOffset": 187, "endOffset": 190}, {"referenceID": 5, "context": "The averaged KL divergence has recently been applied to clustering text documents, such as in the family of the Information Bottleneck clustering algorithms [8], to good effect.", "startOffset": 157, "endOffset": 160}, {"referenceID": 1, "context": "For the testing dataset, we experimented with different similarity measures for three times: without stemming, and with stemming using the Morphological Analyzer from Khoja and Garside [4] , and the Light Stemmer [5], in two case: in the first one, we apply the proposed method above to summarize for the all documents in dataset and then cluster them.", "startOffset": 185, "endOffset": 188}, {"referenceID": 2, "context": "For the testing dataset, we experimented with different similarity measures for three times: without stemming, and with stemming using the Morphological Analyzer from Khoja and Garside [4] , and the Light Stemmer [5], in two case: in the first one, we apply the proposed method above to summarize for the all documents in dataset and then cluster them.", "startOffset": 213, "endOffset": 216}, {"referenceID": 6, "context": "Results The quality of the clustering result was evaluated using two evaluation measures: purity and entropy, which are widely used to evaluate the performance of unsupervised learning algorithms [10] [11].", "startOffset": 196, "endOffset": 200}, {"referenceID": 7, "context": "Results The quality of the clustering result was evaluated using two evaluation measures: purity and entropy, which are widely used to evaluate the performance of unsupervised learning algorithms [10] [11].", "startOffset": 201, "endOffset": 205}, {"referenceID": 1, "context": "In the following, The Table 4 and the Table 5 show the average purity and entropy results for each similarity/distance measure with the Morphological Analyzer from Khoja and Garside [4], the Larkey\u2019s Stemmer [5], and without stemming using the full- text representation.", "startOffset": 182, "endOffset": 185}, {"referenceID": 2, "context": "In the following, The Table 4 and the Table 5 show the average purity and entropy results for each similarity/distance measure with the Morphological Analyzer from Khoja and Garside [4], the Larkey\u2019s Stemmer [5], and without stemming using the full- text representation.", "startOffset": 208, "endOffset": 211}, {"referenceID": 8, "context": "Second, the obtained overall entropy values shown in Tables 6 and 7 proves that the summarizing documents can make their topics salient and improve the clustering performance [13] for two times: with and without stemming.", "startOffset": 175, "endOffset": 179}], "year": 2013, "abstractText": "Arabic Documents Clustering is an important task for obtaining good results with the traditional Information Retrieval (IR) systems especially with the rapid growth of the number of online documents present in Arabic language. Documents clustering aim to automatically group similar documents in one cluster using different similarity/distance measures. This task is often affected by the documents length, useful information on the documents is often accompanied by a large amount of noise, and therefore it is necessary to eliminate this noise while keeping useful information to boost the performance of Documents clustering. In this paper, we propose to evaluate the impact of text summarization using the Latent Semantic Analysis Model on Arabic Documents Clustering in order to solve problems cited above, using five similarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard Coefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler Divergence, for two times: without and with stemming. Our experimental results indicate that our proposed approach effectively solves the problems of noisy information and documents length, and thus significantly improve the clustering performance.", "creator": "PDFCreator Version 1.2.3"}}}