{"id": "1302.2056", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Feb-2013", "title": "Complexity distribution of agent policies", "abstract": "We analyse the complexity of environments according to the policies that need to be used to achieve high performance. The performance results for a population of policies leads to a distribution that is examined in terms of policy complexity and analysed through several diagrams and indicators. The notion of environment response curve is also introduced, by inverting the performance results into an ability scale. We apply all these concepts, diagrams and indicators to a minimalistic environment class, agent-populated elementary cellular automata, showing how the difficulty, discriminating power and ranges (previous to normalisation) may vary for several environments.", "histories": [["v1", "Fri, 8 Feb 2013 15:01:20 GMT  (725kb,D)", "http://arxiv.org/abs/1302.2056v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["jose hernandez-orallo"], "accepted": false, "id": "1302.2056"}, "pdf": {"name": "1302.2056.pdf", "metadata": {"source": "CRF", "title": "Complexity distribution of agent policies", "authors": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "emails": ["(jorallo@dsic.upv.es)"], "sections": [{"heading": null, "text": "Keywords: algorithmic information theory, reinforcement learning, environmental difficulties, discriminatory power, agent policy, task difficulties, psychometry, elementary cellular automatiata.ar Xiv: 130 2.Contents"}, {"heading": "1 Introduction 2", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Where to look at? 4", "text": "......................................................................"}, {"heading": "3 The complexity of environments, actions and policies 10", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 A distribution of agent policies 13", "text": "4.1 Graphic analyses and indicators........................................................................."}, {"heading": "5 Case study: agent-populated elementary cellular automaton 22", "text": "5.1 Agents Colonized Elementary Cellular Automata: Definition and Examples 22 5.2 Experimental Setting........................................... 28 5.3 Distribution Analysis............................................................................................."}, {"heading": "6 Discussion 38", "text": "6.1 Comparison with other types of complexity............. 38 6.2 Consideration of merit-based heuristics........... 41 6.3 Applications, limitations and future work.......... 42"}, {"heading": "1 Introduction", "text": "In this sense, it is important that people are able to determine for themselves what they want and what they do not want. (...) It is important that people are able to determine for themselves. (...) It is important that people are able to determine for themselves. (...) It is important that people are able to determine for themselves. (...) It is important that people are able to determine for themselves. (...) \"(...)\" (...) \"(...)\" (...) \"(...)\" (...) \"(...) (...)\" (...) \"(...\" () (...) \"(...\" () \"(...\") () (() \"(...) ()\" () (... \"() () () (...) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () ()) () () () ()) () () () () ()) () () () () ()) () () ()) () () () () () () () () () ()) () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () ("}, {"heading": "2 Where to look at?", "text": "There is a plethora of terms around the term \"problem difficulty,\" such as task difficulty, problem hardness, or simply complexity, some of which we will review shortly. Instead of classifying them by their origin: psychometry, computer science, complexity theory, biology, etc., we group the approaches in the literature according to what they consider to set parameters such as difficulty or distinctiveness."}, {"heading": "2.1 Looking at the problem", "text": "In fact, most of them are able to determine for themselves what they want and what they don't want."}, {"heading": "2.2 Looking at the search space", "text": "Instead of looking at the complexity of the environment, a natural way of looking at the difficulty of a problem is to rely on the analysis of the search space. For example, in a labyrinth, the search space is the series of trajectories that can be performed in the labyrinth. Each trajectory is a sequence of actions. Analyzing how likely the solutions are in the trajectory space can work because in this particular case the environment is static (the labyrinth does not change according to the agent's actions). However, things are different for other types of environment where the environment really responds to the agent's actions. Generally, the same ideas and principles that are used in the areas of optimization and heuristics are applicable here. For example, it is said that a problem is difficult when there is a \"high density of well-separated almost solutions (local minima), or we have a\" robust solution space. \""}, {"heading": "2.3 Looking at the population of solvers", "text": "In psychometry, we are usually divided into several difficulty categories, and a variety of elements of varying difficulty are used to cover a wide range of skills that need to be measured. Answer Theory (IRT) [14] is a paradigm for the study of elements (tasks) and a well-founded way of designing tests and other instruments that measure skills, especially in the field of (computerized) adaptive tests. IRT is based on mathematical functions and associated probability and information estimates for each element, according to several models. A very common model for discrete score problems is the three-parameter model, in which the item-response function (or curve) corresponds to the probability that an agent with the ability to give such a model a correct response to an element. This model is characterized as follows: p (c + 1 \u2212 c1 \u2212 e \u2212 a), where the discrimination (the maximum slope of the curve) is based on the position, we (the difficulty), or the difficulty is the position of the (the curve)."}, {"heading": "3 The complexity of environments, actions and policies", "text": "The main objective of this work is to examine the difficulty an agent faces in a particular environment, but nevertheless with the formal way. To consider a very general setting (including games, for example), we will consider that there may be other actors in the environment. Multi-agent environments are usually based on a common space, transition rules, and reward system that are shared by all. A more formal definition follows: A multi-agent environment is a tuple < p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p; p) is a multi-agent environment."}, {"heading": "4 A distribution of agent policies", "text": "If the best policy (or one of the best strategies) has low complexity, one could say that the problem is simple. Although this is a simple interpretation, it is too narrow for several reasons. First, it could be the case that the best policy leads \u03c0 to R (\u03c0) = r, while the second best policy leads \u03c0 to R (\u03c0) = r. If it is (comparatively) very small, but K (\u03c0) < < K (\u03c0), we can even say that an almost equally good (but much simpler) policy can be found. Intuitively, the environment would look even simpler. Second, focusing on just one value is too risky if we plan to make a robust approximation of the value of difficulty, especially bearing in mind that in practice we will be forced to analyze samples of the agent population, not the entire population."}, {"heading": "4.1 Graphical analysis and indicators", "text": "If we look at the distributions in this typical number, we see that the distribution of the total distribution (conditional distributions) cannot actually be increased by their complexity. Given a class of agents that have a maximum distribution difference, we can give the following example: The aggregated reward per difficulty is given by the following distribution: R [k], {R]: \u03c0 [k]: We have a number of distributions. We will use the notation [\u2264] to represent all values of complexity from 1 to 1. For example, R [\u2264] = 1, known as the \"accumulated\" 7 version of R [k]. What R [k] looks like depends on the environment so that its form can give us information about its difficulty."}, {"heading": "4.2 Environment response functions", "text": "As we mentioned in Section 2, we can define a probability distribution by defining options for action. [14] This is a paradigm for studying items (tasks) and a principle-driven way of designing tests and other tools that measure ability, especially in the field of (computer-aided) adaptive testing. The most distinguishing feature of IRT is that each item (task) is categorized according to its difficulty and analyzed so that subjects of different skill levels can participate in the task. [16] We saw an example of the three-parameter logistics model and a linear model in Figure 1. Typically, for tasks that are guided by limited rewards, a look at the curves between the two models. Nevertheless, we do not even need to define a parametric model: what we really need is a function to determine that returns the expected reward at a competence level."}, {"heading": "5 Case study: agent-populated elementary cellular au-", "text": "tomatonTheoretically, we can analyze difficulties and distinctiveness for any kind of environment compatible with definition 1, i.e. any deterministic environment with discrete time. In this section, for practical reasons, we will opt for a simplifying environment. First, we are interested in minimalist environments where the number of observations and actions is extremely reduced, and therefore we have some relatively rich phenomena with very simple transitional functions. Second, we are interested in simplifying political languages in order to be able to quickly assess a large number of actors. Third, we want to address scenarios that are well known and already studied in terms of their origin and complexity. The configuration that we present next follows these three conditions."}, {"heading": "5.1 Agent-populated elementary cellular automata: definition and examples", "text": "The environments we work with are elementary cellular automata (ECA) = > 3. The definition defines the complete behavior of this type of environment: definition 5. A single cellular automaton (SAECA) is a special type of environment < p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "5.2 Experimental setting", "text": "In fact, the fact is that most of them are able to move to another world, in which they are able, in which they are able to integrate, and in which they are able, in which they are able to change the world."}, {"heading": "5.3 Analysis of the distribution", "text": "As we mentioned above, we have conducted experiments with a Fresh Strategy and a Chained Strategy, with an average reward of less than 110% of random walk strategies. Figure 8 shows the distributions R [k] for different environments (rules 184, 110, 122, 164) using the Freshstrategy. In the figure, each small circle (in gray) shows a policy. The box charts and envelopes show a higher concentration of values between 5 and 10, because the policy of a uniform distribution between 0 and 20. We see that the plots are very different in terms of average, range and evolution. The environment with rule 110 evolves very quickly for the maximum envelope, while rule 122 exhibits a slower trend."}, {"heading": "5.4 Estimation of the environment response curves", "text": "First of all, we give a few hints on how the environmental reaction curves are calculated for each case, & pos 101 in Equation 11, using k as approximation to K (\u03c0) rule: 184 rule: 110 and then random over the previously built population of 1,900 policies. Since we have to reverse the functions Dpos and Dneg, we calculate 101 different values of \u03b3 in equal intervals from 0 to 1. We calculate whether this value is greater than or equal (Rmax \u2212 Rmean) + Rmean as in Equation 8 for each of these samples, resulting in an arrangement of Boolean values. < this is an arrangement of 1,900. We know whether this value is greater or equal (Rmax \u2212 Rmean) + Rmean as in Equation 8 for each of these samples, resulting in an arrangement of Boolean values. < this is an arrangement of 1,900."}, {"heading": "6 Discussion", "text": "In Section 2 we have mentioned many other approaches to the concepts of difficulty and discriminatory power. We will now compare our proposal and our results with some of these earlier concepts and place the contributions, limitations and future work in this context."}, {"heading": "6.1 Comparing to other kinds of complexity", "text": "In Section 3, we have derived some upper limits by evaluating the complexity of the entire environment. In the setting we have seen in Section 5, this would have implied compressing the definition of the configuration rules. Further compressed implementations are possible, but it is difficult to conceive of an implementation whose length is less than a few thousand bits. So, statement 1 is not useful as an approximation of the difficulty in this case. One way out might consider some basic settings or classes of environments from which we simply add some parameters or rules. We might assume that the reference engine contains the basics for evolutionary cell automata as well as the way in which we deal with the agents and rewards."}, {"heading": "6.2 Taking performance-guided heuristics into account", "text": "In fact, most of us will be able to play by the rules we have set ourselves."}, {"heading": "6.3 Applications, limitations and future work", "text": "This year it is so far that it will be able to erenie.nln the aforementioned lcihsrteeSe rf\u00fc eid eerwtlrteeoiietcnlhsrteeSrteeu rf\u00fc eid nlrsrteeoiietlrVrteeu"}, {"heading": "Acknowledgements", "text": "The implementation of the elementary cellular automatons used in the environments is based on the library \"CellularAutomaton\" by John Hughes for R [47. I am grateful to Fernando Soler-Toscano for informing me about their work [54] on the complexity of 2D objects generated by elementary cellular automatons."}], "references": [{"title": "Programmable reinforcement learning agents", "author": ["D. Andre"], "venue": "PhD thesis, University of California,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Computational depth: Concept and applications", "author": ["L. Antunes", "L. Fortnow", "D. van Melkebeek", "N.V. Vinodchandran"], "venue": "Theoretical Computer Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Chapter 8 working memory, automaticity, and problem difficulty", "author": ["M.H. Ashcraft", "R.D. Donley", "M.A. Halas", "M. Vakali"], "venue": "Jamie I.D. Campbell, editor, The Nature and Origins of Mathematical Skills, volume 91 of Advances in Psychology, pages 301 \u2013 329. North-Holland,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1992}, {"title": "Effective complexity and its relation to logical depth", "author": ["N. Ay", "M. Mueller", "A. Szkola"], "venue": "ArXiv e-prints, 0810.5663, October", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Dissociating working memory from task difficulty in human prefrontal cortex", "author": ["D.M. Barch", "T.S. Braver", "L.E. Nystrom", "S.D. Forman", "D.C. Noll", "J.D. Cohen"], "venue": "Neuropsychologia, 35(10):1373\u20131380,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "A comprehensive survey of multiagent reinforcement learning", "author": ["L. Busoniu", "R. Babuska", "B. De Schutter"], "venue": "Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on, 38(2):156\u2013172,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Algorithmic information theory", "author": ["G.J. Chaitin"], "venue": "IBM J. Res. Develop., 21:350\u2013 359,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1977}, {"title": "Sophistication and logical depth revisited", "author": ["F.B. Chedid"], "venue": "Computer Systems and Applications (AICCSA), 2010 IEEE/ACS International Conference on, pages 1\u20134. IEEE,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Where the really hard problems are", "author": ["P. Cheeseman", "B. Kanefsky", "W.M. Taylor"], "venue": "Proceedings of the 12th IJCAI, pages 331\u2013337,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1991}, {"title": "Numerical evaluation of algorithmic complexity for short strings: A glance into the innermost structure of randomness", "author": ["J.P. Delahaye", "H. Zenil"], "venue": "Applied Mathematics and Computation,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "IQ tests are not for machines, yet", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo"], "venue": "Intelligence, 40(2):77 \u2013 81,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Compression and intelligence: social environments and communication", "author": ["D.L. Dowe", "J. Hern\u00e1ndez-Orallo", "P.K. Das"], "venue": "J. Schmidhuber, K.R. Th\u00f3risson, and M. Looks (eds), editors, Artificial General Intelligence 2011, volume 6830, pages 204\u2013211. LNAI series, Springer,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Theory of computational complexity, volume 58", "author": ["D.Z. Du", "K.I. Ko"], "venue": "Wiley- Interscience,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Item response theory for psychologists", "author": ["S.E. Embretson", "S.P. Reise"], "venue": "Lawrence Erlbaum,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Difficulty, discrimination, and information indices in the linear factor analysis model for continuous item responses", "author": ["P.J. Ferrando"], "venue": "Applied Psychological Measurement, 33(1):9\u201324,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Assessing the discriminating power of item and test scores in the linear factor-analysis model", "author": ["P.J. Ferrando"], "venue": "Psicol\u00f3gica, 33:111\u2013139,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Complexity and information: Measuring emergence, self-organization, and homeostasis at multiple scales", "author": ["C. Gershenson", "N. Fernandez"], "venue": "Complexity,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Problem difficulty and response format in syllogistic reasoning", "author": ["D.K. Hardman", "S.J. Payne"], "venue": "The Quarterly Journal of Experimental Psychology, 48(4):945\u2013 975,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "A note on problem difficulty measures in black-box optimization: Classification, realizations and predictability", "author": ["J. He", "C. Reeves", "C. Witt", "X. Yao"], "venue": "Evolutionary Computation, 15(4):435\u2013443,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Computational measures of information gain and reinforcement in inference processes", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "PhD thesis, DSIC, Universitat de Valencia,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Beyond the Turing Test", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "J. Logic, Language & Information, 9(4):447\u2013466,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "On the computational measurement of intelligence factors", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "A. Meystel, editor, Performance metrics for intelligent systems workshop, pages 1\u20138. National Institute of Standards and Technology, Gaithersburg, MD, U.S.A.,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Thesis: Computational measures of information gain and reinforcement in inference processes", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "AI Communications, 13(1):49\u201350,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2000}, {"title": "On discriminative environments, randomness, two-part compression and MML", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "Technical Report, available at http://users.dsic.upv.es/proy/anynt/,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "A (hopefully) non-biased universal environment class for measuring intelligence of biological and artificial systems", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "M. Hutter et al., editor, Artificial General Intelligence, 3rd Intl Conf, pages 182\u2013183. Atlantis Press, Extended report at http://users.dsic.upv.es/proy/anynt/unbiased.pdf,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "On evaluating agent performance in a fixed period of time", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": "M. Hutter et al., editor, Artificial General Intelligence, 3rd Intl Conf, pages 25\u201330. Atlantis Press,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Measuring universal intelligence: Towards an anytime intelligence test", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "Artificial Intelligence, 174(18):1508 \u2013 1539,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "On more realistic environment distributions for defining, evaluating and developing intelligence", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Insa-Cabrera"], "venue": "J. Schmidhuber, K.R. Th\u00f3risson, and M. Looks (eds), editors, Artificial General Intelligence 2011, volume 6830, pages 82\u201391. LNAI series, Springer,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "On potential cognitive abilities in the machine kingdom", "author": ["J. Hern\u00e1ndez-Orallo", "D.L. Dowe"], "venue": "Minds and Machines,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Turing tests with Turing machines", "author": ["J. Hern\u00e1ndez-Orallo", "J. Insa", "D.L. Dowe", "B. Hibbard"], "venue": "Andrei Voronkov, editor, The Alan Turing Centenary Conference, Turing-100, Manchester, volume 10 of EPiC Series, pages 140\u2013 156,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "A formal definition of intelligence based on an intensional variant of Kolmogorov complexity", "author": ["J. Hern\u00e1ndez-Orallo", "N. Minaya-Collado"], "venue": "Proc. Intl Symposium of Engineering of Intelligent Systems (EIS\u201998), pages 146\u2013163. ICSC Press,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1998}, {"title": "Sat-encodings, search space structure, and local search performance", "author": ["H.H. Hoos"], "venue": "International Joint Conference on Artificial Intelligence, volume 16, pages 296\u2013303. Citeseer,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1999}, {"title": "On measuring social intelligence: Experiments on competition and cooperation", "author": ["J. Insa-Cabrera", "J.L. Benacloch-Ayuso", "Hern\u00e1ndez-Orallo J"], "venue": "AGI, volume 7716 of Lecture Notes in Computer Science,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Comparing humans and AI agents", "author": ["J. Insa-Cabrera", "D.L. Dowe", "S. Espa\u00f1a-Cubillo", "M.V. Hern\u00e1ndez-Lloreda", "J. Hern\u00e1ndez-Orallo"], "venue": "J. Schmidhuber, K.R. Th\u00f3risson, and M. Looks (eds), editors, Artificial General Intelligence 2011, volume 6830, pages 122\u2013132. LNAI series, Springer,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}, {"title": "Sorting and searching, volume 3 of the art of computer programming", "author": ["D.E. Knuth"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1973}, {"title": "What makes some problems really hard: Explorations in the problem space of difficulty", "author": ["K. Kotovsky", "H.A. Simon"], "venue": "Cognitive Psychology, 22(2):143\u2013183,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1990}, {"title": "Machine Super Intelligence", "author": ["S. Legg"], "venue": "Department of Informatics, University of Lugano, June", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["S. Legg", "M. Hutter"], "venue": "Minds and Machines, 17(4):391\u2013444,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "Universal sequential search problems", "author": ["L.A. Levin"], "venue": "Problems of Information Transmission, 9(3):265\u2013266,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1973}, {"title": "Average case complete problems", "author": ["L.A. Levin"], "venue": "SIAM Journal on Computing, 15:285,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 1986}, {"title": "An introduction to Kolmogorov complexity and its applications (3rd ed.)", "author": ["M. Li", "P. Vit\u00e1nyi"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "Motif difficulty (md): a predictive measure of problem difficulty for evolutionary algorithms using network motifs", "author": ["J. Liu", "H.A. Abbass", "D.G. Green", "W. Zhong"], "venue": "Evolutionary Computation, 20(3):321\u2013347,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Transfer of experience between reinforcement learning environments with progressive difficulty", "author": ["M.G. Madden", "T. Howley"], "venue": "Artificial Intelligence Review, 21(3):375\u2013398,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2004}, {"title": "Generalized linear item response theory", "author": ["G.J. Mellenbergh"], "venue": "Psychological Bulletin, 115(2):300,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1994}, {"title": "The magical number seven, plus or minus two: some limits on our capacity for processing information", "author": ["G.A. Miller"], "venue": "Psychological review, 63(2):81,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1956}, {"title": "Instance complexity", "author": ["P. Orponen", "K.I. Ko", "U. Sch\u00f6ning", "O. Watanabe"], "venue": "Journal of the ACM (JACM), 41(1):96\u2013121,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1994}, {"title": "R: A language and environment for statistical computing", "author": ["R Team"], "venue": "R Foundation for Statistical Computing, Vienna,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "Human acquisition of concepts for sequential patterns", "author": ["H.A. Simon", "K. Kotovsky"], "venue": "Psychological Review, 70(6):534,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 1963}, {"title": "Statistical and Inductive Inference by Minimum Message Length", "author": ["C.S. Wallace"], "venue": "Springer-Verlag,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2005}, {"title": "A new kind of science", "author": ["S. Wolfram"], "venue": "Wolfram media Champaign, IL,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning mazes with aliasing states: An LCS algorithm with associative perception", "author": ["Z. Zatuchna", "A. Bagnall"], "venue": "Adaptive Behavior, 17(1):28\u201357,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2009}, {"title": "Compression-based investigation of the dynamical properties of cellular automata and other systems", "author": ["H. Zenil"], "venue": "Complex Systems, 19(1):1\u201328,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2010}, {"title": "Une approche exp\u00e9rimentale \u00e0 la th\u00e9orie algorithmique de la complexit\u00e9", "author": ["H. Zenil"], "venue": "PhD thesis, Dissertation in fulfilment of the degree of Doctor in Computer Science, Universit\u00e9 de Lille,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2011}, {"title": "Two-dimensional kolmogorov complexity and validation of the coding theorem method by compressibility", "author": ["H. Zenil", "F. Soler-Toscano", "J.P. Delahaye", "N. Gauvrit"], "venue": "arXiv preprint arXiv:1212.6745,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "Item Response Theory (IRT) [14], for instance, is a well-founded approach in psychometrics where the difficulty (in terms of response curves) for each item is used to construct more effective tests, in order to derive scores and to obtain reliability measures.", "startOffset": 27, "endOffset": 31}, {"referenceID": 40, "context": "Another distinctive feature of our approach is that we will assess environment difficulty using algorithmic information theory [41] (by estimating the Kolmogorov complexity of each policy) in order to (1) evaluate the information content of the policy, (2) make the complexity measure more independent of the representation language, due to the invariance theorem2, and (3) consider all (or many of) the alternative (but equivalent) expressions of the same policy, so making the estimation more robust, as with Solomonoff\u2019s algorithmic probability (this is exploited by the coding theorem method [10, 53]).", "startOffset": 127, "endOffset": 131}, {"referenceID": 9, "context": "Another distinctive feature of our approach is that we will assess environment difficulty using algorithmic information theory [41] (by estimating the Kolmogorov complexity of each policy) in order to (1) evaluate the information content of the policy, (2) make the complexity measure more independent of the representation language, due to the invariance theorem2, and (3) consider all (or many of) the alternative (but equivalent) expressions of the same policy, so making the estimation more robust, as with Solomonoff\u2019s algorithmic probability (this is exploited by the coding theorem method [10, 53]).", "startOffset": 596, "endOffset": 604}, {"referenceID": 52, "context": "Another distinctive feature of our approach is that we will assess environment difficulty using algorithmic information theory [41] (by estimating the Kolmogorov complexity of each policy) in order to (1) evaluate the information content of the policy, (2) make the complexity measure more independent of the representation language, due to the invariance theorem2, and (3) consider all (or many of) the alternative (but equivalent) expressions of the same policy, so making the estimation more robust, as with Solomonoff\u2019s algorithmic probability (this is exploited by the coding theorem method [10, 53]).", "startOffset": 596, "endOffset": 604}, {"referenceID": 40, "context": "The Kolmogorov complexity of the same object using two different description mechanism is the same, up to a constant that is independent of the object [41].", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "In computational complexity theory [13], the focus is usually put on class complexity, e.", "startOffset": 35, "endOffset": 39}, {"referenceID": 39, "context": "Nonetheless, the notion of instance complexity has also been considered, related to the concept of average-case computational complexity, developed by Leonid Levin in the 1980s [40], which is of course related to the more general notion of average-case performance of algorithms (see, e.", "startOffset": 177, "endOffset": 181}, {"referenceID": 34, "context": ", [35]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 44, "context": "This stance has been taken by psychology and other cognitive sciences, where there is an old history of approaches using the concept of working memory or the number of elements that must be considered at the same time in order to solve a problem [45, 3, 18, 5].", "startOffset": 246, "endOffset": 260}, {"referenceID": 2, "context": "This stance has been taken by psychology and other cognitive sciences, where there is an old history of approaches using the concept of working memory or the number of elements that must be considered at the same time in order to solve a problem [45, 3, 18, 5].", "startOffset": 246, "endOffset": 260}, {"referenceID": 17, "context": "This stance has been taken by psychology and other cognitive sciences, where there is an old history of approaches using the concept of working memory or the number of elements that must be considered at the same time in order to solve a problem [45, 3, 18, 5].", "startOffset": 246, "endOffset": 260}, {"referenceID": 4, "context": "This stance has been taken by psychology and other cognitive sciences, where there is an old history of approaches using the concept of working memory or the number of elements that must be considered at the same time in order to solve a problem [45, 3, 18, 5].", "startOffset": 246, "endOffset": 260}, {"referenceID": 47, "context": "In this line, and closely related to the emerging field of artificial intelligence, Simon and Kotovsky explored, for decades, \u201cthe problem space of difficulty\u201d [48, 36].", "startOffset": 160, "endOffset": 168}, {"referenceID": 35, "context": "In this line, and closely related to the emerging field of artificial intelligence, Simon and Kotovsky explored, for decades, \u201cthe problem space of difficulty\u201d [48, 36].", "startOffset": 160, "endOffset": 168}, {"referenceID": 10, "context": "The use of these measures for non-human subjects (especially for machines) may be misleading since it has been shown that many tasks that are very difficult for humans are very easy for machines and vice versa [11].", "startOffset": 210, "endOffset": 214}, {"referenceID": 40, "context": "A more mathematical (and computational) approach for associating complexity with the number (or the size) of items that are necessary to explain a concept (or solve a problem) is now known as algorithmic information theory (also known as Kolmogorov complexity [41]).", "startOffset": 260, "endOffset": 264}, {"referenceID": 38, "context": "Several alternatives, such as Levin\u2019s Kt [39], logical depth [7], effective complexity [4], computational depth [2], sophistication [8], and others (see [41, chap.", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "Several alternatives, such as Levin\u2019s Kt [39], logical depth [7], effective complexity [4], computational depth [2], sophistication [8], and others (see [41, chap.", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "Several alternatives, such as Levin\u2019s Kt [39], logical depth [7], effective complexity [4], computational depth [2], sophistication [8], and others (see [41, chap.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "Several alternatives, such as Levin\u2019s Kt [39], logical depth [7], effective complexity [4], computational depth [2], sophistication [8], and others (see [41, chap.", "startOffset": 112, "endOffset": 115}, {"referenceID": 7, "context": "Several alternatives, such as Levin\u2019s Kt [39], logical depth [7], effective complexity [4], computational depth [2], sophistication [8], and others (see [41, chap.", "startOffset": 132, "endOffset": 135}, {"referenceID": 30, "context": "For instance, a variant of Kt, known as intensional complexity, accurately captured the difficulty humans found on IQ test series problems [31, 21].", "startOffset": 139, "endOffset": 147}, {"referenceID": 20, "context": "For instance, a variant of Kt, known as intensional complexity, accurately captured the difficulty humans found on IQ test series problems [31, 21].", "startOffset": 139, "endOffset": 147}, {"referenceID": 45, "context": "Outside induction, the idea of instance complexity [46][41, sec.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "For other kinds of problems, such as general inductive and deductive problems, [20, 23, 22] also explored the use of algorithmic information theory, by considering the information gain from the problem to the solution.", "startOffset": 79, "endOffset": 91}, {"referenceID": 22, "context": "For other kinds of problems, such as general inductive and deductive problems, [20, 23, 22] also explored the use of algorithmic information theory, by considering the information gain from the problem to the solution.", "startOffset": 79, "endOffset": 91}, {"referenceID": 21, "context": "For other kinds of problems, such as general inductive and deductive problems, [20, 23, 22] also explored the use of algorithmic information theory, by considering the information gain from the problem to the solution.", "startOffset": 79, "endOffset": 91}, {"referenceID": 36, "context": "This is why some works have advocated for ergodic environments [37], where the agent can always recover from a local \u2018hell\u2019 or \u2018heaven\u2019.", "startOffset": 63, "endOffset": 67}, {"referenceID": 25, "context": "Also, the choice of the aggregate reward function is crucial, especially if time is taken into account [26].", "startOffset": 103, "endOffset": 107}, {"referenceID": 28, "context": "Finally, the evaluated agent can evolve during the process and become a different agent [29].", "startOffset": 88, "endOffset": 92}, {"referenceID": 50, "context": ", maze size, [51]) or the state space [43], have been used as approximations of the \u2018difficulty\u2019 of an environment.", "startOffset": 13, "endOffset": 17}, {"referenceID": 42, "context": ", maze size, [51]) or the state space [43], have been used as approximations of the \u2018difficulty\u2019 of an environment.", "startOffset": 38, "endOffset": 42}, {"referenceID": 26, "context": "In [27], another variant of Kolmogorov complexity (Ktmax) was suggested as a measure of complexity for the environments, but it was already stated that this approximation was unidirectional, since some very complex environments might be easy, i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "Following these ideas, an environment class was introduced in [25], where the difficulty of an environment can be approximated by the Kolmogorov complexity of the rules (as a Markov algorithm) that describe the environment.", "startOffset": 62, "endOffset": 66}, {"referenceID": 33, "context": "The class is used to create intelligence tests in [34], but the approximation of difficulty is not satisfactory.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "In fact, some other related works also mention the importance of being able to assess the difficulty of the environment, such [12] and to achieve discriminating power [24].", "startOffset": 126, "endOffset": 130}, {"referenceID": 23, "context": "In fact, some other related works also mention the importance of being able to assess the difficulty of the environment, such [12] and to achieve discriminating power [24].", "startOffset": 167, "endOffset": 171}, {"referenceID": 5, "context": "For general multiagent systems and multi-agent reinforcement learning [6] in particular, the difficulty of the system depends on the opponents and cooperators [33], and their intelligence [28, 30].", "startOffset": 70, "endOffset": 73}, {"referenceID": 32, "context": "For general multiagent systems and multi-agent reinforcement learning [6] in particular, the difficulty of the system depends on the opponents and cooperators [33], and their intelligence [28, 30].", "startOffset": 159, "endOffset": 163}, {"referenceID": 27, "context": "For general multiagent systems and multi-agent reinforcement learning [6] in particular, the difficulty of the system depends on the opponents and cooperators [33], and their intelligence [28, 30].", "startOffset": 188, "endOffset": 196}, {"referenceID": 29, "context": "For general multiagent systems and multi-agent reinforcement learning [6] in particular, the difficulty of the system depends on the opponents and cooperators [33], and their intelligence [28, 30].", "startOffset": 188, "endOffset": 196}, {"referenceID": 49, "context": ", [50]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 53, "context": "of the patterns that an elementary (1-dimensional) cellular automaton generates [54].", "startOffset": 80, "endOffset": 84}, {"referenceID": 8, "context": "For instance, it is said that a problem is difficult if there is a \u201chigh density of well-separated almost solutions (local minima)\u201d [9] or we have a \u201crugged solution space\u201d [32].", "startOffset": 132, "endOffset": 135}, {"referenceID": 31, "context": "For instance, it is said that a problem is difficult if there is a \u201chigh density of well-separated almost solutions (local minima)\u201d [9] or we have a \u201crugged solution space\u201d [32].", "startOffset": 173, "endOffset": 177}, {"referenceID": 18, "context": "[19] includes an account of approaches for problem difficulty measures in evolutionary computation, appeared in the past twenty years.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Item response theory (IRT) [14] is a paradigm for the study of items (tasks) and a well-grounded way of designing tests and other instruments that measure abilities, especially in the area of (computerised) adaptive testing.", "startOffset": 27, "endOffset": 31}, {"referenceID": 43, "context": "For continuous score items, a very frequent approach is the linear model [44, 15]: X(\u03b8) , z + \u03bb\u03b8 + where z is the intercept (zero-ability expected result), \u03bb is the loading or slope, and is the measurement error.", "startOffset": 73, "endOffset": 81}, {"referenceID": 14, "context": "For continuous score items, a very frequent approach is the linear model [44, 15]: X(\u03b8) , z + \u03bb\u03b8 + where z is the intercept (zero-ability expected result), \u03bb is the loading or slope, and is the measurement error.", "startOffset": 73, "endOffset": 81}, {"referenceID": 15, "context": "Again, the slope \u03bb is positively related to most measures of discriminating power [16].", "startOffset": 82, "endOffset": 86}, {"referenceID": 13, "context": "2 Environment response functions As we mentioned in section 2, item response theory (IRT) [14] is a paradigm for the study of items (tasks) and a more principled way of designing tests and other instruments that measure abilities, especially in the area of (computerised) adaptive testing.", "startOffset": 90, "endOffset": 94}, {"referenceID": 49, "context": "1 Agent-populated elementary cellular automata: definition and examples The environments we will work with are will use elementary cellular automata (ECA) [50] for the space \u03c3 and the transition function \u03c4 , and will let the agent see and modify part of the usual behaviour of the automaton.", "startOffset": 155, "endOffset": 159}, {"referenceID": 49, "context": "The transition function \u03c4 is given by \u03bd, as any of the 22 3 = 256 rules that can be defined looking at each cell and its two neighbours according to the numbering scheme convention introduced in [50].", "startOffset": 195, "endOffset": 199}, {"referenceID": 0, "context": ", [1]).", "startOffset": 2, "endOffset": 5}, {"referenceID": 49, "context": ", [50]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 51, "context": "In particular, we used a common approach [52, 34]: we coded the program as a character string and compressed it (using the memCompress function in R [47], a GNU project implementation of Lempel-Ziv coding).", "startOffset": 41, "endOffset": 49}, {"referenceID": 33, "context": "In particular, we used a common approach [52, 34]: we coded the program as a character string and compressed it (using the memCompress function in R [47], a GNU project implementation of Lempel-Ziv coding).", "startOffset": 41, "endOffset": 49}, {"referenceID": 46, "context": "In particular, we used a common approach [52, 34]: we coded the program as a character string and compressed it (using the memCompress function in R [47], a GNU project implementation of Lempel-Ziv coding).", "startOffset": 149, "endOffset": 153}, {"referenceID": 51, "context": "Fortunately, a deep and insightful analysis of the space-time diagrams have recently been performed by several studies [52, 54].", "startOffset": 119, "endOffset": 127}, {"referenceID": 53, "context": "Fortunately, a deep and insightful analysis of the space-time diagrams have recently been performed by several studies [52, 54].", "startOffset": 119, "endOffset": 127}, {"referenceID": 51, "context": "If we just focus on the four rules we have used throughout the experimental section: 184, 110, 122 and 164, we have that [52], which uses the compression method to approximate Kolmogorov complexity, sorts them (from lowest to highest complexity) as follows: 164, 184, 122, 110.", "startOffset": 121, "endOffset": 125}, {"referenceID": 53, "context": "A related study, [54], which uses the coding theorem method through a \u2018block matrix decomposition\u2019, reaches slightly different values but exactly the same order.", "startOffset": 17, "endOffset": 21}, {"referenceID": 51, "context": "Clearly, rules 164 and 184 lead to less complex space-time diagrams (they are the simplest according to [52, 54] and intuitively in Figure 5), so simple policies in these cases are not expected to create random-like (i.", "startOffset": 104, "endOffset": 112}, {"referenceID": 53, "context": "Clearly, rules 164 and 184 lead to less complex space-time diagrams (they are the simplest according to [52, 54] and intuitively in Figure 5), so simple policies in these cases are not expected to create random-like (i.", "startOffset": 104, "endOffset": 112}, {"referenceID": 38, "context": "Any reasonable (and feasible) sample will typically build short programs first instead of long ones, so this will be closely related to Levin\u2019s optimal search [39] any way, which is known to asymptotically dominate any other search.", "startOffset": 159, "endOffset": 163}, {"referenceID": 48, "context": "heuristics [49]).", "startOffset": 11, "endOffset": 15}, {"referenceID": 18, "context": "This would also connect this work to all the previous analysis on difficulty made in the area of evolutionary computation [19].", "startOffset": 122, "endOffset": 126}, {"referenceID": 41, "context": "In this sense, some recent approaches based on the notion of motif, such as \u2018motif difficulty\u2019 [42] would be worth being investigated in relation to this work.", "startOffset": 95, "endOffset": 99}, {"referenceID": 37, "context": "Note that a meaningful choice of each environment is much more effective than choosing them arbitrarily, can replace or complement those evaluations based on repositories and it is certainly more effective than choosing all the environments or selecting them by a (universal) distribution, as in [38].", "startOffset": 296, "endOffset": 300}], "year": 2013, "abstractText": "We analyse the complexity of environments according to the policies that need to be used to achieve high performance. The performance results for a population of policies leads to a distribution that is examined in terms of policy complexity and analysed through several diagrams and indicators. The notion of environment response curve is also introduced, by inverting the performance results into an ability scale. We apply all these concepts, diagrams and indicators to a minimalistic environment class, agent-populated elementary cellular automata, showing how the difficulty, discriminating power and ranges (previous to normalisation) may vary for several environments.", "creator": "LaTeX with hyperref package"}}}