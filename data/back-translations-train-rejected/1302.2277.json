{"id": "1302.2277", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Feb-2013", "title": "A Time Series Forest for Classification and Feature Extraction", "abstract": "We propose a tree ensemble method, referred to as time series forest (TSF), for time series classification. TSF employs a combination of the entropy gain and a distance measure, referred to as the Entrance (entropy and distance) gain, for evaluating the splits. Experimental studies show that the Entrance gain criterion improves the accuracy of TSF. TSF randomly samples features at each tree node and has a computational complexity linear in the length of a time series and can be built using parallel computing techniques such as multi-core computing used here. The temporal importance curve is also proposed to capture the important temporal characteristics useful for classification. Experimental studies show that TSF using simple features such as mean, deviation and slope outperforms strong competitors such as one-nearest-neighbor classifiers with dynamic time warping, is computationally efficient, and can provide insights into the temporal characteristics.", "histories": [["v1", "Sat, 9 Feb 2013 22:56:45 GMT  (630kb)", "https://arxiv.org/abs/1302.2277v1", null], ["v2", "Mon, 18 Feb 2013 00:10:56 GMT  (651kb)", "http://arxiv.org/abs/1302.2277v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["houtao deng", "george runger", "eugene tuv", "martyanov vladimir"], "accepted": false, "id": "1302.2277"}, "pdf": {"name": "1302.2277.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Houtao Deng", "George Runger"], "emails": ["hdeng3@asu.edu", "hdeng3@asu.edu", "george.runger@asu.edu", "eugene.tuv@intel.com", "vladimir.martyanov@intel.com"], "sections": [{"heading": null, "text": "The TSF uses a combination of entropy gain and a distance measure called entropy gain and distance gain to evaluate the splits. Experimental studies show that entropy gain improves the accuracy of the TSF. TSF randomly selects features at each tree node and has linear computational complexity in the length of the time series and can be constructed using parallel computer techniques. The temporal meaning curve is proposed to capture the temporal features useful for classification. Experimental studies show that TSF is computationally efficient with simple features such as mean, standard deviation and slope, and strong competitors such as one of the closest neighboring classifiers with dynamic time warping.Keywords: decision tree; entropy ensemble; rance gain; interpretability; large margin; time series classification; Hauser; addresses: @ mirgeed.com (@ mored.com)"}, {"heading": "1. Introduction", "text": "In fact, the value of each point in time can be regarded as a characteristic and can lead to unsatisfactory accuracy."}, {"heading": "2. Definition and Related Work", "text": "Given the fact that N training time series (examples) are better than NEuclidean (dynamic time shifts), the classification of time is considered effective. [23,...,..., yN], where yi [1, 2,..., C], the goal of the time series classification is to predict the class labels for test instances. Here, we assume that the values of the time series are measured in evenly graded intervals and also assume that the training and test time series instances of the same length M.Time series classification methods can be successfully divided into instance-based and characterization-based methods. Instance-based classifiers predict a test series on its similarity to training instances. Among instance-based classifiers, the closest classes with Euclidean distance (NNEuclidean distance) or dynamic time shifts (NDTW) are widely and successfully used."}, {"heading": "3. Interval Features", "text": "Interval characteristics are calculated from a time series interval, e.g. \"the interval between time10 and time 30.\" Many types of characteristics within a time interval can be taken into account, but one can prefer simple and interpretable characteristics such as the mean and standard deviation, e.g. \"the average of the time series segment between time 10 and time 30.\" Let K be the number of feature types and fk (\u00b7) (k = 1, 2,..., K) the kth type. Let us consider three types: f1 = mean value, f2 = standard deviation, f3 = slope. Let fk (t1, t2) for 1 \u2264 t2 \u2264 M be the kth interval characteristic calculated using the interval between t1 and t2. Let vi be the value for a time series example i and f2. Then let fk (t1, t2 = 1 \u2212 1) = 1 (t1), f1 = 1 \u2212 1 (1)."}, {"heading": "4. Time Series Forest Classifier", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Splitting criterion", "text": "A time series tree is the basic component of a forest time sequence, and the splitting criterion is used to determine the best method of splitting a node into a tree. (A candidate who splits a node into a time series tree is). (A candidate who splits a node into a time series tree is the following condition (for simplicity and without loss of generality): fk (t1), n), n), n (2), n), n (2), n). (The instances that meet the condition are sent to the left node. Otherwise, the instances will be node.Let {fnk (t1, t2), n), n), n), n), n (n), n), n). (The values of fk (t1, t2) for all education classes at Equation 4, one can select the characteristics of all training classes and then sort the best training classes (the best strategy)."}, {"heading": "4.2. Time Series Tree and Time Series Forest", "text": "The construction of a time series tree follows a top-to-bottom recursive strategy similar to standard decision tree algorithms, but uses random gain as a division criterion. In addition, the random sample strategy applied in Random Forest (RF) [1] is taken into account here. At each node, RF tests only randomly sampled p-characteristics from the complete feature set consisting of p-characteristics. In each time series tree node, we look at the random sampling of O (\u221a M) interval sizes and O (\u221a M) starting positions. Therefore, the feature space is reduced to only O (M). The sampling algorithm is illustrated in Algorithm 1. The time series tree algorithm is represented in Algorithm 2. For simplicity, we assume that different feature types are on the same scale, so that E can be compared to different characteristics, if different characteristics can be applied to different previously."}, {"heading": "4.3. Computational Complexity", "text": "For each node, the calculation of the splitting criterion of a single interval characteristic has the complexity O (nij\u0443), where \u03ba is the number of candidate thresholds. Since O (M) interval characteristics are randomly selected for evaluation, the complexity for the evaluation of characteristics on a node is O (nijM\u0443). Since \u03ba is considered a constant, the complexity on a node is O (nijM). The total number of instances at each depth is at most N (i.e., nationwide j n i j \u2264 N). Therefore, the complexity at ith depth in the tree is O (nationwide j n i jM) \u2264 O (NM). If one assumes the maximum depth of a tree model O (logN) [19], the complexity of a time series tree is O (MN \u2264 N). Therefore, the complexity of the complexity of a tree in most time series is nO (N)."}, {"heading": "4.4. Temporal Importance Curve", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "5. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Experimental Setup", "text": "The main functions of the TSF algorithm were implemented in Matlab, while mathematically expensive sub-functions such as interval function calculations were written in C. The parameters were set as follows: the number of trees = 500, f (\u00b7) = {mean, standard deviation, slope} and the number of candidate thresholds B = 20. TSF was applied to a series of benchmark datasets from time series [9] summarized in Table 1. The setting for training / test is the same as in Keogh et al. [9]. The experiments were conducted on a computer with four cores and the TSF algorithm was built in parallel. The purpose of the experiments is to answer the following questions: (1) Can the temporal importance of the curves of the input script improve accuracy performance and how is the accuracy performance of the TSF useful in comparison with other timeframe classifiers? (2) Is the TSF computationally efficient? (3) The knowledge of some time curve classification?"}, {"heading": "5.2. Results", "text": "We examined the performance of TSF using the entrance gap criterion (referred to as TSF) and used the original entropy gap criterion (referred to as TSF entropy). We also considered alternative classification criteria for comparison: the 1-NN best warping window DTW (DTWBest) and the 1-NN DTW with no warping window (DTWNoWin), which is directly from Keogh et al. [9] has a fixed window that limits the window width and limits searches for the best window size, while DTWNoWin does not have such window.The classification error rates are shown in Table 2."}, {"heading": "5.3. Computational Complexity", "text": "First of all, let us consider the computational complexity of TSF with respect to the length of the time series. We selected the data sets with more than 1000 points of time. For each data set, the points of time were sampled at random \u03bbM, with M being the length of the time series and \u03bb being a multiplier. The computational times for different values of \u03bb are shown in Figure 7 (a). Next, let us consider the computational complexity of TSF with respect to the number of training instances. Data sets with more than 1000 instances of training were selected. For each data set, \u03bbN of the points of time were sampled at random, with N being the number of training instances. In addition, the computational times for different values of \u03bb are shown in Figure 7 (b). It is evident that the computational time must be linear both in the length of the time series and in the number of training instances. Therefore, TSF is a computational classifier for time series. Furthermore, in the current implementation, the characteristics should be repeated dynamically in each case."}, {"heading": "6. Conclusions", "text": "Given that the vast range of features can lead to many splits that have the same entropy gain, computational complexity becomes a concern as the range of features grows. Experimental studies of 45 benchmark datasets proposed here address the challenges by using the following two strategies. First, TSF uses a new splitting criterion called entropy gain that combines entropy gains and distance measurement to identify high-quality splits. Second, TSF uses a new splitting criterion called entropy gain that combines the periods together."}, {"heading": "Acknowledgements", "text": "This research was partially supported by the ONR grant N00014-09-1-0656 and we would also like to thank the editor and anonymous reviewers for their valuable comments."}], "references": [{"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning 45 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Constrained mixture estimation for analysis and robust classification of clinical time series", "author": ["I. Costa", "A. Sch\u00f6nhuth", "C. Hafemeister", "A. Schliep"], "venue": "Bioinformatics 25 ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "The Journal of Machine Learning Research 7 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Multiple comparisons among means", "author": ["O. Dunn"], "venue": "Journal of the American Statistical Association ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1961}, {"title": "Constructing high dimensional feature space for time series classification", "author": ["V. Eruhimov", "V. Martyanov", "E. Tuv"], "venue": "in: Proceedings of the 11th European conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "A comparison of alternative tests of significance for the problem of m rankings", "author": ["M. Friedman"], "venue": "The Annals of Mathematical Statistics 11 ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1940}, {"title": "Pattern extraction for time series classification", "author": ["P. Geurts"], "venue": "in: Proceedings of the 5th European conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), Springer", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Weighted dynamic time warping for time series classification", "author": ["Y. Jeong", "M. Jeong", "O. Omitaomu"], "venue": "Pattern Recognition 44 ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "The ucr time series classification/clustering", "author": ["E. Keogh", "X. Xi", "L. Wei", "C. Ratanamahatana"], "venue": "homepage: www.cs.ucr.edu/ \u0303eamonn/time series data/", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "A shapelet transform for time series classification", "author": ["J. Lines", "L.M. Davis", "J. Hills", "A. Bagnall"], "venue": "in: Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD), ACM", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Feature-based classification of time-series data", "author": ["A. Nanopoulos", "R. Alcock", "Y. Manolopoulos"], "venue": "International Journal of Computer Research 10 ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}, {"title": "Making time-series classification more accurate using learned constraints", "author": ["C. Ratanamahatana", "E. Keogh"], "venue": "in: Proceedings of SIAM International Conference on Data Mining (SDM), SIAM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Three myths about dynamic time warping data mining", "author": ["C. Ratanamahatana", "E. Keogh"], "venue": "in: Proceedings of SIAM International Conference on Data Mining (SDM), SIAM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Interval and dynamic time warping-based decision trees", "author": ["J. Rod\u0155\u0131guez", "C. Alonso"], "venue": "in: Proceedings of the 2004 ACM symposium on Applied computing, ACM", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2004}, {"title": "Boosting interval based literals", "author": ["J. Rod\u0155\u0131guez", "C. Alonso", "H. Bostr\u00f6m"], "venue": "Intelligent Data Analysis 5 ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Support vector machines of interval-based features for time series classification", "author": ["J. Rod\u0155\u0131guez", "C. Alonso", "J. Maestro"], "venue": "Knowledge-Based Systems 18 ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Dynamic programming algorithm optimization for spoken word recognition", "author": ["H. Sakoe"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing 26 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1978}, {"title": "Individual comparisons by ranking methods", "author": ["F. Wilcoxon"], "venue": "Biometrics Bulletin 1 ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1945}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques", "author": ["I. Witten", "E. Frank"], "venue": "Morgan Kaufmann", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast time series classification using numerosity reduction", "author": ["X. Xi", "E. Keogh", "C. Shelton", "L. Wei", "C.A. Ratanamahatana"], "venue": "in: Proceedings of the 23rd international conference on Machine learning (ICML), ACM", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Early prediction on time series: a nearest neighbor approach", "author": ["Z. Xing", "J. Pei", "P. Yu"], "venue": "in: Proceedings of the 21st International Joint Conference on Artifical Intelligence (IJCAI), Morgan Kaufmann", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2009}, {"title": "Extracting interpretable features for early classification on time series", "author": ["Z. Xing", "J. Pei", "P.S. Yu", "K. Wang"], "venue": "in: Proceedings of SIAM International Conference on Data Mining (SDM), SIAM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Time series shapelets: a new primitive for data mining", "author": ["L. Ye", "E. Keogh"], "venue": "in: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD), ACM", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Dynamic time warping constraint learning for large margin nearest neighbor classification", "author": ["D. Yu", "X. Yu", "Q. Hu", "J. Liu", "A. Wu"], "venue": "Information Sciences 181 ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Supervised classification of share price trends", "author": ["Z. Zeng", "H. Yan"], "venue": "Information Sciences 178 ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 24, "context": "Time series classification has been playing an important role in many disciplines such as finance [25] and medicine [2].", "startOffset": 98, "endOffset": 102}, {"referenceID": 1, "context": "Time series classification has been playing an important role in many disciplines such as finance [25] and medicine [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 19, "context": "One-nearest-neighbor with dynamic time warping (NNDTW) is robust to the distortion of the time axis and has proven exceptionally difficult to beat [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 14, "context": "The temporal features calculated over time series intervals [15], referred to as interval features, can capture the temporal characteristics, and can also handle the distortion in the time axis.", "startOffset": 60, "endOffset": 64}, {"referenceID": 14, "context": "Previous work [15] has built decision trees on interval features.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": "[12, 21, 8, 24].", "startOffset": 0, "endOffset": 15}, {"referenceID": 20, "context": "[12, 21, 8, 24].", "startOffset": 0, "endOffset": 15}, {"referenceID": 7, "context": "[12, 21, 8, 24].", "startOffset": 0, "endOffset": 15}, {"referenceID": 23, "context": "[12, 21, 8, 24].", "startOffset": 0, "endOffset": 15}, {"referenceID": 16, "context": "Usually NNDTW performs better than NNEuclidean (dynamic time warping [17] is robust to the distortion in the time axis), and is considered as a strong solution for time series problems [13].", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "Usually NNDTW performs better than NNEuclidean (dynamic time warping [17] is robust to the distortion in the time axis), and is considered as a strong solution for time series problems [13].", "startOffset": 185, "endOffset": 189}, {"referenceID": 10, "context": "[11] extracted statistical features such as the mean and deviation of an entire time series, and then used a multi-layer perceptron neural network for classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Geurts [7] extracted local temporal properties after discretizing the time series.", "startOffset": 7, "endOffset": 10}, {"referenceID": 14, "context": "[15] boosted binary stumps on temporal features from intervals of the time series and Rod\u0155\u0131guez and Alonso [14], Rod\u0155\u0131guez et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] boosted binary stumps on temporal features from intervals of the time series and Rod\u0155\u0131guez and Alonso [14], Rod\u0155\u0131guez et al.", "startOffset": 107, "endOffset": 111}, {"referenceID": 15, "context": "[16] applied classifiers such as a decision tree and a SVM on the temporal features extracted from the boosted binary stumps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "However, only binary stumps were boosted, and the effect of using more complex base learners, such as decision trees, should be studied [15] (but larger tree models impact the computational complexity).", "startOffset": 136, "endOffset": 140}, {"referenceID": 14, "context": "Furthermore, in decision trees [15, 14, 16] class-based measures are often used to evaluate the candidate splits in a node.", "startOffset": 31, "endOffset": 43}, {"referenceID": 13, "context": "Furthermore, in decision trees [15, 14, 16] class-based measures are often used to evaluate the candidate splits in a node.", "startOffset": 31, "endOffset": 43}, {"referenceID": 15, "context": "Furthermore, in decision trees [15, 14, 16] class-based measures are often used to evaluate the candidate splits in a node.", "startOffset": 31, "endOffset": 43}, {"referenceID": 22, "context": "Ye and Keogh [23] briefly discussed strategies of introducing additional measures to break ties, but it was in a different context.", "startOffset": 13, "endOffset": 17}, {"referenceID": 22, "context": "Recently, Ye and Keogh [23] proposed time series shapelets to perform interpretable time series classification.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "Shapelets are time series subsequences which are in some sense maximally representative of a class [23].", "startOffset": 99, "endOffset": 103}, {"referenceID": 22, "context": "Ye and Keogh [23], Xing et al.", "startOffset": 13, "endOffset": 17}, {"referenceID": 21, "context": "[22], Lines et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] have successfully shown that time series shapelets can produce highly interpretable results.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] showed that the shapelet approach is comparable to NNDTW for nine data sets investigated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[5] considered a massive number of features.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Interval features have been shown to be effective for time series classification [15, 14, 16].", "startOffset": 81, "endOffset": 93}, {"referenceID": 13, "context": "Interval features have been shown to be effective for time series classification [15, 14, 16].", "startOffset": 81, "endOffset": 93}, {"referenceID": 15, "context": "Interval features have been shown to be effective for time series classification [15, 14, 16].", "startOffset": 81, "endOffset": 93}, {"referenceID": 14, "context": "[15] considered using only intervals of lengths equal to powers of two, and, therefore, reduced the feature space to O(M logM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Here we consider the random sampling strategy used in a random forest [1] that reduces the feature space to O(M) at each tree node.", "startOffset": 70, "endOffset": 73}, {"referenceID": 13, "context": "To obtain a good threshold \u03c4 in equation 4, one can sort the feature values of all the training instances and then select the best threshold from the midpoints between pairs of consecutive values, but this can be too costly [14].", "startOffset": 224, "endOffset": 228}, {"referenceID": 13, "context": "We consider the strategy employed in Rod\u0155\u0131guez and Alonso [14].", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "Furthermore, the random sampling strategy employed in random forest (RF) [1] is considered here.", "startOffset": 73, "endOffset": 76}, {"referenceID": 18, "context": "Assuming the maximum depth of a tree model is O(logN) [19], the complexity of a time series tree becomes O(MN logN).", "startOffset": 54, "endOffset": 58}, {"referenceID": 8, "context": "TSF was applied to a set of time series benchmark data sets [9] summarized in Table 1.", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "We also considered alternative classifiers for comparison: random forest [1] applied to the interval features with sizes", "startOffset": 73, "endOffset": 76}, {"referenceID": 11, "context": "Table 2: The error rates of TSF using the splitting criterion: Entrance gain (TSF) or entropy gain (TSF-entropy), random forest with 500 trees applied to the interval features with sizes power of two (interRF), 1-NN with Euclidean distance (NNEuclidean), 1-NN with the best warping window DTW (DTWBest) [12], and 1-NN DTW with no warping window (DTWNoWin).", "startOffset": 303, "endOffset": 307}, {"referenceID": 11, "context": "[12] and the 1-NN DTW with no warping window (DTWNoWin) methods", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "To compare multiple classifiers to TSF over multiple data sets, we used the procedure for comparing multiple classifiers with a control over multiple data sets suggested by Dem\u0161ar [3], i.", "startOffset": 180, "endOffset": 183}, {"referenceID": 5, "context": ", the Friedman test [6] followed by the Bonferroni-Dunn test [4] if the Friedman test shows a significant difference between the classifiers.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": ", the Friedman test [6] followed by the Bonferroni-Dunn test [4] if the Friedman test shows a significant difference between the classifiers.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "326 (Table 5(b) in Dem\u0161ar [3]), then according to Equation 10, z0.", "startOffset": 26, "endOffset": 29}, {"referenceID": 17, "context": "In addition to the multi-classifier comparison procedure, we also considered Wilcoxon signed ranks test [18] suggested for comparing a pair of classifiers, as the resolution for the multi-classifier comparison procedure can be too low to distinguish two classifiers with significantly different performance, but with close average ranks.", "startOffset": 104, "endOffset": 108}], "year": 2013, "abstractText": "A tree-ensemble method, referred to as time series forest (TSF), is proposed for time series classification. TSF employs a combination of entropy gain and a distance measure, referred to as the Entrance (entropy and distance) gain, for evaluating the splits. Experimental studies show that the Entrance gain improves the accuracy of TSF. TSF randomly samples features at each tree node and has computational complexity linear in the length of time series, and can be built using parallel computing techniques. The temporal importance curve is proposed to capture the temporal characteristics useful for classification. Experimental studies show that TSF using simple features such as mean, standard deviation and slope is computationally efficient and outperforms strong competitors such as one-nearest-neighbor classifiers with dynamic time warping.", "creator": "LaTeX with hyperref package"}}}