{"id": "1302.2436", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Feb-2013", "title": "Extracting useful rules through improved decision tree induction using information entropy", "abstract": "Classification is widely used technique in the data mining domain, where scalability and efficiency are the immediate problems in classification algorithms for large databases. We suggest improvements to the existing C4.5 decision tree algorithm. In this paper attribute oriented induction (AOI) and relevance analysis are incorporated with concept hierarchys knowledge and HeightBalancePriority algorithm for construction of decision tree along with Multi level mining. The assignment of priorities to attributes is done by evaluating information entropy, at different levels of abstraction for building decision tree using HeightBalancePriority algorithm. Modified DMQL queries are used to understand and explore the shortcomings of the decision trees generated by C4.5 classifier for education dataset and the results are compared with the proposed approach.", "histories": [["v1", "Mon, 11 Feb 2013 10:29:17 GMT  (528kb)", "http://arxiv.org/abs/1302.2436v1", "15 pages, 7 figures, 4 tables, International Journal of Information Sciences and Techniques (IJIST) Vol.3, No.1, January 2013"]], "COMMENTS": "15 pages, 7 figures, 4 tables, International Journal of Information Sciences and Techniques (IJIST) Vol.3, No.1, January 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mohd mahmood ali", "mohd s qaseem", "lakshmi rajamani", "a govardhan"], "accepted": false, "id": "1302.2436"}, "pdf": {"name": "1302.2436.pdf", "metadata": {"source": "CRF", "title": "EXTRACTING USEFUL RULES THROUGH IMPROVED DECISION TREE INDUCTION USING INFORMATION ENTROPY", "authors": ["Mohd. Mahmood Ali", "Mohd. S. Qaseem", "Lakshmi Rajamani", "A. Govardhan"], "emails": ["mahmoodedu@gmail.com", "ms_qaseem@yahoo.com", "drlakshmiraja@gmail.com", "dejntuh@jntuh.ac.in"], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijist.2013.3103 27Classification is a widely used technique in the field of data mining, where scalability and efficiency are the main problems in classification algorithms for large databases. We propose improvements over the existing C4.5 decision tree algorithm. In this paper, attribute-oriented induction (AOI) and relevance analysis are combined with knowledge of the concept hierarchy and the HeightBalancePriority algorithm for the construction of decision trees as well as multi-level mining. the assignment of priorities to attributes is done by evaluating information tropie at different abstraction levels for the construction of decision trees using HeightBalancePriority algorithm. Modified DMQL queries are used to understand and research the inadequacies of decision trees that are generated by C4.5 classifiers for educational data sets and the Height Information approach (the Height Information)."}, {"heading": "1. INTRODUCTION", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2. CLASSIFICATION USING DECISION TREE INDUCTION", "text": "Scalability and efficiency of classification techniques are addressed for large databases that have improved the C4.5 classifier, using the following four steps: generalization by AOI that compresses training data, including storing generalized data in data cubes for quick access [8]; relevance analysis that removes irrelevant attributes, further condensing training data; multi-level mining that combines decision tree induction with knowledge in concept hierarchies; priority-based height balancing trees using entropy; the three best methods for efficient classification are discussed in generalization and decision tree induction [7]; we have also used the above three methods with slight improvements, discussing priority-based decision tree construction using information tropy in section 4."}, {"heading": "2.1. Generalization by AOI (Attribute oriented Induction)", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2.2. Relevance Analysis", "text": "In fact, most of them will be able to play by the rules they have adopted in recent years."}, {"heading": "2.3. Multi-level mining", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2.4. Priority based Decision tree construction", "text": "The C 4.5 classifier prefers multi-value attributes for which information tropie is calculated and the attribute with the highest information tropie is selected as the root node, which continues until the decision tree is fully grown. Similarly, in this priority-based decision tree approach, the information tropie is calculated for all attributes using the concepts explained in Sections 2.1, 2.2 and 2.3. But attributes selected by the user get higher preferences, the priority is taken into account next. If the attribute selected by the user has less information tropie in such a case, the information tropie is replaced by an attribute with the highest information tropie, which continues until all attributes (that are of interest to the user) are selected and replaced on the basis of higher information tropie of other attributes that are least important to the user.Finally, the decision tree is constructed with all attributes that are of interest to the user."}, {"heading": "3. PROPOSED DECISION TREE ALGORITHMS", "text": "The decision tree construction algorithm integrates attribute oriented induction and relevance analysis with modified version, the C4.5 classifier is outlined in algorithm 1. Algorithm 2, is used for merging nodes as a decision tree that is built with all nodes in mind, is shown in Figure 3.Algorithm 2. Merging nodes at multiple levels of abstraction when constructing the decision tree. Input: All nodes with attributes and attribute values are constructed as an information tree. NodeMerge (NodeData _ A, NodeData _ B) {Check priorities for nodes _ A and node _ B; During the process of merging the nodes, the algorithm will allocate priorities for attribute values of attribute values for attribute values of attribute values of unique attribute values attributes attributes attributes attributes during the decision tree construction."}, {"heading": "4. PRIORITY BASED DECISION TREE CONSTRUCTION", "text": "Consider the educational dataset (edu _ dataset) shown in Table 3, with the following attributes avg _ edu _ level, country, and income _ level with attribute values of type {Illiterate, fouryrcollege, twoyrcollege, graduateschool, elementaryschool, Phd}, {Cuba, USA, China, India} and {HIGH, MEDIUM, LOW} respectively. DOI: 10.5121 / ijist.2013.3103 37When the decision tree is constructed using C4.5 / J48 classifier [12] for the edu _ dataset it has cut the attribute values of country attributes as shown in Figure 5. The Constructed decision tree has attribute values of income _ level only which reduces the scalability for large datasetets. The other important aspect is efficiency, which is not achieved, for a proper evaluation of the classification rules; the resulting expenditures achieved with the help of J48 of WEfiKA 4.4."}, {"heading": "5. PERFORMANCE EVALUATION", "text": "In fact, it is as if there is some kind of confusion that will be able to hide itself."}, {"heading": "6. CONCLUSION", "text": "The method for improving the C4.5 classifier has been discussed, and the shortcomings that have occurred in the C4.5 classifier have been analyzed using the WEKA tool by constructing decision trees with different types of data sets, in particular with large data sets (in our case we have taken the T4I5K3N8 education dataset). In order to achieve scalability and efficiency in decision trees, many classification rules are retrieved at each level, which takes into account all nodes. NodeMerge and HeightBalance algorithms, which are applied taking into account the concepts of priority-based decision tree construction, i.e. the HeightBalancePriority algorithm, as discussed in Section 3, are used at each level, which takes into account all nodes. The modified DMQL query is used for a better understanding, as in Section 4 and 5 with many examples taking into account educational data sets (edu _ Priority algorithm). Performance is evaluated on the basis of the parameters mentioned in Table 4."}], "references": [{"title": "Improved use of continuous attributes in C4.5", "author": ["J.R. Quinlan"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "A survey paper on Top 10 algorithm in data", "author": ["Xindong Wu", "Vipin Kumar", "J.Ross Quinlan", "Joydeep Ghosh", "Qiang Yang", "Hiroshi Motoda", "Geoffrey", "J. McLachlan", "Angus Ng", "Bing Liu", "Philip S. Yu", "Zhi-Hua Zhou", "Michael steinbach", "David J. Hand", "Dan Steinberg"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "SLIQ: A fast scalable classifier for data mining", "author": ["M. Mehta", "R. Agrawal", "J. Rissanen"], "venue": "In Proceeding of International Conference on Extending Database Technology (EDBT\u201f96),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1996}, {"title": "SPRINT: A scalable parallel classifier for data mining", "author": ["H J. Shafer", "R. Agrawal", "M. Mehta"], "venue": "In Proceedings of the 22nd International Conference Very Large Databases (VLDB),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "Data Mining Concepts and Techniques, Elsevier Publishers", "author": ["J. Han", "M. Kamber"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Applied Categorical Data", "author": ["D.H. Freeman", "Jr."], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1987}, {"title": "Generalization and Decision Tree Induction: Efficient classification in Data Mining, for IEEE", "author": ["Micheline Kamber", "Lara Wistone", "Wan Gong", "Shan Chen", "Han"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Decision Tree Construction from Multidimensional Structured Data, Sixth IEEE International Conference on Data Mining (ICDMW'06", "author": ["Tomoki Watanuma", "Tomonobu Ozaki", "Takenao Ohkawa"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Data driven discovery of quantitative rules in relational databases, for IEEE Transactions Knowledge and data Engineering, 5:29-40", "author": ["J. Han", "Y. Cai", "N. Cercone"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Intermediate decision trees,", "author": ["L.B. Holder"], "venue": "In Proc. 14th Int., Joint Conf. on Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1995}, {"title": "Decision Tree Induction: Data Classification using Height-Balanced Tree, In Proceedings of International Conference of Information and Knowledge Engineering at (IKE \u201f09), U.S.A", "author": ["Mohd Mahmood Ali", "Lakshmi Rajamani"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2009}, {"title": "The WEKA Data Mining Software: An Update, for SIGKDD Explorations", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Inferring Decision Trees Using the Minimum Description Length Principle", "author": ["J.R. Quinlan", "R.L. Rivest"], "venue": "Journal of Information and Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1989}, {"title": "Decision Tree Induction: Priority Classification, for IEEE", "author": ["Mohammed Mahmood Ali", "Lakshmi Rajamani"], "venue": "Prof CSE , DE JNTU Hyderabad India. ms_qaseem@yahoo.com Dr. Lakshmi Rajamani, Professor, Dept. of C.S.E., Univ. College of Engg.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "5 classifier [1],[2], a well-liked tree based classifier, is used to generate decision tree from a set of training examples.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "5 classifier [1],[2], a well-liked tree based classifier, is used to generate decision tree from a set of training examples.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "Induction of decision trees from very large training sets has been previously addressed by SLIQ [3] and SPRINT [4], but the data stored is without generalization.", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "Induction of decision trees from very large training sets has been previously addressed by SLIQ [3] and SPRINT [4], but the data stored is without generalization.", "startOffset": 111, "endOffset": 114}, {"referenceID": 4, "context": "The generalization concept for evaluating classification rules using DMQL in data cube is proposed [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "This issue which is addressed in data generalization and summarization based characterization [5], consist of three steps attribute-oriented induction, where the low-level", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "3103 28 data is generalized to high-level data using the concept hierarchies, relevance analysis [6], and multi-level mining, where decision trees can be induced at different levels of abstraction.", "startOffset": 97, "endOffset": 100}, {"referenceID": 10, "context": "Therefore, we propose a HeightBalancePriority algorithm which constitutes NodeMerge and HeightBalance algorithms [11] that allows merging of nodes in the decision tree thereby, discouraging over-partitioning of the data.", "startOffset": 113, "endOffset": 117}, {"referenceID": 7, "context": "This includes storage of generalized data in data cube to allow fast accessing [8].", "startOffset": 79, "endOffset": 82}, {"referenceID": 6, "context": "The top three methodologies are discussed in Generalization and Decision tree induction for efficient classification [7], we have also used the above three methodologies with slight improvements, where as the priority based decision tree construction using Information entropy, is discussed in Section 4.", "startOffset": 117, "endOffset": 120}, {"referenceID": 8, "context": "AOI [9], a knowledge discovery tool which allows the generalization of data, offers two major advantages for the mining of large databases.", "startOffset": 4, "endOffset": 7}, {"referenceID": 9, "context": "Generalization is performed with the use of attribute concept hierarchies, where the leaves of a given attribute\u2019s concept hierarchy correspond to the attribute\u2019s values in the data (referred to as primitive level data) [10].", "startOffset": 220, "endOffset": 224}, {"referenceID": 8, "context": "AOI also performs generalization by attribute removal [9].", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "Furthermore, once a decision tree has been derived [5], the concept hierarchies can be used to generalize individual nodes in the tree, allowing attribute roll-up or drill- down, and reclassification of the data for the newly specified abstraction level.", "startOffset": 51, "endOffset": 54}, {"referenceID": 13, "context": "The main idea is to construct a decision tree based on these proposed steps and prune it accordingly based on priority using Height Balancing tree concept [14], without losing any of", "startOffset": 155, "endOffset": 159}, {"referenceID": 6, "context": "In some cases if the attributes selected by the user are not in the data set then thresholds plays a vital role as previously addressed [7] for decision tree construction.", "startOffset": 136, "endOffset": 139}, {"referenceID": 10, "context": "Then under the pruning process, we used two algorithms namely, NodeMerge and HeightBalance algorithms [11], which help in enhancing the efficiency in case of dynamic pruning by avoiding the bushy growth of decision trees, with HeightBalancePriority algorithm, gives priority and builds", "startOffset": 102, "endOffset": 106}, {"referenceID": 11, "context": "5/J48 classifier [12] for the edu_dataset it has pruned the attribute values of country attribute as shown in Figure 5.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "The other important aspect is efficiency is not achieved, for proper evaluation of classification rules; the resultant outputs obtained using J48 classifier of WEKA TOOL [12], reveals that priority to be allocated for attribute values at each level of abstraction when the decision tree is built.", "startOffset": 170, "endOffset": 174}, {"referenceID": 8, "context": "This is achieved with our proposed Algorithms discussed, and for simplicity the working of our algorithms shown, using modified DMQL query [9], with the example 4.", "startOffset": 139, "endOffset": 142}, {"referenceID": 0, "context": "[1], [3], [4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[1], [3], [4].", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "[1], [3], [4].", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "The Minimum Description Length (MDL) principle used to evaluate the cost of tree is discussed [13].", "startOffset": 94, "endOffset": 98}], "year": 2013, "abstractText": "Classification is widely used technique in the data mining domain, where scalability and efficiency are the immediate problems in classification algorithms for large databases. We suggest improvements to the existing C4.5 decision tree algorithm. In this paper attribute oriented induction (AOI) and relevance analysis are incorporated with concept hierarchy\u201fs knowledge and HeightBalancePriority algorithm for construction of decision tree along with Multi level mining. The assignment of priorities to attributes is done by evaluating information entropy, at different levels of abstraction for building decision tree using HeightBalancePriority algorithm. Modified DMQL queries are used to understand and explore the shortcomings of the decision trees generated by C4.5 classifier for education dataset and the results are compared with the proposed approach.", "creator": "Microsoft\u00ae Office Word 2007"}}}