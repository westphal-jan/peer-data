{"id": "1302.3283", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Feb-2013", "title": "StructBoost: Boosting Methods for Predicting Structured Output Variables", "abstract": "Structured learning has found many applications in computer vision recently. Analogues to structured support vector machines, here we propose boosting algorithms for predicting multivariate or structured outputs, which is referred to as StructBoost. As SVM-Struct generalizes SVM, our StructBoost generalizes standard boosting such as AdaBoost, or LPBoost to structured learning. AdaBoost, LPBoost and many other conventional boosting methods arise as special cases of StructBoost. The resulting optimization problem of StructBoost is more challenging than SVM-Struct in the sense that the problem of StructBoost can involve exponentially many variables and constraints. In contrast, for SVM-Struct one usually has an exponential number of constraints and a cutting-plane method is used. In order to efficiently solve StructBoost, we propose an equivalent 1-slack formulation and solve it using a combination of cutting planes and column generation.", "histories": [["v1", "Thu, 14 Feb 2013 01:01:24 GMT  (4663kb,D)", "http://arxiv.org/abs/1302.3283v1", "15 pages"], ["v2", "Thu, 8 Aug 2013 00:37:40 GMT  (6469kb,D)", "http://arxiv.org/abs/1302.3283v2", "19 pages"], ["v3", "Sun, 9 Feb 2014 01:46:31 GMT  (6499kb,D)", "http://arxiv.org/abs/1302.3283v3", "19 pages"]], "COMMENTS": "15 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["chunhua shen", "guosheng lin", "anton van den hengel"], "accepted": false, "id": "1302.3283"}, "pdf": {"name": "1302.3283.pdf", "metadata": {"source": "CRF", "title": "StructBoost: Boosting Methods for Predicting Structured Output Variables", "authors": ["Chunhua Shen", "Guosheng Lin", "Anton van den Hengel"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Boosting, AdaBoost, Structured Learning, Conditional Random Field, Image Segmentation, Object Tracking. FCONTENTS"}, {"heading": "1 Introduction 2", "text": "1.1 Main contributions................................................................................"}, {"heading": "2 Structured boosting 3", "text": "2.1 1-slip formulation for rapid optimization 4 2.2 Interface optimization for solving the 1-slip primary system......... 5"}, {"heading": "3 Special cases of StructBoost 6", "text": "3.1 Binary Classification......... 6 3.2 Multi-Class Funding.......... 6 3.3 Hierarchical Multi-Class Classification.......... 7 3.4 Ordinary Regression and AUC Optimization.............. 7 3.5 Optimization of Pascal Overlap Criterion.............. 7 3.6 StructBoost for CRF Parameter Learning 8"}, {"heading": "4 Experiments 9", "text": "4.1 Binary Classification......... 9 4.2 Ordinary Regression and AUC Optimization.............. 9 4.3 Multi-Class Classification........ 10 4.4 Hierarchical Multi-Class Classification.... 10 4.5 Visual Tracking by Overlap Criterion Optimization........ 10The authors are collaborating with the School of Computer Science and the Australian Center for Visual Technologies, University of Adelaide, Adelaide, SA 5005, Australia. Email: chunhua.shen @ adelaide.edu.au.4.6 CRF Parameter Learning for Image Segmentation.......... 14"}, {"heading": "5 Conclusion 14", "text": "References 14ar Xiv: 130 2,32 83v1 [cs.LG] 1 4Fe b20 132"}, {"heading": "1 INTRODUCTION", "text": "In many cases, however, the results are complex and cannot be represented by scalars, since the classes have dependencies on boarding schools, or the classes are objects (vectors, sequences, trees, etc.), problems known as structured output. Structured support machines (SSVM) generalize boarding dependencies."}, {"heading": "1.1 Main contributions", "text": "Overall, the main contributions to this paper are quadruple. \u2022 To our knowledge, our StructBoost is the first practice-oriented stimulation method for predicting a wide range of structured outcomes. We discuss specific cases of this general structured learning framework, including multi-class classification, ordinal regression, optimization of complex metrics such as the Pascal image overlap criterion and conditional random parameters learning for image segmentation. \u2022 To implement StructBoost, we adapt the efficient cutting plan method - originally developed for efficient linear SVM training [8] - for our purpose. We also reformulate m-slack optimization to a single slack optimization. We show that even conventional LPBoost [9] can benefit from this reformulation to achieve significant acceleration in training. \u2022 We also introduce a new formulation of BoctBoster optimization that can be easily implemented by StructBoost."}, {"heading": "1.2 Related work", "text": "In fact, most people who are able to survive themselves are also able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) (...) () () () () () () () () () () () () () () ()) () () ()) () () () ()) () () () ()) () () () ()) () ()) () ()) () () ()) () () ()) () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () (() () () (() () () () () () (() ((() () () (() () () (() () (((() () () (() () () (() () () (() ((() () () ((() () () (() () ((() () () (((() () () ("}, {"heading": "1.3 Notation", "text": "A bold lowercase letter (u, v) denotes a column vector. An elementary inequality between two vectors or matrices such as u \u2265 v means for all i. Leave (xi; yi), with X-Rd. Unlike classification (Y = {1, 2,.., k}) or regression (Y = R), problems where yi is either a discrete or real scalar. We are interested in the case where elements of Y are structured variables, such as vectors, strings, graphs. We denote F as a series of weak learners (dictionary); the size of F can be infinite. Any ~ j (\u00b7), j = F, j = 1. n, is a function that is an input-output-output-output-output-output-input-input-input (x, y)."}, {"heading": "2 STRUCTURED BOOSTING", "text": "Before introducing the proposed general structured boost framework, we present the general loss for structured learning and then take a look at some specific examples: classification, ordinal regression, optimization of specific criteria such as area under the ROC curve and the overlap rate of the Pascal image area, and learning CRF parameters using StructBoost.To measure the accuracy of a prediction, as in SSVM, we want to learn with any loss functions: Y \u00b7 Y 7 \u2192 R. (y, y \u00b2) calculates the loss associated with a prediction y against the true label value y. Note that we generally assume that (y, y) = 0 and 0 (y, y \u00b2) > 0 are written for any y class. We also assume that the loss is upper bounded.The formulation of StructBoost can be written as (m-slack4 primal) with the model defined in (1): min, Cw and Cm (1) > (3m)."}, {"heading": "3 SPECIAL CASES OF STRUCTBOOST", "text": "In this section we will consider a few special cases of the proposed general structured increase."}, {"heading": "3.1 Binary classification", "text": "The standard binary classification LPBoost can clearly be regarded as a special case of multi-class classification and StructBoost. We write the 1-slip formulation of LPBoost and solve the 1-slip primary using the cutting plane. The primary is: min w, \u0435\u0442 w-1 + C-1 (12a) s.t.: 1m w > [m \u00b2 i = 1 ciyih \"(xi)] \u2265 1 m \u00b2 i = 1 ci \u2212 val, (12b) \u0441\u0430c {0, 1} m, \u0441i = 1,...., m; w \u2265 0. (12c) Here yi \u0445 {\u2212 1, 1} and we define the symbol \u2032 (x) = [~ 1 (x) \u00b7 \u00b7 \u00b7 n (x) >, (13) which is the output of all binary weak classifiers on example x."}, {"heading": "3.2 Multi-class boosting", "text": "We first show the MultiBoost algorithm in Shen and Hao [11], which can be implemented by the StructBoost framework as follows. We then introduce a new multiclass boosting algorithm. Let Y = {1, 2,.., k} and w = w1 \u00b7 \u00b7 wk. Here we stack two vectors. As in [11], wy is the model parameter associated with the y-th class. The multiclass discriminant function in [11] writes F (x, y; w) = w > yh \"(x). Let us now define the orthogonal marking vector for the coding vector: [II (y, 1), II (y, 2), \u00b7, II (y, k)] > k.\" (14) 7 Here II (y, k) is the indicator function for the x-class."}, {"heading": "3.3 Hierarchical multi-class classification", "text": "The flexibility of StructBoost allows us to learn a multi-class classifier that optimizes complex tree loss. In many applications such as object categorization, object classes are organized into taxonomies or hierarchies. An example of a tree structure of image categories is in Figure 1. Similar to [4], here we look at tree loss: trees (y, y). Given a class tree structure T with a height of (i.e., T has layers), a tree (y, y) is the height of the first common parent node of class y and y \"in the tree structure from bottom to top. All we need is a redefinition of the class tree structure T with a height of (i.e. T has layers), a tree (y, y) in the tree structure from bottom to top."}, {"heading": "3.4 Ordinal regression and AUC optimization", "text": "In ordinal regression, the names of training data are ranks. Suppose that the label y-R indicates an ordinal scale and pairs (i, j) in group S have the ratio that example i ranks higher than j, i.e. yi > yj. The primary can be spelled asmin w, insofar as it is 1 + Cm, (i, j), (19a) s.t.: w > [h \"(xi) \u2212 h\" (xj)] \u2265 1 \u2212 \u0451ij, (i, j) \u0441ij, (19b) w \u2265 0; (19c) Note: (19) also optimizes the range under the criterion of receiver operating characteristics (ROC). Generally, the number of limitations is square in the number of training examples. Direct solution (19) can only solve problems with up to a few thousand training examples."}, {"heading": "3.5 Optimization of the Pascal image overlap criterion", "text": "Object recognition / localization has used the overlap of the image area as a loss function [1] - [3], for example, in PASCAL object recognition: \"(y, y\") = 1 \u2212 area (y, y \") area (y, y\"), (20) 8 with y, y \"as boundary coordinates. Y, y\" and y \"y\" are the box overlap and union. In this application, we train the weak learner h (x, y) with the image characteristics extracted from the image field defined by y. For example, we can extract histograms of oriented color gradients (HOG) from the image field y and train a decision stump with the extracted HOG characteristics. This, of course, fits StructBoost.Note that in this case, the most violated limitation in the training step as well as the inference for prediction is generally highly contradictory and it is difficult to find a global solution."}, {"heading": "3.6 StructBoost for CRF parameter learning", "text": "Most work uses lengthy cross-validation to find the optimal values for a small number of parameters. Recently, we have conducted some experiments on the graphic segmentation method [16] and developed a tree-based graph learning method [23] to learn these parameters in a principled manner. We demonstrate CRF parameter learning using StructBoost in the application of image segmentation. Later, we perform some experiments on the Grazer Image Segmentation Database. To speed up compilation, superpixels are used rather as pixels in image segmentation. We define x as an image that is used as a segmentation label of all superpixels in an image.We consider the energy E of an image x and segmentation label y over the nodes N and edges S, which take the following form: E (x, y; w) as a superpixel in a S. We consider the energy of an image x and the energy of an image node"}, {"heading": "4 EXPERIMENTS", "text": "In this section, we perform various experiments on applications including binary classification, ordinal regression, multi-level image classification, hierarchical image classification, visual tracking and image segmentation. We use UCI data sets in binary classification and ordinal regression for training time comparison, we randomly selected 75% of the data as training data and the rest 25% for the test. For each data set, we run 10 times and report the average results. We use 4-fold cross validation for the entire data set to determine the regularization parameter. The value of the regularization parameter C is set from 22 to 26. For all experiments, we set the section level cp to 0.01."}, {"heading": "4.1 Binary classification", "text": "We conduct experiments with some UCI machine learning datasets to compare our StructBoost formulation of binary boost with the standard LPBoost [9]. Table 1 shows the results of the binary classification dataset experiments (we use one class as positive data and the remaining classes as negative if the original datasets have multiple names).It is easy to see that the 1-slip formulation is orders of magnitude faster than the standard LPBoost.dataset method CPU time (n) Training% test%"}, {"heading": "4.2 Ordinal regression and AUC optimization", "text": "The details of StructBoost for AUC optimization are described in Section 3.4. We perform AUC optimization using the m-slack formulation of StructBoost and (solving (3) or its dual) 1-slack formulation of StructBoost (solving (7)). To create unbalanced data, we have used a class of UCI multiclass data sets as positive data and all other labels as negative data. Table 2 reports the results. We can see that the 1-slack formulation of StructBoost is much faster with similar performance. Note that RankBoost can also be applied to this problem [25]. RankBoost is designed to solve ranking problems and it is not a general structured booking method."}, {"heading": "4.3 Multi-class classification", "text": "The details of StructBoost for multiple classes are described in Section 3.2. We run our multi-class examples on two image sets: MNIST2 and Scene15 [26]. Here we have used the linear '1 SVM as a weak classifier. We set the target parameter as C = 106 / (number of examples). To avoid overmatching, we first sort the data weights and select the highest percentage of weighted positive and negative examples to train the SVM (p = 60% for MNIST and 80% for Scene15). For MNIST, we randomly select 100 samples from each class as training sets and use the original test sets of 10,000 samples. We have repeated this procedure five times and reported the average test error. Spatial pyramid HOG characteristics [27] are used here."}, {"heading": "4.4 Hierarchical multi-class classification", "text": "We have constructed two hierarchical image datasets from the SUN dataset [22]. The first dataset contains 6 classes of scenes, it has two category levels. For each scene class we use the first 200 images from the original SUN dataset. So in total there are 1200 images. The second dataset is larger, it contains 15 classes of scenes, and there are a total of 3000 images. We have used the HOG attributes as described in [22]. The details of the hierarchical structure of these two datasets are shown in Figure 1. For each dataset we randomly show 50% examples of training and the rest for testing. The reported results are calculated on 8 random columns. We heuristically set the regulation parameter for the StructBoost in this experiment. The maximum increase in iteration is shown at 500.Table 3."}, {"heading": "4.5 Visual tracking by optimizing the image area overlap criterion", "text": "In fact, most of us are able to trump ourselves by following the rules. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...)"}, {"heading": "4.6 CRF parameter learning for image segmentation", "text": "In this experiment, we expand the superpixels based on the segmentation method [24] with the CRF parameters Learn. Further details are described in Section 3.6. We use the Grazer-02-Dataset3 in this experiment, which contains 3 categories (bike, car and person). Each image contains only one category. For each category, we first use 300 labeled images. Images with the odd indices are for training and the rest for testing. We create superpixels and characteristics that are the same as in [24]: the neighborhood is set to 2; histograms of visual word characteristics are generated for each superpixel; code book size is 200. For StructBoost, we use two uncommon potentials: U = [U1, U2] > and 2 pairs of potentials: V = [V1, V2] > We only use random word characteristics that are random. We only use random sampled images for the formation of Boctost."}, {"heading": "5 CONCLUSION", "text": "Similar to SSVM, where the discriminatory function is learned through a common space of inputs and outputs, the discriminatory function of the proposed StructBoost is a linear combination of weak learners defined through a common space of input-output pairs. Furthermore, StructBoost is flexible in its ability to optimize specific loss functions. In order to efficiently solve the resulting optimization problems, we have introduced a cutting-level method that was originally proposed for fast training of linear SVM. Our extensive experiments show that the proposed algorithm is actually computationally comprehensible. We also show that the test accuracy of our StructBoost is at least comparable or sometimes surpasses conventional approaches for a wide range of applications, such as multi-class classification, AUC optimization, image segmentation with CRF parameter learning. Specifically, we have used StructBoost to compare a general tracker by comparing the few image folding methods with the few we know."}], "references": [{"title": "Struck: Structured output tracking with kernels", "author": ["S. Hare", "A. Saffari", "P. Torr"], "venue": "Proc. IEEE Int. Conf. Comp. Vis., 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Structured learning and prediction in computer vision", "author": ["S. Nowozin", "C.H. Lampert"], "venue": "Foundations & Trends in Computer Graphics & Vision, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning to localize objects with structured output regression", "author": ["M.B. Blaschko", "C.H. Lampert"], "venue": "Proc. Eur. Conf. Comp. Vis., 2008, pp. 2\u201315.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Support vector machine learning for interdependent and structured output spaces", "author": ["I. Tsochantaridis", "T. Hofmann", "T. Joachims", "Y. Altun"], "venue": "Proc. Int. Conf. Mach. Learn., 2004, pp. 104\u2013111.  15", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-class support vector machines", "author": ["J. Weston", "C. Watkins"], "venue": "Proc. Euro. Symp. Artificial Neural Networks, 1999.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "On the algorithmic implementation of multiclass kernel-based vector mchines", "author": ["K. Crammer", "Y. Singer"], "venue": "J. Mach. Learn. Res., vol. 2, pp. 265\u2013292, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "On the dual formulation of boosting algorithms", "author": ["C. Shen", "H. Li"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 12, pp. 2216\u20132231, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "Proc. ACM SIGKDD Int. Conf. Knowledge discovery & data mining, 2006, pp. 217\u2013226.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Linear programming boosting via column generation", "author": ["A. Demiriz", "K.P. Bennett", "J. Shawe-Taylor"], "venue": "Mach. Learn., vol. 46, no. 1-3, pp. 225\u2013254, 2002.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "Proc. Int. Conf. Mach. Learn., 2001, pp. 282\u2013289.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "A direct formulation for totally-corrective multi-class boosting", "author": ["C. Shen", "Z. Hao"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "An introduction to conditional random fields", "author": ["C. Sutton", "A. McCallum"], "venue": "Foundations and Trends in Machine Learning, 2012. [Online]. Available: http://arxiv.org/abs/1011.4088", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-class image segmentation using conditional random fields and global classification", "author": ["N. Plath", "M. Toussaint", "S. Nakajima"], "venue": "Proc. Int. Conf. Mach. Learn., 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Kernelized structural SVM learning for supervised object segmentation", "author": ["L. Bertelli", "T. Yu", "D. Vu", "B. Gokturk"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn. IEEE, 2011, pp. 2153\u20132160.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Discriminative models for multi-class object layout", "author": ["C. Desai", "D. Ramanan", "C.C. Fowlkes"], "venue": "Int. J. Comp. Vis., vol. 95, no. 1, pp. 1\u201312, 2011.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning CRFs using graph cuts", "author": ["M. Szummer", "P. Kohli", "D. Hoiem"], "venue": "Proc. Eur. Conf. Comp. Vis., 2008, pp. 582\u2013595.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Boosting structured prediction for imitation learning", "author": ["N. Ratliff", "D. Bradley", "J.A. Bagnell", "J. Chestnutt"], "venue": "Proc. Adv. Neural Inf. Process. Syst., 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Boosting algorithms as gradient descent", "author": ["L. Mason", "J. Baxter", "P.L. Bartlett", "M.R. Frean"], "venue": "Proc. Adv. Neural Inf. Process. Syst., 1999, pp. 512\u2013518.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Structured gradient boosting", "author": ["C. Parker"], "venue": "2007, PhD thesis, Oregon State University. [Online]. Available: http://hdl.handle. net/1957/6490", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Simple training of dependency parsers via structured boosting", "author": ["Q. Wang", "D. Lin", "D. Schuurmans"], "venue": "Proc. Int. Joint Conf. Artificial Intell., 2007, pp. 1756\u20131762.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Optimized cutting plane algorithm for support vector machines", "author": ["V. Franc", "S. Sonnenburg"], "venue": "Proc. Int. Conf. Mach. Learn., New York, NY, USA, 2008, pp. 320\u2013327. [Online]. Available: http://doi.acm.org/10.1145/1390156.1390197", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "SUN database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K. Ehinger", "A. Oliva", "A. Torralba"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "On parameter learning in CRF-based approaches to object class image segmentation", "author": ["S. Nowozin", "P.V. Gehler", "C.H. Lampert"], "venue": "Proc. Eur. Conf. Comp. Vis., 2010, pp. 98\u2013111.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Class segmentation and object localization with superpixel neighborhoods", "author": ["B. Fulkerson", "A. Vedaldi", "S. Soatto"], "venue": "Proc. Int. Conf. Comp. Vis., 2009.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2009}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer"], "venue": "J. Mach. Learn. Res., vol. 4, pp. 933\u2013969, 2003.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., vol. 2, 2006, pp. 2169 \u2013 2178.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast and accurate digit classification", "author": ["S. Maji", "J. Malik"], "venue": "EECS Department, University of California, Berkeley, Tech. Rep. UCB/EECS-2009-159, Nov 2009. [Online]. Available: http://www. eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-159.html", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2009}, {"title": "Scene classification using a hybrid generative/discriminative approach", "author": ["A. Bosch", "A. Zisserman", "X. Munoz"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 4, pp. 712 \u2013 727, 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "CENTRIST: A visual descriptor for scene categorization", "author": ["J. Wu", "J.M. Rehg"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 8, pp. 1489\u20131501, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiclass learning, boosting, and error-correcting codes", "author": ["V. Guruswami", "A. Sahai"], "venue": "Proc. Annual Conf. Computational Learning Theory. ACM, 1999, pp. 145\u2013155.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1999}, {"title": "Improved boosting algorithms using confidence-rated predictions", "author": ["R.E. Schapire", "Y. Singer"], "venue": "Mach. Learn., 1999, pp. 80\u201391.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1999}, {"title": "Object detection with discriminatively trained part-based models", "author": ["P.F. Felzenszwalb", "R.B. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 9, pp. 1627\u2013 1645, September 2010.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Visual tracking with online multiple instance learning", "author": ["B. Babenko", "M.-H. Yang", "S. Belongie"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2009.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Robust fragments-based tracking using the integral histogram", "author": ["A. Adam", "E. Rivlin", "I. Shimshoni"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2006, pp. 798\u2013805.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "Real-time tracking via on-line boosting", "author": ["H. Grabner", "M. Grabner", "H. Bischof"], "venue": "Proc. British Mach. Vis. Conf., 2006, pp. 47\u201356.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}, {"title": "Visual tracking decomposition", "author": ["J. Kwon", "K.M. Lee"], "venue": "Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2010, pp. 1269\u20131276.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2010}, {"title": "Superpixel tracking", "author": ["S. Wang", "H. Lu", "F. Yang", "M.-H. Yang"], "venue": "Proc. Int. Conf. Comp. Vis., pp. 1323\u20131330, 2011.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Superparsing: Scalable nonparametric image parsing with superpixels", "author": ["J. Tighe", "S. Lazebnik"], "venue": "Proc. Eur. Conf. Comp. Vis., 2010, pp. 352\u2013365.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "1 INTRODUCTION Structured learning has attracted extensive attention recently in machine learning and computer vision [1]\u2013[4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 3, "context": "1 INTRODUCTION Structured learning has attracted extensive attention recently in machine learning and computer vision [1]\u2013[4].", "startOffset": 122, "endOffset": 125}, {"referenceID": 3, "context": "Structured support vector machines (SSVM) [4] generalize the multiclass SVM of [5] and [6] to the much broader problem of learning for interdependent and structured outputs.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "Structured support vector machines (SSVM) [4] generalize the multiclass SVM of [5] and [6] to the much broader problem of learning for interdependent and structured outputs.", "startOffset": 79, "endOffset": 82}, {"referenceID": 5, "context": "Structured support vector machines (SSVM) [4] generalize the multiclass SVM of [5] and [6] to the much broader problem of learning for interdependent and structured outputs.", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "Recently, Shen and Hao proposed a direct formulation for multi-class boosting using the loss functions of multi-class SVM [5], [6].", "startOffset": 122, "endOffset": 125}, {"referenceID": 5, "context": "Recently, Shen and Hao proposed a direct formulation for multi-class boosting using the loss functions of multi-class SVM [5], [6].", "startOffset": 127, "endOffset": 130}, {"referenceID": 6, "context": "Inspired by the general boosting framework of [7], they implemented multi-class boosting with the column generation technique.", "startOffset": 46, "endOffset": 49}, {"referenceID": 7, "context": "\u2022 To implement StructBoost, we adapt the efficient cutting-plane method\u2014originally designed for efficient linear SVM training [8]\u2014for our purpose.", "startOffset": 126, "endOffset": 129}, {"referenceID": 8, "context": "We demonstrate that even conventional LPBoost [9] can benefit from this reformulation to gain significant speedup in training.", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "Since our StructBoost builds upon the fully corrective boosting of Shen and Li [7], it inherits the desirable properties of column generation based boosting, such as a fast convergence rate and a clear explanation from the primal-dual convex optimization perspective.", "startOffset": 79, "endOffset": 82}, {"referenceID": 9, "context": "2 Related work The two state-of-the-art structured learning methods are CRF [10] and SSVM [4], which captures the interdependency among output variables.", "startOffset": 76, "endOffset": 80}, {"referenceID": 3, "context": "2 Related work The two state-of-the-art structured learning methods are CRF [10] and SSVM [4], which captures the interdependency among output variables.", "startOffset": 90, "endOffset": 93}, {"referenceID": 6, "context": "It therefore builds upon the work of column generation boosting [7] and the direct formulation for multi-class boosting [11].", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "It therefore builds upon the work of column generation boosting [7] and the direct formulation for multi-class boosting [11].", "startOffset": 120, "endOffset": 124}, {"referenceID": 10, "context": "Indeed, we show the multiclass boosting of [11] is a special case of the general framework presented here.", "startOffset": 43, "endOffset": 47}, {"referenceID": 9, "context": "For example, the linear chain CRF widely used in natural language processing estimates sequences of labels for sequences of input samples due to the fact that CRF can take context into account [10], [12].", "startOffset": 193, "endOffset": 197}, {"referenceID": 11, "context": "For example, the linear chain CRF widely used in natural language processing estimates sequences of labels for sequences of input samples due to the fact that CRF can take context into account [10], [12].", "startOffset": 199, "endOffset": 203}, {"referenceID": 7, "context": "SSVM achieves so based on the joint feature maps over the input-output pairs, where features can be represented equivalently as in CRF [8].", "startOffset": 135, "endOffset": 138}, {"referenceID": 12, "context": "CRF is particularly of interest in computer vision for its success in semantic image segmentation [13].", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "SSVM can also be used for similar purposes as demonstrated in [14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 2, "context": "Blaschko and Lampert [3] trained SSVM models to predict the bounding box of objects in a given image, by optimizing the Pascal bounding box overlap score.", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "The work in [1] introduced structured learning to real-time object detection and tracking, which also optimizes the Pascal box overlap score.", "startOffset": 12, "endOffset": 15}, {"referenceID": 14, "context": "SSVM has also been used to learn statistics that capture the spatial arrangements of various object classes in images [15].", "startOffset": 118, "endOffset": 122}, {"referenceID": 15, "context": "[16] learned optimal parameters of a CRF, avoiding tedious cross validation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "The survey of [2] has provided a comprehensive review of structured learning and its application in computer vision.", "startOffset": 14, "endOffset": 17}, {"referenceID": 16, "context": "[17] proposed boosting for imitation learning based on structured prediction called maximum margin planning (MMP).", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "In the MMPBoost of [17], a demonstrated policy is provided as example behavior for training and the purpose is to learn a function over features of the environment that produce policies with similar behavior.", "startOffset": 19, "endOffset": 23}, {"referenceID": 17, "context": "Second, MMPBoost is based on the idea of gradient descent boosting [18], and our StructBoost is built upon fully corrective boosting of Shen and Li [7].", "startOffset": 67, "endOffset": 71}, {"referenceID": 6, "context": "Second, MMPBoost is based on the idea of gradient descent boosting [18], and our StructBoost is built upon fully corrective boosting of Shen and Li [7].", "startOffset": 148, "endOffset": 151}, {"referenceID": 18, "context": "Parker [19] developed a margin-based structured perceptron update and showed that it can incorporate general notions of misclassification cost as well as kernels.", "startOffset": 7, "endOffset": 11}, {"referenceID": 18, "context": "Therefore the method in [19] is essentially an online version of SSVM.", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": "[20] learned a local predictor using standard methods, e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "For the time being, let us put aside the difficulty of the large number of constraints, and focus on how to iteratively solve for w using column generation as in [7], [9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 8, "context": "For the time being, let us put aside the difficulty of the large number of constraints, and focus on how to iteratively solve for w using column generation as in [7], [9].", "startOffset": 167, "endOffset": 170}, {"referenceID": 6, "context": "With the primal-dual pair of (3) and (5) and following the general framework of column generation based boosting [7], [9], we can obtain our StructBoost as follows: Iterate the following three steps until converge:", "startOffset": 113, "endOffset": 116}, {"referenceID": 8, "context": "With the primal-dual pair of (3) and (5) and following the general framework of column generation based boosting [7], [9], we can obtain our StructBoost as follows: Iterate the following three steps until converge:", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "The direct formulation for multi-class boosting in [11] can be seen as a specific instance of this approach, which is in general very slow.", "startOffset": 51, "endOffset": 55}, {"referenceID": 7, "context": "Inspired by the cutting-plane method for fast training of linear SVM [8], we can equivalently rewrite the above problem into a \u201c1-slack\u201d form so that an efficient cuttingplane method can be employed to solve the optimization", "startOffset": 69, "endOffset": 72}, {"referenceID": 7, "context": "Proof: The proof adapts the proof in [8].", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "As demonstrated in [8], cutting-plane methods can be used to solve the 1-slack primal problem (7) efficiently.", "startOffset": 19, "endOffset": 22}, {"referenceID": 7, "context": "2 Cutting-plane optimization for solving the 1-slack primal Despite the extra nonnegative-ness constraint w \u2265 0 in our case, it is easy to modify the cutting-plane method in [8] for solving our problem (7).", "startOffset": 174, "endOffset": 177}, {"referenceID": 7, "context": "For the analysis of the cuttingplane method for optimizing the 1-slack primal, readers may refer to [8] for details.", "startOffset": 100, "endOffset": 103}, {"referenceID": 20, "context": "In theory, improved cutting-plane methods such as [21] can also be adapted for solving our optimization problem at each column generation.", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "We show in the experiments that at each iteration of LPBoost, solving (12) is much faster than solving the m-slack primal or dual as shown in [9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 10, "context": "2 Multi-class boosting We first show the MultiBoost algorithm in Shen and Hao [11] can be implemented by the StructBoost framework as follows.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "As in [11], wy is the model parameter associated with the y-th class.", "startOffset": 6, "endOffset": 10}, {"referenceID": 10, "context": "The multi-class discriminant function in [11] writes F (x, y;w) = wyh \u2032(x).", "startOffset": 41, "endOffset": 45}, {"referenceID": 10, "context": "Instead of learning k model parameter (one wr for each class) as in Shen and Hao [11], we learn a single parameter w.", "startOffset": 81, "endOffset": 85}, {"referenceID": 10, "context": "The main difference between (16) and MultiBoost in [11] is that here w \u2208 R, while w \u2208 Rn\u00d7k for MultiBoost, with n being the number of weak learners.", "startOffset": 51, "endOffset": 55}, {"referenceID": 3, "context": "Similar to [4], here we consider the tree loss: \u2206(y, y\u2032).", "startOffset": 11, "endOffset": 14}, {"referenceID": 21, "context": "1: The hierarchical structures of two selected subsets of the SUN dataset [22] used in our experiments for hierarchical image classification.", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "5 Optimization of the Pascal image overlap criterion Object detection/localization has used the image area overlap as the loss function [1]\u2013[3], e.", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "5 Optimization of the Pascal image overlap criterion Object detection/localization has used the image area overlap as the loss function [1]\u2013[3], e.", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "In [3], a branch-and-bound search has been employed to find the global optimum.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "This simple search strategy has been used in [1].", "startOffset": 45, "endOffset": 48}, {"referenceID": 13, "context": "Recently, structured SVM [14], [16] and a tree-based graph learning method [23] have been proposed to learn these parameters in a principled way.", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "Recently, structured SVM [14], [16] and a tree-based graph learning method [23] have been proposed to learn these parameters in a principled way.", "startOffset": 31, "endOffset": 35}, {"referenceID": 22, "context": "Recently, structured SVM [14], [16] and a tree-based graph learning method [23] have been proposed to learn these parameters in a principled way.", "startOffset": 75, "endOffset": 79}, {"referenceID": 15, "context": "Note that our setting (21) differs most CRF learning settings such as [16].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "These traditional CRF methods often use a linear model [16].", "startOffset": 55, "endOffset": 59}, {"referenceID": 13, "context": "presented an image segmentation approach that uses nonlinear kernel for the unary energy term in the CRF model [14].", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "which can be solved efficiently by graph cuts [16], [24].", "startOffset": 46, "endOffset": 50}, {"referenceID": 23, "context": "which can be solved efficiently by graph cuts [16], [24].", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "Similar to [16], the minimization (30) still can be solved efficiently by graph cuts.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "We run experiments on some UCI machine learning datasets to compare our StructBoost formulation of binary boosting against the standard LPBoost [9].", "startOffset": 144, "endOffset": 147}, {"referenceID": 8, "context": "We compare the 1-slack StructBoost formulation of binary boosting agaisnt standard LPBoost [9] (i.", "startOffset": 91, "endOffset": 94}, {"referenceID": 24, "context": "Note that RankBoost may also be applied to this problem [25].", "startOffset": 56, "endOffset": 60}, {"referenceID": 25, "context": "We run our multi-class boosting on two image datasets: MNIST2 and Scene15 [26].", "startOffset": 74, "endOffset": 78}, {"referenceID": 26, "context": "Spatial pyramid HOG features [27] are used here.", "startOffset": 29, "endOffset": 33}, {"referenceID": 27, "context": "An image is divided into 31 sub-windows in a spatial hierarchy manner [28].", "startOffset": 70, "endOffset": 74}, {"referenceID": 28, "context": "CENTRIST [29] is used as the feature descriptor.", "startOffset": 9, "endOffset": 13}, {"referenceID": 29, "context": "ECC [30] and AdaBoost.", "startOffset": 4, "endOffset": 8}, {"referenceID": 30, "context": "MH [31].", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "We have constructed two hierarchical image datasets from the SUN dataset [22].", "startOffset": 73, "endOffset": 77}, {"referenceID": 21, "context": "We have used the HOG features as described in [22].", "startOffset": 46, "endOffset": 50}, {"referenceID": 29, "context": "ECC [30] and AdaBoost.", "startOffset": 4, "endOffset": 8}, {"referenceID": 30, "context": "MH [31] on two image multi-class classification datasets: MNIST and Scene15.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "5 Visual tracking by optimizing the image area overlap criterion In [1], a visual tracking method, termed Struck, was introduced based on SSVM.", "startOffset": 68, "endOffset": 71}, {"referenceID": 0, "context": "Struck50 is structured SVM tracking with a buffer size of 50 [1].", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "Structured SVM of [1] is the second best, which confirms the usefulness of structured training.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "Struck50 is structured SVM tracking with a buffer size of 50 [1].", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "We observe similar results as in Table 4: Our StructBoost outperforms other methods on all the sequences, and structured SVM of [1] is the second best.", "startOffset": 128, "endOffset": 131}, {"referenceID": 31, "context": "For HOG feature, we use the code in [32].", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "We also compare our trackers with a few state-of-the-art tracking methods, including Struck [1] (with a buffer size (a) Testing images", "startOffset": 92, "endOffset": 95}, {"referenceID": 32, "context": "of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36].", "startOffset": 38, "endOffset": 42}, {"referenceID": 33, "context": "of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36].", "startOffset": 69, "endOffset": 73}, {"referenceID": 34, "context": "of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36].", "startOffset": 106, "endOffset": 110}, {"referenceID": 35, "context": "of 50), multi-instance tracking (MIL) [33], fragment tracking (Frag) [34], online AdaBoost tracking (OAB) [35], and visual tracking decomposition (VTD) [36].", "startOffset": 152, "endOffset": 156}, {"referenceID": 32, "context": "See [33].", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "The test video sequences \u201ccoke, tiger1, tiger2, david, girl and sylv\u201d were used in [1].", "startOffset": 83, "endOffset": 86}, {"referenceID": 35, "context": "The sequences \u201cshaking, singer\u201d are obtained from [36], and the rest sequences are from [37].", "startOffset": 50, "endOffset": 54}, {"referenceID": 36, "context": "The sequences \u201cshaking, singer\u201d are obtained from [36], and the rest sequences are from [37].", "startOffset": 88, "endOffset": 92}, {"referenceID": 0, "context": "When Struck uses a Gaussian kernel defined on raw pixels, the performance is slightly different [1], and ours still outperforms Struck in most cases.", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "This might be due to the fact that our StructBoost selects relevant features (300 features selected here), and the SSVM of [1] uses all the image patch information which may contain noises.", "startOffset": 123, "endOffset": 126}, {"referenceID": 23, "context": "The precision=recall point [24] and intersection-union score are used to evaluation our method.", "startOffset": 27, "endOffset": 31}, {"referenceID": 23, "context": "6 CRF parameter learning for image segmentation In this experiment, we extend the super-pixels based segmentation method [24] with CRF parameter learning.", "startOffset": 121, "endOffset": 125}, {"referenceID": 23, "context": "We generate super-pixels and features same as in [24]: the neighborhood size is set to 2; histogram of visual words features are generated for each superpixel; code book size is 200.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Two unary potentials: U1, U2 are constructed using two AdaBoost classifiers; one is trained on the visual word histogram features [24], and the other is trained on color histogram together with the thumbnail feature [38].", "startOffset": 130, "endOffset": 134}, {"referenceID": 37, "context": "Two unary potentials: U1, U2 are constructed using two AdaBoost classifiers; one is trained on the visual word histogram features [24], and the other is trained on color histogram together with the thumbnail feature [38].", "startOffset": 216, "endOffset": 220}, {"referenceID": 23, "context": "at/\u223cpinz/ [24], which is able to discourage small isolated segments.", "startOffset": 10, "endOffset": 14}, {"referenceID": 23, "context": "\u2016x \u2212 x\u20162 calculates the `2 norm of the color difference between two super-pixels in the LUV color-space; `(x,x) is the shared boundary length between two super-pixels, as in [24].", "startOffset": 174, "endOffset": 178}, {"referenceID": 23, "context": "As [24], we use the precision = recall point and intersection-union score to evaluation our method.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "5 CONCLUSION We have presented a boosting method for structural learning, as an alternative to SSVM [4] and CRF [10].", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "5 CONCLUSION We have presented a boosting method for structural learning, as an alternative to SSVM [4] and CRF [10].", "startOffset": 112, "endOffset": 116}], "year": 2017, "abstractText": "Structured learning has found many applications in computer vision recently. Analogues to structured support vector machines (SSVM), here we propose boosting algorithms for predicting multivariate or structured outputs, which is referred to as StructBoost. As SSVM generalizes SVM, our StructBoost generalizes standard boosting such as AdaBoost, or LPBoost to structured learning. AdaBoost, LPBoost and many other conventional boosting methods arise as special cases of StructBoost. The resulting optimization problem of StructBoost is more challenging than SSVM in the sense that the problem of StructBoost can involve exponentially many variables and constraints. In contrast, for SSVM one usually has an exponential number of constraints and a cutting-plane method is used. In order to efficiently solve StructBoost, we propose an equivalent 1-slack formulation and solve it using a combination of cutting planes and column generation. We show the versatility and usefulness of StructBoost on a few problems such as hierarchical multi-class classification, robust visual tracking and image segmentation. In particular, we train a tracking-by-detection based object tracker using the proposed structured boosting. Tracking is implemented as structured output prediction by maximizing the Pascal image area overlap criterion. We show that the structural tracker not only significantly outperforms conventional classification based trackers that do not directly optimize the Pascal image overlap criterion, but also outperforms many other state-of-the-art trackers on the tested videos.", "creator": "TeX"}}}