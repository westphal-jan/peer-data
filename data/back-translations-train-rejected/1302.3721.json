{"id": "1302.3721", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Feb-2013", "title": "Thompson Sampling in Switching Environments with Bayesian Online Change Point Detection", "abstract": "Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumes stationary distributions for the rewards. It is often unrealistic to model the real world as a stationary distribution. In this paper we derive and evaluate algorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem. We propose a Thompson Sampling strategy equipped with a Bayesian change point mechanism to tackle this problem. We develop algorithms for a variety of cases with constant switching rate: when switching occurs all arms change (Global Switching), switching occurs independently for each arm (Per-Arm Switching), when the switching rate is known and when it must be inferred from data. This leads to a family of algorithms we collectively term Change-Point Thompson Sampling (CTS). We show empirical results of the algorithm in 4 artificial environments, and 2 derived from real world data; news click-through[Yahoo!, 2011] and foreign exchange data[Dukascopy, 2012], comparing them to some other bandit algorithms. In real world data CTS is the most effective.", "histories": [["v1", "Fri, 15 Feb 2013 10:48:57 GMT  (324kb,D)", "http://arxiv.org/abs/1302.3721v1", "A version will appear in the Sixteenth international conference on Artificial Intelligence and Statistics (AIStats 2013)"]], "COMMENTS": "A version will appear in the Sixteenth international conference on Artificial Intelligence and Statistics (AIStats 2013)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["joseph mellor", "jonathan shapiro"], "accepted": false, "id": "1302.3721"}, "pdf": {"name": "1302.3721.pdf", "metadata": {"source": "CRF", "title": "Thompson Sampling in Switching Environments with Bayesian Online Change Point Detection", "authors": [], "emails": [], "sections": [{"heading": null, "text": "We show empirical results of the algorithm in four artificial environments and two from real data: news clicks [Yahoo!, 2011] and foreign exchange data [Dukascopy, 2012] and compare them with other bandit algorithms. In real data, CTS is most effective."}, {"heading": "1 Introduction", "text": "Thompson Sampling has recently proven to be optimal in the Bernoulli Multi-Armed Bandit Setting [Kaufmann et al., 2012]. This bandit problem is based on stationary distributions for rewards. It is often unrealistic to model the real world as a stationary distribution, and algorithms such as AdaptEvE have been proposed to solve bandit problems in this environment. In this paper, we deal with a switching multi-armed bandit problem. We first review Thompson Sampling before describing the non-stationary environment we are dealing with. We then propose a method to solve this problem and review the techniques we use. We then test our algorithms on a variety of environments. ar Xiv: 130 2.37 21v1 [cs.LG] 1 5"}, {"heading": "1.1 Thompson Sampling", "text": "Thompson sampling is effectively a probability matching algorithm. The desired strategy is to draw an arm with the probability that the arm is the best arm. This probability can be written in such a way that P (ai = a) is the best arm and D is the history of past rewards. I (x) is the indicator function that is 1 if x is true, and 0 otherwise. Therefore, the strategy can be reduced to scanning from the model distribution P (\u03b8 | D) and then select the arm that is maximum in view of this model. In this work, multi-armed bandits with Bernoulli weapons are considered. Each arm, j, returns a reward of 1 with the probability of 1 and 0 otherwise. In the stationary case, the parameter is limited to selecting the arm that is highest....., where k is the number of arms."}, {"heading": "1.2 Model of Dynamic Environment", "text": "In this paper, we assume that the environment changes over time. We assume an abrupt change, defined by a hazard function, h (t), so that the presented algorithms are designed taking into account 2 such models. The first model, which we call the global switching model, switches at constant speed when an alternating point occurs, all arms change their expected rewards. The second model is called pro-arm switching. In this model, changing points occur independently for each arm, so that when the expected reward for one arm changes, it does not correlate with the time when all other arms change. Examples of this form of change could include stock market data, where share prices can change their statistical character very quickly, depending on external events. Switching behavior has been studied in the financial markets [Preis et al., 2011] and multi-armed bandits were applied to this field before 2007."}, {"heading": "1.3 Regret for Switching Environments", "text": "Garivier and Moulines [2008] use the following definition of regret: R (T) = < T = 0 (m \u00b2 t \u2212 \u00b5it) >, (4) where m \u00b2 t is the highest expected payout of an arm at the time t and where <. > the expectation of possible sequences of regret. Unless otherwise stated, our experiments will report a corresponding size, Rn (T) = < T \u00b2 t = 0 I (m \u00b2 t 6 = \u00b5it) >, (5) which is the expected number of times a sub-optimal arm is drawn, corresponding to the results of Hartland et al. [2007] report."}, {"heading": "2 Switching Thompson Sampling", "text": "In order to perform Thompson sampling, we would like to scan P (\u03b8 | Dt \u2212 1), which is the probability of the arm model given the previous data. In a switching system, the arm model \u03b8 only depends on the data since the last circuit, but we do not know when this happened. If we did, we could simply perform the same Bayesian update as in the standard Bernoulli case to arrive at the distribution of our model. Since we do not know the run length rt, we can introduce it as a latent variable and push it to the edge. If we take Dt \u2212 1 as the previous history of rewards and arm pulls, we can write this asP (\u03b8 | Dt \u2212 1) = \u2211 rtP (\u03b8 | Dt \u2212 1, rt) P (rt | Dt \u2212 1), in order to scan now scan from the P (rt \u2212 1, rt \u2212 1), we only have to scan scan scan from the P (rt \u2212 t \u2212 1) model, which from the run length (rt) to the running direction of the running direction of the Arms and then to the subsequent direction."}, {"heading": "3 Bayesian Online Change Detection", "text": "In fact, the inference procedure is easily derived as a sequence. P (rt) xt \u2212 1, Dt \u2212 2) P (xt \u2212 1, Dt \u2212 2) P (xt \u2212 1, Dt \u2212 2) P (xt \u2212 1, Dt \u2212 2) P (rt \u2212 1, Dt \u2212 2) (7) The numberer can then be expressed as a sequence. (rt) t \u2212 1, Dt \u2212 2) P (rt \u2212 1, Dt \u2212 2)."}, {"heading": "3.1 Particle Filters", "text": "A particle filter is a Monte Carlo method of approximate estimation of a Bayesian sequential model. Particles are used to represent points in the distribution to be estimated, and are assigned weights that correspond to their approximate probabilities. The number of particles can grow with each step, and therefore, some particles must occasionally be discarded, leaving it up to us to assign new weights to the remaining particles."}, {"heading": "3.1.1 Stratified Optimal Resampling", "text": "We would like to reduce a discrete probability distribution supported by N discrete points to a stochastic distribution of M discrete points, where the set of M points is a subset of the original N. The original N points each have a probability mass pi associated with them, and the method finds a reweighting of these probabilities, qi, so that N \u2212 M of the probabilities is 0. The idea is that we want there to be no bias in the sampling process, which means that the expected value of qi should be the original probability mass pi. The algorithm is optimal in the sense that the expected squared difference between the original probabilities pi and the new weights qi is minimized. This can be done by the following procedure; 1. We find out that M = N i = 1 min (1, pi / p) 2. Sample u from an even distribution, U (0, p) 3. Iterate through all pi (If a) b > i = Then the probability becomes subsequent (qi = 2) and Pi = the remaining probability."}, {"heading": "4 Proposed Inference Models", "text": "We have shown that we can perform Thompson sampling in a switching system by dividing the process into a phase where the run length is recorded since a switch, and a phase where samples are taken from the arm model according to that run length.The Global Switching and Per-Arm Switching models stand out for their simplicity. Global Switching only requires deriving a run length distribution that does not depend on the number of weapons the bandit has. The Pro-Arm model can store the run lengths for each arm independently, and the space requirement grows linearly in relation to the number of arms. Other models with more complicated arm-to-arm interdependencies can quickly become insoluble."}, {"heading": "4.1 Global switching", "text": "In global change, there is a single change point process across all arms, because when one arm changes the distribution, all other arms are affected as well. This means that the data from each arm pull contributes to the back length of each length distribution. To effectively capture the back length of the full bandit model, we first have to scan the back distribution of the arms, which gives us an estimate of the run length, which tells us how much data from the past our arms can use. Once the global run length is sampled, we then have to proceed individually from the rear distributions of the arms, specifying only the data since the last change point (determined by the run length). We will then pull the arm with the corresponding maximum sample. We only have to store the rear probabilities of the given run lengths and the hyperparameters for the rear run lengths and the hyperparameters of the run time associated with those run lengths. We will call the length distribution the change point model and the set of the hyperparameters that are associated with each length."}, {"heading": "4.2 Per-arm switching", "text": "The difference in the implementation in terms of global switching is that there is now a run length distribution for each arm. That is, for each arm we have a different set of run length probabilities. In the per-arm switching model, the change point model is updated to {w} t \"Initialization run length distribution for all arms j.\" For all arms j, \"j\" p \"0.\" Hyper parameters for all arms j, \"\u03b2t0\" p. \""}, {"heading": "5 Learning the Switching Rate", "text": "Both Wilson et al. [2010] and Turner et al. [2009] have proposed methods to learn the hazard function from the data. Wilson et al. Method can learn a hazard function that is piecemeal constant via a hierarchical generative model. Turner et al. can learn any parametric hazard rate via gradient descent, but initial investigations have shown that it does not work particularly well when the hazard rate is adjusted at each step. For the purposes of this paper, a constant hazard rate was assumed that was learned using the Wilson et al approach. For the simplest case, where we consider a single constant switching rate, Wilson et al. Model appears to be whether a change point has occurred as a Bernoulli variable. We now calculate the common distribution (2) of a beta distribution, and one can imagine that the number of times the system has switched and the number of times it has not been switched. We calculate the common distribution (xt \u2212 2) as opposed to the original \u2212 xt (P \u2212 1)."}, {"heading": "6 Tracking Changes In The Best Arm", "text": "The algorithms presented so far attempt to track changes in all arms, regardless of whether they are being pulled. The distribution of the \u03b8i for an arm not being pulled is gradually flattening and therefore has a higher variance. One of the stated assumptions of Adapt-EvE was that it was only important to track whether the distribution of the arm perceived as the best has changed. We can change the algorithms to better replicate this assumption. The model for per-arm switching is the easiest to adapt. As both the change-point models and the arm models are independent for each arm, we can track the change of the best arm by updating only the change-point and arm models for the pulled arm. For the global switching model, the change is not so clear, as all arms have a change-point model in common. In this essay we updated the common change-point model and updated only the arm model for the pulled arm. Then, the question arises which hyper-parameters for the undrawn arms should be linked to the new runlength of the zero."}, {"heading": "7 Experiments", "text": "To evaluate our bandit model, 6 different non-stationary environments were used, 4 are based on purely synthetic data, and 2 use data collected from the real world. Parameters for all experiments shown were adjusted based on the PASCAL challenge. Bold indicates the best results in all tables."}, {"heading": "7.1 Global Switching Environment", "text": "We first compare the algorithms in an environment with a constant global switching rate. Global-CTS and NP Global-CTS were designed for this environment and so a priori we would expect them to perform the best. The first set of experiments were a single run of algorithms operating in an instance of this environment with 2 arms. Figures 1 and 2 are characterized by heat maps of the run length distributions of some of the algorithms. At some point, the diagrams show the run length distribution. In the case of PA-CTS and NP PA-CTS, there are 2 floor plans for each algorithm corresponding to the run length distribution for each arm. The payout of the 2 arms was superimposed so that one can see how the run length distribution matches the changes in the environment. From the heatmap numbers, we can see the change point prediction when applied to a bandit problem."}, {"heading": "7.2 Per-Arm Switching Environment", "text": "The closest environment was a switching system where the circuit for each arm was independent of any other arm. PA-CTS and NP PA-CTS were designed with this situation in mind, and again, a priori, better performance can be expected. An experiment comparing the algorithms was conducted with 106 iterations and then repeated 100 times. Results are in Table 2. As expected, the PA-CTS algorithm performs best in this environment. NP PA-CTS, the algorithm that matches PA-CTS that learns the hazard rate, suffers much more from regret, suggesting that the parameters for each model are not learned quickly enough."}, {"heading": "7.3 Bernoulli Armed Bandit with Random Normal Walk", "text": "The PASCAL challenge environments were periodic, which is not the kind of environment our algorithms were designed for, and another simulated environment was researched.In this environment, at that time, t, each arm, i, Bernoulli with probability of success \u03b8i (t).At each step, the success rate of the arm was allowed to drift as a truncated normal path.This is the probability of success for an arm \u03b8i (t), which depends on the probability of success of the phenomenon [0, 1] [0, 1], P (\u03b8i (t) | \u03b8i (t \u2212 1)) = e \u2212 (\u03b8i (t \u2212 1) \u2212 \u03b8i (t))) 2\u04452 \u04452 \u04452 \u04452 dx. (14) Table 3 shows a comparison of the algorithms. The experiment was performed 100 times, with each run having a period of 106."}, {"heading": "7.4 PASCAL Challenge 2006", "text": "The PASCAL Exploration vs. Exploitation Challenge 2006 was a competition in a multi-armed bandit problem [Hussain et al., 2006]. The challenge revolved around the optimization of website content, with the available options corresponding to different content that had to be presented to a user on a website. The challenge is a good general test of the algorithms presented in this essay, in order to function well, it was necessary that the bandit algorithms were able to work in non-stationary environments. The challenge had 6 separate environments in which the algorithms had to perform; Frequent Exchange (FS), Long Gaussian (LG), Weekly Variation (WV), Daily Variation (DV), Weekly Close Variation (WCV) and Constant (C). These environments are artificially generated, with the dynamics of expected payments either periodic Gaussian, sinusoidal or constant signals similar to Hartland algorithms, which we did not directly adapt to this 2007 competition with the Evalgorithm Strategy]."}, {"heading": "7.5 Yahoo! Front Page Click Log Dataset", "text": "Yahoo! [2011] has created a dataset using a bandit algorithm, which provides information about the top story presented to a user on the Yahoo! home page. Each entry in the dataset contains information about a single article presented, the time at which it was presented, contextual information about the user, and whether or not the user \"clicked through\" the article. The dataset was designed for the contextual bandit problem. Given the context of a user, the goal is to select an article from which he can choose to maximize the expected rate at which the user clicks on the article to read more. Articles also change during the dataset, and so bandit algorithms designed specifically for this environment must have the ability to modify the number of weapons from which he can choose. For the purposes of our experiments, we do not care about the contextual case we choose, and just try to integrate them from a specific number instead."}, {"heading": "7.6 Foreign Exchange Rate Data", "text": "We constructed a final test environment from the data of the exchange rate [Dukascopy, 2012], asking for prices for 4 currency conversion rates (GBP-USD, USD-JPY, NZDCHF, EUR-CAD) with a resolution of 2 minutes spanning 7 years, corresponding to approximately 106 data points per exchange rate pair. The bandit problem with these data was determined as follows: Each exchange rate was considered a two-armed bandit. It was imagined that the agent could make fictitious trades and opt for either a long call option (if they believe the rate will rise) or a short call option (if they believe the rate will fall). To turn this into a Bernoulli bandit problem, we ignore the extent of the change and offer a reward of 1 if the bandit correctly predicted the rate up / down and 0 otherwise. If the rate remains unchanged, the agent receives a reward independent of their decision."}, {"heading": "8 Conclusion", "text": "This work has examined several algorithms that Thompson Sampling uses in conjunction with the detection of change points. We have shown that they perform well in the environments they are designed for. Bandit scenarios based on real-world data such as the Yahoo! dataset and the Foreign Exchange also perform well, proving to be less powerful in the PASCAL challenge than adequately matched competing algorithms. However, our results suggest that a strategy that merely pursues changes in the perceived arm (Global-CTS2, PA-CTS2), similar to Adapt-EvE, works well. As the model is extremely modular, it is hoped that further assumptions can be incorporated into the model to improve performance. It is also noteworthy that non-Bernoulli payouts can be used just as easily, for example, normally distributed payouts."}], "references": [{"title": "Bayesian online changepoint detection", "author": ["Ryan Prescott Adams", "David J.C. MacKay"], "venue": null, "citeRegEx": "Adams and MacKay.,? \\Q2007\\E", "shortCiteRegEx": "Adams and MacKay.", "year": 2007}, {"title": "On-line inference for hidden Markov models via particle filters", "author": ["Paul Fearnhead", "Peter Clifford"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Fearnhead and Clifford.,? \\Q2003\\E", "shortCiteRegEx": "Fearnhead and Clifford.", "year": 2003}, {"title": "On-line inference for multiple change points problems", "author": ["Paul Fearnhead", "Zhen Liu"], "venue": "Journal of the Royal Statistical Society B,", "citeRegEx": "Fearnhead and Liu.,? \\Q2007\\E", "shortCiteRegEx": "Fearnhead and Liu.", "year": 2007}, {"title": "On Upper-Confidence Bound Policies for NonStationary Bandit Problems", "author": ["A. Garivier", "E. Moulines"], "venue": "ArXiv e-prints,", "citeRegEx": "Garivier and Moulines.,? \\Q2008\\E", "shortCiteRegEx": "Garivier and Moulines.", "year": 2008}, {"title": "Change Point Detection and Meta-Bandits for Online Learning in Dynamic Environments", "author": ["C\u00e9dric Hartland", "Nicolas Baskiotis", "Sylvain Gelly", "Mich\u00e8le Sebag", "Olivier Teytaud"], "venue": "CAp, pages 237\u2013250,", "citeRegEx": "Hartland et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hartland et al\\.", "year": 2007}, {"title": "Exploration vs. exploitation pascal challenge", "author": ["Z. Hussain", "P. Auer", "N. Cesa-Bianchi", "L. Newnham", "J. Shawe-Taylor"], "venue": "http://pascallin.ecs.soton.ac.uk/Challenges/EEC/,", "citeRegEx": "Hussain et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Hussain et al\\.", "year": 2006}, {"title": "Thompson sampling: An optimal finite time analysis", "author": ["Emilie Kaufmann", "Nathaniel Korda", "Rmi Munos"], "venue": "CoRR, abs/1205.4217,", "citeRegEx": "Kaufmann et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kaufmann et al\\.", "year": 2012}, {"title": "Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms", "author": ["Lihong Li", "Wei Chu", "John Langford", "Xuanhui Wang"], "venue": "WSDM, pages 297\u2013306", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Switching processes in financial markets", "author": ["T. Preis", "J.J. Schneider", "H.E. Stanley"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Preis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Preis et al\\.", "year": 2011}, {"title": "Learning by investing: Evidence from venture capital", "author": ["Morten Sorensen"], "venue": "SIFR Research Report Series 53,", "citeRegEx": "Sorensen.,? \\Q2007\\E", "shortCiteRegEx": "Sorensen.", "year": 2007}, {"title": "Adaptive sequential Bayesian change point detection", "author": ["Ryan Turner", "Yunus Saatci", "Carl Edward Rasmussen"], "venue": "In Advances in Neural Information Processing Systems (NIPS): Temporal Segmentation Workshop,", "citeRegEx": "Turner et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Turner et al\\.", "year": 2009}, {"title": "Bayesian online learning of the hazard rate in change-point problems", "author": ["Robert C. Wilson", "Matthew R. Nassar", "Joshua I. Gold"], "venue": "Neural Comput.,", "citeRegEx": "Wilson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting[Kaufmann et al., 2012].", "startOffset": 99, "endOffset": 122}, {"referenceID": 6, "context": "Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting [Kaufmann et al., 2012].", "startOffset": 100, "endOffset": 123}, {"referenceID": 8, "context": "Switching behaviour has been studied in Financial Markets [Preis et al., 2011], and multi-armed bandits have been applied to this field before [Sorensen, 2007].", "startOffset": 58, "endOffset": 78}, {"referenceID": 9, "context": ", 2011], and multi-armed bandits have been applied to this field before [Sorensen, 2007].", "startOffset": 72, "endOffset": 88}, {"referenceID": 3, "context": "Garivier and Moulines [2008] use the following definition of regret.", "startOffset": 0, "endOffset": 29}, {"referenceID": 4, "context": "This corresponds to the results Hartland et al. [2007] report.", "startOffset": 32, "endOffset": 55}, {"referenceID": 0, "context": "Fearnhead and Liu [2007] as well as Adams and MacKay [2007] have independently done work on calculating the online posterior of the runlength.", "startOffset": 36, "endOffset": 60}, {"referenceID": 1, "context": "The worstcase time complexity of this algorithm is O(N logN), but it has an amortised cost of O(N) [Fearnhead and Clifford, 2003].", "startOffset": 99, "endOffset": 129}, {"referenceID": 10, "context": "Both Wilson et al. [2010] and Turner et al.", "startOffset": 5, "endOffset": 26}, {"referenceID": 10, "context": "[2010] and Turner et al. [2009] have proposed methods for learning the hazard function from the data.", "startOffset": 11, "endOffset": 32}, {"referenceID": 5, "context": "Exploitation Challenge 2006 was a competition in a multi-armed bandit problem[Hussain et al., 2006].", "startOffset": 77, "endOffset": 99}, {"referenceID": 4, "context": "Table 4 shows a comparison of the Change-Point Thompson Sampling algorithms (Global-CTS, PA-CTS, NP Global-CTS NP PA-CTS) against AdaptEvE Meta-Bandit and Meta-p-Bandit [Hartland et al., 2007].", "startOffset": 169, "endOffset": 192}, {"referenceID": 4, "context": "Hartland et al. [2007] won this competition with the Adapt-EvE algorithm.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "The simulation then proceeded as described by Li et al. [2011], the results are presented in table 5.", "startOffset": 46, "endOffset": 63}, {"referenceID": 8, "context": "some of the characteristics of a switching system for which the algorithms were designed [Preis et al., 2011].", "startOffset": 89, "endOffset": 109}], "year": 2013, "abstractText": "Thompson Sampling has recently been shown to be optimal in the Bernoulli Multi-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumes stationary distributions for the rewards. It is often unrealistic to model the real world as a stationary distribution. In this paper we derive and evaluate algorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem. We propose a Thompson Sampling strategy equipped with a Bayesian change point mechanism to tackle this problem. We develop algorithms for a variety of cases with constant switching rate: when switching occurs all arms change (Global Switching), switching occurs independently for each arm (Per-Arm Switching), when the switching rate is known and when it must be inferred from data. This leads to a family of algorithms we collectively term Change-Point Thompson Sampling (CTS). We show empirical results of the algorithm in 4 artificial environments, and 2 derived from real world data; news click-through[Yahoo!, 2011] and foreign exchange data[Dukascopy, 2012], comparing them to some other bandit algorithms. In real world data CTS is the most effective.", "creator": "LaTeX with hyperref package"}}}