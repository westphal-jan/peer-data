{"id": "1302.4874", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "A Labeled Graph Kernel for Relationship Extraction", "abstract": "In this paper, we propose an approach for Relationship Extraction (RE) based on labeled graph kernels. The kernel we propose is a particularization of a random walk kernel that exploits two properties previously studied in the RE literature: (i) the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding the relationship; and (ii) combining information from distinct sources in a kernel may help the RE system make better decisions. We performed experiments on a dataset of protein-protein interactions and the results show that our approach obtains effectiveness values that are comparable with the state-of-the art kernel methods. Moreover, our approach is able to outperform the state-of-the-art kernels when combined with other kernel methods.", "histories": [["v1", "Wed, 20 Feb 2013 11:06:25 GMT  (232kb,D)", "http://arxiv.org/abs/1302.4874v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["gon\\c{c}alo sim\\~oes", "helena galhardas", "david matos"], "accepted": false, "id": "1302.4874"}, "pdf": {"name": "1302.4874.pdf", "metadata": {"source": "CRF", "title": "A Labeled Graph Kernel for Relationship Extraction", "authors": ["Gon\u00e7alo Sim\u00f5es", "David Matos", "Helena Galhardas"], "emails": ["goncalo.simoes@ist.utl.pt", "david.matos@inesc-id.pt", "helena.galhardas@ist.utl.pt"], "sections": [{"heading": null, "text": "Categories and Theme Descriptions H.2 [Storage and Retrieval of Information]: Content Analysis and Indexing - Language ProcessingGeneral Terms Algorithms Keywords Information Extraction, Machine Learning, Graphic Cores"}, {"heading": "1. INTRODUCTION", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a city and in which it is a country."}, {"heading": "2. RELATED WORK", "text": "In the last ten years, the number of people who are able to remain in the EU has multiplied."}, {"heading": "3. PROBLEM DEFINITION", "text": "In general, the problem of finding an n-shaped relationship between companies is a problem for which candidates have a number of n-shaped structures; this approach would generate a large number of candidates, few of whom are actually related. Therefore, this configuration would potentially cause problems (due to the fact that they are candidates) and accuracy problems (due to the imbalance of the data)."}, {"heading": "4. METHOD", "text": "In this section, we will introduce the proposed kernel method. Let's start with the basic idea behind the kernel methods for RE in Section 4.1. Then, in Section 4.2, we will propose a representation of the candidate sets as labeled graphs. In Section 4.3, we will explain the random walk kernel used as the basis for our RE kernel. In Section 4.4, we will present the parameters used to modify the random walk kernel for our problem. Finally, in Section 4.5, we will propose our kernel for RE."}, {"heading": "4.1 Kernel Methods for Relationship Extraction", "text": "In some cases, input objects of a classifier cannot simply be expressed via feature vectors (e.g., if the range of possible features is too large or if the nature of the object does not make clear how the features should be selected), so the process of attribute development can become painfully difficult and lead to high-dimensional attribute spaces and thus arithmetic problems. Core methods are an alternative to feature-based methods that can be used to classify new examples while maintaining their original representation.In core methods, the idea is to use a similarity function (kernel) between input objects, which is used using a discriminatory machine learning methodology to classify new examples.In order for a similarity function to be an acceptable core function, K (x, y) must respect the following properties: (i) it must be a two-dimensional function that converts across the object space X into a number in [0, + inst] in order to be an acceptable core function."}, {"heading": "4.2 Labeled Graph Representation of the Sentences", "text": "In our approach, we assume that the inputs of the learning and classification algorithms are labeled with diagrams of the candidate sets (see Figure 4). In this diagram, each vertex is associated with a word in the sentence and enriched with additional features of the word. In our diagram, the additional features include POS tags, genericPOS tags, the word's lemmas, and uppercase patterns (however, due to the simplicity, we only represent one additional feature in the diagram of Figure 4, which is the POS tag). We could use other potentially useful features such as hypernyms or synsets extracted from WordNet. The edges represent semantic relationships between the words. The nature of the semantic relationship is represented by the edge label. Remember that for a given sentence with K units, when looking for an n-relationship, the number of candidates that are generated (k) is determined."}, {"heading": "4.3 Random Walks Kernel", "text": "The basic idea behind this kernel is the following: There is a pair of graphs, we perform simultaneous random walks between the vertices of the graphs and count the number of matching paths. Formally, the goal of the kernel is to calculate the number of matching paths between the two graphs. To explain this core, we start by defining the graph expected as input. Let G be a designated directional graph and | G | be the number of texts in the graph. All texts in the graph are designated as \"vertex\" and denote i. \"The vertices of the graph are also referred to as\" and eij, \"linking the label of the edge, the vertex i and the vertex.\" In addition, we assume two core functions, Kv \"and Ke.\""}, {"heading": "4.4 Parameters of the Random Walks Kernel for Relationship Extraction", "text": "The kernel we propose is a particularization of this on RE.Remember that our representation of a sentence represented in Section 4.2 corresponds to a labeled graph where the labels of vertex are vectors of tags (containing the word itself, its lemmas, POS tags, and ortographic patterns), and the labels of the edges simply contain the type of semantic relationship between the two entities. Furthermore, each vertex and each edge contains information about whether it is in the shortest path between the two entities, and the labels also contain information about whether they are entities. To use the Random Walk kernel described in Section 4.3, we had to define the kernels between the vertex labels and the kernels between the two entities. Considering that the labels of the vertex are simply vectors of the attributes of the word associated with the vertex, we can linearize the kernel =."}, {"heading": "4.5 Random Walks Kernel for Relationship Extraction", "text": "Using the Random Walk Kernel introduced in Section 4.3 and the parameterization of the RE problem proposed in Section 4.4, we produced three variants of the kernel: (i) Full Graph Kernel; (ii) Shortest Path Kernel; and (iii) No Shortest Path Kernel. The Full Graph Kernel (FGK) corresponds to the application of the Random Walk Kernel to the entire structure described in Section 4.2. The idea of this kernel is to capture the entire view of the graph structure (which is the same for all candidates from a given set), but still to be able to detect the similarity between interesting properties specific to the candidates (i.e. shortest path and entity information).The Shortest Path Kernel (SPK) aims to exploit the shortest path hypothesis presented in [7]. The idea is to apply the Random Walk graph to the shortest structures between the parts of the kernel."}, {"heading": "5. EXPERIMENTS", "text": "In this section, we present the experiments performed to evaluate our solution for RE, and report on the results obtained. First, we present the task of relationship extraction, followed by a description of the data set in Section 5.2. Section 5.3 presents the metrics used to evaluate our kernel, and Section 5.4 presents the method used to support our claims about comparing the kernels. In Section 5.5, we show some details of our implementation experiments. In Section 5.6, we report on the performance of the individual kernels presented in Section 4.5, and in Section 5.7, we report on the combination of these kernels. In Section 5.8, we perform a comparison between our solution and other methods. Finally, in Section 5.9, we report on some experiments when combining our kernels with other methods."}, {"heading": "5.1 Relationship Extraction Task", "text": "In our evaluation, we focused exclusively on extracting relationships that correspond to protein-protein interactions, the idea being that in pairs of entities, there is a relationship between them if the text indicates that the proteins exhibit some kind of biological interaction."}, {"heading": "5.2 Dataset", "text": "We conducted our experiments using a protein-protein interaction dataset called AImed1, which has been used in previous work to evaluate the performance of relationship extraction systems in the extraction of protein-protein interactions [8, 14, 2]. AImed consists of 225 medline abstracts, 200 of which describe interactions between proteins and the other 25 do not refer to interactions. The total number of interacting pairs is 974 and the total number of non-interacting pairs is 4072. In evaluating our model, we used a cross-validation strategy based on splits of the AImed dataset at the level of the document [8, 2]. Table 1 shows the number of positive and negative candidates that can be found in the training and test data of each split."}, {"heading": "5.3 Evaluation Metrics", "text": "Our experiments focus on measuring the quality of the results obtained using our core. In information extraction (and particularly relationship extraction), the quality of the results generated is based on two metrics: retrieval and accuracy. Recall indicates the ratio between the amount of information correctly extracted from the texts and the information available in the texts. The downside of this metric is that it delivers high values when we extract all possible pairs of units as a relationship, regardless of whether they are related to each other or not. Precision is the ratio between the amount of information correctly extracted from the texts, while P represents the total number of relationships to be extracted. The downside of this metric is that it delivers high values when we extract all possible pairs of units as a relationship, regardless of whether they are related to each other or not. Precision is the ratio between the amount of information correctly extracted from the texts and all the information that is extracted."}, {"heading": "5.4 Significance Tests", "text": "To substantiate our assertions during the comparison of each kernel pair, we relied on significance tests. We used a paired t-test between each kernel pair we wanted to compare directly. Details of this significance test are found in most statistical books [5]. For a measurement described in Section 5.3, we enter into the test the result obtained for each split of the dataset. Our assertions are based on a significance level of 5%."}, {"heading": "5.5 Implementation Details", "text": "In our experiments, we used the SVM package jLIBSVM2, a Java port of LIBSVM that allows easy customization when using different kernels. During the experiments, we used most of the standard parameters of jLIBSVM. The only exception was SVM parameter C (which controls the trade-off between SVM errors and margin size), for which, after some empirical experiments, we set its value for all experiments to 50. We used the OpenNLP3 sentence detection module and 2http: / dev.davidsoergel.com / trac / jlibsvm / 3http: / / incubator.apache.org / opennlp / the Stanford parser4 for word segmentation, POS tagging and generation of the marked dependency graph. Finally, we used Parallel Colt5 to perform the matrix operations necessary for our kernel."}, {"heading": "5.6 Performance of the Individual Kernels", "text": "Our first experiment aimed to understand how the individual nuclei we proposed (i.e. FGK, SPK and NSPK presented in Section 4.5) performed. Table 2 shows the results of this experiment. The results achieved are in line with expectations. First, the single nucleus that receives the highest value of F1 is SPK. If we know how the shortest path hypothesis has been used successfully in several other work, this is not surprising. Although the average value of F1 for SPK is higher than that for FGK, the difference after the significance tests is not statistically significant. If we only look at the averages of memory and precision shown in Table 2, it seems that SPK is the best nucleus in terms of memory and FGK is the best in terms of precision. However, when comparing the results of these two nuclei with the significance tests, the differences for both metrics are not significant."}, {"heading": "5.7 Performance of the Combination of Kernels", "text": "After analyzing the performance of each nucleus, we evaluated the performance of the nuclei resulting from their combination. We looked at the following four combinations: (i) FGK + SPK; (ii) FGK + NSPK; (iv) SPK + NSPK; and (iii) ALL = FGK + NSPK.Table 3 shows the results of this experiment. Given the performance of each nucleus reported before us, it was expected that the best combination of nuclei will be either one or the other that combines all individual nuclei (ALL) or the other that combines the two best individual nuclei (FGK + SPK). In fact, the results show that in terms of the average values of memory and F1, the best combination is that combines all individual nuclei (ALL)."}, {"heading": "5.8 Comparison with Other Methods", "text": "In order to compare the performance of our solution with other methods, we have implemented two additional cores described in the literature: (i) a kernel based on shallow linguistic information of sentences, [14]; and (ii) a kernel based on subsequences, [8]. During these experiments, we have always compared these cores with our core combination, which showed better performance on averages of memory, precision, and F1: SPK + NSPK. Table 4 shows the results of this experiment. The most obvious conclusion from observing the results is that our solution is still outperformed by the flat linguistic information core in terms of averages of metrics. However, the significance tests for all metrics indicate that the differences between SPK + NSPK and [14] are not significant. If we compare SPK + NSPK with [8], the results are very different. In fact, the results of the significance tests indicate that there are significant differences between these two cores in relation to SPK and SPK."}, {"heading": "5.9 Combination with Other Kernel Methods", "text": "Finally, we conducted some experiments to assess how the combination of SPK + NSPK with other methods influences the results. Again, we used the two cores with which we compared our solution in Section 5.8. Table 5 presents the results of this experiment.By analyzing the results of this experiment, we determine that the best combination is the one that connects SPK + NSPK with [14]. Furthermore, even the combination of SPK + NSPK with [8] can surpass the combination of [14] and [8]. To understand these results, we remember that [14] is based on several cores, including information about n-grams at three different places in the set: before the first unit, between the entities, and after the second entity. Knowing that n-grams belong to the subsequences of the set, it is easy not to understand that there is some overlapping information when the combination consists of these two cores."}, {"heading": "6. CONCLUSIONS AND FUTURE WORK", "text": "The proposed kernel is a particularization of the Random Walk kernel for generic labeled graphs presented in [17]. To make the kernel suitable for RE tasks, we have used two characteristics that are typically used in this work line: (i) the words between the candidates or their association in a syntactic representation are particularly likely to carry information about the relationship; and (ii) the combination of information from different sources in a kernel can help the RE system make better decisions. Our experiments show that the performance of our solution is comparable to the state of the art on RE. Furthermore, we have shown that combining our solution with other methods for RE leads to significant increases in performance. Interesting topics for future work are the investigation of different parameterizations of the Random Walk kernel for RE."}, {"heading": "7. REFERENCES", "text": "[1] E. Agichtein, L. Gravano, J. Pavel, V. Sokolova, andA. Voskoboynik."}], "references": [{"title": "Snowball: a prototype system for extracting relations from large text collections", "author": ["E. Agichtein", "L. Gravano", "J. Pavel", "V. Sokolova", "A. Voskoboynik"], "venue": "Proceedings of the 2001 ACM SIGMOD international conference on Management of data,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "A Graph Kernel for Protein-Protein Interaction Extraction", "author": ["A. Airola", "S. Pyysalo", "J. Bj\u00f6rne", "T. Pahikkala", "F. Ginter", "T. Salakoski"], "venue": "In BioNLP 2008: Current Trends in Biomedical Natural Language Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "SRA: Description Of The Ie2 System Used for MUC-7", "author": ["C. Aone", "L. Halverson", "T. Hampton", "M. Ramos-Santacruz"], "venue": "In Proceedings of the Seventh Message Understanding Conferences (MUC-7),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Information extraction for the semantic web. In Reasoning Web, volume 3564 of Lecture Notes in Computer Science, pages 95\u201396", "author": ["R. Baumgartner", "T. Eiter", "G. Gottlob", "M. Herzog", "C. Koch"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Statistics for Experimenters: An Introduction to Design, Data Analysis, and Model Building", "author": ["G.E.P. Box", "W.G. Hunter", "J.S. Hunter"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1978}, {"title": "Extracting patterns and relations from the World Wide Web", "author": ["S. Brin"], "venue": "In EDBT\u201998: WebDB Workshop at 6th International Conference on Extending Database Technology,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1998}, {"title": "A shortest path dependency kernel for relation extraction", "author": ["R. Bunescu", "R. Mooney"], "venue": "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP-05),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Subsequence Kernels for Relation Extraction", "author": ["R. Bunescu", "R. Mooney"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "Dependency tree kernels for relation extraction", "author": ["A. Culotta", "J. Sorensen"], "venue": "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Unsupervised relation extraction from web documents", "author": ["K. Eichler", "H. Hemsen", "G. Neumann"], "venue": "In LREC 2008: Procedings of the 6th edition of the International Conference on Language Ressources and Evaluation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Web-Scale Information Extraction in KnowItAll", "author": ["O. Etzioni", "M. Cafarella", "D. Downey", "S. Kok", "A.-M. Popescu", "T. Shaked", "S. Soderland", "D.S. Weld", "A. Yates"], "venue": "In Proceedings of the 13th international  conference on World Wide Web,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Identifying Relations for Open Information Extraction", "author": ["A. Fader", "S. Soderland", "O. Etzioni"], "venue": "In EMNLP 2011: Procedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Searching patterns for relation extraction over the web: rediscovering the pattern-relation duality", "author": ["Y. Fang", "K.C.-C. Chang"], "venue": "In WSDM\u201911: Proceedings of the fourth ACM international conference on Web search and data mining,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2011}, {"title": "Exploiting shallow linguistic information for relation extraction from biomedical literature", "author": ["C. Giuliano", "A. Lavelli", "L. Romano"], "venue": "In Procedings of EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "University Of Sheffield: Description Of The Lasie-Ii System As Used For MUC-7", "author": ["K. Humphreys", "R. Gaizauskas", "S. Azzam", "C. Huyck", "B. Mitchell", "H. Cunningham", "Y. Wilks"], "venue": "In Proceedings of the Seventh Message Understanding Conferences (MUC-7),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "A systematic exploration of the feature space for relation extraction", "author": ["J. Jiang", "C. Zhai"], "venue": "In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Marginalized kernels between labeled graphs", "author": ["H. Kashima", "K. Tsuda", "A. Inokuchi"], "venue": "In Proceedings of the Twentieth International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "Information Extraction: Distilling Structured Data from Unstructured Text", "author": ["A. McCallum"], "venue": "ACM Queue,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "A comprehensive benchmark of kernel methods to extract protein\u2013protein interactions from literature", "author": ["D. Tikk", "P. Thomas", "P. Palaga", "J. Hakenberg", "U. Leser"], "venue": "PLoS Computational Biololgy,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Kernel methods for relation extraction", "author": ["D. Zelenko", "C. Aone", "A. Richardella", "J. K", "T. Hofmann", "T. Poggio", "J. Shawe-taylor"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "In fact, this interest led to huge advances in this area and several solutions were proposed in applications such as Semantic Web [4] and Bioinformatics [14, 2].", "startOffset": 130, "endOffset": 133}, {"referenceID": 13, "context": "In fact, this interest led to huge advances in this area and several solutions were proposed in applications such as Semantic Web [4] and Bioinformatics [14, 2].", "startOffset": 153, "endOffset": 160}, {"referenceID": 1, "context": "In fact, this interest led to huge advances in this area and several solutions were proposed in applications such as Semantic Web [4] and Bioinformatics [14, 2].", "startOffset": 153, "endOffset": 160}, {"referenceID": 17, "context": "Regardless of the application domain, an IE activity can be modeled as a composition of the following high-level tasks [18]:", "startOffset": 119, "endOffset": 123}, {"referenceID": 2, "context": "Most of the first approaches for RE were based on handcrafted rules [3, 15].", "startOffset": 68, "endOffset": 75}, {"referenceID": 14, "context": "Most of the first approaches for RE were based on handcrafted rules [3, 15].", "startOffset": 68, "endOffset": 75}, {"referenceID": 5, "context": "However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques.", "startOffset": 51, "endOffset": 65}, {"referenceID": 0, "context": "However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques.", "startOffset": 51, "endOffset": 65}, {"referenceID": 10, "context": "However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques.", "startOffset": 51, "endOffset": 65}, {"referenceID": 12, "context": "However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques.", "startOffset": 51, "endOffset": 65}, {"referenceID": 9, "context": "However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques.", "startOffset": 83, "endOffset": 91}, {"referenceID": 11, "context": "However, some works have exploited semi-supervised [6, 1, 11, 13] and unsupervised [10, 12] techniques.", "startOffset": 83, "endOffset": 91}, {"referenceID": 15, "context": "Even though feature-based methods for RE work well [16], there has been an increasing interest in exploiting kernel-based methods, due to the fact that sentences are better described as structures (e.", "startOffset": 51, "endOffset": 55}, {"referenceID": 16, "context": "We propose the application of a marginalized kernel to compare labeled graphs [17].", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "In order to make this graph kernel suitable for RE, we modified the kernel to exploit the following properties that were previously introduced proposals of kernels for RE: (i) the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding the relationship [7]; and (ii) combining information from distinct sources in a kernel may help the RE system make better decisions [14].", "startOffset": 338, "endOffset": 341}, {"referenceID": 13, "context": "In order to make this graph kernel suitable for RE, we modified the kernel to exploit the following properties that were previously introduced proposals of kernels for RE: (i) the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding the relationship [7]; and (ii) combining information from distinct sources in a kernel may help the RE system make better decisions [14].", "startOffset": 453, "endOffset": 457}, {"referenceID": 7, "context": "In order to evaluate the model we propose, we performed some experiments with a biomedical dataset called AImed [8].", "startOffset": 112, "endOffset": 115}, {"referenceID": 19, "context": "[20], is a kernel based on shallow parse tree representation of sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In order to overcome these problems, Culotta and Sorensen [9] proposed a generalization of this kernel that, when combined with a bag-of-words kernel, is able to compensate the parsing errors.", "startOffset": 58, "endOffset": 61}, {"referenceID": 6, "context": "In 2005, Bunescu and Mooney [7] proposed a kernel based on the shortest path between entities in a dependency graph.", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "The same authors proposed a different kernel based on subsequences [8].", "startOffset": 67, "endOffset": 70}, {"referenceID": 13, "context": "[14] proposed in 2006 a kernel based only on shallow linguistic information of the sentences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] presented a kernel that combines two graph representations of a sentence: (i) a labeled dependency graph; and (ii) a linear order representation of the sentence.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[19] performed a study to analyze how a very comprehensive set of kernels for relationship extraction performs when dealing the task of extracting protein-protein interactions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "Moreover, they show that a simple kernel, like [14], can still obtain results that are at the level of the best kernels based on dependency parsing.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "First, the entities that are candidate to be related can provide very important clues for detecting if there is a relationship [14].", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "Second, the shortest path hypothesis, formalized in [7], states that the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding their relationship.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "Analogously to [7] and [2], we exploited this hypothesis by defining a predicate called inSP (x) that receives as input a node or an edge of the graph and returns true if they belong to the shortest path between the two entities of the graph.", "startOffset": 15, "endOffset": 18}, {"referenceID": 1, "context": "Analogously to [7] and [2], we exploited this hypothesis by defining a predicate called inSP (x) that receives as input a node or an edge of the graph and returns true if they belong to the shortest path between the two entities of the graph.", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "The random walk kernel used as a basis of our RE kernel was defined in [17] as a marginalized kernel between labeled graphs.", "startOffset": 71, "endOffset": 75}, {"referenceID": 16, "context": "However, [17] demonstrated that this kernel can be efficiently computed by solving a system of linear equations.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "[17] demonstrated that the random walk kernel between graphs, K(G,G\u2032), can be given by Equation 8.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Due to the fact that we have no prior knowledge about the probability distributions, we follow the solution proposed in [17] and consider that all the distributions are uniform.", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "The Shortest Path Kernel (SPK) aims at exploiting the shortest path hypothesis presented in [7].", "startOffset": 92, "endOffset": 95}, {"referenceID": 8, "context": "We used this approach because several works empirically demonstrated that combining kernels using this approach typically improves the performance of individual kernels [9, 14].", "startOffset": 169, "endOffset": 176}, {"referenceID": 13, "context": "We used this approach because several works empirically demonstrated that combining kernels using this approach typically improves the performance of individual kernels [9, 14].", "startOffset": 169, "endOffset": 176}, {"referenceID": 7, "context": "This dataset has been used in previous works to evaluate the performance of relationship extraction systems in the task of extracting protein-protein interactions [8, 14, 2].", "startOffset": 163, "endOffset": 173}, {"referenceID": 13, "context": "This dataset has been used in previous works to evaluate the performance of relationship extraction systems in the task of extracting protein-protein interactions [8, 14, 2].", "startOffset": 163, "endOffset": 173}, {"referenceID": 1, "context": "This dataset has been used in previous works to evaluate the performance of relationship extraction systems in the task of extracting protein-protein interactions [8, 14, 2].", "startOffset": 163, "endOffset": 173}, {"referenceID": 7, "context": "During the evaluation of our model we used a cross-validation strategy that is based on splits of the AImed dataset at the level of document [8, 2].", "startOffset": 141, "endOffset": 147}, {"referenceID": 1, "context": "During the evaluation of our model we used a cross-validation strategy that is based on splits of the AImed dataset at the level of document [8, 2].", "startOffset": 141, "endOffset": 147}, {"referenceID": 4, "context": "Details about this significance test can be found on most statistics text books [5].", "startOffset": 80, "endOffset": 83}, {"referenceID": 13, "context": "[14] 47.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "49% [8] 41.", "startOffset": 4, "endOffset": 7}, {"referenceID": 13, "context": "SPK +NSPK + [14] 49.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "43% SPK +NSPK + [8] 45.", "startOffset": 16, "endOffset": 19}, {"referenceID": 13, "context": "[14] + [8] 45.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[14] + [8] 45.", "startOffset": 7, "endOffset": 10}, {"referenceID": 13, "context": "12% SPK +NSPK + [14] + [8] 46.", "startOffset": 16, "endOffset": 20}, {"referenceID": 7, "context": "12% SPK +NSPK + [14] + [8] 46.", "startOffset": 23, "endOffset": 26}, {"referenceID": 13, "context": "In order to compare the performance of our solution with other methods, we implemented two additional kernels described in the literature: (i) a kernel based on shallow linguistic information of the sentences, [14]; and (ii) a kernel based on subsequences, [8].", "startOffset": 210, "endOffset": 214}, {"referenceID": 7, "context": "In order to compare the performance of our solution with other methods, we implemented two additional kernels described in the literature: (i) a kernel based on shallow linguistic information of the sentences, [14]; and (ii) a kernel based on subsequences, [8].", "startOffset": 257, "endOffset": 260}, {"referenceID": 13, "context": "However, the significance tests for all the metrics indicate that the differences between SPK + NSPK and [14] are not significative.", "startOffset": 105, "endOffset": 109}, {"referenceID": 7, "context": "If we compare SPK +NSPK with [8], the results are very different.", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "In fact, the results of the significance tests show that there are significant differences between these two kernels in terms of recall and precision (SPK +NSPK is better in terms of recall and [8] is better in terms of precision).", "startOffset": 194, "endOffset": 197}, {"referenceID": 13, "context": "The differences of the results of precision and recall of SPK+ NSPK and [14] in comparison to [8] are something worth mentioning: the precision values are not as high as in the subsequences kernel but the values of recall are significantly higher.", "startOffset": 72, "endOffset": 76}, {"referenceID": 7, "context": "The differences of the results of precision and recall of SPK+ NSPK and [14] in comparison to [8] are something worth mentioning: the precision values are not as high as in the subsequences kernel but the values of recall are significantly higher.", "startOffset": 94, "endOffset": 97}, {"referenceID": 13, "context": "By analyzing the results obtained in this experiment, we observe that the best combination is the one that joins SPK+ NSPK with [14].", "startOffset": 128, "endOffset": 132}, {"referenceID": 7, "context": "Moreover, even the combination of SPK+ NSPK with [8] is able to outperform the combination of [14] and [8].", "startOffset": 49, "endOffset": 52}, {"referenceID": 13, "context": "Moreover, even the combination of SPK+ NSPK with [8] is able to outperform the combination of [14] and [8].", "startOffset": 94, "endOffset": 98}, {"referenceID": 7, "context": "Moreover, even the combination of SPK+ NSPK with [8] is able to outperform the combination of [14] and [8].", "startOffset": 103, "endOffset": 106}, {"referenceID": 13, "context": "In order to understand these results, recall that [14] is based on several kernels including information of n-grams in three different locations of the sentence: before the first entity, between the entities and after the second entity.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "Thus, we performed significance tests between SPK + NSPK, [14], [8] and all their combinations presented in Table 5.", "startOffset": 58, "endOffset": 62}, {"referenceID": 7, "context": "Thus, we performed significance tests between SPK + NSPK, [14], [8] and all their combinations presented in Table 5.", "startOffset": 64, "endOffset": 67}, {"referenceID": 13, "context": "In what concerns recall, the differences between the combinations, SPK+NSPK and [14] are not significative.", "startOffset": 80, "endOffset": 84}, {"referenceID": 7, "context": "However, the tests indicate that all the combinations are able to outperform [8].", "startOffset": 77, "endOffset": 80}, {"referenceID": 13, "context": "The tests also obtain the same result for [14].", "startOffset": 42, "endOffset": 46}, {"referenceID": 7, "context": "With [8] the results are different: none of the combinations is able to significantly outperform [8].", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "With [8] the results are different: none of the combinations is able to significantly outperform [8].", "startOffset": 97, "endOffset": 100}, {"referenceID": 13, "context": "When comparing the results of the significance tests for F1, there is only one combination that is able to clearly outperforms SPK+NSPK and [14].", "startOffset": 140, "endOffset": 144}, {"referenceID": 7, "context": "Regarding [8], all the combinations are able to significantly outperform it in terms of F1.", "startOffset": 10, "endOffset": 13}, {"referenceID": 16, "context": "The proposed kernel is a particularization of the Random Walk Kernel for generic labeled graphs presented in [17].", "startOffset": 109, "endOffset": 113}], "year": 2013, "abstractText": "In this paper, we propose an approach for Relationship Extraction (RE) based on labeled graph kernels. The kernel we propose is a particularization of a random walk kernel that exploits two properties previously studied in the RE literature: (i) the words between the candidate entities or connecting them in a syntactic representation are particularly likely to carry information regarding the relationship; and (ii) combining information from distinct sources in a kernel may help the RE system make better decisions. We performed experiments on a dataset of protein-protein interactions and the results show that our approach obtains effectiveness values that are comparable with the state-ofthe art kernel methods. Moreover, our approach is able to outperform the state-of-the-art kernels when combined with other kernel methods.", "creator": "LaTeX with hyperref package"}}}