{"id": "1302.4966", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "Probabilistic Exploration in Planning while Learning", "abstract": "Sequential decision tasks with incomplete information are characterized by the exploration problem; namely the trade-off between further exploration for learning more about the environment and immediate exploitation of the accrued information for decision-making. Within artificial intelligence, there has been an increasing interest in studying planning-while-learning algorithms for these decision tasks. In this paper we focus on the exploration problem in reinforcement learning and Q-learning in particular. The existing exploration strategies for Q-learning are of a heuristic nature and they exhibit limited scaleability in tasks with large (or infinite) state and action spaces. Efficient experimentation is needed for resolving uncertainties when possible plans are compared (i.e. exploration). The experimentation should be sufficient for selecting with statistical significance a locally optimal plan (i.e. exploitation). For this purpose, we develop a probabilistic hill-climbing algorithm that uses a statistical selection procedure to decide how much exploration is needed for selecting a plan which is, with arbitrarily high probability, arbitrarily close to a locally optimal one. Due to its generality the algorithm can be employed for the exploration strategy of robust Q-learning. An experiment on a relatively complex control task shows that the proposed exploration strategy performs better than a typical exploration strategy.", "histories": [["v1", "Wed, 20 Feb 2013 15:22:12 GMT  (673kb)", "http://arxiv.org/abs/1302.4966v1", "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)"]], "COMMENTS": "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["grigoris i karakoulas"], "accepted": false, "id": "1302.4966"}, "pdf": {"name": "1302.4966.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Exploration in Planning while Learning", "authors": ["Grigoris I. Karakoulas"], "emails": ["grigoris@ai.iit.nrc.ca"], "sections": [{"heading": null, "text": "In fact, most people who are in a position to put themselves in the world, to put themselves in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which, in which they, in which they, in which they, in fact, live, in which they, in which they, in the world, in which they, in the world, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in the world, live, in which they, in which they, in"}], "references": [{"title": "Learning and sequential decision-making", "author": ["A.G. Barto", "R.S. Sutton"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Barto and Sutton,? \\Q1989\\E", "shortCiteRegEx": "Barto and Sutton", "year": 1989}, {"title": "Dynamic Programming", "author": ["R.E. Bellman"], "venue": "Princeton University Press.", "citeRegEx": "Bellman,? 1957", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "Neural network exploration using opti\u00ad mal experiment design", "author": ["D. Cohn"], "venue": "Advances in Neural Informa\u00ad tion Processing Systems 6, Morgan Kaufmann. Dean, T.L. and Wellman, M.P. (1991). Planning and Con\u00ad trol. Morgan Kaufmann.", "citeRegEx": "Cohn,? 1994", "shortCiteRegEx": "Cohn", "year": 1994}, {"title": "Inc reases in risk and in risk aversion", "author": ["J. Stiglitz"], "venue": "Journal of Economic Theory,", "citeRegEx": "P. and Stiglitz,? \\Q1974\\E", "shortCiteRegEx": "P. and Stiglitz", "year": 1974}, {"title": "Probabilistic planning with infonnation gathering and contingent exe\u00ad cution", "author": ["D. Draper", "S. Hanks", "D. Weld"], "venue": "AAAI", "citeRegEx": "Draper et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Draper et al\\.", "year": 1994}, {"title": "The Sequential Statistical Anal\u00ad ysis", "author": ["Z. Govindarajulu"], "venue": "American Academic Press.", "citeRegEx": "Govindarajulu,? 1987", "shortCiteRegEx": "Govindarajulu", "year": 1987}, {"title": "COMPOSER: A probabi\u00ad listic solution to the utility problem in speed-up learning", "author": ["J. Gratch", "G. DeJong"], "venue": "In Proceedings of AAA/92,", "citeRegEx": "Gratch and DeJong,? \\Q1992\\E", "shortCiteRegEx": "Gratch and DeJong", "year": 1992}, {"title": "Probabilistic Exploration in Planning while Learning", "author": ["J. Gratch", "S. Chien", "G. DeJong"], "venue": "In Proceedings of AAA/94,", "citeRegEx": "Gratch et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Gratch et al\\.", "year": 1994}, {"title": "A machine learning approach", "author": ["L.P. AC M. Kaelbling"], "venue": "Twentieth Annual ACM Symposium on Theory of Comput\u00ad ing,", "citeRegEx": "Kaelbling,? \\Q1993\\E", "shortCiteRegEx": "Kaelbling", "year": 1993}, {"title": "A Q-learning approach to cost\u00ad effective classification", "author": ["G. Karakoulas"], "venue": "Technical Report, Knowledge Sys\u00ad tems Laboratory, National Re search Council Canada. Karp, R. M. & Luby, M. (1983). Monte-Carlo algorithms for enumeration and reliability problems. In Proceedings", "citeRegEx": "Karakoulas,? 1995b", "shortCiteRegEx": "Karakoulas", "year": 1995}, {"title": "A survey of some results in stochastic adaptive control", "author": ["P.R."], "venue": "SIAM Journal of Control and Optimiza\u00ad tion, 23, 329-375. Lin, L. (1991). Self-improvement based on reinforcement learning frameworks. In Proceedings of Eighth Interna\u00ad", "citeRegEx": "P.R.,? 1985", "shortCiteRegEx": "P.R.", "year": 1985}, {"title": "Robot Learning", "author": ["S. Mahadevan", "J.H. Connell"], "venue": "Workshop on Machine Learning,", "citeRegEx": "Mahadevan and Connell,? \\Q1993\\E", "shortCiteRegEx": "Mahadevan and Connell", "year": 1993}, {"title": "Integrated architectures for learning, planning and reacting based on approximating dynamic programming", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton,? \\Q1990\\E", "shortCiteRegEx": "Sutton", "year": 1990}, {"title": "Practical issues in temporal difference learning", "author": ["G. mann. Te sauro"], "venue": "Machine Learning,", "citeRegEx": "sauro,? \\Q1992\\E", "shortCiteRegEx": "sauro", "year": 1992}, {"title": "Efficient exploration in reinforcement learning", "author": ["S. Thrun"], "venue": "Technical Report C M U-CS-92-102, School of Computer Science, Carnegie- Mellon University. Watkins, CJ.C.H. (1989). Learning from delayed rewards. Ph. D. thesis, King's College, Cambridge University, UK.", "citeRegEx": "Thrun,? 1992", "shortCiteRegEx": "Thrun", "year": 1992}, {"title": "Complexity and coo peration in Q-learning", "author": ["S.D. Whitehead"], "venue": "Technical Report, Department of Computer Science, University of Rochester.", "citeRegEx": "Whitehead,? 1991", "shortCiteRegEx": "Whitehead", "year": 1991}], "referenceMentions": [{"referenceID": 7, "context": ", 1994; Pemberton & Korf, 1994), concept learning (Scott & Markovitch, 1993), speed-up learning (Gratch & DeJong, 1992; Greiner & Jurisica, 1992; Gratch et al., 1994), sys\u00ad tem identification (Cohn, 1994; Dean et al.", "startOffset": 96, "endOffset": 166}, {"referenceID": 2, "context": ", 1994), sys\u00ad tem identification (Cohn, 1994; Dean et al., 1995) and reinforcement learning (Thrun, 1992; Kaelbling, 1993).", "startOffset": 33, "endOffset": 64}, {"referenceID": 14, "context": ", 1995) and reinforcement learning (Thrun, 1992; Kaelbling, 1993).", "startOffset": 35, "endOffset": 65}, {"referenceID": 8, "context": ", 1995) and reinforcement learning (Thrun, 1992; Kaelbling, 1993).", "startOffset": 35, "endOffset": 65}, {"referenceID": 14, "context": "The exploration strategies that have been developed for reinforcement learning are largely of a heuristic nature (Thrun, 1992).", "startOffset": 113, "endOffset": 126}, {"referenceID": 1, "context": "This is mainly due to its origination from the concepts and principles of dynamic programming (Bellman, 1957).", "startOffset": 94, "endOffset": 109}, {"referenceID": 1, "context": "the \"curse of dimen\u00ad sionality\" (Bellman, 1957)).", "startOffset": 32, "endOffset": 47}, {"referenceID": 12, "context": "There has been work on learning state transition models and/or utilizing knowledge generated from the Q-learning process in order to guide exploration (Sutton, 1990; Lin, 1991; Thrun, 1992).", "startOffset": 151, "endOffset": 189}, {"referenceID": 14, "context": "There has been work on learning state transition models and/or utilizing knowledge generated from the Q-learning process in order to guide exploration (Sutton, 1990; Lin, 1991; Thrun, 1992).", "startOffset": 151, "endOffset": 189}, {"referenceID": 5, "context": "Such effi\u00ad ciency can be achieved by incorporating a sequential sta\u00ad tistical procedure (Govindarajulu, 1987) into the hill\u00ad climbing algorithm.", "startOffset": 88, "endOffset": 109}, {"referenceID": 10, "context": "o is dynamically determined for each set of transformations T according to the values of the polic\u00a5,\u00ad improvement operators of the set. The symbol r. 1 denotes the smallest integer greater than or equal to the quantity enclosed. The values of h are given by tables in Dudewicz and Dalal (1975). Specific values for the t .", "startOffset": 115, "endOffset": 294}, {"referenceID": 10, "context": "The results of the two experiments are presented in Fig\u00ad ures 3 and 4. The curves from the learning algorithm with the semi-uniform distribution are depicted with dashed lines. The experiments were run for 30 time-periods. Both learning algorithms converged to the optimal policy a1 = ( -0.69) \u00b7 x1, 1\u2022 This is the same policy that was found by Kemball-Cook (1993). He used a control theory approach for solving this problem.", "startOffset": 25, "endOffset": 365}, {"referenceID": 7, "context": "The explomtion strategies that have been developed for speed-up learning (Gmtch & DeJong, 1992; Greiner & Jurisica, 1992; Gratch et al., 1994) are also based on sequential statistical analysis.", "startOffset": 73, "endOffset": 142}, {"referenceID": 7, "context": "The explomtion strategies that have been developed for speed-up learning (Gmtch & DeJong, 1992; Greiner & Jurisica, 1992; Gratch et al., 1994) are also based on sequential statistical analysis. The selection procedures involved are of only one stage and they are not therefore appropriate for our selection problem which requires a two- or a multi-stage procedure. Kaelbling (1993) has developed a statistical algorithm for exploration in reinforcement learning.", "startOffset": 122, "endOffset": 382}, {"referenceID": 4, "context": "Draper et al. (1994) have developed a probabilistic planning algorithm that perfonns both infonnation-pro\u00ad", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "sification (Karakoulas, 1995b).", "startOffset": 11, "endOffset": 30}], "year": 2011, "abstractText": "Sequential decision tasks with incomplete infor\u00ad mation are characterized by the exploration prob\u00ad lem; namely the trade-off between further exploration for learning more about the environ\u00ad ment and immediate exploitation of the accrued information for decision-making. Within artificial intelligence, there has been an increasing interest in studying planning-while-learning algorithms for these decision tasks. In this paper we focus on the exploration problem in reinforcement learn\u00ad ing and Q-learning in particular. The existing exploration strategies for Q-learning are of a heu\u00ad ristic nature and they exhibit limited scaleability in tasks with large (or infinite) state and action spaces. Efficient experimentation is needed for resolving uncertainties when possible plans are compared (i.e. exploration). The experimenta\u00ad tion should be sufficient for selecting with statis\u00ad tical significance a locally optimal plan (i.e. exploitation). For this purpose, we develop a probabilistic hill-climbing algorithm that uses a statistical selection procedure to decide how much exploration is needed for selecting a plan which is, with arbitrarily high probability, arbi\u00ad trarily close to a locally optimal one. Due to its generality the algorithm can be employed for the exploration strategy of robust Q-learning. An experiment on a relatively complex control task shows that the proposed exploration strategy per\u00ad forms better than a typical exploration strategy. continuous flow of events in time. Effective decision-mak\u00ad ing requires resolution of uncertainty as early as possible. The . te?dency to minimize losses resulting from wrong predictions of future events necessitates the division of the problem solution into steps. A decision at each step must make use of the information from the evolution of the events experienced thus far, but that evolution, in fact, depends on the type of decision made at each step.", "creator": "pdftk 1.41 - www.pdftk.com"}}}