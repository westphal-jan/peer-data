{"id": "1302.4971", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Feb-2013", "title": "On the Complexity of Solving Markov Decision Problems", "abstract": "Markov decision problems (MDPs) provide the foundations for a number of problems of interest to AI researchers studying automated planning and reinforcement learning. In this paper, we summarize results regarding the complexity of solving MDPs and the running time of MDP solution algorithms. We argue that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly. To encourage future research, we sketch some alternative methods of analysis that rely on the structure of MDPs.", "histories": [["v1", "Wed, 20 Feb 2013 15:22:36 GMT  (317kb)", "http://arxiv.org/abs/1302.4971v1", "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)"]], "COMMENTS": "Appears in Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI1995)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["michael l littman", "thomas l dean", "leslie pack kaelbling"], "accepted": false, "id": "1302.4971"}, "pdf": {"name": "1302.4971.pdf", "metadata": {"source": "CRF", "title": "On the Complexity of Solving Markov Decision Problems", "authors": ["Michael L. Littman", "Thomas L. Dean", "Leslie Pack Kaelbling"], "emails": ["}@cs."], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}], "references": [{"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1987\\E", "shortCiteRegEx": "Bellman", "year": 1987}, {"title": "Adaptive aggregation for infinite horizon dynamic programming", "author": ["D.P. Bertsekas", "D.A. Castanon"], "venue": "IEEE Transactions on Automatic Control, 34 (6) :589598.", "citeRegEx": "Bertsekas and Castanon,? 1989", "shortCiteRegEx": "Bertsekas and Castanon", "year": 1989}, {"title": "Linear Programming and Exten\u00ad sions", "author": ["G. Dantzig"], "venue": "Operations Research,", "citeRegEx": "Dantzig,? \\Q1960\\E", "shortCiteRegEx": "Dantzig", "year": 1960}, {"title": "Decomposition tech\u00ad niques for planning in stochastic domains", "author": ["T. Dean", "Lin", "S.-H"], "venue": "In Proceed\u00ad ings of the 1995 International Joint Conference on A", "citeRegEx": "Dean et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Dean et al\\.", "year": 1995}, {"title": "Dynamic Programming and Markov Processes", "author": ["R.A. Howard"], "venue": null, "citeRegEx": "Howard,? \\Q1960\\E", "shortCiteRegEx": "Howard", "year": 1960}, {"title": "A subexponential randomized sim\u00ad plex algorithm", "author": ["G. Kalai"], "venue": "Proceedings of 24th Annual ACM Symposium on the Theory of Computing, pages 475482.", "citeRegEx": "Kalai,? 1992", "shortCiteRegEx": "Kalai", "year": 1992}, {"title": "A polynomial algorithm for linear programming", "author": ["L.G. Khachian"], "venue": "Soviet Math Dokl. , 20:191-194.", "citeRegEx": "Khachian,? 1979", "shortCiteRegEx": "Khachian", "year": 1979}, {"title": "How good is the simplex algorithm", "author": ["V. Klee", "G.J. Minty"], "venue": "In Shisha,", "citeRegEx": "Klee and Minty,? \\Q1972\\E", "shortCiteRegEx": "Klee and Minty", "year": 1972}, {"title": "Decomposition of systems governed by Markov chains", "author": ["H.J. Kushner", "Chen", "C.-H."], "venue": "IEEE Trans\u00ad actions on Automatic Control, AC-19(5):501-507.", "citeRegEx": "Kushner et al\\.,? 1974", "shortCiteRegEx": "Kushner et al\\.", "year": 1974}, {"title": "On the complexity of the policy iteration algorithm for stochastic games", "author": ["M. Melekopoglou", "A. Condon"], "venue": "Technical Report CS-TR-90-941, Computer Sciences Department, University of Wiscon\u00ad sin Madison. To appear in the ORSA Journal on Com\u00ad", "citeRegEx": "Melekopoglou and Condon,? 1990", "shortCiteRegEx": "Melekopoglou and Condon", "year": 1990}, {"title": "Memory\u00ad based reinforcement learning: Efficient computation with prioritized sweeping", "author": ["A.W. Moore", "C.G. Atkeson"], "venue": "Advances in Neural In\u00ad formation Processing Systems 5, pages 263-270, San Mateo, California. Morgan Kaufmann.", "citeRegEx": "Moore and Atkeson,? 1993", "shortCiteRegEx": "Moore and Atkeson", "year": 1993}, {"title": "The parti\u00ad game algorithm for variable resolution reinforcement learning in multidimensional state spaces", "author": ["A.W. Moore", "C.G. Atkeson"], "venue": "To appear in Machine Learning.", "citeRegEx": "Moore and Atkeson,? 1995", "shortCiteRegEx": "Moore and Atkeson", "year": 1995}, {"title": "The complexity of Markov chain decision processes", "author": ["C.H. Papadimitriou", "J.N. Tsitsiklis"], "venue": "Math\u00ad ematics of Operations Research, 12(3):441-450.", "citeRegEx": "Papadimitriou and Tsitsiklis,? 1987", "shortCiteRegEx": "Papadimitriou and Tsitsiklis", "year": 1987}, {"title": "Efficient learning and planning within the dyna framework", "author": ["J. Peng", "R.J. Williams"], "venue": "Adaptive Behavior, 1(4):437-454.", "citeRegEx": "Peng and Williams,? 1993", "shortCiteRegEx": "Peng and Williams", "year": 1993}, {"title": "Fast approximation algorithms for fractional packing and covering problems", "author": ["S.A. Plotkin", "D.B. Shmoys", "E. Tardos"], "venue": "32nd Annual Symposium on Foundations of Computer Science, pages 495-504.", "citeRegEx": "Plotkin et al\\.,? 1991", "shortCiteRegEx": "Plotkin et al\\.", "year": 1991}, {"title": "Markov Decision Processes", "author": ["M.L. Puterman"], "venue": "John Wiley & Sons, New York.", "citeRegEx": "Puterman,? 1994", "shortCiteRegEx": "Puterman", "year": 1994}, {"title": "Modified policy iteration algorithms for discounted Markov de\u00ad cision processes", "author": ["M.L. Puterman", "M.C. Shin"], "venue": "Management Science, 24: 1 127-1137.", "citeRegEx": "Puterman and Shin,? 1978", "shortCiteRegEx": "Puterman and Shin", "year": 1978}, {"title": "Theory of linear and integer pro\u00ad gramming", "author": ["A. Schrijver"], "venue": "Wiley-Interscience.", "citeRegEx": "Schrijver,? 1986", "shortCiteRegEx": "Schrijver", "year": 1986}, {"title": "Aggregation methods for large Markov chains", "author": ["P.J. Schweitzer"], "venue": "Mathematical Computer Perfor\u00ad mance and Reliability,", "citeRegEx": "Schweitzer,? \\Q1984\\E", "shortCiteRegEx": "Schweitzer", "year": 1984}, {"title": "Solving H-horizon, stationary Markov decision problems in time proportional to log(H)", "author": ["P. Tseng"], "venue": "Operations Research Letters, 9(5):287-297.", "citeRegEx": "Tseng,? 1990", "shortCiteRegEx": "Tseng", "year": 1990}, {"title": "Tight per\u00ad formance bounds on greedy policies based on imper\u00ad fect value functions", "author": ["R.J. Williams", "L.C.I. Baird"], "venue": "Technical Report NU-CCS-93-13, Northeastern University, College of Computer Science, Boston, MA.", "citeRegEx": "Williams and Baird,? 1993", "shortCiteRegEx": "Williams and Baird", "year": 1993}], "referenceMentions": [{"referenceID": 15, "context": "MDPs employ dynamical models based on well-understood stochastic processes and performance criteria based on established theory in operations research, economics, combinatorial opti\u00ad mization, and the social sciences (Puterman, 1994) .", "startOffset": 217, "endOffset": 233}, {"referenceID": 6, "context": "There are algorithms for solving rational LPs that take time polynomial in the number of variables and constraints as well as the number of bits used to represent the coefficients (Karmarkar, 1984; Khachian, 1979).", "startOffset": 180, "endOffset": 213}, {"referenceID": 2, "context": "The most popular (and practical) methods for solving linear programs are variations of Dantzig's (1963) sim\u00ad plex method.", "startOffset": 87, "endOffset": 104}, {"referenceID": 6, "context": "Although simplex methods seem to perform well in practice, Klee and Minty (1972) showed that one of Dantzig's choices of pivoting rule could lead the sim\u00ad plex algorithm to take an exponential number of it\u00ad erations on some problems.", "startOffset": 59, "endOffset": 81}, {"referenceID": 5, "context": "While progress has been made on speeding up linear programming algorithms (such as a subexponen\u00ad tial simplex algorithm which uses a randomized pivot\u00ad ing rule (Bland, 1977; Kalai, 1992)), MDP-specific al\u00ad gorithms hold more promise for efficient solution.", "startOffset": 160, "endOffset": 186}, {"referenceID": 4, "context": "One of the best known of these algorithms is due to Howard (1960) and is known as policy iteration.", "startOffset": 52, "endOffset": 66}, {"referenceID": 15, "context": "Since there are only M N distinct policies, and each new policy dominates the previous one (Puterman, 1994), it is obvious that policy iteration terminates in at most an exponential number of steps.", "startOffset": 91, "endOffset": 107}, {"referenceID": 9, "context": "Figure 1: Simple policy iteration requires an expo\u00ad nential number of iterations to generate an optimal solution to the family of MDPs illustrated here (af\u00ad ter (Melekopoglou and Condon, 1990)) .", "startOffset": 161, "endOffset": 192}, {"referenceID": 9, "context": "Melekopoglou and Condon (1990) examine the problem of solving expected cost-to-target MDPs using several variations on the sequential improvement policy iteration algo\u00ad rithm.", "startOffset": 0, "endOffset": 31}, {"referenceID": 19, "context": "When combined with a result by Tseng (1990) (de\u00ad scribed in more detail in the next section) which bounds the time needed for value iteration to find an optimal policy, this shows that policy iteration takes polynomial time, for a fixed discount rate.", "startOffset": 31, "endOffset": 44}, {"referenceID": 20, "context": "By examining the Bellman residual during value itera\u00ad tion and stopping when it gets below some threshold, f1 = E(1-1)/(21), we can guarantee that the resulting policy will be t:-optimal (Williams and Baird, 1993).", "startOffset": 187, "endOffset": 213}, {"referenceID": 15, "context": "This is the standard \"contraction mapping\" result for discounted MDPs (Puterman, 1994).", "startOffset": 70, "endOffset": 86}, {"referenceID": 17, "context": "A standard result in the theory of linear programming is that the solution to such a linear program can be written as rational numbers where each component is represented using a number of bits polynomial in the size of the system and B, B* (Schrijver, 1986).", "startOffset": 241, "endOffset": 258}, {"referenceID": 15, "context": "Puterman and Shin (1978) describe a general method called modified policy iteration that includes policy iteration and value iteration as special cases.", "startOffset": 0, "endOffset": 25}, {"referenceID": 10, "context": "A promising approach from this literature involves a heuristic for dynami\u00ad cally choosing which states to update in value itera\u00ad tion according to how likely such an update would be to improve the estimated total-cost function (Moore and Atkeson, 1993; Peng and Williams, 1993).", "startOffset": 227, "endOffset": 277}, {"referenceID": 13, "context": "A promising approach from this literature involves a heuristic for dynami\u00ad cally choosing which states to update in value itera\u00ad tion according to how likely such an update would be to improve the estimated total-cost function (Moore and Atkeson, 1993; Peng and Williams, 1993).", "startOffset": 227, "endOffset": 277}, {"referenceID": 14, "context": "1 (Plotkin et al., 1991).", "startOffset": 2, "endOffset": 24}, {"referenceID": 18, "context": "Aggregation has long been an active topic of research in operations research and optimal con\u00ad trol (Schweitzer, 1984).", "startOffset": 99, "endOffset": 117}, {"referenceID": 11, "context": ", 1995b; Dean and Lin, 1995) and reinforcement learning (Kael\u00ad bling, 1993; Moore and Atkeson, 1995) have been exploring aggregation and decomposition techniques for solving large MDPs.", "startOffset": 56, "endOffset": 100}, {"referenceID": 1, "context": "In particular, Bertsekas and Castanon (1989) describe adaptive aggregation tech\u00ad niques that might be very important for large, struc\u00ad tured state spaces, and Kushner and Chen (1974) de\u00ad scribe how to use Dantzig-Wolfe LP decomposition techniques (1960) for solving large MDPs.", "startOffset": 15, "endOffset": 45}, {"referenceID": 1, "context": "In particular, Bertsekas and Castanon (1989) describe adaptive aggregation tech\u00ad niques that might be very important for large, struc\u00ad tured state spaces, and Kushner and Chen (1974) de\u00ad scribe how to use Dantzig-Wolfe LP decomposition techniques (1960) for solving large MDPs.", "startOffset": 15, "endOffset": 183}, {"referenceID": 1, "context": "In particular, Bertsekas and Castanon (1989) describe adaptive aggregation tech\u00ad niques that might be very important for large, struc\u00ad tured state spaces, and Kushner and Chen (1974) de\u00ad scribe how to use Dantzig-Wolfe LP decomposition techniques (1960) for solving large MDPs.", "startOffset": 15, "endOffset": 254}], "year": 2011, "abstractText": "Markov decision problems (MDPs) provide the foundations for a number of problems of interest to AI researchers studying au\u00ad tomated planning and reinforcement learn\u00ad ing. In this paper, we summarize results regarding the complexity of solving MDPs and the running time of MDP solution al\u00ad gorithms. We argue that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly. To encourage future research, we sketch some alternative methods of analysis that rely on the struc\u00ad ture of MDPs.", "creator": "pdftk 1.41 - www.pdftk.com"}}}