{"id": "1302.5056", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Jan-2013", "title": "Pooling-Invariant Image Feature Learning", "abstract": "Unsupervised dictionary learning has been a key component in state-of-the-art computer vision recognition architectures. While highly effective methods exist for patch-based dictionary learning, these methods may learn redundant features after the pooling stage in a given early vision architecture. In this paper, we offer a novel dictionary learning scheme to efficiently take into account the invariance of learned features after the spatial pooling stage. The algorithm is built on simple clustering, and thus enjoys efficiency and scalability. We discuss the underlying mechanism that justifies the use of clustering algorithms, and empirically show that the algorithm finds better dictionaries than patch-based methods with the same dictionary size.", "histories": [["v1", "Tue, 15 Jan 2013 18:47:11 GMT  (2733kb,D)", "http://arxiv.org/abs/1302.5056v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["yangqing jia", "oriol vinyals", "trevor darrell"], "accepted": false, "id": "1302.5056"}, "pdf": {"name": "1302.5056.pdf", "metadata": {"source": "CRF", "title": "Pooling-Invariant Image Feature Learning", "authors": ["Yangqing Jia", "Oriol Vinyals", "Trevor Darrell"], "emails": ["jiayq@eecs.berkeley.edu", "vinyals@eecs.berkeley.edu", "trevor@eecs.berkeley.edu"], "sections": [{"heading": "1. Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2. Background", "text": "It is about the question of whether it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about which it is about a way in which it is about a way in which it is about which it is about a way and a way in which it is about which it is about which it is about a way in which it is about a way in which it is about which it is about which it is about which it is about which it is about which it is about a way in which it is about which it is"}, {"heading": "3. Pooling-Invariant Dictionary Learning", "text": "We are interested in developing a simple but effective dictionary learning algorithm that takes into account the pooling phase of the feature extraction pipeline and models the general invariances between the pooled features. Taking into account the effectiveness of clustering methods in dictionary learning, we propose to learn a final dictionary of size K in two steps: First, we use the patch-based K-mean algorithm to learn a more complete start-up dictionary of size M (M > K); then, coding and pooling with the dictionary, learn the last, smaller dictionary of size K from the statistics of the M-pooled features. The motivation of such an idea is that K-mean is a highly parallelizable algorithm that could be scaled by simply sharing the data, allowing us to have an efficient dictionary learning algorithm."}, {"heading": "3.1. Feature Selection with Affinity Propagation", "text": "The first step of our algorithm is identical to the patch-based K-mean algorithm with a start-up dictionary size M. After that, we can scan a series of image superfields of the same size as the merging regions and derive the M-dimensional summarized characteristics from them. We would then like to find a K-dimensional subspace that best represents the M-dimensional characteristics. Specifically, the similarity between two pooled dimensions (which correspond to two codes in the start dictionary) i and code j ass (i, j) = 2Cij \u2212 2 (1), where C is the covariance matrix matrix calculated from the random sample of pooled characteristics k."}, {"heading": "3.2. Visualization of Selected Filters", "text": "To visually show which codes are selected by affinity propagation, we applied our approach to the CIFAR-10 dataset by first training an over-complete dictionary of 3200 codes and then performing affinity propagations on the 3200-dimensional pooled characteristics to obtain 256 centroids, which we visualize in Figure 3. Translational invariance seems to be the most dominant factor, as many clusters contain translated versions of the same gabor-like code, especially for grade scale codes. On the other hand, clusters capture more than translation: clusters such as column 5 focus on finding the contrasting colors than finding edges of the exact same angle, and clusters such as the last column contain invariant edges of different color. We note that the selected codes are not necessarily centered (which is the case for convolutionary approaches), as the centrals are selected exclusively from the patent static responses."}, {"heading": "4. Why Does Clustering Work?", "text": "In fact, most of us are able to set out in search of new ways to conquer the world. (...) It is not that we set out in search of new ways. (...) It is not that we set out in search of new ways. (...) It is not that we set out in search of new ways. (...) It is not that we set out in search of new ways. \"(...) It is not that we set out in search of new ways.\" (...)"}, {"heading": "5. Experiments", "text": "We apply our Pooling Invariant Dictionary Learning (PDL) algorithm to several benchmark tasks, including the CIFAR 10 and STL datasets, where performance can be systematically analyzed, and the fine-grained classification task of bird classification, where we demonstrate that feature learning provides significant performance gains over traditional methods."}, {"heading": "5.1. CIFAR-10 and STL", "text": "The CIFAR-102 and STL3 datasets are widely used to analyze the behavior of feature extraction pipelines. CIFAR-10 contains a large amount of training and test data, while STL contains a very small amount of training data and a large amount of unlabeled images. As our algorithm works with all encoding and pooling operations, we have adopted the default setting that is usually performed on the dataset: Extracting local 6 x 6 fields with subtracted and normalized mean and contrast, brightening the fields with ZCA, and then training the dictionary with normalized K means. The features are then encoded using one-sided threshold coding with \u03b1 = 0.25 and average interaction across the four quadrants (2 x 2) of the image. For STL, we followed [5] and changed them to 32 x 32. For the PDL quadrant, we do a different pool coding instead of each other."}, {"heading": "5.2. Statistics for Feature Selection", "text": "We first verify that the learned codes capture pooling invariance, as we claimed in the previous paragraph. To this end, we start from the selected characteristics in Section 3.2 and randomly select three types of filter responses: (a) paired filter responses before merging between codes in the same cluster, (b) paired filter responses after merging between codes in the same cluster, and (c) paired filter responses after merging between the selected centroids. Distribution of these responses is shown in Figure 4. The result confirms our guess well: First, codes that produce uncorrelated responses before pooling can be correlated after the pooling phase (comparison of 4 (a) and 4 (b))) that could be effectively identified by the affinity propagation algorithm; second, we are able to select uncorrelated values by explicit consideration of pool behavior (comparison of which is a subset of 4), with which is a subset of hazardous characteristics (4)."}, {"heading": "5.3. Classification Performances", "text": "Figure 5 shows the relative improvement of CIFAR-10 when we use a budgeted dictionary of size 200, but perform the feature selection from a larger dictionary, as indicated by the X-axis. PCA performance will also be included in the figure as the logical upper limit of the original dictionary, and it will be possible that the number of unequal eigenvalues will be 256 for the purposes."}, {"heading": "5.4. Fine-grained Classification", "text": "To demonstrate the performance of the feature-learning algorithm in real-world image classification, we tested the performance of our algorithm on the fine-grained classification task with the 2011 Caltech-UCSD Birds Dataset [18] 5. Classifying delicate categories represents a significant challenge for contemporary vision algorithms, as these classification tasks usually require the identification of localized phenomena such as \"birds with spotted feathers on their bellies.\" Recent work on the fine-grained classification focuses on localizing parts [7, 24, 6] and still uses manually designed features. Yao et al suggests using a template-based approach for more powerful features, but the approach is difficult to scale as the number of templates is greater than the number of templates."}, {"heading": "6. Conclusion", "text": "We have proposed a novel algorithm to efficiently account for the invariance of learned traits after the spatial pooling phase. Empirically, the algorithm is demonstrably redundant between patch-based codes and delivers dictionaries that produce better classification accuracy than simple patch-based approaches. To explain the performance gain, we proposed to create a matrix approximation overview of dictionary learning and demonstrate the close link between the proposed methods and the Nystro-m method. The proposed method does not introduce overheads during the classification period and could easily be \"plugged\" into the existing image classification pipelines."}], "references": [{"title": "Learning mid-level features for recognition", "author": ["Y Boureau", "F Bach", "Y LeCun", "J Ponce"], "venue": "CVPR,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic segmentation with second-order pooling", "author": ["J Carreira", "R Caseiro", "J Batista", "C Sminchisescu"], "venue": "ECCV,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "An analysis of single-layer networks in unsupervised feature learning", "author": ["A Coates", "H Lee", "A Ng"], "venue": "AISTATS,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "The importance of encoding versus training with sparse coding and vector quantization", "author": ["A Coates", "A Ng"], "venue": "ICML,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Selecting receptive fields in deep networks", "author": ["A Coates", "AY Ng"], "venue": "NIPS,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Discovering localized attributes for fine-grained recognition", "author": ["K Duan", "D Parikh", "D Crandall", "K Grauman"], "venue": "CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Birdlets: Subordinate categorization using volumetric primitives and pose-normalized appearance", "author": ["R Farrell", "O Oza", "N Zhang", "VI Morariu", "T Darrell", "LS Davis"], "venue": "CVPR,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Spectral grouping using the nystrom method", "author": ["C Fowlkes", "S Belongie", "F Chung", "J Malik"], "venue": "IEEE TPAMI, 26(2):214\u2013225,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2004}, {"title": "Clustering by passing messages between data points", "author": ["BJ Frey", "D Dueck"], "venue": "Science, 315(5814):972\u2013976,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A Krizhevsky", "I Sutskever", "GE Hinton"], "venue": "NIPS,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Sampling methods for the nystr\u00f6m method", "author": ["S Kumar", "M Mohri", "A Talwalkar"], "venue": "JMLR, 13(Apr):981\u20131006,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations", "author": ["H Lee", "R Grosse", "R Ranganath", "AY Ng"], "venue": "ICML,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding", "author": ["J Mairal", "F Bach", "J Ponce", "G Sapiro"], "venue": "JMLR, 11:19\u201360,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse coding with an overcomplete basis set: a strategy employed by V1", "author": ["B Olshausen", "DJ Field"], "venue": "Vision research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Are sparse representations really relevant for image classification", "author": ["R Rigamonti", "MA Brown", "V Lepetit"], "venue": "In CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "On random weights and unsupervised feature learning", "author": ["A Saxe", "PW Koh", "Z Chen", "M Bhand", "B Suresh", "A Ng"], "venue": "ICML,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Localityconstrained linear coding for image classification", "author": ["J Wang", "J Yang", "K Yu", "F Lv", "T Huang", "Y Gong"], "venue": "CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Caltech-UCSD Birds 200", "author": ["P Welinder", "S Branson", "T Mita", "C Wah", "F Schroff", "S Belongie", "P Perona"], "venue": "Technical Report CNS-TR- 2010-001, Caltech,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J Yang", "K Yu", "Y Gong"], "venue": "CVPR,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Efficient highly over-complete sparse coding using a mixture model", "author": ["J Yang", "K Yu", "T Huang"], "venue": "ECCV,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "A codebook-free and annotationfree approach for fine-grained image categorization", "author": ["B Yao", "G Bradski", "L Fei-Fei"], "venue": "CVPR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Deconvolutional networks", "author": ["MD Zeiler", "D Krishnan", "GW Taylor", "R Fergus"], "venue": "CVPR,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Improved nystr\u00f6m low-rank approximation and error analysis", "author": ["K Zhang", "IW Tsang", "JT Kwok"], "venue": "ICML,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Pose pooling kernels for subcategory recognition", "author": ["N Zhang", "R Farrell", "T Darrell"], "venue": "CVPR,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 13, "context": "The patches are then encoded into an over-complete representation using various algorithms such as sparse coding [14, 17] and simple inner product with a non-linear post-processing [4, 10].", "startOffset": 113, "endOffset": 121}, {"referenceID": 16, "context": "The patches are then encoded into an over-complete representation using various algorithms such as sparse coding [14, 17] and simple inner product with a non-linear post-processing [4, 10].", "startOffset": 113, "endOffset": 121}, {"referenceID": 3, "context": "The patches are then encoded into an over-complete representation using various algorithms such as sparse coding [14, 17] and simple inner product with a non-linear post-processing [4, 10].", "startOffset": 181, "endOffset": 188}, {"referenceID": 9, "context": "The patches are then encoded into an over-complete representation using various algorithms such as sparse coding [14, 17] and simple inner product with a non-linear post-processing [4, 10].", "startOffset": 181, "endOffset": 188}, {"referenceID": 18, "context": "After encoding, spatial pooling with average or max operations are carried out to form a global image representation [19, 1].", "startOffset": 117, "endOffset": 124}, {"referenceID": 0, "context": "After encoding, spatial pooling with average or max operations are carried out to form a global image representation [19, 1].", "startOffset": 117, "endOffset": 124}, {"referenceID": 12, "context": "Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or locality [17].", "startOffset": 132, "endOffset": 139}, {"referenceID": 3, "context": "Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or locality [17].", "startOffset": 132, "endOffset": 139}, {"referenceID": 13, "context": "Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or locality [17].", "startOffset": 295, "endOffset": 307}, {"referenceID": 18, "context": "Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or locality [17].", "startOffset": 295, "endOffset": 307}, {"referenceID": 19, "context": "Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or locality [17].", "startOffset": 295, "endOffset": 307}, {"referenceID": 16, "context": "Dictionary learning algorithms have been discussed to find a set of basis that reconstructs local image patches or descriptors well [13, 4], and several encoding methods have been proposed to map the original data to a high-dimensional space that emphasizes certain properties, such as sparsity [14, 19, 20] or locality [17].", "startOffset": 320, "endOffset": 324}, {"referenceID": 2, "context": "A particularly interesting finding in the recent papers [3, 15, 4, 16] is that very simple patch-based dictionary learning algorithms like K-means or random selection, combined with feed-forward encoding methods with a naive nonlinearity, produces state-of-the-art performance on various datasets.", "startOffset": 56, "endOffset": 70}, {"referenceID": 14, "context": "A particularly interesting finding in the recent papers [3, 15, 4, 16] is that very simple patch-based dictionary learning algorithms like K-means or random selection, combined with feed-forward encoding methods with a naive nonlinearity, produces state-of-the-art performance on various datasets.", "startOffset": 56, "endOffset": 70}, {"referenceID": 3, "context": "A particularly interesting finding in the recent papers [3, 15, 4, 16] is that very simple patch-based dictionary learning algorithms like K-means or random selection, combined with feed-forward encoding methods with a naive nonlinearity, produces state-of-the-art performance on various datasets.", "startOffset": 56, "endOffset": 70}, {"referenceID": 15, "context": "A particularly interesting finding in the recent papers [3, 15, 4, 16] is that very simple patch-based dictionary learning algorithms like K-means or random selection, combined with feed-forward encoding methods with a naive nonlinearity, produces state-of-the-art performance on various datasets.", "startOffset": 56, "endOffset": 70}, {"referenceID": 15, "context": "Explanation of such phenomenon often focuses on the local image patch statistics, such as the frequency selectivity of random samples [16].", "startOffset": 134, "endOffset": 138}, {"referenceID": 3, "context": ", threshold encoding [4]) or speedup algorithms (e.", "startOffset": 21, "endOffset": 24}, {"referenceID": 16, "context": ", LLC [17]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 1, "context": "Second, reasonably sized dictionary helps to more easily learn further tasks that depends on the encoded features; this is especially true when we have more than one coding-pooling stage such as stacked deep networks, or when one applies more complex pooling stages such as second-order pooling [2], as a large encoding output would immediately drive up the number of parameters in the next layer.", "startOffset": 295, "endOffset": 298}, {"referenceID": 11, "context": "Prior work on addressing such problem often resorts to convolutional approaches [12, 22].", "startOffset": 80, "endOffset": 88}, {"referenceID": 21, "context": "Prior work on addressing such problem often resorts to convolutional approaches [12, 22].", "startOffset": 80, "endOffset": 88}, {"referenceID": 10, "context": "It turns out that under this perspective, the performance of various dictionary learning methods can be explained by the recent findings in Nystr\u00f6m subsampling [11, 23].", "startOffset": 160, "endOffset": 168}, {"referenceID": 22, "context": "It turns out that under this perspective, the performance of various dictionary learning methods can be explained by the recent findings in Nystr\u00f6m subsampling [11, 23].", "startOffset": 160, "endOffset": 168}, {"referenceID": 3, "context": "encoding methods, in which the activation of one code does not rely on other codes, such as threshold encoding [4], which computes the inner product between p and each code, with a fixed threshold parameter \u03b1: fk(x) = max{0,dk p \u2212 \u03b1}.", "startOffset": 111, "endOffset": 114}, {"referenceID": 9, "context": "[10]) also suggests that such simple nonlinearity may suffice to learn a good classifier in the later stages.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Learning the dictionary: Recently, it has been found that relatively simple dictionary learning and encoding approaches lead to surprisingly good performances [3, 16].", "startOffset": 159, "endOffset": 166}, {"referenceID": 15, "context": "Learning the dictionary: Recently, it has been found that relatively simple dictionary learning and encoding approaches lead to surprisingly good performances [3, 16].", "startOffset": 159, "endOffset": 166}, {"referenceID": 2, "context": "We refer to [3] for a detailed comparison about different dictionary learning and encoding algorithms.", "startOffset": 12, "endOffset": 15}, {"referenceID": 11, "context": "Convolutional approaches [12, 22] are usually able to find dictionaries that are more spatially invariant than patchbased K-means, but learning may not scale as well as simple clustering algorithms, especially with hundreds or thousands of codes.", "startOffset": 25, "endOffset": 33}, {"referenceID": 21, "context": "Convolutional approaches [12, 22] are usually able to find dictionaries that are more spatially invariant than patchbased K-means, but learning may not scale as well as simple clustering algorithms, especially with hundreds or thousands of codes.", "startOffset": 25, "endOffset": 33}, {"referenceID": 8, "context": "We then use affinity propagation [9], which is a version of the K-centroids algorithm, to select centroids from existing features.", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "[9]:", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "And we refer to [9] for details about the nature of such message passing algorithms.", "startOffset": 16, "endOffset": 19}, {"referenceID": 19, "context": "Considering that larger dictionaries almost always increases performance [20], and in the limit one could use all the patches P as the dictionary, leading to a N -dimensional space and an N \u00d7N covariance matrix CP .", "startOffset": 73, "endOffset": 77}, {"referenceID": 7, "context": "The Nystr\u00f6m method has been used to approximate large matrices for spectral clustering [8], and here enables us to explain the mechanism of dictionary learning.", "startOffset": 87, "endOffset": 90}, {"referenceID": 10, "context": "Recent research in the machine learning field, notably [11], supports the recent empirical observations in vision: first, it is known that uniformly sampling the columns of the matrix CP already works well in reconstruction, which explains the good performance of random patches in feature learning [16]; second, theoretical results [11, 23] have shown that clustering algorithms works particularly better than other methods as a data-driven way in finding good subsets to approximate the original matrix, justifying the use of clustering in the dictionary learning works.", "startOffset": 55, "endOffset": 59}, {"referenceID": 15, "context": "Recent research in the machine learning field, notably [11], supports the recent empirical observations in vision: first, it is known that uniformly sampling the columns of the matrix CP already works well in reconstruction, which explains the good performance of random patches in feature learning [16]; second, theoretical results [11, 23] have shown that clustering algorithms works particularly better than other methods as a data-driven way in finding good subsets to approximate the original matrix, justifying the use of clustering in the dictionary learning works.", "startOffset": 299, "endOffset": 303}, {"referenceID": 10, "context": "Recent research in the machine learning field, notably [11], supports the recent empirical observations in vision: first, it is known that uniformly sampling the columns of the matrix CP already works well in reconstruction, which explains the good performance of random patches in feature learning [16]; second, theoretical results [11, 23] have shown that clustering algorithms works particularly better than other methods as a data-driven way in finding good subsets to approximate the original matrix, justifying the use of clustering in the dictionary learning works.", "startOffset": 333, "endOffset": 341}, {"referenceID": 22, "context": "Recent research in the machine learning field, notably [11], supports the recent empirical observations in vision: first, it is known that uniformly sampling the columns of the matrix CP already works well in reconstruction, which explains the good performance of random patches in feature learning [16]; second, theoretical results [11, 23] have shown that clustering algorithms works particularly better than other methods as a data-driven way in finding good subsets to approximate the original matrix, justifying the use of clustering in the dictionary learning works.", "startOffset": 333, "endOffset": 341}, {"referenceID": 4, "context": "For STL, we followed [5] and resized them to 32 \u00d7 32.", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "edu/ \u0303acoates/stl10/, [3]", "startOffset": 22, "endOffset": 25}, {"referenceID": 17, "context": "To show the performance of the feature learning algorithm in the real-world image classification tasks, we tested the performance of our algorithm on the fine-grained classification task, using the 2011 Caltech-UCSD Birds dataset [18]5.", "startOffset": 230, "endOffset": 234}, {"referenceID": 6, "context": "Recent work on fine-grained classification usually focuses on the localization of parts [7, 24, 6], and still uses manually designed features.", "startOffset": 88, "endOffset": 98}, {"referenceID": 23, "context": "Recent work on fine-grained classification usually focuses on the localization of parts [7, 24, 6], and still uses manually designed features.", "startOffset": 88, "endOffset": 98}, {"referenceID": 5, "context": "Recent work on fine-grained classification usually focuses on the localization of parts [7, 24, 6], and still uses manually designed features.", "startOffset": 88, "endOffset": 98}, {"referenceID": 20, "context": "[21] proposed to use a template-based approach for more powerful features, but the approach may be difficult to scale as the number of templates grow larger6.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "For the image pre-processing we followed the same setting as [24, 21] by cropping the images to be cen-", "startOffset": 61, "endOffset": 69}, {"referenceID": 20, "context": "For the image pre-processing we followed the same setting as [24, 21] by cropping the images to be cen-", "startOffset": 61, "endOffset": 69}, {"referenceID": 6, "context": "html 6Due to computation complexity, some early work such as [7, 21] do not scale up well, and only reported performance on subsets of the whole data [personal communication].", "startOffset": 61, "endOffset": 68}, {"referenceID": 20, "context": "html 6Due to computation complexity, some early work such as [7, 21] do not scale up well, and only reported performance on subsets of the whole data [personal communication].", "startOffset": 61, "endOffset": 68}, {"referenceID": 23, "context": "BoW SIFT Baseline [24] 18.", "startOffset": 18, "endOffset": 22}, {"referenceID": 23, "context": "60 Pose Pooling + linear SVM [24] 24.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "21 Pose Pooling + \u03c7 SVM [24] 28.", "startOffset": 24, "endOffset": 28}, {"referenceID": 4, "context": "18 K-means + linear SVM (as in [5]) 38.", "startOffset": 31, "endOffset": 34}, {"referenceID": 23, "context": "Our classification results, together with previous stateof-the-art baselines from [24], are reported in Table 2.", "startOffset": 82, "endOffset": 86}], "year": 2013, "abstractText": "Unsupervised dictionary learning has been a key component in state-of-the-art computer vision recognition architectures. While highly effective methods exist for patchbased dictionary learning, these methods may learn redundant features after the pooling stage in a given early vision architecture. In this paper, we offer a novel dictionary learning scheme to efficiently take into account the invariance of learned features after the spatial pooling stage. The algorithm is built on simple clustering, and thus enjoys efficiency and scalability. We discuss the underlying mechanism that justifies the use of clustering algorithms, and empirically show that the algorithm finds better dictionaries than patch-based methods with the same dictionary size.", "creator": "LaTeX with hyperref package"}}}