{"id": "1302.5348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Feb-2013", "title": "Graph-based Generalization Bounds for Learning Binary Relations", "abstract": "We investigate the generalizability of learned binary relations: functions that map pairs of instances to a logical indicator. This problem has application in numerous areas of machine learning, such as ranking, entity resolution and link prediction. Our learning framework incorporates an example labeler that, given a sequence $X$ of $n$ instances and a desired training size $m$, subsamples $m$ pairs from $X \\times X$ without replacement. The challenge in analyzing this learning scenario is that pairwise combinations of random variables are inherently dependent, which prevents us from using traditional learning-theoretic arguments. We present a unified, graph-based analysis, which allows us to analyze this dependence using well-known graph identities. We are then able to bound the generalization error of learned binary relations using Rademacher complexity and algorithmic stability. The rate of uniform convergence is partially determined by the labeler's subsampling process. We thus examine how various assumptions about subsampling affect generalization; under a natural random subsampling process, our bounds guarantee $\\tilde{O}(1/\\sqrt{n})$ uniform convergence.", "histories": [["v1", "Thu, 21 Feb 2013 17:30:42 GMT  (23kb,D)", "https://arxiv.org/abs/1302.5348v1", null], ["v2", "Wed, 27 Feb 2013 03:33:47 GMT  (23kb,D)", "http://arxiv.org/abs/1302.5348v2", null], ["v3", "Fri, 31 May 2013 21:12:47 GMT  (23kb,D)", "http://arxiv.org/abs/1302.5348v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ben london", "bert huang", "lise getoor"], "accepted": false, "id": "1302.5348"}, "pdf": {"name": "1302.5348.pdf", "metadata": {"source": "CRF", "title": "Graph-based Generalization Bounds for Learning Binary Relations", "authors": ["Ben London", "Bert Huang", "Lise Getoor"], "emails": ["blondon@cs.umd.edu", "bert@cs.umd.edu", "getoor@cs.umd.edu"], "sections": [{"heading": null, "text": "We investigate the generalizability of learned binary relationships: functions that map instance pairs into a logical indicator, a problem that is applied in many areas of machine learning, such as ranking, unit resolution, and link prediction. Our learning framework includes an example labeler that distinguishes pairs of X \u00b7 X pairs from pairs of X \u00b7 X. The challenge in analyzing this learning scenario is that paired combinations of random variables are inherently dependent, preventing us from using traditional learning theory arguments. We present a unified, graph-based analysis that allows us to analyze this dependence based on known graph identities, and then we are able to tie the generalization error of learned binary relationships using Rademacher's complexity and algorithmic stability."}, {"heading": "1 Introduction", "text": "We examine the generalizability of a learned binary relationship: a characteristic function r: X 2 \u2192 {\u00b1 1} that indicates whether the input pair fulfills a relationship. For example, if r is an equivalence relationship, r (x, x) = 1 indicates that x \u2261 x \u2032; if r is a total order, r (x, x \u2032) = 1 means that x \u2264 x \u2032. Binary relationships are found in many learning problems, such as ranking (total order), entity resolution (equivalence) and linkage prediction. There has been significant searching in each of these fields individually, but not a uniform view of the learning problem. We formulate the learning goal (in Section 2) as an inductive inference to a product space X 2, where substances are drawn from an arbitrary distribution over X. In the face of a set of n independently and identically distributed (i.e.) instances X-Xn, as well as a substance from X2 that we have."}, {"heading": "1.1 Related Work", "text": "Goldman et al. [1993] were the first to deal with learning a binary relationship; their learning environment is completely different from ours in that there is no underlying distribution from which instances are generated, and they are not interested in generalizing invisible data. In their environment, the problem size is polynomial in a finite number of objects, and our learning environment subsumes these one. Our most important analytical tools come from two areas of learning theory: wheel-maker complexity and algorithmic stability. Canonical wheel-maker boundaries are due to Koltchinskii and Panchenko [2002] and Bartlett and Mendelson [2003], while the canonical stability boundaries are due to Bousquet and Elisseeff [2002]. Although all these techniques are fundamentally based on independence, there are newer applications for non-i.i.d. problems. Inspired by Janson [2004], Usunier et al al al al al al al al al al al al al al al al al al al al al al."}, {"heading": "2 Preliminaries", "text": "In this section, the necessary background concepts are presented. In order to clarify certain probabilities, we have Pr\u03c9 [\u00b7] designate the probability of an event w.r.t. as a random drawing of \u03c9 from an implicit sampling space \u0445. Similarly, we have E\u03c9 [\u00b7] designate the expectation of a random variable w.r.t. as a random drawing of \u03c9 \u0435\u0435, corresponding to an implicit probability distribution."}, {"heading": "2.1 Problem Setting", "text": "We are interested in learning a binary relationship r: X 2 \u2192 {\u00b1 1}. We limit our analysis to relationships that are reflexive and either symmetrical or antisymmetrical, examples of which are equivalence and total order. We define the learning process as follows: We get a sequence of instances X, (x1,., xn), Xn, which are independently and identically drawn from an arbitrary distribution over X. We also get access to a Labeler S, a black box process that returns a subset of pairwise labeled examples. Specifically, when entering X and a training quantity m, Sm (X) returns a subset of X2 (sampled without replacement) that has been labeled without replacement. In practice, this could be a crowd sourcing application or a targeted measurement process."}, {"heading": "2.2 Generalization", "text": "For a given cost function c: R2 \u2192 R +, we define the loss of \"a hypothesis h w.r.t. a pair z, (x, x \u2032) as\" (h, z), c (r (z), h (z)). Although there are various cost functions, we are most interested in learning binary relationships for the so-called 0-1 loss, \"1 (h, z), 1 [r (z) 6 = h (z)].The quantity we are primarily interested in is the generalization error or risk. This is the expected loss of a learned hypothesis hZ w.r.t. a random pair z, (x, x), in which x and x \u2032 are queried independently and identically from the underlying distribution over X. We refer to this quantity by R (hZ), Ez ['(hZ, z) as the deviation from the probability (z).In practice, we can estimate the true risk with the training error calculated as the average loss above the examples in training."}, {"heading": "3 Graphical Representation", "text": "In this section, we analyze the learning problem and its inherent dependencies using a graphical representation."}, {"heading": "3.1 Training Data", "text": "Remember that the training set returned by the label S is an arbitrary subset of X2 labeled to r. We can represent this as graph G, (V, E) in which each vertex vi, V corresponds to an instance xi, and each training example (xi, xj, r (xi, xj)) defines an (un) directed edge (i, j) E. Thus, the pattern of substance sampling is reduced to a graph to instances X. Note that if r is antisymmetrical, then G is directed; however, since only one example per pair is needed, the corresponding undirected graph is simple (i.e., not a multigraph). This graph representation simplifies the analysis of generalization and allows the marker to view the substance sample as one of many well-studied graph generation processes."}, {"heading": "3.2 Dependency Structure", "text": "For each instance xi define a random variable Xi and remember that they are dependencies. For each example pair (xi, xj) found in Z, define a random variable Zi, j, (Xi, Xj). Since each instance can occur in several pairs, we have that these random variables are not independent of each other. To make this more concrete, consider the set {Z1,2, Z2,3, Z3,4}. The observation (X1, X2) reveals nothing about (X3, X4); hence P (Z3,4 | Z1,2) = P (Z3,4) and vice versa, so they are independent of each other."}, {"heading": "3.3 Chromatic Properties", "text": "In this section, we will discuss the chromatic properties of the above graphics, which introduce a key dilemma used in our generalization boundaries. For the sake of completeness, we will first consider some background information on graph coloring. G, (V, E) is an arbitrary, undirected graph for the following definitions. Definition 2 (vertex coloring). Correct k-texted coloring (often simply referred to as kcoloring) is an assignment of V to a series of k color classes, so that no two adjacent vertices have the same color. Likewise, it is a partitioning {Cj: Cj V} kj = 1, so that k = 1 Cj = V, k = 1 Cj = G, and each subset of Cj is independent. The chromatic number (G) is the minimum number of colors required to achieve a correct coloring. Definition 3 (Coloring Edge-correction) is a coloring of two."}, {"heading": "4 Generalization Bounds", "text": "In this section we develop risk limits for learning binary relationships using both the Rademacher complexity and algorithmic stability."}, {"heading": "4.1 Concentration Inequalities", "text": "A key component in any generalization analysis is the concentration of random variables. We now present two limits used in our proofs. Theorem 2 (McDiarmid, 1989): Z, {Zi} ni = 1 are a set of i.i.d. random variables that assume values in Z. Let f: Zn \u2192 R be a measurable function, for which constants {\u03b1i} ni = 1 exist in such a way that for all i [n] and all inputs Z, Z \u2032 Zn, which differ only in the ith variable, | f (Z) \u2212 f (Z \u2032) | \u2264 \u03b1i. Then for all > 0, Pr [f (Z) \u2212 E [f (Z)] \u2264 exp (\u2212 2 \u00b2 n i = 1 \u03b1 2 i).Theorem 3 (Usunier et al., 2006)."}, {"heading": "4.2 Rademacher Complexity", "text": "\"We have to adjust to the fact that we're not going to be able to be ourselves, to be able to show that we're going to be able to be ourselves, to be able to be ourselves, to be able to show that we're going to be able, to be able to be able, to be able to be able to be ourselves, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able, to be able to be able to be a random, to be able to be able to adapt, to be able to be able to be able, to be able to be able to be able to be a random, to be able to be able to adapt.\""}, {"heading": "4.3 Algorithmic Stability", "text": "This chapter derives another generalization used for learning pairs of relationships using our previous graph and algorithmic stabilization. (Definition 5 (Uniform Stability). (Definition 5 (Uniform Stability). (Definition). (Definition 5 (Uniform Stability). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (Definition). (. (.). (. (.). (.). (. (.). (.). (. (.). (.). (.). (. (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.). (.). (.).). (. (.).). (.). (. (.).). (. (.).). (.). (. (.).). (.).). (.). (."}, {"heading": "4.4 Discussion of Bounds", "text": "We refer to the inversion of this ratio as the effective training quantity. If we let \u03c1i specify the frequency (i.e. the degree) of instance xi, we have the following frequency for each regular G: m = 2\u03c1 \u00b2 n i \u00b2 n i \u00b2 n i \u00b2 n i \u00b2. It is easy to show that this quantity is minimized if G is regular - i.e., if \u03c1i is uniform. In fact, for each regular G we have such a number / m = 2 / n.Assuming that no new examples can be obtained, one can discard examples to create a regular diagram that gives the optimal ratio. This is equivalent to finding a credential sub-diagram (i.e., k factor) that is effective for some k \u00b2 factors without reducing the number of instances."}, {"heading": "5 On Subsampling and the Rate of Uniform Convergence", "text": "We have shown that the empirical risk converges at a rate of O = > 2m convergence for optimal convergence (2m convergence), depending primarily on the size of the training set and the maximum frequency of each instance. While m can be determined by your own note or calculation budget, \u03c1 depends on the sample used to select the training set. In this section, we will examine the relationship between the sample sampling process used by the labeller and the rate of uniform convergence. Note that the labeller cannot base a substance sample on the values of the input data, but patterns that help or violate generalization. If the labeller works against the learner, he can select pairs so that one instance appears in all training examples, meaning that our limitations indicate that a hypothesis we have learned from this training set may not be generalizable, as opposed to the one that works with the previous laboratory pair."}, {"heading": "Acknowledgements", "text": "This work was partially supported by NSF-CAREER grant 0746930 and NSF grant IIS1218488."}, {"heading": "A Proof of Lemma 5", "text": "For each pair of vertices {vi, vj} define a random variable egg, j, 1 [{i, j} \u0441E]. Note that deg (vi) = \u2211 j 6 = iEi, j and, via the linearity of the expectation, E [deg (vi)] = \u2211 j 6 = i E [egg, j] = (n \u2212 1) \u00b7 (n2) \u2212 1 m \u2212 1) ((((n2) m) = 2mn. Since the expected degree is uniform, allow \u00b5, E [deg (vi)]. Using the composite boundary for each t > 0, it results that Pr [\u2206 (G) \u2265 t] \u0445i = 1 Pr [deg (vi) \u0432i = 1 Pr \u0445 j 6 = i egg, j \u0430 t. Although the variables {egg, j} j 6 = i are dependent, it is easy to show that they are negatively correlated."}], "references": [{"title": "Generalization bounds for ranking algorithms via algorithmic stability", "author": ["S. Agarwal", "P. Niyogi"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Agarwal and Niyogi.,? \\Q2009\\E", "shortCiteRegEx": "Agarwal and Niyogi.", "year": 2009}, {"title": "Learning with equivalence constraints, and the relation to multiclass learning", "author": ["A. Bar-Hillel", "D. Weinshall"], "venue": "In Proceedings of the Conference on Computational Learning Theory,", "citeRegEx": "Bar.Hillel and Weinshall.,? \\Q2003\\E", "shortCiteRegEx": "Bar.Hillel and Weinshall.", "year": 2003}, {"title": "Rademacher and gaussian complexities: risk bounds and structural results", "author": ["P. Bartlett", "S. Mendelson"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bartlett and Mendelson.,? \\Q2003\\E", "shortCiteRegEx": "Bartlett and Mendelson.", "year": 2003}, {"title": "Stability and generalization", "author": ["O. Bousquet", "A. Elisseeff"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bousquet and Elisseeff.,? \\Q2002\\E", "shortCiteRegEx": "Bousquet and Elisseeff.", "year": 2002}, {"title": "Ranking and empirical minimization of U-statistics", "author": ["S. Cl\u00e9men\u00e7on", "G. Lugosi", "N. Vayatis"], "venue": "The Annals of Statistics,", "citeRegEx": "Cl\u00e9men\u00e7on et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cl\u00e9men\u00e7on et al\\.", "year": 2008}, {"title": "An introduction to support vector machines and other kernel-based learning methods", "author": ["N. Cristianini", "J. Shawe-Taylor"], "venue": null, "citeRegEx": "Cristianini and Shawe.Taylor.,? \\Q2000\\E", "shortCiteRegEx": "Cristianini and Shawe.Taylor.", "year": 2000}, {"title": "Learning binary relations and total orders", "author": ["S. Goldman", "R. Schapire", "R. Rivest"], "venue": "SIAM J. Computing,", "citeRegEx": "Goldman et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Goldman et al\\.", "year": 1993}, {"title": "Large deviations for sums of partly dependent random variables", "author": ["S. Janson"], "venue": "Random Structures Algorithms,", "citeRegEx": "Janson.,? \\Q2004\\E", "shortCiteRegEx": "Janson.", "year": 2004}, {"title": "Regularized distance metric learning: Theory and algorithm", "author": ["R. Jin", "S. Wang", "Y. Zhou"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Jin et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2009}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "author": ["V. Koltchinskii", "D. Panchenko"], "venue": "Annals of Statistics,", "citeRegEx": "Koltchinskii and Panchenko.,? \\Q2002\\E", "shortCiteRegEx": "Koltchinskii and Panchenko.", "year": 2002}, {"title": "Probability in Banach spaces: isoperimetry and processes", "author": ["M. Ledoux", "M. Talagrand"], "venue": "Ergebnisse der Mathematik und ihrer Grenzgebiete. SpringerVerlag,", "citeRegEx": "Ledoux and Talagrand.,? \\Q1991\\E", "shortCiteRegEx": "Ledoux and Talagrand.", "year": 1991}, {"title": "On the method of bounded differences. In Surveys in Combinatorics, volume 141 of London Mathematical Society Lecture Note Series, pages 148\u2013188", "author": ["C. McDiarmid"], "venue": null, "citeRegEx": "McDiarmid.,? \\Q1989\\E", "shortCiteRegEx": "McDiarmid.", "year": 1989}, {"title": "Rademacher complexity bounds for non-i.i.d. processes", "author": ["M. Mohri", "A. Rostamizadeh"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mohri and Rostamizadeh.,? \\Q2009\\E", "shortCiteRegEx": "Mohri and Rostamizadeh.", "year": 2009}, {"title": "Stability bounds for stationary \u03c6-mixing and \u03b2-mixing processes", "author": ["M. Mohri", "A. Rostamizadeh"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Mohri and Rostamizadeh.,? \\Q2010\\E", "shortCiteRegEx": "Mohri and Rostamizadeh.", "year": 2010}, {"title": "Chromatic PAC-bayes bounds for non-iid data: Applications to ranking and stationary \u03b2-mixing processes", "author": ["L. Ralaivola", "M. Szafranski", "G. Stempfel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Ralaivola et al\\.,? \\Q1956\\E", "shortCiteRegEx": "Ralaivola et al\\.", "year": 1956}, {"title": "Generalization error bounds for classifiers trained with interdependent data", "author": ["N. Usunier", "M.-R. Amini", "P. Gallinari"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Usunier et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Usunier et al\\.", "year": 2006}, {"title": "A theory of the learnable", "author": ["L. Valiant"], "venue": "In Proceedings of the Sixteenth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "Valiant.,? \\Q1984\\E", "shortCiteRegEx": "Valiant.", "year": 1984}, {"title": "On an estimate of the chromatic class of a p-graph", "author": ["V. Vizing"], "venue": "Diskret. Analiz.,", "citeRegEx": "Vizing.,? \\Q1964\\E", "shortCiteRegEx": "Vizing.", "year": 1964}], "referenceMentions": [{"referenceID": 6, "context": "The canonical Rademacher bounds are due to Koltchinskii and Panchenko [2002] and Bartlett and Mendelson [2003], while the canonical stability bounds are due to Bousquet and Elisseeff [2002].", "startOffset": 43, "endOffset": 77}, {"referenceID": 2, "context": "The canonical Rademacher bounds are due to Koltchinskii and Panchenko [2002] and Bartlett and Mendelson [2003], while the canonical stability bounds are due to Bousquet and Elisseeff [2002].", "startOffset": 81, "endOffset": 111}, {"referenceID": 2, "context": "The canonical Rademacher bounds are due to Koltchinskii and Panchenko [2002] and Bartlett and Mendelson [2003], while the canonical stability bounds are due to Bousquet and Elisseeff [2002]. Though all of these techniques fundamentally rely on independence, there have been recent applications to non-i.", "startOffset": 81, "endOffset": 190}, {"referenceID": 2, "context": "The canonical Rademacher bounds are due to Koltchinskii and Panchenko [2002] and Bartlett and Mendelson [2003], while the canonical stability bounds are due to Bousquet and Elisseeff [2002]. Though all of these techniques fundamentally rely on independence, there have been recent applications to non-i.i.d. problems. Inspired by Janson [2004], Usunier et al.", "startOffset": 81, "endOffset": 344}, {"referenceID": 2, "context": "The canonical Rademacher bounds are due to Koltchinskii and Panchenko [2002] and Bartlett and Mendelson [2003], while the canonical stability bounds are due to Bousquet and Elisseeff [2002]. Though all of these techniques fundamentally rely on independence, there have been recent applications to non-i.i.d. problems. Inspired by Janson [2004], Usunier et al. [2006] developed a theory of chromatic Rademacher analysis, using it to derive generic risk bounds for dependent data, with bipartite ranking as a motivating example.", "startOffset": 81, "endOffset": 367}, {"referenceID": 2, "context": "The canonical Rademacher bounds are due to Koltchinskii and Panchenko [2002] and Bartlett and Mendelson [2003], while the canonical stability bounds are due to Bousquet and Elisseeff [2002]. Though all of these techniques fundamentally rely on independence, there have been recent applications to non-i.i.d. problems. Inspired by Janson [2004], Usunier et al. [2006] developed a theory of chromatic Rademacher analysis, using it to derive generic risk bounds for dependent data, with bipartite ranking as a motivating example. (Ralaivola et al. [2010] used a similar chromatic analysis to derive PAC-Bayes bounds.", "startOffset": 81, "endOffset": 552}, {"referenceID": 0, "context": "Agarwal and Niyogi [2009] consider a random subsampling process similar to one we discuss and derive risk bounds for ranking using algorithmic stability.", "startOffset": 0, "endOffset": 26}, {"referenceID": 16, "context": "We will use the canonical probably approximately correct (PAC) framework [Valiant, 1984], in which we allow the defect to exceed an arbitrary > 0 with probability \u03b4 \u2208 (0, 1).", "startOffset": 73, "endOffset": 88}, {"referenceID": 17, "context": "Theorem 1 (Vizing, 1964).", "startOffset": 10, "endOffset": 24}, {"referenceID": 11, "context": "Theorem 2 (McDiarmid, 1989).", "startOffset": 10, "endOffset": 27}, {"referenceID": 15, "context": "Theorem 3 (Usunier et al., 2006).", "startOffset": 10, "endOffset": 32}, {"referenceID": 5, "context": "One such class of hypotheses is reproducing kernel Hilbert spaces (RKHS), which subsume the popular support vector machine (SVM) [Cristianini and Shawe-Taylor, 2000].", "startOffset": 129, "endOffset": 165}, {"referenceID": 4, "context": "One such class of hypotheses is reproducing kernel Hilbert spaces (RKHS), which subsume the popular support vector machine (SVM) [Cristianini and Shawe-Taylor, 2000]. Formally, for some mapping \u03c6 : Z \u2192 Z\u0303, where Z is an instance space and Z\u0303 is a Hilbert space, endowed with an inner product \u3008\u00b7, \u00b7\u3009, a kernel \u03ba : Z \u2192 R is a function such that, for all (z, z\u2032) \u2208 Z, \u03ba(z, z\u2032) = \u3008\u03c6(z), \u03c6(z\u2032)\u3009. The only requirement is that the kernel\u2019s Gram matrix K : Ki,j = \u03ba(zi, zj) be symmetric and positive semidefinite. RKHS hypotheses are generally of the form h\u03ba,Z(z) , \u2211m z\u2032\u2208Z \u03ba(z, z \u2032), where Z \u2208 Z is a set of reference points from the problem domain (e.g., support vectors). We may reasonably assume that the norm of the kernel mapping is uniformly bounded by a constant C; i.e., the mapped data is contained within a ball of radius C. We denote the class of kernel functions by H\u03ba. Borrowing results from Koltchinskii and Panchenko [2002] and Bartlett and Mendelson [2003], we are able to prove the following risk bounds for kernel-based hypotheses with bounded kernels, in the context of learning binary relations.", "startOffset": 130, "endOffset": 932}, {"referenceID": 2, "context": "Borrowing results from Koltchinskii and Panchenko [2002] and Bartlett and Mendelson [2003], we are able to prove the following risk bounds for kernel-based hypotheses with bounded kernels, in the context of learning binary relations.", "startOffset": 61, "endOffset": 91}, {"referenceID": 10, "context": "To bound Rn(`\u03b3 \u25e6 H\u03ba \u25e6 Sm), we use Talagrand\u2019s contraction lemma [Ledoux and Talagrand, 1991] and borrow a result from Bartlett and Mendelson [2003, Lemma 22], from which we obtain", "startOffset": 64, "endOffset": 92}, {"referenceID": 15, "context": "Note that this analysis slightly improves upon that of Usunier et al. [2006] in that we use the regular Rademacher complexity instead of their so-called fractional Rademacher complexity.", "startOffset": 55, "endOffset": 77}, {"referenceID": 3, "context": "Lemma 2 (Bousquet and Elisseeff, 2002).", "startOffset": 8, "endOffset": 38}, {"referenceID": 3, "context": "Lemma 3 (Bousquet and Elisseeff, 2002).", "startOffset": 8, "endOffset": 38}, {"referenceID": 3, "context": "Using a stability result from Bousquet and Elisseeff [2002], we obtain a risk bound for SVM classification.", "startOffset": 30, "endOffset": 60}, {"referenceID": 3, "context": "Lemma 4 (Bousquet and Elisseeff, 2002).", "startOffset": 8, "endOffset": 38}, {"referenceID": 0, "context": "Whereas Agarwal and Niyogi [2009] invoke parameterized families of edge distributions, we consider a simple, intuitive learning setup in which the only parameter is the size m of the training set.", "startOffset": 8, "endOffset": 34}], "year": 2013, "abstractText": "We investigate the generalizability of learned binary relations: functions that map pairs of instances to a logical indicator. This problem has application in numerous areas of machine learning, such as ranking, entity resolution and link prediction. Our learning framework incorporates an example labeler that, given a sequenceX of n instances and a desired training size m, subsamples m pairs from X \u00d7X without replacement. The challenge in analyzing this learning scenario is that pairwise combinations of random variables are inherently dependent, which prevents us from using traditional learning-theoretic arguments. We present a unified, graph-based analysis, which allows us to analyze this dependence using well-known graph identities. We are then able to bound the generalization error of learned binary relations using Rademacher complexity and algorithmic stability. The rate of uniform convergence is partially determined by the labeler\u2019s subsampling process. We thus examine how various assumptions about subsampling affect generalization; under a natural random subsampling process, our bounds guarantee \u00d5(1/ \u221a n) uniform convergence.", "creator": "TeX"}}}