{"id": "1302.6210", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Feb-2013", "title": "A Homogeneous Ensemble of Artificial Neural Networks for Time Series Forecasting", "abstract": "Enhancing the robustness and accuracy of time series forecasting models is an active area of research. Recently, Artificial Neural Networks (ANNs) have found extensive applications in many practical forecasting problems. However, the standard backpropagation ANN training algorithm has some critical issues, e.g. it has a slow convergence rate and often converges to a local minimum, the complex pattern of error surfaces, lack of proper training parameters selection methods, etc. To overcome these drawbacks, various improved training methods have been developed in literature; but, still none of them can be guaranteed as the best for all problems. In this paper, we propose a novel weighted ensemble scheme which intelligently combines multiple training algorithms to increase the ANN forecast accuracies. The weight for each training algorithm is determined from the performance of the corresponding ANN model on the validation dataset. Experimental results on four important time series depicts that our proposed technique reduces the mentioned shortcomings of individual ANN training algorithms to a great extent. Also it achieves significantly better forecast accuracies than two other popular statistical models.", "histories": [["v1", "Mon, 25 Feb 2013 20:09:19 GMT  (419kb)", "http://arxiv.org/abs/1302.6210v1", "8 pages, 4 figures, 2 tables, 26 references, international journal"]], "COMMENTS": "8 pages, 4 figures, 2 tables, 26 references, international journal", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["ratnadip adhikari", "r k agrawal"], "accepted": false, "id": "1302.6210"}, "pdf": {"name": "1302.6210.pdf", "metadata": {"source": "META", "title": "A Homogeneous Ensemble of Artificial Neural Networks for Time Series Forecasting", "authors": ["Ratnadip Adhikari", "R. K. Agrawal"], "emails": [], "sections": [{"heading": null, "text": "Recently, artificial neural networks (ANNs) have found extensive applications in many practical prediction problems. However, the standard ANN training algorithm for back propagation has some critical problems, such as slow convergence rates and frequent convergence to a local minimum, the complex pattern of error interfaces, the lack of appropriate selection methods for training parameters, etc. To overcome these drawbacks, various improved training methods have been developed in the literature, but none of them can be guaranteed to be the best for all problems. In this paper, we propose a novel weighted ensemble scheme that intelligently combines several training parameters to increase ANN prediction accuracies. Weight for each training algorithm is determined by the performance of the corresponding ANN model on the validation dataset. Experimental results over four important time series show that our proposed technique reduces the shortcomings of ANN training algorithms to a high degree."}, {"heading": "1. INTRODUCTION", "text": "In this context, it should be noted that the case is an accident."}, {"heading": "2. ARTIFICIAL NEURAL NETWORKS", "text": "The most common ANNs for predicting time series are Multilayer Perceptrons (MLPs) [2-5], which are characterized by the feedback architecture of an input layer, one or more hidden layers, and an output layer. Nodes in each layer are connected by acyclic links to those in the next layer. In practical applications, it is sufficient to look at a single hidden layer structure [2,4,5]. The output of an MLP with p-input and h-hidden nodes is called [2,4]: 0 01 1.pht j ij t ij t ij i i y G F y (1) Here, 1.2 t iy i p are the network inputs; ij jk are the connection weights 1,2, 1,2, i p j h; 0 0, j the terms, and F G each the hidden and output layer activation functions. Normally, logistics and identity functions are used especially for F and G, i.e. (F) (x) (1) (x) (1)."}, {"heading": "2.1 Backpropagation Training", "text": "The training is the iterative process for determining optimal network weights and biases. In this phase, the ANN model gradually learns from successive input patterns and target values and modifies the weights and biases accordingly. In a time series prediction problem with training data set 1 2,,,,, Ny y y y y y a (p, h, 1) ANN model consists of (N-p) training patterns with input vectors 1 1,,,, T i i i i i i py y y y y and targets, i py (i = 1, 2,..., N-p). The propagation is a supervised training algorithm in which network weights and preset updates are performed by minimizing the error function [4,, 5]: 211, 2Nt tt pE y y y (2), where ty is the network performance, calculated by equation (1) and ty the corresponding target. The algorithm begins with a beginning with a vector weight and a vy y y y y y (2) literature, where ty is the values are updated by Equation (1) and ty the corresponding target."}, {"heading": "2.1.1 The RPROP Algorithm", "text": "In the steepest downward method, the gradient often reaches a very low value, resulting in negligible changes in weights and distortions, even though they are actually far from their optimum values. To eliminate this disadvantage, the RPROP training algorithm was proposed, which only takes into account the signs of partial derivatives of the error functions to determine the direction of the weight changes [9,18]. The order of magnitude of the derivatives does not affect weight updates. This algorithm is very efficient and easy to use."}, {"heading": "2.1.2 The Conjugate Gradient Algorithms", "text": "Although the steepest sink direction causes the fastest reduction in power function, it does not necessarily mean the fastest convergence. Due to this fact, conjugated gradient methods perform a search in conjugated directions of the gradient. An efficient method of this kind is the Scaled Conjugate Gradient (SCG) algorithm developed by Moller, which is used in this paper as one of the constituent training algorithms."}, {"heading": "2.1.3 The Quasi-Newton Algorithms", "text": "This class of methods uses second-order derivatives for weight and bias modifications. The optimal search direction is calculated by 1, E H w, with H being the Hessian matrix of the error function. However, due to the expensive calculation requirements, the Hessian matrix is not calculated directly, but assumed as a function of the iteratively approximated gradient [18]. The most successful and widely used quasi-Newton method in literature is the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm [12,18]. Two more of this type used in the present paper are: the Levenberg-Marquardt (LM) [8] - and One Step Secant (OSS) [11] algorithms. LM in particular is the fastest training method so far in terms of convergence rate, but requires enormous amounts of memory and mathematical calculations [18]."}, {"heading": "2.1.4 PSO-Based Training Algorithms", "text": "PSO is an evolutionary optimization technique originally inspired by the intelligent work paradigm in flocks of birds and fish. [13,14] Consider a (p, h, q) neural network structure with N particles in the swarm. Each particle represents an individual ANN structure with the dimension D = h (p + q + 1) + q, the total number of network parameters. The PSO algorithm begins with the assignment of randomized positions and velocities to each particle. Particles are moved through the D-dimensional search space until any error minimization criterion is met. Each particle evaluates a fitness function (error function in this study) for this. Particle movements are regulated by two best positions, namely personal and global, which are each the current criterion for error minimization."}, {"heading": "3. PROPOSED ENSEMBLE TECHNIQUE", "text": "Consider a time series of 1 2,,,, NY y y y y y y ny divided into three subsets, namely validation, training, and testing, which are used to select the best prediction model, estimate the model parameters, and assess the accuracy of the predictions resulting from the sample. Let L1, L2,... Lk are k (preferably an odd number) training algorithms used after determining the correct ANN structure for the time series. Our ensemble approach is now described below. The calculated ANN model is trained with each Li on the training dataset. These ANNs are then used to predict the validation and measure its subsequent forecasting performance, defining three error statistics: Mean Absolute Error (MAE) = 1 Mean Squared Error (MSE) = 1 Mean Absolute Percentage Error."}, {"heading": "4. TWO STATISTICAL MODELS", "text": "In order to compare the predictive accuracy of our proposed method, two other well-known models are used in this paper, namely Autoregressive Integrated Moving Average (ARIMA) and Support Vector Machine (SVM)."}, {"heading": "4.1 ARIMA Models", "text": "The ARIMA or Box-Jenkins models [1] are based on the assumption that the observations of a time series are generated on the basis of a linear function of the past values and a random noise process [1,2,4], which is actually a generalization of the autoregressive moving average (ARMA) models [1] dealing with non-stationary time series. Mathematically, an ARIMA (p, d, q) model can be represented as follows: 1111wo, 1, 1andd t tp iii qj jt tL y LL LLLy y y y y y y (7) Here, p, d, q are the order of the model referring to the autoregressive, degree of differentiation and moving average processes; yt is an actual time series and \u03b5t is a random noise process; perception (L) and \u03b8 (L) are downstream polynomials of orders, q, with the moving average processes; yt is an arbitrary time series and \u03b5t is a random noise process; L (L) and \u03b8 (L) are downstream polynomials of an order, q, with the coefficient, cop, cop, etc. (2), Ippi, an arbitrary time series (L), Ippi, Ippi, an arbitrary time series (1), Ippi, (1), Ippi, (1, 1, Ippi), (1, Ippi)."}, {"heading": "4.2 SVM Model", "text": "SVM is a new statistical learning theory developed by Vapnik and colleagues at the AT & T Bell laboratories in 1995 [21], based on the principle of Structural Risk Minimization (SRM), and its goal is to find a decision rule with good generalizability by selecting some specific data points known as support vectors [21,22]. Time series predictions are a branch of Support Vector Regression (SVR), in which an optimal separation level is constructed to correctly classify real values. In the face of a training data set of N points 1, Ni i y x with, ni x iy, SVM tries to approximate the unknown file creation function in the following form: f (x) = w \u00b7 (x) + b, where w is the weight vector, \u03c6 is the nonlinear mapping to a higher dimensional feature space and b the bias term."}, {"heading": "5. EXPERIMENTS AND DISCUSSIONS", "text": "To investigate the effectiveness of our proposed ensemble technology, four widely used real time series from different areas are used - the Canadian lynx, the wolf sunspots, the monthly international passengers, and the monthly sales of red wine time series. All of these four datasets are collected by the Time Series Data Library (TSDL). Table 1 provides the necessary descriptions of them, and Figure 2 shows the corresponding time series. Selecting an appropriate validation set is critical to the success of our ensemble scheme. Here, we select the lengths of the validation datasets in a manner roughly consistent with the lengths of the corresponding test sets of the four time series. All the experiments in this paper are implemented by MATLAB. The appropriate ANN structures are determined on the basis of common model selection criteria as discussed in details by Zhang et al. [4] The appropriate ANN model for each dataset al 2000 is trained for epoch."}, {"heading": "6. CONCLUSIONS", "text": "The great popularity of ANNs in the prediction community can be attributed to their many distinctive and outstanding characteristics. However, the standard method of the backpropagation network often suffers from a number of inherent disadvantages, such as: the complex pattern of error interfaces, slow convergence rates, being stuck at local minima, etc. Although various improvements to the basic baking propagation technology have been developed in the literature, none of them has been able to overcome all of their shortcomings. Furthermore, there is currently no rigorous method for selecting a training algorithm specific to a particular problem. In light of these facts, this paper proposes a novel weighted ensemble technique for combining multiple ANN training algorithms. Assigning weights based on the predicted performance of a training algorithm on the validation dataset. In this paper, seven different training algorithms are applied for the combination. Experiments conducted on four real time series suggest that this effect of ANNs can be better combined with other ANGs."}, {"heading": "7. ACKNOWLEDGMENTS", "text": "The first author thanks the German Council for Scientific and Industrial Research (CSIR) for the financial support received, which was very helpful in carrying out this research. 0 2 4 6 8 10 12 14 22.42.62.833.23.43.6N um be ro fly nx tra pp e dARIMA SVM Ensemble Actual (a)"}, {"heading": "8. REFERENCES", "text": "[1] G.E.P. Box, G.M. Jenkins, Time Series Analysis: Forecastingand Control, 3rd ed. Holden-Day, California, 1970. [2] G.P. Zhang, \u2012 Time series predicting using a hybrid ARIMA and neural network model, Mr. Neurocomputing 50, pp.159-175, 2003 [3] G.P. Zhang, \u2012 A neural network ensemble method with jittered training data for time series predicting, J.B. Lettering 177, pp. 5346, 2007. [4] G. Zhang, B.E. Patuwo, M.Y. Hu, \u2012 Forecasting with artificial neural networks: The state of the art, International Journal of Forecasting 14, pp.35-62, 1998. [5] J. Kamruzzaman, R. Begg, R. Sarker, Artificial Neural Networks in Finance and Manufacturing, Idea GroupPublishing, 2006."}], "references": [{"title": "Time Series Analysis: Forecasting and Control", "author": ["G.E.P. Box", "G.M. Jenkins"], "venue": "3 ed. Holden-Day, California", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1970}, {"title": "Time series forecasting using a hybrid ARIMA and neural network model,", "author": ["G.P. Zhang"], "venue": "Neurocomputing 50,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "A neural network ensemble method with jittered training data for time series forecasting,", "author": ["G.P. Zhang"], "venue": "Information Sciences", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Forecasting with artificial neural networks: The state of the art,", "author": ["G. Zhang", "B.E. Patuwo", "M.Y. Hu"], "venue": "International Journal of Forecasting 14,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Artificial Neural Networks in Finance and Manufacturing", "author": ["J. Kamruzzaman", "R. Begg", "R. Sarker"], "venue": "Idea Group Publishing", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "F", "author": ["M. Adya"], "venue": "Collopy, \u2015How effective are neural networks at forecasting and prediction? A review and evaluation,\u2016 Journal of Forecasting 17, pp. 481\u2013495", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "R", "author": ["D.E. Rumelhart", "G.E. Hinton"], "venue": "J. Williams, \u2015Learning representations by back-propagating errors,\u2016 Nature 323 (6188), pp. 533-536", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1986}, {"title": "Training feedforward networks with the marquardt algorithm,", "author": ["M. Hagan", "M. Menhaj"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "H", "author": ["M. Reidmiller"], "venue": "Braun, \"A direct adaptive method for faster backpropagation learning: The rprop algorithm,\" In Proceedings of the IEEE Int. Conference on Neural Networks (ICNN), San Francisco, pp. 586\u2013591", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1993}, {"title": "A scaled conjugate gradient algorithm for fast supervised learning,", "author": ["M.F. Moller"], "venue": "Neural Networks", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1993}, {"title": "One step secant conjugate gradient,", "author": ["R. Battiti"], "venue": "Neural Computation", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1992}, {"title": "Numerical Methods for Unconstrained Optimization and Nonlinear Equations", "author": ["J.E. Dennis", "R.B. Schnabel"], "venue": "Englewood Cliffs, NJ: Prentice-Hall", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1983}, {"title": "Swarm Intelligence", "author": ["J. Kennedy", "R.C. Eberhart", "Y. Shi"], "venue": "Morgan Kaufmann, San Francisco, CA", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "P", "author": ["G.K. Jha"], "venue": "Thulasiraman, R.K. Thulasiram, \u2015PSO based neural network for time series forecasting,\u2016 In Proceedings of the IEEE International Joint Conference on Neural Networks, Atlanta, Georgia, USA, pp. 1422\u20131427 June 14\u2013 19", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Practical Methods of Optimization", "author": ["R. Fletcher"], "venue": "2 ed. John Wiley, Chichester", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1987}, {"title": "Analysis of univariate time series with connectionist nets: a case study of two classical examples,", "author": ["C. de Groot", "D. Wurtz"], "venue": "Neurocomputing", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1991}, {"title": "Combining Forecasts", "author": ["J. Scott Armstrong"], "venue": "Principles of Forecasting: A Handbook for Researchers and Practitioners; J. Scott Armstrong (ed.): Norwell, MA: Kluwer Academic Publishers, 2001.  1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67  0  50  100  150 200 O  b  se  rv  e  d  n  u  m  b  e  r  o  f  su  n  sp  o  ts  ARIMA SVM Ensemble Actual (b) 0  2  4  6  8  10 12  350  400  450  500  550  600  650 N  u  m  b  e  r  o  f  p  a  ss  e  n  g  e  rs  (  '0  0  0  s)  ARIMA SVM Ensemble Actual ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2500}, {"title": "Neural Network Toolbox User's Guide", "author": ["H. Demuth", "M. Beale", "M. Hagan"], "venue": "Natic, MA, the MathWorks", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "The particle swarm optimization algorithm: convergence analysis and parameter selection,", "author": ["I. Trelea"], "venue": "Information Processing Letters", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Time Series Data Library", "author": ["R.J. Hyndman"], "venue": "URL: http://robjhyndman.com/TSDL/, January", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "New York, Springer- Verlag", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Least squares support vector machines classifiers", "author": ["J.A.K. Suykens", "J. Vandewalle"], "venue": "Neural Processing Letters, vol. 9, no. 3, pp. 293\u2013300", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "Dynamic least square support vector machine", "author": ["Y. Fan", "P. Li", "Z. Song"], "venue": "Proceedings of the 6 World Congress on Intelligent Control and Automation (WCICA), Dalian, China, pp. 4886-4889, June 21\u201323", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}, {"title": "PSOt-A Particle Swarm Optimization Toolbox for use with Matlab", "author": ["B. Birge"], "venue": "Proceedings of the IEEE Swarm Intelligence Symposium,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2003}, {"title": "Time Series Modelling of Water Resources and Environmental Systems", "author": ["K.W. Hipel", "A.I. McLeod"], "venue": "Amsterdam, Elsevier", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1994}, {"title": "Improving artificial neural networks performance in seasonal time series forecasting,", "author": ["C. Hamzacebi"], "venue": "Information Sciences", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "In time series forecasting, the historical observations are carefully studied to build up a proper model which is then used to forecast unseen future values [1].", "startOffset": 157, "endOffset": 160}, {"referenceID": 0, "context": "Over the years, various linear and nonlinear forecasting models have been developed in literature [1\u20133].", "startOffset": 98, "endOffset": 103}, {"referenceID": 1, "context": "Over the years, various linear and nonlinear forecasting models have been developed in literature [1\u20133].", "startOffset": 98, "endOffset": 103}, {"referenceID": 2, "context": "Over the years, various linear and nonlinear forecasting models have been developed in literature [1\u20133].", "startOffset": 98, "endOffset": 103}, {"referenceID": 1, "context": "During the last two decades, Artificial Neural Networks (ANNs) have been widely used as attractive and effective alternative tools for time series modeling and forecasting [2,4].", "startOffset": 172, "endOffset": 177}, {"referenceID": 3, "context": "During the last two decades, Artificial Neural Networks (ANNs) have been widely used as attractive and effective alternative tools for time series modeling and forecasting [2,4].", "startOffset": 172, "endOffset": 177}, {"referenceID": 1, "context": "Their most distinguishing feature is the nonlinear, nonparametric, data-driven and selfadaptive nature [2,4,5].", "startOffset": 103, "endOffset": 110}, {"referenceID": 3, "context": "Their most distinguishing feature is the nonlinear, nonparametric, data-driven and selfadaptive nature [2,4,5].", "startOffset": 103, "endOffset": 110}, {"referenceID": 4, "context": "Their most distinguishing feature is the nonlinear, nonparametric, data-driven and selfadaptive nature [2,4,5].", "startOffset": 103, "endOffset": 110}, {"referenceID": 3, "context": "[4], Kamruzzaman et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] and Adya and Collopy [6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[5] and Adya and Collopy [6].", "startOffset": 25, "endOffset": 28}, {"referenceID": 1, "context": "[2,4].", "startOffset": 0, "endOffset": 5}, {"referenceID": 3, "context": "[2,4].", "startOffset": 0, "endOffset": 5}, {"referenceID": 6, "context": "[7] is the bestknown training method.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "negative of the gradient; hence, backpropagation is also known as the gradient steepest descent method [4,5,7].", "startOffset": 103, "endOffset": 110}, {"referenceID": 4, "context": "negative of the gradient; hence, backpropagation is also known as the gradient steepest descent method [4,5,7].", "startOffset": 103, "endOffset": 110}, {"referenceID": 6, "context": "negative of the gradient; hence, backpropagation is also known as the gradient steepest descent method [4,5,7].", "startOffset": 103, "endOffset": 110}, {"referenceID": 3, "context": "Despite its simplicity and popularity, this algorithm suffers from a number of drawbacks, which are listed here [4,5]:", "startOffset": 112, "endOffset": 117}, {"referenceID": 4, "context": "Despite its simplicity and popularity, this algorithm suffers from a number of drawbacks, which are listed here [4,5]:", "startOffset": 112, "endOffset": 117}, {"referenceID": 7, "context": "Some important among them include the Levenberg-Marquardt (LM) [8], Resilient Propagation (RP) [9], Scaled Conjugate Gradient (SCG) [10], One Step Secant (OSS) [11], and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton [12] algorithms.", "startOffset": 63, "endOffset": 66}, {"referenceID": 8, "context": "Some important among them include the Levenberg-Marquardt (LM) [8], Resilient Propagation (RP) [9], Scaled Conjugate Gradient (SCG) [10], One Step Secant (OSS) [11], and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton [12] algorithms.", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "Some important among them include the Levenberg-Marquardt (LM) [8], Resilient Propagation (RP) [9], Scaled Conjugate Gradient (SCG) [10], One Step Secant (OSS) [11], and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton [12] algorithms.", "startOffset": 132, "endOffset": 136}, {"referenceID": 10, "context": "Some important among them include the Levenberg-Marquardt (LM) [8], Resilient Propagation (RP) [9], Scaled Conjugate Gradient (SCG) [10], One Step Secant (OSS) [11], and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton [12] algorithms.", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "Some important among them include the Levenberg-Marquardt (LM) [8], Resilient Propagation (RP) [9], Scaled Conjugate Gradient (SCG) [10], One Step Secant (OSS) [11], and Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton [12] algorithms.", "startOffset": 223, "endOffset": 227}, {"referenceID": 12, "context": "Recently, Particle Swarm Optimization (PSO) [13,14] has also received considerable attentions in this area; e.", "startOffset": 44, "endOffset": 51}, {"referenceID": 13, "context": "Recently, Particle Swarm Optimization (PSO) [13,14] has also received considerable attentions in this area; e.", "startOffset": 44, "endOffset": 51}, {"referenceID": 13, "context": "[14] has effectively used two PSO-based training algorithms (viz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Although, the modified algorithms have improved the performance of backpropagation training on many occasions [15,16], they could not overcome all its drawbacks.", "startOffset": 110, "endOffset": 117}, {"referenceID": 15, "context": "Although, the modified algorithms have improved the performance of backpropagation training on many occasions [15,16], they could not overcome all its drawbacks.", "startOffset": 110, "endOffset": 117}, {"referenceID": 3, "context": "Moreover, there is no straightforward way of selecting the best training algorithm specific to a particular problem [4].", "startOffset": 116, "endOffset": 119}, {"referenceID": 16, "context": "It is a well-known fact that combining forecasts improves the overall accuracy much better than the individual methods [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 1, "context": "ARTIFICIAL NEURAL NETWORKS The most widely used ANNs for time series forecasting are Multilayer Perceptrons (MLPs) [2\u20135].", "startOffset": 115, "endOffset": 120}, {"referenceID": 2, "context": "ARTIFICIAL NEURAL NETWORKS The most widely used ANNs for time series forecasting are Multilayer Perceptrons (MLPs) [2\u20135].", "startOffset": 115, "endOffset": 120}, {"referenceID": 3, "context": "ARTIFICIAL NEURAL NETWORKS The most widely used ANNs for time series forecasting are Multilayer Perceptrons (MLPs) [2\u20135].", "startOffset": 115, "endOffset": 120}, {"referenceID": 4, "context": "ARTIFICIAL NEURAL NETWORKS The most widely used ANNs for time series forecasting are Multilayer Perceptrons (MLPs) [2\u20135].", "startOffset": 115, "endOffset": 120}, {"referenceID": 1, "context": "In practical applications, it is enough to consider a single hidden layer structure [2,4,5].", "startOffset": 84, "endOffset": 91}, {"referenceID": 3, "context": "In practical applications, it is enough to consider a single hidden layer structure [2,4,5].", "startOffset": 84, "endOffset": 91}, {"referenceID": 4, "context": "In practical applications, it is enough to consider a single hidden layer structure [2,4,5].", "startOffset": 84, "endOffset": 91}, {"referenceID": 1, "context": "The output of an MLP with p input and h hidden nodes is expressed as [2,4]:", "startOffset": 69, "endOffset": 74}, {"referenceID": 3, "context": "The output of an MLP with p input and h hidden nodes is expressed as [2,4]:", "startOffset": 69, "endOffset": 74}, {"referenceID": 13, "context": "The model, given by the expression (1) is commonly referred as a (p, h, 1) ANN model [14].", "startOffset": 85, "endOffset": 89}, {"referenceID": 3, "context": "The backpropagation is a supervised training algorithm in which network weights and bias updating is carried out through the minimization of the error function [4,,5]:", "startOffset": 160, "endOffset": 166}, {"referenceID": 4, "context": "The backpropagation is a supervised training algorithm in which network weights and bias updating is carried out through the minimization of the error function [4,,5]:", "startOffset": 160, "endOffset": 166}, {"referenceID": 8, "context": "It considers only the signs of partial derivatives of the error functions to determine the directions of weight changes [9,18].", "startOffset": 120, "endOffset": 126}, {"referenceID": 17, "context": "It considers only the signs of partial derivatives of the error functions to determine the directions of weight changes [9,18].", "startOffset": 120, "endOffset": 126}, {"referenceID": 9, "context": "An efficient method of this kind is the Scaled Conjugate Gradient (SCG) algorithm, developed by Moller [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "However, due to the expensive computational demand, the Hessian matrix is not calculated directly; rather it is assumed to be a function of the gradient which is iteratively approximated [18].", "startOffset": 187, "endOffset": 191}, {"referenceID": 11, "context": "The most successful and widely applied quasi Newton method in literature is the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm [12,18].", "startOffset": 130, "endOffset": 137}, {"referenceID": 17, "context": "The most successful and widely applied quasi Newton method in literature is the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm [12,18].", "startOffset": 130, "endOffset": 137}, {"referenceID": 7, "context": "Two others of this kind, used in the present paper are: the Levenberg-Marquardt (LM) [8] and One Step Secant (OSS) [11] algorithms.", "startOffset": 85, "endOffset": 88}, {"referenceID": 10, "context": "Two others of this kind, used in the present paper are: the Levenberg-Marquardt (LM) [8] and One Step Secant (OSS) [11] algorithms.", "startOffset": 115, "endOffset": 119}, {"referenceID": 17, "context": "In particular, LM is so far the fastest training method in terms of convergence rate; however, it requires enormous amount of storage memory and mathematical computations [18].", "startOffset": 171, "endOffset": 175}, {"referenceID": 12, "context": "4 PSO-Based Training Algorithms PSO is an evolutionary optimization technique which is originally inspired from the intelligent working paradigm in birds flocks and fish schools [13,14].", "startOffset": 178, "endOffset": 185}, {"referenceID": 13, "context": "4 PSO-Based Training Algorithms PSO is an evolutionary optimization technique which is originally inspired from the intelligent working paradigm in birds flocks and fish schools [13,14].", "startOffset": 178, "endOffset": 185}, {"referenceID": 18, "context": "Trelea [19].", "startOffset": 7, "endOffset": 11}, {"referenceID": 13, "context": "In practice, 24 to 30 swarm particles are considered [14,19].", "startOffset": 53, "endOffset": 60}, {"referenceID": 18, "context": "In practice, 24 to 30 swarm particles are considered [14,19].", "startOffset": 53, "endOffset": 60}, {"referenceID": 0, "context": "1 ARIMA Models The ARIMA or Box\u2013Jenkins models [1] are based on the assumption that the observations of a time series are generated from a linear function of the past values and a random noise process [1,2,4].", "startOffset": 47, "endOffset": 50}, {"referenceID": 0, "context": "1 ARIMA Models The ARIMA or Box\u2013Jenkins models [1] are based on the assumption that the observations of a time series are generated from a linear function of the past values and a random noise process [1,2,4].", "startOffset": 201, "endOffset": 208}, {"referenceID": 1, "context": "1 ARIMA Models The ARIMA or Box\u2013Jenkins models [1] are based on the assumption that the observations of a time series are generated from a linear function of the past values and a random noise process [1,2,4].", "startOffset": 201, "endOffset": 208}, {"referenceID": 3, "context": "1 ARIMA Models The ARIMA or Box\u2013Jenkins models [1] are based on the assumption that the observations of a time series are generated from a linear function of the past values and a random noise process [1,2,4].", "startOffset": 201, "endOffset": 208}, {"referenceID": 0, "context": "These are actually a generalization of the Autoregressive Moving Average (ARMA) models [1] to deal with nonstationary time series.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "model building, parameter estimation, and diagnostic checking [1,2].", "startOffset": 62, "endOffset": 67}, {"referenceID": 1, "context": "model building, parameter estimation, and diagnostic checking [1,2].", "startOffset": 62, "endOffset": 67}, {"referenceID": 0, "context": "For seasonal time series forecasting, a variation of the basic ARIMA model, commonly known as the SARIMA(p,d,q)\u00d7(P,D,Q) model (s is the seasonal period) was developed by Box and Jenkins [1], which is also used in this paper.", "startOffset": 186, "endOffset": 189}, {"referenceID": 20, "context": "2 SVM Model SVM is a new statistical learning theory, developed by Vapnik and co-workers at the AT & T Bell laboratories in 1995 [21].", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "It is based on the Structural Risk Minimization (SRM) principle and its aim is to find a decision rule with good generalization ability through selecting some special data points, known as support vectors [21,22].", "startOffset": 205, "endOffset": 212}, {"referenceID": 21, "context": "It is based on the Structural Risk Minimization (SRM) principle and its aim is to find a decision rule with good generalization ability through selecting some special data points, known as support vectors [21,22].", "startOffset": 205, "endOffset": 212}, {"referenceID": 20, "context": "Using the Vapnik\u2019s \u03b5-insensitive loss function [21,22], the SVM regression is converted to a Quadratic Programming Problem (QPP) to minimize the empirical risk:", "startOffset": 47, "endOffset": 54}, {"referenceID": 21, "context": "Using the Vapnik\u2019s \u03b5-insensitive loss function [21,22], the SVM regression is converted to a Quadratic Programming Problem (QPP) to minimize the empirical risk:", "startOffset": 47, "endOffset": 54}, {"referenceID": 21, "context": "Usually, a Radial Basis Function (RBF) kernel, given by K(x, y)=exp(\u2013||x\u2013y|| \u20442\u03c3) (\u03c3 is a tuning parameter) is preferred [22,23].", "startOffset": 121, "endOffset": 128}, {"referenceID": 22, "context": "Usually, a Radial Basis Function (RBF) kernel, given by K(x, y)=exp(\u2013||x\u2013y|| \u20442\u03c3) (\u03c3 is a tuning parameter) is preferred [22,23].", "startOffset": 121, "endOffset": 128}, {"referenceID": 21, "context": "Following other works [22,23], grid search and cross validation techniques are used in this paper for finding optimal SVM parameters.", "startOffset": 22, "endOffset": 29}, {"referenceID": 22, "context": "Following other works [22,23], grid search and cross validation techniques are used in this paper for finding optimal SVM parameters.", "startOffset": 22, "endOffset": 29}, {"referenceID": 19, "context": "All these four datasets are collected from the Time Series Data Library (TSDL) repository [20].", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "[4].", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "The PSO toolbox, developed by Birge [24] is used for implementing PSO-Trelea1 and PSO-Trelea2.", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "Following previous studies [14], the number of swarm particles is chosen from the range of 24 to 30.", "startOffset": 27, "endOffset": 31}, {"referenceID": 1, "context": "Our findings agree with those by other works regarding these two time series [2].", "startOffset": 77, "endOffset": 80}, {"referenceID": 1, "context": "AR(9)) model [2] is used for the sunspot data.", "startOffset": 13, "endOffset": 16}, {"referenceID": 1, "context": "As suggested by Zhang [2], the logarithms (to the base 10) of the lynx data are used in the present analysis.", "startOffset": 22, "endOffset": 25}, {"referenceID": 0, "context": "The airline passenger series has been used by many researchers [1,26] for modeling trend and seasonal effect and now it is considered as a benchmark for seasonal datasets.", "startOffset": 63, "endOffset": 69}, {"referenceID": 25, "context": "The airline passenger series has been used by many researchers [1,26] for modeling trend and seasonal effect and now it is considered as a benchmark for seasonal datasets.", "startOffset": 63, "endOffset": 69}, {"referenceID": 0, "context": "Box and Jenkins were the first to determine that SARIMA(0,1,1)\u00d7(0,1,1) is the best stochastic model for the airline passenger series [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 25, "context": "For ANN modeling of these two series, the Seasonal ANN (SANN) structure, developed by Hamzacebi [26] is considered in this paper.", "startOffset": 96, "endOffset": 100}, {"referenceID": 25, "context": "This model is quite simple to understand and implement, yet very efficient in modeling seasonal data, as shown by the research work [26].", "startOffset": 132, "endOffset": 136}], "year": 2011, "abstractText": "Enhancing the robustness and accuracy of time series forecasting models is an active area of research. Recently, Artificial Neural Networks (ANNs) have found extensive applications in many practical forecasting problems. However, the standard backpropagation ANN training algorithm has some critical issues, e.g. it has a slow convergence rate and often converges to a local minimum, the complex pattern of error surfaces, lack of proper training parameters selection methods, etc. To overcome these drawbacks, various improved training methods have been developed in literature; but, still none of them can be guaranteed as the best for all problems. In this paper, we propose a novel weighted ensemble scheme which intelligently combines multiple training algorithms to increase the ANN forecast accuracies. The weight for each training algorithm is determined from the performance of the corresponding ANN model on the validation dataset. Experimental results on four important time series depicts that our proposed technique reduces the mentioned shortcomings of individual ANN training algorithms to a great extent. Also it achieves significantly better forecast accuracies than two other popular statistical models. General Terms Time Series Forecasting, Artificial Neural Network, Ensemble Technique, Backpropagation.", "creator": "Microsoft\u00ae Office Word 2007"}}}