{"id": "1302.6822", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2013", "title": "A Logic for Default Reasoning About Probabilities", "abstract": "A logic is defined that allows to express information about statistical probabilities and about degrees of belief in specific propositions. By interpreting the two types of probabilities in one common probability space, the semantics given are well suited to model the influence of statistical information on the formation of subjective beliefs. Cross entropy minimization is a key element in these semantics, the use of which is justified by showing that the resulting logic exhibits some very reasonable properties.", "histories": [["v1", "Wed, 27 Feb 2013 14:17:31 GMT  (843kb)", "http://arxiv.org/abs/1302.6822v1", "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)"]], "COMMENTS": "Appears in Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI1994)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["manfred jaeger"], "accepted": false, "id": "1302.6822"}, "pdf": {"name": "1302.6822.pdf", "metadata": {"source": "CRF", "title": "A Logic for Default Reasoning About Probabilities", "authors": ["Manfred Jaeger"], "emails": [], "sections": [{"heading": null, "text": "1 INTRODUCTIONIt has often been pointed out that \"probability\" is a term with a dual purpose: it can be applied to the frequency of occurrence of a certain property in a large sample of objects, and to the degree of belief granted to a presumption. While some have argued that only one of these two aspects in terms grasps the true meaning of probability [Jay78], others have attempted to analyze both areas of application of the term in their own right and to clarify the relationship between the two aspects of probability. Carnap was one of the first to do so ([Car50]). Although his primary interest is in probabilities as subjective beliefs (or \"degree of confirmation\"), he also formulates direct (inductive) conclusions as a prerequisite to arrive at subjective beliefs based on given relative frequencies: if it is known that objects from a class cl are also members of a class c2 with a frequency, and a specific object can then be used as a long-lived object."}], "references": [{"title": "Representing and Reasoning With Probabilistic Knowledge", "author": ["F. Bacchus"], "venue": "In Proc. National Conference on Ar\u00ad", "citeRegEx": "Bacchus.,? \\Q1990\\E", "shortCiteRegEx": "Bacchus.", "year": 1990}, {"title": "Statistical foundations for de\u00ad fault reasoning", "author": ["F. Bacchus", "A. Grove", "J.Y. Halpern", "D. Koller"], "venue": "Proc. of International Joint Conference on Artificial Intelligence (IJCAI-93)", "citeRegEx": "BGHK93", "shortCiteRegEx": null, "year": 1993}, {"title": "Logical Foundations of Prob\u00ad ability", "author": ["R. Carnap"], "venue": "The University of Chicago Press", "citeRegEx": "Car50", "shortCiteRegEx": null, "year": 1950}, {"title": "-divergence geometry of proba\u00ad bility distributions and minimization prob\u00ad lems", "author": ["I. Csiszar"], "venue": "Annals of Probability,", "citeRegEx": "Csiszar.,? \\Q1975\\E", "shortCiteRegEx": "Csiszar.", "year": 1975}, {"title": "Asymptotic conditional probabilities for first-order logic", "author": ["A.J. Grove", "J.Y. Halpern", "D. Koller"], "venue": "Proc. 24th ACM Symp. on Theory of Computing", "citeRegEx": "GHK92a", "shortCiteRegEx": null, "year": 1992}, {"title": "Random worlds and maximum entropy", "author": ["A.J. Grove", "J.Y. Halpern", "D. Koller"], "venue": "Proc. 7th IEEE Symp. on Logic in Com\u00ad puter Science", "citeRegEx": "GHK92b", "shortCiteRegEx": null, "year": 1992}, {"title": "Artificial Intelligence", "author": ["J .Y. Halpern. An analysis of first-order logics of probability"], "venue": "46:311-350,", "citeRegEx": "Hal90", "shortCiteRegEx": null, "year": 1990}, {"title": "Jaynes", "author": ["E.T"], "venue": "Where do we stand on maximum entropy? In R.D. Levine and M. Tribus, editors, The Maximum Entropy Formalism, pages 15-118. MIT Press,", "citeRegEx": "Jay78", "shortCiteRegEx": null, "year": 1978}, {"title": "The Logic of Decision", "author": ["R.C. Jeffrey"], "venue": "McGraw-Hill", "citeRegEx": "Jef65", "shortCiteRegEx": null, "year": 1965}, {"title": "Probability quantifiers", "author": ["H.J. Keisler"], "venue": "J. Barwise and S. Feferman, editors, Model- Theoretic Logics, pages 509-556. Springer-Verlag", "citeRegEx": "Kei85", "shortCiteRegEx": null, "year": 1985}, {"title": "Information Theory and Statistics", "author": ["S. Kullback"], "venue": "Wiley", "citeRegEx": "Kul59", "shortCiteRegEx": null, "year": 1959}, {"title": "Circumscription - a form of non-monotonic reasoning", "author": ["J. McCarthy"], "venue": "Artificial Intel\u00ad ligence, 13:27-39", "citeRegEx": "McC80", "shortCiteRegEx": null, "year": 1980}, {"title": "On the ap\u00ad plicability of maximum entropy to inexact reasoning", "author": ["J.B. Paris", "A. Vencovska"], "venue": "International Journal of Ap\u00ad proximate Reasoning, 3:1-34", "citeRegEx": "PV89", "shortCiteRegEx": null, "year": 1989}, {"title": "A method for updating that justifies minimum cross entropy", "author": ["J .B. Paris", "A. Vencovska"], "venue": "International Journal of Approx\u00ad imate Reasoning,", "citeRegEx": "Paris and Vencovska.,? \\Q1992\\E", "shortCiteRegEx": "Paris and Vencovska.", "year": 1992}, {"title": "A logic for default reasoning", "author": ["R. Reiter"], "venue": "Artificial Intelligence,", "citeRegEx": "Reiter.,? \\Q1980\\E", "shortCiteRegEx": "Reiter.", "year": 1980}, {"title": "Relative entropy, probabilistic inference, and ai", "author": ["J .E. Shore"], "venue": "Uncertainty in Artificial Intelligence. Elsevier,", "citeRegEx": "Shore.,? \\Q1986\\E", "shortCiteRegEx": "Shore.", "year": 1986}, {"title": "Axiomatic derivation of the principle of maximum en\u00ad tropy and the principle of minimum cross\u00ad entropy", "author": ["J.E. Shore", "R.W. Johnson"], "venue": "IEEE Transactions on Informa\u00ad tion Theory,", "citeRegEx": "Shore and Johnson.,? \\Q1980\\E", "shortCiteRegEx": "Shore and Johnson.", "year": 1980}, {"title": "Proper\u00ad ties of cross-entropy minimization", "author": ["J.E. Shore", "R.W. Johnson"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Shore and Johnson.,? \\Q1981\\E", "shortCiteRegEx": "Shore and Johnson.", "year": 1981}], "referenceMentions": [{"referenceID": 7, "context": "While some have argued that only one of these two in\u00ad terpretations captures the true meaning of probability [Jay78], others have tried to analyze both usages of the term in their own right, and to clarify the relationship between the two aspects of probability.", "startOffset": 109, "endOffset": 116}, {"referenceID": 2, "context": "Carnap was among the first to do this ([Car50]).", "startOffset": 39, "endOffset": 46}, {"referenceID": 8, "context": "When, instead of firmly believing that a is an element of C1, one only has several conflicting pieces of evi\u00ad dence about the true nature of a, these can be com\u00ad bined to form a degree of belief for a being in c2 by using Jeffrey's rule [Jef65], as illustrated in the follow\u00ad ing example.", "startOffset": 237, "endOffset": 244}, {"referenceID": 11, "context": "[McC80], [Rei80]), it shares the nonmonotinicity of these logics: in the light of additional (probabilistic or definite) informa\u00ad tion, earlier inferences may be retracted.", "startOffset": 0, "endOffset": 7}, {"referenceID": 4, "context": "An additional strategy to arrive at subjective be\u00ad liefs on the basis of statistical information is devel\u00ad oped in [Bac91], [GHK92a], [GHK92b], [BGHK92], and [BGHK93].", "startOffset": 124, "endOffset": 132}, {"referenceID": 5, "context": "An additional strategy to arrive at subjective be\u00ad liefs on the basis of statistical information is devel\u00ad oped in [Bac91], [GHK92a], [GHK92b], [BGHK92], and [BGHK93].", "startOffset": 134, "endOffset": 142}, {"referenceID": 1, "context": "An additional strategy to arrive at subjective be\u00ad liefs on the basis of statistical information is devel\u00ad oped in [Bac91], [GHK92a], [GHK92b], [BGHK92], and [BGHK93].", "startOffset": 158, "endOffset": 166}, {"referenceID": 6, "context": "\u2022 The expressive power of the language used is smaller than in [Hal90], [Bac90].", "startOffset": 63, "endOffset": 70}, {"referenceID": 9, "context": "1 is standard and can be found similarly in [Kei85], [Hal90], [Bac90].", "startOffset": 44, "endOffset": 51}, {"referenceID": 6, "context": "1 is standard and can be found similarly in [Kei85], [Hal90], [Bac90].", "startOffset": 53, "endOffset": 60}, {"referenceID": 6, "context": "2 differs from its counterparts in [Hal90] and [Bac90] in that prob(\u00a2 I 1/J) 2:: p is seen as a statement about a distinguished subset of the constant symbols appearing in \u00a2 and '1/J, and \u00a2, 'ljJ are not allowed to contain, in turn, a formula of the form prob(\u00a2' I '1/J') 2:: p' .", "startOffset": 35, "endOffset": 42}, {"referenceID": 9, "context": "A semantical structure in which \ufffdo- can be interpreted is defined along the same lines as in [Kei85], [Hal90], [Bac90]:", "startOffset": 93, "endOffset": 100}, {"referenceID": 6, "context": "A semantical structure in which \ufffdo- can be interpreted is defined along the same lines as in [Kei85], [Hal90], [Bac90]:", "startOffset": 102, "endOffset": 109}, {"referenceID": 10, "context": "Cross entropy ([Kul59]) commonly is interpreted as a \"measure of information dissimilarity\" for two proba\u00ad bility measures [Sho86].", "startOffset": 15, "endOffset": 22}, {"referenceID": 12, "context": "In [PV89] and [PV92] Paris and Vencovska consider basically the same inference problem as is discussed in the present paper.", "startOffset": 3, "endOffset": 9}], "year": 2011, "abstractText": "A logic is defined that allows to express in\u00ad formation about statistical probabilities and about degrees of belief in specific proposi\u00ad tions. By interpreting the two types of proba\u00ad bilities in one common probability space, the semantics given are well suited to model the influence of statistical information on the for\u00ad mation of subjective beliefs. Cross entropy minimization is a key element in these se\u00ad mantics, the use of which is justified by show\u00ad ing that the resulting logic exhibits some very reasonable properties.", "creator": "pdftk 1.41 - www.pdftk.com"}}}