{"id": "1302.6927", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Feb-2013", "title": "Online Learning for Time Series Prediction", "abstract": "In this paper we address the problem of predicting a time series using the ARMA (autoregressive moving average) model, under minimal assumptions on the noise terms. Using regret minimization techniques, we develop effective online learning algorithms for the prediction problem, without assuming that the noise terms are Gaussian, identically distributed or even independent. Furthermore, we show that our algorithm's performances asymptotically approaches the performance of the best ARMA model in hindsight.", "histories": [["v1", "Wed, 27 Feb 2013 17:14:14 GMT  (124kb)", "http://arxiv.org/abs/1302.6927v1", "17 pages, 6 figures"]], "COMMENTS": "17 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["oren anava", "elad hazan", "shie mannor", "ohad shamir"], "accepted": false, "id": "1302.6927"}, "pdf": {"name": "1302.6927.pdf", "metadata": {"source": "CRF", "title": "Online Learning for Time Series Prediction", "authors": ["Oren Anava", "Elad Hazan", "Shie Mannor", "Ohad Shamir"], "emails": ["soanava@tx.technion.ac.il", "ehazan@ie.technion.ac.il", "shie@ee.technion.ac.il", "ohadsh@microsoft.com"], "sections": [{"heading": null, "text": "ar Xiv: 130 2.69 27v1 [cs.LG] 2 7Fe b"}, {"heading": "1 Introduction", "text": "Auto-Regressive (AR), Moving Averages (MA) and Auto-Regressive Moving Averages (ARMA) are often used for the modeling, analysis and prediction of time series. These models have been successfully used in a wide range of applications such as voice analysis, noise suppression and stock market analysis ([Ham94, BJR94, SS05, BD09]). Broadly speaking, they are based on the assumption that each new signal is a noisy linear combination of the final signals and independent noise terminals. Much work has been done in parameter identification and signal prediction with these models, mainly in the environment of \"proper learning,\" in which the adapted model attempts to mimic the assumed underlying model. Most of this work was based on strong assumptions regarding noise conditions, such as independence and identical Gaussian distribution. These assumptions are generally quite strict, but the following statement from [Tho.94] is sometimes quoted with the assumption that the functions are both real and beneficial to the world."}, {"heading": "1.1 Summary of results", "text": "We present and analyze two online algorithms for the prediction problem, one for general convex loss functions and the other for exp-concave. Each of these algorithms achieves sublinear regret, which is retrospectively tied to the best ARMA prediction, under weak noise assumptions. We apply our results to the most commonly used loss function in time series analysis, quadratic loss, and achieve a remorse limit of O (log2 (T)) over the best ARMA prediction after the event. Finally, we present an empirical study that confirms our theoretical results."}, {"heading": "1.2 Related work", "text": "In the standard time series analysis, the square loss is usually taken into account and the noise condition is assumed to be independent with limited variance and zero mean. In this particular environment, it can be assumed without loss of generality that the noise condition has an identical Gaussian distribution (see [Ham94, BJR94, BD09] for more information).This allows the use of statistical methods such as minimum squares and maximum probability calculation for the tasks of analysis and prediction. However, when different loss functions are taken into account, these assumptions do not generally apply, and the above methods are not applicable.We are not aware of any previous approach that attempts to loosen these assumptions for general convex loss functions. We note that there has been previous work that attempted to loosen such assumptions for square loss, usually under additional modelling assumptions such as the t distribution of noise (e.g. [DES89, TWB00])."}, {"heading": "2 Preliminaries and model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Time series modelling", "text": "A time series is a sequence of signals measured at consecutive times, which are assumed to be delimited at even distances from each other. We call the signal measured at a time t Xt and the noise date at a time. The AR (k) model (short for autoregressive), which is parameterized by a horizon k and a coefficient vector \u03b1-Rk, assumes that the time series is generated according to the following model, where the noise date is 0 mean: Xt = k \u2211 i = 1\u03b1iXt \u2212 i +. (1) In other words, the model assumes that each Xt is a noisy linear combination of the previous k signals. A more complicated model is the ARMA (k, q) model (short for autoregressive moving averages), which is parameterized by two horizon terms k, q and coefficient vectors \u03b1-Rk and \u03b2-Rq."}, {"heading": "2.2 The online setting for ARMA prediction", "text": "Online learning is usually defined in a game theory framework, in which the data, instead of being stochastically selected, is arbitrarily selected (\u03b2 = \u03b2 = 1), possibly by an almighty opponent with full knowledge of our learning algorithm (see, for example, [CBL06]). In our context, we will describe the setting as follows: First, some coefficient vectors (\u03b2, \u03b2) are set by the opponent. At any time, the opponent selects t \u2212 t for the signal and generates the resulting signal Xt using the formula in Equation 2. We emphasize that (\u03b2, \u03b2) and the noise terms are never disclosed to us.When iterating t, we have to make a prediction X \u2212 t for the signal after which the real signal Xt is detected, and we suffer a loss, which is indicated by the prediction t (Xt, X, X). Our goal is to minimize the sum of losses over a predefined number of iterations T, which is not to attempt to be a reasonable benchmark, which is not much worse than the ART."}, {"heading": "2.3 Our assumptions", "text": "In Section 3, we start from the following assumptions: 1. Noise concepts are generated stochastically and independently of each other, each from a zero-mean distribution that could be chosen contrary (up to the following assumptions); in Section 4, we show how this assumption can be loosened to adversarial noise; and we assume that the E [Mmax < Mmax < and E [\u0445t (Xt, Xt \u2212 \u0445 t)] < \u221e is for everyone; 2. The loss function is not Lipshitz continuous L > 0 for some Lipshitz constants; this is a standard assumption and applies in particular to square loss and other convex loss functions with a compact domain. 3. The coefficients are satisfactory."}, {"heading": "3 Online time series prediction", "text": "As already mentioned, we cannot use the existing convex optimization algorithms in the range of coefficient vectors (\u03b1, \u03b2), because we are not aware of the noise terms at any point in time. Instead, we use an improper learning approach, in which our predictions do not at any point in time start from an ARMA model that attempts to imitate the underlying model. Specifically, we fix some m + N, and at any time we choose a (m + k) -dimensional coefficient vector determined by X-t (g) = \u2211 m + k i = 1 \u03b3iXt \u2212 i. As a result, our loss in iteration t is determined by the loss function mt (\u03b3 t) = \u0445t (Xt, X-t) = \u0445t (Xt) (m + k). This leads to one of our main results: we can determine this ARM model (MA-k value) using 3, m (MA value) = horizon (k), which can be correctly regarded as a model with AR + 5."}, {"heading": "3.1 Algorithm parameters definition and calculation", "text": "Before introducing the algorithm and setting out our main theorem, we must define the following parameters: The decision law K is the set of candidates ((m + k) -dimensional coefficient vectors from which we can choose for each iteration; it is defined as K = {\u03b3-Rm + k, | \u03b3j | \u2264 1, j = 1,..., m}. Intuitively, the structure of K is derived from assumptions 3-4 to \u03b1 and \u03b2, which limit our improper learning variable. We use D to denote the diameter of K and the limit: D = sup \u03b31, \u03b32 \u04411 \u2212 \u03b32 \u0445 2 = \u221a 2 (m + k)."}, {"heading": "3.2 ARMA Online Newton Step (ARMA-ONS)", "text": "The notation \"AtK\" refers to the projection on K in the norm, that of At, i.e., \"AtK\" (y) = argminx \"K\" (y \u2212 x). The notation \"AtK\" refers to the projection on K in the norm, that of At, i.e., \"AtK\" (y) = argminx \"K\" (y \u2212 x). The notation \"AtK\" (k, q).3: Input: ARMA order k, q; learning rate (m + k) \u00b7 (m + k) \u00b7 (m + k) matrix A0. 2: Set m = q \"log1\" (TLMmax) \u2212 1. \"4: for\" K. \"4: for t = 1 to (T \u2212 1) we do 5: Predict X\" t \"(t) = 1.\" 6: Observe Xt \"and we suffer.\""}, {"heading": "3.3 ARMA Online Gradient Descent (ARMA-OGD)", "text": "We now turn to a different algorithm for selecting \u03b3t at any time. This algorithm is applicable to general convex loss functions, as well as to exp-concave loss functions. It is mathematically simpler, but has a slightly inferior theoretical (and empirical) performance compared to the previous one, if one considers an exp-concave loss function. The notation \u0394K refers to the euclidean projection on K, i.e., it becomes an Argminx-K (y) = x-x-2. Algorithm 2 ARMA-OGD (k, q) 1: Input: ARMA order k, q. learning rate. 2: Set m = q \u00b7 log1 \u2212 \u03b5 ((((TLMmax) \u2212 1) 3: Choose 1-K \u2212 x-2."}, {"heading": "4 Additional results", "text": "In this section we present an analysis for the case where noise concepts may be contractive, as well as an application of theorem 3.1 for quadratic losses."}, {"heading": "4.1 Adversarial noise", "text": "The results presented in theorems 3.1 and 3.5 are based on the assumption that the noise concepts are independent and not mediocre, and among these assumptions, the best coefficient vectors in retrospect are those that generated the signal. However, if we allow the noise concepts to be generated in the opposite direction (the opponent chooses the coefficient vector that generated the signal), the best coefficient vectors in retrospect are not necessarily those that were used to generate the signal. In this case, we have theorem 4.1. Name (\u03b1 \u2032, \u03b2 \u2032) the coefficient vectors that generated the signal, and assume that {Xt} Tt = 1 fulfills assumptions 2-5 from section 2.3, if the noise concepts can be chosen contrary."}, {"heading": "4.2 Application of Theorem 3.1 to squared loss", "text": "As already mentioned, the quadratic loss is the most commonly used loss function in time series analysis. It is defined as \u0435t (Xt, X-t) = (Xt-X-t) 2 for the prediction X-t and the signal Xt. In our case, the predictions are derived from an AR model with horizon (m + k), and therefore the substitution of the values of G, D and \u03bb as defined and calculated in Section 3.1 for the quadratic loss results in the following result: T-t = 1\u0445 mt (\u03b3 t) \u2212 min\u03b1, \u03b2T-t = 1E [ft (\u03b1, \u03b2)] = O (k log (T) + q log2 (T)). (12) This result implies that the average loss suffered by algorithm 1 is asymptotically equivalent to the average loss most affected by the SIMA deviation."}, {"heading": "5 Experiments", "text": "The following experiments show the predictive effectiveness of the proposed algorithms under some different settings. We compare the performance with the ARMA-RLS algorithm presented in [DSC06]. In a few words: The ARMA-RLS is a \"real learning algorithm\" - it tries to mimic the underlying model. It estimates the noises using a recursive method based on the smallest squares and fulfills a prediction with these estimates and the preceding signals. ARMA-RLS does not assume standard noise or ergodicity. We also name the standard Yule-Walker estimation method 3. The results are shown in the figures below. In all cases, the x-axis is the time (number of samples) and the y-axis is the average square loss."}, {"heading": "5.1 Experiments with artificial data", "text": "In all experimental settings, we have averaged the results over 20 runs for stability. We also select the order of our AR prediction to m + \u03b2 = 10 in all settings.Setting 1. We started with a simple health check using noise. We generated a stationary ARMA process using the coefficient vectors \u03b1 = [0.6, \u2212 0.5, \u2212 0.4, \u2212 0.3] and \u03b2 \u2212 \u2212 \u2212 \u2212 0.2] when the noise terms are uncorrelated and normally distributed as N (0, 0.32). Note that since predicting noise is impossible, a perfect predictor will experience an average error rate of at least the deviation of noise - 0.09 in this setting. As in Figure 1 (a), the ARMA ONS algorithm performs the other online algorithms quickly due to its lower regret in this setting of exp-concave loss functions and converges with performance."}, {"heading": "5.2 Experiments with real data", "text": "In this section, we present some preliminary results on real data time series and show that even for such data, our online learning approach is reasonably effective compared to existing approaches. For reasons of robustness, we look at time series from different fields. The first time series comes from the field of weather research. Each data point in this time series is the monthly average temperature of the sea surface measured at a certain point. Data comes from the website of the Global Climate Observing System (GCOS). However, since we are dealing with a weather-related time series, and given the monthly average temperature, it is quite reasonable for the time series to follow a certain pattern. As shown in Figure 2 (a), this pattern can be well learned from the ARMA model of all four algorithms. However, the results in Figure 2 (b) clearly show the superiority of online algorithms. The second time series comes from the financial sector. Each data point in this time series is the daily yield of the S & P 500 index."}, {"heading": "6 Conclusion and discussion", "text": "In this paper, we developed a new approach to time series analysis - an online learning approach. Our main finding in this paper is that it is possible to predict time series and the best ARMA model, regardless of the loss function under consideration, under weak assumptions about the noise condition - the zero medium distribution. However, this result is reinforced by the fact that the noise conditions in the underlying model are unknown to us at all times. We overcome this difficulty by using improper learning techniques. In addition, we present an analytical extension of our approach to contradictory noise-related terms. The main strong characteristics of the online approach, as shown in our work, are universality, simplicity and efficiency compared to existing methods. There are three questions that still need to be clarified for further research: First, we assume that q i = 1 | < 1 \u2212 \u03b5 for some of these online approaches that seem to limit the freedom of cost-efficiency."}], "references": [{"title": "Time Series Analysis: Forecasting and Control", "author": ["G. Box", "G. Jenkins", "G. Reinsel"], "venue": "PrenticeHall, 3 edition,", "citeRegEx": "Box et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Box et al\\.", "year": 1994}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "El-Shaarawi. ARMA models with double-exponentially distributed noise", "author": ["A.H.E. Damsleth"], "venue": "Journal of the Royal Statistical Society. Series B,", "citeRegEx": "Damsleth,? \\Q1989\\E", "shortCiteRegEx": "Damsleth", "year": 1989}, {"title": "Performance analysis of estimation algorithms of nonstationary ARMA processes", "author": ["F. Ding", "Y. Shi", "T. Chen"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Ding et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ding et al\\.", "year": 2006}, {"title": "Autoregressive conditional heteroscedasticity with estimates of the variance of united kingdom", "author": ["R.F. Engle"], "venue": "inflation. Econometrica,", "citeRegEx": "Engle.,? \\Q1982\\E", "shortCiteRegEx": "Engle.", "year": 1982}, {"title": "Logarithmic regret algorithms for online convex optimization", "author": ["E. Hazan", "A. Agarwal", "S. Kale"], "venue": "Machine Learning,", "citeRegEx": "Hazan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hazan et al\\.", "year": 2007}, {"title": "Time Series Analysis", "author": ["J. Hamilton"], "venue": null, "citeRegEx": "Hamilton.,? \\Q1994\\E", "shortCiteRegEx": "Hamilton.", "year": 1994}, {"title": "Efficient learning algorithms for changing environments", "author": ["E. Hazan", "C. Seshadhri"], "venue": "In ICML, page", "citeRegEx": "Hazan and Seshadhri.,? \\Q2009\\E", "shortCiteRegEx": "Hazan and Seshadhri.", "year": 2009}, {"title": "Time Series Analysis and Its Applications", "author": ["R. Shumway", "D. Stoffer"], "venue": null, "citeRegEx": "Shumway and Stoffer.,? \\Q2005\\E", "shortCiteRegEx": "Shumway and Stoffer.", "year": 2005}, {"title": "Jackknifing multiple-window spectra", "author": ["D. Thomson"], "venue": "In ICASSP,", "citeRegEx": "Thomson.,? \\Q1994\\E", "shortCiteRegEx": "Thomson.", "year": 1994}, {"title": "Time series models in non-normal situations: Symmetric innovations", "author": ["M. Tiku", "W.K. Wong", "D. Vaughan", "G. Bian"], "venue": "Journal of Time Series Analysis,", "citeRegEx": "Tiku et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tiku et al\\.", "year": 2000}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["M. Zinkevich"], "venue": "In ICML,", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [], "year": 2013, "abstractText": "In this paper we address the problem of predicting a time series using the ARMA (autoregressive moving average) model, under minimal assumptions on the noise terms. Using regret minimization techniques, we develop effective online learning algorithms for the prediction problem, without assuming that the noise terms are Gaussian, identically distributed or even independent. Furthermore, we show that our algorithm\u2019s performances asymptotically approaches the performance of the best ARMA model in hindsight.", "creator": "LaTeX with hyperref package"}}}