{"id": "1302.7263", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Feb-2013", "title": "Online Similarity Prediction of Networked Data from Known and Unknown Graphs", "abstract": "We consider online similarity prediction problems over networked data. We begin by relating this task to the more standard class prediction problem, showing that, given an arbitrary algorithm for class prediction, we can construct an algorithm for similarity prediction with \"nearly\" the same mistake bound, and vice versa. After noticing that this general construction is computationally infeasible, we target our study to {\\em feasible} similarity prediction algorithms on networked data. We initially assume that the network structure is {\\em known} to the learner. Here we observe that Matrix Winnow \\citep{w07} has a near-optimal mistake guarantee, at the price of cubic prediction time per round. This motivates our effort for an efficient implementation of a Perceptron algorithm with a weaker mistake guarantee but with only poly-logarithmic prediction time. Our focus then turns to the challenging case of networks whose structure is initially {\\em unknown} to the learner. In this novel setting, where the network structure is only incrementally revealed, we obtain a mistake-bounded algorithm with a quadratic prediction time per round.", "histories": [["v1", "Thu, 28 Feb 2013 17:15:55 GMT  (2207kb,D)", "https://arxiv.org/abs/1302.7263v1", null], ["v2", "Fri, 1 Mar 2013 16:57:09 GMT  (2206kb,D)", "http://arxiv.org/abs/1302.7263v2", null], ["v3", "Fri, 15 Mar 2013 12:52:33 GMT  (1919kb,D)", "http://arxiv.org/abs/1302.7263v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["claudio gentile", "mark herbster", "stephen pasteris"], "accepted": false, "id": "1302.7263"}, "pdf": {"name": "1302.7263.pdf", "metadata": {"source": "CRF", "title": "Online Similarity Prediction of Networked Data from Known and Unknown Graphs", "authors": ["Claudio Gentile"], "emails": ["claudio.gentile@uninsubria.it", "m.herbster@cs.ucl.ac.uk", "s.pasteris@cs.ucl.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Online class and similarity prediction", "text": "In this section, we examine the agreement in the predictive power (error limits) between the classification and similarity prediction models. Preliminaries. The set of all finite sequences from a sentence X is called \"X.\" We use the Iverson bracket notation [predicate] = 1 if the predicate is true and [predicate] = 0 if false. In the K class prediction in the online error-bound model, an example sequence (x1, y1) is used (xT, yT) when the predicate \"X\" * is revealed incrementally, where X is a set of patterns and Y: = {1,.., K} is the set of K class designations. The goal in the t-th study is to predict the class yt against the previous t \u2212 1 pattern / label pairs and xt. \"The general goal of an algorithm is to minimize the number of its predictions."}, {"heading": "3 Class and similarity prediction on graphs", "text": "Let us now imagine the specific representation of the graph. Let us then G = (V, E) undirected and connected graphics with n = # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "3.1 Class prediction on graphs", "text": "Roughly speaking, algorithms and boundaries for sequential class predictions on graphs that are divided between two types: those that approach the original graph with a tree or those that maintain the original graph. By approaching the graph with a tree, extremely efficient algorithms with strong guarantees of optimality are achieved. By exploiting the complete graph, algorithms are obtained that take advantage of connectivity to achieve sharp boundaries when the graph contains, for example, dense clusters. Relevant literature on this subject includes [27, 25, 28, 29, 24, 12]. Known representatives of the first type are boundaries of the form O (1 + log n | T |) or the form O (logDT) [11], in which we have a tree of G and DT."}, {"heading": "3.2 Similarity prediction on graphs", "text": "In algorithm 1, we give a simple application of the matrix Winnow (superscript \"w\") and Perceptron (superscript \"p\") algorithms for predicting similarity on graphs. \u2212 The key aspect of construction (common to many methods of metric learning) is the creation of matrices that correspond to the similarity of \"instances\" (see (3). \u2212 The key aspect of construction (typical of many methods of metric learning) is the creation of matrices that correspond to the similarity of \"instances\" (see (3). A key observation is that the quared Frobenius standard of the (un-normalized) instance matrices is limited by the quared resistance diameter of the graph, and the squared Frobenius standard of the (un-normalized) \"comparator\" Winceptrix \"is that the quared Frobenius standard is limited by the (un-normalized) instance of the normalized (un-normalized) instance of the normed (un-normed) instance."}, {"heading": "4 Efficient similarity prediction on graphs", "text": "We present adjustments of the matrix Winnow and Matrix Perceptron to the case when the original graph G is replaced by a linearized and rebalanced random voltage limit of G. This sparsification technology, called the binary support tree (BST), brings the double advantage of an improved error limit and faster prediction algorithms. More specifically, the use of a BST replaces the (perhaps very large) resistance term max (i, j).V 2 R G i, j in the error limits of Proposition 2 by a logarithmic term, the other term in error avoidance (if we deal with the expected number of errors) only a logarithmic factor greater than the (often much smaller) sum of resistance variables in an spanning tree."}, {"heading": "4.1 Implementing Matrix Perceptron on BST", "text": "The algorithm works on BST B by first setting a (2n \u2212 1) \u00b7 (2n \u2212 1) symmetrical matrix F with integer input to zero. In due course, when we get the leaf pair (it, jt), 3 we assume that n = | V | has a power of 2. Otherwise, we can add dummy \"leaves.\" The algorithm constructs Pt, the (unique) path in B that connects it to jt. Then the prediction y, jt, (0, 1} is calculated so that it is easy to jt = {1 if \u2211, \"\" (4) After receiving the label yit, jt, the algorithm F updates as follows. (First, the algorithm is driven incorrectly, so an update only takes place when yit, jt 6 = y, jt. \""}, {"heading": "5 The Unknown Graph Case", "text": "We consider the case when the graph G = (V, E) is previously unknown to the learner. Therefore, the graph structure is revealed incrementally as more and more pairs (es, jt) produced by the opponent. A reasonable online protocol that includes progressive graph disclosure is as follows: At the beginning of the round t = 1 the learner does not know anything about G, but the number of nodes n - the prior knowledge of n makes the presentation easier, but could easily be removed from this setting. In the general round t, the opponent introduces both pairs to the learner (es, jt).V \u00b7 V and a path within G to jt from him. The learner is then forced to predict whether the two nodes are similar or not. Notice that although the path presented may have intersections, there may be alternative paths in G that connect the two nodes without intersections. The learner does not have to see them."}, {"heading": "5.1 Algorithm and analysis", "text": "Algorithm 2 contains the pseudo-code of our algorithm. If interpreted as being based on vectors, the algorithm is simply an r-norm perceptron algorithm [20, 17] with a non-zero threshold and a threshold of 2 log (n-1) 2 = 4 log (n-1), where (n-1) 2 is the length of the vectors maintained throughout the world, and the long vector vc (Xt) from the ranking one matrix Xt > t, where vec is the default vectorisation of one matrix. To construct the text from the other, the algorithm maintains a forest where vec (\u00b7) is the default vectorisation of a matrix stacking its columns."}, {"heading": "A Proofs", "text": "This appendix contains all the excluded proofs. Notation is as in main text. A.1 Missing proofs from Section 2The series of sample sequences compatible with a concept f for class prediction is denoted by Sc (f): = ({(x, f (x)))} x (x)) x (x)) x (x) x (x) x (x) x (x) x (x) x (x) x (x) x) x (x) x (x) x (x) x) x (x) x (x) x). A prediction algorithm is a figure A: (X) x (Y) * \u2192 YX from sample prediction sequences. So if A is a prediction function and S = (x1, y1). We. \"We\" is an example sequence, then the online prediction error area MA (S): = 1 (S)."}, {"heading": "A digression on cuts and directed paths", "text": "rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrln rf\u00fc ide eeisrrrrteeeVrrrrrrrrrrrrrteeeeeeeeeeeeeeeVrrrrrrrrrrrrrrrrrteeeeeeeeeeeeeeeeeeeeeu in rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "<lb>We consider online similarity prediction problems over networked data. We begin by relat-<lb>ing this task to the more standard class prediction problem, showing that, given an arbitrary<lb>algorithm for class prediction, we can construct an algorithm for similarity prediction with<lb>\u201cnearly\u201d the same mistake bound, and vice versa. After noticing that this general construction<lb>is computationally infeasible, we target our study to feasible similarity prediction algorithms on<lb>networked data. We initially assume that the network structure is known to the learner. Here<lb>we observe that Matrix Winnow [47] has a near-optimal mistake guarantee, at the price of cubic<lb>prediction time per round. This motivates our effort for an efficient implementation of a Percep-<lb>tron algorithm with a weaker mistake guarantee but with only poly-logarithmic prediction time.<lb>Our focus then turns to the challenging case of networks whose structure is initially unknown<lb>to the learner. In this novel setting, where the network structure is only incrementally revealed,<lb>we obtain a mistake-bounded algorithm with a quadratic prediction time per round.", "creator": "LaTeX with hyperref package"}}}