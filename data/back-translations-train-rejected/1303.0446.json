{"id": "1303.0446", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Mar-2013", "title": "Statistical sentiment analysis performance in Opinum", "abstract": "The classification of opinion texts in positive and negative is becoming a subject of great interest in sentiment analysis. The existence of many labeled opinions motivates the use of statistical and machine-learning methods. First-order statistics have proven to be very limited in this field. The Opinum approach is based on the order of the words without using any syntactic and semantic information. It consists of building one probabilistic model for the positive and another one for the negative opinions. Then the test opinions are compared to both models and a decision and confidence measure are calculated. In order to reduce the complexity of the training corpus we first lemmatize the texts and we replace most named-entities with wildcards. Opinum presents an accuracy above 81% for Spanish opinions in the financial products domain. In this work we discuss which are the most important factors that have impact on the classification performance.", "histories": [["v1", "Sun, 3 Mar 2013 01:38:03 GMT  (678kb)", "http://arxiv.org/abs/1303.0446v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["boyan bonev", "gema ram\\'irez-s\\'anchez", "sergio ortiz rojas"], "accepted": false, "id": "1303.0446"}, "pdf": {"name": "1303.0446.pdf", "metadata": {"source": "CRF", "title": "Statistical sentiment analysis performance in Opinum", "authors": ["Boyan Bonev", "Gema Ram\u0131\u0301rez-S\u00e1nchez", "Sergio Ortiz Rojas"], "emails": ["boyan@prompsit.com", "gramirez@prompsit.com", "sortiz@prompsit.com"], "sections": [{"heading": null, "text": "ar Xiv: 130 3.04 46v1 [cs.CL] 3 MThe classification of opinion contributions into positive and negative is very limited in this area. Opinum's approach is based on the sequence of words without using syntactic and semantic information. It consists in developing a probabilistic model for the positive and another for the negative opinions, then comparing the test opinions with both models and calculating a decision and confidence metric. In order to reduce the complexity of the training corpus, we first lemmatise the texts and replace most of the name identities with wildcards. Opinum presents an accuracy of over 81% for Spanish opinions in the field of financial products. In this paper, we discuss which are the most important factors that have an impact on the classification performance.Key words: Mood analysis, opinion classification, language models.Opinum represents an accuracy of over 81% for Spanish opinions in the field of financial products."}, {"heading": "1. Introduction", "text": "Most of the texts written by people reflect some kind of feeling. The interpretation of these feelings probably depends on the linguistic skills and emotional intelligence of both the author and the reader, but above all, this interpretation is subjective for the reader. They do not really exist in a string, because they are subjective states of mind. Therefore, the sensation analysis is a prediction of how most readers would react to a particular text. There are texts that intend to be objective and texts that are intentionally subjective. The latter is the case of opinion texts in which the authors intentionally use appropriate language to express their positive or negative feelings. Preprint, the arXiv March 5, 2013 works on something we are working on classifying opinions into two classes: expressing positive feelings (the author is for something) and expressing negative feelings, and we refer to them as positive opinions and negative opinions."}, {"heading": "2. Hypothesis", "text": "When people read an opinion, even if they do not fully understand it due to technical details or domain-specific terminology, they can in most cases determine whether it is positive or negative, because the author of the opinion, consciously or not, uses nuances and structures that exhibit a positive or negative feeling. Usually, when a user writes an opinion about a product, the intention is to communicate that subjective feeling, apart from describing the experience with the product and giving some technical details. The hypothesis underlying the traditional keyword or lexicon-based approach (Blair-Goldenohn et al. (2008); Hu and Liu (2004)) is to look for some specific positive or negative words. For example, \"big\" should be positive and \"disgusting\" should be negative. Of course, there are some exceptions such as \"not great,\" and some approaches recognize negative words negativity in order to reverse the meaning of the words."}, {"heading": "3. The Opinum approach", "text": "The proposed approach is based on N-gram language models, so building a consistent model is key to its success. In the field of machine translation, a 500 MB corpus is usually sufficient to create a 5 gram language model, depending on the morphological complexity of the language. In the field of sentiment analysis, it is very difficult to find a large corpus of context-specific opinions. Opinions with stars or a positive / negative label can be automatically downloaded from different customers \"websites. The size of the corpus thus collected ranges between 1 MB and 20 MB for positive and negative opinions. Such a small amount of text would be suitable for bigrams and would capture the difference between\" not good \"and\" really good, \"but that is not enough for longer sequences such as\" offer that you cannot refuse. \"To build consistent 5 gram language models, we need to simplify language complexity by removing the entire morphology and replacing the surface shapes with their onical ones."}, {"heading": "3.1. Lemmatization", "text": "There are some lexical forms for which we maintain the surface shape or add some morphological information to the token. These exceptions are the subject pronouns, the object pronouns and the possessive forms. The reason for this is that for some phrases personal information is the key to deciding the positive or negative sense. Let's assume that some opinions contain the sequence esot = \"They made money out of me,\" oi = \"I made money out of them.\" Their lemmatization, which is called L0, would be2L0 (ot) = L0 (oi) = \"SubjectPronoun make moneyfrom ObjectPronoun.\""}, {"heading": "3.2. Named entities replacement", "text": "For generalization purposes, we make the texts independent of specific entities. We make a distinction between place names, 2The notation we use here is for readability purposes and it is slightly different from the one we use in Opinum.people and organizations / companies. We also recognize data, phone numbers, emails and URLs / IP. We replace them all with different placeholders. All other numbers are replaced by a \"Num\" placeholder. For example, the following sequence would have an L2 (oe) Lemmatization + designated entity substitution: oe = \"Joe bought 300 shares of Acme Corp in 2012.\" L2 (oe) = \"Person buy Num subrange of companies in Date.\" The designated entity recognition task is integrated into the lemmatization process. We collected a list of names of people, places, companies and organizations to complete the morphological dictionary of Atium."}, {"heading": "3.3. Language models", "text": "The language models we build are based on N-gram word sequences. They model the probability of a word wi against the sequence of n-1 previous words, P (wi | wi \u2212 (n \u2212 1),..., wi \u2212 1. This type of model assumes the independence between the word wi and the words that do not belong to the n-gram, wj, j < i \u2212 n. This is a disadvantage for unlimited dependencies, but we are not interested in grasping the complete grammatical relationships. We intend to grasp the probabilities of smaller constructions that may have positive / negative feelings. Another assumption we make is the independence between different sentences. In the opinum, the words lemmata (or placeholders that replace entities), and the number of words under which we assume dependence is n = 5. A maximum n of 5 or 6 is common in machine translation, where huge amounts of text are used for the construction of a language model, but we do not have moral (in our case, we have a small amount of cohesive)."}, {"heading": "3.4. Evaluation and confidence", "text": "In the Opinum system, we request the M p, M n models with the Heafield (2011) KenLM open source library because it answers the questions very quickly and has a short load time suitable for a web application. It also has efficient memory management that is positive for simultaneous queries to the server. Queries are performed at the record level. Each sentence is assigned a score that is the log probability of the sentence generated by the language model. Decisions are made by comparing its values for the positive and for the negative models. For a given opinion, the log probability sums can be taken: dot = s-otlogP (s-M p) \u2212 imperative logP (s-ot logP) n) 0If this difference is close to zero, | dot | wot < \u03b50, one can assume that the classification is neutral. The number of words wot is used as a normalization factor."}, {"heading": "4. Experiments", "text": "iD eeisrcnlrlrlrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "5. Discussion on performance", "text": "This year it is more than ever before."}, {"heading": "6. Conclusions and future work", "text": "The approach is based on morphological simplifications, substitutions, and N-gram language models that can be easily adapted to other classification goals (30 languages are currently available), as well as a labeled dataset that depends on the size of the corpus but usually depends on the size of the corpus."}], "references": [{"title": "Building a sentiment summarizer for local service reviews. In: In NLP in the Information Explosion Era, NLPIX2008", "author": ["S. Blair-Goldensohn", "T. Neylon", "K. Hannan", "G.A. Reis", "R. Mcdonald", "J. Reynar"], "venue": null, "citeRegEx": "Blair.Goldensohn et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Blair.Goldensohn et al\\.", "year": 2008}, {"title": "Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "In ACL. pp. 187\u2013205", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Comparative experiments on sentiment classification for online product reviews. In: proceedings of the 21st national conference on Artificial intelligence - Volume 2. AAAI\u201906", "author": ["H. Cui", "V. Mittal", "M. Datar"], "venue": null, "citeRegEx": "Cui et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2006}, {"title": "Efficient handling of n-gram language models for statistical machine translation", "author": ["M. Federico", "M. Cettolo"], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation. StatMT \u201907. Association for Computational Linguistics,", "citeRegEx": "Federico and Cettolo,? \\Q2007\\E", "shortCiteRegEx": "Federico and Cettolo", "year": 2007}, {"title": "Kenlm: faster and smaller language model queries", "author": ["K. Heafield"], "venue": "Proceedings of the Sixth Workshop on Statistical Machine Translation. WMT \u201911. Association for Computational Linguistics,", "citeRegEx": "Heafield,? \\Q2011\\E", "shortCiteRegEx": "Heafield", "year": 2011}, {"title": "Mining and summarizing customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. KDD \u201904. ACM,", "citeRegEx": "Hu and Liu,? \\Q2004\\E", "shortCiteRegEx": "Hu and Liu", "year": 2004}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions. ACL \u201907", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Structured models for fine-to-coarse sentiment analysis", "author": ["R. Mcdonald", "K. Hannan", "T. Neylon", "M. Wells", "J. Reynar"], "venue": "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics", "citeRegEx": "Mcdonald et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mcdonald et al\\.", "year": 2007}, {"title": "Thumbs up? sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "In proceedings of EMNLP", "citeRegEx": "Pang et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2002}, {"title": "Multimodal subjectivity analysis of multiparty conversation", "author": ["S. Raaijmakers", "K.P. Truong", "T. Wilson"], "venue": null, "citeRegEx": "Raaijmakers et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Raaijmakers et al\\.", "year": 2008}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["M. Taboada", "J. Brooke", "M. Tofiloski", "K. Voll", "M. Stede"], "venue": "Comput. Linguist", "citeRegEx": "Taboada et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Taboada et al\\.", "year": 2011}, {"title": "Free/open-source resources in the apertium platform for machine translation research and development", "author": ["F.M. Tyers", "F. S\u00e1nchez-Mart\u0131\u0301nez", "S. Ortiz-Rojas", "M.L. Forcada"], "venue": "The Prague Bulletin of Mathematical Linguistics", "citeRegEx": "Tyers et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Tyers et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "For instance, in the review of a film there is an objective part and then the opinion (Raaijmakers et al. (2008)).", "startOffset": 87, "endOffset": 113}, {"referenceID": 6, "context": "For instance, in the review of a film there is an objective part and then the opinion (Raaijmakers et al. (2008)). In our case we work directly with opinion texts and we do not make such difference. We have noticed that in customer reviews, even when stating objective facts, some positive or negative sentiment is usually expressed. Many works in the literature of sentiment analysis take lexicon-based approaches, like Taboada et al. (2011). For instance Hu and Liu (2004); Blair-Goldensohn et al.", "startOffset": 87, "endOffset": 443}, {"referenceID": 4, "context": "For instance Hu and Liu (2004); Blair-Goldensohn et al.", "startOffset": 13, "endOffset": 31}, {"referenceID": 0, "context": "For instance Hu and Liu (2004); Blair-Goldensohn et al. (2008) use WordNet to extend the relation of positive and negative words to other related lexical units.", "startOffset": 32, "endOffset": 63}, {"referenceID": 0, "context": "For instance Hu and Liu (2004); Blair-Goldensohn et al. (2008) use WordNet to extend the relation of positive and negative words to other related lexical units. However the combination of which words appear together may also be important and there are comparisons of different Machine learning approaches (Pang et al. (2002)) in the literature, like Support Vector Machines, kNearest Neighbours, Naive-Bayes, and other classifiers based on global features.", "startOffset": 32, "endOffset": 325}, {"referenceID": 6, "context": "In the work of Mcdonald et al. (2007), structured models are used to infer the sentiment from different levels of granularity.", "startOffset": 15, "endOffset": 38}, {"referenceID": 1, "context": "A study of domain adaptation for sentiment analysis is presented in the work of Blitzer et al. (2007). In Opinum different classifiers would be built for different domains.", "startOffset": 80, "endOffset": 102}, {"referenceID": 0, "context": "The hypothesis underlying the traditional keyword or lexicon-based approaches (Blair-Goldensohn et al. (2008); Hu and Liu (2004)) consist in looking for some specific positive or negative words.", "startOffset": 79, "endOffset": 110}, {"referenceID": 0, "context": "The hypothesis underlying the traditional keyword or lexicon-based approaches (Blair-Goldensohn et al. (2008); Hu and Liu (2004)) consist in looking for some specific positive or negative words.", "startOffset": 79, "endOffset": 129}, {"referenceID": 11, "context": "Thanks to its modularized architecture (described in Tyers et al. (2010)) we use its morphological analyser and its part-of-speech disambiguation module in order to take one lexical form as the most probable one, in case there are several possibilities for a given surface.", "startOffset": 53, "endOffset": 73}, {"referenceID": 4, "context": "A maximum n of 5 or 6 is common in machine translation where huge amounts of text are used for building a language model (Koehn et al. (2007)).", "startOffset": 122, "endOffset": 142}, {"referenceID": 2, "context": "Details about the process can be found in Federico and Cettolo (2007). Another language model approach based on n-grams was reported in Cui et al.", "startOffset": 42, "endOffset": 70}, {"referenceID": 2, "context": "Another language model approach based on n-grams was reported in Cui et al. (2006), where they used the CMU-Cambridge Language Modeling Toolkit on the original texts.", "startOffset": 65, "endOffset": 83}, {"referenceID": 4, "context": "In the Opinum system we query the M p ,M n models with the Heafield (2011) KenLM open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application.", "startOffset": 59, "endOffset": 75}, {"referenceID": 2, "context": "In Cui et al. (2006) the authors evaluate opinion classification based on ngrams with a larger dataset consisting of 159, 558 positive and 27, 366 negative opinions.", "startOffset": 3, "endOffset": 21}], "year": 2013, "abstractText": "The classification of opinion texts in positive and negative is becoming a subject of great interest in sentiment analysis. The existence of many labeled opinions motivates the use of statistical and machine-learning methods. First-order statistics have proven to be very limited in this field. The Opinum approach is based on the order of the words without using any syntactic and semantic information. It consists of building one probabilistic model for the positive and another one for the negative opinions. Then the test opinions are compared to both models and a decision and confidence measure are calculated. In order to reduce the complexity of the training corpus we first lemmatize the texts and we replace most namedentities with wildcards. Opinum presents an accuracy above 81% for Spanish opinions in the financial products domain. In this work we discuss which are the most important factors that have an impact on the classification performance.", "creator": "LaTeX with hyperref package"}}}