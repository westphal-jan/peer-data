{"id": "1303.1703", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2013", "title": "Concept-based indexing in text information retrieval", "abstract": "Traditional information retrieval systems rely on keywords to index documents and queries. In such systems, documents are retrieved based on the number of shared keywords with the query. This lexical-focused retrieval leads to inaccurate and incomplete results when different keywords are used to describe the documents and queries. Semantic-focused retrieval approaches attempt to overcome this problem by relying on concepts rather than on keywords to indexing and retrieval. The goal is to retrieve documents that are semantically relevant to a given user query. This paper addresses this issue by proposing a solution at the indexing level. More precisely, we propose a novel approach for semantic indexing based on concepts identified from a linguistic resource. In particular, our approach relies on the joint use of WordNet and WordNetDomains lexical databases for concept identification. Furthermore, we propose a semantic-based concept weighting scheme that relies on a novel definition of concept centrality. The resulting system is evaluated on the TIME test collection. Experimental results show the effectiveness of our proposition over traditional IR approaches.", "histories": [["v1", "Thu, 7 Mar 2013 14:46:17 GMT  (472kb)", "http://arxiv.org/abs/1303.1703v1", "18 pages, 5 tables, 3 figures"]], "COMMENTS": "18 pages, 5 tables, 3 figures", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["fatiha boubekeur", "wassila azzoug"], "accepted": false, "id": "1303.1703"}, "pdf": {"name": "1303.1703.pdf", "metadata": {"source": "CRF", "title": "CONCEPT-BASED INDEXING IN TEXT INFORMATION RETRIEVAL", "authors": ["Fatiha Boubekeur", "Wassila Azzoug"], "emails": ["amirouchefatiha@mail.ummto.dz", "azzoug_w@umbb.dz"], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijcsit.2013.5110 119Traditional information retrieval systems rely on keywords to index documents and queries. In such systems, documents are retrieved based on the number of keywords shared with the query. This lexical query results in inaccurate and incomplete results when different keywords are used to describe documents and queries. Semantically focused query approaches attempt to solve this problem by relying on concepts rather than indexing and query keywords. The goal is to retrieve documents that are semantically relevant to a given user query. This paper addresses this problem by proposing a solution at the indexing level. Specifically, we propose a novel approach to semantic indexing based on concepts identified from a linguistic resource. Specifically, our approach relies on the sharing of Word Net and Word conceptual databases to identify domains."}, {"heading": "1. INTRODUCTION", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. WordNet", "text": "WordNet [6] is an electronic lexical database that covers the majority of nouns, verbs, adjectives and adverbs in the English language that it structures in a network of nodes and nodes. A node, also known as synset, is a set of synonymous terms that are interchangeable in a context. A synset represents a concept or a sense of the word. A synset is lexically represented by a term from its synonyms. Almost every WordNet synset has a gloss expressed in English that defines this synset. The gloss of a synset may also include comments and / or one or more examples of how the words are used in the synset [8]. Table 1 presents the synsets associated with the word \"bank,\" including the first synset, {depository financial institution, bank, banking company, banking group, banking company} which is defined by the gloss: -- a financial institution that accepts money and deposits into credit facilities."}, {"heading": "2.2. WordNet-Based Semantic Relatedness Measures", "text": "Numerous approaches to measuring the semantic relationship between WordNet synsets are proposed in the literature, which are divided into two main types: path-based metrics and information-based metrics. \u2212 For path-based metrics, WordNet is considered a graph of concepts and a semantic relationship between two concepts by edge counting (i.e. path length) between their corresponding nodes in the graph. \u2212 The underlying principle is that the shortest path from one node to another is the more similar the more similar the concepts are. \u2212 For information-based metrics, the semantic relationship between two concepts is measured on the information they have in common. \u2212 For information-based metrics, the semantic relationship between two concepts is measured on the information they have in common."}, {"heading": "2.3. WordNetDomains", "text": "WordNetDomains [7] is an extension of the lexical WordNet database, resulting from the annotation of each synthesis with one or more domain names from a set of 176 domains that are hierarchically organized by the submission (specialization / generalization) relationship (e.g. tennis is a more specific domain than sports and architecture is a more general domain than buildings).Part of the WordNetDomains hierarchy is in Table 3. The top domain is the root of this hierarchy. Factotum is a functional domain (as opposed to semantic domain) that includes generic synsets that are difficult to classify in a particular domain, and stop sense synsets (such as colors, numbers, etc.) that frequently occurred in different contexts [15]. Factotum is independent of the top domain and its hierarchy."}, {"heading": "3. RELATED WORK", "text": "Concept-based indexing represents both documents and queries using semantic units, the concepts, rather than (or in addition to) lexical units, the keywords. The query is then performed in this terminology space. Concept-based indexing approaches promise that the representation of documents and queries (or improving their BOW representation) using concepts will result in a retrieval model that is less dependent on index terms [16]. In fact, documents in such a model could be retrieved even if the same concept is described in the query and in the documents with different terms, thereby mitigating the synonymy problem and increasing the recall rate3 [17]. Similarly, if the correct concepts for ambiguous words that appear in the query and in the documents are selected, non-relevant documents would not be retrieved, reducing the relationship between the text used and the text used."}, {"heading": "3.1. Concept Identification", "text": "The idea is that people who are able to determine themselves are able to determine themselves what they want and what they want. The idea behind it is that people who are able to determine themselves and act are able to understand themselves. The idea behind it is that people who are able to identify themselves and understand what they want are able to identify themselves. The idea behind it is that people who are able to identify themselves and understand what they want are able to identify themselves. The idea behind it is that people who are able to identify themselves and understand what they want are able to identify themselves."}, {"heading": "3.2. Concept Weighting", "text": "In fact, most of them will be able to orient themselves in a different direction than the direction in which they are moving."}, {"heading": "4. CONCEPT-BASED DOCUMENT INDEXING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Definitions and Notations", "text": "Let me be a word of the text to be indexed. \u2212 We call instance of m, every occurrence of mi from m in the given text. \u2212 An instance of word is a word. It is associated with a single part of the speech (noun, verb, adverb, etc.) in the sentence in which it occurs. \u2212 An instance of word m appears in a sentence. The number of index terms in this sentence defines the local context of mi that is noted. \u2212 The global context of mi is the union of all local contexts in which mi appears with the same part of the speech. Thus, each global context defines a different meaning for m. \u2212 The global context of mi is denoted by.i\u0445 \u2212 The local expression of mi of size s + 1 is the string that arises from the concatenation of the word mi and the consecutive words that lie immediately to the right of mi, using an underscore (_) between them. \u2212 The size of a local expression is the number of words it contains."}, {"heading": "4.2. Description of the Approach", "text": "The main objective of our approach is to represent the document (or query) with a semantic index composed of two types of terms: concepts and orphaned keywords. \u2212 Concepts are (unique) WordNet entries (synsets) that are identified from the text of the document. Concepts are designated either by simple words or collocations. \u2212 Orphan keywords are (not empty) simple words of the document that are not entries in WordNet. Concepts are first identified in the document (or query) by an identification / disambiguation process (which also allows identification of orphaned keywords) and then weighted."}, {"heading": "4.2.1 Concept Identification", "text": "The main objective of this step is to identify the representative concepts of a particular document (or query). Concepts are entries in WordNet that correspond to the representative terms of the indexed document (or query). Concept recognition means (a) identifying index terms from the text under consideration and then assigning them to WordNet synsets and (b) clearly defining ambiguous terms (an ambiguous term corresponds to more than one synthesis)."}, {"heading": "4.2.1.1 Identifying Index Terms", "text": "The purpose of this step is to identify the set of representative terms (words or collocations) of the material under consideration, which is based on WordNet. This step begins with the identification of collocations. To this end, we have first created a list of all collocations of WordNet. Then, for each word analyzed, we extract from Coloc the set i of collocations starting with mi. i, first in decreasing order of its element size, then each element in i is projected onto the local expression egg of size i. If a collocation matches a local expression, it is retained and inserted into the set expresses of the identified collocations. If no collocation matches the local expression of mi, mi is a simple word. If this simple word has an entry in WordNet, it is inserted into the group Simplescio of simple words. Otherwise, it is added to the orphan islands that orphel.The definition algorithm is given in Table 4."}, {"heading": "4.2.1.2 Term Disambiguation", "text": "This step aims to assign a meaning to each ambiguous index term based on the context in which it occurs. Collocations are almost unambiguous terms, so the disambiguation process, which relies mainly on WordNet, includes only the simple words that have entries in this lexical database. Therefore, only the words of Simplescio are summarized. A word of Simplescio can have multiple entries (synsets) in WordNet that correspond to different senses. The purpose of this step is to select the appropriate synthesis of the word based on its context. To clarify the words, we propose an approach based on three levels of disambiguity: \u2212 The first level is part of language identification (POS). This level aims to determine the POS of each word mi in the document using the Stanford POS Tagger. \u2212 The second level is domain disambigualization. This level is aimed at determining the context of a word (s) in the context of a document."}, {"heading": "4.2.1.2.1 Part of speech Identification", "text": "A word can have several instances in a document, each of which is characterized by its POS. The goal of this step is to identify the POS of each instance in a particular material. To this end, we simply rely on the Stanford POS Tagger. Since the synsets associated with the instance of a word in WordNet are grouped according to the language part, this step aims to limit the synsets that will be examined in the next steps of the discourse to those that have the same POS as the target instance."}, {"heading": "4.2.1.2.2. Domain Disambiguation", "text": "() () () () () () () () () () () () () () () () () () () ()) () () () () () ()) () () ()) () () () () ()) () () ()) () ()) () () ()) () ()) () () () ()) () () ()) () ()) () () () () () () ()) () () ()) () ()) ()) () ()) () ()) () ()) () () ()) () () () () () () ()) () () () () () () () () () () () ()) () () () () () ()) () () () () () ()) () () () ()) () () () ()) () () ()) () () ()) () ()) () ()) () ()) () ()) () ()) ()) () ()) () ()) ()) () ()) () ()) () ()) () ()) () ()) () ()) () ()) () () () () ()) () () ())) () ()) () () () ()) () () () () ()) () () () ()) () () () () () () () ()) () () () () ()) () () () () () () () ()) () () () () () ()) () () () () () ()) () () () () () () () () () () () () () () () () () ()) () () ()"}, {"heading": "4.2.1.2.3 Word sense disambiguation", "text": "At this stage, each word mi in Simples\u0442 is associated with a single domain Di in its context, but it can still be associated with more than one synset (sense) in that area. In this case, it must be clearly defined. The goal is to select from all synsets associated with mi in Di the appropriate sense (sense) of mi in its context. To decipher the word mi in its context, we associate a score with each synset [] kS ji) (based on its semantic relationship with other synsets that are associated with the other terms of the context. The synset with the highest score is selected as the appropriate meaning of the word mi in its context. Formally, [] () [] [] [] kS ji] (based on its semantic relationship with other synsets that are associated with the other terms of the context. The synset with the highest score is selected as the appropriate meaning of the word it in its context."}, {"heading": "4.2.2. Concepts Weighting", "text": "The aim of this step is to assign weights to the identified concepts (synsets), which express their meaning in the documentation. Starting from the idea that the more a concept is locally central in the document and globally central in the collection, the more it is representative of the content of the document, we will weight concepts according to their local and global centralities as follows: The local centrality of a concept Ci in a document d, referred to as cc (Ci, d), is defined on the basis of its relevance in the document on the one hand and its occurrence frequency on the other hand. The relevance of the concept is measured by its semantic relevance with the other concepts in the document. Its frequency is the cumulative frequency of all its representative concepts in the document. Formally: () () ()"}, {"heading": "5. EXPERIMENTAL EVALUATION", "text": "In this section, we present our experimental assessment of the proposed approach, which aims to achieve two things: (1) to measure the effectiveness of our concept-based indexing approach versus classical indexing approaches, and (2) to measure the performance of our semantic weighting scheme (cc-idc) versus classical weighting schemas.In the following, we present the experimental setting (test survey and evaluation protocol), and then present and discuss the evaluation results."}, {"heading": "5.1. Experimental Setting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1 Test Collection", "text": "For our experiments we used the test collection Time5. This data set contains 423 documents, consisting of newspaper articles from TIME magazine and a large number of searches (83). Each search query is associated with human-relevant judgments. We selected the 15 searches that provide the most meaningful results in the classic search based on the tf * idf weighting."}, {"heading": "5.1.2. Evaluation Protocol", "text": "The approach is implemented by a concept-based vector IR system. In this system, documents and queries are considered as vectors of weighted concepts and compared by the classical cosinous metric, evaluated according to the TREC protocol. It is based on two5 ftp: / / ftp.cs.cornell.edu / pub / smart / time / measures: precision at cutoff x, P @ x (x = 5, 10, 20, 50, 100, 200, 500), and mean average precision (MAP).precision at cutoff x, P @ x is the ratio of relevant documents among the top x returned documents. Formally, one considers the number of relevant documents among the x first retrieved documents, then: xRR xP x = @ MAP is the arithmetic mean over all queries of average precision (calculated at the ranks where relevant documents are retrieved).Formally: Qi = 11j = weighted documents for the classical system."}, {"heading": "5.2. Concept-Based Indexing Evaluation", "text": "In order to evaluate the effectiveness of our concept-based indexing in violation of the proposed weighting scheme, we consider our semantic index weighted firstly according to tf * idf (Index Sem-TF-IDF) and secondly weighted according to Okapi-BM25 (Index Sem-BM25), and then compare the results from these indices with those of Baselines Classic-TF-IDF and Classic-BM25. This approach allows isolated concepts to make a contribution from weight.The evaluation results for these different models are shown in Figure 1.6 http: / / trec.nist.gov / trec _ eval / Based on these results, we find that the Sem-TF-IDF-IDF approach is better than the classical TF-IDF baseline approach. Significant improvement rates (better than 25%) of 100% for P @ 5, 103.33% for P-100% for P @."}, {"heading": "5.3. Concept Weighting Evaluation", "text": "The second series of experiments focuses on the evaluation of our concept of weighting schemes introduced in Section 4.2.2. In practice, we aim to measure the impact of the weighting scheme cc-idc on call effectiveness. To this end, we compare retrieval results from our semantic index weighted by cc-idc (Sem-CC-IDC) with those of Sem-TF-IDF and Sem-BM25. Since the proposed weighting scheme cc-idc depends on the \u03b1 weight parameter, preliminary experiments are necessary to determine the corresponding related value."}, {"heading": "5.3.1.  Selection", "text": "We conducted a series of experiments to determine the appropriate value for the \u03b1 weight parameter. In these experiments, we vary \u03b1 values between 0 and 1, resulting in different weighting schemes that are used successively to weight our semantic index. Afterwards, the weighted indices obtained are evaluated on the basis of their retrieval results. The evaluation is done according to the protocol presented above (Section 5.1.2); it is based on two measures: the average accuracy MP @ x and MAP, where MP @ x is the arithmetic mean of P @ x (x = 1,2,3,4,5,10,15,30,50,100). Table 5 presents the results of this evaluation. These results show that the overall best retrieval performance (according to MP @ x and MAP) is achieved for \u03b1 = 0.2."}, {"heading": "P @ 1 0,1333 0,1333 0,1333 0,1333 0,1333 0,1333 0,1333 0,1333 0,1333", "text": "In the following, we set the value of \u03b1 to 0.2. This value prefers the relevance of the concept over the frequency of the concept in the corresponding weighting scheme."}, {"heading": "5.3.2. Experimental Evaluation", "text": "The purpose of this step is to evaluate the impact on the effectiveness of the query of the proposed semantic weight scheme cc-idc compared to the classic weight schemes tf * idf and Okapi-BM25. To this end, we compare the results of our semantic index Sem-CC-IDC (for \u03b1 = 0.2) with those of the Sem-TF-IDF or Sem-BM25. The experimental results are shown in Figure 3. Based on these results, we found that - the Sem-CC-IDC index performs better than the Sem-TF-IDF index. Significant improvement rates (better than 25%) of 200% for P @ 5, 80.33% for P @ 10, 33.36% for P @ 15, 33.33% for P @ 20, 54.47% for P @ 30, 35.71% for P @ 50 and 218.34% for MAP are clearly better."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we presented a novel approach to automatic concept-based document indexing. Our contribution is twofold: firstly, we have introduced a novel concept identification approach based on a novel domain-based meaning-disambiguation framework based on the sharing of WordNet and WordNetDomains, and secondly, we have defined a novel semantic weight scheme based on the centrality of the concept. In our proposal, the centrality of a concept is based on its obvious importance (measured by its frequency of occurrence) in the document on the one hand, and on its latent meaning (measured by its semantic relationship to other concepts) in the document on the other. Our experimental results have shown that the proposed concept-based index approach is more effective than classical keyword-based indexing approaches. Furthermore, our cc-idc weighting approach (measured by the fixed value of the weight-weight parameter IF) works better than classical BMF systems."}], "references": [{"title": "Relevance feedback in information retrieval", "author": ["J.J. Rocchio"], "venue": "In The SMART Retrieval System, in Experiments in Automatic Document Processing. G.Salton editor,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1971}, {"title": "A fuzzy linguistic approach generalizing Boolean information retrieval: a model and its evaluation,", "author": ["G. Bordogna", "G. Pasi"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "A model for a weighted retrieval system,", "author": ["D.A. Buell", "D.H. Kraft"], "venue": "Journal of the American Society for Information Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1981}, {"title": "Term-weighting approaches in automatic text retrieval", "author": ["G Salton", "C Buckley"], "venue": "Information Processing and Management", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1988}, {"title": "Beyond the keyword bariier: knowledge-based information retrieval. Information services and use", "author": ["Mauldin M", "J. Carbonell", "R. Thomason"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1987}, {"title": "WordNet : A Lexical database for English", "author": ["G. Miller"], "venue": "Actes de ACM", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1995}, {"title": "Revising WordNet Domains Hierarchy: Semantics, Coverage, and Balancing", "author": ["Luisa Bentivogli", "Pamela Forner", "Bernardo Magnini", "Emanuele Pianta"], "venue": "COLING", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "ed.) WordNet: An Electronic Lexical Database", "author": ["Christiane Fellbaum"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Combining local context and WordNet similarity for word sense identification", "author": ["C. Leacock", "M. Chodorow"], "venue": "In C. Fellbaum Ed., MIT Press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "Verb semantics and lexical selection", "author": ["Zhibiao Wu", "Martha Palmer"], "venue": "In 32nd. Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language", "author": ["P. Resnik"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "An information-theoretic definition of similarity", "author": ["D. Lin"], "venue": "In Proceedings of 15th International Conference On Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Semantic similarity based on corpus statistics and lexical taxonomy", "author": ["Jay J. Jiang", "David W. Conrath"], "venue": "In Proceedings of International Conference on Research", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "EvaluatingWordNet-based Measures of Lexical Semantic Relatedness", "author": ["A. Budanitsky", "G. Hirst"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Chinese WordNet Domains: Bootstrapping Chinese WordNet with Semantic Domain Labels", "author": ["Lung-Hao Lee", "Yu-Ting Yu", "Chu-Ren Huang"], "venue": "In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Concept-Based Information Retrieval using Explicit Semantic Analysis", "author": ["O. Egozi", "S. Markovitch", "E. Gabrilovich"], "venue": "ACM Transactions on Information Systems, Volume 29 Issue", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Document indexing: a concept-based approach to term weight estimation", "author": ["B.Y. Kang", "S.J. Lee"], "venue": "In Journal of Information Processing & Management. Volume 41, Issue", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "An exploratory analysis of phrases in text retrieval", "author": ["Jeremy Pickens", "W. Bruce Croft"], "venue": "RIAO", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2000}, {"title": "Word sense disambiguation for free-text indexing using a massive semantic network", "author": ["M. Sussna"], "venue": "2nd International Conference on Information and Knowledge Management", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1993}, {"title": "Disambiguating noun groupings with respect to WordNet senses", "author": ["P. Resnik"], "venue": "3thWorkshop on Very Large Corpora,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1995}, {"title": "Using WordNet to disambiguate word senses for text retrieval. Association for Computing Machinery Special Interest Group on Information Retrieval", "author": ["E.M. Voorhees"], "venue": "Journal of Computer Science & Information Technology (IJCSIT)", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "A Conceptual Indexing Approach based on Document content Representation. Dans : CoLIS5 : Fifth International Conference on Conceptions of Libraries and Information Science, Glasgow, UK, 4 juin 8 juin", "author": ["M. Baziz", "M. Boughanem", "N. Aussenac-Gilles"], "venue": "Lecture Notes in Computer Science LNCS Volume", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Using WordNet for Concept-based document indexing in information retrieval", "author": ["F. Boubekeur", "M. Boughanem", "L. Tamine", "M. Daoud"], "venue": "Fourth International Conference on Semantic Processing (SEMAPRO),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Retrieval effectiveness of an ontology-based model for information selection", "author": ["L.R. Khan", "D. Mc Leod", "E.Hovy"], "venue": "The VLDB Journal", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2004}, {"title": "Word Sense Disambiguation using WordNetDomains", "author": ["S.G. Kolte", "S.G. Bhirud"], "venue": "In First International Conference on Emerging Trends in Engineering and Technology", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "Indexation s\u00e9mantique de documents textuels\u00bb, 14e Colloque International sur le Document Electronique", "author": ["F. Boubekeur", "W. Azzoug", "S. Chiout", "M. Boughanem"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Vers une approche statistique pour l\u2019indexation s\u00e9mantique des documents multilingues ", "author": ["F. Harrathi", "C. Roussey", "L. Maisonnasse", "S. Calabretto"], "venue": "Actes du XXVIII\u00b0 congre\u0300s INFORSID,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "A new factor for computing the relevance of a document to a query (regular paper)", "author": ["M. Boughanem", "I. Mallak", "H. Prade"], "venue": "Dans : IEEE World Congress on Computational Intelligence (WCCI", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Vers un mod\u00e8le d'indexation s\u00e9mantique adapt\u00e9 aux dossiers m\u00e9dicaux de patients", "author": ["D. Dinh", "L.Tamine"], "venue": "(short paper). Dans : Confe\u0301rence francophone en Recherche d'Information et Applications (CORIA", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Automatic sense disambiguation using machine readable dictionaries : How to tell a pine cone from a nice cream cone", "author": ["M.E. Lesk"], "venue": "In Proceedings of the SIGDOC Conference. Toronto,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1986}, {"title": "The probability ranking principle in IR", "author": ["S.E. Robertson"], "venue": "Journal of Documentation", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1977}], "referenceMentions": [{"referenceID": 0, "context": "Representing the user\u2019s information need involves a one step or multi-step query formulation by means of prior terms expressed by the user and/or additive information driven by iterative query improvements like relevance feedback [1].", "startOffset": 230, "endOffset": 233}, {"referenceID": 1, "context": "Weights are associated with document or query keywords [2], [3] to express their importance in the considered material.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Weights are associated with document or query keywords [2], [3] to express their importance in the considered material.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "The weighting scheme is generally based on variations of the well known tf*idf formula [4].", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "This leads to a \u201clexical focused\u201d relevance estimation which is less effective than a \u201csemantic focused\u201d one [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "To identify accurate concepts in the considered material, the approach relies on a two-steps word sense disambiguation (WSD) approach based on the joint use of WordNet [6] and its extension WordNetDomains [7].", "startOffset": 168, "endOffset": 171}, {"referenceID": 6, "context": "To identify accurate concepts in the considered material, the approach relies on a two-steps word sense disambiguation (WSD) approach based on the joint use of WordNet [6] and its extension WordNetDomains [7].", "startOffset": 205, "endOffset": 208}, {"referenceID": 5, "context": "WordNet [6] is an electronic lexical database which covers the majority of nouns, verbs, adjectives and adverbs of the English language, which it structured in a network of nodes and links.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "A synset's gloss may also contain comments and/or one or more examples of how the words in the synset are used [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "Leackock and Chodrow measure [9] and Wu-Palmer measure [10] range from this category.", "startOffset": 29, "endOffset": 32}, {"referenceID": 9, "context": "Leackock and Chodrow measure [9] and Wu-Palmer measure [10] range from this category.", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "Resnik measure [11], lin measure [12], and Jiang and Conrath measure [13] range from this category.", "startOffset": 15, "endOffset": 19}, {"referenceID": 11, "context": "Resnik measure [11], lin measure [12], and Jiang and Conrath measure [13] range from this category.", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "Resnik measure [11], lin measure [12], and Jiang and Conrath measure [13] range from this category.", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "We refer the interested reader to [14] for an overview on the cited measures.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "WordNetDomains [7] is an extension of WordNet lexical database that results from the annotation of each synset with one or more domain label from a set of 176 domains hierarchically organized through the subsumption (specialization/generalization) relation (for example, Tennis is a more specific domain than Sport, and Architecture is a more general domain than Buildings).", "startOffset": 15, "endOffset": 18}, {"referenceID": 14, "context": ") which appeared frequently in different contexts [15].", "startOffset": 50, "endOffset": 54}, {"referenceID": 15, "context": "Indeed, in such a model, documents could be retrieved even when the same concept is described by different terms in the query and the documents, thus alleviating the synonymy problem and increasing recall [17].", "startOffset": 205, "endOffset": 209}, {"referenceID": 16, "context": "These concepts describe the content of a given text to different extents [18].", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": ") [19].", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "To disambiguate a term, WSD approaches generally exploit local context and definitions from the ontology [20], [21], [22], [23], [24].", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "To disambiguate a term, WSD approaches generally exploit local context and definitions from the ontology [20], [21], [22], [23], [24].", "startOffset": 111, "endOffset": 115}, {"referenceID": 20, "context": "To disambiguate a term, WSD approaches generally exploit local context and definitions from the ontology [20], [21], [22], [23], [24].", "startOffset": 117, "endOffset": 121}, {"referenceID": 21, "context": "To disambiguate a term, WSD approaches generally exploit local context and definitions from the ontology [20], [21], [22], [23], [24].", "startOffset": 123, "endOffset": 127}, {"referenceID": 22, "context": "To disambiguate a term, WSD approaches generally exploit local context and definitions from the ontology [20], [21], [22], [23], [24].", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "Approaches in [22], [23], [25], [26], [27] are based on these principles.", "startOffset": 14, "endOffset": 18}, {"referenceID": 21, "context": "Approaches in [22], [23], [25], [26], [27] are based on these principles.", "startOffset": 20, "endOffset": 24}, {"referenceID": 23, "context": "Approaches in [22], [23], [25], [26], [27] are based on these principles.", "startOffset": 38, "endOffset": 42}, {"referenceID": 20, "context": "To disambiguate an ambiguous word, Voorhees [22], classified each synset of this word based on the number of words collocated between a neighborhood of this synset in the WordNet is-a hierarchy and the local context (the sentence in which the word occurs) of the target word.", "startOffset": 44, "endOffset": 48}, {"referenceID": 23, "context": "[27], proposed an approach based on the semantic closeness of concepts.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23], assigned a score to each sense of each ambiguous word.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "In a more recent work, the authors in [28] proposed a domain-oriented disambiguation approach that relies on first identifying the correct domain of a word in the document based on WordNetDomains, and then disambiguating the word in the identified domain based on WordNet.", "startOffset": 38, "endOffset": 42}, {"referenceID": 25, "context": "Based on a similar principle, we proposed in [29] to disambiguate a domain on the basis of its semantic relatedness to other domains associated with other terms in the same context.", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "This importance can be estimated either statistically through its frequency distribution in the document (or query) content using a normalized version of the classical tf*idf scheme as in [22], [23], [30], or semantically through its centrality (ie.", "startOffset": 188, "endOffset": 192}, {"referenceID": 21, "context": "This importance can be estimated either statistically through its frequency distribution in the document (or query) content using a normalized version of the classical tf*idf scheme as in [22], [23], [30], or semantically through its centrality (ie.", "startOffset": 194, "endOffset": 198}, {"referenceID": 26, "context": "This importance can be estimated either statistically through its frequency distribution in the document (or query) content using a normalized version of the classical tf*idf scheme as in [22], [23], [30], or semantically through its centrality (ie.", "startOffset": 200, "endOffset": 204}, {"referenceID": 16, "context": "its semantic relatedness to other concepts) in the document (or query) as in [18], [29], [31], [32].", "startOffset": 77, "endOffset": 81}, {"referenceID": 25, "context": "its semantic relatedness to other concepts) in the document (or query) as in [18], [29], [31], [32].", "startOffset": 83, "endOffset": 87}, {"referenceID": 27, "context": "its semantic relatedness to other concepts) in the document (or query) as in [18], [29], [31], [32].", "startOffset": 89, "endOffset": 93}, {"referenceID": 28, "context": "its semantic relatedness to other concepts) in the document (or query) as in [18], [29], [31], [32].", "startOffset": 95, "endOffset": 99}, {"referenceID": 26, "context": "[30], Baziz et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] and Voorhees [22] rely on this principle.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[23] and Voorhees [22] rely on this principle.", "startOffset": 18, "endOffset": 22}, {"referenceID": 8, "context": "Based on the extended vector space model introduced in [9], in which every vector consists of a set of subvectors of various concept types (called ctypes), Voorhees [22] proposed to weight the concepts using a normalized classic tf*idf scheme.", "startOffset": 55, "endOffset": 58}, {"referenceID": 20, "context": "Based on the extended vector space model introduced in [9], in which every vector consists of a set of subvectors of various concept types (called ctypes), Voorhees [22] proposed to weight the concepts using a normalized classic tf*idf scheme.", "startOffset": 165, "endOffset": 169}, {"referenceID": 21, "context": "[23], extends the tf*idf scheme to taking into account the compound terms (or multi-words).", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "In [30], the proposed tf*ief weighting scheme is an adapted version of tf*idf to concepts weighting relatively to a given element of an XML document.", "startOffset": 3, "endOffset": 7}, {"referenceID": 27, "context": "In [31], [32], this importance is estimated through the number of semantic relations between the target concept and other concepts in a document.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "In [31], [32], this importance is estimated through the number of semantic relations between the target concept and other concepts in a document.", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "These relations are also weighted in [18].", "startOffset": 37, "endOffset": 41}, {"referenceID": 27, "context": "In [31], the number of relations a concept has with other concepts in the document defines its centrality.", "startOffset": 3, "endOffset": 7}, {"referenceID": 28, "context": "The authors in [32] combine centrality and specificity to estimate the importance of a concept in a document.", "startOffset": 15, "endOffset": 19}, {"referenceID": 22, "context": "In our semantic indexing approach proposed in [24], the importance of a concept in a document is expressed through its (cumulative) semantic relatedness to other concepts in the document.", "startOffset": 46, "endOffset": 50}, {"referenceID": 25, "context": "This has been combined with the statistical frequency measure in our proposal in [29].", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "where ( ) k j D D Sim , denotes the semantic relatedness of domains Dj and Dk estimated through the Wu-Palmer [10] measure which we adapt to the WordNetDomains hierarchy as follows:", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "Where [ ] [ ] ( ) n S k S Sim m l j i ) ( ) ( , estimates the semantic relatedness (or semantic similarity) between the concepts [ ] k S j i ) ( and [ ] n S m l ) ( on the basis of the Resnik measure [11] (or any other WordNetbased similarity measure [12], [33] .", "startOffset": 200, "endOffset": 204}, {"referenceID": 11, "context": "Where [ ] [ ] ( ) n S k S Sim m l j i ) ( ) ( , estimates the semantic relatedness (or semantic similarity) between the concepts [ ] k S j i ) ( and [ ] n S m l ) ( on the basis of the Resnik measure [11] (or any other WordNetbased similarity measure [12], [33] .", "startOffset": 251, "endOffset": 255}, {"referenceID": 29, "context": "Where [ ] [ ] ( ) n S k S Sim m l j i ) ( ) ( , estimates the semantic relatedness (or semantic similarity) between the concepts [ ] k S j i ) ( and [ ] n S m l ) ( on the basis of the Resnik measure [11] (or any other WordNetbased similarity measure [12], [33] .", "startOffset": 257, "endOffset": 261}, {"referenceID": 10, "context": "This factor is determined experimentally, - Sim(C, C) measures the semantic similarity between concepts C and C, calculated on the basis of the Resnik measure [11], - tf(C, d) is the occurrence frequency of the concept C in the document.", "startOffset": 159, "endOffset": 163}, {"referenceID": 30, "context": "In our experiments, we consider two baselines: - The first (denoted Classic-TF_IDF) corresponds to a classical index based on tf*idf weighted keywords, - The second (denoted Classic-BM25) is a classical index based on Okapi-BM25weighted keywords [34].", "startOffset": 246, "endOffset": 250}], "year": 2013, "abstractText": "Traditional information retrieval systems rely on keywords to index documents and queries. In such systems, documents are retrieved based on the number of shared keywords with the query. This lexicalfocused retrieval leads to inaccurate and incomplete results when different keywords are used to describe the documents and queries. Semantic-focused retrieval approaches attempt to overcome this problem by relying on concepts rather than on keywords to indexing and retrieval. The goal is to retrieve documents that are semantically relevant to a given user query. This paper addresses this issue by proposing a solution at the indexing level. More precisely, we propose a novel approach for semantic indexing based on concepts identified from a linguistic resource. In particular, our approach relies on the joint use of WordNet and WordNetDomains lexical databases for concept identification. Furthermore, we propose a semantic-based concept weighting scheme that relies on a novel definition of concept centrality. The resulting system is evaluated on the TIME test collection. Experimental results show the effectiveness of our proposition over traditional IR approaches.", "creator": "Microsoft Office Word"}}}