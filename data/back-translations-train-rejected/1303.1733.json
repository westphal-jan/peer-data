{"id": "1303.1733", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Mar-2013", "title": "Multi-relational Learning Using Weighted Tensor Decomposition with Modular Loss", "abstract": "We propose a modular framework for multi-relational learning via tensor decomposition. In our learning setting, the training data contains multiple types of relationships among a set of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries. To do so, we model each relationship as a function of a linear combination of latent factors. We learn this latent representation by computing a low-rank tensor decomposition, using quasi-Newton optimization of a weighted objective function. Sparsity in the observed data is captured by the weighted objective, leading to improved accuracy when training data is limited. Exploiting sparsity also improves efficiency, potentially up to an order of magnitude over unweighted approaches. In addition, our framework accommodates arbitrary combinations of smooth, task-specific loss functions, making it better suited for learning different types of relations. For the typical cases of real-valued functions and binary relations, we propose several loss functions and derive the associated parameter gradients. We evaluate our method on synthetic and real data, showing significant improvements in both accuracy and scalability over related factorization techniques.", "histories": [["v1", "Thu, 7 Mar 2013 16:10:44 GMT  (1086kb,D)", "https://arxiv.org/abs/1303.1733v1", null], ["v2", "Fri, 31 May 2013 21:09:20 GMT  (1086kb,D)", "http://arxiv.org/abs/1303.1733v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ben london", "theodoros rekatsinas", "bert huang", "lise getoor"], "accepted": false, "id": "1303.1733"}, "pdf": {"name": "1303.1733.pdf", "metadata": {"source": "CRF", "title": "Multi-relational Learning Using Weighted Tensor Decomposition with Modular Loss", "authors": ["Ben London", "Theodoros Rekatsinas", "Bert Huang"], "emails": ["blondon@cs.umd.edu", "thodrek@cs.umd.edu", "bert@cs.umd.edu", "getoor@cs.umd.edu"], "sections": [{"heading": null, "text": "In our learning environment, the training data contain several types of relationships between a number of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries by modelling each relationship as a function of a linear combination of latent factors. We learn this latent representation by calculating a low tensor using a quasi-Newton optimization of a weighted objective function. Thrift in the observed data is captured by the weighted target, which results in improved accuracy when the training data is limited. Taking advantage of frugality also improves efficiency, potentially up to an order of magnitude compared with unweighted approaches. In addition, our framework takes into account arbitrary combinations of smooth task-specific loss functions, making it more appropriate to learn different types of relationships. In typical cases of real functions and binary relationships, we demonstrate multiple loss functions and derive the associated combinations of both our task-specific loss function and actual loss function results."}, {"heading": "1 Introduction", "text": "In network or relational data, one often finds multiple types of relationships on a set of objects. For example, in social networks, relationships between individuals can be personal, familial or professional. We refer to this type of data as multi-relational. In this paper, we propose a tensor decomposition model for the transduction of onmulti-relational data. We consider a scenario in which we obtain a fixed set of objects, a set of relationships and a small training set, sampled from the full set of all potential paired relationships; our goal is to predict the unobserved relationships. The relationships we are looking at may be binary, discrete ordinary or real valued functions of the object pairs; for the binary rated relationships, the training labels include both positive and negative examples. There has been a growing interest in tensor methods within machine learning, partly due to their natural representation of multirelational data (Kashima et al). Many posts (Dunlaal al, 2006, Xio, 2011)."}, {"heading": "2 Preliminaries", "text": "This section introduces our notation and defines the problem of multirelational transduction. We denote tensors and matrices with bold uppercase letters; similarly, we use bold lowercase letters to denote vectors; for a tensor X, we denote xi, j, k the (i, j) th element of the kth frontal disk; with Xk, we denote the matrix containing the kth frontal disk; with respect to V.Fix, we denote the gradient of f a series of m objects and a series of n relations.1 To simplify our analysis, we assume that all relationships are symmetrical, although an analog derivative for asymmetric relationships can be obtained with only slightly more work."}, {"heading": "3 Proposed Method", "text": "In this section, we present our proposed method, which we call the multi-relational weighted tensor decom-1. In this section, we use the term relation loose to include not only strict relationships where relationships either exist or not, but also real value functions. 2For example, the zero value for binary weighted relationships in {\u00b1 1} is 0, position (MrWTD). We start by describing our tensor representation of multirelational data. We then define an optimization target used to calculate this representation and discuss how to solve the optimization."}, {"heading": "3.1 Representation as Tensor Decomposition", "text": "Our basic assumption is that any relationship is the same as an assignment applied to an element xi, j, k. For example, for binary relationships in {\u00b1 1}, \u03a6k is the drawing function. We also assume that any Xk can be considered as a rank-r decomposition, but n instances of Rk > + bk, (1) where A-Rm, Rk-Rr-r, and bk-R. (Figure 1 illustrates this decomposition.) Note that there is a single A matrix, but n instances of Rk and bk. Also note that we do not impose any constraints on A or Rk; the columns of A do not need to be independently linked, and Rk does not need to be positive-semidefinite. To derive the values of the missing (or uncertain) entries, we predict each yi object, j-Rk, the row, the factors of aij > i-k, the composition of objects, and the interexactity of Rexk."}, {"heading": "3.2 Related Models", "text": "Our tensor model is comparable to Harshman's DEDICOM (1978). Bader et al. (2007) applied the DEDICOM model to the task of time left prediction (in a single network) using the third mode as the time dimension. Recently, Nickel et al. (2011) proposed a relaxed DEDICOM, called RESCAL, to solve several canonical multirelational learning tasks. Of the previous approaches, our underlying decomposition is most similar to RESCAL, and equation 1 would be identical to RESCAL decomposition without the bias term. Beyond decomposition, the main difference is that RESCAL directly disassembles the input tensor, rather than mapping X to Y. RESCAL also ignores the potential sparseness and uncertainty in the observations, while we explicitly state in Section 4 that our formulation produces more precise predictions, even if they are inconsistent in composition (although they are)."}, {"heading": "3.3 Objective", "text": "To calculate the definition of specific prediction tasks, we minimize the following regularized targets: f (A, R, B), p (A), p (A), p (A), p (A), p (A), p (A), p), p (A), p), p (A), p), p (A), p), p (B), p), p (B), p (A), p), p (B), p (B), p (B)."}, {"heading": "3.4 Weighting and Efficiency", "text": "The weighting tensor W is a particularly important component of our framework. Without W, the objective function would attach the same importance to the adjustment of both observed and unobserved values. If the observed tensor is very sparse (as is often the case in real training data), this will result in the application of a large number of \"phantom zeros.\" This approach is similar to Acar et al. \"s (2010), although their analysis is limited to minimizing the square loss for CP decomposition. Weighting the object by W also improves efficiency. If W is sparse, the objective and gradient calculations are fairly lightweight, because any expression that includes W can be arithmetic."}, {"heading": "3.5 Optimization", "text": "To minimize the goal in Eq.2, we use limited memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) optimization methods. Since quasiNewton methods like L-BFGS avoid calculating the Hessian, they are efficient for optimization problems with many variables. All of this requires objective function and gradients in equations 3-5. Since our optimization problem is not convex, we cannot guarantee that L-BFGS will find the global minimum, but in practice, the algorithm typically finds useful, though possibly local, minima.To mize the possibility of detecting local minima, we initialize the parameters using the intrinsic composition of each input section, which is close to the desired factorization. This technique is similar to the initialization used by Bader et al. (2007) and Nickel et al. (2011), which have similar compositions."}, {"heading": "4 Experiments", "text": "In this section, we compare the variants of MrWTD with RESCAL (Nickel et al., 2011), MMMF (Rennie and Srebro, 2005a) and Bayesian probabilistic tensor factorization (BPTF) (Xiong et al., 2010) in several experiments using real and synthetic data. The real data sources are kinship data from the Australian Alyawarra tribe and two social interaction datasets from the MIT Media Lab. The comparisons highlight the key advantages of MrWTD: namely, the ability to learn from limited training data, to deal with a mix of learning goals, to transmit information about relationships for collective learning, and to exploit the scarcity to improve efficiency. To test the effect of the ranking parameter, we conduct an experiment that varies only the rank of decomposition across a range of values. The results support our hypothesis that L2 regulation reduces the effects of ranking and effectively controls the model."}, {"heading": "4.1 Compared Methods", "text": "The variant called MrWTD-Q uses the square loss for all relationships, regardless of their nature. MrWTD-H and MrWTD-L therefore use the square loss for real-valued slices and the smooth loss, respectively, for binary slices.The RESCAL model approaches each part of the input tensor as Yk-ARkA >. In (Nickel et al., 2011), binary relationships are represented by {0, 1}. Unfortunately, since RESCAL does not take into account that missing relationships are simply treated as negative examples. To distinguish between observed relationships and negative examples, we use {\u00b1 1} for observed data and zeros elsewhere. In our experiments, we find that this modification improves RESCAL's performance over the original method. Since RESCAL compares the square loss universally."}, {"heading": "4.2 Synthetic Data Experiments", "text": "In order to generate the real data, we construct a real relation, which we again put at 500 million euros. \"We have not made it,\" he says, \"but we have made it.\" \"We have made it.\" \"We have made it.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \".\" \"\" We. \"\" \"\". \"\" \"\" \".\" \"\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \"\" \".\" \"\" \"\". \"\" \".\". \"\". \".\" \".\" \"\" \".\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \".\". \".\". \".\". \"\". \".\". \"\" \".\" \"\". \"\". \"\" \".\" \".\" \"\" \"\" \".\". \".\". \".\". \".\" \"\". \"\". \"\". \"\" \"\" \".\". \"\" \"\" \"\" \"\". \".\" \".\" \"\" \".\". \"\" \"\". \".\" \"\" \".\". \"\". \"\" \"\". \"\". \".\" \".\". \"\" \"\" \"\". \"\". \".\". \".\" \"\". \"\". \"\" \".\". \".\" \".\". \".\". \"\" \".\". \"\" \"\". \"\" \"\". \".\". \".\" \".\". \"\". \".\" \".\" \".\" \".\". \".\". \".\" \"\". \".\". \"\" \"\" \"\". \".\" \".\" \"\". \"\". \"\". \"\". \"\" \"\" \".\" \"\". \".\" \"\" \".\" \".\" \".\". \"\" \"\". \".\" \"\" \".\". \".\" \"\" \"\". \"\" \".\" \".\" \".\". \".\". \".\". \"\". \"\" \".\" \".\" \".\". \"\""}, {"heading": "4.3 Real Data Experiments", "text": "The first dataset consists of relationship data of the Australian Alyawarra tribe as recorded by Denham and White (2005), previously used by Kemp and al. (2006) for multirelational link forecasting, containing m = 104 tribal members and n = 23 types of relationship (binary). In total, the dataset includes 125,580 related pairs, resulting in a tensor Y [\u00b1 1} 104 \u00d7 104 \u00d7 23. The remaining datasets come from the human Dy-8We measuring statistical significance in all experiments using a 2-sample t test with reject threshold 05.9The original data contains 26 relationships, but the 24-26 relationships are extremely sparse, with fewer than 6 instances shown, so we exclude them."}, {"heading": "4.4 Rank Experiment", "text": "To measure the influence of the rank parameter on the performance of each algorithm, we re-run the experiment of social evolution, varying r = {5, 10, 20, 40} and leaving the training ratio at 25%. Results of this experiment are shown in Figure 5 in the appendix. There is a small increase in AUC from r = 5 to r = 10, which is expected since 5 is relatively low. However, we note that the effect of the rank for r \u2265 10 is minimal; the standard deviation in all runs in this range is < 0.02 for each algorithm. This supports our hypothesis that the regulator above a certain threshold is the primary regulator of model complexity."}, {"heading": "4.5 Timing Experiment", "text": "Finally, we measure the runtime of each of the above tensor methods to better understand their scalability in scenarios where the training data is limited. We create a sequence of synthetic data sets (using the technique in Section 4.2), each with n = 3 binary sections for the quantities m = {500, 1000, 2000, 4000, 8000}. For training, we use random 10% of the tensor. We compare the variant of smooth hinge loss of MrWTD, RESCAL and BPTF using predefined regulation and hyperparameters. We run these experiments on a machine with two 6-core Intel R \u00a9 Xeon R \u00a9 X5650 processors with 2.66 GHz and 48 GB of RAM. The temporal results, averaged over 10 passes per problem size, are shown in Figure 4. BPTF takes considerably more time than the others, since the sampling imation of Gibbs \"BTF BTF cannot apply the problem to the problem because the SALS function is equal to two times, and the SALS function can only apply two times."}, {"heading": "5 Conclusion", "text": "The decomposition we use provides an intuitive interpretation for the multirelational domain, where objects have global latent representations and relationships are determined by a function of their linear combinations; we show that global latent representations allow the transfer of information between relationship types during model estimation; we show that the weighted target and support for multiple loss functions in our framework improves accuracy over similar models; and finally, we show that our method exploits the scarcity of limited training data to achieve an order of magnitude of acceleration over unweighted methods; we also plan to expand MrWTD to learn from large-scale data by adapting hash methods from matrix factorization literature (Karatzoglou et al., 2010); we would like to compare our method with Sutskever et al.'s BTF algorithm (2009) to further investigate the benefits of matrix factorization literature (Karatzoglou, 2010)."}, {"heading": "Acknowledgements", "text": "This work was partially supported by NSF-CAREER grant 0746930 and NSF grant IIS1218488."}, {"heading": "A Supplementary Material", "text": "Figure 5 shows the results of the range experiment (Section 4.4). Table 1 shows the results of the timing experiment (Section 4.5). In Table 2 and Table 3 we list the complete results of the synthetic (Section 4.2) and real experiments (Section 4.3)."}], "references": [{"title": "Scalable tensor factorizations with missing data", "author": ["E. Acar", "D. Dunlavy", "T. Kolda", "M. M\u00f8rup"], "venue": "In Proc. of the 2010 SIAM International Conf. on Data Mining (SDM),", "citeRegEx": "Acar et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Acar et al\\.", "year": 2010}, {"title": "Temporal analysis of semantic graphs using ASALSAN", "author": ["B. Bader", "R. Harshman", "T. Kolda"], "venue": "In Proc. of the 7th IEEE International Conf. on Data Mining (ICDM),", "citeRegEx": "Bader et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bader et al\\.", "year": 2007}, {"title": "Stability of transductive regression algorithms", "author": ["C. Cortes", "M. Mohri", "D. Pechyony", "A. Rastogi"], "venue": "In Proc. of the 25th International Conf. on Machine Learning (ICML),", "citeRegEx": "Cortes et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2008}, {"title": "Multiple measures of Alyawarra kinship", "author": ["W. Denham", "D. White"], "venue": "Field Methods,", "citeRegEx": "Denham and White.,? \\Q2005\\E", "shortCiteRegEx": "Denham and White.", "year": 2005}, {"title": "Multilinear algebra for analyzing data with multiple linkages", "author": ["D. Dunlavy", "T. Kolda", "W. Kegelmeyer"], "venue": "Technical Report,", "citeRegEx": "Dunlavy et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Dunlavy et al\\.", "year": 2006}, {"title": "Temporal link prediction using matrix and tensor factorizations", "author": ["D. Dunlavy", "T. Kolda", "E. Acar"], "venue": "ACM Trans. on Knowledge Discovery from Data,", "citeRegEx": "Dunlavy et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Dunlavy et al\\.", "year": 2011}, {"title": "Inferring friendship network structure by using mobile phone data", "author": ["N. Eagle", "A. Pentland", "D. Lazer"], "venue": "Proc. of the National Academy of Sciences,", "citeRegEx": "Eagle et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Eagle et al\\.", "year": 2009}, {"title": "Transductive Rademacher complexity and its applications", "author": ["R. El-Yaniv", "D. Pechyony"], "venue": "J. Artificial Intelligence Research (JAIR),", "citeRegEx": "El.Yaniv and Pechyony.,? \\Q2009\\E", "shortCiteRegEx": "El.Yaniv and Pechyony.", "year": 2009}, {"title": "Link pattern prediction with tensor decomposition in multirelational networks", "author": ["S. Gao", "L. Denoyer", "P. Gallinari"], "venue": "In IEEE Symposium on Comp. Intell. and Data Mining,", "citeRegEx": "Gao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2011}, {"title": "Models for analysis of asymmetrical relationships", "author": ["R. Harshman"], "venue": "In First Joint Meeting of the Psychometric Society and the Society for Mathematical Psychology,", "citeRegEx": "Harshman.,? \\Q1978\\E", "shortCiteRegEx": "Harshman.", "year": 1978}, {"title": "Collaborative filtering on a budget", "author": ["A. Karatzoglou", "A. Smola", "M. Weimer"], "venue": "In Proc. of the 13th International Conf. on Artificial Intelligence and Statistics,", "citeRegEx": "Karatzoglou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Karatzoglou et al\\.", "year": 2010}, {"title": "Link propagation: a fast semisupervised learning algorithm for link prediction", "author": ["H. Kashima", "T. Kato", "Y. Yamanishi", "M. Sugiyama", "K. Tsuda"], "venue": "In SIAM International Conference on Data Mining (SDM),", "citeRegEx": "Kashima et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kashima et al\\.", "year": 2009}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["C. Kemp", "J. Tenenbaum", "T. Griffiths", "T. Yamada", "N. Ueda"], "venue": "In Proc. of the 21st National Conf. on Artificial Intelligence,", "citeRegEx": "Kemp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Kemp et al\\.", "year": 2006}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["M. Nickel", "V. Tresp", "H. Kriegel"], "venue": "In Proc. of the 28th International Conf. on Machine Learning (ICML),", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Fast maximum margin matrix factorization for collaborative prediction", "author": ["J. Rennie", "N. Srebro"], "venue": "In In Proc. of the 22nd International Conf. on Machine Learning (ICML),", "citeRegEx": "Rennie and Srebro.,? \\Q2005\\E", "shortCiteRegEx": "Rennie and Srebro.", "year": 2005}, {"title": "Loss functions for preference levels: regression with discrete ordered labels", "author": ["J. Rennie", "N. Srebro"], "venue": "In IJCAI Multidisciplinary Workshop on Adv. in Preference Handling,", "citeRegEx": "Rennie and Srebro.,? \\Q2005\\E", "shortCiteRegEx": "Rennie and Srebro.", "year": 2005}, {"title": "Generalization error bounds for collaborative prediction with lowrank matrices", "author": ["N. Srebro", "N. Alon", "T. Jaakkola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}, {"title": "Maximummargin matrix factorization", "author": ["N. Srebro", "J. Rennie", "T. Jaakkola"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Srebro et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Srebro et al\\.", "year": 2005}, {"title": "Modelling relational data using Bayesian clustered tensor factorization", "author": ["I. Sutskever", "R. Salakhutdinov", "J. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Sutskever et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2009}, {"title": "Temporal collaborative filtering with Bayesian probabilistic tensor factorization", "author": ["L. Xiong", "X. Chen", "T. Huang", "J. Schneider", "J. Carbonell"], "venue": "In SIAM International Conference on Data Mining (SDM),", "citeRegEx": "Xiong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 11, "context": "There has been a growing interest in tensor methods within machine learning, partially due to their natural representation of multi-relational data (Kashima et al., 2009).", "startOffset": 148, "endOffset": 170}, {"referenceID": 8, "context": "Many contributions (Dunlavy et al., 2006, 2011; Gao et al., 2011; Xiong et al., 2010) use the canonical polyadic (CP) decomposition, a generalization of singular value decomposition to tensors.", "startOffset": 19, "endOffset": 85}, {"referenceID": 19, "context": "Many contributions (Dunlavy et al., 2006, 2011; Gao et al., 2011; Xiong et al., 2010) use the canonical polyadic (CP) decomposition, a generalization of singular value decomposition to tensors.", "startOffset": 19, "endOffset": 85}, {"referenceID": 1, "context": "Others (Bader et al., 2007) have proposed models based on decomposition into directional components (DEDICOM) (Harshman, 1978).", "startOffset": 7, "endOffset": 27}, {"referenceID": 9, "context": ", 2007) have proposed models based on decomposition into directional components (DEDICOM) (Harshman, 1978).", "startOffset": 90, "endOffset": 106}, {"referenceID": 13, "context": "We propose a similar decomposition (based on (Nickel et al., 2011)) which is more appropriate for multi-relational data, for reasons discussed in Section 3.", "startOffset": 45, "endOffset": 66}, {"referenceID": 8, "context": "Our tensor model is comparable to Harshman\u2019s DEDICOM (1978). Bader et al.", "startOffset": 34, "endOffset": 60}, {"referenceID": 1, "context": "Bader et al. (2007) applied the DEDICOM model to the task of temporal link prediction (in a single network), using the third mode as the time dimension.", "startOffset": 0, "endOffset": 20}, {"referenceID": 1, "context": "Bader et al. (2007) applied the DEDICOM model to the task of temporal link prediction (in a single network), using the third mode as the time dimension. Recently, Nickel et al. (2011) proposed a relaxed DEDICOM, referred to as RESCAL, to solve several canonical multi-relational learning tasks.", "startOffset": 0, "endOffset": 184}, {"referenceID": 8, "context": "Other tensor factorization models have been proposed for multi-relational data, though they typically use the CP decomposition (Dunlavy et al., 2006, 2011; Gao et al., 2011; Xiong et al., 2010).", "startOffset": 127, "endOffset": 193}, {"referenceID": 19, "context": "Other tensor factorization models have been proposed for multi-relational data, though they typically use the CP decomposition (Dunlavy et al., 2006, 2011; Gao et al., 2011; Xiong et al., 2010).", "startOffset": 127, "endOffset": 193}, {"referenceID": 4, "context": "Other tensor factorization models have been proposed for multi-relational data, though they typically use the CP decomposition (Dunlavy et al., 2006, 2011; Gao et al., 2011; Xiong et al., 2010). In the CP decomposition, each entry is the inner product of three vectors; this would be similar to our decomposition if each Rk slice were constrained to be diagonal. The richer interactions of the relaxed DEDICOM and the global latent representation of the objects often make it better suited for multi-relational learning, as was corroborated empirically by Nickel et al. (2011).", "startOffset": 128, "endOffset": 577}, {"referenceID": 0, "context": "This approach is similar to Acar et al.\u2019s (2010), though their analysis is limited to the minimizing the quadratic loss for a CP decomposition.", "startOffset": 28, "endOffset": 49}, {"referenceID": 1, "context": "This technique is similar to the initialization used by Bader et al. (2007) and Nickel et al.", "startOffset": 56, "endOffset": 76}, {"referenceID": 1, "context": "This technique is similar to the initialization used by Bader et al. (2007) and Nickel et al. (2011), which have similar decompositions.", "startOffset": 56, "endOffset": 101}, {"referenceID": 1, "context": "Note that when the objective function uses only quadratic loss, one can compute the parameter updates using the alternating simultaneous approximation, least squares and Newton (ASALSAN) algorithm (Bader et al., 2007), which produces an approximate solution and has been shown to converge quickly.", "startOffset": 197, "endOffset": 217}, {"referenceID": 13, "context": "In this section, we compare variants of MrWTD with RESCAL (Nickel et al., 2011), MMMF (Rennie and Srebro, 2005a) and Bayesian probabilistic tensor factorization (BPTF) (Xiong et al.", "startOffset": 58, "endOffset": 79}, {"referenceID": 19, "context": ", 2011), MMMF (Rennie and Srebro, 2005a) and Bayesian probabilistic tensor factorization (BPTF) (Xiong et al., 2010) in several experiments, using both real and synthetic data.", "startOffset": 96, "endOffset": 116}, {"referenceID": 13, "context": "In (Nickel et al., 2011), binary relationships are represented by {0, 1}.", "startOffset": 3, "endOffset": 24}, {"referenceID": 14, "context": "The \u201cfast\u201d variant of the algorithm (Rennie and Srebro, 2005a) adds a bias term and uses the smooth hinge loss. The optimization objective is very similar to ours, but with different gradients, due to the decomposition. Our implementation of fast MMMF differs from that of Rennie and Srebro (2005a) only in the way we solve the optimization (using L-BFGS, rather than conjugate gradient descent) and the fact that the input is assumed to be symmetric.", "startOffset": 37, "endOffset": 299}, {"referenceID": 3, "context": "The first dataset consists of kinship data from the Australian Alyawarra tribe, as recorded by Denham and White (2005). This data has previously been used by Kemp et al.", "startOffset": 95, "endOffset": 119}, {"referenceID": 3, "context": "The first dataset consists of kinship data from the Australian Alyawarra tribe, as recorded by Denham and White (2005). This data has previously been used by Kemp et al. (2006) for multi-relational link prediction.", "startOffset": 95, "endOffset": 177}, {"referenceID": 6, "context": "From the first dataset, named Reality Mining (Eagle et al., 2009), we use the survey-annotated network, consisting of n = 3 types of binary relationships annotated by the subjects: friendship, in-lab interaction and out-of-lab interaction.", "startOffset": 45, "endOffset": 65}, {"referenceID": 2, "context": "We also intend to analyze the theoretical properties of our framework, such as generalization error, using existing learning theory literature (Srebro et al., 2005a; Cortes et al., 2008; El-Yaniv and Pechyony, 2009).", "startOffset": 143, "endOffset": 215}, {"referenceID": 7, "context": "We also intend to analyze the theoretical properties of our framework, such as generalization error, using existing learning theory literature (Srebro et al., 2005a; Cortes et al., 2008; El-Yaniv and Pechyony, 2009).", "startOffset": 143, "endOffset": 215}, {"referenceID": 14, "context": "We would also like to compare our method to Sutskever et al.\u2019s BTF algorithm (2009), to further investigate the benefit of the Bayesian approach.", "startOffset": 44, "endOffset": 84}], "year": 2013, "abstractText": "We propose a modular framework for multirelational learning via tensor decomposition. In our learning setting, the training data contains multiple types of relationships among a set of objects, which we represent by a sparse three-mode tensor. The goal is to predict the values of the missing entries. To do so, we model each relationship as a function of a linear combination of latent factors. We learn this latent representation by computing a low-rank tensor decomposition, using quasi-Newton optimization of a weighted objective function. Sparsity in the observed data is captured by the weighted objective, leading to improved accuracy when training data is limited. Exploiting sparsity also improves efficiency, potentially up to an order of magnitude over unweighted approaches. In addition, our framework accommodates arbitrary combinations of smooth, task-specific loss functions, making it better suited for learning different types of relations. For the typical cases of real-valued functions and binary relations, we propose several loss functions and derive the associated parameter gradients. We evaluate our method on synthetic and real data, showing significant improvements in both accuracy and scalability over related factorization techniques.", "creator": "TeX"}}}