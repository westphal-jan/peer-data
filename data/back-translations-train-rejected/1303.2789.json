{"id": "1303.2789", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2013", "title": "A Cooperative Q-learning Approach for Real-time Power Allocation in Femtocell Networks", "abstract": "In this paper, we address the problem of distributed interference management of cognitive femtocells that share the same frequency range with macrocells (primary user) using distributed multi-agent Q-learning. We formulate and solve three problems representing three different Q-learning algorithms: namely, centralized, distributed and partially distributed power control using Q-learning (CPC-Q, DPC-Q and PDPC-Q). CPCQ, although not of practical interest, characterizes the global optimum. Each of DPC-Q and PDPC-Q works in two different learning paradigms: Independent (IL) and Cooperative (CL). The former is considered the simplest form for applying Qlearning in multi-agent scenarios, where all the femtocells learn independently. The latter is the proposed scheme in which femtocells share partial information during the learning process in order to strike a balance between practical relevance and performance. In terms of performance, the simulation results showed that the CL paradigm outperforms the IL paradigm and achieves an aggregate femtocells capacity that is very close to the optimal one. For the practical relevance issue, we evaluate the robustness and scalability of DPC-Q, in real time, by deploying new femtocells in the system during the learning process, where we showed that DPC-Q in the CL paradigm is scalable to large number of femtocells and more robust to the network dynamics compared to the IL paradigm", "histories": [["v1", "Tue, 12 Mar 2013 07:00:04 GMT  (1500kb)", "http://arxiv.org/abs/1303.2789v1", null]], "reviews": [], "SUBJECTS": "cs.MA cs.LG", "authors": ["hussein saad", "amr mohamed", "tamer elbatt"], "accepted": false, "id": "1303.2789"}, "pdf": {"name": "1303.2789.pdf", "metadata": {"source": "CRF", "title": "A Cooperative Q-learning Approach for Real-time Power Allocation in Femtocell Networks", "authors": ["Hussein Saad", "Amr Mohamed"], "emails": ["hussein.saad@nileu.edu.eg", "amrm@qu.edu.qa", "telbatt@nileuniversity.edu.eg"], "sections": [{"heading": null, "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "II. SYSTEM MODEL", "text": "We consider a wireless network consisting of a macro cell (with a single transmit and receive antenna Macro Base Station (MBS)) that coexists with Nf femto cells, each having a single transmit and receive antenna Femto Base Station (FBS). Nf femto cells happen to be located within the macro and femto cell carrier. Femto cells within the same range can exchange partial information during the learning process to improve their performance. p (k) o and p (k) n denote the transmission powers of the MBS and FBS n on subcarrier k respectively the maximum transmit power (oo) Femto cells within the same range can exchange partial information during the learning process. p (k) o and p (k) n denote the transmission powers of the subcarrier k or FBS n respectively. Furthermore, the maximum transmit cell power femto (femto) refers to the transmit carrier (oo)."}, {"heading": "III. MULTI-AGENT Q-LEARNING", "text": "The scenario of distributed cognitive femtocells can be mathematically formulated with the help of stochastic games when Q-Q results are performed, where the learning process of each femtocell is described by a task defined by the quintile {N, S, A, P, R (s, ~ a)}, where: \u2022 N = {1, 2, \u00b7, Nf} is the amount of agents (i.e. femtocells). \u2022 S = \u2212 s2, \u00b7, sm} is the number of possible states that each agent can occupy, where m is the number of possible states. \u2022 A = {a1, a2, \u00b7, \u00b7, al} is the amount of possible actions that each agent can perform for each task, where l is the number of possible actions. \u2022 P is the probable transition function that defines the probability that an agent will transfer from one state to another, since the joint action is performed by all agents. \u2022 R (a) is the reward agent that determines the agent sent back to the agent."}, {"heading": "IV. POWER ALLOCATION USING Q-LEARNING", "text": "In this section, the three proposed Q-Learning-based power distribution algorithms are presented:"}, {"heading": "A. Distributed Power Control Using Q-learning (DPC-Q)", "text": "DPC-Q is a distributed algorithm in which multiple agents (i.e: femtocells) aim at suboptimal decision-making (i.e: power allocation) by repeatedly interacting with the environment. The DPC-Q algorithm is proposed in two different learning paradigms: \u2022 Independent Learning (IL): In this paradigm, each agent learns independently of other agents (i.e: ignores the actions of other agents and considers other agents as part of the environment). Although this can lead to oscillations and convergence problems, the IL paradigm has shown good results in many applications [8]. \u2022 Cooperative Learning (CL): In this paradigm, each agent shares a portion of its Q table with all other cooperating agents1, which aims to improve the performance of femtocells. CL is performed as follows: Each agent shares the series of its Q table corresponding to its current state with all other cooperating agents."}, {"heading": "B. Partially Distributed Power Control Using Q-learning (PDPC-Q)", "text": "PDPC-Q is a partially distributed algorithm, which is a multi-agent algorithm, but only agent-dependent (i.e. the states, actions and reward functions are defined for each agent across all subcarriers).As a DPC-Q, PDPC-Q works in both IL and CL paradigms. The agents, states, actions and reward functions used for the PDPC-Q algorithm are defined as follows: \u2022 Agents: FBSn, \u01921 \u2264 n \u2264 Nf \u2022 States: At the time t, the state is defined as: st = {It} whereIt {0, 1} indicates the level of interference measured by the macro user across all subcarriers at the time t."}, {"heading": "C. Centralized Power Control Using Q-learning (CPC-Q)", "text": "CPC-Q is a centralized performance control algorithm used to evaluate the performance of our proposed DPC-Q algorithm. CPC-Q can be considered a single-agent version of the DPC-Q, and therefore its convergence with the optimal Q values and thus optimal powers is guaranteed. However, using a centralized controller in multi-agent scenarios is not practical. Therefore, CPC-Q only works for small problems. The agent, states, actions and reward functions used for CPC-Q are defined as follows: \u2022 Agents: A centralized controller. \u2022 States: The same as PDPC-Q. \u2022 Actions: For the central controller, the action set is defined as matrices in which each matrix represents the forces of all femtocells across all subcarriers. However, the size of this set exponentially increases with the number of femtocells and the number of subcarriers."}, {"heading": "V. PERFORMANCE EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Simulation Scenario", "text": "We consider a wireless network consisting of a macro cell that serves Um = 1 macro user, supported by Nf femtocells. Each femtocell serves Uf = 1 femtouser, who happens to be in the femtocell coverage area. All macro2In the simulations, the forces used to form the matrices and vectors in CPC-Q or PDPC-Q share the same frequency band, which is composed of K subcarriers, assuming orthogonal downlink transmission. In the simulations, K changes according to the algorithm used: for DPC-Q, K = 6, while for both CPC-Q and PDPC-Q, K = 3. The channel gain between each transmitter i and each receiver j on subcarrier-K is assumed to be the maximum transmission rate of K."}, {"heading": "B. Numerical Results", "text": "This year, it will be able to leave the country to return to the EU, where it will be able to integrate into the EU."}, {"heading": "VI. CONCLUSION", "text": "In this paper, three Q-learning-based algorithms for cognitive femtocells are presented: DPC-Q, CPC-Q and PDPC-Q. Although DPC-Q has been presented in previous work, this paper expands it in its two learning paradigms: IL and CL to evaluate its performance, robustness and scalability. In terms of performance, DPC-Q is extended to PDPC-Q and then compared to CPC-Q, where simulations showed that the CL paradigm outperforms IL and achieves an overall capacity of femtocells that is very close to optimum. In terms of robustness, the CL paradigm was found to be much more robust compared to the use of new femtocells during the learning process, with the results showing that the CL paradigm outperforms the IL paradigm in terms of convergence, 2) learns better (i.e. responds better to dynamics when dynamics are used particularly well when convergence is not used)."}, {"heading": "ACKNOWLEDGMENT", "text": "This work is supported by the Qatar Telecom (Qtel) Grant No. QUEX-Qtel-09 / 10-10."}], "references": [{"title": "Femtocell networks: a survey", "author": ["V. Chandrasekhar", "J. Andrews", "A. Gatherer"], "venue": "Communications Magazine, IEEE, vol. 46, September 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Femtocells: Opportunities and Challenges for Business and Technology", "author": ["S. Saunders", "S. Carlaw", "A. Giustina"], "venue": "Great Britain: John Wiley and Sons Ltd,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Open vs closed access femtocells in the uplink", "author": ["P. Xia", "V. Chandrasekhar", "J.G. Andrews"], "venue": "CoRR, vol. abs/1002.2964, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Cambridge MA, MIT press,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Technical note Q-learning", "author": ["J.C.H. Watkins", "P. Dayan"], "venue": "Journal of Machine Learning, vol. 8, 1992.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Collaborative multiagent reinforcement learning by payoff propagation", "author": ["J.R. Kok", "N. Vlassis"], "venue": "J. Mach. Learn. Res., vol. 7, December 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Cognition and docition in OFDMA-based femtocell networks", "author": ["A. Galindo-Serrano", "L. Giupponi", "M. Dohler"], "venue": "Proceeding of the IEEE Global Telecommunications Conference (GLOBECOM), may 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed Q-learning for interference control in OFDMA-based femtocell networks", "author": ["A. Galindo-Serrano", "L. Giupponi"], "venue": "Proceedings of the 71st IEEE Vehicular Technology Conference, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Distributed cooperative Qlearning for power allocation in cognitive femtocell networks", "author": ["H. Saad", "A. Mohamed", "T. ElBatt"], "venue": "Proceedings of the IEEE 76th Vehicular Technology Conference, Sep. 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Labeled initialized adaptive play qlearning for stochastic games", "author": ["A. Burkov", "B. Chaib-draa"], "venue": "Proceedings of the AAMAS\u201907 Workshop on Adaptive and Learning Agents (ALAg\u201907), May 2007.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "Convergence of Q-learning: A simple proof", "author": ["F.S. Melo"], "venue": "Institute Of Systems and Robotics, Tech. Rep., 2001.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "One of the most daunting challenges is their interference on macro-users and other femtocells [1], [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "One of the most daunting challenges is their interference on macro-users and other femtocells [1], [2].", "startOffset": 99, "endOffset": 102}, {"referenceID": 2, "context": "Based on these observations, in this paper, we focus on closed access femtocells [3] working in the same bandwidth with macrocells (i.", "startOffset": 81, "endOffset": 84}, {"referenceID": 3, "context": "In order to handle the interference generated by the femtocells on the macrocell users, we will use a distributed reinforcement learning [4] technique called multi-agent Q-learning [5] and [6].", "startOffset": 137, "endOffset": 140}, {"referenceID": 4, "context": "In order to handle the interference generated by the femtocells on the macrocell users, we will use a distributed reinforcement learning [4] technique called multi-agent Q-learning [5] and [6].", "startOffset": 181, "endOffset": 184}, {"referenceID": 5, "context": "In order to handle the interference generated by the femtocells on the macrocell users, we will use a distributed reinforcement learning [4] technique called multi-agent Q-learning [5] and [6].", "startOffset": 189, "endOffset": 192}, {"referenceID": 6, "context": "In such context, Q-Learning offers significant advantages to achieve optimal decision policies through realtime learning of the environment [7].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "In [8], authors used independent learning (IL) Q-learning to perform power allocation in order to control the interference generated by the femtocells on the macrocell user.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "In [7], authors introduced a new concept called docitive femtocells where a new femtocell can fasten its learning process by learning the policies acquired by the already deployed femtocells, instead of learning from scratch.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "In [9], we developed a distributed power allocation algorithm called distributed power control using Q-learning (DPC-Q).", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "However, in [9] we did not evaluate the performance of DPC-Q against the networks dynamics, specially after convergence.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "\u2022 we compare our proposed DPC-Q in both IL and CL paradigms to the idea of docitive femtocells presented in [7].", "startOffset": 108, "endOffset": 111}, {"referenceID": 9, "context": "The scenario of distributed cognitive femtocells can be mathematically formulated using stochastic games [10], where the learning process of each femtocell is described by a task defined by the quintuple {N,S,A, P,R(s,~a)}, where:", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "The Q-value Q(sm, al) is defined to be the expected discounted reward over an infinite time when action al is performed in state sm, and an optimal policy is followed thereafter [5].", "startOffset": 178, "endOffset": 181}, {"referenceID": 4, "context": "the reward function is stationary for each state-action pair) [5], [11].", "startOffset": 62, "endOffset": 65}, {"referenceID": 10, "context": "the reward function is stationary for each state-action pair) [5], [11].", "startOffset": 67, "endOffset": 71}, {"referenceID": 7, "context": "Although, this may lead to oscillations and convergence problems, the IL paradigm showed good results in many applications [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 8, "context": "The main idea behind this strategy is explained in details in [9].", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "the capacities, states, actions, reward functions are defined for each agent over each subcarrier) [9]:", "startOffset": 99, "endOffset": 102}, {"referenceID": 8, "context": "This reward function was compared to the reward function defined in [9]:", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "1 [7] and [9].", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": "1 [7] and [9].", "startOffset": 10, "endOffset": 13}, {"referenceID": 6, "context": "Moreover, in these figures we compare the performance of DPC-Q to the docitive idea presented in [7].", "startOffset": 97, "endOffset": 100}], "year": 2013, "abstractText": "In this paper, we address the problem of distributed interference management of cognitive femtocells that share the same frequency range with macrocells (primary user) using distributed multi-agent Q-learning. We formulate and solve three problems representing three different Q-learning algorithms: namely, centralized, distributed and partially distributed power control using Q-learning (CPC-Q, DPC-Q and PDPC-Q). CPCQ, although not of practical interest, characterizes the global optimum. Each of DPC-Q and PDPC-Q works in two different learning paradigms: Independent (IL) and Cooperative (CL). The former is considered the simplest form for applying Qlearning in multi-agent scenarios, where all the femtocells learn independently. The latter is the proposed scheme in which femtocells share partial information during the learning process in order to strike a balance between practical relevance and performance. In terms of performance, the simulation results showed that the CL paradigm outperforms the IL paradigm and achieves an aggregate femtocells capacity that is very close to the optimal one. For the practical relevance issue, we evaluate the robustness and scalability of DPC-Q, in real time, by deploying new femtocells in the system during the learning process, where we showed that DPC-Q in the CL paradigm is scalable to large number of femtocells and more robust to the network dynamics compared to the IL paradigm.", "creator": "LaTeX with hyperref package"}}}