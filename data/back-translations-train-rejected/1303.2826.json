{"id": "1303.2826", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Mar-2013", "title": "Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA", "abstract": "This article presents a probabilistic generative model for text based on semantic topics and syntactic classes called Part-of-Speech LDA (POSLDA). POSLDA simultaneously uncovers short-range syntactic patterns (syntax) and long-range semantic patterns (topics) that exist in document collections. This results in word distributions that are specific to both topics (sports, education, ...) and parts-of-speech (nouns, verbs, ...). For example, multinomial distributions over words are uncovered that can be understood as \"nouns about weather\" or \"verbs about law\". We describe the model and an approximate inference algorithm and then demonstrate the quality of the learned topics both qualitatively and quantitatively. Then, we discuss an NLP application where the output of POSLDA can lead to strong improvements in quality: unsupervised part-of-speech tagging. We describe algorithms for this task that make use of POSLDA-learned distributions that result in improved performance beyond the state of the art.", "histories": [["v1", "Tue, 12 Mar 2013 10:20:50 GMT  (154kb,D)", "http://arxiv.org/abs/1303.2826v1", "Currently under review for the journal Computational Linguistics"]], "COMMENTS": "Currently under review for the journal Computational Linguistics", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["william m darling", "fei song"], "accepted": false, "id": "1303.2826"}, "pdf": {"name": "1303.2826.pdf", "metadata": {"source": "CRF", "title": "Probabilistic Topic and Syntax Modeling with Part-of-Speech LDA", "authors": ["William Darling", "Fei Song"], "emails": ["william.darling@xrce.xerox.com.", "fsong@uoguelph.ca."], "sections": [{"heading": null, "text": "Probabilistic Topic and Syntax Modeling with Part-of-Speech LDAWilliam Darling \u0445 Xerox Research Centre EuropeFei Song \u043a University of GuelphThis article presents a probabilistic generative model for text based on semantic topics and syntactic classes called Part-of-Speech LDA (POSLDA). POSLDA simultaneously detects syntactic patterns (syntax) and semantic patterns (topics) that exist in document collections, resulting in word distributions that are specific to both topics (sports, education,...) and parts of the language (nouns, verbs,...). For example, multinomic distributions of words that can be understood as \"nouns about the weather\" or \"verbs about the law\" are revealed. We describe the model and an approximate inference algorithm, and then demonstrate the quality of the topics learned both qualitatively and quantitatively. Then we discuss a NLP application that can represent improvements in the POSDA's results beyond the task of learning the POSDA."}, {"heading": "1. Introduction", "text": "The first is the increasing level of access to textual data sources via the Internet, such as classical works of literature (The Gutenberg Project), structured explanatory patterns of the world (Wikipedia) that combine all sorts of terms and concepts (Twitter), and the second is the growth of interest in machine learning algorithms that can exploit the vast amounts of data that are made available and that help to find a sense of language itself, describe machine learning techniques, and contrast them with others. One of the most important of these dichotomies is the division between supervisors and unmonitored learning approaches (Lead, Griffiths, and Jordan 2010)."}, {"heading": "2. Topics and Syntax", "text": "The syntactical sense of a word helps to explain its functional purpose in a sentence, while the semantic meaning is related to its lexical-thematic theme. However, the former is based on short-term dependencies at the sentence level, while the latter realizes its far-reaching dependencies at the document level by focusing on the functional purpose of a word in a sentence. We are therefore interested in exploiting the far-reaching dependencies of words that occur in common. Here, we want to add the short-term dependencies to the model, and we do so by focusing on the functional purpose of a word in a sentence. We are therefore interested in the part of the redecal category that a word - in its given context - belongs to. These include nouns, adjectives, adjectives, attachments, etc. The canonical tool for obsolete word syntax modeling is the hidden Markov model (MDA 1990)."}, {"heading": "3. POSLDA", "text": "While LDA is in a sense a simple extension of probabilistic semantic analysis (Hofmann 2001), this approach is not necessary as it can be considered the first fully generative theme model due to the Dirichlet approach, which is based on the document topic areas that the model de facto depends on specific training data. However, since its inception, LDA has been expanded in many respects, and in particular by questioning the model with additional factors. Word distributions can become more specific if we take into account that generated words depend not only on the current topic, but also on other latent aspects such as the feeling of writing and the personality of the writer or the ideological perspective (Paul and Girju 2010), which allows to detect such word distributions as \"positive / negative words about the weather from the perspective of Americans / Sweden / Australians.\" In fact, this approach is so powerful that it has been implemented in techniques that are easy to add specific factors to theme models (Paul and Dredze 2012)."}, {"heading": "3.1 Relations to Other Models", "text": "The idea of discrete word distributions for the cross-product of topics and classes is related to multi-layered topic models, in which word marks are linked to several latent variables (Paul and Girju 2010; Ahmed and Xing 2010; Paul and Dredze 2012). In such models, words can be explained by both a latent topic and a second (or n) underlying variable such as the author's perspective or dialect, and words may depend on both (or more) factors. In our case, the second variable is the word portion - or functional purpose - of the token. POSLDA is also similar to a newer model called Nested HMM-LDA (nHMMLDA) (Jiang 2009). The model described is very similar to POSLDA, but contains certain limitations. In principle, it is not possible for every word to be generated from any K topic, but all words from a set of an LSEM model must share the same topic, since this is a strong MDA-type of case, as MDA-it is a strong assumption that MDA-1."}, {"heading": "3.2 Approximate Inference", "text": "The main calculation problem in probability topics / syntax models is the subordinate conclusion (Lead and Lafferty 2009). As it is based on LDA and Bayesian HMM, the exact conclusion in the POSLDA model is also unfeasible. Therefore, following many others, we use the MCMC-based approximate inference technique, which requires scanning Gibbs (Griffiths and Steyvers 2004; Heinrich 2004). Here, the multinomic parameters are first integrated and we take the latent variables ci and zi directly into the sample. In POSLDA, the class is called syntactical, then it only depends on the class \u2212 \u2212 the counts n (ci, zi) wi, which correspond to the number of times in which the word wi is assigned to the class ci and the subject zi. Our sample equation is then as follows: p (ci, zi | c \u2212 i, w)."}, {"heading": "4. Experiments and Results", "text": "In this section, we will present a series of experiments on the POSLDA model to demonstrate its capabilities as a theme and syntax model of language. We will demonstrate both qualitatively and quantitatively the model's ability to grasp the semantic and syntactic information axes of a corpus. We will begin qualitatively with the interpretability of topics and then present quantitative results on the ability of POSLDA as a predictive language model. We will then demonstrate its ability as a model for performing unattended POS tagging."}, {"heading": "4.1 Topic Interpretability", "text": "In fact, we are able to go in search of a solution that will enable us to go in search of a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that will enable us to find a solution that we are able to find a solution. \""}, {"heading": "4.2 Quantitative Results", "text": "There are several methods commonly used to evaluate novel probability models in literature (Wallach et al. 2009).The original LDA paper - and many others - uses perplexity, which is a standard variable in the information retrieval literature (Lead, Ng, and Jordan 2003; Teh et al. 2006).A probabilistic model can also be evaluated by considering its performance on an extrinsic task. Here, we first focus on the topic of perplexity and use it to measure the performance of POSLDA as a predictive language model, and then discuss with the model for unsupervised POS tagging.4.1 Predictive Language Modelling. Following standard practice in modeling research (Lead, Ng, and Jordan 2003; Griffiths et al. 2005; Teh et al. 2006; Zhu, Lead, and Lafferty 2006), we adapt a model to education."}, {"heading": "4.3 Unsupervised POS Tagging", "text": "While the results are impressive, unattended approaches continue to fall far short of the accuracy required by introducing semantic approaches as an additional source of information for these task.The word \"seal\" appears preferred in many situations, especially when there is no access to large amounts of training data in a specific area or language. We therefore aim to further improve accuracy with unmonitored approaches by adding an additional source of information for these task.The word \"seal\" appears both as a verb (to seal a large amount of training data in a particular area or language) and the word \"seal\" appears both as a verb (to seal a leak) and as a marine mammoth pinniped in the WSJ treebank data.The HMM approach can adequately characterize each of these occurrences, but there are cases where it will fail."}, {"heading": "5. Conclusions and Future Work", "text": "In this article, we introduced the combined theme and syntax model, Part-of-Speech LDA or POSLDA. We also demonstrated its use as an improved model for performing unattended POS tagging. Our overarching goal is to show that combining the two axes of meaning of words - syntax and semantics - can improve both the learned topic distributions and the NLP tasks such as POS tagging. We demonstrated that incorporating semantic information into the HMM model led to improved results for this task. Furthermore, we demonstrated that combining the two axes of word information leads to a language model that achieves lower perplexity - and therefore better prediction capability - than other similar probability models. In future work, we would like to apply the POSLDA model to other NLP tasks that also rely on learned word distributions. These include text summarization, text segmentation to OSDA, and other ways of oversegmenting OSDA is a more interesting way of language."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the Natural Sciences and Engineering Research Council of Canada for partially funding this work through a doctoral fellowship for William Darling. The authors would also like to acknowledge the financial support provided by the Ontario Centres of Excellence (OCE) through the OCE / Precarn Alliance Program."}], "references": [{"title": "Staying informed: supervised and semi-supervised multi-view topical analysis of ideological perspective", "author": ["Ahmed", "Xing2010]Ahmed", "Amr", "Eric P. Xing"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ahmed et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ahmed et al\\.", "year": 2010}, {"title": "Probabilistic topic models", "author": ["M. David"], "venue": "Commun. ACM,", "citeRegEx": "David,? \\Q2012\\E", "shortCiteRegEx": "David", "year": 2012}, {"title": "The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies", "author": ["Blei", "Griffiths", "Jordan2010]Blei", "David M", "Thomas L. Griffiths", "Michael I. Jordan"], "venue": "J. ACM,", "citeRegEx": "Blei et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2010}, {"title": "Latent dirichlet allocation", "author": ["Blei", "Ng", "Jordan2003]Blei", "David M", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Reading tea leaves: How humans interpret topic models", "author": ["Chang et al.2009]Chang", "Jonathan", "Jordan Boyd-Graber", "Chong Wang", "Sean Gerrish", "David M. Blei"], "venue": "In Neural Information Processing Systems", "citeRegEx": "al.2009.Chang et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al.2009.Chang et al\\.", "year": 2009}, {"title": "Modeling general and specific aspects of documents with a probabilistic topic model", "author": ["Chemudugunta", "Smyth", "Steyvers2006]Chemudugunta", "Chaitanya", "Padhraic Smyth", "Mark Steyvers"], "venue": "In NIPS,", "citeRegEx": "Chemudugunta et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chemudugunta et al\\.", "year": 2006}, {"title": "Generalized Probabilistic Topic and Syntax Models for Natural Language Processing", "author": ["M. William"], "venue": "Ph.D. thesis,", "citeRegEx": "William,? \\Q2012\\E", "shortCiteRegEx": "William", "year": 2012}, {"title": "Unsupervised part-of-speech tagging in noisy and esoteric domains with a syntactic-semantic bayesian hmm", "author": ["Darling", "Paul", "Song2012]Darling", "William M", "Michael J. Paul", "Fei Song"], "venue": "In Proceedings of the EACL 2012 Workshop on Semantic Analysis in Social Media", "citeRegEx": "Darling et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Darling et al\\.", "year": 2012}, {"title": "Probabilistic document modeling for syntax removal in text summarization", "author": ["Darling", "Song2011]Darling", "William M", "Fei Song"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Darling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Darling et al\\.", "year": 2011}, {"title": "Part-of-speech tagging for twitter: Annotation, features, and experiments", "author": ["Gimpel et al.2011]Gimpel", "Kevin", "Nathan Schneider", "Brendan O\u2019Connor", "Dipanjan Das", "Daniel Mills", "Jacob Eisenstein", "Michael Heilman", "Dani Yogatama", "Jeffrey Flanigan", "Noah A. Smith"], "venue": "In Proceedings of the Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "al.2011.Gimpel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "al.2011.Gimpel et al\\.", "year": 2011}, {"title": "A fully Bayesian approach to unsupervised part-of-speech tagging", "author": ["Goldwater", "Griffiths2007]Goldwater", "Sharon", "Tom Griffiths"], "venue": "In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,", "citeRegEx": "Goldwater et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Goldwater et al\\.", "year": 2007}, {"title": "Integrating topics and syntax", "author": ["Griffiths et al.2005]Griffiths", "Thomas L", "Mark Steyvers", "David M. Blei", "Joshua B. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "al.2005.Griffiths et al\\.,? \\Q2005\\E", "shortCiteRegEx": "al.2005.Griffiths et al\\.", "year": 2005}, {"title": "Overview of the first text retrieval conference (trec-1)", "author": ["Harman1992]Harman", "Donna"], "venue": "In TREC,", "citeRegEx": "Harman1992.Harman and Donna.,? \\Q1992\\E", "shortCiteRegEx": "Harman1992.Harman and Donna.", "year": 1992}, {"title": "The elements of statistical learning: data mining, inference, and prediction: with 200 full-color illustrations", "author": ["Hastie", "Tibshirani", "Friedman2001]Hastie", "Trevor", "Robert Tibshirani", "J.H. Friedman"], "venue": null, "citeRegEx": "Hastie et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Hastie et al\\.", "year": 2001}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["Hofmann2001]Hofmann", "Thomas"], "venue": null, "citeRegEx": "Hofmann2001.Hofmann and Thomas.,? \\Q2001\\E", "shortCiteRegEx": "Hofmann2001.Hofmann and Thomas.", "year": 2001}, {"title": "Modeling syntactic structures of topics with a nested hmm-lda", "author": ["Jiang2009]Jiang", "Jing"], "venue": "In Proceedings of the 2009 Ninth IEEE International Conference on Data Mining,", "citeRegEx": "Jiang2009.Jiang and Jing.,? \\Q2009\\E", "shortCiteRegEx": "Jiang2009.Jiang and Jing.", "year": 2009}, {"title": "Foundations of Statistical Natural Language Processing. MIT Press, Cambridge (Mass.) and London", "author": ["Manning", "Sch\u00fctze1999]Manning", "Christopher D", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Manning et al\\.", "year": 1999}, {"title": "Comparing clusterings\u00e2\u0102\u0164an information based distance", "author": ["M. Meil\u01ce2007]Meil\u01ce"], "venue": "Journal of Multivariate Analysis,", "citeRegEx": "Meil\u01ce2007.Meil\u01ce,? \\Q2007\\E", "shortCiteRegEx": "Meil\u01ce2007.Meil\u01ce", "year": 2007}, {"title": "Tagging english text with a probabilistic model", "author": ["Merialdo1993]Merialdo", "Bernard"], "venue": "Computational Linguistics,", "citeRegEx": "Merialdo1993.Merialdo and Bernard.,? \\Q1993\\E", "shortCiteRegEx": "Merialdo1993.Merialdo and Bernard.", "year": 1993}, {"title": "Factorial lda: Sparse multi-dimensional text models", "author": ["Paul", "Dredze2012]Paul", "Michael", "Mark Dredze"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Paul et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2012}, {"title": "You are what you tweet: Analyzing twitter for public health", "author": ["Paul", "Dredze2011]Paul", "Michael J", "Mark Dredze"], "venue": "In 5th International AAAI Conference on Weblogs and Social Media (ICWSM", "citeRegEx": "Paul et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2011}, {"title": "A two-dimensional topic-aspect model for discovering multi-faceted topics", "author": ["Paul", "Girju2010]Paul", "Michael J", "Roxana Girju"], "venue": null, "citeRegEx": "Paul et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Paul et al\\.", "year": 2010}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["R. Lawrence"], "venue": "Readings in speech recognition", "citeRegEx": "Lawrence,? \\Q1990\\E", "shortCiteRegEx": "Lawrence", "year": 1990}, {"title": "Characterizing microblogs with topic models", "author": ["Ramage", "Dumais", "Liebling2010]Ramage", "Daniel", "Susan Dumais", "Dan Liebling"], "venue": null, "citeRegEx": "Ramage et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ramage et al\\.", "year": 2010}, {"title": "Labeled lda: a supervised topic model for credit attribution in multi-labeled corpora", "author": ["Ramage et al.2009]Ramage", "Daniel", "David Hall", "Ramesh Nallapati", "Christopher D. Manning"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1,", "citeRegEx": "al.2009.Ramage et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al.2009.Ramage et al\\.", "year": 2009}, {"title": "Unsupervised modeling of twitter conversations", "author": ["Ritter", "Cherry", "Dolan2010]Ritter", "Alan", "Colin Cherry", "Bill Dolan"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Ritter et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2010}, {"title": "Introduction to Modern Information Retrieval", "author": ["Salton", "G. McGill1983]Salton", "M. McGill"], "venue": null, "citeRegEx": "Salton et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Salton et al\\.", "year": 1983}, {"title": "Contrastive estimation: training log-linear models on unlabeled data", "author": ["Smith", "Eisner2005]Smith", "Noah A", "Jason Eisner"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Smith et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2005}, {"title": "Rethinking lda: Why priors matter", "author": ["Wallach", "Mimno", "McCallum2009]Wallach", "Hanna", "David Mimno", "Andrew McCallum"], "venue": null, "citeRegEx": "Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Wallach et al\\.", "year": 2009}, {"title": "Structured Topic Models for Language", "author": ["M. Hanna"], "venue": "Ph.D. thesis,", "citeRegEx": "Hanna,? \\Q2008\\E", "shortCiteRegEx": "Hanna", "year": 2008}, {"title": "Evaluation methods for topic models", "author": ["Wallach et al.2009]Wallach", "Hanna M", "Iain Murray", "Ruslan Salakhutdinov", "David Mimno"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "al.2009.Wallach et al\\.,? \\Q2009\\E", "shortCiteRegEx": "al.2009.Wallach et al\\.", "year": 2009}, {"title": "Term weighting schemes for latent dirichlet allocation", "author": ["Wilson", "Chew2010]Wilson", "Andrew T", "Peter A. Chew"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Wilson et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Wilson et al\\.", "year": 2010}, {"title": "No free lunch theorems for optimization", "author": ["Wolpert", "D.H. Macready1997]Wolpert", "W.G. Macready"], "venue": "Trans. Evol. Comp,", "citeRegEx": "Wolpert et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Wolpert et al\\.", "year": 1997}, {"title": "Taglda: Bringing document structure knowledge into topic models", "author": ["Zhu", "Blei", "Lafferty2006]Zhu", "Xiaojin", "David M. Blei", "John Lafferty"], "venue": "Technical Report TR-1553,", "citeRegEx": "Zhu et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2006}], "referenceMentions": [{"referenceID": 28, "context": "There are several methods that are commonly employed to evaluate novel probabilistic models in the literature (Wallach et al. 2009).", "startOffset": 110, "endOffset": 131}], "year": 2013, "abstractText": "This article presents a probabilistic generative model for text based on semantic topics and syntactic classes called Part-of-Speech LDA (POSLDA). POSLDA simultaneously uncovers short-range syntactic patterns (syntax) and long-range semantic patterns (topics) that exist in document collections. This results in word distributions that are specific to both topics (sports, education, ...) and parts-of-speech (nouns, verbs, ...). For example, multinomial distributions over words are uncovered that can be understood as \u201cnouns about weather\u201d or \u201cverbs about law\u201d. We describe the model and an approximate inference algorithm and then demonstrate the quality of the learned topics both qualitatively and quantitatively. Then, we discuss an NLP application where the output of POSLDA can lead to strong improvements in quality: unsupervised partof-speech tagging. We describe algorithms for this task that make use of POSLDA-learned distributions that result in improved performance beyond the state of the art.", "creator": "LaTeX with hyperref package"}}}