{"id": "1303.3632", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Mar-2013", "title": "Statistical Regression to Predict Total Cumulative CPU Usage of MapReduce Jobs", "abstract": "Recently, businesses have started using MapReduce as a popular computation framework for processing large amount of data, such as spam detection, and different data mining tasks, in both public and private clouds. Two of the challenging questions in such environments are (1) choosing suitable values for MapReduce configuration parameters e.g., number of mappers, number of reducers, and DFS block size, and (2) predicting the amount of resources that a user should lease from the service provider. Currently, the tasks of both choosing configuration parameters and estimating required resources are solely the users responsibilities. In this paper, we present an approach to provision the total CPU usage in clock cycles of jobs in MapReduce environment. For a MapReduce job, a profile of total CPU usage in clock cycles is built from the job past executions with different values of two configuration parameters e.g., number of mappers, and number of reducers. Then, a polynomial regression is used to model the relation between these configuration parameters and total CPU usage in clock cycles of the job. We also briefly study the influence of input data scaling on measured total CPU usage in clock cycles. This derived model along with the scaling result can then be used to provision the total CPU usage in clock cycles of the same jobs with different input data size. We validate the accuracy of our models using three realistic applications (WordCount, Exim MainLog parsing, and TeraSort). Results show that the predicted total CPU usage in clock cycles of generated resource provisioning options are less than 8% of the measured total CPU usage in clock cycles in our 20-node virtual Hadoop cluster.", "histories": [["v1", "Thu, 14 Mar 2013 22:40:32 GMT  (906kb)", "http://arxiv.org/abs/1303.3632v1", "16 pages- previously published as \"On Modelling and Prediction of Total CPU Usage for Applications in MapReduce Enviornments\" in IEEE 12th International Conference on Algorithms and Architectures for Parallel Processing (ICA3PP-12), Fukuoka, Japan, 4-7 September, 2012"]], "COMMENTS": "16 pages- previously published as \"On Modelling and Prediction of Total CPU Usage for Applications in MapReduce Enviornments\" in IEEE 12th International Conference on Algorithms and Architectures for Parallel Processing (ICA3PP-12), Fukuoka, Japan, 4-7 September, 2012", "reviews": [], "SUBJECTS": "cs.DC cs.LG cs.PF", "authors": ["nikzad babaii rizvandi", "javid taheri", "reza moraveji", "albert y zomaya"], "accepted": false, "id": "1303.3632"}, "pdf": {"name": "1303.3632.pdf", "metadata": {"source": "CRF", "title": "Statistical Regression to Predict Total Cumulative CPU Usage of MapReduce Jobs", "authors": ["Nikzad Babaii Rizvandi", "Javid Taheri", "Reza Moraveji", "Albert Y. Zomaya"], "emails": ["nikzad@it.usyd.edu.au"], "sections": [{"heading": null, "text": "Two of the most challenging questions in such environments are (1) selecting suitable values for the MapReduce configuration parameters - such as the number of mappers, the number of reducers, and the block size of DFS - and (2) predicting the amount of resources a user should lease from the service provider. Currently, the tasks of selecting the configuration parameters and estimating the resources required are the sole responsibility of users. In this paper, we present an approach to delivering the total CPU usage in cycles of jobs in the MapReduce environment. For a MapReduce job, a profile of the total CPU usage is created in clock cycles from past versions with different values of two configuration parameters, such as the number of mappers and the reducers. Subsequently, a polynomial regression is used to model the relationship between these configuration parameters and overall CPU usage."}, {"heading": "1. Introduction", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "2. Related Work", "text": "In this context, it has to be said that the measures we have mentioned are not only measures, but also measures that have been taken by the policy."}, {"heading": "3. Application Modelling in MapReduce", "text": "In commercial clouds (such as Amazon EC2), the problem of allocating an appropriate number of machines for a reasonable timeframe depends heavily on an application; the user is responsible for setting these values correctly [15]. In order to estimate how much resources (in terms of CPU costs, I / O costs and storage costs) a job requires overall, the user can therefore make educated decisions to hire an appropriate number of machines. In MapReduce environments, this problem becomes more important as the number of machines cannot be changed after starting a job."}, {"heading": "3.1. Profiling total CPU usage in clock cycles", "text": "For each application, we generate a set of jobs - i.e., an experiment of application on MapReduce environment- with different values of two MapReduce configuration parameters - i.e. number of mappers and reductions - on a specific platform. During the execution of each job, CPU usage is recorded in the clock cycles of a job to achieve traceability for future use; such data is easily captured by functions in XenAPI with almost no overhead. Within the system, we will record CPU usage in the clock cycles of a job for each machine, from the time the mappers start up to the time intervals reducers."}, {"heading": "3.2. Total CPU usage in clock cycles model using polynomial regression", "text": "The next step is to create a model for an application on Maputo (2). (1) The second step is to create a model for an application on Maputo (2). (2) The third step is to create a model for an application (2). (2) The third step is to create a model for a specific requirement. (2) The third step is to create a model for a specific requirement. (3) The third step is to create a model for a specific requirement. (3) The third step is to create a model. (4) The third step is to create a model. (4) The third step is to create a model. (4) The third step is to create a model. (4) The fourth step is to create a model."}, {"heading": "4. Experimental Validation", "text": "In this section, we evaluate the effectiveness of our models based on three realistic applications."}, {"heading": "4.1. Experimental setting", "text": "Three realistic applications are used to evaluate the effectiveness of our method. Our method was implemented and evaluated on a private cloud with the following specifications: Physical H / W: includes five servers, each of which is an Intel Genuine with 3.00GHz clock, 1GB of memory, 1GB of cache and 50GB of shared SAN hard disk. For virtualization, the Xen Cloud Platform (XCP) was built on top of the physical H / W. The XenAPI [18] provides functionality for directly managing virtual machines within XCP. It provides binding in high-level languages such as Java, C # and Python. Using these bindings, it was possible to measure the performance of all virtual machines in a data center and to migrate them live. Virtual nodes are implemented on top of the XCP. The number of virtual nodes is selected in high-level languages such as Java, C # and Python."}, {"heading": "4.2 Evaluation Criteria", "text": "We evaluate the accuracy of the adjusted models generated from a regression based on a number of metrics [30]: Mean Absolute Percentage Error (MAPE), PRED (25), Root Mean Squared Error (RMSE) and R2 Prediction Accuracy."}, {"heading": "4.2.1. Mean Absolute Percentage Error (MAPE)", "text": "The mean absolute percentage error [30] for a prediction model is described as follows:"}, {"heading": "4.2.2. PRED(25)", "text": "The measurement variable PRED (25) [30] is given as follows: (25) = 20002050210021502200225023004 8 12 16 20 24 28 32WordCountR = 4 R = 12 R = 20 R = 28TeraSortIt is the percentage of observations whose predictive accuracy is within 25% of the actual value. Closer values from PRED (25) to 1.0 imply a better adaptation of the predictive model."}, {"heading": "4.2.3. Root Mean Squared Error (RMSE)", "text": "The metric root Mean Square Error (RMSE) [30] is: 0 () \u2212 () 2 = 1 1A more effective prediction results from a lower RMSE value."}, {"heading": "4.2.4. \ud835\udc79\ud835\udfd0 Prediction Accuracy", "text": "The accuracy of the 2 prediction [30] - usually applied to models of linear regression as a measure of the fit accuracy of the prediction model - is calculated as follows: 0 2 = 1 \u2212 \u2211 (() \u2212 ()) 2 = 1 (() ()) 2 = 1.0."}, {"heading": "4.3. Results", "text": "In fact, the fact is that most of them are able to move around without being able to move around to achieve their goals."}, {"heading": "4.3. Discussion and future work", "text": "Although the model obtained can successfully predict the degree of total CPU usage in clock cycles required for a few MapReduce applications, it has some drawbacks. First, the total CPU usage is modeled in clock cycles of a job by calculating the total CPU usage in clock cycles of the entire job from multiple tracks. Many applications exhibit quite different behavior between their map and reduce phases: in some cases the map is computationally intensive, in other cases reduce, or even both. Taking this into account, we would like to expand our model to a finer-grained extent. To this end, we would like to split this model to cover CPU usage in clock cycles of both phases separately; i.e. instead of using a uniform average, we prefer to rely on a weighted average that emphasizes CPU usage in clock cycles at each phase of the mapce reduction; i.e., instead of using a unified average, we prefer to rely on a weighted average that emphasizes the CPU usage in clock cycles of both phases; i.e., the parameters of our two successful applications are almost complete in the evaluation of the mainline reduction."}, {"heading": "6. Conclusion", "text": "In this paper, we proposed an accurate modeling technique to predict total CPU usage in clock cycles of jobs in the MapReduce environment prior to their actual use on clusters and / or clouds. Such predictions can vastly improve both application performance and effective resource usage. To achieve this, we have developed an approach to modeling total CPU usage in application clock cycles and the applied polynomic regression model to identify the correlation between two important configuration parameters of MapReduce (number of mappers and number of reducers) and total CPU usage in application clock cycles. Our modeling technology can be used by both users / consumers (e.g. application developers) and cloud service providers for effective resource usage. Evaluation results show that prediction errors of the total cycle of certain applications can be less than 8%."}, {"heading": "Acknowledgment", "text": "The work of Mr. N. Babaii Rizvandi is supported by National ICT Australia (NICTA). Professor A.Y. Zomaya's work is supported by an Australian research grant LP0884070."}], "references": [{"title": "MapReduce Implementation of Prestack Kirchhoff Time Migration (PKTM) on Seismic Data", "author": ["N.B. Rizvandi", "A.J. Boloori", "N. Kamyabpour", "A. Zomaya"], "venue": "presented at the The 12th International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT), Gwangju, Korea 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Cloud-enabling Sequence Alignment with Hadoop MapReduce: A Performance Analysis", "author": ["K. Arumugam", "Y.S. Tan", "B.S. Lee", "R. Kanagasabai"], "venue": "presented at the 2012 4th International Conference on Bioinformatics and Biomedical Technology, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "An Analysis of Traces from a Production MapReduce Cluster", "author": ["S. Kavulya", "J. Tan", "R. Gandhi", "P. Narasimhan"], "venue": "presented at the Proceedings of the 2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing, 2010. Figure 5. The error between actual total CPU usage in clock cycle and the model prediction. The X-axis is job ID while Y-axis is percentage of error  -20  -10  0  10  20 1 3 5 7 9 11 13 15  P  rc  e  n  ta  ge Error WordCount Exim MainLog TeraSort  14", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards Optimizing Hadoop Provisioning in the Cloud", "author": ["K. Kambatla", "A. Pathak", "H. Pucha"], "venue": "presented at the the 2009 conference on Hot topics in cloud computing, San Diego, California, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Improving MapReduce Performance in Heterogeneous Environments", "author": ["M. Zaharia", "A. Konwinski", "A.D.Joseph", "R. Katz", "I. Stoica"], "venue": "8th USENIX Symposium on Operating Systems Design and Implementation (OSDI 2008), pp. 29-42, 18 December 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Resource Provisioning Framework for MapReduce Jobs with Performance Goals", "author": ["A. Verma", "L. Cherkasova", "R.H. Campbell"], "venue": "presented at the ACM/IFIP/USENIX 12th International Middleware Conference (Middleware), Lisbon, Portugal, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Statistical Workloads for Energy Efficient MapReduce", "author": ["Y. Chen", "A.S. Ganapathi", "A. Fox", "R.H. Katz", "D.A. Patterson"], "venue": "University of California at Berkeley,Technical Report No. UCB/EECS-2010-6, 2010.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "A model of computation for MapReduce", "author": ["H. Karloff", "S. Suri", "S. Vassilvitskii"], "venue": "presented at the Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete Algorithms, Austin, Texas, 2010.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards optimizing hadoop provisioning in the cloud", "author": ["K. Kambatla", "A. Pathak", "H. Pucha"], "venue": "presented at the Proceedings of the 2009 conference on Hot topics in cloud computing, San Diego, California, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "A Study on Using Uncertain Time Series Matching Algorithms in Map-Reduce Applications", "author": ["N.B. Rizvandi", "J. Taheri", "A.Y. Zomaya", "R. Moraveji"], "venue": "Concurrency and Computation: Practice and Experience, 2012.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Brief Announcement: Modelling MapReduce for Optimal Execution in the Cloud", "author": ["A. Wieder", "P. Bhatotia", "A. Post", "R. Rodrigues"], "venue": "presented at the Proceeding of the 29th ACM SIGACT-SIGOPS symposium on Principles of distributed computing, Zurich, Switzerland, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Conductor: orchestrating the clouds", "author": ["A. Wieder", "P. Bhatotia", "A. Post", "R. Rodrigues"], "venue": "presented at the 4th International Workshop on Large Scale Distributed Systems and Middleware, Zurich, Switzerland, 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Bag-of-Tasks Scheduling under Budget Constraints \" presented at the IEEE", "author": ["A.-m. Oprescu", "T. Kielmann"], "venue": "Second International Conference on Cloud Computing Technology and Science (CloudCom),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Profiling and Modeling Resource Usage of Virtualized Applications", "author": ["T. Wood", "L. Cherkasova", "K. Ozonat", "a. P. Shenoy"], "venue": "presented at the Proceedings of the ACM/IFIP/USENIX 9th International Middleware Conference, Leuven, Belgium, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "An Accurate Fir Approximation of Ideal Fractional Delay Filter with Complex Coefficients in Hilbert Space", "author": ["N.B. Rizvandi", "A. Nabavi", "S. Hessabi"], "venue": "Journal of Circuits, Systems, and Computers, vol. 14, pp. 497-506, 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "The definitive guide to the xen hypervisor., first ed.", "author": ["D. Chisnall"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Optimizing MapReduce for Multicore Architectures", "author": ["a. Mao", "R. Morris", "M.F. Kaashoek"], "venue": "Massachusetts Institute of Technology2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards automatic optimization of MapReduce programs", "author": ["S. Babu"], "venue": "presented at the 1st ACM symposium on Cloud computing, Indianapolis, Indiana, USA, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "A Simulation Approach to Evaluating Design Decisions in MapReduce Setups \" presented at the MASCOTS", "author": ["G. Wang", "A.R. Butt", "P. P", "K. Gupta"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Using realistic simulation for performance analysis of mapreduce setups", "author": ["G. Wang", "A.R. Butt", "P. Pandey", "K. Gupta"], "venue": "presented at the Proceedings of the 1st ACM workshop on Large-Scale system and application performance, Garching, Germany, 2009.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimizing intermediate data management in MapReduce computations", "author": ["D. Moise", "T.-T.-L. Trieu", "L. Boug", "#233", "G. Antoniu"], "venue": "presented at the Proceedings of the First International Workshop on Cloud Computing Platforms, Salzburg, Austria, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "MapReduce: Distributed Computing for Machine  Learning", "author": ["D. Gillick", "A. Faria", "J. DeNero"], "venue": "www.icsi.berkeley.edu/~arlo/publications/gillick_cs262a_proj.pdf2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "On using Pattern Matching Algorithms in MapReduce Applications", "author": ["N.B. Rizvandi", "J. Taheri", "A.Y. Zomaya"], "venue": "presented at the The 9th IEEE International Symposium on Parallel and Distributed Processing with Applications (ISPA), Busan, South Korea, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Empirical prediction models for adaptive resource provisioning in the cloud", "author": ["S. Islam", "J. Keung", "K. Lee", "A. Liu"], "venue": "Future Generation Comp. Syst., vol. 28, pp. 155-162, 2012. Figure 6. total CPU usage in clock cycles and scalability in input data size  0  50  100  150  200 250 0  2  4  6 8 To  ta  l  C  P  U  u  sa  ge  (  in  t  e  ra  c  lo  ck  c  yc  le  ) input data (in GB) Scalability in input data size WordCount Exim Terasort  16", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Another example is Seismic imaging data where fix number of ultrasound senders/receivers produce earth underground information in a specific region; therefore, the size of output file \u2013usually in the order of terabyte\u2013 is usually consistent [2].", "startOffset": 241, "endOffset": 244}, {"referenceID": 1, "context": "The other example is to find a sequence matching between a new RNA and RNAs in a database [3], where the size of such databases (such as NCBI [4]) is almost unchanged over adjacent periods of time.", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "Although there are a few recent methodologies to estimate resource provisioning of MapReduce jobs (mostly on execution time prediction [5-8]), to best of our knowledge, there is no practice to study the dependency between performance of executing a job and the configuration parameters.", "startOffset": 135, "endOffset": 140}, {"referenceID": 3, "context": "Although there are a few recent methodologies to estimate resource provisioning of MapReduce jobs (mostly on execution time prediction [5-8]), to best of our knowledge, there is no practice to study the dependency between performance of executing a job and the configuration parameters.", "startOffset": 135, "endOffset": 140}, {"referenceID": 4, "context": "Although there are a few recent methodologies to estimate resource provisioning of MapReduce jobs (mostly on execution time prediction [5-8]), to best of our knowledge, there is no practice to study the dependency between performance of executing a job and the configuration parameters.", "startOffset": 135, "endOffset": 140}, {"referenceID": 5, "context": "Although there are a few recent methodologies to estimate resource provisioning of MapReduce jobs (mostly on execution time prediction [5-8]), to best of our knowledge, there is no practice to study the dependency between performance of executing a job and the configuration parameters.", "startOffset": 135, "endOffset": 140}, {"referenceID": 4, "context": "Early works on analysing/improving MapReduce performance started almost since 2005; such as an approach by Zaharia et al [7] that addressed problem of improving the performance of Hadoop for heterogeneous environments.", "startOffset": 121, "endOffset": 124}, {"referenceID": 6, "context": "A statistics-driven workload modelling was introduced in [9] to effectively evaluate design decisions in scaling, configuration and scheduling.", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "Authors in [10] proposed a theoretical study on the MapReduce programming model which characterizes the features of mixed sequential and parallel processing in MapReduce.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "In [11], the variation effect of Map and Reduce slots on the performance has been studied.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "The idea of pattern matching was used in [12] to find the similarity between CPU time patters of a new application and applications in database.", "startOffset": 41, "endOffset": 45}, {"referenceID": 2, "context": "Authors in [5] also used historical execution traces of applications on MapReduce environment for profiling and performance modelling and prediction.", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "A modelling method was proposed in [7] to predict the total execution time of a MapReduce application; they used Kernel Canonical Correlation Analysis to obtain the correlation between the performance feature vectors extracted from MapReduce job logs, and map time, reduce time, and total execution time.", "startOffset": 35, "endOffset": 38}, {"referenceID": 10, "context": "Authors in [13, 14] reported a basic model for MapReduce computation utilizations.", "startOffset": 11, "endOffset": 19}, {"referenceID": 11, "context": "Authors in [13, 14] reported a basic model for MapReduce computation utilizations.", "startOffset": 11, "endOffset": 19}, {"referenceID": 5, "context": "Another study in [8] proposed a resource provisioning framework to predict how much resources a user job needs to be completed by a certain time.", "startOffset": 17, "endOffset": 20}, {"referenceID": 12, "context": "In commercial clouds (such as Amazon EC2), the problem of allocating appropriate number of machines for a proper time frame strongly depends on an application; user is responsible to set these values properly [15].", "startOffset": 209, "endOffset": 213}, {"referenceID": 13, "context": "The problem of such a modeling \u2013based on linear regression\u2013 involves choosing of suitable coefficients for the model to better approximate a real system response time [16, 17].", "startOffset": 167, "endOffset": 175}, {"referenceID": 14, "context": "The problem of such a modeling \u2013based on linear regression\u2013 involves choosing of suitable coefficients for the model to better approximate a real system response time [16, 17].", "startOffset": 167, "endOffset": 175}, {"referenceID": 14, "context": "then the model satisfying the above error will be calculated as [17]:", "startOffset": 64, "endOffset": 68}, {"referenceID": 15, "context": "The XenAPI [18] provides functionality to directly manage virtual machines inside XCP.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].", "startOffset": 89, "endOffset": 92}, {"referenceID": 16, "context": "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].", "startOffset": 98, "endOffset": 102}, {"referenceID": 4, "context": "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].", "startOffset": 120, "endOffset": 123}, {"referenceID": 17, "context": "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].", "startOffset": 207, "endOffset": 215}, {"referenceID": 18, "context": "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].", "startOffset": 251, "endOffset": 259}, {"referenceID": 19, "context": "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].", "startOffset": 251, "endOffset": 259}, {"referenceID": 20, "context": "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].", "startOffset": 279, "endOffset": 283}, {"referenceID": 21, "context": "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].", "startOffset": 300, "endOffset": 304}, {"referenceID": 9, "context": "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].", "startOffset": 332, "endOffset": 340}, {"referenceID": 22, "context": "Our benchmark applications are WordCount (used by leading researchers in Intel [21], IBM [6], MIT [22], and UC-Berkeley [7]), TeraSort (as a standard benchmark in the international TeraByte sort competition [23, 24] as well as many researchers in IBM [25, 26], Intel [21], INRIA [27] and UC-Berkeley [28]), and Exim Mainlog parsing [12, 29].", "startOffset": 332, "endOffset": 340}, {"referenceID": 23, "context": "We evaluate the accuracy of the fitted models, generated from regression based on a number of metrics [30]: Mean Absolute Percentage Error (MAPE), PRED(25) , Root Mean Squared Error (RMSE) and R2 Prediction Accuracy .", "startOffset": 102, "endOffset": 106}, {"referenceID": 23, "context": "Mean Absolute Percentage Error (MAPE) The Mean Absolute Percentage Error[30] for a prediction model is described as:", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "PRED(25) The measure PRED(25)[30] is given as:", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "Root Mean Squared Error (RMSE) The metric Root Mean Square Error (RMSE)[30] is given by:", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "R Prediction Accuracy The R Prediction Accuracy[30] \u2013 commonly applied to Linear Regression models as a measure of the goodness-of-fit of the prediction model\u2013 is calculated as:", "startOffset": 47, "endOffset": 51}], "year": 2013, "abstractText": "recently, businesses have started using MapReduce as a popular computation framework for processing large amount of data, such as spam detection, and different data mining tasks, in both public and private clouds. Two of the challenging questions in such environments are (1) choosing suitable values for MapReduce configuration parameters \u2013e.g., number of mappers, number of reducers, and DFS block size\u2013, and (2) predicting the amount of resources that a user should lease from the service provider. Currently, the tasks of both choosing configuration parameters and estimating required resources are solely the users\u2019 responsibilities. In this paper, we present an approach to provision the total CPU usage in clock cycles of jobs in MapReduce environment. For a MapReduce job, a profile of total CPU usage in clock cycles is built from the job past executions with different values of two configuration parameters e.g., number of mappers, and number of reducers. Then, a polynomial regression is used to model the relation between these configuration parameters and total CPU usage in clock cycles of the job. We also briefly study the influence of input data scaling on measured total CPU usage in clock cycles. This derived model along with the scaling result can then be used to provision the total CPU usage in clock cycles of the same jobs with different input data size. We validate the accuracy of our models using three realistic applications (WordCount, Exim MainLog parsing, and TeraSort). Results show that the predicted total CPU usage in clock cycles of generated resource provisioning options are less than 8% of the measured total CPU usage in clock cycles in our 20-node virtual Hadoop cluster. Keywordtotal CPU usage in clock cycles, MapReduce, Hadoop, Resource provisioning, Configuration parameters, input data scaling", "creator": "Microsoft\u00ae Word 2010"}}}