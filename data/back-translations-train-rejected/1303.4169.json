{"id": "1303.4169", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2013", "title": "Markov Chain Monte Carlo for Arrangement of Hyperplanes in Locality-Sensitive Hashing", "abstract": "Since Hamming distances can be calculated by bitwise computations, they can be calculated with less computational load than L2 distances. Similarity searches can therefore be performed faster in Hamming distance space. The elements of Hamming distance space are bit strings. On the other hand, the arrangement of hyperplanes induce the transformation from the feature vectors into feature bit strings. This transformation method is a type of locality-sensitive hashing that has been attracting attention as a way of performing approximate similarity searches at high speed. Supervised learning of hyperplane arrangements allows us to obtain a method that transforms them into feature bit strings reflecting the information of labels applied to higher-dimensional feature vectors. In this p aper, we propose a supervised learning method for hyperplane arrangements in feature space that uses a Markov chain Monte Carlo (MCMC) method. We consider the probability density functions used during learning, and evaluate their performance. We also consider the sampling method for learning data pairs needed in learning, and we evaluate its performance. We confirm that the accuracy of this learning method when using a suitable probability density function and sampling method is greater than the accuracy of existing learning methods.", "histories": [["v1", "Mon, 18 Mar 2013 07:14:15 GMT  (114kb)", "http://arxiv.org/abs/1303.4169v1", "13 pages, 10 figures"]], "COMMENTS": "13 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yui noma", "makiko konoshima"], "accepted": false, "id": "1303.4169"}, "pdf": {"name": "1303.4169.pdf", "metadata": {"source": "CRF", "title": "Markov Chain Monte Carlo for Arrangement of Hyperplanes in Locality-Sensitive Hashing", "authors": ["Yui Noma", "Makiko Konoshima"], "emails": ["makiko@jp.fujitsu.com"], "sections": [{"heading": null, "text": "ar Xiv: 130 3.41 69v1 [cs.LG] 1 8M ar2 013Since hamming distances can be calculated by bit, they can be calculated with less computing load than L2 distances. Therefore, similarity searches can be performed faster in Hamming Distance Space. The elements of Hamming Distance Space are bit strings. On the other hand, the arrangement of hyperplanes induces the transformation from feature vectors to feature bit strings. This transformation method is a kind of spatial-sensitive hashing that has attracted attention to perform approximate similarity searches at high velocity. Monitored learning of hyperplane arrangements allows us to obtain a method that converts them into feature bit strings that reflect the information of labels applied to higher dimensional feature vectors. In this essay, we propose a supervised learning method for hyperplane arrangements in the feature space, which we will consider the Markov chain of probability (We will use the Monte Carlo Method for evaluating the MC), the performance function of this assessment, and the Monte Carlo Method for evaluating the probability chain."}, {"heading": "1 Introduction", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is a country in which it is a country, a country in which it is a country, a country, a country, a country, a country, a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "2 Background and related work", "text": "In this section, we describe the use of hyperplanes for local hashing, which is the basis of the proposed technique, and then describe some related existing techniques."}, {"heading": "2.1 Conventional locality-sensitive hashing with hyperplanes", "text": "A space V in which there are higher dimensional characteristic magnitudes is considered to be N-dimensional vector space; the configurations of several hyperplanes in V. Consider B-hyperplanes that go through the origin of V. A hyperplane that goes through the origin is identified by its normal vector; an N-dimensional characteristic vector ~ x is converted into a bit sequence by registering a 1 if its point product is positive with any normal vector, and a zero otherwise. Therefore, the length of the bit sequence is equal to the number of hyperplanes B. A hyperplane that does not cross the origin can easily be constructed from a hyperplane that does so. In relation [11], an experiment is conducted in which the hashing of hyperplanes that do not go through the origin is learned by learning the hashing of hyperplanes that go through the origin."}, {"heading": "2.2 Minimal loss hashing", "text": "An existing learning method is Minimal Loss Hashing (MLH) [9]. In MLH, the goal is to minimize an empirical loss function to (SN \u2212 1) B. However, the empirical loss function is discontinuous, so it is not possible to apply learning methods based on gradients. Therefore, empirical loss is replaced by a differentiable upper limit function g, and instead gradients are used to minimize g. The point that indicates the minimum value determines the coordinates of the learned hyperplanes B. Function g has several parameters that need to be adjusted. Some of these parameters depend on the data pairs used for training. All data pairs for learning are randomly selected."}, {"heading": "2.3 Locality-sensitive hashing with margin based feature selection", "text": "In this subsection, we describe the concept of an existing learning method called locality-sensitive hashing with margin-based feature selection (S-LSH) [10]. In S-LSH, the normal vectors of hyperplanes are not used directly for learning. B-hyperplanes (B-LSH) are randomly provided. Each hyperplane is assigned a weighted measure of meaning, and these meanings are calculated by learning. Degrees of meaning are arranged in descending order, and the uppermost B-normal vectors. distance calculations during learning are performed using weighted hamming distances. During learning, two types of data pairs are used. Learning data pairs are selected as follows: A feature vector a is randomly selected from the learning data. the first data type consists of the pair (a, b), with b being the feature vector with the smallest weighted hamming distance in the dataset, which has a common identifier."}, {"heading": "3 The proposed method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Motivation", "text": "To achieve a high rate of similarity, we need to take a closer look at the latent similarities of the unstructured data. In the following, we will point to a data pair with a common name called a \"positive pair\" and a data pair that does not have common names like a \"negative pair.\" The configuration space of a series of B hyperplanes is (SN \u2212 1)."}, {"heading": "3.2 Learning hyperplanes with the Markov chain Monte Carlo method", "text": "In this area, we are in a position to look for a solution."}, {"heading": "3.3 Evaluation function", "text": "When learning is performed by M-LSH, the type of evaluation function must be determined. A number of possible evaluation function types are considered below. First, some nomenclatures are introduced: PP denotes the set of all given positive pairs, and NP denotes the set of all given negative pairs. Angle subtended by the two characteristic vectors of a pair p in relation to the normal vector of a hyperplane are 1 (p) or 2 (p), respectively. The following subsets are defined.PP +: = {p \u00b2 PP | cos (p)).NP (2 (p).cos (p) > 0}, (1) NP \u2212: = {p \u00b2 NP cos | (p).cos (p).cos (p) < 0}."}, {"heading": "3.4 Sampling method for training data", "text": "In fact, most of us are able to play by the rules that they have adopted in recent years."}, {"heading": "4 Experiments and evaluation", "text": "We conducted experiments to measure the impact of the proposed method on a number of different sets of data. In these experiments, supervised learning was performed on the basis of data already labeled, and the data labels used in these experiments are all known. If the search results are available, the hamming distance between the query and the data in the database is calculated, and the top search results are sorted in ascending order of distance. For this purpose, the capture rate is defined as follows. Collection: = Number of data collected by the search results Total number of data searched for. (7) To evaluate performance, we used the precision rate and retrieval rate defined below. Precision: = Number of data elements with a composite label as a query for which search results were obtained. Number of data elements for which search results were obtained, (8) Recall: = Number of data elements with a composite label as a query for the search results."}, {"heading": "4.1 Experiments with an artificial data set", "text": "Using an artificial dataset, we confirmed the effects of M-LSH on learning. This artificial dataset consists of 300 data taken from a three-dimensional standard normal distribution. With the axes designated as x, y and z, we classified the data into two classes, depending on whether the x component was positive or not. As can be seen from the way in which the data is labeled, we want a hyperplane whose normal vector is ~ n = (\u00b1 1, 0, 0). Figure 4 shows the effects of LSH and M-LSH on learning with a bit string length of 1,024. The parameters of learning with M-LSH were as follows: Number of processing steps: 5, Number of time development steps in batch processing: 100, Number of data pairs used for learning in each stack process: 2,000, Number of evaluation functions used in learning with histomatic hit, Number of marginal hits with COUNomling method: Samping."}, {"heading": "4.2 Experimental data", "text": "This year it will be able to mention the aforementioned brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated csrcsrteeeaeFnln."}, {"heading": "4.3 Evaluation function performance", "text": "This year, it is so far that it is only a matter of time before it is ready, until it is ready."}, {"heading": "4.4 Evaluation of sampling methods", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "5 Comparison with existing", "text": "In this section we compare the performance of MLSH with that of the existing learning methods LSH, MLH and S-LSH. M-LSH uses the evaluation function COUNT and the method Randomhit-Boundarymiss. The number of sample data pairs is 1,000 for both the positive and the negative pairs. Figure 10 shows the recall precision curves for different data sets. Here, the number of bits is 1,024. These results show that M-LSH exceeds the existing learning methods for all data sets except LabelMe. In LabelMe, there are small regions where the S-LSH curve increases over the accuracy and reminder curves for M-LSH, but it can be said that a better overall performance is achieved with M-LSH. From the above results it is concluded that the proposed M-LSH learning method performs well."}, {"heading": "6 Summary and future works", "text": "In this paper, we have proposed a learning method for hyperplanes that MCMC uses. We have also considered evaluation functions and sampling methods in this learning method and evaluated their performance. As a result, we have confirmed that this proposed method outperforms the performance of existing learning methods. Finally, we mention the direction of future research. When using the MCMC method for learning, the ultimate positions of the particles are not at points that maximize the evaluation function. One way to solve this problem is to capture the particle localities and figure out at which point the evaluation function is maximized."}], "references": [{"title": "An optimal algorithm for approximate nearest neighbor searching fixed dimensions", "author": ["Sunil Arya", "David M. Mount", "Nathan S. Netanyahu", "Ruth Silverman", "Angela Y. Wu"], "venue": "J. ACM,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "idistance: An adaptive b+tree based indexing method for nearest neighbor search", "author": ["H.V. Jagadish", "Beng Chin Ooi", "Kian-Lee Tan", "Cui Yu", "Rui Zhang"], "venue": "ACM Trans. Database Syst.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "A quantitative analysis and performance study for similarity-search methods in highdimensional spaces", "author": ["Roger Weber", "Hans-J\u00f6rg Schek", "Stephen Blott"], "venue": "In Proceedings of the 24rd International Conference on Very Large Data Bases,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1998}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Moses S. Charikar"], "venue": "In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Learning reconfigurable hashing for diverse semantics", "author": ["Yadong Mu", "Xiangyu Chen", "Tat-Seng Chua", "Shuicheng Yan"], "venue": "ACM International Conference on Multimedia Retrieval,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Lost in binarization: query-adaptive ranking for similar image search with compact codes", "author": ["Yu-Gang Jiang", "Jun Wang", "Shih-Fu Chang"], "venue": "In Proceedings of the 1st ACM International Conference on Multimedia Retrieval,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Minimal loss hashing for compact binary codes", "author": ["Mohammad Norouzi", "David J. Fleet"], "venue": "In Lise Getoor and Tobias Scheffer, editors,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Localitysensitive hashing with margin based feature selection", "author": ["Makiko Konoshima", "Yui Noma"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Hyperplane arrangements and locality-sensitive hashing with lift", "author": ["Yui Noma", "Makiko Konoshima"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Monte carlo sampling methods using markov chains and their applications", "author": ["W K Hastings"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1970}, {"title": "Modeling the shape of the scene: A holistic representation of the spatial envelope", "author": ["Aude Oliva", "Antonio Torralba"], "venue": "Int. J. Comput. Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Small codes and large image databases for recognition", "author": ["Antonio Torralba", "Rob Fergus", "Yair Weiss"], "venue": "Proceedings of the IEEE Conf on Computer Vision and Pattern Recognition,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "For example, the literatures [1, 2] are two of them.", "startOffset": 29, "endOffset": 35}, {"referenceID": 1, "context": "For example, the literatures [1, 2] are two of them.", "startOffset": 29, "endOffset": 35}, {"referenceID": 2, "context": "Consequently, searches in higher-dimensional data using these methods end up having processing times that are similar to those of searches performed without using a special index [3].", "startOffset": 179, "endOffset": 182}, {"referenceID": 3, "context": "In a method called locality-sensitive hashing [4], the feature vectors are transformed into bit strings.", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "For this transformation, methods that involve the use of hyperplanes in feature space have been intensively studied [5\u20138].", "startOffset": 116, "endOffset": 121}, {"referenceID": 5, "context": "For this transformation, methods that involve the use of hyperplanes in feature space have been intensively studied [5\u20138].", "startOffset": 116, "endOffset": 121}, {"referenceID": 6, "context": "For this transformation, methods that involve the use of hyperplanes in feature space have been intensively studied [5\u20138].", "startOffset": 116, "endOffset": 121}, {"referenceID": 7, "context": "Studies aimed at increasing the precision of feature quantities associated with the hyperplane hashing method include the following references [9\u201311].", "startOffset": 143, "endOffset": 149}, {"referenceID": 8, "context": "Studies aimed at increasing the precision of feature quantities associated with the hyperplane hashing method include the following references [9\u201311].", "startOffset": 143, "endOffset": 149}, {"referenceID": 9, "context": "Studies aimed at increasing the precision of feature quantities associated with the hyperplane hashing method include the following references [9\u201311].", "startOffset": 143, "endOffset": 149}, {"referenceID": 9, "context": "In reference [11], an experiment is performed where the hashing of hyperplanes that do not pass through the origin is learned by learning the hashing of hyperplanes that do pass through the origin.", "startOffset": 13, "endOffset": 17}, {"referenceID": 4, "context": "In one hashing method, the B hyperplanes are set randomly [5].", "startOffset": 58, "endOffset": 61}, {"referenceID": 5, "context": "Other references such as [6\u201310] describe hashing methods that use hyperplanes.", "startOffset": 25, "endOffset": 31}, {"referenceID": 6, "context": "Other references such as [6\u201310] describe hashing methods that use hyperplanes.", "startOffset": 25, "endOffset": 31}, {"referenceID": 7, "context": "Other references such as [6\u201310] describe hashing methods that use hyperplanes.", "startOffset": 25, "endOffset": 31}, {"referenceID": 8, "context": "Other references such as [6\u201310] describe hashing methods that use hyperplanes.", "startOffset": 25, "endOffset": 31}, {"referenceID": 7, "context": "In particular, MLH [9] and S-LSH [10] are described in subsections 2.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "In particular, MLH [9] and S-LSH [10] are described in subsections 2.", "startOffset": 33, "endOffset": 37}, {"referenceID": 7, "context": "One existing learning method is Minimal Loss Hashing (MLH) [9].", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "In this subsection, we describe the concept of an existing learning method called locality-sensitive hashing with margin based feature selection (S-LSH) [10].", "startOffset": 153, "endOffset": 157}, {"referenceID": 8, "context": "S-LSH has been shown to have good learning performance in many data sets [10].", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "We use the Metropolis-Hastings algorithm for MCMC [12].", "startOffset": 50, "endOffset": 54}, {"referenceID": 8, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "\u2022 LabelMe LabelMe data using 512-dimensional Gist feature quantities [15] extracted from image data published in Ref.", "startOffset": 69, "endOffset": 73}, {"referenceID": 12, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Since we used a fixed value of T = 1 in this evaluation, the index of the evaluation function is confined to the range [0, 2] or [0, 4].", "startOffset": 119, "endOffset": 125}, {"referenceID": 3, "context": "Since we used a fixed value of T = 1 in this evaluation, the index of the evaluation function is confined to the range [0, 2] or [0, 4].", "startOffset": 129, "endOffset": 135}], "year": 2013, "abstractText": "Since Hamming distances can be calculated by bitwise computations, they can be calculated with less computational load than L2 distances. Similarity searches can therefore be performed faster in Hamming distance space. The elements of Hamming distance space are bit strings. On the other hand, the arrangement of hyperplanes induce the transformation from the feature vectors into feature bit strings. This transformation method is a type of locality-sensitive hashing that has been attracting attention as a way of performing approximate similarity searches at high speed. Supervised learning of hyperplane arrangements allows us to obtain a method that transforms them into feature bit strings reflecting the information of labels applied to higher-dimensional feature vectors. In this paper, we propose a supervised learning method for hyperplane arrangements in feature space that uses a Markov chain Monte Carlo (MCMC) method. We consider the probability density functions used during learning, and evaluate their performance. We also consider the sampling method for learning data pairs needed in learning, and we evaluate its performance. We confirm that the accuracy of this learning method when using a suitable probability density function and sampling method is greater than the accuracy of existing learning methods. Keyword: Higher dimensional feature vector, Locality-sensitive hashing, Arrangement of hyperplanes, Similarity search, Markov chain Monte Carlo, Low-temperature limit", "creator": "LaTeX with hyperref package"}}}