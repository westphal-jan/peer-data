{"id": "1303.4207", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Mar-2013", "title": "Improving CUR Matrix Decomposition and the Nystr\\\"{o}m Approximation via Adaptive Sampling", "abstract": "The Nystrom method has been widely used to compute low-rank approximations to positive semidefinite matrices. Much work has been done on the sampling techniques for the Nystrom method, and many were guaranteed with upper error bounds. However, the lower bounds, especially the relative error bounds, were largely unknown. We explore in this paper the lower bounds of the Nystrom method to show how bad the Nystrom method can perform. We establish both additive and relative error lower bounds for the Nystrom method. Interestingly, some of the relative-error lower bounds we established can be attained by some algorithms, which indicates some lower bounds we establish are tight. However, some lower bounds reveal the downside of the Nystrom method that they are much worse than the column selection problem and the CUR problem. Sadly, we also show that some bounds cannot be improved by the ensemble Nystrom method.", "histories": [["v1", "Mon, 18 Mar 2013 11:17:55 GMT  (76kb)", "http://arxiv.org/abs/1303.4207v1", "15 pages, 2 figures, preprint"], ["v2", "Wed, 20 Mar 2013 04:20:45 GMT  (61kb)", "http://arxiv.org/abs/1303.4207v2", "15 pages, 2 figures, preprint"], ["v3", "Tue, 9 Apr 2013 08:28:43 GMT  (168kb)", "http://arxiv.org/abs/1303.4207v3", "A proportion of this work has appeared in NIPS 2012 [arXiv:1210.1461]"], ["v4", "Tue, 28 May 2013 07:17:40 GMT  (208kb,D)", "http://arxiv.org/abs/1303.4207v4", "A proportion of this work has appeared in NIPS 2012 [arXiv:1210.1461]"], ["v5", "Fri, 31 May 2013 05:12:41 GMT  (217kb,D)", "http://arxiv.org/abs/1303.4207v5", "A proportion of this work has appeared in NIPS 2012"], ["v6", "Thu, 12 Sep 2013 06:56:13 GMT  (226kb,D)", "http://arxiv.org/abs/1303.4207v6", null], ["v7", "Tue, 1 Oct 2013 06:31:11 GMT  (226kb,D)", "http://arxiv.org/abs/1303.4207v7", null]], "COMMENTS": "15 pages, 2 figures, preprint", "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["shusen wang", "zhihua zhang"], "accepted": false, "id": "1303.4207"}, "pdf": {"name": "1303.4207.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zhihua Zhang"], "emails": ["wss@zju.edu.cn", "zhzhang@zju.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 130 3.42 07v1 [cs.LG] 1 8"}, {"heading": "1. Introduction", "text": "In Section 1.1 we give a non-technical introduction to the Nystro \ufffd m method and the related problem of column selection and the CUR problem. In Section 1.2 we summarize our contributions and list our main findings. In Section 1.3 we give an overview of the rest of this work."}, {"heading": "1.1 Backgrounds", "text": "The Nystro-m method is a kind of low-rank matrix approximation approach that is mainly used to accelerate kernel methods.The Nystro-m method can approximate any positive semi-defined matrix that uses only a subset of its columns so that it mitigates computation and memory problems when the kernel matrix is large.The Nystro-m method has been widely applied in the machine learning community (e.g. the Nystro-m method was applied to the Gaussian Williams and Seeger process (2001), Kernel SVM (Kumar et al., 2012, Zhang et al., 2008), spectral clustering method (Nashorn et al., 2004) and Kernel PCA method (Talwalkar et al., 2008, Zhang et al., 2008?).Williams and Seeger (2001) are the first to apply the Nystro-m method for low-rank approximation."}, {"heading": "1.2 Summary of Contributions", "text": "To our knowledge, we are the first to determine the lower limits of relative error for the Nystro-m method. Interestingly enough, two of our lower limits of relative error can be reached by some algorithms in (Gittens and Mahoney, 2013), which indicates that the two lower limits are narrow and the corresponding upper limits (Gittens and Mahoney, 2013) are almost optimal. We summarize the lower limits in the table below. Here, A - refers to the original matrix, A - the approach to A, Ak the best approach to A - A - A - A - A - F maxi, J - A - A - A - A - A - A - A - 2 maxi, j - C - (NY - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A - A"}, {"heading": "1.3 Organization of This Paper", "text": "In section 2 we define the notations, describe the Nystro-m method and the Ensemble-Nystro-m method, and construct the contradiction case for deriving the lower limits. In section 3 we define the lower limits for the Nystro-m method and the Ensemble-Nystro-m method. In section 4 we discuss some previous results related to this work. All technical evidence can be found in the appendix."}, {"heading": "2. Preliminaries", "text": "In Section 2.1 we present the notations used in this essay. In Section 2.2 we briefly describe the Nystro m method and the Ensemble Nystro m method. In Section 2.3 we construct the contradiction case used in this essay to derive the lower limits."}, {"heading": "2.1 Notations", "text": "For a matrix A = [aij], which consists of its i to j-th columns, a (i) should be its i-th row, aj its j-th column, and Ai: j should be a submatrix consisting of its i to j-th columns. Furthermore, the m \u00b7 m identity matrix should be denoted, 1m stands for an all-one column vector of size m, 1mn stands for a m \u00b7 n all-one matrix, and 0 stands for an all-zero matrix with a corresponding size. If we leave in the sense of rank (A) and k \u2264, the singular value decomposition (SVD) of A can be written as A = 0\u03c3A, iuA, iv T A, i = UAV AV T T T T T T A = [UA, k UA, k UA, k \u2264] is the universal value, we in the sense of universal - and we in the sense of universal A."}, {"heading": "2.2 The Nystro\u0308m Methods", "text": "Given a m \u00b7 m positive semidefinitive matrixA = [W AT21 A21 A22], the standard Nystro \ufffd m method is used to construct c columns of A. Without loss of generality, we assume that the first c columns are selected, i.e. C = [W A21]. Then the Rang-c Nystro \ufffd m method in (Kumar et al., 2009a) selects a collection of t samples, each sample C (i), (i = 1, \u00b7 \u00b7, t) contains c columns of A. Then the ensemble method combines the samples to construct an approximation A."}, {"heading": "2.3 Construction of The Adversary Case", "text": "We construct a m \u00b7 m positive definitive matrix A as follows: A = (1 \u2212 \u03b1) Im + \u03b11mm = 1 \u03b1 \u00b7 \u00b7 \u03b1 \u03b1 1 \u00b7 \u00b7 \u00b7 \u03b1......... \u03b1 \u00b7 \u00b7 \u00b7 \u00b7 1 = [W AT21 A21 A22], (1) where \u03b1 [0, 1). It is easy to verify xTAx \u2265 0 for all x-Rm and xTAx > 0 if x 6 = 0. We show some properties of A in Lemma 1.Lemma 1 Let Ak be the closest rank-k approximation to the matrix A constructed in (1). Then we have these."}, {"heading": "3. Main Results", "text": "In this section we show both the additive and relative lower limits of the Nystro-m method and the Ensemble Nystro-m method."}, {"heading": "3.1 Lower Bounds of The Standard Nystro\u0308m Method", "text": "Theorem 2 (Additive error lower limits of the Nystro-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A"}, {"heading": "3.2 Lower Bounds of The Ensemble Nystro\u0308m Method", "text": "Theorem 5 (additive error lower limits of the Nystro-m method) Suppose the Nystro-m method of the ensemble selects a collection of t samples, each sample containing C (i = 1, \u00b7 \u00b7 \u00b7, t) c columns of A without overlapping. In a matrix with all diagonal samples equal to one and off-diagonal samples equal \u03b1 [0, 1), the approximate error of the Nystro-m method is lower if A-enst, c-A-enst, c-A-enst, c-A-enst, c-F \u2265 (1 \u2212 \u03b1) (m \u2212 2c + c t) (1 + m \u2212 2 (c + 1 \u2212 \u03b1) 2), where A-enst, c = 1 t-enst i = 1 C (i) W (i) W (i) c (i).Theorem 6 (relative error lower limits of the Nystro-m method)."}, {"heading": "4. Comparisons with Related Work", "text": "In Section 4.1, we compare with the previous work of Jin et al. (2011) and show the superiority of our results. In Section 4.2, we show that the lower limits of relative error in spectral and nuclear standards are actually narrow, as they coincide exactly with an upper limit. In Section 4.3, we compare the Nystro \ufffd m method with the CUR method and show the disadvantages of the Nystro \ufffd m method."}, {"heading": "4.1 An Additive-Error Lower Bound in Spectral Norm", "text": "The previous work by Jin et al. (2011) established a lower error margin for the Nystro-m method, which we show in the following proposal. Proposal 8 There is a positive semidefinitive matrix A-Rm-m, in which (i) all of its diagonal entries are 1, and (ii) the first c + 1 eigenvalues of A are in the order of A (m / c), so that for any sampling strategy that selects c columns to form C, the approximation error of the Nystro-m method is lower than the approximation error of A-CW-CT-2 (mc), provided that c > 64 (log 4) 2m2.Note 9 This error is not particularly interesting in (Jin et al., 2011, theorem 4) because it falls below the approximation error on a matrix with very large standards."}, {"heading": "4.2 Optimal Relative-Error Upper Bounds", "text": "Recent work by Gittens and Mahoney (2013) has shown several relative error limits for the Nystro-m Method. We show their results in the following proposal. Proposal 10 If the semidefinitive matrix A-Rm \u00b7 m is positive and an integer k \u2264 m is used, the coherence of a dominant k-dimensional invariant subspace of A. If we fix a non-zero error probability and an accuracy factor of 0, 1. If c-2k log (k / \u03b4) columns of A are uniformly randomly selected, then the columns of A-CT are uniformly selected, then the probability of A-CW \u00b2 2-Ak \u00b2 2 \u2264 1 + m (1 \u2212 \u0432) c, and the relative error limit of A-CW \u00b2 A \u2212 Ak-2k log (k / \u043c) columns of A + 1 (1 \u2212 \u043c) are randomly selected. Proposal 10 shows that if c is large, the relative error limit upper limit in the spectral and the relative limit exactly matches the relative limit in our standard 3-m."}, {"heading": "4.3 Relative-Error Upper Bound of CUR", "text": "We discussed in the introductory section that the Nystro-m method is closely related to the CUR problem. We show in the following sentence that the state-of-the-art CUR algorithm in (Wang and Zhang, 2012) has achieved a relative error ratio of E-A-CUR-2 F-A-Ak-2F \u2264 1 + 4kc (1 + o (1)). Proposal 11 Given a matrix A-Rm \u00b7 n and a positive integer k-min {m, n}, a CUR algorithm randomly selects c = 4kc (1 + o (1)) columns from A to construct C-Rm-c, and then selects r = 4cTherefore (1 + o (1) rows from A to construct R-Rr-x-n. The relative error ratio is determined by E-CUR-2F-C-C (C-AR).2F-R-small (1 + o (1) to construct R-Rx-n."}, {"heading": "5. Conclusions", "text": "In this paper, we have examined the lower limits of the Nystro-m method and the Ensemble Nystro-m method. This is the first work to perform a systematic analysis of the lower limits of the Nystro-m method. Among the lower limits we have defined, two relative error limits are narrow and achievable, while the density of the remaining limits remains unknown. The lower limits show that the Nystro-m method can sometimes be very bad; the lower error limits can be even worse than the upper limits of some CUR algorithms. Therefore, we have continued to study the Ensemble Nystro-m method in order to remedy the situation, but having found only the Nystro-m method does not help to improve the lower limits."}, {"heading": "Appendix A. Proof of Lemma 1", "text": "Evidence The square Frobenius standard of A is: \"A-2F = \u2211 i, j-a-2 ij = m + (m 2 \u2212 m) \u03b12.\" The spectral standard, i.e. the largest singular value of A is \"A-2 = \u03c3A,\" \"1 = max\" x-2 \u2264 1 \"Ax-2 = 1 + m\u03b1 \u2212 \u03b1.It is also easy to verify that\" A, \"\" 2 = \u00b7 \u00b7 \u00b7 \u00b7 A, m. Since \"A-2F\" = \"mi = 1 \u03c3A,\" \"i,\" for each j > 1, \"we have\" 2A, \"\" J = 1 m \u2212 1 (\"A-2F \u2212 \u03c32A, 1) = (1 \u2212 \u03b1) 2.Thus, we have\" A-Ak \"2 = 1 \u2212 \u03b1 for all 1 \u2264 k < m. Finally, we have\" A-Ak, \"2F =\" A-2F \u2212 k \"(m \u2212 k) (1),\" k \u00b2, \"(A-\u00b2),\" () (m)."}, {"heading": "Appendix B. Proof of Theorem 2", "text": "The proof of the residuality of the Nystro-m-approximation is: A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-A-"}, {"heading": "Appendix C. Proof of Theorem 5", "text": "The proof that we use the matrix A, which is constructed in Theorem 3, is easy to verify that W (1) = \u00b7 \u00b7 W (t), so instead we use the notation W \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212"}], "references": [{"title": "Spectral methods in machine learning and new strategies for very large datasets", "author": ["Mohamed-Ali Belabbas", "Patrick J Wolfe"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Belabbas and Wolfe.,? \\Q2009\\E", "shortCiteRegEx": "Belabbas and Wolfe.", "year": 2009}, {"title": "Generalized Inverses: Theory and Applications", "author": ["Adi Ben-Israel", "Thomas N.E. Greville"], "venue": "Second Edition. Springer,", "citeRegEx": "Ben.Israel and Greville.,? \\Q2003\\E", "shortCiteRegEx": "Ben.Israel and Greville.", "year": 2003}, {"title": "CUR from a sparse optimization viewpoint", "author": ["Jacob Bien", "Ya Xu", "Michael W Mahoney"], "venue": null, "citeRegEx": "Bien et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bien et al\\.", "year": 2010}, {"title": "Near-optimal column-based matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "CoRR, abs/1103.0995,", "citeRegEx": "Boutsidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2011}, {"title": "Near optimal column-based matrix reconstruction", "author": ["Christos Boutsidis", "Petros Drineas", "Malik Magdon-Ismail"], "venue": "In Proceedings of the 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Boutsidis et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Boutsidis et al\\.", "year": 2011}, {"title": "On the impact of kernel approximation on learning accuracy", "author": ["Corinna Cortes", "Mehryar Mohri", "Ameet Talwalkar"], "venue": "In Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Cortes et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cortes et al\\.", "year": 2010}, {"title": "Efficient volume sampling for row/column subset selection", "author": ["Amit Deshpande", "Luis Rademacher"], "venue": "In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science,", "citeRegEx": "Deshpande and Rademacher.,? \\Q2010\\E", "shortCiteRegEx": "Deshpande and Rademacher.", "year": 2010}, {"title": "Matrix approximation and projective clustering via volume sampling", "author": ["Amit Deshpande", "Luis Rademacher", "Santosh Vempala", "Grant Wang"], "venue": "Theory of Computing,", "citeRegEx": "Deshpande et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Deshpande et al\\.", "year": 2006}, {"title": "Pass-efficient algorithms for approximating large matrices", "author": ["Petros Drineas"], "venue": "Proceeding of the 14th Annual ACM-SIAM Symposium on Dicrete Algorithms,", "citeRegEx": "Drineas.,? \\Q2003\\E", "shortCiteRegEx": "Drineas.", "year": 2003}, {"title": "On the Nystr\u00f6m method for approximating a gram matrix for improved kernel-based learning", "author": ["Petros Drineas", "Michael W. Mahoney"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Drineas and Mahoney.,? \\Q2005\\E", "shortCiteRegEx": "Drineas and Mahoney.", "year": 2005}, {"title": "Fast monte carlo algorithms for matrices iii: Computing a compressed approximate matrix decomposition", "author": ["Petros Drineas", "Ravi Kannan", "Michael W. Mahoney"], "venue": "SIAM Journal on Computing,", "citeRegEx": "Drineas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2006}, {"title": "Relative-error CUR matrix decompositions", "author": ["Petros Drineas", "Michael W. Mahoney", "S. Muthukrishnan"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "Drineas et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Drineas et al\\.", "year": 2008}, {"title": "Spectral grouping using the nystrom method", "author": ["Charless Fowlkes", "Serge Belongie", "Fan Chung", "Jitendra Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "Fowlkes et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Fowlkes et al\\.", "year": 2004}, {"title": "Fast monte-carlo algorithms for finding low-rank approximations", "author": ["Alan Frieze", "Ravi Kannan", "Santosh Vempala"], "venue": "Journal of the ACM,", "citeRegEx": "Frieze et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Frieze et al\\.", "year": 2004}, {"title": "The spectral norm error of the naive nystrom extension", "author": ["Alex Gittens"], "venue": "arXiv preprint arXiv:1110.5305,", "citeRegEx": "Gittens.,? \\Q2011\\E", "shortCiteRegEx": "Gittens.", "year": 2011}, {"title": "Revisiting the nystrom method for improved largescale machine learning", "author": ["Alex Gittens", "Michael W Mahoney"], "venue": "arXiv preprint arXiv:1303.1849,", "citeRegEx": "Gittens and Mahoney.,? \\Q2013\\E", "shortCiteRegEx": "Gittens and Mahoney.", "year": 2013}, {"title": "A theory of pseudoskeleton approximations", "author": ["S.A. Goreinov", "E.E. Tyrtyshnikov", "N.L. Zamarashkin"], "venue": "Linear Algebra and Its Applications,", "citeRegEx": "Goreinov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goreinov et al\\.", "year": 1997}, {"title": "Pseudo-skeleton approximations by matrices of maximal volume", "author": ["S.A. Goreinov", "N.L. Zamarashkin", "E.E. Tyrtyshnikov"], "venue": "Mathematical Notes,", "citeRegEx": "Goreinov et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Goreinov et al\\.", "year": 1997}, {"title": "Optimal column-based low-rank matrix reconstruction", "author": ["Venkatesan Guruswami", "Ali Kemal Sinop"], "venue": "In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "Guruswami and Sinop.,? \\Q2012\\E", "shortCiteRegEx": "Guruswami and Sinop.", "year": 2012}, {"title": "Improved bound for the Nystr\u00f6m method and its application to kernel classification", "author": ["Rong Jin", "Tianbao Yang", "Mehrdad Mahdavi"], "venue": "CoRR, abs/1111.2262,", "citeRegEx": "Jin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Jin et al\\.", "year": 2011}, {"title": "Ensemble nystrom method", "author": ["Sanjiv Kumar", "Mehryar Mohri", "Ameet Talwalkar"], "venue": "In Neural Information Processing Systems,", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "On sampling-based approximate spectral decomposition", "author": ["Sanjiv Kumar", "Mehryar Mohri", "Ameet Talwalkar"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "Kumar et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2009}, {"title": "Sampling methods for the nystr\u00f6m method", "author": ["Sanjiv Kumar", "Mehryar Mohri", "Ameet Talwalkar"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Kumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kumar et al\\.", "year": 2012}, {"title": "Making large-scale nystr\u00f6m approximation possible", "author": ["Mu Li", "James T Kwok", "Bao-Liang Lu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Divide-and-conquer matrix factorization", "author": ["Lester Mackey", "Ameet Talwalkar", "Michael I. Jordan"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Mackey et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mackey et al\\.", "year": 2011}, {"title": "Randomized algorithms for matrices and data", "author": ["Michael W. Mahoney"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Mahoney.,? \\Q2011\\E", "shortCiteRegEx": "Mahoney.", "year": 2011}, {"title": "CUR matrix decompositions for improved data analysis", "author": ["Michael W. Mahoney", "Petros Drineas"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Mahoney and Drineas.,? \\Q2009\\E", "shortCiteRegEx": "Mahoney and Drineas.", "year": 2009}, {"title": "Fast embedding of sparse music similarity graphs", "author": ["John C Platt"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Platt.,? \\Q2004\\E", "shortCiteRegEx": "Platt.", "year": 2004}, {"title": "Global versus local methods in nonlinear dimensionality reduction", "author": ["Vin de Silva", "Joshua B Tenenbaum"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Silva and Tenenbaum.,? \\Q2002\\E", "shortCiteRegEx": "Silva and Tenenbaum.", "year": 2002}, {"title": "Four algorithms for the the efficient computation of truncated pivoted QR approximations to a sparse matrix", "author": ["G.W. Stewart"], "venue": "Numerische Mathematik,", "citeRegEx": "Stewart.,? \\Q1999\\E", "shortCiteRegEx": "Stewart.", "year": 1999}, {"title": "Matrix coherence and the nystrom method", "author": ["Ameet Talwalkar", "Afshin Rostamizadeh"], "venue": "arXiv preprint arXiv:1004.2008,", "citeRegEx": "Talwalkar and Rostamizadeh.,? \\Q2010\\E", "shortCiteRegEx": "Talwalkar and Rostamizadeh.", "year": 2010}, {"title": "Large-scale manifold learning", "author": ["Ameet Talwalkar", "Sanjiv Kumar", "Henry Rowley"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "Talwalkar et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Talwalkar et al\\.", "year": 2008}, {"title": "Incomplete cross approximation in the mosaic-skeleton", "author": ["Eugene E. Tyrtyshnikov"], "venue": "method. Computing,", "citeRegEx": "Tyrtyshnikov.,? \\Q2000\\E", "shortCiteRegEx": "Tyrtyshnikov.", "year": 2000}, {"title": "A scalable CUR matrix decomposition algorithm: Lower time complexity and tighter bound", "author": ["Shusen Wang", "Zhihua Zhang"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Wang and Zhang.,? \\Q2012\\E", "shortCiteRegEx": "Wang and Zhang.", "year": 2012}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["Christopher Williams", "Matthias Seeger"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Williams and Seeger.,? \\Q2001\\E", "shortCiteRegEx": "Williams and Seeger.", "year": 2001}, {"title": "Improved nystr\u00f6m low-rank approximation and error analysis", "author": ["Kai Zhang", "Ivor W Tsang", "James T Kwok"], "venue": "In Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "Zhang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 12, "context": ", 2008), spectral clustering (Fowlkes et al., 2004), and kernel PCA (Talwalkar et al.", "startOffset": 29, "endOffset": 51}, {"referenceID": 29, "context": "For example, the Nystr\u00f6m method has been applied to Gaussian process Williams and Seeger (2001), kernel SVM (Kumar et al.", "startOffset": 69, "endOffset": 96}, {"referenceID": 12, "context": ", 2008), spectral clustering (Fowlkes et al., 2004), and kernel PCA (Talwalkar et al., 2008, Zhang et al., 2008, ?). Williams and Seeger (2001) are the first to use the Nystr\u00f6m method for low-rank matrix approximation.", "startOffset": 30, "endOffset": 144}, {"referenceID": 12, "context": ", 2008), spectral clustering (Fowlkes et al., 2004), and kernel PCA (Talwalkar et al., 2008, Zhang et al., 2008, ?). Williams and Seeger (2001) are the first to use the Nystr\u00f6m method for low-rank matrix approximation. The uniform column selection proposed by Williams and Seeger (2001) is 1", "startOffset": 30, "endOffset": 287}, {"referenceID": 25, "context": "However, as was argued in the review paper (Mahoney, 2011), additive-error bounds are less convincing than the relative-error bounds.", "startOffset": 43, "endOffset": 58}, {"referenceID": 7, "context": "The most advanced relative-error columns selection techniques include the adaptive sampling algorithm in (Deshpande et al., 2006), the nearoptimal algorithm in (Boutsidis et al.", "startOffset": 105, "endOffset": 129}, {"referenceID": 18, "context": ", 2011b), and the optimal algorithm in (Guruswami and Sinop, 2012) whose upper bound matches the lower bound of column selection problem (Boutsidis et al.", "startOffset": 39, "endOffset": 66}, {"referenceID": 33, "context": "3 that the lower error bound of the Nystr\u00f6m method is even worse than the upper bound of the state-of-the-art CUR algorithm in (Wang and Zhang, 2012).", "startOffset": 127, "endOffset": 149}, {"referenceID": 1, "context": "the simplest but the most widely used in practice (Fowlkes et al., 2004, Talwalkar et al., 2008, Gittens and Mahoney, 2013). Following the work of Williams and Seeger (2001), many uniform and non-uniform sampling techniques are proposed and analyzed (Drineas and Mahoney, 2005, Zhang et al.", "startOffset": 51, "endOffset": 174}, {"referenceID": 0, "context": ", 2008, Belabbas and Wolfe, 2009, Kumar et al., 2009b,a, Cortes et al., 2010, Li et al., 2010, Talwalkar and Rostamizadeh, 2010, Gittens, 2011, Jin et al., 2011, Mackey et al., 2011, Kumar et al., 2012, Gittens and Mahoney, 2013). Much work has been done on the upper bounds of the sampling techniques for the Nystr\u00f6m method. Most of the work, e.g., (Drineas and Mahoney, 2005, Li et al., 2010, Kumar et al., 2009a, Jin et al., 2011, Kumar et al., 2012), studied the additive-error bound. By further making assumptions on matrix coherence, better additive bounds were obtained by Talwalkar and Rostamizadeh (2010), Jin et al.", "startOffset": 8, "endOffset": 614}, {"referenceID": 0, "context": ", 2008, Belabbas and Wolfe, 2009, Kumar et al., 2009b,a, Cortes et al., 2010, Li et al., 2010, Talwalkar and Rostamizadeh, 2010, Gittens, 2011, Jin et al., 2011, Mackey et al., 2011, Kumar et al., 2012, Gittens and Mahoney, 2013). Much work has been done on the upper bounds of the sampling techniques for the Nystr\u00f6m method. Most of the work, e.g., (Drineas and Mahoney, 2005, Li et al., 2010, Kumar et al., 2009a, Jin et al., 2011, Kumar et al., 2012), studied the additive-error bound. By further making assumptions on matrix coherence, better additive bounds were obtained by Talwalkar and Rostamizadeh (2010), Jin et al. (2011), Mackey et al.", "startOffset": 8, "endOffset": 633}, {"referenceID": 0, "context": ", 2008, Belabbas and Wolfe, 2009, Kumar et al., 2009b,a, Cortes et al., 2010, Li et al., 2010, Talwalkar and Rostamizadeh, 2010, Gittens, 2011, Jin et al., 2011, Mackey et al., 2011, Kumar et al., 2012, Gittens and Mahoney, 2013). Much work has been done on the upper bounds of the sampling techniques for the Nystr\u00f6m method. Most of the work, e.g., (Drineas and Mahoney, 2005, Li et al., 2010, Kumar et al., 2009a, Jin et al., 2011, Kumar et al., 2012), studied the additive-error bound. By further making assumptions on matrix coherence, better additive bounds were obtained by Talwalkar and Rostamizadeh (2010), Jin et al. (2011), Mackey et al. (2011). However, as was argued in the review paper (Mahoney, 2011), additive-error bounds are less convincing than the relative-error bounds.", "startOffset": 8, "endOffset": 655}, {"referenceID": 0, "context": ", 2008, Belabbas and Wolfe, 2009, Kumar et al., 2009b,a, Cortes et al., 2010, Li et al., 2010, Talwalkar and Rostamizadeh, 2010, Gittens, 2011, Jin et al., 2011, Mackey et al., 2011, Kumar et al., 2012, Gittens and Mahoney, 2013). Much work has been done on the upper bounds of the sampling techniques for the Nystr\u00f6m method. Most of the work, e.g., (Drineas and Mahoney, 2005, Li et al., 2010, Kumar et al., 2009a, Jin et al., 2011, Kumar et al., 2012), studied the additive-error bound. By further making assumptions on matrix coherence, better additive bounds were obtained by Talwalkar and Rostamizadeh (2010), Jin et al. (2011), Mackey et al. (2011). However, as was argued in the review paper (Mahoney, 2011), additive-error bounds are less convincing than the relative-error bounds. The recent work of Gittens (2011) provided the relative-error bounds for the first time, and then the bounds were sharpened by Gittens and Mahoney (2013).", "startOffset": 8, "endOffset": 824}, {"referenceID": 0, "context": ", 2008, Belabbas and Wolfe, 2009, Kumar et al., 2009b,a, Cortes et al., 2010, Li et al., 2010, Talwalkar and Rostamizadeh, 2010, Gittens, 2011, Jin et al., 2011, Mackey et al., 2011, Kumar et al., 2012, Gittens and Mahoney, 2013). Much work has been done on the upper bounds of the sampling techniques for the Nystr\u00f6m method. Most of the work, e.g., (Drineas and Mahoney, 2005, Li et al., 2010, Kumar et al., 2009a, Jin et al., 2011, Kumar et al., 2012), studied the additive-error bound. By further making assumptions on matrix coherence, better additive bounds were obtained by Talwalkar and Rostamizadeh (2010), Jin et al. (2011), Mackey et al. (2011). However, as was argued in the review paper (Mahoney, 2011), additive-error bounds are less convincing than the relative-error bounds. The recent work of Gittens (2011) provided the relative-error bounds for the first time, and then the bounds were sharpened by Gittens and Mahoney (2013). Furthermore, a method called the ensemble Nystr\u00f6m method was proposed by Kumar et al.", "startOffset": 8, "endOffset": 944}, {"referenceID": 15, "context": "Interestingly, two of our relative-error lower bounds can be attain by some algorithms in (Gittens and Mahoney, 2013), which indicates the two lower bounds are tight and the corresponding upper bounds (Gittens and Mahoney, 2013) are near optimal.", "startOffset": 90, "endOffset": 117}, {"referenceID": 15, "context": "Interestingly, two of our relative-error lower bounds can be attain by some algorithms in (Gittens and Mahoney, 2013), which indicates the two lower bounds are tight and the corresponding upper bounds (Gittens and Mahoney, 2013) are near optimal.", "startOffset": 201, "endOffset": 228}, {"referenceID": 1, "context": "Furthermore, let A\u2020 = VA,\u03c1\u03a3 \u22121 A,\u03c1U T A,\u03c1 be the Moore-Penrose inverse of A (Ben-Israel and Greville, 2003).", "startOffset": 76, "endOffset": 107}, {"referenceID": 17, "context": "1 the lower bounds established in Theorem 2 and 3 are better than the previous work of Jin et al. (2011). Actually, relative-error lower bounds in spectral norm and nuclear norm we established are tight because they are attained by some algorithms in (Gittens and Mahoney, 2013, Lemma 4).", "startOffset": 87, "endOffset": 105}, {"referenceID": 19, "context": "1 we compare with the previous work of Jin et al. (2011) and show the superiority of our results.", "startOffset": 39, "endOffset": 57}, {"referenceID": 19, "context": "1 An Additive-Error Lower Bound in Spectral Norm The previous work of Jin et al. (2011) established a lower error bound for the Nystr\u00f6m method.", "startOffset": 70, "endOffset": 88}, {"referenceID": 14, "context": "2 Optimal Relative-Error Upper Bounds The very recent work of Gittens and Mahoney (2013) established several relative-error upper bound for the Nystr\u00f6m method.", "startOffset": 62, "endOffset": 89}, {"referenceID": 33, "context": "We show in the following proposition that the state-of-the-art CUR algorithm in (Wang and Zhang, 2012) achieved a relative-error ratio of E\u2016A\u2212CUR\u20162 F \u2016A\u2212Ak\u2016F \u2264 1 + 4k c (", "startOffset": 80, "endOffset": 102}, {"referenceID": 14, "context": "It also shows the relative-error upper bounds in spectral norm and nuclear norm in Gittens and Mahoney (2013) are near optimal.", "startOffset": 83, "endOffset": 110}], "year": 2017, "abstractText": "The Nystr\u00f6m method has been widely used to compute low-rank approximations to positive semidefinite matrices. Much work has been done on the sampling techniques for the Nystr\u00f6m method, and many were guaranteed with upper error bounds. However, the lower bounds, especially the relative error bounds, were largely unknown. We explore in this paper the lower bounds of the Nystr\u00f6m method to show how bad the Nystr\u00f6m method can perform. We establish both additive and relative error lower bounds for the Nystr\u00f6m method. Interestingly, some of the relative-error lower bounds we established can be attained by some algorithms, which indicates some lower bounds we establish are tight! However, some lower bounds reveal the downside of the Nystr\u00f6m method that they are much worse than the column selection problem and the CUR problem. Sadly, we also show that some bounds cannot be improved by the ensemble Nystr\u00f6m method.", "creator": "LaTeX with hyperref package"}}}