{"id": "1303.5709", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Theory Refinement on Bayesian Networks", "abstract": "Theory refinement is the task of updating a domain theory in the light of new cases, to be done automatically or with some expert assistance. The problem of theory refinement under uncertainty is reviewed here in the context of Bayesian statistics, a theory of belief revision. The problem is reduced to an incremental learning task as follows: the learning system is initially primed with a partial theory supplied by a domain expert, and thereafter maintains its own internal representation of alternative theories which is able to be interrogated by the domain expert and able to be incrementally refined from data. Algorithms for refinement of Bayesian networks are presented to illustrate what is meant by \"partial theory\", \"alternative theory representation\", etc. The algorithms are an incremental variant of batch learning algorithms from the literature so can work well in batch and incremental mode.", "histories": [["v1", "Wed, 20 Mar 2013 15:29:57 GMT  (476kb)", "http://arxiv.org/abs/1303.5709v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["wray l buntine"], "accepted": false, "id": "1303.5709"}, "pdf": {"name": "1303.5709.pdf", "metadata": {"source": "CRF", "title": "Theory Refinement on Bayesian Networks", "authors": ["Wray Buntine"], "emails": ["wray@ptolemy.arc.nasa.gov"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}], "references": [{"title": "Advanced Econometrics. Har\u00ad vard", "author": ["T. Amemiya"], "venue": null, "citeRegEx": "Amemiya,? \\Q1985\\E", "shortCiteRegEx": "Amemiya", "year": 1985}, {"title": "Sta.ti.st-ica.l Decision Theory and Bayes\u00b7ian Analysis", "author": ["J. Berger"], "venue": null, "citeRegEx": "Berger,? \\Q1985\\E", "shortCiteRegEx": "Berger", "year": 1985}, {"title": "Learning classification trees", "author": ["W. Buntine"], "venue": "Technical Report FIA-90-12-19-01, RIACS and NASA Ames Research Center, Moffett Field, CA. Paper presented at Third International Workshop on Artificial Intelligence and Statistics", "citeRegEx": "Buntine,? \\Q1990\\E", "shortCiteRegEx": "Buntine", "year": 1990}, {"title": "A Theory of Learning Classifi\u00ad cation Rules", "author": ["W. Buntine"], "venue": "PhD thesis,", "citeRegEx": "Buntine,? \\Q1990\\E", "shortCiteRegEx": "Buntine", "year": 1990}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G. Cooper", "E. Herskovits"], "venue": "Technical Report KSL-91-02,", "citeRegEx": "Cooper and Herskovits,? \\Q1991\\E", "shortCiteRegEx": "Cooper and Herskovits", "year": 1991}, {"title": "Extensions to the CART algo\u00ad rithm", "author": ["S. Crawford"], "venue": "International Journal of Man-Machine Stud\u00ad ies,", "citeRegEx": "Crawford,? \\Q1989\\E", "shortCiteRegEx": "Crawford", "year": 1989}, {"title": "Max\u00ad imum likelihood from incomplete data via the EM algorithm", "author": ["A. Dempster", "N. Laird", "D. Rubin"], "venue": "J. Roy. Statist. Soc. B,", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Au\u00ad tomatic knowledge base refinement for classification systems", "author": ["A. Ginsberg", "S. Weiss", "P. Politakis"], "venue": "Artificial Intelligence,", "citeRegEx": "Ginsberg et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Ginsberg et al\\.", "year": 1988}, {"title": "Towards efficient inference in mul\u00ad tiply connected belief networks", "author": ["M. Henrion"], "venue": null, "citeRegEx": "Henrion,? \\Q1990\\E", "shortCiteRegEx": "Henrion", "year": 1990}, {"title": "Local com\u00ad putations with probabilities on graphical structures and their application to expert systems", "author": ["S. Lauritzen", "D. Spiegelhalter"], "venue": "J. Roy. Statist. Soc. B,", "citeRegEx": "Lauritzen and Spiegelhalter,? \\Q1988\\E", "shortCiteRegEx": "Lauritzen and Spiegelhalter", "year": 1988}, {"title": "Generalization as search", "author": ["T. Mitchell"], "venue": "Artificial Intelligence,", "citeRegEx": "Mitchell,? \\Q1982\\E", "shortCiteRegEx": "Mitchell", "year": 1982}, {"title": "Generalised per\u00ad formance of Bayes optimal classification algorithm for learning a pcrceptron", "author": ["M. Opper", "D. Haussler"], "venue": "Work\u00ad shop on Compu.tational Learn\u00b7ing Theory. Morgan Kaufmann. Manuscript", "citeRegEx": "Opper and Haussler,? \\Q1991\\E", "shortCiteRegEx": "Opper and Haussler", "year": 1991}, {"title": "Changing the rules: A comprchcusive approach to theory rcfinc\u00ad mcut", "author": ["D. Ourston", "R. Mooney"], "venue": "In Eiyhth Na.t\u00b7ional Conference on Ari'ificia.l Intelligence,", "citeRegEx": "Ourston and Mooney,? \\Q1990\\E", "shortCiteRegEx": "Ourston and Mooney", "year": 1990}, {"title": "Probabilistic Reasoning in Intelligent Systems", "author": ["Pearl", ".J"], "venue": null, "citeRegEx": "Pearl and .J.,? \\Q1988\\E", "shortCiteRegEx": "Pearl and .J.", "year": 1988}, {"title": "Induction of decision trees", "author": ["Quinlan", ".J"], "venue": "Ma\u00ad chine Learning,", "citeRegEx": "Quinlan and .J.,? \\Q1986\\E", "shortCiteRegEx": "Quinlan and .J.", "year": 1986}, {"title": "Algorithmic Program Debugging", "author": ["E. Shapiro"], "venue": null, "citeRegEx": "Shapiro,? \\Q1983\\E", "shortCiteRegEx": "Shapiro", "year": 1983}, {"title": "Sequen\u00ad tial updating of conditional probabilities on directed graphical structures", "author": ["D. Spiegelhalter", "S. Lauritzen"], "venue": "Research Report R-89-10,", "citeRegEx": "Spiegelhalter and Lauritzen,? \\Q1989\\E", "shortCiteRegEx": "Spiegelhalter and Lauritzen", "year": 1989}, {"title": "An algorithm for fast recovery of sparse causal graphs. Report CMU\u00ad LCL-90-4, Laboratory for Computational Linguis\u00ad", "author": ["P. Spirtes", "C. Glymour"], "venue": null, "citeRegEx": "Spirtes and Glymour,? \\Q1990\\E", "shortCiteRegEx": "Spirtes and Glymour", "year": 1990}, {"title": "Sim\u00ad ulation studies of the reliability of computer-aided model specification using TETRAD II. EQS and LISREL programs", "author": ["P. Spirtes", "R. Scheines", "C. Glymour"], "venue": "Socialogical Methods and Re\u00ad search,", "citeRegEx": "Spirtes et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Spirtes et al\\.", "year": 1990}, {"title": "Auto\u00ad mated construction of sparse Bayesian networks", "author": ["S. Srinivas", "S. Russell", "A. Agogino"], "venue": null, "citeRegEx": "Srinivas et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Srinivas et al\\.", "year": 1990}, {"title": "Refinement of approximate domain theories by knowledge-based neural networks", "author": ["G. Towell", "J. Shavlik", "M. Nom\u00b7dewier"], "venue": "In E\u00b7ighth Na\u00ad tional Conference on Artificial Intelligence,", "citeRegEx": "Towell et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Towell et al\\.", "year": 1990}, {"title": "Equivalence and syn\u00ad thesis of causal modek In Sixth WoTkshop on Un\u00ad certainty \u00b7in A", "author": ["T. Verma", "Pearl", ".J"], "venue": null, "citeRegEx": "Verma et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Verma et al\\.", "year": 1990}], "referenceMentions": [{"referenceID": 15, "context": "Shapiro (Shapiro, 1983), for in\u00ad stance, developed a comprehensive theory and suite of algorithms for the task of refining Horn clause theo\u00ad ries (logic programs).", "startOffset": 8, "endOffset": 23}, {"referenceID": 12, "context": "Recent research in this area (Ourston and Mooney, 1990; Towell ct at., 1990) grew out the need to make the many iuductive leamiug algorithms avail\u00ad", "startOffset": 29, "endOffset": 76}, {"referenceID": 16, "context": "tial model needs to be quantified (Spiegelhalter and Lauritzen, 1989) (for instance, how many examples was it based on) in order to do refinement carefully.", "startOffset": 34, "endOffset": 69}, {"referenceID": 4, "context": "The general approach developed here is based on Bayesian principles for belief updating that form the basis of several learning algorithms (Buntine, 1990b; Cooper and Herskovits, 1991).", "startOffset": 139, "endOffset": 184}, {"referenceID": 17, "context": "Some of these algorithms also make the assumption (Geiger et a!., 1990; Spirtes and Glymour, 1990; Verma and Pearl, 1990) that the unknown probability distribution is a DAG-isomorph (Pearl, 1988).", "startOffset": 50, "endOffset": 121}, {"referenceID": 4, "context": "Simple learning approaches approximate this space of alternative theories by taking a single high poste\u00ad rior structure (Cooper and Herskovits, 1991; Buntine, 1990a) however experiments show that averaging over a larger sized space yields considerable improvement (Buntine, 1990a)1.", "startOffset": 120, "endOffset": 165}, {"referenceID": 8, "context": "This improved performance corre\u00ad sponds to the improved accuracy gained in the TOP N system when the system approximates posteriors using a thousand alternative disease sets instead of a single disease set (Henrion, 1990).", "startOffset": 206, "endOffset": 221}, {"referenceID": 9, "context": "conditional and marginal likelihoods between variables (Lauritzen and Spiegelhalter, 1988).", "startOffset": 55, "endOffset": 90}, {"referenceID": 1, "context": "We choose a prior that is a conjugate prior (it yields a posterior in the same functional form, so makes the mathematics sim\u00ad ple (Berger, 1985)) and assumes the least amount of information is known about the conditional probability tables.", "startOffset": 130, "endOffset": 144}, {"referenceID": 1, "context": "This is a product of standard non-informative priors on multinomial distributions (each conditional probability distribution is a multinomial), the sym\u00ad metric Dirichlet prior (Buntine, 1990b; Berger, 1985), and assumes prior independence between cells in the conditional probability table:", "startOffset": 176, "endOffset": 206}, {"referenceID": 10, "context": "This space of parent structures and the additional information can be thought of as similar to a version space (Mitchell, 1982).", "startOffset": 111, "endOffset": 127}, {"referenceID": 1, "context": "The counts nx=ili arc the only parameters in the pos\u00ad terior affected by the training sample and they are referred to as sufficient statistics (Berger, 1985); these need to be maintained during incremental learning.", "startOffset": 143, "endOffset": 157}, {"referenceID": 5, "context": "This is the problem of repeated restruc\u00ad turing reported by Crawford to occur in incremental learning algorithms (Crawford, 1989).", "startOffset": 113, "endOffset": 129}, {"referenceID": 6, "context": "Both problems can be handled the EM algorithm (Dempster et al., 1977).", "startOffset": 46, "endOffset": 69}, {"referenceID": 0, "context": "So with a dominant likelihood term, the posterior on the parameters is unimodal and the maxinmm posterior parameters can be found using search methods such as scoring, N ewton-Raphson, or conjugate gradient (Amemiya, 1985).", "startOffset": 207, "endOffset": 222}, {"referenceID": 16, "context": "A weaker approximation for batch learning (which finds a single high posterior network) has been re\u00ad ported to work well empirically (Cooper and Her\u00ad skovits, 1991), and the parameter updating com\u00ad ponent of the algorithm corresponds to previous work (Spiegelhalter and Lauritzen, 1989).", "startOffset": 251, "endOffset": 286}, {"referenceID": 8, "context": "These parameters allow one to trade-off space/time com\u00ad plexity with (average-case) quality of learned the\u00ad ories (compare with (Buntine, 1990a; Henrion, 1990)).", "startOffset": 128, "endOffset": 159}], "year": 2011, "abstractText": "Theory refinement is the task of updating a domain theory in the light of new cases, to be done automatically or with some expert as\u00ad sistance. The problem of theory refinement under uncertainty is reviewed here in the con\u00ad text of Bayesian statistics, a theory of belief revision. The problem is reduced to an incre\u00ad mental learning task as follows: the learning system is initially primed with a partial the\u00ad ory supplied by a domain expert, and there\u00ad after maintains its own internal representa\u00ad tion of alternative theories which is able to be interrogated by the domain expert and able to be incrementally refined from data. Algo\u00ad rithms for refinement of Bayesian networks are presented to illustrate what is meant by \"partial theory\", \"alternative theory repre\u00ad sentation\", etc. The algorithms are an incre\u00ad mental variant of batch learning algorithms from the literature so can work well in batch and incremental mode.", "creator": "pdftk 1.41 - www.pdftk.com"}}}