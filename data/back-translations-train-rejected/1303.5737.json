{"id": "1303.5737", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Integrating Probabilistic Rules into Neural Networks: A Stochastic EM Learning Algorithm", "abstract": "The EM-algorithm is a general procedure to get maximum likelihood estimates if part of the observations on the variables of a network are missing. In this paper a stochastic version of the algorithm is adapted to probabilistic neural networks describing the associative dependency of variables. These networks have a probability distribution, which is a special case of the distribution generated by probabilistic inference networks. Hence both types of networks can be combined allowing to integrate probabilistic rules as well as unspecified associations in a sound way. The resulting network may have a number of interesting features including cycles of probabilistic rules, hidden 'unobservable' variables, and uncertain and contradictory evidence.", "histories": [["v1", "Wed, 20 Mar 2013 15:32:18 GMT  (310kb)", "http://arxiv.org/abs/1303.5737v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gerhard paass"], "accepted": false, "id": "1303.5737"}, "pdf": {"name": "1303.5737.pdf", "metadata": {"source": "CRF", "title": "Integrating Probabilistic Rules into Neural Networks: A Stochastic EM Learning Algorithm", "authors": ["Gerhard Paass"], "emails": ["paass@icsi.berkeley.edu", "paass@gmdzi.gmd.de"], "sections": [{"heading": null, "text": "In fact, most of them are able to survive by themselves if they do not put themselves in the role of the individual."}, {"heading": "4 THE STOCHASTIC EM-ALGORITHM", "text": "As an alternative, we consider a sample-based method for determining the parameters of p (x). Essentially, we reconstruct the missing points of the pooled sample S =: (xp),.., X (n))), the elements of which are designated by X (i). This is just the approach of the stochastic EM algorithm (Celeux & Diebolt 1988), which represents a random distribution of a general method for handling missing data in maximum probability problems (Dempster et al. 1977). This algorithm begins with some arbitrary parameter vector, and iterates the wrapping steps: Assume X (j) = (Y) is an arbitrary record of the comprehensive sample S and leaves the vector of the values actually observed. Then, for each Y (i), the value of the values actually observed becomes vector."}], "references": [{"title": "Simulated Annealing and Boltzmann Machines", "author": ["E. A arts", "J. Karst"], "venue": "Wiley, Chichester", "citeRegEx": "arts and Karst,? 1988", "shortCiteRegEx": "arts and Karst", "year": 1988}, {"title": "A Learning Algorithm for the Boltzmann machine", "author": ["D. Ackley", "G.E. Hinton", "T.J. Sejnowski"], "venue": "Cognitive Science, Vol.9 pp.147-169 Anderson, J.A., Rosenfeld, E. (1988). Neurocom\u00ad puting: Foundations of Research. MIT Press,", "citeRegEx": "Ackley et al\\.,? 1985", "shortCiteRegEx": "Ackley et al\\.", "year": 1985}, {"title": "Spatial Interaction and Statis\u00ad tical Analysis of Lattice Systems", "author": ["J. Besag"], "venue": "Journal of The Royal Statistical Society, Series B., p.192-236 Celeux, G., Diebolt, J. (1988). A Random Im\u00ad putation Principle: The Stochastic EM Algorithm.", "citeRegEx": "Besag,? 1974", "shortCiteRegEx": "Besag", "year": 1974}, {"title": "A Method of Computing Generalized Bayesian Probability Values for Ex\u00ad pert Systems", "author": ["P. Cheeseman"], "venue": "Proc. IJCAI'83. Kaufmann, Los Altos, California.", "citeRegEx": "Cheeseman,? 1983", "shortCiteRegEx": "Cheeseman", "year": 1983}, {"title": "Generalized It\u00ad erative Scaling for Log-Linear Models", "author": ["J.N. Darroch", "D. Ratcliff"], "venue": "The Annals of Mathematical Statistics, Vol.43, p.1470-1480", "citeRegEx": "Darroch and Ratcliff,? 1972", "shortCiteRegEx": "Darroch and Ratcliff", "year": 1972}, {"title": "Maximum Likelihood from Incomplete Data via the EM algorithm (with discussion)", "author": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society, Vol.B-39, p.1-38", "citeRegEx": "Dempster et al\\.,? 1977", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Gibbs Sampling in Bayesian Networks", "author": ["T. Hrycej"], "venue": "Artificial Intelligence. Vol. 46, p.351363", "citeRegEx": "Hrycej,? 1990", "shortCiteRegEx": "Hrycej", "year": 1990}, {"title": "In\u00ad troduction to the Theory of Neural Computation", "author": ["A. Krogh", "R.G. Palmer"], "venue": null, "citeRegEx": "Krogh and Palmer,? \\Q1991\\E", "shortCiteRegEx": "Krogh and Palmer", "year": 1991}, {"title": "Markov Ran\u00ad dom Fields and their Applications", "author": ["R. Kindermann", "J.L. Snell"], "venue": "American", "citeRegEx": "Kindermann and Snell,? 1980", "shortCiteRegEx": "Kindermann and Snell", "year": 1980}, {"title": "Probabilistic Logic", "author": ["G. Paass"], "venue": "In: Smets, P., A. Mamdani, D.Dubois, H.Prade (eds.) Non\u00ad Standard Logics for Automated Reasoning, Aca\u00ad", "citeRegEx": "Paass,? 1988", "shortCiteRegEx": "Paass", "year": 1988}, {"title": "Structured Probabilistic Neural Networks", "author": ["G. Paass"], "venue": "Proc. Neuro-Nimes '89 p.345-359", "citeRegEx": "Paass,? 1989", "shortCiteRegEx": "Paass", "year": 1989}, {"title": "Probabilistic Reasoning in Intelli\u00ad gent Systems", "author": [], "venue": null, "citeRegEx": ".1988..,? \\Q1988\\E", "shortCiteRegEx": ".1988..", "year": 1988}, {"title": "Some Asymptotic Results for Learning in Single Layer Feedforward Net\u00ad work Models", "author": ["H. White"], "venue": "J. American Statistical Association. Vol.84, p.1003-1013.", "citeRegEx": "White,? 1989", "shortCiteRegEx": "White", "year": 1989}, {"title": "On the Convergence Properties of the EM algorithm", "author": ["C.F. Wu"], "venue": "Annals of Statistics. Vol.ll, p.95-103.", "citeRegEx": "Wu,? 1983", "shortCiteRegEx": "Wu", "year": 1983}], "referenceMentions": [{"referenceID": 1, "context": "The Boltzmann machine (Ackley et al. 1985) is a neural network which modifies its variables accord\u00ad ing to a joint probability distribution.", "startOffset": 22, "endOffset": 42}, {"referenceID": 4, "context": "The functional form of maximum entropy distri\u00ad butions of discrete variables subject to constraints has been derived twenty years ago by Darroch and Ratcliff (1972). Paass (1989) proposed the integra\u00ad tion of neural networks and probabilistic inference", "startOffset": 137, "endOffset": 165}, {"referenceID": 4, "context": "The functional form of maximum entropy distri\u00ad butions of discrete variables subject to constraints has been derived twenty years ago by Darroch and Ratcliff (1972). Paass (1989) proposed the integra\u00ad tion of neural networks and probabilistic inference", "startOffset": 137, "endOffset": 179}, {"referenceID": 6, "context": "Recently Hrycej (1990) dis\u00ad", "startOffset": 9, "endOffset": 23}, {"referenceID": 3, "context": "Similar to (Cheeseman 1983) we se\u00ad lect the distribution from P which maximizes the", "startOffset": 11, "endOffset": 27}, {"referenceID": 10, "context": "Such a structure is called a Markov random field (Kindermann & Snell 1980) and corresponds to a a nearest neighbor Gibbs potential (Paass 1989; Hrycej 1990).", "startOffset": 131, "endOffset": 156}, {"referenceID": 6, "context": "Such a structure is called a Markov random field (Kindermann & Snell 1980) and corresponds to a a nearest neighbor Gibbs potential (Paass 1989; Hrycej 1990).", "startOffset": 131, "endOffset": 156}, {"referenceID": 10, "context": "In (Paass 1989) the derivatives of this likelihood function with respect to the parameters Ar are calculated.", "startOffset": 3, "endOffset": 15}, {"referenceID": 11, "context": "(1988) found that the Boltz\u00ad mann machine achieved considerably better accu\u00ad racy than a backpropagation network.", "startOffset": 0, "endOffset": 7}, {"referenceID": 5, "context": "This is just the approach of the stochastic EM-algorithm (Celeux & Diebolt 1988), which is a random ver\u00ad sion of a general procedure for handling missing data in maximum likelihood problems (Dempster et al. 1977).", "startOffset": 190, "endOffset": 212}, {"referenceID": 11, "context": "It has been shown (Celeux & Diebolt 1988) that for n --> oo under rather general conditions the parameter \ufffd estimated by the stochastic EM algo\u00ad rithm corresponds to a local minimum of the likeli\u00ad hood function. Empirical evidence shows that the stochastic imputation step allows the algorithm to escape from local minima. The convergence prop\u00ad erties of the usual EM-algorithm are discussed by Wu (1983).", "startOffset": 35, "endOffset": 405}, {"referenceID": 2, "context": "According to the Hammersley-Clifford theorem (Besag 1974) the distribution p( x) is completely determined if we know the conditional distributions p(x; I x;), i = 1, .", "startOffset": 45, "endOffset": 57}, {"referenceID": 10, "context": "In contrast to (Paass 1989) it is not necessary to solve a nonlinear equation system for each itera\u00ad tion.", "startOffset": 15, "endOffset": 27}], "year": 2011, "abstractText": "The EM-algorithm is a general procedure to get maximum likelihood estimates if part of the observations on the variables of a network are missing. In this paper a stochastic version of the algorithm is adapted to probabilistic neural networks describing the associative dependency of variables. These networks have a proba\u00ad bility distribution, which is a special case of the distribution generated by proba\u00ad bilistic inference networks. Hence both types of networks can be combined al\u00ad lowing to integrate probabilistic rules as well as unspecified associations in a sound way. The resulting network may have a number of interesting features including cycles of probabilistic rules, hidden 'un\u00ad observable' variables, and uncertain and contradictory evidence.", "creator": "pdftk 1.41 - www.pdftk.com"}}}