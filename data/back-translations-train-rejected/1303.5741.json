{"id": "1303.5741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Mar-2013", "title": "Formal Model of Uncertainty for Possibilistic Rules", "abstract": "Given a universe of discourse X-a domain of possible outcomes-an experiment may consist of selecting one of its elements, subject to the operation of chance, or of observing the elements, subject to imprecision. A priori uncertainty about the actual result of the experiment may be quantified, representing either the likelihood of the choice of :r_X or the degree to which any such X would be suitable as a description of the outcome. The former case corresponds to a probability distribution, while the latter gives a possibility assignment on X. The study of such assignments and their properties falls within the purview of possibility theory [DP88, Y80, Z783. It, like probability theory, assigns values between 0 and 1 to express likelihoods of outcomes. Here, however, the similarity ends. Possibility theory uses the maximum and minimum functions to combine uncertainties, whereas probability theory uses the plus and times operations. This leads to very dissimilar theories in terms of analytical framework, even though they share several semantic concepts. One of the shared concepts consists of expressing quantitatively the uncertainty associated with a given distribution. In probability theory its value corresponds to the gain of information that would result from conducting an experiment and ascertaining an actual result. This gain of information can equally well be viewed as a decrease in uncertainty about the outcome of an experiment. In this case the standard measure of information, and thus uncertainty, is Shannon entropy [AD75, G77]. It enjoys several advantages-it is characterized uniquely by a few, very natural properties, and it can be conveniently used in decision processes. This application is based on the principle of maximum entropy; it has become a popular method of relating decisions to uncertainty. This paper demonstrates that an equally integrated theory can be built on the foundation of possibility theory. We first show how to define measures of in formation and uncertainty for possibility assignments. Next we construct an information-based metric on the space of all possibility distributions defined on a given domain. It allows us to capture the notion of proximity in information content among the distributions. Lastly, we show that all the above constructions can be carried out for continuous distributions-possibility assignments on arbitrary measurable domains. We consider this step very significant-finite domains of discourse are but approximations of the real-life infinite domains. If possibility theory is to represent real world situations, it must handle continuous distributions both directly and through finite approximations. In the last section we discuss a principle of maximum uncertainty for possibility distributions. We show how such a principle could be formalized as an inference rule. We also suggest it could be derived as a consequence of simple assumptions about combining information. We would like to mention that possibility assignments can be viewed as fuzzy sets and that every fuzzy set gives rise to an assignment of possibilities. This correspondence has far reaching consequences in logic and in control theory. Our treatment here is independent of any special interpretation; in particular we speak of possibility distributions and possibility measures, defining them as measurable mappings into the interval [0, 1]. Our presentation is intended as a self-contained, albeit terse summary. Topics discussed were selected with care, to demonstrate both the completeness and a certain elegance of the theory. Proofs are not included; we only offer illustrative examples.", "histories": [["v1", "Wed, 20 Mar 2013 15:32:37 GMT  (258kb)", "http://arxiv.org/abs/1303.5741v1", "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)"]], "COMMENTS": "Appears in Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence (UAI1991)", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["arthur ramer"], "accepted": false, "id": "1303.5741"}, "pdf": {"name": "1303.5741.pdf", "metadata": {"source": "CRF", "title": "Formal Model of Uncertainty for Possibilistic Rules", "authors": ["Arthur Ramer", "Leslie Lander"], "emails": [], "sections": [{"heading": null, "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "1r1 181 1r2 : (x, y) >-+ min(1r1(x), 1r2(y)).", "text": "Faced with an arbitrary mapping 1r to a product space X X Y, we define its marginal mapping 1r1 to X and 1r11 to Y as1r1 (x) = max 1r (x, y), yEY 1r11 (y) = max 1r (x, y). xEX It is also convenient to define an extension of 1r from its domain X to a larger set Y: J X. We define a possible mapping s (1r) = 1r (y), y $X and 1r (y) = 0 otherwise. Finally, it is convenient to define an extension of 1r (..., Xn) and a permutation s of {1,..., n} we define a possible mapping s (1r) s (1r) (x;) = 1r (x): 1,2 CONTINUOUS DOMAINSThis structure generalizes comprehensively x."}, {"heading": "2 INFORMATION FUNCTIONS IN POSSIBILITY THEORY", "text": "It is a matter of time until there will be a solution. (It is a matter of time until there will be a solution.) It is a matter of time until there will be a solution. (It is a matter of time until there will be a solution.) It is a matter of time until there will be a solution. (It is a matter of time until there will be a solution.) It is a matter of time until there will be a solution. (It is a matter of time until there will be a solution.) It is a matter of time until there will be a solution. (It is a matter of time until there will be a solution.) It is a matter of time until there will be a solution. (It is a matter of time until there will be a solution.) It is a matter of time until there will be a solution. \""}], "references": [{"title": "Information Theory and Ap\u00ad", "author": ["S. G11 GUIASU"], "venue": null, "citeRegEx": "GUIASU,? \\Q1977\\E", "shortCiteRegEx": "GUIASU", "year": 1977}, {"title": "On the no\u00ad tion of distance representing information close\u00ad", "author": ["HK83 HIGASHI", "G. KLIR"], "venue": "ness, Int. J. Gen. Syst.,", "citeRegEx": "HIGASHI et al\\.,? \\Q1983\\E", "shortCiteRegEx": "HIGASHI et al\\.", "year": 1983}, {"title": "Measures of uncertainty and information based on possibility distributions Int", "author": ["M HIGASHI"], "venue": "KLIR,G.,", "citeRegEx": "HIGASHI,? \\Q1982\\E", "shortCiteRegEx": "HIGASHI", "year": 1982}, {"title": "On the rationale of maximum entropy methods", "author": ["E. J82 JAYNES"], "venue": "Proc. IEEE,", "citeRegEx": "JAYNES,? \\Q1982\\E", "shortCiteRegEx": "JAYNES", "year": 1982}, {"title": "Structure of possibilistic in\u00ad formation metrics and distances", "author": ["A. R90 RAMER"], "venue": "Int. J. Gen. Syst.,", "citeRegEx": "RAMER,? \\Q1990\\E", "shortCiteRegEx": "RAMER", "year": 1990}, {"title": "Concepts of fuzzy informa\u00ad tion measures on continuous domains", "author": ["A. R89 RAMER"], "venue": "Int. J. Gen. Syst.,", "citeRegEx": "RAMER,? \\Q1989\\E", "shortCiteRegEx": "RAMER", "year": 1989}, {"title": "Aspects of possibilistic uncer\u00ad tainty", "author": ["Y. Y80 YAGER"], "venue": "Int. J. Man-Machine Studies,", "citeRegEx": "YAGER,? \\Q1980\\E", "shortCiteRegEx": "YAGER", "year": 1980}], "referenceMentions": [], "year": 2011, "abstractText": "The study of such assignments and their properties falls within the purview of possibility theory [DP88, Y80, Z78]. It, like probability theory, assigns values between 0 and 1 to express likelihoods of outcomes. Here, however, the similarity ends. Possibility theory uses the maximum and minimum functions to com\u00ad bine uncertainties, whereas probability theory uses the plus and times operations. This leads to very dissim\u00ad ilar theories in terms of analy tical framework, even though they share several semantic concepts. One of the shared concepts consists of expressing quantita\u00ad tively the uncertainty associated with a given distribu\u00ad tion. In probability theory its value corresponds to the gain of information that would result from conduct\u00ad ing an experiment and ascertaining an actual result. This gain of information can equally well be viewed as a decrease in uncertainty about the outcome of an experiment. In this case the standard measure of in\u00ad formation, and thus uncertainty, is Shannon entropy [AD75, G77]. It enjoys several advantages-it is char\u00ad acterized uniquely by a few, very natural properties, and it can be conveniently used in decision processes. This application is based on the principle of maximum entropy; it has become a popular method of relating decisions to uncertainty.", "creator": "pdftk 1.41 - www.pdftk.com"}}}