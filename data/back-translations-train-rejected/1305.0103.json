{"id": "1305.0103", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2013", "title": "Clustering Unclustered Data: Unsupervised Binary Labeling of Two Datasets Having Different Class Balances", "abstract": "We consider the unsupervised learning problem of assigning labels to unlabeled data. A naive approach is to use clustering methods, but this works well only when data is properly clustered and each cluster corresponds to an underlying class. In this paper, we first show that this unsupervised labeling problem in balanced binary cases can be solved if two unlabeled datasets having different class balances are available. More specifically, estimation of the sign of the difference between probability densities of two unlabeled datasets gives the solution. We then introduce a new method to directly estimate the sign of the density difference without density estimation. Finally, we demonstrate the usefulness of the proposed method against several clustering methods on various toy problems and real-world datasets.", "histories": [["v1", "Wed, 1 May 2013 06:32:12 GMT  (76kb)", "http://arxiv.org/abs/1305.0103v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["marthinus christoffel du plessis", "masashi sugiyama"], "accepted": false, "id": "1305.0103"}, "pdf": {"name": "1305.0103.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Masashi Sugiyama"], "emails": ["christo@sg.cs.titech.ac.jp,", "sugi@cs.titech.ac.jp"], "sections": [{"heading": null, "text": "ar Xiv: 130 5.01 03v1 [cs.LG] 1 M"}, {"heading": "1 Introduction", "text": "In this paper, we look at the problem of labelling, which aims to achieve labelling of the label, and labelling of the label is not assigned."}, {"heading": "2 Problem Formulation and Fundamental Approaches", "text": "In this section, we formulate the problem of labelling, set out our basic strategy and look at two naive approaches."}, {"heading": "2.1 Problem Formulation", "text": "Suppose that there are two probability distributions p (x, y) and p (x, y) on x-R d and y (1, \u2212 1), which differ only in class balances: p (y) 6 = p (y), but p (x | y) = p (x | y). (1) From these distributions we obtain two sets of unlabeled samples: Xp = {xi} n i = 1 i.i.d. \u0445 p (x) and Xp \u2032 = {x \u2032 j} n \u2032 j = 1 i.i.i.d. \u0445 p \u2032 (x).The aim of the label is to obtain a label for the two samples, Xp and Xp \u2032, the underlying class labels {yi} ni = 1 and {y \u2032 j} n \u2032 j = 1. Unlike the classification, however, we do not receive a correct class label, but receive a correct class label pending comment."}, {"heading": "2.2 Fundamental strategy", "text": "We want a label for samples in Xp and Xp. Here we show that we can get the solution for the case where the class priorities are the same. We can write the class posterior distribution for the same previous case: asq (y = \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 x) = p (x | y) q (x) q (x) q (y = 1), where q (y = 1) = q (y = \u2212 1) p (y = 1) = 1. A class label can then be assigned to a point by the weighting mark [q (y = 1 | x) \u2212 q (y = \u2212 1 | x). (We can asq (y = 1 | x) p (y = 1 | x) \u2212 q (y) p (y = \u2212 x = \u2212 x | x) = \u2212 x (y = 1)."}, {"heading": "2.3 Kernel Density Estimation", "text": "A naive approach to estimating the density difference is the use of kernel density estimators (KDEs) [3]. For Gaussian nuclei, the KDE solutions of p (x), p (n) and p (x), p (x), p (n) and j (1exp) are given. Gaussian latitudes \u03c3 and \u03c3 \"can be determined on the basis of the cross-validation of the smallest squares [4]. Finally, you get a label asy = characters [p (x) \u2212 p (x), p (x)]. (3)"}, {"heading": "2.4 Direct Estimation of the Density Difference", "text": "KDE is a nice density estimator, but it is not necessarily suitable for density difference estimates, because a small estimation error that occurs with each density estimate can cause a big error in the final density difference estimate. More intuitively, good density estimates tend to be smooth, and therefore a density difference estimate derived from such smooth density estimates tends to be smoothed out [5,6]. In this approach, we directly adjust a model g (x) to the density difference below the square loss: g = argmin g12 (g (x) \u2212 (p (x) \u2212 p \u2032 (x))) 2 dx, which can be efficiently obtained for a kernel density difference model. Finally, a comprehensive overview of LSDD can be found in Appendix B. A label is simple = characters [g (x)]."}, {"heading": "3 Direct Estimation of the Sign of the Density Difference", "text": "We expect LSDD to provide an improved solution compared to KDEs due to the more direct nature of LSDD. However, LSDD is still indirect, as the density difference sign is checked after estimating the density difference. In this section we will show how to estimate the density difference sign directly."}, {"heading": "3.1 Derivation of the Objective Function", "text": "By decreasing the L1 distance between the probability densities, defined as \u0432 (x) - p (x) - p (x) - dx, (4), we obtain the sign of the density difference. Let us begin by considering the following self-evident relation: | t | \u2265 tz, if | z | \u2264 1. We can apply this relation at any point x to obtain the narrowest lower limit by applying the above inequality to equality (4) and maximizing it in relation to g (x) - g (x) - p (x) - p (x) - p (x) - p (x) - p (x), we can obtain the narrowest lower limit."}, {"heading": "3.2 Optimization", "text": "Here we discuss briefly how the problem can be solved in Eq. (6) A more detailed explanation is given in Appendix A. (6) The function in Eq. (6) should fulfil the condition. (6) We can consider a stripped-down version of the function that always fulfils the condition. (7) The function in Eq. (6) should fulfil the condition that R (z) = 1, - 1, - 2, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 1, - 2, - 2, -, - 2, -, -, - 2, -, -, -, -, 3, -, -, -, -, -, -, -, -, 2, -, -, -, -, 2, -, -, -, -, 2, -, -, -, 2, -, -, -, 2, -, -, -, 2, -, -, -, 2, -, -, -, 2, -, -, -, 1, -, -, -, -, 1, -, 1, -, -, -, -, -, 1, -, -, 1, -, -, -,"}, {"heading": "4 Experiments", "text": "First, we demonstrate how our method works and characterize the errors of other methods using various toy examples. Then, we use real-world benchmark data to demonstrate the superiority of our algorithm."}, {"heading": "4.1 Numerical Illustration", "text": "Suppose the class-related density for the two classes is given as asp (x | y = 1) = Nx (\u2212 12, I2 \u00b7 2) and p (x | y = \u2212 1) = Nx (12, I2 \u00b7 2), where Nx (\u00b5, \u03a3) indicates the normal density with the mean \u00b5 and the covariance sum w.r.t. x. The result is shown in Figure 1. As can be seen from this example, we are able to obtain a label for the classes that roughly correspond to the true (unknown) label p (y = 1) = 0.7."}, {"heading": "4.2 Benchmark Datasets", "text": "For each experiment, we constructed the data sets Xp = R = R = - by dragging n and n \u2032 samples from the positive and negative classes of the data sets according to a previous estimate rate of p (y = 1) and p \u2032 (y = 1). The labeling was then done using these two data sets. As we can get a label but cannot determine the original class labels, we cannot directly measure the performance based on the misclassification rate. Assuming that the label for the sample xi isli = {\u2212 1 p (xi) \u2212 q (xi) < 01 otherwise.The misclassification rate (MCR) assumes that the current labels are the opposite isMCR: = 1n i: lj 6 = yi1 + 1n \u2032 j: l \u2032 j 6 = y \u2032 i1.The misclassification rate assumes that the labels are the correct."}, {"heading": "5 Conclusion", "text": "First, we showed that this problem can be solved when two unlabeled data sets with different class balances are available, and the solution can be achieved by estimating the sign of the difference between the probability densities. We introduced a method to directly estimate the sign of the density difference and to avoid density estimations.The method was demonstrated on different data sets to surpass competing methods that either estimate the density difference or use the cluster structure of the data. As the sign of the density difference corresponds to the optimal classifier Bayes under the same class balance, it can be estimated by any classifier that separates Xp and Xp. Following this idea, we tested the Support Vector Machine (SVM) to estimate the sign of the density difference. However, this did not work well due to the high overlap of Xp and Xp \u2032 - both data sets are mixtures of two classes, only with different mixing ratios."}, {"heading": "A Optimization", "text": "This section describes the optimization of the equation (7) with the convex concave method (7). The non-convex function R (z) can be rewritten. \u2212 The convex part of the objective function can then be designated as Jvex (z) = 1n \u00b2 n \u00b2. \u2212 The convex part of the objective function can then be designated as Jvex (z) = 1n \u00b2 n \u00b2. \u2212 The convex part as Jcave (z) = 1n \u00b2 (b \u00b2) replaces the objective function. \u2212 The convex part as Jcave (z) = 1n \u00b2 (x \u00b2). \u2212 The convex part as Jcave (\u03b1) = \u2212 1n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 n \u00b2 (b \u00b2)."}, {"heading": "B Least-squares estimation of the density difference", "text": "In [9] it was proposed to estimate the density difference directly by adjusting a model g (x) to the true density difference f (x) below a square loss. \u2212 The density difference was modelled using a linear parameter model g (x) \u2212 The density difference was modelled using a linear model g (x) \u2212 The difference is a b-dimensional base function p (x) \u2212 The difference is a b-dimensional base function p (x) \u2212 The difference is a b-dimensional base function p (x) \u2212 The difference is a b-dimensional base function p (x) \u2212 The difference is a b-dimensional base function p (x) \u2212 The difference is a b-dimensional base function p (x)."}], "references": [{"title": "The Nature of Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "Statistics for Engineering and Information Science Series. Springer", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Density-difference estimation", "author": ["M. Sugiyama", "T. Kanamori", "T. Suzuki", "M.C. du Plessis", "S. Liu", "I. Takeuchi"], "venue": "In Bartlett, P., Pereira, F., Burges, C., Bottou, L., Weinberger, K., eds.: Advances in Neural Information Processing Systems 25.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Density estimation for statistics and data analysis", "author": ["B. Silverman"], "venue": "Chapman and Hall, London, UK", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1986}, {"title": "Nonparametric and semiparametric models", "author": ["W. H\u00e4rdle", "M. M\u00fcller", "S. Sperlich", "A. Werwatz"], "venue": "Springer", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "On nonparametric discrimination using density differences", "author": ["P. Hall", "M.P. Wand"], "venue": "Biometrika 75(3)", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1988}, {"title": "Two-sample test statistics for measuring discrepancies between two multivariate probability density functions using kernelbased density estimates", "author": ["N.H. Anderson", "P. Hall", "D. Titterington"], "venue": "Journal of Multivariate Analysis 50(1)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1994}, {"title": "The concave-convex procedure (CCCP)", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "Advances in Neural Information Processing Systems 14, MIT Press", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2002}, {"title": "DC programming: overview", "author": ["R. Horst", "N.V. Thoai"], "venue": "Journal of Optimization Theory and Applications 103(1)", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "Density-difference estimation", "author": ["M. Sugiyama", "T. Kanamori", "T. Suzuki", "M.C. du Plessis", "S. Liu", "I. Takeuchi"], "venue": "Neural Computation", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on 22(8)", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "On informationmaximization clustering: Tuning parameter selection and analytic solution", "author": ["M. Sugiyama", "M. Yamada", "M. Kimura", "H. Hachiya"], "venue": "In Getoor, L., Scheffer, T., eds.: Proceedings of 28th International Conference on Machine Learning (ICML2011), Bellevue, Washington, USA", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Kernel Methods for Pattern Analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": "Cambridge University Press, New York, NY, USA", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "However, this naive procedure violates Vapnik\u2019s principle[1]: If you possess a restricted amount of information for solving some problem, try to solve the problem directly and never solve a more general problem as an intermediate step.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "Recently, a method was introduced to directly estimate the density difference, called the least-squares density difference (LSDD) estimator [2].", "startOffset": 140, "endOffset": 143}, {"referenceID": 2, "context": "3 Kernel Density Estimation A naive approach to estimating the sign of density-difference is to use kernel density estimators (KDEs) [3].", "startOffset": 133, "endOffset": 136}, {"referenceID": 3, "context": "The Gaussian widths \u03c3 and \u03c3 may be determined based on least-squares crossvalidation [4].", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "More intuitively, good density estimators tend to be smooth and thus a densitydifference estimator obtained from such smooth density estimators tends to be over-smoothed [5,6].", "startOffset": 170, "endOffset": 175}, {"referenceID": 5, "context": "More intuitively, good density estimators tend to be smooth and thus a densitydifference estimator obtained from such smooth density estimators tends to be over-smoothed [5,6].", "startOffset": 170, "endOffset": 175}, {"referenceID": 1, "context": "The density difference can be estimated in a single shot using the least-squares density difference (LSDD) approach [2].", "startOffset": 116, "endOffset": 119}, {"referenceID": 6, "context": "Although the above is a non-convex problem, we can efficiently find a local optimal solution using the convex-concave procedure (CCCP) [7] (also known as difference of convex (d.", "startOffset": 135, "endOffset": 138}, {"referenceID": 7, "context": ") programming [8]).", "startOffset": 14, "endOffset": 17}, {"referenceID": 2, "context": "p(x|y = 1) = 1 2 Nx([3 0] \u22a4 , I2\u00d72) + 1 2 Nx([\u22123 0] \u22a4 , I2\u00d72) p(x|y = \u22121) = 1 2 Nx([0 3] \u22a4 , I2\u00d72) + 1 2 Nx([0 \u2212 3] \u22a4 , I2\u00d72).", "startOffset": 20, "endOffset": 25}, {"referenceID": 2, "context": "p(x|y = 1) = 1 2 Nx([3 0] \u22a4 , I2\u00d72) + 1 2 Nx([\u22123 0] \u22a4 , I2\u00d72) p(x|y = \u22121) = 1 2 Nx([0 3] \u22a4 , I2\u00d72) + 1 2 Nx([0 \u2212 3] \u22a4 , I2\u00d72).", "startOffset": 83, "endOffset": 88}, {"referenceID": 8, "context": "\u2013 Least-Squares Density Difference (LSDD) Estimation: Estimate sign [p(x)\u2212 p(x)] by estimating p(x)\u2212 p(x) using the least squares fitting method [9].", "startOffset": 145, "endOffset": 148}, {"referenceID": 9, "context": "\u2013 Spectral Clustering (SC): Cluster the data into two clusters using the spectral clustering algorithm [10].", "startOffset": 103, "endOffset": 107}, {"referenceID": 10, "context": "\u2013 Squared-loss Mutual Information based Clustering (SMIC) : Cluster the data according to the SMIC method [11].", "startOffset": 106, "endOffset": 110}], "year": 2013, "abstractText": "We consider the unsupervised learning problem of assigning labels to unlabeled data. A naive approach is to use clustering methods, but this works well only when data is properly clustered and each cluster corresponds to an underlying class. In this paper, we first show that this unsupervised labeling problem in balanced binary cases can be solved if two unlabeled datasets having different class balances are available. More specifically, estimation of the sign of the difference between probability densities of two unlabeled datasets gives the solution. We then introduce a new method to directly estimate the sign of the density difference without density estimation. Finally, we demonstrate the usefulness of the proposed method against several clustering methods on various toy problems and real-world datasets.", "creator": "LaTeX with hyperref package"}}}