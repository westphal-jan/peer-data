{"id": "1305.0208", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-May-2013", "title": "Perceptron Mistake Bounds", "abstract": "We present a brief survey of existing mistake bounds and introduce novel bounds for the Perceptron or the kernel Perceptron algorithm. Our novel bounds generalize beyond standard margin-loss type bounds, allow for any convex and Lipschitz loss function, and admit a very simple proof.", "histories": [["v1", "Wed, 1 May 2013 15:45:34 GMT  (22kb)", "https://arxiv.org/abs/1305.0208v1", null], ["v2", "Tue, 23 Jul 2013 02:13:57 GMT  (22kb)", "http://arxiv.org/abs/1305.0208v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mehryar mohri", "afshin rostamizadeh"], "accepted": false, "id": "1305.0208"}, "pdf": {"name": "1305.0208.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Mehryar Mohri", "Afshin Rostamizadeh"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 130 5.02 08v2 [cs.LG] 2 3Ju l 201 3"}, {"heading": "1 Introduction", "text": "The Perceptron algorithm belongs to the large family of online learning algorithms (see Cesa-Bianchi and Lugosi [2006] for a survey) and allows for a large number of variants. The algorithm learns a linear separator by processing the training sample online and examining a single example with each iteration [Rosenblatt, 1958]. With each round, the current hypothesis is updated if it makes an error, i.e. if it misclassifies the newly processed training point. The full pseudo-code of the algorithm is in Figure 1. Below, we assume that w0 = 0 and \u03b7 = 1 apply to the simplicity of representation, but the more general case also allows for similar guarantees, which can be derived using the same methods we present. In this essay, some existing error limits for the Perceptron algorithm are briefly examined and new ones are introduced, which can be used to derive generalization limits in a stoic setting."}, {"heading": "2 Separable case", "text": "The basic work of Novikoff [1962] provided the first margin-based assumptions for the Perceptron algorithm (one of the early results in learning theory and probably one of the first to be based on the concept of margin. Assuming that the data is divisible by a certain margin, Novikoff showed that the number of errors made by the Perceptron algorithm can be limited as a function of the normalized margin. \u2212 Assuming that R is the radius of the sphere containing the training instances. We begin with a lemma that can be used to prove Novikoff's theorem and that is used consistently."}, {"heading": "3 Non-separable case", "text": "However, it is possible to give a margin-based error, which in this general case is limited in relation to the radius of the sphere containing the sample and the margin-based loss of any weight vector. We present two different types of limits: First, a limit that depends on the L1 norm of the margin loss vector, or the vector of general losses that we will describe, next a limit that depends on the L2 norm of margin loss, which extends the original results of the L1 norm errors represented by Freund and Schapire. First, we present a simple proof of an error limited to the perceptron algorithms that depend on the L1 norm of losses."}, {"heading": "3.3 Discussion", "text": "A natural question raised by this survey is the respective quality of the L1 and L2 standard limits. Comparing (4) and (8) for the difference loss shows that the limits of a fixed difference value differ only in the following two quantities: min-yt (u), min-yt (u), 1 = min-yt (u), 1-yt (u), I (1 \u2212 yt (u \u00b7 xt), + min-yt (u), 2 +. These two quantities are data dependent and generally not comparable. For a vector u where the individual losses (1 \u2212 yt (u \u00b7 xt) are all smaller than one, we have a difference value of 1 \u2212 yt (u), whereas the opposite is true if the individual losses are greater than one."}, {"heading": "4 Generalization Bounds", "text": "In this section, we will consider the case in which the training sample is evaluated according to some distribution. (Under some mild conditions on the loss function, the hypotheses returned by an online learning algorithm can then be combined to define a hypothesis whose generalization error can be limited with respect to its regret. (Such a hypothesis can be determined by generalizing errors presented in the previous section to derive generalizations for the perceptron predictor.) Given the fact that a sequence of marked examples (x1, y1), a sequence of hypotheses h1,. (hT) and a loss function L, the risk minimizes the hypotheses as h."}, {"heading": "5 Kernel Perceptron algorithm", "text": "The Perceptron algorithm in Figure 1 can easily be extended to define a nonlinear hyphen that uses a positive definitive kernel K. [Aizerman et al., 1964] Figure 2 indicates the pseudo-code of this algorithm, known as the Kernel Perceptron Algorithm. The sgn (h) classifier learned from the algorithm is defined by h: x 7 \u2192 \u2211 Tt = 1 \u03b1tytK (xt, x). The results of the preceding sections similarly apply to the Kernel Perceptron Algorithm, where the number \u04452 is replaced by K (xt, xt). Specifically, the quantity occurring in several learning guarantees can be replaced by the familiar track Tr [K] of the kernel matrix K = [K (xi, xj)] i."}], "references": [{"title": "Theoretical foundations of the potential function method in pattern recognition learning", "author": ["Mark A. Aizerman", "E.M. Braverman", "Lev I. Rozono\u00e8r"], "venue": "Automation and Remote Control,", "citeRegEx": "Aizerman et al\\.,? \\Q1964\\E", "shortCiteRegEx": "Aizerman et al\\.", "year": 1964}, {"title": "Prediction, Learning, and Games", "author": ["Nicol\u00f2 Cesa-Bianchi", "Gabor Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi.", "year": 2006}, {"title": "On the generalization ability of on-line learning algorithms", "author": ["Nicol\u00f2 Cesa-Bianchi", "Alex Conconi", "Claudio Gentile"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Cesa.Bianchi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Cesa.Bianchi et al\\.", "year": 2004}, {"title": "Large margin classification using the perceptron algorithm", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Machine Learning,", "citeRegEx": "Freund and Schapire.,? \\Q1999\\E", "shortCiteRegEx": "Freund and Schapire.", "year": 1999}, {"title": "From on-line to batch learning", "author": ["Nick Littlestone"], "venue": "In COLT,", "citeRegEx": "Littlestone.,? \\Q1989\\E", "shortCiteRegEx": "Littlestone.", "year": 1989}, {"title": "On convergence proofs on perceptrons", "author": ["Albert B.J. Novikoff"], "venue": "In Proceedings of the Symposium on the Mathematical Theory of Automata,", "citeRegEx": "Novikoff.,? \\Q1962\\E", "shortCiteRegEx": "Novikoff.", "year": 1962}, {"title": "The perceptron: A probabilistic model for information storage and organization in the brain", "author": ["Frank Rosenblatt"], "venue": "Psychological Review,", "citeRegEx": "Rosenblatt.,? \\Q1958\\E", "shortCiteRegEx": "Rosenblatt.", "year": 1958}], "referenceMentions": [{"referenceID": 6, "context": "The algorithm learns a linear separator by processing the training sample in an on-line fashion, examining a single example at each iteration [Rosenblatt, 1958].", "startOffset": 142, "endOffset": 160}, {"referenceID": 1, "context": "The Perceptron algorithm belongs to the broad family of on-line learning algorithms (see Cesa-Bianchi and Lugosi [2006] for a survey) and admits a large number of variants.", "startOffset": 89, "endOffset": 120}, {"referenceID": 5, "context": "The seminal work of Novikoff [1962] gave the first margin-based bound for the Perceptron algorithm, one of the early results in learning theory and probably one of the first based on the notion of margin.", "startOffset": 20, "endOffset": 36}, {"referenceID": 6, "context": "Perceptron algorithm [Rosenblatt, 1958].", "startOffset": 21, "endOffset": 39}, {"referenceID": 5, "context": "Theorem 1 ([Novikoff, 1962]).", "startOffset": 11, "endOffset": 27}, {"referenceID": 3, "context": "We present two different types of bounds: first, a bound that depends on the L1-norm of the vector of \u03c1-margin hinge losses, or the vector of more general losses that we will describe, next a bound that depends on the L2-norm of the vector of margin losses, which extends the original results presented by Freund and Schapire [1999].", "startOffset": 306, "endOffset": 333}, {"referenceID": 2, "context": "The mistake bound (3) appears already in Cesa-Bianchi et al. [2004] but we could not find its proof either in that paper or in those it references for this bound.", "startOffset": 41, "endOffset": 68}, {"referenceID": 3, "context": "2 L2-norm mistake bounds The original results of this section are due to Freund and Schapire [1999]. Here, we extend their proof to derive finer mistake bounds for the Perceptron algorithm in terms of the L2-norm of the vector of hinge losses of an arbitrary weight vector at points where an update is made.", "startOffset": 73, "endOffset": 100}, {"referenceID": 3, "context": "Such a hypothesis can be determined via cross-validation Littlestone [1989] or using the online-tobatch theorem of Cesa-Bianchi et al.", "startOffset": 57, "endOffset": 76}, {"referenceID": 2, "context": "Such a hypothesis can be determined via cross-validation Littlestone [1989] or using the online-tobatch theorem of Cesa-Bianchi et al. [2004]. The latter can be combined with any of the mistake bounds presented in the previous section to derive generalization bounds for the Perceptron predictor.", "startOffset": 115, "endOffset": 142}, {"referenceID": 2, "context": "Theorem 4 (Cesa-Bianchi et al. [2004]).", "startOffset": 11, "endOffset": 38}, {"referenceID": 0, "context": "The Perceptron algorithm of Figure 1 can be straightforwardly extended to define a non-linear separator using a positive definite kernel K [Aizerman et al., 1964].", "startOffset": 139, "endOffset": 162}], "year": 2013, "abstractText": "We present a brief survey of existing mistake bounds and introduce novel bounds for the Perceptron or the kernel Perceptron algorithm. Our novel bounds generalize beyond standard margin-loss type bounds, allow for any convex and Lipschitz loss function, and admit a very simple proof.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}