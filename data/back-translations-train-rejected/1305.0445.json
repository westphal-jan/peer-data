{"id": "1305.0445", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-May-2013", "title": "Deep Learning of Representations: Looking Forward", "abstract": "Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forward-looking research directions aimed at overcoming these challenges.", "histories": [["v1", "Thu, 2 May 2013 14:33:28 GMT  (46kb)", "http://arxiv.org/abs/1305.0445v1", null], ["v2", "Fri, 7 Jun 2013 02:35:21 GMT  (54kb)", "http://arxiv.org/abs/1305.0445v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio"], "accepted": false, "id": "1305.0445"}, "pdf": {"name": "1305.0445.pdf", "metadata": {"source": "CRF", "title": "Deep Learning of Representations: Looking Forward", "authors": ["Yoshua Bengio"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 130 5.04 45v1 [cs.LG] 2 May 201 3"}, {"heading": "1 Background on Deep Learning", "text": "They have important empirical successes in a number of traditional AI applications such as computer vision and artificial language processing. See (Bengio, 2009; Bengio et al., 2013c) for reviews and the other chapters of the book (Montavon and Muller, 2012) for practical guidelines. Deep learning attracts a lot of attention from both academic and industrial communities. Companies such as Google, Apple, IBM and Baidu invest in deep learning, with the first widely used products from consumers targeting speech recognition. Deep learning is also used for object recognition (Google Goggles), image and music information retrieval (Google Image Search, Google Music), and computational advertising (Corrado, 2012)."}, {"heading": "2 Quick Overview of Deep Learning Algorithms", "text": "The central concept behind all deep learning methods is the automated discovery of abstraction, believing that more abstract representations of data such as images, video and audio signals tend to be more useful: they represent the semantic content of the data, detached from the low properties of the raw data (e.g. pixels, voxels or waveforms). Deep architectures lead to abstract representations because more abstract concepts can often be constructed in terms of less abstract ones. Deep learning algorithms are special cases of imaging learning with the property that they learn multiple levels of representation. Deep learning algorithms often use flat (single-layered) imaging learning algorithms as sub-routines. Before treating the unattended imaging learning algorithms, we quickly review the basic principles behind monitored imaging learning algorithms such as Bengio's good old, multi-layered neural networks. Supervised and unattended targets can be naturally combined with coefficient parameters (such as Raroefficient)."}, {"heading": "2.1 Deep Supervised Nets, Convolutional Nets, Dropout", "text": "Prior to 2006, it was believed that training deep neural networks (Rumelhart et al., 1986) was too difficult (and indeed did not work), and the first breakthrough in training occurred in Geoff Hinton's laboratory with unsupervised lecturers, which is why RBMs (Hinton et al., 2006) discussed it in the next subsection. However, more recently, it has been found that one can delve deep into the networks by properly initializing them, just large enough for gradations to convey useful information (Glorot et al., 2010; Sutskever, 2012). Another interesting aspect of training is that the deep superordinate networks of Glorot and Bengio (and later Krizhevsky et al.) is the presence of rectifying non-linearity (such as max. (0, x)) instead of sigmoidal non-linearity."}, {"heading": "2.2 Unsupervised or Supervised Layer-wise Pre-Training", "text": "One of the key findings of recent years of deep learning research is that deep compositions of nonlinearity - as applied in deep feedback networks or in recurring networks over long sequences - can be very sensitive to initialization (some initializations may lead to much better or much worse results after training).The first type of approach that has proven useful to reduce this sensitivity is based on greedy layered pre-training (Hinton et al., 2006; Bengio et al., 2007).The idea is to train one layer at a time, starting at lower layers (on the top of the input), so that there is a clear training target for the currently added layer (which usually avoids the need to undo error gradients through many layers of nonlinearity).With unattended preliminaries, each layer is trained to model the distribution of the values that output will be created as the previous layer that can be used as a new representation for the previous one."}, {"heading": "2.3 Directed and Undirected Graphical Models with Anonymous Latent Variables", "text": "In fact, the fact is that most of them are able to assert themselves, that they are able to assert themselves, that they are able to assert themselves."}, {"heading": "2.4 Regularized Auto-Encoders", "text": "The idea is that the reconstruction errors in the reconstruction examples should be low, but a high reconstruction error must be avoided in most other input configurations. In the case of auto-encoders, a good generalization means that test examples (sampled from the same distribution as training examples) must also have low reconstruction errors. Auto-encoders must be regulated to prevent them from simply learning the identity function r (x) = x, which would be useless. Regulated auto-encoders include the old bottleneck auto-encoders (as in PCA) with less hidden units as input."}, {"heading": "2.5 Sparse Coding and PSD", "text": "It is not only a matter of time until it will be done, but also of time until it will be done, until it can be done, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "3 Scaling Computations", "text": "From a mathematical point of view, how do we scale the recent successes of deep learning to much larger models and huge datasets, so that the models are actually richer and collect a very large amount of information?"}, {"heading": "3.1 Scaling Computations: The Challenge", "text": "The beginnings of learning in recent years have led to people finding themselves in another world in the countries in which they live. (...) It is not that they find themselves in another world. (...) It is that they find themselves in another world. (...) It is not that they find themselves in another world. (...) It is that they find themselves in another world. (...) It is that they find themselves in another world. (...) It is that they find themselves in another world. (...) It is that they find themselves in another world. (...) It is that they find themselves in another world. (...) It is as if they find themselves in another world. (...) It is that way. (...) It is that way. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. (...) It is. It is. (...) It is. (... It is. It is. It is. (...) It is. It is. (...) It is. It is. It is. It is. (...) It is. It is. It is. () It is. It is. (... It is. It is. (). It is. It is. It is. (). It is. It is. (. It is. It is. It is. (). It is. It is."}, {"heading": "3.2 Scaling Computations: Solution Paths", "text": "In fact, it is the case that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live."}, {"heading": "4 Optimization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Optimization: The Challenge", "text": "As we look at larger and larger datasets (which grow faster than the size of the models), training errors and generalization errors seem to merge. Furthermore, there is much evidence for the results of deep learning experiments to suggest that the formation of deep networks (including recurring networks) requires difficult optimization (Bengio, 2013b; Gulcehre and Bengio, 2013). It is not yet clear how much of the difficulty is due to local minimums and how much is due to the inadequacy of optimization problems. It is therefore interesting to study the optimization methods and the difficulties in deep learning processes that can be observed to achieve a better generalization."}, {"heading": "4.2 Optimization: Solution Paths", "text": "Despite these recent encouraging results, several recent experimental results once again point to a fundamental difficulty in training intermediate and lower layers.Diminishing Returns with Larger Networks. First, Dauphin and Bengio (2013) show that with well-optimized SGD training, as the size of a neural network increases, the \"return on investment\" (number of training errors removed per additional hidden unit) decreases, given a specified number of training sessions, to the point where it goes below 1 (which is the return on investment that would be achieved by a brain-dead learning mechanism - such as Parzen Windows - which only copies a mislabeled example into the weights of the additional hidden unit to produce just the right answer for this example).This suggests that larger models may be generally more difficult to train, probably because there are now more second-order interactions between the parameters that increase the number of the state of the second derivatives of the second matrix in Hessen."}, {"heading": "5 Inference and Sampling", "text": "All models studied for deep learning other than the modest RBM require a non-trivial form of conclusion (i.e. guessing the values of the latent variable h suitable for the given visible input x); several forms of conclusion have been studied in the past: MAP conclusion is formulated as an optimization problem (in search of h that maximizes approximately P (h | x)); MCMC conclusions are attempted to sense a sequence of h from P (h | x); variational conclusions are formulated as an optimization problem (when the norm of the gradient is above a threshold, it is reduced to a simple (typically factorial) harmless posterior qx (h), which usually involves an iterative optimization process. See an up-to-date textbook on machine learning (Bishop, 2006; Barber, 2011; Murphy, 2012)."}, {"heading": "5.1 Inference and Sampling: The Challenge", "text": "There are several challenges associated with all these inferences and sampling techniques."}, {"heading": "5.2 Inference and Sampling: Solution Paths", "text": "The idea of moderation (Iba, 2001) for MCMCs is comparable to the idea of simulated nebulization (Kirkpatrick et al., 1983) for optimization, and it is therefore very attractive to solve the problem of mixing mode: If we look at a smooth version (higher temperature, which is achieved only by dividing the energy by a temperature greater than 1) of the distribution of interest; it therefore spreads the probability that the mass is more uniform, so one can mix between the modes at this high-temperature version of the model, and then gradually react coolly to the target distribution as MCMC moves on to ensure that we end up in one of the \"islands of high probability.\" Desjardins et al. (2010); Cho et al. (2010); Salakhutdinov (2010b, a) have18 See examples of images generated with some of the current state-of-the-art models of couret (2011), al."}, {"heading": "6 Disentangling", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Disentangling: The Challenge", "text": "One answer might be that these factors can be controlled separately. Complex data results from the rich interaction of many sources. These factors interact in a complex network that can complicate the object's tasks. By identifying and separating these factors, we will almost have solved the learning problem."}, {"heading": "6.2 Disentangling: Solution Paths", "text": "There are some encouraging signs that our current unattended representation learning algorithms reduce the \"entanglement\" of the underlying factors26 as we apply them to raw data (or to the output of an earlier representation learning procedure, such as when we completely replace RBMs or regulated auto encoders).First, there are experimental observations suggesting that sparse Constitutional RBMs and sparsely denoising auto encoders in their hidden units achieve a higher level of disentanglement than in their inputs (Goodfellow et al., 2009; Glorot et al., 2011b) What these authors have found is that some hidden units are particularly sensitive to a known factor of variation, while they are more insensitive (i.e., invariant) to others. For example, in a sentiment analysis model that sees unlabeled paragraphs of customer comments from the Amazon site."}, {"heading": "7 Conclusion", "text": "Deep learning and more general representation learning are current areas of research in machine learning, and in recent years of research several major challenges have been clearly identified to address the performance of these algorithms from a human perspective. We have divided these challenges into four broad areas: scaling calculations, reducing the difficulties in optimizing parameters, designing (or avoiding) expensive conclusions and samples, and helping to learn representations that better unravel the unknown underlying the 19 variation factors. There is room for exploring many ways to solve all of these problems, and we have presented some appealing research directions for these challenges here."}, {"heading": "Acknowledgments", "text": "The author is extremely grateful for the feedback and discussions he has had with his collaborators Ian Goodfellow, Guillaume Desjardins, Aaron Courville, Pascal Vincent, Roland Memisevic and Nicolas Chapados. He has greatly contributed to shaping the ideas presented here and refining this manuscript. He is also grateful for the financial support of NSERC, CIFAR, the Canadian research leaders, and Compute Canada.BibliographyAlain, G. and Bengio, Y. (2012). He is also grateful for the dissemination of data. Technical Report Arxiv 1211.4246, Universit\u00e9s de Montr\u00e9es, al. Bach, F., Jenatton, Mairal, and Obozinski, G. (2011). Structured thrift through Convex Optimization. Technical Report, arXiv.1109.2397. Bagnell, J. and Bradley."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forwardlooking research directions aimed at overcoming these challenges. 1 Background on Deep Learning Deep learning is an emerging approach within the machine learning research community. Deep learning algorithms have been proposed in recent years to move machine learning systems towards the discovery of multiple levels of representation. They have had important empirical successes in a number of traditional AI applications such as computer vision and natural language processing. See (Bengio, 2009; Bengio et al., 2013c) for reviews and Bengio (2013c) and the other chapters of the book (Montavon and Muller, 2012) for practical guidelines. Deep learning is attracting much attention both from the academic and industrial communities. Companies like Google, Microsoft, Apple, IBM and Baidu are investing in deep learning, with the first widely distributed products being used by consumers aimed at speech recognition. Deep learning is also used for object recognition (Google Goggles), image and music information retrieval (Google Image Search, Google Music), as well as computational advertising (Corrado, 2012). A deep learning building block (the restricted Boltzmann machine, or RBM) was used as a crucial part of the winning entry of a million-dollar machine learning competition (the Netflix competition) (Salakhutdinov et al., 2007; T\u00f6scher et al., 2009). The New York Times covered the subject twice in 2012, with front-page articles.1 Another series of articles (including a third New York Times article) covered a more recent event showing off the application of deep learning in a major Kaggle competition for drug discovery (for example see \u201cDeep Learning The Biggest Data Science Breakthrough of the Decade\u201d2. Much more recently, Google bought out 1 http://www.nytimes.com/2012/11/24/science/scientists-see-advancesin-deep-learning-a-part-of-artificial-intelligence.html 2 http://oreillynet.com/pub/e/2538 (\u201cacqui-hired\u201d) a company (DNNresearch) created by University of Toronto professor Geoffrey Hinton (the founder and leading researcher of deep learning) and two of his PhD students, Ilya Sutskever and Alex Krizhevsky, with the press writing titles such as \u201cGoogle Hires Brains that Helped Supercharge Machine Learning\u201d (Robert McMillan for Wired, March 13th, 2013). The performance of many machine learning methods is heavily dependent on the choice of data representation (or features) on which they are applied. For that reason, much of the actual effort in deploying machine learning algorithms goes into the design of preprocessing pipelines that result in a hand-crafted representation of the data that can support effective machine learning. Such feature engineering is important but labor-intensive and highlights the weakness of many traditional learning algorithms: their inability to extract and organize the discriminative information from the data. Feature engineering is a way to take advantage of human ingenuity and prior knowledge to compensate for that weakness. In order to expand the scope and ease of applicability of machine learning, it would be highly desirable to make learning algorithms less dependent on feature engineering, so that novel applications could be constructed faster, and more importantly for the author, to make progress towards artificial intelligence (AI). A representation learning algorithm discovers explanatory factors or features. A deep learning algorithm is a particular kind of representation learning procedure that discovers multiple levels of representation, with higher-level features representing more abstract aspects of the data. This area of research was kick-started in 2006 by a few research groups, starting with Geoff Hinton\u2019s group, who initially focused on stacking unsupervised representation learning algorithms to obtain deeper", "creator": "LaTeX with hyperref package"}}}