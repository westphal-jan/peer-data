{"id": "1305.0665", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2013", "title": "Spectral Classification Using Restricted Boltzmann Machine", "abstract": "The spectra of a celestial body is composed of a great number of spectrum, thus it's necessary for us to explore the appropriate latent features to represent the celestial body well. Restricted Boltzmann Machine (RBM) is a bipartite generative graphical model composed of two layers (one visible layer and one hidden layer), which can extract higher level features to represent the original data. Despite generative, RBM can be used for classification when modified with free energy and softmax function. In this paper, to apply the binary RBM for stellar spectral classification, we first perform \"threshold binaryzation\" on the original data by some rule (for detail, c.f. Section 4), then we resort to binary RBM to classify cataclysmic variables (CVs) from non-CVs (one half of all the given data for training and the other half for testing). The experiment result shows state-of-the-art accuracy of 100%, and it outperforms support vector machine (SVM), with accuracy of 99.14%.", "histories": [["v1", "Fri, 3 May 2013 10:20:02 GMT  (20kb)", "https://arxiv.org/abs/1305.0665v1", null], ["v2", "Sun, 13 Oct 2013 01:03:56 GMT  (111kb)", "http://arxiv.org/abs/1305.0665v2", "8 pages, 2 figures, Accepted in PASA for publication"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fuqiang chen", "yan wu", "yude bu", "guodong zhao"], "accepted": false, "id": "1305.0665"}, "pdf": {"name": "1305.0665.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["yanwu@tongji.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 130 5.06 65v2 [cs.LG] 1 3O ct2 013Keywords: astronomical instrumentation, methods and techniques - Methods: analytical - Methods: data analysis - Methods: statistical"}, {"heading": "1 Introduction", "text": "With the rapid development of both astronomical instruments and various machine learning algorithms, we can apply the spectral properties of stars to classify the stars. A large number of astronomical observatories have been built to obtain the spectra, such as the Large Sky Area Multi-Object Fibre Spectroscopic Telescope (LAMOST) in China. A variety of machine learning methods, such as main component analysis (PCA), local linear embedding (LLE), artificial neural networks (ANN) and decision tree, etc., have been used to classify these spectra in an automatic and efficient manner. In this study, we apply a novel machine learning method, limited by Boltzmann machine, to classify the CVs and non-CVs.CVs. CVs consist of the narrow binary systems containing a white dwarf whose magnetic material from its companion Warner 2003 (10 hours in orbit) are small in general."}, {"heading": "1.1 Previous work in spectral classification in astronomy", "text": "In 1998, Singh et al. applied principle component analysis (PCA) and artificial neural networks (ANN) to stellar spectral classification (Singh, Gulati, & Gupta 1998) on O to M type stars, where O type stars are the hottest and the letter sequence (O to M) shows successively cooler stars up to the coolest M type stars. They adopted PCA for dimension reduction, in which they reduced the dimension to 20, with cumulative percentages greater than 99.9%. Then they used multilayered backpropagation (BP) neural networks for classification. In 2006, Sarty and Wu applied two well-known multivariate analysis methods, i.e. PCA and discriminatory functional analyses, to analyze the spectroscopic emission data collected by Williams (1983). Using the PCA method, they found that the source of the variation had orbital binary period correlation."}, {"heading": "1.2 Previous application of RBM", "text": "In this section, we present some representative applications of the RBM algorithm that are so advanced. In 2007, Salakhutdinov et al. (Salakhutdinov, Mnih, & Hinton 2007) used RBM for collaborative filtering, which is closely related to recommendation systems in the machine learning community. In 2008, Gunawardana et al. (Gunawardana & Meek 2008) used RBM for cold start recommendation.In 2009, Taylor and Hinton (Taylor & Hinton 2009) used RBM for modeling movement styles.In 2010, Dahl et al. (Dahl et al. 2010) used RBM for telephone recognition of the TIMIT dataset.In 2011, Schluter and Osendorfer (Schluter & Osendorfer 2011) used RBM to estimate the similarity of music. In 2012, Tang et al. (Salakhutdinov, Tang & Hindinov) applied the RBM to public databases."}, {"heading": "1.3 Our work", "text": "In this study, we used the binary RBM algorithm to classify the spectra of resumes and non-resumes from the SDSS. Generally, before using a classifier for classification, we normalize the original data, for example normalization, in order to obtain better properties and thus achieve better performance. Finally, we use the binary RBM for classifying the data, one half of all data given for training and the other half for testing. The experiment results show that the classification accuracy is 100%, which is state of the art. And RBM surpasses the dominant SVM classifier with an accuracy of 99.14% (Bu et al. 2013). The rest of this paper is as follows. In section 2, we introduce the results for the training site Boltzmann-3."}, {"heading": "2 Prerequisites", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Markov Chain", "text": "A Markov chain is a sequence composed of a number of random variables. Each element in the sequence can randomly transfer from one state to another. In fact, a Markov chain belongs to a stochastic process (Andrieu et al. 2003). Generally, the number of possible states for each element or random variable in a Markov chain is finite, and a Markov chain is a random process without memory. It is the current state, not the states preceding the current state, that can affect the next state of a Markov chain. This is the well-known Markov property (Xiong, Jiang, & Wang 2012).Mathematically, a Markov chain is a sequence, X1, X2, X3,. with the following property: P (Xn + 1 = xn + 1 | ger = 1, X2 = x2,."}, {"heading": "2.2 MCMC", "text": "The Markov chain Monte Carlo (MCMC) is a sampling algorithm consisting of a certain probability distribution. For the detailed information of the MCMC, readers are referred to Andrieu et al. (2003).The sampling process takes the form of a Markov chain and the goal of the MCMC is to obtain a desired distribution, or rather the equilibrium distribution, by performing many inference steps. The greater the number of iterations, the better the performance of the MCMC. And the MCMC can be used for unattended learning with some hidden variables or maximum probability estimation (MLE) when learning some unknown parameters (Andrieu et al. 2003)."}, {"heading": "2.3 Gibbs Sampling", "text": "Gibbs sampling method can be used to obtain a sequence of approximate samples from a particular probability distribution, where direct sampling is not usually easy to implement. To obtain detailed information about Gibbs sampling, readers are referred to Gelfand (2000). The sequence obtained with the Gibbs sampling method can be applied to determine joint distribution and marginal distribution with respect to (v.r.t.) one of the variables, etc. In general, Gibbs sampling method is a method for probable conclusions. Gibbs sampling method can produce a Markov chain of samples on condition that each of the samples correlates with the nearby sample, or rather the probability of selecting the next sample is equal to 1 in Gibbs sample (Andrieu et al. 2003)."}, {"heading": "3 RBM", "text": "In view of the fact that RBM is a generalized version of the Boltzmann machine (BM), we must first consider BM = = 1 weight.For detailed information from BM, readers are referred to Ackley, Hinton, & Sejnowski (1985).BM can be considered a two-part graphical model consisting of two layers in which there are a number of units with connections between layer and lay.One layer is a visible layer v with binary visible units vi, i.e. vi = 0 or vi = 1 (i = 1, 2,.., m).For each unit in the visible layer, the corresponding value is observable.The other layer is a hidden (latent) layer h with binary hidden units hj. As in the visible layer, hj = 0 or hj = 1 (j = 1, 2,.,., n).For each unit or neuron in the hidden layer, the hidden layer is the hidden layer, the ornified layer."}, {"heading": "3.1 Contrastive Divergence", "text": "Contrastive divergence (CD) is suggested by Hinton and can be used to train RBM (Hinton, Osindero, & Teh 2006).First, we get vi (i = 1, 2,.., m), then we can get hj (j = 1, 2,.., n) by the sigmoid function given above, and the value of hj is determined by comparing a random value r in the range of 0 to 1 with the probability p (hj = 1 | v).Then we can reconstruct v by p (vi = 1 | h).We can repeat the above process backwards and forwards until the reconstruction error is small enough or reaches the maximum number of iterations specified beforehand. To update the weights and distortions in an RBM, it is necessary to calculate the following partial derivative of evi (v, h)."}, {"heading": "3.2 Free energy and Soft-max", "text": "To use RBM for classification, we can use the following technique. We can form an RBM for each specific class =. And for classification, we need the free energy and the soft-max function for help. For a specific visible input vector v, its free energy can be calculated as follows: F (v) = [single configuration must have and it corresponds to the sum of probabilities of all configurations that contain v.], (6) where xj = bj = bj i viwij.For a specific test vector v, its free energy can be calculated as follows: F (v) = \u2212 ivici + jlog (1 + exj)], where xj = bj i viwij.For a specific test vector v, after training the RBMc on a specific class c, the protocol probability that RBMc can be calculated according to the following formula."}, {"heading": "4 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data description", "text": "In fact, the fact is that most of them are able to move around without being able to move."}, {"heading": "3522 is the dimension of the original data.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2 Parameter chosen", "text": "In this section we present the parameters of our experiment. We selected all parameters related to Hinton (2012), the learning rate during the update process was set at 0.1, the momentum for smoothness and to avoid overadjustment was set at 0.5, the maximum number of eras was set at 50, the weight decay factor, penalty, was set at 2 x 10 \u2212 4, the initial weights were randomly generated from the normal standard distribution, while the distortion vectors b and c were initialized at 0. To illustrate them, we present them in the following table (Table 3)."}, {"heading": "4.3 Experiment result", "text": "We first normalized the data so that they have unit l2 standard, i.e. for a specific vector x = [x1, x2,.., xn] corresponds to the l2 standard of the vector \"i x 2 i = 1. Then we could get two matrices, one was A = 6818 \u00b7 3522 and the other was B = 208 \u00b7 3522. Then we found the maximum element and the minimum element for CVs or non-CVs. Finally, to apply the binary RBM for classification, we found a parameter to determine the value of the variables in our experiment with 0 or 1, or rather, the binarizable numbers.Mathematically, we set ifS (i, j) \u2212 minS (i, j) < (maxS) \u2212 minS (i, j) \u2212 minS (i, j), and the value of VIII), then we set S (i, j) to 1."}, {"heading": "5 Conclusion and future work", "text": "Through the introduction of free energy and soft-max functionality, RBM can be used for classification. In this work, we use a restricted Boltzmann machine (RBM) for the spectral classification of non-CVs and CVs. And the experimental result shows that the4You can use any two integers to represent the labels of samples belonging to non-CVs and CVs, and this does not affect the result of the experiment. the classification accuracy is 100%, which corresponds to the state of the art and surpasses the more prevalent classifiers SVM. Since RBM is the building block of Deep-Faith Networks (DBNs) and Deep-Boltzmann-Machines (DBM), we can conclude that Deep-Boltzmann machines (Salakhutdinov & Hinton 2009) and Deep-Faith Networks can also work well on the Deep-Faith Classification, which is our future work."}, {"heading": "Acknowledgments", "text": "The authors are very grateful to the anonymous reviewer for a thorough reading, many valuable comments and helpful suggestions. The authors thank the editor Bryan Gaensler very much for the helpful suggestions in organizing the manuscript. The authors also thank Jiang Bin for providing the resume data."}], "references": [{"title": "Cataclysmic variable stars (Cam", "author": ["B. Warner"], "venue": null, "citeRegEx": "1025", "shortCiteRegEx": "1025", "year": 2003}], "referenceMentions": [], "year": 2013, "abstractText": "In this study, a novel machine learning algorithm, restricted Boltzmann machine (RBM), is introduced. The algorithm is applied for the spectral classification in astronomy. RBM is a bipartite generative graphical model with two separate layers (one visible layer and one hidden layer), which can extract higher level features to represent the original data. Despite generative, RBM can be used for classification when modified with a free energy and a soft-max function. Before spectral classification, the original data is binarized according to some rule. Then we resort to the binary RBM to classify cataclysmic variables (CVs) and non-CVs (one half of all the given data for training and the other half for testing). The experiment result shows state-of-the-art accuracy of 100%, which indicates the efficiency of the binary RBM algorithm.", "creator": "LaTeX with hyperref package"}}}