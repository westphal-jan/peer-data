{"id": "1305.0698", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-May-2013", "title": "Learning from Imprecise and Fuzzy Observations: Data Disambiguation through Generalized Loss Minimization", "abstract": "Methods for analyzing or learning from \"fuzzy data\" have attracted increasing attention in recent years. In many cases, however, existing methods (for precise, non-fuzzy data) are extended to the fuzzy case in an ad-hoc manner, and without carefully considering the interpretation of a fuzzy set when being used for modeling data. Distinguishing between an ontic and an epistemic interpretation of fuzzy set-valued data, and focusing on the latter, we argue that a \"fuzzification\" of learning algorithms based on an application of the generic extension principle is not appropriate. In fact, the extension principle fails to properly exploit the inductive bias underlying statistical and machine learning methods, although this bias, at least in principle, offers a means for \"disambiguating\" the fuzzy data. Alternatively, we therefore propose a method which is based on the generalization of loss functions in empirical risk minimization, and which performs model identification and data disambiguation simultaneously. Elaborating on the fuzzification of specific types of losses, we establish connections to well-known loss functions in regression and classification. We compare our approach with related methods and illustrate its use in logistic regression for binary classification.", "histories": [["v1", "Fri, 3 May 2013 13:26:24 GMT  (184kb,D)", "http://arxiv.org/abs/1305.0698v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["eyke h\\\"ullermeier"], "accepted": false, "id": "1305.0698"}, "pdf": {"name": "1305.0698.pdf", "metadata": {"source": "CRF", "title": "Learning from Imprecise and Fuzzy Observations: Data Disambiguation through Generalized Loss Minimization", "authors": ["Eyke H\u00fcllermeier"], "emails": ["eyke@mathematik.uni-marburg.de"], "sections": [{"heading": null, "text": "Methods for analyzing or learning fuzzy data have attracted increasing attention in recent years. However, in many cases, existing methods (for precise, non-fuzzy data) are extended ad hoc and without careful consideration of the interpretation of a fuzzy set of data in modeling to the fuzzy case. We distinguish between an ontic and an epistemic interpretation of fuzzy, specified data and focus on the latter, arguing that \"fuzzification\" of learning algorithms based on an application of the generic augmentation principle is not appropriate. In fact, the augmentation principle does not properly exploit the inductive distortion underlying statistical and machine learning methods, although this distortion provides, at least in principle, a means of \"disambiguating\" the fuzzy data. Alternatively, we propose a method based on the generalization of loss functions in empirical risk mitigation, simultaneously modeling and data identification."}, {"heading": "1 Introduction", "text": "In fact, it is an attempt that is able to move in a different direction, in a direction in which one is able to move, in which one is able to move in a different direction."}, {"heading": "2 Notation and Basic Setting", "text": "We look at the problem of model induction, which, broadly speaking, consists in moving from a specific data sample to a general (although hypothetical) model that describes the process of data generation or at least certain characteristics of this process. In this context, a learning (data analysis) algorithm ALG is given as input of a setD = {zi} N = 1 (1) of data points zi \u00b2 Z (1). As an output, the algorithm produces a model M \u0445 M, where M is a predefined model class. Formally, the algorithm can therefore be regarded as a mappingALG: D \u2192 M, (2) where D is the space of potentially observable data samples. For example, the data points could be vectors in Z = Rd, and the model could represent a division of the data into a finite group of disjunct groups (clusters), or the model could be a probability density function characterizing the underlying data generalization process."}, {"heading": "3 Data Disambiguation", "text": "Considering a learning algorithm ALG for precise data, the simplest procedure in dealing with a fuzzy sample (3) is not the application of the known extension principle (Zadeh, 1975) to Figure (2). Formally, we define an instance of fuzzy sample (3) as a reasonable procedure (3), however, the sample D = {zi} N = 1 of the precise data points in which the sample D (1) N = 1 of the precise data points in which the sample D (D) N = min {\u00b5S = 1 of the precise data points in which zi (zi) the degree of membership in Zi is plausible. Then, the result of applying ALG to the fuzzy data (3) is a fuzzy set of models in M, with the degree of membership of M (M) given."}, {"heading": "4 A Loss Minimization Approach", "text": "How can the model induction be combined with the data disambiguation? Here we propose an approach based on the concept of (direct) loss minimization. Broadly speaking, instead of generalizing the learning algorithm, as the extension principle does, we \"fuzzify\" an underlying loss function that is to be minimized by this algorithm. So, instead of first fixing an instance and then adjusting a model to this data, we look for an optimal instance given to a model; the model itself is then evaluated on the basis of this instantiation.In supervised learning, the main goal is typically to find a model with minimum risk, i.e., expected losses R (M) = L (y, M (x))))) dP (x, y), y (6), where L: Y \u00d7 Y \u2192 R is a loss function: For an input X-X function, the prediction is verified with M = probability (a corresponding problem with x) in terms of quantity and performance."}, {"heading": "4.1 The Case of Set-Valued Data", "text": "To make it even simpler, we first look at the case with a fixed value before turning to the more general unclear case. Furthermore, we look at the inaccuracy only for the output part, while the inputs are supposed to be precise. If we look at a candidate model M and an inaccurate observation (x, Y). With y = M (x), the amount of possible losses of M in this observation is then given by {L (y, y, y, Y). In accordance with the idea of data disambiguity, we should look at the smallest of these losses, namely L (Y, y) = min {L (y, y) = min {L (y, y, Y}, and the value for which it is achieved: 1y = arg min {L (y, y, y, y, Y) | y, Y}. Given the model M, this value appears to be the most plausible in Y."}, {"heading": "4.2 The Case of Fuzzy Data", "text": "In the defined case, each candidate model M is evaluated using a generalized empirical approach, i.e. a risk function based on a generalized loss, which can be expressed according to a standard empirical risk on a correctly selected (instantiated) data sample: Remp (M) = 1N N (1 L (yMi), M (xMi), (14), where (xMi, y M i) = SEL (Xi, Yi, M) (15) = arg min min {L (yi, M), (xi), M (yi), Xi), the disambiguation of (Xi, Yi) under M (Xi, M i) = arg min M Remp (M), (16), which is considered unique here, which in turn results in a unique disambigulation."}, {"heading": "4.3 Fuzzy Losses for Regression", "text": "The fuzzy loss function (21) compares a fuzzy value Y with a (predicted) precise value y. \"An example of such a loss is shown in Figure 4 in the case of regression. \u2212 This function is thus a blurred version of the absolute (L1) lossL (y, y, y) = | y \u2212 y function, which is represented as a dashed line (as a function of the y value for fixed y = 5.5).The fuzzy loss (solid line) is given by the map y 7 \u2192 L (Y, y), where Y is the trapezoidal fuzzy function, which is shown in gray.Interestingly, a blur of the L1 value based on a triangular fuzzy set Y with center point y and support (y, y +) results in a kind of Huber loss (Huber, 1981): L (Y, y, y): 2 (y, y, 2) = (function) when blurred."}, {"heading": "4.4 Fuzzy Losses for Classification", "text": "In classification problems, the output space Y is a finite amount consisting of K classes, although in fact it is not fully considered as such. (The most typical loss function is the 0 / 1 loss prediction L (y, y) = Jy 6 = y-K. Now let us assume that the results are characterized by a fuzzy subset Y of Y, that is, by a fuzzy degree of positive loss in the predicted class. (The higher degree of membership in the predicted class y, that is, the lower degree of loss in the predicted class y, indicates that the lower value of loss in the predicted class y, the lesser special case for a fuzzy composition of the type\u00b5Y classes is achieved. (1) If there is a fuzzy prediction in the predicted class y - w if the results in the predicted class y (y) are wrong. (22) The higher the degree of membership in the predicted class, the smaller the predicted loss is the predicted."}, {"heading": "5 Comparison with Denoeux\u2019s Approach", "text": "Specifically, he addresses the problem of learning imprecise data, which is presented in terms of unclear sentences or belief functions, within a probable framework and for this purpose an extension of the maximum probability is proposed. (Without going into technical details, we will try to highlight the most important conceptual differences between the two approaches, which are hereafter referred to as GMLI for generalized maximum likelihood inference, and present our notions of the former in terms of our notation. (3Routinely speaking, a sample of imprecise data D = {Zi} Ni = 1, Denoeux defines the plausibility of a model MI in terms of normalized likelihood; the probability of such a process is in turn defined by the probability that the data generation process is specified."}, {"heading": "6 Illustration", "text": "This section illustrates our approach in a simple classification setting. Before explaining the setup, we stress that our experiments are not seen as empirical validation of our approach, let alone as a comparison with alternative methods in terms of specific performance measures. However, since we consider the contribution of this paper to be more conceptual than methodological examples, and indeed have proposed a simple binary classification problem rather than a concrete method, such a comparison is probably not appropriate at this point. Nevertheless, we would like to demonstrate the potential usefulness of our fuzzy loss functions using a practical example. To this end, we consider a simple binary classification problem with normally distributed classes in R2, the positive with medium \u00b5 + = (1) and the negative with the middle class. \u2212 1) As training data, we assume that a sample consists of 100 randomly generated instances of both classes; a typical example is shown in such a sample."}, {"heading": "7 Conclusion", "text": "We have introduced a conceptual framework for (supervised) learning from imprecise and blurred data, based on generalizing loss functions in empirical risk mitigation. In contrast to the generic augmentation principle, our approach implicitly exploits the inductive bias underlying the learning method while simultaneously performing model identification and data disambiguation.Our advanced loss functions allow direct \"comparison\" of a (precise) prediction with an imprecise observation, thus providing the basis for adapting a precise model to inaccurate data. The principle we have applied to augmenting a standard loss function is consistent with our conception of data disambiguity and can be regarded as a spot-specific \"modulation\" of original losses. Interestingly, our blurred set-based generalization of loss functions covers several existing methods as special cases, including insignificant (regression) and (regression) sensitive (regression)."}], "references": [{"title": "On the possibilistic approach to linear regression", "author": ["M. Cerny", "M. Rada"], "venue": null, "citeRegEx": "Cerny and Rada,? \\Q2011\\E", "shortCiteRegEx": "Cerny and Rada", "year": 2011}, {"title": "Fuzzy regression methods\u2014a comparative", "author": ["Y. Changa", "B. Ayyubb"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2001}, {"title": "Learning from partial labels", "author": ["T. Cour", "B. Sapp", "B. Taskar"], "venue": null, "citeRegEx": "Cour et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cour et al\\.", "year": 2011}, {"title": "On the variability of the concept of variance", "author": ["I. Couso", "D. Dubois"], "venue": "Machine Learning Research,", "citeRegEx": "Couso and Dubois,? \\Q2009\\E", "shortCiteRegEx": "Couso and Dubois", "year": 2009}, {"title": "Fuzzy regression analysis", "author": ["P. Diamond", "H. Tanaka"], "venue": "Slowinski, R., editor, Fuzzy Sets in Decision Analysis, Operations Research and Statistics, pages 349\u2013387. Kluwer.", "citeRegEx": "Diamond and Tanaka,? 1998", "shortCiteRegEx": "Diamond and Tanaka", "year": 1998}, {"title": "Ontic vs", "author": ["D. Dubois"], "venue": "epistemic fuzzy sets in modeling and data processing tasks. In Madani, K., Kacprzyk, J., and Filipe, J., editors, Proc. IJCCI (NCTA), International Conference on Neural Computation Theory and Applications, Paris.", "citeRegEx": "Dubois,? 2011", "shortCiteRegEx": "Dubois", "year": 2011}, {"title": "Gradual elements in a fuzzy set", "author": ["D. Dubois", "H. Prade"], "venue": "Soft Computing, 12(2):165\u2013175.", "citeRegEx": "Dubois and Prade,? 2008", "shortCiteRegEx": "Dubois and Prade", "year": 2008}, {"title": "A linear regression model for imprecise response", "author": ["M. Ferraro", "R. Coppi", "G. Gonzalez-Rodriguez", "A. Colubi"], "venue": "Int. Journal of Approximate Reasoning, 51:759\u2013770.", "citeRegEx": "Ferraro et al\\.,? 2010", "shortCiteRegEx": "Ferraro et al\\.", "year": 2010}, {"title": "Estimation of a simple linear regression model for fuzzy random variables", "author": ["G. Gonzalez-Rodriguez", "A. Blanco", "A. Colubi", "M. Lubiano"], "venue": "Fuzzy Sets and Systems, 160(3):357\u2013370.", "citeRegEx": "Gonzalez.Rodriguez et al\\.,? 2009", "shortCiteRegEx": "Gonzalez.Rodriguez et al\\.", "year": 2009}, {"title": "Robust Statistics", "author": ["P. Huber"], "venue": "Wiley.", "citeRegEx": "Huber,? 1981", "shortCiteRegEx": "Huber", "year": 1981}, {"title": "Learning from ambiguously labeled examples", "author": ["E. H\u00fcllermeier", "J. Beringer"], "venue": "Intelligent Data Analysis, 10(5):419\u2013440.", "citeRegEx": "H\u00fcllermeier and Beringer,? 2006", "shortCiteRegEx": "H\u00fcllermeier and Beringer", "year": 2006}, {"title": "Statistics with Vague Data", "author": ["R. Kruse", "D. Meyer"], "venue": "D. Reidel, Dordrecht.", "citeRegEx": "Kruse and Meyer,? 1987", "shortCiteRegEx": "Kruse and Meyer", "year": 1987}, {"title": "Fuzzy random variables I: Definitions and theorems", "author": ["H. Kwakernaak"], "venue": "Information Sciences, 15:1\u201329.", "citeRegEx": "Kwakernaak,? 1978", "shortCiteRegEx": "Kwakernaak", "year": 1978}, {"title": "Fuzzy random variables II: Algorithms and examples for the discrete case", "author": ["H. Kwakernaak"], "venue": "Information Sciences, 17:253\u2013278.", "citeRegEx": "Kwakernaak,? 1979", "shortCiteRegEx": "Kwakernaak", "year": 1979}, {"title": "Robust linear and support vector regression", "author": ["O. Mangasarian", "D. Musicant"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(9).", "citeRegEx": "Mangasarian and Musicant,? 2000", "shortCiteRegEx": "Mangasarian and Musicant", "year": 2000}, {"title": "Fuzzy random variables", "author": ["M. Puri", "D. Ralescu"], "venue": "Journal of Mathematical Analysis and Applications, 114:409\u2013422.", "citeRegEx": "Puri and Ralescu,? 1986", "shortCiteRegEx": "Puri and Ralescu", "year": 1986}, {"title": "Margin maximizing loss functions", "author": ["S. Rosset", "J. Zhu", "T. Hastie"], "venue": "Proceedings NIPS-2003, Advances in Neural Information Processing.", "citeRegEx": "Rosset et al\\.,? 2003", "shortCiteRegEx": "Rosset et al\\.", "year": 2003}, {"title": "Advocating the use of imprecisely observed data in genetic fuzzy systems", "author": ["L. Sanchez", "I. Couso"], "venue": "IEEE Transactions on Fuzzy Systems, 15(4):551\u2013562. 28", "citeRegEx": "Sanchez and Couso,? 2007", "shortCiteRegEx": "Sanchez and Couso", "year": 2007}, {"title": "The strength of weak learnability", "author": ["R. Schapire"], "venue": "Machine Learning, 5(2):197\u2013 227.", "citeRegEx": "Schapire,? 1990", "shortCiteRegEx": "Schapire", "year": 1990}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A. Smola"], "venue": "MIT Press.", "citeRegEx": "Sch\u00f6lkopf and Smola,? 2001", "shortCiteRegEx": "Sch\u00f6lkopf and Smola", "year": 2001}, {"title": "Improving predictive inference under covariate shift by weighting the log-likelihood function", "author": ["H. Shimodaira"], "venue": "Journal of Statistical Planning and Inference, 90(2):227\u2013244.", "citeRegEx": "Shimodaira,? 2000", "shortCiteRegEx": "Shimodaira", "year": 2000}, {"title": "Possibilistic Data Analysis for Operations Research", "author": ["H. Tanaka", "P. Guo"], "venue": "Physika-Verlag, Heidelberg.", "citeRegEx": "Tanaka and Guo,? 1999", "shortCiteRegEx": "Tanaka and Guo", "year": 1999}, {"title": "Interval-valued regression and classification models in the framework of machine learning", "author": ["L. Utkin", "F. Coolen"], "venue": "Proc. ISIPTA 2011, 7th International Symposium on Imprecise Probability: Theories and Applications, Innsbruck, Austria.", "citeRegEx": "Utkin and Coolen,? 2011", "shortCiteRegEx": "Utkin and Coolen", "year": 2011}, {"title": "Statistical Learning Theory", "author": ["V. Vapnik"], "venue": "John Wiley & Sons.", "citeRegEx": "Vapnik,? 1998", "shortCiteRegEx": "Vapnik", "year": 1998}, {"title": "Statistical Methods for Fuzzy Data", "author": ["R. Viertl"], "venue": "Wiley.", "citeRegEx": "Viertl,? 2011", "shortCiteRegEx": "Viertl", "year": 2011}, {"title": "Towards fast and accurate algorithms for processing fuzzy data: Interval computations revisited", "author": ["G. Xianga", "V. Kreinovich"], "venue": "INternational Journal of General Systems.", "citeRegEx": "Xianga and Kreinovich,? 2013", "shortCiteRegEx": "Xianga and Kreinovich", "year": 2013}, {"title": "The concept of a linguistic variable and its application to approximate reasoning, parts 1-3", "author": ["L. Zadeh"], "venue": "Information Science, 8/9. 29", "citeRegEx": "Zadeh,? 1975", "shortCiteRegEx": "Zadeh", "year": 1975}], "referenceMentions": [{"referenceID": 17, "context": "The learning of models from imprecise data, such as interval data or, more generally, data modeled in terms of fuzzy subsets of an underlying reference space, has gained increasing interest in recent years (Sanchez and Couso, 2007; Denoeux, 2011; Denoeux, 2013; Cour et al., 2011; Viertl, 2011).", "startOffset": 206, "endOffset": 294}, {"referenceID": 2, "context": "The learning of models from imprecise data, such as interval data or, more generally, data modeled in terms of fuzzy subsets of an underlying reference space, has gained increasing interest in recent years (Sanchez and Couso, 2007; Denoeux, 2011; Denoeux, 2013; Cour et al., 2011; Viertl, 2011).", "startOffset": 206, "endOffset": 294}, {"referenceID": 24, "context": "The learning of models from imprecise data, such as interval data or, more generally, data modeled in terms of fuzzy subsets of an underlying reference space, has gained increasing interest in recent years (Sanchez and Couso, 2007; Denoeux, 2011; Denoeux, 2013; Cour et al., 2011; Viertl, 2011).", "startOffset": 206, "endOffset": 294}, {"referenceID": 4, "context": "Indeed, while problems such as fuzzy regression analysis (Diamond, 1988; Diamond and Tanaka, 1998; Tanaka and Guo, 1999; Changa and Ayyubb, 2001; Gonzalez-Rodriguez et al., 2009; Ferraro et al., 2010) have already been studied for a long time, the scope is currently broadening, both in terms of the problems tackled (e.", "startOffset": 57, "endOffset": 200}, {"referenceID": 21, "context": "Indeed, while problems such as fuzzy regression analysis (Diamond, 1988; Diamond and Tanaka, 1998; Tanaka and Guo, 1999; Changa and Ayyubb, 2001; Gonzalez-Rodriguez et al., 2009; Ferraro et al., 2010) have already been studied for a long time, the scope is currently broadening, both in terms of the problems tackled (e.", "startOffset": 57, "endOffset": 200}, {"referenceID": 8, "context": "Indeed, while problems such as fuzzy regression analysis (Diamond, 1988; Diamond and Tanaka, 1998; Tanaka and Guo, 1999; Changa and Ayyubb, 2001; Gonzalez-Rodriguez et al., 2009; Ferraro et al., 2010) have already been studied for a long time, the scope is currently broadening, both in terms of the problems tackled (e.", "startOffset": 57, "endOffset": 200}, {"referenceID": 7, "context": "Indeed, while problems such as fuzzy regression analysis (Diamond, 1988; Diamond and Tanaka, 1998; Tanaka and Guo, 1999; Changa and Ayyubb, 2001; Gonzalez-Rodriguez et al., 2009; Ferraro et al., 2010) have already been studied for a long time, the scope is currently broadening, both in terms of the problems tackled (e.", "startOffset": 57, "endOffset": 200}, {"referenceID": 5, "context": "In particular, an ontic interpretation of (fuzzy) set-valued data should be carefully distinguished from an epistemic one (Dubois, 2011).", "startOffset": 122, "endOffset": 136}, {"referenceID": 12, "context": "This difference is reflected, for example, in different approaches to fuzzy statistics, where fuzzy random variables can be formalized in an epistemic (Kwakernaak, 1978; Kwakernaak, 1979; Kruse and Meyer, 1987) as well as an ontic way (Puri and Ralescu, 1986); see (Couso and Dubois, 2009) for a comparison of these views in this context.", "startOffset": 151, "endOffset": 210}, {"referenceID": 13, "context": "This difference is reflected, for example, in different approaches to fuzzy statistics, where fuzzy random variables can be formalized in an epistemic (Kwakernaak, 1978; Kwakernaak, 1979; Kruse and Meyer, 1987) as well as an ontic way (Puri and Ralescu, 1986); see (Couso and Dubois, 2009) for a comparison of these views in this context.", "startOffset": 151, "endOffset": 210}, {"referenceID": 11, "context": "This difference is reflected, for example, in different approaches to fuzzy statistics, where fuzzy random variables can be formalized in an epistemic (Kwakernaak, 1978; Kwakernaak, 1979; Kruse and Meyer, 1987) as well as an ontic way (Puri and Ralescu, 1986); see (Couso and Dubois, 2009) for a comparison of these views in this context.", "startOffset": 151, "endOffset": 210}, {"referenceID": 15, "context": "This difference is reflected, for example, in different approaches to fuzzy statistics, where fuzzy random variables can be formalized in an epistemic (Kwakernaak, 1978; Kwakernaak, 1979; Kruse and Meyer, 1987) as well as an ontic way (Puri and Ralescu, 1986); see (Couso and Dubois, 2009) for a comparison of these views in this context.", "startOffset": 235, "endOffset": 259}, {"referenceID": 3, "context": "This difference is reflected, for example, in different approaches to fuzzy statistics, where fuzzy random variables can be formalized in an epistemic (Kwakernaak, 1978; Kwakernaak, 1979; Kruse and Meyer, 1987) as well as an ontic way (Puri and Ralescu, 1986); see (Couso and Dubois, 2009) for a comparison of these views in this context.", "startOffset": 265, "endOffset": 289}, {"referenceID": 0, "context": "The possibilistic interpretation of fuzzy sets in the epistemic case, that we focus on in this paper, naturally suggests a \u201cfuzzification\u201d of learning algorithms based on an application of the generic extension principle (Cerny and Rada, 2011; Xianga and Kreinovich, 2013).", "startOffset": 221, "endOffset": 272}, {"referenceID": 25, "context": "The possibilistic interpretation of fuzzy sets in the epistemic case, that we focus on in this paper, naturally suggests a \u201cfuzzification\u201d of learning algorithms based on an application of the generic extension principle (Cerny and Rada, 2011; Xianga and Kreinovich, 2013).", "startOffset": 221, "endOffset": 272}, {"referenceID": 26, "context": "Given a learning algorithm ALG for precise data, the most straightforward approach to handling a fuzzy sample (3) is to apply the well-known extension principle (Zadeh, 1975) to the mapping (2).", "startOffset": 161, "endOffset": 174}, {"referenceID": 10, "context": "Exploiting this insight in order to differentiate between more and less plausible instantiations is something that we refer to as data disambiguation (H\u00fcllermeier and Beringer, 2006).", "startOffset": 150, "endOffset": 182}, {"referenceID": 22, "context": "This approach is connected to the \u201cminimin\u201d strategy for model selection under imprecision as proposed in (Utkin and Coolen, 2011).", "startOffset": 106, "endOffset": 130}, {"referenceID": 6, "context": "In (Dubois and Prade, 2008), a mapping of that type is called a gradual element (in a fuzzy set).", "startOffset": 3, "endOffset": 27}, {"referenceID": 9, "context": "Interestingly, a fuzzification of the L1 loss based on a triangular fuzzy set Y with mid-point y and support (y \u2212 \u03b4, y + \u03b4) leads to a kind of Huber-loss (Huber, 1981): L ( Y, \u0177 ) = { 1 2 (y \u2212 \u0177)/\u03b4 if \u0177 \u2264 \u03b4 |y \u2212 \u0177| \u2212 1 2 \u03b4 if \u0177 > \u03b4 This loss behaves like the quadratic (L2) loss for small errors and like the L1 loss for larger deviations.", "startOffset": 154, "endOffset": 167}, {"referenceID": 14, "context": "As can be seen, our approach to learning from fuzzy data based on generalized loss functions includes methods such as M-estimation with Huber-loss as specific cases; methods for Huber M-estimation have been studied quite intensively in the literature (Mangasarian and Musicant, 2000).", "startOffset": 251, "endOffset": 283}, {"referenceID": 19, "context": "Another important loss function we can mimic is the -insensitive loss that plays an important role in support vector regression (Sch\u00f6lkopf and Smola, 2001):", "startOffset": 128, "endOffset": 155}, {"referenceID": 20, "context": "Again, learning from weighted examples (aka instance weighting) has been studied intensively in the literature (Shimodaira, 2000).", "startOffset": 111, "endOffset": 129}, {"referenceID": 16, "context": "An important class of loss functions in binary classification is the so-called margin losses (Rosset et al., 2003).", "startOffset": 93, "endOffset": 114}, {"referenceID": 23, "context": "Important examples of (23) include the hinge loss L(y, s) = f(ys) = max ( 1\u2212 ys, 0 ) (24) used in support vector machines (Vapnik, 1998; Sch\u00f6lkopf and Smola, 2001), the exponential loss L(y, s) = f(ys) = exp(\u2212ys) (25) used in boosting algorithms (Schapire, 1990), and the logistic loss", "startOffset": 122, "endOffset": 163}, {"referenceID": 19, "context": "Important examples of (23) include the hinge loss L(y, s) = f(ys) = max ( 1\u2212 ys, 0 ) (24) used in support vector machines (Vapnik, 1998; Sch\u00f6lkopf and Smola, 2001), the exponential loss L(y, s) = f(ys) = exp(\u2212ys) (25) used in boosting algorithms (Schapire, 1990), and the logistic loss", "startOffset": 122, "endOffset": 163}, {"referenceID": 18, "context": "Important examples of (23) include the hinge loss L(y, s) = f(ys) = max ( 1\u2212 ys, 0 ) (24) used in support vector machines (Vapnik, 1998; Sch\u00f6lkopf and Smola, 2001), the exponential loss L(y, s) = f(ys) = exp(\u2212ys) (25) used in boosting algorithms (Schapire, 1990), and the logistic loss", "startOffset": 246, "endOffset": 262}], "year": 2013, "abstractText": "Methods for analyzing or learning from \u201cfuzzy data\u201d have attracted increasing attention in recent years. In many cases, however, existing methods (for precise, non-fuzzy data) are extended to the fuzzy case in an ad-hoc manner, and without carefully considering the interpretation of a fuzzy set when being used for modeling data. Distinguishing between an ontic and an epistemic interpretation of fuzzy set-valued data, and focusing on the latter, we argue that a \u201cfuzzification\u201d of learning algorithms based on an application of the generic extension principle is not appropriate. In fact, the extension principle fails to properly exploit the inductive bias underlying statistical and machine learning methods, although this bias, at least in principle, offers a means for \u201cdisambiguating\u201d the fuzzy data. Alternatively, we therefore propose a method which is based on the generalization of loss functions in empirical risk minimization, and which performs model identification and data disambiguation simultaneously. Elaborating on the fuzzification of specific types of losses, we establish connections to well-known loss functions in regression and classification. We compare our approach with related methods and illustrate its use in logistic regression for binary classification.", "creator": "LaTeX with hyperref package"}}}