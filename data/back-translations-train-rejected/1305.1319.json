{"id": "1305.1319", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-May-2013", "title": "New Alignment Methods for Discriminative Book Summarization", "abstract": "We consider the unsupervised alignment of the full text of a book with a human-written summary. This presents challenges not seen in other text alignment problems, including a disparity in length and, consequent to this, a violation of the expectation that individual words and phrases should align, since large passages and chapters can be distilled into a single summary phrase. We present two new methods, based on hidden Markov models, specifically targeted to this problem, and demonstrate gains on an extractive book summarization task. While there is still much room for improvement, unsupervised alignment holds intrinsic value in offering insight into what features of a book are deemed worthy of summarization.", "histories": [["v1", "Mon, 6 May 2013 20:27:55 GMT  (210kb,D)", "http://arxiv.org/abs/1305.1319v1", "This paper reflects work in progress"]], "COMMENTS": "This paper reflects work in progress", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["david bamman", "noah a smith"], "accepted": false, "id": "1305.1319"}, "pdf": {"name": "1305.1319.pdf", "metadata": {"source": "CRF", "title": "New Alignment Methods for Discriminative Book Summarization Work in Progress", "authors": ["David Bamman", "Noah A. Smith"], "emails": ["dbamman@cs.cmu.edu", "nasmith@cs.cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "The purpose of the extractive summary is to select a subset of sentences from a source document to present as a summary. Supervised approaches to this problem include using training data in the form of source documents paired with existing summaries (Marcu, 1999; Jing and McKeown, 1999; Ceylan and Mihalcea, 2009). These methods learn what the characteristics of a source set are likely to yield in the summary; for news articles, for example, are strong predictions in a document (it used to be better), the sentence length (it is shorter), and the number of words in a sentence that is among the most that occur in the documentation."}, {"heading": "2 Related Work", "text": "This work builds on a long history of unattended word and phrase alignment originating in machine translation literature, both in terms of the task of learning alignment between parallel text (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003; DeNero et al., 2008) and between monolingual (Quirk et al., 2004) and comparable corpora (Barzilay and Elhadad, 2003). For the related task of aligning document and abstract, we refer to the work in the document summary (Marcu, 1999; Osborne, 2002; Daum\u00e9 and Marcu, 2005). A notable exception is Ceylan (2011), which uses both short stories (Kazantseva and Szpakowicz, 2010) and books (Mihalcea and Ceylan, 2007)."}, {"heading": "3 Methods", "text": "We present two methods, both of which involve the estimation of the parameters of a hidden Markov model (HMM). HMM differ in their definitions of states, observations and parameters of the emission distributions. We first present a generic HMM, which we then instantiate with each of our two models and alternately discuss their respective inference and learning algorithms. Let us assign probability to S the amount of hidden states andK = | S |. An observation sequence t = < t1,..., tn >, each t'V: p (t | n) = x-z-Sn \u03c0z1 (n-z, t'\u03b3z ', z' + 1) (1), where z is the sequence of hidden states, p-K is the distribution over the start states, and for all S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-S-T, the hidden sequence is the transition states, where K and transition states are the distributions."}, {"heading": "3.1 Passage Model", "text": "The intuition behind this approach is the following: while word and phrase alignment attempts to capture fine-grained correspondence between a source and a target document, longer documents distilled into comparatively short summaries will instead have long, topically coherent passages summarized in a single sentence. Thus, the following summary sentence is summarized in a Wikipedia plot synopsis several long episodes in the adventures of Tom Sawyer: After he has soiled his clothes in a battle from school, Tom will flush out the fence as punishment for the next day. Our goal is to find the sequence of passages in the source document that follow the sequence of the summary sentences."}, {"heading": "3.1.1 Inference", "text": "Given a source document b and a target summary t, our goal is to derive from it the most likely passage z 'for each sentence t'. This depends on the parameters (\u03c0, \u03b7 and \u03b3) and the passages associated with each state, so we also estimate them and try to maximize the probability. Our approach is an EM-like algorithm (Dempster et al., 1977); after initialization, it iterates in three steps: \u2022 E-step. Calculate p (t) and the rear distributions q (zk | t) for each sentence tk. This is done using the forward-backward algorithm. \u2022 M-step. Estimate the numbers from the rear, using the usual HMM step. \u2022 S-step. Samples of new passages for each state tk. The sample distributions for each state take into account that the movement of the non-overlapping passage and js is subject."}, {"heading": "3.1.2 Sampling chunk boundaries", "text": "While the S-step represents a marginal distribution in the source code, this step could lead to sub-optimal global results, so instead we arrive at sub-optimal global results. (...) While the S-step defines a marginal distribution in the source code of the source code in the source code, we expect most of the source code to be radically reduced to smaller ranges that are likely to match the target sentences. (...) Beyond the following iterations, this is a way to do so. (...) We could at any step shift the boundaries between the non-essential words between the old and new boundaries. (...) A greedy step - analogous to the M-step use to estimate the parameters - is one way to do so. (...) We could at any step shift the boundaries between the positions that define the non-essential words between the old and new boundaries. (...) A greedy step - analogous to the M-step use to estimate the parameters - is one way to do so. (...) We could at any step shift the boundaries between the positions that we can model the subjective boundaries that overwork the linguistic decisions... (however, the good ones overwork the local boundaries)."}, {"heading": "3.2 Token Model", "text": "Jing and McKeown (1999) introduced an HMM whose states correspond to tokens in the source text documentation. (The observation is the sequence of target summaries (which are limited to these types in the source text documentation). The emission probabilities are fixed to be one if the source and target words match, zero if they do not. Therefore, any instance of v-V in the target text documentation is more likely than transitions between sentences; the transition parameters are set manually to simulate a ranking of transition types (e.g. transitions within the same sentence are more likely than transitions between sentences). No parameter estimation is used to find the most likely orientation. The allowable transition space is limited by F 2, where F 2 is the frequency of the most common tokens in the source text document. The resulting model is scalable to large source documents (Ceycelan, this concept, 2011 and Ceylan, is 2009; Ceylan)."}, {"heading": "4 Experiments", "text": "To evaluate these two alignment methods and compare them with previous work, we evaluate the downstream task of the extractive book summary. 2http: / / www.gutenberg.org / ebooks / 10681"}, {"heading": "4.1 Data", "text": "The available data includes 14,120 book summaries extracted from the English Wikipedia3 dump and 31,393 English books extracted from Project Gutenberg on November 2, 2012. 4. We limit the book / abstract pairs to those where the full text of the book contains at least 10,000 words and the paired summary contains at least 100 words (with the exception of stopwords and punctuation), resulting in a data set of 439 book / abstract pairs where the average book length is 43,223 words and the average abstract is 369 words (again excluding stopwords and punctuation).The ratio between abstracts and complete books in this data set is about 1.2%, much smaller than in previous work for each domain, even for earlier work that includes literary novels: Ceylan (2009) uses a collection of 31 books paired with relatively long summaries from SparkNotes, CliffsNotes and Gradever, in which the average abstract length is 6,800 words."}, {"heading": "4.2 Discriminative summarization", "text": "All the experiments described below use 10-fold cross-validation, in which we divide the data into ten disjunctive sentences, train nine of them, and then test them on the remaining unattended partitions. In total, ten evaluations are performed, with the specified accuracy being the average across all ten sentences. First, all source words and summarized summaries in the training set are aligned using one of the three unattended methods described above (Passage HMM, Token HMM, Jing 1999). Further, all sentences are characterized on the source page of the book / summary pairs; all sentences aligned to a sentence in the summary are labeled 1 (summary) and 0 otherwise (not summary)."}, {"heading": "5 Evaluation", "text": "In fact, we calculate the RUGE score between the generated summaries and the obtained reference data for each book. We consider both RUGE-1, which measures the overlap of the unigrams, and RUGE-2, which overlap. In case a single reference translation is produced, the RUGE-N is calculated as follows: The RUGE-1, which measures the overlap of the unigrams, and the RUGE-2, which overlaps."}, {"heading": "6 Conclusion", "text": "In fact, it is a kind of country where people are able to flourish, and where they are able to flourish, where they are able to flourish, \"he said in an interview with the New York Times."}], "references": [{"title": "Sentence alignment for monolingual comparable corpora", "author": ["Barzilay", "Elhadad2003] Regina Barzilay", "Noemie Elhadad"], "venue": "In Proceedings of the 2003 conference on Empirical methods in natural language processing,", "citeRegEx": "Barzilay et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Barzilay et al\\.", "year": 2003}, {"title": "A statistical approach to machine translation", "author": ["Brown et al.1990] Peter F. Brown", "John Cocke", "Stephen A. Della Pietra", "Vincent J. Della Pietra", "Fredrick Jelinek", "John D. Lafferty", "Robert L. Mercer", "Paul S. Roossin"], "venue": "Comput. Linguist.,", "citeRegEx": "Brown et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1990}, {"title": "The decomposition of human-written book summaries", "author": ["Ceylan", "Mihalcea2009] Hakan Ceylan", "Rada Mihalcea"], "venue": "In CICLing\u201909,", "citeRegEx": "Ceylan et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ceylan et al\\.", "year": 2009}, {"title": "Investigating the Extractive Summarization of Literary Novels", "author": ["Hakan Ceylan"], "venue": "Ph.D. thesis,", "citeRegEx": "Ceylan.,? \\Q2011\\E", "shortCiteRegEx": "Ceylan.", "year": 2011}, {"title": "Induction of word and phrase alignments for automatic document summarization", "author": ["Daum\u00e9", "III Marcu2005] Hal Daum\u00e9", "Daniel Marcu"], "venue": "Comput. Linguist.,", "citeRegEx": "Daum\u00e9 et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Daum\u00e9 et al\\.", "year": 2005}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["M.N. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "Dempster et al\\.,? \\Q1977\\E", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Sampling alignment structure under a bayesian translation model", "author": ["DeNero et al.2008] John DeNero", "Alexandre BouchardC\u00f4t\u00e9", "Dan Klein"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "DeNero et al\\.,? \\Q2008\\E", "shortCiteRegEx": "DeNero et al\\.", "year": 2008}, {"title": "The decomposition of humanwritten summary sentences", "author": ["Jing", "McKeown1999] Hongyan Jing", "Kathleen R. McKeown"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Jing et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Jing et al\\.", "year": 1999}, {"title": "Summarizing short stories", "author": ["Kazantseva", "Szpakowicz2010] Anna Kazantseva", "Stan Szpakowicz"], "venue": "Computational Linguistics,", "citeRegEx": "Kazantseva et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kazantseva et al\\.", "year": 2010}, {"title": "Automatic evaluation of summaries using ngram co-occurrence statistics", "author": ["Lin", "Hovy2003] Chin-Yew Lin", "Eduard Hovy"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human", "citeRegEx": "Lin et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2003}, {"title": "The automatic construction of large-scale corpora for summarization research", "author": ["Daniel Marcu"], "venue": "In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "Marcu.,? \\Q1999\\E", "shortCiteRegEx": "Marcu.", "year": 1999}, {"title": "Explorations in automatic book summarization", "author": ["Mihalcea", "Ceylan2007] Rada Mihalcea", "Hakan Ceylan"], "venue": "In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learn-", "citeRegEx": "Mihalcea et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2007}, {"title": "A systematic comparison of various statistical alignment models", "author": ["Och", "Ney2003] Franz Josef Och", "Hermann Ney"], "venue": "Comput. Linguist.,", "citeRegEx": "Och et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Och et al\\.", "year": 2003}, {"title": "Using maximum entropy for sentence extraction", "author": ["Miles Osborne"], "venue": "In Proceedings of the ACL-02 Workshop on Automatic Summarization - Volume", "citeRegEx": "Osborne.,? \\Q2002\\E", "shortCiteRegEx": "Osborne.", "year": 2002}, {"title": "Monolingual machine translation for paraphrase generation", "author": ["Quirk et al.2004] Chris Quirk", "Chris Brockett", "William Dolan"], "venue": "Proceedings of EMNLP", "citeRegEx": "Quirk et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Quirk et al\\.", "year": 2004}, {"title": "Document summarization using conditional random fields", "author": ["Shen et al.2007] Dou Shen", "Jian-Tao Sun", "Hua Li", "Qiang Yang", "Zheng Chen"], "venue": "In Proceedings of the 20th international joint conference on Artifical intelligence,", "citeRegEx": "Shen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2007}, {"title": "Hmm-based word alignment in statistical translation", "author": ["Vogel et al.1996] Stephan Vogel", "Hermann Ney", "Christoph Tillmann"], "venue": "In Proceedings of the 16th conference on Computational linguistics - Volume", "citeRegEx": "Vogel et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Vogel et al\\.", "year": 1996}, {"title": "A Monte Carlo Implementation of the EM Algorithm and the Poor Man\u2019s Data Augmentation Algorithms", "author": ["Wei", "Tanner1990] Greg C.G. Wei", "Martin A. Tanner"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Wei et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Wei et al\\.", "year": 1990}, {"title": "Text summarization using a trainable summarizer and latent semantic analysis", "author": ["Yeh et al.2005] Jen-Yuan Yeh", "Hao-Ren Ke", "Wei-Pang Yang", "I-Heng Meng"], "venue": "Inf. Process. Manage.,", "citeRegEx": "Yeh et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Yeh et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 10, "context": "Supervised approaches to this problem make use of training data in the form of source documents paired with existing summaries (Marcu, 1999; Osborne, 2002; Jing and McKeown, 1999; Ceylan and Mihalcea, 2009).", "startOffset": 127, "endOffset": 206}, {"referenceID": 13, "context": "Supervised approaches to this problem make use of training data in the form of source documents paired with existing summaries (Marcu, 1999; Osborne, 2002; Jing and McKeown, 1999; Ceylan and Mihalcea, 2009).", "startOffset": 127, "endOffset": 206}, {"referenceID": 1, "context": "For short texts and training pairs where a one-to-one alignment between source and abstract sentences can be expected, standard techniques from machine translation can be applied, including word-level alignment (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003) and longer phrasal alignment (Daum\u00e9 and Marcu, 2005), especially as adapted to the monolingual setting (Quirk et al.", "startOffset": 211, "endOffset": 270}, {"referenceID": 16, "context": "For short texts and training pairs where a one-to-one alignment between source and abstract sentences can be expected, standard techniques from machine translation can be applied, including word-level alignment (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003) and longer phrasal alignment (Daum\u00e9 and Marcu, 2005), especially as adapted to the monolingual setting (Quirk et al.", "startOffset": 211, "endOffset": 270}, {"referenceID": 14, "context": ", 1996; Och and Ney, 2003) and longer phrasal alignment (Daum\u00e9 and Marcu, 2005), especially as adapted to the monolingual setting (Quirk et al., 2004).", "startOffset": 130, "endOffset": 150}, {"referenceID": 10, "context": "While the ratio between abstracts and source documents in the benchmark Ziff-Davis corpus of newswire (Marcu, 1999) is approximately 12% (133 words vs.", "startOffset": 102, "endOffset": 115}, {"referenceID": 1, "context": "This work builds on a long history of unsupervised word and phrase alignment originating in the machine translation literature, both for the task of learning alignments across parallel text (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003; DeNero et al., 2008) and between monolingual (Quirk et al.", "startOffset": 190, "endOffset": 270}, {"referenceID": 16, "context": "This work builds on a long history of unsupervised word and phrase alignment originating in the machine translation literature, both for the task of learning alignments across parallel text (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003; DeNero et al., 2008) and between monolingual (Quirk et al.", "startOffset": 190, "endOffset": 270}, {"referenceID": 6, "context": "This work builds on a long history of unsupervised word and phrase alignment originating in the machine translation literature, both for the task of learning alignments across parallel text (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003; DeNero et al., 2008) and between monolingual (Quirk et al.", "startOffset": 190, "endOffset": 270}, {"referenceID": 14, "context": ", 2008) and between monolingual (Quirk et al., 2004) and comparable corpora (Barzilay and Elhadad, 2003).", "startOffset": 32, "endOffset": 52}, {"referenceID": 10, "context": "For the related task of document/abstract alignment, we draw on work in document summarization (Marcu, 1999; Osborne, 2002; Daum\u00e9 and Marcu, 2005).", "startOffset": 95, "endOffset": 146}, {"referenceID": 13, "context": "For the related task of document/abstract alignment, we draw on work in document summarization (Marcu, 1999; Osborne, 2002; Daum\u00e9 and Marcu, 2005).", "startOffset": 95, "endOffset": 146}, {"referenceID": 1, "context": "This work builds on a long history of unsupervised word and phrase alignment originating in the machine translation literature, both for the task of learning alignments across parallel text (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003; DeNero et al., 2008) and between monolingual (Quirk et al., 2004) and comparable corpora (Barzilay and Elhadad, 2003). For the related task of document/abstract alignment, we draw on work in document summarization (Marcu, 1999; Osborne, 2002; Daum\u00e9 and Marcu, 2005). Past approaches to fictional summarization, including both short stories (Kazantseva and Szpakowicz, 2010) and books (Mihalcea and Ceylan, 2007), have tended toward nondiscriminative methods; one notable exception is Ceylan (2011), which applies the Viterbi alignment method of Jing and McKeown (1999) to a set of 31 literary novels.", "startOffset": 191, "endOffset": 748}, {"referenceID": 1, "context": "This work builds on a long history of unsupervised word and phrase alignment originating in the machine translation literature, both for the task of learning alignments across parallel text (Brown et al., 1990; Vogel et al., 1996; Och and Ney, 2003; DeNero et al., 2008) and between monolingual (Quirk et al., 2004) and comparable corpora (Barzilay and Elhadad, 2003). For the related task of document/abstract alignment, we draw on work in document summarization (Marcu, 1999; Osborne, 2002; Daum\u00e9 and Marcu, 2005). Past approaches to fictional summarization, including both short stories (Kazantseva and Szpakowicz, 2010) and books (Mihalcea and Ceylan, 2007), have tended toward nondiscriminative methods; one notable exception is Ceylan (2011), which applies the Viterbi alignment method of Jing and McKeown (1999) to a set of 31 literary novels.", "startOffset": 191, "endOffset": 819}, {"referenceID": 16, "context": "The transition distribution from state s \u2208 S, \u03b3s is operationalized following the HMM word alignment formulation of Vogel et al. (1996). The transition events between ordered pairs of states are binned by the difference in two passages\u2019 ranks within the source document.", "startOffset": 116, "endOffset": 136}, {"referenceID": 5, "context": "Our approach is an EM-like algorithm (Dempster et al., 1977); after initialization, it iterates among three steps:", "startOffset": 37, "endOffset": 60}, {"referenceID": 6, "context": "The sampling distribution considers, for each state s, moving is subject to the no-overlapping constraint and js, and then moving js subject to the no-overlapping constraint and is (DeNero et al., 2008).", "startOffset": 181, "endOffset": 202}, {"referenceID": 3, "context": "The resulting model is scalable to large source documents (Ceylan and Mihalcea, 2009; Ceylan, 2011).", "startOffset": 58, "endOffset": 99}, {"referenceID": 3, "context": "The resulting model is scalable to large source documents (Ceylan and Mihalcea, 2009; Ceylan, 2011). One potential issue with this model is that it lacks the concept of a null source, not articulated in the original HMM alignment model of Vogel et al. (1996) but added by Och and Ney (2003).", "startOffset": 59, "endOffset": 259}, {"referenceID": 3, "context": "The resulting model is scalable to large source documents (Ceylan and Mihalcea, 2009; Ceylan, 2011). One potential issue with this model is that it lacks the concept of a null source, not articulated in the original HMM alignment model of Vogel et al. (1996) but added by Och and Ney (2003). Without such a null source, every word in the summary must be generated by some word in the source document.", "startOffset": 59, "endOffset": 291}, {"referenceID": 3, "context": "The resulting model is scalable to large source documents (Ceylan and Mihalcea, 2009; Ceylan, 2011). One potential issue with this model is that it lacks the concept of a null source, not articulated in the original HMM alignment model of Vogel et al. (1996) but added by Och and Ney (2003). Without such a null source, every word in the summary must be generated by some word in the source document. The consequence of this decision is that a Viterbi alignment over the summary must pick a perhaps distant, low-probability word in the source document if no closer word is available. Additionally, while the choice to enforce lexical identity constrains the state space, it also limits the range of lexical variation captured. Our second model extends Jing\u2019s approach in three ways. First, we introduce parameter inference to learn the values of start probabilities and transitions that maximize the likelihood of the data, using the EM algorithm. We operationalize the transition probabilities again following Vogel et al. (1996), but constrain the state space by only measuring transititions between fixed bucket lengths, rather than between the absolute position of each source word.", "startOffset": 59, "endOffset": 1031}, {"referenceID": 3, "context": "2%, much smaller than that used in previous work for any domain, even for past work involving literary novels: Ceylan (2009) makes use of a collection of 31 books paired with relatively long summaries from SparkNotes, CliffsNotes and GradeSaver, where the average summary length is 6,800 words.", "startOffset": 111, "endOffset": 125}, {"referenceID": 18, "context": "Following previous work, we devise sentencelevel features that can be readily computed in comparison both with the document in which the sentence in found, and in comparison with the collection of documents as whole (Yeh et al., 2005; Shen et al., 2007).", "startOffset": 216, "endOffset": 253}, {"referenceID": 15, "context": "Following previous work, we devise sentencelevel features that can be readily computed in comparison both with the document in which the sentence in found, and in comparison with the collection of documents as whole (Yeh et al., 2005; Shen et al., 2007).", "startOffset": 216, "endOffset": 253}], "year": 2013, "abstractText": "We consider the unsupervised alignment of the full text of a book with a human-written summary. This presents challenges not seen in other text alignment problems, including a disparity in length and, consequent to this, a violation of the expectation that individual words and phrases should align, since large passages and chapters can be distilled into a single summary phrase. We present two new methods, based on hidden Markov models, specifically targeted to this problem, and demonstrate gains on an extractive book summarization task. While there is still much room for improvement, unsupervised alignment holds intrinsic value in offering insight into what features of a book are deemed worthy of summarization.", "creator": "LaTeX with hyperref package"}}}