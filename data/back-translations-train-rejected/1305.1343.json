{"id": "1305.1343", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-May-2013", "title": "Towards an Author-Topic-Term-Model Visualization of 100 Years of German Sociological Society Proceedings", "abstract": "Author co-citation studies employ factor analysis to reduce high-dimensional co-citation matrices to low-dimensional and possibly interpretable factors, but these studies do not use any information from the text bodies of publications. We hypothesise that term frequencies may yield useful information for scientometric analysis. In our work we ask if word features in combination with Bayesian analysis allow well-founded science mapping studies. This work goes back to the roots of Mosteller and Wallace's (1964) statistical text analysis using word frequency features and a Bayesian inference approach, tough with different goals. To answer our research question we (i) introduce a new data set on which the experiments are carried out, (ii) describe the Bayesian model employed for inference and (iii) present first results of the analysis.", "histories": [["v1", "Mon, 6 May 2013 22:24:20 GMT  (810kb,D)", "http://arxiv.org/abs/1305.1343v1", "Accepted: 14th International Society of Scientometrics and Informetrics Conference, Vienna Austria 15-19th July 2013"]], "COMMENTS": "Accepted: 14th International Society of Scientometrics and Informetrics Conference, Vienna Austria 15-19th July 2013", "reviews": [], "SUBJECTS": "cs.DL cs.CL cs.IR", "authors": ["arnim bleier", "andreas strotmann"], "accepted": false, "id": "1305.1343"}, "pdf": {"name": "1305.1343.pdf", "metadata": {"source": "CRF", "title": "Towards an Author-Topic-Term-Model Visualization of 100 Years of German Sociological Society Proceedings", "authors": ["Arnim Bleier", "Andreas Strotmann"], "emails": ["andreas.strotmann}@gesis.org"], "sections": [{"heading": null, "text": "On the Way to an Author-Theme Model for the Visualization of 100 Years of Procedures of the German Sociological Society Arnim Bleier and Andreas Strotmann {arnim.bleier, andreas.strotmann} @ gesis.org GESIS - Leibniz Institute for Social Sciences, Unter Sachsenhausen 6-8, Cologne"}, {"heading": "Introduction", "text": "The authors of the cocidation studies (Zhao & Strotmann, 2008) use factor analysis to reduce high-dimensional cocidation matrices to low-dimensional and possibly interpretable factors, but these studies do not use information from the text bodies of publications. We believe that term frequencies can provide useful information for scientometric analysis. In our paper, we ask whether word characteristics combined with Bayesian analysis provide sound scientific mapping studies, going back to the roots of statistical text analysis by Mosteller and Wallace (1964) using word frequency characteristics and a Bayesian inference approach that tackles different objectives. To answer our research question, we present (i) the dataset on which the experiments are conducted, (ii) describe the Bayesian model used for inferences, and (iii) present initial results of the analysis."}, {"heading": "The DGS Dataset", "text": "The collection of Documents D we use in the experiment covers approximately 100 years of sessions (from 1910 to 2006) of the German Sociological Society (DGS), totaling 5,010 documents. Early procedures were scanned and OCRed used, others in original digital form. The metadata of the documents included 3,661 unique full names of authors J. From each document, the words of the 21st to 320th vocabulary were extracted. After we had standardized stopwords, short and / or rare words (< 4 letters; > 10 occurrences; mostly OCR fragments) and words in more than half of the documents, 1,067,128 occurrences were removed from a vocabulary V containing 12,665 unique words."}, {"heading": "Statistical Model", "text": "We review the statistical model that we use to relate authors and documents across a flexible number of topics. According to a common notation (RosenZvi, 2004), a document d is modeled as a vector of! words,!, with the wording. \"Each document d is associated with a number of authors. Our model assumes that documents are generated in the following steps: 1. Draw a common discrete probability distribution from a Dirichlet process (DP) (Teh et al. 2006) using a basic measurement H and previous concentration variables as a global mix of topics"}, {"heading": "Posterior Analysis and Visualization", "text": "The generative model is structured as a directed acyclic graph, starting with the causes and ending with the observed words in the documents. Bayes \"rule reverses causality, and the parameters of interest can be estimated based on the observed data and priorities. We use an MCMC sampler for this posterior analysis. After the sampler has been performed in 2,000 steps with Priors =.5, =.5 and =.2, the model condensed to 89 components. Examples of! and! are in Figure 1 and Table 1. The reader is referred to Rosen-Zvi et al. (2004), Teh et al. (2006) and Bleier (2012) for an in-depth discussion of this method. Due to space constraints, we restrict the visualization in Figure 1 (using Pajek) to four of 89 components. Authors and topics are presented as square and circular nodes, respectively as constants. The size of the topic nodes is proportional to their respective use and probability of differentiality, whereby the severity of each arc is high."}, {"heading": "Discussion", "text": "Our scientific mapping approach uses a flexible latent dirichlet mapping to (i) identify an optimal number and topic category for a given set of documents based on the words it contains, (ii) to identify the most relevant words describing each topic, and (iii) to identify weighted links between authors and the topics of their writings.The statistical model takes into account that documents were written by multiple authors, that authors write to varying degrees on different topics, and that words relate to different topics to varying degrees.Figure 1 shows a small fragment of a map of German sociology based on approximately 100 years of DGS methodology, inspired by the visualization of the results of the quote-based factor analysis in Zhao & Strotmann (2008), but generated fully automatically from the results of applying these statistical analyses to full texts. While a full evaluation is pending, these results show some promising opportunities for application of this sciometric methodology."}], "references": [{"title": "A simple non-parametric Topic Mixture for Authors and Documents", "author": ["A. Bleier"], "venue": "Pre-print arXiv:1211.6248 [cs.LG].", "citeRegEx": "Bleier,? 2012", "shortCiteRegEx": "Bleier", "year": 2012}, {"title": "Inference and Disputed Authorship: The Federalists. AddisonWesley", "author": ["F. Mosteller", "D. Wallace"], "venue": null, "citeRegEx": "Mosteller and Wallace,? \\Q1964\\E", "shortCiteRegEx": "Mosteller and Wallace", "year": 1964}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "Proceedings of the 20 Conference on Uncertainty in artificial intelligence (UAI", "citeRegEx": "Rosen.Zvi et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2004}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Teh et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Teh et al\\.", "year": 2006}, {"title": "Information Science during the first decade of the Web: An enriched author co-citation analysis", "author": ["D. Zhao", "A. Strotmann"], "venue": "Journal of the American Society for Information Science and Technology,", "citeRegEx": "Zhao and Strotmann,? \\Q2008\\E", "shortCiteRegEx": "Zhao and Strotmann", "year": 2008}], "referenceMentions": [{"referenceID": 1, "context": "This work goes back to the roots of Mosteller and Wallace\u2019s (1964) statistical text analysis using word frequency features and a Bayesian inference approach, tough with different goals.", "startOffset": 36, "endOffset": 67}, {"referenceID": 3, "context": "Draw a shared discrete probability distribution from a Dirichlet Process (DP) (Teh et al. 2006) with base measure H and prior concentration parameter", "startOffset": 78, "endOffset": 95}, {"referenceID": 1, "context": "The reader is referred to Rosen-Zvi et al. (2004), Teh et al.", "startOffset": 26, "endOffset": 50}, {"referenceID": 1, "context": "The reader is referred to Rosen-Zvi et al. (2004), Teh et al. (2006) and Bleier (2012) for an in-depth discussion of this method.", "startOffset": 26, "endOffset": 69}, {"referenceID": 0, "context": "(2006) and Bleier (2012) for an in-depth discussion of this method.", "startOffset": 11, "endOffset": 25}], "year": 2013, "abstractText": "Introduction Author co-citation studies (Zhao & Strotmann, 2008) employ factor analysis to reduce highdimensional co-citation matrices to low dimensional and possibly interpretable factors, but these studies do not use any information from the text bodies of publications. We hypothesise that term frequencies may yield useful information for scientometric analysis. In our work we ask if word features in combination with Bayesian analysis allows for well-founded science mapping studies. This work goes back to the roots of Mosteller and Wallace\u2019s (1964) statistical text analysis using word frequency features and a Bayesian inference approach, tough with different goals. To answer our research question we (i) introduce the data set on which the experiments are carried out, (ii) describe the Bayesian model employed for inference and (iii) present first results of the analysis.", "creator": "LaTeX with hyperref package"}}}