{"id": "1305.1925", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-May-2013", "title": "Speech: A Challenge to Digital Signal Processing Technology for Human-to-Computer Interaction", "abstract": "This software project based paper is for a vision of the near future in which computer interaction is characterized by natural face-to-face conversations with lifelike characters that speak, emote, and gesture. The first step is speech. The dream of a true virtual reality, a complete human-computer interaction system will not come true unless we try to give some perception to machine and make it perceive the outside world as humans communicate with each other. This software project is under development for listening and replying machine (Computer) through speech. The Speech interface is developed to convert speech input into some parametric form (Speech-to-Text) for further processing and the results, text output to speech synthesis (Text-to-Speech)", "histories": [["v1", "Wed, 8 May 2013 05:55:50 GMT  (90kb)", "http://arxiv.org/abs/1305.1925v1", "Pages: 06 Figures : 06. arXiv admin note: text overlap witharXiv:1305.1429,arXiv:1305.1428"]], "COMMENTS": "Pages: 06 Figures : 06. arXiv admin note: text overlap witharXiv:1305.1429,arXiv:1305.1428", "reviews": [], "SUBJECTS": "cs.HC cs.CL", "authors": ["urmila shrawankar", "anjali mahajan"], "accepted": false, "id": "1305.1925"}, "pdf": {"name": "1305.1925.pdf", "metadata": {"source": "META", "title": "Speech_Urmila", "authors": ["Urmila Shrawankar", "Anjali Mahajan"], "emails": ["urmilas@rediffmail.com", "armahajan@rediffmail.com"], "sections": [{"heading": null, "text": "ProcessorSpeech InputSpeech ToTextDigital SignalProcessingText ToSpeech Synthesis Voice OutputI. INTRODUCTIONWhen we think of user interfaces, the very first question that comes to mind is: Why do we need an interface to interact with a machine (computer)? The answer is simple, human-computer interaction is not simply human-to-human interaction. Human-to-human interaction is mainly based on speech, emotion and gesture, where as a human-machine interaction is based on either text user interface (TUI) or graphical user interface (GUI). If we provide artificial intelligence to train a machine to interact with speech signals, this paper focuses on developing a software-based user interface to accept speech input by microphone and gives speech output by speakers connected to the computer. My attempt is to develop an independent and text-dependent model, i.e., to understand the completion of a set of language systems suitable for a specific group of people, i.e., to complete a set of trainings suitable for a variety of speech systems)."}, {"heading": "II. ROLE OF DIGITAL SIGNAL PROCESSING", "text": "IN SPEECH RECOGNITION Signal processing is the process of efficiently and robustly extracting relevant information from the voice signal. A speech recognition system encompasses a collection of algorithms from a variety of disciplines, including statistical pattern recognition, communication theory, signal processing, combination mathematics, and linguistics. Although each of these areas relies to varying degrees on different recognition systems, perhaps the greatest common denominator of all recognition systems is the front-end of signal processing, which transforms the speech waveform into a kind of parametric representation for further analysis and processing."}, {"heading": "A. Speech Signal Processing", "text": "Speech recognition can be defined as the process of converting an acoustic signal captured by a microphone or phone into a set of words. After Text-to-Speech (TTS) and Interactive Speech Response Systems (IVR), automatic speech recognition (ASR) is one of the fastest-developing fields in linguistics and engineering. As a new generation of computer technology, it is emerging as the next great innovation in human interaction. Speech recognition systems can recognize thousands of words. ASR's evolution has many applications in many aspects of our daily lives, such as telephone applications, applications for the physically handicapped and illiterate, and many others in the field of computer science. Speech recognition is considered both input and output during Human Computer Interaction (HCI) design. HCI includes the implementation and evaluation of interactive systems related to the user's task and work."}, {"heading": "B. Speech Recognition Systems", "text": "Speech recognition is a technology that enables machines to be controlled by voice in the form of isolated or connected word strings. It involves the recognition and understanding of speech by machine. Speech recognition is based on pattern recognition technology. The aim is to take an input pattern, the speech signal, and classify it as a sequence of precisely defined stored patterns. These stored patterns can consist of units we call phonemes. If speech patterns were invariant and unchanged, there would be no problem; simply compare sequences of features with the stored patterns and find exact matches when they occur. However, the basic difficulty of speech recognition is that the speech signal is highly variable due to different speakers, different speech rates, different contents and different acoustic conditions. The task is to determine which of the deviations in the language are relevant for speech recognition and which are not relevant."}, {"heading": "III. FEATURE EXTRACTIONS AND FEATURE", "text": "MATCHINGFeature extraction is the process that extracts a small amount of data from the voice, which can later be used to represent each word. Feature matching involves the actual process of identifying the new word by comparing extracted features from his / her speech input with those from a range of known words. All speech recognition systems must pass two phases: the first is the enrollment or training phase, while the other is the testing phase."}, {"heading": "A. Speech Feature Extraction", "text": "The purpose of this module is to convert the speech waveform into a kind of parametric representation for further analysis and processing, often referred to as a signal processing frontend. There is a wide range of options for parametric representation of the speech signal and the speech recognition task, such as Mel-Frequency Cepstrum Coefficient (MFCC), Linear Prediction Coding (LPC), Spectrum Analysis Model Filter Base, Vector Quantization and others. The LPC model will be implemented in this project."}, {"heading": "B. Linear Predictive Coding (LPC) Model", "text": "Linear Predictive Coding (LPC) is one of the most powerful speech analysis techniques and a useful method for encoding high-quality speech at a low bit rate. It provides accurate estimates of speech parameters and is efficient for calculations. The LPC system is used to determine the formants from the speech signal. The basic solution is a differential equation that expresses each sample of the signal as a linear combination of previous samples. Such an equation is called a linear predictor, which is why it is called linear predictive coding.The basic idea behind the LPC model is that a given voice sample can be approximated at a certain time n, s (n), as a linear combination of the past p speech samples. After completion of the steps, as shown in Fig. LPC, we obtain parameters from the speech signal, and then use them for training purposes.s (n) Preemphasis FrameBlocking Windowing Autocorrelation Cerivative AnalysWeightW (LPC) m m (LPC) Parameter."}, {"heading": "IV. TRAINING and RECOGNITION", "text": "During the training phase, a data file is created and the samples recorded by different users are stored. These samples are further matched and used to recognize the word."}, {"heading": "A. Training with Artificial Neural Networks", "text": "Neural networks are often used as a powerful differentiator for automatic speech recognition tasks. They have several advantages over parametric classifiers, but there are drawbacks in terms of the amount of training data required and the duration of training time. Some architectures of neural networks are: Feedforward Perceptrons Trained With BackPropagationRadial Basis Function (RBF) NetworksLearning Vector Quantization (LVQ) Networks"}, {"heading": "B. Training with Hidden Markov Model", "text": "In the context of statistical methods of speech recognition, Hidden Markov Models (HMM) have become a well-known and widely used statistical approach to characterizing the spectral properties of speech frames. Hidden Markov Model is a doubly stochastic process in which the observed data is considered to be the result of having gone through the true (hidden) process through a function that (observes) the second process, the hidden process consisting of a collection of states (which are abstractly identical to states of the language production process) connected by transitions. (Refer fig HMM) Each transition is described by two sets of probabilities: \u2022 A transition forecast that provides the probability of making a transition from one state to another. \u2022 An output probability density function that defines the conditional probability of observing a set of language characteristics when a particular transition occurs."}, {"heading": "V. SPEECH SYNTHESIS (TEXT-TO-SPEECH)", "text": "Speech synthesis is the reverse recognition process. Advances in this area are improving the ease of use of the computer for visually impaired people in text-to-phoneme conversion: once the synthesis processor has determined the amount of words to be spoken, it must derive pronunciations for each word. Word pronunciations can be comfortably described as phoneme sequences, which are units of sound in a language used to distinguish one word from another."}, {"heading": "VI. THE SOFTWARE", "text": "The software VOICE ENGINE was developed for the training and testing of Speech Interface. As an example application, the machine is trained on numbers (0-9) and some mathematical operators. Input numbers and operators are provided to the machine via microphone in the form of WAV files. Properties are extracted from these speech signals and sent to parametric forms for further processing. Extracted parameters are sent to the training unit. In total, 25 samples are collected for the training of 5 different users of the age group (men and women), five attempts by each user for each word. The generated results are output via loudspeakers."}, {"heading": "VI. ADVERSE CONDITIONS IN SPEECH RECOGNITION", "text": "During the development of this project, it was observed that some unfavourable conditions impair the performance of the speech recognition system."}, {"heading": "A. Noise", "text": "If we use the noise-free environment for training and testing, we get an accuracy of about 80%. But if the room is noisy either during the training phase or during the testing phase, the accuracy is reduced to about 60%."}, {"heading": "B. Distortion", "text": "In order to realize this project, we do not need any special hardware except the computer machine, a microphone, speakers or headphones with microphone. If these attachments are not installed and configured correctly, we receive distorted input signals, which reduces the accuracy."}, {"heading": "C. (Human) Articulation Effects", "text": "Many factors affect the way each individual speaks, such as the distance of the microphone from the user and its position, even the language that is added with psychological effect as it provides input, these factors affect accuracy."}, {"heading": "VIII. CONCLUSIONS:", "text": "A technology without a social aspect is useless. Nowadays, the computer has become a part of everyday work. After the acquisition of the graphical user interface (GUI) it is very easy to interact with the computer, but it is still difficult for physically handicapped people, especially for Amelia (absence of disembodied people) and blind or elderly people. Language is an interface for human-computer interaction, so that they can interact with the computer without a keyboard or mouse. This project software was developed for the partial fulfillment of the M.Tech. (Computer Science and Engineering) degree. The software is developed with VoiceBox and H2M toolbox from MatLab 7.1 and is based on the approach of artificial intelligence speech recognition. Apart from microphone and speakers no other computer hardware is required. This software gives 80% accuracy in a clean environment and about 60% in a noisy environment."}], "references": [{"title": "Fundamentals of Speech Recognition", "author": ["Lawrence Rabiner", "Biing-Hwang Juang"], "venue": "Pearson Education,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "H.L.Hansen, \"A new perspective on Feature Extraction for Robust Invehicle Speech Recognition", "author": ["Umit H. Yapanel", "John"], "venue": "Proceedings of Eurospeech'03,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "A Concept Graph based Confidence Measure", "author": ["Kadri Hacioglu", "Wayne Ward"], "venue": "Proc. IEEE International Conference on Acoustic,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Sonic: The University of Colorado Continuous Speech Recognizer", "author": ["B. Pellom"], "venue": "Technical Report TR-CSLR-2001-01, CSLR, University of Colorado,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Maximum Likelihood Linear Transformations for HMM-Based Speech Recognition", "author": ["M.J.F. Gales"], "venue": "Technical Report CUED/F- INFENG/TR 291, Cambridge University, May 1997", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models", "author": ["C.J. Legetter", "P.C. Woodland"], "venue": "Computer Speech & Language, Vol. 9, pp. 171-185, 1995.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition", "author": ["Lawrence Rabiner"], "venue": "Roceedings of The IEEE,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1989}], "referenceMentions": [], "year": 2006, "abstractText": "This software project based paper is for a vision of the near future in which computer interaction is characterised by natural face-to-face conversations with lifelike characters that speak, emote, and gesture. The first step is speech. The dream of a true virtual reality, a complete human-computer interaction system will not come true unless we try to give some perception to machine and make it perceive the outside world as humans communicate with each other. This software project is under development for \u201clistening and replying machine (Computer) through speech\u201d. The Speech interface is developed to convert speech input into some parametric form (Speech-to-Text) for further processing and the results, text output to speech synthesis (Text-to-Speech)", "creator": "deskPDF 2.5"}}}