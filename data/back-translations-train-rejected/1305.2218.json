{"id": "1305.2218", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2013", "title": "Stochastic gradient descent algorithms for strongly convex functions at O(1/T) convergence rates", "abstract": "With a weighting scheme proportional to t, a traditional stochastic gradient descent (SGD) algorithm achieves a high probability convergence rate of O({\\kappa}/T) for strongly convex functions, instead of O({\\kappa} ln(T)/T). We also prove that an accelerated SGD algorithm also achieves a rate of O({\\kappa}/T).", "histories": [["v1", "Thu, 9 May 2013 21:31:47 GMT  (17kb,D)", "http://arxiv.org/abs/1305.2218v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["shenghuo zhu"], "accepted": false, "id": "1305.2218"}, "pdf": {"name": "1305.2218.pdf", "metadata": {"source": "CRF", "title": "Stochastic gradient descent algorithms for strongly convex functions at O(1/T ) convergence rates", "authors": ["Shenghuo Zhu"], "emails": ["zsh@nec-labs.com"], "sections": [{"heading": null, "text": "We assume that a stochastic function x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}], "references": [{"title": "Minimum contrast estimators on sieves: exponential bounds and rates of convergence", "author": ["L. Birg\u00e9", "P. Massart"], "venue": null, "citeRegEx": "Birg\u00e9 and Massart,? \\Q1998\\E", "shortCiteRegEx": "Birg\u00e9 and Massart", "year": 1998}, {"title": "Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: a generic algorithmic framework. Optimization-online", "author": ["S. Ghadimi", "G. Lan"], "venue": null, "citeRegEx": "Ghadimi and Lan,? \\Q2012\\E", "shortCiteRegEx": "Ghadimi and Lan", "year": 2012}, {"title": "Accelerated gradient methods for stochastic optimization and online learning. NIPS\u201909", "author": ["C. Hu", "J.T. Kwok", "W. Pan"], "venue": "Neural Information Processing Systems", "citeRegEx": "Hu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hu et al\\.", "year": 2009}, {"title": "Efficient methods for stochastic composite optimization", "author": ["G. Lan"], "venue": "SIAM Journal on Optimization", "citeRegEx": "Lan,? \\Q2008\\E", "shortCiteRegEx": "Lan", "year": 2008}, {"title": "Making gradient descent optimal for strongly convex stochastic optimization", "author": ["A. Rakhlin", "O. Shamir", "K. Sridharan"], "venue": null, "citeRegEx": "Rakhlin et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Rakhlin et al\\.", "year": 2012}, {"title": "Smallest singular value of a random rectangular matrix", "author": ["M. Rudelson", "R. Vershynin"], "venue": "Communications on Pure and Applied Mathematics,", "citeRegEx": "Rudelson and Vershynin,? \\Q2009\\E", "shortCiteRegEx": "Rudelson and Vershynin", "year": 2009}, {"title": "Estimating the approximation error in learning theory", "author": ["S. Smale", "Zhou", "D.-X"], "venue": "Anal. Appl. (Singap.),", "citeRegEx": "Smale et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Smale et al\\.", "year": 2003}], "referenceMentions": [{"referenceID": 3, "context": "Some nonsmooth cases have been studied in (Lan, 2008) and (Ghadimi & Lan, 2012).", "startOffset": 42, "endOffset": 53}, {"referenceID": 4, "context": "There are studies on the high probability convergence rate of stochastic algorithm on strongly convex functions, such as (Rakhlin et al., 2012).", "startOffset": 121, "endOffset": 143}, {"referenceID": 2, "context": "Although SAGE (Hu et al., 2009) also provided a stochastic algorithm based on Nesterov\u2019s method for strongly convexity, the high probability bound was not given in the paper.", "startOffset": 14, "endOffset": 31}, {"referenceID": 3, "context": "We prove Lemma 6, which is the same as Lemma 7 of (Lan, 2008) except for the strong convexity.", "startOffset": 50, "endOffset": 61}, {"referenceID": 3, "context": "Similar to Lemma 9 of (Lan, 2008), we have the following lemma for Algorithm 2 with the consideration of strongly convex cases.", "startOffset": 22, "endOffset": 33}], "year": 2013, "abstractText": "With a weighting scheme proportional to t, a traditional stochastic gradient descent (SGD) algorithm achieves a high probability convergence rate of O(\u03ba/T ) for strongly convex functions, instead of O(\u03ba ln(T )/T ). We also prove that an accelerated SGD algorithm also achieves a rate of O(\u03ba/T ).", "creator": "TeX"}}}