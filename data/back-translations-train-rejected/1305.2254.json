{"id": "1305.2254", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2013", "title": "Programming with Personalized PageRank: A Locally Groundable First-Order Probabilistic Logic", "abstract": "In many probabilistic first-order representation systems, inference is performed by \"grounding\"---i.e., mapping it to a propositional representation, and then performing propositional inference. With a large database of facts, groundings can be very large, making inference and learning computationally expensive. Here we present a first-order probabilistic language which is well-suited to approximate \"local\" grounding: every query $Q$ can be approximately grounded with a small graph. The language is an extension of stochastic logic programs where inference is performed by a variant of personalized PageRank. Experimentally, we show that the approach performs well without weight learning on an entity resolution task; that supervised weight-learning improves accuracy; and that grounding time is independent of DB size. We also show that order-of-magnitude speedups are possible by parallelizing learning.", "histories": [["v1", "Fri, 10 May 2013 04:16:15 GMT  (321kb,D)", "http://arxiv.org/abs/1305.2254v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["william yang wang", "kathryn mazaitis", "william w cohen"], "accepted": false, "id": "1305.2254"}, "pdf": {"name": "1305.2254.pdf", "metadata": {"source": "CRF", "title": "Programming with Personalized PageRank: A Locally Groundable First-Order Probabilistic Logic", "authors": ["William Y. Wang", "Kathryn Mazaitis"], "emails": [], "sections": [{"heading": null, "text": "In many probabilistic representation systems of the first order, the deduction is made by \"grounding\" - i.e. by assignment to a meaningfulness and subsequent meaningfulness. With a large database of facts, grounding can be very extensive, which makes inferences and learning mathematically expensive. At this point, we present a higher probability language that is well suited to approximate a \"local\" grounding: Each Q query can be grounded roughly with a small diagram. Language is an extension of stochastic logic programs in which inference is performed by a variant of personalized PageRankings. Experimentally, we show that the approach works well without weight learning on a unit resolution task; that supervised weight learning improves accuracy; and that grounding time is independent of the size of the database. We also show that speeds of scale are possible through parallelization of learning."}, {"heading": "1 INTRODUCTION", "text": "In many probable first-order representation systems, including Markov Logic Networks [13] and Probabilistic Similarity Logic [3], inference is performed by mapping a first-order program to a propository representation, and performing approximate conclusions in this propository representation. However, this figure is often referred to as grounding. Figure 1 shows a simple MLN.1 As is often the case, this MLN has two parts: the rules R1, R2, which contain weighted first-order clauses; and the database DB, which consists of facts (unit clauses) of the form links (a, b) for constants a, b.1This MLN does a very simple type of label propagation through hyperlinks.Figure 1: A Markov logical network program and its grounding are clique potentials associated with rule R2, solid lines with rule R1."}, {"heading": "2 Programming with Personalized PageRank (PROPPR)", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 LOGIC PROGRAM INFERENCE AS GRAPH SEARCH", "text": "We will now describe our \"locally substantiated\" firststorder probability language, which we call ProPPR = Q. The conclusion for ProPPR is based on a personalized PageRank process above the evidence constructed by Prolog's Selective Linear Definite (SLD) theorem testers. To define the semantics, we use notation from logical programming [8]. Let LP be a program that contains a set of specific clauses c1,..., cn, and consider a Q subjunctive query about the predicates occurring in LP. A traditional prolog interpreter can be regarded as having the following actions. First, construct a \"root point\" v0 that is a pair (Q, Q) and add it to an otherwise blank graph G \u2032 Q, LP. (For Brevity, we will use the G \u00b2 subscripts where possible.)"}, {"heading": "2.2 FROM STOCHASTIC LOGIC PROGRAMS TO PROPPR", "text": "In stochastic logic programs (SLPs) [5], a randomized method for exceeding the graph G \u00b2 is defined, which thus defines a probability distribution over depressions V and therefore (by selecting only the basis of the solution) a distribution over transformed queries (i.e. answers) Q \u00b2. Thus, the randomized method generates a distribution over possible answers that can be applied to the annotations after the hash marks and the margins in the proof course. To match, only R1,. Rk is shown in each node u = (Q, R1,., Rk) by learning to obtain answers and downweights. In the past, the randomized exceedance of G \u00b2 was defined by a probability selection from which the clause is applied, based on a weight for each clause."}, {"heading": "2.3 LOCALLY GROUNDING A QUERY", "text": "Note that this approach includes both inferences (by calculating a distribution by letter Q\u03b8) and \"reasons\" of the query (by constructing a diagram G. ProPPR conclusion for this query can be efficiently realized by executing an ordinary PPR process on G. This is useful for faster learning. Unfortunately, the grounding G can be very large: It does not have to include the entire database, but if the number of iterations to convergence for the example program of Table 1 on the query Q = over (d, Y), G will construct a more compact local grounding graph for each page within T of d.To, we will adapt an approximate personalized PageRank method called PageRank-Nibble [1]. This method was used for the problem of local partitioning: in local partitioning, the goal is to find a small, low conductive component of a large graphicity G that contains a given-nibnoble."}, {"heading": "2.4 LEARNING FOR PROPPR", "text": "As mentioned above, the conclusion for a Q query in ProPPR is based on a personalized PagensRank procedure above the graph associated with the SLD detection of a query target G. More precisely, the edges u \u2192 v of the graph G are noted with feature vectors, and of these feature vectors weights are calculated using a parameter vector, and finally the existence of the re-start link is used as another important role in this program, as it avoids a kind of \"label bias problem\" where local decisions are difficult to adjust. Therefore, the \"grounded\" version of the inference is a personalized version of the PageRank process via a graph annotated by a feature vector edges.In previous work, Backstrom and Leskovic [2] outline a family of supervised learning procedures for this type of annotated graph."}, {"heading": "3 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 A SAMPLE TASK", "text": "To evaluate this method, we use data from an entity resolution task that was previously investigated as a test case for MLNs [15]. The program we use in the experiments is presented in Table 4: It roughly corresponds to the MLN (B + T) approach of Singla and Domingos.8 To evaluate the accuracy, we use the CORA dataset, a collection of 1295 bibliographic citations that relate to 132 different papers. During the experiments, we set the regulation coefficient \u00b5 to 0.001, the total number of epochs to 5, and the learning rate parameter \u03b7 to 1. In our objective function, a standard log loss function was used."}, {"heading": "3.2 RESULTS", "text": "We first consider the cost of PageRank-NibbleProve conclusions / grounding methods. Table 5 shows the time required for inference (with uniform weights) for a set of 52 randomly selected entity resolution tasks from the CORA dataset. (We report on the time in seconds8The principal difference is that we do not include tests for the absence of words in a field in our exams. (Figure 3: Run-time for inference in ProPPR (with a single thread) as a function of the number of entities in the database. The basis of the protocol is 2.for all 52 tasks as well as the mean mean mean mean mean precision of scoring for each query. It is clear that PageRank-Nibble-Prove offers a considerable speed dup to these problems with little loss of accuracy."}, {"heading": "4 RELATED WORK", "text": "Although we have chosen here to compare experimentally with MLNs [13, 15], ProPPR represents a rather different philosophy toward language design: instead of starting with a highly expressive but insoluble logical core, we start with a limited logical sequence scheme and add a minimal set of extensions to it that allow probabilistic thinking while maintaining stable, efficient inference and learning. While ProPPR is less meaningful than MLNs (for example, it is limited to definite clause theories), it is also much more efficient. However, this philosophy is similar to that exemplified by probabilistic similarity logic (PSL) [3]; unlike ProPSPR, PSL does not include a \"local\" grounding process that leads to small inference problems, even for large databases. Technically, ProPSPR is most similar to stoic logic programs (SLPs)."}, {"heading": "5 CONCLUSIONS", "text": "We have described a new first order probabilistic language designed with the goal of highly efficient reasoning and rapid learning. ProPPR takes Prologue's theoretical evidence for SLD, extends it with a probabilistic proof method, and then further limits this method by including a \"restart\" step that distorts the system to short proofs. This means that ProPPR has a simple polynomial time-stable method based on the well-studied Personalized PageRank method (PPR). Having previously worked on PPR-like methods, we have developed a local grounding method for ProPPR based on local partitioning methods [1] that results in an inference scheme that is an order of magnitude faster than the conventional power iteration approach for calculating PPR, Time O (1 \u03b1) requires simultaneous grounding for ProPPR, regardless of the size of the database. This ability to \"weight\" each query locally, making it possible for many to perform a different grading task."}], "references": [{"title": "Local partitioning for directed graphs using pagerank", "author": ["Reid Andersen", "Fan R.K. Chung", "Kevin J. Lang"], "venue": "Internet Mathematics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Group formation in large social networks: membership, growth, and evolution", "author": ["Lars Backstrom", "Dan Huttenlocher", "Jon Kleinberg", "Xiangyang Lan"], "venue": "In KDD \u201906: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Probabilistic similarity logic", "author": ["Matthias Brocheler", "Lilyana Mihalkova", "Lise Getoor"], "venue": "In Proceedings of the Conference on Uncertainty in Artificial intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Dynamic personalized PageRank in entity-relation graphs", "author": ["Soumen Chakrabarti"], "venue": "In WWW \u201907: Proceedings of the 16th international conference on World Wide Web,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Parameter estimation in stochastic logic programs", "author": ["James Cussens"], "venue": "Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Markov Logic: An Interface Layer for Artificial Intelligence", "author": ["Pedro Domingos", "Daniel Lowd"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Relational retrieval using a combination of path-constrained random walks", "author": ["Ni Lao", "William W. Cohen"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Foundations of Logic Programming: Second Edition", "author": ["J.W. Lloyd"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1987}, {"title": "Efficient clustering of high-dimensional data sets with application to reference matching", "author": ["Andrew McCallum", "Kamal Nigam", "Lyle H. Ungar"], "venue": "In Knowledge Discovery and Data Mining,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "Tuffy: Scaling up statistical inference in markov logic networks using an RDBMS", "author": ["Feng Niu", "Christopher R\u00e9", "AnHai Doan", "Jude Shavlik"], "venue": "Proceedings of the VLDB Endowment,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Feng Niu", "Benjamin Recht", "Christopher R\u00e9", "Stephen J Wright"], "venue": "arXiv preprint arXiv:1106.5730,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "The PageRank citation ranking: Bringing order to the web", "author": ["Larry Page", "Sergey Brin", "R. Motwani", "T. Winograd"], "venue": "In Technical Report,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Speeding up inference in markov logic networks by preprocessing to reduce the size of the resulting grounded network", "author": ["Jude Shavlik", "Sriraam Natarajan"], "venue": "In Proceedings of the Twentyfirst International Joint Conference on Artificial Intelligence (IJCAI-09),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Entity resolution with markov logic", "author": ["Parag Singla", "Pedro Domingos"], "venue": "In Data Mining,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Memoryefficient inference in relational domains", "author": ["Parag Singla", "Pedro Domingos"], "venue": "In Proceedings of the national conference on Artificial intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1999}, {"title": "Lifted firstorder belief propagation", "author": ["Parag Singla", "Pedro Domingos"], "venue": "In Proceedings of the 23rd national conference on Artificial intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Fast random walk with restart and its applications", "author": ["Hanghang Tong", "Christos Faloutsos", "Jia-Yu Pan"], "venue": "In ICDM,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2006}, {"title": "Slow learners are fast", "author": ["Martin Zinkevich", "Alex Smola", "John Langford"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Parallelized stochastic gradient descent", "author": ["Martin Zinkevich", "Markus Weimer", "Alex Smola", "Lihong Li"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}], "referenceMentions": [{"referenceID": 2, "context": "In many probabilistic first-order representation systems, including Markov Logic Networks [13] and Probabilistic Similarity Logic [3], inference is performed by mapping a first-order program to a propositional representation, and performing inference in that propositional representation.", "startOffset": 130, "endOffset": 133}, {"referenceID": 4, "context": "We present an extension to stochastic logic programs (SLP) [5] that is biased towards short derivations, and show that this is related to personalized PageRank (PPR) [12, 4] on a linearized version of", "startOffset": 59, "endOffset": 62}, {"referenceID": 11, "context": "We present an extension to stochastic logic programs (SLP) [5] that is biased towards short derivations, and show that this is related to personalized PageRank (PPR) [12, 4] on a linearized version of", "startOffset": 166, "endOffset": 173}, {"referenceID": 3, "context": "We present an extension to stochastic logic programs (SLP) [5] that is biased towards short derivations, and show that this is related to personalized PageRank (PPR) [12, 4] on a linearized version of", "startOffset": 166, "endOffset": 173}, {"referenceID": 7, "context": "To define the semantics we will use notation from logic programming [8].", "startOffset": 68, "endOffset": 71}, {"referenceID": 4, "context": "In stochastic logic programs (SLPs) [5], one defines a randomized procedure for traversing the graph G\u2032 which thus defines a probability distribution over vertices v, and hence (by selecting only solution vertices) a distribution over transformed queries (i.", "startOffset": 36, "endOffset": 39}, {"referenceID": 16, "context": "These links make SLP\u2019s graph traversal a personalized PageRank (PPR) procedure, sometimes known as random-walk-with-restart [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 11, "context": "Putting this all together with the standard iterative approach to computing personalized PageRank over a graph [12], we arrive at the following inference algorithm for answering a query Q, using a weight vector", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "To construct a more compact local grounding graph G, we adapt an approximate personalized PageRank method called PageRank-Nibble [1].", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": "shown [1] that the subgraph \u011c (of the full proof space) is in some sense a \u201cuseful\u201d subset: for an appropriate setting of , if there is a low-conductance subgraph G\u2217 of the full graph that contains v0, then G\u2217 will be contained in \u011c: thus if there is a subgraph G\u2217 containing v0 that approximates the full graph well, PageRank-Nibble will find (a supergraph of) G\u2217.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "In spite of its efficient inference procedure, and its limitation to only definite clauses, ProPPR appears to have much of the expressive power of MLNs [6], in that many useful heuristics can apparently be encoded.", "startOffset": 152, "endOffset": 155}, {"referenceID": 1, "context": "In prior work, Backstrom and Leskovic [2] outlined a family of supervised learning procedures for this sort of annotated graph.", "startOffset": 38, "endOffset": 41}, {"referenceID": 18, "context": "We implemented SGD because it is fast and has been adapted to parallel learning tasks [20, 11].", "startOffset": 86, "endOffset": 94}, {"referenceID": 10, "context": "We implemented SGD because it is fast and has been adapted to parallel learning tasks [20, 11].", "startOffset": 86, "endOffset": 94}, {"referenceID": 17, "context": "Although the shared parameter vector is a potential bottleneck [19], it is not a severe one, as the gradient computation dominates the learning cost.", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "To evaluate this method, we use data from an entity resolution task previously studied as a test case for MLNs [15].", "startOffset": 111, "endOffset": 115}, {"referenceID": 13, "context": "Following Singla and Domingos [15] we report performance as area under the ROC curve (AUC).", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "The line MLN(S&D) shows analogous results for the best-performing MLN from [15].", "startOffset": 75, "endOffset": 79}, {"referenceID": 13, "context": "that the MLN results reproduced here are not identical to previous-reported ones [15].", "startOffset": 81, "endOffset": 85}, {"referenceID": 8, "context": ", one of these was combining MLNs with a heuristic, TFIDF-based matching procedure based on canopies [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 13, "context": "Although we have chosen here to compare experimentally to MLNs [13, 15], ProPPR represents a rather", "startOffset": 63, "endOffset": 71}, {"referenceID": 2, "context": "This philosophy is similar to that illustrated by probabilistic similarity logic (PSL) [3]; however, unlike ProPPR, PSL does not include a \u201clocal\u201d grounding procedure, which leads to small inference problems, even for large databases.", "startOffset": 87, "endOffset": 90}, {"referenceID": 4, "context": "Technically, ProPPR is most similar to stochastic logic programs (SLPs) [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 12, "context": "There has been some prior work on reducing the cost of grounding probabilistic logics: noteably, Shavlik et al [14] describe a preprocessing algorithm called FROG that uses various heuristics to greatly reduce grounding size and inference cost, and Niu et al [10] describe a more efficient bottom-up grounding proce-", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "There has been some prior work on reducing the cost of grounding probabilistic logics: noteably, Shavlik et al [14] describe a preprocessing algorithm called FROG that uses various heuristics to greatly reduce grounding size and inference cost, and Niu et al [10] describe a more efficient bottom-up grounding proce-", "startOffset": 259, "endOffset": 263}, {"referenceID": 15, "context": ", [17]) and \u201clazy\u201d inference methods (e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": ", [16]); in fact, the LazySAT inference scheme for Markov networks is broadly similar algorithmically", "startOffset": 2, "endOffset": 6}, {"referenceID": 6, "context": "ProPPR is also closely related to the Path Ranking Algorithm (PRA), learning algorithm for link prediction [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "Following prior work on PPR-like methods, we designed a local grounding procedure for ProPPR, based on local partitioning methods [1], which leads to an inference scheme that is an order of magnitude faster that the conventional power-iteration approach to computing PPR, takes time O( 1 \u03b1\u2032 ), independent of database size.", "startOffset": 130, "endOffset": 133}], "year": 2013, "abstractText": "In many probabilistic first-order representation systems, inference is performed by \u201cgrounding\u201d\u2014i.e., mapping it to a propositional representation, and then performing propositional inference. With a large database of facts, groundings can be very large, making inference and learning computationally expensive. Here we present a firstorder probabilistic language which is wellsuited to approximate \u201clocal\u201d grounding: every query Q can be approximately grounded with a small graph. The language is an extension of stochastic logic programs where inference is performed by a variant of personalized PageRank. Experimentally, we show that the approach performs well without weight learning on an entity resolution task; that supervised weight-learning improves accuracy; and that grounding time is independent of DB size. We also show that order-of-magnitude speedups are possible by parallelizing learning.", "creator": "LaTeX with hyperref package"}}}