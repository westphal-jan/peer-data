{"id": "1305.2415", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-May-2013", "title": "Exponentiated Gradient LINUCB for Contextual Multi-Armed Bandits", "abstract": "We present Exponentiated Gradient LINUCB, an algorithm for con-textual multi-armed bandits. This algorithm uses Exponentiated Gradient to find the optimal exploration of the LINUCB. Within a deliberately designed offline simulation framework we conduct evaluations with real online event log data. The experimental results demonstrate that our algorithm outperforms surveyed algorithms.", "histories": [["v1", "Fri, 10 May 2013 11:13:14 GMT  (422kb)", "http://arxiv.org/abs/1305.2415v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["djallel bouneffouf"], "accepted": false, "id": "1305.2415"}, "pdf": {"name": "1305.2415.pdf", "metadata": {"source": "CRF", "title": "Exponentiated Gradient LINUCB for Contextual Multi- Armed Bandits", "authors": ["Djallel Bouneffouf"], "emails": ["Djallel.Bouneffouf@it-sudparis.eu"], "sections": [{"heading": null, "text": "This algorithm uses Exponentiated Gradient to find the optimal exploration of LINUCB. Within a consciously designed offline simulation framework, we perform evaluations with real online event log data. Experimental results show that our algorithm exceeds the measured algorithms. Keywords: machine learning; exploration / exploitation dilemma; artificial intelligence; LINUCB, exposed gradient."}, {"heading": "1 Introduction", "text": "A bandit algorithm B uses its past experience to select actions that occur more frequently. Moreover, these seemingly optimal actions may actually be suboptimal due to the inaccuracy in B's knowledge. To avoid this undesirable case, B must explore actions by selecting seemingly suboptimal actions to gather more information about them. Exploitation can reduce the satisfaction of short-term users, as some suboptimal actions can be selected. However, gathering information about the average rewards of actions (i.e. exploration) can refine B's assessment of the rewards of actions and in turn increase long-term user satisfaction. Clearly, neither a purely exploratory nor a purely exploitative algorithm works well, and a good trade-off is needed. An interesting solution to the complex banking problem is LINUCB's strategy. This is an extension of the UCB algorithm to essentially improve the exploration case."}, {"heading": "2 Key Notions", "text": "We define formally contextual bandit problems and bandit algorithms. Definition (Contextual bandit problem).In a contextual bandit problem, there is a distribution P over (x, r1,..., rk), where x is context, a \"1,.., k\" is one of the k arms to be drawn, and \"ra\" [0,1] is the reward for arm a. The problem is a repetitive game: in each round, a sample (x, r1,..., rk) is pulled from P, the context x is announced, and then for exactly one arm selected by the player its reward ra is revealed. Definition (Contextual bandit algorithm) B. A contextual bandit algorithm B determines an arm a, \"1,.., k\" to draw step t at any point in time, based on the previous observation sequence (x1, a1, ra, 1)."}, {"heading": "3 Related Work", "text": "Few research papers are devoted to the study of the contextual bandit problem on recommendation systems, where they consider the behavior of the user as the context of the bandit problem. In [6], the authors expand the \u03b5-greedy strategy by dynamically updating the \u03b5-exploration value. At each iteration, they perform a sampling procedure to select a new \u03b5 from a limited number of candidates. The probabilities associated with the candidates are uniformly initialized and updated with the Exposed Gradient (EG) [5]. This updating rule increases the probability of a candidate \u03b5 if it leads to a click by the user. In [9], the authors model the recommendation as a contextual bandit problem. They propose an approach in which a learning algorithm selects documents sequentially to serve the user on the basis of his behavior. To maximize the total number of clicks, this EG click result represents an efficient approach."}, {"heading": "3.1 The LINUCB algorithm", "text": "Assuming that the expected reward of a document is linear in its properties, the authors propose an approach in which a learning algorithm sequentially selects documents that serve the users based on contextual information about the users and the documents. Algorithm 2 The LINUCB algorithm Input: \u03b1-dimensional identity matrix) ba 0d * 1 (d-dimensional zero vector) ends when ErfolgsgaA 1 a * ba pt; a-dimensional zero vector) ends when ErfolgsgaA 1 a * pt; a-dimensional zero vector; a-dimensional zero vector; a-dimensional zero vector; a-dimensional zero vector; a-dimensional difference; a-dimensional difference; a-dimensional difference; a-dimensional difference; a-dimensional difference."}, {"heading": "3.2 The Gra dien -LINUCB algorithm", "text": "To view the random exploration on the LINUCB algorithm, the Grade LINUCB algorithm dynamically updates the exploration value \u03b5. In each iteration, they perform a scanning procedure to select a new \u03b5 from a series of candidates. The probabilities associated with the candidates are uniformly initialized and updated with the Exposed Gradient (EG) [34]. This refresh rule increases the probability of a candidate \u03b5 if it leads to a user clicking. algorithm 2 The Grade LINUCBInput: (\u03b51,..., \u03b5T): Candidate values for \u03b5, UC (d1,..., dn): Documentation method, \u03b2 and k: Parameters for EG N: Number of iterations Pk 1 = T and wk 1; k = 1,..., T for i = 1 doSample \u03b5d from Discrete (p1,..., pT))) (), (), (UdC iqik = (TRIK) (1), iCK = (1)."}, {"heading": "4 Experimental Evaluation", "text": "In order to evaluate empirically the performance of our approach, we are now looking at an online advertising application. Given a user visiting a publisher's page, the problem is to select the best advertisement for that user. A key element in this matching problem is the estimate of the click rate (CTR): What is the likelihood that a given ad will be clicked based on a context (user, page visited)? In fact, in a cost-per-click (CPC) campaign, the advertiser only pays when his ad is clicked. is the reason why it is important to select ads with high CTRs. Of course, there is a basic exploration / exploitation dilemma here: to learn the CTR of an ad, it must be displayed, which leads to a potential loss of short-term revenue. On average, we cannot find more details about the data and the ad advertising (Agarwal al al al al al al al al al al al al al al al al al al al al. Test, 2010 algorithm) to test the UCB algorithm proposed for the grading."}, {"heading": "5 Conclusion", "text": "In this paper, we have introduced an Exponentiated Gradient LINUCB algorithm for context-dependent, multi-armed bandits, which uses Exponentiated Gradient to find the optimal exploration of the LINUCB. Experimental results show that our algorithm delivers average better CTR performance in various configurations. In the future, we plan to evaluate the scalability of the algorithm onboard a mobile device and investigate other public benchmarks."}], "references": [{"title": "A Contextual-Bandit Algorithm for Mobile Context-Aware Recommender System. ICONIP", "author": ["Bouneffouf D", "A. Bouzeghoub", "L. Gan\u00e7arski A"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Hybrid-\u03b5-greedy for Mobile ContextAware Recommender System", "author": ["Bouneffouf D", "A. Bouzeghoub", "L. Gan\u00e7arski A"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Finite Time Analysis of the Multiarmed Bandit Problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning, vol 2, pp. 235\u2013256. Kluwer Academic Publishers, Hingham", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "PAC Bounds for Multi-Armed Bandit and Markov Decision Processes", "author": ["E. Even-Dar", "S. Mannor", "Y. Mansour"], "venue": "In Fifteenth Annual Conference on Computational Learning Theory, 255\u2013270", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2002}, {"title": "Warmuth", "author": ["J. Kivinen", "K. Manfred"], "venue": "Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, vol 132.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "A Contextual-Bandit Approach to Personalized News Document Recommendation", "author": ["Lihong", "Li.", "C. Wei", "J. Langford"], "venue": "Proceedings of the 19th international conference on World wide web, app. 661-670, ACM, New York", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Some aspects of the sequential design of experiments", "author": ["H. Robbins"], "venue": "Bulletin of the American Mathematical Society. Vol 58, pp.527-535", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1952}, {"title": "Learning from Delayed Rewards", "author": ["C. Watkins"], "venue": "Ph.D. thesis. Cambridge University", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1989}, {"title": "Exploitation and Exploration in a Performance based Contextual Advertising System", "author": ["Wei", "Li.", "X. Wang", "R. Zhang", "Y. Cui", "J. Mao", "R. Jin"], "venue": "Proceedings of the International Conference on Knowledge discovery and data mining: pp. 27-36. ACM, New York", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 8, "context": "This is an extension of the UCB algorithm to the parametric case [9].", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": ", k} is one of the k arms to be pulled, and ra \u2208 [0,1] is the reward for arm a.", "startOffset": 49, "endOffset": 54}, {"referenceID": 6, "context": "Very frequently used in reinforcement learning to study the exr/exp tradeoff, the multi-armed bandit problem was originally described by Robbins [7].", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "In [6], the authors extend the \u03b5-greedy strategy by dynamically updating the \u03b5 exploration value.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "The probabilities associated to the candidates are uniformly initialized and updated with the Exponentiated Gradient (EG) [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 8, "context": "In [9], authors model the recommendation as a contextual bandit problem.", "startOffset": 3, "endOffset": 6}], "year": 2013, "abstractText": "We present Exponentiated Gradient LINUCB, an algorithm for contextual multi-armed bandits. This algorithm uses Exponentiated Gradient to find the optimal exploration of the LINUCB. Within a deliberately designed offline simulation framework we conduct evaluations with real online event log data. The experimental results demonstrate that our algorithm outperforms surveyed algorithms.", "creator": "Microsoft\u00ae Word 2010"}}}