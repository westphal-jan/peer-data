{"id": "1305.2732", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-May-2013", "title": "An efficient algorithm for learning with semi-bandit feedback", "abstract": "We consider the problem of online combinatorial optimization under semi-bandit feedback. The goal of the learner is to sequentially select its actions from a combinatorial decision set so as to minimize its cumulative loss. We propose a learning algorithm for this problem based on combining the Follow-the-Perturbed-Leader (FPL) prediction method with a novel loss estimation procedure called Geometric Resampling (GR). Contrary to previous solutions, the resulting algorithm can be efficiently implemented for any decision set where efficient offline combinatorial optimization is possible at all. Assuming that the elements of the decision set can be described with d-dimensional binary vectors with at most m non-zero entries, we show that the expected regret of our algorithm after T rounds is O(m sqrt(dT log d)). As a side result, we also improve the best known regret bounds for FPL in the full information setting to O(m^(3/2) sqrt(T log d)), gaining a factor of sqrt(d/m) over previous bounds for this algorithm.", "histories": [["v1", "Mon, 13 May 2013 10:39:47 GMT  (15kb)", "http://arxiv.org/abs/1305.2732v1", "submitted to ALT 2013"]], "COMMENTS": "submitted to ALT 2013", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gergely neu", "g\\'abor bart\\'ok"], "accepted": false, "id": "1305.2732"}, "pdf": {"name": "1305.2732.pdf", "metadata": {"source": "CRF", "title": "An Efficient Algorithm for Learning with Semi-Bandit Feedback", "authors": ["Gergely Neu", "G\u00e1bor Bart\u00f3k"], "emails": ["gergely.neu@gmail.com", "bartok@inf.ethz.ch"], "sections": [{"heading": null, "text": "ar Xiv: 130 5.27 32v1 [cs.LG] 1 3 \u221a dT log d). As a result, we also improve the best known repentance limits for FPL in the complete information setting to O (m3 / 2 \u221a T log d), giving us a factor of \u221a d / m over previous limits for this algorithm. Keywords: follow-the-perturbed-leader, bandit problems, online learning, combinatorial optimization."}, {"heading": "1 Introduction", "text": "In this case, it is a complete problem, where the learner is able to minimize the loss of the learner. We allow the learner to rely on the previous decisions he has made..., Vt \u2212 1 made by the learners is not dependent on the previous decisions., Vt \u2212 1 made by the learners, that is, to minimize non-oblivious environments, to minimize the cumulative losses., Then, the performance of the learner is measured in relation to the overall expectation of expected resignation."}, {"heading": "2 Loss estimation by geometric resampling", "text": "It is very easy to show that it is a biascetic estimate of loss. (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is). (It is. (It is). (It is). (It is). (It is). (It is). (It is). (It is. (It is). (It is). (It is). (It is). (It is). (It is). (It is. (It is). (It is). (It is). (It is). (It is. (It is). (It is). (It is. (It is). (It is). (It is). (It is)."}, {"heading": "3 An efficient algorithm for learning with semi-bandit feedback", "text": "First, we generalize the geometric method for calculating loss estimates in the Semi-Bandit study. (To this end, it is necessary that we do not (1), but (2), (2), (3), (4), (4), (4), (4), (4), (4), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5, (5), (5), (5), (5), (5), (5, (5), (5), (5, (5), (5), (5), (5), (5), (5), (5, (5), (5), (5), (5), (5), (5), (5), (5), (5,"}, {"heading": "3.1 Running time", "text": "Let us now turn our attention to arithmetic questions. Since we cut off the number of scans of the decision vectors, the maximum number of scans per time step is M = \u221a dT. This implies an O (T 3 / 2d1 / 2) worst-case runtime. However, the expected runtime is much more comfortable. Theorem 2. The expected number of scans of the algorithm up to time step T can be limited upwards by dT.Proof. Let us first change our algorithm so that it pulls even more actions! Let us now assume that for each coordinate that the original arm had 1, we continue scanning until we get 1 in the same coordinate again. Let us also assume that we do not use scanning. Instead, we keep scanning until the desired 1 repeats."}, {"heading": "4 Improved bounds for learning with full information", "text": "Our evidence technology also allows us to tighten the guarantees for FPL in the full information setting. In particular, we consider the algorithm that chooses the index, independent of an exponential distribution of parameters. \u2212 In the following theory, we express our improved regret regarding this algorithm. \u2212 Theorem 3. Let the CT = 1 t = 1 t and the components of Zt be independent of an exponential distribution of parameters. \u2212 Then, the overall expected regret about FPL will be satisfactoryRn \u2264 m (log d + 1) + previous information. \u2212 In particular, setting such an algorithm (log d + 1) / (mT) can be regret above the limit."}, {"heading": "5 Conclusions and open problems", "text": "In this paper, we have described the first general efficient algorithm for online combinatorial optimization under semi-bandit feedback. We have proven that the regret of our algorithm O (m \u221a dT log d) is in this setting, and have also shown that FPL O (m3 / 2 \u221a T log d) can reach limits in the full information case if matched correctly. While these limits are eliminated by a factor of \u221a m log d and \u221a m from the respective Minimax results, they coincide exactly with the best known regrets for the well-studied exponentially weighted forecaster (EWA). Whether the above-mentioned gaps for FPL-like algorithms can be closed (e.g. by using more complicated perturbation programs) remains an important open question. Nevertheless, we consider our contribution a significant step toward understanding of the inherent trade-offs between computational efficiency and performance guarantees in online combinatorial optimization, and general linear optimization."}, {"heading": "6 In particular, for an MDP with state and action spaces X and A and worst-case", "text": "Mixing time \u03c4 > 0, calculation of probabilities qt, k can take up to O (| X | 3 | A |) time, GR provides sufficiently good estimates by generating O (| X | | A |) length curves \u03c4. The decision which approach is more efficient depends on the problem parameters X, A and \u03c4."}], "references": [{"title": "Hannan consistency in on-line learning in case of unbounded losses under partial monitoring", "author": ["C. Allenberg", "P. Auer", "L. Gy\u00f6rfi", "Ottucs\u00e1k", "Gy"], "venue": "In ALT,", "citeRegEx": "Allenberg et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Allenberg et al\\.", "year": 2006}, {"title": "Minimax policies for bandits games", "author": ["Audibert", "J.-Y", "S. Bubeck"], "venue": "COLT", "citeRegEx": "Audibert et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2009}, {"title": "Regret bounds and minimax policies under partial monitoring", "author": ["Audibert", "J.-Y", "S. Bubeck"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Audibert et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2010}, {"title": "Regret in online combinatorial optimization", "author": ["J.Y. Audibert", "S. Bubeck", "G. Lugosi"], "venue": "To appear in Mathematics of Operations Research", "citeRegEx": "Audibert et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Audibert et al\\.", "year": 2013}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Comput.,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Adaptive routing with end-to-end feedback: distributed learning and geometric approaches", "author": ["B. Awerbuch", "R.D. Kleinberg"], "venue": "In Proceedings of the 36th ACM Symposium on Theory of Computing,", "citeRegEx": "Awerbuch and Kleinberg,? \\Q2004\\E", "shortCiteRegEx": "Awerbuch and Kleinberg", "year": 2004}, {"title": "High probability regret bounds for online optimization", "author": ["P. Bartlett", "V. Dani", "T. Hayes", "S. Kakade", "A. Rakhlin", "A. Tewari"], "venue": "In Proceedings of the 21st Annual Conference on Learning Theory (COLT)", "citeRegEx": "Bartlett et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2008}, {"title": "Towards minimax policies for online linear optimization with bandit feedback", "author": ["S. Bubeck", "N. Cesa-Bianchi", "S.M. Kakade"], "venue": "Journal of Machine Learning Research - Proceedings Track,", "citeRegEx": "Bubeck et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bubeck et al\\.", "year": 2012}, {"title": "Prediction, Learning, and Games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2006}, {"title": "Combinatorial bandits", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Cesa.Bianchi and Lugosi,? \\Q2012\\E", "shortCiteRegEx": "Cesa.Bianchi and Lugosi", "year": 2012}, {"title": "The price of bandit information for online optimization", "author": ["V. Dani", "T. Hayes", "S. Kakade"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Prediction by random-walk perturbation", "author": ["L. Devroye", "G. Lugosi", "G. Neu"], "venue": "Accepted to the Twenty-Sixth Conference on Learning Theory", "citeRegEx": "Devroye et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Devroye et al\\.", "year": 2013}, {"title": "The on-line shortest path problem under partial monitoring", "author": ["A. Gy\u00f6rgy", "T. Linder", "G. Lugosi", "Ottucs\u00e1k", "Gy"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Gy\u00f6rgy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Gy\u00f6rgy et al\\.", "year": 2007}, {"title": "Approximation to Bayes risk in repeated play. Contributions to the theory of games, 3:97\u2013139", "author": ["J. Hannan"], "venue": null, "citeRegEx": "Hannan,? \\Q1957\\E", "shortCiteRegEx": "Hannan", "year": 1957}, {"title": "Efficient algorithms for online decision problems", "author": ["A. Kalai", "S. Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala", "year": 2005}, {"title": "Hedging structured concepts", "author": ["W. Koolen", "M. Warmuth", "J. Kivinen"], "venue": "In Proceedings of the 23rd Annual Conference on Learning Theory (COLT),", "citeRegEx": "Koolen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koolen et al\\.", "year": 2010}, {"title": "Online geometric optimization in the bandit setting against an adaptive adversary", "author": ["H.B. McMahan", "A. Blum"], "venue": "In Proceedings of the Eighteenth Conference on Computational Learning Theory,", "citeRegEx": "McMahan and Blum,? \\Q2004\\E", "shortCiteRegEx": "McMahan and Blum", "year": 2004}, {"title": "The online loop-free stochastic shortest-path problem", "author": ["G. Neu", "A. Gy\u00f6rgy", "Szepesv\u00e1ri", "Cs"], "venue": "In Proceedings of the Twenty-Third Conference on Computational Learning Theory,", "citeRegEx": "Neu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2010}, {"title": "Online Markov decision processes under bandit feedback", "author": ["G. Neu", "A. Gy\u00f6rgy", "Szepesv\u00e1ri", "Cs", "A. Antos"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Neu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Neu et al\\.", "year": 2010}, {"title": "FPL analysis for adaptive bandits", "author": ["J. Poland"], "venue": "Symposium on Stochastic Algorithms, Foundations and Applications (SAGA\u201905),", "citeRegEx": "Poland,? \\Q2005\\E", "shortCiteRegEx": "Poland", "year": 2005}, {"title": "Online prediction under submodular constraints. In Algorithmic Learning Theory, volume 7568 of Lecture Notes in Computer Science, pages 260\u2013274", "author": ["D. Suehiro", "K. Hatano", "S. Kijima", "E. Takimoto", "K. Nagano"], "venue": null, "citeRegEx": "Suehiro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Suehiro et al\\.", "year": 2012}, {"title": "Paths kernels and multiplicative updates", "author": ["E. Takimoto", "M. Warmuth"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Takimoto and Warmuth,? \\Q2003\\E", "shortCiteRegEx": "Takimoto and Warmuth", "year": 2003}], "referenceMentions": [{"referenceID": 4, "context": "The most well-known instance of our problem is the (adversarial) multiarmed bandit problem considered in the seminal paper of Auer et al. (2002): in each round of this problem, the learner has to select one of N arms and minimize regret against the best fixed arm, while only observing the losses of the chosen arm.", "startOffset": 126, "endOffset": 145}, {"referenceID": 15, "context": "The most popular example of online learning problems with actual combinatorial structure is the shortest path problem first considered by Takimoto and Warmuth (2003) in the full information scheme.", "startOffset": 138, "endOffset": 166}, {"referenceID": 8, "context": "The same problem was considered by Gy\u00f6rgy et al. (2007), who proposed an algorithm that works with semi-bandit information.", "startOffset": 35, "endOffset": 56}, {"referenceID": 1, "context": "Since then, we have come a long way in understanding the \u201cprice of information\u201d in online combinatorial optimization\u2014see Audibert et al. (2013) for a complete overview of results concerning all of the presented information schemes.", "startOffset": 121, "endOffset": 144}, {"referenceID": 1, "context": "Since then, we have come a long way in understanding the \u201cprice of information\u201d in online combinatorial optimization\u2014see Audibert et al. (2013) for a complete overview of results concerning all of the presented information schemes. The first algorithm directly targeting general online combinatorial optimization problems is due to Koolen et al. (2010): their method named Component Hedge guarantees an optimal regret of O(m \u221a T log d) in the full information setting.", "startOffset": 121, "endOffset": 353}, {"referenceID": 1, "context": "Since then, we have come a long way in understanding the \u201cprice of information\u201d in online combinatorial optimization\u2014see Audibert et al. (2013) for a complete overview of results concerning all of the presented information schemes. The first algorithm directly targeting general online combinatorial optimization problems is due to Koolen et al. (2010): their method named Component Hedge guarantees an optimal regret of O(m \u221a T log d) in the full information setting. In particular, this algorithm is an instance of the more general algorithm class known as Online Stochastic Mirror Descent (OSMD) or Follow-The-Regularized-Leader (FTRL) methods. Audibert et al. (2013) show that OSMD/FTRL-based methods can also be used for proving optimal regret bounds of O( \u221a mdT ) for the semi-bandit setting.", "startOffset": 121, "endOffset": 671}, {"referenceID": 1, "context": "Since then, we have come a long way in understanding the \u201cprice of information\u201d in online combinatorial optimization\u2014see Audibert et al. (2013) for a complete overview of results concerning all of the presented information schemes. The first algorithm directly targeting general online combinatorial optimization problems is due to Koolen et al. (2010): their method named Component Hedge guarantees an optimal regret of O(m \u221a T log d) in the full information setting. In particular, this algorithm is an instance of the more general algorithm class known as Online Stochastic Mirror Descent (OSMD) or Follow-The-Regularized-Leader (FTRL) methods. Audibert et al. (2013) show that OSMD/FTRL-based methods can also be used for proving optimal regret bounds of O( \u221a mdT ) for the semi-bandit setting. Finally, Bubeck et al. (2012) show that the natural extension of the EWA forecaster (coupled with an intricate exploration scheme) can be applied to obtain a O(m \u221a dT log d) upper bound on the regret when assuming full bandit feedback.", "startOffset": 121, "endOffset": 829}, {"referenceID": 1, "context": "Since then, we have come a long way in understanding the \u201cprice of information\u201d in online combinatorial optimization\u2014see Audibert et al. (2013) for a complete overview of results concerning all of the presented information schemes. The first algorithm directly targeting general online combinatorial optimization problems is due to Koolen et al. (2010): their method named Component Hedge guarantees an optimal regret of O(m \u221a T log d) in the full information setting. In particular, this algorithm is an instance of the more general algorithm class known as Online Stochastic Mirror Descent (OSMD) or Follow-The-Regularized-Leader (FTRL) methods. Audibert et al. (2013) show that OSMD/FTRL-based methods can also be used for proving optimal regret bounds of O( \u221a mdT ) for the semi-bandit setting. Finally, Bubeck et al. (2012) show that the natural extension of the EWA forecaster (coupled with an intricate exploration scheme) can be applied to obtain a O(m \u221a dT log d) upper bound on the regret when assuming full bandit feedback. This upper bound is off by a factor of \u221a m log d from the lower bound proved by Audibert et al. (2013). For completeness, we note that the EWA forecaster attains a regret of O(m \u221a T log d) in the full information case and O(m \u221a dT log d) in the semibandit case.", "startOffset": 121, "endOffset": 1138}, {"referenceID": 10, "context": "First, methods based on exponential weighting of each decision vector can only be efficiently implemented for a handful of decision sets S\u2014see Koolen et al. (2010) and Cesa-Bianchi and Lugosi (2012) for some examples.", "startOffset": 143, "endOffset": 164}, {"referenceID": 5, "context": "(2010) and Cesa-Bianchi and Lugosi (2012) for some examples.", "startOffset": 11, "endOffset": 42}, {"referenceID": 1, "context": "Furthermore, as noted by Audibert et al. (2013), OSMD/FTRL-type methods can be efficiently implemented by convex programming if the convex hull of the decision set can be described by a polynomial number of constraints.", "startOffset": 25, "endOffset": 48}, {"referenceID": 1, "context": "Furthermore, as noted by Audibert et al. (2013), OSMD/FTRL-type methods can be efficiently implemented by convex programming if the convex hull of the decision set can be described by a polynomial number of constraints. Details of such an efficient implementation are worked out by Suehiro et al. (2012), whose algorithm runs in O(d) time, which can still be prohibitive in practical problems.", "startOffset": 25, "endOffset": 304}, {"referenceID": 1, "context": "Furthermore, as noted by Audibert et al. (2013), OSMD/FTRL-type methods can be efficiently implemented by convex programming if the convex hull of the decision set can be described by a polynomial number of constraints. Details of such an efficient implementation are worked out by Suehiro et al. (2012), whose algorithm runs in O(d) time, which can still be prohibitive in practical problems. While Koolen et al. (2010) list some further examples where OSMD/FTRL can be implemented efficiently, we conclude that results concerning general efficient methods for online combinatorial optimization are lacking for (semi or full) bandit information problems.", "startOffset": 25, "endOffset": 421}, {"referenceID": 13, "context": "The Follow-the-Perturbed-Leader (FPL) prediction method (first proposed by Hannan (1957) and later rediscovered by Kalai and Vempala (2005)) method", "startOffset": 75, "endOffset": 89}, {"referenceID": 13, "context": "The Follow-the-Perturbed-Leader (FPL) prediction method (first proposed by Hannan (1957) and later rediscovered by Kalai and Vempala (2005)) method", "startOffset": 75, "endOffset": 140}, {"referenceID": 5, "context": "However, this result was recently improved to O(m \u221a T polylog d) by Devroye et al. (2013). \u2013 It is commonly believed that the standard proof techniques for FPL do not apply directly against adaptive adversaries (see, e.", "startOffset": 68, "endOffset": 90}, {"referenceID": 1, "context": "g, the comments of Audibert et al. (2013, Section 2.3) or Cesa-Bianchi and Lugosi (2006, Section 4.3)). On the other hand, a direct analysis for non-oblivious adversaries is given by Poland (2005) in the multi-armed bandit setting.", "startOffset": 19, "endOffset": 197}, {"referenceID": 1, "context": "g, the comments of Audibert et al. (2013, Section 2.3) or Cesa-Bianchi and Lugosi (2006, Section 4.3)). On the other hand, a direct analysis for non-oblivious adversaries is given by Poland (2005) in the multi-armed bandit setting. \u2013 Considering bandit information, no efficient FPL-style algorithm is known to achieve a regret of O( \u221a T ). Awerbuch and Kleinberg (2004) and McMahan and Blum (2004) proposed FPL-based algorithms for learning with full bandit feedback in shortest path problems, and proved O(T ) bounds on the regret (1).", "startOffset": 19, "endOffset": 371}, {"referenceID": 1, "context": "g, the comments of Audibert et al. (2013, Section 2.3) or Cesa-Bianchi and Lugosi (2006, Section 4.3)). On the other hand, a direct analysis for non-oblivious adversaries is given by Poland (2005) in the multi-armed bandit setting. \u2013 Considering bandit information, no efficient FPL-style algorithm is known to achieve a regret of O( \u221a T ). Awerbuch and Kleinberg (2004) and McMahan and Blum (2004) proposed FPL-based algorithms for learning with full bandit feedback in shortest path problems, and proved O(T ) bounds on the regret (1).", "startOffset": 19, "endOffset": 399}, {"referenceID": 1, "context": "g, the comments of Audibert et al. (2013, Section 2.3) or Cesa-Bianchi and Lugosi (2006, Section 4.3)). On the other hand, a direct analysis for non-oblivious adversaries is given by Poland (2005) in the multi-armed bandit setting. \u2013 Considering bandit information, no efficient FPL-style algorithm is known to achieve a regret of O( \u221a T ). Awerbuch and Kleinberg (2004) and McMahan and Blum (2004) proposed FPL-based algorithms for learning with full bandit feedback in shortest path problems, and proved O(T ) bounds on the regret (1). Poland (2005) proved bounds of O( \u221a NT logN) in the N -armed bandit setting, however, the proposed algorithm requires O(T ) computations per time step.", "startOffset": 19, "endOffset": 552}, {"referenceID": 3, "context": "While for many algorithms (such as the Exp3 algorithm of Auer et al. (2002) and the Green algorithm of Allenberg et al.", "startOffset": 57, "endOffset": 76}, {"referenceID": 0, "context": "(2002) and the Green algorithm of Allenberg et al. (2006)), the probabilities pt,i are readily available and the estimates (2) can be computed efficiently, this is not necessarily the case for all algorithms.", "startOffset": 34, "endOffset": 58}, {"referenceID": 14, "context": "While the main ingredients of the proof presented below are rather common (we borrow several ideas from Poland (2005), the proofs of Theorems 3 and 8 of Audibert et al.", "startOffset": 104, "endOffset": 118}, {"referenceID": 1, "context": "While the main ingredients of the proof presented below are rather common (we borrow several ideas from Poland (2005), the proofs of Theorems 3 and 8 of Audibert et al. (2013) and the proof of Corollary 4.", "startOffset": 153, "endOffset": 176}, {"referenceID": 1, "context": "While the main ingredients of the proof presented below are rather common (we borrow several ideas from Poland (2005), the proofs of Theorems 3 and 8 of Audibert et al. (2013) and the proof of Corollary 4.5 in Cesa-Bianchi and Lugosi (2006)), these elements are carefully combined in our proof to get the desired result.", "startOffset": 153, "endOffset": 241}, {"referenceID": 8, "context": "1 of Cesa-Bianchi and Lugosi (2006) (sometimes referred to as the \u201cbe-the-leader\u201d lemma) for the sequence ( l\u03021 \u2212 Z\u0303, l\u03022, .", "startOffset": 5, "endOffset": 36}, {"referenceID": 8, "context": "5 in Cesa-Bianchi and Lugosi (2006). Also note that this trick only applies in the case of non-negative losses.", "startOffset": 5, "endOffset": 36}, {"referenceID": 1, "context": "Similarly to the proof of Theorem 8 of Audibert et al. (2013), the last term can be upper bounded as", "startOffset": 39, "endOffset": 62}, {"referenceID": 6, "context": "Learning algorithms for this problem usually require that the pseudoinverse of the covariance matrix Pt = E [ VtV \u22a4 t \u2223Ft\u22121 ] is readily available for the learner at each time step (see, e.g., McMahan and Blum 2004; Bartlett et al. 2008; Dani et al. 2008; Cesa-Bianchi and Lugosi 2012; Bubeck et al. 2012).", "startOffset": 181, "endOffset": 305}, {"referenceID": 10, "context": "Learning algorithms for this problem usually require that the pseudoinverse of the covariance matrix Pt = E [ VtV \u22a4 t \u2223Ft\u22121 ] is readily available for the learner at each time step (see, e.g., McMahan and Blum 2004; Bartlett et al. 2008; Dani et al. 2008; Cesa-Bianchi and Lugosi 2012; Bubeck et al. 2012).", "startOffset": 181, "endOffset": 305}, {"referenceID": 9, "context": "Learning algorithms for this problem usually require that the pseudoinverse of the covariance matrix Pt = E [ VtV \u22a4 t \u2223Ft\u22121 ] is readily available for the learner at each time step (see, e.g., McMahan and Blum 2004; Bartlett et al. 2008; Dani et al. 2008; Cesa-Bianchi and Lugosi 2012; Bubeck et al. 2012).", "startOffset": 181, "endOffset": 305}, {"referenceID": 7, "context": "Learning algorithms for this problem usually require that the pseudoinverse of the covariance matrix Pt = E [ VtV \u22a4 t \u2223Ft\u22121 ] is readily available for the learner at each time step (see, e.g., McMahan and Blum 2004; Bartlett et al. 2008; Dani et al. 2008; Cesa-Bianchi and Lugosi 2012; Bubeck et al. 2012).", "startOffset": 181, "endOffset": 305}], "year": 2013, "abstractText": "We consider the problem of online combinatorial optimization under semi-bandit feedback. The goal of the learner is to sequentially select its actions from a combinatorial decision set so as to minimize its cumulative loss. We propose a learning algorithm for this problem based on combining the Follow-the-Perturbed-Leader (FPL) prediction method with a novel loss estimation procedure called Geometric Resampling (GR). Contrary to previous solutions, the resulting algorithm can be efficiently implemented for any decision set where efficient offline combinatorial optimization is possible at all. Assuming that the elements of the decision set can be described with d-dimensional binary vectors with at most m non-zero entries, we show that the expected regret of our algorithm after T rounds is O(m \u221a dT log d). As a side result, we also improve the best known regret bounds for FPL in the full information setting to O(m \u221a T log d), gaining a factor of \u221a d/m over previous bounds for this algorithm.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}