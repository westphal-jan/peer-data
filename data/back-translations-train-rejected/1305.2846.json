{"id": "1305.2846", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-May-2013", "title": "Opportunities & Challenges In Automatic Speech Recognition", "abstract": "Automatic speech recognition enables a wide range of current and emerging applications such as automatic transcription, multimedia content analysis, and natural human-computer interfaces. This paper provides a glimpse of the opportunities and challenges that parallelism provides for automatic speech recognition and related application research from the point of view of speech researchers. The increasing parallelism in computing platforms opens three major possibilities for speech recognition systems: improving recognition accuracy in non-ideal, everyday noisy environments; increasing recognition throughput in batch processing of speech data; and reducing recognition latency in realtime usage scenarios. This paper describes technical challenges, approaches taken, and possible directions for future research to guide the design of efficient parallel software and hardware infrastructures.", "histories": [["v1", "Thu, 9 May 2013 08:42:26 GMT  (66kb)", "http://arxiv.org/abs/1305.2846v1", "Pages: 05 Figures : 01 Proceedings of the International Conference BEATS 2010, NIT Jalandhar, INDIA"]], "COMMENTS": "Pages: 05 Figures : 01 Proceedings of the International Conference BEATS 2010, NIT Jalandhar, INDIA", "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["rashmi makhijani", "urmila shrawankar", "v m thakare"], "accepted": false, "id": "1305.2846"}, "pdf": {"name": "1305.2846.pdf", "metadata": {"source": "CRF", "title": "Opportunities and Challenges in Automatic Speech Recognition", "authors": ["Rashmi Makhijani", "Urmila Shrawankar", "V. M. Thakare"], "emails": [], "sections": [{"heading": null, "text": "Applications in today's world can no longer rely on significant increases in processor clock speed for performance improvements, as clock speed is now limited by factors such as power dissipation.4 Rather, parallel scalability (an application's ability to efficiently utilize an increasing number of processing elements) is required for software to achieve sustained performance improvements for successive generations of processors. Automatic Speech Recognition (ASR) is an application that consistently leverages advances in computing capacity.With the availability of a new generation of highly parallel single-chip computing platforms, ASR researchers face the question of unlimited computing power to improve speech recognition. The goal of the work is to explore plausible approaches to improving ASR in three ways: 1. Improving accuracy: Addressing noisy and reverberating environments where current systems are malfunctioning, thereby extending the reach of speech technology."}, {"heading": "2.1 Current Approach", "text": "The current preferred approach to robust functional extraction is to generate many functional streams with different spectrotemporal characteristics. Thus, some streams may be more sensitive to speech that varies at a slow syllable rate (e.g. 2 per second), and others may be more sensitive to signals that vary at a higher rate (e.g. 6 syllables per second). The most commonly used combination techniques are: (1) appending all characteristics to a single stream; (2) combining posterior distributions by means of a product rule with or without scaling; (3) combining posterior distributions by means of an additive rule with or without scaling; and (4) combining posterior distributions by means of a time-limited derivative; (5) combining posterior distributions by means of an additive rule, with or without scaling; and (4) combining posterior distributions by means of another ML that may also use other features."}, {"heading": "2.2 Future Directions", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "3.1 Current Approach", "text": "More recently, a data-parallel automated voice recognition inference engine has been implemented on the graphics processor unit (GPU), which delivers more than 11-fold acceleration compared to a SIMD-optimized sequential implementation on an Intel Core i7 CPU. With less than 8% sequential overhead, the solution promises more acceleration on future parallel platforms [8]. This acceleration has been made possible by the design of the detection engine's software architecture, which can run efficiently on single-chip multi-core processors, and four key implementation decisions have contributed to this acceleration:"}, {"heading": "3.1.1 Exposing fine-grained parallelism", "text": "Based on the Hidden Markov Model (HMM), the inference algorithm dictates that an outer iteration processes one inference feature vector at a time. Within each iteration, there is a sequence of algorithmic steps that implement inference processes with maximum probability. Application parallelism is located within the individual algorithmic steps in which the inference engine tracks thousands to tens of thousands of alternative interpretations of the input waveform. The challenge is that each algorithmic step executes only tens to hundreds of instructions to each alternative interpretation, causing synchronizations between algorithmic steps to cause sequential overheads. In parallel platforms with multiple chips, synchronization effort deteriorates significantly and parallel acceleration can be achieved by single-chip manycore parallel processors."}, {"heading": "3.1.2 Implementing all parts of an algorithm on the GPU", "text": "Current GPUs are accelerator subsystems that are managed by a CPU via the PCIe data bus. Figure 1. In the inference engine, there is a computationally intensive phase and a communication-intensive phase of execution in each inference iteration.The computationally intensive phase calculates the sum of the differences of a characteristic vector compared to Gaussian mixtures in the acoustic model and can be easily parallelized.The communication-intensive phase tracks thousands of alternative interpretations and manages their passage through a complex finite-state transducer that represents pronunciation and speech models.While 17.7 times the acceleration for the computationally intensive phase was achieved compared to the sequential execution on the CPU, the communication-intensive phase is much more difficult to parallelize and received 4.4 times the acceleration. However, because the algorithm is fully implemented on the GPU, it has achieved 11.3 times the acceleration of the entire inference engine."}, {"heading": "3.1.3 Leveraging fast hardware atomic operation support:", "text": "The inference process consists of data-parallel graph transfers on the detection network. The graph transfer routines run in parallel on differential cores and often need to update the same memory location, which means that race conditions like the same data must be read and conditionally written by multiple command streams at the same time. However, when using hardware-based support for atomic operations, the operations must be carefully sequenced as atomic operations on the same memory address by a sequence of parallel algorithmic steps in the application software or by using hardware-based support for atomic operations."}, {"heading": "3.1.4 Construct runtime data buffers to maximally regularize data access patterns:", "text": "The detection network is an irregular network, and the reversal through the network is controlled by user input that is only available at runtime. In each iteration of the inference machine, the data to be accessed is collected during iteration in a consecutive vector that acts as a runtime data buffer, so that the algorithmic steps in iteration are able to load and store the results one cache line at a time, maximizing the usage of the available data bandwidth to memory. These four important implementation decisions make it possible to overcome the parallelization challenges imposed by the application and design and implement a scalable parallel solution for decoding speech recognition inferences."}, {"heading": "3.2 Future Directions", "text": "In fact, the fact is that most of them will be able to play by the rules they have established in the past, and they will be able to put themselves in a position to put themselves in a position to put themselves in the position they are in."}, {"heading": "4.1 Current Approach", "text": "A first approach to online diarization was presented in the evaluations of NIST Rich Transcription 2009. The system consisted of a training step and an online recognition step. For the training step, the first 1000 seconds of input are taken and the offline diarization of the speakers with the system described above is performed. Afterwards, speaker models are trained and a speech / non-speech model is taken from the output of the system. This is done by concatenating a random 60-second piece of the segmented data of each speaker and another piece for the non-speech segments. In the online recognition step, the remaining speaker models are recognized with the trained models. The sampled audio data is reduced in noise and converted into MFCC characteristics. For each frame, the probability for each set of characteristics is calculated on the basis of each group of Gaussian mixtures achieved in the training step, i.e. each speaker model and the non-speaker model. A total of 250 10-frames are used to determine majority tuning over the offshore classification result."}, {"heading": "4.2 Future Directions", "text": "In fact, it is the case that most people are able to understand themselves and understand what is at stake, namely, the question of how they should behave and how they should behave. (...) In fact, it is the case that people are able to understand themselves. (...) It is the case that people are able to understand themselves. (...) It is the case that people are able to understand themselves. (...) It is the case that they are able to understand themselves. (...) It is as if they are able to understand themselves. (...) It is as if they are able to understand themselves. (...) It is as if they are able to understand themselves. (...) It is as if they understand themselves, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, as if they do it, what they do it, what they do it, if they do it, what they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do what they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, what they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do it, what they do it, if they do it, if they do it, what they do it, if they do it, if they do it, if they do it, if they do it, if they do it, if they do, what they do it, if they do it, if they do, if they do it, if they do it, if they do, if they do it, if they do it, if they do it, if they do, if they do, if they do it, if they are able, if they do it, if they are able, if they are able, if they do it, if they are"}], "references": [{"title": "A Robust Speaker Clustering Algorithm", "author": ["J. Ajmera", "C. Wooters"], "venue": "Proceedings of IEEE Workshop on Automatic Speech Recognition Understanding,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "How do humans process and recognize speech", "author": ["J.B. Allen"], "venue": "IEEE Transactions on Speech and Audio Processing,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Robust speaker segmentation for meetings: The ICSI-SRI spring 2005 diarization system", "author": ["X. Anguera", "C.Wooters", "B. Peskin", "M. Aguilo"], "venue": "In Proceeding of the NIST MLMI Meeting Recognition Workshop,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "The landscape of parallel computing research: A view from Berkeley", "author": ["K. Asanovic", "R. Bodik", "B.C. Catanzaro", "J.J. Gebis", "P. Husbands", "K. Keutzer", "D.A. Patterson", "W.L. Plishker", "J. Shalf", "S.W. Williams", "K.A. Yelick"], "venue": "Technical Report UCB/EECS-2006-183,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "A new ASR approach based on independent processing and recombination of partial frequency bands", "author": ["H. Bourlard", "S. Dupont"], "venue": "In Proceedings of the International Conference on Speech and Language Processing (ICSLP),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Multi-stream speech recognition", "author": ["H. Bourlard", "S. Dupont", "C. Ris"], "venue": "Technical Report RR 96-07,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1996}, {"title": "GPU accelerated acoustic likelihood computations", "author": ["P. Cardinal", "P. Dumouchel", "G. Boulianne", "M. Comeau"], "venue": "Proc. Interspeech", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "A fully data parallel WFST-based large vocabulary continuous speech recognition on a graphics processing unit", "author": ["J. Chong", "E. Gonina", "Y. Yi", "K. Keutzer"], "venue": "In Proceeding of the 10th Annual Conference of the International Speech Communication Association,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Spectrotemporal response field characterization with dynamic ripples in ferret primary auditory cortexa", "author": ["D. Depireux", "J. Simon", "D. Klein", "S. Shamma"], "venue": "Journal of Neurophysiology, 85(3)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2001}, {"title": "Fast acoustic computations using graphics processors", "author": ["P.R. Dixon", "T. Oonishi", "S. Furui"], "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Taipei, Taiwan", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Building an ASR system for noisy environments: SRI\u2019s 2001 SPINE evaluation system", "author": ["V. Gadde", "A. Stolcke", "D. Vergyri", "J. Zheng", "K. Sonmez", "A. Venkataraman"], "venue": "Proceedings of the International Conference on Speech and Language Processing (ICSLP),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "C", "author": ["Y. Huang", "O. Vinyals", "G. Friedland"], "venue": "M \u0308uller, N. Mirghafori, and C. Wooters. A fast-match approach for robust, faster than real-time speaker diarization. In Proceedings of the IEEE Automatic Speech Recognition Understanding Workshop", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Parallel LVCSR algorithm for cellphoneoriented multicore processors", "author": ["S. Ishikawa", "K. Yamabana", "R. Isotani", "A. Okumura"], "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Toulouse, France", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Multistream: Ready for prime-time", "author": ["A. Janin", "D.W. Ellis", "N. Morgan"], "venue": "In Proceedings of the European Conference on Speech Communication and Technology (Eurospeech),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "Robust spectrotemporal reverse correlation for the auditory system: optimizing stimulus design", "author": ["D. Klein", "D. Depireux", "J. Simon", "S. Shamma"], "venue": "Journal of Computational Neuroscience, 9(1):85\u2013111", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Dynamic programming search for continuous speech recognition", "author": ["H. Ney", "S. Ortmanns"], "venue": "IEEE Signal Processing Magazine, 16:64\u201383", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Parallel speech recognition", "author": ["S. Phillips", "A. Rogers"], "venue": "Intl. Journal of Parallel Programming, 27(4):257\u2013288", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1999}, {"title": "Parallel implementation of fast beam search for speaker-independent continuous speech recognition", "author": ["M. Ravishankar"], "venue": "Technical report, Computer Science and Automation, Indian Institute of Science, Bangalore, India", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "The ICSI RT07s Speaker Diarization System", "author": ["C. Wooters", "M. Huijbregts"], "venue": "CLEAR 2007 and RT", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "OpenMP-based parallel implementation of a continuous speech recognizer on a multicore system", "author": ["K. You", "Y. Lee", "W. Sung"], "venue": "Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing (ICASSP), Taipei, Taiwan", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 3, "context": "Applications in today\u2019s world can no longer rely on significant increases in processor clock rate for performance improvements, as clock rate is now limited by factors such as power dissipation [4].", "startOffset": 194, "endOffset": 197}, {"referenceID": 4, "context": "As has been shown for a number of years [5, 6, 15, 11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks.", "startOffset": 40, "endOffset": 54}, {"referenceID": 5, "context": "As has been shown for a number of years [5, 6, 15, 11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks.", "startOffset": 40, "endOffset": 54}, {"referenceID": 13, "context": "As has been shown for a number of years [5, 6, 15, 11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks.", "startOffset": 40, "endOffset": 54}, {"referenceID": 10, "context": "As has been shown for a number of years [5, 6, 15, 11], incorporating multiple feature sets consistently improves performance for both small and large ASR tasks.", "startOffset": 40, "endOffset": 54}, {"referenceID": 15, "context": "The inference engine traverses a graph-based recognition network based on the Viterbi search algorithm [17] and infers the most likely word sequence based on the extracted speech features and the recognition network.", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "Fine-grained concurrency was mapped onto the multiprocessor with distributed memory in [20].", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "[14] explored coarse-grained concurrency in speech recognition and implemented a pipeline of tasks on a cellphone-oriented multicore architecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[22] proposed a parallel speech recognizer implementation on a commodity multicore system using OpenMP.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The parallel recognition system proposed in [19] also uses a weighted finite state transducer (WFST) and data parallelism when traversing the recognition network.", "startOffset": 44, "endOffset": 48}, {"referenceID": 9, "context": "Prior works such as [10, 7] leveraged many core processors and focused on speeding up the compute-intensive phase (i.", "startOffset": 20, "endOffset": 27}, {"referenceID": 6, "context": "Prior works such as [10, 7] leveraged many core processors and focused on speeding up the compute-intensive phase (i.", "startOffset": 20, "endOffset": 27}, {"referenceID": 9, "context": "Both [10, 7] demonstrated", "startOffset": 5, "endOffset": 12}, {"referenceID": 6, "context": "Both [10, 7] demonstrated", "startOffset": 5, "endOffset": 12}, {"referenceID": 7, "context": "With less than 8% sequential overhead, the solution promises more speedup on future more parallel platforms [8].", "startOffset": 108, "endOffset": 111}, {"referenceID": 18, "context": ", [21].", "startOffset": 2, "endOffset": 6}, {"referenceID": 0, "context": "The ICSI system [1, 3] then performs the following iterations: Re-Segmentation: Run Viterbi alignment to find the optimal path of frames and models.", "startOffset": 16, "endOffset": 22}, {"referenceID": 2, "context": "The ICSI system [1, 3] then performs the following iterations: Re-Segmentation: Run Viterbi alignment to find the optimal path of frames and models.", "startOffset": 16, "endOffset": 22}, {"referenceID": 11, "context": "As a result of different sequential optimization approaches [13], the current implementation runs at about 0.", "startOffset": 60, "endOffset": 64}], "year": 2013, "abstractText": "Automatic speech recognition enables a wide range of current and emerging applications such as automatic transcription, multimedia content analysis, and natural human-computer interfaces. This paper provides a glimpse of the opportunities and challenges that parallelism provides for automatic speech recognition and related application research from the point of view of speech researchers. The increasing parallelism in computing platforms opens three major possibilities for speech recognition systems: improving recognition accuracy in non-ideal, everyday noisy environments; increasing recognition throughput in batch processing of speech data; and reducing recognition latency in realtime usage scenarios. This paper describes technical challenges, approaches taken, and possible directions for future research to guide the design of efficient parallel software and hardware infrastructures.", "creator": "PScript5.dll Version 5.2"}}}