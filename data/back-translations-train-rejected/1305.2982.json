{"id": "1305.2982", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2013", "title": "Estimating or Propagating Gradients Through Stochastic Neurons", "abstract": "Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we \"back-propagate\" through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients.", "histories": [["v1", "Tue, 14 May 2013 00:29:42 GMT  (15kb)", "http://arxiv.org/abs/1305.2982v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yoshua bengio"], "accepted": false, "id": "1305.2982"}, "pdf": {"name": "1305.2982.pdf", "metadata": {"source": "CRF", "title": "Estimating or Propagating Gradients Through Stochastic Neurons", "authors": ["Yoshua Bengio"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 130 5.29 82v1 [cs.LG]"}, {"heading": "1 Introduction and Background", "text": "This year is the highest in the history of the country."}, {"heading": "1.1 More Motivations", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "1.2 Prior Work", "text": "The idea of having stochastic neuron models is, of course, very old, with one of the most important families of algorithms based on such neurons being the Boltzmann machine (Hinton et al., 1984). In Section 4.2 we examine a connection between Boltzmann machine log liquidity gradients and perturbation-based estimators of the course discussed here. Another biologically motivated suggestion for synaptic strength learning was suggested by Fiete and Seung (2006). It is based on small zero-mean i.e. disturbances applied to any stochastic neuron potential (before non-linearity) and a Taylor expansion of the expected reward as a function of these variations. Fiete and Seung (2006) end with a gradient estimator that looks like a correlation between reward and disturbance, just like what we get in Section 3."}, {"heading": "2 Semi-Hard Stochastic Neurons", "text": "One way to achieve gradient-based learning in networks of stochastic neurons is to build an architecture in which noise is injected so that gradients sometimes flow into the neuron and then adjust it (and its predecessors in computer graphics). Generally, we can consider the results of a stochastic neuron as the application of a determining function that depends on its inputs (usually a vector containing the outputs of other neurons), consider its internal parameters, which usually affect the bias and incoming weights of the neurons), and look at a noise source zi: hi = f (xi, empirical)."}, {"heading": "3 Unbiased Estimator of Gradient for Stochastic Binary Neurons", "text": "Consider the case in which we want a component of our model to make a hard decision, but allow that decision to be stochastic, with a probability that is a continuous function of some sets, based on parameters that we want to learn. We also assume that many such decisions can be made in parallel with independent sources of sound that drive the stochastic samples. Without loss of universality, we are looking at a series of binary decisions, i.e., the construction corresponds to a set of stochastic binary neurons whose output hi influences an observed future loss L. Within the framework of Equivalent 1, we could have for examplehi = f (xi, zi, \u03b8i) = 1zi > \u03c3 (ai) (2), where zi \u0445 U [0, 1] is uniform and conditioning (u) = 1 / (1 + exp (\u2212 u) is the sigmoid function. In the case of the traditional artificial neuron, we would estimate haveai = bi + Wi \u00b7 xii = bi (), as an effect unit of influence \u2212 it."}, {"heading": "3.1 Derivation of Unbiased Estimator", "text": "Theoretically you can define hi as follows: Eq. 2, then g-i = hi-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-"}, {"heading": "3.2 Training a Lower-Variance Biased Estimator", "text": "One potential problem with the above unbiased estimators is that their variance could be large enough to significantly slow the training compared to the use of stochastic gradients with backward propagated gradients. We are proposing a general class of solutions to meet this challenge, but for this purpose we need a biased but low-variance estimator."}, {"heading": "3.2.1 Biased Low-Variance Estimator", "text": "A plausible unbiased estimator is the one developed below. Let's calculate G-j as an estimator of the expected loss gradient with respect to the activation (prelinearity) of unit j, and let's calculate unit j as its activation as a deterministic smooth function of the output height of unit i (for example aj = \u2211 iWjihi), then we can clearly get an estimator of the gradient with respect to hi. The problem is to replicate ourselves by the binary threshold function that hi produced from noise zi and activation ai (equation 2). The distorted estimator we suggest is simply G-i = \u2211 jG-j-hias the estimator of the expected loss gradient with respect to ai, i.e., we ignore the derivative of the threshold function f-2 already investigated by Goeff Hinton (Hinton, 2012), lecture 15b."}, {"heading": "3.2.2 Combining a Low-Variance High-Bias Estimator with a High-Variance Low-Bias Estimator", "text": "Suppose someone gives us two estimators G-i and g-i, the first having a low variance but a high bias, while the second has a high variance and a low bias, how could we use them to get a better estimator? What we are proposing is this: let us train a function Gi that takes the biased estimator with low variance G-i as input and predicts the biased high variance estimator g-i. Since Gi is a deterministic function of a low variance magnitude (we could add other inputs to help him predict it, but they should not be too loud), it should also have a low variance. Also, constructively, and to the extent that the learning task is feasible, the prediction Gi will strive to get as close as possible to the expected value of the unbiased estimator, i.e., Gi-E [g-i | G-i] is therefore a way that G-in which i might be helpful."}, {"heading": "4 Efficiency of Reward Correlation Estimators", "text": "One of the questions that the future work should address is the efficiency of estimators like the ones presented above."}, {"heading": "4.1 The Unbiased Estimator as Reward Correlator", "text": "In this respect, it is interesting to note how the proposed unbiased estimator (especially the centred one) is very similar to the equitable estimate of the correlation between the stochastic decision hi and the resulting loss L: Correlation = E [(hi \u2212 E [hi | ci]) (L \u2212 E [L | ci]) | ci] Note how this is the correlation between hi and L in the context of the other noise sources ci that affect hi. Note that a particular \"noise source\" is only the input of the model."}, {"heading": "4.2 The Boltzmann Machine Gradient as Unnormalized Reward Correlation", "text": "The log probability gradient via a bias (offset) parameter bi associated with a unit Xi (visible or hidden) from a Boltzmann machine with distribution P and a training example v (associated with visible units V) is \u2202 logP (V = v) \u2202 bi = E [Xi | V = v] \u2212 E [Xi], where the expectation lies about the distribution of the model, which defines a common distribution between all units of the model, including the visible. The unconditional distribution of the binary unit Xi vis-\u00e0-vis the other units is provided by P (Xi = 1 | X \u2212 i) = \u03c3 (ai) = \u03c3 (bi + j 6 = iWijXj).where ai is the unit activation (prior to the application of the sigmoid).An unbiased estimator of the aforementioned gradient isX + i \u2212 iwhere X + is a configuration obtained while V is a training example v, and X \u2212 is a configuration achieved without this limitation."}, {"heading": "5 Conclusion", "text": "In this paper, we have examined motivated estimators of the gradient by highly nonlinear non-differentiable functions (such as those corresponding to an indicator function), especially in networks involving noise sources, such as neural networks with stochastic neurons. They can be useful as biologically motivated models and they could be useful for engineering (computational efficiency) reasons when trying to reduce the computation by conditional computation or reduce the interactions between parameters by sparse updates (Bengio, 2013). We have also discussed the case of a general class of stochastic neurons for which the gradient is accurate for defined noise sources, but in which the nonlinearity is not saturated in its entire range (semi-hard stochastic neurons) and can be used for the ordinary back prop. We have also discussed the case of fully saturating nonlinearities, for which we have shown the existence of an unbiased estimator based on a correlation with observed nonlinearity."}, {"heading": "Acknowledgements", "text": "The author thanks NSERC, the Canada Research Chairs and CIFAR for their support."}], "references": [{"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Now Publishers. 12", "citeRegEx": "Bengio,? 2009", "shortCiteRegEx": "Bengio", "year": 2009}, {"title": "Deep learning of representations: Looking forward", "author": ["Y. Bengio"], "venue": "Technical Report arXiv:1305.0445, Universite de Montreal.", "citeRegEx": "Bengio,? 2013", "shortCiteRegEx": "Bengio", "year": 2013}, {"title": "Unsupervised feature learning and deep learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI).", "citeRegEx": "Bengio et al\\.,? 2013", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["S. El Hihi", "Y. Bengio"], "venue": "NIPS 8. MIT Press.", "citeRegEx": "Hihi and Bengio,? 1996", "shortCiteRegEx": "Hihi and Bengio", "year": 1996}, {"title": "Gradient learning in spiking neural networks by dynamic perturbations of conductances", "author": ["I.R. Fiete", "H.S. Seung"], "venue": "Physical Review Letters, 97(4).", "citeRegEx": "Fiete and Seung,? 2006", "shortCiteRegEx": "Fiete and Seung", "year": 2006}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "AISTATS.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "ICML\u20192013.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Neural networks for machine learning", "author": ["G. Hinton"], "venue": "Coursera, video lectures.", "citeRegEx": "Hinton,? 2012", "shortCiteRegEx": "Hinton", "year": 2012}, {"title": "Boltzmann machines: Constraint satisfaction networks that learn", "author": ["G.E. Hinton", "T.J. Sejnowski", "D.H. Ackley"], "venue": "Technical Report TR-CMU-CS-84-119, Carnegie-Mellon University, Dept. of Computer Science.", "citeRegEx": "Hinton et al\\.,? 1984", "shortCiteRegEx": "Hinton et al\\.", "year": 1984}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Teh", "Y.-W."], "venue": "Neural Computation, 18, 1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Technical report, arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS\u20192012.", "citeRegEx": "Krizhevsky et al\\.,? 2012a", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in Neural Information Processing Systems 25 (NIPS\u20192012).", "citeRegEx": "Krizhevsky et al\\.,? 2012b", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "ICML\u201910.", "citeRegEx": "Nair and Hinton,? 2010", "shortCiteRegEx": "Nair and Hinton", "year": 2010}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, 323, 533\u2013536.", "citeRegEx": "Rumelhart et al\\.,? 1986", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Semantic hashing", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "International Journal of Approximate Reasoning.", "citeRegEx": "Salakhutdinov and Hinton,? 2009", "shortCiteRegEx": "Salakhutdinov and Hinton", "year": 2009}, {"title": "Multivariate stochastic approximation using a simultaneous perturbation gradient approximation", "author": ["J.C. Spall"], "venue": "IEEE Transactions on Automatic Control, 37, 332\u2013341.", "citeRegEx": "Spall,? 1992", "shortCiteRegEx": "Spall", "year": 1992}], "referenceMentions": [{"referenceID": 14, "context": "This was what motivated the move from neural networks made of socalled formal neurons, with a hard threshold output, to neural networks whose units are based on a sigmoidal non-linearity, and the well-known back-propagation algorithm to compute the gradients (Rumelhart et al., 1986).", "startOffset": 259, "endOffset": 283}, {"referenceID": 5, "context": "Although it had been taken for granted by most researchers that smoothness of this graph was a necessary condition for exact gradient-based training methods to work well, recent successes of deep networks with rectifiers and other \u201csemi-hard\u201d non-linearities (Glorot et al., 2011; Krizhevsky et al., 2012a; Goodfellow et al., 2013) clearly question that belief: see Section 2 for a deeper discussion.", "startOffset": 259, "endOffset": 331}, {"referenceID": 11, "context": "Although it had been taken for granted by most researchers that smoothness of this graph was a necessary condition for exact gradient-based training methods to work well, recent successes of deep networks with rectifiers and other \u201csemi-hard\u201d non-linearities (Glorot et al., 2011; Krizhevsky et al., 2012a; Goodfellow et al., 2013) clearly question that belief: see Section 2 for a deeper discussion.", "startOffset": 259, "endOffset": 331}, {"referenceID": 6, "context": "Although it had been taken for granted by most researchers that smoothness of this graph was a necessary condition for exact gradient-based training methods to work well, recent successes of deep networks with rectifiers and other \u201csemi-hard\u201d non-linearities (Glorot et al., 2011; Krizhevsky et al., 2012a; Goodfellow et al., 2013) clearly question that belief: see Section 2 for a deeper discussion.", "startOffset": 259, "endOffset": 331}, {"referenceID": 10, "context": "As discussed here (Section 2), semi-hard non-linearities and stochastic perturbations can be combined to obtain reasonably low-variance estimators of the gradient, and a good example of that success is with the recent advances with dropout (Hinton et al., 2012; Krizhevsky et al., 2012b; Goodfellow et al., 2013).", "startOffset": 240, "endOffset": 312}, {"referenceID": 12, "context": "As discussed here (Section 2), semi-hard non-linearities and stochastic perturbations can be combined to obtain reasonably low-variance estimators of the gradient, and a good example of that success is with the recent advances with dropout (Hinton et al., 2012; Krizhevsky et al., 2012b; Goodfellow et al., 2013).", "startOffset": 240, "endOffset": 312}, {"referenceID": 6, "context": "As discussed here (Section 2), semi-hard non-linearities and stochastic perturbations can be combined to obtain reasonably low-variance estimators of the gradient, and a good example of that success is with the recent advances with dropout (Hinton et al., 2012; Krizhevsky et al., 2012b; Goodfellow et al., 2013).", "startOffset": 240, "endOffset": 312}, {"referenceID": 3, "context": "Although it had been taken for granted by most researchers that smoothness of this graph was a necessary condition for exact gradient-based training methods to work well, recent successes of deep networks with rectifiers and other \u201csemi-hard\u201d non-linearities (Glorot et al., 2011; Krizhevsky et al., 2012a; Goodfellow et al., 2013) clearly question that belief: see Section 2 for a deeper discussion. In principle, even if there are hard decisions (such as the treshold function typically found in formal neurons) in the computational graph, it is possible to obtain estimated gradients by introducing perturbations in the system and observing the effects. Although finite-difference approximations of the gradient appear hopelessly inefficient (because independently perturbing each of N parameters to estimate its gradient would be N times more expensive than ordinary back-propagation), another option is to introduce random perturbations, and this idea has been pushed far (and experimented on neural networks for control) by Spall (1992) with the Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm.", "startOffset": 260, "endOffset": 1043}, {"referenceID": 0, "context": "The symmetry-breaking and induced sparsity may also compensate for the extra variance and possibly help to reduce ill-conditioning, as hypothesized by Bengio (2013). However, it is appealing to consider noise whose amplitude can be modulated by the signals computed in the computational graph, such as with stochastic binary neurons, which output a 1 or a 0 according to a sigmoid probability.", "startOffset": 151, "endOffset": 165}, {"referenceID": 9, "context": "This is encouraging, since various Boltzmann machines (in particular the restricted Boltzmann machine) have been quite successful in recent years (Hinton et al., 2006; Bengio, 2009).", "startOffset": 146, "endOffset": 181}, {"referenceID": 0, "context": "This is encouraging, since various Boltzmann machines (in particular the restricted Boltzmann machine) have been quite successful in recent years (Hinton et al., 2006; Bengio, 2009).", "startOffset": 146, "endOffset": 181}, {"referenceID": 8, "context": "Until this question is resolved by biological observations, it is interesting to study how such noise, which has motivated the Boltzmann machine (Hinton et al., 1984), may impact computation and learning in neural networks.", "startOffset": 145, "endOffset": 166}, {"referenceID": 0, "context": "Stochastic neurons with binary outputs are also interesting because they can easily give rise to sparse representations (that have many zeros), a form of regularization that has been used in many representation learning algorithms (Bengio, 2009; Bengio et al., 2013).", "startOffset": 231, "endOffset": 266}, {"referenceID": 2, "context": "Stochastic neurons with binary outputs are also interesting because they can easily give rise to sparse representations (that have many zeros), a form of regularization that has been used in many representation learning algorithms (Bengio, 2009; Bengio et al., 2013).", "startOffset": 231, "endOffset": 266}, {"referenceID": 15, "context": "Binary representations are also useful as keys for a hash table, as in the semantic hashing algorithm (Salakhutdinov and Hinton, 2009).", "startOffset": 102, "endOffset": 134}, {"referenceID": 0, "context": "Stochastic neurons with binary outputs are also interesting because they can easily give rise to sparse representations (that have many zeros), a form of regularization that has been used in many representation learning algorithms (Bengio, 2009; Bengio et al., 2013). Sparsity of the representation corresponds to the prior that, for a given input scene x, most of the explanatory factors are irrelevant (and that would be represented by many zeros in the representation). Semi-hard stochastic neurons such as those studied in Section 2 also give rise to sparse gradients, i.e., such that for most examples, the gradient vector (with respect to parameters) has many zeros. Indeed, for weights into units that are shut off or are in a flat saturation region (e.g., at 0 or 1), the derivative will be zero. As argued in Bengio (2013), sparse gradients may be useful to reduce the optimization difficulty due to ill-conditioning, by reducing the number of interactions between parameters to those parameters that are simultaneously \u201cactive\u201d (with a non-zero gradient) for a particular example.", "startOffset": 232, "endOffset": 832}, {"referenceID": 0, "context": "Stochastic neurons with binary outputs are also interesting because they can easily give rise to sparse representations (that have many zeros), a form of regularization that has been used in many representation learning algorithms (Bengio, 2009; Bengio et al., 2013). Sparsity of the representation corresponds to the prior that, for a given input scene x, most of the explanatory factors are irrelevant (and that would be represented by many zeros in the representation). Semi-hard stochastic neurons such as those studied in Section 2 also give rise to sparse gradients, i.e., such that for most examples, the gradient vector (with respect to parameters) has many zeros. Indeed, for weights into units that are shut off or are in a flat saturation region (e.g., at 0 or 1), the derivative will be zero. As argued in Bengio (2013), sparse gradients may be useful to reduce the optimization difficulty due to ill-conditioning, by reducing the number of interactions between parameters to those parameters that are simultaneously \u201cactive\u201d (with a non-zero gradient) for a particular example. As argued by Bengio (2013), sparse representations may be a useful ingredient of conditional computation, by which only a small subset of the model parameters are \u201cactivated\u201d (and need to be visited) for any particular example, thereby greatly reducing the number of computations needed per example.", "startOffset": 232, "endOffset": 1118}, {"referenceID": 8, "context": "The idea of having stochastic neuron models is of course very old, with one of the major family of algorithms relying on such neurons being the Boltzmann machine (Hinton et al., 1984).", "startOffset": 162, "endOffset": 183}, {"referenceID": 16, "context": "Gradient estimators based on stochastic perturbations have been shown for long (Spall, 1992) to be much more efficient than standard finite-difference approximations.", "startOffset": 79, "endOffset": 92}, {"referenceID": 16, "context": "Instead, a perturbation-based estimator such as found in Simultaneous Perturbation Stochastic Approximation (SPSA) (Spall, 1992) chooses a random perturbation vector z (e.", "startOffset": 115, "endOffset": 128}, {"referenceID": 4, "context": "Another biologically motivated proposal for synaptic strength learning was proposed by Fiete and Seung (2006). It is based on small zero-mean i.", "startOffset": 87, "endOffset": 110}, {"referenceID": 4, "context": "Another biologically motivated proposal for synaptic strength learning was proposed by Fiete and Seung (2006). It is based on small zero-mean i.i.d. perturbations applied at each stochastic neuron potential (prior to a non-linearity) and a Taylor expansion of the expected reward as a function of these variations. Fiete and Seung (2006) end up proposing a gradient estimator that looks like a correlation between the reward and the perturbation, just like what we obtain in Section 3.", "startOffset": 87, "endOffset": 338}, {"referenceID": 4, "context": "Another biologically motivated proposal for synaptic strength learning was proposed by Fiete and Seung (2006). It is based on small zero-mean i.i.d. perturbations applied at each stochastic neuron potential (prior to a non-linearity) and a Taylor expansion of the expected reward as a function of these variations. Fiete and Seung (2006) end up proposing a gradient estimator that looks like a correlation between the reward and the perturbation, just like what we obtain in Section 3. However, their estimator is only unbiased in the limit of small perturbations. Gradient estimators based on stochastic perturbations have been shown for long (Spall, 1992) to be much more efficient than standard finite-difference approximations. Consider N quantities ai to be adjusted in order to minimize an expected loss L(a). A finite difference approximation is based on measuring separately the effect of changing each one of the parameters, e.g., through L(a)\u2212L(a\u2212\u01ebei) \u01eb , or even better, through L(a+\u01ebei)\u2212L(a\u2212\u01ebei) 2\u01eb , where ei = (0, 0, \u00b7 \u00b7 \u00b7 , 1, 0, 0, \u00b7 \u00b7 \u00b7 , 0) where the 1 is at position i. With N quantities (and typically O(N) computations to calculate L(a)), the computational cost of the gradient estimator is O(N). Instead, a perturbation-based estimator such as found in Simultaneous Perturbation Stochastic Approximation (SPSA) (Spall, 1992) chooses a random perturbation vector z (e.g., isotropic Gaussian noise of variance \u03c3) and estimates the gradient of the expected loss with respect to ai through L(\u03b8+z)\u2212L(\u03b8\u2212z) 2zi . So long as the perturbation does not put too much probability around 0, this estimator is as efficient as the finite-difference estimator but requires O(N) less computation. However, like the algorithm proposed by Fiete and Seung (2006) this estimator becomes unbiased only as the perturbations go towards 0.", "startOffset": 87, "endOffset": 1765}, {"referenceID": 4, "context": "Another biologically motivated proposal for synaptic strength learning was proposed by Fiete and Seung (2006). It is based on small zero-mean i.i.d. perturbations applied at each stochastic neuron potential (prior to a non-linearity) and a Taylor expansion of the expected reward as a function of these variations. Fiete and Seung (2006) end up proposing a gradient estimator that looks like a correlation between the reward and the perturbation, just like what we obtain in Section 3. However, their estimator is only unbiased in the limit of small perturbations. Gradient estimators based on stochastic perturbations have been shown for long (Spall, 1992) to be much more efficient than standard finite-difference approximations. Consider N quantities ai to be adjusted in order to minimize an expected loss L(a). A finite difference approximation is based on measuring separately the effect of changing each one of the parameters, e.g., through L(a)\u2212L(a\u2212\u01ebei) \u01eb , or even better, through L(a+\u01ebei)\u2212L(a\u2212\u01ebei) 2\u01eb , where ei = (0, 0, \u00b7 \u00b7 \u00b7 , 1, 0, 0, \u00b7 \u00b7 \u00b7 , 0) where the 1 is at position i. With N quantities (and typically O(N) computations to calculate L(a)), the computational cost of the gradient estimator is O(N). Instead, a perturbation-based estimator such as found in Simultaneous Perturbation Stochastic Approximation (SPSA) (Spall, 1992) chooses a random perturbation vector z (e.g., isotropic Gaussian noise of variance \u03c3) and estimates the gradient of the expected loss with respect to ai through L(\u03b8+z)\u2212L(\u03b8\u2212z) 2zi . So long as the perturbation does not put too much probability around 0, this estimator is as efficient as the finite-difference estimator but requires O(N) less computation. However, like the algorithm proposed by Fiete and Seung (2006) this estimator becomes unbiased only as the perturbations go towards 0. When we want to consider all-or-none perturbations (like a neuron sending a spike or not), it is not clear if these assumptions are appropriate. The advantage of the unbiased estimator proposed here is that it does not require that the perturbations be small. Another estimator of the expected gradient through stochastic neurons was proposed by Hinton (2012) in his lecture 15b.", "startOffset": 87, "endOffset": 2197}, {"referenceID": 10, "context": "Dropout noise (Hinton et al., 2012) and masking noise (in denoising auto-encoders (Vincent et al.", "startOffset": 14, "endOffset": 35}, {"referenceID": 15, "context": ", 2008)) is multiplied (just after the neuron non-linearity), while in semantic hashing (Salakhutdinov and Hinton, 2009) noise is added (just before the non-linearity).", "startOffset": 88, "endOffset": 120}, {"referenceID": 13, "context": "The prototypical example of that situation is the rectifier unit (Nair and Hinton, 2010; Glorot et al., 2011), whose non-linearity is simply max(0, arg).", "startOffset": 65, "endOffset": 109}, {"referenceID": 5, "context": "The prototypical example of that situation is the rectifier unit (Nair and Hinton, 2010; Glorot et al., 2011), whose non-linearity is simply max(0, arg).", "startOffset": 65, "endOffset": 109}, {"referenceID": 7, "context": "2already explored by Goeff Hinton (Hinton, 2012), lecture 15b", "startOffset": 34, "endOffset": 48}, {"referenceID": 1, "context": "They can be useful as biologically motivated models and they might be useful for engineering (computational efficiency) reasons when trying to reduce computation via conditional computation or to reduce interactions between parameters via sparse updates (Bengio, 2013).", "startOffset": 254, "endOffset": 268}, {"referenceID": 16, "context": "We have also discussed the case of completely saturating nonlinearities, for which we have demonstrated the existence of an unbiased estimator based on correlating the perturbation with the observed reward, which is related to but different from the SPSA (Spall, 1992) estimator.", "startOffset": 255, "endOffset": 268}], "year": 2013, "abstractText": "Stochastic neurons can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic neurons, i.e., can we \u201cback-propagate\u201d through these stochastic neurons? We examine this question, existing approaches, and present two novel families of solutions, applicable in different settings. In particular, it is demonstrated that a simple biologically plausible formula gives rise to an an unbiased (but noisy) estimator of the gradient with respect to a binary stochastic neuron firing probability. Unlike other estimators which view the noise as a small perturbation in order to estimate gradients by finite differences, this estimator is unbiased even without assuming that the stochastic perturbation is small. This estimator is also interesting because it can be applied in very general settings which do not allow gradient back-propagation, including the estimation of the gradient with respect to future rewards, as required in reinforcement learning setups. We also propose an approach to approximating this unbiased but high-variance estimator by learning to predict it using a biased estimator. The second approach we propose assumes that an estimator of the gradient can be back-propagated and it provides an unbiased estimator of the gradient, but can only work with non-linearities unlike the hard threshold, but like the rectifier, that are not flat for all of their range. This is similar to traditional sigmoidal units but has the advantage that for many inputs, a hard decision (e.g., a 0 output) can be produced, which would be convenient for conditional computation and achieving sparse representations and sparse gradients.", "creator": "LaTeX with hyperref package"}}}