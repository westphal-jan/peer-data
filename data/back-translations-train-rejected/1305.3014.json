{"id": "1305.3014", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-May-2013", "title": "Scalable Audience Reach Estimation in Real-time Online Advertising", "abstract": "Online advertising has been introduced as one of the most efficient methods of advertising throughout the recent years. Yet, advertisers are concerned about the efficiency of their online advertising campaigns and consequently, would like to restrict their ad impressions to certain websites and/or certain groups of audience. These restrictions, known as targeting criteria, limit the reachability for better performance. This trade-off between reachability and performance illustrates a need for a forecasting system that can quickly predict/estimate (with good accuracy) this trade-off. Designing such a system is challenging due to (a) the huge amount of data to process, and, (b) the need for fast and accurate estimates. In this paper, we propose a distributed fault tolerant system that can generate such estimates fast with good accuracy. The main idea is to keep a small representative sample in memory across multiple machines and formulate the forecasting problem as queries against the sample. The key challenge is to find the best strata across the past data, perform multivariate stratified sampling while ensuring fuzzy fall-back to cover the small minorities. Our results show a significant improvement over the uniform and simple stratified sampling strategies which are currently widely used in the industry.", "histories": [["v1", "Tue, 14 May 2013 03:48:09 GMT  (392kb,D)", "http://arxiv.org/abs/1305.3014v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DB", "authors": ["ali jalali", "santanu kolay", "peter foldes", "ali dasdan"], "accepted": false, "id": "1305.3014"}, "pdf": {"name": "1305.3014.pdf", "metadata": {"source": "CRF", "title": "Scalable Audience Reach Estimation in Real-time Online Advertising", "authors": ["Ali Jalali", "Santanu Kolay", "Peter Foldes", "Ali Dasdan"], "emails": ["ajalali@turn.com", "skolay@turn.com", "pfoldes@turn.com", "adasdan@turn.com"], "sections": [{"heading": "1 Introduction", "text": "In practice, the predictive result is expected to be returned in a few seconds, allowing advertisers to adjust their audience and contextual targeting. One way to generate fast and accurate reports is to take a relatively small sample from the original big data and conduct queries against that sample. In the end, the result is scaled to compensate for the sampling rate. In such a scheme, the accuracy of the report will depend on how good the sample is."}, {"heading": "2 Background and Related Work", "text": "In this section, we first formalize the problem and explain the structure. Then, we examine the related work in the context of our problem. We cannot cover the entire literature, but quote more recent work that is closer to our environment and formulation."}, {"heading": "2.1 Problem Setup and Motivation", "text": "Suppose our large data are in the form of a sentence D = {X1,.., XN} of K feature vectors Xi = (x1,.., xK) for all i-1,.., N)., However, we can assume that we consider all characteristics as inconsistent, and if not, we can assume that we consider a uniform number of 1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 6, 7, 6, 7, 7, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8"}, {"heading": "2.2 Related Work", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "3 Identifying Strata", "text": "The first step towards better scanning is to identify subsets of similar feature vectors known as strata. In this section, we present a challenge to divide the data into layers so that each layer is not too large to include non-similar feature vectors and not too small to require a large sample size. In this section, we propose a method for identifying such properties that any other feature highly correlates with these sets. If we then reduce the dimension of feature vectors to this subset, we can represent the entire population fairly well. To find such a subset of features, we use Markov Random Fields (MRFields) [18].Considering each feature as node."}, {"heading": "4 Fuzzy Fall-back", "text": "In fact, most of these strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...) Most strategies are not able to establish themselves. (...)"}, {"heading": "5 Distributed Implementation", "text": "The stratified sample offers the advantage of improved accuracy over the uniform sample with the same sample size limitation. However, accuracy still depends on the sample size. To handle a larger sample size, we distribute the sample to a number of servers. As the queries are trivially parallelized, the latency of the overall system decreases significantly. We are also able to support many simultaneous queries over the same sample. We describe the requirements of the distributed system in \u00a7 5.1. The system architecture is presented and explained in \u00a7 5.2."}, {"heading": "5.1 System requirements", "text": "The following non-functional requirements attempt to capture the most important needs of the system: 1. Almost real-time: the response of the system is expected in the order of seconds.2. Progressive Results Update: Results are updated in real-time as the scanning of the sample progresses.The main advantage of this approach is that users are able to get a very fast estimate of the results before the full calculation is completed.4. Scalability: We consider two main aspects: (a) Horizontal scalability: Adding more servers should allow the system to run the sample faster or run larger samples for greater accuracy. (b) Increased query handling: As system usage by users or automated software services increases, the system can handle the increased number of query requests without these significant performance requirements or performance reports affecting the availability of these samples and not be considered a guarantee of reliability and reliability of these decisions."}, {"heading": "5.2 System architecture", "text": "The distributed system consists of two types of nodes: counter nodes and aggregate nodes. The counter nodes are responsible for exceeding the part of the stratified sample present on that node and carry out the necessary query. The aggregator nodes select a set of counter nodes from a pool of counter nodes, distribute the query to each selected counter node and obtain partial results from counters and aggregate them to a single end result. This simple distinction of responsibilities allows the system to increase the amount of data, adding only a small constant overhead to network communication and introducing the increased time of partial report aggregation. This approach has been effectively evangelized by well-known search engines."}, {"heading": "5.2.1 Sample management pipeline", "text": "Zookeeper manages the servers and the sample in the following ways: 1. Count nodes capture sub-samples through a leader selection process controlled by the Zookeeper service, which prevents the sub-sample from being loaded into multiple counter nodes. If the sub-sample is loaded into multiple counter nodes, this results in lower overall accuracy. Given a fixed number of counter machines, it is always better to have each machine load a separate sub-sample, as we handle all non-reacting counter nodes gracefully. Zookeeper service is also responsible for alerting when new offline sub-samples are generated. 2. Aggregator selection is done via a distributed semaphor, which ensures a fixed number of aggregator nodes in a pool of aggregator nodes to be presented. 3. Zookeeper also orchestrates the loading of the sample in a staggered manner, so that not all sub-nodes are ready to reload during the reload time."}, {"heading": "5.2.2 Request queue pipeline", "text": "Each prediction request received from one of the prediction nodes is wrapped in a collector and added to a queue. Collectors are responsible for collecting the partial results, whether by processing the sample as in the case of counter nodes, or by continuously receiving partial results from other nodes as in the case of aggregator nodes. In the case of counter nodes, the collector orchestrates a series of threads that process data fed by consumer threads that iterate over the data. This strategy of consuming and processing data is useful in our case of use, especially when data points need to be compressed and decompressed to achieve memory optimization. This strategy continues to ensure that the system can scale with additional requests, since, in addition to the additional overhead of data point evaluation for the new request, there is no additional cost to optimize the data."}, {"heading": "5.2.3 Handling information on the sample for multiple servers", "text": "It is not enough for the counting nodes to know about the sample they have acquired, it is also important that other nodes, especially aggregator nodes, know it so that they can scale the results successfully. It may also be that one node has not loaded the entire available sample into memory. This has the advantage that the system is refined in terms of performance and accuracy. To ensure that the latest information is available, nodes automatically send it to each other node. When a node goes online, it receives and sends this information to each other node as well. On the other hand, ongoing forecasts should not be affected by these changes. If you take a scenario in which a counter produces a result, is pushed to the aggregator and a new sample is loaded, the aggregator should continue to use the old sample information for scaling. Therefore, each aggregator collector not only stores the partial results for each collector, but also the summed sample information of all participating nodes in the forecast."}, {"heading": "5.3 Analysis of the design", "text": "Section 5.1 describes in detail the requirements of the system. The following analysis describes how these requirements are met by the system described above and the various trade-offs."}, {"heading": "5.3.1 Performance", "text": "The performance of the system can be modeled as the total load of the system defined by td + ts + tc + tm = O (N + SN), (7) where tm is the time to distribute the query to N counter nodes, ts is the time to scan the sample per node, tc is the communication latency overhead, tm is the time to merge the results on the aggregator node for a sample of size S. The above equation can be used to fine-tune the system. Reducing the number of queries of a particular result increases the performance but slows down the progressive report. The same is achieved by increasing the delay between the counter nodes reporting partial results. The greatest flexibility comes with matching the sample size available per counter node, as tm is the most significant contribution to the overall load of the system. A larger representative sample can increase the accuracy, especially for a more granular filtering, while the small sample generation is completed faster during the light unification."}, {"heading": "5.3.2 User experience", "text": "There are several research findings that show that incremental loading of pages significantly reduces perceived waiting time [3]. In our system, we strive to implement incremental publication of results so that users can have a significantly good assessment of the search result very quickly. This is very important during the initial build-up of an ad campaign when the user tries to explore multiple what-if scenarios. As aggregate nodes continuously receive partial results from counters that they can merge and scale by using how much of the sample has been processed, partial results are always available."}, {"heading": "5.3.3 Availability", "text": "Through Zookeeper, we can monitor how many nodes are active and alive, and allocate responsibilities accordingly. A node failure can result in a temporary decrease in performance or accuracy until a new node is introduced. In the case of aggregator nodes, it is possible that some report generation will fail if the node fails. This is considered acceptable because the error happens quickly; the failed forecast reports are easy to submit again. As long as another aggregator node is available, the request can be met. It is possible that in extreme circumstances, too many nodes will fail at the same time. While the counternodes are able to receive requests on their own and produce a fully utilizable result, the potential error margin may be too large for just one machine to use the report. However, we also offer a margin of error for all our estimates and the user can choose to ignore the results if the error limits are beyond expectations."}, {"heading": "5.3.4 Scalability", "text": "Scalability is defined in the requirements in two different ways: the ability to add servers with ease to be able to predict faster or more accurately is achieved by sharing responsibilities. Since adding new counting nodes involves little effort in communication, the sample size can be increased for greater accuracy. It is also possible to redistribute the current sample to include the newly added node, reducing the time needed to process the sample per server. Processing increased quantities of prediction queries is achieved by taking two different factors into account: (1) because new aggregators can be introduced to handle communication from the outside and to ensure that no request is lost, which nevertheless allows callers to deliver partial results, and (2) because the size of the sub-cluster used for prediction can be matched. It is possible to select a smaller subset of counting nodes so that the nodes are not exhausted, but the prediction is potentially reduced."}, {"heading": "6 Experimental Results", "text": "We provide two sets of experimental results: one group of results demonstrates the improvement of our sampling strategy over a uniform and simple stratified sample, while the other group of results deals with the implementation of distributed systems."}, {"heading": "6.1 Sampling Performance", "text": "The simple stratified sample is our algorithm minus the blurred fallback step. We record the relative error of the queries in relation to the size of the query. We make a sample of the size n = 300K from a total population of N = 1.5B attribute vectors. Figure 5 shows that our sampling error is less than 8% of the uniform sampling error and less than 18% of the simple stratified sampling error for a query of size 1M. Obviously, the estimation error decreases with the size of the query, but our relative error (compared to other methods) also decreases. This shows that our method not only performs better than the other two methods, but also offers more advantages as the size of the query increases."}, {"heading": "6.2 System Performance", "text": "In Equation (7), some metrics are introduced as the descriptive factors of system performance. We conduct experiments to get a sense of how the system behaves by generating reports with different filter criterias.Table 1 shows the messages communicated within the system. Since initiate reports, sample information, and distribution requests are rare compared to partial results and potentially fetch replies, it is more important to focus on the performance of the latter two. Our measurements show that downloading the results takes significantly more time than pushing the partial results from counternodes to the aggregator nodes. This is not due to the message size, which is almost identical to the size of the partial outcomes, but to the time needed to merge the outcomes. In an attempt to see how much influence retrieving the results has on the overall system, Table 6 shows how the total report time changes as a function of the interval, the performance is not required in the table of the relative results."}, {"heading": "7 Conclusions", "text": "Predictions are critical for advertisers to determine their exact targets and budget expenditures in order to reach the best segments of the audience with the optimum score. Therefore, predictions must be accurate. In addition, predictions must be fast so that advertisers can conduct interactive exploration while finalizing their campaign parameters during campaign build-up. Predictions are also useful for evaluating the advertiser's DSP partner to compare a before-and-after picture of what was predicted and how the campaign came back. The same assessment is also used by DSPs in debugging campaign delivery and performance measurement. In this essay, we propose a prediction solution that can be solved both in terms of accuracy and speed. Our Markov Random Fields-based algorithm performs an accurate layered sample of the audience through the DSP. We offer a blurred case-quantum mechanism to allow small algorithms to split it in seconds."}, {"heading": "Acknowledgments", "text": "The authors thank Changgull Song for testing the entire framework in the production and Kuang-Chih Lee for his valuable comments."}], "references": [{"title": "Congressional samples for approximate answering of groupby queries", "author": ["S. Acharya", "P.B. Gibbons", "V. Poosala"], "venue": "ACM SIGMOD Record,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Web search for a planet: The google cluster architecture", "author": ["L.A. Barroso", "J. Dean", "U. Holzle"], "venue": "Micro, IEEE,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Integrating user-perceived quality into web server design", "author": ["N. Bhatti", "A. Bouch", "A. Kuchinsky"], "venue": "Computer Networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Stholes: a multidimensional workload-aware histogram", "author": ["N. Bruno", "S. Chaudhuri", "L. Gravano"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2001}, {"title": "Approximate query processing using wavelets", "author": ["K. Chakrabarti", "M. Garofalakis", "R. Rastogi", "K. Shim"], "venue": "The VLDB Journal,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Optimized stratified sampling for approximate query processing", "author": ["S. Chaudhuri", "G. Das", "V. Narasayya"], "venue": "ACM Transactions on Database Systems (TODS),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Vertex cover: further observations and further improvements", "author": ["J. Chen", "I.A. Kanj", "W. Jia"], "venue": "Journal of Algorithms,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Introduction to algorithms", "author": ["T.H. Cormen", "C.E. Leiserson", "R.L. Rivest", "C. Stein"], "venue": "MIT press,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Lcs-hist: taming massive high-dimensional data cube compression", "author": ["A. Cuzzocrea", "P. Serafino"], "venue": "In Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Icicles: Self-tuning samples for approximate query answering", "author": ["V. Ganti", "M.-L. Lee", "R. Ramakrishnan"], "venue": "In Proceedings of the 26th International Conference on Very Large Data Bases,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2000}, {"title": "Approximate query processing: Taming the terabytes", "author": ["M. Garofalakis", "P.B. Gibbons"], "venue": "In Proc. of the 27th Intl. Conference on Very Large Data Bases,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Selectivity estimation using probabilistic models", "author": ["L. Getoor", "B. Taskar", "D. Koller"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "New sampling-based summary statistics for improving approximate query answers", "author": ["P.B. Gibbons", "Y. Matias"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1998}, {"title": "Approximating multi-dimensional aggregate range queries over real attributes", "author": ["D. Gunopulos", "G. Kollios", "V.J. Tsotras", "C. Domeniconi"], "venue": "In ACM SIGMOD Record,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2000}, {"title": "Zookeeper: Wait-free coordination for internet-scale systems", "author": ["P. Hunt", "M. Konar", "F.P. Junqueira", "B. Reed"], "venue": "In USENIX ATC,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "On learning discrete graphical models using greedy methods", "author": ["A. Jalali", "C.C. Johnson", "P. Ravikumar"], "venue": "In Neural Information Processing Conference (NIPS),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Scalable approximate query processing with the dbo engine", "author": ["C. Jermaine", "S. Arumugam", "A. Pol", "A. Dobra"], "venue": "ACM Transactions on Database Systems (TODS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "Markov random fields and their applications", "author": ["R. Kindermann", "J.L. Snell"], "venue": "American Mathematical Society Providence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1980}, {"title": "High-dimensional olap: A minimal cubing approach", "author": ["X. Li", "J. Han", "H. Gonzalez"], "venue": "In Proceedings of the Thirtieth international conference on Very large data bases-Volume", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Stratified sampling for data mining on the deep web", "author": ["T. Liu", "F. Wang", "G. Agrawal"], "venue": "Frontiers of Computer Science,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Wavelet-based histograms for selectivity estimation", "author": ["Y. Matias", "J.S. Vitter", "M. Wang"], "venue": "ACM SIGMOD Record,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "On the normal approximation to the hypergeometric distribution", "author": ["W. Nicholson"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1956}, {"title": "Combinatorial optimization: algorithms and complexity", "author": ["C.H. Papadimitriou", "K. Steiglitz"], "venue": "Dover publications,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1998}], "referenceMentions": [{"referenceID": 21, "context": "Using the Normal approximation summarized in Theorem 2 of [22], we have", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 55, "endOffset": 68}, {"referenceID": 3, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 55, "endOffset": 68}, {"referenceID": 13, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 55, "endOffset": 68}, {"referenceID": 8, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 55, "endOffset": 68}, {"referenceID": 4, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 84, "endOffset": 91}, {"referenceID": 20, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 84, "endOffset": 91}, {"referenceID": 12, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 111, "endOffset": 119}, {"referenceID": 16, "context": "There are three main approaches to AQP-Histogram based [1, 4, 14, 9], wavelet based [5, 21] and sampling based [13, 17].", "startOffset": 111, "endOffset": 119}, {"referenceID": 10, "context": "[11] provides a tutorial introduction to the subject.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] provide a stratified sampling based approach to approximate query processing under a workload distribution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "Authors of [10] introduce a novel way of maintaining dynamic samples for AQP.", "startOffset": 11, "endOffset": 15}, {"referenceID": 8, "context": "In [9], authors study the problem of building multidimensional histograms to answers queries approximately over very high dimensional data cubes.", "startOffset": 3, "endOffset": 6}, {"referenceID": 18, "context": "Even though the range splitting and bucket merging has some similarity with our approach, our fundamental problem is to derive a set of user samples that can be used for arbitrary queries, not necessarily OLAP style [19] aggregates.", "startOffset": 216, "endOffset": 220}, {"referenceID": 11, "context": "Some authors have proposed probabilistic methods for selectivity estimation [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 19, "context": "The problem of estimating the result of aggregation query with low selectivity has been tackled by other work related to deep Web [20], with the additional constraint of not having full access to the data set.", "startOffset": 130, "endOffset": 134}, {"referenceID": 0, "context": "Though our method bears some resemblance to the methods proposed in [1, 9], our data set is very high-dimensional (250K) and consists of categorical variables only.", "startOffset": 68, "endOffset": 74}, {"referenceID": 8, "context": "Though our method bears some resemblance to the methods proposed in [1, 9], our data set is very high-dimensional (250K) and consists of categorical variables only.", "startOffset": 68, "endOffset": 74}, {"referenceID": 17, "context": "To find such subset of features we use Markov Random Fields (MRFs) [18].", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "If we remove the edges with matrix \u0398j,k equal to zero, the remaining graph has the property that each feature conditioned on its neighbors in the graph is independent from the rest of the graph [18].", "startOffset": 194, "endOffset": 198}, {"referenceID": 15, "context": "A distributed and fast algorithm to learn the MRF graph from data is detailed in [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 7, "context": "Once the MRF graph among features is learned, we need to find a minimum vertex cover [8] of that graph.", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "Minimum vertex cover is known to be an NP-hard problem; however, there are good approximations such as the algorithm proposed in [7, 23].", "startOffset": 129, "endOffset": 136}, {"referenceID": 22, "context": "Minimum vertex cover is known to be an NP-hard problem; however, there are good approximations such as the algorithm proposed in [7, 23].", "startOffset": 129, "endOffset": 136}, {"referenceID": 1, "context": "This approach has been evangelized by well known search engines [2] effectively.", "startOffset": 64, "endOffset": 67}, {"referenceID": 14, "context": "Distribution of the sample between the counter nodes and the notification when a new set of sample data is available is handled by a Zookeeper [15] cluster, a distributed lock service widely used in the industry.", "startOffset": 143, "endOffset": 147}, {"referenceID": 2, "context": "There are multiple research showing that having incremental page loading capability reduces the perceived waiting time significantly [3].", "startOffset": 133, "endOffset": 136}], "year": 2013, "abstractText": "Online advertising has been introduced as one of the most efficient methods of advertising throughout the recent years. Yet, advertisers are concerned about the efficiency of their online advertising campaigns and consequently, would like to restrict their ad impressions to certain websites and/or certain groups of audience. These restrictions, known as targeting criteria, limit the reachability for better performance. This trade-off between reachability and performance illustrates a need for a forecasting system that can quickly predict/estimate (with good accuracy) this trade-off. Designing such a system is challenging due to (a) the huge amount of data to process, and, (b) the need for fast and accurate estimates. In this paper, we propose a distributed fault tolerant system that can generate such estimates fast with good accuracy. The main idea is to keep a small representative sample in memory across multiple machines and formulate the forecasting problem as queries against the sample. The key challenge is to find the best strata across the past data, perform multivariate stratified sampling while ensuring fuzzy fall-back to cover the small minorities. Our results show a significant improvement over the uniform and simple stratified sampling strategies which are currently widely used in the industry.", "creator": "LaTeX with hyperref package"}}}