{"id": "1305.3932", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-May-2013", "title": "Inferring the Origin Locations of Tweets with Quantitative Confidence", "abstract": "Social Internet content plays an increasingly critical role in many domains, including public health, disaster management, and politics. However, its utility is sharply limited by lack of geographic information; for example, fewer than 1.6% of Twitter messages (tweets) contain a geotag. We propose a scalable, content-based approach to estimate the location of tweets using a novel yet simple variant of gaussian mixture models tuned for this task. Further, given that real-world applications depend on quantified uncertainty of such estimates, we propose novel metrics of accuracy, precision, and calibration, and we evaluate our approach accordingly. Experiments on 13 million global, comprehensively multi-lingual tweets show that our approach yields reliable, well-calibrated results competitive with previous computationally intensive methods. Our results also show that a relatively small number of training data are required for good estimates (roughly 30,000 tweets), and trained models are quite time-invariant (effective on tweets many weeks newer than the training set). Finally, we offer an analysis of which types of content provide the most useful location signals.", "histories": [["v1", "Thu, 16 May 2013 20:47:05 GMT  (741kb,D)", "https://arxiv.org/abs/1305.3932v1", "15 pages, 8 figures"], ["v2", "Fri, 26 Jul 2013 22:48:26 GMT  (741kb,D)", "http://arxiv.org/abs/1305.3932v2", "15 pages, 8 figures. Version 2: Move mathematics to appendix, 2 new references, various other presentation improvements"], ["v3", "Sat, 16 Nov 2013 00:06:38 GMT  (4438kb,D)", "http://arxiv.org/abs/1305.3932v3", "14 pages, 6 figures. Version 2: Move mathematics to appendix, 2 new references, various other presentation improvements. Version 3: Various presentation improvements, accepted at ACM CSCW 2014"]], "COMMENTS": "15 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.SI cs.HC cs.LG", "authors": ["reid priedhorsky", "aron culotta", "sara y del valle los alamos national laboratory", "illinois institute of technology)"], "accepted": false, "id": "1305.3932"}, "pdf": {"name": "1305.3932.pdf", "metadata": {"source": "META", "title": "Inferring the Origin Locations of Tweets with Quantitative Confidence", "authors": ["Reid Priedhorsky", "Aron Culotta", "Sara Y. Del Valle"], "emails": ["reidpr@lanl.gov", "sdelvall@lanl.gov", "aculotta@iit.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "The results of this study, however, are limited because the geographical origin of the content is largely unknown. Thus, there is a growing interest in the task of location determination: faced with an element that measures its geographical origin and actual origin, we propose a method based on a two-dimensional distribution method based on the geographical distribution of people. [22] Our models are trained on geotagged tweets, i.e. messages with the user profile and the geographical true origin of points. [22] For each unique n-gram, we adapt to a two-dimensional GMM that models its geographical distribution. To derive the origin of a new tweet, we combine previously trained GMMs for the n-grams it contains using weights of data; Figure 1 shows an example estimate. This approach is simple, scalable and competitive with more complex approaches: http / LA / LA."}, {"heading": "2. RELATED WORK", "text": "In recent years, the problem of the origin of social Internet content has become an increasingly active field of research. In the following, we summarize the four main areas of work and compare them with this paper."}, {"heading": "2.1 Geocoding", "text": "Perhaps the simplest approach to location determination is geocoding: looking up the user profile's free text field in a gazetteer (list of toponyms), and if a match is found, it can be concluded that the message comes from the appropriate place. Researchers have used commercial geocoding services such as Yahoo! Geocoders [32], US Geological Survey Data [26], and Wikipedia [16] to do so. This technique can be extended to the message text itself by first using a geoparser called entity recodizer to extract toponyms [13]. Schulz et al. [30] recently reported on precise results using a scheme that combines multiple geocoding sources, including Internet queries. Crucial to their performance was the discovery that an additional 26% of tweets can be matched with precise coordinates using text analysis and can be incorporated by following links to location-based services (a patch approach, etc.)."}, {"heading": "2.2 Statistical classifiers", "text": "These approaches build a statistical representation of text to classify pre-defined regions such as cities and countries (i.e., treat the \"place of origin\" as belonging to one of these classes rather than as a geographical point); therefore, any token can be used to inform location conclusions. We categorize this work by type of classifier and location granularity. For example, Cheng et al. use a variant of naive Bayes to classify messages by city [6], Hecht et al. use a similar classifier at federal and state level [16], and Kinsella et al. use language models to classify messages by neighborhood, city, state, zip code and country [19]. Mahmud et al. classify users by city with greater accuracy than Cheng et al. by combining a hierarchical classifier with many heuristics and gazetteers to structure them."}, {"heading": "2.3 Geographic topic models", "text": "Eisenstein et al. developed a cascading topic model that generates region-specific topics and used these topics to derive the locations of Twitter users [10]; the following work uses sparse additive models to more efficiently combine region-specific, user-specific and non-informative topics [11, 17]. Topic modeling does not require explicit predefined regions. However, regions are derived as a pre-processing step: Eisenstein et al. with a mixture of dirichlet process [10] and Hong et al. with K-mean clusters [17]. The latter also suggests that more regions increase the accuracy of conclusions. While these approaches lead to precise models, the majority of modeling and computing complexity results from the need to produce geographically coherent topics [10] and Hong et al."}, {"heading": "2.4 Social network information", "text": "Recent work suggests that the use of social link information (e.g. followers or friends) can be helpful in determining location [4, 8]. We consider these approaches to be complementary to our own; accordingly, we are not exploring them further at this time."}, {"heading": "2.5 Contrasting our approach", "text": "Compared to previous work, we offer the following basic distinctions: (a) location estimates are multimodal probability distributions, not points or regions, and are evaluated rigorously as such; (b) because we deal directly with geographical coordinates, there is no need to pre-define regions of interest; (c) no gazetteers or other supplementary data are required; and (d) we evaluate using a data set that is more comprehensive in time (one year of data), geographical (global) and linguistic (all languages except Chinese, Thai, Laos, Cambodian and Burmese)."}, {"heading": "3. EXPERIMENT DESIGN", "text": "In this section, we present three characteristics of good site estimation, metrics and experiments to measure them, and new algorithms that are motivated by them."}, {"heading": "3.1 What makes a good location estimate?", "text": "An estimate of the origin of a message should be able to answer two closely related but distinct questions: Q1. What is the true origin of the message? That is, at what geographical point was the person who wrote the message when he did so? Q2. Was the true origin within a particular geographical region? That is, we argue that a location estimate should be a geographical density estimate: a function that estimates the probability that every point on the globe is the true origin. Viewed through this lens, a high-quality estimate has the following characteristics: \u2022 It is accurate: the density of the estimate is strongly skewed toward the true origin (i.e., the estimate rates point closer to the true origin than far away). Then, Q1 can be answered effectively because the most likely Q2 distribution is the most likely, because the most likely Q2 region is effectively the true origin."}, {"heading": "3.2 Metrics", "text": "This section presents our metrics and their intuitive reasoning; rigorous mathematical implementations are within the scope. 3.2.1 Accuracy Our core metrics for evaluating the accuracy of an estimate are Comprehensive Coverage Errors (CAE): The expected distance between the true origin and a randomly selected point in the density of the estimate, or in other words, the mean distance between the true origin and any point on Earth, weighted by the coverage of the coverage of the coverage. The goal is to provide an idea of the distance from the true origin to the density of the estimate as a whole. These contrasts with a common previous metric that we call simple accuracy: the distance from the best individual performance we consider to be true coverage. Figure 2 illustrates this contrast. The narrow clusters around Washington, D.C. 2A similar metrics, called error, have been proposed by Cho et for another task."}, {"heading": "3.3 Experiment implementation", "text": "In this section, we will explain the basic structure of our experiments: data source, pre-processing and tokenization, as well as testing procedures. \u2022 3.3.1 Data We used the Twitter Streaming API to collect an approximately continuous 1% sample of all global tweets from January 25, 2012 to January 23, 2013. \u2022 Between 0.8% and 1.6% of these tweets, depending on the timeframe, included a geotag (i.e., specific geographic coordinates marking the true origin of the tweet, derived from GPS or other automated means), allowing a total of about 13 million geotagged tweets.3We used the text of the message (tx), the user description (ds) and the user position (lo) of the fields that are free text in bigrams by splitting the Unicode character category and script boundaries, and then further subdividing the bigrams that appear as Japanese, using the TinySegment Algorithm [15] which covers all but a few languages."}, {"heading": "4. OUR APPROACH: GEOGRAPHIC GMMS", "text": "At this point, we present our approach to location determination. First, we motivate and summarize it, then we detail the specific algorithms that we have tested. (For mathematical implementations, see the appendix.)"}, {"heading": "4.1 Motivation", "text": "Examining the geographical distribution of n-grams may suggest appropriate inference models. Let's remember, for example, Figure 2 above; the two clusters and the scattered locations elsewhere suggest that a multimodal distribution of two-dimensional Gaussians could be an appropriate fit. On the basis of this intuition, and in conjunction with the above desired data, we propose an estimator that uses one of the more mature density estimation techniques: Gaussian mixing models (GMMs). These models are exactly the weighted sum of several Gaussian (normal) distributions and have natural probability interpretations. In addition, they have already been applied to human mobility patterns [7, 14]. Our algorithm is summarized as follows: 1. For each n-gram that appears more than a threshold number of times in the training data, a GMM should be adjusted to the true origins of the tweets in the training set containing this n-gram."}, {"heading": "4.2 Weighting by quality properties", "text": "We tested 15 quality properties that measured this in different ways. [5] In fact, Eisenstein et al. attributed the poor performance of several of their baselines to this tendency of uninformative words to dilute the predictive power of informative words. [10] We also tried to weight each GMM based on the inversion of (1) the number of adapted points, (2) the spatial variance of these points, and (3) the number of components in the mixture. We also tried metrics based on the covariance matrices of the Gaussian components: the inversion of (4) the sum of all elements and (5) the sum of the products of the elements in each matrix. Finally, we tried normalization based on both the number of adapted points (properties 6-9) and the number of adapted components (10-13)."}, {"heading": "4.3 Weighting by error", "text": "Another approach is to weight each n-gram based on its error within the training set. Specifically, for each n-gram of the learned model, we calculate the error of its GMM (CAE or SAE) based on each point at which it was attached. We then increase this error to a strength (to increase the dominance of relatively good n-grams over relatively poor ones) and use the inversion of this value as the weight of the n-gram (i.e. larger errors yield smaller weights).We refer to these algorithms as (for example) GMM-Err-SAE4, which uses the SAE error metric and an exponent of 4. We have tried exponent values from 0.5 to 10 as well as CAE and SAE; since the latter was faster and yielded comparable results, we report only SAE."}, {"heading": "4.4 Weighting by optimization", "text": "Our basic approach is to assign each n-gram a set of characteristics with their own weights, to keep the weight of each n-gram as a linear combination of characteristic weights, and to find characteristic weights by means of gradient descent, so that the overall error over all n-grams is minimized (i.e. the total geo-positioning accuracy is maximized).For optimization, we have tried three types of n-gram characteristics: 1. The above quality characteristics (atr).2. Identity characteristics. The first n-gram had characteristic 1 and no other, the second n-gram characteristic 2 and no other, and so on (ID).3. Both types of characteristics (both).Finally, we further classify these algorithms by optimizing for each n-gram (GMM) or a single Gaussian characteristic (ID) and only GMM characteristics."}, {"heading": "4.5 Baseline weighting algorithms", "text": "As final baselines, we considered GMM-All-Tweets, which match all tweets in the training set on a single GMM and return this GMM for all localization operations, and GMM-One, which weights all n-gram mixtures equally."}, {"heading": "5. RESULTS", "text": "In this section, we present our experimental results and discussions in the context of our four research questions. (In addition to the experiments described in detail above, we have tried several variants that have had limited benefits, and these results are summarized in the appendices.)"}, {"heading": "5.1 RQ1: Improved approach", "text": "In fact, it is not as if it had not been, as if it had not been, as if......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5.2 RQ2: Training size", "text": "We evaluated the accuracy of GMM-Err-SAE4 on the basis of different training duration, no gap, all fields except the user description and minimum instances of 3. We used a 13-day step to establish performance.Figure 4 shows our results. The knee of the curve is 1 training day (i.e. about 30,000 tweets), with error plateauing and training time increasing rapidly as more data is added.6 Accordingly, we use 1 training day in our other experiments. We also evaluated the accuracy in variing minimum instances (the frequency threshold for retaining n-grams), setting training days to 1; Figure 5 shows the outcomes. Specifically, the inclusion of n-grams, which occur only three times in the training set, improves accuracy at a modest time cost (and therefore we use this value in our other experiments), which could be partially explained by the well-known long tail distribution of word frequencies; that is, while the overall informativeness of each tweet may be so low (and the fact that each gram may be)."}, {"heading": "5.3 RQ3: Time dependence", "text": "We investigated the accuracy of GMM-Err-SAE4 at different time intervals between training and test, maintaining a fixed training duration of one day and minimum n-gram instances of 3. Figure 6 summarizes our results. Location is surprisingly time-invariant: While errors increase linearly with the duration of the gap, this happens slowly - there are only about 6% additional errors with a four-month gap. We suspect this is simply because location-related n-grams that are time-dependent (e.g. those associated with a music festival) are relatively rare."}, {"heading": "5.4 RQ4: Location signal source", "text": "We wanted to understand what types of content provide useful location information under our algorithm. For example, Figure 1 on the first page illustrates a successful estimate of 6We also observed a deterioration in calibration beyond 1 day; this may explain some of the accuracy improvements and should be explored."}, {"heading": "6. IMPLICATIONS", "text": "We also propose a simple, scalable method of location determination that is comparable to more complex ones, and we validate this approach against our new criteria based on a dataset of tweets that is comprehensive in time, geography and language, with implications for both site conclusions research and applications that depend on such a conclusion. Specifically, our metrics can help these and related inferential domains to improve the balance between accuracy and retrieval, and to argue correctly in the presence of uncertainty. Our findings also have implications for privacy. Specifically, they suggest that social Internet users wishing to maximize their privacy should mention toponyms only at the state or country level, or perhaps not at all, (b) do not use languages with a small geographical footprint, and mention them for maximum privacy reasons, (c) decoy locations."}, {"heading": "7. ACKNOWLEDGMENTS", "text": "Susan M. Mniszewski, Geoffrey Fairchild and other members of our research team were on hand to help. We thank our anonymous reviewers for important information and the Twitter users whose content we examined. This work is supported by NIH / NIGMS / MIDAS, Grant U01-GM097658-01. The calculation was done with Darwin, a cluster operated by CCS-7 at LANL and funded by the Accelerated Strategic Computing Program; we thank Ryan Braithwaite for his technical assistance. Maps were drawn using Quantum GIS; 8 spatial data of the base map are from Natural Earth.9 LANL is operated by Los Alamos National Security, LLC on behalf of the Department of Energy under contract DE-AC52-06NA25396."}, {"heading": "8. APPENDIX: MATHEMATICAL IMPLEMENTATIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Metrics", "text": "To do this, we use the following vocabulary. (Let m be a message represented by a binary attribute vector of n-gram (i.e., sequences of up to n adjacent tokens; we use n = 2) m = {w1}, w j = 1 means that n-gram w j appears in message m, and V is the total size of the vocabulary. Let us represent a geographic point (for example, latitude and longitude) somewhere on the surface of the Earth. We represent the true origin of a message as a new message m, and our goal is to construct a geographic density estimate f (y | m), a function that estimates the probability of each point, the true origin of m 8http: / / qgis.org 9http: / / naturalearthdata.com. These implementations are relevant to any estimate."}, {"heading": "8.2 Gaussian mixture models", "text": ", we construct our site model by referring to geographical data consisting of a number of n (message, true origin) pairs extracted from our database geotagged Tweets; i.e., D = (mi, y, i) ni = 1. For each n-gram w | j we fit one gaussian mixing model g (y, w) on the basis of examples in D. Um then to estimate the place of origin of a new message m, we combine mixing models for all n-grams in m to represent a new density f (y, m). These steps are detailed. We estimate g for each (sufficiently frequent) n-gram w j in D as follows. In the beginning we collect a quantity of true origin of all messages containing w j j, and then we fit a gaussian mixing model of r-grams in m to a new density f (y, m). These steps are: g (j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, k, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j component, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j, j"}, {"heading": "8.3 Setting \u03b4 j weights by inverse error", "text": "Mathematically, the inverse error approach presented above can be described as a non-iterative optimization problem. Specifically, we specify a multinomial distribution for the message mi (results with CAE are comparable). Let e j be the average error of n-grams w j: e j = 1N j = N j = 1 ei j, where Nj is the number of messages containing w j. We introduce a model parameter that applies a non-linear (exponential) penalty to error terms e j. The problem is to minimize the negative protocol probability, with limitations to ensure that it is a probability distribution:."}, {"heading": "8.4 Setting \u03b4 j weights by optimization", "text": "This section describes the data-driven optimization algorithm presented above. We mark each n-gram density function = n-gram density function with a feature vector. This vector contains the ID of the n-gram density function, the quality properties, or both of these. For12Our implementation first assigns \u03b4k = 1e\u03b1k, then normalizes the weights per message as in Equation 9.Example, the feature vector for the n-gram Dallas could be {id = 1234, variance = 0.56, BIC = 0.01,...}. We designate the feature vector for n-gram-j as the sum of weights per message w (w j), with elements such as Equation (w j), Equity combination (w j). This feature vector is paired with a corresponding real parameter vector vector vector vector vector-1, BIC = 0.01,...}. Setting the feature vector for the single density-gram value (n-density-equal density) is called the sum-g-equal density (w-value)."}, {"heading": "9. APPENDIX: TOKENIZATION ALGORITHM", "text": "This section describes our algorithm for converting a text string into a sequence of n-grams that are used to split the message text, user description, and user location fields into bigrams (i.e. n = 2).13\u03bb could be matched to validation data.1. Divide the string into candidate tokens, each consisting of a sequence of characters with the same Unicode category and font. Candidates who do not belong to the letter category will either be discarded, and letters will be converted to lowercase letters (Thai, Lao, Khmer, and Myanmar, all of which have very little use on Twitter), or they will not really be letters (Common, Inherited).Such scripts represent tokenization difficulties that we will leave for future work.3. These scripts in Chinese characters will be divided in very different ways (Hikana and Japanese characters could be very long)."}, {"heading": "10. APPENDIX: RESULTS OF PILOT EXPERIMENTS", "text": "This section briefly describes three directions that we investigated but did not follow in detail because they seemed to be of limited potential value. \u2022 Unify Fields. The field boundaries were slightly ignored, so we kept these boundaries (i.e. the same n-gram that occurs in different fields is treated as several, separate n-grams). \u2022 Head section. We tried to sort n-grams by frequency and remove various fractions of the most common n-grams. In some cases this resulted in a slightly better MCAE, but also a slight reduction in the success rate; hence we keep common n-grams. \u2022 Map projection. We tried plate carr\u00e9 (i.e. WGS84 longitude and latitude used as planar X and Y coordinates), Miller and Mollweide projections."}, {"heading": "11. REFERENCES", "text": "1. H. Akaike. A New Look at Statistical Model Identification. Automatic Control, 19 (6): 716-723, 1974. 10.1109 / TAC.1974.1100705.2. D. M. Lead and others. Deferred Dirichlet Allocation. [http: / / www.facebook.com / 2012]. [http: / / www.facebook.com / www.facebook.com / www.facebook.com / [http: / / www.facebook.com]. [http: / / www.facebook.com].993-1022, 2003. [http: / dl.acm.org / citation.m.org / citation.cfm? id = 944937.3. L. D. Brown et al. Intervals for a Binomial Ratio and Asymptotic Expansions. Annals of Statistics, 30 (1), 2002. 10.1214 / aos / 1015362189.4."}], "references": [{"title": "A new look at the statistical model identification", "author": ["H. Akaike"], "venue": "Automatic Control, 19(6):716\u2013723,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1974}, {"title": "and others", "author": ["D.M. Ble"], "venue": "Latent Dirichlet allocation. Machine Learning Research, 3:993\u20131022,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "et al", "author": ["L.D. Brow"], "venue": "Confidence intervals for a binomial proportion and asymptotic expansions. Annals of Statistics, 30(1),", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "et al", "author": ["S. Chandr"], "venue": "Estimating Twitter user location using social interactions \u2013 A content based approach. In Proc. Privacy, Security, Risk and Trust (PASSAT),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["H. Chan"], "venue": "@Phillies tweeting from Philly? Predicting Twitter user locations with spatial word usage. In Proc. Advances in Social Networks Analysis and Mining (ASONAM),", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["Z. Chen"], "venue": "You are where you tweet: A content-based approach to geo-locating Twitter users. In Proc. Information and Knowledge Management (CIKM),", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["E. Ch"], "venue": "Friendship and mobility: User movement in location-based social networks. In Proc. Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["C.A. Davis Jr"], "venue": "Inferring the location of Twitter messages based on user relationships. Transactions in GIS, 15(6),", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "How social media will change public health", "author": ["M. Dredze"], "venue": "Intelligent Systems, 27(4):81\u201384,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["J. Eisenstei"], "venue": "A latent variable model for geographic lexical variation. In Proc. Empirical Methods in Natural Language Processing,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "et al", "author": ["J. Eisenstei"], "venue": "Sparse additive generative models of text. In Proc. Machine Learning (ICML),", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Predictive Inference: An Introduction", "author": ["S. Geisser"], "venue": "Chapman and Hall,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "Geo-parsing messages from microtext", "author": ["J. Gelernter", "N. Mushegian"], "venue": "Transactions in GIS, 15(6):753\u2013773,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["M.C. Gonz\u00e1le"], "venue": "Understanding individual human mobility patterns. Nature, 453(7196):779\u2013782,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "et al", "author": ["B. Hech"], "venue": "Tweets from Justin Bieber\u2019s heart: The dynamics of the location field in user profiles. In Proc. CHI,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["L. Hon"], "venue": "Discovering geographical topics in the Twitter stream. In Proc. WWW,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["E. Jone"], "venue": "SciPy: Open source scientific tools for Python,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "et al", "author": ["S. Kinsell"], "venue": "\u201cI\u2019m eating a sandwich in Glasgow\u201d: Modeling locations with Tweets. In Proc. Workshop on Search and Mining User-Generated Content (SMUC),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["J. Mahmu"], "venue": "Where is this Tweet from? Inferring home locations of Twitter users. In Proc. ICWSM,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Leveraging geospatially-oriented social media communications in disaster response", "author": ["S. McClendon", "A.C. Robinson"], "venue": "In Proc. Information Systems for Crisis Response and Management (ISCRAM),", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Finite Mixture Models", "author": ["G. McLachlan", "D. Peel"], "venue": "Wiley & Sons,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "An examination of procedures for determining the number of clusters in a data set", "author": ["G.W. Milligan", "M.C. Cooper"], "venue": "Psychometrika, 50(2):159\u2013179,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1985}, {"title": "Markov chain sampling methods for Dirichlet process mixture models", "author": ["R.M. Neal"], "venue": "Computational and Graphical Statistics, 9(2),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "Modeling locations with social media", "author": ["N. O\u2019Hare", "V. Murdock"], "venue": "Information Retrieval,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Geotagging tweets using their content", "author": ["S. Paradesi"], "venue": "In Proc. Florida Artificial Intelligence Research Society,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["F. Pedregos"], "venue": "Scikit-learn: Machine learning in Python. Machine Learning Research, 12:2825\u20132830,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["S. Rolle"], "venue": "Supervised text-based geolocation using language models on an adaptive grid. In Proc. Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Twitter as medium and message", "author": ["N. Savage"], "venue": "CACM, 54(3):18\u201320,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "et al", "author": ["A. Schul"], "venue": "A multi-indicator approach for geolocalization of tweets. In Proc. ICWSM,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Estimating the dimension of a model", "author": ["G. Schwarz"], "venue": "Annals of Statistics, 6(2):461\u2013464,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1978}, {"title": "Location extraction from social networks with commodity software and online data", "author": ["G. Valkanas", "D. Gunopulos"], "venue": "In Proc. Data Mining Workshops (ICDMW),", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "et al", "author": ["C. Wan"], "venue": "Mining geographic knowledge using location aware topic model. In Proc. Workshop on Geographical Information Retrieval (GIR),", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2007}, {"title": "Simple supervised document geolocation with geodesic grids", "author": ["B. Wing", "J. Baldridge"], "venue": "In Proc. Association for Computational Linguistics (ACL),", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 8, "context": "INTRODUCTION Applications in public health [9], politics [29], disaster management [21], and other domains are increasingly turning to social Internet data to inform policy and intervention strategies.", "startOffset": 43, "endOffset": 46}, {"referenceID": 27, "context": "INTRODUCTION Applications in public health [9], politics [29], disaster management [21], and other domains are increasingly turning to social Internet data to inform policy and intervention strategies.", "startOffset": 57, "endOffset": 61}, {"referenceID": 19, "context": "INTRODUCTION Applications in public health [9], politics [29], disaster management [21], and other domains are increasingly turning to social Internet data to inform policy and intervention strategies.", "startOffset": 83, "endOffset": 87}, {"referenceID": 20, "context": "We propose an inference method based on gaussian mixture models (GMMs) [22].", "startOffset": 71, "endOffset": 75}, {"referenceID": 30, "context": "Researchers have used commercial geocoding services such as Yahoo! Geocoder [32], U.", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "Geological Survey data [26], and Wikipedia [16] to do this.", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "Geological Survey data [26], and Wikipedia [16] to do this.", "startOffset": 43, "endOffset": 47}, {"referenceID": 12, "context": "This technique can be extended to the message text itself by first using a geoparser named-entity recognizer to extract toponyms [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 28, "context": "[30] recently reported accurate results using a scheme which combines multiple geocoding sources, including Internet queries.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In addition to one or more accurate, comprehensive gazetteers, these approaches require careful text cleaning before geocoding is attempted, as grossly erroneous false matches are common [16], and they tend to favor precision over recall (because only toponyms are used as evidence).", "startOffset": 187, "endOffset": 191}, {"referenceID": 5, "context": "apply a variant of naiv\u0308e Bayes to classify messages by city [6], Hecht et al.", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": "use a similar classifier at the state and country level [16], and Kinsella et al.", "startOffset": 56, "endOffset": 60}, {"referenceID": 17, "context": "use language models to classify messages by neighborhood, city, state, zip code, and country [19].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "by combining a hierarchical classifier with many heuristics and gazetteers [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 23, "context": "Other work instead classifies messages into arbitrary regions of fixed [25, 34] or dynamic size [28].", "startOffset": 71, "endOffset": 79}, {"referenceID": 32, "context": "Other work instead classifies messages into arbitrary regions of fixed [25, 34] or dynamic size [28].", "startOffset": 71, "endOffset": 79}, {"referenceID": 26, "context": "Other work instead classifies messages into arbitrary regions of fixed [25, 34] or dynamic size [28].", "startOffset": 96, "endOffset": 100}, {"referenceID": 5, "context": "All of these require aggressively smoothing estimates for regions with few observations [6]", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "[5] classified tweet text by city using GMMs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "3 Geographic topic models These techniques endow traditional topic models [2] with location awareness [33].", "startOffset": 74, "endOffset": 77}, {"referenceID": 31, "context": "3 Geographic topic models These techniques endow traditional topic models [2] with location awareness [33].", "startOffset": 102, "endOffset": 106}, {"referenceID": 9, "context": "developed a cascading topic model that produces region-specific topics and used these topics to infer the locations of Twitter users [10]; followon work uses sparse additive models to combine regionspecific, user-specific, and non-informative topics more efficiently [11, 17].", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "developed a cascading topic model that produces region-specific topics and used these topics to infer the locations of Twitter users [10]; followon work uses sparse additive models to combine regionspecific, user-specific, and non-informative topics more efficiently [11, 17].", "startOffset": 267, "endOffset": 275}, {"referenceID": 15, "context": "developed a cascading topic model that produces region-specific topics and used these topics to infer the locations of Twitter users [10]; followon work uses sparse additive models to combine regionspecific, user-specific, and non-informative topics more efficiently [11, 17].", "startOffset": 267, "endOffset": 275}, {"referenceID": 9, "context": "with a Dirichlet Process mixture [10] and Hong et al.", "startOffset": 33, "endOffset": 37}, {"referenceID": 15, "context": "with K-means clustering [17].", "startOffset": 24, "endOffset": 28}, {"referenceID": 3, "context": ", followers or friends) can aid in location inference [4, 8].", "startOffset": 54, "endOffset": 60}, {"referenceID": 7, "context": ", followers or friends) can aid in location inference [4, 8].", "startOffset": 54, "endOffset": 60}, {"referenceID": 6, "context": "for a different task of user tracking [7].", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "2 Precision In order to evaluate precision, we extend the notion of onedimensional prediction intervals [3, 12] to two dimensions.", "startOffset": 104, "endOffset": 111}, {"referenceID": 11, "context": "2 Precision In order to evaluate precision, we extend the notion of onedimensional prediction intervals [3, 12] to two dimensions.", "startOffset": 104, "endOffset": 111}, {"referenceID": 2, "context": "(This measure is common in the statistical literature for one-dimensional problems [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 9, "context": "For example, an experiment with training size of one day, no gap, and stride of 6 days would schedule 61 tests across our 12 months of data and yield results which were the mean of the 58 3As in prior work [10, 17, 28], we ignore the sampling bias introduced by considering only geotagged tweets.", "startOffset": 206, "endOffset": 218}, {"referenceID": 15, "context": "For example, an experiment with training size of one day, no gap, and stride of 6 days would schedule 61 tests across our 12 months of data and yield results which were the mean of the 58 3As in prior work [10, 17, 28], we ignore the sampling bias introduced by considering only geotagged tweets.", "startOffset": 206, "endOffset": 218}, {"referenceID": 26, "context": "For example, an experiment with training size of one day, no gap, and stride of 6 days would schedule 61 tests across our 12 months of data and yield results which were the mean of the 58 3As in prior work [10, 17, 28], we ignore the sampling bias introduced by considering only geotagged tweets.", "startOffset": 206, "endOffset": 218}, {"referenceID": 6, "context": "Further, they have previously been applied to human mobility patterns [7, 14].", "startOffset": 70, "endOffset": 77}, {"referenceID": 13, "context": "Further, they have previously been applied to human mobility patterns [7, 14].", "startOffset": 70, "endOffset": 77}, {"referenceID": 9, "context": "attribute the poor performance of several of their baselines to this tendency of uninformative words to dilute the predictive power of informative words [10].", "startOffset": 153, "endOffset": 157}, {"referenceID": 0, "context": "Additionally, we tried two metrics designed specifically to test goodness of fit: (14) Akaike information criterion [1] and (15) Bayesian information criterion [31], transformed into weights by subtracting from the maximum observed value.", "startOffset": 116, "endOffset": 119}, {"referenceID": 29, "context": "Additionally, we tried two metrics designed specifically to test goodness of fit: (14) Akaike information criterion [1] and (15) Bayesian information criterion [31], transformed into weights by subtracting from the maximum observed value.", "startOffset": 160, "endOffset": 164}, {"referenceID": 9, "context": "[10], using mean and median SAE (as these were the only metrics reported).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] 373 Eisenstein et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] 845 501 GMM-Opt-ID 870 534 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] 897 432 Eisenstein et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] 900 494 GMM-Err-SAE6 946 588 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "[34] 967 479 GMM-Err-SAE4 985 684 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "To adapt them to our message-based algorithms, we concatenate all tweets from each user, treating them as a single message, as in [17].", "startOffset": 130, "endOffset": 134}, {"referenceID": 10, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": ", GMM-Err-SAE16 has lower median SAE than [11] but a higher mean SAE.", "startOffset": 42, "endOffset": 46}, {"referenceID": 10, "context": "report the best mean SAE but have much higher median SAE [11].", "startOffset": 57, "endOffset": 61}, {"referenceID": 15, "context": "report the best median SAE but do not report mean at all [17].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "[11] may coincide with worse precision or calibration.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "This finding supports Wing & Baldridge\u2019s suggestion [34] that Eisenstein et al.", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "[10] pruned too aggressively by setting this threshold to 40.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": ", [16]), toponyms provide the most important signals; below, we explore this hypothesis in more detail.", "startOffset": 2, "endOffset": 6}, {"referenceID": 14, "context": "found that 66% of user profiles contain some type of geographic information in their location field [16], which is comparable to the 67% success rate of our model using only location field.", "startOffset": 100, "endOffset": 104}, {"referenceID": 12, "context": "These results and those in the previous section offer a key insight into gazetteer-based approaches [13, 16, 26, 30, 32], which favor accuracy over success rate by considering only toponyms.", "startOffset": 100, "endOffset": 120}, {"referenceID": 14, "context": "These results and those in the previous section offer a key insight into gazetteer-based approaches [13, 16, 26, 30, 32], which favor accuracy over success rate by considering only toponyms.", "startOffset": 100, "endOffset": 120}, {"referenceID": 24, "context": "These results and those in the previous section offer a key insight into gazetteer-based approaches [13, 16, 26, 30, 32], which favor accuracy over success rate by considering only toponyms.", "startOffset": 100, "endOffset": 120}, {"referenceID": 28, "context": "These results and those in the previous section offer a key insight into gazetteer-based approaches [13, 16, 26, 30, 32], which favor accuracy over success rate by considering only toponyms.", "startOffset": 100, "endOffset": 120}, {"referenceID": 30, "context": "These results and those in the previous section offer a key insight into gazetteer-based approaches [13, 16, 26, 30, 32], which favor accuracy over success rate by considering only toponyms.", "startOffset": 100, "endOffset": 120}, {"referenceID": 25, "context": "We fit \u03c0 and S independently for each n-gram using the expectation maximization algorithm, as implemented in the Python package scikit-learn [27].", "startOffset": 141, "endOffset": 145}, {"referenceID": 22, "context": "While Dirichlet process mixtures [24] are a common solution, they can scale poorly.", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "For simplicity, we instead investigated a number of heuristic approaches from the literature [23]; in our case, r = min(m, log(n)/2) worked well, where n is the number of points to be clustered, and m is a parameter.", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "The vectors \u03b8 and \u03c6 are passed through the logistic function to ensure the final weights \u03b4 are in the interval [0,1]:", "startOffset": 111, "endOffset": 116}, {"referenceID": 16, "context": "We set Equation 22 to 0 and solve for \u03b8 using L-BFGS as implemented in the SciPy Python package [18].", "startOffset": 96, "endOffset": 100}], "year": 2013, "abstractText": "Social Internet content plays an increasingly critical role in many domains, including public health, disaster management, and politics. However, its utility is limited by missing geographic information; for example, fewer than 1.6% of Twitter messages (tweets) contain a geotag. We propose a scalable, content-based approach to estimate the location of tweets using a novel yet simple variant of gaussian mixture models. Further, because real-world applications depend on quantified uncertainty for such estimates, we propose novel metrics of accuracy, precision, and calibration, and we evaluate our approach accordingly. Experiments on 13 million global, comprehensively multi-lingual tweets show that our approach yields reliable, well-calibrated results competitive with previous computationally intensive methods. We also show that a relatively small number of training data are required for good estimates (roughly 30,000 tweets) and models are quite time-invariant (effective on tweets many weeks newer than the training set). Finally, we show that toponyms and languages with small geographic footprint provide the most useful location signals.", "creator": "LaTeX with hyperref package"}}}