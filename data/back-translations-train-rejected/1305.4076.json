{"id": "1305.4076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-May-2013", "title": "Contractive De-noising Auto-encoder", "abstract": "Autoencoder is a special kind of neural network based on reconstruction. Denoising autoencoder (DAE) is an improved autoencoder which is robust to the input by corrupting the original data first and then reconstructing the original input. And contractive autoencoder (CAE) is another kind of improved autoender to learn robust feature by introducing the Frobenius norm of the Jacobbian matrix of the learned feature with respect to the original input. In this paper, we combine denoising autoencoder and contractive autoencoder, and propose another improved autoencoder, contractive denoising autoencoder (CDAE), which is robust to both the original input and the learned feature. The experiment result shows that our proposed CDAE performed better than both DAE and CAE, proving the effective of our method.", "histories": [["v1", "Fri, 17 May 2013 13:42:49 GMT  (14kb)", "https://arxiv.org/abs/1305.4076v1", null], ["v2", "Thu, 23 May 2013 04:22:44 GMT  (12kb)", "http://arxiv.org/abs/1305.4076v2", null], ["v3", "Thu, 30 May 2013 00:01:45 GMT  (209kb)", "http://arxiv.org/abs/1305.4076v3", null], ["v4", "Mon, 10 Mar 2014 13:41:32 GMT  (425kb)", "http://arxiv.org/abs/1305.4076v4", "Figures edited"], ["v5", "Wed, 23 Apr 2014 11:40:12 GMT  (323kb)", "http://arxiv.org/abs/1305.4076v5", "Figures edited"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["fu-qiang chen", "yan wu", "guo-dong zhao", "jun-ming zhang", "ming zhu", "jing bai"], "accepted": false, "id": "1305.4076"}, "pdf": {"name": "1305.4076.pdf", "metadata": {"source": "CRF", "title": "Contractive De-noising Auto-encoder", "authors": ["Fu-qiang Chen", "Yan Wu", "Guo-dong Zhao", "Jun-ming Zhang", "Ming Zhu", "Jing Bai"], "emails": ["yanwu@tongji.edu.cn"], "sections": [{"heading": null, "text": "Denoising Auto-Encoder (PCS) is an enhanced auto-encoder that is resilient to input by first corrupting the original data and then reconstructing the original input by minimizing the reconstruction error function. And Contractive Auto-Encoder (CAE) is another type of enhanced auto-encoder to learn a robust function by introducing the Frobenius standard of the Jacobin matrix of the learned function with respect to the original input. In this essay, we combine Denoising Auto-Encoder and Contractive Auto-Encoder and propose another improved auto-encoder, the Contractive De-Noising Auto-Encoder (CDAE), which is robust to both the original input and the learned function."}, {"heading": "1 Introduction", "text": "This year, the time has come for an agreement to be reached, with only half of it still to be reached."}, {"heading": "2 CDAE", "text": "In this section, we will present our proposed algorithm, CDAE. First, we will present the formulation of the generic auto-encoder and its two variants, i.e. DAE and CAE."}, {"heading": "2.1 AE", "text": "Let's (in this paper, a column vector, unless otherwise specified) represent a specific input or example function for AE, the size of the input layer or the dimension for input, the size of the hidden layer or the number of neurons in the hidden layer. To encode into the hidden layer, we can use the following activation function: = (+), (1) where () =. Another frequently used activation function is () =.In formula (1), a matrix of \u00d7 X (usually < X), while the hidden layer is a bias vector. After that, + is a column vector, and the sigmoid function maps + element by element, then we can decode a column vector of size \u00d7 1. Then we must decode the hidden layer to the output layer, using the following formula: = (+), where the subscript's function is a \u00d7 reconstruct function itself, and the auto encoding function is the output value."}, {"heading": "2.2 DAE", "text": "In DAE, we must first corrupt the original data. There are two common ways to corrupt the original data: additive isotropic Gaussian noise and binary masking noise. 2 In finite set, the cardinality of a set is the number of elements in the set. For details, refer to http: / / en.wikipedia.org / wiki / CardinalityHere, calling the corrupt version of the original input sample x. And then, for additive isotropic 3 Gaussian noise = + (,), where \u03c3 is a small constant, perhaps less than 0.5 [6] and is the identity matrix, meaning that all elements in the diagonal are equal to 1, while other elements are all equal to 0. And in binary masking noise, a small fraction of the input sample is set to zero, and the fraction can take values to 10%, 25%, and so on. In specific input, we must first correct the object of the automatic version of the DAE (we can subsequently construct the X)."}, {"heading": "2.3 CAE", "text": "The added term is the Frobenius norm4 of the Jacobin matrix of the3 Here, the \"isotropy\" means that the deviations of all input dimensions are the same. 4 For details of the Frobenius norm, the readers are referred to the acquired property in the hidden layer in relation to the original input. The corresponding formula can be represented as follows: Minimize = [(\u2212) + [()]. (4) In formula (4), \u03bb is a parameter and assumes values of 0.1 in this essay. [7] For activation function, the second term in formula (4) can be calculated as follows:"}, {"heading": "2.4 CDAE", "text": "Using the above formula (2-4), we can see that PCS improves traditional AE by corruption reconstruction (first it corrupts the original data and then reconstructs the original data by minimizing the reconstruction loss function), which makes the auto encoder robust to the original input. While CAE improves traditional AE by introducing a penalty term into the object function for each sample, Frobenius proposes the norm of the Jacobin matrix of the learned hidden function in relation to the original input. CAE makes AE robust by changing the object function, which actually makes some neurons in the hidden layer less active to learn more helpfully and robustly. To combine the benefits of CAE and DAE, we propose another improved AE. Here we call it contractive de-noising auto encoder (CDAE), which actually makes some neurons in the hidden layer less active to combine the benefits of CAE and DAE, to make it more helpful and to combine the advantages of CAE."}, {"heading": "3 Experiment", "text": "Considering the fact that we apply support vector machine (SVM) [8] in our experiment, we will briefly introduce SVM in this section. SVM is a commonly accepted classifier in various applications. It is based on core function, dual problem and convex square programming. For two specific inputs, and y, there are three common core functions in support vector machine as follows: (,) = (+ 1), (6) =, (7) = activating square examples (\u2219 \u2212). (8) The formula (6-8) is polynomial core function; Gauss radial base kernel function; Gauss radial base kernel function and kernel function respectively. Among them, the Gauss radial base kernel function is the most popular. And we adopt Gauss radial base function in our experiment. Transforming the original problem into the dual form of the original problem is another feature of SVM ali.If the original problem is difficult to solve, we can sometimes resort to dual problem."}, {"heading": "4 Conclusion", "text": "In this paper, we proposed CDAE, a novel improved auto-encoder by combining CAE and PCS. CAE can activate fewer neurons in the hidden layer to learn more meaningful, abstract and robust features for classification, while PCS is resilient to input by first corrupting the original data and then minimizing the recovery loss function. Our proposed method CDAE combines the benefits of CAE and DAE. The experiment (SVM classification) shows that CDAE outperforms both CAE and DAE, which also shows the effectiveness of CDAE. Since CDAE has performed better at MNIST, we will try CDAE on other issues in the future."}, {"heading": "Acknowledgement", "text": "The authors thank Yuan-fang Ren for the helpful discussion."}], "references": [{"title": "Auto-association by multilayer perceptrons and singular value decomposition", "author": ["H. Bourlard", "Y. Kamp"], "venue": "Biological cybernetics, vol.59", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1988}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, vol. 323,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vo. 313,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.A. Manzagol"], "venue": "Proceedings of the 25th international conference on Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.A. Manzagol"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Contractive auto- encoders: Explicit invariance during feature Extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "A tutorial on support vector machines for pattern recognition\u201d, Data mining and knowledge discovery", "author": ["C.J. Burges"], "venue": "vo.l.2, no", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "LIBSVM: a library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "On optimization methods for deep learning", "author": ["J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "A. Ng", "Q.V. Le"], "venue": "Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "In 1988, Bourlard and Kamp [1] first proposed auto-association, a kind of neural network which replaces the output layer with the data same as the input layer, while in", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "It can be learned by error back propagation (BP) [2].", "startOffset": 49, "endOffset": 52}, {"referenceID": 2, "context": "In 2006, Hinton and Salakhutdinov [3] stacked several layers of auto-encoders to get deep auto-encoders for dimension reduction, and they pre-trained the deep auto-encoder with restricted Boltzmann machine (RBM) [4], which is a special kind of neural network based on Boltzmann machine.", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "In 2006, Hinton and Salakhutdinov [3] stacked several layers of auto-encoders to get deep auto-encoders for dimension reduction, and they pre-trained the deep auto-encoder with restricted Boltzmann machine (RBM) [4], which is a special kind of neural network based on Boltzmann machine.", "startOffset": 212, "endOffset": 215}, {"referenceID": 4, "context": "[5] proposed de-noising auto-encoder (DAE), a kind of improved auto-encoder which first corrupts the original data and then transforms or encodes the corrupted input to the hidden layer and then decodes to the 'corrupted' 1", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "stacked de-noising auto-encoder to form deep neural network to learn useful representations [6], and they showed that SDAE performed better than deep belief nets (DBN) in several cases.", "startOffset": 92, "endOffset": 95}, {"referenceID": 5, "context": "Besides, they found DAE could learn Gaborlike edge detectors from natural image patches and larger stroke detectors from digit images [6].", "startOffset": 134, "endOffset": 137}, {"referenceID": 6, "context": "[7] proposed another kind of improved auto-encoder, contractive auto-encoder (CAE).", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "5 [6].", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": "1 in this paper [7].", "startOffset": 16, "endOffset": 19}, {"referenceID": 7, "context": "Considering that we'll apply support vector machine (SVM) [8] in our experiment, we'll introduce SVM briefly in this section.", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "And we applied one of the most common classifier, support vector machine (SVM) [11] for classification.", "startOffset": 79, "endOffset": 83}, {"referenceID": 9, "context": "In our experiment, the architecture of the deep auto-encoder is 784-200- 100-200-784 and 784-200-50-200-784 [12].", "startOffset": 108, "endOffset": 112}, {"referenceID": 9, "context": "Besides, we initialize the weight matrix with each element in the following range [12]:", "startOffset": 82, "endOffset": 86}], "year": 2014, "abstractText": "Auto-encoder is a special kind of neural network based on reconstruction. De-noising auto-encoder (DAE) is an improved auto-encoder which is robust to the input by corrupting the original data first and then reconstructing the original input by minimizing the reconstruction error function. And contractive auto-encoder (CAE) is another kind of improved auto-encoder to learn robust feature by introducing the Frobenius norm of the Jacobean matrix of the learned feature with respect to the original input. In this paper, we combine de-noising auto-encoder and contractive auto-encoder, and propose another improved auto-encoder, contractive de-noising auto-encoder (CDAE), which is robust to both the original input and the learned feature. We stack CDAE to extract more abstract features and apply SVM for classification. The experiment result on benchmark dataset MNIST shows that our proposed CDAE performed better than both DAE and CAE, proving the effective of our method.", "creator": "\u00fe\u00ff"}}}