{"id": "1305.5078", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-May-2013", "title": "A Comparison of Random Forests and Ferns on Recognition of Instruments in Jazz Recordings", "abstract": "In this paper, we first apply random ferns for classification of real music recordings of a jazz band. No initial segmentation of audio data is assumed, i.e., no onset, offset, nor pitch data are needed. The notion of random ferns is described in the paper, to familiarize the reader with this classification algorithm, which was introduced quite recently and applied so far in image recognition tasks. The performance of random ferns is compared with random forests for the same data. The results of experiments are presented in the paper, and conclusions are drawn.", "histories": [["v1", "Wed, 22 May 2013 10:43:25 GMT  (34kb,D)", "http://arxiv.org/abs/1305.5078v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.IR cs.SD", "authors": ["alicja a wieczorkowska", "miron b kursa"], "accepted": false, "id": "1305.5078"}, "pdf": {"name": "1305.5078.pdf", "metadata": {"source": "CRF", "title": "A Comparison of Random Forests and Ferns on Recognition of Instruments in Jazz Recordings", "authors": ["Alicja A. Wieczorkowska", "Miron B. Kursa", "M.B. Kursa"], "emails": ["alicja@poljap.edu.pl", "M.Kursa@icm.edu.pl"], "sections": [{"heading": null, "text": "Keywords: Music Information Retrieval, Random Ferns, Random ForestThis is a preprint of a paper that was accepted for the ISMIS conference 2012."}, {"heading": "1 Introduction", "text": "The joy of music is great, especially when it is played in a certain area."}, {"heading": "2 Classifiers", "text": "The classifiers we use in our research include random ferns and random forests. RFo performed reasonably well in the instrument identification research we did previously [7], but their training is time-consuming, while the training of RFe is faster. The computational complexity of the classification performed with the pre-trained classifiers is similar (linearly proportional to the number of trees / ferns and their average size), but in the case of ferns there are fewer branches and memory accesses, which should lead to faster classification."}, {"heading": "2.1 Random Forests", "text": "RFo is a classifier consisting of a series of weak, poorly correlated, and unbiased decision trees constructed according to a procedure that minimizes bias and correlations between individual trees [2]. Each tree is built using a different N-element bootstraps example from the Training N-element set. Elements of the bootstraps sample are randomly selected at each stage of tree construction, i.e. for each node of a particular tree in RFo (K < P, often K = \u221a P). The Gini impurity criterion (GIC) is used to find the best split (characteristics) on these K attributes. GIC measures how often an element would be mislabeled if labeled randomly."}, {"heading": "2.2 Random Ferns", "text": "A fern is defined as a simplified binary decision tree of fixed height D (called depth of a fern) and with the requirement that all division criteria are equal at a certain depth i (Ci). Each leaf node of a fern stores the distribution of classes over objects aligned to that node. In this way, a fern can be perceived as a D-dimensional distribution field indexed by a vector of D-splitting criteria, see Figure 1. The fern forest is a collection of Nf ferns. When classifying a new object, each fern in a forest provides a vector of probabilities that this object belongs to certain decision classes. Ferns are treated as independent, so all thosevectors are combined by simple multiplication, and the final classification results for the forest are a class that obtains the highest probability, see Figure 2. While the original RFe implementation [17,18] is used for an object identification problem, we are writing the problem in FR-13]."}, {"heading": "3 Sound Parameterization", "text": "This year it has come to the point where it is only a matter of time before it will happen, until it does."}, {"heading": "3.1 Audio Data", "text": "The audio data we used for both training and testing represent recordings in 44.1kHz / 16-bit format. The training was based on three repositories of individual, isolated sounds from musical instruments, namely McGill University Master Samples [15], The University of Iowa Musical Instrument Samples [23] and RWC Musical Instrument Sound Database [3]. Clarinet, trombone and trumpet sounds were taken from these repositories. Furthermore, we used sousaphone sounds recorded by R. Rudnicki in one of his recording sessions [20], as there were no sousaphone sounds available in the above repositories. Training data was in mono format for RWC data and sousaphone and stereo for the rest of the data. Test data came from R. Rudnicki's jazz band stereo recordings [20], and included the following pieces, the clarinet, trombone, sapphire and stereo."}, {"heading": "4 Methodology of Training of the Classifiers", "text": "We use a set of binary classifiers (RFe or RFo), where each set (which we call a battery) is trained to detect whether or not a target instrument is playing in an audio frame. Target classes are clarinet, trombone, trumpet and sousaphone, i.e. instruments that play in the analyzed jazz tape recordings. Classifiers are trained to identify target instruments when accompanied by other instruments, so we use instrument sounds as input data in training. In preparing the training data, we start with individual isolated sounds of each target instrument. After removing and ending the silence [7], each file representing the entire individual sound is normalized so that the RMS value is one. Then we perform parameterizations and train a classifier to identify each instrument - even if it is accompanied by other sounds."}, {"heading": "5 Experiments and Results", "text": "The RFo and RFe classifiers were next used to identify instruments playing in jazz recordings, as described in Section 3.1. The data were prepared by careful manual labeling [7], based on the first recordings of each instrument track. Accuracy was evaluated using precision and retrieval values, which were weighted by the RMS of a given frame (unlike in our previous paper [7], in which RMS was calculated for frames taken from the instrument channels), in order to reduce the impact of softer frames, which are very difficult to perform reasonable identification of instruments as their volume is close to the noise level. Therefore, our true positive score Tp for an instrument i is a sum of RMS frames, both annotated as and classified as i. Accuracy is calculated by classifying the sum of frames reclassified by RMS as the frames of RMS."}, {"heading": "5.1 Comparison of Random Forests and Random Ferns", "text": "The results of the performance analysis of RFe and RFo models are in Table 1. As can be seen, RFo was higher than RFe for three pieces; Ferns, on the other hand, tend to remember better. However, the overall performance of both classifiers measured with the F note is similar for all pieces. Detailed comparison of the performance analysis of RFe and RFo models for certain instruments is in Table 2. Sousaphone and trumpet are always fairly accurately identified, while trombone usually provides lower precision for all pieces, and clarinet for one piece. Recall is lower than precision, but still much better than our previous results [7]. Again, a reasonably high memory of sousaphone is achieved and is rather good for trumpets, whereas the worst memory of RFo is achieved for trombone sampling. On average, RFe and RFo classification lead 25x and 8x faster than the actual music speed; the RFe means RFo means RFo in 3."}, {"heading": "6 Summary and Conclusions", "text": "The experiments presented in this paper show that the identification of all instruments played in real music recordings is possible with both RFe and RFe-based classifiers, which has led to quite good results. Compared to our previous research [7], we observed an improved recall; we improved the RMS weighting previously calculated for separate instrument channels, and in this work, the RMS of all channels was used together for weighting. Our results are still in need of improvement, but the recall obtained (and precision) is satisfactory as the task of identifying all instruments played in a short segment is difficult and also presents a challenge for human listeners. The measured classification speed of RFe suggests that it is a promising method for performing real-time comments, even on low-performance devices."}], "references": [{"title": "Image Classification using Random Forests and Ferns", "author": ["A. Bosch", "A. Zisserman", "X. Munoz"], "venue": "2007 IEEE 11th International Conference on Computer Vision, pp. 1\u20138. IEEE", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Random Forests", "author": ["L. Breiman"], "venue": "Machine Learning 45, 5\u201332", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "RWC Music Database: Music Genre Database and Musical Instrument Sound Database", "author": ["M. Goto", "H. Hashiguchi", "T. Nishimura", "R. Oka"], "venue": "Proceedings of ISMIR, pp. 229\u2013230", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatic Classification of Pitched Musical Instrument Sounds", "author": ["P. Herrera-Boyer", "A. Klapuri", "M. Davy"], "venue": "Klapuri, A., Davy, M. (eds.): Signal Processing Methods for Music Transcription. Springer Science+Business Media LLC", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Instrument Identification in Polyphonic Music: Feature Weighting to Minimize Influence of Sound Overlaps", "author": ["T. Kitahara", "M. Goto", "K. Komatani", "T. Ogata", "H.G. Okuno"], "venue": "EURASIP J. on Advances in Signal Processing, Vol.2007, pp.1\u201315", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "All That Jazz in the Random Forest", "author": ["E. Kubera", "M.B. Kursa", "W.R. Rudnicki", "R. Rudnicki", "A.A. Wieczorkowska"], "venue": "Kryszkiewicz, M., Rybi\u0144ski, H., Skowron, A., Ra\u015b, Z.W. (eds.): ISMIS 2011. LNAI, vol. 6804, pp. 543-553. Springer, Heidelberg", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Musical Instruments in Random Forest", "author": ["M.B. Kursa", "W. Rudnicki", "A. Wieczorkowska", "E. Kubera", "A. Kubik-Komar"], "venue": "J. Rauch, Z.W. Ra\u015b, P. Berka, T. Elomaa (eds.): ISMIS 2009, LNAI vol. 5722, pp. 281\u2013290 Heidelberg. Springer,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Random Musical Bands Playing in Random Forests", "author": ["M.B. Kursa", "E. Kubera", "W.R. Rudnicki", "Wieczorkowska", "A.A"], "venue": "Szczuka, M., Kryszkiewicz, M., Ramanna, S., Jensen, R., Hu, Q. (eds.): RSCTC 2010. LNAI, vol. 6086, pp. 580\u2013589. Springer, Heidelberg", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Random ferns method implementation for the general-purpose machine learning", "author": ["M.B. Kursa"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Classification and Regression by randomForest", "author": ["A. Liaw", "M. Wiener"], "venue": "R News 2(3), 18\u201322", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2002}, {"title": "Implementation of MFCC vector generation in classification context", "author": ["D. Niewiadomy", "A. Pelikant"], "venue": "J. Applied Computer Science, Vol. 16, No. 2, pp. 55\u201365", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "MUMS \u2014 McGill University Master Samples", "author": ["F. Opolko", "J. Wapnick"], "venue": "CD\u2019s", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1987}, {"title": "Action Recognition Using Randomised Ferns", "author": ["O. Oshin", "A. Gilbert", "J. Illingworth", "R. Bowden"], "venue": "Computer Vision Workshops (ICCV Workshops), 2009 IEEE 12th International Conference, pp. 530\u2013537. IEEE", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2009}, {"title": "Fast Keypoint Recognition in Ten Lines of Code", "author": ["M. \u00d6zuysal", "P. Fua", "V. Lepetit"], "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition, IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast Keypoint Recognition using Random Ferns", "author": ["\u00d6zuysal", "M.M. Calonder", "V. Lepetit", "P. Fua"], "venue": "Image Processing http://dx.doi.org/10.1109/TPAMI.2009.23", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Jazz band", "author": ["R. Rudnicki"], "venue": "Recording and mixing. Arrangements by M. Postle. Clarinet \u2014 J. Murgatroyd, trumpet \u2014 M. Postle, harmonica, trombone \u2014 N. Noutch, sousaphone \u2013 J. M. Lancaster", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Intelligent Music Information Systems: Tools and Methodologies", "author": ["J. Shen", "J. Shepherd", "B. Cui", "Liu", "L. (eds."], "venue": "Information Science Reference, Hershey", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2008}, {"title": "Real-time Detection and Tracking for Augmented Reality on Mobile Phones", "author": ["D. Wagner", "G. Reitmayr", "A. Mulloni", "T. Drummond", "D. Schmalstieg"], "venue": "IEEE transactions on visualization and computer graphics 16 (3), 355\u2013368", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 16, "context": "This area has been broadly explored last years [19], [22], and as a result we can enjoy finding pieces of music through query-by-humming [14], and identify music through query-by-example, including excerpts replayed on mobile devices [21], [24].", "startOffset": 53, "endOffset": 57}, {"referenceID": 3, "context": "[4], [6], [7]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4], [6], [7]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "[4], [6], [7]).", "startOffset": 10, "endOffset": 13}, {"referenceID": 13, "context": "Random ferns are classifiers introduced in 2007 [17] and named as such in 2008 [18].", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "Random ferns are classifiers introduced in 2007 [17] and named as such in 2008 [18].", "startOffset": 79, "endOffset": 83}, {"referenceID": 0, "context": "Random ferns have been applied so far in image classification tasks, including video data [1], [16], and they have also been adjusted to be used on low-end embedded platforms, such as mobile phones [25].", "startOffset": 90, "endOffset": 93}, {"referenceID": 12, "context": "Random ferns have been applied so far in image classification tasks, including video data [1], [16], and they have also been adjusted to be used on low-end embedded platforms, such as mobile phones [25].", "startOffset": 95, "endOffset": 99}, {"referenceID": 17, "context": "Random ferns have been applied so far in image classification tasks, including video data [1], [16], and they have also been adjusted to be used on low-end embedded platforms, such as mobile phones [25].", "startOffset": 198, "endOffset": 202}, {"referenceID": 5, "context": "Additionally, we would like to compare the performance of Random Ferns (RFe) with Random Forests (RFo), which yielded quite good results in our previous research [7], [8], [9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 6, "context": "Additionally, we would like to compare the performance of Random Ferns (RFe) with Random Forests (RFo), which yielded quite good results in our previous research [7], [8], [9].", "startOffset": 167, "endOffset": 170}, {"referenceID": 7, "context": "Additionally, we would like to compare the performance of Random Ferns (RFe) with Random Forests (RFo), which yielded quite good results in our previous research [7], [8], [9].", "startOffset": 172, "endOffset": 175}, {"referenceID": 8, "context": "RFe are simpler and more computationally efficient than RFo [10].", "startOffset": 60, "endOffset": 64}, {"referenceID": 5, "context": "RFo performed quite well in the research on instrument identification we performed before [7], but their training is time consuming, whereas the training of RFe is faster.", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "RFo is a classifier consisting of a set of weak, weakly correlated and non-biased decision trees, constructed using a procedure minimizing bias and correlations between individual trees [2].", "startOffset": 186, "endOffset": 189}, {"referenceID": 9, "context": "In this work, the RFo implementation from the R [13] package randomForest [11] was used.", "startOffset": 74, "endOffset": 78}, {"referenceID": 13, "context": "While the original RFe implementation [17,18] was written for a problem of object detection in images, we use the RFe generalization implemented in the R [13] package rFerns [10]; it trains the fern forest model in the following way.", "startOffset": 38, "endOffset": 45}, {"referenceID": 14, "context": "While the original RFe implementation [17,18] was written for a problem of object detection in images, we use the RFe generalization implemented in the R [13] package rFerns [10]; it trains the fern forest model in the following way.", "startOffset": 38, "endOffset": 45}, {"referenceID": 8, "context": "While the original RFe implementation [17,18] was written for a problem of object detection in images, we use the RFe generalization implemented in the R [13] package rFerns [10]; it trains the fern forest model in the following way.", "startOffset": 174, "endOffset": 178}, {"referenceID": 5, "context": "Our feature vector consists of the following 91 parameters [7]:", "startOffset": 59, "endOffset": 62}, {"referenceID": 10, "context": "The 13 coefficient is the 0-order coefficient of MFCC, corresponding to the logarithm of the energy [12]; \u2013 Zero Crossing Rate; a zero-crossing is a point where the sign of the timedomain representation of the sound wave changes; \u2013 Roll Off \u2014 the frequency below which an experimentally chosen percentage equal to 85% of the accumulated magnitudes of the spectrum is concentrated; parameter originating from speech recognition, where it is applied to distinguish between voiced and unvoiced speech; \u2013 NonMPEG7 - Audio Spectrum Centroid \u2014 a linear scale version of Audio Spectrum Centroid ; \u2013 NonMPEG7 - Audio Spectrum Spread \u2014 a linear scale version of Audio Spectrum Spread ; \u2013 changes (measured as differences) of the above features for a 30 ms subframe of the given 40 ms frame (starting from the beginning of this frame) and the next 30 ms sub-frame (starting with 10 ms shift), calculated for all the features shown above; \u2013 Flux \u2014 the sum of squared differences between the magnitudes of the DFT points calculated for the starting and ending 30 ms sub-frames within the main 40 ms frame; this feature by definition describes changes of magnitude spectrum, thus it is not calculated in a static version.", "startOffset": 100, "endOffset": 104}, {"referenceID": 5, "context": "This feature set was already used for instrument identification purposes using RFo, requiring no feature selection [7], and yielded good results, so we decided to use this feature set in both RFo and RFe classification.", "startOffset": 115, "endOffset": 118}, {"referenceID": 11, "context": "Training was based on three repositories of single, isolated sounds of musical instruments, namely McGill University Master Samples [15], The University of Iowa Musical Instrument Samples [23], and RWC Musical Instrument Sound Database [3].", "startOffset": 132, "endOffset": 136}, {"referenceID": 2, "context": "Training was based on three repositories of single, isolated sounds of musical instruments, namely McGill University Master Samples [15], The University of Iowa Musical Instrument Samples [23], and RWC Musical Instrument Sound Database [3].", "startOffset": 236, "endOffset": 239}, {"referenceID": 15, "context": "Rudnicki in one of his recording sessions [20], since no sousaphone sounds were available in the above mentioned repositories.", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "Rudnicki [20], and include the following pieces played by clarinet, trombone, trumpet, and sousaphone (i.", "startOffset": 9, "endOffset": 13}, {"referenceID": 5, "context": "After removing starting and ending silence [7], each file representing the whole single sound is normalized so that the RMS value equals one.", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "Ground-truth data were prepared through careful manual labelling [7], based on initial recordings of each instrument track separately.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "These measures were weighted by the RMS of a given frame (differently than in our previous work [7], where RMS was calculated for frames taken from instrument channels), in order to diminish the impact of softer frames, which are very hard to perform reasonable identification of instruments, because their loudness is near the noise level.", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "Recall is lower than precision, but still much improved comparing to our previous results [7].", "startOffset": 90, "endOffset": 93}, {"referenceID": 5, "context": "We observed improved recall comparing to our previous research [7]; we improved here the RMS weighting, which was previously calculated for separate instrument channels, and in this work, the RMS of all channels together was used for weighting.", "startOffset": 63, "endOffset": 66}], "year": 2013, "abstractText": "In this paper, we first apply random ferns for classification of real music recordings of a jazz band. No initial segmentation of audio data is assumed, i.e., no onset, offset, nor pitch data are needed. The notion of random ferns is described in the paper, to familiarize the reader with this classification algorithm, which was introduced quite recently and applied so far in image recognition tasks. The performance of random ferns is compared with random forests for the same data. The results of experiments are presented in the paper, and conclusions are drawn.", "creator": "LaTeX with hyperref package"}}}