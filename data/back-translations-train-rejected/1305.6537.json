{"id": "1305.6537", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2013", "title": "A Cooperative Coevolutionary Genetic Algorithm for Learning Bayesian Network Structures", "abstract": "We propose a cooperative coevolutionary genetic algorithm for learning Bayesian network structures from fully observable data sets. Since this problem can be decomposed into two dependent subproblems, that is to find an ordering of the nodes and an optimal connectivity matrix, our algorithm uses two subpopulations, each one representing a subtask. We describe the empirical results obtained with simulations of the Alarm and Insurance networks. We show that our algorithm outperforms the deterministic algorithm K2.", "histories": [["v1", "Tue, 28 May 2013 15:42:51 GMT  (110kb)", "http://arxiv.org/abs/1305.6537v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI", "authors": ["arthur carvalho"], "accepted": false, "id": "1305.6537"}, "pdf": {"name": "1305.6537.pdf", "metadata": {"source": "CRF", "title": "A Cooperative Coevolutionary Genetic Algorithm for Learning Bayesian Network Structures", "authors": ["Arthur Carvalho"], "emails": ["a3carval@cs.uwaterloo.ca"], "sections": [{"heading": null, "text": "ar Xiv: 130 5,65 37v1 [cs.NE] 2 8M ay2 01Categories and Subject Descriptions I.2.8 [Artificial Intelligence]: Problem Solving, Control Methods and Search - Heuristic Methods; I.2.6 [Artificial Intelligence]: LearningGeneral Terms AlgorithmsKeywords Cooperative Coevolutionary Genetic Algorithms, Bayesian Networks, Structure Learning"}, {"heading": "1. INTRODUCTION", "text": "The core of a Bayesian network is a directional acyclic graph, whose nodes represent the random variables, and whose edges indicate the conditional assumptions of independence between the random variables. After construction, a Bayesian network is an efficient tool for performing probabilistic conclusions. A Bayesian network can either be constructed \"by hand\" or learned from direct empirical observations. Manual network construction is usually impractical, since the amounts of digital or hard copies of this work are granted for personal or clinical use without charge, provided that copies are not made for profit or distributed for commercial gain, and the copies bear this disadvantage on the first page."}, {"heading": "2. BAYESIAN NETWORKS", "text": "A Bayesian network is a probabilistic graphical model that represents a common distribution over a series of random variables, X1,.., Xn, using conditional independence properties of that distribution to allow for a compact and natural representation. At the core of a Bayesian network is a directed acyclic graph (DAG) G, in which nodes represent the random variables and edges that correspond to the direct influence of one variable on another. Nodes that are not connected represent variables that are conditionally independent of each other. Let pa (Xi) be the theorem of Xi's parents in G. Thus, there is an edge of each element of pa (Xi) that decomposes the distribution of the common probability distribution in Xi. (X1,., Xn) into a product of conditional probability distributions observed over each variable of Xi in G. There is an edge of each element of pa (Xi)."}, {"heading": "2.1 Bayesian Score Function", "text": "A traditional method of deriving a score function for evaluating Bayesian network structures is based on Bayesian considerations, i.e., whenever we have uncertainty about anything, we should set a distribution above it. In practice, this means a structure before, p (G), which sets a previous probability on different graph structures, and a parameter before, p (D), which sets a probability on different parameters, which is a chart G (D), P (D), P (G), P (G), where the equality follows from Bayes \"theorem, P (D), G (G), D (G), Equos (G), D (D), D (D), D (G), D (G), D (G), D (G), D (G)."}, {"heading": "2.2 Equivalence Classes of Bayesian Networks", "text": "Different Bayesian network structures are equivalent if they encode the same set of conditional independence claims [16, 17]. Consequently, we cannot distinguish between equivalent networks based on observed independence, which suggests that we should not expect to distinguish between equivalent networks based on observed data cases. We say that a Bayesian score function S is equivalent if we have this S (G) = S (G) for all equivalent networks. In other words, score equivalence implies that all networks of the same equivalence class learn the same Score.Heckerman et al. [7] show that the BDe score is equivalent for all equivalent networks. Furthermore, if we insist on the use of dirichlet priors and the decomposability property, then the only way to fulfil the score equivalence is to use the BDe score. This term of network equivalence plays an equally important role in performing a variety of Bayesian structures, which are optimal for performing a multidirectional task."}, {"heading": "3. GENETIC ALGORITHMS", "text": "A genetic algorithm (GA) is a search heuristics inspired by the theory of evolution. Faced with a population of individuals, i.e. potential solutions to an optimization problem, each encoded with a chromosome-like data structure (strings), GAs work by using dedicated operators inspired by the natural evolutionary process (e.g. selection, crossover, and mutation) until a termination criterion is met. The purpose of a GA is to find the individual from the search space (population) with the best \"genetic material.\" The quality of an individual is measured with an objective function, also called fitness function. Since the learning task for structures can be divided into two interdependent subtasks, it is natural to consider a genetic algorithm that develops two different subpopulations (species) cooperatively, with each individual species as part of a complete solution."}, {"heading": "3.1 Cooperative Coevolutionary Genetic Algorithms", "text": "The most common types of coevolution are based on either competition or cooperation. They differ from each other in the way in which the fitness of an individual species is calculated. In competitive coevolution, this fitness is the result of direct competition between different species. In cooperative coevolution, the fitness of a single species results from its collaboration with other species. Our solution to the task of learning Bayesian network structures is based on the cooperative coevolutionary genetic algorithm (CCGA) proposed by Potter and De Jong. CCGA breaks down a problem into a fixed number of subcomponents, each represented by a different subpopulation. For example, if the solution to an optimization problem consists of x-parameters (variables), then a natural decomposition of the x-populations, each of which contains competing values for a particular parameter."}, {"heading": "3.2 Representation", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "D to B, and so on. The value 1 means that an edge exists between the two nodes, while 0 means the opposite. (Bottom Right) The individual species that", "text": "Our representation always produces legal network structures because the cyclicality restriction is never violated, i.e. the underlying graphs will never have cycles. This is due to the fact that individuals from the binary underpopulation are represented by strictly upper triangular matrices and that we implicitly specify nodal orders. Therefore, our representation is correct and consequently we do not need to use repair operators to convert invalid DAGs into valid ones (see for example [5, 10]). Furthermore, we do not need to recognize cycles and thus avoid additional calculations. Our representation is also complete because every single Bayesian network structure can be represented therein."}, {"heading": "3.3 Initialization", "text": "Each individual from the binary subpopulation is semirandomally initialized in such a way that each individual node in a walkthrough has only one parent, with the exception of the root. For the permutation subpopulation, each individual is randomly initialized without additional procedures."}, {"heading": "3.4 Selection", "text": "In short, each individual in a subpopulation is duplicated, the individuals are then mated, and finally the best individual in each pair is selected to produce the offspring. Thus, each individual participates in exactly two tournaments. This operator has better or more equivalent convergence and computational capabilities than other selection operators existing in the GA literature [6]."}, {"heading": "3.5 Crossover", "text": "The selected individuals are paired again, and with the likelihood that each pair will produce two new individuals. Otherwise, the offspring are exact copies of the parents. For the binary subpopulation, we use the traditional two-point crossover, in which each parent is randomly divided into three segments, and then the offspring are generated by taking alternative segments from the parents. For the permutation subpopulation, we must take into account the fact that the absolute position of a node within an individual is important, because this defines the ancestral relationships. A crossover operator that tries to preserve as much information as possible about the absolute positions in which elements occur is the Crossover cycle [12]. This operator begins by dividing the elements into cycles as described in Algorithm 3. Afterwards, the offspring are generated by selecting alternating cycles from each parent. It is important to note that neither the two-point crossover nor the cycle-crossover constraints, such as contradict the cycle-crossover constraints."}, {"heading": "3.6 Mutation", "text": "For the binary subpopulation, we use the traditional bit-flip mutation, in which each gene is reversed with a low probability (i.e. from 1 to 0 or 0 to 1). Intuitively, we add (or remove) a certain edge to a DAG with a low probability of pmb. For the permutation subpopulation, we use the swap mutation. With probability pmp, this operator randomly selects two genes and reverses their allele values. Intuitively, we randomly modify ancestor-descendant relationships within an individual. Interestingly, both mutation operators are closed operators."}, {"heading": "3.7 Replacement", "text": "In order to preserve and use previously found best individuals in subsequent generations, we use an elite replacement strategy. For each subpopulation, the currently best individual is preserved in Ps (gene \u2212 1) and automatically copied to the next generation, Ps (gene). The rest of Ps (gene) is composed of the offspring, with the exception of the child with the worst fitness. In this way, the statistics of the respective best solution for the subpopulation cannot be degraded with generations."}, {"heading": "4. EXPERIMENTS", "text": "We compared the performance of our method with the parameters shown in Table 2 with the deterministic algorithm K2 [4]. The K2 algorithm can be considered a greedy heurist. It starts with the assumption that each node has no parents. Therefore, it adds incrementally to a particular node the parent whose addition increases the value of the resulting structure the most. It stops adding parents when the addition of a parent cannot increase the total value. This algorithm has a major disadvantage, which requires a predefined order of the nodes. In our experiments, we used random permutations as input to K2. This algorithm also requires an upper limit on the number of parents a node can have. We set this value to ten. This is a reasonable value, since none of the structures used in our experiments have a node with ten or more parents. Our implementation of the K2 algorithm is based on the BayyeNet toolbox, well-known [11] we used two insurance networks."}, {"heading": "4.1 Results", "text": "For each data set, we ran both algorithms 100 times. In each run, we stored the best structure found by CCGA. Results related to the alarm network and insurance network are shown in Tables 3 and 4 respectively. In addition to standard descriptive statistics, these tables also contain pvalues from one-tailed t-tests, the alternative hypothesis being that the mean of the results resulting from CCGA is greater than the mean of the results resulting from K2. Below the name of each data set is the score of the original DAG. We note that such scores are not necessarily the highest: the data sets that may not represent all (in) dependencies within the original structure."}, {"heading": "5. RELATED WORK", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "6. CONCLUSION", "text": "We proposed a cooperative coevolutionary genetic algorithm for learning Bayesian network structures from fully observable datasets. Our proposed presentation takes advantage of the fact that this learning problem can be broken down into two interdependent subtasks, i.e. finding an optimal arrangement of the nodes and an optimal connectivity matrix. We compared the performance of our solution with the deterministic algorithm K2 using six datasets generated from two traditional Bayesian networks, the alarm network and the insurance network. The results showed that our solution has obtained better averages for all datasets. There are several exciting directions for future research. First, we find that our algorithm does not restrict the number of parents that a node can have. However, the number of entries in a conditional probability table grows exponentially with the number of parents of the underlying node. Therefore, the statistical costs of adding a parent to a node are very interesting."}, {"heading": "7. REFERENCES", "text": "[1] Beinlich, I. A., Suermondt, H. J., Chavez, R. M., andCooper, G. F. The ALARM Monitoring System: A Case Study with Two Probabilistic Inference Techniques for Belief Networks. In Second European Conference on Artificial Intelligence in Medicine, pp. 247-256, 1989. [2] J. Binder, D. Koller, S. Russell, and K. Kanazawa. Adaptive probabilistic networks with hidden variables. Machine Learning, 29 (2-3), pp. 247-256, 1989. [3] D. M. Chickering, D. Heckerman, and D. Bayersche Netze ist NP-Hard. Technische Bericht, Microsoft Research, 1994. [4] G. F. Cooper and E. Herskovits. A Bayesian method for the induction of probabilistic networks from data. Machine Learning, 9: 309-347, 1992."}], "references": [{"title": "The ALARM Monitoring System: A Case Study with Two Probabilistic Inference Techniques for Belief Networks", "author": ["I.A. Beinlich", "H.J. Suermondt", "R.M. Chavez", "G.F. Cooper"], "venue": "In Second European Conference on Artificial Intelligence in Medicine,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1989}, {"title": "Adaptive probabilistic networks with hidden variables", "author": ["J. Binder", "D. Koller", "S. Russell", "K. Kanazawa"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Learning Bayesian Networks is NP-Hard", "author": ["D.M. Chickering", "D. Geiger", "D. Heckerman"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "A Bayesian method for the induction of probabilistic networks from data", "author": ["G.F. Cooper", "E. Herskovits"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1992}, {"title": "Two Evolutionary Methods for Learning Bayesian Network Structures", "author": ["A. Delaplace", "T. Brouard", "H. Cardot"], "venue": "In Computational Intelligence and Security,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "A comparative analysis of selection schemes used in genetic algorithms", "author": ["D.E. Goldberg", "K. Deb"], "venue": "In Foundations of Genetic Algorithms,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1991}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2009}, {"title": "Learning Bayesian Network Structures by Searching For the Best Ordering With Genetic Algorithms", "author": ["P. Larra\u00f1aga", "C.M.H. Kuijpers", "R.H. Murga", "Y. Yurramendi"], "venue": "IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1996}, {"title": "Structure Learning of Bayesian Networks by Genetic Algorithms: A Performance Analysis of Control Parameters", "author": ["P. Larra\u00f1aga", "M. Poza", "Y. Yurramendi", "R.H. Murga", "C.M.H. Kuijpers"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1994}, {"title": "The Bayes Net Toolbox for Matlab", "author": ["K. Murphy"], "venue": "Computing Science and Statistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "A study of permutation crossover operators on the traveling salesman problem", "author": ["I.M. Oliver", "D.J. Smith", "J.R.C. Holland"], "venue": "In Proceedings of the Second International Conference on Genetic Algorithms and their application,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1987}, {"title": "Probabilistic reasoning in intelligent systems: networks of plausible inference", "author": ["J. Pearl"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "A cooperative coevolutionary approach to function optimization", "author": ["M. Potter", "K. De Jong"], "venue": "In Third Conference on Parallel Problem Solving from Nature,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1994}, {"title": "Counting Unlabeled Acyclic Digraphs", "author": ["R.W. Robinson"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1977}, {"title": "Equivalence and synthesis of causal models", "author": ["T. Verma", "J. Pearl"], "venue": "In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1991}, {"title": "An algorithm for deciding if a set of observed independencies has a causal explanation", "author": ["T. Verma", "J. Pearl"], "venue": "In Proceedings of the Eighth Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1992}, {"title": "Data mining of Bayesian networks using cooperative coevolution", "author": ["M.L. Wong", "S.Y. Lee", "K.S. Leung"], "venue": "Decision Support Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}], "referenceMentions": [{"referenceID": 12, "context": "Bayesian networks are graphical models for representing and reasoning under uncertainty [13].", "startOffset": 88, "endOffset": 92}, {"referenceID": 3, "context": "The task of learning Bayesian network structures from a fully observable data set can be formulated as an optimization problem [4, 7].", "startOffset": 127, "endOffset": 133}, {"referenceID": 6, "context": "The task of learning Bayesian network structures from a fully observable data set can be formulated as an optimization problem [4, 7].", "startOffset": 127, "endOffset": 133}, {"referenceID": 2, "context": "It is proved that this problem is NP-Hard [3].", "startOffset": 42, "endOffset": 45}, {"referenceID": 14, "context": "The number of possible structures is superexponential in the number of nodes [15].", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": ", Xn, by exploiting conditional independence properties of this distribution in order to allow a compact and natural representation [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 12, "context": "Figure 1: Adapted from [13].", "startOffset": 23, "endOffset": 27}, {"referenceID": 7, "context": "Roughly speaking, there are three approaches to learning the structure of a Bayesian network [8].", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "Koller and Friedman [8] show that although this prior is indeed a bias towards certain structures, in fact, it plays a relatively minor role in Equation 2.", "startOffset": 20, "endOffset": 23}, {"referenceID": 3, "context": "If we use a Dirichlet parameter prior for all parameters in the network, then the likelihood P (D|G) can be obtained in closed form [4, 7]:", "startOffset": 132, "endOffset": 138}, {"referenceID": 6, "context": "If we use a Dirichlet parameter prior for all parameters in the network, then the likelihood P (D|G) can be obtained in closed form [4, 7]:", "startOffset": 132, "endOffset": 138}, {"referenceID": 2, "context": "[3] prove that this problem is NP-Hard.", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Robinson [15] shows that r(n), the number of different structures for a network with n nodes, is given by the recursive formula:", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "An interesting property of the BDe score that is often used in order to make this search effective is the score decomposability [8].", "startOffset": 128, "endOffset": 131}, {"referenceID": 7, "context": "Asymptotically, consistent score functions prefer structures that exactly fit the (in)dependencies in the data [8].", "startOffset": 111, "endOffset": 114}, {"referenceID": 15, "context": "Different Bayesian network structures are equivalent when they encode the same set of conditional independence assertions [16, 17].", "startOffset": 122, "endOffset": 130}, {"referenceID": 16, "context": "Different Bayesian network structures are equivalent when they encode the same set of conditional independence assertions [16, 17].", "startOffset": 122, "endOffset": 130}, {"referenceID": 6, "context": "[7] show that the BDe score is equivalent.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Our solution to the task of learning Bayesian network structures from a fully observable data set is based on the cooperative coevolutionary genetic algorithm (CCGA) proposed by Potter and De Jong [14].", "startOffset": 197, "endOffset": 201}, {"referenceID": 13, "context": "The CCGA used in this paper is actually called CCGA-2 by Potter and De Jong [14].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "Thus, our representation is correct and, consequently, we do not need to use repair operators to convert invalid DAGs into valid ones (for example, see [5, 10]).", "startOffset": 152, "endOffset": 159}, {"referenceID": 9, "context": "Thus, our representation is correct and, consequently, we do not need to use repair operators to convert invalid DAGs into valid ones (for example, see [5, 10]).", "startOffset": 152, "endOffset": 159}, {"referenceID": 5, "context": "This operator has better or equivalent convergence and computational time complexity properties than other selection operators that exist in the GA literature [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 11, "context": "preserve as much information as possible about the absolute positions in which elements occur is the cycle crossover [12].", "startOffset": 117, "endOffset": 121}, {"referenceID": 3, "context": "We compared the performance of our method, using the parameters shown in Table 2, with the deterministic algorithm K2 [4].", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "Our implementation of the K2 algorithm is based on the Bayes Net Toolbox [11].", "startOffset": 73, "endOffset": 77}, {"referenceID": 0, "context": "The Alarm network [1] was constructed for monitoring patients in intensive care.", "startOffset": 18, "endOffset": 21}, {"referenceID": 1, "context": "The Insurance network [2] was constructed for evaluating car insurance risks.", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] propose a hybrid genetic algorithm that searches for an optimal ordering of the nodes that is passed on to the K2 algorithm.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] propose a hybrid method that combines characteristics of constrainedbased and score-based approaches.", "startOffset": 0, "endOffset": 4}], "year": 2013, "abstractText": "We propose a cooperative coevolutionary genetic algorithm for learning Bayesian network structures from fully observable data sets. Since this problem can be decomposed into two dependent subproblems, that is to find an ordering of the nodes and an optimal connectivity matrix, our algorithm uses two subpopulations, each one representing a subtask. We describe the empirical results obtained with simulations of the Alarm and Insurance networks. We show that our algorithm outperforms the deterministic algorithm K2.", "creator": "LaTeX with hyperref package"}}}