{"id": "1305.6650", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-May-2013", "title": "Active Sensing as Bayes-Optimal Sequential Decision Making", "abstract": "Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience. An important but poorly understood aspect of sensory processing is the role of active sensing. Here, we present a Bayes-optimal inference and control framework for active sensing, C-DAC (Context-Dependent Active Controller). Unlike previously proposed algorithms that optimize abstract statistical objectives such as information maximization (Infomax) [Butko &amp; Movellan, 2010] or one-step look-ahead accuracy [Najemnik &amp; Geisler, 2005], our active sensing model directly minimizes a combination of behavioral costs, such as temporal delay, response error, and effort. We simulate these algorithms on a simple visual search task to illustrate scenarios in which context-sensitivity is particularly beneficial and optimization with respect to generic statistical objectives particularly inadequate. Motivated by the geometric properties of the C-DAC policy, we present both parametric and non-parametric approximations, which retain context-sensitivity while significantly reducing computational complexity. These approximations enable us to investigate the more complex problem involving peripheral vision, and we notice that the difference between C-DAC and statistical policies becomes even more evident in this scenario.", "histories": [["v1", "Tue, 28 May 2013 22:46:35 GMT  (356kb,D)", "http://arxiv.org/abs/1305.6650v1", "Scheduled to appear in UAI 2013"]], "COMMENTS": "Scheduled to appear in UAI 2013", "reviews": [], "SUBJECTS": "cs.AI cs.CV", "authors": ["sheeraz ahmad", "angela j yu"], "accepted": false, "id": "1305.6650"}, "pdf": {"name": "1305.6650.pdf", "metadata": {"source": "CRF", "title": "Active Sensing as Bayes-Optimal Sequential Decision-Making", "authors": ["Sheeraz Ahmad", "Angela J. Yu"], "emails": [], "sections": [{"heading": null, "text": "An important, but poorly understood, aspect of sensory processing is the role of active scanning. Here we present a Bayes-optimized inference and control framework for active scanning, C-DAC (Context-Dependent Active Controller). In contrast to previously proposed algorithms for optimizing abstract statistical goals such as information maximization (Infomax) [Butko and Movellan, 2010] or single-stage predictive accuracy [Najemnik and Geisler, 2005], our active scanning model directly minimizes a combination of behavioral costs such as time delay, reaction errors, and sensor repositioning costs. Motivated by the geometric properties of CDAC policy, we simulate these algorithms with a simple visual search task to illustrate scenarios where context sensitivity is particularly advantageous and optimization is particularly inadequate in terms of generic statistical goals."}, {"heading": "1 Introduction", "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "2 The Model: C-DAC", "text": "We consider a scenario in which the observer must generate a response based on sequentially observed noisy sensory input (e.g. identification of the target position in a search task or scene category in a classification task), deciding where and for how long the sensory input should be collected."}, {"heading": "2.1 Sensory Processing: Bayesian Inference", "text": "We use a Bayesian generative model to gather the observer's knowledge of the statistical relationship between hidden causes or variables and how they lead to loud sensory inputs, as well as previous beliefs about hidden variables. We assume that they use exact Bayesian conclusions in the detection model to maintain a statistically optimal representation of the hidden state of the world based on the loud data stream. Depending on the destination (s) and the sequence of fixation locations (\u03bb1,.,..), the actor continuously observes id inputs (xt: = {x1,. \u2212, xt}): p (xt | s; \u03bbt) = t-i = 1 p (xi) (xi) (1) (1), where fs (xt) is the probability function. These variables can be scalars or vectors depending on the specific problem."}, {"heading": "2.2 Action Selection: Bayes Risk Minimization", "text": "The action selection component of the active vision is a stochastic control problem in which the agent selects the sensor location and the number of data points collected (Q = Q = cost) and we assume that the agent can dynamically optimize this process based on ongoing data collection and the size of sensory data. (Q = cost = cost = cost = cost = cost) The goal is to find a good decision policy that divides the advanced belief state (xt, \u03bbt) into an action in which A consists of a series of termination actions that pause a response and select a series of continuation actions to determine data points from a specific observation location. Politics produces for each observation sequence (x1,., xt,.), a pause time (number of data points observed), a sequence of fixation options: = (G1,.,.,.) and an eventual target selection."}, {"heading": "3 Case Study: Visual Search", "text": "In this section, we apply the active capture model to a simple visual search task with three locations, where we calculate the exact optimal policy (up to discrediting state space) and compare its performance with statistical strategies [Butko and Movellan, 2010, Najemnik and Geisler, 2005]. Aim and distraction differ in terms of the probability of the observations they receive when we look at them."}, {"heading": "3.1 C-DAC Policy", "text": "For the sake of simplicity, we assume that the observations are distributed binary and Bernoulli (iid determined by target and fixation locations): p (x | s = i; \u03bbt = j) = 1 {i = j} \u03b2x1 (1 \u2212 \u03b21) 1 \u2212 x + 1 {i 6 = j} \u03b2x0 (1 \u2212 \u03b20) 1 \u2212 xThe difficulty of the task is determined by the distinguishability between target and distractor or the difference between \u03b21 and \u03b20. For the sake of simplicity, we assume that the only available holding action is to select the current fixed location: s-ig (\u03c4; \u03bb\u0443 = j) = j. To reduce the parameter space, we insert \u03b20 = 1 \u2212 \u03b21, which is a reasonable assumption that the distractor and the target stimulus differ only in one way (e.g. opposite direction of motion using random point impulses with the coherence of AP points, we first place a short description of the infomax on the infomax)."}, {"heading": "3.2 Greedy MAP Policy", "text": "The greedy MAP algorithm [Najemnik and Geisler, 2005] suggests that agents should try to maximize the expected one-step predictive probability of finding the target. Therefore, the reward function is: Rg (pt, j) = Ext + 1 [max i P (s = i | xt, xt + 1, \u03bbt, \u03bbt + 1 = j)] = Ext + 1 [max i (pit + 1) | xt + 1, \u03bbt + 1 = j] To keep the notations consistent, we define the associated Q factor, cost, and policy as follows: Qg (pt, j) = \u2212 Rg (pt, j) V g (pt, j) = minj Qg (pt, j) \u03bbgt + 1 = argmin j Qg (pt, j)"}, {"heading": "3.3 Infomax Policy", "text": "The Infomax algorithm [Butko and Movellan, 2010] attempts to maximize the information gained from each fixation by minimizing the expected cumulative future entropy. Similar to [Butko and Movellan, 2010], we can define the Q factors, costs, and policies as follows: Qim (pt, j) = T = t + 1 Ext \u2032 [H (pt) | xt \u2032, \u03bbt + 1 = j] V im (pt, j) = min j Qim (pt, j) \u03bbimt + 1 = argmin j Qim (pt, j), where H (p, j) = \u2212 \u2211 i pilogpi is Shannon's entropy. Note that neither the original greedy MAP nor the Infomax algorithm provide a principle-based answer to when the search and response must stop. They must be expanded as soon as the maximum probability that a location containing the target exceeds a fixed threshold."}, {"heading": "3.4 Model Comparison", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able, in which they are able, in which they are able, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "3.5 Approximate Control", "text": "Our model is formally a variant of POMDP to a state of belief MDP is indeed a predominant problem, but we cite a different space or, more precisely, a Mixed Observability Markov Decision Process (MOMDP) [Ong et al., 2010, Araya-Lo \u0301 pez et al., 2010], which differs from ordinary POMDP in this part of the state space, is partially hidden (destination in our case) and partially observable (current fixation point in our case). Generally, POMDPs are difficult to resolve as the decision in each step of time depends on all past actions and observations, creating enormous storage requirements. This is known as a curse of history and is the first major hurdle toward a practical solution. An elegant way to alleviate this is to use beliefs that serve as sufficient statistics for the process history, requiring the maintenance of a single distribution rather than the entire history."}, {"heading": "3.6 Visual Search with Peripheral Vision", "text": "In the very simple three-dimensional visual search problem that we have looked at above, \u03b2\u03b22 does not have the possibility of peripheral vision, or the more general possibility that a sensor is positioned at a particular location, can also have distance-dependent, degraded information about nearby locations. Therefore, we will consider a simple example with peripheral vision (see Fig. 5B), whereby the observer can sanction most likely destinations, which can trigger reduced information about either two (scanning locations at the edges of the triangle) or three (scanning locations in the middle) stimuli. This is motivated by experimental observations that people not only fix most likely destinations, but sometimes also focus locations that lie between two or more destinations. [Findley, Zelinsky et al, 1997] We need an action map, the notion that it is possible to gain information about peripheral locations in the periphery. \""}, {"heading": "3.7 Model Comparison", "text": "We first present the guidelines and, similar to our discussion of simple visual search tasks, show only the C-DAC guidelines that look at the first location (l1) (the other fixation-dependent guidelines are rotationally symmetrical). However, Fig. 6 shows that the C-DAC guidelines differ from the Infomax policy even when no switch costs are taken into account, indicating a more fundamental difference between the two. Note that C-DAC never opts for the parameters used here to look at the l123 center, but for other parameter settings (not shown). However, Infomax never considers the actual potential locations and favors only the middle location before explaining the target location. To compare performance in terms of behavior output, we again examine two scenarios: (1) no switch costs, (2) with switch costs. The threshold for Infomax switches is set so that the accuracy for Infomax switches is adjusted to facilitate a fair comparison."}, {"heading": "4 Discussion", "text": "This year, it has come to the point that it has never come as far as it has this year."}], "references": [{"title": "A closer look at momdps", "author": ["Mauricio Araya-L\u00f3pez", "Vincent Thomas", "Olivier Buffet", "Fran\u00e7ois Charpillet"], "venue": "In Tools with Artificial Intelligence (ICTAI),", "citeRegEx": "Araya.L\u00f3pez et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Araya.L\u00f3pez et al\\.", "year": 2010}, {"title": "On the theory of dynamic programming", "author": ["R Bellman"], "venue": "PNAS, 38(8):716\u2013719,", "citeRegEx": "Bellman.,? \\Q1952\\E", "shortCiteRegEx": "Bellman.", "year": 1952}, {"title": "Strong supervision from weak annotation: Interactive training of deformable part models", "author": ["S Branson", "P Perona", "S Belongie"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "Branson et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Branson et al\\.", "year": 2011}, {"title": "Radial basis functions: theory and implementations, volume 12", "author": ["M.D. Buhmann"], "venue": "Cambridge university press,", "citeRegEx": "Buhmann.,? \\Q2003\\E", "shortCiteRegEx": "Buhmann.", "year": 2003}, {"title": "Infomax control of eyemovements", "author": ["N J Butko", "J R Movellan"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Butko and Movellan.,? \\Q2010\\E", "shortCiteRegEx": "Butko and Movellan.", "year": 2010}, {"title": "A decisionmaking framework for control strategies in probabilistic search", "author": ["Timothy H. Chung", "Joel W. Burdick"], "venue": "Intl. Conference on Robotics and Automation", "citeRegEx": "Chung and Burdick.,? \\Q2007\\E", "shortCiteRegEx": "Chung and Burdick.", "year": 2007}, {"title": "Active gesture recognition using partially observable markov decision processes", "author": ["T. Darrell", "A. Pentland"], "venue": "In Pattern Recognition,", "citeRegEx": "Darrell and Pentland.,? \\Q1996\\E", "shortCiteRegEx": "Darrell and Pentland.", "year": 1996}, {"title": "Gaussian process modelling of dependencies in multiarmed bandit problems", "author": ["L. Dorard", "D. Glowacka", "J. Shawe-Taylor"], "venue": "In Int. Symp. Op. Res,", "citeRegEx": "Dorard et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dorard et al\\.", "year": 2009}, {"title": "Global processing for saccadic eye movements", "author": ["J M Findley"], "venue": "Vision Research,", "citeRegEx": "Findley.,? \\Q1982\\E", "shortCiteRegEx": "Findley.", "year": 1982}, {"title": "Bandit processes and dynamic allocation indices", "author": ["J C Gittins"], "venue": "J. Royal Stat. Soc.,", "citeRegEx": "Gittins.,? \\Q1979\\E", "shortCiteRegEx": "Gittins.", "year": 1979}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["D. Golovin", "A. Krause", "D. Ray"], "venue": "arXiv preprint arXiv:1010.3091,", "citeRegEx": "Golovin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Golovin et al\\.", "year": 2010}, {"title": "Bayesian surprise attracts human attention", "author": ["L Itti", "P Baldi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Itti and Baldi.,? \\Q2006\\E", "shortCiteRegEx": "Itti and Baldi.", "year": 2006}, {"title": "A saliency-based search mechanism for overt and covert shifts of visual attention", "author": ["L Itti", "C Koch"], "venue": "Vision Research,", "citeRegEx": "Itti and Koch.,? \\Q2000\\E", "shortCiteRegEx": "Itti and Koch.", "year": 2000}, {"title": "Nonmyopic multiaspect sensing with partially observable markov decision processes", "author": ["S. Ji", "R. Parr", "L. Carin"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Ji et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2007}, {"title": "Point-based POMDP solvers: Survey and comparative analysis", "author": ["R. Kaplow"], "venue": "PhD thesis, McGill University,", "citeRegEx": "Kaplow.,? \\Q2010\\E", "shortCiteRegEx": "Kaplow.", "year": 2010}, {"title": "Shifts in selective visual attention: towards the underlying neural circuitry", "author": ["C Koch", "S Ullman"], "venue": "Hum. Neurobiol.,", "citeRegEx": "Koch and Ullman.,? \\Q1985\\E", "shortCiteRegEx": "Koch and Ullman.", "year": 1985}, {"title": "Reinforcement learning for sensing strategies", "author": ["C. Kwok", "D. Fox"], "venue": "In Intelligent Robots and Systems,", "citeRegEx": "Kwok and Fox.,? \\Q2004\\E", "shortCiteRegEx": "Kwok and Fox.", "year": 2004}, {"title": "Least-squares policy iteration", "author": ["M.G. Lagoudakis", "R. Parr"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Lagoudakis and Parr.,? \\Q2003\\E", "shortCiteRegEx": "Lagoudakis and Parr.", "year": 2003}, {"title": "An information-theoretic framework for understanding saccadic behaviors", "author": ["Tai Sing Lee", "Stella Yu"], "venue": "In Advance in Neural Information Processing Systems,", "citeRegEx": "Lee and Yu.,? \\Q2000\\E", "shortCiteRegEx": "Lee and Yu.", "year": 2000}, {"title": "Indexability of restless bandit problems and optimality of whittle index for dynamic multichannel access", "author": ["K. Liu", "Q. Zhao"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Liu and Zhao.,? \\Q2010\\E", "shortCiteRegEx": "Liu and Zhao.", "year": 2010}, {"title": "Computationally feasible bounds for partially observed markov decision processes", "author": ["W.S. Lovejoy"], "venue": "Operations research,", "citeRegEx": "Lovejoy.,? \\Q1991\\E", "shortCiteRegEx": "Lovejoy.", "year": 1991}, {"title": "A reinforcement learning model of selective visual attention", "author": ["S Minut", "S Mahadevan"], "venue": "In Proceedings of the Fifth International Conference on Autonomous Agents, Montreal,", "citeRegEx": "Minut and Mahadevan.,? \\Q2001\\E", "shortCiteRegEx": "Minut and Mahadevan.", "year": 2001}, {"title": "Active m-ary sequential hypothesis testing", "author": ["M. Naghshvar", "T. Javidi"], "venue": "In Information Theory Proceedings (ISIT),", "citeRegEx": "Naghshvar and Javidi.,? \\Q2010\\E", "shortCiteRegEx": "Naghshvar and Javidi.", "year": 2010}, {"title": "Optimal eye movement strategies in visual search", "author": ["J Najemnik", "W S Geisler"], "venue": "Nature, 434(7031):387\u2013", "citeRegEx": "Najemnik and Geisler.,? \\Q2005\\E", "shortCiteRegEx": "Najemnik and Geisler.", "year": 2005}, {"title": "Planning under uncertainty for robotic tasks with mixed observability", "author": ["Sylvie CW Ong", "Shao Wei Png", "David Hsu", "Wee Sun Lee"], "venue": "The International Journal of Robotics Research,", "citeRegEx": "Ong et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ong et al\\.", "year": 2010}, {"title": "Multiarmed bandit problems with dependent arms", "author": ["S. Pandey", "D. Chakrabarti", "D. Agarwal"], "venue": "In Proceedings of the 24th international conference on Machine learning,", "citeRegEx": "Pandey et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pandey et al\\.", "year": 2007}, {"title": "Anytime pointbased approximations for large pomdps", "author": ["J. Pineau", "G. Gordon", "S. Thrun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pineau et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2006}, {"title": "Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703", "author": ["W.B. Powell"], "venue": null, "citeRegEx": "Powell.,? \\Q2007\\E", "shortCiteRegEx": "Powell.", "year": 2007}, {"title": "Planning to see: A hierarchical approach to planning visual actions on a robot using pomdps", "author": ["M Sridharan", "J Wyatt", "R Dearden"], "venue": "Artificial Intelligence,", "citeRegEx": "Sridharan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sridharan et al\\.", "year": 2010}, {"title": "Optimal policies for a class of restless multiarmed bandit scheduling problems with applications to sensor management", "author": ["R. Washburn", "M. Schneider"], "venue": "Journal of Advances in Information Fusion", "citeRegEx": "Washburn and Schneider.,? \\Q2008\\E", "shortCiteRegEx": "Washburn and Schneider.", "year": 2008}, {"title": "Restless bandits: activity allocation in a changing world", "author": ["P Whittle"], "venue": "J. App. Probability,", "citeRegEx": "Whittle.,? \\Q1988\\E", "shortCiteRegEx": "Whittle.", "year": 1988}, {"title": "Gaussian processes for regression", "author": ["C K I Williams", "C E Rasmussen"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Williams and Rasmussen.,? \\Q1996\\E", "shortCiteRegEx": "Williams and Rasmussen.", "year": 1996}, {"title": "Yarbus. Eye Movements and Vision", "author": ["F A"], "venue": null, "citeRegEx": "A,? \\Q1967\\E", "shortCiteRegEx": "A", "year": 1967}, {"title": "Eye movements reveal the spatio-temporal dynamics of visual search", "author": ["G J Zelinsky", "R P Rao", "M M Hayhoe", "D H Ballard"], "venue": null, "citeRegEx": "Zelinsky et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Zelinsky et al\\.", "year": 1997}], "referenceMentions": [{"referenceID": 4, "context": "Unlike previously proposed algorithms that optimize abstract statistical objectives such as information maximization (Infomax) [Butko and Movellan, 2010] or", "startOffset": 127, "endOffset": 153}, {"referenceID": 23, "context": "one-step look-ahead accuracy [Najemnik and Geisler, 2005], our active sensing model directly minimizes a combination of behavioral costs, such as temporal delay, response error, and sensor repositioning cost.", "startOffset": 29, "endOffset": 57}, {"referenceID": 23, "context": "Separately, in another active formulation, it has been proposed that saccades are chosen to maximize the greedy, one-step look-ahead probability of finding the target (greedy MAP), conditioned on self knowledge about visual acuity map [Najemnik and Geisler, 2005].", "startOffset": 235, "endOffset": 263}, {"referenceID": 1, "context": "Bellman\u2019s dynamic programming equation [Bellman, 1952] tells us that the problem is optimized if at each time point, the agent chooses the action associated with the lowest expected cost (the Q-factor for that action), given his current knowledge or belief state, pt.", "startOffset": 39, "endOffset": 54}, {"referenceID": 23, "context": "The greedy MAP algorithm [Najemnik and Geisler, 2005] suggests that agents should try to maximize the expected one-step look-ahead probability of finding the", "startOffset": 25, "endOffset": 53}, {"referenceID": 4, "context": "The infomax algorithm [Butko and Movellan, 2010] tries to maximize the information gained from each fixation, by minimizing the expected cumulative future entropy.", "startOffset": 22, "endOffset": 48}, {"referenceID": 4, "context": "Similar to [Butko and Movellan, 2010], we can define the Q-factors, cost and the policy as:", "startOffset": 11, "endOffset": 37}, {"referenceID": 20, "context": "of the grid is increased [Lovejoy, 1991].", "startOffset": 25, "endOffset": 40}, {"referenceID": 3, "context": "[Buhmann, 2003].", "startOffset": 0, "endOffset": 15}, {"referenceID": 31, "context": "We thus also implement a nonparametric variation of the algorithm, whereby we use Gaussian Process Regression (GPR) [Williams and Rasmussen, 1996] to estimate the value function (step 4, 6 and 8).", "startOffset": 116, "endOffset": 146}, {"referenceID": 6, "context": "In [Darrell and Pentland, 1996], the problem of active gesture recognition is studied, by using historic state representation and nearest neighbor Q-function approximation.", "startOffset": 3, "endOffset": 31}, {"referenceID": 16, "context": "Sensing strategies for robots in RoboCup competition is studied in [Kwok and Fox, 2004], which uses states augmented with associated uncertainty and model-free Least Square Policy Iteration (LSPI) ap-", "startOffset": 67, "endOffset": 87}, {"referenceID": 17, "context": "proximation [Lagoudakis and Parr, 2003].", "startOffset": 12, "endOffset": 39}, {"referenceID": 13, "context": "Context dependent goals are considered in [Ji et al., 2007] and [Naghshvar and Javidi, 2010].", "startOffset": 42, "endOffset": 59}, {"referenceID": 22, "context": ", 2007] and [Naghshvar and Javidi, 2010].", "startOffset": 12, "endOffset": 40}, {"referenceID": 26, "context": "The former concentrates on multi-sensor multi-aspect sensing using Point Based Value Iteration (PBVI) approximation [Pineau et al., 2006].", "startOffset": 116, "endOffset": 137}, {"referenceID": 21, "context": "A Reinforcement Learning paradigm where reward is not dependent on information gain but on how close a saccade brings the target to the optical axis has also been proposed [Minut and Mahadevan, 2001].", "startOffset": 172, "endOffset": 199}, {"referenceID": 28, "context": "trol strategies like random search, sequential sweeping search, \u201cDrosophila-inspired\u201d search [Chung and Burdick, April 2007] and hierarchical POMDPs for visual action planning [Sridharan et al., 2010] have also been proposed.", "startOffset": 176, "endOffset": 200}, {"referenceID": 4, "context": "the digital eye [Butko and Movellan, 2010]).", "startOffset": 16, "endOffset": 42}, {"referenceID": 9, "context": "A related problem domain, not typically studied as POMDP or MDP, is Multi-Armed Bandits (MAB) [Gittins, 1979].", "startOffset": 94, "endOffset": 109}, {"referenceID": 30, "context": "Firstly, the problem is an instance of restless bandits [Whittle, 1988], where the state of an arm can change even when it is not played.", "startOffset": 56, "endOffset": 71}, {"referenceID": 29, "context": "[Washburn and Schneider, 2008] and [Liu and Zhao, 2010]), but not optimal in general.", "startOffset": 0, "endOffset": 30}, {"referenceID": 19, "context": "[Washburn and Schneider, 2008] and [Liu and Zhao, 2010]), but not optimal in general.", "startOffset": 35, "endOffset": 55}, {"referenceID": 25, "context": "arms for specific structure of correlation, like clustered arms [Pandey et al., 2007] and Gaussian process bandits [Dorard et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 7, "context": ", 2007] and Gaussian process bandits [Dorard et al., 2009], but so far there is no general strategy for handling this scenario.", "startOffset": 37, "endOffset": 58}, {"referenceID": 10, "context": "This problem is investigated in [Golovin et al., 2010], and a near-optimal greedy solution is proposed along with performance guarantees.", "startOffset": 32, "endOffset": 54}], "year": 2013, "abstractText": "Sensory inference under conditions of uncertainty is a major problem in both machine learning and computational neuroscience. An important but poorly understood aspect of sensory processing is the role of active sensing. Here, we present a Bayes-optimal inference and control framework for active sensing, C-DAC (Context-Dependent Active Controller). Unlike previously proposed algorithms that optimize abstract statistical objectives such as information maximization (Infomax) [Butko and Movellan, 2010] or one-step look-ahead accuracy [Najemnik and Geisler, 2005], our active sensing model directly minimizes a combination of behavioral costs, such as temporal delay, response error, and sensor repositioning cost. We simulate these algorithms on a simple visual search task to illustrate scenarios in which contextsensitivity is particularly beneficial and optimization with respect to generic statistical objectives particularly inadequate. Motivated by the geometric properties of the CDAC policy, we present both parametric and non-parametric approximations, which retain context-sensitivity while significantly reducing computational complexity. These approximations enable us to investigate a more complex search problem involving peripheral vision, and we notice that the performance advantage of C-DAC over generic statistical policies is even more evident in this scenario.", "creator": "LaTeX with hyperref package"}}}