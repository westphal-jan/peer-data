{"id": "1305.7111", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-May-2013", "title": "Test cost and misclassification cost trade-off using reframing", "abstract": "Many solutions to cost-sensitive classification (and regression) rely on some or all of the following assumptions: we have complete knowledge about the cost context at training time, we can easily re-train whenever the cost context changes, and we have technique-specific methods (such as cost-sensitive decision trees) that can take advantage of that information. In this paper we address the problem of selecting models and minimising joint cost (integrating both misclassification cost and test costs) without any of the above assumptions. We introduce methods and plots (such as the so-called JROC plots) that can work with any off-the-shelf predictive technique, including ensembles, such that we reframe the model to use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then deployed by setting missing values on the attributes that are deemed ineffective for reducing the joint cost. As the number of feature configuration combinations grows exponentially with the number of features we introduce quadratic methods that are able to approximate the optimal configuration and model choices, as shown by the experimental results.", "histories": [["v1", "Thu, 30 May 2013 13:52:32 GMT  (332kb,D)", "http://arxiv.org/abs/1305.7111v1", "Keywords: test cost, misclassification cost, missing values, reframing, ROC analysis, operating context, feature configuration, feature selection"]], "COMMENTS": "Keywords: test cost, misclassification cost, missing values, reframing, ROC analysis, operating context, feature configuration, feature selection", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["celestine periale maguedong-djoumessi", "jos\\'e hern\\'andez-orallo"], "accepted": false, "id": "1305.7111"}, "pdf": {"name": "1305.7111.pdf", "metadata": {"source": "CRF", "title": "Test cost and misclassification cost trade-off using reframing", "authors": ["Jos\u00e9 Hern\u00e1ndez-Orallo"], "emails": ["cemadj@gmail.com", "jorallo@dsic.upv.es"], "sections": [{"heading": null, "text": "Many cost-sensitive classification (and regression) solutions are based on some or all of the following assumptions: We have full knowledge of the cost context at the time of training; we can easily retrain when the cost context changes; and we have technology-specific methods (such as cost-sensitive decision trees) that can use this information. In this paper, we address the problem of selecting models and minimizing community costs (integrating both misclassification costs and testing costs) without the above assumptions. We introduce methods and charts (such as the so-called JROC diagrams) that can work with any common prediction technique, including ensembles, so that we redesign the model so that we use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then used by setting missing values for the attributes to reduce the number of attributes that we consider effective to reduce the number of available attributes."}, {"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Motivation", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "3 Reframing the model with missing values on purpose", "text": "This is the core of the selection techniques. In fact, the performance of the model can be improved even further by using some of the original attributes. Although we think about the cost, most of the work to minimize the cost has been done using this approach. However, we can also take into account that the model has already been trained (with possibly all the attributes) and that we want to use the model with fewer available attributes when we cannot afford it. \"Some of the tests included in the model are important to say that we consider models developed by experts or by automated analysis tools, or both. Further education can be a poor choice if we have an expert model (human-made) when we use ensembles or other techniques with high training costs, when the budget context is no longer available."}, {"heading": "4 The MC/TC trade-off: JROC plots", "text": "The diagrams we have seen in the previous section are very informative for a given operating context. If the diagrams are drawn on a validation set, we simply choose the model and the functional configuration that minimizes the JC. However, there are some problems with the previous diagrams: If we have multiple models, the graph becomes too full. Also, the curves are usually too sawtooth. Finally, we need to change the curves when we change the operating context. While some of the above problems are difficult to solve completely, mainly because we have m + c (c \u2212 1) \u2212 1 degree of freedom, we can see a more convenient alternative that minimizes these problems. The alternative is based on a graphical visualization of the MC / TC trade-off, which we call JROC. Definition 5. A JROC chart shows the testing costs (TC) on the x axis and the misclassification costs (MC) on the y \u2212 is.Figure 4 shows JROC plots we have for Iris and Iris."}, {"heading": "5 Approximating the JROC hull", "text": "The previous procedure allows us to determine the best model and configuration given the operating conditions. We just need to calculate where all the points are located, calculate the convex hull and find the one that corresponds to each possible \u03b1 in the application time. While this is easy to do, there is a big problem. However, as the number of attributes increases, the number of points for a model exponentially: a grid for m attributes has 2 nodes. Thus, we need to explore some ways to increase the number of configurations that are evaluated, while we still have a good approximation of attributes subsets, and calculating their expected TC and MC would be impracticable."}, {"heading": "6 Experiments", "text": "To do this, we consider six UCI repository datasets with a number of attributes ranging from 4 to 11, as in Table 1. We could not use larger datasets in this first experiment because the Complete Method is too slow, as the number of elements examined in the grid increases exponentially. We consider two different contexts: a uniform context \u03b8u and a random context in which each value of the misclassification cost matrix and the test cost vector is achieved by multiplying the original value of the uniform context with k, where k = e\u03b2 \u00d7 (k0 \u2212 0.5), k0 as a random number between 0 and 1 using a uniform distribution, and \u03b2 is a factor of how irregular we want the vector and matrix for the following experiments. We set \u03b2 = 10 for the following experiments."}, {"heading": "6.1 Uniform context", "text": "First, we give the results for the uniform context. Table 2 shows the mean and standard deviation of the results for each data set and each method. We see that Full cannot be improved by the other methods, as it explores all the possibilities. In general, the RND method is worse than the backward methods, except for record 1 (the smaller, iris, where the number of configurations examined is 4 x 5 x 1 x 11 before a total of 16, which is not a big difference). In fact, for the large data sets, where the difference in the configuration studied grows exponentially, we see that the backward methods come close to the complete methods that provide support for this approximation. Table 3 shows the results aggregated for all data sets, but shows each value of \u03b1. This means (8 rows with 10 repetitions) that we cannot detect the differences between the two data sets. In this case, we can see that the backward methods are consistently better than the RND method and are close to the full method."}, {"heading": "6.2 Variable context", "text": "To see what happens in a more realistic situation, let's look at the results for the non-uniform context. Table 5 shows the mean and standard deviation of the results for each data set and each method. Here, we see that not all backward-facing methods are equal, but interestingly, we see that BMC is now consistently better than RND for all data sets. Table 6 again shows the results for all data sets aggregated, but each value of \u03b1. This means (8 data sets with 10 repetitions) Now we see that the results are not particularly different depending on \u03b1.Finally, if we look at the big picture and apply a statistical test, in Table 7 we see that the methods applied backwards are better than the RND method, but now we find a difference between them. In fact, BTC is significantly better than BMC and BJC. Although some more definitive conclusions about which method is generally the best, more data sets and repetitions would be required (although the results here are significant enough)."}, {"heading": "7 Conclusion", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "Acknowledgements", "text": "This work was supported by the MEC / MINECO projects CONSOLIDER-INGENIO CSD2007-00022 and TIN 2010-21062-C02-02, the GVA project Prometeo / 2008 / 051, the COST - European Cooperation in the Field of Scientific and Technical Research IC0801 AT and the REFRAME project of the European Coordinated Research on Long-term Challenges in Information and Communication Sciences & Technologies ERA-Net (CHIST-ERA) and funded by the respective national research councils and ministries."}], "references": [{"title": "Quantification via probability estimators", "author": ["A. Bella", "C. Ferri", "J. Hern\u00e1ndez-Orallo", "M.J. Ram\u0131\u0301rez-Quintana"], "venue": "In 2010 IEEE International Conference on Data Mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Aggregative quantification for regression", "author": ["A. Bella", "C. Ferri", "J. Hern\u00e1ndez-Orallo", "M.J. Ram\u0131\u0301rez-Quintana"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Ram\u0131\u0301rez-Quintana. Using negotiable features for prescription problems", "author": ["A. Bella", "C. Ferri", "J. Hern\u00e1ndez-Orallo", "M.J"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Ram\u0131\u0301rez-Quintana. Estimating the class probability threshold without training data", "author": ["R. Blanco-Vega", "C. Ferri-Ram\u0131\u0301rez", "J. Hern\u00e1ndez-Orallo", "M.J"], "venue": "ROC Analysis in Machine Learning, ICML2006 workshop,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Analysing the trade-off between comprehensibility and accuracy in mimetic models", "author": ["R. Blanco-Vega", "J. Hern\u00e1ndez-Orallo", "M.J. Ram\u0131\u0301rez-Quintana"], "venue": "In Discovery Science, 7th International Conference,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Knowledge acquisition through machine learning: minimising expert\u2019s effort", "author": ["R. Blanco-Vega", "J. Hern\u00e1ndez-Orallo", "M.J. Ram\u0131\u0301rez-Quintana"], "venue": "Fourth International Conference on Machine Learning and Applications,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "The use of the area under the ROC curve in the evaluation of machine learning algorithms", "author": ["A.P. Bradley"], "venue": "Pattern Recognition,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Statistical comparisons of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Cost Curves: An Improved Method for Visualizing Classifier Performance", "author": ["C. Drummond", "R.C. Holte"], "venue": "Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "The foundations of cost-sensitive learning", "author": ["C. Elkan"], "venue": "In International Joint Conference on Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Simple mimetic classifiers", "author": ["V. Estruch", "C. Ferri", "J. Hernandez-Orallo", "M. Ramirez-Quintana"], "venue": "Machine Learning and Data Mining in Pattern Recognition,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2003}, {"title": "An introduction to ROC analysis", "author": ["T. Fawcett"], "venue": "Pattern Recognition Letters,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Delegating classifiers", "author": ["C. Ferri", "P.A. Flach", "J. Hern\u00e1ndez-Orallo"], "venue": "Machine Learning, Proceedings of the Twenty-first International Conference (ICML", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Cautious classifiers", "author": ["C. Ferri", "J. Hern\u00e1ndez-Orallo"], "venue": "Proceedings of the 1st International Workshop on ROC Analysis in Artificial Intelligence", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "An experimental comparison of performance measures for classification", "author": ["C. Ferri", "J. Hern\u00e1ndez-Orallo", "R. Modroiu"], "venue": "Pattern Recognition Let.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Ram\u0131\u0301rez-Quintana. From ensemble methods to comprehensible models", "author": ["C. Ferri", "J. Hern\u00e1ndez-Orallo"], "venue": "Discovery Science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Machine Learning: The Art and Science of Algorithms that Make Sense of Data", "author": ["P. Flach"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Decision support for data mining", "author": ["P. Flach", "H. Blockeel", "C. Ferri", "J. Hern\u00e1ndez-Orallo", "J. Struyf"], "venue": "Data Mining and Decision Support,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "A coherent interpretation of AUC as a measure of aggregated classification performance", "author": ["P. Flach", "J. Hern\u00e1ndez-Orallo", "C. Ferri"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "An extension on statistical comparisons of classifiers over multiple data sets for all pairwise comparisons", "author": ["S. Gar\u0107\u0131a", "F. Herrera"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "The weka data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "A graphical analysis of cost-sensitive regression problems", "author": ["J. Hern\u00e1ndez-Orallo"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "The 1st workshop on ROC analysis in artificial intelligence (ROCAI-2004)", "author": ["J. Hern\u00e1ndez-Orallo", "C. Ferri", "N. Lachiche", "P. Flach"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Brier curves: a new cost-based visualisation of classifier performance", "author": ["J. Hern\u00e1ndez-Orallo", "P. Flach", "C. Ferri"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "A unified view of performance metrics: Translating threshold choice into expected classification loss", "author": ["J. Hern\u00e1ndez-Orallo", "P. Flach", "C. Ferri"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "ROC curves in cost space", "author": ["J. Hern\u00e1ndez-Orallo", "P. Flach", "C. Ferri"], "venue": "Machine Learning,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Evaluating Learning Algorithms: A Classification Perspective", "author": ["N. Japkowicz", "M. Shah"], "venue": "Cambridge Univ Pr,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Feature space theory - a mathematical foundation for data mining", "author": ["H.X. Li", "L.D. Xu"], "venue": "Knowledgebased systems,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2001}, {"title": "Decision trees with minimal costs", "author": ["C.X. Ling", "Q. Yang", "J. Wang", "S. Zhang"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2004}, {"title": "A survey of cost-sensitive decision tree induction algorithms", "author": ["S. Lomax", "S.l Vadera"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Selecting features in microarray classification using ROC curves", "author": ["H. Mamitsuka"], "venue": "Pattern Recognition,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "Optimizing abstaining classifiers using ROC analysis", "author": ["T. Pietraszek"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "Better decisions through science", "author": ["J.A. Swets", "R.M. Dawes", "J. Monahan"], "venue": "Scientific American,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}, {"title": "Types of cost in inductive concept learning", "author": ["P. Turney"], "venue": "Canada National Research Council Publications Archive,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2000}, {"title": "Missing is useful: missing values in cost-sensitive decision trees", "author": ["S. Zhang", "Z. Qin", "C.X. Ling", "S. Sheng"], "venue": "IEEE transactions on knowledge and data engineering,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Missing value estimation for mixed-attribute data sets. Knowledge and Data Engineering", "author": ["X. Zhu", "S. Zhang", "Z. Jin", "Z. Zhang", "Z. Xu"], "venue": "IEEE Transactions on,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2011}], "referenceMentions": [{"referenceID": 27, "context": "1 Introduction The feature space (including both input and output variables) characterises a data mining problem [29].", "startOffset": 113, "endOffset": 117}, {"referenceID": 33, "context": "This frequently represents a utility or cost-sensitive learning dilemma [35, 11] between misclassification (or regression error) costs and tests costs, both being integrated into a joint cost.", "startOffset": 72, "endOffset": 80}, {"referenceID": 9, "context": "This frequently represents a utility or cost-sensitive learning dilemma [35, 11] between misclassification (or regression error) costs and tests costs, both being integrated into a joint cost.", "startOffset": 72, "endOffset": 80}, {"referenceID": 35, "context": "One possible option is known as missing value imputation [37], but this approach is not usually appropriate when test costs are considered.", "startOffset": 57, "endOffset": 61}, {"referenceID": 34, "context": "First, imputing missing values \u201cis regarded as unnecessary for cost-sensitive learning that also considers the test costs\u201d [36].", "startOffset": 123, "endOffset": 127}, {"referenceID": 28, "context": "Decision trees are the usual choice [30, 36, 31] because the use of attributes can be customised in many different ways.", "startOffset": 36, "endOffset": 48}, {"referenceID": 34, "context": "Decision trees are the usual choice [30, 36, 31] because the use of attributes can be customised in many different ways.", "startOffset": 36, "endOffset": 48}, {"referenceID": 29, "context": "Decision trees are the usual choice [30, 36, 31] because the use of attributes can be customised in many different ways.", "startOffset": 36, "endOffset": 48}, {"referenceID": 32, "context": "This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]).", "startOffset": 63, "endOffset": 83}, {"referenceID": 17, "context": "This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]).", "startOffset": 63, "endOffset": 83}, {"referenceID": 22, "context": "This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]).", "startOffset": 63, "endOffset": 83}, {"referenceID": 11, "context": "This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]).", "startOffset": 63, "endOffset": 83}, {"referenceID": 30, "context": "This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]).", "startOffset": 63, "endOffset": 83}, {"referenceID": 21, "context": "This is exactly the way ROC analysis works (for classification [34, 19, 24, 13, 32] and for regression [23]).", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "Throughout the paper we will use several classifiers from Weka [22].", "startOffset": 63, "endOffset": 67}, {"referenceID": 32, "context": "Reuse of learned models is of critical importance in the majority of knowledgeintensive application areas, particularly because the operating context can be expected to vary from training to deployment and we need to make the best decision according to that context [34, 19].", "startOffset": 266, "endOffset": 274}, {"referenceID": 17, "context": "Reuse of learned models is of critical importance in the majority of knowledgeintensive application areas, particularly because the operating context can be expected to vary from training to deployment and we need to make the best decision according to that context [34, 19].", "startOffset": 266, "endOffset": 274}, {"referenceID": 33, "context": "In fact, as Turney [35] points out, we can only rationally determine whether it is worthwhile to pay the cost of test when we know the cost of misclassification errors.", "startOffset": 19, "endOffset": 23}, {"referenceID": 28, "context": "Also, if we think about costs, most works on minimising costs have taken this approach [30, 36, 31].", "startOffset": 87, "endOffset": 99}, {"referenceID": 34, "context": "Also, if we think about costs, most works on minimising costs have taken this approach [30, 36, 31].", "startOffset": 87, "endOffset": 99}, {"referenceID": 29, "context": "Also, if we think about costs, most works on minimising costs have taken this approach [30, 36, 31].", "startOffset": 87, "endOffset": 99}, {"referenceID": 2, "context": "Instead, we can invent or negotiate over the attribute [4].", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": ", this is said to be a non-negotiable attribute in terms of [4]) so we can clearly save the cost of getting the value for this attribute.", "startOffset": 60, "endOffset": 63}, {"referenceID": 20, "context": "In our case, it just worked smoothly with Weka [22].", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "2We show accuracy, but we could show other measures such as AUC or MSE [16, 26] 5", "startOffset": 71, "endOffset": 79}, {"referenceID": 24, "context": "2We show accuracy, but we could show other measures such as AUC or MSE [16, 26] 5", "startOffset": 71, "endOffset": 79}, {"referenceID": 26, "context": "In order to assess the significance of the experimental results we will use a custom procedure, following [28] and [18, ch.", "startOffset": 106, "endOffset": 110}, {"referenceID": 7, "context": "12], which in turn is mostly based on [9].", "startOffset": 38, "endOffset": 41}, {"referenceID": 19, "context": "We agree with [21] that the Nemenyi test is a \u201cvery conservative procedure and many of the obvious differences may not be detected\u201d, but we prefer to be conservative given our experimental setting and the use of a 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 34, "context": "In fact, this has to be compared to the usual approach which is specific on decision trees, with several approaches according to [36, 31]: (a) KV, a tree is rebuilt when missing values are found, (b) Null strategy: replace by an extra label (model is not rebuilt), (c) Internal node: creates nodes for examples with missing values (model is not rebuilt), and (d) C4.", "startOffset": 129, "endOffset": 137}, {"referenceID": 29, "context": "In fact, this has to be compared to the usual approach which is specific on decision trees, with several approaches according to [36, 31]: (a) KV, a tree is rebuilt when missing values are found, (b) Null strategy: replace by an extra label (model is not rebuilt), (c) Internal node: creates nodes for examples with missing values (model is not rebuilt), and (d) C4.", "startOffset": 129, "endOffset": 137}], "year": 2013, "abstractText": "Many solutions to cost-sensitive classification (and regression) rely on some or all of the following assumptions: we have complete knowledge about the cost context at training time, we can easily re-train whenever the cost context changes, and we have technique-specific methods (such as cost-sensitive decision trees) that can take advantage of that information. In this paper we address the problem of selecting models and minimising joint cost (integrating both misclassification cost and test costs) without any of the above assumptions. We introduce methods and plots (such as the so-called JROC plots) that can work with any off-the-shelf predictive technique, including ensembles, such that we reframe the model to use the appropriate subset of attributes (the feature configuration) during deployment time. In other words, models are trained with the available attributes (once and for all) and then deployed by setting missing values on the attributes that are deemed ineffective for reducing the joint cost. As the number of feature configuration combinations grows exponentially with the number of features we introduce quadratic methods that are able to approximate the optimal configuration and model choices, as shown by the experimental results.", "creator": "LaTeX with hyperref package"}}}