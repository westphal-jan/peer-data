{"id": "1306.0125", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2013", "title": "Understanding ACT-R - an Outsider's Perspective", "abstract": "The ACT-R theory of cognition developed by John Anderson and colleagues endeavors to explain how humans recall chunks of information and how they solve problems. ACT-R also serves as a theoretical basis for \"cognitive tutors\", i.e., automatic tutoring systems that help students learn mathematics, computer programming, and other subjects. The official ACT-R definition is distributed across a large body of literature spanning many articles and monographs, and hence it is difficult for an \"outsider\" to learn the most important aspects of the theory. This paper aims to provide a tutorial to the core components of the ACT-R theory.", "histories": [["v1", "Sat, 1 Jun 2013 15:48:58 GMT  (100kb,D)", "http://arxiv.org/abs/1306.0125v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jacob whitehill"], "accepted": false, "id": "1306.0125"}, "pdf": {"name": "1306.0125.pdf", "metadata": {"source": "CRF", "title": "Understanding ACT-R \u2013 an Outsider\u2019s Perspective", "authors": ["Jacob Whitehill"], "emails": ["jake@mplab.ucsd.edu"], "sections": [{"heading": "1 Introduction", "text": "The Adaptive Character of Thinking - Rational (ACT-R) is a theory of cognition developed in principle by John Anderson at Carnegie-Mellon University. [4] ACT-R models of how people retrieve \"chunks\" of information from memory and how they solve problems by dividing it into sub-targets and mechanically applying knowledge from working memory. ACT-R is the latest in a series of cognitive models in the ACT family; ACT, ACTE and ACT * succeed. With ACT * [2] Anderson sought to describe human memory and thinking skills mechanically - that is, to describe the mechanisms by which memory and cognition take place - by describing how goals are broken down using \"production rules.\" Later, in 1990, Anderson took a step back from the existing ACT * and asked how cognitive processes such as memory, categorization, causal reasoning and problem solving can be considered optimal solutions."}, {"heading": "1.1 Roadmap", "text": "We first make the crucial distinction between declarative knowledge and procedural knowledge in Section 2. The document then proceeds from top to bottom: Assuming that the agent (a human or possibly a computer) already has all the knowledge he needs, in Section 3 we examine how the decision-making process under ACT-R takes place on a rational basis. In particular, we describe Xiv: 130 6.01 25v1 [cs.LG] 1 Jun 201 3 the mechanism by which a certain \"production rule\" corresponding to the \"actions\" of ACT-R is selected from many possible alternatives. In Section 4 we remove the assumption that knowledge is already available and describe the ACT-R processes by which new knowledge is acquired, which includes both the creation of new memories and the strengthening (and decay) of existing memories. Finally, in Section 6 and 7 we discuss how ACT-R partially models the spacing effect and the power laws of learning / forgetting."}, {"heading": "2 Declarative versus Procedural Knowledge", "text": "Under ACT-R, human knowledge is divided into two disjunctive but related groups of knowledge - declarative and procedural. Declarative knowledge encompasses many chunks of knowledge that represent the current set of facts that are known, and active goals. Two examples of chunks are \"The bank is closed on Sundays\" and \"The current goal is to walk up a mountain.\" Note that each chunk can refer to other chunks. For example, our first chunk i refers to the concepts \"bank,\" \"closed\" and \"Sunday,\" which are presumably all chunks in their own right. If a chunk i refers to another chunk or is referred to by another chunk j, then it is said that chunks i is associated with chunks j. This relationship is not clearly defined in ACT-R - for example, whether the relationship is always symmetrical or whether it can be reflexive (i.e. a chunk that refers to itself), a chunk j."}, {"heading": "2.1 Example", "text": "Let's consider a concrete setting to make the above ideas more concrete. ACT-R lends itself best to areas where tasks can be broken down into well-defined component operations. Suppose the current task the person is working on is to add two multi-digit numbers. Table 1 suggests a plausible series of productions that can solve this problem. Let's assume that the agent (a math student, presumably) already has the productions contained in Table 1 in his procedural memory, and also that he / she can perform single-digit addition. This latter assumption could be fulfilled in two ways: Either the student has memorized these basic addition facts by storing them in his / her declarative memory; or he / she knows that some counting procedures (e.g. by performing simple addition). The latter assumption corresponds to another production rule that is memorized in his / her own right to be stored in the declarative memory; or he / she knows that some counting procedures (e.g., by performing simple addition) are not complied with by another production rule that is stored in his / her own right, this basic addition is not complied with either by the red addition or by the student who knows it in another way."}, {"heading": "3 Decision Making in ACT-R", "text": "If all parts of the prerequisite of a production rule are met (remember that a prerequisite is a logical conjunction), then the production rule itself is considered to be congruent / congruent. In view of several production rules that coincide, a decision still needs to be made on which production rule actually fires (is executed), based on the expected value V of each production that coincides, as well as the point in time at which the congruence occurs. The expected value V is learned by the agent through experience, and the point in time of congruence t is dictated by the latency of congruence that decreases with learning. We will discuss the learning in section 4, but for now we assume that the point in time of congruence of t and the expected value V of each production is already known by the agent."}, {"heading": "3.1 Flowchart", "text": "Suppose the production si had coincided at a certain point in time and the expected value of si is Vi = piG \u2212 Ci, where G is the reward for reaching the target, p is the probability of reaching the target when production si is triggered, and Ci is the expected cost (both immediate and future) of si. Note that Ci refers to the \"real cost\" of Vi when the production process of the production process (e.g. the cost of gasoline, if driving is involved, for example), not the storage costs (which are currently under discussion). According to ACT-R, the actor must decide whether to select (fire) production si or wait for better production sj (with higher value Vj) instead. This decision is made by assessing whether the expected gain of waiting exceeds the expected cost - we call this summary.In ACT-R, the cost of waiting is modeled by a single constant that reflects the storage costs."}, {"heading": "3.2 Selecting from Multiple Productions", "text": "Each production rule has a related expectation value: V = pG \u2212 C (2), where G is the reward earned by achieving the current target, p is the probability that the target will be achieved when production is selected, and C is the expected immediate and future cost of selecting the production.Let's assume that a production i matches the value Vi at time t, and let's assume that V * is the highest value production that can ever match the current set of declarative bits of knowledge. (Let's remember that productions have a latency before they match.) Then production i iff: E [V-Vi] \u2264 1Note that under this distribution, higher rated productions are given a higher probability of occurrence. It's unclear whether this was intended. This expectation is calculated via the distribution Zt (V-Vi), which is the probability of a production fire with the value V *."}, {"heading": "4 Learning in ACT-R", "text": "In ACT-R, learning is about creating new snippets of knowledge and production rules in the memory and strengthening those memories through use."}, {"heading": "4.1 Creating New Knowledge Chunks", "text": "Very little is said about how ACT-R models the initial generation of a declarative memory: either a new chunk is \"generated as an encoding of external events\" ([4], p. 70), or a chunk is written into memory as a result of an executed production rule; the letter case is illustrated in Figure 1, where the knowledge block (variable k) containing the currently edited column is updated (k is increased by 1 in P2)."}, {"heading": "4.2 Creating New Production Rules", "text": "New rules of production are formed by several processes, including: proceduralization, composition, generalization and analogy."}, {"heading": "4.2.1 Proceduralization", "text": "Production rules often contain variables that relate to specific values on which production is based. An example was shown in Figure 1 for the addition of multi-digit numbers. After the same exact task has been performed many times, new productions may arise in the process memory that \"hard codes\" the parameter values into it. For example, if the addition of 36 + 23 is repeated, the laborious application of the six production rules could be skipped by creating a new production: \"If the goal is to add 36 and 23, then write 59.\" This is known as procedural dural time and is a form of compilation under ACT-R [1]. However, ACT-R does not propose a model of when or under what conditions such a process will take place."}, {"heading": "4.2.2 Composition", "text": "If two productions perform problem-solving steps in succession, the productions can be combined. As the application of each production requires some cost, performing both productions in one step can potentially save computational resources such as time and / or effort. For example, if P1 (not the one in Table 1) has the effect of multiplying both sides of the equation 45 x = 1 x 5 and P2 divides both sides by 4 (to solve x), then a new production - let's call it P3 - could combine these steps and multiply both sides by 54."}, {"heading": "4.2.3 Generalization", "text": "A generalization occurs when many similar production rules are already stored, from which a more general rule can be learned inductively. For example, a child may already know that the plural of \"cat\" is \"cat\" and that the plural of \"dog\" is \"dogs.\" From these rules, one can conclude that a plural can be formed by adding an \"s\" to the singular. An astute reader will notice that there are many exceptions to this rule - for example, \"ox\" and \"ox\" or \"octopus\" and \"octopus.\" 2 This requires so-called discrimination, where additional production rules are learned to deal with exceptions.2The correct English plural form of \"octopus\" is either \"octopus\" or \"octopus.\""}, {"heading": "4.2.4 Analogy", "text": "When a person is confronted with a new problem for which no solution (in the form of production rules) is known, he or she can argue by analogy of previously seen example solutions to similar problems. If, for example, he or she has encountered the programming problem where a variable is to be increased by 4, the solution of which (in C) is x + = 4, he or she can argue that the solution to the similar problem of multiplying a variable by 4 could be x * = 4, provided that the syntax for multiplication (*) was already known. Analog-based learning requires that the learner is able to create a mapping of the known example to the current problem. In the above case, the mapping of addition to multiplication was. The new production rule arises by applying the same mapping to the solution of the example problem, i.e.:... \"Then write x + = 4\" will be... \"Then write Figure x * = 4.\" The analogy may also occur, even if the current problem (possibly inefficient) can be solved by the assumption of the production model."}, {"heading": "4.3 Strengthening Existing Chunks", "text": "In ACT-R, the strength in the memory of a chunk of knowledge is called activation. A chunk can gain activation through use: a chunk is used when it coincides with a production rule that fires. This definition is also consistent with the idea of memory strength increasing through practice, by introducing a trivial \"activation\" of production: defining the current target as \"practice i\" and a production as \"If the goal is to practice i, then call back i \u2212.\" Anderson assumes that the activation A (t) = B (t) + jjjsij (3), where B (t) the base activation of the knowledge element is the base activation of the knowledge element that ultimately leads to ignition. 3 The activation A (t) of a particular chunk is defined as: A (t) = B (t) + jjjsij (3), where B (t) the base activation of the knowledge element is the base activation of the knowledge element that ultimately leads to ignition."}, {"heading": "4.4 Learning Associative Weights", "text": "The concept of context in ACT-R is not clearly defined, but it is roughly described as \"the elements in the current target block and the elements currently being processed in the perception field\" ([4], p. 51). Presumably, the elements of the target block relate to those parts to which the target block is connected (in the sense of section 2). Suppose the chunk j is in the current context. Then, its effect on Ai (t) is as follows: If i is connected to j, then the log odds of the chunk i relate to a production that (i.e., the activation of i) is increased by wjsij. The wj elements are hardly defined except that each wj element in the context [0, 1] and that it \"reflects the meaning or validity of the [related] element j.\""}, {"heading": "4.5 Learning Production Success Probability and Production Costs", "text": "In order to estimate the value of a production V = pG \u2212 C, both terms p and C must be calculated, with p being the probability that production will lead to the achievement of the target, and C the expected immediate and future cost of completing that production. The first probability mentioned in [4] q can be broken down into the probability that production itself will be successful, i.e. that it will have its intended impact, multiplied by the probability that the target will be achieved, depending on the success of the production. The first probability mentioned in [4] q can be estimated by updating a beta distribution each time that a production is either successful or fails. ACT-R heuristically estimates the second variable, called r, in one of two different ways: the costs already incurred in achieving the target (the assumption is that the higher the total cost is already spent, the lower the probability that a production will be successful or fail), or the similarity between the current state and the target state."}, {"heading": "4.6 Strengthening Existing Productions", "text": "Analogous to the activation of pieces of knowledge, the measure of the memory strength of a production rule is the production strength, p. In [4] it is assumed that S are the log odds for which the production rule fires, and it is defined as: S (t) = ln \u2211 k t \u2212 dk + B (5), where the k indicates events in which production fires ([4], p. 293), and B is a constant. (From [4] it is unclear whether the B in 5 is the same B as in 4.)"}, {"heading": "4.6.1 Cautionary Note", "text": "Equation 3 supposedly models the log odds that a particular chunk corresponds to the firing production, and 5 models the log odds of a particular production rule. However, these probabilities are not independent of each other: For example, if the chunk i is the only chunk referred to in the production rule s, and if Ai (t) = \u2212 \u221e (i.e., the chunk has no chance of matching the firing rule), then S (t) must also be equal \u2212 \u221e. Since the production fire is modeled as competition between several firing production rules, the log odds of a particular production fire cannot be calculated independently of other productions. For both of the above reasons, the equations associated with it cannot really correspond to the log odds that they supposedly represent."}, {"heading": "4.7 Latency of Production Rule Matching", "text": "The latency of a production rule is defined as the period between the first moment when all chunks of knowledge in the precondition of s were in memory until the point at which s actually matches. This latency is modelled as: L (t) = \u2211 i Be \u2212 b (Ai (t) + S (t))) (6), where the sum is above the chunk of knowledge required for production, Ai (t) is the \"activation\" (a measure of memory strength, described below) of the chunk of knowledge at the time t, and S (t) is the strength of production itself at the time t. B and b are constants."}, {"heading": "5 Memory", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Relationship of Latency to Probability of Recall", "text": "The probability of retrieving a chunk of knowledge is P (t) = 11 + e \u2212 (A (t) \u2212 \u03c4) / s, which means that a chunk of knowledge tends to be forgotten when its activation A (t) is below \u03c4. This is consistent with ACT-R, since certain productions never fire when their latency is too long."}, {"heading": "6 The Spacing Effect", "text": "Distance effect is a psychological phenomenon in which the likelihood of remembering an object during the test period can be increased by scheduling training sessions so that there is a significant time difference between consecutive sessions. Intuitively, it makes little difference to a person's memory if they only drilled a particular object a few seconds ago when they immediately drilled it again. ACT-R can model the distance effect in at least two different ways, as explained in Section 4.3."}, {"heading": "7 The Power Laws of Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Latency", "text": "Although much is lauded for the supposed modeling of the power law of learning, ACT-R actually has only a weak relationship to this phenomenon. One form of the power law of learning is that the latency of a correctly retrieved declarative memory L decays as a power function, which has the general form L (t) = At \u2212 k for constants A and k, provided that the number of practice sessions increases with t. \u2212 In ACT \u2212 R, however, for this relationship to be maintained, it must be assumed that the decay rate of d multiple practice sessions is constant (which then destroys the modeling of the distance effect), and also that the time between practice sessions tk \u2212 tk \u2212 tk is also a constant. One must also take into account the latency of a piece of knowledge in isolation from a particular production, as in [9] - in the ACT Rdefinition of [4], where latency is only explicitly defined as d."}], "references": [{"title": "Acquisition of cognitive skill", "author": ["J.R. Anderson"], "venue": "Psychological Review, 89(4):369\u2013406", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1982}, {"title": "The Architecture of Cognition", "author": ["J.R. Anderson"], "venue": "Harvard University Press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1983}, {"title": "Adaptive Character of Thought", "author": ["J.R. Anderson"], "venue": "Lawrence Erlbaum Associates", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1993}, {"title": "Rules of the Mind", "author": ["J.R. Anderson"], "venue": "Lawrence Erlbaum Associates", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "Practice and retention: A unifying analysis", "author": ["J.R. Anderson", "J.M. Fincham", "S. Douglass"], "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition, 25(5):1120\u20131136", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1999}, {"title": "Reflections of the environment in memory", "author": ["J.R. Anderson", "L.J. Schooler"], "venue": "Psychological Science, 2:396\u2013408", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1991}, {"title": "Memory", "author": ["D. Fum", "A. Stocco"], "venue": "emotion, and rationality: An ACT-R interpretation for gambling task results. In Proceedings of the sixth International Conference on Cognitive Modeling", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Using a model to compute the optimal schedule of practice", "author": ["P.I. Pavlik", "J.R. Anderson"], "venue": "Journal of Experimental Psychology: Applied, 14(2):101\u2013117", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Optimizing knowledge component learning using a dynamic structural model of practice", "author": ["P.I. Pavlik", "N. Presson", "J.R. Anderson"], "venue": "Proceedings of the Eighth International Conference of Cognitive Modeling", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 3, "context": "The Adaptive Character of Thought - Rational (ACT-R) is a theory of cognition developed principally by John Anderson at Carnegie-Mellon University [4].", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "With ACT* [2], Anderson endeavored to describe the human memory and reasoning faculties mechanistically \u2013 i.", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "It is difficult to infer from [4] the exact process by which the above rational decision is made.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "Zt is defined as (see [4], p.", "startOffset": 22, "endOffset": 25}, {"referenceID": 2, "context": "62, and [3], p.", "startOffset": 8, "endOffset": 11}, {"referenceID": 3, "context": "Note that the probability of a production with value x occurring decreases with time; this was intended by Anderson to model the idea that higher-valued productions should match earlier than lower-valued productions [4].", "startOffset": 216, "endOffset": 219}, {"referenceID": 3, "context": "Under ACT-R, Z is dependent on t in order to \u201callow for the possibility that later instantiations are less likely to be as good as earlier instantiations\u201d ([4], p.", "startOffset": 156, "endOffset": 159}, {"referenceID": 0, "context": "V (V \u2217 \u2212 V )Zt(V )dV \u2217 \u2264 \u03c4 (Note that G is the maximum value a production could have since p \u2208 [0, 1] and C > 0.", "startOffset": 95, "endOffset": 101}, {"referenceID": 3, "context": "Very little is said about how ACT-R models the initial creation of declarative memory: Either a new chunk is \u201ccreated as the encoding of external events\u201d ([4], p.", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "\u201d This is known as proceduralization and is a form of compilation under ACT-R [1].", "startOffset": 78, "endOffset": 81}, {"referenceID": 3, "context": "where B(t) is the base activation of the knowledge chunk at time t, and the summation is the associative strength of the chunk dependent on related chunks, defined as \u201cthe elements in the current goal chunk and the elements currently being processed in the perceptual field\u201d ([4], p.", "startOffset": 276, "endOffset": 279}, {"referenceID": 0, "context": "Each wj \u2208 [0, 1] \u201creflects the salience or validity of [related] element j.", "startOffset": 10, "endOffset": 16}, {"referenceID": 3, "context": "The decay rate d is defined in the ACT-R literature in various ways: \u2022 In [4], the decay rate d is a constant for all k.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "\u2022 In [6], the decay rate of activation for the particular learning event k is defined as dk = max{d1, b(tk\u22121\u2212 tk) \u2212d1}4 where d1 (a constant) is the decay rate of the first learning event for the chunk, and b is a constant.", "startOffset": 5, "endOffset": 8}, {"referenceID": 7, "context": "\u2022 In [8], dk = cek\u22121 + \u03b1, where c and \u03b1 are constants, and mk\u22121 is the activation of the chunk at the time of the previous learning event (k \u2212 1).", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "3In fact, he defines activation in two conflicting ways: in [4], p.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "64, chunk activation is the log-odds that the chunk matches a production that fires; in [4], p.", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "4In [6], the base was actually tk \u2212 tk\u22121, but since older events have longer decay times t, this would result in a negative base, and hence a complex number for the decay, which was presumably unintended.", "startOffset": 4, "endOffset": 7}, {"referenceID": 3, "context": "The notion of context in ACT-R is not clearly defined, but it is described roughly as \u201cthe elements in the current goal chunk and the elements currently being processed in the perceptual field\u201d ([4], p.", "startOffset": 195, "endOffset": 198}, {"referenceID": 0, "context": "The wj are barely defined at all, except that each wj \u2208 [0, 1], and that it \u201creflects the salience or validity of [related] element j.", "startOffset": 56, "endOffset": 62}, {"referenceID": 6, "context": "\u201d [7] suggests that the wj express \u201cattentional weighting, i.", "startOffset": 2, "endOffset": 5}, {"referenceID": 3, "context": "\u201d The sij \u2208 [\u2212\u221e,+\u221e] are \u201cthe strengths of association to i from elements j in the current context\u201d ([4], p.", "startOffset": 100, "endOffset": 103}, {"referenceID": 3, "context": "sij = log p(Cj | Ni) p(Cj | Ni) \u2248 log p(Cj | Ni) p(Cj) = log p(Ni | Cj) p(Ni) The justification for this approximation given in [4] is that knowing the outcome ofNi cannot substantially affect the probability of event Ni since there are presumably very many chunks in all of declarative memory.", "startOffset": 128, "endOffset": 131}, {"referenceID": 3, "context": "In [4], a prior probability for both of these terms was also employed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "The first probability, which is called q in [4], can be estimated by updating a Beta distribution every time that a production either succeeds or fails.", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "As stated by the author himself in [3], the proposed methods of estimating p and C are not optimal, but rather only plausible.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "In [4], S is supposed to be the log-odds that the production rule will fire, and it is defined as: S(t) = ln \u2211", "startOffset": 3, "endOffset": 6}, {"referenceID": 3, "context": "where the k indexes events in which the production fires ([4], p.", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "(It is unclear from [4] if the B in 5 is the same B as in 4.", "startOffset": 20, "endOffset": 23}, {"referenceID": 8, "context": "One must also consider the latency of a knowledge chunk in isolation from any particular production, as in [9] \u2013 in the ACT-R", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "definition in [4], however, latency is only explicitly defined for the instantiation of a production rule (matching of its pre-condition to chunks); hence, strictly speaking, this too is an approximation.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "(The proof below is adapted from [5]).", "startOffset": 33, "endOffset": 36}, {"referenceID": 8, "context": "The latency of recall of a particular chunk as defined in [9] is: L(t) = Fe + C", "startOffset": 58, "endOffset": 61}], "year": 2013, "abstractText": "The ACT-R theory of cognition developed by John Anderson and colleagues endeavors to explain how humans recall chunks of information and how they solve problems. ACT-R also serves as a theoretical basis for \u201ccognitive tutors\u201d, i.e., automatic tutoring systems that help students learn mathematics, computer programming, and other subjects. The official ACT-R definition is distributed across a large body of literature spanning many articles and monographs, and hence it is difficult for an \u201coutsider\u201d to learn the most important aspects of the theory. This paper aims to provide a tutorial to the core components of the ACT-R theory.", "creator": "LaTeX with hyperref package"}}}