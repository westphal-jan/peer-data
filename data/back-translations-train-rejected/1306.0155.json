{"id": "1306.0155", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jun-2013", "title": "Dynamic Ad Allocation: Bandits with Budgets", "abstract": "We consider an application of multi-armed bandits to internet advertising (specifically, to dynamic ad allocation in the pay-per-click model, with uncertainty on the click probabilities). We focus on an important practical issue that advertisers are constrained in how much money they can spend on their ad campaigns. This issue has not been considered in the prior work on bandit-based approaches for ad allocation, to the best of our knowledge.", "histories": [["v1", "Sat, 1 Jun 2013 22:00:03 GMT  (16kb)", "http://arxiv.org/abs/1306.0155v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["aleksandrs slivkins"], "accepted": false, "id": "1306.0155"}, "pdf": {"name": "1306.0155.pdf", "metadata": {"source": "CRF", "title": "Dynamic ad allocation: bandits with budgets", "authors": ["Aleksandrs Slivkins"], "emails": ["slivkins@microsoft.com."], "sections": [{"heading": null, "text": "ar Xiv: 130 6.01 55v1 [cs.LG] 1 Jun 2We define a simple, stylized model in which an algorithm selects an ad that appears in each round and each ad has a budget: the maximum amount of money that can be spent on that ad. This model allows a natural variation of UCB1, a well-known algorithm for multi-armed bandits with stochastic rewards. We derive strong verifiable guarantees for this algorithm."}, {"heading": "1 Introduction", "text": "The aim of the algorithm is to maximize the total expected reward over time, in terms of advertising. These are advertisements and advertisements that have been studied since 1930 in Operations Research, Economics and several branches of computer science. Such problems arise initially in different areas, e.g., the design of medical experiments, dynamic pricing and routing on the Internet. In the last ten years, an increase in interest in MAB problems is due to their applications in web search and Internet advertising. In the most basic MAB problem [2], an algorithm repeatedly selects between several possible actions (traditionally referred to as arms) and observes the reward for the chosen arm. The reward is stochastical: the selection of a given arm is an independent sample that depends on distribution, but not on the round in which that arm is selected. This reward distribution is not disclosed to the algorithm."}, {"heading": "1.2 Our contributions", "text": "We look at a natural algorithm and prove that it works quite well. While the algorithm is essentially the first thing a researcher familiar with previous work on MAB would suggest, our technical contribution is the analysis of this algorithm and, in particular, the \"coupling argument\" in it. The conceptual contribution is that we provide an assurance that the natural approach works from a theoretical point of view and suggest the strengths and limitations of this approach. Our algorithm, called BudgetedUCB, is a natural modification of UCB1 [2], a well-known algorithm for MAB with stochastic rewards. UCB1 maintains a numerical value (index) for each arm and in each turn UCB selects an arm with the largest index. The index of Arm i is essentially the best upper trust available, tied to the expected reward from that arm. BudgetedUCB selects an exhaustive index among all available arms in each turn."}, {"heading": "1.3 Discussion", "text": "A common criticism of the work on non-Bayesian (pre-independent, minimizing remorse) MAB problems is that the algorithmic ideas and evidentiary techniques adopted for the numerous MAB models studied in the literature are too specific to their respective models, and cannot easily be generalized to more general attitudes common in applications. In light of this criticism, it is useful to identify general ideas and techniques to build on when working on the (more general) UCB settings, and to provide concrete examples of how to build on them. This paper contributes to this direction: we are building on the algorithmic idea of the \"UCB indexes,\" and a particular evidence technique to analyze them (both from [2]). These ideas have been enormously useful in several other UCB settings with stochastic rewards, e.g. [19, 30, 12, 24, 1].It is worth noting that UCB budgets do not have to flow into."}, {"heading": "3 Analysis: proof of Theorem 1.1", "text": "The technical contribution of this paper is the analysis of BudgetedUCB. The crux of this is the \"coupling argument,\" which is summarized in Lemma 3.5. To argue about random clicks, an important conceptual step is to consider two different representations of realized clicks (defined below). In addition, we build on the technique from the analysis of UCB1 [2], which is in Lemma 3.4.Notation. consider an execution of BudgetedUCB. For each arm i, let ni (t) be the number of impressions of Arm i before round. Let ni = ni (T + 1) be the total number of impressions of Arm i. Let ~ n = (n1,.) be the impressions vector for BudgetedUCB. Likewise, let the impressions be vector for BudgetedUCB."}, {"heading": "4 Related work", "text": "The basic formulation for MAB with stochastic rewards is well understood ([20, 2] and the follow-up work, see [11] for references and discussions).Our formulation is a specific case of sleep problems [19], in which the individual areas of the study since 1933 [27], in Operations Research, Economics, and several branches of computer science [24] are available, in which the individual areas of work on MAB go beyond the scope of this paper; however, a reader is encouraged to refer to [13, 11] for the background to pre-independent MAB, and to [26, 16] for the background to Bayesian MAB. Starting from [22], much of the work on MAB has been motivated by Internet advertising. Below, we will discuss only the work directly relevant to this paper. The present paper continues the line of work on pre-independent MAB with stochastic rewards (where the reward of a given arm is an i.i.i.i.i.) and the follow-up formulation is basic for some time-invasive MAB discussions (see)."}, {"heading": "Acknowledgements", "text": "The author thanks Ashwin Badanidiyuru, Sebastien Bubeck and Robert Kleinberg for many stimulating conversations about heavily armed bandits."}], "references": [{"title": "Improved algorithms for linear stochastic bandits", "author": ["Y. Abbasi-Yadkori", "D. P\u00e1l", "C. Szepesv\u00e1ri"], "venue": "25th Advances in Neural Information Processing Systems (NIPS), pages 2312\u20132320,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning, 47(2-3):235\u2013256, 2002. Preliminary version in 15th ICML,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "The nonstochastic multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire"], "venue": "SIAM J. Comput., 32(1):48\u201377, 2002. Preliminary version in 36th IEEE FOCS,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "Dynamic pricing with limited supply", "author": ["M. Babaioff", "S. Dughmi", "R. Kleinberg", "A. Slivkins"], "venue": "13th ACM Conf. on Electronic Commerce (EC),", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Truthful mechanisms with implicit payment computation", "author": ["M. Babaioff", "R. Kleinberg", "A. Slivkins"], "venue": "11th ACM Conf. on Electronic Commerce (EC), pages 43\u201352,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Characterizing truthful multi-armed bandit mechanisms", "author": ["M. Babaioff", "Y. Sharma", "A. Slivkins"], "venue": "10th ACM Conf. on Electronic Commerce (EC), pages 79\u201388,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning on a budget: posted price mechanisms for online procurement", "author": ["A. Badanidiyuru", "R. Kleinberg", "Y. Singer"], "venue": "13th ACM Conf. on Electronic Commerce (EC), pages 128\u2013145,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Bandits with knapsacks", "author": ["A. Badanidiyuru", "R. Kleinberg", "A. Slivkins"], "venue": "A technical report on arxiv.org., May", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic auctions: A survey", "author": ["D. Bergemann", "M. Said"], "venue": "Wiley Encyclopedia of Operations Research and Management Science. John Wiley & Sons,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Dynamic pricing without knowing the demand function: Risk bounds and near-optimal algorithms", "author": ["O. Besbes", "A. Zeevi"], "venue": "Operations Research, 57:1407\u20131420,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems", "author": ["S. Bubeck", "N. Cesa-Bianchi"], "venue": "Foundations and Trends in Machine Learning, 5(1):1\u2013122,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Open Loop Optimistic Planning", "author": ["S. Bubeck", "R. Munos"], "venue": "23rd Conf. on Learning Theory (COLT), pages 477\u2013489,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Cambridge Univ. Press,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "The price of truthfulness for pay-per-click auctions", "author": ["N. Devanur", "S.M. Kakade"], "venue": "10th ACM Conf. on Electronic Commerce (EC), pages 99\u2013106,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "A Truthful Learning Mechanism for Contextual Multi-Slot Sponsored Search Auctions with Externalities", "author": ["N. Gatti", "A. Lazaric", "F. Trovo"], "venue": "13th ACM Conf. on Electronic Commerce (EC),", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-Armed Bandit Allocation Indices", "author": ["J. Gittins", "K. Glazebrook", "R. Weber"], "venue": "John Wiley & Sons,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-armed bandits with metric switching costs", "author": ["S. Guha", "K. Munagala"], "venue": "Proc. 36th International Colloquium on Automata, Languages, and Programming (ICALP), pages 496\u2013507,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Approximation algorithms for correlated knapsacks and non-martingale bandits", "author": ["A. Gupta", "R. Krishnaswamy", "M. Molinaro", "R. Ravi"], "venue": "52nd IEEE Symp. on Foundations of Computer Science (FOCS), pages 827\u2013836,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Regret bounds for sleeping experts and bandits", "author": ["R. Kleinberg", "A. Niculescu-Mizil", "Y. Sharma"], "venue": "21st Conf. on Learning Theory (COLT), pages 425\u2013436,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Asymptotically efficient Adaptive Allocation Rules", "author": ["T.L. Lai", "H. Robbins"], "venue": "Advances in Applied Mathematics, 6:4\u201322,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1985}, {"title": "Algorithmic Game Theory", "author": ["N. Nisan", "T. Roughgarden", "E. Tardos", "V.V. (eds"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Bandits for Taxonomies: A Model-based Approach", "author": ["S. Pandey", "D. Agarwal", "D. Chakrabarti", "V. Josifovski"], "venue": "SIAM Intl. Conf. on Data Mining (SDM),", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning diverse rankings with multi-armed bandits", "author": ["F. Radlinski", "R. Kleinberg", "T. Joachims"], "venue": "25th Intl. Conf. on Machine Learning (ICML), pages 784\u2013791,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Contextual Bandits with Similarity Information", "author": ["A. Slivkins"], "venue": "24th Conf. on Learning Theory (COLT),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning optimally diverse rankings over large document collections", "author": ["A. Slivkins", "F. Radlinski", "S. Gollapudi"], "venue": "J. of Machine Learning Research (JMLR), 14(Feb):399\u2013436, 2013. Preliminary version in 27th ICML,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Generalized Bandit Problems", "author": ["R.K. Sundaram"], "venue": "D. Austen-Smith and J. Duggan, editors, Social Choice and Strategic Decisions: Essays in Honor of Jeffrey S. Banks (Studies in Choice and Welfare), pages 131\u2013162. Springer, 2005. First appeared as Working Paper, Stern School of Business,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2003}, {"title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "author": ["W.R. Thompson"], "venue": "Biometrika, 25(3-4):285294,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1933}, {"title": "\u01eb-first policies for budget-limited multi-armed bandits", "author": ["L. Tran-Thanh", "A. Chapman", "E.M. de Cote", "A. Rogers", "N.R. Jennings"], "venue": "In Proc. Twenty-Fourth AAAI Conference on Artificial Intelligence", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Knapsack based optimal policies for budgetlimited multi-armed bandits", "author": ["L. Tran-Thanh", "A. Chapman", "A. Rogers", "N.R. Jennings"], "venue": "Proc. Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI-12), pages 1134\u20131140,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Algorithms for Infinitely Many-Armed Bandits", "author": ["Y. Wang", "J.-Y. Audibert", "R. Munos"], "venue": "Advances in Neural Information Processing Systems (NIPS), pages 1729\u20131736,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 25, "context": "Multi-armed bandits (henceforth, MAB), and more generally online decision problems with partial feedback and exploration-exploitation tradeoff, has been studied since 1930\u2019s in Operations Research, Economics and several branches of Computer Science [26, 13, 16, 11].", "startOffset": 249, "endOffset": 265}, {"referenceID": 12, "context": "Multi-armed bandits (henceforth, MAB), and more generally online decision problems with partial feedback and exploration-exploitation tradeoff, has been studied since 1930\u2019s in Operations Research, Economics and several branches of Computer Science [26, 13, 16, 11].", "startOffset": 249, "endOffset": 265}, {"referenceID": 15, "context": "Multi-armed bandits (henceforth, MAB), and more generally online decision problems with partial feedback and exploration-exploitation tradeoff, has been studied since 1930\u2019s in Operations Research, Economics and several branches of Computer Science [26, 13, 16, 11].", "startOffset": 249, "endOffset": 265}, {"referenceID": 10, "context": "Multi-armed bandits (henceforth, MAB), and more generally online decision problems with partial feedback and exploration-exploitation tradeoff, has been studied since 1930\u2019s in Operations Research, Economics and several branches of Computer Science [26, 13, 16, 11].", "startOffset": 249, "endOffset": 265}, {"referenceID": 1, "context": "In the most basic MAB problem [2], an algorithm repeatedly chooses among several possible actions (traditionally called arms), and observes the reward for the chosen arm.", "startOffset": 30, "endOffset": 33}, {"referenceID": 21, "context": ", see [22]).", "startOffset": 6, "endOffset": 10}, {"referenceID": 0, "context": "Each ad i is characterized by the following three quantities: CTR \u03bci \u2208 [0, 1], payment-per-click bi and budget Bi.", "startOffset": 71, "endOffset": 77}, {"referenceID": 1, "context": "Our algorithm, called BudgetedUCB, is a natural modification of UCB1 [2], a well-known algorithm for MAB with stochastic rewards.", "startOffset": 69, "endOffset": 72}, {"referenceID": 19, "context": "3 Both guarantees are nearly optimal for this special case, respectively up to O(log T ) factors and up to constant factors [20, 2, 3].", "startOffset": 124, "endOffset": 134}, {"referenceID": 1, "context": "3 Both guarantees are nearly optimal for this special case, respectively up to O(log T ) factors and up to constant factors [20, 2, 3].", "startOffset": 124, "endOffset": 134}, {"referenceID": 2, "context": "3 Both guarantees are nearly optimal for this special case, respectively up to O(log T ) factors and up to constant factors [20, 2, 3].", "startOffset": 124, "endOffset": 134}, {"referenceID": 1, "context": "The present paper contributes to this direction: we build on the algorithmic idea of \u201cUCB indices\u201d, and a certain proof technique to analyze them (both from [2]).", "startOffset": 157, "endOffset": 160}, {"referenceID": 18, "context": "[19, 30, 12, 24, 1].", "startOffset": 0, "endOffset": 19}, {"referenceID": 29, "context": "[19, 30, 12, 24, 1].", "startOffset": 0, "endOffset": 19}, {"referenceID": 11, "context": "[19, 30, 12, 24, 1].", "startOffset": 0, "endOffset": 19}, {"referenceID": 23, "context": "[19, 30, 12, 24, 1].", "startOffset": 0, "endOffset": 19}, {"referenceID": 0, "context": "[19, 30, 12, 24, 1].", "startOffset": 0, "endOffset": 19}, {"referenceID": 1, "context": "Our algorithm, called BudgetedUCB, is a natural extension of the well-known algorithm UCB1 [2].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "The original algorithm UCB1 in [2] is, essentially, a special case of BudgetedUCBwhen all arms are available and all values are bi = 1.", "startOffset": 31, "endOffset": 34}, {"referenceID": 18, "context": "Moreover, the algorithm in [19] for sleeping bandits with stochastic rewards coincides with ours for bi \u2261 1 (but the analysis from [19] does not carry over to our setting, see Section 4 for more discussion).", "startOffset": 27, "endOffset": 31}, {"referenceID": 18, "context": "Moreover, the algorithm in [19] for sleeping bandits with stochastic rewards coincides with ours for bi \u2261 1 (but the analysis from [19] does not carry over to our setting, see Section 4 for more discussion).", "startOffset": 131, "endOffset": 135}, {"referenceID": 1, "context": "In particular, the algorithms in [2] and [19] have log t there.", "startOffset": 33, "endOffset": 36}, {"referenceID": 18, "context": "In particular, the algorithms in [2] and [19] have log t there.", "startOffset": 41, "endOffset": 45}, {"referenceID": 1, "context": "Also, we build on the technique from the analysis of UCB1 [2], which is encapsulated in Lemma 3.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "This is the crucial part of a UCB-style analysis, and it incorporates the main trick from the original analysis in [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 22, "context": "in [23, 25]) suggests that a smaller value such as C = 1 can be used in practice.", "startOffset": 3, "endOffset": 11}, {"referenceID": 24, "context": "in [23, 25]) suggests that a smaller value such as C = 1 can be used in practice.", "startOffset": 3, "endOffset": 11}, {"referenceID": 26, "context": "MAB has been an active area of investigation since 1933 [27], in Operations Research, Economics and several branches of Computer Science: machine learning, theoretical computer science, AI, and algorithmic economics.", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "A survey of prior work on MAB is beyond the scope of this paper; a reader is encouraged to refer to [13, 11] for background on prior-independent MAB, and to [26, 16] for background on Bayesian MAB.", "startOffset": 100, "endOffset": 108}, {"referenceID": 10, "context": "A survey of prior work on MAB is beyond the scope of this paper; a reader is encouraged to refer to [13, 11] for background on prior-independent MAB, and to [26, 16] for background on Bayesian MAB.", "startOffset": 100, "endOffset": 108}, {"referenceID": 25, "context": "A survey of prior work on MAB is beyond the scope of this paper; a reader is encouraged to refer to [13, 11] for background on prior-independent MAB, and to [26, 16] for background on Bayesian MAB.", "startOffset": 157, "endOffset": 165}, {"referenceID": 15, "context": "A survey of prior work on MAB is beyond the scope of this paper; a reader is encouraged to refer to [13, 11] for background on prior-independent MAB, and to [26, 16] for background on Bayesian MAB.", "startOffset": 157, "endOffset": 165}, {"referenceID": 21, "context": "Starting from [22], much of the work on MAB has been motivated by internet advertising.", "startOffset": 14, "endOffset": 18}, {"referenceID": 19, "context": "The basic formulation for MAB with stochastic rewards is well-understood ([20, 2] and the follow-up work, see [11] for references and discussion).", "startOffset": 74, "endOffset": 81}, {"referenceID": 1, "context": "The basic formulation for MAB with stochastic rewards is well-understood ([20, 2] and the follow-up work, see [11] for references and discussion).", "startOffset": 74, "endOffset": 81}, {"referenceID": 10, "context": "The basic formulation for MAB with stochastic rewards is well-understood ([20, 2] and the follow-up work, see [11] for references and discussion).", "startOffset": 110, "endOffset": 114}, {"referenceID": 18, "context": "Our formulation is a special case of sleeping bandits [19, 24] where in each round, a subset of arms is not available (\u201casleep\u201d) and the goal is to compete with the best available arm.", "startOffset": 54, "endOffset": 62}, {"referenceID": 23, "context": "Our formulation is a special case of sleeping bandits [19, 24] where in each round, a subset of arms is not available (\u201casleep\u201d) and the goal is to compete with the best available arm.", "startOffset": 54, "endOffset": 62}, {"referenceID": 18, "context": "However, this adversary in [19, 24] is oblivious (it decides its selections for all rounds before round 1), whereas in our problem it is adaptive (it decides its selection for round t only after observing what happened before).", "startOffset": 27, "endOffset": 35}, {"referenceID": 23, "context": "However, this adversary in [19, 24] is oblivious (it decides its selections for all rounds before round 1), whereas in our problem it is adaptive (it decides its selection for round t only after observing what happened before).", "startOffset": 27, "endOffset": 35}, {"referenceID": 18, "context": "To the best of our knowledge, the results in [19, 24] do not extend to settings where available arms are chosen by an adaptive adversary.", "startOffset": 45, "endOffset": 53}, {"referenceID": 23, "context": "To the best of our knowledge, the results in [19, 24] do not extend to settings where available arms are chosen by an adaptive adversary.", "startOffset": 45, "endOffset": 53}, {"referenceID": 10, "context": "Contextual bandits have been a subject of much recent work, see [11] for a survey.", "startOffset": 64, "endOffset": 68}, {"referenceID": 9, "context": "A typical example is \u201cdynamic selling\u201d[10, 4], where a seller has a limited supply of items and offers one item for sale in each round; the arms correspond to the offered prices.", "startOffset": 38, "endOffset": 45}, {"referenceID": 3, "context": "A typical example is \u201cdynamic selling\u201d[10, 4], where a seller has a limited supply of items and offers one item for sale in each round; the arms correspond to the offered prices.", "startOffset": 38, "endOffset": 45}, {"referenceID": 6, "context": "Other examples include \u201cdynamic buying\u201d [7] (where a buyer has a limited budget of money and interacts with a new seller in each round), and several versions in which the resource consumption for a given arm is deterministic [17, 18, 28, 29].", "startOffset": 40, "endOffset": 43}, {"referenceID": 16, "context": "Other examples include \u201cdynamic buying\u201d [7] (where a buyer has a limited budget of money and interacts with a new seller in each round), and several versions in which the resource consumption for a given arm is deterministic [17, 18, 28, 29].", "startOffset": 225, "endOffset": 241}, {"referenceID": 17, "context": "Other examples include \u201cdynamic buying\u201d [7] (where a buyer has a limited budget of money and interacts with a new seller in each round), and several versions in which the resource consumption for a given arm is deterministic [17, 18, 28, 29].", "startOffset": 225, "endOffset": 241}, {"referenceID": 27, "context": "Other examples include \u201cdynamic buying\u201d [7] (where a buyer has a limited budget of money and interacts with a new seller in each round), and several versions in which the resource consumption for a given arm is deterministic [17, 18, 28, 29].", "startOffset": 225, "endOffset": 241}, {"referenceID": 28, "context": "Other examples include \u201cdynamic buying\u201d [7] (where a buyer has a limited budget of money and interacts with a new seller in each round), and several versions in which the resource consumption for a given arm is deterministic [17, 18, 28, 29].", "startOffset": 225, "endOffset": 241}, {"referenceID": 7, "context": "A very recent, yet unpublished, paper [8], concurrent with respect to this paper, considers a generalization of our setting in which the budgets can be specified for arbitrary subsets of ads.", "startOffset": 38, "endOffset": 41}, {"referenceID": 7, "context": ") However, the guarantees in [8] for BudgetedAdsMAB are much weaker than ours.", "startOffset": 29, "endOffset": 32}, {"referenceID": 20, "context": "A more detailed discussion of this work is beyond the scope of this paper; see Chapter 28 of [21] for background.", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "There are two somewhat distinct directions: dynamic auctions, in which the advertisers submit bids over time (see [9] for a survey), and MAB mechanisms [6, 14, 5, 15], where the advertisers submit bids only once, and the mechanism allocates ads over time.", "startOffset": 114, "endOffset": 117}, {"referenceID": 5, "context": "There are two somewhat distinct directions: dynamic auctions, in which the advertisers submit bids over time (see [9] for a survey), and MAB mechanisms [6, 14, 5, 15], where the advertisers submit bids only once, and the mechanism allocates ads over time.", "startOffset": 152, "endOffset": 166}, {"referenceID": 13, "context": "There are two somewhat distinct directions: dynamic auctions, in which the advertisers submit bids over time (see [9] for a survey), and MAB mechanisms [6, 14, 5, 15], where the advertisers submit bids only once, and the mechanism allocates ads over time.", "startOffset": 152, "endOffset": 166}, {"referenceID": 4, "context": "There are two somewhat distinct directions: dynamic auctions, in which the advertisers submit bids over time (see [9] for a survey), and MAB mechanisms [6, 14, 5, 15], where the advertisers submit bids only once, and the mechanism allocates ads over time.", "startOffset": 152, "endOffset": 166}, {"referenceID": 14, "context": "There are two somewhat distinct directions: dynamic auctions, in which the advertisers submit bids over time (see [9] for a survey), and MAB mechanisms [6, 14, 5, 15], where the advertisers submit bids only once, and the mechanism allocates ads over time.", "startOffset": 152, "endOffset": 166}], "year": 2013, "abstractText": "We consider an application of multi-armed bandits to internet advertising (specifically, to dynamic ad allocation in the pay-per-click model, with uncertainty on the click probabilities). We focus on an important practical issue that advertisers are constrained in how much money they can spend on their ad campaigns. This issue has not been considered in the prior work on bandit-based approaches for ad allocation, to the best of our knowledge. We define a simple, stylized model where an algorithm picks one ad to display in each round, and each ad has a budget: the maximal amount of money that can be spent on this ad. This model admits a natural variant of UCB1, a well-known algorithm for multi-armed bandits with stochastic rewards. We derive strong provable guarantees for this algorithm.", "creator": "LaTeX with hyperref package"}}}