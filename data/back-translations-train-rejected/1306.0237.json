{"id": "1306.0237", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jun-2013", "title": "Guided Random Forest in the RRF Package", "abstract": "Random Forest (RF) is a powerful supervised learner and has been popularly used in many applications such as bioinformatics. In this work I propose a new and enhanced RF called the guided random forest (GRF) for high-dimensional classification and feature selection. Similar to a feature selection method called guided regularized random forest (GRRF), GRF is built using the importance scores from an ordinary RF. The trees in GRRF are built sequentially, are highly correlated (which deteriorates accuracy performance as a classifier) and do not allow for parallel computing, while the trees in GRF are built independently and can be implemented in a distributed computing framework. Experiments on 10 high-dimensional gene data sets show that, with a fixed parameter value (without tuning the parameter), GRF outperforms RF on 8 data sets and 7 of them have significant differences at the 0.05 level. GRF uses, on average, only tens of features in the model, while RF uses thousands of features. Therefore, both accuracy and interpretability are significantly improved. I also found that, as a classifier, GRF outperforms GRRF on all the data sets and 7 of them have significant differences at the 0.05 level. The computational complexity of GRF is only about twice as much as an ordinary RF. GRF can be used in \"RRF\" v1.4, a package that also includes the regularized random forest methods.", "histories": [["v1", "Sun, 2 Jun 2013 18:30:45 GMT  (14kb)", "https://arxiv.org/abs/1306.0237v1", null], ["v2", "Sat, 12 Oct 2013 03:56:07 GMT  (14kb)", "http://arxiv.org/abs/1306.0237v2", null], ["v3", "Mon, 18 Nov 2013 08:52:49 GMT  (13kb)", "http://arxiv.org/abs/1306.0237v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["houtao deng"], "accepted": false, "id": "1306.0237"}, "pdf": {"name": "1306.0237.pdf", "metadata": {"source": "CRF", "title": "Guided Random Forest in the RRF Package", "authors": ["Houtao Deng"], "emails": ["hdeng3@asu.edu"], "sections": [{"heading": null, "text": "ar Xiv: 130 6.02 37v3 [cs.LG] 1 8N ov2 013Similar to a feature selection method called Guided Regularized Random Forest (GRRF), GRF is created using the importance values of an ordinary RF. However, the trees in GRRF are sequentially constructed, correlate strongly and do not allow parallel computation, while the trees in GRF are independently constructed and can be implemented in parallel. Experiments on 10 high-dimensional gene datasets show that with a fixed parameter value (without tuning the parameter), RF applied to characteristics selected by GRF, exceeds all characteristics on 9 datasets and 7 of them show significant differences at the level 0.05. Therefore, both accuracy and interpretability are significantly improved. GRF selects more characteristics than GRRF, but leads to better classification accuracy."}, {"heading": "1 INTRODUCTION", "text": "Random Forests (RF) (Breiman, 2001; Liaw and Wiener, 2002) have been widely used in many areas, including bioinformatics applications (Riddick et al., 2011; Yuan et al., 2012). RF is able to process categorical and numerical traits, multiple classes, are insensitive to the scale of traits and have been considered a powerful supervised learner. RF can provide importance values of traits to understand the contribution of each trait. However, there can be an enormous number of traits for high-dimensional problems (all gene datasets included in our experiments have more than 1000 traits), and it is difficult to study the importance of thousands of traits. Therefore, it is desirable to develop a trait selection algorithm for RF. Deng and Runger's proposed guided regulated random forest (GRRF) uses the importance values from an RF, which is built on the complete training data, to provide information in a local cluster."}, {"heading": "2 METHODS", "text": "Let gain (Xi) specify the Gini information gain by using a Xi attribute to divide a tree node. GRF's key idea is weighting gain (Xi) based on the weighting of an RF value (Xi) = \u03bbi gain (Xi) (1), where \u03bbi as\u03bbi = 1 \u2212 \u03b3 + \u03b3 ImpiImp * (2) is calculated, where Impi is the weighting of Xi from an RF, Imp \u043c is the maximum weighting value, ImpiImp \u043a [0, 1] is the normalized weighting nucleus, and GP [0, 1] controls the weighting of the weighting of the weights from an RF. It turns out that attributes of lesser importance from RF are more strongly punished, and the penalty increases with increasing GF (GRF becomes RF = 0). In this work, I use the maximum penalty (i.e. \u03b3 [0, 1] controls the weight of the weights from RF) to use a small number of attributes in GRF."}, {"heading": "3 EXAMPLES AND RESULTS", "text": "< Code 1 shows an example of GRF * 0-1; Y [ix] < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < - > < < - > < - > < - > < - > < < - > < < - > < < - > < < - > < - > < - > < - > < - > < - > < - > < - > < - > < - > < - > < - > < - > < - > < - > < - <"}, {"heading": "4 CONCLUSIONS", "text": "Experiments show that GRF-RF is not only significantly better than RF in terms of accuracy performance, but also uses many fewer features in the model. In this thesis I will discuss the advantages of GRF for high-dimensional gene sets. It may also be valuable to find other cases where GRF has advantages over other methods, with the option to tune the parameter \u03b3 in Equation (2) (fixed as 1 here). Furthermore, in this thesis \u03bbi is determined by the importance of the Xi attribute from an ordinary random forest. However, \u03bbi can also be specified in other ways, e.g. by F-Score or human knowledge."}], "references": [{"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1), 5\u201332.", "citeRegEx": "Breiman,? 2001", "shortCiteRegEx": "Breiman", "year": 2001}, {"title": "Gene selection with guided regularized random forest", "author": ["H. Deng", "G. Runger"], "venue": "Pattern Recognition. to appear.", "citeRegEx": "Deng and Runger,? 2013", "shortCiteRegEx": "Deng and Runger", "year": 2013}, {"title": "Gene selection and classification of microarray data using random forest", "author": ["R. D\u0131\u0301az-Uriarte", "S. De Andres"], "venue": "BMC bioinformatics,", "citeRegEx": "D\u0131\u0301az.Uriarte and Andres,? \\Q2006\\E", "shortCiteRegEx": "D\u0131\u0301az.Uriarte and Andres", "year": 2006}, {"title": "Classification and regression by randomforest", "author": ["A. Liaw", "M. Wiener"], "venue": "R News, 2(3), 18\u201322.", "citeRegEx": "Liaw and Wiener,? 2002", "shortCiteRegEx": "Liaw and Wiener", "year": 2002}, {"title": "Predicting in vitro drug sensitivity using random forests", "author": ["G. Riddick", "H. Song", "S. Ahn", "J. Walling", "D. Borges-Rivera", "W. Zhang", "H.A. Fine"], "venue": "Bioinformatics, 27(2), 220\u2013224.", "citeRegEx": "Riddick et al\\.,? 2011", "shortCiteRegEx": "Riddick et al\\.", "year": 2011}, {"title": "Predicting the lethal phenotype of the knockout mouse by integrating comprehensive genomic data", "author": ["Y. Yuan", "Y. Xu", "J. Xu", "R.L. Ball", "H. Liang"], "venue": "Bioinformatics, 28(9), 1246\u20131252. 2", "citeRegEx": "Yuan et al\\.,? 2012", "shortCiteRegEx": "Yuan et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002) has been widely used in many fields including bioinformatics applications (Riddick et al.", "startOffset": 19, "endOffset": 57}, {"referenceID": 3, "context": "Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002) has been widely used in many fields including bioinformatics applications (Riddick et al.", "startOffset": 19, "endOffset": 57}, {"referenceID": 4, "context": "Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002) has been widely used in many fields including bioinformatics applications (Riddick et al., 2011; Yuan et al., 2012).", "startOffset": 132, "endOffset": 173}, {"referenceID": 5, "context": "Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002) has been widely used in many fields including bioinformatics applications (Riddick et al., 2011; Yuan et al., 2012).", "startOffset": 132, "endOffset": 173}, {"referenceID": 1, "context": "However, the trees in GRRF can be highly correlated and GRRF can not be built in parallel (Deng and Runger, 2013).", "startOffset": 90, "endOffset": 113}, {"referenceID": 0, "context": "Random forest (RF) (Breiman, 2001; Liaw and Wiener, 2002) has been widely used in many fields including bioinformatics applications (Riddick et al., 2011; Yuan et al., 2012). RF is able to handle mixed categorical and numerical features, multiple classes, are insensitive to the scale of features, and have been considered as a powerful supervised learner. RF can provide importance scores of features to understand the contribution of each feature. However, there can be a huge number of features for high-dimensional problems (all the gene data sets considered in our experiments have more than 1000 features), and it is challenging to investigate the importance scores from thousands of features. Therefore, it is desirable to develop a feature selection algorithm for RF. . The guided regularized random forest (GRRF) proposed by Deng and Runger (2013) uses the importance scores from an RF built on the complete training data to complement the information gain in a local node.", "startOffset": 20, "endOffset": 857}, {"referenceID": 1, "context": "1) and GRRF-RF (RF applied to the feature subset selected by GRRF), all with 1000 trees, to 10 gene data sets used in D\u0131\u0301az-Uriarte and De Andres (2006); Deng and Runger (2013). The references of the data sets are provided in a supplementary file to save space.", "startOffset": 154, "endOffset": 177}, {"referenceID": 1, "context": "1) and GRRF-RF (RF applied to the feature subset selected by GRRF), all with 1000 trees, to 10 gene data sets used in D\u0131\u0301az-Uriarte and De Andres (2006); Deng and Runger (2013). The references of the data sets are provided in a supplementary file to save space. I obtained the average error rates and average number of features for each method using the same procedure as Deng and Runger (2013), i.", "startOffset": 154, "endOffset": 395}, {"referenceID": 1, "context": "1) and GRRF-RF (RF applied to the feature subset selected by GRRF), all with 1000 trees, to 10 gene data sets used in D\u0131\u0301az-Uriarte and De Andres (2006); Deng and Runger (2013). The references of the data sets are provided in a supplementary file to save space. I obtained the average error rates and average number of features for each method using the same procedure as Deng and Runger (2013), i.e., calculated from 100 replicates of training/testing splits with a ratio of 2:1. The results of RF and GRRF-RF are slightly different from the results of Deng and Runger (2013) due to randomness.", "startOffset": 154, "endOffset": 577}], "year": 2013, "abstractText": "Summary: Random Forest (RF) is a powerful supervised learner and has been popularly used in many applications such as bioinformatics. In this work we propose the guided random forest (GRF) for feature selection. Similar to a feature selection method called guided regularized random forest (GRRF), GRF is built using the importance scores from an ordinary RF. However, the trees in GRRF are built sequentially, are highly correlated and do not allow for parallel computing, while the trees in GRF are built independently and can be implemented in parallel. Experiments on 10 high-dimensional gene data sets show that, with a fixed parameter value (without tuning the parameter), RF applied to features selected by GRF outperforms RF applied to all features on 9 data sets and 7 of them have significant differences at the 0.05 level. Therefore, both accuracy and interpretability are significantly improved. GRF selects more features than GRRF, however, leads to better classification accuracy. Note in this work the guided random forest is guided by the importance scores from an ordinary random forest, however, it can also be guided by other methods such as human insights (by specifying \u03bbi). GRF can be used in \u201cRRF\u201d v1.4 (and later versions), a package that also includes the regularized random forest methods. Availability: The RRF package is freely distributed under GNU General Public License (GPL) and is available from CRAN (http://cran.r-project.org/), the official R package archive. Contact: hdeng3@asu.edu", "creator": "LaTeX with hyperref package"}}}