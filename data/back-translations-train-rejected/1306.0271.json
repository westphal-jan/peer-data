{"id": "1306.0271", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2013", "title": "KERT: Automatic Extraction and Ranking of Topical Keyphrases from Content-Representative Document Titles", "abstract": "We introduce KERT (Keyphrase Extraction and Ranking by Topic), a framework for topical keyphrase generation and ranking. By shifting from the unigram-centric traditional methods of unsupervised keyphrase extraction to a phrase-centric approach, we are able to directly compare and rank phrases of different lengths. We construct a topical keyphrase ranking function which implements the four criteria that represent high quality topical keyphrases (coverage, purity, phraseness, and completeness). The effectiveness of our approach is demonstrated on two collections of content-representative titles in the domains of Computer Science and Physics.", "histories": [["v1", "Mon, 3 Jun 2013 01:44:28 GMT  (50kb,D)", "http://arxiv.org/abs/1306.0271v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["marina danilevsky", "chi wang", "nihit desai", "jingyi guo", "jiawei han"], "accepted": false, "id": "1306.0271"}, "pdf": {"name": "1306.0271.pdf", "metadata": {"source": "CRF", "title": "KERT: Automatic Extraction and Ranking of Topical Keyphrases from Content-Representative Document Titles", "authors": ["Marina Danilevsky", "Chi Wang", "Nihit Desai", "Jingyi Guo", "Jiawei Han"], "emails": ["danilev1@illinois.edu,", "chiwang1@illinois.edu,", "nhdesai2@illinois.edu,", "jingyi@cs.umass.edu,", "hanj@cs.uiuc.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "2 Related Work", "text": "The state-of-the-art approaches to unguarded keyphrase extraction are typically based on graphical, unigram-centric ranking methods, which first extract unique items from the text and then combine them into keyphrases. TextRank (Mihalcea and Tarau, 2004) constructs keyphrases from the best-placed unique items in a collection of documents. Topical PageRank (Liu et al., 2010) splits the documents into themes and creates keyphrases from topical unique items. Some previous methods have used clustering techniques for keyphrase extraction (Liu et al., 2009; Grineva et al., 2009), relying on external knowledge bases such as Wikipedia to calculate the meaning of words and their relationship. Barker and Cornacchia (2000) use natural language processing methods to select noun phrases as keyphrases."}, {"heading": "3 KERT Framework", "text": "A key aspect of our framework is that we do not follow the traditional unigram-centric approach of keyphrase extraction, where words are first extracted and classified individually and then combined into phrases. Instead, we construct current phrases immediately after clustering the Unirams. By switching from an unigram-centric to a phrase-centric approach, we are able to extract current keyphrases and implement a ranking function that can directly compare keyphrases of different lengths, as we explain in Section 3.3.3. Our steps to generate and rank current keyphrases are as follows: Step 1. Cluster words in the document record in T-foreground topics and a background topic, using background LDA.Step 2. Extract candidate keyphrases for each topic according to the word assignment. Step 3. Place the keyphrases in each foreground topic in the..."}, {"heading": "3.1 Clustering Words using Topic Modeling", "text": "Most titles consist of a few technical keywords and background words. Background words are found across titles belonging to different themes. \u2022 Ambiguous unigrams. A title can contain a sparse mix of different themes, despite its short length. LDA (Lead et al., 2003) and its extensions have proven effective for modeling textual documents. Therefore, we use a modified LDA model that includes an additional background theme z = 0. Each title is modeled by distributing over the foreground themes z = 1,..., k and the background theme d."}, {"heading": "3.2 Candidate Keyphrase Generation", "text": "There are two ways to discover keyphrases: Either we extract them from the text (strings of words) that actually occur in the text, or we construct them automatically (Frank et al., 1999), an approach that is considered both more intelligent and more difficult (Hammouda et al., 2005; Manning and Guests, 1999).In a dataset of {content-representative titles}, extracting phrases directly from the text is quite limiting, as this approach is also too sensitive to the whims of different writing styles. Consider, for example, that two computer science titles, one containing \"common patterns in mining\" and the other \"common patterns in mining,\" should clearly discuss and treat the same topic. A keyphrase can also be separated by other words: \"frequent closed patterns in mining\" is also part of the theme of frequent pattern mining, in addition to incorporating secondary themes of common and closed patterns."}, {"heading": "3.3 Ranking Measures and Function", "text": "As discussed in Section 1, a ranking function must be able to directly compare current keyphrases with respect to the four criteria of coverage, purity, phraseworthiness, and completeness, implying that the function should be able to directly compare keyphrases of different lengths that we call comparable. For example, the keyphrases \"classification,\" \"decision trees,\" and \"support vector engines\" should all rank high in the integrated list of machine learning keyphrases, despite different lengths. Traditional probabilistic modeling approaches such as language models or topic models do not have the comparative properties. You can model the probability of seeing an n-gram on a topic, but the probabilities of n-grams with different lengths (unigrams, bigrams, etc.) are not well comparable. These approaches simply find longer n-grams to have a much lower probability than shorter ones, because the probabilities of all possible predictors sum up to 1, and so we represent a possible number."}, {"heading": "3.3.1 Coverage", "text": "A representative key phrase for a topic should cover many documents within that topic. For example, \"Information Retrieval\" has better coverage than \"Cross Language Information Retrieval\" in the topic Information Retrieval. We quantify the coverage size of a phrase directly as the probability of seeing an expression in a random topic t phrase: \u03c0covt (p) = P (et (p)) = ft (p) | Dt | (1)"}, {"heading": "3.3.2 Purity", "text": "A phrase is purely in theme t, if it only occurs frequently in documents about topic t and not frequently in documents about other topics. For example, \"query processing\" is a purer key phrase than \"query\" in the current database.We measure the purity of a key phrase by comparing the probability of seeing a phrase in the theme t collection of words, and the probability of seeing it in another theme t collection (t \u2032 = 0, 1,.., k, t \u2032 6 = t).A reference collection Dt, t \u2032 = Dt \u2032 is a mixture of theme t and theme t \u2032 titles. If there is a theme, so the probability of seeing a phrase p in a reference collection Dt, t \u2032 (p) = \"similar or even greater than the probability of seeing p in Dt, the phrase p \u00b2 and t \u00b2 t \u2032 t \u00b2 t \u00b2 titles.The purity of a keyphrase compares the probability of seeing it in the theme collection, Dt \u00b2 and the probability of seeing the phrase Dt, t \u00b2 and the probability of seeing it, t \u00b2 p, and even greater than the probability of seeing p in Dt \u00b2, p, p, p, and \u00b2 p, p, and \u00b2 p, p, p, and \u00b2 p, p, p, p, and \u00b2 p, p, p, p, and \u00b2 p, p, p, p, p, p, p, p, p, p, p, p, p, p, and \u00b2 \u00b2, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, and \u00b2 \u00b2 \u00b2, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p \u00b2 \u00b2 \u00b2 \u00b2 and p, p, p, p \u00b2 p, p, p, p, p, p, p, p, p, p, p \u00b2 \u00b2 \u00b2 and p, p, p, p, p \u00b2 p, p, p, p, p, p, p, p \u00b2, p, p, p, p, p, p, p, p, p, p, p,"}, {"heading": "3.3.3 Phraseness", "text": "A group of words should be grouped into one phrase if they occur together much more frequently than the expected frequency of occurrence, since each word occurs independently of each other in the phrase. For example, while \"active learning\" is a good key phrase as in the topic of \"machine learning,\" \"learning classification\" is not because the latter two words only occur together because both are popular in the topic. We therefore compare the probability of seeing a phrase p = {w1... wn} and the n words w1.... wn independently of each other in the subject t documents: \u03c0phrt (p) = log P (et (p)) = log p (et (w)))) (3) = log ft (p) | Dt |."}, {"heading": "3.3.4 Completeness", "text": "A phrase p is not complete if a longer phrase p \u2032 containing p is usually accompanied by p. For example, \"vector machines\" is not a complete phrase, but \"support vector machines\" because \"support\" almost always goes hand in hand with \"vector machines.\" Therefore, we measure the completeness of a phrase p by examining the conditional probability of observing p \u2032 given p in a theme t title: \u03c0comt (p) = 1 \u2212 max p \u2032% p (et (p) | et (p))) (4) = 1 \u2212 max w P (et (p) | et (p) = 1 \u2212 maxw ft (p) (p)) ft (p)."}, {"heading": "3.3.5 Combined Ranking Function", "text": "We combine these 4 measures into a comprehensive function for the precedence of a current key phrase: rt (p) = q (0 \u03c0comt \u2264 \u03b3 \u03c0covt [(1 \u2212 \u03c9) \u03c0 pure t + \u03c9\u03c0 phr t] (p) o.w. (5), where \u03b3, \u03c9 [0, 1] are two parameters. The completeness criterion is used as a filtering condition to remove incomplete phrases where \u03b3 controls how aggressively we cut back. \u03b3 = 0 corresponds to ignoring the criteria and maintaining all maximum phrases where no supersets have the same topical support. As \u03b3 approaches 1, more phrases are filtered and only closed phrases remain (where no supersets occur sufficiently frequently) p. The other three criteria then calculate the precedence for the key phrases that pass the completeness filter. In a sense, the coverage criterion is the most important, since a key phrase with low coverage will be obviously of low quality."}, {"heading": "4 Experiments", "text": "In our evaluation, we use two collections of content-representative titles: the first - the DBLP dataset - is a compilation of titles of recently published computer science publications from the fields of databases, data mining, information retrieval, machine learning, and natural language processing. These titles come from DBLP1, a bibliographic website for computer science publications; and the second collection - the arXiv dataset - is a selection of titles of physical work published within the last decade and designated by its authors as belonging to the sub-areas of optics, fluid dynamics, atomic physics, instrumentation, and detectors, or plasma physics. This collection of titles comes from arXiv2, an online archive for electronic preprints of scientific papers. 3Both datasets were minimally pre-processed by removing all stopwords from the titles. After pre-processing, the DBLP dataset comprised 33,313 titles of 18,598 unique words, and the 79,522 words consisting of the unique words."}, {"heading": "4.1 DBLP Dataset Experiments", "text": "We will use the DBLP dataset to assess the ability of our method to construct topical keyphrases that appear to be of high quality to human judges by means of a user study. We will first describe the methods we used for comparison, and then present a sample of the keyphrases that were actually generated by these methods and that the participants of the user study came across. We will then explain the details of our user study and present 1http: / / www.dblp.org / 2http: / / arxiv.org 3Both datasets will be available online."}, {"heading": "4.1.1 Methods for Comparison", "text": "We use a Newton-Raphson iteration method (Minka, 2000) to learn the hyperparameters, and empirically use \u03bb = 0.1, resulting in generally coherent results for our data.4 To evaluate the performance of KERT, we implemented several variations of the function as well as two basic functions. Baselines are from Zhao et al (2011), which focus on topical keyphrase extraction in microblogs, but claim that their method can be used for other collections of text. We implement their two best-performing methods: kpRelInt * and kpRelation.5 We also construct variations of KERT where the keyphrase extraction steps are the same, but each of the four ranking criteria is ignored in turn."}, {"heading": "4.1.2 Qualitative Results", "text": "Table 1 shows the top 10 rankings of current keyphrases generated by each machine learning methodology. kpRel and kpRelInt * produce very similar results, with both clearly preferring unique keyphrases. However, kpRel also rates several keyphrases highly that are not very meaningful, such as \"Learning Classification\" and \"Selection Learning.\" Removing the coverage from our ranking function results in the worst results and confirms the intuition that a high-quality keyphrase must have at least good coverage. Without purity, the function prefers bigrams and trigrams, which all appear very meaningful, although several high-quality unigrams such as \"Learning\" and \"Classification\" no longer exist. Removing phrases, on the other hand, results in significant unigrams, but very few bigrams and looks quite similar to the kpRelInt * baseline. Finally, without completeness, phrases will no longer be classified as \"Support Vector\" and \"Vector\" even though they should be high. \""}, {"heading": "4.1.3 User Study and Quantitative Results", "text": "To quantitatively measure the keyphrase quality, we invited people to judge the generated keyphrases generated by the various methods. Since the DBLP dataset generated topics in computer science, we recruited 10 computer science students - who could thus be considered highly knowledgeable judges - for a user study. We generated 5 topics from the DBLP dataset and found 4 of them uniquely interpretable as machine learning, databases, data mining, and information scale retrieval. For each of the four topics we retrieved the top-20 ranked keyphrases by each method. These keyphrases were collected per topic and submitted in random order, and users were asked to rate the quality of each keyphrase on a 5-click scale scale. To scale the performance of each method in light of the results of the user study we apply, we adapt the nKQM measurement (normalized keyphrase quality of the scale phrase from a scale scale-scale-scale-scale-scale-scale-scale-scale-scale-scale)."}, {"heading": "4.2 arXiv Dataset Experiments", "text": "We use the arXiv dataset, which contains labeled titles, to explore what method maximizes mutual information between the phrases represented by themes and titles. Since the collection has 5 categories, we set T = 5.For each method, we make multiple runs for different values of K (the number of top-ranked phrases per topic considered), and calculate the mutual information MIK for this method as a function of K. To calculate MIK, we label each of the uppermost K phrases in each topic with the topic in which it is highest rated. We then check each title to see if it contains any of these top phrases. If so, we update the number of events that \"see a topic t and a category c\" for t = 1.. T, with the average number of phrases contained in the title; otherwise, we update the number of events that \"see a topic t and a category c\" for t = 1."}, {"heading": "5 Conclusion", "text": "In this paper, we introduce KERT (Keyphrase Extraction and Ranking by Topic), a framework for automatically extracting and ranking current keyphrases using collections of content-representative document titles. In contrast to existing techniques, our phrase-centric approach is able to construct topical keyphrases in such a way that our ranking function can directly compare the quality of keyphrases of different lengths. Our method delivers high-quality topical keyphrases with over 50% improvement over a basic method according to human judgement and over 20% improvement according to mutual information. In the future, we would like to further investigate why human judgment seems to be consistently distorted against purity criteria, as opposed to quantitative metrics such as mutual information. We are also interested in extending our approach to work with longer texts."}], "references": [{"title": "Using noun phrase heads to extract document keyphrases", "author": ["Barker", "Cornacchia2000] Ken Barker", "Nadia Cornacchia"], "venue": "In Proceedings of the 13th Biennial Conference of the Canadian Society on Computational Studies of Intelligence: Advances in Artificial", "citeRegEx": "Barker et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Barker et al\\.", "year": 2000}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Domain-specific keyphrase extraction", "author": ["Frank et al.1999] Eibe Frank", "Gordon W. Paynter", "Ian H. Witten", "Carl Gutwin", "Craig G. Nevill-Manning"], "venue": "In Proceedings of the 16th international joint conference on Artificial intelligence - Volume", "citeRegEx": "Frank et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Frank et al\\.", "year": 1999}, {"title": "Extracting key terms from noisy and multitheme documents", "author": ["Maxim Grinev", "Dmitry Lizorkin"], "venue": "In Proceedings of the 18th international conference on World wide web,", "citeRegEx": "Grineva et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Grineva et al\\.", "year": 2009}, {"title": "Corephrase: keyphrase extraction for document clustering", "author": ["Diego N. Matute", "Mohamed S. Kamel"], "venue": "In Proceedings of the 4th international conference on Machine Learning and Data Mining in Pattern Recogni-", "citeRegEx": "Hammouda et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Hammouda et al\\.", "year": 2005}, {"title": "Mining frequent patterns without candidate generation: A frequent-pattern tree approach", "author": ["Han et al.2004] Jiawei Han", "Jian Pei", "Yiwen Yin", "Runying Mao"], "venue": "Data Min. Knowl. Discov.,", "citeRegEx": "Han et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Han et al\\.", "year": 2004}, {"title": "Unsupervised learning by probabilistic latent semantic analysis", "author": ["Thomas Hofmann"], "venue": "Mach. Learn.,", "citeRegEx": "Hofmann.,? \\Q2001\\E", "shortCiteRegEx": "Hofmann.", "year": 2001}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["J\u00e4rvelin", "Kek\u00e4l\u00e4inen2002] Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen"], "venue": "ACM Trans. Inf. Syst.,", "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2002}, {"title": "Clustering to find exemplar terms for keyphrase extraction", "author": ["Liu et al.2009] Zhiyuan Liu", "Peng Li", "Yabin Zheng", "Maosong Sun"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Foundations of statistical natural language processing", "author": ["Manning", "Hinrich Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Manning et al\\.", "year": 1999}, {"title": "Automatic labeling of multinomial topic models", "author": ["Mei et al.2007] Qiaozhu Mei", "Xuehua Shen", "ChengXiang Zhai"], "venue": "In Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Mei et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Mei et al\\.", "year": 2007}, {"title": "TextRank: Bringing Order into Texts", "author": ["Mihalcea", "Tarau2004] Rada Mihalcea", "Paul Tarau"], "venue": "In Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Mihalcea et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2004}, {"title": "Estimating a dirichlet distribution", "author": ["Thomas P. Minka"], "venue": null, "citeRegEx": "Minka.,? \\Q2000\\E", "shortCiteRegEx": "Minka.", "year": 2000}, {"title": "A language model approach to keyphrase extraction", "author": ["Tomokiyo", "Hurst2003] Takashi Tomokiyo", "Matthew Hurst"], "venue": "In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment - Volume", "citeRegEx": "Tomokiyo et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Tomokiyo et al\\.", "year": 2003}, {"title": "Learning algorithms for keyphrase extraction", "author": ["Peter D. Turney"], "venue": "Inf. Retr.,", "citeRegEx": "Turney.,? \\Q2000\\E", "shortCiteRegEx": "Turney.", "year": 2000}, {"title": "Topical n-grams: Phrase and topic discovery, with an application to information retrieval", "author": ["Wang et al.2007] Xuerui Wang", "Andrew McCallum", "Xing Wei"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Topical keyphrase extraction from twitter", "author": ["Zhao et al.2011] Wayne Xin Zhao", "Jing Jiang", "Jing He", "Yang Song", "Palakorn Achananuparp", "Ee-Peng Lim", "Xiaoming Li"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguis-", "citeRegEx": "Zhao et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "Keyphrases have traditionally been defined as terms or phrases which summarize the topics in a document (Turney, 2000).", "startOffset": 104, "endOffset": 118}, {"referenceID": 16, "context": "However, recently there has been some interest in working with documents consisting of very short text, such as a collection of tweets (Zhao et al., 2011), in order to summarize the document collection.", "startOffset": 135, "endOffset": 154}, {"referenceID": 14, "context": "However, it has long been known that unigrams account for only a small fraction of human-assigned index terms (Turney, 2000).", "startOffset": 110, "endOffset": 124}, {"referenceID": 8, "context": "Some previous methods have used clustering techniques on word graphs for keyphrase extraction (Liu et al., 2009; Grineva et al., 2009), relying on external knowledge bases such as Wikipedia to calculate term importance and relatedness.", "startOffset": 94, "endOffset": 134}, {"referenceID": 3, "context": "Some previous methods have used clustering techniques on word graphs for keyphrase extraction (Liu et al., 2009; Grineva et al., 2009), relying on external knowledge bases such as Wikipedia to calculate term importance and relatedness.", "startOffset": 94, "endOffset": 134}, {"referenceID": 3, "context": ", 2009; Grineva et al., 2009), relying on external knowledge bases such as Wikipedia to calculate term importance and relatedness. Barker and Cornacchia (2000) use natural language processing techniques to select noun phrases as keyphrases.", "startOffset": 8, "endOffset": 160}, {"referenceID": 3, "context": ", 2009; Grineva et al., 2009), relying on external knowledge bases such as Wikipedia to calculate term importance and relatedness. Barker and Cornacchia (2000) use natural language processing techniques to select noun phrases as keyphrases. Tomokiyo and Hurst (2003) take a language modeling approach, requiring a document collection with known topics as input and training a language model to define their ranking criteria.", "startOffset": 8, "endOffset": 267}, {"referenceID": 6, "context": "Topic modeling techniques such as PLSA (probabilistic latent semantic analysis) (Hofmann, 2001) and LDA (latent Dirichlet allocation) (Blei et al.", "startOffset": 80, "endOffset": 95}, {"referenceID": 1, "context": "Topic modeling techniques such as PLSA (probabilistic latent semantic analysis) (Hofmann, 2001) and LDA (latent Dirichlet allocation) (Blei et al., 2003) take documents as input, model them as mixtures of different topics, and discover word distributions for each topic.", "startOffset": 134, "endOffset": 153}, {"referenceID": 15, "context": "Some previous work has developed topic modeling to discover topical phrases comprised of consecutive words (Wang et al., 2007).", "startOffset": 107, "endOffset": 126}, {"referenceID": 10, "context": "Therefore our work is tangentially related to automatic topic labeling (Mei et al., 2007).", "startOffset": 71, "endOffset": 89}, {"referenceID": 1, "context": "LDA (Blei et al., 2003) and its extensions have been shown to be effective for modeling textual documents.", "startOffset": 4, "endOffset": 23}, {"referenceID": 2, "context": "There are two ways to discover keyphrases: either extract them from the text (sequences of words) which actually occur in the text, or to automatically construct them (Frank et al., 1999), an approach which is regarded as both more intelligent and more difficult (Hammouda et al.", "startOffset": 167, "endOffset": 187}, {"referenceID": 4, "context": ", 1999), an approach which is regarded as both more intelligent and more difficult (Hammouda et al., 2005; Manning and Sch\u00fctze, 1999).", "startOffset": 83, "endOffset": 133}, {"referenceID": 5, "context": "We may then mine frequent word sets from ptDt = {p t d|d \u2208 Dt} using any efficient pattern mining algorithm, such as FP-growth (Han et al., 2004).", "startOffset": 127, "endOffset": 145}, {"referenceID": 16, "context": "While previous work has used various heuristics to correct this bias during post-processing steps, (Tomokiyo and Hurst, 2003; Zhao et al., 2011), our approach is cleaner and more principled.", "startOffset": 99, "endOffset": 144}, {"referenceID": 12, "context": "We resort to a Newton-Raphson iteration method (Minka, 2000) to learn the hyperparameters, and empirically set \u03bb = 0.", "startOffset": 47, "endOffset": 60}, {"referenceID": 12, "context": "We resort to a Newton-Raphson iteration method (Minka, 2000) to learn the hyperparameters, and empirically set \u03bb = 0.1, which leads to generally coherent results for our dataset.4 To evaluate the performance of KERT, we implemented several variations of the function, as well as two baseline functions. The baselines come from Zhao et al (2011), who focus on topical keyphrase extraction in microblogs, but claim that their method can be used for other text collections.", "startOffset": 48, "endOffset": 345}, {"referenceID": 16, "context": "To measure the performance of each method given the user study results, we adapt the nKQM@K measure (normalized keyphrase quality measure for top-K phrases) from (Zhao et al., 2011), which is itself a version of the nDCG metric from information retrieval (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002).", "startOffset": 162, "endOffset": 181}, {"referenceID": 16, "context": "Unlike in (Zhao et al., 2011), we have more than 2 judges, so we define scoreaw as the agreementweighted average score for the Mt,j keyphrase, which is a weighted mean of the judges\u2019 score by the weighted Cohen\u2019s \u03ba.", "startOffset": 10, "endOffset": 29}], "year": 2013, "abstractText": "We introduce KERT (Keyphrase Extraction and Ranking by Topic), a framework for topical keyphrase generation and ranking. By shifting from the unigram-centric traditional methods of unsupervised keyphrase extraction to a phrase-centric approach, we are able to directly compare and rank phrases of different lengths. We construct a topical keyphrase ranking function which implements the four criteria that represent high quality topical keyphrases (coverage, purity, phraseness, and completeness). The effectiveness of our approach is demonstrated on two collections of contentrepresentative titles in the domains of Computer Science and Physics.", "creator": "LaTeX with hyperref package"}}}