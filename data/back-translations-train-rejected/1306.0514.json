{"id": "1306.0514", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2013", "title": "Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences", "abstract": "We introduce persistent contextual neural networks (PCNNs) as a probabilistic model for learning symbolic data sequences, aimed at discovering complex algorithmic dependencies in the sequence. PCNNs are similar to recurrent neural networks but feature an architecture inspired by finite automata and a modified time evolution to better model memory effects. An effective training procedure using a gradient ascent in a metric inspired by Riemannian geometry is developed: this produces an algorithm independent from design choices such as the encoding of parameters and unit activities. This metric gradient ascent is designed to have an algorithmic cost close to backpropagation through time for sparsely connected networks.", "histories": [["v1", "Mon, 3 Jun 2013 17:36:14 GMT  (169kb)", "https://arxiv.org/abs/1306.0514v1", "Preliminary version"], ["v2", "Tue, 27 Aug 2013 16:19:13 GMT  (244kb)", "http://arxiv.org/abs/1306.0514v2", "Preliminary version. 2nd version: recurrent tensor-square differential metric added, more thorough experiments, title changed"], ["v3", "Sat, 12 Jul 2014 14:35:22 GMT  (248kb)", "http://arxiv.org/abs/1306.0514v3", "3rd version: minor changes"], ["v4", "Tue, 3 Feb 2015 18:35:36 GMT  (254kb)", "http://arxiv.org/abs/1306.0514v4", "4th version: some changes in notation, more experiments"]], "COMMENTS": "Preliminary version", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["yann ollivier"], "accepted": false, "id": "1306.0514"}, "pdf": {"name": "1306.0514.pdf", "metadata": {"source": "CRF", "title": "Riemannian metrics for neural networks II: recurrent networks and learning symbolic data sequences", "authors": ["Yann Ollivier"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is so that it is a way in which it is about a way in which people are able to put themselves and their environment at the center. (...) It is not so that people are able to understand themselves. (...) It is so that people are able to understand the world. (...) It is so that people are able to understand the world. (...) It is so that people are able to understand themselves. (...) It is so that people are able to understand the world. (...) It is so that people are able to understand the world. (...) It is so that they live in the world of the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world, in the world in the world, in the world, in the world in the world, in the world in the world, in the world, in the world in the world, in the world, in the world in the world, in the world in the, in the world in the, in the world in the, in the world in the, in the world in the, in the, in the world in the, in the world in the, in the, in the world in the world in the, in the world in the, in the, in the, in the world in the, in the, in the world in the, in the world in the, in the, in the, in the world in the, in the, in the world in the, in the, in the, in the, in the, in the world in the, in the, in the world in the, in the world in the, in the, in the, in the, in the world in the, in the, in the world in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the, in the"}, {"heading": "1 Definition of the models", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Generative models for sequential data", "text": "A generative model for symbolic sequences is a model that produces an infinite random sequence of symbols (x0,.., xt,.) via a finite alphabet A. The model depends on a number of internal parameters \u03b8: Each \u03b8 defines a probability distribution Pr\u03b8 ((xt) t = 0.1,...) over the set of infinite sequences. In the face of an actual training sequence (xt), the goal of learning is to find the value of the prognostics that maximizes the probability of the training sequence (xt): \u03b8 = argmax promiscuous ability (xt) t = 0.1,...) = argmax promiscuous ability (xt)."}, {"heading": "1.2 Recurrent neural network models", "text": "We present the recurrent network models discussed in this paper, including ordinary recurrent neural networks (RNNs), gated neural networks (GNNs), and leaky GNNs (GLNs).Neural network-based models use a finite-oriented graph N, the network, over a series of units. Internal state is a real-evaluated function over N (the activities), and edges in the graph indicate which units of the network currently t contribute to the calculation of the state of the units. At each time step, each unit i in the network has an activation level. As usual for neural networks, we include a special alway-activated unit i = 0 with at0, used to represent the so-called \"biases.\" The activation levels are used to calculate the output of the network at the time t and the activation levels."}, {"heading": "1.2.1 Recurrent Neural Networks", "text": "In this article, we use the following transition function to calculate the RNN activation levels at step t + 1 (see for example [Jae02]): V t + 1j: = \u03c1jxt + \u2211 i\u03c4ija t i (8) at + 1j: = s (V t + 1 j), (9) where s is a fixed activation function, xt-A is the symbol printed at the time and the sum runs over all edges ij in the network. In the sum, the always activated unit i = 0 is also included to represent \"distortions.\" The parameters to be carried are the input parameters \u03c1jxt and the transition parameters \u03c4ij. The parameter \u03c1ixt can be equivalent as the connecting weight of an input unit activated on reading x.Two standard decisions for the activation function are the logistic function s (V): = eV / (1 + eV) = 1 / e \u2212 V) and the hyperbolic tangent (V)."}, {"heading": "1.2.2 Gated Neural Networks", "text": "Such models were also used in [SMH11], the main difference being the non-linear Softmax function (6), which we use for the output. In GNNs, the activation levels are given at step t + 1 of V t + 1j: = s (V t + 1 j), (11) where s is the same activation function as in RNNs. The sum includes the always activated unit i = 0.In the above line, xt is the symbol that is printed at step t, and the parameters are the transition weights from unit i to unit j given context x x x. The sum includes the always activated unit i = 0.In the above string A is the symbol that is printed at step t, and the parameters are the transition weights from unit i to unit j given context x x."}, {"heading": "1.2.3 Gated Leaky Neural Networks", "text": "Gated leaky neural networks are a variation on GNNs that allow a better handling of some remote temporal dependencies and are better understood by a detour through continuous time models. In GNNs, we have V t + 1j = \u2211 i \u03c4ijxta t i. One possible way to define a continuous time analog is to set dV tj dt = \u2211 i\u03c4ijxta t i (12) and activate atj = s (V t j) as before. See [Jae02] for \"continuous time\" or \"leaky\" neural networks. This creates an \"integration effect\": units are activated when a certain signal text occurs and remain activated until another event occurs. Importantly, the transition coefficient from i to i itself provides feedback control. For this reason, loops i \u2192 i are always included in the graph of the network."}, {"heading": "2 An algorithm for GLNN training", "text": "In Section 3, we set out the theoretical principles along which the Riemannian algorithms for RNN, GNN and GLNN education are to be built. In Section 3, we set out the theoretical principles along which the Riemannian algorithms for RNN, GNN and GLNN education are to be built. In Section 3, we will first collect the explicit form of the final algorithm predicted for GLNNs and discuss its algorithmic costs. These derivatives will be converted into a parameter that translates the log probability of training data in terms of write and transition weights, can be calculated by means of backpropagation by the time adapted to GLNNNNs (appendix B). These derivatives will be converted into a parameter that will be updated."}, {"heading": "3 Constructing invariant algorithms for recurrent", "text": "NetworkWe now give the main ideas for constructing the above algorithm. This approach is not specific to GLNs and also applies to classic recurring networks."}, {"heading": "3.1 Gradients and metrics", "text": "One reason for this is that the gradient ascending path depends on the selected numerical representation of the parameter (see the introduction of [Oll13]). This is clear from the following angle of view. \u2212 The gradient ascending function f, which is to be maximized in dependence on a vector-evaluated parameter, can be maximized the gradient ascending parameter variability of the capabilities used. \u2212 The gradient ascending variable of the capabilities used-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variation-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-variable-"}, {"heading": "3.2 The Fisher metric on the output units and writing weights", "text": "Thus, the totality of Fisher metrics and conditional Fisher metrics that know the preceding symbols are two types of divergence. The metrics for neural networks are initially based on the choice of a metric based on the output of the network [Oll13]. Here, the output of the network is interpreted as a probability distribution based on the sequence (xt) printed by the network. However, Amari's natural gradient and the metrics we use are both based on the Fisher metric [AN00] in the space of probability distributions. One way to define Fisher metrics is as an infinitesimal kullback-Leibler divergence between two infinitely close probability distributions. For recurrent neural networks, there is a choice as to what probability distribution should be taken into consideration. One can either consider the network as the definition of a probability distribution Pr across all output sequences (x0,.,.,.,., conditional,.) of a probability equivalence."}, {"heading": "3.3 Invariant metrics for recurrent networks", "text": "The natural gradients resulting from the changes in time of the parameters are algorithmically expensive to calculate the neural networks (although the \"Hessianfree\" method in terms of the number of transformations of the parameters or activities).We now present metrics for recurring networks that enjoy some of the main characteristics of Fisher metrics (in particular, to build an invariant metric for recurring networks, the \"time-unfolding\" the networks in terms of time-unfolding. \"Any invariant metric for feedforward networks can be used to define an invariant number of\" time-unfolding \"the networks as backpropagation through time [RHW87, Jae02] and then by defining the parameters of the recurrent network as the sum of the corresponding changes in parameters in time."}, {"heading": "3.4 The recurrent unitwise outer product metric", "text": "Suppose we have a loss function L as a function of a parameter that depends on x, and also that L resolves as a sum or average L = Ex = D (x) of a loss function that is used in a dataset by individual data samples x. The outer products of the differentials averaged by x provide a metric of the actual results, namely Ex-D values, which are given by the matrixCij = Ex values. The outer products of the differentials averaged by x provide a metric of the actual results, namely Ex-D values, i.e.: Ex values, x values given by the matrixCij = Ex values. The outer products of the differentials given by x values averaged by x values are. (53) This is the outer product (OP). The associated gradients that are above L values that are above L values are given by x values."}, {"heading": "3.5 The recurrent backpropagated metric", "text": "We are now working on an explicit form of change resulting from the change. (Here, the Fisher metric is on the other hand) One can define a metric on each unit inductively by defining the quadratic norm on the other side of the boundary. (Here, the sum of the quadratic norms of the resulting activity changes is directly influenced by i, i.e. \"backpropagating\" the definition of the metric units on inner units. The metric units on the other side of the boundary are then converted into a metric unit on the other side of the quadratic norms of the resulting activity changes on the other side.) The definition of the quadratic units on the other side is then converted into a metric unit."}, {"heading": "3.6 Invariance of the algorithms", "text": "Amari [Ama98, AN00] pioneered the use of \"invariant\" algorithms for statistical learning that do not depend on a chosen numerical representation (parameterization) of the parameter space of the model. Invariance can often improve performance; for example, in the standard RNNs in the experiments below, by replacing the standard inverse Hessian diagonal with the (invariant) quasi-diagonal inversion, bringing the performance of the RNNs closer to that of GLNs, at very low computing costs. The increase in gradient presented above is invariant by repairing the activities and by repairing the incoming parameters to each unit (but not by repairing the incoming parameters to different units, as is the natural gradient).This comes from its construction using a metric that only depends on the behavior of the network."}, {"heading": "4 Preliminary experiments", "text": "Here we report on a comparison of the performance of GLNN and more traditional RNNs on some synthetic data examples: the \"alphabet with insertion\" (Example 1 from the introduction), synthetic music (Example 3), the remote XOR problem (Example 2), and finally the anbn problem (Example 4). LSTMs are used as additional benchmarks.GLNs to define either the recurring backward metric or the recurring outer product metric, as in Section 2.The RNN reference was learned using traditional (but not na\u00efve) techniques as described below. Remote XOR performance is known to use the \"Hessian-free\" technique [MS11], so we do not test RNN on this example and instead compare performance directly with [MS11]."}, {"heading": "A Parameter initialization, the linearized regime,", "text": "This will give us some insight into the time-integrating effects of the model and also suggest relevant initializations of the parameter values prior to the introduction of the gradient ascent, as presented in the algorithm shown above. In the GLNN evolutionary equation V t + 1j = V + jxta t i = jxta t i = j and the unit i = 0 that is always activated. Substitution atj = s (V t j) and a t solution 1 we getV t + 1j = V + jxts (V t j + jxts) + jxt jxt + jxt jxt = jxt i6 = jxta t i (70) Since s (V tj) is an increasing function of V tj, the contribution i = jxxt offers a fixed loop: jxt i6 = jxt = jxt = jxt if we are a fixed loop."}, {"heading": "B Derivative of the log-likelihood: Backpropaga-", "text": "The derivative of the log probability (xt) t = 0,..., T \u2212 1 is an observed sequence of T symbols in alphabet A. Here we calculate the derivatives of the log probability that a GLNN expression (xt) with respect to the GLNN parameters, using the standard back propagation technique through time.Since we must predict all symbols in the sequence (i.e., the algorithms in section 2 give the probability of the model (x0,., xT \u2212 1). Here, for simplification, we assume that all symbols in the sequence must be predicted (i.e., the algorithms in section 2 give the formulas for the general case.Proposition 11 (log probability for GLNs). The derivative of the log probability of a sequence x = (xt), T \u2212 1 with respect to the parameters of a justified sequence of GLNs is."}, {"heading": "C Fisher metric for the output distribution \u03c0t", "text": "Let us calculate the x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x"}], "references": [{"title": "Natural gradient works efficiently in learning", "author": ["Shun-ichi Amari"], "venue": "Neural Comput.,", "citeRegEx": "Amari.,? \\Q1998\\E", "shortCiteRegEx": "Amari.", "year": 1998}, {"title": "Methods of information geometry, volume 191 of Translations of Mathematical Monographs", "author": ["Shun-ichi Amari", "Hiroshi Nagaoka"], "venue": "American Mathematical Society, Providence, RI,", "citeRegEx": "Amari and Nagaoka.,? \\Q2000\\E", "shortCiteRegEx": "Amari and Nagaoka.", "year": 2000}, {"title": "Adaptive method of realizing natural gradient learning for multilayer perceptrons", "author": ["Shun-ichi Amari", "Hyeyoung Park", "Kenji Fukumizu"], "venue": "Neural Computation,", "citeRegEx": "Amari et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Amari et al\\.", "year": 2000}, {"title": "Alpha-nets: A recurrent \u2019neural\u2019 network architecture with a hidden Markov model interpretation", "author": ["John S. Bridle"], "venue": "Speech Communication,", "citeRegEx": "Bridle.,? \\Q1990\\E", "shortCiteRegEx": "Bridle.", "year": 1990}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John C. Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Sequence transduction with recurrent neural networks", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Learning precise timing with lstm recurrent networks", "author": ["Felix A. Gers", "Nicol N. Schraudolph", "J\u00fcrgen Schmidhuber"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["Salah El Hihi", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems 8,", "citeRegEx": "Hihi and Bengio.,? \\Q1995\\E", "shortCiteRegEx": "Hihi and Bengio.", "year": 1995}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Tutorial on training recurrent neural networks, covering BPTT, RTRL, EKF and the \u2018\u2018echo state network\u2019", "author": ["Herbert Jaeger"], "venue": "Technical Report 159,", "citeRegEx": "Jaeger.,? \\Q2002\\E", "shortCiteRegEx": "Jaeger.", "year": 2002}, {"title": "A clockwork RNN", "author": ["Jan Koutn\u00edk", "Klaus Greff", "Faustino J. Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Koutn\u00edk et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Koutn\u00edk et al\\.", "year": 2014}, {"title": "Deep learning via hessian-free optimization", "author": ["James Martens"], "venue": "Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Martens.,? \\Q2010\\E", "shortCiteRegEx": "Martens.", "year": 2010}, {"title": "Training deep and recurrent neural networks with Hessian-free optimization", "author": ["James Martens", "Ilya Sutskever"], "venue": "Neural Networks: Tricks of the Trade,", "citeRegEx": "Martens and Sutskever.,? \\Q2012\\E", "shortCiteRegEx": "Martens and Sutskever.", "year": 2012}, {"title": "Riemannian metrics for neural networks I: feedforward networks", "author": ["Yann Ollivier"], "venue": null, "citeRegEx": "Ollivier.,? \\Q2013\\E", "shortCiteRegEx": "Ollivier.", "year": 2013}, {"title": "Learning internal representations by error propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1987}, {"title": "Topmoumoute online natural gradient algorithm", "author": ["Nicolas Le Roux", "Pierre-Antoine Manzagol", "Yoshua Bengio"], "venue": null, "citeRegEx": "Roux et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Roux et al\\.", "year": 2007}], "referenceMentions": [], "year": 2015, "abstractText": "Recurrent neural networks are powerful models for sequential data, able to represent complex dependencies in the sequence that simpler models such as hidden Markov models cannot handle. Yet they are notoriously hard to train. Here we introduce a training procedure using a gradient ascent in a Riemannian metric: this produces an algorithm independent from design choices such as the encoding of parameters and unit activities. This metric gradient ascent is designed to have an algorithmic cost close to backpropagation through time for sparsely connected networks. We use this procedure on gated leaky neural networks (GLNNs), a variant of recurrent neural networks with an architecture inspired by finite automata and an evolution equation inspired by continuous-time networks. GLNNs trained with a Riemannian gradient are demonstrated to effectively capture a variety of structures in synthetic problems: basic block nesting as in context-free grammars (an important feature of natural languages, but difficult to learn), intersections of multiple independent Markov-type relations, or long-distance relationships such as the distant-XOR problem. This method does not require adjusting the network structure or initial parameters: the network used is a sparse random graph and the initialization is identical for all problems considered. The problem considered here is to learn a probabilistic model for an observed sequence of symbols (x0, . . . , xt, . . .) over a finite alphabet A. Such a model can be used for prediction, compression, or generalization. Hidden Markov models (HMMs) are frequently used in such a setting. However, the kind of algorithmic structures HMMs can represent is limited because of the underlying finite automaton structure. Examples of simple sequential data that cannot be, or cannot conveniently be, represented by HMMs are discussed below; for instance, subsequence insertions, or intersections of multiple independent constraints.", "creator": "gnuplot 4.6 patchlevel 3"}}}