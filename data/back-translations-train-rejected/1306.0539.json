{"id": "1306.0539", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jun-2013", "title": "On the Performance Bounds of some Policy Search Dynamic Programming Algorithms", "abstract": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on Policy Search algorithms, that compute an approximately optimal policy by following the standard Policy Iteration (PI) scheme via an -approximate greedy operator (Kakade and Langford, 2002; Lazaric et al., 2010). We describe existing and a few new performance bounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern et al., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI) (Kakade and Langford, 2002). By paying a particular attention to the concentrability constants involved in such guarantees, we notably argue that the guarantee of CPI is much better than that of DPI, but this comes at the cost of a relative--exponential in $\\frac{1}{\\epsilon}$-- increase of time complexity. We then describe an algorithm, Non-Stationary Direct Policy Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon situation or 2) a simplified version of the Non-Stationary PI with growing period of Scherrer and Lesner (2012). We provide an analysis of this algorithm, that shows in particular that it enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a time complexity similar to that of DPI.", "histories": [["v1", "Mon, 3 Jun 2013 19:13:53 GMT  (785kb,D)", "http://arxiv.org/abs/1306.0539v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["bruno scherrer"], "accepted": false, "id": "1306.0539"}, "pdf": {"name": "1306.0539.pdf", "metadata": {"source": "CRF", "title": "On the Performance Bounds of some Policy Search Dynamic Programming Algorithms", "authors": ["Bruno Scherrer"], "emails": ["bruno.scherrer@inria.fr"], "sections": [{"heading": null, "text": "- Increase in time complexity. Then we describe an algorithm, the non-stationary direct iteration of guidelines (NSDPI), which can be considered either 1) a variation of the search for guidelines by dynamic programming by Bagnell et al. (2003) on the situation of the infinite horizon or 2) a simplified version of the non-stationary PI with increasing duration by Scherrer and Reader (2012). We provide an analysis of this algorithm, which shows in particular that it enjoys the best of both worlds: its performance guarantee is similar to that of the CPI, but within a time complexity similar to that of the DPI."}, {"heading": "1 Introduction", "text": "The results of the approach to the Dynamic Programming Algorithms include a function (such as alignment with the classification). - Some of the first important results collected by Bertsekas and Tsitsiklis (1996) include a detailed analysis (such as the results of Bertsekas and Tsitsiklis (1996), which describe the proximity to the optimality of the calculated policy as a function of the maximum standard error during iteration. - Some of the first important results are fraught with some errors; it is known that the value of the policy generated by the algorithms may come close to the optimal policy if the errors are small enough to capture the sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-sup-su"}, {"heading": "2 Background", "text": "We consider an infinite horizon discounted by each state. Markov Decision Process Puterman (1994), Bertsekas and Tsitsiklis (1996), where S is a possibly infinite state space, A is a finite space of action, P (ds), a), for all (s, a), is a probability core on S, r: S \u2192 [\u2212 Rmax, Rmax] is a reward function limited by Rmax, and A (0, 1) is a discount factor. A stationary deterministic policy."}, {"heading": "3 Direct Policy Iteration (DPI)", "text": "We start by describing the Direct Policy Iteration (DPI), which was introduced by Lagoudakis and Parr (2003). (Fern et al. (2006) and analyzed by Lazaric et al. (2010). The analysis of this algorithm is based on the following algorithm 1 DPIinput: an initial policy \u03c00 (a distribution \u03bd (for k = 0, 1, 2,. Do more \u2212 \u2212 \u2212 \u2212 \u2212 sum for return: vympkcoefficient, which refer to the distribution of interest for the measurement of the loss) and (the parameters of the algorithm): Definition 1. Let c (1), c (2),. Be the smallest coefficients in [1, sp.) {\u00b2 so that we apply to all i and all sets of policy \u03c01,. (2,.)."}, {"heading": "4 Conservative Policy Iteration (CPI)", "text": "We now turn to the description of Conservative Policy Iteration (CPI) proposed by Cockade and Langford (2002). < CPI (described in Algorithm 2) uses the distribution d\u03c0k, \u03bd = (1 \u2212). Furthermore, it uses an adaptive step variable \u03b1 to generate a stochastic mix of all the actions caused by the successive calls to the approximate greedy operator, which explains the adjective \"conservative.\" Algorithm 2 CPI input: an initial policy procedure > 0 for k = 0, 1,. we do nothing other than the properties returned by the successive calls to the approximate greedy operator. This explains the adjective \"conservative.\" CPI input: an initial policy procedure > 0 for k = 0, 1,."}, {"heading": "5 Non-stationary Direct Policy Iteration (NSDPI)", "text": "We will describe an algorithm that has a similar grade to DPI - in the sense that at every step it makes a complete step towards a new policy - but also has a conservative taste like CPI - in the sense that the policy will evolve more and more slowly and slowly. This algorithm is based on a finite horizon that includes non-stationary measures. We will write the k-horizon policy, the first action under paragraph 1, then the second action under paragraph 2, etc. Its value is based on a finite horizon that includes non-stationary measures. We will write the \"empty\" policy that is not stationary."}, {"heading": "6 Discussion, Conclusion and Future Work", "text": "In this article, we have described two algorithms of the literature, DPI and CPI, and introduced the NSDPI algorithm, which absorbs ideas from both DPI and CPI, while also having some very close algorithmic links with PSDP Bagnell et al. (2003) and the non-stationary PI algorithm with increasing duration from Scherrer and Reader (2012). Figure 1 synthesizes the theoretical guarantees we have discussed about these algorithms. For each such guarantee, we offer the dependence of the performance-related and the number of iterations with respect to 11 \u2212 \u03b3 and the concentration coefficients (for CPI, we assume, like Kakade and Langford (2002), that we have the limits that are new5.One of the most important messages of our work is that what is normally hidden in the constants of performance limits."}, {"heading": "A Proof of Theorem 1", "text": "Our multiple proof here is even slightly more general than what is required: We return the result for all reference policies \u03c0 (and not only for the optimal policy \u03c0). Let's write ek + 1 = max\u03bdp \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 p \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 p \u2212 \u2212"}, {"heading": "B Proof of Theorem 2", "text": "We contain a precise and independent proof, which essentially follows the steps in (Cacade and Langford, 2002). (i) We first show that the value increases as a function of the iterations. Let us consider some iteration k of the algorithm. Based on the facts, the T\u03c0k + 1v\u03c0k = (1 \u2212 \u03b1k + 1) v\u03c0k + 1T\u03c0k \u00b2 k + 1v\u03c0k \u00b2 k and P\u03c0k + 1 = (1 \u2212 \u03b1k + 1) P\u03c0k + 1Pzipk \u00b2 k + 1, we can see that the iteration as a function of (1 \u2212 \u03b5k + 1) v\u03c0k + 1T\u03c0k \u00b2 k \u00b2 k \u00b2 k = (1 \u2212 v\u03c0k \u00b2) is the iteration as a function of (I \u2212 vzipk \u00b2) and the iteration as a function of (I \u2212 vzipk \u2212 1) is the iteration as a function of (I \u2212 vzipk \u00b2) and the iteration as a function of (1)."}, {"heading": "C Proof of Theorem 3", "text": "Using the facts, the T\u03c0k + zipk + zipk + zipk = zipk = zipk = zipk = zipk = zipk = zipk = zipk = zipk = psp = psp = psp = psp + psp = psp + psp = psp + psp + psp + psp + psp + psp + psp + psp + psp + psp + psp + psp = psp + psp = psp + psp = psp + psp = psp + psp = psp + psp + psp = psp + psp = psp + psp = psp + psp = psp + psp = psp + psp = psp + psp + psp \u2212 psp \u2212 psp \u2212 psp + psp = psp \u2212 psp \u2212 psp + psp \u2212 psp \u2212 psp \u2212 psp \u2212 psp + psp \u2212 psp + psp \u2212 psp \u2212 psp \u2212 psp + psp \u2212 psp \u2212 psp \u2212 psp + psp \u2212 psp \u2212 psp + psp \u2212 psp \u2212 psp + psp \u2212 psp \u2212 psp \u2212 psp + psp \u2212 psp \u2212 psp \u2212 psp \u2212 psp \u2212 psp \u2212 psp psp psp = psp psp psp psp = psp psp psp psp = psp = psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp psp p"}, {"heading": "D Proof of Theorem 4", "text": "We start with the first relation. For all of us we have havev\u03c0 \u2212 v\u03c3k = T\u03c0v\u03c0 \u2212 T\u03c0v\u03c3k \u2212 1 \u2264 \u03b3P\u03c0 (v\u03c0 \u2212 v\u03c3k \u2212 1) + ekwhere we get = max\u03c0 \u2032 T\u03c3k \u2212 1 \u2212 T\u03c0v\u03c3k \u2212 1 By multiplying both sides by \u00b5, using the definition 3 and the fact that \u03bdjej is \u2264, we get: \u00b5 (v\u03c0 \u2212 v\u03c3k) \u2264 1 \u0445 i = 0 (\u03b3P\u03c0) iek \u2212 i + \u03b3 kVmax (6) \u2264 k \u2212 1 \u0445 i = 0 \u0445jej \u2264 and the fact that \u03bdjej \u2264 kmax Vmax (v\u03c0 \u2212 v\u03c3k) \u2264 1 \u0445 i = 0 \u0441P\u03c0 \u2212 kVmax = 0 kVmax (6) \u2264 kVmax \u2264 and the fact that \u03bdjej \u2264 kmax Vmax (v\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441"}, {"heading": "E Experiments", "text": "In this section, we present some experiments to illustrate the various algorithms discussed in the publication and to get some insight into their empirical behavior. CPI, as described in Algorithm 2, can be very slow (in an example experiment on a 100 degree problem, it made very slow progress and took several million iterations before it was stopped) and we did not evaluate it further. Instead, we consider two variations: CPI +, which are identical to CPI except that it selects the step in each iteration by performing a line search toward the political output of the classification, and CPI (\u03b1) with \u03b1 = 0.1, which makes \"relative but not too small\" steps in each iteration. We start by describing the domain and the approximation considerately. To assess its quality, we consider finite problems where the exact value function can be calculated."}], "references": [], "referenceMentions": [], "year": 2013, "abstractText": "We consider the infinite-horizon discounted optimal control problem formalized by Markov Decision Processes. We focus on Policy Search algorithms, that compute an approximately optimal policy by following the standard Policy Iteration (PI) scheme via an -approximate greedy operator (Kakade and Langford, 2002; Lazaric et al., 2010). We describe existing and a few new performance bounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern et al., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI) (Kakade and Langford, 2002). By paying a particular attention to the concentrability constants involved in such guarantees, we notably argue that the guarantee of CPI is much better than that of DPI, but this comes at the cost of a relative\u2014exponential in 1 \u2014 increase of time complexity. We then describe an algorithm, Non-Stationary Direct Policy Iteration (NSDPI), that can either be seen as 1) a variation of Policy Search by Dynamic Programming by Bagnell et al. (2003) to the infinite horizon situation or 2) a simplified version of the Non-Stationary PI with growing period of Scherrer and Lesner (2012). We provide an analysis of this algorithm, that shows in particular that it enjoys the best of both worlds: its performance guarantee is similar to that of CPI, but within a time complexity similar to that of DPI.", "creator": "LaTeX with hyperref package"}}}