{"id": "1306.1031", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Jun-2013", "title": "LLAMA: Leveraging Learning to Automatically Manage Algorithms", "abstract": "Algorithm portfolio approaches have achieved remarkable improvements over single solvers. However, the implementation of such systems is often highly specialized and specific to the problem domain. This makes it difficult for researchers to explore different techniques for their specific problems. We present LLAMA, a modular and extensible toolkit that facilitates the exploration of a range of different portfolio techniques on any problem domain. We describe the current capabilities and limitations of the toolkit and illustrate its usage on a set of example SAT problems.", "histories": [["v1", "Wed, 5 Jun 2013 09:35:35 GMT  (39kb,D)", "https://arxiv.org/abs/1306.1031v1", null], ["v2", "Fri, 5 Jul 2013 13:31:08 GMT  (1345kb,D)", "http://arxiv.org/abs/1306.1031v2", null], ["v3", "Wed, 30 Apr 2014 12:55:03 GMT  (1359kb,D)", "http://arxiv.org/abs/1306.1031v3", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["lars kotthoff"], "accepted": false, "id": "1306.1031"}, "pdf": {"name": "1306.1031.pdf", "metadata": {"source": "CRF", "title": "LLAMA: Leveraging Learning to Automatically Manage Algorithms", "authors": ["Lars Kotthoff"], "emails": [], "sections": [{"heading": null, "text": "This document corresponds to LLAMA version 0.6.ar Xiv: 130 6.10 31 One-page Quick StartSo you know about algorithm portfolios and choices and just want to get started. In your R shell, typeinstall.packages (\"Lama\") require (Lama) to install and load LLAMA. We assume that you have two input CSV files for your data - one with features and one with times. The lines denote problem cases and the column function and solution names. Both files have an \"ID\" column that makes it possible to link them. Load them into the data structure required by LLAMA as a follower. Data = Input (read.csv (\"features.csv\"), read.csv (\"times.csv\") You can also use the SAT solver data associated with LLAMA."}, {"heading": "1 Background 3", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 Anatomy of LLAMA 4", "text": "2.1 Implementation................................... 6"}, {"heading": "3 LLAMA for domestic use 6", "text": "3.1 Installation of LLAMA............................................................................................................................."}, {"heading": "4 Advanced functionality 14", "text": "4.1 Processing of input data....................................................................................................................................................................................................................................................."}, {"heading": "5 Case study: SATzilla 19", "text": "........................................................................................"}, {"heading": "6 Case study: Visualising the data 23", "text": "7 Further domestication of LLAMA 31"}, {"heading": "1 Background", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far, until it is so far."}, {"heading": "2 Anatomy of LLAMA", "text": "The focus of the work is on finding a solution to the problem of inequality and the imbalance between the sexes."}, {"heading": "2.1 Implementation", "text": "LLAMA is implemented as an R package, and this approach has many advantages; one of the main advantages is that all the functionality available in R can be used to create algorithm selection models, and this is not limited to the functionality implemented in R itself - there are interfaces to many other packages, such as the well-known Weka Machine Learning Toolkit [6]. The large number of machine learning approaches and algorithms available in R allows LLAMA to quickly evaluate a range of different techniques for selecting algorithms from given data, as illustrated in [15]. The ability to do this is critical for good performance in practice. LLAMA has minimal requirements for implementing machine learning, which allows the user to take full advantage of the functionality that R and third-party packages provide."}, {"heading": "3 LLAMA for domestic use", "text": "LLAMA is implemented as an R package and can be found at http: / / cran. r-project.org / web / packages / llama /, the development repository is located at http: / / bitbucket.org / lkotthoff / llama. One of the main advantages of the R package implementation is that all the functionality available in R can be used to build a performance model. This is not limited to the functionality implemented in R itself - there are interfaces to many other packages, such as the well-known Weka Machine Learning Toolkit [6]. The large number of machine learning approaches and algorithms available in R makes it possible to quickly evaluate a number of different techniques for selecting algorithms from given data, such as in [15]. The ability to do this is crucial for good performance in practice."}, {"heading": "3.1 Installing LLAMA", "text": "LLAMA is available on CRAN. On a computer connected to the Internet, all you need to do is open an R terminal and type in stall.packages (\"llama\"). Alternatively, you can use the graphical package manager provided by your R distribution, or download and install the package file yourself. Once the package is installed, you can load it by typing require (llama)."}, {"heading": "3.2 Reading data", "text": "This year it is so far that it is only a matter of time before it will be so far, until it is so far, until it is so far."}, {"heading": "3.2.1 Example data", "text": "LLAMA contains some sample data that you can play around with at first. It is runtime data from 19 SAT solvers to 2433 SAT instances [8]. 36 features have been measured for each instance. Success data (i.e. whether an algorithm is time-controlled or not) are also available, but the calculation costs for features are not recorded. To use this data, Typedata (satsolvers) are satsolvers.If you want to execute the following commands with this data, run data = satsolvers."}, {"heading": "3.3 Slicing and dicing the data", "text": "This is to avoid overmatching the data in training and test sets, where the learned model is so specific to the data on which it was trained that predictions about everything else are very inaccurate. LLAMA provides functions to split a data set into training and test sets. This is one of the lengthy and error-prone steps that researchers have to deal with in practice and that LLAMA aims to make less painful. To split the data into 60% training and 40% test sets, we can run the following command, provided that your data is available in the data variable.split = trainTest (data) The second (optional) argument of the function specifies which fraction of the total data should be used for training. If we want a 70-30 split instead of a 60-40 part, all we have to do is split = trainTest (data, 0.7).By default, the test and test partitions are split."}, {"heading": "3.4 Training and evaluating models", "text": "In fact, most of them are able to survive on their own."}, {"heading": "3.5 Other available model types", "text": "This year it is more than ever before."}, {"heading": "4 Advanced functionality", "text": "The previous section gave an insight into the core functionality of LLAMA. Beyond that, however, there is much more functionality. All the functions we have used so far require additional arguments with which to adjust them. Model-building functions can work with multiple machine learning algorithms instead of just one. There are other functions that do exciting things 2.2 For appropriate definitions of \"exciting.\""}, {"heading": "4.1 Processing the input data", "text": "Characteristics and performance data are often chaotic - values are missing, the values of some traits are the same in all cases, or there is no correlation between trait values and performance. All of this can affect the performance of machine learning models. LLAMA provides functions to mitigate this."}, {"heading": "4.1.1 Selecting the most important features", "text": "Feature selection is a process in which the features that are relevant to making a particular prediction are identified. If, for example, the values of a feature are the same in all problem cases, the feature does not contribute and can be omitted. However, if the values of a feature change, such as the prediction that needs to be changed, we definitely want to include this feature. LLAMA does not offer actual feature filtering algorithms, just as it does not offer machine learning algorithms. Rather, it provides the infrastructure to use existing algorithms that are usually abundant. As an example, we will use the FSelector package that provides a number of such functions. Feature filtering in LLAMA is done via the feature filtering function. It requires a filter algorithm and data frame to use it, which must be in the usual LLAMA format, i.e. what is returned by the input function."}, {"heading": "4.1.2 Normalising feature values", "text": "For some types of models, it may be desirable to normalize the characteristic values so that they cover the same range over all the characteristics. For example, if we calculate the distance between two instances based on the characteristic values in Euclidean space, and the values for a particular characteristic are randomly 1000 times greater than the others, this characteristic will have the greatest impact on the result, even if it may not be meaningful.LLAMA provides a function that makes it possible to normalize the values of characteristics before they are passed on to the model learners. This functionality differs from the characteristic filtering, which is independent of further operations. In order to normalize the characteristic values, scaling factors must be calculated. The same scaling factors must be applied when working with new data, i.e. when the predictor function returned by the model builder is used. This is why normalization is implemented as an optional argument for the model build functions, and not autonomous functions."}, {"heading": "4.1.3 Imputing censored runtimes", "text": "Working with empirical performance data is often difficult. If the problem cases are challenging, some of the algorithms may take a very long time to solve - longer than you are willing to wait for. Normally, algorithms are executed with a timeout. That is, if the algorithm does not find a solution after a certain time, it is terminated - its runtime is censored. While it is possible to collect data in more reasonable amounts of time, the result complicates machine learning - if an algorithm has expired in time, the recorded runtime is not really the value we want to predict. One way to solve this problem is to transfer the censored runtimes by learning a machine learning model to predict runtime to instances that have not been executed, and then apply it to the instances that have expired in time. This process can be repeated to get better models and estimates [20]. This method is implemented in LLAMA in the implemented function-based data that are not linked to the LAMA's arguments."}, {"heading": "4.2 Meta-learning techniques \u2013 ensembles and stacking", "text": "In fact, most of them are able to survive on their own, without being able to afford it. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...)"}, {"heading": "4.3 Parallel execution", "text": "LLAMA uses the parallelMap construct from the parallelMap package 3 to parallelise execution across cross-validation folds, i.e. the model for each iteration is trained and tested in parallel. The parallelMap construct provides transparent parallelisation that is executed sequentially if no suitable parallel backend is loaded. To enable parallel execution, the user only needs to load a parallel backend, for example, by parallelStartSocket (). Libraries used in the generated processes should use parallelLibrary.library (parallelMap) parallelStartSocket (2) #, # use 2 CPUs parallelLibrary (\"llama,\" \"RWeka\") # LLAMA code parallelStop () After executing these commands, all subsequent calls to LLAMA models should use parallelLibrary (\"# parallama\"), \"# parallama,\" # Wellka."}, {"heading": "5 Case study: SATzilla", "text": "Algorithm selection systems often have additional components for which there is no explicit support in LLAMA. However, this is not necessarily an obstacle, much additional functionality can be achieved by partitioning the data appropriately and using other functionality provided by R. This section outlines how a SATzilla-like system can be implemented using LLAMA and R. Full implementation is outside the scope of this document; instead, this section serves as a guide for researchers who want to implement such measures. There are several techniques that use the different versions of SATzilla to make algorithm portfolios more efficient in practice. We will outline the implementation of each one after the other before we merge them all. The techniques used in this section are described in more detail in the SATzilla papers (e.g. [24, 26.27]). The main difference between the implementation outlined here to calculate SATZilla's good use is that we always handle all the weights of the problem without any idea being equal in the practice."}, {"heading": "5.1 Presolver", "text": "Using a portfolio is always going3https: / / github.com / berndbischl / parallelMapto slower for these problems. To avoid this problem, we can run a presolver for a short period of time. This step is largely independent of selection models for training algorithms. However, we must bear in mind that the problems solved by the presolver within its time limit do not require algorithms to be selected for them, so we should remove them from the data before training the model. It is easy in R.performance = read.csv (\"performance. csv\") presolveLimit = 1 presolver = \"minisat\" newPerformance Data = subset (performance data, performance data = read.presolver] > Asmit (\"performance.csv\") presolveData (\") presolve.csv\") presolveLimit = 1 presolver = \"resolver.\""}, {"heading": "5.2 Prediction of satisfiability", "text": "In practice, it may be useful to distinguish between these instances in algorithm portfolios, which requires a greater number of machine learning models. First, we need to be able to predict whether a given instance is satisfactory or not. Second, we need different algorithm selection models for the satisfactory and unsatisfactory cases. To achieve this, we need additional data on whether an instance is satisfactory or not."}, {"heading": "5.3 Prediction of feature computation time", "text": "Calculating the characteristics of an instance is an integral part of algorithm selection systems. This information can be related to the performance of the algorithms to make predictions about which algorithm to use in a particular case. However, sometimes the calculation of the characteristics may take longer than it would take to resolve the instance. It is clearly desirable to identify these cases before we start calculating the characteristics. To achieve this, we can train another machine learning model that predicts the time needed to calculate the remaining characteristics given a small and inexpensively calculable subset of the characteristics. If the time is too long, we simply run a backup solver on the instance. Such instances should not be used in training and evaluating the algorithm selection."}, {"heading": "5.4 Putting it all together", "text": "In fact, it is a way in which most people are able to survive themselves, \"he told\" Welt am Sonntag \":\" It is very important that people are able to survive themselves. \"(bit.ly)\" It is very important that people are able to survive themselves. \"(bit.ly)\" It is very important that people are able to survive themselves. \"(bit.ly)\" It is very important that people are able to survive themselves. \"(bit.ly)\" It is very important that people are able to survive themselves. \"(bit.n)"}, {"heading": "6 Case study: Visualising the data", "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to live, in which they want to live, in which they live, in which they want to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live."}, {"heading": "7 The further domestication of LLAMA", "text": "The functionality currently implemented in LLAMA allows us to explore the performance of many different approaches to algorithm selection. We intend to develop LLAMA into a platform that not only facilitates the exploration and comparison of different existing approaches, but also facilitates the therapeutic prototyping of new approaches. LLAMA's features take care of the infrastructure required to rigorously train and evaluate machine learning models. Ultimately, the responsibility for interpreting and validating the results lies with the user. Nevertheless, we have used LLAMA for a number of applications, found and fixed a few bugs, and are reasonably confident that it will be useful for other people."}, {"heading": "Acknowledgements", "text": "This work is supported by the EU-FP7 ICT-FET Scholarship 284715 (ICON).The drawing of a llama is kindly available at http: / / www.bluebison.net /. We would like to thank everyone who contributed to LLAMA."}], "references": [{"title": "Pattern Recognition and Machine Learning", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "Ensemble methods in machine learning", "author": ["Thomas G. Dietterich"], "venue": "In International Workshop on Multiple Classifier Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2000}, {"title": "Learning when to use lazy learning in constraint solving", "author": ["Ian P. Gent", "Christopher A. Jefferson", "Lars Kotthoff", "Ian Miguel", "Neil Moore", "Peter Nightingale", "Karen E. Petrie"], "venue": "In 19th European Conference on Artificial Intelligence,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "The WEKA data mining software: An update", "author": ["Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H. Witten"], "venue": "SIGKDD Explor. Newsl.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "An economics approach to hard computational problems", "author": ["Bernardo A. Huberman", "Rajan M. Lukose", "Tad Hogg"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Proteus: A hierarchical portfolio of solvers and transformations", "author": ["Barry Hurley", "Lars Kotthoff", "Yuri Malitsky", "Barry O\u2019Sullivan"], "venue": "In CPAIOR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Algorithm selection and scheduling", "author": ["Serdar Kadioglu", "Yuri Malitsky", "Ashish Sabharwal", "Horst Samulowitz", "Meinolf Sellmann"], "venue": "In 17th International Conference on Principles and Practice of Constraint Programming,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "ISAC instance-specific algorithm configuration", "author": ["Serdar Kadioglu", "Yuri Malitsky", "Meinolf Sellmann", "Kevin Tierney"], "venue": "In Proceeding of the 2010 conference on ECAI 2010: 19th European Conference on Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "author": ["Ron Kohavi"], "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1995}, {"title": "Algorithm selection for combinatorial search problems: A survey", "author": ["Lars Kotthoff"], "venue": "Technical Report arXiv:1210.7959,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Hybrid regression-classification models for algorithm selection", "author": ["Lars Kotthoff"], "venue": "In 20th European Conference on Artificial Intelligence,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Algorithm selection for combinatorial search problems: A survey", "author": ["Lars Kotthoff"], "venue": "AI Magazine,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "An evaluation of machine learning in algorithm selection for search problems", "author": ["Lars Kotthoff", "Ian P. Gent", "Ian Miguel"], "venue": "AI Communications,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Programs for Machine Learning", "author": ["J. Ross Quinlan. C"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1993}, {"title": "A game theoretic approach to measure contributions in algorithm portfolios", "author": ["Talal Rahwan", "Tomasz P. Michalak"], "venue": "Technical Report RR-13-11,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "The algorithm selection problem", "author": ["John R. Rice"], "venue": "Advances in Computers,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1976}, {"title": "A simple method for regression analysis with censored data", "author": ["Josef Schmee", "Gerald J. Hahn"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1979}, {"title": "Data Mining: Practical Machine Learning Tools and Techniques", "author": ["Ian H. Witten", "Eibe Frank", "Mark A. Hall"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Stacked generalization", "author": ["David H. Wolpert"], "venue": "Neural Networks,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1992}, {"title": "No free lunch theorems for optimization", "author": ["David H. Wolpert", "William G. Macready"], "venue": "IEEE Transactions on Evolutionary Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1997}, {"title": "Hierarchical hardness models for SAT", "author": ["Lin Xu", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "In CP,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Hydra: Automatically configuring algorithms for portfolio-based selection", "author": ["Lin Xu", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "In Twenty-Fourth Conference of the Association for the Advancement of Artificial Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "SATzilla: portfolio-based algorithm selection for SAT", "author": ["Lin Xu", "Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": "J. Artif. Intell. Res.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Hydra- MIP: automated algorithm configuration and selection for mixed integer programming. In RCRA Workshop on Experimental Evaluation of Algorithms for Solving Problems with Combinatorial Explosion", "author": ["Lin Xu", "Frank Hutter", "Holger H. Hoos", "Kevin Leyton-Brown"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "[1, 16,21].", "startOffset": 0, "endOffset": 10}, {"referenceID": 17, "context": "[1, 16,21].", "startOffset": 0, "endOffset": 10}, {"referenceID": 4, "context": "An algorithm portfolio [5,7] is a collection of state of the art solvers that are all capable of solving the same kind of problem.", "startOffset": 23, "endOffset": 28}, {"referenceID": 19, "context": "This is known as the no free lunch theorem [23].", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "The most prominent system is probably SATzilla [26], which has dominated SAT solver competitions when it was introduced.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "More recent systems include ISAC [10], Hydra [25] and 3S [9].", "startOffset": 33, "endOffset": 37}, {"referenceID": 21, "context": "More recent systems include ISAC [10], Hydra [25] and 3S [9].", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "More recent systems include ISAC [10], Hydra [25] and 3S [9].", "startOffset": 57, "endOffset": 60}, {"referenceID": 15, "context": "The concept is closely related to the Algorithm Selection Problem [19], which is concerned with identifying the most suitable algorithm for solving a problem.", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "A lot more background information can be found in [14] (even more in the extended version [12]) and the overview table of the relevant literature at http: //4c.", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "A lot more background information can be found in [14] (even more in the extended version [12]) and the overview table of the relevant literature at http: //4c.", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "This is not limited to the functionality that is implemented in R itself \u2013 there are interfaces to many other packages, such as the well-known Weka machine learning toolkit [6].", "startOffset": 173, "endOffset": 176}, {"referenceID": 12, "context": "The large number of machine learning approaches and algorithms available in R makes it possible to use LLAMA to quickly evaluate a range of different techniques for algorithm selection on given data, such as presented in [15].", "startOffset": 221, "endOffset": 225}, {"referenceID": 3, "context": "This is not limited to the functionality that is implemented in R itself \u2013 there are interfaces to many other packages, such as the well-known Weka machine learning toolkit [6].", "startOffset": 173, "endOffset": 176}, {"referenceID": 12, "context": "The large number of machine learning approaches and algorithms available in R makes it possible to use LLAMA to quickly evaluate a range of different techniques for algorithm selection on given data, such as presented in [15].", "startOffset": 221, "endOffset": 225}, {"referenceID": 5, "context": "The data is runtime data for 19 SAT solvers on 2433 SAT instances [8].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "In addition to a simple train-test split, LLAMA also provides a function to create data folds for cross-validation [11], which is in general seen as a more reliable way of evaluating the performance of a learning algorithm.", "startOffset": 115, "endOffset": 119}, {"referenceID": 2, "context": "This approach is used for example in [4].", "startOffset": 37, "endOffset": 40}, {"referenceID": 13, "context": "5 decision tree learner [17], J48 in Weka.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "model$predictions[[1]][[1]] algorithm score 1 clasp 1", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "model$predictions[[1]][[1]] algorithm score 1 clasp 1", "startOffset": 23, "endOffset": 26}, {"referenceID": 22, "context": "A different approach is used for example in older versions of SATzilla [26].", "startOffset": 71, "endOffset": 75}, {"referenceID": 0, "context": "model$predictions[[1]][[1]] algorithm score 1 rsat -1734.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "model$predictions[[1]][[1]] algorithm score 1 rsat -1734.", "startOffset": 23, "endOffset": 26}, {"referenceID": 23, "context": "The approach used in the most recent version of SATzilla is to train classifiers that predict the better algorithm for each pair of algorithms [27].", "startOffset": 143, "endOffset": 147}, {"referenceID": 0, "context": "model$predictions[[1]][[1]] algorithm score 1 clasp 18 2 march_rw 17 3 mxc 16 4 lingeling 15 5 cirminisat 14 6 picosat 13 7 precosat 12 8 glueminisat 11 9 rsat 10 10 kcnfs 7 11 qutersat 7 12 cryptominisat 6 13 minisat 6 14 MPhaseSAT64 6 15 riss 6 16 glucose 3 17 minisat_noelim 3 18 sat4j 1", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "model$predictions[[1]][[1]] algorithm score 1 clasp 18 2 march_rw 17 3 mxc 16 4 lingeling 15 5 cirminisat 14 6 picosat 13 7 precosat 12 8 glueminisat 11 9 rsat 10 10 kcnfs 7 11 qutersat 7 12 cryptominisat 6 13 minisat 6 14 MPhaseSAT64 6 15 riss 6 16 glucose 3 17 minisat_noelim 3 18 sat4j 1", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "A different approach to algorithm selection that is used for example in ISAC [10] is to cluster the training problems and assign the best algorithm", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "model$predictions[[1]][[1]] algorithm score 1 glucose 35080.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "model$predictions[[1]][[1]] algorithm score 1 glucose 35080.", "startOffset": 23, "endOffset": 26}, {"referenceID": 16, "context": "This process can be repeated to get better models and estimates [20].", "startOffset": 64, "endOffset": 68}, {"referenceID": 1, "context": "The two meta-learning concepts implemented in LLAMA are ensembles [3] and stacking [22].", "startOffset": 66, "endOffset": 69}, {"referenceID": 18, "context": "The two meta-learning concepts implemented in LLAMA are ensembles [3] and stacking [22].", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "ensembleModel$predictions[[1]][[1]]", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "ensembleModel$predictions[[1]][[1]]", "startOffset": 31, "endOffset": 34}, {"referenceID": 10, "context": "For regression models, stacking as described in [13] is implemented.", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "stackedModel = regression(LinearRegression , folds, combine=OneR, expand=function(x) { cbind(x, combn(c(1:ncol(x)), 2, function(y) { abs(x[,y[1]] - x[,y[2]]) })) }) While the function given here may appear cryptic at first, it demonstrates one o the advantages of the implementation of LLAMA as an R package.", "startOffset": 141, "endOffset": 144}, {"referenceID": 20, "context": "[24, 26,27]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 22, "context": "[24, 26,27]).", "startOffset": 0, "endOffset": 11}, {"referenceID": 23, "context": "[24, 26,27]).", "startOffset": 0, "endOffset": 11}], "year": 2014, "abstractText": "Algorithm portfolio and selection approaches have achieved remarkable improvements over single solvers. However, the implementation of such systems is often highly customised and specific to the problem domain. This makes it difficult for researchers to explore different techniques for their specific problems. We present LLAMA, a modular and extensible toolkit implemented as an R package that facilitates the exploration of a range of different portfolio techniques on any problem domain. It implements the algorithm selection approaches most commonly used in the literature and leverages the extensive library of machine learning algorithms and techniques in R. We describe the current capabilities and limitations of the toolkit and illustrate its usage on a set of example SAT problems. This document corresponds to LLAMA version 0.6. ar X iv :1 30 6. 10 31 v3 [ cs .A I] 3 0 A pr 2 01 4 One-page quick start So you know about algorithm portfolios and selection and just want to get started. Here we go. In your R shell, type install.packages(\"llama\") require(llama) to install and load LLAMA. We\u2019re going to assume that you have two input CSV files for your data \u2013 one with features and one with times. The rows designate problem instances and the columns feature and solver names. Both files have an \u201cID\u201d column that allows to link them. Load them into the data structure required by LLAMA as follows. data = input(read.csv(\"features.csv\"), read.csv(\"times.csv\")) You can also use the SAT solver data that comes with LLAMA by running data(satsolvers) followed by data = satsolvers. Now partition the entire set of instances into training and test sets for cross-validation. folds = cvFolds(data) This will give you 10 folds for cross-validation. Now we\u2019re ready to train our first model. To do that, we\u2019ll need some machine learning algorithms \u2013 we\u2019re going to use a random forest classifier. Load the randomForest package and train a simple classification model that predicts the best algorithm. require(randomForest) model = classify(randomForest , folds) Great! Now let\u2019s see how well this model is doing and compare its performance to the virtual best solver (VBS) and the single best solver in terms of average PAR10 score. mean(parscores(folds, model)) mean(parscores(data, vbs)) mean(parscores(data, singleBest)) You can use any other classification algorithms instead of randomForest of course. You can also train regression or cluster models, use different train/test splits or preprocess the data by selecting the most important features. More details in the on-line documentation, or just continue reading for an in-depth tour of LLAMA.", "creator": "LaTeX with hyperref package"}}}