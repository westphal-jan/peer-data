{"id": "1306.1326", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2013", "title": "Performance analysis of unsupervised feature selection methods", "abstract": "Feature selection (FS) is a process which attempts to select more informative features. In some cases, too many redundant or irrelevant features may overpower main features for classification. Feature selection can remedy this problem and therefore improve the prediction accuracy and reduce the computational overhead of classification algorithms. The main aim of feature selection is to determine a minimal feature subset from a problem domain while retaining a suitably high accuracy in representing the original features. In this paper, Principal Component Analysis (PCA), Rough PCA, Unsupervised Quick Reduct (USQR) algorithm and Empirical Distribution Ranking (EDR) approaches are applied to discover discriminative features that will be the most adequate ones for classification. Efficiency of the approaches is evaluated using standard classification metrics.", "histories": [["v1", "Thu, 6 Jun 2013 07:42:33 GMT  (542kb)", "http://arxiv.org/abs/1306.1326v1", "7 pages, Conference Publications"]], "COMMENTS": "7 pages, Conference Publications", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["a nisthana parveen", "h hannah inbarani", "e n sathishkumar"], "accepted": false, "id": "1306.1326"}, "pdf": {"name": "1306.1326.pdf", "metadata": {"source": "CRF", "title": "Performance Analysis of Unsupervised Feature Selection Methods", "authors": ["A.Nisthana Parveen", "H.Hannah Inbarani", "E.N. Sathish Kumar"], "emails": ["nisthana@gmail.com", "hhinba@gmail.com", "en.sathishkumar@yahoo.co.in"], "sections": [{"heading": null, "text": "In some cases, too many redundant or irrelevant features can overwhelm the main features for classification, and feature selection can solve this problem, thereby improving predictive accuracy and reducing the computational effort of classification algorithms. The main objective of feature selection is to identify a minimal feature subset from a problem domain while maintaining a reasonably high level of accuracy in the representation of the original features. In this paper, Principal Component Analysis (PCA), Rough PCA, Unsupervised Quick Reduct (USQR) algorithm and Empirical Distribution Ranking (EDR) approaches are used to identify discriminatory features that are most suitable for classification."}, {"heading": "Keywords: Feature Selection, Principal Component Analysis, Rough-PCA, Empirical Distribution, Unsupervised Quick Reduct.", "text": "The goal of the feature selection is to identify the characteristics of the data set as important and to discard any other feature as irrelevant and redundant. As the feature selection reduces the dimensionality of the data, it provides the possibility of a more effective and faster operation of the data set (i.e., data mining algorithms can be operated faster and more effectively by using feature selection).Conventionally monitored FS methods evaluate different feature subgroups using an evaluation function or metric to select only the characteristics related to the decision classes of the data in question."}, {"heading": "II. FEATURE SELECTION METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Principal Component Analysis", "text": "In fact, most of them are able to orient themselves in a direction in which they are in a position in which they are in."}, {"heading": "Algorithm: PCA Input: Data Matrix Output: Reduced set of features", "text": "Step-1: Create X N x d data matrix, with one row vector xn per data point. Step-2: X subtracts the mean x from each row vector xn in X. Step-3: \u03a3 covariance matrix of X. Step-4: Search for eigenvectors and eigenvalues of \u03a3. Step-5: PC is the M eigenvector with the largest eigenvalues. Step-6: Output PCs.Algorithm1: Major component analysis"}, {"heading": "B. Rough-PCA", "text": "This year, the number of people of working age who are able to stay in the region has multiplied."}, {"heading": "Algorithm: Rough PCA Input: Data Matrix Output: Reduced set of features", "text": "Step-1: Normalization of the original dataset. Step-2: Calculation of the main components using the Singular Value Decomposition of the Normalized data matrix. Step-3: Determination of the number of meaningful PCs to be obtained. Step-4: Calculation of the reduced dataset using the reduced PCs. Step-5: Discretization of the dataset. Step-6: Determination of the reduction using the Rough Set Theory (RST)."}, {"heading": "C. Empirical Distribution Ranking", "text": "Then the empirical distribution function is defined as (t) = 1 \u2264 = 1 (12). Where t is the mean value of Xi, is the so-called indicator random variable, which is defined as equal to 1 for property A, and otherwise as 0. Thus, while the distribution function is a function of the t probability with which each of the random variables will be Xi, the empirical distribution function calculated from data indicates the therelative frequency with which the observed values \u2264 t. If you sort the values of (t), you then select the minimum value attributes for ranking [3]."}, {"heading": "Algorithm: EDR Input: Data Matrix Output: Reduced set of features", "text": "Step-1: Sort the original dataset. 1 \"< 2\" < Step-2: Calculate the mean of sorted data Step-3: Find ED with (t). (t) = 1 \u2264 = 1Step-4: Ranking of functions based on ED."}, {"heading": "D. Unsupervised Quick Reduct (USQR) Algorithm", "text": "The USQR algorithm tries to calculate a reduction without exhausting all possible subsets. It starts with an empty set and adds once the attributes that result in the largest increase in the coarse specified dependency metric until it yields the maximum possible value for the data set [2]. According to the algorithm, the meaning dependency of each attribute subgroup is calculated and the best candidate is selected:"}, {"heading": "III. EXPERIMENTAL RESULTS", "text": "In our experiment, PCA, RoughPCA, Unsupervised Quick Reduct and Empirical distribution were implemented with the help of Matlab. A short experimental evaluation for benchmark datasets is presented; the information of the datasets includes names of datasets, number of objects, number of classes and number of attributes listed in Table 1. Characteristics are reduced by PCA, Rough-PCA, Unsupervised Quick Reduct and Empirical Distribution Algorithms."}, {"heading": "A. Weka Classification", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "IV. CONCLUSION", "text": "In this paper, PCA, EDR, Unsupervised Quick Reduct and Rough-PCA were implemented based on rough set theory on some synthetic and biological datasets from data repositories. WEKA is used to calculate the classification accuracy of the selected subset of characteristics. EDR outperforms other methods for multiple datasets than other methods and has proven to be the best method for unattended selection of characteristics."}], "references": [{"title": "Singular value decomposition for genomewide expression data processing and modeling", "author": ["O. Alter", "P.O. Brown", "D Bostein"], "venue": "Proc. Natl. Acad. Sci. USA,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2000}, {"title": "Unsupervised Quick Reduct Algorithm Using Rough Set Theory", "author": ["C. Velayutham", "K. Thangavel"], "venue": "Journal Of Electronic Science And Technology,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Principal component analysis\u2016,New", "author": ["I.T. Jolliffe"], "venue": "York: Springer-Verlag,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "A Hybridized Rough-PCA Approach of Attribute Reduction for High Dimensional Data Set", "author": ["Rajashree Dash", "Rasmita Dash", "Debahuti Mishra"], "venue": "European Journal of Scientific Research, ISSN 1450-216X Vol.44(1),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Rough Sets: Theoretical Aspects of Reasoning about Data", "author": ["Z. Pawlak"], "venue": "Dordrecht: Kluwer Academic Publishers,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}], "referenceMentions": [{"referenceID": 2, "context": "Thus we propose unsupervised feature selection algorithms based on eigenvectors analysis to identify critical original features for principal component [5].", "startOffset": 152, "endOffset": 155}, {"referenceID": 4, "context": "We may partition the attribute set A into two subsets C and D, called condition and decision attributes, respectively[8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "The resulting algorithm is given below [5].", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "It starts off with an empty set and adds in turn, one at a time, those attributes that result in the greatest increase in the rough set dependency metric, until this produces its maximum possible value for the dataset [2].", "startOffset": 218, "endOffset": 221}, {"referenceID": 1, "context": "Weka is freely available on the World-Wide Web and accompanies a new text on data mining [2] which documents and fully explains all the algorithms it contains.", "startOffset": 89, "endOffset": 92}], "year": 2013, "abstractText": "Feature selection (FS) is a process which attempts to select more informative features. In some cases, too many redundant or irrelevant features may overpower main features for classification. Feature selection can remedy this problem and therefore improve the prediction accuracy and reduce the computational overhead of classification algorithms. The main aim of feature selection is to determine a minimal feature subset from a problem domain while retaining a suitably high accuracy in representing the original features. In this paper, Principal Component Analysis (PCA), Rough PCA, Unsupervised Quick Reduct (USQR) algorithm and Empirical Distribution Ranking (EDR) approaches are applied to discover discriminative features that will be the most adequate ones for classification. Efficiency of the approaches is evaluated using standard classification metrics.", "creator": "Microsoft\u00ae Office Word 2007"}}}