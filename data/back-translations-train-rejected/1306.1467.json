{"id": "1306.1467", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2013", "title": "Highly Scalable, Parallel and Distributed AdaBoost Algorithm using Light Weight Threads and Web Services on a Network of Multi-Core Machines", "abstract": "AdaBoost is an important algorithm in machine learning and is being widely used in object detection. AdaBoost works by iteratively selecting the best amongst weak classifiers, and then combines several weak classifiers to obtain a strong classifier. Even though AdaBoost has proven to be very effective, its learning execution time can be quite large depending upon the application e.g., in face detection, the learning time can be several days. Due to its increasing use in computer vision applications, the learning time needs to be drastically reduced so that an adaptive near real time object detection system can be incorporated. In this paper, we develop a hybrid parallel and distributed AdaBoost algorithm that exploits the multiple cores in a CPU via light weight threads, and also uses multiple machines via a web service software architecture to achieve high scalability. We present a novel hierarchical web services based distributed architecture and achieve nearly linear speedup up to the number of processors available to us. In comparison with the previously published work, which used a single level master-slave parallel and distributed implementation [1] and only achieved a speedup of 2.66 on four nodes, we achieve a speedup of 95.1 on 31 workstations each having a quad-core processor, resulting in a learning time of only 4.8 seconds per feature.", "histories": [["v1", "Thu, 6 Jun 2013 16:38:26 GMT  (677kb)", "http://arxiv.org/abs/1306.1467v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["munther abualkibash", "ahmed elsayed", "ausif mahmood"], "accepted": false, "id": "1306.1467"}, "pdf": {"name": "1306.1467.pdf", "metadata": {"source": "CRF", "title": "MULTI-CORE MACHINES", "authors": ["Munther Abualkibash", "Ahmed ElSayed", "Ausif Mahmood"], "emails": ["mabualki@bridgeport.edu,", "aelsayed@bridgeport.edu,", "mahmood@bridgeport.edu"], "sections": [{"heading": null, "text": "DOI: 10.5121 / ijdps.2013.4303 29In this paper, we focus on the method used in the latter category. Viola and Jones have used this method in [4] and their algorithm has proven very successful in real-time face recognition. To learn the classifier for face recognition, Viola Jones uses AdaBoost [5] on a training set of about 5,000 faces and 10,000 non-faces. As the training set and the number of possible functions used in learning a classifier is quite large, the runtime of AdaBoost can be very long and the time needed to train a feature can be in the order of several minutes. Depending on the total number of desired features, the learning time of the algorithm can be several days. For example, to obtain a classifier with 200 features takes about a day on a modern workstation."}, {"heading": "1.1. Related Work", "text": "A recent work on a parallel and distributed implementation of AdaBoost was reported in [1]. Their system consists of a client computer and a server pool. Parallelization of AdaBoost is achieved through the use of feature blocks. They employ four computing nodes in the distributed model, thus achieving an acceleration of only 2.66. Another parallel implementation of face recognition was performed on GPUs in [7] and have demonstrated improving the facial recognition time of the hair feature-based adaboost algorithm to about 30 frames per second (FPS) using CUDA on the GeForce GTX 480 GPU. The main problem in their work is that they focused on the time of object recognition, not on the training time of the classifier, so that the training process is repeated when the training record is updated. Our main focus in this essay is to take the parallelization of the training of the hair feature classifier so that we can organize the training process in close to real-time before making the training process of the training process in short term."}, {"heading": "2. ADABOOST ALGORITHM", "text": "One of the main contributions of Viola and Jones is the integral image [4]. The advantage of using an integral image is to speed up the calculation of the rectangular features used in AdaBoost. We check the calculations in the integral image below and then describe the AdaBoost algorithm."}, {"heading": "2.1. Integral Image", "text": "To get the integral value of the image at position x, y, take the sum of all pixel values located at the top and left of x, y. Figure 1 explains this concept. The following equation explains the calculations to get the integral image: (,) = (,), where (,) represents the integral image and (,) represents the original image. To get the integral image only in Figure 2, the following equation follows: () = 4 + 1 \u2212 (2 + 3)"}, {"heading": "2.2. Extracted Feature Types and Selection", "text": "The features extraction in Viola Jones' algorithm are based on basic hair functions [4, 8]. Five types of features were used in the original algorithm, as in Figure 3. (a) (c) Figure 3. Jones shows two rectangles horizontal and vertical features, Figure (b) shows three rectangles horizontal and vertical features, and Figure (c) shows a four rectangles feature. To calculate the value of each of the features, the sum of the pixels in the white side of the rectangle is subtracted from the dark side."}, {"heading": "2.3. AdaBoost Algorithm used by Viola and Jones", "text": "Suppose there are N numbers of images as a training set. Each image is labeled as 0 for negative images and 1 for positive images, as shown in Table 1. Initialization of the weight for each image in the first round, as shown in the following table: Table 2. Example of images and labels and weights. Images w, w, w, w, w, caption 1 0 0 1 weight 12l 1 m 1 2m 12l where l is the total number of positive images, i.e. faces, and m is the total number of non-surfaces. For t = 1 to T: 1. Normalization of the weight of each image in each round as follows: w, =, where w is the total weight of all images in the same round. 2. Calculation of the error of all features until the feature shows the minimum error. The selected feature is the best weak classifier in each round."}, {"heading": "3. OUR PARALLEL AND DISTRIBUTED WEB SERVICES- BASED ADABOOST ARCHITECTURE", "text": "The original AdaBoost determines the best weak classifier in each round based on the minimum classification error. It must go through all the features and determine which feature produces the minimum error. As there are a large number of features, the execution time is high during the learning phase. Our parallel approach speeds up execution time by efficiently paralleling the AdaBoost algorithm. We implement a triple approach to get results in the shortest time. We run the most important computational part of AdaBoost in parallel using the Task parallel Library (TPL). The parallel task library is a built-in library in the Microsoft.NET framework. The advantage of using TPL is noted in multi-core CPUs, where the declared parallel load is automatically distributed to the various CPU cores by creating lightweight threads called Tasks [6]. In order to improve the execution time of AdaBoost workstations we will continue to run multiple Adarches on the Web, using five of our first features."}, {"heading": "3.1. The Parallel AdaBoost Algorithm", "text": "The AdaBoost algorithm implements three approaches to accelerate execution: parallel execution. Web services and parallel execution on one hierarchy level. Web services and parallel execution on two hierarchy levels."}, {"heading": "3.3.1. Parallel execution", "text": "All features are grouped by type, e.g. three rectangles horizontally, three rectangles vertically, two rectangles horizontally, two rectangles vertically, and four rectangles. Each group is loaded into system memory in parallel. Once all are loaded, the AdaBoost rounds start from 1 to T. Since the goal is to find the minimum error in each group in parallel in each round, five features are selected from five groups. Among them, the feature with the least error is selected, and based on this, the weight for the next round of the AdaBoost algorithm is updated. As the selection of a minimum error function runs in parallel, the execution time is reduced by a factor of five."}, {"heading": "3.3.2. Web Services and Parallel execution on one hierarchal level", "text": "Each set of features is distributed to a separate PC. As there are five groups, five PCs are used to classify features, and one master coordinates the five PCs as shown in Figure 4. The parallel and distributed pseudo-code for this approach is described below. Pseudo-code of a master and five slaves Parallel Adaboost sample images (,),..., (,) where = 0.1 for negative and positive examples. Prepare one master workstation and five slaves. Each slave is assigned to a specific feature type. (Slave 1, Three Rectangles Horizontal) (Slave 2, Three Rectangles Vertical) (Slave 3, Two Rectangles Horizontal) (Slave 4, Two Rectangles Vertical) (Slave 5, Four Rectangles) On Slave's workstations:, Three Rectangles Horizontal (Slave 2, Three Rectangles Vertangles Vertical) (Slave 3, Two Rectangles Vertangles Vertical) (4) (4)."}, {"heading": "3.3.3. Web Services and Parallel Execution on two Hierarchal Levels", "text": "The previous technique divided the work based on the function type. Now we distribute the calculations in one function type to another group of machines in the next hierarchy level, as shown in Figure 5. Prepare a master workstation, five sub-masters and 25 slaves. Each sub-master and his slaves are assigned to a specific function type. (Sub-master 1 and 5 slaves, Three rectangles Horizontal) (Sub-master 2 and 5 slaves, Three-master Vertical) (Sub-master 3 and 5 slaves, Two-master 5 slaves, Two-master 4 and 5 slaves, Two-slaves, Two-master 4 and 5 slaves, Two-master, Two-master, Two-master, Vertical)."}, {"heading": "3. EXPERIMENTAL RESULTS", "text": "We developed four variants of the AdaBoost algorithm, as follows: sequential algorithm. Parallel on a machine that uses only TPL. Web services and parallel execution on a hierarchical level. Web services and parallel execution on two hierarchical levels. Table 3 shows a comparison of the different approaches we have implemented. These results show a significant improvement in acceleration compared to the previous work reported in [1]. We achieve an acceleration of 95.1 compared to an acceleration of 2.6 reported in [1]. Figure 6 shows the parallel execution time of our implementation by increasing the number of slaves. 31 machines achieve an execution time per function of 4.8 seconds. In order to be able to predict the acceleration for any number of available machines, we develop the following predictive equation for calculating parallel execution time based on the number of nodes in the last level connected to a sub-node (see figure 7 and middle node)."}, {"heading": "5. CONCLUSIONS", "text": "We have developed a parallel and distributed hybrid implementation of the AdaBoost algorithm, which utilizes the multiple cores of a CPU using lightweight threads, and we also use multiple machines using web service software architecture to achieve high scalability. In addition, we are developing a novel hierarchical web services-based distributed architecture to maximize parallelism in the AdaBoost algorithm. We demonstrate near-linear acceleration to the number of processors available to us and can manage learning a feature in the AdaBoost algorithm within a few seconds. This can be particularly useful when the classifier needs to be dynamically adapted to changing training data, e.g. when recognizing vehicle models. Compared to the previously published work that used a single-stage master slave in parallel and distributed implementation [8], and only an acceleration of 2.66 by using four nodes, we achieve quadruple acceleration from 931 to 4.8 workstations each, which can have an acceleration of 5.8 to 4.8 in one workstation."}], "references": [{"title": "A distributed parallel AdaBoost algorithm for face detection", "author": ["H. ZheHuang", "S. Xiaodong"], "venue": "Intelligent Computing and Intelligent Systems (ICIS), 2010 IEEE International Conference on, 2010, pp. 147-150.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "A Survey of Recent Advances in Face Detection", "author": ["Z.Z.C. Zhang"], "venue": "Technical Report 66, Microsoft Research, Redmond, Washington, USA (June 2010).", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Detecting faces in images: a survey", "author": ["Y. Ming-Hsuan"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 24, pp. 34-58, 2002.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Robust Real-Time Face Detection", "author": ["P. Viola", "M.J. Jones"], "venue": "Int. J. Comput. Vision, vol. 57, pp. 137-154, 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "presented at the Proceedings of the Second European Conference on Computational Learning Theory, 1995.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "The design of a task parallel library", "author": ["D. Leijen"], "venue": "presented at the Proceedings of the 24th ACM SIGPLAN conference on Object oriented programming systems languages and applications, Orlando, Florida, USA, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Haar Classifiers for Object Detection with CUDA", "author": ["A. Obukhov"], "venue": "GPU Computing Gems Emerald Edition (Applications of GPU Computing Series). vol. (1), W.-m. W. Hwu, Ed., ed: Morgan Kaufmann's Applications of GPU Computing Series, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "A General Framework for Object Detection", "author": ["C.P. Papageorgiou"], "venue": "presented at the Proceedings of the Sixth International Conference on Computer Vision, 1998.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 0, "context": "In comparison with the previously published work, which used a single level master-slave parallel and distributed implementation [1] and only achieved a speedup of 2.", "startOffset": 129, "endOffset": 132}, {"referenceID": 0, "context": ", [1-4].", "startOffset": 2, "endOffset": 7}, {"referenceID": 1, "context": ", [1-4].", "startOffset": 2, "endOffset": 7}, {"referenceID": 2, "context": ", [1-4].", "startOffset": 2, "endOffset": 7}, {"referenceID": 3, "context": ", [1-4].", "startOffset": 2, "endOffset": 7}, {"referenceID": 2, "context": "A survey on face detection techniques presented in [3] classifies recent work into four major categories: First, knowledge-based methods where human knowledge is the basis for face determination.", "startOffset": 51, "endOffset": 54}, {"referenceID": 3, "context": "Viola and Jones have used this method in [4] and their algorithm has proven to be very successful in real time face detection.", "startOffset": 41, "endOffset": 44}, {"referenceID": 4, "context": "For learning of the face detection classifier, Viola Jones\u2019 algorithm uses AdaBoost [5] on a training set of approximately 5000 faces and 10,000 non faces.", "startOffset": 84, "endOffset": 87}, {"referenceID": 5, "context": ", Task Parallel Library [6] allows creation of light weight threads where the cost to launch a thread is only 50 assembly language instructions as opposed to approximately 200 instructions for a regular thread.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "A recent work at a parallel and distributed implementation of AdaBoost has been reported in [1].", "startOffset": 92, "endOffset": 95}, {"referenceID": 6, "context": "Another parallel implementation of face detection has carried out on GPUs in [7] and have demonstrated the improvement in face detection time of the Haar feature based Adaboost algorithm to about 30 frames per second (FPS) using CUDA on the GeForce GTX 480 GPU.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "One of the main contributions of Viola and Jones is the integral image [4].", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "The features extraction in Viola Jones\u2019 algorithm are based on Haar basis functions [4, 8].", "startOffset": 84, "endOffset": 90}, {"referenceID": 7, "context": "The features extraction in Viola Jones\u2019 algorithm are based on Haar basis functions [4, 8].", "startOffset": 84, "endOffset": 90}, {"referenceID": 3, "context": "To calculate the value of each one of the features, the sum of pixels located in the white side of the rectangle are subtracted from the dark side [4].", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "The set of faces which we use for training purpose is the same one that has been used by Viola and Jones for face detection [4].", "startOffset": 124, "endOffset": 127}, {"referenceID": 4, "context": "Viola and Jones have used AdaBoost to combine weak classifiers into a stronger classifier [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 3, "context": "The conventional AdaBoost algorithm works by assigning good features relatively higher weight and the poor ones a smaller weight to determine the best weak classifier [4].", "startOffset": 167, "endOffset": 170}, {"referenceID": 5, "context": "The advantage of using TPL is noticed in multi-core CPUs, where the declared parallel workload is automatically distributed between the different CPU cores by creating light weight threads called tasks [6].", "startOffset": 202, "endOffset": 205}, {"referenceID": 0, "context": "These results show a significant improvement in speedup as compared to the previous work reported in [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "6 reported in [1].", "startOffset": 14, "endOffset": 17}, {"referenceID": 7, "context": "In comparison with the previously published work, which used a single level master slave parallel and distributed implementation [8], and only achieved a speedup of 2.", "startOffset": 129, "endOffset": 132}], "year": 2013, "abstractText": "AdaBoost is an important algorithm in machine learning and is being widely used in object detection. AdaBoost works by iteratively selecting the best amongst weak classifiers, and then combines several weak classifiers to obtain a strong classifier. Even though AdaBoost has proven to be very effective, its learning execution time can be quite large depending upon the application e.g., in face detection, the learning time can be several days. Due to its increasing use in computer vision applications, the learning time needs to be drastically reduced so that an adaptive near real time object detection system can be incorporated. In this paper, we develop a hybrid parallel and distributed AdaBoost algorithm that exploits the multiple cores in a CPU via light weight threads, and also uses multiple machines via a web service software architecture to achieve high scalability. We present a novel hierarchical web services based distributed architecture and achieve nearly linear speedup up to the number of processors available to us. In comparison with the previously published work, which used a single level master-slave parallel and distributed implementation [1] and only achieved a speedup of 2.66 on four nodes, we achieve a speedup of 95.1 on 31 workstations each having a quad-core processor, resulting in a learning time of only 4.8 seconds per feature.", "creator": null}}}