{"id": "1306.1553", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Jun-2013", "title": "Direct Uncertainty Estimation in Reinforcement Learning", "abstract": "Optimal probabilistic approach in reinforcement learning is computationally infeasible. Its simplification consisting in neglecting difference between true environment and its model estimated using limited number of observations causes exploration vs exploitation problem. Uncertainty can be expressed in terms of a probability distribution over the space of environment models, and this uncertainty can be propagated to the action-value function via Bellman iterations, which are computationally insufficiently efficient though. We consider possibility of directly measuring uncertainty of the action-value function, and analyze sufficiency of this facilitated approach.", "histories": [["v1", "Thu, 6 Jun 2013 20:57:19 GMT  (130kb)", "http://arxiv.org/abs/1306.1553v1", "AGI-13 Workshop paper"], ["v2", "Tue, 25 Jun 2013 14:32:12 GMT  (57kb)", "http://arxiv.org/abs/1306.1553v2", "AGI-13 Workshop paper"]], "COMMENTS": "AGI-13 Workshop paper", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["sergey rodionov", "alexey potapov", "yurii vinogradov"], "accepted": false, "id": "1306.1553"}, "pdf": {"name": "1306.1553.pdf", "metadata": {"source": "CRF", "title": "Direct Uncertainty Estimation in Reinforcement Learning", "authors": ["Sergey Rodionov", "Alexey Potapov", "Yurii Vinogradov"], "emails": ["potapov@aideus.com", "rodionov@aideus.com"], "sections": [{"heading": null, "text": "Direct estimation of the uncertainty in the area of Reinforcement LearningSergey Rodionov1,2, Alexey Potapov1,3, Yurii Winogradov31AIDEUS, Russia 2Aix Marseille Universit\u00e9, CNRS, LAM (Laboratoire d'Astrophysique de Marseille) UMR7326, 13388, Marseille, France 3National Research University of Information Technology, Mechanics and Optics, St. Petersburg, Russia {potapov, rodionov} @ aideus.com. Optimal probabilistic approach in the area of Arcement Learning is mathematically not feasible. Its simplification, which consists in neglecting the difference between the actual environment and its model, which is estimated on the basis of a limited number of observations, causes an exploration versus exploitation problem. Uncertainty can be expressed in the form of a probability distribution over the space of environmental models, and this uncertainty can be transferred via Bellman to the action-value function, which is inefficient from the computer."}, {"heading": "1 Introduction", "text": "There is no generally accepted formal description of this term. Probability theory is the most traditional way of describing uncertainty, but an adequate interpretation of probability itself is not so clear. However, this can be seen by numerous paradoxes in probability theory, which is why some attempts have been made to expand probability theory, the most well-known of which is the fuzzy set theory. However, fuzzy operations can be regarded as probabilistic operations with some additional assumptions about operations (e.g. their independence) that allow simplification of their calculations; the main difference is not in formalities, but in their interpretation and application. Differences in interpretations often occur because complex systems (e.g. verbal approaches) are analyzed. Some simple measures of uncertainty cannot be applied in such cases without being part of a model of intelligence."}, {"heading": "2 Related Works", "text": "Consider the traditional settings for RL agents [2]. Let an RL agent in a Markov environment Q \u2032 Q \u2032 as a true environment (defined by a state room S, a set of possible actions A, transition probabilities P (s '| s, a), 1,0 [: \u2192 \u00b7 \u00b7 SASP, and a reward function R (s, a, s'), R \u2192 \u00b7 \u00b7 \u00b7 \u00b7 SASR: where R is a set of possible reward values. The value function V\u03c0 (s) for some political developments is calculated as the added uncertainty of future rewards."}, {"heading": "3 Direct Estimation of Uncertainty of Q", "text": "Uncertainty propagation has a clear sense, but it violates the main advantage of model-free amplification Q (Q) updates. Is it possible to estimate the uncertainty of Q (Q) without explicitly constructing environmental models, as it can be done while estimating Q itself? Or is it at least possible to avoid the propagation of uncertainty? One can try to empirically evaluate variations of Q (s, a) for each pair (s, a). In fact, changes in Q (s, a) can be caused by the lack of knowledge of the agent. Therefore, the actor should simply select averaged Qt2 (s, a) in addition to averaged Qt (s, a) for each averaged environment. Unfortunately, this approach only works in deterministic environments. Therefore, it is fairly clear that the stochasticity of the environment causes persistent variations of Q values. Consider the following well-known update rule as an example of Q (st, at, at, at st, at, at, at (Q) st (Q) (Q) (Q)."}, {"heading": "4 Experiments", "text": "In our experiments we looked at \"stratified\" environments with the following structure. Zero level (l = 0) Q-split it-split value; all other levels (l = 1... m) have n states per level. Single state on zero level (l = 0), has n possible actions, and each of them leads with probability p = 1.0 to corresponding state on l = 1. Each state on the last level (l = m) has only one possible action leading to the single state on zero level with p = 1.0. Each state on intermediate levels l = 1... m-1 has k possible actions, each of which has two possible results leading to one of two states on l = i + 1 (probabilities of possible outcomes of each action are chosen randomly). Here we used m = 20, n = 10, k = 2.We compared three algorithms: (1) ordinary Q-learning with \u03b5-greedy strategy, which is turned off after some big steps (when Q-function is learned)."}, {"heading": "Acknowledgements", "text": "This work was supported by the Ministry of Education and Science of the Russian Federation."}], "references": [{"title": "Algorithmic Probability, Heuristic Programming and AGI", "author": ["R. Solomonoff"], "venue": "Baum, E., Hutter, M., Kitzelmann, E. (eds). Advances in Intelligent Systems Research, vol. 10 (proc. 3 Conf. on Artificial General Intelligence), pp. 151\u2013157", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT Press, Cambridge, MA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1998}, {"title": "Uncertainty in Reinforcement Learning \u2014 Awareness, Quantisation, and Control", "author": ["D. Schneegass", "A. Hans", "S. Udluft"], "venue": "Robot Learning, S. Jabin (ed.), pp. 65\u201390", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient Uncertainty Propagation for Reinforcement Learning with Limited Data", "author": ["A. Hans", "S. Udluft"], "venue": "Proc. Int. Conf. on Artificial Neural Networks (ICANN 2009), Cyprus, Part I, LNCS 5768, pp. 70\u201379", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "Exactly this case helps to reveal difficulties in classic probability theory, which are solved within algorithmic probability theory [1].", "startOffset": 133, "endOffset": 136}, {"referenceID": 1, "context": "Consider traditional settings for RL-agents [2].", "startOffset": 44, "endOffset": 47}, {"referenceID": 2, "context": "For example, uncertainty in transition probabilities is represented by covariance matrices in [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 3, "context": "In [4], this approach is simplified by ignoring covariances (only dispersions are used).", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "Authors of [3] proposed to introduce some modified action-value function for uncertainty-aware policy improvement", "startOffset": 11, "endOffset": 14}, {"referenceID": 2, "context": "Other approaches to incorporating uncertainty in RL exist (see references in [3, 4]), but the approach mentioned above is the most appropriate for our objectives.", "startOffset": 77, "endOffset": 83}, {"referenceID": 3, "context": "Other approaches to incorporating uncertainty in RL exist (see references in [3, 4]), but the approach mentioned above is the most appropriate for our objectives.", "startOffset": 77, "endOffset": 83}, {"referenceID": 0, "context": "The acceptance-rejection method is used here: pi are randomly sampled from [0, 1]; they", "startOffset": 75, "endOffset": 81}, {"referenceID": 0, "context": "0); then they are accepted if the value randomly chosen from the range [0, 1] is smaller than their likelihood (prior probabilities are assumed to be uniformly distributed).", "startOffset": 71, "endOffset": 77}], "year": 2013, "abstractText": "Optimal probabilistic approach in reinforcement learning is computationally infeasible. Its simplification consisting in neglecting difference between true environment and its model estimated using limited number of observations causes exploration vs exploitation problem. Uncertainty can be expressed in terms of a probability distribution over the space of environment models, and this uncertainty can be propagated to the action-value function via Bellman iterations, which are computationally insufficiently efficient though. We consider possibility of directly measuring uncertainty of the action-value function, and analyze sufficiency of this facilitated approach.", "creator": "PScript5.dll Version 5.2"}}}