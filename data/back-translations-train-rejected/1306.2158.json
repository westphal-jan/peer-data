{"id": "1306.2158", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2013", "title": "\"Not not bad\" is not \"bad\": A distributional account of negation", "abstract": "With the increasing empirical success of distributional models of compositional semantics, it is timely to consider the types of textual logic that such models are capable of capturing. In this paper, we address shortcomings in the ability of current models to capture logical operations such as negation. As a solution we propose a tripartite formulation for a continuous vector space representation of semantics and subsequently use this representation to develop a formal compositional notion of negation within such models.", "histories": [["v1", "Mon, 10 Jun 2013 10:29:09 GMT  (25kb,D)", "http://arxiv.org/abs/1306.2158v1", "9 pages, to appear in Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality"]], "COMMENTS": "9 pages, to appear in Proceedings of the 2013 Workshop on Continuous Vector Space Models and their Compositionality", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["karl moritz hermann", "edward grefenstette", "phil blunsom"], "accepted": false, "id": "1306.2158"}, "pdf": {"name": "1306.2158.pdf", "metadata": {"source": "CRF", "title": "\u201cNot not bad\u201d is not \u201cbad\u201d: A distributional account of negation", "authors": ["Karl Moritz Hermann", "Edward Grefenstette", "Phil Blunsom"], "emails": ["firstname.lastname@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "These models, mathematically instantiated as sets of vectors in high-dimensional vector spaces, have therefore been applied to tasks such as logical extraction (Grefenstette, 1994; Curran, 2004), literal discrimination (Schatz, 1998), automated essayistic labeling (Landauer and Dumais, 1997), and so on. In recent years, research has shifted from the use of distribution methods to model words to use words to model the semantics of larger linguistic units such as phrases or whole sentences, to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012)."}, {"heading": "2 Motivation and Related Work", "text": "The various approaches to linking logic and distributed semantics can be broadly classified into three categories: those that use distribution models to improve existing logical tools; those that attempt to replicate logic with the mathematical constructions of distribution models; and those that provide new mathematical definitions of logical operations within distribution semantics. The work presented in this paper is in the third category, but in this section we will also give a brief overview of related practices in the other two areas to better describe the work of this paper."}, {"heading": "3 Logic in text", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "4 A general matrix-vector model", "text": "After discussing above how to divide the vector component of a word into domain and value, we now turn to the partition between semantic content and function. A good candidate for modeling this partition would be a dual-space representation similar to that of Socher et al. (2012). In this section, we show that this type of representation is not well suited to modeling negation. Models that use dual-space representations have been proposed in several recent publications, notably in Turney (2012) and Socher et al. (2012). We use the class of recursive matrix-vector models as the basis for our study; for a detailed introduction, see the MV-RNN model described in Socher et al. (2012). We start by describing the composition for a general dual-space model, and apply this model to the notion of recursive matrix-vector models."}, {"heading": "4.1 The question of non-linearities", "text": "Whereas nonlinearity g could be given greater expressiveness, as in the boolean experiment in Socher et al. (2012), the aim of this paper is to place the burden of compositivity on atomic representations instead. Therefore, we treat g as identity function and WM, Wv as simple additive matrices in this study by setting g = I Wv = WM = [I] where I am an identity matrix. This simplification is justified for several reasons. A simple nonlinearity such as the frequently used hyperbolic tangent or sigmoid function will not be sufficient to overcome the problems outlined in this paper. Only a highly complex nonlinear function would be able to meet the requirements of vector space-based logic as discussed above. However, such a function would frustrate the point by shifting the \"heavy lift\" from the model structure into a separate function."}, {"heading": "4.2 Negation", "text": "We have outlined our formal requirements for negation in the previous section, and from these requirements we can derive four equations = 4 values relating to the effects of negation and double negation on the semantic representation and function of a term. J\u00b5 and J\u03bd (shown in Figure 2) describe a partially scaled and inverted identity matrix in which 0 < \u00b5, \u03bd < 1.fv (not, a) = J\u00b5va (4) fM (not, a) \u2248 Ma (5) fv (not, a) = Jumbo (not, a) = Jumbo (not, a))). Based on our assumption of constant domain and interaction via negation, we can approximate equality by a strict equality in Equations 5 and 7. Furthermore, we assume that both Ma 6 = I and Ma 6 = 0, i.e. that A has a specific and non-zero functional representation."}, {"heading": "5 Analysis", "text": "The problem identified with the above models of the MV-RNN style extends to a number of other models of compositivity with vector spacing. It can be considered a problem of an uninformed composition caused by a composition function that does not take into account the syntax and hence the scope. Of course, determining the extent of the negation is in itself a difficult problem - see, for example, the common task * SEM 2012 (Morante and Blanco, 2012). However, at least for simple cases, we can derive the scope by looking at the parse tree of a sentence: if we consider the parse tree for this car to be not blue, it becomes clear that the extent of the expressed negation includes the color, but not the car (Figure 3). While the MV-RNN model in Socher et al. (2012) includes parse trees to derive the order of its compositional steps, it is a uniform compositional function in all steps."}, {"heading": "6 An improved model", "text": "As we have outlined in this paper, an essential prerequisite for a compositional principle motivated by formal semantics is the ability to propagate functional representations, but also not to propagate these representations when this happens, semantically inadequate. At this point, we propose a modification of the class MV-RNN of models that can grasp this inclination without the need to shift the compositional logic into non-linearity. We add a parameter \u03b1 to the representation of each word and control the degree to which its functional representation spreads after it has been applied in its own compositional stage. Therefore, the compositional step of the new model requires three equations: Mp = WM [\u03b1a\u03b1a\u03b1a + \u03b1b Ma\u03b1b Mb] (8) vp = g (Wv [Mavb Mbva]) (9) \u03b1p = max (\u03b1a, \u03b1b) denagate (10)."}, {"heading": "7 Discussion and Further Work", "text": "In this paper, we examined the ability of continuous vector-spatial models to capture the semantics of logical operations in non-Boolean cases. Recursive and recursive vector-spatial models of meaning have achieved a considerable amount of success in recent years and it has been shown that the complexity and subsequent power of these models comes at a cost that it can be difficult to determine the aspect of a model that is responsible for what behavior. This problem was recently highlighted by an investigation of recursive autoencoders for sensory analysis (Scheible and Protectors, 2013). Thus, one of the most important challenges in this field of research is the question of how the power of these models is controlled."}, {"heading": "Acknowledgements", "text": "The first author is supported by the UK Engineering and Physical Sciences Research Council (EPSRC), the second by the EPSRC Grant EP / I03808X / 1."}], "references": [{"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["M. Baroni", "R. Zamparelli."], "venue": "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages", "citeRegEx": "Baroni and Zamparelli.,? 2010", "shortCiteRegEx": "Baroni and Zamparelli.", "year": 2010}, {"title": "Montague meets markov: Deep semantics with probabilistic logical form", "author": ["I. Beltagy", "C. Chau", "G. Boleda", "D. Garrette", "E. Erk", "R. Mooney."], "venue": "June.", "citeRegEx": "Beltagy et al\\.,? 2013", "shortCiteRegEx": "Beltagy et al\\.", "year": 2013}, {"title": "A comparison of vector-based representations for semantic composition", "author": ["W. Blacoe", "M. Lapata."], "venue": "Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing.", "citeRegEx": "Blacoe and Lapata.,? 2012", "shortCiteRegEx": "Blacoe and Lapata.", "year": 2012}, {"title": "Wide-coverage semantic analysis with boxer", "author": ["J. Bos."], "venue": "Proceedings of the 2008 Conference on Semantics in Text Processing, pages 277\u2013286. Association for Computational Linguistics.", "citeRegEx": "Bos.,? 2008", "shortCiteRegEx": "Bos.", "year": 2008}, {"title": "Mathematical Foundations for a Compositional Distributional Model of Meaning", "author": ["B. Coecke", "M. Sadrzadeh", "S. Clark."], "venue": "March.", "citeRegEx": "Coecke et al\\.,? 2010", "shortCiteRegEx": "Coecke et al\\.", "year": 2010}, {"title": "From distributional to semantic similarity", "author": ["J.R. Curran."], "venue": "Ph.D. thesis.", "citeRegEx": "Curran.,? 2004", "shortCiteRegEx": "Curran.", "year": 2004}, {"title": "A structured vector space model for word meaning in context", "author": ["K. Erk", "S. Pad\u00f3."], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP \u201908, (October):897.", "citeRegEx": "Erk and Pad\u00f3.,? 2008", "shortCiteRegEx": "Erk and Pad\u00f3.", "year": 2008}, {"title": "A synopsis of linguistic theory 19301955", "author": ["J.R. Firth."], "venue": "Studies in linguistic analysis.", "citeRegEx": "Firth.,? 1957", "shortCiteRegEx": "Firth.", "year": 1957}, {"title": "Integrating logical representations with probabilistic information using markov logic", "author": ["D. Garrette", "K. Erk", "R. Mooney."], "venue": "Proceedings of the Ninth International Conference on Computational Semantics, pages 105\u2013114. Association for Computational", "citeRegEx": "Garrette et al\\.,? 2011", "shortCiteRegEx": "Garrette et al\\.", "year": 2011}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["E. Grefenstette", "M. Sadrzadeh."], "venue": "Proceedings of EMNLP, pages 1394\u20131404.", "citeRegEx": "Grefenstette and Sadrzadeh.,? 2011", "shortCiteRegEx": "Grefenstette and Sadrzadeh.", "year": 2011}, {"title": "Multi-step regression learning for compositional distributional semantics", "author": ["E. Grefenstette", "G. Dinu", "Y. Zhang", "M. Sadrzadeh", "M. Baroni."], "venue": "Proceedings of the Tenth International Conference on Computational Semantics. Association for Compu-", "citeRegEx": "Grefenstette et al\\.,? 2013", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2013}, {"title": "Towards a formal distributional semantics: Simulating logical calculi with tensors", "author": ["E. Grefenstette."], "venue": "Proceedings of the Second Joint Conference on Lexical and Computational Semantics.", "citeRegEx": "Grefenstette.,? 2013", "shortCiteRegEx": "Grefenstette.", "year": 2013}, {"title": "Explorations in automatic thesaurus discovery", "author": ["G. Grefenstette"], "venue": null, "citeRegEx": "Grefenstette.,? \\Q1994\\E", "shortCiteRegEx": "Grefenstette.", "year": 1994}, {"title": "The role of syntax in vector space models of compositional semantics", "author": ["K.M. Hermann", "P. Blunsom."], "venue": "Proceedings of ACL, Sofia, Bulgaria, August. Association for Computational Linguistics.", "citeRegEx": "Hermann and Blunsom.,? 2013", "shortCiteRegEx": "Hermann and Blunsom.", "year": 2013}, {"title": "A unified sentence space for categorical distributionalcompositional semantics: Theory and experiments", "author": ["D. Kartsaklis", "M. Sadrzadeh", "S. Pulman."], "venue": "Proceedings of 24th International Conference on Computational Linguistics (COLING 2012):", "citeRegEx": "Kartsaklis et al\\.,? 2012", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2012}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais."], "venue": "Psychological review.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Vector-based models of semantic composition", "author": ["J. Mitchell", "M. Lapata."], "venue": "Proceedings of ACL, volume 8.", "citeRegEx": "Mitchell and Lapata.,? 2008", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2008}, {"title": "Composition in Distributional Models of Semantics", "author": ["J. Mitchell", "M. Lapata."], "venue": "Cognitive Science.", "citeRegEx": "Mitchell and Lapata.,? 2010", "shortCiteRegEx": "Mitchell and Lapata.", "year": 2010}, {"title": "English as a Formal Language", "author": ["R. Montague."], "venue": "Formal Semantics: The Essential Readings.", "citeRegEx": "Montague.,? 1974", "shortCiteRegEx": "Montague.", "year": 1974}, {"title": "SEM 2012 shared task: resolving the scope and focus of negation", "author": ["R. Morante", "E. Blanco."], "venue": "Proceedings of the First Joint Conference on Lexical and Computational Semantics - Volume 1: Proceedings of the main conference and the shared task,", "citeRegEx": "Morante and Blanco.,? 2012", "shortCiteRegEx": "Morante and Blanco.", "year": 2012}, {"title": "Markov logic networks", "author": ["M. Richardson", "P. Domingos."], "venue": "Machine learning, 62(1-2):107\u2013136.", "citeRegEx": "Richardson and Domingos.,? 2006", "shortCiteRegEx": "Richardson and Domingos.", "year": 2006}, {"title": "Cutting recursive autoencoder trees", "author": ["C. Scheible", "H. Sch\u00fctze."], "venue": "Proceedings of the International Conference on Learning Representations.", "citeRegEx": "Scheible and Sch\u00fctze.,? 2013", "shortCiteRegEx": "Scheible and Sch\u00fctze.", "year": 2013}, {"title": "Automatic word sense discrimination", "author": ["H. Sch\u00fctze."], "venue": "Computational linguistics, 24(1):97\u2013123.", "citeRegEx": "Sch\u00fctze.,? 1998", "shortCiteRegEx": "Sch\u00fctze.", "year": 1998}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["R. Socher", "E.H. Huang", "J. Pennington", "A.Y. Ng", "C.D. Manning."], "venue": "Advances in Neural Information Processing Systems, 24:801\u2013809.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["R. Socher", "B. Huval", "C.D. Manning", "A.Y. Ng."], "venue": "Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing, pages 1201\u20131211.", "citeRegEx": "Socher et al\\.,? 2012", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Domain and function: A dualspace model of semantic relations and compositions", "author": ["P.D. Turney."], "venue": "Journal of Artificial Intelligence Research, 44:533\u2013 585.", "citeRegEx": "Turney.,? 2012", "shortCiteRegEx": "Turney.", "year": 2012}, {"title": "Word vectors and quantum logic: Experiments with negation and disjunction", "author": ["D. Widdows", "S. Peters."], "venue": "Mathematics of language, 8(141-154).", "citeRegEx": "Widdows and Peters.,? 2003", "shortCiteRegEx": "Widdows and Peters.", "year": 2003}, {"title": "Estimating linear models for compositional distributional semantics", "author": ["F.M. Zanzotto", "I. Korkontzelos", "F. Fallucchi", "S. Manandhar."], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 1263\u20131271. Associa-", "citeRegEx": "Zanzotto et al\\.,? 2010", "shortCiteRegEx": "Zanzotto et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 7, "context": "Distributional models of semantics characterize the meanings of words as a function of the words they co-occur with (Firth, 1957).", "startOffset": 116, "endOffset": 129}, {"referenceID": 12, "context": "These models, mathematically instantiated as sets of vectors in high dimensional vector spaces, have been applied to tasks such as thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Sch\u00fctze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on.", "startOffset": 152, "endOffset": 186}, {"referenceID": 5, "context": "These models, mathematically instantiated as sets of vectors in high dimensional vector spaces, have been applied to tasks such as thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Sch\u00fctze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on.", "startOffset": 152, "endOffset": 186}, {"referenceID": 22, "context": "These models, mathematically instantiated as sets of vectors in high dimensional vector spaces, have been applied to tasks such as thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Sch\u00fctze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on.", "startOffset": 214, "endOffset": 229}, {"referenceID": 15, "context": "These models, mathematically instantiated as sets of vectors in high dimensional vector spaces, have been applied to tasks such as thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Sch\u00fctze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on.", "startOffset": 255, "endOffset": 282}, {"referenceID": 16, "context": "This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al.", "startOffset": 97, "endOffset": 210}, {"referenceID": 17, "context": "This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al.", "startOffset": 97, "endOffset": 210}, {"referenceID": 9, "context": "This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al.", "startOffset": 97, "endOffset": 210}, {"referenceID": 2, "context": "This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al.", "startOffset": 97, "endOffset": 210}, {"referenceID": 24, "context": "This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), and semantic relation classification (ibid.", "startOffset": 231, "endOffset": 279}, {"referenceID": 13, "context": "This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), and semantic relation classification (ibid.", "startOffset": 231, "endOffset": 279}, {"referenceID": 16, "context": "Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Zanzotto et al., 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al.", "startOffset": 130, "endOffset": 207}, {"referenceID": 17, "context": "Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Zanzotto et al., 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al.", "startOffset": 130, "endOffset": 207}, {"referenceID": 27, "context": "Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Zanzotto et al., 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al.", "startOffset": 130, "endOffset": 207}, {"referenceID": 0, "context": ", 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al.", "startOffset": 43, "endOffset": 145}, {"referenceID": 4, "context": ", 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al.", "startOffset": 43, "endOffset": 145}, {"referenceID": 10, "context": ", 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al.", "startOffset": 43, "endOffset": 145}, {"referenceID": 14, "context": ", 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al.", "startOffset": 43, "endOffset": 145}, {"referenceID": 23, "context": ", 2012), or through the use of recursive auto-encoding (Socher et al., 2011; Hermann and Blunsom, 2013) or neural-networks (Socher et al.", "startOffset": 55, "endOffset": 103}, {"referenceID": 13, "context": ", 2012), or through the use of recursive auto-encoding (Socher et al., 2011; Hermann and Blunsom, 2013) or neural-networks (Socher et al.", "startOffset": 55, "endOffset": 103}, {"referenceID": 24, "context": ", 2011; Hermann and Blunsom, 2013) or neural-networks (Socher et al., 2012).", "startOffset": 54, "endOffset": 75}, {"referenceID": 0, "context": ", 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al., 2011; Hermann and Blunsom, 2013) or neural-networks (Socher et al., 2012). On the non-compositional front, Erk and Pad\u00f3 (2008) keep word vectors separate, using syntactic information from sentences to disambiguate words in context; likewise Turney (2012) treats the compositional aspect of phrases and sentences as a matter of similarity measure composition rather than vector composition.", "startOffset": 44, "endOffset": 336}, {"referenceID": 0, "context": ", 2010), matrix or tensor-based approaches (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette et al., 2013; Kartsaklis et al., 2012), or through the use of recursive auto-encoding (Socher et al., 2011; Hermann and Blunsom, 2013) or neural-networks (Socher et al., 2012). On the non-compositional front, Erk and Pad\u00f3 (2008) keep word vectors separate, using syntactic information from sentences to disambiguate words in context; likewise Turney (2012) treats the compositional aspect of phrases and sentences as a matter of similarity measure composition rather than vector composition.", "startOffset": 44, "endOffset": 464}, {"referenceID": 23, "context": "In Section 4, we present matrix-vector models similar to that of Socher et al. (2012) as a good candidate for expressing this tripartite representation.", "startOffset": 65, "endOffset": 86}, {"referenceID": 3, "context": "This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pad\u00f3 (2008) to deal with issues polysemy and ambiguity.", "startOffset": 103, "endOffset": 114}, {"referenceID": 20, "context": "This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pad\u00f3 (2008) to deal with issues polysemy and ambiguity.", "startOffset": 142, "endOffset": 173}, {"referenceID": 5, "context": "The work which best exemplifies this strand of research is found in the efforts of Garrette et al. (2011) and, more recently, Beltagy et al.", "startOffset": 83, "endOffset": 106}, {"referenceID": 1, "context": "(2011) and, more recently, Beltagy et al. (2013). This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pad\u00f3 (2008) to deal with issues polysemy and ambiguity.", "startOffset": 27, "endOffset": 49}, {"referenceID": 1, "context": "(2011) and, more recently, Beltagy et al. (2013). This line of research converts logical representations obtained from syntactic parses using Bos\u2019 Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pad\u00f3 (2008) to deal with issues polysemy and ambiguity.", "startOffset": 27, "endOffset": 308}, {"referenceID": 18, "context": "(2010) postulate a mathematical framework generalising the syntax-semantic passage of Montague Grammar (Montague, 1974) to other forms of syntactic and semantic representation.", "startOffset": 103, "endOffset": 119}, {"referenceID": 4, "context": "Coecke et al. (2010) postulate a mathematical framework generalising the syntax-semantic passage of Montague Grammar (Montague, 1974) to other forms of syntactic and semantic representation.", "startOffset": 0, "endOffset": 21}, {"referenceID": 11, "context": "Recently, Grefenstette (2013) showed that the examples from this framework could be extended to model a full quantifier-free predicate logic using tensors of rank 3 or lower.", "startOffset": 10, "endOffset": 30}, {"referenceID": 11, "context": "Recently, Grefenstette (2013) showed that the examples from this framework could be extended to model a full quantifier-free predicate logic using tensors of rank 3 or lower. In parallel, Socher et al. (2012) showed that propositional logic can be learned using tensors of rank 2 or lower (i.", "startOffset": 10, "endOffset": 209}, {"referenceID": 4, "context": "The work of Coecke et al. (2010) and Grefenstette (2013) limits itself to defining, rather than learning, distributional representations of logical operators for distributional models that simulate logic, and makes no pretense to the provision of operations which generalise to higher-dimensional distributional semantic representations.", "startOffset": 12, "endOffset": 33}, {"referenceID": 4, "context": "The work of Coecke et al. (2010) and Grefenstette (2013) limits itself to defining, rather than learning, distributional representations of logical operators for distributional models that simulate logic, and makes no pretense to the provision of operations which generalise to higher-dimensional distributional semantic representations.", "startOffset": 12, "endOffset": 57}, {"referenceID": 4, "context": "The work of Coecke et al. (2010) and Grefenstette (2013) limits itself to defining, rather than learning, distributional representations of logical operators for distributional models that simulate logic, and makes no pretense to the provision of operations which generalise to higher-dimensional distributional semantic representations. As for the non-linear approach of Socher et al. (2012), we will discuss, in Section 4 below, the limitations with this model with regard to the task of modelling logic for higher dimensional representations.", "startOffset": 12, "endOffset": 393}, {"referenceID": 26, "context": "The seminal work in this area is found in the work of Widdows and Peters (2003), who define negation and other logical operators algebraically for high dimensional semantic vectors.", "startOffset": 54, "endOffset": 80}, {"referenceID": 24, "context": "This is a unification of various recent approaches to the problem of semantic representation in continuous distributional semantic modelling (Socher et al., 2012; Turney, 2012; Hermann and Blunsom, 2013).", "startOffset": 141, "endOffset": 203}, {"referenceID": 25, "context": "This is a unification of various recent approaches to the problem of semantic representation in continuous distributional semantic modelling (Socher et al., 2012; Turney, 2012; Hermann and Blunsom, 2013).", "startOffset": 141, "endOffset": 203}, {"referenceID": 13, "context": "This is a unification of various recent approaches to the problem of semantic representation in continuous distributional semantic modelling (Socher et al., 2012; Turney, 2012; Hermann and Blunsom, 2013).", "startOffset": 141, "endOffset": 203}, {"referenceID": 0, "context": "(2012) and others (Baroni and Zamparelli, 2010; Coecke et al., 2010) the idea that the information words refer to is of two sorts: first the semantic content of the word, which can be seen as the sense or reference to the concept the word stands for, and is typically modelled as a semantic vector; and second, the function the word has, which models the effect the word has on other words it combines with in phrases and sentences, and is typically modelled as a matrix or higher-order tensor.", "startOffset": 18, "endOffset": 68}, {"referenceID": 4, "context": "(2012) and others (Baroni and Zamparelli, 2010; Coecke et al., 2010) the idea that the information words refer to is of two sorts: first the semantic content of the word, which can be seen as the sense or reference to the concept the word stands for, and is typically modelled as a semantic vector; and second, the function the word has, which models the effect the word has on other words it combines with in phrases and sentences, and is typically modelled as a matrix or higher-order tensor.", "startOffset": 18, "endOffset": 68}, {"referenceID": 21, "context": "We borrow from Socher et al. (2012) and others (Baroni and Zamparelli, 2010; Coecke et al.", "startOffset": 15, "endOffset": 36}, {"referenceID": 0, "context": "(2012) and others (Baroni and Zamparelli, 2010; Coecke et al., 2010) the idea that the information words refer to is of two sorts: first the semantic content of the word, which can be seen as the sense or reference to the concept the word stands for, and is typically modelled as a semantic vector; and second, the function the word has, which models the effect the word has on other words it combines with in phrases and sentences, and is typically modelled as a matrix or higher-order tensor. We borrow from Turney (2012) the idea that the semantic aspect of a word should not be modelled as a single vector where everything is equally important, but ideally as two or more vectors (or, as we do here, two or more regions of a vector) which stand for the aspects of a word relating to its domain, and those relating to its value.", "startOffset": 19, "endOffset": 524}, {"referenceID": 25, "context": "Although we will represent domain and value as two regions of a vector, there is no reason for these not to be treated as separate vectors at the time of comparison, as done by Turney (2012).", "startOffset": 177, "endOffset": 191}, {"referenceID": 0, "context": "Inspired by the distributional interpretation (Baroni and Zamparelli, 2010; Coecke et al., 2010) of syntactically-paramatrized semantic composition functions from Montogovian semantics (Montague, 1974), we will also assume the function part of our representation to be parametrized principally by syntax and domain rather than value.", "startOffset": 46, "endOffset": 96}, {"referenceID": 4, "context": "Inspired by the distributional interpretation (Baroni and Zamparelli, 2010; Coecke et al., 2010) of syntactically-paramatrized semantic composition functions from Montogovian semantics (Montague, 1974), we will also assume the function part of our representation to be parametrized principally by syntax and domain rather than value.", "startOffset": 46, "endOffset": 96}, {"referenceID": 18, "context": ", 2010) of syntactically-paramatrized semantic composition functions from Montogovian semantics (Montague, 1974), we will also assume the function part of our representation to be parametrized principally by syntax and domain rather than value.", "startOffset": 96, "endOffset": 112}, {"referenceID": 23, "context": "A good candidate for modelling this partition would be a dual-space representation similar to that of Socher et al. (2012). In this section, we show that this sort of representation is not well adapted to the modelling of negation.", "startOffset": 102, "endOffset": 123}, {"referenceID": 23, "context": "A good candidate for modelling this partition would be a dual-space representation similar to that of Socher et al. (2012). In this section, we show that this sort of representation is not well adapted to the modelling of negation. Models using dual-space representations have been proposed in several recent publications, notably in Turney (2012) and Socher et al.", "startOffset": 102, "endOffset": 348}, {"referenceID": 23, "context": "A good candidate for modelling this partition would be a dual-space representation similar to that of Socher et al. (2012). In this section, we show that this sort of representation is not well adapted to the modelling of negation. Models using dual-space representations have been proposed in several recent publications, notably in Turney (2012) and Socher et al. (2012). We use the class of recursive matrix-vector models as the basis for our investigation; for a detailed introduction see the MV-RNN model described in Socher et al.", "startOffset": 102, "endOffset": 373}, {"referenceID": 23, "context": "A good candidate for modelling this partition would be a dual-space representation similar to that of Socher et al. (2012). In this section, we show that this sort of representation is not well adapted to the modelling of negation. Models using dual-space representations have been proposed in several recent publications, notably in Turney (2012) and Socher et al. (2012). We use the class of recursive matrix-vector models as the basis for our investigation; for a detailed introduction see the MV-RNN model described in Socher et al. (2012). We begin by describing composition for a general dual-space model, and apply this model to the notion of compositional logic in a tripartite representation discussed earlier.", "startOffset": 102, "endOffset": 544}, {"referenceID": 23, "context": "In the case of Socher et al. (2012) these functions are as follows:", "startOffset": 15, "endOffset": 36}, {"referenceID": 23, "context": "logic experiment in Socher et al. (2012)), the aim of this paper is to place the burden of compositionality on the atomic representations instead.", "startOffset": 20, "endOffset": 41}, {"referenceID": 19, "context": "the *SEM 2012 shared task (Morante and Blanco, 2012).", "startOffset": 26, "endOffset": 52}, {"referenceID": 23, "context": "While the MV-RNN model in Socher et al. (2012) incorporates parse trees to guide the order of its composition steps, it uses a single composition function across all steps.", "startOffset": 26, "endOffset": 47}, {"referenceID": 21, "context": "This issue was recently highlighted by an investigation into recursive autoencoders for sentiment analysis (Scheible and Sch\u00fctze, 2013).", "startOffset": 107, "endOffset": 135}, {"referenceID": 19, "context": "(2012) but also more specific tasks such as the *SEM shared task on negation scope and reversal (Morante and Blanco, 2012).", "startOffset": 96, "endOffset": 122}, {"referenceID": 22, "context": "Possible tasks for this include sentiment analysis and relation extraction tasks such as in Socher et al. (2012) but also more specific tasks such as the *SEM shared task on negation scope and reversal (Morante and Blanco, 2012).", "startOffset": 92, "endOffset": 113}], "year": 2013, "abstractText": "With the increasing empirical success of distributional models of compositional semantics, it is timely to consider the types of textual logic that such models are capable of capturing. In this paper, we address shortcomings in the ability of current models to capture logical operations such as negation. As a solution we propose a tripartite formulation for a continuous vector space representation of semantics and subsequently use this representation to develop a formal compositional notion of negation within such models.", "creator": "TeX"}}}