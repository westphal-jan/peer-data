{"id": "1306.2295", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Jun-2013", "title": "Markov random fields factorization with context-specific independences", "abstract": "Markov random fields provide a compact representation of joint probability distributions by representing its independence properties in an undirected graph. The well-known Hammersley-Clifford theorem uses these conditional independences to factorize a Gibbs distribution into a set of factors. However, an important issue of using a graph to represent independences is that it cannot encode some types of independence relations, such as the context-specific independences (CSIs). They are a particular case of conditional independences that is true only for a certain assignment of its conditioning set; in contrast to conditional independences that must hold for all its assignments. This work presents a method for factorizing a Markov random field according to CSIs present in a distribution, and formally guarantees that this factorization is correct. This is presented in our main contribution, the context-specific Hammersley-Clifford theorem, a generalization to CSIs of the Hammersley-Clifford theorem that applies for conditional independences.", "histories": [["v1", "Mon, 10 Jun 2013 19:36:31 GMT  (14kb)", "http://arxiv.org/abs/1306.2295v1", "7 pages"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["alejandro edera", "facundo bromberg", "federico schl\\\"uter"], "accepted": false, "id": "1306.2295"}, "pdf": {"name": "1306.2295.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 130 6.22 95v1 [cs.AI] 10 Jun 20Markov random fields provide a concise representation of the common probability distributions by presenting their independence properties in an undirected diagram. However, the well-known Hammersley-Clifford theorem uses these conditional independence relations to factorise a Gibbs distribution into a number of factors. However, an important problem when using a diagram to represent independence is that there are some types of independence relations, such as context-specific independence relations (CSIs), that cannot encode. It is a special case of conditional independence that applies only to a certain allocation of its conditioning set; unlike conditional independence, which must apply to all its assignments, this work represents a method for factorizing a Markov random field according to a distribution of existing CSIs in a CSIs and formally guarantees that this factorization is correctly presented in our Hammerley theory, the main text contribution of the CSIs."}, {"heading": "1 Introduction", "text": "Markov random fields (MRFs), also known as undirected graphical models or Markov networks, belong to the family of probabilistic graphical models (Koller and Friedman, 2009), a well-known computational framework for the compact representation of common hammer sample allocation distributions. These models consist of an independence structure and a number of numerical parameters. The independence structure is an undirected graph that compactly breaks down the conditional complexity of their representation, time complexity of the variables in the Do-Main. In view of the structure, the numerical parameters quantify the relationships in the structure. Probability distributions, which in practice have important complexity deficits, with exponential spatial complexity of their representation, time complexity of inference and sample complexity when learned from data. Based on the structure of independents, it is possible to efficiently represent the common probability distribution by factorization (or factors)."}, {"heading": "2 Related work", "text": "There are several papers in the literature (Della Pietra et al., 1997; Lee et al., 2006; Lowd and Davis, 2010; Van Haaren and Davis, 2012) that directly learn loglinear models by presenting different methods for selecting characteristics from data. None of these papers discusses CSIs, nor is there any guarantee as to how the log-linear model relates to the underlying distribution. CSIs were first introduced by (Boutilier et al., 1996) by encoding locally within probability tables (factors) of Bayesian networks as decision trees. Their approach is hybrid and encodes conditional independence in the directed diagram and CSIs as decision trees over the variables of a conditional probability table. Also, their work presents theoretical results for a solid graphical representation. This paper instead proposes a uniform representation of CSIs and conditional independence in a log-linear model. As such, it requires initial theoretical guarantees, such as a distribution based on this model of effective results (which is not required for investigation)."}, {"heading": "3 Preliminaries", "text": "This is an example of how the Xa function represents all values of the Xa domain, and Val (XU) represents all possible values of the XU (Xi, i, U) variable. (Xa, xb, xn) The Xa function returns all values of the Xa domain, and Val (XU) returns all possible values of the XU (Xi, i, U) variable. (Xa, xb, xn) The values of Xa are denoted by Xa, and Val (XU) returns all possible values of the XU variable. (Xa, i, U) Leave x = (Xa, xb) a complete mapping of Xa. The values of Xa are denoted by Xa (Xa), where j = 1, Val (Xa)."}, {"heading": "3.1 Undirected graphs factorization", "text": "AMRF uses an undirected diagram G and a set of numerical parameters, a distribution most represented by the above distribution. < The fully linked sub-diagrams of G (a.k.a., cliques) can be used to factorise the distribution into a set of potential functions. < The following theorems show how to factorise the distribution into a set of potential functions: Theorem 1 (Hammersley-Clifford (Hammersley and Clifford, 1971). Let p (X) be a positive distribution over the domain of variable X and let G be an undirected graph over X. If G is an I map of p (X), then p (X) can be factored as."}, {"heading": "4 Context-specific", "text": "In this section, the main contribution of this thesis is presented: a generalization of the Hammersley-Clifford theorem to factorize a distribution based on a log-linear dependency model Ic CSI-map of p (X). To do this, we start by defining the following dependency on the HammersleyClifford theorem: Corollary 1 (XCSIfford-map) XCSIfford map, then the XCSIfford map can be embedded in a set of potential functions (XCi), and let me define a diagram isomorphic dependency model via X. If I can define the XCSIfford map of p (X), then p (X) can be embedded in a set of potential functions (XCi)."}, {"heading": "5 Conclusions", "text": "We have presented a theoretical method for factorizing a Markov random field according to the CSIs present in a formally guaranteed distribution, which is presented by the context-specific Hammersley-Clifford theorem as a generalization to the CSIs of the Hammersley-Clifford theorem applicable to conditional independence. According to our theoretical conclusion, we believe that it is worthwhile to orient our future work on implementing algorithms to learn from data the structure of MRFs for any possible context and then factoring the distribution based on the learned structures. Intuitively, it seems likely to achieve improvements in time, space, and sample complexity compared to other approaches that use conditional dependencies encoded by the graph."}], "references": [{"title": "Context-specific independence in Bayesian networks", "author": ["C. Boutilier", "N. Friedman", "M. Goldszmidt", "D. Koller"], "venue": "Proceedings of the Twelfth international conference on Uncertainty in artificial intelligence, pages 115\u2013123. Morgan Kaufmann", "citeRegEx": "Boutilier et al\\.,? 1996", "shortCiteRegEx": "Boutilier et al\\.", "year": 1996}, {"title": "A Bayesian Approach to Learning Bayesian Networks with Local Structure", "author": ["D.M. Chickering", "D. Heckerman", "C. Meek"], "venue": "Uncertainty in Artificial Intelligence, pages 80\u201389. Morgan Kaufmann Publishers Inc.", "citeRegEx": "Chickering et al\\.,? 1997", "shortCiteRegEx": "Chickering et al\\.", "year": 1997}, {"title": "Inducing Features of Random Fields", "author": ["S. Della Pietra", "V.J. Della Pietra", "J.D. Lafferty"], "venue": "IEEE Trans. PAMI., 19(4):380\u2013393.", "citeRegEx": "Pietra et al\\.,? 1997", "shortCiteRegEx": "Pietra et al\\.", "year": 1997}, {"title": "Context-specific independence in directed relational probabilistic models and its influence on the efficiency of Gibbs sampling", "author": ["D. Fierens"], "venue": "European Conference on Artificial Intelligence, pages 243\u2013248.", "citeRegEx": "Fierens,? 2010", "shortCiteRegEx": "Fierens", "year": 2010}, {"title": "Knowledge representation and inference in similarity networks and Bayesian multinets", "author": ["D. Geiger", "D. Heckerman"], "venue": "Artificial Intelligence, 82:45\u201374.", "citeRegEx": "Geiger and Heckerman,? 1996", "shortCiteRegEx": "Geiger and Heckerman", "year": 1996}, {"title": "Learning efficient markov networks", "author": ["V. Gogate", "W. Austin", "W.P. Domingos"], "venue": null, "citeRegEx": "Gogate et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Gogate et al\\.", "year": 2010}, {"title": "Markov fields on finite graphs and lattices", "author": ["J.M. Hammersley", "P. Clifford"], "venue": null, "citeRegEx": "Hammersley and Clifford,? \\Q1971\\E", "shortCiteRegEx": "Hammersley and Clifford", "year": 1971}, {"title": "Learning Bayesian networks: The combination of knowledge and statistical data", "author": ["D. Heckerman", "D. Geiger", "D.M. Chickering"], "venue": "Machine Learning.", "citeRegEx": "Heckerman et al\\.,? 1995", "shortCiteRegEx": "Heckerman et al\\.", "year": 1995}, {"title": "Probabilistic Graphical Models: Principles and Techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT Press, Cambridge.", "citeRegEx": "Koller and Friedman,? 2009", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "Learning Bayesian belief networks: an approach based on the MDL principle", "author": ["W. Lam", "F. Bacchus"], "venue": "Computational Intelligence, 10:269\u2013293.", "citeRegEx": "Lam and Bacchus,? 1994", "shortCiteRegEx": "Lam and Bacchus", "year": 1994}, {"title": "Efficient structure learning of Markov networks using L1-regularization", "author": ["S. Lee", "V. Ganapathi", "D. Koller"], "venue": "Neural Information Processing Systems. Citeseer.", "citeRegEx": "Lee et al\\.,? 2006", "shortCiteRegEx": "Lee et al\\.", "year": 2006}, {"title": "Learning markov network structure with decision trees", "author": ["D. Lowd", "J. Davis"], "venue": "Data Mining (ICDM), 2010 IEEE 10th International Conference on, pages 334\u2013343. IEEE.", "citeRegEx": "Lowd and Davis,? 2010", "shortCiteRegEx": "Lowd and Davis", "year": 2010}, {"title": "Efficiently inducing features of conditional random fields", "author": ["A. McCallum"], "venue": "Proceedings of Uncertainty in Artificial Intelligence (UAI).", "citeRegEx": "McCallum,? 2003", "shortCiteRegEx": "McCallum", "year": 2003}, {"title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "author": ["J. Pearl"], "venue": "Morgan Kaufmann Publishers, Inc., 1re edition.", "citeRegEx": "Pearl,? 1988", "shortCiteRegEx": "Pearl", "year": 1988}, {"title": "GRAPHOIDS : A graph based logic for reasonning ab relevance relations", "author": ["J. Pearl", "A. Paz"], "venue": "Technical Report 850038 (R-53-L), Cognitive Systems Laboratory, University of California, Los Angeles.", "citeRegEx": "Pearl and Paz,? 1985", "shortCiteRegEx": "Pearl and Paz", "year": 1985}, {"title": "Exploiting contextual independence in probabilistic inference", "author": ["D. Poole", "N.L. Zhang"], "venue": "J. Artif. Intell. Res. (JAIR), 18:263\u2013313.", "citeRegEx": "Poole and Zhang,? 2003", "shortCiteRegEx": "Poole and Zhang", "year": 2003}, {"title": "High-dimensional Ising model selection using L1-regularized logistic regression", "author": ["P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty"], "venue": "Annals of Statistics, 38:1287\u20131319.", "citeRegEx": "Ravikumar et al\\.,? 2010", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2010}, {"title": "Causation, Prediction, and Search", "author": ["P. Spirtes", "C. Glymour", "R. Scheines"], "venue": "Adaptive Computation and Machine Learning Series. MIT Press.", "citeRegEx": "Spirtes et al\\.,? 2000", "shortCiteRegEx": "Spirtes et al\\.", "year": 2000}, {"title": "Inference for multiplicative models", "author": ["Y. Wexler", "C. Meek"], "venue": "Uncertainty in Artificial Intelligence, pages 595\u2013602.", "citeRegEx": "Wexler and Meek,? 2008", "shortCiteRegEx": "Wexler and Meek", "year": 2008}, {"title": "Markov network structure learning: A randomized feature generation approach", "author": ["J. Van Haaren", "J. Davis"], "venue": "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (2012).", "citeRegEx": "Haaren and Davis,? 2012", "shortCiteRegEx": "Haaren and Davis", "year": 2012}], "referenceMentions": [{"referenceID": 8, "context": "Markov random fields (MRFs), also known as undirected graphical models, or Markov networks, belong to the family of probabilistic graphical models (Koller and Friedman, 2009), a well-known computational framework for compact representation of joint probability distributions.", "startOffset": 147, "endOffset": 174}, {"referenceID": 6, "context": "This factorization can be done by using the well-known Hammersley-Clifford theorem (Hammersley and Clifford, 1971).", "startOffset": 83, "endOffset": 114}, {"referenceID": 0, "context": "An important issue of using a graph to represent independences is that it cannot encode some types of independence relations, such as the context-specific independences (CSIs) (Boutilier et al., 1996).", "startOffset": 176, "endOffset": 200}, {"referenceID": 1, "context": "(Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010).", "startOffset": 0, "endOffset": 132}, {"referenceID": 3, "context": "(Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010).", "startOffset": 0, "endOffset": 132}, {"referenceID": 15, "context": "(Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010).", "startOffset": 0, "endOffset": 132}, {"referenceID": 18, "context": "(Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010).", "startOffset": 0, "endOffset": 132}, {"referenceID": 11, "context": "(Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010).", "startOffset": 0, "endOffset": 132}, {"referenceID": 16, "context": "(Chickering et al., 1997; Fierens, 2010; Poole and Zhang, 2003; Wexler and Meek, 2008; Lowd and Davis, 2010; Ravikumar et al., 2010).", "startOffset": 0, "endOffset": 132}, {"referenceID": 8, "context": "For this, a log-linear model is used as a more fine-grained representation of the MRFs (Koller and Friedman, 2009).", "startOffset": 87, "endOffset": 114}, {"referenceID": 10, "context": "There are several works in the literature (Della Pietra et al., 1997; Lee et al., 2006; Lowd and Davis, 2010; Van Haaren and Davis, 2012) that learn log-linear models directly by presenting different procedures for selecting features from data.", "startOffset": 42, "endOffset": 137}, {"referenceID": 11, "context": "There are several works in the literature (Della Pietra et al., 1997; Lee et al., 2006; Lowd and Davis, 2010; Van Haaren and Davis, 2012) that learn log-linear models directly by presenting different procedures for selecting features from data.", "startOffset": 42, "endOffset": 137}, {"referenceID": 0, "context": "CSIs were first introduced by (Boutilier et al., 1996) by coding them locally within conditional probability tables (factors) of Bayesian networks as decision trees.", "startOffset": 30, "endOffset": 54}, {"referenceID": 5, "context": "The work of (Gogate et al., 2010) is the closests to our work, presenting an algorithm for factorizing a loglinear model according to CSIs.", "startOffset": 12, "endOffset": 33}, {"referenceID": 13, "context": "Conditional independences are regularities of distributions that has been extensively studied in the field of statistics, demonstrating how they can be effectively and soundly used for reducing the dimensionality of the distribution (Pearl, 1988; Spirtes et al., 2000; Koller and Friedman, 2009).", "startOffset": 233, "endOffset": 295}, {"referenceID": 17, "context": "Conditional independences are regularities of distributions that has been extensively studied in the field of statistics, demonstrating how they can be effectively and soundly used for reducing the dimensionality of the distribution (Pearl, 1988; Spirtes et al., 2000; Koller and Friedman, 2009).", "startOffset": 233, "endOffset": 295}, {"referenceID": 8, "context": "Conditional independences are regularities of distributions that has been extensively studied in the field of statistics, demonstrating how they can be effectively and soundly used for reducing the dimensionality of the distribution (Pearl, 1988; Spirtes et al., 2000; Koller and Friedman, 2009).", "startOffset": 233, "endOffset": 295}, {"referenceID": 14, "context": "A necessary and sufficient condition for dependency models to be graph-isomorph is that all its independence assertions satisfy the following independence axioms, commonly called the Pearl axioms (Pearl and Paz, 1985):", "startOffset": 196, "endOffset": 217}, {"referenceID": 8, "context": "Definition 3 (Pairwise Markov property (Koller and Friedman, 2009)).", "startOffset": 39, "endOffset": 66}, {"referenceID": 0, "context": "In this work we focus in a finer-grained type of independences: the context-specific independences (CSI) (Boutilier et al., 1996; Geiger and Heckerman, 1996; Chickering et al., 1997; Koller and Friedman, 2009).", "startOffset": 105, "endOffset": 209}, {"referenceID": 4, "context": "In this work we focus in a finer-grained type of independences: the context-specific independences (CSI) (Boutilier et al., 1996; Geiger and Heckerman, 1996; Chickering et al., 1997; Koller and Friedman, 2009).", "startOffset": 105, "endOffset": 209}, {"referenceID": 1, "context": "In this work we focus in a finer-grained type of independences: the context-specific independences (CSI) (Boutilier et al., 1996; Geiger and Heckerman, 1996; Chickering et al., 1997; Koller and Friedman, 2009).", "startOffset": 105, "endOffset": 209}, {"referenceID": 8, "context": "In this work we focus in a finer-grained type of independences: the context-specific independences (CSI) (Boutilier et al., 1996; Geiger and Heckerman, 1996; Chickering et al., 1997; Koller and Friedman, 2009).", "startOffset": 105, "endOffset": 209}, {"referenceID": 0, "context": "Definition 4 (Context-specific independence (Boutilier et al., 1996)).", "startOffset": 44, "endOffset": 68}, {"referenceID": 8, "context": "Since each CSIs is defined for a specific context, they cannot be represented all together in a single undirected graph (Koller and Friedman, 2009).", "startOffset": 120, "endOffset": 147}, {"referenceID": 0, "context": "If every independence assertion contained in Ic holds for p(X), Ic is said to be an CSI-map of p(X) (Boutilier et al., 1996).", "startOffset": 100, "endOffset": 124}, {"referenceID": 6, "context": "Theorem 1 (Hammersley-Clifford (Hammersley and Clifford, 1971)).", "startOffset": 31, "endOffset": 62}], "year": 2013, "abstractText": "Markov random fields provide a compact representation of joint probability distributions by representing its independence properties in an undirected graph. The well-known Hammersley-Clifford theorem uses these conditional independences to factorize a Gibbs distribution into a set of factors. However, an important issue of using a graph to represent independences is that it cannot encode some types of independence relations, such as the context-specific independences (CSIs). They are a particular case of conditional independences that is true only for a certain assignment of its conditioning set; in contrast to conditional independences that must hold for all its assignments. This work presents a method for factorizing a Markov random field according to CSIs present in a distribution, and formally guarantees that this factorization is correct. This is presented in our main contribution, the context-specific Hammersley-Clifford theorem, a generalization to CSIs of the Hammersley-Clifford theorem that applies for conditional independences.", "creator": "LaTeX with hyperref package"}}}