{"id": "1306.2663", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Jun-2013", "title": "Large Margin Low Rank Tensor Analysis", "abstract": "Other than vector representations, the direct objects of human cognition are generally high-order tensors, such as 2D images and 3D textures. From this fact, two interesting questions naturally arise: How does the human brain represent these tensor perceptions in a \"manifold\" way, and how can they be recognized on the \"manifold\"? In this paper, we present a supervised model to learn the intrinsic structure of the tensors embedded in a high dimensional Euclidean space. With the fixed point continuation procedures, our model automatically and jointly discovers the optimal dimensionality and the representations of the low dimensional embeddings. This makes it an effective simulation of the cognitive process of human brain. Furthermore, the generalization of our model based on similarity between the learned low dimensional embeddings can be viewed as counterpart of recognition of human brain. Experiments on applications for object recognition and face recognition demonstrate the superiority of our proposed model over state-of-the-art approaches.", "histories": [["v1", "Tue, 11 Jun 2013 21:39:56 GMT  (1379kb,D)", "http://arxiv.org/abs/1306.2663v1", "30 pages"]], "COMMENTS": "30 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NA", "authors": ["guoqiang zhong", "mohamed cheriet"], "accepted": false, "id": "1306.2663"}, "pdf": {"name": "1306.2663.pdf", "metadata": {"source": "CRF", "title": "Large Margin Low Rank Tensor Analysis", "authors": ["Guoqiang Zhong", "Mohamed Cheriet"], "emails": ["guoqiang.zhong@synchromedia.ca,", "mohamed.cheriet@etsmtl.ca."], "sections": [{"heading": null, "text": "Unlike vector representations, the direct objects of human cognition are generally high-order tensors, such as 2D images and 3D textures. Of course, two interesting questions arise from this: How does the human brain represent these tensor perceptions in a \"manifold\" way and how can they be recognized in a \"manifold\" way? In this article, we present a supervised model to learn the intrinsic structure of tensors embedded in a high-dimensional Euclidean space. Using fixed-point continuation techniques, our model automatically and collectively discovers the optimal dimensionality and the rep-ar Xiv: 1 resentment of low-dimensional embedding, making it an effective simulation of the cognitive process of the human brain. Furthermore, the generalization of our model on the similarity between the learned low-dimensional embedding as a counterpart to the recognition of the human brain. Experiments on the application of object recognition and demonstrating the presupposition of our model."}, {"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before an agreement is reached."}, {"heading": "2 Previous work", "text": "In order to grasp the effectiveness of low dimensions in the data world, we have to deal with the question of whether it is in fact a real process or a real one."}, {"heading": "3 Large margin low rank tensor analysis (LMLRTA)", "text": "In this section, we first present the notation used and some basic terminologies for tensor operations (Kolda and Bader, 2009; Dai and Yeung, 2006), and then we detail our model, LMLRTA, including its formulation and optimization. Theoretical analyses of LMLRTA, such as its convergence, are also presented."}, {"heading": "3.1 Notation and terminologies", "text": "We define the vector by using lower-case bold letters, such as v, matrix by using upper-case bold letters, such as M, and tensor by using upper-case calligraphic letters, such as A \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 IL = \u00b7 \u00b7 IL, the order of A is L and the smallest dimension (or mode) of A is Il. Furthermore, we define the index of a single entry within a tensor by subscription, such as Ai1,..., iL. Definition 1 The scalar product < A > of two tensors A, B > of two tensors A, I2 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 IL is defined as < A > = a subscription, such as Ai1,..., iLB."}, {"heading": "3.2 Formulation of LMLRTA", "text": "As explained by the researchers, the number of learners in the field of cognitive psychology is based on the similarity of examples (Rosh, 1973), here we formulate our model foundations for the local similarity between tensor data. Moreover, thanks to the existence of many \"teachers,\" we can generally obtain the categorical information of the examples before or during learning. Let's take, for example, the moment when someone introduces an individual model to his friend. His friend will probably remember the name of the individual, and then his face and his voice. In this case, the name of the individual corresponds to a categorical label, while their face and his voice are characteristics to formulate our learning model in a superior form. Ntensor data, {A1,.,., AN}.IL, with the associated class labels {1, yN}."}, {"heading": "3.3 Optimization", "text": "Similar to previous approaches to reducing tensor dimensionality (Dai and Yeung, 2006; Wang et al., 2007), here we solve a problem (4) with an iterative optimization algorithm. In each iteration we can refine one projection matrix by fixing the others. Here, problem (4) is a semi-definitive programming problem that can be solved using off-the-shelf algorithms, such as SeDuMi1 and CVX (Grant and Boyd, 2008).1http: / sedumi.ie.lehigh.lehigh.lehigh.edu / However, the computational costs of semi-definite programming approaches is generally very high. Here, we solve the problem by a modified fixed point continuation (MFPC) method (Ma et al., 2011).MFPC is an iterative optimization method."}, {"heading": "3.4 Generalization to new tensor data", "text": "For the detection of invisible tensors, we use the Frobenius norm-based k-next neighbor classifier as a distinguishing feature by measuring the local similarity between training data and test data in the subspace of the low-dimensional tensor (Rosch, 1973)."}, {"heading": "4 Experiments", "text": "In this section, we report on the experimental results of two real-world applications: object recognition and face recognition. In particular, for the task of face recognition on the ORL dataset, we used the 3D Gabor transformation of the facial images as input signals. This is mainly due to the fact that the nuclei of the Gabor filters resemble the receptive field profiles of the simple cells of the mammalian cortex (Daugman, 1988), which improves our learning model to better mimic the way of human perception. Below, we report in detail on the parameter settings and experimental results."}, {"heading": "4.1 Parameter settings", "text": "To demonstrate the effectiveness of our method for intrinsic representation learning and recognition, we conducted experiments on the COIL-20 data set2 and the ORL facial data set3. The COIL-20 dataset includes 20 classes of objects and 72 samples within each class. The size of the images is 32 x 32. The ORL dataset contains 400 images from 40 subjects, with each image reduced to a size of 32 x 32. For each facial image, we used 28 Gabor filters to extract structural features. In the end, each face was represented as a 32 x 28 tensor image2http: / / www.cs.columbia.edu / CAVE / software / softlib / coil-20.php. 3http: / / www.cl.cam.ac.uk / research / attarchive / facedatabase.html.what was represented as a 32 x 28 tensor. On the COIL-20 dataset, we used 5-fold cross validation to evaluate the performance of the methods compared."}, {"heading": "4.2 Visualization", "text": "Figure 1 (a) and Figure 1 (b) illustrate the 2D embedding of the object images from the COIL-20 dataset and the 3D Gabor transformation of the face images from the ORL dataset, respectively. The t-distribution-based embedding (t-SNE) algorithm (van der Maaten and Hinton, 2008) was used to learn these 2D embedding, with the distances between the data based on Tensor Frobenius standard. Figure 1 (a) and Figure 1 (b) show that in the original space of these two datasets most classes are embedded on a sub-multiplicity in the surrounding space."}, {"heading": "4.3 Object recognition results on the COIL-20 data set (2D tensors)", "text": "In this experiment, we compare LMLRTA with some related approaches to the application of object recognition. The comparative approaches include LDA, MFA, SDAE, CMDA, CTMFA, and classification in the original space. We conducted an experiment with the COIL-20 dataset. To perform this experiment, we empirically tested the dimensionality of the LDA subspace and the MFA subspace and fixed it to 19 and 33, respectively. For the SDAE algorithm, we used a 6-layer neural network model. The size of the layers was 1024, 512, 256, 64, 32, and 20. For LMLRTA, CMDA, and CTMFA, we merely followed the settings introduced in Section 4.1. Figure 4 shows the classification accuracy and standard deviation achieved by the compared methods. It is easy to see that LMLRTA performed best among all the compared methods, because the structural accuracy of the FDA over all the local MDA mapping represents a large error."}, {"heading": "4.4 Face recognition results on the ORL data set (3D tensors)", "text": "Figure 6 shows the classification accuracy and standard deviation achieved on the ORL dataset. Due to the high computational complexity of LDA, MFA, and SDAE (the vector representations of the tensors are of dimensionality 32 \u00d7 32 \u00d7 28 = 28672), we are only comparing LMLRTA with CMDA, CTMFA, and classification in the original dataset space. Figure 6 shows that LMLRTA consistently exceeds the comparable convergent approaches to reduce the tensor dimensionality. More importantly, since LMLRTA gradually reduces the series of projection matrices during optimization, it can automatically learn the dimensionality of the intrinsic low-dimensional sensor space from the data. However, with traditional algorithms to reduce the sensor dimensionality, the parameter must be specified manually before it can be applied, which can lead to unsatisfactory results on the applications."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a method for reducing the tensor dimensionality, called Large Margin Low Rank Tensor Analysis (LMLRTA). LMLRTA can be used to automatically and collectively learn the dimensionality and representations of low-dimensional embedded tensors, a property that makes it an effective simulation of the way the human brain represents perceived signals. To detect new incoming data, we use classifiers based on similarities in the learned tensor space, which corresponds to the detection method of the human brain (Rosch, 1973). Object recognition and facial recognition experiments show the superiority of LMLRTA over classical vector-based dimension reduction approaches, deep neural network models, and existing approaches to reduce tensor dimensionality. In future work, we will attempt to actively trigger LMLRTA on the transfer modules of the CTA of the 1994 and Yan Learning Tang (2010), and Yan (2010)."}, {"heading": "Acknowledgments", "text": "We thank the Social Sciences and Humanities Research Council of Canada (SSHRC) and the Natural Sciences and Engineering Research Council of Canada (NSERC) for their financial support."}], "references": [{"title": "Generalized Discriminant Analysis Using a Kernel Approach", "author": ["G. Baudat", "F. Anouar"], "venue": "Neural Comput., 12(10):2385\u20132404.", "citeRegEx": "Baudat and Anouar,? 2000", "shortCiteRegEx": "Baudat and Anouar", "year": 2000}, {"title": "Laplacian Eigenmaps for Dimensionality Reduction and Data Representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Computation, 15(6):1373\u20131396.", "citeRegEx": "Belkin and Niyogi,? 2003", "shortCiteRegEx": "Belkin and Niyogi", "year": 2003}, {"title": "Greedy Layer-Wise Training of Deep Networks", "author": ["Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle"], "venue": "NIPS, pages 153\u2013160.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering", "author": ["Y. Bengio", "Paiement", "J.-F.", "P. Vincent", "O. Delalleau", "N.L. Roux", "M. Ouimet"], "venue": "NIPS.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Graph Theory with Applications", "author": ["J.A. Bondy", "U.S.R. Murty"], "venue": "Elsevier, North-Holland.", "citeRegEx": "Bondy and Murty,? 1976", "shortCiteRegEx": "Bondy and Murty", "year": 1976}, {"title": "Exact Matrix Completion via Convex Optimization", "author": ["E. Cand\u00e8s", "B. Recht"], "venue": "Commun. ACM, 55(6):111\u2013119. 26", "citeRegEx": "Cand\u00e8s and Recht,? 2012", "shortCiteRegEx": "Cand\u00e8s and Recht", "year": 2012}, {"title": "The Power of Convex Relaxation: Near-optimal Matrix Completion", "author": ["E. Cand\u00e8s", "T. Tao"], "venue": "IEEE Transactions on Information Theory, 56(5):2053\u20132080.", "citeRegEx": "Cand\u00e8s and Tao,? 2010", "shortCiteRegEx": "Cand\u00e8s and Tao", "year": 2010}, {"title": "Spectral Graph Theory", "author": ["F.R.K. Chung"], "venue": "American Mathematical Society.", "citeRegEx": "Chung,? 1997", "shortCiteRegEx": "Chung", "year": 1997}, {"title": "Improving Generalization with Active Learning", "author": ["D. Cohn", "R. Ladner", "A. Waibel"], "venue": "Machine Learning, pages 201\u2013221.", "citeRegEx": "Cohn et al\\.,? 1994", "shortCiteRegEx": "Cohn et al\\.", "year": 1994}, {"title": "Tensor Embedding Methods", "author": ["G. Dai", "Yeung", "D.-Y."], "venue": "AAAI, pages 330\u2013 335.", "citeRegEx": "Dai et al\\.,? 2006", "shortCiteRegEx": "Dai et al\\.", "year": 2006}, {"title": "Complete Discrete 2D Gabor Transforms by Neural Networks for Image Analysis and Compression", "author": ["J.G. Daugman"], "venue": "Acoustics, Speech and Signal Processing, IEEE Transactions on, 36(7):1169\u20131179.", "citeRegEx": "Daugman,? 1988", "shortCiteRegEx": "Daugman", "year": 1988}, {"title": "Tensor Rank and the Ill-Posedness of the Best LowRank Approximation Problem", "author": ["V. de Silva", "Lim", "L.-H"], "venue": "SIAM J. Matrix Analysis Applications,", "citeRegEx": "Silva et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Silva et al\\.", "year": 2008}, {"title": "The Use of Multiple Measurements in Taxonomic Problems", "author": ["R.A. Fisher"], "venue": "Annals of Eugenics, 7(7):179\u2013188.", "citeRegEx": "Fisher,? 1936", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "Image Classification Using Correlation Tensor Analysis", "author": ["Y. Fu", "T.S. Huang"], "venue": "Image Processing, IEEE Transactions on, 17(2):226\u2013234.", "citeRegEx": "Fu and Huang,? 2008", "shortCiteRegEx": "Fu and Huang", "year": 2008}, {"title": "Graph Implementations for Nonsmooth Convex Programs", "author": ["M. Grant", "S. Boyd"], "venue": "Blondel, V., Boyd, S., and Kimura, H., editors, Recent Advances in Learning and Control, Lecture Notes in Control and Information Sciences, pages 95\u2013110. Springer-Verlag Limited. 27", "citeRegEx": "Grant and Boyd,? 2008", "shortCiteRegEx": "Grant and Boyd", "year": 2008}, {"title": "Locality Preserving Projections", "author": ["X. He", "P. Niyogi"], "venue": "NIPS.", "citeRegEx": "He and Niyogi,? 2003", "shortCiteRegEx": "He and Niyogi", "year": 2003}, {"title": "A Fast Learning Algorithm for Deep Belief Nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation, 18(7):1527\u20131554.", "citeRegEx": "Hinton et al\\.,? 2006", "shortCiteRegEx": "Hinton et al\\.", "year": 2006}, {"title": "Reducing the Dimensionality of Data with Neural Networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, 313(5786):504\u2013507.", "citeRegEx": "Hinton and Salakhutdinov,? 2006", "shortCiteRegEx": "Hinton and Salakhutdinov", "year": 2006}, {"title": "Tensor Decompositions and Applications", "author": ["T.G. Kolda", "B.W. Bader"], "venue": "SIAM Review, 51(3):455\u2013500.", "citeRegEx": "Kolda and Bader,? 2009", "shortCiteRegEx": "Kolda and Bader", "year": 2009}, {"title": "Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models", "author": ["N.D. Lawrence"], "venue": "Journal of Machine Learning Research, 6:1783\u20131816.", "citeRegEx": "Lawrence,? 2005", "shortCiteRegEx": "Lawrence", "year": 2005}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Intelligent Signal Processing, pages 306\u2013351. IEEE Press.", "citeRegEx": "LeCun et al\\.,? 2001", "shortCiteRegEx": "LeCun et al\\.", "year": 2001}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, 401(6755):788\u2013791.", "citeRegEx": "Lee and Seung,? 1999", "shortCiteRegEx": "Lee and Seung", "year": 1999}, {"title": "Sparse Non-negative Tensor Factorization Using Columnwise Coordinate Descent", "author": ["J. Liu", "J. Liu", "P. Wonka", "J. Ye"], "venue": "Pattern Recognition, 45(1):649\u2013656.", "citeRegEx": "Liu et al\\.,? 2012", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Tensor Distance Based Multilinear LocalityPreserved Maximum Information Embedding", "author": ["Y. Liu", "Y. Liu", "K.C.C. Chan"], "venue": "IEEE Transactions on Neural Networks, 21(11):1848\u20131854. 28", "citeRegEx": "Liu et al\\.,? 2010", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Nonlinear Component Analysis", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "M\u00fcller", "K.-R"], "venue": "Linear Embedding. Science,", "citeRegEx": "Sch\u00f6lkopf et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 1998}, {"title": "Dimensionality Reduction of Multimodal Labeled Data by Local", "author": ["M. Sugiyama"], "venue": null, "citeRegEx": "Sugiyama,? \\Q2007\\E", "shortCiteRegEx": "Sugiyama", "year": 2007}, {"title": "General Tensor Discriminant", "author": ["X. Li", "X. Wu", "S.J. Maybank"], "venue": "Fisher Discriminant Analysis. Journal of Machine Learning Research,", "citeRegEx": "D. et al\\.,? \\Q2007\\E", "shortCiteRegEx": "D. et al\\.", "year": 2007}, {"title": "How to Grow a Mind: Statistics, Structure, and Abstraction", "author": ["J.B. Tenenbaum", "C. Kemp", "T.L. Griffiths", "N.D. Goodman"], "venue": "Science, 331(6022):1279\u20131285.", "citeRegEx": "Tenenbaum et al\\.,? 2011", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2011}, {"title": "Visualizing Data using t-SNE", "author": ["L. van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A."], "venue": "Journal of Machine Learning Research, 11:3371\u20133408.", "citeRegEx": "Vincent et al\\.,? 2010", "shortCiteRegEx": "Vincent et al\\.", "year": 2010}, {"title": "A Convengent Solution to Tensor Subspace Learning", "author": ["H. Wang", "S. Yan", "T.S. Huang", "X. Tang"], "venue": "IJCAI, pages 629\u2013634.", "citeRegEx": "Wang et al\\.,? 2007", "shortCiteRegEx": "Wang et al\\.", "year": 2007}, {"title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "NIPS.", "citeRegEx": "Weinberger et al\\.,? 2005", "shortCiteRegEx": "Weinberger et al\\.", "year": 2005}, {"title": "Graph Embedding and Extensions: A General Framework for Dimensionality Reduction", "author": ["S. Yan", "D. Xu", "B. Zhang", "Zhang", "H.-J.", "Q. Yang", "S. Lin"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 29(1):40\u201351.", "citeRegEx": "Yan et al\\.,? 2007", "shortCiteRegEx": "Yan et al\\.", "year": 2007}, {"title": "Two-Dimensional PCA: A New Approach to Appearance-Based Face Representation and Recognition", "author": ["J. Yang", "D. Zhang", "A.F. Frangi", "Yang", "J.-Y."], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 26(1):131\u2013137.", "citeRegEx": "Yang et al\\.,? 2004", "shortCiteRegEx": "Yang et al\\.", "year": 2004}, {"title": "Two-Dimensional Linear Discriminant Analysis", "author": ["J. Ye", "R. Janardan", "Q. Li"], "venue": "NIPS.", "citeRegEx": "Ye et al\\.,? 2004", "shortCiteRegEx": "Ye et al\\.", "year": 2004}, {"title": "Gaussian Process Latent Random Field", "author": ["G. Zhong", "Li", "W.-J.", "Yeung", "D.-Y.", "X. Hou", "Liu", "C.-L."], "venue": "AAAI. 30", "citeRegEx": "Zhong et al\\.,? 2010", "shortCiteRegEx": "Zhong et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 33, "context": "Specifically, some representative tensor dimensionality reduction approaches include (Yang et al., 2004; Ye et al., 2004) and (Wang et al.", "startOffset": 85, "endOffset": 121}, {"referenceID": 34, "context": "Specifically, some representative tensor dimensionality reduction approaches include (Yang et al., 2004; Ye et al., 2004) and (Wang et al.", "startOffset": 85, "endOffset": 121}, {"referenceID": 30, "context": ", 2004) and (Wang et al., 2007).", "startOffset": 12, "endOffset": 31}, {"referenceID": 30, "context": "In particular, the approach presented in (Wang et al., 2007) is theoretically guaranteed to converge to a local optimal solution of the learning problem.", "startOffset": 41, "endOffset": 60}, {"referenceID": 4, "context": "LMLRTA is aimed at learning the low dimensional representations of tensors using techniques of multi-linear algebra (Northcott, 1984) and graph theories (Bondy and Murty, 1976).", "startOffset": 153, "endOffset": 176}, {"referenceID": 33, "context": "Furthermore, unlike previous tensor dimensionality reduction approaches (Yang et al., 2004; Ye et al., 2004; Wang et al., 2007), which can only learn", "startOffset": 72, "endOffset": 127}, {"referenceID": 34, "context": "Furthermore, unlike previous tensor dimensionality reduction approaches (Yang et al., 2004; Ye et al., 2004; Wang et al., 2007), which can only learn", "startOffset": 72, "endOffset": 127}, {"referenceID": 30, "context": "Furthermore, unlike previous tensor dimensionality reduction approaches (Yang et al., 2004; Ye et al., 2004; Wang et al., 2007), which can only learn", "startOffset": 72, "endOffset": 127}, {"referenceID": 24, "context": "kernel principal component analysis (KPCA) (Sch\u00f6lkopf et al., 1998) and generalized discriminant analysis (GDA) (Baudat and Anouar, 2000).", "startOffset": 43, "endOffset": 67}, {"referenceID": 0, "context": ", 1998) and generalized discriminant analysis (GDA) (Baudat and Anouar, 2000).", "startOffset": 52, "endOffset": 77}, {"referenceID": 3, "context": "However, these methods only work on a given set of data points, and cannot be easily extended to out-of-sample data (Bengio et al., 2003).", "startOffset": 116, "endOffset": 137}, {"referenceID": 15, "context": "To alleviate this problem, locality preserving projections (LPP) (He and Niyogi, 2003) and local fisher discriminant analysis (LFDA) (Sugiyama, 2007) were proposed to approximate the manifold structure in a linear subspace by preserving local similarity between data points.", "startOffset": 65, "endOffset": 86}, {"referenceID": 25, "context": "To alleviate this problem, locality preserving projections (LPP) (He and Niyogi, 2003) and local fisher discriminant analysis (LFDA) (Sugiyama, 2007) were proposed to approximate the manifold structure in a linear subspace by preserving local similarity between data points.", "startOffset": 133, "endOffset": 149}, {"referenceID": 32, "context": "proposed a general framework known as graph embedding for dimensionality reduction (Yan et al., 2007).", "startOffset": 83, "endOffset": 101}, {"referenceID": 0, "context": ", 1998) and generalized discriminant analysis (GDA) (Baudat and Anouar, 2000). Since about a decade ago, many manifold learning approaches have been proposed. These manifold learning approaches, including isometric feature mapping (Isomap) (Tenenbaum et al., 2000) and locally linear embedding (LLE) (Roweis and Saul, 2000), can faithfully preserve global or local geometrical properties of the nonlinear structure of data. However, these methods only work on a given set of data points, and cannot be easily extended to out-of-sample data (Bengio et al., 2003). To alleviate this problem, locality preserving projections (LPP) (He and Niyogi, 2003) and local fisher discriminant analysis (LFDA) (Sugiyama, 2007) were proposed to approximate the manifold structure in a linear subspace by preserving local similarity between data points. In particular, Yan et al. proposed a general framework known as graph embedding for dimensionality reduction (Yan et al., 2007). Most of the spectral learning-based approaches, either linear or nonlinear, either supervised or unsupervised, are contained in this framework. Furthermore, based on this framework, the authors proposed the marginal Fisher analysis (MFA) algorithm for supervised linear dimensionality reduction. In the research of probabilistic learning models, Lawrence (2005) proposed the Gaussian process latent variable models (GPLVM), which extends PCA to a probabilistic nonlinear formulation.", "startOffset": 53, "endOffset": 1329}, {"referenceID": 0, "context": ", 1998) and generalized discriminant analysis (GDA) (Baudat and Anouar, 2000). Since about a decade ago, many manifold learning approaches have been proposed. These manifold learning approaches, including isometric feature mapping (Isomap) (Tenenbaum et al., 2000) and locally linear embedding (LLE) (Roweis and Saul, 2000), can faithfully preserve global or local geometrical properties of the nonlinear structure of data. However, these methods only work on a given set of data points, and cannot be easily extended to out-of-sample data (Bengio et al., 2003). To alleviate this problem, locality preserving projections (LPP) (He and Niyogi, 2003) and local fisher discriminant analysis (LFDA) (Sugiyama, 2007) were proposed to approximate the manifold structure in a linear subspace by preserving local similarity between data points. In particular, Yan et al. proposed a general framework known as graph embedding for dimensionality reduction (Yan et al., 2007). Most of the spectral learning-based approaches, either linear or nonlinear, either supervised or unsupervised, are contained in this framework. Furthermore, based on this framework, the authors proposed the marginal Fisher analysis (MFA) algorithm for supervised linear dimensionality reduction. In the research of probabilistic learning models, Lawrence (2005) proposed the Gaussian process latent variable models (GPLVM), which extends PCA to a probabilistic nonlinear formulation. Combining a Gaussian Markov random field prior with GPLVM, Zhong et al. (2010) proposed the Gaussian process latent random field model, which can be", "startOffset": 53, "endOffset": 1530}, {"referenceID": 16, "context": "To exploit the effect of deep architecture for dimensionality reduction, some other deep neural network models were also introduced, such as deep belief nets (DBN) (Hinton et al., 2006), stacked autoencoder (SAE) (Bengio et al.", "startOffset": 164, "endOffset": 185}, {"referenceID": 2, "context": ", 2006), stacked autoencoder (SAE) (Bengio et al., 2006) and stacked denoise autoencoder (SDAE) (Vincent et al.", "startOffset": 35, "endOffset": 56}, {"referenceID": 29, "context": ", 2006) and stacked denoise autoencoder (SDAE) (Vincent et al., 2010).", "startOffset": 47, "endOffset": 69}, {"referenceID": 33, "context": "To alleviate these problems, 2DPCA (Yang et al., 2004) and 2DLDA (Ye et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 34, "context": ", 2004) and 2DLDA (Ye et al., 2004) were proposed to extend the original PCA and LDA algorithms to work directly on 2D matrices rather than 1D vectors.", "startOffset": 18, "endOffset": 35}, {"referenceID": 32, "context": "In recent years, many other approaches (Yan et al., 2007; Tao et al., 2007; Fu and Huang, 2008; Liu et al., 2012, 2010) were also proposed to deal with high-order tensor problems.", "startOffset": 39, "endOffset": 119}, {"referenceID": 13, "context": "In recent years, many other approaches (Yan et al., 2007; Tao et al., 2007; Fu and Huang, 2008; Liu et al., 2012, 2010) were also proposed to deal with high-order tensor problems.", "startOffset": 39, "endOffset": 119}, {"referenceID": 13, "context": "In the area of neural network research, Hinton and Salakhutdinov (2006) proposed a deep neural network model called autoencoder for dimensionality reduction.", "startOffset": 40, "endOffset": 72}, {"referenceID": 2, "context": ", 2006), stacked autoencoder (SAE) (Bengio et al., 2006) and stacked denoise autoencoder (SDAE) (Vincent et al., 2010). These studies show that deep neural networks can generally learn high level representations of data, which can benefit subsequent recognition tasks. All of the above approaches assume that the input data are in the form of vectors. In many real world applications, however, the objects are essentially represented as highorder tensors, such as 2D images or 3D textures. One have to unfold these tensors into one-dimensional vectors first before the dimensionality reduction approaches can be applied. In this case, some useful information in the original data may not be sufficiently preserved. Moreover, high-dimensional vectorized representations suffer from the curse of dimensionality, as well as high computational cost. To alleviate these problems, 2DPCA (Yang et al., 2004) and 2DLDA (Ye et al., 2004) were proposed to extend the original PCA and LDA algorithms to work directly on 2D matrices rather than 1D vectors. In recent years, many other approaches (Yan et al., 2007; Tao et al., 2007; Fu and Huang, 2008; Liu et al., 2012, 2010) were also proposed to deal with high-order tensor problems. In particular, Wang et al. (2007) proposed a tensor dimensionality reduction method based on the graph embedding framework, which is proved that it is the first method to give a convergent solution.", "startOffset": 36, "endOffset": 1259}, {"referenceID": 18, "context": "In this section, we first introduce the used notation and some basic terminologies on tensor operations (Kolda and Bader, 2009; Dai and Yeung, 2006).", "startOffset": 104, "endOffset": 148}, {"referenceID": 7, "context": "It can be considered as a graph Laplacian-parameterized loss function with respect to the low dimensional embeddings (Chung, 1997; Belkin and Niyogi, 2003; Tenenbaum et al., 2011), where each node corresponds to one tensor datum in the given data set.", "startOffset": 117, "endOffset": 179}, {"referenceID": 1, "context": "It can be considered as a graph Laplacian-parameterized loss function with respect to the low dimensional embeddings (Chung, 1997; Belkin and Niyogi, 2003; Tenenbaum et al., 2011), where each node corresponds to one tensor datum in the given data set.", "startOffset": 117, "endOffset": 179}, {"referenceID": 27, "context": "It can be considered as a graph Laplacian-parameterized loss function with respect to the low dimensional embeddings (Chung, 1997; Belkin and Niyogi, 2003; Tenenbaum et al., 2011), where each node corresponds to one tensor datum in the given data set.", "startOffset": 117, "endOffset": 179}, {"referenceID": 6, "context": "Following recent work in matrix completion (Cand\u00e8s and Tao, 2010; Cand\u00e8s and Recht, 2012), we replace it with its convex envelope \u2014 the nuclear norm of Ul, which is defined as the sum of its singular values, i.", "startOffset": 43, "endOffset": 89}, {"referenceID": 5, "context": "Following recent work in matrix completion (Cand\u00e8s and Tao, 2010; Cand\u00e8s and Recht, 2012), we replace it with its convex envelope \u2014 the nuclear norm of Ul, which is defined as the sum of its singular values, i.", "startOffset": 43, "endOffset": 89}, {"referenceID": 15, "context": "Remark 1 (Relation to previous works) 1) LMLRTA can be considered as a supervised multi-linear extension of locality preserving projections (LPP) (He and Niyogi, 2003), in that the second term of the objective function in Problem (4) forces neighboring data in a same class to be close in the low dimensional tensor subspace;", "startOffset": 146, "endOffset": 167}, {"referenceID": 32, "context": "2) LMLRTA can also be considered as a reformulation of tensor marginal Fisher analysis (TMFA) (Yan et al., 2007).", "startOffset": 94, "endOffset": 112}, {"referenceID": 30, "context": "However, TMFA is not guaranteed to converge to a local optimum of the optimization problem (Wang et al., 2007), but LMLRTA is guaranteed as proved in Section 3.", "startOffset": 91, "endOffset": 110}, {"referenceID": 31, "context": "3) For Problem (4), we can consider it as a variant of the Large Margin Nearest Neighbor (LMNN) algorithm (Weinberger et al., 2005) for distance metric learning in tensor space.", "startOffset": 106, "endOffset": 131}, {"referenceID": 16, "context": "5) Unlike deep neural network models (Hinton et al., 2006; Bengio et al., 2006; Vincent et al., 2010), which simulate human brain\u2019s hierarchical structure, LMLRTA mimics the way of human perception.", "startOffset": 37, "endOffset": 101}, {"referenceID": 2, "context": "5) Unlike deep neural network models (Hinton et al., 2006; Bengio et al., 2006; Vincent et al., 2010), which simulate human brain\u2019s hierarchical structure, LMLRTA mimics the way of human perception.", "startOffset": 37, "endOffset": 101}, {"referenceID": 29, "context": "5) Unlike deep neural network models (Hinton et al., 2006; Bengio et al., 2006; Vincent et al., 2010), which simulate human brain\u2019s hierarchical structure, LMLRTA mimics the way of human perception.", "startOffset": 37, "endOffset": 101}, {"referenceID": 30, "context": "Similar to previous approaches on tensor dimensionality reduction (Dai and Yeung, 2006; Wang et al., 2007), here we solve Problem (4) using an iterative optimization algorithm.", "startOffset": 66, "endOffset": 106}, {"referenceID": 14, "context": "Here, for each Wl, problem (4) is a semi-definite programming problem, which can be solved using off-the-shelf algorithms, such as SeDuMi1 and CVX (Grant and Boyd, 2008).", "startOffset": 147, "endOffset": 169}, {"referenceID": 10, "context": "This is mainly based on the fact that the kernels of the Gabor filters resemble the receptive field profiles of the mammalian cortical simple cells (Daugman, 1988), which enhances our learning model to better mimic the way of human perception.", "startOffset": 148, "endOffset": 163}, {"referenceID": 12, "context": "To show the advantage of our proposed method, LMLRTA, we compared it with two classic vector representation-based dimensionality reduction approaches \u2013 linear discriminant analysis (LDA) (Fisher, 1936) and marginal Fisher analysis (MFA) (Yan et al.", "startOffset": 187, "endOffset": 201}, {"referenceID": 32, "context": "To show the advantage of our proposed method, LMLRTA, we compared it with two classic vector representation-based dimensionality reduction approaches \u2013 linear discriminant analysis (LDA) (Fisher, 1936) and marginal Fisher analysis (MFA) (Yan et al., 2007), one deep neural networks model called stacked denoising autoencoder (SDAE) (Vincent et al.", "startOffset": 237, "endOffset": 255}, {"referenceID": 29, "context": ", 2007), one deep neural networks model called stacked denoising autoencoder (SDAE) (Vincent et al., 2010), and two state-of-the-art tensor dimensionality reduction methods \u2013 convergent multi-linear discriminant analysis (CMDA) and convergent tensor margin Fisher analysis (CTMFA) (Wang et al.", "startOffset": 84, "endOffset": 106}, {"referenceID": 30, "context": ", 2010), and two state-of-the-art tensor dimensionality reduction methods \u2013 convergent multi-linear discriminant analysis (CMDA) and convergent tensor margin Fisher analysis (CTMFA) (Wang et al., 2007).", "startOffset": 182, "endOffset": 201}, {"referenceID": 8, "context": "In future work, we attempt to extend LMLRTA to the scenarios of transfer learning (Pan and Yang, 2010) and active learning (Cohn et al., 1994), to simulate the way how human brain transfers knowledge from some source domains to a target domain, and the way how human brain actively generates questions and learns knowledge.", "startOffset": 123, "endOffset": 142}, {"referenceID": 20, "context": "Furthermore, we plan to combine LMLRTA with deep neural networks (LeCun et al., 2001) and non-negative matrix factorization", "startOffset": 65, "endOffset": 85}, {"referenceID": 21, "context": "models (Lee and Seung, 1999), to solve challenging large scale problems.", "startOffset": 7, "endOffset": 28}], "year": 2013, "abstractText": "Other than vector representations, the direct objects of human cognition are generally high-order tensors, such as 2D images and 3D textures. From this fact, two interesting questions naturally arise: How does the human brain represent these tensor perceptions in a \u201cmanifold\u201d way, and how can they be recognized on the \u201cmanifold\u201d? In this paper, we present a supervised model to learn the intrinsic structure of the tensors embedded in a high dimensional Euclidean space. With the fixed point continuation procedures, our model automatically and jointly discovers the optimal dimensionality and the repar X iv :1 30 6. 26 63 v1 [ cs .L G ] 1 1 Ju n 20 13 resentations of the low dimensional embeddings. This makes it an effective simulation of the cognitive process of human brain. Furthermore, the generalization of our model based on similarity between the learned low dimensional embeddings can be viewed as counterpart of recognition of human brain. Experiments on applications for object recognition and face recognition demonstrate the superiority of our proposed model over state-of-the-art approaches.", "creator": "LaTeX with hyperref package"}}}